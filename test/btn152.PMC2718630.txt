BIOINFORMATICS Vol.24 ISMB 2008, pages i147 i155 doi:10.1093/bioinformatics/btn152 Alignment and classiﬁcation of time series gene expression in clinical studies Tien-ho Lin1, Naftali Kaminski2 and Ziv Bar-Joseph1, 1School of Computer Science, Carnegie Mellon University and 2Simmons Center for Interstitial Lung Disease, University of Pittsburgh Medical School, Pittsburgh, PA 15213, USA ABSTRACT Motivation: Classiﬁcation of tissues using static gene-expression data has received considerable attention.
Recently, a growing number of expression datasets are measured as a time series.
Methods that are speciﬁcally designed for this temporal data can both utilize its unique features (temporal evolution of proﬁles) and address its unique challenges (different response rates of patients in the same class). Results: We present a method that utilizes hidden Markov models (HMMs) for the classiﬁcation task.
We use HMMs with less states than time points leading to an alignment of the different patient response rates.
To focus on the differences between the two classes we develop a discriminative HMM classiﬁer. Unlike the traditional generative HMM, discriminative HMM can use examples from both classes when learning the model for a speciﬁc class.
We have tested our method on both simulated and real time series expression data.
As we show, our method improves upon prior methods and can suggest markers for speciﬁc disease and response stages that are not found when using traditional classiﬁers. Availability: Matlab implementation is available from http://www.cs.cmu.edu/ thlin/tram/ Contact: zivbj@cs.cmu.edu 1 INTRODUCTION Several methods have been developed for classifying tissues using gene-expression data.
Starting with the seminal work of Golub et al. (1999) which used ideal proﬁles to classify acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) cancer samples, researchers have been developing and applying classiﬁcation methods to a wide range of diseases using expression data (Alizadeh et al., 2000 Baranzini et al., 2005). Recently, some of these methods have been commercialized, creating expression- based diagnostic and treatment suggestion tools (van t Veer et al., 2002). To date most of the research on classifying expression data focused on static (snapshot) datasets. While these are appropriate for many cases (most notably diagnostics) they are less appropriate for longer term follow-up.
Consider for example transplant patients.
For these patients physicians need to determine if and when their body starts rejecting the new organ in order to start treatment with immunosuppressing drugs.
Another example are patients who have been admitted to the hospital following an accident and are monitored for organ failures.
In these and other scenarios classiﬁcation is improved if one can take into account not only the current state of the patient but also its past state and the changes that  To whom correspondence should be addressed.
for time series clinical challenge A unique have occurred over time.
Indeed, large scale efforts are under way to collect and analyze such time series expression datasets so that they can be better utilized in clinical settings (Inﬂammation, 2008). expression classiﬁcation is to account for the patient-speciﬁc rate of disease development or treatment response (Kaminski and Bar-Joseph, 2007 Sterrenburg et al., 2004 Weinstock-Guttman et al., 2003). While the overall trajectory of the expression proﬁle may be similar between patients, different patients may progress at different speeds.
Thus, a classiﬁer for these time series datasets should be able to take into account the varying response rates.
This makes methods that treat the input data as static, such as support vector machines (SVM) with default kernels, less appropriate for this task.
To address these issue we present a method that can both classify the time series expression datasets and account for the differences in patient rates.
Our method uses hidden Markov models (HMMs) to represent the expression proﬁles of the two classes.
The HMMs we use contain fewer states for each class than the actual number of time points.
Using the probabilistic transitions between these states results in alignment of patients to the model and can account for the varying rates of progress.
We further extend the model to learn discriminative HMMs (Gopalakrishnan et al., 1991 Normandin et al., 1994 Woodland and Povey, 2002) in which the parameters are chosen to maximize the difference between the two classes.
Finally, we use feature selection to reduce the model complexity.
The two resulting models are then used to classify new time series expression data based on the likelihood of the data given in the models.
We have tested our method on simulated and real time-series expression datasets. For all cases we show that our HMM-based classiﬁers achieve large improvements over methods that have been suggested in the past for this task.
1.1 Related work There has been a lot of previous work on classifying static expression datasets. In addition to the work of Golub et al. (1999) mentioned above, many other classiﬁers including SVMs (Furey et al., 2000), principle component analysis (Bicciato et al., 2003) and K nearest neighbor (KNN Nutt et al., 2003) were suggested for this task.
More recently a few methods for classifying time series expression data were presented.
Baranzini et al. (2005) used an exhaustive search strategy to identify genes for a Bayes classiﬁer of time series expression data of multiple sclerosis (MS) patients response to interferon-β (IFNβ). While their goal was to classify time series data, their method only used the ﬁrst time point, so it could not take advantage of the full set of data available.
Borgwardt et al. (2006) used SVM with specialized kernels that accounted for temporal data.
A Kalman Filter was trained generatively for each  2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[19:31 18/6/03 Bioinformatics-btn152.tex] Page: i147 i147 i155  T.-h.Lin et al. class, and then the kernel was computed using the trained parameters of the two Kalman Filters.
While their methods utilized the entire time series it does not account for the rate differences discussed above, which may lead to inaccuracies.
A number of methods have been suggested for aligning time series datasets to overcome these rate differences.
These either rely on dynamic programming (Aach and Church, 2001) or on continuous representation of expression data (Bar-Joseph et al., 2003). However, these methods were primarily developed for clustering expression data.
It is not clear how to use these methods for classiﬁcation of time series data.
HMMs have been used to cluster time series expression data (Schliep et al., 2005), to model dynamic regulatory networks (Ernst et al., 2007), to align Liquid Chromatography Mass Spectrometry time series (Listgarten et al., 2004), and to identify timing differences in time series expression data (Yoneya and Mamitsuka, 2007). However, we are not aware of any method that used these models for classifying time series data.
Interestingly, each of these different applications used a different number of states w.r.t. the number of time points measured.
Schliep et al. s clustering model used fewer states than time points.
Ernst et al. s regulatory networks model used the same number of states as time points and Listgarten et al. used more states than time points for modeling Mass Spectrometry time series data.
In this article we have investigated all three options as we discuss in Section 4.
On the other hand, Yoneya and Mamitsuka s timing difference model used two type of states.
Control states have self-loops similar to Schliep et al. s model feature states can jump over control states, similar to Listgarten et al. s model.
This special state space is designed to infer the ordering between conditions, but not designed to model a gene expression proﬁle. HMMs are typically trained generatively using maximum likelihood estimation (MLE). For classiﬁcation tasks, generative training only utilizes the positive examples for each class, while discriminative training can utilize positive and negative examples.
Here we extended a discriminative training method that was originally developed for speech recognition: the maximum mutual information estimate (MMIE Gopalakrishnan et al., 1991). We discuss this method in more detail in the following Section.
2 HMMS FOR ALIGNING TIME SERIES GENE EXPRESSION To account for different and varying response rate of each patient, we use HMMs. Using HMMs we align a patient s time series gene expression to a common proﬁle. For a classiﬁcation task we generate two such HMMs, one for good responders and one for poor responders. Conceptually, a hidden state in our HMM correspond to a phase in the treatment response.
Since different patients progress at different rates, they enter these states at different times and may stay in one state for more than one time point.
The emission distribution of gene expression in each state is modeled by a multivariate Gaussian distribution whose dimension equals the number of genes used by the classiﬁer. To avoid overﬁtting, the covariance matrix is assumed to be diagonal.
We considered three state space topologies, all being left-right models that conform to the temporal ordering as shown in Figure 1.
The ﬁrst topology is a left-right model with self-loops, i.e. a state indexed i has transitions to i or i+1. The second is a simple left-right model i148 (a) Loop HMM (b) Equal-length HMM (c) Jump HMM Fig.1.
Three HMM topologies considered in this article.
All three are left- right models that conform to temporal ordering.
(a): left-right models with self-loops (less states than time points) (b): left-right models without loops or jumps (equal number of states and time points) and (c): left-right models with jumps (more states than time points). without loops or jumps, so each state exactly matches one time point.
The third is a left-right model with jumps, i.e. a state i has transitions to i+ 1,i+ 2,...,i+ J, where the maximum jump step J is a ﬁxed constant.
In the ﬁrst topology the number of states is less than the number of time points, while in the second topology these two are equal, and in the third topology the number of states is larger than the number of time points.
The ﬁrst and third topologies can be used to align patients by modifying their transition probabilities based on the observed expression data.
We will use the following notations.
We are given the time the expression of G genes for each patient at T time points, series gene expressions of K patients, O1,O2,...,OK. We measure represented by a G-dimensional multivariate time series Ok = (Ok1,Ok2,...,OkT ). For gene selection, Oktg denotes the expression of gene g at time t for patient k. The class patient k belongs to is denoted as ck, ck  1,2.
For notational simplicity, we assume the patients are from two classes, which is true for many clinical patient classiﬁcation tasks.
However, the algorithm discussed below is applicable for multiclass classiﬁcation as well.
A HMM λ(m) with multivariate Gaussian emission probability is trained for each class m, m 1,2.
Let , µ(m)  , σ (m) λ(m)= (cid:2) (cid:1)   ,  a(m) ij j j j j ij where a(m)  is the transition probability from state i to j and  are mean and SD for the Gaussian distribution of , σ (m) µ(m) state j.
The mean and SD of gene g in state j is denoted as µ(m) jg and σ (m) jg , when it is necessary to specify which gene.
We ﬁx the ﬁrst state for each topology to represent pre-treatment levels.
The hidden states of time series Ok are denoted as xk =(xk1,xk2,..., xkT ). We also use the standard notation for the sufﬁcient statistics of HMM: γ (m) (j) is the posterior probability of state j at time t kt of observation Ok, conditioned on the model λ(m). ξ (m) (i,j) is kt the probability of a transition from state i to state j at time t of observation Ok, conditioned on the model λ(m). 2.1 Generative training of HMM Given labeled expression data we can learn the parameters of a HMM using the Baum Welch algorithm for each of the three [19:31 18/6/03 Bioinformatics-btn152.tex] Page: i148 i147 i155  topologies mentioned above.
The resulting models are generative as training only utilize data from expression experiments of patients which belong to that class (good or poor responders). Using such a generative model we can classify a new dataset by building one model for each class.
Class assignment is based on maximum conditional likelihood.
It has been shown that MLE is optimal if the true model is indeed the assumed HMM and there is inﬁnite data (Nadas, 1983). Unfortunately, it is unlikely that the data is truly generated by a HMM.
Furthermore, the number of training examples for clinical time series classiﬁcation is very small.
Thus, it would be beneﬁcial if we could take advantage of both positive and negative data when building the models for each of the classes.
This would allow the models to focus on the differences between the two sets of expression datasets, rather than on the most visible features (which could be the same for all groups of patients, for example stress response which is a common feature in disease response but may not be a useful feature for discriminating good and bad responders). 2.2 Discriminative training of HMM To model the difference between positive and negative examples, we need to optimize a discriminative criteria, such as the conditional likelihood of the true classes given the data.
This criteria is also called conditional maximum likelihood estimation (CMLE), often used in discriminative training methods, e.g. logistic regression.
Here we use the MMIE technique which was originally developed for speech recognition, to trains HMMs in order to optimize this discriminative criteria.
The standard training algorithm for MMIE is an extended version of the Baum Welch algorithm (Gopalakrishnan et al., 1991). However, unlike generative training, the HMMs for both classes are learned concurrently and parameters in one of the models are affected by the parameters estimated for the other model.
The MMIE objective function can be written as, FMMIE= (cid:3) log k (cid:4) (cid:5) (cid:4) (cid:5) Ok λ(ck) p λ(ck) p p(Ok λ(1))p(λ(1))+p(Ok λ(2))p(λ(2)) (1) Where ck is the class (1 or 2) of patient k. That is, our goal is to ﬁnd parameters that will maximize the probability ratio of the good and poor responders models.
The denominator in Equation (1) will be represented by the likelihood of a combined HMM, λden, such that p(Ok λden)= p(Ok λ(1))p(λ(1))+p(Ok λ(2))p(λ(2)) λden is called the denominator model.
In practice we learn new models for p(λ(1)) and p(λ(2)) in each iteration and use them to revise p(Ok λden). Thus the denominator model is constructed by combining the state space of the two HMMs λ(1) and λ(2), and assigning initial probability to the beginning states according to the priors p(λ(1)) and p(λ(2)). During training, the denominator model is constructed in each iteration after the HMMs λ(1) and λ(2) are updated.
While updating one class, the HMM for that class is called the numerator model.
We ﬁrst discuss the E-step in MMIE which involves the estimation of expected counts summarizing the current parameter settings.
This estimation is similar to the ones in the Baum Welch algorithm and the counts are collected for both the numerator and denominator models.
For example, when λ(1) is being updated, γ num is the j Alignment and classiﬁcation of time series gene expression ij expected count of state j in the positive examples according to the numerator model λ(1) ξ num is the expected count of transition from state i to state j in the positive examples according to λ(1). θ num (O) are weighted sums of expression values in the positive j examples, and θ num (O2) are weighted sums of squared values, where the weightings are the posterior probability of state j.
Similarly, γ den (O2) are expected counts in all examples according to the denominator model λden. The calculations when updating λ(2) is similar.
These expected counts are obtained from the dynamic matrices of the forward backward algorithm.
Formally, (O) and θ den ,θ den ,ξ den ij j j j j = γ num j (O)= θ num j (O2)= θ num j = ξ num ij (cid:3) k ck=1 (cid:3) k ck=1 (cid:3) k ck=1 (cid:3) k ck=1 t (cid:3) (cid:3) (cid:3) (cid:3) t t t γ (1) kt (j), γ den j = γ (1) kt (j)Okt , θ den j γ (1) kt (j)O2 kt , θ den j ξ (1) kt (i,j), ξ den ij = (cid:3) (cid:3) k t (O)= (O2)= (cid:3) (cid:3) k t γ den kt (j) (cid:3) (cid:3) (cid:3) (cid:3) k t k t ξ den kt (i,j) γ den kt (j)Okt γ den kt (j)O2 kt j (O)/γ num The major difference between generative and discriminative HMMs is in the M-step.
MLE for the generative model only updates ˆµj = the parameters in the direction of positive examples, e.g. θ num. In contrast, MMIE updates the parameters by j moving them toward the positive examples and away from the denominator model.
This leads to greater focus on emission and transition probabilities that differ between the two models (either across states or at speciﬁc states for each gene) contributing to increased discrimination between the two models.
However, such subtraction may generate negative transition probabilities or negative variances in emission probabilities.
A smoothing constant needs to be added to both the numerator terms and the denominator terms to avoid this.
Hence the reestimation formulas of MMIE are, j (O) θ den j γ den γ num j (O2) θ den j γ num j ξ den ij ij(cid:4) ξ den (O)+DE µj +DE (O2)+DE(σ 2 γ den +DT aij ij(cid:4) +DT aij(cid:4) +DE j j ˆµj = θ num j = θ num j ˆσ 2 j ˆaij = ξ num ij ξ num (cid:6) j(cid:4) +µ2 j )  ˆµ2 j (2) (3) (4) where DE and DT are smoothing constants for emission and transition probabilities, respectively.
It has been shown that Equation (2) (4) will converge to a local maximum of the MMIE objective function, given sufﬁciently large smoothing constants DE and DT (Normandin et al., 1994). However, it is not known how large they must be for the objective function to converge.
If the smoothing constants are too small, update may not increase the (discriminative) objective function, but if they are too large, convergence will be too slow.
A useful lower bound is the minimal values that ensures that the HMM parameters remain valid.
Empirically, setting the smoothing constants to twice the lower bound leads to fast convergence (Woodland and Povey, 2002). Thus i149 [19:31 18/6/03 Bioinformatics-btn152.tex] Page: i149 i147 i155  T.-h.Lin et al. we set DE to twice the minimal value that makes all variances ˆσ 2 j positive DT is set to twice the minimal value that makes all transition probabilities ˆaij positive.
See Appendix A for details on how these values can be computed.
The HMM parameters are updated by a weighted average of the previous parameters and the reestimations. We follow previous works and set the learning rate, the weight of reestimations, as the error rate (Normandin et al., 1994). Hence the learning rate is larger in the beginning and smaller when nearing convergence.
In each selection step, genes are ranked by the contribution dg, and the gene with lowest score is eliminated.
Following the elimination step, new HMMs are trained using the remaining genes and the gene selection step is repeated.
In order to determine the ﬁnal number of selected genes we use internal cross-validation within the training data.
Note that internal cross-validation does not utilize the test data in any way.
The HMM-RFE algorithm is summarized in the following procedure: 3 GENE SELECTION FOR TIME SERIES EXPRESSION CLASSIFICATION Gene selection is critical in clinical gene expression classiﬁcation for several reasons.
First, the number of patients (data points) is small compared to the number of genes (features), resulting in overﬁtting. It is expected that restricting to a subset of relevant genes will improve classiﬁcation accuracy.
Second, a small subset of genes that discriminate between the classes can lead to biomarker discovery.
The selected genes can be further examined by more experiments to ﬁnd out the causal factors of different response to a treatment.
We consider the problem of gene selection as a feature selection problem and will use the terms gene and features interchangeably.
There are two primary approaches for feature selection the wrapper approach and the ﬁlter approach (Xing, 2002). The wrapper approach evaluates the classiﬁer on different feature subset, and searches in the space of all possible feature subsets using the speciﬁc classiﬁcation strategy.
The ﬁlter approach does not rely on the underlying classiﬁer, but instead uses a simpler criteria to ﬁlter out irrelevant features.
Typically the ﬁlter approach is faster, while the wrapper approach can ﬁt the speciﬁc need of a classiﬁer and obtain better performance.
In pursuit of higher classiﬁcation accuracy, the feature selection method we used here is a wrapper method.
We used a backward stepwise feature selection method that utilizes the alignment to the HMM proﬁles based on recursive feature elimination (RFE) algorithm, termed HMM RFE (Guyon et al., 2002). The basic procedure of RFE is as follows: train the classiﬁer, eliminate the feature whose contribution to the discrimination is minimal, and repeat iteratively until the stopping criteria is met.
It is also possible to eliminate several features in one step, especially when the number of features is large.
To estimate the contribution to discrimination of a speciﬁc gene, we note that since the covariance matrix is diagonal, gene-expression levels are independent given the hidden states.
Thus, if the states are known, the likelihood can be decomposed into terms involving each gene separately.
However, such a decomposition does not exist when the hidden states are unknown.
Instead we use a heuristic to approximate this decomposition.
(cid:2) We deﬁne the contribution to log odds of a gene g, dg, as (cid:2) ( 1)δ(ck=1)log (cid:3) (cid:1) (cid:1) dg= (cid:6) kt (j)N j γ (1) (cid:6) kt (j)N j γ (2) Oktg µ(1) Oktg µ(2) jg jg ,σ (1) jg ,σ (2) jg k,t (5) where δ(ck =1) is 1 if ck =1 and 0 otherwise.
See Appendix B for a detailed derivation.
Brieﬂy, the equation above uses an estimate of the states (γ (1) kt (j)) to compute the discriminative contribution of genes for the two classes.
kt (j) and γ (2) 1.
Given G genes, deﬁne gene sets with a decreasing number of genes, G= G0  G1  G2    GN , such that Gi genes are selected at the i-th iteration.
Initially, the active gene set includes all genes, and i=0. 2.
At i-th selection, train the HMMs λ(1) and λ(2) using genes in the active gene set.
3.
Calculate the discrimination score dg for each gene g, using Equation (5). Select Gi genes with highest scores.
4.
Record the cross-validation accuracy of the active gene set.
5.
Set i i+1, repeat Step 2 until i= N 1.
6.
Choose the optimal gene number G leading to the highest cross-validation accuracy.
Although we use internal cross-validation to determine the optimal gene set, it is not used in gene ranking.
The reason is computational.
Internal cross-validation for genes would increase the complexity by a factor of G (total number of genes) which could be a substantial increase for microarray expression data measuring thousands of genes.
4 RESULTS We ﬁrst tested our method on simulated data.
Next, we applied our method to a clinical dataset measuring MS patients response to IFNβ, one of the most common treatments to control MS. 4.1 Simulated dataset The expression of genes in response to treatments often follows a bifurcating pattern diverging as time progresses (Ernst et al., 2007). This is also the case in clinical settings, as can be seen in Figure 2.
In that ﬁgure we plot the average expressions of four genes in MS patients treated with IFNβ (Baranzini et al., 2005). The patients are divided into two groups, good and poor responders (red and green curves, respectively). As the ﬁgure indicates, while these genes display similar levels at the early time points they diverge at the later time points.
A classiﬁer that only utilizes the ﬁrst time point is likely to perform much worse when compared to a classiﬁer that utilizes the entire time series.
To generate the simulated data we have also tried to mimic this type of expression pattern as we describe below.
We generated expression proﬁles for 100 patients.
Of these, 50 patients were in class 1 ( good responders ) and 50 in class 2 ( poor responders ). 100 genes were measured for each patients, with a maximum of 8 time points per patient.
For each gene g, we generated the Class 1 response proﬁle by randomly selecting a segment of a sine wave, of length 1.5π between 0 to 4π. Denote this proﬁle as a function, f (1) g (t). We selected 10 out of the 100 genes to be differential, the other 90 where assigned the same values for Class 2.
For differential genes, the gene-expression proﬁle of poor responders i150 [19:31 18/6/03 Bioinformatics-btn152.tex] Page: i150 i147 i155  Alignment and classiﬁcation of time series gene expression IFN gRa 4.25 4.2 4.15 4.1 4.05 l e v e l  i n o s s e r p x e   g o L 4 0 3 6 9 12 15 18 21 24 l e v e l  i n o s s e r p x e   g o L 4.2 4.1 4 3.9 3.8 STAT2 0 3 6 9 12 15 18 21 24 4.6 4.4 4.2 4 3.8 3.6 4.2 4 3.8 3.6 3.4 Simulated time series classification IBIS Linear SVM Loop HMM Loop HMM, discriminative Equal length HMM Jump HMM STAT1 0 3 6 9 12 15 18 21 24 MX1 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 y c a r u c c A Good responders Poor responders 0 3 6 9 12 15 18 21 24 0.4 1 2 3 4 5 Time points 6 7 8 Time (month) Time (month) Fig.2.
Averaged log expression of good and poor responders for four bifurcating genes.
Expression levels are absolute difference and not log ratios to the ﬁrst time point.
f (2) g (t) is the good responders proﬁle curve plus a piecewise linear function, g (t)= f (1) g (t)+ fg(t), f (2) fg(t)= ag max(t bg,0) where the gradient ag and the offset bg are gene-speciﬁc parameters.
ag is +5 or 5, and bg is uniformly selected at random between 0.1 and 0.3.
After the proﬁles are generated, we simulate patient-speciﬁc response rate by randomly choosing a scaling value sk between 0.5 to 1.5 for patient k. sk is used to transform the time series for patient k by stretching or shrinking the curves.
Thus, the time series proﬁle for each genes of patient k is the linearly scaled proﬁle time series.
Finally, we add Gaussian noise, with mean 0 and gene-speciﬁc variance σ 2 g.
Formally, g (skt)+ , Oktg= f (ck)  N (0,σg), sk Uniform(0.5,1.5) We tried a number of different values for σ 2 and all resulted in similar performance.
g based on real datasets We compared our HMM-based classiﬁcation with two baseline classiﬁers: linear SVM (default parameters of SVM light is used), and the Integrated Bayesian Inference System (IBIS) method of Baranzini et al. (2005). IBIS only uses the ﬁrst time point as we discussed in Section 1.1.
We note that we have tried to obtain the code for the Kalman ﬁlter SVM of Borgwardt et al. (2006) for direct comparison.
Unfortunately, the code is not available online.
Despite several e-mail requests we were unable to obtain their code and we thus cannot present direct comparison of the two methods.
Fig.3.
Classiﬁcation accuracy of simulated data, based on ﬁve random permutations and 4-fold cross-validation (20 different training testing splits). For the loop HMM, 2 to 4 time points use 1 state, 5 to 7 time points use 2 states, and 8 time points use 3 states.
For the jump HMM, 4, 5, 6, 8, 10, 10 and 12 states are used from 2 to 8 time points.
For both loop and jump HMM, 10 random initialization are carried out and the initialization with the highest likelihood is chosen.
Classiﬁcation accuracy of different methods are shown in Figure 3.
Due to large amount of noise and limited information at earlier time points, classiﬁcation using data from these points is close to random, causing difﬁculties for IBIS.
With more time points, HMMs provide satisfactory results.
HMMs using the three topologies all outperform SVM from 6 to 8 time points.
Overall, the loop model results in highest accuracy with more time points.
For the loop HMM, we performed discriminative training on the HMMs trained generatively after gene selection.
Although it is possible to incorporate discriminative training in HMM RFE, the computation would be much heavier because discriminative training requires more iterations (e.g. 500 iterations) to converge comparing to generative training (about 20 iterations). As can be seen, accuracy is higher except for 2 time points where classiﬁcation is close to random.
Hence discriminative training can further improve the performance as it utilize both positive and negative data.
We also veriﬁed whether gene selection found the true differential genes.
Since different splits results in different gene selection, we listed the median number of selected genes, and veriﬁed the correctness of overlapping genes: genes selected in 90% of the splits, in Table 1.
Note that all overlapping genes are correct after 5 time points, and the number of selected genes is small after 6 time points, especially when using 7 and 8 time points resulting in better accuracy.
4.2 MS dataset We next tested our model using a clinical expression dataset. This dataset contains time series expression data for 70 genes in 52 MS patients treated with IFNβ Baranzini et al. (2005). Of the 52 patients, i151 [19:31 18/6/03 Bioinformatics-btn152.tex] Page: i151 i147 i155  T.-h.Lin et al. Table 1.
Selected genes in simulated data MS IFN-β response classification Time Accuracy Median Number of overlapping genes Precision of selected genes (%) (%) 44 47 56 58 73 87 86 2 3 4 5 6 7 8 19 18 16 17 7 2 4 1 2 2 2 2 1 2 0 0 0 100 100 100 100 Time is the number of time points used to construct the HMM model.
Accuracy is the best accuracy using these time points (loop HMM using discriminative training). Median is the median number of selected genes.
Number of overlapping genes presents the number of genes selected in at least 90% of the training testing splits.
Precision is the percent of overlapping genes that were indeed part of the 10 assigned differentially expressed genes.
33 responded well to the treatment ( good responders ) and 19 did not respond well ( poor responders ). The 70 genes included were preselected by experts based on relevance to MS. For each patient there are 7 time points, measured every 3 month in the ﬁrst year following treatment and every 6 month in the second year.
Some patients miss certain measurements, especially at the 7th time point, causing an entire measurement to be a missing value.
As we did with the simulated data, we compared our method to the original IBIS algorithm and to linear SVM. We note again that we were unable to compare our method to the Kalman ﬁlter SVM of Borgwardt et al. (2006) since we could not obtain their code.
The classiﬁcation accuracy is evaluated using 4-fold cross- validation.
For each possible number of time points (2, 3, etc.) we train a new instance of each potential classiﬁcation model (SVM, HMMs, etc.). Figure 4 presents the results.
In that ﬁgure we plot the accuracy versus.
the number of time points for the different classiﬁers. is similar As can be seen, HMM with equal number of states and time points (equivalent to Gaussian Naive Bayes classiﬁer) achieves classiﬁcation accuracy that to SVM with default parameters (tuning of SVM parameters improves the accuracy, but it is still signiﬁcantly lower than the other HMMs). In contrast, the other two HMMs that allow for alignment perform much better on this clinical dataset, indicating the alignment is indeed an important issue for time series classiﬁcation. The loop HMM model performs better than the jump model from 3 time points a possible explanation is that fewer emission parameters reduced overﬁtting. The best results when using all data (7 points) were obtained by the loop HMM after discriminative training (85%). In addition, when using 2 or 3 time points the discriminative HMM outperformed the generative model.
However, for the other sets of time points the two models (discriminative and generative) achieved similar results.
It is important to note that we do not use the test data in gene selection during training, so that the evaluation would be closer to the performance on new data.
The results presented in Figure 4 differ from the results in the original IBIS paper (Baranzini et al., 2005) even though we have followed exactly the same (and simple) procedure as described in that paper.
We believe that the reason for this discrepancy is that, while exhaustive search of all triplets and full covariance matrix in IBIS gives very good results on the i152 IBIS Linear SVM Loop HMM Loop HMM, discriminative Equal length HMM Jump HMM 0.86 0.84 0.82 y c a r u c c A 0.8 0.78 0.76 0.74 0 3 6 9 12 15 18 21 24 Time (month) Fig.4.
Classiﬁcation accuracy of MS patients response to IFNβ, based on 5 random permutations and 4-fold cross validation (20 different training- testing splits). For the loop HMM, 2 and 3 time points use 1 state, 4 to 6 time points use 2 states, and 7 time points use 4 states.
For the jump HMM, 3, 4, 7, 8, 10, 12 states are used from 2 to 7 time points.
For both loop and jump HMM, 10 random initialization were carried out and the initialization resulting in the highest likelihood was selected.
training data, it may lead to overﬁtting of the validation accuracy, which becomes much higher than test accuracy.
4.3 Selected genes and the advantages of patient alignment We next examined the selected genes for the different classiﬁers (trained with different number of time points). Table 2 lists the selected genes for models constructed from the different sets of time points.
Again we only list genes selected in at least 90% of the training testing splits.
With more time points the classiﬁers stabilize between splits leading to more selected genes.
We compared our list to a previous list of 12 genes selected by Baranzini et al. (2005) based on expression values prior to treatment (ﬁrst time point). Caspase 10 and Caspase 3 which were also listed in the original paper, are almost always selected regardless of how many time points are being used.
However Jak2, IL12Rb2 and RAIDD are only selected using more time points, and are not on the list of 12 genes in that paper.
These uniquely selected genes are due to the ability of our method to consider later time points, as shown in Figure 5.
The left column in Figure 5 plots the mean and variance of gene expression at different time points.
Some genes like Jak2 differs in later time points more strongly between the two classes when compared to the ﬁrst time point.
For some genes, the divergence is visible only in the aligned expression models, shown in the right column of Figure 5.
To obtain the aligned expression, we use the Viterbi algorithm to align the time points of a patient to the most likely states of the HMM.
IL12Rb2 and RAIDD, for example, are more separated between classes in the aligned expression.
The large variances (and hence overlap) in unaligned time series could be due to poor responders entering the third state earlier or good responders staying in the second state longer, which is resolved after the alignment.
Note that [19:31 18/6/03 Bioinformatics-btn152.tex] Page: i152 i147 i155  Table 2.
Selected genes in MS dataset Time Accuracy Median Selected genes (%) 78 77 81 85 84 85 2 3 4 5 6 7 15 13.5 11.5 26 13.5 23.5 Caspase 3, Caspase 10, IL-4Ra Caspase 3, Caspase 10, Jak2 Caspase 10, Caspase 2, Jak2 Caspase 10, MAP3K1, IRF8, Caspase 3, Caspase 2, Jak2, IL-4Ra, IL12Rb2 Caspase 10, Caspase 3, Jak2, IRF4, Caspase 2 Caspase 10, Caspase 3, Jak2, IL-4Ra, MAP3K1, RAIDD, Caspase 2 See Table 1 for description of the time, accuracy and median columns.
Selected genes are genes selected in at least 90% of the training testing splits.
Accuracy is based on loop HMM using discriminative training.
the alignment is on the patient level, based on the expression of all genes.
To isolate the effect of alignment we applied a linear SVM to the alignment model determined by the HMM.
Unlike the regular SVM that uses the measured values, the alignment SVM uses the average expression of time points aligned to each of the HMM states.
As Figure 6 shows, such a classiﬁer leads to much higher accuracy than SVM based on unaligned expressions.
However, while this classiﬁer considers the alignment, it ignores the temporal ordering of the states which is why it is outperformed by discriminative HMM, at least in some cases.
These results highlight the importance of alignment when working with clinical expression data.
IL12RB2, complementary studies.
Some of the genes we uniquely identiﬁed are also validated by recent an important autoimmunity gene is expressed in activated T-cells and is a marker of TH1 inﬂammatory response.
Its consistent increase in poor responders suggests lack of response to treatment.
This becomes more evident as time goes by leading to maximal difference after a year (Fig. 5). A recent paper found that it was a good marker for lack of response to glatiramer acetate in MS (Grossman et al., 2007), and as our results indicate it might be a good marker to the IFNβ treatment.
Another genes we identiﬁed, JAK2, is phosphorylated by the activation of the IL12 receptor.
Again, this might cause a delayed response leading to stronger differences at later time points.
5 DISCUSSION A major challenge in classifying time series clinical expression data is the varying response rates of individuals.
In this article we propose the use of HMMs for this task.
HMMs can naturally model non- linear patient-speciﬁc response rates.
The hidden states represent the temporal clustering of gene expression, and can be interpreted as disease phases.
Transition probabilities allows different patients to progress at different rates.
To overcome the small number of training examples we have used discriminative HMMs. Unlike generative HMMs, discriminative HMMs can utilize both positive and negative examples when generating a model for each class.
Using both simulated data and clinical expression data of MS patients, we show that HMMs outperforms classiﬁers that do not take the temporal ordering into account.
We further compared three left- right HMM models: the loop model, the equal-length model, and the Alignment and classiﬁcation of time series gene expression Original expression 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 0 1   e s a p s a C 3   e s a p s a C 4.5 4 3.5 4.5 4 3.5 4.5 2 k a J 4 2 b R 2 1 L I 3.5 4.5 4 3.5 4.5 D D A R I 4 Aligned expression 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 4.5 4 3.5 4.5 4 3.5 4.5 4 3.5 4.5 4 3.5 4.5 4 3.5 1 2 4 5 3 Time points 6 7 Good responders Poor responders 3.5 1 2 3 States 4 Fig.5.
Mean and variance of expression proﬁles of unaligned and aligned genes selected for models using 5 or more time points.
Plots in the same row are for the same gene.
The right column presents, for each of the genes, the aligned expression proﬁles corresponding to the four states using the Viterbi algorithm.
Alignment is based on the best discriminative HMM of all training testing splits.
In the learned model, the selected genes go up in the second state and back to initial level in the third state.
The fourth state basically models outliers in the 6th and 7th time point, and hence transition probability into this state is small.
The overlap between classes on the second and third states of all the ﬁve genes is smaller after the alignment leading to better discrimination between poor and good responders. This is critical for the correct identiﬁcation of IL12Rb2 and RAIDD as two important features for later time points.
jump model.
Of these, the loop model performs best which is likely the result of the small training data for these types of experiments.
Discriminative HMMs improve upon generative HMMs in most, though not all, cases.
In addition to learning discriminative models we also carry out gene selection.
The selected genes in simulated datasets correctly contain the truly differential genes.
While we do not have the ground i153 [19:31 18/6/03 Bioinformatics-btn152.tex] Page: i153 i147 i155  Simulated time series classification Linear SVM Loop HMM, disc Loop HMM, disc + SVM T.-h.Lin et al. 1 0.9 0.8 0.7 0.6 0.5 y c a r u c c A 0.4 1 2 3 4 5 Time points 6 7 8 MS IFN-β response classification Linear SVM Loop HMM, disc Loop HMM, disc + SVM 0.88 0.86 y c a r u c c A 0.84 0.82 0.8 0.78 0.76 0.74 0 3 6 9 12 15 18 21 24 Time (month) Fig.6.
Applying SVM to the averaged expressions of aligned time points.
The alignments are obtained by running the Viterbi algorithm on the best discriminative HMM.
The SVM uses the linear kernel and default parameters.
truth for the MS dataset, many of the selected genes can be explained based on current knowledge of disease progression.
As more time series expression data accumulates we would like to test our method on additional types of response data.
We would also like to extend our model to better represent the interactions between genes.
The current diagonal covariance emission model ignores such interactions.
When more data becomes available, models that compute more covariance terms can be learned from data leading to better models and improved accuracy.
We are also interested in other clinical applications of our HMM, e.g. predicting rejection events for transplant patients.
Alignment of time series gene expression between group of genes or species could also be important in other biological experiments.
ACKNOWLEDGEMENTS This work was supported in part by NIH grant NO1 AI-5001 and NSF CAREER award 0448453 to ZBJ. Conﬂict of Interest: none declared.
i154 REFERENCES Aach,J. and Church,G.M. (2001) Aligning gene expression time series with time warping algorithms.
Bioinformatics, 17, 495 508.
Alizadeh,A. et al. (2000) Distinct types of diffuse large b-cell lymphoma identiﬁed by gene expression proﬁling. Science, 403, 503 510.
Bar-Joseph,Z. et al. (2003) Continuous representations of time series gene expression data.
J.Comput.Biol, 3 4, 341 356.
Baranzini,S.E. et al. (2005) Transcription-based prediction of response to IFNbeta using supervised computational methods.
PLoS Biol., 3, e2. Bicciato,S. et al. (2003) Pca disjoint models for multiclass cancer analysis using gene expression data.
Bioinformatics, 19, 571 578.
Borgwardt,K.M. et al. (2006) Class prediction from time series gene expression proﬁles using dynamical systems kernels.
In Proceedings of Paciﬁc. Symposium on Biocomputing (PSB), Maui, HI, pp.
547 558.
Ernst,J. et al. (2007) Reconstructing dynamic regulatory maps.
Mol. Syst. Biol., 3, 74.
Furey,T. et al. (2000) Support vector machine classiﬁcation and validation of cancer tissue samples using microarray expression data.
Bioinformatics, 16, 906 914.
Golub,T. et al. (1999) Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression monitoring.
Science, 286, 531 537.
Gopalakrishnan,P. et al. (1991) An inequality for rational functions with applications to some statistical estimation problems.
IEEE Trans.
Inf.
Theory, 37 107 113.
Grossman,I. et al. (2007) Pharmacogenetics of glatiramer acetate therapy for multiple sclerosis reveals drug-response markers.
Pharmacogenet. Genomics, 17, 657 666.
Guyon,I. et al. (2002) Gene selection for cancer classiﬁcation using support vector machines.
Mach.
Learn., 46, 389 422.
Inﬂammation (2008) Inﬂammation and the Host Response to Injury.
Available at: www.gluegrant.org (last accessed date 15 January 2008). Kaminski,N. and Bar-Joseph,Z. (2007) A patient-gene model for temporal expression proﬁles in clinical studies.
J. Comput. Biol., 14, 324 338.
Listgarten,J. et al. (2004) Multiple alignment of continuous time series.
Neural Information Processing Systems 17, MIT Press: Cambridge, MA, pp.
817 824.
Nadas,A. (1983) A decision theoretic formulation of a training problem in speech recognition and a comparison of training by unconditional versus conditional maximum likelihood.
IEEE Trans.
Acoust. Speech Signal Process., 31, 814 817.
Normandin,Y. et al. (1994) High-performance connected digit recognition using maximum mutual information estimation.
IEEE Trans.
Speech Audio Process., 2, 299 311.
Nutt,C. et al. (2003) Gene expression-based classiﬁcation of malignant gliomas than histological classiﬁcation. Cancer Res., correlates better with survival 63, 1602 1607.
Schliep,A.P et al. (2005) Analyzing gene expression time-courses.
IEEE/ACM Trans.
Comput. Biol.
Bioinform., 2, 179 193.
Sterrenburg,E. et al. (2004) Large-scale gene expression analysis of human skeletal myoblast differentiation.
Neuromuscul. Disord., 14, 507 518. van t Veer,L.J. et al. (2002) Gene expression proﬁling predicts clinical outcome of breast cancer.
Nature, 415, 530 536.
Weinstock-Guttman,B. et al. (2003) Genomic effects of IFN-beta in multiple sclerosis patients.
J.
Immunol., 171, 2694 2702.
Woodland,P. and Povey,D. (2002) Large scale discriminative training of hidden markov models for speech recognition.
Comput. Speech Lang., 16, 25 47.
Xing,E. (2002) Feature selection in microarray analysis Ch.6. In Berrar, D. et al. eds., A Practical Approach to Microarray Data Analysis., Kluwer Academic Publishers, London, pp.
110 131.
Yoneya,T. and Mamitsuka,H. (2007) A hidden markov model-based approach for identifying timing differences in gene expression under different experimental factors.
Bioinformatics, 23, 842 849.
APPENDIX A For discriminative training of HMM using MMIE, the smoothing constants are set to twice the minimal value that ensures the probabilities to be valid.
Here we show how to calculate the DT and DE smoothing constants for transition and emission probabilities, respectively.
By setting ˆaij in Equation (4) to be positive, DT can be calculated as (cid:8) (cid:7) DT =2max i,j (ξ num ij  ξ den ij ) 0, 1 aij [19:31 18/6/03 Bioinformatics-btn152.tex] Page: i154 i147 i155  By plugging Equation (2) into Equation (3) and setting ˆσ 2 positive, we have a quadratic inequality of DE, (O2)+DE(σ 2 +µ2 j ) γ den (cid:10)2 θ num j (cid:9) j j θ num j   to be j (O2) θ den j +DE γ num j (O) θ den (O)+DE µj j γ den +DE γ num j (cid:5)+(cid:4) j (cid:4) γ num j θ num j  γ den (cid:5)(cid:12) j  0 (cid:5) (O2) (O2) θ den j or (cid:11) (cid:4) + σ 2 j D2 E 2µj +(cid:4) (cid:4) +µ2 j ) (σ 2 j (O) θ den j (cid:5)(cid:4) θ num j γ den (O) θ den j j γ num j θ num j DE (O) (O2) θ den (cid:5)2 0 j θ num j (O) This quadratic inequality can be solved to obtain a lower bound, and DE is set to twice the lower bound.
APPENDIX B In gene selection, we need to estimate the contribution to discrimination of each gene.
Because the covariance matrix is diagonal, the gene expressions are independent given the hidden states.
That is, the probability of a time series gene expression Ok given a HMM λ(1) and the hidden states xk can be decomposed as, (cid:2) p(Ok xk ,λ(1))= Oktg µ(1) Oktg xkt (cid:5)= (cid:13) (cid:13) (cid:13) (cid:13) N (cid:1) (cid:4) p ,σ (1) jg jg t g t g What we need is to decompose the marginalized likelihood p(Ok λ(1)) into terms involving each gene only, q(1) p(Ok λ(1))= = (cid:14) (cid:3) (cid:13) (cid:13) (cid:13) xk t g t p(xkt xk,t 1) q(1) ktg(Oktg) (cid:4) ktg(Oktg): (cid:5)(cid:15) Oktg xkt (cid:13) p g Unfortunately, expressions levels for individual genes are independent once the hidden state are not known, so the not above decomposition does not exist.
We will use a heuristic (cid:5) (O2) log (cid:1) Ok λ(1) (cid:4) Ok λ(2) p p (cid:2) (cid:5)  (cid:3) (cid:3) g t log q(1) ktg q(2) ktg (cid:5) (cid:5) (cid:4) (cid:4) Oktg Oktg Alignment and classiﬁcation of time series gene expression to approximate this decomposition.
Since the hidden states are unknown, we approximate it by the posterior probabilities, γ (1) kt (j). We approximate the term q(1) ktg(Oktg) as the weighted average of the Gaussian emission probabilities, the weights being γ (1) kt (j): ktg(Oktg)= q(1) = kt (j)p(Oktg xkt = j) γ (1) Oktg µ(1) kt (j)N γ (1) (cid:1) jg ,σ (1) jg (cid:2) (cid:3) (cid:3) j j We can approximate the contribution of each gene to the log odds, Then the total log odds of the correct model versus the incorrect model can be expressed as the sum of contribution of each gene, dg, log (cid:3) k ck=1 (cid:3) = p (cid:1) Ok λ(1) (cid:4) Ok λ(2) (cid:2) (cid:5) + p ( 1)δ(ck=1) log (cid:3) (cid:3) k (cid:3) (cid:3) k   = g t dg (cid:1) Ok λ(2) (cid:4) Ok λ(1) (cid:2) (cid:5) p p log (cid:3) k ck=2 p(Ok λ(1)) p(Ok λ(2)) q(1) ktg(Oktg) q(2) ktg(Oktg) ( 1)δ(ck=1) log g where δ(ck =1) is 1 if ck =1 and 0 otherwise.
Thus we have contribution to log likelihood ratio of a gene dg, deﬁned as dg= = (cid:3) k,t (cid:3) k,t ( 1)δ(ck=1) log ( 1)δ(ck=1) log q(1) ktg(Oktg) q(2) ktg(Oktg) (cid:6) kt (j)N j γ (1) (cid:6) kt (j)N j γ (2) (cid:1) Oktg µ(1) (cid:1) Oktg µ(2) jg jg (cid:2) (cid:2) ,σ (1) jg ,σ (2) jg i155 [19:31 18/6/03 Bioinformatics-btn152.tex] Page: i155 i147 i155 