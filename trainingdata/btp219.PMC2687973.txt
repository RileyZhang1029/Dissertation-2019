BIOINFORMATICS  Vol.
25 ISMB 2009, pages i276 i280 doi:10.1093/bioinformatics/btp219  A framework to reﬁne particle clusters produced by EMAN Liya Fan1,2, Fa Zhang1, , Gongming Wang1,2 and Zhiyong Liu1,  1Institute of Computing Technology, Chinese Academy of Sciences and 2Graduate University of Chinese Academy of Sciences, Beijing, China  ABSTRACT Motivation: EMAN is one of the most popular software packages for single particle reconstruction.
But the particle clusters produced during its model reﬁning stage are of low qualities.
We attempt to reﬁne the particle clusters by more accurately determining orientations of particles, and thereby achieving higher resolutions of consequent 3D structures.
Results: A particle reclustering framework (PRF) is introduced, which consists of three components.
Each of them is responsible for one of the basic tasks of PRF: normalization, threshold determination and reclustering.
Our implementation is also described and proved to meet the constraints proposed by PRF.
Experiments revealed that our implementation improved resolutions of consequent structures for most cases, but only a little extra execution time was incurred.
Therefore, it is practical to incorporate PRF in EMAN to improve qualities of generated 3D structures.
Availability and Implementation: Implementation of our algorithm is available upon request from the authors.
Contact: fanliya@ict.ac.cn  zf@ncic.ac.cn  zyliu@ict.ac.cn  1 INTRODUCTION In many biological applications, it is required to determine 3D structures of protein macromolecules.
So far, several technologies have been applied to deal with this problem, like X-ray crystallography, nuclear magnetic resonance (NMR) and electron microscopy single particle reconstruction.
In recent years, electron microscopy single particle reconstruction has been gaining more and more popularities because it offers some advantages that are not available for other technologies.
For example, it preserves the natural states of protein molecules, and suitable for protein molecules larger than 200 kDa (Ludtke et al., 1999).
Today, some software packages have been widely used in practice for single particle reconstruction, like EMAN (Ludtke et al., 1999), SPIDER (Frank et al., 1996), IMIRS (Liang et al., 2002), and so on.
One of the major problems with them is that it is extremely time consuming to construct 3D structures by means of these tools (Scheres et al., 2007).
Sometimes, each experiment may take several months.
One solution to this problem is to parallelize these programs, and conduct the computation by high-performance parallel computers.
Some efforts have been made to address the problem in this way, like the parallel SPIDER program (Yang et al., 2007).
The other solution is to improve the algorithm and accelerate the convergent process of the algorithms.
As a result, fewer rounds of iteration are needed to get the desired resolution, or higher resolutions can be obtained within the same amount of time.
To whom correspondence should be addressed.
Projecting  Clustering  Reclustering  Constructing  Averaging  Fig.
1.
Flow of the model reﬁning stage of EMAN.
Each round of the iteration consists of four steps: projecting, clustering, averaging and constructing.
A new step called reclustering is added to the ﬂow by us.
This study explores the way to accelerate the convergent process of EMAN.
A particle reclustering framework (PRF) is introduced to improve the clusters produced by EMAN.
By processing by PRF, the orientations of particles can be determined more accurately, so that the consequent 3D structures will have higher resolutions.
The rest of this article is organized as follows.
Section 2 gives some general information about the algorithm of EMAN, and presents the problem.
In Section 3, the framework is described and analyzed in details.
Experimental results are shown in Section 4.
Finally, we summarize and conclude in Section 5.
2 PROBLEM DESCRIPTION For EMAN, three stages are involved in the process of single particle reconstruction (Ludtke et al., 1999).
In the ﬁrst stage, molecule particles are selected from micrographs.
Second, an initial 3D model is generated.
In the last stage, the initial model of the second step is reﬁned through an iterative process.
The third stage is the most time-consuming one  moreover, it directly decides the resolution of the ﬁnal 3D structure.
The third stage can be further divided into four steps, as illustrated by Figure 1.
First, projections of the initial model from various orientations are generated.
Second, selected particles are grouped into a set of clusters with respect to the projections generated in the ﬁrst step.
Third, a class average is generated for each cluster.
Finally, a new 3D model is constructed based on the class averages from the third step.
The basic operation of the third step is estimating the similarity between each projection and each particle.
Speciﬁcally, a score sij is evaluated to reﬂect the similarity between the i-th projection and the j-th particle.
The larger the score is, the closer the projection and particle are.
For the following discussions, we suppose there are totally m projections and n particles.
Thus, the j-th particle can be represented by an m-dimensional similarity vector  pj =(s1j,s2j,...,smj)T 1  j  n    2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:07 15/5/2009 Bioinformatics-btp219.tex]  Page: i276  i276 i280   The algorithm of EMAN ﬁnds the element, say skj, with the largest value in the vector.
The cluster associated with the particle is then determined by the position of such element.
In this example, it is k, and the corresponding cluster is ck.
Repeating this operation for each projection and particle, a set of m clusters C= c1,c2,  , cm  can  be obtained.
Each cluster corresponds to a projection, and contains the indices of particles that are most similar to the projection.
Intuitively, this method makes sense, because the position of the largest element represents the projection that is most similar to the particle.
In reality, however, we found some problems.
In some cases, one particle may be equally similar to more than one projection.
The scores of the particle with respect to these projections are so close that it is difﬁcult to tell which projection is more similar.
A proof of the existence of such problem is that the same program compiled by different compilers produced different sets of clusters.
Besides, for each similarity vector, only the largest score is utilized, and others are simply discarded, but they also provide important information about the particle.
In view of these problems, we introduced a framework called PRF to reﬁne clusters provided by EMAN.
After the clustering step of EMAN, each particle goes through an additional step called reclustering, if it satisﬁes some predeﬁned conditions.
Actual results show that the reﬁned clusters determine the orientations of particles more accurately, and the consequent 3D structures have higher resolutions.
3 THE PARTICLE REFINING FRAMEWORK In order to construct the framework, three key questions need to be answered.
The ﬁrst is how to normalize similarity vectors, so that the normalized vectors are more suitable to be processed by other methods, while preserving the information granted by similarity vectors.
The second is how to decide which particles need to go through the additional reclustering step.
The third is how to reﬁne clusters by means of other methods.
In the following sections, these questions are answered one after another.
3.1 Normalization Similarity vectors have some properties that are not suitable to be processed by other algorithms.
For example, different similarity vectors may have quite different value ranges, which make it unreasonable to compare different particles by directly comparing values in their similarity vectors.
Normalization is aimed at solving these problems.
It transforms similarity vectors to some other forms which can be processed more easily by other algorithms.
This is achieved by a function f : Rm   Rm.
Suppose p=(s1,s2,  , sm)T is the similarity vector for an arbitrary particle, and q=(v1,v2,   ,vm)T is its normalized vector, namely q= f (p), properties:  then function f should satisfy the following  (ii) Suppose  (i) For any i1,i2  1, 2,  , m , if si1  si2, then vi1  vi2.
vmin(j)=min  vij 1  i  m ,  and vmax(j)=max  vij 1  i  m ,1  j  n, then vmin(1)= vmin(2) =  = vmin(n), and vmax(1)= vmax(2)=   = vmax(n).
The ﬁrst property means that the relative ranks of elements in the vector should be preserved after normalization.
The second property makes sure that vectors of different particles have the same values  Framework to reﬁne particle clusters  (2)  (1)  ranges after normalization.
In our implementation, the normalized vector q= f (p) is carried out as follows:  smin =min si 1  i  m  smax=max si 1  i  m  ti= si smin 1  i  m smax smin vi= eti  1 e 1 It can be veriﬁed that this implementation satisﬁes the two properties.
First, property (i) is satisﬁed because formulas (3) and (4) are both monotonous increasing functions.
It can be easily proved that min ti 1  i  m =0, and max ti 1  i  m =1, so we can get min vi  1  i   m  = 0 and max vi  1  i  m  = 1.
Because p is an arbitrary similarity vector, this proof can be applied to any particles.
Therefore, vmin(1)= vmin(2)=   = vmin(n) = 0 and vmax(1) = vmax(2) =     = vmax(n) = 1 are true, and property (ii) is veriﬁed.
1  i  m  (3)  (4)  3.2 Threshold determination This section deals with the problem of choosing particles for reclustering.
Intuitively, the more uncertain the cluster of a particle is, the more likely the particle will participate in reclustering.
We deﬁne the clustering certainty of a particle as the difference between values of the largest and second largest elements in its normalized vector.
Therefore, the clustering certainty can be used as a criterion to decide whether a particle needs reclustering.
For a particle with a large clustering certainty, the most similar projection can be easily identiﬁed, so reclustering is not necessary for it.
On the other hand, for a particle with a small cluster certainty, the difference between the largest two elements in the normalized vector is trivial, and the most similar projection is unclear, so reclustering is needed to help identify the correct cluster.
Once the clustering certainty is known for each particle, a threshold can be determined, and the decision of choosing particles for reclustering can be made.
If the clustering certainty of a particle is greater than the threshold, then it skips the reclustering step, with its cluster remaining unchanged  otherwise, it participates in reclustering.
The threshold can be determined in one of the two ways.
First, the threshold can be a ﬁxed constant.
Second, the threshold can take the value so that a ﬁxed percentage of particles participate in reclustering.
In our implementation, both methods were used, and corresponding results will be given in Section 4.
3.3 Reclustering This section describes the method to determine the cluster associated with each particle that participates in reclustering.
Here, we borrowed the idea of the K-Means algorithm (Willett, 1980), one of the most popular algorithms for clustering.
The idea of K- Means is simple and easy to implement, but a major problem is how to properly select initial centroids, so as to determine initial clusters, because this will greatly inﬂuence qualities of the ﬁnal clusters.
In many applications of K-Means, initial centroids are chosen randomly, as a result, qualities of the consequent clusters are usually low and unstable.
i277  [10:07 15/5/2009 Bioinformatics-btp219.tex]  Page: i277  i276 i280   L.Fan et al.
However, that is not a problem for the case here.
We take the clusters produced by the clustering step of EMAN (c1,c2, ,cm) as initial clusters.
This is reasonable because for most cases, clusters produced by EMAN are close enough to the ﬁnal  corrected clusters.
Suppose n normalized vectors q1,q2,  , qn are available, obtaining initial centroids is simple and straightforward.
Each centroid can also be represented as an m-dimensional vector: qj i=1,2, ..., m  (5)  (cid:2) j ci  si= 1 ci   The distance between the j-th particle and the i-th centroid can be characterized by norm of the vector qj si.
Besides, similarity scores should also be incorporated in the distance function because they also reﬂect the distances between particles and centroids.
This is accomplished by giving a group of weights w1,w2,  , wm, to each particle, and incorporating them in the distance function as:  d(q,si)= wi(cid:6)q si(cid:6) i=1,2,...,m  (6)  where q is an arbitrary normalized vector.
The ﬁrst term of (6) represents the clustering decision made by similarity scores, and the second term reﬂects the preference of the K-Means algorithm.
Suppose q=(v1,v2,  , vm)T , the weights should have the property then wi1  wi2, for any 1  i1,i2  m. In our  if vi1  vi2,  that implementation, we set  (cid:3)  wi=  1 vi  vi(cid:8)=0 +  vi=0 for norms of vectors.
There are many deﬁnitions implementation, we chose the inﬁnity norm, which is deﬁned as:  In our  x=(x1,x2,...,xn)T n(cid:2) i=1  (cid:6)x(cid:6) = lim p  (   xi p)1/p=max  xi  1  i  m   (8)  (9)  By evaluating distances to all centroids, each particle s associated cluster can be determined.
Speciﬁcally, for a particle with the normalized vector q, if its associated cluster is ck, then the following equality must hold.
d(q,sk)=min d(q,si) 1  i  m   (10)  It can be noticed that all values of the normalized vectors are made use of during the reclustering process, which overcomes the shortcoming of the algorithm used by EMAN.
3.4 Summary of the framework So far, every detail of the framework has been described.
Our implementation has also been given, and proved to meet the constraints proposed by each component of the framework.
The framework can be summarized as follows:  PRF(P= p1,p2   pn ,C= c1,c2   cm )  qj = normalize (pj) ccj = clustering_certainty(qj)  1 for j = 1 to n do 2 3 4 endfor 5 threshold = get_threshold(cc1, cc2   ccn) 6  for i = 1 to m do  (cid:4) j ci  qj  7  si= 1 ci   endfor for j = 1 to n do  8 9 10 11 12 13 14 15 16 17 18 endif 19 endfor  (cid:5)(cid:5)  endfor  if ccj   threshold then (cid:5)(cid:5)qj si for i = 1 to m do d(qj,si)= wij d(qj,sk) = min d(qj,si)   1   i   m  Find the cluster cl, so that j cl cl   cl  j  ck   ck   j   Inputs of PRF are the similarity vectors for all particles, as well as the set of clusters provided by EMAN.
The loop of lines 1 4 calculates the normalized vector and clustering certainty for each particle.
For our implementation, it takes O(m) time to calculate a normalized vector, and O(m) to calculate the clustering certainty, so the total time of the loop of lines 1 4 is O(mn).
The threshold is evaluated in line 5, of which the time complexity is O(n), if a ﬁxed percentage of particles participate in reclustering, or O(1) if the threshold is chosen to be a constant.
The loop of lines 6-9 calculates a set of centroids.
Any centroid si can be determined in time O(m ci ), by observing  (7)  m   ci = mn  (11)  m(cid:2) i=1  the total time of the loop is O(mn).
The task of reclustering is performed by the loop of lines 9 19.
It ﬁrst compares the clustering certainties with the threshold, only particles with cluster certainties smaller than or equal to the threshold go through the rest of the iteration.
The inner loop of lines 11 13 calculates the distances to all centroids.
In our implementation, a distance can be calculated in time O(m), so the total time for line 12 cannot exceed O(nm2).
Line 14 ﬁnds the smallest distance to all centroids, which also decides the new cluster of the particle.
This operation can be ﬁnished in time O(m).
Lines 15 17 get the old cluster of the particle, and update contents of both the old and new clusters.
Each of these operations can be ﬁnished in time O(1), if implemented properly.
Therefore, the loop of lines 9 19 takes O(nm2) time in total.
Based on analysis of three parts of the framework, we can conclude that the time complexity of our implementation is O(nm2).
4 EXPERIMENTAL RESULTS PRF has been applied to actual protein single particle reconstruction experiments.
This section presents results of these experiments.
In order to identify the beneﬁts gained by PRF, experimental results of the original EMAN program are also given.
In our experiments 10 datasets were used, ﬁve of which were initial 3D models of the Hepatitis B Virus, and the other ﬁve were initial 3D models of the Chaperonin β subunit from the Thermoacidophilic Archaeon.
They were generated by other experiments of single particle reconstruction.
Speciﬁcally, for each of the two macromolecules, EMAN was applied to one initial model, and ﬁve rounds of iteration were executed, which produced ﬁve new 3D models.
These new 3D models acted as our initial 3D models in  i278  [10:07 15/5/2009 Bioinformatics-btp219.tex]  Page: i278  i276 i280   Framework to reﬁne particle clusters  Table 1.
Resolutions of structures of Hepatitis B Virus  Dataset  HBV1 HBV2 HBV3 HBV4 HBV5  Orig-EMAN  10.9221 10.0515 9.18231 9.23309 9.29348  Table 2.
Resolutions of structures of Chaperonin β subunit  DATASET  ORIG-EMAN  Beta1 Beta2 Beta3 Beta4 Beta5  33.5746 32.2496 31.1206 32.6046 31.3382  PRF  10.6633 9.75645 9.20543 8.932203 9.29836  PRF  32.9683 31.5523 31.0678 32.5191 31.1551  within red boxes, it can be noticed that the structure processed by PRF is smoother, with less noise.
For all datasets, we compared resolutions of their consequent structures produced by different algorithms.
Resolutions of these structures were estimated by means of Fourier shell correlation (Penczek, 1998), as shown in Tables 1 and 2.
The column of Orig- EMAN corresponds to the results of the original EMAN program, and the column of PRF corresponds to the program with PRF.
From Tables 1 and 2, we can see that for 8 of the 10 datasets, PRF processed structures had higher resolutions.
The greatest increase was about 0.3 Å for Hepatitis B Virus, and 0.7 Å for Chaperonin β subunit.
The reason might be that PRF corrected some particles  associated clusters, so their orientations were determined more accurately, which lead to higher qualities of consequent structures.
For one dataset (HBV3), PRF processed structure had a resolution slightly lower than the structure produced by the original EMAN program.
For the remaining dataset (HBV5), resolutions of the PRF processed structure and the structure generated by Orig- EMAN were almost the same.
This may be due to the fact that reclustering may mistakenly decide clusters of particles in some cases.
For example, when too many particles  clusters are incorrect, the centroids calculated by PRF are incorrect either.
Or if a particle is equally close to more than one projection according to the distance function of the framework.
In that case, PRF may randomly select a cluster, which gives rise to an incorrect cluster.
We implemented PRF in C++ programming language on Linux platform, and Table 3 displays the execution time for each dataset.
For all datasets, time was  40 s, which is trivial compared with other steps of EMAN.
This implies that PRF can be used in practice to improve 3D structures produced by EMAN, without incurring too much extra execution time.
the longest  5 CONCLUSIONS In this study, a PRF is introduced to reﬁne clusters produced by EMAN, with the purpose of determining orientations of particles more accurately and achieving higher resolutions of consequent 3D  i279  (a)  (b)  Fig.
2.
The 3D structures of HBV1 produced by different algorithms.
(a) The structure produced by the original EMAN program, and (b) the structure processed by PRF.
It can be noticed from the parts within red boxes that structure processed by PRF is smoother, with less noise.
the following experiments.
In this study, they are denoted by HBV1, HBV2,  , HBV5, and Beta1, Beta2,  , Beta5.
For experiments of the Hepatitis B Virus, 4239 selected particles were used, which were grouped into 379 clusters, and for experiments of the Chaperonin β subunit, 2334 particles were grouped into 85 clusters.
The two strategies for evaluating the threshold were both adopted in our experiments.
For Hepatitis B Virus, a ﬁxed percentage (10%) of particles participated in reclustering, and for Chaperonin β subunit, a ﬁxed constant threshold (0.005) was used.
Figure 2 gives an example of comparison between 3D structures produced by different algorithms.
Figure 2a is the 3D structure of HBV1 constructed by the original EMAN program, and Figure 2b is the structure of HBV1 processed by PRF.
By comparing the parts  [10:07 15/5/2009 Bioinformatics-btp219.tex]  Page: i279  i276 i280   L.Fan et al.
Table 3.
Execution time of our implementation of PRF  Dataset  Time (s)  Dataset  Time (s)  HBV1 HBV2 HBV3 HBV4 HBV5  31.029 32.562 32.027 31.629 32.194  Beta1 Beta2 Beta3 Beta4 Beta5  10.623 10.334 10.358 10.521 10.532  ACKNOWLEDGEMENTS We would like to thank Fei Sun for providing the experimental datasets, and Steven Ludtke and Wen Jiang for critical reading of the manuscript.
Funding: National Natural Science Foundation for China (90612019, 60752001, 60736012 and 60503060)  Chinese Academy of Sciences knowledge innovation key project (KGCX1-YW-13).
structures.
It has three components, each one dealing with a basic problem.
First, a function is required to convert similarity vectors to normalized vectors.
The function must satisfy two conditions described in Section 3.1.
Second, a criterion is needed to choose particles for reclustering.
This is accomplished by obtaining a threshold and comparing the clustering certainty of each particle with it.
The last component determines the associated clusters with particles by means of a new method.
The clustering decisions made by EMAN must also be taken into account by the new method.
Our implementation of PRF is provided, and corresponding results reveals that for most cases, this implementation is capable of improving resolutions of consequent 3D structures.
Moreover, this implementation does not incorporate too much extra execution time.
All these suggest that PRF can be applied in practice to improve resolutions of 3D structures produced by EMAN, and hence accelerate the convergent process of EMAN.
Conﬂict of Interest: none declared.
REFERENCES Frank,J.
et al.
(1996) SPIDER and WEB: processing and visualization of images in 3D  electron microscopy and related ﬁelds.
J. Struct.
Biol., 116, 190 199.
Liang,Y.
et al.
(2002) IMIRS: a high-resolution 3D reconstruction package integrated  with a relational image database.
J. Struct.
Biol., 137, 292 304.
Ludtke,S.J.
et al.
(1999) EMAN: semiautomated software for high-resolution single-  particle reconstructions.
J. Struct.
Biol., 128, 82 97.
Penczek,P.
(1998) Measures of resolution using Fourier shell correlation.
J. Mol.
Biol.,  280, 115 116.
Scheres,S.H.W.
et al.
(2007) Disentangling conformational states of macromolecules  in 3D-EM through likelihood optimization.
Nat.
Methods, 4, 27 29.
Willett,P.
(1980) Document clustering using an inverted ﬁle approach.
J.
Inform.
Sci.,  2, 223 231.
Yang,C.
et al.
(2007) The parallelization of SPIDER on distributed-memory computers  using MPI.
J. Struct.
Biol., 157, 240 249.  i280  [10:07 15/5/2009 Bioinformatics-btp219.tex]  Page: i280  i276 i280
