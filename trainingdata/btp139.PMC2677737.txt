BIOINFORMATICS ORIGINAL PAPER Vol.
25 no.
10 2009, pages 1300 1306  doi:10.1093/bioinformatics/btp139  Systems biology Gaussian process regression bootstrapping: exploring the effects of uncertainty in time course data Paul D. W. Kirk  and Michael P. H. Stumpf   Centre for Bioinformatics, Division of Molecular Biosciences, Imperial College London, London SW7 2AZ, UK Received on November 21, 2008  revised on February 3, 2009  accepted on March 7, 2009 Advance Access publication March 16, 2009 Associate Editor: Limsoon Wong  the effects that  ABSTRACT Motivation: Although widely accepted that high-throughput biological data are typically highly noisy, this uncertainty has upon the conclusions we draw from these data are often overlooked.
However, in order to assign any degree of conﬁdence to our conclusions, we must quantify these effects.
Bootstrap resampling is one method by which this may be achieved.
Here, we present a parametric bootstrapping approach for time- course data, in which Gaussian process regression (GPR) is used to ﬁt a probabilistic model from which replicates may then be drawn.
This approach implicitly allows the time dependence of the data to be taken into account, and is applicable to a wide range of problems.
Results: We apply GPR bootstrapping to two datasets from the literature.
In the ﬁrst example, we show how the approach may be used to investigate the effects of data uncertainty upon the estimation of parameters in an ordinary differential equations (ODE) model of a cell signalling pathway.
Although we ﬁnd that the parameter estimates inferred from the original dataset are relatively robust to data uncertainty, we also identify a distinct second set of estimates.
In the second example, we use our method to show that the topology of networks constructed from time-course gene expression data appears to be sensitive to data uncertainty, although there may be individual edges in the network that are robust in light of present data.
Availability: Matlab code for performing GPR bootstrapping is available from our web site: http://www3.imperial.ac.uk/theoreticalsystemsbiology/data-software/ Contact: paul.kirk@imperial.ac.uk, m.stumpf@imperial.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION The use of data obtained from high-throughput technologies such as microarrays has become standard in systems biology.
There are many ways in which these data are exploited, such as reverse engineering putative pathways and networks directly from the data (e.g.
Lèbre, 2007  Opgen-Rhein and Strimmer, 2007), or inferring the values of unknown parameters in mechanistic models (e.g.
Barenco et al., 2006  Swameye et al., 2003).
The methods used to obtain the data are often subject to signiﬁcant levels of measurement noise, and so we might expect repetitions of the experiments to    To whom correspondence should be addressed.
yield quantitatively different datasets.
However, the costs associated with high-throughput experiments usually mean that the number of technical replicates is restricted, and so it is difﬁcult to quantify the effects of data uncertainty upon the inferences we draw.
Clearly, if our aim is to attach biological meaning to our results (for example, by proposing putative pathways), then we need to have some degree of conﬁdence that any conclusions we make are robust to the uncertainty in the data.
That is, we need to be sure that what we infer (whether it be the rate constants of a biochemical reaction, the topology of a gene regulatory network or any other unknown quantity or reverse engineered model) is not speciﬁc to the particular noisy dataset that we happened to observe.
Bootstrapping is a well-known resampling method that may be used to assess properties (such as the standard error) of an inferred quantity or statistical estimator (Efron, 1979  Efron and Tibshirani, 1993).
The process that generated the data is estimated by an approximating distribution from which samples may be drawn.
Bootstrap datasets are then obtained from this distribution, and the statistical estimator is calculated for each.
This induces a sampling distribution over the estimator, from which we may assess, for example, its variance amongst all of the bootstrap datasets.
Previous biological applications of bootstrapping include, to name a few examples, placing conﬁdence intervals on phylogenies (Felsenstein, 1985), assessing the reliability of conclusions drawn from clustering expression data (Kerr and Churchill, 2001), and constructing  robust  estimates of gene networks (Imoto et al., 2005).
We here consider a parametric bootstrap for time-course data in which the time-dependent process that generated the data is modelled using Gaussian process regression (GPR).
In recent years, this Bayesian non-linear regression technique has grown in popularity, and has been applied in several systems biology contexts (Gao et al., 2008  Lawrence et al., 2007  Yuan, 2006).
To our knowledge, GPR has not previously been used as a method for bootstrapping time-course data.
However, it would seem to be ideally suited to this task, since it provides a method for ﬁtting a plausible probabilistic model that captures the time dependence of the data, and from which it is easy to draw bootstrap samples.
We demonstrate GPR bootstrapping using two examples from the systems biology literature: estimating the parameters of an ordinary differential equation model for the STAT5 signalling pathway (Swameye et al., 2003)  and inference of gene regulatory networks in Arabidopsis thaliana (Smith et al., 2004).
Below, we ﬁrst provide an overview of GPR and how it may be used in general as a bootstrapping method (Section 2), and then we describe how the approach may be applied (Section 3).
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:03 8/4/2009 Bioinformatics-btp139.tex]  Page: 1300  1300 1306   In Section 4, we summarize our ﬁndings for two examples, and discuss the implications in Section 5.
We conclude by highlighting the importance of bootstrapping in general as a method for assessing the effects of data uncertainty.
2 APPROACH GPR is a Bayesian non-linear regression method, which has been used to good effect in a number of studies (Gao et al., 2008  Lawrence et al., 2007  Yuan, 2006).
Formally, a Gaussian process (GP) is a collection of random variables, any ﬁnite number of which have a joint Gaussian (normal) distribution (Rasmussen and Williams, 2006).
A GP is deﬁned by a mean function and a covariance function, which specify the mean vectors and covariance matrices for each ﬁnite collection of the random variables.
GP theory is discussed in more detail by MacKay (1998) and Rasmussen and Williams (2006), but for completeness and convenience we present an overview of standard GPR theory in addition to our own contribution of how it may be used to perform a parametric bootstrap.
2.1 Regression In a regression problem, we are interested in elucidating the relationship between a collection of covariates or inputs x1, ..., xp, and a continuous dependent output variable, z.
We assume that x1, ..., xp,z are all real-valued, and we write the collection of covariates as a p-component column vector, x=[x1, ..., xp](cid:2)  Rp.
It is assumed that there is an unknown deterministic function, f , which wholly describes the relationship between z and x, so that z= f (x).
Our aim is therefore to ﬁnd the function, f .
In practice, the methods by which measurements of z are obtained introduce experimental noise.
We hence deﬁne a random variable, y, to represent the experimentally observable version of z.
We assume that y may be written as y= z+ , where   is a noise term.
For convenience, we also assume that   N (0,σ 2) and that   is independent of x.
For the time being, we consider the case in which the variance, σ 2, is known, but shall return later to the problem of how it may be estimated.
To summarize, we have,  (1)  y(x)= f (x)+ , where   N (0,σ 2).
parametric form on f [such as f (x)=(cid:1) One way to approach the regression problem is to impose a ﬁxed βiφi(x), where the βi are parameters, the φi are a set of basis functions and M  N], and then to estimate its parameters from a set of experimentally obtained observations using methods such as ordinary least squares.
An alternative is to recognize that the function, f , is unknown, and hence is itself a source of uncertainty  GPR provides us with a means by which to do this.
i=1 M  GPR belongs to a class of approaches known as non-parametric Bayesian methods.
Such methods can be viewed broadly as providing probability models on function spaces (Müller and Quintana, 2004).
Apart from GPs, the other well-known non- parametric Bayesian methods are those based on Dirichlet processes (DPs).
These were introduced by Ferguson (1973) and Antoniak (1974), and provide a framework for the probabilistic modelling of unknown probability distributions.
That is, rather than assuming that a given sample has been drawn from a probability distribution of  Gaussian process regression bootstrapping  known parametric form (but with unknown parameters), DP-based approaches model the uncertainty in the probability distribution itself.
In contrast, GP approaches provide a framework for the probabilistic modelling of unknown functions rather than unknown distributions.
2.2 GP priors In GPR, we assume a GP prior for f (x) with mean function m and covariance function k. This means the following:  (cid:127) For any input, x   Rp, we regard the value taken by f (x) at x=x  to be a random variable.
The notation f (x ) should now be understood to denote this random variable.
(cid:127) Given a ﬁnite collection of covariate vectors, x1, ...,xn, the random variables f (x1), ..., f (xn) are assumed to be jointly distributed according to a multivariate Gaussian with mean m= [m(x1), ..., m(xn)](cid:2) and covariance matrix (K)ij = k(xi,xj).
We thus write f (x) GP(m,k).
Note that if we assume the regression model of Equation (1), the GP prior over f (x) induces a GP prior over the observable outputs y(x).
That is, assuming Equation (1) and that f (x) GP(m,k), it follows that,  y(x) GP(m,l),  (2) where l(xi,xj)= k(xi,xj)+σ 2δ(xi,xj).
Here, δ(xi,xj) is the standard Kronecker delta function.
), ..., f (xs   , ...,xs  2.3 From prior to posterior We now suppose that, having assumed a GP prior f (x) GP(m,k), we proceed to obtain a set of output measurements y1, ..., yr at the covariate vectors x1, ...,xr.
We are interested in determining how we may update our GP prior in light of these observed data.
We show below that, given any ﬁnite collection x1 of covariate vectors, the joint conditional probability of the function values f (x1 ) given the observations is again described by a multivariate normal.
We hence obtain a GP posterior over f (x).
We view y1, ..., yr as realizations of the random variable y(x) at  the inputs x.
We know from Equation (2) that,  (cid:3) xr where mo=[m(x1), ..., m(xr)](cid:2) (cid:3) (cid:2) (cid:2) For notational brevity, we henceforth write [y(x)] y  (cid:4)(cid:5)(cid:2) N (mo,Ko), (cid:3)  (3) = k(xi,xj)+σ 2δ(xi,xj).
to mean  .
be another ﬁnite collection of covariate vectors.
From our assumption of a GP prior over f , together with Equation (3), it is straightforward to see that,  (cid:4)(cid:5)(cid:2) (cid:3) xr  , ...,xs  x1 Let x1  , ..., y  , ..., y  and  Ko  x1  (cid:3)  (cid:4)  (cid:4)  (cid:4)  (cid:2)     y  ij  (cid:5)(cid:2) N )](cid:2)     (cid:8)  (cid:6)(cid:7) mo (cid:3) m  K    (cid:6) (cid:4)  ,  (cid:9)(cid:9)  Ko K  (cid:2)  K   K  ,xj = k(xi  ,  (4)  ,  ij        (cid:4)  ), ..., m(xs  = k(xi,xj  (cid:3) where, m =[m(x1 K  ).
From Equations (3) and (4), and using standard properties of Gaussian distributions (von Mises, 1964), it follows that the function values f (x1 ) conditioned on the observed outputs y are also jointly distributed according to a multivariate normal.
), ..., f (xs  and           ij  )  (cid:2)  y(x),f (x1     ), ..., f (xs     )  [17:03 8/4/2009 Bioinformatics-btp139.tex]  Page: 1301  1300 1306  1301   P.D.W.Kirk and M.P.H.Stumpf  )     (cid:2)  (5)  (cid:12)  f (x1  ), ..., f (xs  Speciﬁcally,    (cid:5)(cid:2)(cid:10)(cid:10)(cid:10)(cid:11) where y=[y1, ..., yr](cid:2), and, (cid:2)  mpost=m +K (cid:2)  Kpost= K   K  [y(x)](cid:2)=y (cid:11) Ko+σ 2Ir (cid:11) Ko+σ 2Ir Here, Ir is the r r identity matrix.
Since Equation (5) is true for any s  N, it follows that the function outputs, f (x), conditioned on the observations, y, deﬁne a GP, which is referred to as the GP posterior.
N (mpost,Kpost), (cid:12) 1(cid:3) (cid:12) 1  y mo K .
(6)  (7)  (cid:4)  ,        ), ..., f (xs  2.4 Using the GP posterior Equation (5) provides the joint posterior distribution of the function values f (x1 ), given the GP prior and the observations, y.
Since the mean of a Gaussian distribution is also its mode, the maximum a posteriori prediction of [f (x1 is simply the mean vector mpost.
Thus, GPR allows the prediction of f (x) at any ﬁnite collection of covariate vectors, x1 .
The covariance matrix, Kpost, describes the variability of the distribution about the mean, and hence may be used to place conﬁdence intervals around this prediction.
Figure 1A illustrates the use of GPR to make predictions and specify conﬁdence intervals.
), ..., f (xs  , ...,xs  )](cid:2)       In this article, we are concerned not only with ﬁtting a regressor to the dataset, but also with sampling from the regression model in order to obtain bootstrap datasets.
This is similar to the work of Kerr and Churchill (2001), who also generate bootstrap samples by ﬁrst ﬁtting a model to a set of time-course data (in their case, an  A  l  i     e v e L n o s s e r p x E  B  l  i     e v e L n o s s e r p x E  20  15  10  5  0 0  20  15  10  5  0 0  Original Data Predicted Underlying Function  5  10  Time  15  20  25  Bootstrapped Data A Particular Bootstrap Sample  5  10  Time  15  20  25  Fig.
1.
Using a GP regressor to ﬁt gene expression time-course data and draw bootstrap samples [data taken from the  arth800  dataset of the R package  GeneNet  (Schäfer et al., 2006)].
(A) The red line shows the mean of the posterior process, while the grey region is a 99% credible interval around this mean.
(B) The marginal distribution over the observable output values at each time point is univariate Gaussian, centred at the mean of the posterior process.
The crosses represent samples drawn from the posterior, with blue crosses used to highlight one particular draw.
1302  ANOVA model).
The advantages of GPR are its non-linearity, that it implicitly allows us to model the uncertainty in the underlying function, f , and that it is relatively easy to apply.
Generating samples from our GP regressor is also fairly simple.
We know that the joint posterior distribution of any ﬁnite collection, f (x1 ), is a multivariate normal [as given in Equation (5)], and hence we may simulate samples using standard methods for such distributions (Press et al., 2007).
), ..., f (xs        Since we are concerned with the generation of plausible datasets, rather than just plausible samples of the underlying function values, it follows that we are actually interested in y(x) rather than f (x).
However, if we can sample function outputs, f (x), and if we know (or can estimate) the variance σ 2, then we can use Equation (1) in order to obtain samples of y(x).
Thus, in practice, we proceed by ﬁrst sampling [f (x1 ), ..., f (xs from the multivariate normal described by Equations (5), (6) and (7), and then adding Gaussian noise sampled from N (0,σ 2Is).
In this study, we generate bootstrap samples at the same points as those at which the data were observed (i.e.
we choose s= r and set  =x1, ...,xs  =xr).
Figure 1B provides an example of a number x1 of bootstrap samples obtained using a GP regressor ﬁtted to a gene expression time course.
)](cid:2)        2.5 The mean and covariance functions In order to specify a GP prior, it is clearly necessary to provide a mean function, m, and a covariance function, k. The covariance function is the more important of these, as it describes how we believe the value of the function outputs, f (x), covary with one another, and hence allows us to express our beliefs about fundamental properties of f , such as how rapidly it changes.
For the sake of simplicity and parsimony, the mean function is often chosen to be zero, and this is the approach we adopt here.
This does not present a serious limitation: as we can see from the regressor in Figure 1A, the mean of the posterior process (represented by the red line) is certainly not constrained to be zero, and we are able to obtain a good ﬁt to the data.
Of course, other mean functions may be chosen to express stronger prior beliefs about the underlying function.
There are many possible choices for the covariance function, k, and we here consider two of the more popular options, the squared exponential covariance function,  kSE(xi,xj)= σ 2    1 2l1   xi xj 2  ,  g exp  (8) where     denotes the Euclidean distance  and a standard Matérn covariance function, kM (xi,xj)= σ 2  1+  (cid:13)  (cid:14)  (cid:14)  (cid:13)  exp        (9)    3 xi xj  l2  3 xi xj  l2  .
f  (cid:6)  (cid:9)  Here, the constants σg,σf ,l1 and l2 are hyperparameters.
Although its smoothness properties have been criticized as unrealistic (Stein, 1999), the squared exponential covariance function remains the most frequently used  default  choice for GPR, largely because of its simplicity.
There are many examples of covariance function (Rasmussen and Williams, 2006, ch.
4), which allow the GP prior to be tailored to speciﬁc scenarios.
In this article, we employ kSE and kM , as they are simple, yet sufﬁciently ﬂexible to allow a good ﬁt to the data.
[17:03 8/4/2009 Bioinformatics-btp139.tex]  Page: 1302  1300 1306   The hyperparameters of the covariance function provide us with another means to encode our prior beliefs about the nature of f .
We can see, for example, that if l1 (or l2) is very large, then f (xi) and f (xj) will only tend to vary together if  xi xj  is small: the value of the function at xi will only affect the value at xj if xi and xj are close together.
Ideally, we would either use prior knowledge to specify the hyperparameters, or adopt a fully Bayesian approach and integrate them out.
However, we are rarely able to express our prior beliefs so precisely, and while a full Bayesian approach is possible (using, for example, Markov chain Monte Carlo (MCMC)), the associated computational expense is often undesirable.
This is certainly the case here: in the example presented in Section 3.2 (which we expect may represent a typical application), we are required to ﬁt regressors to 800 gene expression time-course datasets, so we wish to minimize the costs of ﬁtting the GPR model.
An alternative and computationally cheaper method is to estimate the hyperparameters in order to maximize the (log) likelihood of the observed data.
We also use this approach to estimate the variance of the noise term, σ 2, in Equation (1).
From Equation (3) and the deﬁnition of a multivariate normal, the likelihood of y is given by,  p(y(x)=y θ,σ 2)= exp  (cid:2)  (cid:15)(cid:3)   1 2 (y mo) (cid:4)r det  2π  (cid:3)  Ko(θ,σ 2)  (cid:3)   1(y mo) (cid:4)(cid:4)  ,  Ko  θ,σ 2  (cid:11)  where θ is the vector of the covariance function s hyperparameters, det( ) denotes the determinant, and we write Ko(θ,σ 2) to make explicit the dependence of Ko on the hyperparameters.
Taking logs and removing constant terms, we deduce that the maximum likelihood values for θ and σ 2 are given by,  (cid:16)  (cid:11)  (cid:11)  (cid:11)  (cid:12)(cid:12)(cid:12)  log  det  Ko  θ,σ 2  ˆθ , ˆσ 2=argmax  θ,σ 2    1 2  (cid:12)  (cid:17)  .
(y mo)  (cid:2)    1 2  Ko (θ,σ 2 )   1(y mo)  This optimization can be approached using standard methods for optimization (Press et al., 2007), such as the Nelder Mead simplex method or gradient descent.
3 APPLICATIONS In order to demonstrate the potential applications of the GPR bootstrap, we consider two examples from the literature: estimating the parameters of a model of the STAT 5 signalling pathway, and inferring a gene network.
3.1 Parametric ODE modelling of signalling pathways The JAK-STAT pathway is a well-studied signalling pathway that describes a mechanism by which signals carried by cytokines may be transduced to the cell nucleus via STAT activation, dimerization and relocation to the nucleus (Aaronson and Horvath, 2002  Horvath, 2000).
Swameye et al.
(2003) suggested a number of parametric ODE models to describe the JAK2- STAT5 signalling pathway, the parameters of which were estimated from experimental data.
We consider one of the proposed models (taken from Swameye et al., 2003, Supplementary Material), and using data from the original experiments apply our GPR bootstrapping approach in order to assign conﬁdence intervals to the parameter estimates.
Gaussian process regression bootstrapping  3.1.1 The Model The model we consider is as follows,  = r1v1D+ 2r4v4 = r3v3 + 0.5v2  2  dv1 dt dv3 dt  = r1v1D  v2 = r3v3   r4v4.
2  dv2 dt dv4 dt  (10)  Here, v1,v2 and v3 represent the concentrations of (respectively) unphos- phorylated STAT5, phosphorylated monomeric STAT5 and phosphorylated dimeric STAT5 in the cytoplasm.
The variable v4 denotes the concentration of STAT5 in the nucleus, and D is an experimentally determined quantity (which varies over time) related to the amount of Epo-induced phosphorylation of the EpoR (Swameye et al., 2003).
The ri s are parameters (see Swameye et al., 2003  Supplementary Material).
The initial values of v2,v3 and v4 at time t=0 are assumed to be zero (since it is supposed that all STAT5 in the cell is initially cytoplasmic and unphosphorylated), while the initial concentration of unphosphorylated cytoplasmic STAT5, v1(t=0), is treated as an unknown parameter.
The quantities v1,v2,v3 and v4 were not measured individually.
Instead, the amount of phosphorylated STAT5 in the cytoplasm, y1, and the total amount of cytoplasmic STAT5 (phosphorylated and unphosphorylated), y2, were recorded.
These can be written in terms of the vi s as follows,  y1= r5(v2+2v3) y2= r6(v1+v2+2v3),  where r5 and r6 are two unknown scaling parameters, which must also be estimated.
In total, there are thus six unknown parameters in this model [r1,r3,r4,r5,r6 and v1(0)].
3.1.2 GPR bootstrapping and parameter estimation Swameye et al.
(2003) measured y1 and y2 at a number of discrete time points in order to obtain several sets of experimental data.
We focus on just one of these datasets (the  DATA1_hall  set, available from the original authors at http://webber.physik.uni-freiburg.de/ jeti/), which we use together with our GPR bootstrapping approach in order to obtain 1500 bootstrapped datasets.
To deﬁne our GP prior, we choose a zero mean function and the squared exponential function of Equation (8).
In order to learn the hyperparameters and ﬁt  the GP regressor to the dataset, we make use of the gpml suite of Matlab functions accompanying Rasmussen and Williams (2006), available from http://www.
gaussianprocess.org/gpml/.
For each of our bootstrapped datasets, we estimate the unknown parameters of the ODE system presented in Equation (10) using the stochastic ranking evolutionary strategy (SRES) of Runarsson and Yao (2000), as implemented in the libSRES C library (Ji and Xu, 2006).
This allows us to ﬁnd the parameter values which minimize the sum of squared differences between the data and the predictions made by the ODE model.
This optimization problem is susceptible to the usual difﬁculty of becoming stuck in a local minimum.
The evolutionary nature of SRES goes some of the way toward mitigating this difﬁculty, but to reduce the impact of becoming stuck in a local minimum yet further, we also run the algorithm for a large number of iterations and rerun eight times for each dataset (taking as our ﬁnal estimate the  best  amongst these eight runs).
Before considering the bootstrapped data, we use SRES to estimate parameter values from the original dataset.
The values so obtained are: v1(0)=0.996, r1=2.43, r3=0.256, r4=0.303, r5=1.27 and r6=0.944.
For this  optimal  set of parameters the model provides a reasonable ﬁt to the observed data which is comparable with the ﬁt obtained in the original paper (Swameye et al., 2003).
The aim of our bootstrapping approach is to determine whether or not these parameter estimates are robust to the uncertainty in the data.
3.2 Gene network inference When considering how our GPR bootstrapping approach may be applied in order to investigate the effects of data uncertainty on the reverse engineering  1303  [17:03 8/4/2009 Bioinformatics-btp139.tex]  Page: 1303  1300 1306   P.D.W.Kirk and M.P.H.Stumpf  of gene regulatory networks we consider only relevance networks (Butte et al., 2000) and graphical Gaussian models (GGMs).
However, our method could just as easily be applied in order to assess the effects of data uncertainty on network inference methods (such as Lèbre, 2007) more generally.
We consider temporal expression data for the 800 A.thaliana genes from (Smith et al., 2004) which are provided in the  arth800  dataset of the R package  GeneNet  (Schäfer et al., 2006).
3.2.1 Gene relevance networks Butte et al.
(2000) introduced the idea of a gene relevance network a type of graphical model in which vertices represent genes and in which we draw an edge between genes g1 and g2 if and only if the expressions of g1 and g2 are correlated.
Thus, relevance networks provide us with a means to represent (linear) dependencies between genes.
Correlations are calculated between genes in a pairwise fashion  it is decided whether or not to draw an edge between g1 and g2 without reference to the presence or absence of edges between any other genes.
To determine whether or not to place an edge between genes g1 and g2, we ﬁrst calculate the (in our case, Pearson) correlation between their gene expression time courses, square this value to obtain a score s, and then place an edge if s   r for some prespeciﬁed threshold value r.  are used to represent 3.2.2 Graphical Gaussian models GGMs dependencies between genes that have been detected by partial correlations.
In contrast to relevance networks (where a missing edge between two genes indicates marginal independence), the absence of an edge between genes g1 and g2 in a GGM means that g1 and g2 are conditionally independent.
We use the R package  GeneNet  in order to infer GGMs from time-course gene expression data (according to Opgen-Rhein and Strimmer, 2007), and make use of the package s capability to calculate an empirical posterior probability, pe(g1,g2) (Schäfer and Strimmer, 2005), for the existence of the edge between g1 and g2.
If pe(g1,g2)   τ , where τ is some prespeciﬁed threshold (cut-off) value for the probability, then an edge is drawn between g1 and g2.
3.2.3 Bootstrapping the data We apply our GPR bootstrapping approach to the A.thaliana data.
This dataset comprises measurements taken for 800 genes at 11 times, with two measurements at each time point (Fig.
1A illustrates the data for one of the genes).
We proceed as in the previous example, but this time make use of the following covariance function,  k(ti,tj)= kSE(ti,tj)+kM (ti,tj),  (11)  where kSE and kM are as previously described.
Using the method of Section 2 we obtain 1000 bootstrap datasets, each one consisting of two measurements at 11 time points for 800 genes.
4 RESULTS 4.1 Parametric ODE modelling of signalling pathways For each of our 1500 bootstrapped datasets, we ﬁnd the  optimal  set of parameters using SRES.
This induces a joint sampling distribution over the optimal parameters for the ODE model, whose marginals are represented by the histograms in Figure 2.
Note that the joint sampling distribution is conceptually very different to the joint posterior parameter distribution that might be sought using a Bayesian approach: in the former, we ﬁnd a single parameter estimate for each of a large number of sampled datasets, whereas in the latter we ﬁrst specify a prior distribution over the parameters and then seek to update this in light of observed data.
Figure 2 shows that the marginal sampling distributions are generally quite narrow.
This can be quantiﬁed by calculating the coefﬁcient of variation, cv, the parameters, cv(v1(0))=0.0338, cv(r1)=0.175, cv(r3)=1.77, cv(r4)=0.214,  for each of  1304  200  100  0 0.8  500  0 0  500  y c n e u q e r F  v  (0) 1  1.2  1 r 3  2  4  r 5  r 1  3 r 4  1  2  4  5  0.2 0.4 0.6 0.8  r 6  300  200  100  0  500  0  200  100  0 0.5  1  1.5  0  2.5  2 Parameter Value  0.8  1  1.2  Fig.
2.
Histograms showing the marginal sampling distributions over the optimal parameter values.
Although these distributions are generally quite narrow and centred about the parameter estimates obtained from the original dataset (vertical black dashed lines), note that for r3 there is a small amount of probability mass located at r3 5 (shown as a red bar and ringed by a red circle).
cv(r5)=0.0914 and cv(r6)=0.0434.
The coefﬁcient of variation for r3 is signiﬁcantly greater than for the other parameters.
This is due to the inﬂuence of bootstrap samples for which the optimal estimate for r3 was  5 (see the red bar in Fig.
2).
Indeed, across  all of the bootstrap samples, there appear to be two distinct sets of estimated optimal parameter values.
The ﬁrst (much larger) set comprises estimates centred around the values obtained from the original dataset.
The second set comprises estimates for which r3 5, and contains only 28 elements.
Although obtained for just a small number of the bootstrap samples ( 2%), the parameter estimates in this second set still provide a good ﬁt to the original data (see Supplementary Fig.
1).
To quantify this, the average mean square error (MSE) obtained using parameter estimates from the second set was 0.10 for the ﬁt to the y1 values, and 0.031 for the ﬁt to the y2 values.
By comparison, the MSE obtained using the parameter values estimated from the original data was 0.026 for the ﬁt to the y1 values, and 0.0043 for the y2 values.
This suggests that parameter estimates from the second set provide (on average) a slightly worse ﬁt overall, but do a marginally better job of ﬁtting y2 than those derived from the original dataset.
4.2 Gene network inference We start by considering the inferred GGMs.
Let NO(τ ) denote the network inferred from the original dataset using threshold τ , and similarly let N(i) B (τ ) be the network inferred from the i-th bootstrap dataset.
To assess how similar the bootstrapped networks are to NO(τ ), we calculate the proportion, ρ(i)(τ ), of edges in the original network that also appear in N(i) B (τ ).
We hence obtain a sampling distribution over ρ(i)(τ ) for a given τ .
In addition to considering the data sampled using GPR bootstrapping, we also performed a non-parametric bootstrap of the data, and calculated a  [17:03 8/4/2009 Bioinformatics-btp139.tex]  Page: 1304  1300 1306   Gaussian process regression bootstrapping  τ = 0.95  τ = 0.85  500  0 0  500  0 0  500  y c n e u q e r F  500  0 0  500  0 0  500  0.1  0.2  0.3  τ = 0.75  0.1  0.2  0.3  τ = 0.55  0.1  0.2  0.3  τ = 0.65  0.1  0.2  0.3  τ = 0.45  0 0  0.1  0.2  0 0  0.3 Value of ρ(τ)  0.1  0.2  0.3  Fig.
4.
A high conﬁdence subnetwork constructed from the A.Thaliana data set.
Edges drawn in blue appear in 100% of the bootstrap data samples  green edges appear in 99%  and red edges appear in 98%.
Vertices have the same colour as whichever of their edges appears in the largest number of bootstrap samples.
Fig.
3.
Plots showing the sampling distributions over ρ(i)(τ ) for different values of τ .
Black: GPR bootstrap.
White: non-parametric bootstrap.
edges that do not appear in at least q% of the bootstrap samples.
In this way, we can construct  high conﬁdence  relevance networks.
sampling distribution over ρ(i)(τ ) based upon 1000 (non-parametric) bootstrap datasets.
Histograms describing the sampling distributions obtained for different values of τ are presented in Figure 3.
In each case, the sampling distribution is approximately normal.
For smaller values of τ , the mean of the sampling distribution over ρ(i)(τ ) is greater than for larger values.
This is unsurprising: as τ gets smaller, the value of the empirical posterior probability required for an edge gets lower (until at the extreme case, τ =0, edges are drawn between all vertices, regardless of the data).
Thus, for smaller values of τ , the sensitivity to the data is reduced.
This has the effect of making the inferred network more robust to perturbations in the data, but unfortunately also makes the network increasingly meaningless.
For more meaningful values of τ (say, of around 0.85 or above), the degree of similarity between the bootstrapped networks and NO(τ ) [as measured by the mean value of ρ(i)(τ )] is disappointingly low.
This suggests that the original inferred GGM is highly sensitive to uncertainty in the data.
for  We repeated the above analysis  relevance networks, and obtained similar results (see Supplementary Material).
To summarize, the mean values of ρ(i)(r) for r values of 0.95, 0.85, 0.75, 0.65, 0.55 and 0.45 were (respectively) 0.062, 0.18, 0.29, 0.42, 0.51 and 0.61.
Yet again, these values are low, indicating that the topology of relevance networks is sensitive to uncertainty in the data (note that r and τ are not directly comparable, so it is difﬁcult to compare the relative tolerance to data uncertainty of relevance networks and GGMs).
Although the overall topology of the network seems to be sensitive to data uncertainty, there are individual edges that demonstrate a much higher degree of tolerance.
For example, taking r to be 0.85, we may look for edges that appear in 100% of the networks obtained from the bootstrapped datasets.
If we do this, then we ﬁnd 16 edges connecting 13 vertices, as shown in Figure 4.
We can use this approach more generally to construct networks that have a required level of tolerance to data uncertainty, omitting any  5 DISCUSSION Our results highlight the necessity of accounting for data uncertainty when trying to draw conclusions from experimentally obtained data.
In the case of the parametric ODE model of the JAK2-STAT5 signalling pathway, we showed that in addition to the parameter estimates obtained from the original dataset, there is a second set of plausible estimates which (had stochastic effects provided us with a slightly different dataset) we may well have concluded were the  correct  values.
We believe that the presence of the distinct second set of plausible parameter estimates indicates that the error function (i.e.
the sum of squared differences between the data and the model predictions) that was minimized in order to ﬁt the model to the original data possesses a second (local) minimum.
As well as demonstrating the importance of taking into account the noise in experimental datasets, our results could also be viewed as an endorsement for Bayesian methods, which do not seek to identify a single optimal parameter set, but instead approximate the whole posterior parameter distribution.
The results from our network inference investigation perhaps provide an even better illustration of the effect that data uncertainty can have upon inference.
Our approach demonstrates that the inference of an edge between two vertices is highly sensitive to the level of noise in the data, and hence it is likely that the false positive rate for each individual edge is high.
We also showed how GPR bootstrapping may be used to construct  high conﬁdence  relevance networks for which we would expect the false positive rate to be lower.
6 CONCLUSION Determining the effects of uncertainty in experimental data is imperative if we are to have any degree of conﬁdence in the conclusions that we draw or the models that we reverse engineer from biological data.
GPR bootstrapping is a widely applicable  1305  [17:03 8/4/2009 Bioinformatics-btp139.tex]  Page: 1305  1300 1306   P.D.W.Kirk and M.P.H.Stumpf  and easily implemented approach that allows us to investigate and quantify these effects.
We have illustrated the use of GPR bootstrapping using two examples, and discussed the impact that data uncertainty has upon inference.
Although we have here concentrated on time-course data, our approach could easily be applied in situations where the independent variable is something other than time.
Given the current levels of noise in post-genomic data, approaches such as GPR bootstrapping are vital in order to allow us to make the most of currently available information and to provide us with a means to assess the conclusions we draw.
Funding: Wellcome Trust (080713/Z/06/Z).
Conﬂict of Interest: none declared.
REFERENCES Aaronson,D.S.
and Horvath,C.M.
(2002) A road map for those who don t know JAK-  STAT.
Science, 296, 1653 1655.
Antoniak,C.E.
(1974) Mixtures of Dirichlet processes with applications to Bayesian  nonparametric problems.
Ann.
Stat., 2, 1152 1174.
Barenco,M.
et al.
(2006) Ranked prediction of p53 targets using hidden variable  dynamic modeling.
Genome Biol., 7, R25.
Butte,A.J.
et al.
(2000) Discovering functional relationships between RNA expression and chemotherapeutic susceptibility using relevance networks.
Proc.
Natl Acad.
Sci.
USA, 97, 12182 12186.
Efron, B.
(1979) Bootstrap methods: Another look at the jackknife.
Ann.
Stat., 7, 1 26.
Efron,B.
and Tibshirani,R.J.
(1993) An Introduction to the Bootstrap.
Chapman and  Hall, New York.
Felsenstein,J.
(1985) Conﬁdence limits on phylogenies: an approach using the bootstrap.
Evolution, 39, 783 791.
Ferguson,T.S.
(1973) A Bayesian analysis of some nonparametric problems.
Ann.
Stat.,  1, 209 230.
Gao,P.
et al.
(2008) Gaussian process modelling of latent chemical species: applications  to inferring transcription factor activities.
Bioinformatics, 24, i70 i75.
Horvath,C.M.
(2000) STAT proteins and transcriptional responses to extracellular  signals.
Trends Biochem.
Sci., 25, 496 502.
Imoto,S.
et al.
(2005) Residual bootstrapping and median ﬁltering for robust estimation of gene networks from microarray data.
In Computational Methods in Systems Biology, Vol.
3082, Springer-Verlag, Berlin, pp.
149 160.
Ji,X.
and Xu,Y.
(2006) libsres: a c library for stochastic ranking evolution strategy for  parameter estimation.
Bioinformatics, 22, 124 126.
Kerr,M.K.
and Churchill,G.A.
(2001) Bootstrapping cluster analysis: assessing the reliability of conclusions from microarray experiments.
Proc.
Natl Acad.
Sci.
USA, 98, 8961 8965.
Lawrence,N.D.
et al.
(2007) Modelling transcriptional regulation using Gaussian processes.
In Schölkopf,B.
et al.
(eds) Advances in Neural Information Processing Systems 19.
MIT Press, Cambridge, MA, pp.
785 792.
Lèbre,S.
(2007) Inferring dynamic genetic networks with low order independencies.
arXiv.org, arXiv:0704.2551v4, 1 36.
MacKay,D.J.C.
(1998) Introduction to Gaussian processes.
In Bishop,C.M.
(ed.)
Neural Networks and Machine Learning, NATO ASI Series.
Springer, Berlin, pp.
133 165.
Müller,P.
and Quintana,F.A.
(2004) Nonparametric Bayesian data analysis.
Stat.
Sci.,  19, 95 110.
Opgen-Rhein,R.
and Strimmer,K.
(2007) From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data.
BMC Syst.
Biol., 1, 37.
Press,W.H.
et al.
(2007) Numerical Recipes: The Art of Scientiﬁc Computing.
Cambridge  University Press, New York.
Rasmussen,C.E.
and Williams,C.K.I.
(2006) Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning).
The MIT Press, Cambridge, MA.
Runarsson,T.P.
and Yao,X.
(2000) Stochastic ranking for constrained evolutionary  optimization.
IEEE Trans.
Evol.
Comput., 4, 284 294.
Schäfer, J. and Strimmer, K. (2005) An empirical Bayes approach to inferring large-scale  gene association networks.
Bioinformatics, 21, 754 764.
Schäfer,J.
et al.
(2006) Reverse engineering genetic networks using the GeneNet  package.
R News, 6, 50 53.
Smith,S.M.
et al.
(2004) Diurnal changes in the transcriptome encoding enzymes of starch metabolism provide evidence for both transcriptional and posttranscriptional regulation of starch metabolism in Arabidopsis leaves.
Plant Physiol., 136, 2687 2699.
Stein,M.L.
(1999) Interpolation of Spatial Data : Some Theory for Kriging (Springer  Series in Statistics).
Springer, New York.
Swameye,I.
et al.
(2003) Identiﬁcation of nucleocytoplasmic cycling as a remote sensor in cellular signaling by databased modeling.
Proc.
Natl Acad.
Sci.
USA, 100, 1028 1033.  von Mises,R.
(1964) Mathematical Theory of Probability and Statistics.
Academic  Press, New York.
Yuan,M.
(2006) Flexible temporal expression proﬁle modelling using the Gaussian  process.
Comput.
Stat.
Data Anal., 51, 1754 1764.
1306  [17:03 8/4/2009 Bioinformatics-btp139.tex]  Page: 1306  1300 1306
