BIOINFORMATICS  Vol.
30 ECCB 2014, pages i386 i392 doi:10.1093/bioinformatics/btu443  Two-dimensional segmentation for analyzing Hi-C data Celine L evy-Leduc1,*, M. Delattre1, T. Mary-Huard1,2 and S. Robin1 1AgroParisTech/INRA MIA 518, 75005 Paris and 2UMR de G en etique V eg etale, INRA/Univ.
Paris-Sud/CNRS, 91190 Gif-sur-Yvette, France  ABSTRACT Motivation: The spatial conformation of the chromosome has a deep influence on gene regulation and expression.
Hi-C technology allows the evaluation of the spatial proximity between any pair of loci along the genome.
It results in a data matrix where blocks corresponding to (self-)interacting regions appear.
The delimitation of such blocks is critical to better understand the spatial organization of the chromatin.
From a computational point of view, it results in a 2D segmentation problem.
Results: We focus on the detection of cis-interacting regions, which appear to be prominent in observed data.
We define a block-wise segmentation model for the detection of such regions.
We prove that the maximization of the likelihood with respect to the block boundaries can be rephrased in terms of a 1D segmentation problem, for which the standard dynamic programming applies.
The perform- ance of the proposed methods is assessed by a simulation study on both synthetic and resampled data.
A comparative study on public data shows good concordance with biologically confirmed regions.
Availability and implementation: The HiCseg R package is available from the Comprehensive R Archive Network and from the Web page of the corresponding author.
Contact: celine.levy-leduc@agroparistech.fr  1 INTRODUCTION  Many key steps of the cell development and cycle, such as DNA replication and gene expression are influenced by the 3D struc- ture of the chromatin (Dixon et al., 2012).
The folding of the chromosome in the space defines chromosomal territories, the function of which has been studied for few years now (Lieberman-Aiden et al., 2009).
Typically, topologically associat- ing domains contain clusters of genes that are co-regulated (Nora et al., 2012).
Thus, the detection of chromosomal regions having close spatial location in the nucleus will provide insights for a better understanding of the influence of the chromosomal con- formation on the cells functioning.
Several  chromosome  conformation capture  technologies have been developed in the past decade, among which Hi-C is the most recent.
This technology is based on a deep sequencing approach and provides read pairs corresponding to pairs of genomic loci that physically interact in the nucleus (Lieberman- Aiden et al., 2009).
The raw measurement provided by Hi-C is therefore a list of pairs of locations along the chromosome, at the nucleotide resolution.
These measurement are often summarized as a square matrix Y, where Yi,j stands for the total number of read pairs matching in position i and position j, respectively.
Positions refer here to a sequence of  *To whom correspondence should be addressed.
non-overlapping windows of equal sizes covering the genome.
The number n of windows may vary from one study to another: Lieberman-Aiden et al.
(2009) considered an Mb resolution, whereas Dixon et al.
(2012) went deeper and used windows of 100 kb.
Blocks of higher intensity arise among this matrix, revealing both cis- and trans-interacting regions (Fraser et al., 2009).
Although both types of interaction are likely to exist, cis-inter- acting regions seem to be prominent in the data (see Dixon et al., 2012, and Figs 7 and 8, for instance), and some have been confirmed to host co-regulated genes (Nora et al., 2012).
Such regions result in block of higher signal along the diagonal of the data matrix.
The purpose of the statistical analysis is then to provide a fully automated and efficient strategy to determine these regions.
A first attempt was presented in Dixon et al.
(2012), where the author strategy is first to summarize the 2D data into a 1D index, called the directionality index, then to apply a regular hidden Markov model to the summary data to retrieve the segmentation.
In this article, we show that such a two-step strategy can be avoided, and that summarizing the data is not required to solve the segmentation problem.
Detecting diagonal blocks can be seen as a particular 2D segmentation issue.
The 2D segmentation has been widely investigated for the detection of contour with arbi- trary shape in images (see, for example, Darbon and Sigelle, 2006a, b  Hochbaum, 2001).
From a computational point of view, image segmentation is an open problem because no prede- fined ordering exists that could be used to provide exact and efficient algorithms.
Compared with contour detection, it is worth noticing that Hi-C data segmentation displays a specific pattern that did not receive any special attention from the image processing community.
One of our contributions is to prove that this 2D segmentation problem boils down to a 1D segmen- tation problem for which efficient dynamic programming algo- rithms apply (Bellman, 1961  Lavielle, 2005  Picard et al., 2005).
Our formulation of the problem also allows us to solve some non- block diagonal segmentation problems (see the end of Section 2.2).
The article is organized as follows.
In Section 2, we define a general statistical model for Hi-C data, which can deal with both raw and normalized data.
We prove that the maximum likelihood estimates of the block boundaries can be efficiently retrieved.
In Section 3, we first present an extensive simulation study to assess the performance of our approach on both simulated and resampled data.
We then apply the proposed methodology to the data studied by Dixon et al.
(2012), which are publicly available, and compare our results with their regions.
The package implementing the proposed method is presented in Section 4 where some open problems are also discussed.
ß The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com   2 STATISTICAL FRAMEWORK  2.2  Inference  Two-dimensional segmentation for analyzing Hi-C data  2.1 Statistical modeling  We first define our statistical model.
Because the Hi-C data matrix is symmetric, we only consider its upper triangular part denoted by Y, in which Yi j ð1   i   j   nÞ stands for the inten- sity of the interaction between positions i and j.
We suppose that all intensities are independent random variables with distribution   i j=EðYi jÞ  Yi j   pð    i jÞ   ð1Þ where the matrix of means ð i jÞ1 i j n is an upper triangular block diagonal matrix.
An example of such a matrix is displayed in Figure 1 (left).
Namely, we define the (half) diagonal blocks k ðk=1  .
.
.
K*Þ as D* k=fði  jÞ : t* D* 15    5t*  where true block boundaries and K* for the true number of blocks.
We further define E* 0 as the set of positions lying outside these blocks:  k 1   i   j   t* K* =n+1  k   1g for  05t*  1=t*  stand  ð2Þ  the        0=fði  jÞ : 1   i   j   ng \ [D* E*  k     ð3Þ  where A denotes the complement of the set A.
The parameters ð i jÞ are then supposed to be block-wise constant: k  k=1  .
.
.
K*    i j= * k  ð4Þ  if ði  jÞ 2 D* if ði  jÞ 2 E* 0:  = * 0  As for the distribution pð    i jÞ defined in (1), we will consider  Gaussian, Poisson or negative binomial distributions:  ðGÞ : ðPÞ : ðBÞ :  Yi j   Nð i j   2Þ  Yi j   Pð i jÞ  Yi j   NBð i j   Þ:  ð5Þ  The Gaussian modeling (G) will be typically used for dealing with normalized Hi-C data and the others [(P) to deal with raw Hi-C data, which are count and (B)] data.
In Models (G) and (B), note that the parameters   and   are assumed to be constant and depend neither on i nor on j.
Fig.
1.
Examples of block diagonal and extended block diagonal matrices ð i jÞ1 i j n. Left: Model (4), right: Model (9)  the block boundaries  We now consider the estimation of kÞ0 k K* in the case where the number of blocks K* is known.
ðt* Model selection issues will be discussed in Section 2.3.
We con- sider a maximum likelihood approach.
For an arbitrary set of blocks Dk, with boundaries ðtkÞ0 k K and parameters ð kÞ0 k K, the log-likelihood of the data satisfying (1) and (4) writes  X log pðYi j   ijÞ  ðYÞ= X X  1 i j n  K  =  k=1  ði jÞ2 Dk  X  ði jÞ2 E0  log pðYi j   kÞ+  log pðYi j   0Þ   where Dk and E0 are defined as in (2) and (3), respectively, except that the t*  ks are replaced by the tks.
Parameter estimation For given boundaries t0  .
.
.
tK, the esti- mation of the block parameters  k is straightforward for each of the distribution considered in (5).
Denoting  kðYi jÞ and  0ðYi jÞ the contribution of each data point to the log-likelihood (up to some constants), in Dk and E0, respectively, we get, for known parameters   and  0,  0 ðYi jÞ=  ðYi j    0Þ2   k ðYi jÞ=  ðYi j   YkÞ2   G  G kðYi jÞ=Yi j log ðYkÞ   Yk   P  P kðYi jÞ=    log ð +YkÞ+Yi j log ðYk=ð +YkÞÞ   B 0 ðYi jÞ=    log ð + 0Þ+Yi j log ð 0=ð + 0ÞÞ   B  0ðYi jÞ=Yi j log ð 0Þ    0   X  where Yk=  ði jÞ 2 Dk  Yi j=jDkj, for k in f1  .
.
.
Kg jAj denoting  the cardinality of the set A.
Dynamic programming algorithm Let us now consider the esti- mation of the boundaries t0  .
.
.
tK.
The objective function can be rewritten as follows:  X  ðYÞ=   X  k=1  K  X X  ði jÞ 2 Dk  K  X X  ði jÞ 2 E0   kðYi jÞ+   0ðYi jÞ  !
=  k=1  ði jÞ 2 Dk   kðYi jÞ+   0ðYi jÞ  ði jÞ 2 Rk  where Rk corresponds to the rectangle above Dk (see Fig.
1), namely, Rk=fði  jÞ : tk 1   j   tk   1  1   i   tk 1   1g.
(Note that R1 is empty.)
Note that the rectangles Rk do not overlap and that E0=[ Rk, so the last equality holds.
The important point here is that the objective function is now additive with respect to the successive intervals ftk 1  .
.
.
tk   1g  1   k   K.  k  Defining the gain function  X  X  Cðtk 1  tk   1Þ=   kðYi jÞ+   0ðYi jÞ   ð6Þ  ði jÞ 2 Dk  ði jÞ 2 Rk  we have to maximize w.r.t.
1=t05t15 .
.
.
5tK=n+1  X  K  Cðtk 1  tk   1Þ   k=1  i387   C.L  evy-Leduc et al.
which can be done using the standard dynamic programming recursion (Bellman, 1961).
For any 1   L   K and 15    n, we define  X  L  ILð Þ=  max  1=t05t15   5tL= +1  k=1  Cðtk 1  tk   1Þ  the value of the objective function for the optimal segmentation of the submatrix made of the first   rows and columns of Y into L blocks.
Clearly, we have I1ð Þ=Cð1   Þ,  I2ð Þ= max  15t15 +1  = max  15t15 +1  Cð1  t1   1Þ+Cðt1   Þ I1ðt1   1Þ+Cðt1   Þ  and, for 3   L   K,  ILð Þ= max  15tL 15 +1  IL 1ðtL 1   1Þ+CðtL 1   Þ:  ð7Þ  Hence, the optimal segmentation can be recovered with complex- ity OðKn2Þ, once the Cð   Þ have been computed.
Common parameters The optimization procedure described above applies when both  0 and   are known.
Estimates of these parameters can be obtained in the following way.
The es- timate ^ 0 of  0 can be computed as the empirical mean of the observations lying in the right upper corner of the matrix Y, for instance,  T0=fði  jÞ : 1   i   n=4 ð3n=4+1Þ   j   ng:  ð8Þ  As for the overdispersion parameter of the negative binomial 0   ^ 0Þ  distribution  , we computed ^  as follows: ^ = ^ 2 where ^  2 0 corresponds to the empirical variance of the observa- tions lying in the same right upper corner of the matrix Y as for ^ 0.
0=ð ^  2  Non-block diagonal segmentation problem Observe that a simi- lar procedure could be used for dealing with a more general matrix ð i jÞ1 i j n defined by   i j= * k = 0  k  *  if ði  jÞ 2 D* if ði  jÞ 2 R*  k  k=1  .
.
.
K*  k  k=2  .
.
.
K*   ð9Þ  where the diagonal blocks D* k are defined as above (see Fig.
1, right).
In this case, no prior estimation of any mean parameter is required, as each  0 * is specific to one single rectangle.
k and the rectangles R*  k  2.3 Model selection issue In the case where the value of K* in the model defined by (1) and (4) is known a priori ð ^tkÞ1 k  K* can be obtained from the recur- sion (7), which actually gives the values of ð ^tkÞ1 k K for all 1   K   Kmax, where Kmax is a given upper bound for the number of blocks.
If K* is unknown, it can be estimated by ^K defined as follows:  ^K=Argmax1 K Kmax IKðnÞ:  ð10Þ  This strategy is illustrated in the next section.
i388  3 RESULTS  Dixon et al.
(2012) studied intrachromosomal interaction matri- ces for various chromosomes in both the human genome and the mouse genome at different resolutions (20 and 40 kb) and iden- tified topological domains for each analyzed chromosome.
Both the data and the topological domains found by Dixon et al.
(2012) are available from the following Web page http:// chromosome.sdsc.edu/mouse/hi-c/download.html.
We worked on the same data, at a resolution 40 kb, to study the performance of our approach described above.
3.1 Application to synthetic data  We conducted several Monte Carlo simulations first on synthetic data and then on resampled real data to assess the sensibility of our method to block size and signal-to-noise ratio.
The synthetic data are generated by using the domains found by Dixon et al.
(2012) for Chromosome 19 of the cortex mouse.
As for the resampled data, they are generated by using the Hi-C data of the chromosomes of the human embryonic stem cells (hESCs) provided by Dixon et al.
(2012).
The different simulation strate- gies are further described hereafter.
(2012)  3.1.1 Fixed block design To evaluate the performance of our methodology in the negative binomial framework, we generated block diagonal matrices according to Model (5) (B) where ð i jÞ is defined by (4).
More precisely, we generated 50 block diagonal interaction matrices of size n = 300 with a structure inspired by the one found by Dixon et al.
for the inter- action matrix of Chromosome 19 of the mouse cortex.
The different parameters  * 0 and   are estimated from this including five diagon- matrix.
This al blocks such that  * 4=4:33   * 5=11:99   * 0=0:09 and  =0:67.
Then, for each simulated dataset, new matrices were derived by multiplying the  * ks by a constant c 2 f0:1  0:2  0:3  .
.
.
1g to reduce the signal-to-noise ratio.
For each simulated dataset and each constant, we com- puted ^K and the corresponding ^tks using the procedure described in Section 2.  resulted in matrices  3=7:92   *  1=2:87   *  2=4:85   *  k   *  The upper part of Figure 2 displays the histograms of the estimated change-points for c = 0.1, c = 0.2 and c = 0.5.
The black dots correspond to the true change-points, and the bars indicate the frequency of each estimated change-point.
One can observe that both the change-points and the number of change- points are well estimated even in low signal-to-noise ratio frame- works (except for c = 0.1).
The bottom part of Figure 2 displays the log-likelihood curves (up to some constants) with respect to K for the same values of c, obtained on a given simulated matrix.
The dotted line indicates the location of the estimated number of change-points.
Even when the signal-to-noise ratio is small, the estimated number of change-points ^K corresponds to the true number of change-points K*.
When the signal-to-noise ratio is too small, i.e.
for c = 0.1 here, some model selection issues arise.
Figure 2 shows that for such signal-to-noise ratio, the method provides some spurious change-points within the blocks having the lowest mean.
When c = 0.1, the value of the mean in the first diagonal block is very low (0.28) and very close to  0.
Nevertheless, when taking the true number of blocks, the true change-points are recovered.
We also assessed the performance   Two-dimensional segmentation for analyzing Hi-C data  .
0 1  8 0  .
.
6 0  4 0  .
2  .
0  0 0  .
0 0 0 9 2    d o o h                 0  50  100  150  200  250  300  i l  e k  i l    g o L  0 0 0 0 3    .
0 1  8 0  .
.
6 0  4 0  .
2  .
0  0 0  .
0 0 0 8 3    d o o h  i l                 0  50  100  150  200  250  300  e k  i l    g o L  0 0 0 0 4    0 1  .
8 0  .
.
6 0  4 0  .
2  .
0  0 0  .
0 0 0 0 5    d o o h                 0  50  100  150  200  250  300  0 0 0 5 5    i l  e k  i l    g o L  0 0 0 1 3    5  10 K  15  20  0 0 0 2 4    0 0 0 0 6    5  10 K  15  20  5  10 K  15  20  Fig.
2.
First line: Histograms of the estimated change-points in a fixed block design for different signal-to-noise ratios in the negative binomial framework (from left to right: c = 0.1, c = 0.2, c = 0.5).
The dots correspond to the true change-points, and the bars indicate the frequency of each estimated change-points.
Second line: plots of the log-likelihood as a function of the number of change-points for one simulated dataset in the negative binomial framework for different signal-to-noise ratios (from left to right: c = 0.1, c = 0.2, c = 0.5).
The dotted and solid lines give the value of the log-likelihood (up to some constants) for ^K and K*, respectively  of our methodology in the Poisson framework, and we obtained similar results, which are not reported here.
3.1.2 Resampling of the data In this second analysis, we first get the boundaries found by Dixon et al.
(2012) in all the chromo- somes of the hESCs.
We shall call the corresponding blocks the Ren domains.
From these domains, we generate a set of diagonal blocks ðD1  :::  DKÞ, such that (i) the size of each block is drawn in the empirical distribution of Ren domain lengths and (ii) the cumulated number of positions is not4300.
Once the block sizes are drawn, we choose at random a human chromosome, and for each diagonal block Dk, a Ren domain in this chromosome is randomly selected, and observations in block Dk are resampled from the Ren domain data.
Accordingly, the data outside the diagonal blocks are simulated by resampling from the data of the E0 Ren domain in the selected chromosome.
This strategy is repeated 100 times to obtain 100 interaction matrices.
Compared with the previous simulation design, one can observe that the change-point positions now change from one dataset to the other, and that the data are not anymore simulated according to a negative binomial distribution.
While the statis- tical analysis of datasets generated from this second simulation setting is more difficult, it allows one to visit more realistic data configurations closely similar to real data.
We report here the results obtained when the simulated data are analyzed with Model (5) (B), the results obtained with Model (5) (P) being similar.
Figure 3 (left and center) displays two log-likelihood curves (up to some constants) as a function of the number of change- points.
The solid and dotted lines indicate locations of the true and estimated number of change-points, respectively.
One can  observe that while the maximum is not always achieved at the true number of change-points K , the estimated value ^K corres- ponding to the maximum likelihood is still fairly close to K .
The true and estimated numbers of change-points are identical for 91 of the 100 simulations, and the absolute difference j ^K   K j is never 42 except for one example.
To further assess the quality of the estimated segmentation compared with the true one, we computed the Hausdorff dis- tance between these two segmentations defined in the segmenta- tion framework as follows, see Boysen et al.
(2009) and Harchaoui and L evy-Leduc (2010):                          d t*  ^t  =max d1 t*  ^t    d2 t*  ^t     where t*=ðt*  1  .
.
.
t*  K*Þ  ^t=ð ^t1  .
.
.
^t ^KÞ and ja   bj  d1 a  bð  Þ= sup b 2 b  inf a 2 a  d2 a  bð  Þ=d1 b  að Þ:  ð11Þ  ð12Þ  ð13Þ  A small value of d2 (distance from true to estimate) means that an estimated change-point is likely to be close to a true change- point.
A small value of d1 (distance from estimate to true) means that a true change-point is likely to be close to each estimated change-point.
A perfect segmentation results in both null d1 and d2.
Oversegmentation results in a small d2 and a large d1.
Undersegmentation results in a large d2 and a small d1, provided that the estimated change-points are correctly located.
The two parts d1 and d2 of the Hausdorff distance were computed in the right part of Figure 3.
Both distances d2 ( true to estimate ) and d1 ( estimate to true ) were not 41 for 96 of the 100 simulations.
i389   C.L  evy-Leduc et al.
0 0 9 1    d o o h  i l  0 0 1 2    e k  i l    g o L  0 0 4 3    d o o h  i l  e k  i l    g o L  0 0 8 3    0 0 3 2    0  10  20 K  30  40  0 0 2 4    0  10  20 K  5 1  0 1  5  0  30  40  True to Estimate  Estimate to True  Fig.
3.
Left, center: Two examples of a log-likelihood curve (up to some constants) as a function of the number of change-points.
Solid and dotted lines indicate the true and estimated number of change-points, respectively.
Right: Two parts of the Hausdorff distances computed by taking the true (respectively the estimated) segmentation as reference  0 5 1  s t  i  n o p   e g n a h c   f  0 0 1  o     b N  0 5  14  18  16 Chromosome  20  22  Fig.
4.
Number of change-points for the Chromosomes 13 22 found by the Bing Ren approach ( * ), by HiCseg with Model (5) (P) (   ) and (5) (B) ( 䉭 )  3.2 Application to real data  In this section, we applied our methodology to the raw inter- action matrices of Chromosomes 13 22 of the hESCs at reso- lution 40 kb, and we compared the estimated number of blocks and the estimated change-points found with our approach to those obtained by Dixon et al.
(2012) on the same data, as no ground truth is available for those datasets.
From Figure 4, we can first see that the approach of Dixon et al.
(2012) tends to produce, in general, more change-points than our strategy except for Chromosome 22.
This can also be seen in Figure 5, which displays the log-likelihood curves (up to some constants) with respect to K as well as the number of change-points proposed by Dixon et al.
(2012) (dotted line) and our approach (solid line).
We also compared both methodologies by computing the two parts of the Hausdorff distance defined in (12) and (13) for Chromosomes 13 22.
More precisely, Figure 6 displays the box- plots of the d1 and d2 parts of the Hausdorff distance without taking the supremum.
We can observe from this figure that some differences exist between the segmentations produced by the two approaches, but that the boundaries of the blocks are close.
To further illustrate the differences that exist between both approaches, we display in Figures 7 and 8 the segmentations provided by both approaches in the case of Chromosomes 17 and 19, respectively.
In the case of Chromosome 17, we can only provide the segmentation obtained with Model (5) (P) because the overdispersion parameter ^  is infinite (the mean  i390  5 0 + e 5    d o o h  5 0 + e 7    i l  e k  i l    g o L  5 0 + e 9    0 0 0 0 5 3    d o o h  i l  e k  i l    g o L  0 0 0 0 5 4    0  100  200  K  300  400  500  0  100  200 K  300  400  Fig.
5.
Left: Log-likelihood (up to some constants) as a function of K for the analysis of Chromosome 15 using Model (5) (P).
The dotted vertical lines is the number of blocks chosen by the Dixon et al.
(2012) approach, and the solid one correspond to the one of our approach.
Right: The same for Chromosome 19 using Model (5) (B)  (a)  0 6  0 4  0 2  0  (c)  0 0 2  0 5 1  0 0 1  0 5  0  (b)  0 4  0 3  0 2  0 1  0  13  15  18  Chromosome  20  22  13  15  18  Chromosome  20  22  (d)  0 3  5 2  0 2  5 1  0 1  5  0  13  15  18  Chromosome  20  22  13  15  18  Chromosome  20  22  Fig.
6.
Boxplots for the infimum parts of the Hausdorff distances (left part) and d2 d1 change-points (right part) between the found by Dixon et al.
(2012) and our approach for Chromoso- mes 13 22 for Model (5) (P) [(a) and (b)] and for Model (5) (B) [(c) and (d)]   Two-dimensional segmentation for analyzing Hi-C data  under the hypothesis that all observations from T0 arise from the same Poisson distribution.
(5)  Following this rule, we chose Model  (B) only for Chromosomes 1 and 2.
We can see from this figure that with the naked eye, the diagonal blocks found with our strategy pre- sent a lot of similarities with those found by Dixon et al.
(2012).
We did not report the segmentations that we obtained for the Chromosomes 1 22, but they are available from the Web page of the corresponding author http://www.agroparistech.fr/mmip/ maths/essaimia/_media/equipes:membres:page:supplementary_ eccb.pdf.
4 CONCLUSION  4.1 HiCseg R package In this article, we propose a new method for detecting cis-inter- acting regions in Hi-C data and compare it with a methodology proposed by Dixon et al.
(2012).
Our approach described in Section 2 is implemented in the R package HiCseg, which is available from the Web page of the corresponding author http://www.agroparistech.fr/mmip/maths/essaimia/_media/equip es:membres:page:hicseg_1.1.tar.gz and from the Comprehensive R Archive Network.
In the course of this study, we have shown that HiCseg is an efficient technique for achieving such a segmentation based on a maximum likelihood approach.
More precisely, HiCseg package has two main features, which make it attractive.
Firstly, it gives access to the exact solution of the maximum likelihood approach.
Secondly, as we can see from Figure 9 and Table 1, which give the computational times on synthetic data following Models (5) (G), (P) or (B), HiCseg is computationally efficient, which makes its use possible on real data coming from Hi-C experiments.
Note that the computational times of Figure 9 were obtained with a computer having the following configuration: RAM 3.8 GB, CPU 1.6 GHz and those of Table 1 with a computer having the following configuration: RAM 33 GB, CPU 8  2.3 GHz.
4.2 Open questions  Our methodology could be extended, both to improve the algorithmic efficiency of our method and the modeling of the data.
On the one hand, all available approaches work with data binned at the resolution of several kb.
However, the original data are collected at the nucleotide resolution.
One of the main challenges would be to alleviate the computational burden of the algorithm to fully take advantage of the Hi-C technology high resolution.
Recent advances in segmentation algorithms for 1D data, such as those proposed by Killick et al.
(2012) or Rigaill (2010), seem promising for dealing with this issue.
On the other hand, the modeling could be improved in two directions.
First, as observed by Phillips-Cremins et al.
(2013), Hi-C interaction matrices display a hierarchical structure corres- ponding to regions interacting at different scales.
The proposed segmentation model does not account for such a structure but could be improved in such a direction.
Second, a more refined modeling of the dispersion could be considered.
While assuming a common dispersion parameter for non-diagonal blocks is  i391  Fig.
7.
Topological domains detected by Dixon et al.
(2012) (lower tri- angular part of the matrix) and by our method (upper triangular part of the matrix) from the interaction matrix of Chromosome 17 of the hESCs using Model (5) (P)  Fig.
8.
Topological domains detected by Dixon et al.
(2012) (lower tri- angular part of the matrix) and by our method (upper triangular part of the matrix) from the interaction matrix of Chromosome 19 of the hESCs using Model (5) (P)  ði jÞ 2 T0  P P  and the variance outside the diagonal blocks are of the same order).
In the other case where Models (5) (P) and (B) can be applied, we used the following test procedure for overdispersion under the Poisson model to decide between both segmentations.
Considering the data lying in T0 as defined in (8), we first estimate the mean within this region by ^ = Yi=N0 where N0 stands for the number of data points within T0.
We statistic i =N0.
Reminding that, if Y has a Poisson distri- Q0= bution with mean  , we and VðY2Þ=4 3+6 2+ , it follows that q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Q0   ð ^ + ^ 2Þ 4 ^ 3+6 ^ 2+ ^   EðY2Þ= + 2    Nð0  1Þ  then consider  ﬃﬃﬃﬃﬃﬃ p  ði jÞ 2 T0  have  test  the  N0  Y2   C.L  evy-Leduc et al.  )
s d n o c e s   n i (   e m T  i  4  3  2  1  0  200  400  600 n  ACKNOWLEDGEMENTS  The authors would like to thank the French National Research Agency ANR, which partly supported this research through the ABS4NGS project.
Funding: Part of this work was supported by the ABS4NGS ANR project (ANR-11-BINF-0001-06).
Conflicts of interest: none declared.
800  1000  1200  REFERENCES  Bellman,R.
(1961) On the approximation of curves by line segments using dynamic  programming.
Commun.
ACM, 4, 284.
Boysen,L.
et al.
(2009) Consistencies and rates of convergence of jump penalized  least squares estimators.
Ann.
Stats., 37, 157 183.
Darbon,J.
and Sigelle,M.
(2006a) Image restoration with discrete constrained total variation part I: Fast and exact optimization.
J.
Math.
Imaging Vision, 26, 261 276.
Darbon,J.
and Sigelle,M.
(2006b) Image restoration with discrete constrained total variation part II: Levelable functions, convex priors and non-convex case.
J.
Math.
Imaging Vision, 26, 277 291.
Dixon,J.R.
et al.
(2012) Topological domains in mammalian genomes identified by  analysis of chromatin interactions.
Nature, 485, 376 380.
Fraser,J.
et al.
(2009) Chromatin conformation signatures of cellular differentiation.
Genome Biol., 10, R37.
Harchaoui,Z.
and L evy-Leduc,C.
(2010) Multiple change-point estimation with a  total variation penalty.
J.
Am.
Statis.
Assoc., 105, 1480 1493.
Hochbaum,D.S.
(2001) An efficient algorithm for image segmentation, markov  random fields and related problems.
J. ACM, 48, 686 701.
Killick,R.
et al.
(2012) Optimal detection of changepoints with a linear computa-  tional cost.
J.
Am.
Statis.
Assoc., 107, 1590 1598.
Lavielle,M.
(2005) Using penalized contrasts for the change-point problem.
Signal  Proc., 85, 1501 1510.
Lieberman-Aiden,E.
et al.
(2009) Comprehensive mapping of long-range inter- actions reveals folding principles of the human genome.
Science, 326, 289 293.
Nora,E.P.
et al.
(2012) Spatial partitioning of the regulatory landscape of the x-  inactivation centre.
Nature, 485, 381 385.
Phillips-Cremins,J.E.
et al.
(2013) Architectural protein subclasses shape 3D organ-  ization of genomes during lineage commitment.
Cell, 153, 1281 1295.
Picard,F.
et al.
(2005) A statistical approach for array CGH data analysis.
BMC  Bioinformatics, 6, 27. www.biomedcentral.com/1471-2105/6/27.
Rigaill,G.
(2010) Pruned dynamic programming for optimal multiple change-point  detection.
ArXiv, 1004.0887.
Fig.
9.
Computational times for Model (5) (G) (   ), (P) (   ) and (B) (   )  Table 1.
Computational times (in seconds) for Model (5) (G), (P) and (B)  n  1000  2000  3000  4000  5000  6000  7000  (G) (P) (B)  1.96 1.92 1.95  17.01 16.47 16.60  60.56 57.22 58.07  143.68 134.91 135.52  280.53 264.15 264.62  513.87 453.99 457.15  834.01 755.21 783.05  sensible because the signal is very low (and therefore, there is little room for large changes in dispersion), the strategy that we propose could incorporate non-homogeneous dispersion param- eters for the diagonal blocks.
This could be achieved, for in- stance, by estimating a dispersion parameter per diagonal block.
Note that these two extensions could be implemented in the same efficient algorithmic framework as the one proposed in the article.
These extensions will be the subject of a future work.
i392
