BIOINFORMATICS ORIGINAL PAPER Vol.
24 no.
16 2008, pages 1772 1778  doi:10.1093/bioinformatics/btn308  Sequence analysis Memory-efﬁcient dynamic programming backtrace and pairwise local sequence alignment Lee A. Newberg1,2 1Center for Bioinformatics, Wadsworth Center, New York State Department of Health, Albany, NY 12208-3425 and 2Department of Computer Science, Rensselaer Polytechnic Institute, Troy, NY 12180-3590, USA Received on April 23, 2008  revised on June 5, 2008  accepted on June 10, 2008 Advance Access publication June 16, 2008 Associate Editor: Thomas Lengauer  ABSTRACT Motivation: A backtrace through a dynamic programming algo- rithm s intermediate results in search of an optimal path, or to sample paths according to an implied probability distribution, or as the second stage of a forward backward algorithm, is a task of fundamental importance in computational biology.
When there is insufﬁcient space to store all intermediate results in high-speed memory (e.g.
cache) existing approaches store selected stages of the computation, and recompute missing values from these checkpoints on an as-needed basis.
Results: Here we present an optimal checkpointing strategy, and demonstrate its utility with pairwise local sequence alignment of sequences of length 10 000.
Availability: Sample C++-code for optimal backtrace is available in the Supplementary Materials.
Contact: leen@cs.rpi.edu Supplementary information: Supplementary data is available at Bioinformatics online.
are  the  that of  examples  computation.
Typical  1 INTRODUCTION Dynamic programming algorithms are often used to ﬁnd an optimal solution by backtracking through intermediate values of chain matrix multiplication, string algorithms such as longest common subsequence, the Viterbi (1967) algorithm for hidden Markov models, and sequence alignment algorithms such as those of Needleman and Wunsch (1970) and Smith and Waterman (1981).
Dynamic programming algorithm backtraces are also used for random sampling, where the score for each possible backtrace path is deemed to be (proportional to) the probability of the path, and it is desired to choose a path according to that probability distribution.
A typical example is the algorithm of Ding and Lawrence (1999) for the sampling of RNA secondary structure.
A third use for dynamic programming backtraces is as the second step of a forward  backward algorithm, such as that of Baum Welch (Baum et al., 1970) for ﬁnding the parameters of a hidden Markov model.
Sometimes a trade-off with run time allows a problem to be solved without a backtrace through stored results, e.g.
sequence alignment (Durbin et al., 2006, Section 2.6  Myers and Miller, 1998  Waterman, 1995, page 211) and Baum Welch (Miklós and Meyer, 2005), but this is not always the case.
When there is not enough space to store all intermediate results in high-speed memory, checkpointing strategies are employed, whereby selected stages of the computation are stored, and missing information is recomputed from these checkpoints on an as- needed basis.
A stage of the computation, also known as a frontier, is a set of intermediate values that are sufﬁcient for making subsequent computations.
For instance, in a 2D dynamic programming algorithm that computes a small number of values for each (i,j) in a grid from the neighboring  earlier  values associated with (i 1,j), (i,j 1) and (i 1,j 1), we could deﬁne a stage as a row of the computation grid.
In this case, stage k would be the values associated with the cells  (k,j): j= jmin ...jmax , and the  stage k values would be sufﬁcient for computing values for cell (i,j) for any i   k. Similarly one could use columns to deﬁne stages.
In many cases it makes sense to have overlapping stages  in the above example stage k might be the k-th diagonal frontier, i.e.
the computation values associated with the cells  (i,j): i+j  k 1,k  .
Herein we will describe an optimal checkpointing strategy that provably minimizes the number of stage re-computations necessary in performing a backtrace with limited high-speed memory.
The algorithm is simple and efﬁcient.
Note that, because this limited- memory approach can be used to allow signiﬁcant increases in locality of reference, it can provide more efﬁcient computations even when the amount of high-speed memory might otherwise be considered sufﬁcient.
We build upon a previous approach that is fairly memory-efﬁcient, which is described in Bioinformatics (Grice et al., 1997  Wheeler and Hughey, 2000).
With memory enough to store M stages, their  2-level  algorithm uses the memory to compute the ﬁrst M stages, but then retains only the M-th stage as a checkpoint, discarding the previous ones.
Using the remaining M 1 memory locations, the algorithm computes stages M+1,...,2M 1, and then uses the (2M 1)th stage as the second checkpoint.
It continues this process, using the (M+(M 1)+(M 2))th stage as its third checkpoint, and so forth, up to and including M+(M 1)+   +1= M(M+1)/2 as its M-th checkpoint.
Thus, if N = M(M+1)/2 stages are needed   in the backtrace, they can be achieved with M=O( N) memory locations  in the backtrace, each missing stage is computed using the space freed by discarding the checkpoints that are no longer needed.
Because the algorithm needs to compute each stage at most twice, once in the forward pass to create the checkpoints and once during the backtrace, the overall number of stage computations of the    2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[16:24 8/8/03 Bioinformatics-btn308.tex]  Page: 1772  1772 1778   memory-reduced algorithm is at most double what it would have been.
Dynamic programming backtrace  Wheeler and Hughey (2000) also generalize their 2-level algorithm to an  L-level  algorithm, where L is any positive integer.
With M memory locations, the L-level algorithm can compute  (cid:2)  (cid:1) NWH(M,L)= (cid:3)  M+L 1 (cid:4)  L  M     ML L!
(1)  stages, where this formula works for any integers L and M so long is deﬁned to be n!/d!
(n d)!
when as the binomial coefﬁcient d 0 and n d 0, and zero otherwise.
The asymptotic limit is as M   for ﬁxed L. For the M, L 1 algorithm, the k-th stage to be checkpointed is  n d  Ck(M,L)= k(cid:5)  j=1  NWH(M (j 1),L 1)  (2)  That is, the ﬁrst stage to be a checkpoint is the last stage that would be a checkpoint under the (L 1)-level algorithm.
Generally, the k-th stage to be checkpointed is beyond the (k 1)th checkpoint by an amount that would be the last checkpoint for an (L 1)-level algorithm that uses the remaining M (k 1) memory locations.
Equation (2) solves to  Ck(M,L)= NWH(M,L) NWH(M k,L) .
(3)  The number of stage computations for the L-level algorithm to compute NWH(M,L) stages using M memory locations is given by the recursion  TWH(M,L)        =  M  (cid:3)  (cid:9) NWH(M,L)+ k=1 M  TWH(k,L 1) 1  (cid:4)  if L=1 and if L 2,  (4)  it  the L-level checkpoints   because the L-level algorithm ﬁrst computes all NWH(M,L) stages, to get then provides access to the stages in reverse order by working with the L-level checkpoints the L-level algorithm uses the (L 1)-level in reverse order  algorithm to generate the missing intervening stages.
However, 1 is subtracted, because the last computation of each (L 1)-level algorithm invocation produces an L-level checkpoint that we already had available.
Thus, this last computation for each (L 1)-level algorithm invocation is not performed.
Equation (4) solves to  (cid:2)  +1  M+L 1 L 1  (cid:2)  (cid:1)   (cid:2) M 1 M  +1  TWH(M,L)  (cid:1)  =  M+L 1 (cid:1) 1 M 1 L 1  = NWH(M,L) M     ML (L 1)!
.
(cid:1)  L  (cid:2)  (5)  where this formula works for any integers L and M so long as the is deﬁned to be (a+b+c)!/a!b!c!
trinomial coefﬁcient when a,b,c 0, and zero otherwise.
Thus we have a multiplier for  a+b+c c a  b  Fig.
1.
Low memory comparison of algorithms.
This ﬁgure exhibits the effect on the run time at low memory levels.
Clockwise from the top, the curves come in 12 pairs, one each for M=2,3,4,5,6,7,8,9,10,12,15 and 20 memory locations.
Within each pair, the curve for the L-level algorithm of Wheeler and Hughey (2000) is ﬁrst  as M increases these curves become increasingly  fractal , with jumps in the run time at several scales.
The curve for the optimal checkpointing algorithm is second in each pair  these curves are piecewise linear.
L!N the number of stage computations of approximately L for M  L memory locations.
Computed values of NWH(M,L) and TWH(M,L) for small M and L are given in Table 1 and plotted in Figure 1.
The main drawback to the L-level algorithm is that it can perform badly for a value of N that falls between NWH(M,L 1) and NWH(M,L), for some L. Wheeler and Hughey (2000) propose an  (L,L 1)-level  algorithm that performs better these intermediate values of N, but it is not optimal.
Wheeler and Hughey (2000) also discuss (L,L 1,...)-level algorithms and an optimal checkpointing algorithm, but do not provide a quick computation for choosing optimal checkpoints, nor do they give formulae to compute the number of stage computations for general values of M and N.  for  2 METHODS The choice of the stages to checkpoint can be framed as an optimization problem.
We write T(M,N) for the number of stage computations that are needed by the optimal checkpointing algorithm for a backtrace through N stages, using room for M stages.
When N   M, there is ample memory, and T(M,N)= N. At the other extreme, if M=1 there is room to compute only one stage, and N  1 stages cannot be computed even with an inﬁnite amount of time available.
[Following the lead of Wheeler and Hughey (2000) we bar in-place calculations.]
If N   M  1, and if we choose C as the ﬁrst stage to checkpoint, then we begin by computing the ﬁrst C stages, 1,...,C, by alternating the use of two memory locations.
We store stage C, discarding the previous ones.
Then, using the remaining M 1 memory locations, we recursively perform an optimal backtrace on the ﬁnal N C stages, C+1,...,N. Next, we present the retained stage C to the user.
Finally, we discard stage C and recursively perform an optimal backtrace on the initial C 1 stages, using the full M  1773  [16:24 8/8/03 Bioinformatics-btn308.tex]  Page: 1773  1772 1778   L.A.Newberg  Table 1.
The number of stages and run time for both the algorithm of Wheeler and Hughey (2000) and the optimal checkpointing algorithm  Alg.
L  T WH(M,L)/N WH(M,L)  T opt(M,L)/N opt(M,L)  7  6  5  4  3  2  1 M=1 1/1 1/1 1/1 1/1 1/1 1/1 3/2 6/4 22/7 16/6 4/3 2 2/2 3/3 13/8 113/28 71/21 3 3/3 9/6 4/4 22/13 211/56 379/84 4 4/4 16/10 5/5 33/19 5 5/5 25/15 505/126 1009/210 6/6 46/26 6 6/6 36/21 141/56 421/126 1051/252 2311/462 7 7/7 49/28 217/84 721/210 1981/462 4753/924 10297/1716 7/7 61/34  1/1 1/1 11/5 7/4 41/15 21/10 46/20 106/35 85/35 225/70  1/1 29/8 169/36 631/120 1849/330 4621/792  1  2  3  4  5  6  7  1/1 1/1 56/14 12/6 308/63 34/15 1092/203 70/29 3066/539 123/49 7392/1253 196/76 292/111 1020/293 2910/671 7194/1385 15 972/2639  1/1 1/1 1/1 42/12 30/10 20/8 203/48 125/35 70/24 350/90 644/139 170/54 343/104 798/195 1638/335 616/181 1596/377 3612/713  For the L-level backtracking algorithm of Wheeler and Hughey (2000) with memory suitable for storage of M stages, the left side of this table shows both NWH(M,L), the number of stages that can be produced in reverse order, and TWH(M,L), the number of stage computations required for that backtrace.
[See Equations (1) and (5).]
For instance, to perform a backtrace on N =36 stages with M=3 memory locations requires the (L=7)-level algorithm and requires T =169 stage computations.
It is not straightforward to predict the number of stage computations for other values of N (Fig.
1).
For the optimal checkpointing algorithm presented here, the right side of this table shows both Nopt(M,L), a number of stages that can be produced in reverse order, and T(M,Nopt(M,L)), the number of stage computations required for that backtrace.
[See Equations (7) and (11).]
When the number of stages is between Nopt(M,L) and Nopt(M,L+1), the optimal number of stage computations T(M,N) is computed via linear interpolation.
For instance, to do the backtrace on N =36 stages with M=3 memory locations, we observe that N falls between Nopt(M=3,L=5)=35 and Nopt(M=3,L=6)=48.
Thus, the algorithm requires T(M=3,N =36)= T(M=3,N =35)+6(36 35)=131 stage computations.
Thus, in this case, the number of stage computations for the L-level algorithm of Wheeler and Hughey (2000) is 29% higher.
memory locations.
Thus, with an optimal choice for C, we have the recursion for T(M,N):                T(M,N)=      C+T(M 1,N C) +T(M,C 1)  N +   minC  (6)  if N   M,    if N   M=1,   if N   M  1.
Note that Wheeler and Hughey (2000) give a different recursion for optimal checkpointing.
Translated into our notation, their recursion is T(M,N)=minC C+T(M 1,N C)+T(M,C) 1 .
This error may have impeded their further progress.
A straightforward computation of this recursion would require O(MN 2) time (Wheeler and Hughey, 2000).
However, in the following we will show a mathematical solution to the recursion that permits the calculation of all the needed checkpoints in O(N) time.
As with the analysis of the L-level algorithm of Wheeler and Hughey (2000), we ﬁnd it easier to initially restrict our attention to special values of N. The main contribution of this work is our subsequent generalization to arbitrary values of N.  2.1 Special values for the number of stages With storage for M 1 stages, and for any integer L 1, we will deﬁne a special number of stages Nopt(M,L), and we will calculate Topt(M,L), the number of stage computations required for a backtrace through Nopt(M,L) (cid:2) stages using M memory locations.
Let + (cid:2)(cid:1)  Nopt(M,L)=  M+L 1  (cid:1) (cid:1)  (cid:1)  (7)  L  M+L 2 L 1 L  (cid:2)  1 (cid:2)  1 M     ML L!
M+L 1  =  1+  M+L 1 For M,L  1, with N = Nopt(M,L) stages, we set  L  C(M,N)= Nopt(M,L 1)+1= N Nopt(M 1,L) ,  .
(8)  the unique optimum checkpoint the problem with M memory locations and N = Nopt(M,L) stages (see Appendix for proofs).
stage for  1774  Plugging C(M,N)= Nopt(M,L 1)+1 and N C(M,N)= Nopt(M 1,L) into Equation (6), we obtain  Topt(M,L)            =  This solves to  Topt(M,L) =  N Nopt(M,L 1)+1 +Topt(M 1,L)+Topt(M,L 1)  if L=1 or M=1, and  if L,M  1.
(cid:1)  +  (cid:1)  (cid:2) M+L 1 1 M 1 L 1 (cid:2) (cid:1) M+L 2  2 L 2 (cid:4)(cid:1) = (cid:3) Nopt(M,L)+1 (cid:1)  M 1 M  L  (cid:2)  (cid:2)  M+L 2 1 M 1 L 2 (cid:2)  (9)  (10)  (11)     1+  1  (M 1)(M+2L 1)  M     ML  (L 1)!
,  where Equation (11) is deﬁned for M 2 only.
Computed values of Nopt(M,L) and Topt(M,L) for small M and L are given in Table 1 and plotted in Figure 1.
As with the L-level algorithm of Wheeler and Hughey (2000), with the   optimal checkpointing algorithm, we have a multiplier for the number of stage computations of approximately L for M  L L!N memory locations.
However, we shall now see that, with the optimal checkpointing algorithm, we easily achieve this multiplier for the number of stage computations even when the number of stages N is arbitrary.
2.2 General values for the number of stages When N falls between two optimal values, Nopt(M,L) and Nopt(M,L+1), we can compute the number of stage computations T(M,N) by linear interpolation between Nopt(M,L) and Nopt(M,L+1) (see Appendix for proofs).
Noting that  T(M,Nopt(M,L+1)) T(M,Nopt(M,L))  Nopt(M,L+1) Nopt(M,L)  = L+1  (12)  [16:24 8/8/03 Bioinformatics-btn308.tex]  Page: 1774  1772 1778   void backtrace(  Dynamic programming backtrace  BIGINT Mckpt, BIGINT M, BIGINT Nckpt, BIGINT N, BIGINT L, BIGINT Nopt, void (*advance)(BIGINT Mfrom, BIGINT Mto, BIGINT Nto, void *p), void (*available)(BIGINT M, BIGINT N, void *p), void *p)  // previous checkpoint memory location // M unused memory locations // previous checkpoint stage // backtrace through N stages // applicable level // Nopt(M, L)  // See the ﬁgure legend for advance, available, and p  while (N   M)    // Nopt(M   1, L) BIGINT Noptm  // Nopt(M, L   1) BIGINT Noptl   // Nckpt + C is the next stage to checkpoint BIGINT C  Noptm = ((Nopt + 1)(M   1)(M + 2L   2))/((M + 2L   1)(M + L   2))   1  Noptl = ((Nopt + 1)L(M + 2L   3))/((M + 2L   1)(M + L   2))   1  C = min(Nopt + 1, N   Noptm)  // Compute stage Nckpt + C from stage Nckpt, by alternate use of two memory locations Mckpt + 1 and Mckpt + 2 (*advance)(Mckpt, Mckpt + 1 + ((C   1)%2), Nckpt + 1, p)  for (BIGINT i = 2  i   C  ++i)  (*advance)(Mckpt + 2   ((C   i)%2), Mckpt + 1 + ((C   i)%2), Nckpt + i, p)  // Backtrace through stages Nckpt + C + 1, .
.
.
, Nckpt + N backtrace(Mckpt + 1, M   1, Nckpt + C, N   C, L, Noptm, advance, available, p)  // Present stage Nckpt + C to the user (*available)(Mckpt + 1, Nckpt + C, p)  // Backtrace through stages Nckpt + 1, .
.
.
Nckpt + C   1 via tail recursion N = C   1  L = L   1  Nopt = Noptl     // Handle ample memory case, N   M for (BIGINT i = 1  i   N  ++i) for (BIGINT i = N  i   1    i)  (*advance)(Mckpt + i   1, Mckpt + i, Nckpt + i, p)   (*available)(Mckpt + i, Nckpt + i, p)         (cid:14)  L: Nopt(M,L)  N  (cid:15) Fig.
2.
The optimal checkpointing algorithm in pseudo-C++, for a backtrace through N stages using memory sufﬁcient for M stages.
Using Equation (7), ﬁnd .
For the convention that the memory locations are labeled 0,...,M 1 and the stages are labeled 0,...,N 1, invoke the level L=max backtrace( 1, M,  1, N, L, Nopt(M,L), advance, available, p)  where advance is a pointer to a callback function that computes stage Nto, to be stored in memory location Mto, from the immediately preceding stage, which is stored in memory location Mfrom unless Nto is the ﬁrst stage  where available is a pointer to a callback function invoked during backtrace so that the user can make use of stage N, stored in memory location M  and where p is a user-supplied pointer to applicable stage-independent information.
BIGINT should be an integer type able to handle integers a little larger than NM2.
Note that, although the backtrace routine directs the callback routines on the use of the memory locations, the actual allocation and access of the memory is not handled by the backtrace routine.
Further, note that if the generality is not required, the pointer parameters, advance, available and p, can be eliminated, and their use in the body of the function can be replaced by  hard-wired  calls to appropriate functions.
See the Supplementary Materials for C++ source code.
we derive  T(M,N)= T(M,Nopt(M,L))+(L+1)(N Nopt(M,L)) ,  (13)  for N   M  1.
Furthermore, for N between Nopt(M,L) and Nopt(M,L+1), it is optimal to choose the initial checkpoint C(M,N) so that C(M,N) 1 and N C(M,N) fall between the values that they would have had to equal, if N had equaled Nopt(M,L) or Nopt(M,L+1).
That is, we must choose C(M,N) so as to  simultaneously satisfy  Nopt(M,L 1)  C(M,N) 1  Nopt(M,L)  and  Nopt(M 1,L)  N C(M,N)  Nopt(M 1,L+1) .
In practice, we choose the largest legal value,  C(M,N)=min Nopt(M,L)+1,N Nopt(M 1,L)  .
(14)  (15)  (16)  1775  [16:24 8/8/03 Bioinformatics-btn308.tex]  Page: 1775  1772 1778   L.A.Newberg  The optimal checkpointing algorithm is presented in Figure 2.
Note that we include not just M, and N, but also a level L and a special stage Nopt(M,L) in the parameter list for the recursive subroutine, because the availability of values for L and Nopt(M,L) greatly speeds the calculations of Nopt(M  1,L) and Nopt(M,L 1), which are needed in the calculations of optimal checkpoints:  Nopt(M,L 1) = (cid:3) Nopt(M 1,L) = (cid:3)  Nopt(M,L)+1  Nopt(M,L)+1  (cid:4)  (cid:4)  L(M+2L 3)  (M+2L 1)(M+L 2)   1  (M 1)(M+2L 2) (M+2L 1)(M+L 2)   1 .
(17)  (18)  It is imperative that the required calculations be impervious to integer overﬂows.
We initially prevented overﬂow in integer calculations, such as abc/de for Equations (17) and (18), by canceling all common factors between each variable in the numerator and each variable in the denominator.
This approach requires 3 2=6 invocations of Euclid s algorithm for ﬁnding a greatest common divisor.
Such a procedure leaves the value of each denominator variable at 1 and, when Nopt(M,L)+1 can be represented as an integer, the numerator variable values are sufﬁciently small enough to permit all the needed computations so long as extra care is taken when verifying that the initial value of N is less than Nopt(M,L+1).
To handle computations where the number of stages exceeds the largest unsigned integer, often 232 1 4 109, we modiﬁed our C++ software implementation to use a C++ class that manipulates integers of arbitrary size.
3 SOFTWARE Sample C++-code for optimal backtrace is available in the Supplementary Materials.
4 RESULTS We applied the algorithm to pairwise local alignments (Smith and Waterman, 1981) of sequences of up to 3000 nucleotides of human DNA with sequences of up to 2864 nucleotides of rodent DNA.
For the largest of the alignments, to keep within a 125 MB limit for total memory use, we restricted ourselves to M = 486 stages of storage for the N = 2864 stages.
For these choices, L = 1, Nopt(M = 486, L = 1)= 486, and T(M = 486, N = 2864)= 5242.
Thus, the multiplier for the number of stage computations is T /N = 5242/2864  1.83 for memory use M/N =486/2864 17%.
The algorithm ran in 70 s, but would have run much more slowly if it had tried to use memory for all 2864 stages, because the resulting memory swapping would have been onerous.
In contrast, the 2-level algorithm of Wheeler and Hughey (2000) computes checkpoints for stages 486, 971, 1455, 1938 and 2420, and requires two computations for all other stages with index under 2420.
Thus its total number of stage computations is 2864+ (2420 5)= 5279, only slightly worse than 5242.
The same calculation performed on a pair of sequences, each takes 12 min to run in 125 MB of 10 000 nucleotides long, memory, a memory size sufﬁcient to store only 138 stages instead of the full 10 000 stages.
For a problem of this size,  1776  L = 2, Nopt(M = 138, L = 2)=9728, and T(M = 138, N = 10000)= 20134.
Thus, the multiplier for the number of stage computations is T /N = 20134/10000  2.01 for memory use M/N =138/10000  1.4%.
In contrast, the 3-level algorithm of Wheeler and Hughey (2000) is at a particular disadvantage in that it computes its only 3-level checkpoint at stage 9591, with subsequent 2-level checkpoints at 9728, 9864, and 9999.
The algorithm requires 29 448 stage computations, signiﬁcantly worse than 20 134.
With 1 GB of memory, sufﬁcient for storing 1104 stages for the pairwise alignment of sequences of length 10 000 nucleotides, the optimal checkpointing algorithm requires 18 896 stage computations, whereas the 2-level algorithm of Wheeler and Hughey (2000) requires 19 891 stage computations, almost 1000 more.
On similar datasets, using a probabilistic model that deﬁnes a probability distribution on the set of possible alignments, we used the optimal backtrace algorithm to compute a centroid (Ding and Lawrence, 1999), also known as a posterior decoding (Miyazawa, 1995).
This task requires a dynamic programming calculation during the backtrace that is comparable to the calculation performed during the forward pass.
With sufﬁcient memory, the total computation would require 2N stage computations, thus the multiplier for the number of stage computations with limited memory is  Ttwoway(M,N) Ttwoway(N ,N)  = T(M,N)+N  2N  M     L+1  2     (19)  this value is better than L, the multiplier of the number of stage computations for the cheaper backtrace tasks.
We also can draw independent samples from the probability distribution on the set of possible alignments.
Here, the run time is as slow as the centroid calculation only when the number of samples to be drawn is of the order of the smaller of the two sequence lengths.
5 DISCUSSION We have provided an algorithm for optimal backtrace through a dynamic programming algorithm when memory is limited.
The algorithm improves upon previous work via the simplicity and speed of the calculation for the index of the optimal checkpoint, and via achievement of optimal performance for a problem of arbitrary size.
A few variations are worthy of consideration.
Generally, for backtrace computations, whether or not they are achieved with the optimal checkpointing algorithm described here, the ﬁrst stage is computed from initial or boundary conditions, and each subsequent stage is computed from the immediately preceding stage.
Thus, at least conceptually, the ﬁrst stage requires special treatment.
If this distinction makes implementation of the advance callback routine difﬁcult, it may be prudent to compute and permanently store the ﬁrst stage in the ﬁrst memory location, and to run the optimal checkpointing algorithm so as to provide an optimally computed backtrace through the remaining N 1 stages using M 1 memory locations.
The number of stage computations for this approach is 1+T(M 1,N 1).
As already described for both optimal checkpointing and the L-level algorithm of Wheeler and Hughey (2000), in the limit as the number of memory locations M goes to inﬁnity with a ﬁxed multiplier L for the number of stage computations, we can backtrace through N   ML/L!
stages with T   ML/(L 1)!
stage computations.
However, for the case when memory is severely limited, it is instructive to look at the asymptotics for a ﬁxed value  [16:24 8/8/03 Bioinformatics-btn308.tex]  Page: 1776  1772 1778   TWH(M,L) .
(23)  presently.
of M, with L tending to inﬁnity.
For this situation we have  NWH(M,L)  TWH(M,L)  Nopt(M,L)  Topt(M,L)  M 1  L     LM 1 (M 1)!
L     LM (M 1) (cid:1) (cid:16)    M!
M 1 M LM 1 L     2 (M 1)!
LM (M 1) L     2 (cid:17) (cid:18)   (cid:17)  M!
M 1 (cid:18) M  (cid:19)  M 1     M 1  1 2  (20)  (cid:2) (M 1)!
M M 1  N  (21)  (22)  (cid:19)  M M 1  N  (M 1)!
2  Thus, in these low-memory situations, the optimal algorithm is asymptotically faster than the L-level algorithm of Wheeler and   Hughey (2000), even when the latter is applied to its optimal problem sizes NWH(M,L).
The speed multiplier is M 1 2, which is approximately 1+(0.693/(M 1.347)) for moderate values of M. See Table 1 and Figure 1.
6 CONCLUSION When high-speed memory is limited, dynamic programming algorithm backtraces make use of checkpoints for re-computing needed intermediate values.
We have provided an easy-to-use algorithm for optimally selecting the checkpoints.
ACKNOWLEDGEMENTS We thank the Computational Molecular Biology and Statistics Core Facility at the Wadsworth Center for the computing resources for the pairwise local sequence alignment calculations.We wish to acknowledge use of the Maple software package by Waterloo Maple, Inc., without which the calculations in this article would have been much more difﬁcult.
Funding: This research was supported by the National Institutes of Health / National Human Genome Research Institute grant K25 HG003291.
Conﬂict of Interest: none declared.
REFERENCES Baum,L.E.
et al.
(1970) A maximization technique occurring in the statistical analysis  of probabilistic functions of Markov chains.
Ann.
Math.
Statist., 41, 164 171.
Ding,Y.
and Lawrence,C.E.
(1999) A Bayesian statistical algorithm for RNA secondary  structure prediction.
Comput.
Chem., 23, 387 400.
Durbin,R.
et al.
(2006) Biological Sequence Analysis: Probabilistic Models of Proteins  and Nucleic Acids.
Cambridge University Press, Cambridge, UK.
Grice,J.A.
et al.
(1997) Reduced space sequence alignment.
Comput.
Appl.
Biosci., 13,  45 53.
Miklós,I.
and Meyer,I.M.
(2005) A linear memory algorithm for Baum-Welch training.
BMC Bioinformatics, 6, 231.
Dynamic programming backtrace  Miyazawa,S.
(1995) A reliable sequence alignment method based on probabilities of  residue correspondences.
Protein Eng., 8, 999 1009.
Myers,E.W.
and Miller,W.
(1998) Optimal alignments in linear space.
Comput.
Appl.
Biosci., 4, 11 17.
Needleman,S.B.
and Wunsch,C.D.
(1970) A general method applicable to the search for similarities in the amino acid sequence of two proteins.
J. Mol.
Biol., 48, 443 453.
Smith,T.F.
and Waterman,M.S.
(1981) Comparison of biosequences.
Adv.
Appl.
Math.,  2, 482 489.
Viterbi,A.J.
(1967) Error bounds for convolutional codes and an asymptotically  optimum decoding algorithm.
IEEE T. Inform.
Theory, 13, 260 269.
Waterman,M.S.
(1995) Introduction to Computational Biology.
Maps, Sequences and  Genomes.
Chapman & Hall / CRC, London, UK.
Wheeler,R.
and Hughey,R.
(2000) Optimizing reduced-space sequence analysis.
Bioinformatics, 16, 1082 1090.
APPENDIX: PROOF OF RUN TIME So that this section is self-contained, we will restate the relevant assumptions and deﬁnitions.
We deﬁne the following functions and will prove their usefulness  M+L 1  L  M+L 2 L 1  (cid:2)  (cid:1)  + (cid:1)  +  (cid:1)  Nopt(M,L)= (cid:1)  Topt(M,L)  =  (cid:2) M+L 1 1 M 1 L 1 (cid:2) (cid:1) M+L 2  2 L 2  M+L 2 1 M 1 L 2  (cid:2)   1 (cid:2)  (A1)  (A2)  We take as given that the number of stage computations required satisﬁes:                T(M,N)=      C+T(M 1,N C) +T(M,C 1)  N +   minC  (A3)  if N   M,    if N   M=1,   if N   M  1,  where a choice of value for C that minimizes this last expression for a given N and M is called an optimal ﬁrst checkpoint, C(M,N).
Theorem.
Let N be the number of stages to be made available in a backtrace using storage for M stages.
For a choice of N and M, deﬁne  L = max L: N   Nopt(M,L)  .
(A4)  We will show that  T(M,N)= T(M,Nopt(M,L))+(L+1)(N Nopt(M,L))  (A5) for N   M 1, where the second term is deemed zero if N = Nopt(M,L), even when L=+ .
We will show that, when N   M  1, the value of an optimal ﬁrst checkpoint C(M,N) satisﬁes  Nopt(M,L 1)  C(M,N) 1  Nopt(M,L)  (A6)  and  Nopt(M 1,L)  N C(M,N)  Nopt(M 1,L+1) .
(A7) Proof.
The proof will be by induction on N and M. We have two base cases: ﬁrst, a base case for a low value of N and, second, a base case for a low value of M. Base case, N = M 1 We wish to show that Equation (A5) correctly matches Equation (A3) when N = M 1.
[16:24 8/8/03 Bioinformatics-btn308.tex]  Page: 1777  1772 1778  1777   L.A.Newberg  We put L=1 into Equation (A1) for Nopt(M,L) and Equation (A2) for  Topt(M,L):  Nopt(M,L)=  (cid:1)  (cid:2)  +  (cid:2)  (cid:1) M 1 (cid:2) 0  M 1  (cid:1)   1= M  Topt(M,L)=  M M 1  +0 0= M ,  (A9) where the trinomial terms with L 2 vanish by our convention.
Thus, for this base case, Equation (A4) indicates that L=1.
It follows that Equation (A5) gives T(M,N)= M, which is in agreement with Equation (A3) because N = M. Base Case, N   M=1 We wish to show that Equation (A5) correctly matches Equation (A3) when N   M=1.
We put M=1 into Equation (A1) for Nopt(M,L) and Equation (A2) for  Nopt(M,L)=  (cid:1)  (cid:2)  L 1 L 1 (cid:2)  (cid:1)  (cid:1) (cid:2)  L L  (cid:2) + (cid:1)  +   1=1 (cid:1)  L 1 L 2  (cid:2)  Topt(M,L)=  L L 1  (A11) Thus, for this base case, Equation (A4) indicates that L=+ .
Because N is strictly greater than Nopt(M,L) (for any L) it follows that Equation (A5) gives T(M=1,N  1)=+ , in agreement with Equation (A3), as desired.
=1  L 1 L 2   2  Topt(M,L):  (A10)  (cid:9)  (cid:9),M  (cid:9)  M  (cid:9) 1, when N  )(cid:10)=(N ,M).
We will show that checkpoint choices C (cid:9)+1 will give the same number of stage computations if both C  General Case, N   M  1 (cid:9)  N and We assume that the theorem is proved true for N (cid:9)  M but (N and M (cid:9)(cid:9)= C and C (cid:9)(cid:9) (cid:9)(cid:9) satisfy the restrictions of Equations (A6) and (A7).
We will show that if C C is too high to satisfy these restrictions then use of C will lead to fewer stage is too low to satisfy these restrictions computations  we will show that if C will lead to fewer stage computations.
Together these will then use of C demonstrate that the restrictions are optimal and that they can be satisﬁed simultaneously.
We will then show that satisfaction of the restrictions implies Equation (A5).
(cid:9)(cid:9)  (cid:9) (cid:9)  (cid:9)  (cid:9)  Let T  (M,N) and T  (cid:9)(cid:9)  (M,N) be the number of stage computations required , respectively, is chosen as the ﬁrst checkpoint, and optimal  given that C checkpoints are used in all remaining subproblems:  or C  (cid:9)  (cid:9)  (cid:9)(cid:9)  (cid:9)  (M,N)= C  (cid:9)+T(M 1,N C  (cid:9)  )+T(M,C  (cid:9) 1) ,  T  and  (cid:9)(cid:9)  (M,N)= C  (cid:9)(cid:9)+T(M 1,N C  (cid:9)(cid:9)  )+T(M,C  (cid:9)(cid:9) 1) .
T  Set L1 to be the unique integer such that  Nopt(M,L1 1)  C  (cid:9) 1   Nopt(M,L1) ,  (A8)  and observe that  Nopt(M,L1 1)   C  (cid:9)(cid:9) 1  Nopt(M,L1) .
Set L2 to be the unique integer such that  Nopt(M 1,L2)   N C  (cid:9)  Nopt(M 1,L2+1) ,  and observe that  Nopt(M 1,L2)  N C  (cid:9)(cid:9)   Nopt(M 1,L2+1) .
(A14)  (A15)  (A16)  (A17)  Using our induction hypothesis [Equation (A5)], we thus compute that  (cid:9)(cid:9)  T  (M,N) T = (C  (cid:9) (cid:9)(cid:9) C  (cid:9)  )  (M,N)  ) T(M 1,N C  (cid:9)(cid:9)  +(T(M 1,N C +(T(M,C  (cid:9)(cid:9) 1) T(M,C = 1 (L2+1)+L1= L1 L2 .
(cid:9) 1))  (cid:9)  (cid:9)(cid:9)  (cid:9)  ))  (A18)  (A19)  (A20)  We observe that when both C  Equations (A6) and (A7) then L1= L2 and the stages C good choices as a checkpoint.
When C L1   L2, and when C have veriﬁed that the restrictions deﬁne an optimal ﬁrst checkpoint.
satisfy the restrictions given as make equally is too large for the restrictions then is too small for the restrictions then L1   L2.
Thus, we  and C (cid:9)(cid:9)  and C  (cid:9)(cid:9)  (cid:9)  (cid:9)  Finally, using the induction hypothesis, we compute T(M,N) to verify that it yields Equation (A5), using a value of C satisfying the restrictions of Equations (A6) and (A7).
T(M,N)  = C+T(M 1,N C)+T(M,C 1) = C  +Topt(M 1,L)+(L+1)(N C Nopt(M 1,L)) +Topt(M,L 1)+L(C 1 Nopt(M,L 1))  = Topt(M 1,L)+Topt(M,L 1)+Nopt(M,L 1)+1  +(L+1)(N Nopt(M 1,L) Nopt(M,L 1) 1)  (A21)  (A22)  (A23)  = Topt(M,L)+(L+1)(N Nopt(M,L)) ,  (A24) as desired.
For the last equality, we have used Nopt(M,L)= Nopt(M  1,L)+Nopt(M,L 1)+1 and Topt(M,L)= Topt(M 1,L)+Topt(M,L 1)+ Nopt(M,L 1)+1, which are easily proved from Equations (A1) and (A2).
(A12)  (A13)  1778  [16:24 8/8/03 Bioinformatics-btn308.tex]  Page: 1778  1772 1778
