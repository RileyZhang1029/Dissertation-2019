BIOINFORMATICS ORIGINAL PAPER Vol.
24 no.
24 2008, pages 2857 2864  doi:10.1093/bioinformatics/btn546  Sequence analysis Prediction of kinase-speciﬁc phosphorylation sites using conditional random ﬁelds Thanh Hai Dang1, Koenraad Van Leemput2, Alain Verschoren1 and Kris Laukens1,  1Intelligent Systems Laboratory and 2Advanced Database Research and Modelling, Department of Mathematics and Computer Science, Middelheimlaan 1, B-2020 Antwerpen, Belgium Received on July 14, 2008  revised on September 12, 2008  accepted on October 17, 2008 Advance Access publication October 20, 2008 Associate Editor: Alex Bateman  ABSTRACT Motivation: Phosphorylation is a crucial post-translational protein modiﬁcation mechanism with important regulatory functions in biological systems.
It is catalyzed by a group of enzymes called kinases, each of which recognizes certain target sites in its substrate proteins.
Several authors have built computational models trained from sets of experimentally validated phosphorylation sites to predict these target sites for each given kinase.
All of these models suffer from certain limitations, such as the fact that they do not take into account the dependencies between amino acid motifs within protein sequences in a global fashion.
Results: We propose a novel approach to predict phosphorylation sites from the protein sequence.
The method uses a positive dataset random ﬁeld (CRF) model.
The negative training dataset is used to specify the decision threshold corresponding to a desired false positive rate.
Application of the method on experimentally veriﬁed benchmark phosphorylation data (Phospho.ELM) shows that it performs well compared to existing methods for most kinases.
This is to our knowledge that the ﬁrst report of the use of CRFs to predict post-translational modiﬁcation sites in protein sequences.
Availability: The source code of the implementation, called CRPhos, is available from http://www.ptools.ua.ac.be/CRPhos/ Contact: kris.laukens@ua.ac.be Suplementary Information: Supplementary data are available at http://www.ptools.ua.ac.be/CRPhos/  to train a conditional  1 INTRODUCTION Protein phosphorylation is an essential type of post-translational modiﬁcation that consists of the addition of a phosphate (PO4) group to serine (S), threonine (T), tyrosine (Y) and to a lesser extent histidine (H) residues.
The process is catalyzed by a group of enzymes called kinases, and can be reverted by phosphatases.
Phosphorylation has important implications on the function of a protein.
If an enzyme gets phosphorylated its activity may be stimulated or inhibited, for example, leading to altered metabolic ﬂuxes in the case of a metabolic enzyme, or resulting in the modulation of a regulatory effect if the substrate protein plays a regulatory role.
The human genome encodes more than 500 different kinases, many of which have been related to cancer and    To whom correspondence should be addressed.
other diseases (Manning et al., 2002).
They regulate a diverse range of biochemical pathways and biological functions and are often indispensable signal integrators in a living system.
Being one of the most important reversible mechanisms of post-translational modiﬁcation, phosphorylation is a prevalent subject of research in biochemistry.
A ﬁrst step towards elucidating the phosphorylation network consists of the determination of the phosphorylated residues in a substrate protein for a given kinase.
Revealing the exact position of a phosphorylation in a sequence is essential to get irrefutable evidence for the assignment of a protein as a kinase substrate.
It also provides powerful clues for biomedical drug design or other biotechnological applications.
Phosphorylation sites on substrates are usually experimentally determined by mass spectrometry- based techniques (reviewed by Jensen, 2004).
This has led to several databases of phosphorylation sites, often tied to speciﬁc species, such as  The Phosphorylation Site Database  (Gnad et al., 2007),  Phospho.ELM  (Diella et al., 2004, 2008),  PhosphoSite  (Hornbeck, 2004) and  PhosPhAt  (Heazlewood et al., 2008).
Performing such experiments, however, remains time consuming, labor intensive and expensive.
These disadvantages have been anticipated by the bioinformatics community with the development of predictive models that are trained with experimentally annotated and known phosphorylation sites.
These models can be used to predict potential target sequences and thus signiﬁcantly reduce the number of sequences that need to be veriﬁed by mass spectrometry.
Several computational models have been built and applied with varying success to predict phosphorylation sites, including hidden Markov models (HMMs) (Huang et al., 2005b), neural networks (Blom et al., 1999, 2004  Ingrell et al., 2007), group- based scoring method (Xue et al., 2005  Zhou et al., 2004), Bayesian decision theory (Xue et al., 2006), support vector machines (SVMs) (Kim et al., 2004  Plewczynski et al., 2005, 2008  Wong et al., 2007) and algorithms to identify short protein sequence motifs on recognized substrates (Neuberger et al., 2007  Obenauer et al., 2003).
Particularly the ﬂanking sequence (typically  4, +4) around the potential sites (S/Y/T) is often used to develop these models.
Apart from the protein sequence, some additional information has also been integrated, including disorder information (Iakoucheva et al., 2004), structure information (Blom et al., 1999) and the distribution of the phosphorylated sites (Moses et al., 2007).
The majority of the computational models dedicated to predicting phosphorylation sites use the experimentally validated    2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
T.H.Dang et al.
database Phospho.ELM (Diella et al., 2004, 2008) for training and for the evaluation of their performance.
Due to the fact that for some particular kinases in Phospho.ELM only a small number of phosphorylated sites is known, the annotated Swiss-Prot database (Boeckmann et al., 2003) is often used in complement to increase the size of the training and testing dataset.
In this article, we introduce a novel machine learning scheme that overcomes several disadvantages associated with existing methods.
The model is based on conditional random ﬁelds (CRFs) (Lafferty et al., 2001) and allows prediction of phosphorylated sites for each speciﬁc kinase separately.
The positive and negative datasets are ﬂanking sequences of amino acids around the potentially phosphorylated residues.
Information about the chemical classes that individual amino acids belong to is also incorporated.
The CRF model is trained from only the positive training dataset.
The key idea of this approach is to generate the probability distribution for the positive data samples.
This derived distribution takes the probability values of the positive training dataset, calculated from the corresponding learned CRF model, as its values.
Within a set of protein sequences, the number of truly phosphorylated sites is always small compared to the number of non-phosphorylated sites.
To overcome this difﬁculty, we apply Chebyshev s Inequality from statistics theory to ﬁnd high conﬁdence boundaries of the derived distribution.
These boundaries are used to select a part of the negative training data, which is then used to calculate a decision threshold based on a user-provided allowed false positive rate.
To evaluate the performance of the method, k-fold cross-validations were performed on the experimentally veriﬁed phosphorylation dataset.
This new method performs well according to commonly used measures.
2 METHODS CRFs were introduced initially for solving the problem of labeling sequence data that arises in scientiﬁc ﬁelds such as bioinformatics and natural language processing.
In sequence labeling problems, each data item xi is a sequence of observations  xi1,xi2,...,xiT .
The purpose of the technique is to make a prediction of the sequence labels, that is, yi = yi1,yi2,...,yiT , corresponding to this sequence of observations.
So far,  in addition to CRFs, some probabilistic models have been introduced to tackle this problem, such as HMMs (Freitag and McCallum et al., 2000) and maximum entropy Markov models (MEMMs) (McCallum, et al., 2000).
In this section, we review and compare these models, before motivating and discussing our choice for the CRFs scheme.
2.1 Review of existing models An HMM is one of the most common methods for performing sequence labeling.
It is a generative model that maximizes the joint probability distribution p(X,Y), where X and Y are random variables whose values take on all observation sequences and corresponding label sequences, respectively.
To calculate the joint probability, HMMs need to enumerate all possible observation sequences.
This is intractable when the number of atomic observations becomes large.
Moreover the interacting range between positions in a sequence is often long.
First-order HMMs relax these strict constraints by working with two assumptions.
The ﬁrst one is the fact that a prediction of a future observation only depends on the present one (or on the immediate previous one).
As a result we have p(Xt+1 Xt ,Xt 1,...,X1)= p(Xt+1 Xt).
The second assumption is the time invariant or stationary: p(Xt+1 Xt)= p(X2 X1).
These limitations of HMMs in particular and generative models in general are the motivation behind the introduction of conditional models.
By maximizing the conditional probability p(Y X ) from the training dataset,  2858  conditional models do not explicitly model the observation sequences.
Furthermore, these models remain valid if dependencies between arbitrary features exist in the observation sequences, and they do not need to account for these arbitrary dependencies.
The probability of a transition between labels may not only depend on the current observation but also on past and future observations.
MEMMs (McCallum et al., 2000) are a typical group of conditional probabilistic models.
Each state in a MEMM has an exponential model that takes the observation features as input, and outputs the distribution over the possible next states.
These exponential models are trained by an appropriate iterative scaling method in the maximum entropy framework.
On the other hand, MEMMs and non-generative ﬁnite state models based on next-state classiﬁers are all victims of a weakness called label bias (Lafferty et al., 2001).
In these models, the transitions leaving a given state compete only against each other, rather than against all other transitions in the model.
The total score mass arriving at a state must be distributed and observed over all next states.
An observation may affect which state will be the next, but does not affect the total weight passed on to it.
This will result in a bias in the distribution of the total score weight at a state with fewer next states.
In particular, if a state has only one out-going transition, the total score weight will be transferred regardless of the observation.
A simple example of the label bias problem has been introduced in the work of Lafferty et al.
(2001).
2.2 Conditional random ﬁelds CRFs are discriminative probabilistic models that not only inherit all advantages of MEMMs but also overcome the label bias weakness.
While MEMMs use exponential models of the current state to calculate the conditional probabilities of the next states, CRFs use a single exponential model for the conditional probability of all training labels, given the observation sequence.
Therefore, the weight of an arbitrary feature can be learned from its global interactions with all the other features.
This means that the weights of all the features within CRFs can be traded-off against each other.
CRFs have been applied to some common problems in natural language processing, such as NP (noun phrase)-chunking, POS (part of speech)-tagging and text segmentation (Sha and Pereira, 2003), and the experimental results are signiﬁcantly better than those from HMMs and MEMMs.
In CRFs, the dependencies between the label components of a random variable Y are represented by an undirected graph G = (E,V).
Let C be a set of cliques in graph G. Suppose that there exists a set of K feature functions fk(c,X) predeﬁned in each clique c  C, where k = 1...K. According to the Hammersley Clifford theorem, the conditional probability of a label sequence given the observation sequence is calculated as follows (Sha and Pereira, 2003):  φ  c,X  (1)  Here Zo is the normalization function deﬁned over all possible label sequences and φ(c,X) is called the potential function of clique c. This is a non-negative real-valued function and is deﬁned as follows:  φ  c,X  fk  c,X  (2)  The parameters αk are learned globally from a labeled training dataset.
Although the graph G of Y may have a general structure for the problem of modeling the sequence the most simple and important structure is the linear chain structure.
Several authors have previously applied CRFs with a linear structure and obtained good performances (Lafferty et al., 2001  Sha and Pereira, 2003).
Within a linear structure, each clique is an edge with two end points.
The conditional probability formula can then be rewritten as follows:  (cid:1)  (cid:2)= 1  Zo  p  Y ,X  exp     (cid:7)  e E,k  (cid:1)  e,Y e ,X  λkhk  v,Y v ,X  µkgk  (cid:1)  (cid:2)+  (cid:7) v V ,k  (cid:2)    (3)  (cid:1)  Y X  p  (cid:2)= 1  Zo  (cid:3) c C  (cid:1)  (cid:4)  (cid:2)= e     αk  (cid:1)  (cid:1)  (cid:2)  (cid:2)   In this formula Y e ,Y v are components of the random variable Y corresponding to the edges and vertices of graph G, respectively.
The function gk and hk are the respective feature functions for the state observation pair and the state state pair.
These are real-valued functions but are often deﬁned as Boolean functions.
In the domain of phosphorylation site prediction, these feature functions, g1 for example, can be deﬁned as follows:  1 if AA 3 =  R and AA 2 =  K and L 0 otherwise (cid:1)  (4) (cid:2)=  Phos  means  The label of the current amino acid Here AA 3 =  R  means  The amino acid three positions left from current AA is R  and L AA0 is phosphorylated .
(cid:2)=  Phos   g1 =  AA0  (cid:10)  (cid:1)  As explained in the Section 3.1, the state state pair feature functions (hk in formula 3) are not declared in our implementation.
Several authors have proposed methods to efﬁciently induce such feature functions from datasets (Lafferty et al., 2001  McCallum, 2003  Pietra et al., 1997).
The weights of the CRFs are learned from the training dataset  xi,yi  to maximize the conditional log likelihood of label sequences  yi  (Sha and Pereira, 2003).
(cid:7)  L =  (cid:1)  xi,yi  logp  i  (cid:11)(cid:7)  (cid:7)  (cid:7)  (cid:2)=  i  c  k  (cid:1)     fk  αk,c  c,xi  (cid:2)(cid:12)  (cid:2) logZo  (cid:1)  xi  (5)  This likelihood function in CRFs is convex when the training label sequences (i.e.
a series of the labels  phosphorylated  and  non-phosphorylated ) make the state sequences (i.e.
a series of amino acids) unambiguous (McCallum, 2003).
In the case of phosphorylation site prediction this means that the training labels do corroborate the substrate speciﬁcity of the kinase.
This situation happens often in practice.
It guarantees that the global maximum value of the log likelihood of the conditional probability L will be found.
+  +  2.3 Proposed algorithm In this section, we introduce an algorithm that has all of the advantages of the CRFs discussed in the above section.
The algorithm follows a novelty detection approach, as previously successfully implemented in gene prioritization by De Bie et al.
(2007).
It builds a CRF model M for all training data objects that belong to the positive class.
In this application, we designed the features or patterns according to the motifs described in the biochemical literature on phosphorylation site prediction (reviewed by Kobe et al., 2005).
All patterns used are listed in the Supplementary Material.
If this set of features and patterns is well designed, the probabilities p(+ x,M ) that a positive training data object x is labeled as positive (+) are guaranteed to be the global maximum.
This is due to the convex characteristic of the conditional log likelihood function in CRFs.
They will distribute mainly near the largest probability value 1.
Furthermore, according to Chebyshev s Inequality (Ewens and Grant, 2001), given a random variable X and a real number n   0, p( X E(X)   nσ ) 1/n2.
Here E(X)and σ 2 denote the expected value and the variance of variable X, respectively.
This means that the conﬁdence degree of a value of X belonging to the range [E(X) n  σ ] is larger than (1 1/n2).
For example, with n = 3, the conﬁdence degree is  89%.
From now this interval will be referred to as the n-conﬁdence interval.
When applied to the distribution of the probability values p(+ x,M ), the expected value can be estimated by the average value of all values p(+ x,M ), with x being the positive training data objects.
The n-conﬁdence interval is enlarged by increasing the value n until the upper bound equals 1.
This interval is used in the proposed algorithm to overcome the difﬁculty that the number of examples in the positive training dataset is very small.
Due to the guarantee of obtaining the global maximum of the CRFs, the n-conﬁdence interval is expected to contain all values p(+ x,M + ) of all real positive data objects.
σ , E(X)+n  +  +  Moreover, the negative training dataset may contain some phosphorylated residues that have not yet been experimentally veriﬁed as such.
These negative data will then get high probabilities within the n-conﬁdence interval of being labeled as positive, and will not be considered during the process of controlling the false positive rate of the obtained classiﬁer.
Conditional random ﬁelds for phosphorylation site prediction  Algorithm Input:  (cid:127) Positive training dataset D (cid:127) Predeﬁned False Positive Rate (PFPR) of the obtained predictor.
and Negative training dataset D  .
+     Output:  (cid:127) A predictor including a model M  +  and a decision threshold θ so that  the observed False Positive Rate is expected to equal PFPR.
(1) Generate the positive CRF model M  from the positive training  +  data set D  +  .
(2) Initialize an empty array Thres.
(3) For each data object x  D  +  (4) Calculate probability of predicting x as positive (+) given the  model M  +  , P+ = p(+ x,M  +  )  (5) Calculate the n-conﬁdence interval of the distribution of P+ so that (6) For each data object y  D  the up bound equals 1.
(7) Calculate probability of predicting y as positive (+) given the ) and insert into array Thres if  +  +  , P  = p(+ y,M model M P  /  n-conﬁdence interval.
(cid:1) (cid:2) 1  (cid:13)(cid:1)  (8) Sort the array Thres according to ascending order.
(9) θ = Thres length + (10) Return (Model M  Thres , Decision threshold θ)  (cid:2) PFPR length  (cid:1)  Thres  (cid:2)(cid:14)  A new data object will be classiﬁed as positive if the probability of classifying is greater than or equal to the threshold θ. it as positive given the model M In all experiments, we used the open source software tool CRF++  +    http://crfpp.sourceforge.net/   to build the model.
experimentally  Implementation  including the central  Phospho.ELM contains  3 RESULTS AND DISCUSSION 3.1 We used the Phospho.ELM (Diella et al., 2008) (version 0707) database to experimentally evaluate our approach.
This dataset has been used as a benchmark to test the performance of most computational phosphorylation prediction models previously published.
veriﬁed phosphorylation sites in eukaryotic proteins, manually curated from the literature.
It stores information about substrate proteins with the exact positions of the residues that are experimentally veriﬁed to be phosphorylated by a given kinase.
For each potentially phosphorylated residue (S, T or Y), we extracted the nine amino acid sequence, residue, surrounding it (from  4 to +4).
All of these sequences of which the central residue was annotated as phosphorylated by a given kinase were considered as the positive set, whereas all remaining 9mer sequences on the same substrate proteins, were considered as negative examples.
Following Kim et al.
(2004), we discarded highly homologous sequences (over 70% identity) from the positive and negative training dataset to avoid overestimation on accuracy when cross-validating.
Such bias appears if the testing data are highly homologous to the training data.
The number of positive and negative samples for different kinases, after removing the redundancies, is shown in Table 1.
There are clearly much more negative samples than positive ones.
Apart from the amino acid itself, the chemical/structural group that an amino acid belongs to is used as an additional feature for each residue.
Twenty amino acids were grouped into eight different clusters (Table 2) according  2859   T.H.Dang et al.
Table 1.
The size of positive and negative datasets for some common protein kinases, obtained from Phospho.ELM version 0707  N   R   K   Q   S   W   F   D   H  Amide  Base  Base  Amide SmallHydroxy Aromatic Aromatic  Acid  Base    Protein kinase  Positive size Negative size  Transformed to an object S   45 55 50  104 42 226 20  Abl (Proto-oncogene tyrosine-protein kinase) ATM (Ataxia telangiectasia mutated) CaM-KII (Calcium/calmodulin-dependent protein kinases) CDK (Cyclin-dependent kinases) CK1 (Casein kinases 1) CK2 (Casein kinases 2) DNA-PK (DNA-dependent protein kinase catalytic subunit) EGFR (Epidermal growth factor receptor) Fyn (Proto-oncogene tyrosine-protein kinase) GSK-3 (Glycogen synthase kinases 3) InsR (Insulin receptor) Met (Hepatocyte growth factor receptor) mTOR (FK506 binding protein 12-rapamycin associated protein 1) 310 PKA (cAMP-dependent protein kinase) 79 PKB (Protein kinases B) PKC (Protein kinase) 227 Src (Proto-oncogene tyrosine-protein kinase) 141 Syk (Tyrosine-protein kinase) 45  44 48 32 44 13 13  1209 1882 1829  1990 1051 3875 632  823 1409 866 724 132 50  8823 3563 4428 2681 680  Table 2.
The chemical classes to which the 20 amino acids belong, based on Wong et al.
(2007)  Group name  Sulfur Aliphatic 1 Aliphatic 2 Acid Base Aromatic Amide Small hydroxy  Amino Acids  C, M A, G, P I, L, V D, E H, K, R F, W, Y N, Q S, T  g2 =  to their common chemical/structural properties (Wong et al., 2007).
For each position in the positive sequence data, a set of Boolean value feature functions was declared, including functions for amino acids (e.g.
formula 4), for chemical groups (e.g.
formula 6) and for combinations of amino acids and chemical groups (e.g.
formula 7).
(cid:10) 1 if G 3 =  Sulfur and G 2 =  Base and L (cid:1) (cid:10)  (cid:2)=  Phos  (cid:2)=  Phos  g3 = Here G 3 =  Sulfur means  The chemical group of the amino acid Sulfur  and L(AA0)=  Phos  means  The label of the current amino  1 if A 3 =  R and G 2 =  Base and L  (AA) three positions left from the current AA belongs to the cluster  0 otherwise  0 otherwise  AA0  AA0  (6)  (7)  (cid:1)  acid is phosphorylated .
S((N, Amide), (R, Base), (K, Base), (Q, Amide), (W, Aromatic),     (F, Aromatic), (D, Acid), (H, Base))   Fig.
1.
Method for transforming an amino acid sequence to a data object of the central amino acid.
When applying the algorithm (Section 2.3) to build a predictive model from the positive (i.e.
central residue is phosphorylated) and negative (i.e.
central residue is not phosphorylated) sequence data, the conditional probabilities in Steps 4 and 7 are probabilities of the central residues in the sequence data having the label  Phos  (i.e.
phosphorylated ).
These probabilities are equivalent to the total sum of the probabilities of all possible label sequences of which the central labels are  Phos , assigned by a CRF given the ﬂanking sequence of amino acids.
This increases the computational complexity of the algorithm due to the required enumeration of all possible surrounding labels.
To tackle this problem, we introduce a transferring method that is applied to the sequence data as follows.
Each nine-residue long amino acid sequence is represented in an equivalent form, where the center residue (S, Y or T) is a data object and the surrounding residues themselves and their corresponding features become the new features (Fig.
1).
The information about the positions of the residues is conserved, thus the CRF model still has the ability to exploit the meaning of residue positions if suitable feature functions (gk) are used.
The state state feature functions (hk, formula 3) are not further declared since the dependencies between labeling information of the surrounding amino acids is omitted in this new representation.
3.2 Evaluation To evaluate the performance of the algorithm, k-fold cross-validation was used for the model trained from the large datasets, whereas Jackknife cross-validation was applied when the models were trained with less than 30 positives.
Each cross-validation was performed 20 times, and after each round we calculated Sensitivity (Sn) = TP/(TP+FN) and Speciﬁcity (Sp) = TN/(TN+FP).
Here TP, TN, FP and FN are true positive, true negative, false positive and false negative values, respectively.
The average values after 20 runs were used as the ﬁnal measure of the performance for the model.
For each kinase-speciﬁc phosphorylation predictor, the ROC (receiver operating characteristic) curve, which shows the tradeoff between sensitivity and speciﬁcity, was generated from the ﬁnal average.
The ROC curves obtained from different k-fold cross- validations (k = 2, 4, 6, 8, 10) were approximately the same (data not shown).
For the sake of clarity, all shown ROC curves are the result from 10-fold cross-validation (Fig.
3 and Supplementary ﬁgures, blue lines).
All ROC curves, except CDK1 and PKB, reach 100% sensitivity with a speciﬁcity of at least 20%.
Because the number of positives is much smaller than the number of negatives, this implies a signiﬁcant reduction in the number of required validations, even if no false negatives are desired.
2860   Conditional random ﬁelds for phosphorylation site prediction  a performance that is comparable or better than other methods.
(SVMs-based approaches applied in Predphospho (Kim et al., 2004) and KinasePhos 2.0 (Wong et al., 2007) do perform better in some instances (e.g.
both in CK2, KinasePhos 2.0 in PKC, PredPhospho in CDK), but worse in other cases (both in PKA, PredPhospho in PKC).
However, both predictors have been validated on data of which the size of the negative and positive subset has been equalized, in contrast to this article.
Compared with PPSP (Xue et al., 2006), CRPhos performs better for the majority of the kinases, but worse or similar for a few.
From all kinases, only the prediction for CK2 by CRPhos is generally worse than those by other prediction methods, although even then CRPhos achieves both sensitivity and speciﬁcity values above 80%.
NetphosK could only be compared for PKA and ATM, yielding worse and better performance, respectively.
Except for CK2, CRPhos performs similar or better than the other methods, including GPS (Zhou et al., 2004), Scansite (Obenauer et al., 2003) and KinasePhos 1.0 (Huang et al., 2005a).
There is a chance that the version of the dataset, which is different for previously published models, affects the above comparison.
An ideal solution to perform an unbiased comparison is running new cross-validations on all existing methods using the same dataset that we used.
This is practically hard to achieve since trainable versions of most tools are not available.
An alternative solution consists of testing and comparing our method and other existing ones on the same testing dataset.
There is however a high chance to get a biased comparison if some testing data are already learned by one of the methods.
To eliminate this problem, a more rigorous approach was (2008).
They generated a recently deployed by Wan et al.
subset of Phospho.ELM, called MetaPS06, which contains the phosphorylation sites that were only recently added, after publication of existing prediction models.
This MetaPS06 set does not overlap with any previously used training data.
By testing this dataset against different prediction tools, Wan and Colleagues (2008) obtained comparable performance measurements that represent the predictive power of each tool.
To generate equivalent performance values, we removed from Phosphos.ELM version 07 all phosphorylated sites originated from Phospho.ELM version 06 (with annotation data  12/31/2004), as described (Wan et al., 2008).
For this experiment the removed dataset was used to train the CRPhos model, whereas the remaining fraction was used for testing.
The results (Fig.
4) demonstrate that the performance of CRPhos remains better than the performance of most other methods.
Unlike other methods, CRPhos learns the model only from the  golden  positive dataset and not from the  un-golden  negative dataset.
This negative dataset could contain some real phosphorylated (positive) data that have not yet been experimentally validated.
This may cause a bias in the prediction by models that are trained from both positive and negative data.
Moreover, we also cross-validated our model using the older versions of Phospho.ELM.
versions 06 & 1206.
Supplementary Figure 2 demonstrates that this has almost no effect on the performance.
A signiﬁcant advantage of the method described in this article lies in the fact that it is able to generate predictions for all possible speciﬁcity values.
Any classiﬁer, deﬁned by a point in the ROC curve, can be readily obtained, whereas other approaches are only able to generate one classiﬁer with a ﬁxed sensitivity/speciﬁcity.
2861  Fig.
2.
Relation between expected and observed speciﬁcity values of obtained predictor.
All lines are generated using linear regression.
We also validated whether the observed speciﬁcity value of a classiﬁer generated from the method is close to the expected value.
For each value of an expected speciﬁcity, a 4-fold cross- validation procedure was implemented 20 times.
The average observed speciﬁcity was calculated and compared with the expected value (Fig.
2).
These values were identical for kinases with a negative training dataset larger than 1500.
For kinases with a smaller negative training set, the smallest regression coefﬁcient was 0.97, for the  mTOR  kinase, of which the number of negative training sequences was only 50.
As a consequence, the algorithm can return any desired point (classiﬁer) on the ROC curve based on taking into account an expected speciﬁcity value as input.
The model proposed in this article uses the positive dataset for training, and uses the negative data to calculate a decision threshold.
In order to demonstrate the efﬁciency of this approach, we also tested a conventional approach, using both the positive and negative data for training a CRF model.
For this experiment, the nine amino acid protein sequences from both the positive and the negative dataset were taken as input to the learning algorithm of the CRFs.
The derived ROC curves are shown in red in Figure 3 and in the Supplementary Material.
For most kinases, this conventional approach results in a slightly worse ROC curve, indicating that our approach outperforms the application of CRFs trained on both positive and negative data.
3.3 Comparison The derived ROC curves allow for easy comparison of our method with reported performance measures from other methods.
We followed two different approaches.
The approach applied by most authors of phosphorylation site prediction methods, is the direct comparison of obtained results with previously reported performances (Huang et al., 2005a  Kim et al., 2004  Zhou et al., 2004).
If available, performance values, reported in literature as pairs of sensitivities/speciﬁcities, were shown as colored dots on the ROC plots for each kinase method (Fig.
3 and Supplementary Fig.
1).
These values can be considered worse or better, depending on whether these dots fall below or above the CRPhos ROC curve, respectively.
In most cases, CRPhos yielded   T.H.Dang et al.
Fig.
3.
ROC curves of our method for some well-studied kinases, using 10-fold cross-validation (CRPhos).
CRF* stands for the equivalent curve for a CRF model learned from both the positive and negative training dataset.
For comparison, corresponding performance measures reported in literature are shown: PPSP (Xue et al., 2006), Scansite (Obenauer et al., 2003), NetPhosK (Blom et al., 2004), KinasePhos 1.0 (Huang et al., 2005a), KinasePhos 2.0 (Wong et al., 2007), GPS (Zhou et al., 2004) and PredPhospho (Kim et al., 2004).
4 CONCLUSION In this article, we introduced a novel approach based on CRFs to predict kinase-speciﬁc phosphorylation sites.
Upon validation with a real dataset of phosphorylation sites, the method yielded accurate predictions that were similar or better than predictions obtained with existing methods.
This is consistent with the theoretical advantages of CRFs, including the convergence to the global maximum of the log likelihood conditional probability and the capability of capturing all amino acid motifs and their interactions in a global fashion.
Our approach employs Chebyshev s Inequality to ﬁnd the conﬁdence interval for the distribution of the real positive data.
As a result, it overcomes the difﬁculty that, in reality, the size of the experimentally veriﬁed positive data is very small compared to that of the negative data.
Moreover, the use of Chebyshev s Inequality also allows eliminating the noisy negative data, which may contain target sites that have not yet been experimentally assigned as positive.
Finally, this method allows obtaining an optimal prediction for any given allowed false positive rate.
This gives the end-user extra ﬂexibility, especially when applied in situations where either incomplete detection, or false positives are undesired.
ACKNOWLEDGEMENTS The authors are grateful to Koen Smets for valuable feedback on the article.
They also wish to thank Taku Kudo for releasing the CRF++ tool under an open source license.
Francesca Diella and the Phospho.ELM team are gratefully acknowledged for providing the Phospho.ELM dataset and for offering useful suggestions.
(IWT-600450) of  Funding: SBO grant the Flemish Institute supporting Scientiﬁc Technological Research in industry (IWT)  the EU project  Inductive Queries for Mining Patterns and Models  (IQ).
Conﬂict of Interest: none declared.
2862   Conditional random ﬁelds for phosphorylation site prediction  Fig.
4.
Performance of CRPhos with the testing dataset that is created according to the scheme in Wan et al.
(2008).
The remaining dataset after removing this testing data from Phospho.ELM v.07 was used to train CRPhos.
The performance measure of other existing methods, reported by Wan et al.
(2008), are shown for comparison.
REFERENCES Blom,N.
et al.
(1999) Sequence and structure-based prediction of eukaryotic protein  phosphorylation sites.
J. Mol.
Biol., 294, 1351 1362.
Blom,N.
et al.
(2004) Prediction of post-translational glycosylation and phosphorylation  of proteins from the amino acid sequence.
Proteomics, 4, 1633 1649.
Boeckmann,B.
et al.
(2003) The Swiss-Prot protein knowledgebase and its supplement  TrEMBL in 2003.
Nucleic Acids Res., 31, 365 370.
De Bie,T.
et al.
(2007) Kernel-based data fusion for gene prioritization.
Bioinformatics,  23, i125 i132.
Diella,F.
et al.
(2004) Phospho.ELM: a database of experimentally veriﬁed  phosphorylation sites in eukaryotic proteins.
BMC Bioinformatics, 5, 79.
Diella,F.
et al.
(2008) Phospho.ELM: a database of phosphorylation sites update 2008.
Nucleic Acids Res., 36, D240 D244.
Ewens,W.J.
and Grant,G.R.
(2001) Statistical Methods in Bioinformatics: An  Introduction.
Springer, Philadelphia, PA.  Freitag,D.
and McCallum,A.
(2000) Information extraction with HMM structures learned by stochastic optimization.
In Proceedings of the Seventeenth National Conference on Artiﬁcial Intelligence and Twelfth Conference on Innovative Applications of Artiﬁcial Intelligence.
AAAI Press/The MIT Press, pp.
584 589.
Available at http://portal.acm.org/citation.cfm id=723414&dl=GUIDE  Gnad,F.
et al.
(2007) PHOSIDA (phosphorylation site database): management, structural and evolutionary investigation, and prediction of phosphosites.
Genome Biol., 8, R250.
Heazlewood,J.L.
et al.
(2008) PhosPhAt: a database of phosphorylation sites in Arabidopsis thaliana and a plant speciﬁc phosphorylation site predictor.
Nucleic Acids Res., 36, 1015 1021.
Hornbeck,P.V.
(2004) PhosphoSite: a bioinformatics resource dedicated to physiological  protein phosphorylation.
Proteomics, 4, 1551 1561.
Huang,H.D.
et al.
(2005a) KinasePhos: a web tool for identifying protein kinase-speciﬁc  phosphorylation sites.
Nucleic Acids Res., 33, 226 229.
Huang,H.D.
et al.
(2005b) Incorporating hidden Markov model for identifying protein  kinase-speciﬁc phosphorylation sites.
J. Comput.
Chem., 26, 1032 1041.
Iakoucheva,L.M.
et al.
(2004) The importance of intrinsic disorder for protein  phosphorylation.
Nucleic Acids Res., 32, 1037 1049.
Ingrell,C.R.
et al.
(2007) NetPhosYeast: prediction of protein phosphorylation sites in  yeast.
Bioinformatics, 7, 895 897.
Jensen,O.N.
et al.
(2004) Modiﬁcation-speciﬁc proteomics: characterization of post- translational modiﬁcations by mass spectrometry.
Curr.
Opin.
Chem.
Biol., 8, 33 41.
Kim,J.H.
et al.
(2004) Prediction of phosphorylation sites using SVMs.
Bioinformatics,  20, 3179 3184.
2863   T.H.Dang et al.
Kobe,B.
et al.
(2005) Substrate speciﬁcity of protein kinases and computational  Pietra,D.S.
et al.
(1997) Inducing features of random ﬁelds.
IEEE Trans.
Pattern Anal.
prediction of substrates.
Biochim.
Biophys.
Acta, 1754, 200 209.
Match.
Intell., 19, 380 393.
Lafferty,J.D.
et al.
(2001) Conditional random ﬁelds: probabilistic models for segmenting and labeling sequence data.
the Eighteenth International Conference on Machine Learning.
Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, pp.
282 289.
In Proceedings of  Manning,G.
et al.
(2002) The protein kinase complement of the human genome.
Science,  298, 1912 1934.
McCallum,A.
(2003) Efﬁciently inducing features of conditional random ﬁelds.
In Proceedings of the 19th Conference in Uncertainty in Articiﬁcal Intelligence.
Morgan Kaufmann, Acapulco, Mexico, pp.
403 410.
McCallum,A.
et al.
(2000) Maximum entropy Markov models for information extraction and segmentation.
In Proceedings of ICML 2000.
Stanford, California, pp.
591 598.
Moses,A.M.
et al.
(2007) Spatial clustering of phosphorylation site recognition motifs can be exploited to predict the targets of cyclin-dependent kinase.
Genome Biol., 8, R23.
Neuberger,G.
et al.
(2007) pkaPS: prediction of protein kinase A phosphorylation sites  Plewczynski,D.
et al.
(2005) A support vector machine approach to the identiﬁcation  of phosphorylation sites.
Cell.
Mol.
Biol.
Lett., 10, 73 89.
Plewczynski,D.
et al.
(2008) Automotif server for prediction of phosphorylation sites  in proteins using vector machine.
J. Mol.
Model., 14, 69 76.
(2003) Shallow parsing with conditional random ﬁelds.
Sha,F.
and Pereira,F.
In Proceedings of the 2003 Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics.
(HLT/NAACL-03).
Association for Computational Linguistics, Morristown, NJ, USA.
Wan,J.
et al.
(2008) Meta-prediction of phosphorylation sites with weighted voting and  restricted grid search parameter selection.
Nucleic Acids Res., 36, e22.
Wong,Y.H.
et al.
(2007) KinasePhos 2.0: a web server for identifying protein kinase- speciﬁc phosphorylation sites based on sequences and coupling patterns.
Nucleic Acids Res., 35, W588 W594.
Xue,Y.
et al.
(2005) GPS: a comprehensive www server for phosphorylation sites  prediction.
Nucleic Acids Res., 33, W184 W187.
with the simpliﬁed kinase-substrate binding model.
Biol.
Direct, 2, 1.
Xue,Y.
et al.
(2006) PPSP: prediction of PK-speciﬁc phosphorylation site with Bayesian  Obenauer,J.C.
et al.
(2003) Scansite 2.0: proteome-wide prediction of cell signaling interactions using short sequence motifs.
Nucleic Acids Res., 31, 3635 3641.  decision theory.
BMC Bioinformatics, 7, 163.
Zhou,F.F.
et al.
(2004) GPS: a novel group-based phosphorylation predicting and scoring  method.
Biochem.
Biophys.
Res.
Commun., 325, 443 1448.
2864
