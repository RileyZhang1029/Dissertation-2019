BIOINFORMATICS  Vol.
29 ISMB/ECCB 2013, pages i316 i325 doi:10.1093/bioinformatics/btt218  Poly(A) motif prediction using spectral latent features from human DNA sequences Bo Xie1, Boris R. Jankovic2,3, Vladimir B. Bajic2,3, Le Song1,* and Xin Gao2,3,* 1College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA, 2Computational Bioscience Research Center, King Abdullah University of Science and Technology (KAUST), Thuwal 23955-6900, Saudi Arabia and 3Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal 23955-6900, Saudi Arabia  ABSTRACT Motivation: Polyadenylation is the addition of a poly(A) tail to an RNA molecule.
Identifying DNA sequence motifs that signal the addition of poly(A) tails is essential to improved genome annotation and better understanding of the regulatory mechanisms and stability of mRNA.
Existing poly(A) motif predictors demonstrate that information ex- tracted from the surrounding nucleotide sequences of candidate poly(A) motifs can differentiate true motifs from the false ones to a great extent.
A variety of sophisticated features has been explored, including sequential, structural, statistical, thermodynamic and evolu- tionary properties.
However, most of these methods involve extensive manual feature engineering, which can be time-consuming and can require in-depth domain knowledge.
Results: We propose a novel machine-learning method for poly(A) motif prediction by marrying generative learning (hidden Markov models) and discriminative learning (support vector machines).
Generative learning provides a rich palette on which the uncer- tainty and diversity of sequence information can be handled, while discriminative learning allows the performance of the classification task to be directly optimized.
Here, we used hidden Markov models for fitting the DNA sequence dynamics, and developed an efficient spectral algorithm for extracting latent variable informa- tion from these models.
These spectral latent features were then fed into support vector machines to fine-tune the classification performance.
We evaluated our proposed method on a comprehensive human poly(A) dataset that consists of 14 740 samples from 12 of the most abundant variants of human poly(A) motifs.
Compared with one of the previous state-of-the-art methods in the literature (the random forest model with expert-crafted features), our method reduces the average error rate, false-negative rate and false-positive rate by 26, 15 and 35%, respectively.
Meanwhile, our method makes  30% fewer error predictions relative to the other string kernels.
Furthermore, our method can be used to visualize the importance of oligomers and positions in predicting poly(A) motifs, from which we can observe a number of characteristics in the sur- rounding regions of true and false motifs that have not been reported before.
Availability: http://sfb.kaust.edu.sa/Pages/Software.aspx Contact: lsong@cc.gatech.edu or xin.gao@kaust.edu.sa Supplementary information: Supplementary data are available at Bioinformatics online.
*To whom all correspondence should be addressed.
1 INTRODUCTION  Roughly speaking, when DNA is transcribed to RNA, a string of adenine (A) nucleotides, referred to as the polyadenylation tail or the poly(A) tail, is added to the 30-end of the primary RNA transcript.
Such a process is called polyadenylation, which is a step to protect RNA stability, nuclear export and translation (Bernstein and Ross, 1989  Leung et al., 2011).
A poly(A) tail is  10 30 nt downstream of a signaling site that consists of 6 nt, which in human cells most commonly is AAUAAA (Beaudoing et al., 2000).
This signaling site is known as a poly(A) signal and the corresponding 6 nt subsequences in DNA are called poly(A) motifs.
Mutations in poly(A) signals can be associated with dis- eases, e.g.
colorectal cancer (Kim et al., 2002  Pastrello et al., 2006) and compromised immunodeficiency (Das et al., 1997  Langemeier et al., 2012).
The poly(A) signal prediction problem has been studied for decades (Proudfoot, 2011).
There are two versions of the prob- lem: predicting poly(A) signals in mRNA sequences and predict- ing poly(A) motifs in DNA sequences.
Intuitively, the former version is much simpler than the latter one because once the mRNA sequence is given, the poly(A) tail can be identified rela- tively easily and we need only to search for the poly(A) signal(s) in a window of at most 30 nt upstream the poly(A) tail.
In DNA, however, the presence of introns in eukaryotes and the absence of poly(A) tails make the recognition much more challenging.
A number of studies have demonstrated that information from relatively short upstream and downstream sequences of the can- didate poly(A) motifs can specify the true poly(A) motifs to a great extent (Ahmed et al., 2009  Akhtar et al., 2010  Chang et al., 2011  Cheng et al., 2006  Graber et al., 1999  Ji et al., 2010  Kalkatawi et al., 2012  Legendre and Gautheret, 2003  Liu et al., 2005  Salamov and Solovyev, 1997  Tabaska and Zhang, 1999).
Statistical properties of the surrounding sequences were explored in different species, such as yeast (van Helden et al., 2000), fly (Retelska et al., 2006), Arabidopsis and rice (Ji et al., 2010) and human (Chang et al., 2011  Retelska et al., 2006  Tabaska and Zhang, 1999).
Although significant progress has been made to the accuracy of poly(A) motif predictors, especially in human DNA sequences, such methods are all based on using sophisticated features that require additional efforts to extract and are highly dependent on domain knowledge.
We therefore ask the following question: can we use machine-learning tech- niques to automatically extract useful features from human DNA sequences and achieve state-of-the-art poly(A) motif clas- sification results  If the answer to this question is yes, then we can automate this genome annotation task to a great extent and  ÃŸ The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com   potentially speed up our understanding of biology.
the underlying  Automatic feature extraction techniques have been explored in many other sequence classification problems.
By far, the most successful methods are hidden Markov models (HMMs), e.g.
in gene finding (Lukashin and Borodovsky, 1998  Stanke and Waack, 2003), and string kernels with support vector machines (SVM), e.g.
in protein classification (Leslie et al., 2004), tran- scription start-site recognition (Sonnenburg et al., 2006) and splice-site prediction (Sonnenburg et al., 2007).
The former methods are generative models that capture the uncertainty in data using probabilistic languages, while the latter are discrim- inative methods that optimize specifically for the classification results.
The advantages of each class of methods have rarely been combined systematically to yield even better feature extractors.
In this article, we propose a novel method for poly(A) motif prediction by marrying generative learning (HMMs) and dis- criminative learning (SVMs).
Generative learning provides us with a rich palette for handling the uncertainty and diversity of sequence information, while discriminative learning allows us to directly optimize performance for the classification task.
Here, diversity means that for the same position in the surrounding sequence of a candidate poly(A) motif, there may be multiple subsequences indicating the class label (a true motif or a false one), and uncertainty means that each of these subsequences does not give deterministic information about the class label.
In par- ticular, we use HMMs as a probabilistic generative model for DNA sequences, and develop an efficient spectral algorithm for extracting latent variable information from these models.
The HMMs model not only the diversity and uncertainty of sequence information, but also long-range dependencies between subse- quences at different positions, which cannot be simultaneously captured by string kernels as is discussed in Section 2.2.
The spectral latent features are then fed into support vector machines to fine-tune the classification performance.
2 RELATED WORKS  We will first review the two classes of methods, expert-crafted features and string kernels, for addressing the poly(A) motif clas- sification problem.
2.1 Features based on domain knowledge  Bioinformatics experts can craft highly informative features based on prior knowledge of the biology and physics of DNA sequences.
The drawback of using these features is that they re- quire extensive expert domain knowledge for their design as well as additional efforts to extract them.
Salamov and Solovyev developed POLYAH (Salamov and Solovyev, 1997), a tool that used a linear discriminant func- tion-based classifier to extract features from 100 nt upstream and 200 nt downstream of a candidate poly(A) motif.
Later on, polyadq was developed based on quadratic discriminant func- tions by encoding features from 100 nt downstream only (Tabaska and Zhang, 1999).
Several support vector machine- based predictors were then proposed and they performed well on recognizing true poly(A) motifs.
Such methods include  Poly(A) motif prediction  DNAFSMiner, which was based on both 100 nt upstream and downstream (Liu et al., 2005), Polya_svm, which encoded cis- regulatory element features (Cheng et al., 2006), and polyApred, which extracted features from 100 nt upstream and downstream of the candidate poly(A) motifs (Ahmed et al., 2009).
In 2010, Akhtar et al.
proposed POLYAR (Akhtar et al., 2010), a linear discriminant analysis-based method that used features from 300 nt upstream and downstream of the candidate poly(A) motifs.
Recently, Kalkatawi et al.
developed an artificial neural net- work (ANN)-based method and a random forest (RF)-based method to predict poly(A) motifs in human DNA sequences (Kalkatawi et al., 2012).
Their methods were based on a variety of expert-crafted features, such as thermodynamic and structural features of dinucleotides, electron ion interaction potentials and position-weight matrices of upstream and downstream regions relative to the candidate poly(A) motifs.
In total, they extracted 274 features.
They compiled a large-scale benchmark set that contained 14 740 sequences for the 12 main variants of human poly(A) motifs.
The ANN and RF models significantly outper- formed all previously reported studies.
2.2 String kernels  f  String kernels are positive definite functions to compute similar- ity between two sequences, which is then used in support vector machines to learn classifiers.
These kernels essentially map se- quences into high-dimensional feature spaces corresponding to subsequences and then compute inner products between two fea- ture vectors.
The drawback of string kernels is that they simply count raw sequence matches and do not explicitly take into ac- count the uncertainty and diversity of sequence information.
Many string kernels have been designed over the years, but so far few have effectively made use of generative models to deal with data uncertainty.
More specifically, given an alphabet  , here the DNA nucleo- g, let x 2  k be a sequence of length k tides   Â¼ A, G, C, T (or k-mer).
The k-spectrum (SPE) kernel  Ã°x, yÃž counts pairs of P identical k-mers between two sequences x and y (of length L and L0, respectively) independently of their position (Leslie et al., 2002):  Ã°x, yÃž Â¼ , where xt:tÃ¾k 1 :Â¼ xtxtÃ¾1 .
.
.
xtÃ¾k 1 denotes a subsequence of x that starts at position t and has length k, and I  f g returns 1 if the two k-mers are the same and otherwise 0.
This kernel effectively maps each sequence into a feature space where each dimension counts the number of occurrences of a particular k-mer and uses the inner product as the similarity between two sequences.
To take uncertainty and diversity in k-mer features into account, one can also heuristically include counts on the mismatch of k-mers (Leslie et al., 2002).
I xt:tÃ¾k 1 Â¼ yt0:t0Ã¾k 1  L0 kÃ¾1 t0Â¼1  L kÃ¾1 tÂ¼1  P     P  P  In contrast to the SPE kernel, the weighted degree (WD) kernel explicitly takes into account the absolute positions of the k-mers in the sequence (Ra  tsch and Sonnenburg, 2004):  Ã°x, yÃž Â¼ : Analogously, one can also heuristically incorporate the counts for mismatches in the WD kernel to account for a certain degree of sequence un- certainty and diversity.
I xt:tÃ¾l 1 Â¼ yt:tÃ¾l 1  L lÃ¾1 tÂ¼1  k lÂ¼1  l     However, heuristic ways of handling mismatches may result in underutilization of the sequence information.
A principle way to  i317   B.Xie et al.
deal with such uncertainty is to model mismatches as random variables.
Along this direction, the probability product kernel (PPK  Jebara et al., 2004) has been proposed for sequence ana- lysis.
This kernel also compares sequences of equal length and assumes that the absolute position in the sequence carries dis- criminative information.
The key idea is to fit a probabilistic generative model (e.g.
HMMs) to each sequence separately, and then use an inner product between these generative models to define the kernel.
As a result, the PPK allows us to combine discriminative learning of support vector machines with genera- tive modeling of data.
The seminal work on PPKs has several limitations.
First, a different HMM is fitted to each sequence, which can lead to poorly estimated model parameters and bury the useful signals.
Second, training HMMs with a traditional expectation maxi- mization (EM) algorithm (Dempster et al., 1977) can be time- consuming.
Third, while the hidden variables model the  clean  signals, they are summed out and not directly used for sequence comparison.
Fourth, the label information for the training se- quences is not used for defining the kernel.
Owing to these limi- tations, the PPK does not perform as well as the SPE kernel or the WD kernel as we show in later experiments.
In the following, we first describe our method, which also combines generative and discriminative learning, but overcomes the above four limitations.
3 METHODS  The key contribution of our article is a new way of extracting and using sequence features for classification problems.
Our method fits only two HMMs for the entire training set, one for sequences in the positive class and another one for those in the negative class.
Then, we use the posterior distributions of the hidden states from each sequence as our features.
When learning the parameters of the HMMs, we use an efficient spectral algorithm recently proposed in machine learning (Hsu et al., 2012).
The novel combination of the extracted spectral latent features and support vector machines leads to state-of-the-art results in poly(A) motif prediction.
3.1 Sequence latent features We use HMMs to take into account the uncertainty and diversity of the sequence information and hypothesize that there is a  clean  poly(A) signal hidden in the observed sequences.
The fact that each hidden vari- able is related to the previously observed positions allows us to accom- modate long-range dependencies.
We use capital letters to denote random variables and lower case letters for their instantiations.
We combine multiple adjacent nucleotides in a DNA sequence into a  mega-observation .
For example, we treat the k-mer AAT as a single observation.
Thus, each mega-observation has n Â¼ 4k possible states.
Essentially, we transform a DNA sequence into a sequence of mega-ob- servations using an overlapping sliding window of size k, and then asso- ciate each mega-observation with a variable Xt.
For example, a DNA sequence of length L0 is transformed to a sequence of L Â¼ L0   k Ã¾ 1 mega-observations, with each mega-observation being a k-mer.
We esti- mate the HMM for these transformed sequences.
Specifically, an HMM contains a Markov chain of hidden variables Q1:L :Â¼ Q1 .
.
.
Qt .
.
.
QL that generates the observed sequence of variables X1:L Â¼ X1 .
.
.
Xt .
.
.
XL.
Let m and n denote the number of states for the hidden and observed variables, respectively.
We can fully specify an HMM by an m   m transition probability matrix of the hidden variables, with the Ã°i, jÃž-th entry Tij Â¼ PrÃ°QtÃ¾1 Â¼ ijQt Â¼ jÃž, an n   m emission  i318  probability matrix Oij Â¼ PrÃ°Xt Â¼ ijQt Â¼ jÃž and an m dimension prior dis- tribution vector over the hidden states  i Â¼ PrÃ°Q1 Â¼ iÃž.
With these model Q parameters, we can compute the joint distribution of the hidden and observed variables as PrÃ°X1:L, Q1:LÃž Â¼ PrÃ°Q1Ãž L tÂ¼1 PrÃ°XtjQtÃž as PrÃ°X1:LÃž Â¼  tÂ¼2 PrÃ°QtÃ¾1jQtÃž  and the distribution of  PrÃ°X1:L, Q1:LÃž.
the observed variables  P  Q  L  q1:L  We define the latent feature at position t of the sequence as the pos- terior distribution of the hidden variable Qt, given the sequence up to position t:  Â½ft i Â¼ PrÃ°Qt Â¼ ijx1:tÃž  Ã°1Ãž  This is also called the forward belief of the hidden variable Qt, which captures the uncertainty about the  clean  signal given the observed se- quence up to position t (We can easily extend the approach to use the posterior distribution of Qt given the entire sequence x as features by involving both a forward and a backward recursion.)
Note that ft is an m- dimensional vector that sums to 1.
The latent features for the entire se- quence according to the HMM are thus the concatenation of features at all positions, i.e.
f Â¼ Ã°f   .
Calculating ft requires us to margin- 1 , .
.
.
, f alize over all previous hidden variables Q1, .
.
.
, Qt 1, which can be car- ried out in a recursive fashion:  LÃž     X  PrÃ°qtjx1:tÃž /  qt 1 Â¼  X PrÃ°xtjqtÃž PrÃ°qtjqt 1Ãž PrÃ°qt 1jx1:t 1Ãž Oxt, qt Tqt, qt 1 PrÃ°qt 1jx1:t 1Ãž  qt 1  Ã°2Ãž  Reexpressing the above relation in matrix form gives  ft / TdiagÃ°Oxt, Ãžft 1  Ã°3Ãž where Oxt,  denotes the xt-th row of the emission probability matrix O.
This matrix multiplication effectively implements the marginalization over variable Qt 1.
Let Sxt :Â¼ TdiagÃ°Oxt, Ãž and e be a vector of all ones of size m. Equation (3) suggests a recursive algorithm that efficiently extracts the latent features.
f0 :Â¼  ,  ft :Â¼ Sxt ft 1 e Sxt ft 1  ,  8t Â¼ 1, .
.
.
, L  Ã°4Ãž  Ã¾g and f  , T , O  which requires OÃ°Lm2Ãž time, once we have learned the HMM parameters, i.e.
the set of quantities f , T, Og.
We note that we learn two HMMs f Ã¾, TÃ¾, O  g from training sequences, one for those with positive labels and the other for those with negative labels.
Then for   Ã¾ a new test sequence, we can extract two latent feature vectors, f , and f   Ãž  and use the concatenation of these two features f :Â¼ Ã°f as the sequence features.
Ã¾   , f  One straightforward but computationally intensive way to learn the HMM is by using the EM.
However, EM has the problem of slow con- vergence and convergence only to a local minimum.
These two problems will affect the efficiency and efficacy of these latent feature extractions.
To overcome the shortcomings of the EM algorithm, we use a fast and local- minimum-free spectral algorithm to learn an alternative parameterization of HMMs, which is described next.
3.2 Efficient spectral algorithm for latent features Traditional HMM learning algorithms try to recover the parameters  , T and O.
The resulting maximum-likelihood estimation problem is not convex and algorithms can only find a local optimum.
These parameters  , T and O characterize the relations between hidden and observed vari- ables that cannot be directly observed during training and are usually not uniquely identifiable.
However, we may not need to recover them exactly to extract latent features.
Instead, it is sufficient to recover them up to some invertible transformation if we subsequently learn a linear classifier such as a support vector machine.
More specifically, suppose matrix A of size m   m is invertible.
Define the transformed HMM parameters     h1 :Â¼ A  1        h0 :Â¼ A ,  e, Hx :Â¼ ASxA   1  Ã°5Ãž  Then we can compute the transformed latent feature as  Â½C3, x, 1 ij    1  DÃ°L   2Ãž  X  D  X  L 2  X     dÂ¼1  tÂ¼1  i, j, x2 k  I xd  t:tÃ¾2 Â¼ Ã°i, x, jÃž     Ã°14Ãž  Poly(A) motif prediction  Â¼ Aft  ht Â¼ Hxt ht 1 h  1Hxt ht 1  Ã°6Ãž  1 during matrix multiplica- since the invertible matrix A cancels with A tion.
Then, the latent features of the final sequence that we use in our experiments are h :Â¼ Ã°hÃ¾ , h  Ãž  by concatenating the transformed fea- tures from positive and negative HMMs.
It is easy to see that the trans- formed feature will achieve the same performance as the original one with a linear classifier because the transformation matrix can be incorporated into the classifier weight vector.
That is, if we learn a binary classifier w will achieve the  signÃ°w f Ã¾ bÃž, then signÃ°e  w h Ã¾ bÃž with  e w Â¼ Ã°A   1Ãž   same classification accuracy.
A natural question is how to choose an A such that the transformed parameters fh0, h1, Hxg can be easier to estimate without using an EM algorithm.
To solve this problem, we use a construction of A by Hsu et al.
(2012), which allows these transformed parameters to be estimated from just tri-gram information of the observed sequences.
Formally, let Ã°X1, X2, X3Ãž be a triple of adjacent variables in the HMM and define the marginal probabilities of observation singletons, pairs and triples as Ã°7Ãž Ã°8Ãž  Â½c1 i Â¼ PrÃ°X1 Â¼ iÃž  Â½C2, 1 ij Â¼ PrÃ°X2 Â¼ i, X1 Â¼ jÃž Â½C3, x, 1 ij Â¼ PrÃ°X3 Â¼ i, X2 Â¼ x, X1 Â¼ jÃž,  1   x   n  Ã°9Ãž c1 is an n dimensional vector, C2, 1 is an n   n matrix and C3, x, 1 is a series of n   n matrices indexed by x. Hsu et al.
(2012) showed that setting A Â¼ U O directly links the above three quantities to the transformed parameters in (5), where U is the leading m principal left singular vectors of C2, 1.
Specifically, the transformed parameters can be directly re- covered from single sequence statics as  h0 Â¼ U    c1,  Hx Â¼ U    2, 1UÃžyc1 h1 Â¼ Ã°C   C2, 1Ãžy  C3, x, 1Ã°U    where Ã° Ãžy computes the pseudo-inverse of a matrix.
Ã°10Ãž  Ã°11Ãž  Algorithm 1 Spectral learning of transformed HMM parameters  Require: m the number of hidden states, k the number of nt to com- bine.
Ensure: transformed HMM parameters fh0, h1, Hxg.
1: Transform all training sequences by combining k consecutive nt into  a  mega-observation .
2: Use all triples Ã°x1, x2, x3Ãž from the transformed sequences to esti-  mate c1, C2, 1 and C3, x, 1 according to (12), (13) and (14).
3: Compute the Singular Value Composition (SVD) of C2, 1, and let U be the matrix of left singular vectors corresponding to the m largest singular values.
4: Compute transformed model parameters using (10) and (11).
The learning algorithm for HMMs is summarized in Algorithm 1.
It  first estimates the quantities in (7), (8) and (9) using training data  Â½c1 i   1 DL  Â½C2, 1 ij    1  DÃ°L   1Ãž     I xd  t Â¼ i    X  D  X  L  X     dÂ¼1  X  D  tÂ¼1  X  L 1  i2 k  X  dÂ¼1  tÂ¼1  i, j2 k  I xd  t:tÃ¾1 Â¼ Ã°i, jÃž     Ã°12Ãž  Ã°13Ãž  where D is the number of training sequences.
These estimates are subsequently used to compute the transformed HMM model parameters according to (10) and (11).
The algorithm has two parameters m and k that can be tuned by cross-validation.
The major computation is an SVD of C2, 1 and hence the name  spectral algorithm .
We note that we learn two HMMs, one for the positive class and the other for the negative class.
Then, we use both HMMs to extract features for each test sequence and concatenate the features, which are summarized in Algorithm 2.
Algorithm 2 Spectral latent feature extraction algorithm Require: Ã°x1, .
.
.
, xtÃž a test sequence, k the number of nt to combine, fhÃ¾ x g learned HMM models for positive and 0 , hÃ¾ negative classes.
Ensure: spectral latent features h.  x g and fh   1, HÃ¾  1, H   0 , h   1: Transform the input sequence by combining k adjacent nt into a   mega-observation .
2: For t Â¼ 1 .
.
.
L, compute hÃ¾ 3: For t Â¼ 1 .
.
.
L, compute h  4: Concatenate features h Â¼ Ã°hÃ¾   t Â¼ HÃ¾ t Â¼ H   xt  xt  .
xt  hÃ¾ t 1= hÃ¾  t 1= h   h  L , h    1 HÃ¾ 1 H   hÃ¾ t 1 h  t 1 .
L Ãž  , .
.
.
, h    xt  1  .
, .
.
.
, hÃ¾   1            3.2.1 Fast implementation The runtime of algorithm 1 is dominated by the SVD computation of an n   n matrix C2, 1, and the memory re- quirement is dominated by storing the tri-gram statistics C3, x, 1 for each x.
One technical challenge is that n, the number of possible values of  mega- observation , can grow as n Â¼ 4k, exponential in k. It seems, at first sight, that we may need to decompose a huge matrix C2, 1, and the memory requirement for C3, x, 1 is prohibitively large.
However, most entries in these matrices are zero because some k-mers do not exist in the training sequences.
Moreover, the total number of non-zero entries is at most the number of  mega-observations  in the training set.
Taking advantage of this property, we can do sparse matrix SVD and store all the tri-gram statistics in a sparse matrix, thus facilitating efficient computation and manipulation.
The computational complexity thus grows linearly with the number of  mega-observations  in the worst case.
3.3 Visualizing the importance of k-mers and positions Besides accurately classifying the sequences, we are also interested in k- mers and positions that are most informative for motif classification.
Sonnenburg et al.
(2008) proposed positional oligomer importance matri- ces (POIMs) for WD kernels to analyze the importance of substrings in different locations of the sequence.
Here, we also develop a technique for visualizing the importance of the k-mer at each position t for the classi- fication problem based on our spectral latent features.
Intuitively, we want to use the contribution of the k-mer at position t to the support vector classifier as its importance score.
In particular, we make use of the margin of a training sequence in the support vector machine.
For example, if most positive sequences with large positive margins all contain k-mer AAGC at position t, then this k-mer is import- ant for correct classifications.
More formally, let the support vector clas- sifier learned from our spectral latent features be signÃ°w h Ã¾ bÃž and let the margin corresponding to a sequence be sÃ°xÃž Â¼ w hÃ°xÃž Ã¾ b, where we use hÃ°xÃž to indicate that the features are extracted from sequence x.
Then, we define the importance of k-mer y1:k at position t as  X   Ã°y1:k@tÃž :Â¼  x2 L sÃ°xÃž PrÃ°xjxt:tÃ¾k 1 Â¼ y1:kÃž  Ã°15Ãž  i319   B.Xie et al.
That is, given the sequences that have y1:k at position t, we compute the importance as the weighted sum of the margin of these sequences.
In practice, we only have a finite number of training sequences, and we will use the finite sample average to estimate the importance score.
That is,  Ã°y1:k@tÃž   the set of training sequences with k-mer y1:k occurring at position t.  P x2T Ã°y1:k@tÃž sÃ°xÃž, where T Ã°y1:k@tÃž denotes  jT Ã°y1:k@tÃžj  1  Once we have computed the importance score for every k-mer at every position t, we can visualize it in a few different ways.
One way is to visualize scores as a heatmap of k-mer versus sequence position.
For longer k-mers, there are 4k possible values that cannot be easily visua- lized.
Instead, we sum the absolute values of all k-mer importance scores at each position, as is done in Sonnenburg et al.
(2008), and visualize the importance is,  Ã°@tÃž :Â¼  P y1:k2 k  Ã°y1:k@tÃž.
score as a function of  the position t. That  4.2 Experimental settings  f  We then searched for m 2 2, 4, .
.
.
, 40  The proposed method was tested on each of the 12 datasets using 5-fold cross-validation.
Each dataset was randomly partitioned into five subsets, four of which were used for training and val- idation, and the remaining one was used for testing in each fold.
g and k 2 3, 4, 5, 6, 7 g using cross-validations (Supplementary Fig.
S1).
For each par- ameter combination and each dataset, two HMMs were learned for the positive samples and the negative ones in the training data.
For each HMM, the parameters, fh0, h1, Hxg, were learned by Algorithm 1.
For each training sequence of 206 nt, the spec- tral latent feature vectors, Ã°h  , were then calcu- lated for the positive and negative HMMs and concatenated (Algorithm 2).
A linear Support Vector Machine (SVM) was then trained using these spectral latent features.
1 , .
.
.
, h   206 kÃ¾1Ãž   f  4 RESULTS  4.1 Datasets  is equal  The proposed method was tested on the benchmark set proposed in Kalkatawi et al.
(2012).
The dataset contains 14 740 sequences (7370 with true poly(A) motifs positive samples, and 7370 with false poly(A) motifs negative samples) for the 12 main variants of human poly(A) motifs (see Table 1 for these variants and their respective sizes).
For each variant, the number of positive se- quences to the number of negative sequences.
Furthermore, for each variant, the positive motifs with the sur- rounding sequences were extracted from human mRNA and mapped back to the human genome, whereas the same numbers of negative motifs were randomly selected from human chromo- some 21.
Each sample is a candidate 6 nt poly(A) motif sur- rounded by 100 nt upstream and downstream.
This represents a comprehensive benchmark set for poly(A) motifs in human DNA sequences.
The goal is to predict which candidate motifs are true poly(A) motifs.
Given a testing sequence of 206 nt, the spectral latent feature vectors were extracted by Algorithm 2 using the learned HMMs for the same poly(A) motif.
The concatenated feature vector was then given to the corresponding SVM model to predict whether it was a true poly(A) motif or a false one.
The grid search for different parameters with respect to the training and testing errors indicated that the parameter ranges that had highest ac- curacy on the training set were k Â¼ 4, 5, 6 and m   20 (Supplementary Fig.
S1).
4.3 Comparison to other string kernels  We first compare the classification performance of the proposed method (HMM) with the previous state-of-the-art string kernels, namely, the PPK, SPE kernel and WD kernel.
The best k for these alternative kernels, except PPK, was also searched using the cross-validation between three and seven, and we report here the best results.
In Table 1, all reported errors are the average over the 5-fold cross-validation.
It can be seen that the WD  Table 1.
Comparison of the error rates of our method (HMM) with PPK, SPE and WD  Variants  Size  Error rate (%)  False-negative rate (%)  False-positive rate (%)  PPK SPE WD  HMM Rel  PPK SPE WD  HMM Rel  PPK SPE WD  HMM Rel  AATAAA 5190   ATTAAA 2400 AAGAAA 1250 AAAAAG 1230 AATACA 880 TATAAA 780 ACTAAA 690 AGTAAA 670 GATAAA 460 AATATA 410 CATAAA 410 AATAGA 370 Average      27.13 31.28 15.04 31.48 29.87 40.72 31.19 25.43 29.51 32.68 24.05  23.08 20.17 14.72 13.25 18.98 16.28 24.35 20.90 17.39 15.85 18.78 8.11 19.56  23.72 18.29 16.72 7.80 23.18 18.46 30.29 23.88 14.13 18.78 22.20 14.86 20.22  18.59 16.21 9.36 5.45 15.34 11.15 16.96 14.33 9.57 9.27 12.68 5.14 14.42  19.45   19.63 36.41 58.90 19.16 31.50 30.36 31.43 45.00 41.54 32.47 36.67 28.09    32.50 37.12 25.20 35.91 34.36 43.48 33.73 35.22 31.22 40.98 22.16  21.93 22.83 14.08 8.94 19.55 22.31 28.41 30.75 21.74 23.90 22.93 6.49 20.60  23.70 21.50 19.68 8.46 30.68 21.54 39.42 25.67 16.96 25.85 27.80 9.73 22.47  18.54 18.17 11.36 6.02 19.09 15.64 20.00 20.60 10.43 14.63 20.49 6.49 16.26  15.47   20.44 19.32 32.73 2.33 29.89 29.59 33.01 52.00 38.78 10.64 0.00 20.75    21.75 25.44 4.88 27.05 25.38 37.97 28.66 15.65 27.80 24.39 25.95  24.24 17.50 15.36 17.56 18.41 10.26 20.29 11.04 13.04 7.80 14.63 9.73 18.52  23.74 15.08 13.76 7.15 15.68 15.38 21.16 22.09 11.30 11.71 16.59 20.00 17.96  18.65 14.25 7.36 4.88 11.59 6.67 13.91 8.06 8.70 3.90 4.88 3.78 12.59  23.05 18.57 52.08 72.22 37.04 35.00 31.43 27.03 33.33 50.00 66.67 61.11 34.17   Average  denotes the weighted average of the corresponding column.
Size  denotes the number of samples for the corresponding motif variant.
Error rate  is the proportion of false results in the dataset, which equals one minus accuracy.
False-negative rate  is the proportion of true poly(A) motifs that are predicted to be false, which equals one minus sensitivity.
False-positive rate  is the proportion of false poly(A) motifs that are predicted to be true, which equals one minus specificity.
Rel  denotes the relative improvement of HMM with respect to SPE.
The lowest error rate for each motif variant is indicated in bold.
PPK could not finish running within 48 h on AATAAA.
i320   Poly(A) motif prediction  kernel compares favorably with the SPE kernel, with slightly higher false-negative rate and slightly lower false-positive rate.
Our method, which simultaneously takes into account location information, sequence uncertainty and training labels, performs consistently and significantly better than PPK, SPE and WD.
The PPK kernel has the worst results.
As discussed in Section 2, PPK can suffer from severe overfitting by fitting each sequence to a separate HMM and it discards important discriminative information by not using the training labels.
Next, we compare the runtime of different methods using two variants, AATAAA and ATTAAA.
Our the largest method is significantly faster than alternatives at training time, while being comparable in speed at test time (Table 2).
In this experiment, the training time is equal to the time for kernel (or feature) computation for training data plus that for learning SVM models  and the test time is equal to the time for kernel (or feature) computation for test data plus that for classification.
At training time, PPK, SPE and WD need to compute a square kernel matrix of size D   D, and the asso- ciated SVM models need to be trained in the dual form.
In contrast, our method computes a feature matrix of size D   2mL, and the associated SVM model can be trained in the primal form (usually faster than in the dual form).
At test time, PPK, SPE and WD need to compute the kernel values between each support vector (the number of support vectors  can be large) and each test data point.
In contrast, our method only computes a feature vector of length 2mL for each data point, and then performs an inner product with the learned SVM model w, a vector of length 2mL.
4.4 Comparison to state-of-the-art method: the RF model  Table 3 compares the proposed method with a state-of-the-art by 5-fold cross-validation on the 12 variants of human poly(A) motifs: the RF model using domain-specific features by Kalkatawi et al.
(2012).
As shown in Table 3, our method is always significantly more accurate than RF (much lower error rates) and is more sensitive than RF on 11 out of the 12 variants.
In fact, our method improves the error rates by 7 72% as com- pared with RF on the 12 motif variants.
On average, our method has an improvement over RF in terms of the error rate, false- positive rate and false-negative rate by  26, 15 and 35%, respectively.
These percentages imply that the significant im- provement on the error rate is not just the result of a better trade off between sensitivity and specificity, but it is the result of being a better method in both senses.
By comparing the results in Tables 1 and 3, it can be seen that the RF model outperforms other string kernels (PPK, SPE and WD) in terms of accuracy for poly(A) motif prediction.
Our method that systematically extracts spectral latent features significantly improves upon RF  Table 2.
Runtime comparisons on two variants AATAAA and ATTAAA for one train/test split, with kÂ¼ 3 and all other parameters set to optimal  Time (s)  AATAAA  ATTAAA  Training Testing  PPK       SPE  46.16 6.81  WD  37.38 0.94  HMM  PPK  7.59 1.43  2722.81 674.08  SPE  9.46 1.54  WD  6.47 0.69  HMM  3.86 0.67  Note: PPK could not finish running within 48 h on AATAAA.
The values in bold indicate better results.
Table 3.
Comparison of our method (HMM) with RF  Variants  Size  Error rate (%)  False-negative rate (%)  False-positive rate (%)  RF  HMM  Rel  RF  HMM  Rel  AATAAA ATTAAA AAAAAG AAGAAA TATAAA AATACA AGTAAA ACTAAA GATAAA CATAAA AATATA AATAGA Average  5190 2400 1250 1230 880 780 690 670 460 410 410 370    20.06 18.42 16.64 11.06 19.55 19.36 27.83 22.09 20.00 18.54 24.88 18.38 19.19  18.59 16.21 9.36 5.45 15.34 11.15 16.96 14.33 9.57 9.27 12.68 5.14 14.42  7.31 12.01 43.75 50.75 21.53 42.39 39.07 35.14 52.17 50.01 49.02 72.06 25.62  19.74 18.68 16.53 11.92 18.10 18.13 25.24 20.69 21.01 16.92 24.12 19.37 18.83  18.54 18.17 11.36 6.02 19.09 15.64 20.00 20.60 10.43 14.63 20.49 6.49 16.26  6.10 2.75 31.28 49.53  5.47 13.73 20.76 0.45 50.33 13.51 15.06 66.51 14.81  RF  20.37 18.15 16.75 10.15 20.87 20.49 29.92 23.36 18.92 20.00 25.59 17.32 19.48  HMM  Rel  18.65 14.25 7.36 4.88 11.59 6.67 13.91 8.06 8.70 3.90 4.88 3.78 12.59  8.44 21.49 56.06 51.94 44.46 67.46 53.50 65.50 54.04 80.49 80.94 78.15 35.40  Note: The performance of both RF and HMM is evaluated on the same 5-fold cross-validation.
Rel  denotes the relative improvement of HMM with respect to RF.
The lowest value for each criterion of each motif variant is indicated in bold.
i321   B.Xie et al.
(a)  (c)  (e)  (g)  (i)  (k)  (b)  (d)  (f)  (h)  (j)  (l)  Fig.
1.
Visualization of the importance of different dimers at different positions for the 12 variants of human poly(A) motifs.
The x-axis gives the positions in the sequence.
The y-axis lists all 16 possible dimers.
The colors denote the levels of importance: the light green color for the positions 0 6 is the background color, which indicates that no effects differentiate true and false motifs  the darker the red, the more important the dimer at that position is to identifying true motifs  the darker the blue, the more important the dimer at that position is to identifying false motifs.
and other string kernels on the same task.
This supports our assumption that uncertainty and diversity information is import- ant for this problem and there is a  clean  DNA signal hidden in the observed sequences.
4.5 Visualizing importance scores of dimers and positions  Another advantage of our method over previous state-of-the-art poly(A) motif predictors is that our method can be used to visu- alize the importance of k-mers or positions to the prediction task.
i322   Poly(A) motif prediction  (a)  (c)  (e)  (g)  (i)  (k)  (b)  (d)  (f)  (h)  (j)  (l)  Fig.
2.
Visualization of the importance of different positions for the 12 motif variants.
The x-axis gives the position in the sequence.
For each k from 1 to 5, the y-axis is the importance score of a position by summing over the absolute values of the importance for all possible k-mers at that position  This can provide researchers or users a direct and intuitive way to study the patterns and characteristics of DNA sequences.
More importantly, most previous studies on poly(A) motifs try to reveal statistics of the surrounding regions of true motifs.
Our method, in contrast, can reveal patterns for false motifs at the same time.
In Figure 1, we present the importance scores  Ã°dimer@tÃž for dimers (subsequences of 2 nt, e.g.
AG) in determining whether or not a candidate poly(A) motif is a true motif.
A number of interesting observations can be made about this figure:  AA is an informative subsequence to differentiate true poly(A) motifs from false ones in all 12 variants of human poly(A) motifs.
When AA appears frequently within 30 nt downstream of the candidate poly(A) motif, this strongly suggests that it is a  true poly(A) motif.
This is expected because the mRNA cleavage site is often 15 25 nt downstream of the poly(A) motif, and the region between the cleavage site and the motif is known to be A- rich (Retelska et al., 2006).
Interestingly, when AA appears fre- quently within 100 nt upstream of the candidate motif, the can- didate is likely to be a false motif.
CG is an interesting dimer.
Its positions carry important in- formation for determining both true and false poly(A) motifs.
When CG appears beyond 40 nt upstream of the motif, this sug- gests that the motif is a true poly(A) motif.
On the other hand, CG serves as a strong sign for false predictions in the immediate 20 nt downstream and then becomes a sign for true predictions for further downstream sequences.
In Hu et al.
(2005), it was found that  100/ 41 and Ã¾41/Ã¾100 regions of poly(A) motifs  i323   B.Xie et al.
are C- and G-rich regions.
Our results suggest that looking at CG together rather than individually may capture more informative patterns.
TA or AT is one of the characteristics for false poly(A) motifs among all 12 variants.
No matter if it appears frequently in downstream or upstream nt sequences of the candidate poly(A) motif, TA or AT suggests that the candidate is a false one.
Between them, TA is more informative than AT to specify false motifs.
On the one hand, our findings coincide with previ- ous studies that TA and AT are important features around the poly(A) motifs and TA is more frequent than AT [e.g.
the TATATA oligonucleotide is more over-represented than the ATATAT oligonucleotide (Hu et al., 2005  van Helden et al., 2000)].
On the other hand, our findings reveal that TA and AT appear much more often in sequences around the false motifs than around the true motifs.
In van Helden et al.
(2000)  Hu et al.
found that TATATA and ATATAT are the most over-represented oligonucleotides downstream of the poly(A) motifs.
However, by analyzing the negative motifs, our results imply that although TA and AT appear often in positive motifs, they appear even more often in negative ones.
it was  (2005),  TG can determine false motifs at alternate positions upstream or downstream of the candidate motif of AATAGA (Fig.
1(l)).
This is not the case for the two most frequent motifs, AATAAA and ATTAAA, which partially supports our hypothesis that the intrinsic characteristics of the frequent motifs and rare motifs are different.
Thus, a good poly(A) motif predictor should have dif- ferent models for different motif variants.
Figure 2 shows the importance scores for different positions with k from 1 to 5.
Again, the 6 nt positions for the candidate motifs offer no information to the prediction.
However, because the k-mers overlapping with the motif regions contain subse- quences of the motifs, the motif regions do not have absolutely  zero  importance.
Again, we list key observations here:  The longer the subsequences are (bigger k), the smoother the importance curves are.
This is expected because considering more nt at the same time will average the effects caused by in- dividual positions.
In almost all the variants, the 50 nt downstream of the candi- date motifs are informative.
Specifically, motif variants AATAAA, CATAAA and AATATA have important information at the positions around the 25th nt downstream of the candidate motifs (Fig.
2a, j and k).
This coincides with the fact that the mRNA cleavage site is 15 25 nt downstream of poly(A) motifs (van Helden et al., 2000  Retelska et al., 2006).
Our method can be directly applied to other sequence classi- fication problems and achieve state-of-the-art results, such as transcription start-site prediction and splice-site prediction (Supplementary Material S1 and Supplementary Table S1).
the  Our method, currently, requires a fixed length of upstream and downstream sequences for the training and testing data.
Such prior knowledge has to be given as input.
We are trying to gen- eralize and extend our method to take varying lengths of se- quences for different samples.
Furthermore, there may be longer-range dependency between the latent variables, and HMMs of higher orders may be needed for the feature extraction purpose, for which junction tree-type algorithms can be applied (Parikh et al., 2012).
Similar to the WD kernel with shifts (Ra  tsch et al., 2005), our method can also be straightforwardly extended to take shifted matches into account.
ACKNOWLEDGEMENT  We thank Virginia Unkefer for editorial work on the manuscript.
Funding: This work was supported in part by an AEA grant awarded by the King Abdullah University of Science and Technology Office of Competitive Research Funds under the title  Association of genetic variation with phenotype at the net- work and function level  and an NSF grant (IIS1218749).
Conflict of Interest: none declared.
REFERENCES  Ahmed,F.
et al.
(2009) Prediction of polyadenylation signals in human DNA se-  quences using nucleotide frequencies.
In Silico Biol, 9, 135 148.
Akhtar,M.N.
et al.
(2010) Polyar, a new computer program for prediction of poly(a)  sites in human sequences.
BMC Genomics, 11, 646.
Beaudoing,E.
et al.
(2000) Patterns of variant polyadenylation signal usage in  human genes.
Genome Res., 10, 1001 1010.
Bernstein,P.
and Ross,J.
(1989) Poly(a), poly(a) binding protein and the regulation  of mRNA stability.
Trends Biochem.
Sci., 14, 373 377.
Chang,T.H.
et al.
(2011) Characterization and prediction of mRNA polyadenyla-  tion sites in human genes.
Med.
Biol.
Eng.
Comput., 49, 463 472.
Cheng,Y.
et al.
(2006) Prediction of mRNA polyadenylation sites by support vector  machine.
Bioinformatics, 22, 2320 2325.
Das,A.T.
et al.
(1997) A conserved hairpin motif in the r-u5 region of the human immunodeficiency virus type 1 RNA genome is essential for replication.
J.
Virol., 71, 2346 2356.
Dempster,A.P.
et al.
(1977) Maximum likelihood from incomplete data via the EM  algorithm.
J. R. Stat.
Soc.
B, 39, 1 22.
Graber,J.H.
et al.
(1999) In silico detection of control signals: mRNA 30-end-pro- in diverse species.
Proc.
Natl Acad.
Sci.
USA, 96,  cessing sequences 14055 14060.
5 CONCLUSION AND FUTURE WORKS  In this article, we proposed a novel method to extract fea- tures from upstream and downstream regions of candidate poly(A) motifs in human DNA sequences.
Our proposed spectral latent feature-based method achieves state-of-the-art results.
The proposed method systematically explores the informa- tion encoded in nucleotide sequences by learning sequence dynamics and matching latent distributions on each position, and it can be easily extended to visualize the importance of subsequences and positions, thus providing a general method for sequence-based classification problems in bioinformatics.
Hsu,D.
et al.
(2012) A spectral algorithm for learning hidden Markov models.
J. Comput.
Syst.
Sci., 78, 1460 1480.
Hu,J.
et al.
(2005) Bioinformatic identification of candidate cis-regulatory elements  involved in human mrna polyadenylation.
RNA, 11, 1485 1493.
Jebara,T.
et al.
(2004) Probability product kernels.
J. Mach.
Learn.
Res., 5, 819 844.
Ji,G.
et al.
(2010) A classification-based prediction model of messenger rna poly-  adenylation sites.
J. Theor.
Biol., 265, 287 296.
Kalkatawi,M.
et al.
(2013) Dragon PolyA Spotter: predictor of poly(A) motifs within human genomic DNA sequences.
Bioinformatics, [Epub ahead of print, doi: 10.1093/bioinformatics/btt161, April 15, 2013].
Kim,K.M.
et al.
(2002) Polya deletions in hereditary nonpolyposis colorectal cancer:  mutations before a gatekeeper.
Am.
J.
Pathol., 160, 1503 1506.
Langemeier,J.
et al.
(2012) A complex immunodeficiency is based on u1 snrnp-  mediated poly(a) site suppression.
EMBO J., 31, 4035 4044.  i324   Poly(A) motif prediction  Legendre,M.
and Gautheret,D.
(2003) Sequence determinants in human polyade-  Ra  tsch,G.
et al.
(2005) Rase: recognition of alternatively spliced exons in c. elegans.
nylation site selection.
BMC Genomics, 4, 7.
Bioinformatics, 21 (Suppl.
1), i369 i377.
Leslie,C.
et al.
(2002) The spectrum kernel: A string kernel for svm protein classi- fication.
In Proceedings of the Pacific Symposium on Biocomputing, Vol.
7, pp 566 575.
Hawaii, USA.
Leslie,C.
et al.
(2004) Mismatch string kernels for discriminative protein classifica-  tion.
Bioinformatics, 20, 467 476.
Retelska,D.
et al.
(2006) Similarities and differences of polyadenylation signals in  human and fly.
BMC Genomics, 7, 176.
Salamov,A.A.
and Solovyev,V.V.
(1997) Recognition of 30-processing sites of  human mrna precursors.
Comput.
Appl.
Biosci., 13, 23 28.
Sonnenburg,S.
et al.
(2006) Arts: accurate recognition of transcription starts in  Leung,A.K.
et al.
(2011) Poly(adp-ribose) regulates stress responses and microrna  human.
Bioinformatics, 22, e472 e480.
activity in the cytoplasm.
Mol.
Cell, 42, 489 499.
Sonnenburg,S.
et al.
(2007) Accurate splice site prediction using support vector  Liu,H.
et al.
(2005) Dnafsminer: a web-based software toolbox to recognize two  machines.
BMC Bioinformatics, 8 (Suppl.
10), S7.
types of functional sites in dna sequences.
Bioinformatics, 21, 671 673.
Lukashin,A.
and Borodovsky,M.
(1998) Genemark.hmm: new solutions for gene  finding.
Nucleic Acids Res., 26, 1107 1115.
Parikh,A.
et al.
(2012) A spectral algorithm for latent junction trees.
In Uncertainty  in Artificial Intelligence.
AUAI Press, Catalina Island, USA.
Pastrello,C.
et al.
(2006) Stability of bat26 in tumours of hereditary nonpolyposis colorectal cancer patients with msh2 intragenic deletion.
Eur.
J. Hum.
Genet., 14, 63 68.
Proudfoot,N.J.
(2011) Ending the message: poly(a) signals then and now.
Genes  Dev., 25, 1770 1782.
Ra  tsch,G.
and Sonnenburg,S.
(2004) Accurate splice site detection for caenorhab- ditis elegans.
In Kernel Methods in Computational Biology.
MIT press, p. 277.
Sonnenburg,S.
et al.
(2008) POIMs: positional oligomer importance matrices  understanding support vector machine-based signal detectors.
Bioinformatics, 24, i6 i14.
Stanke,M.
and Waack,S.
(2003) Gene prediction with a hidden Markov model and  a new intron submodel.
Bioinformatics, 19 (Suppl.
2), ii215 ii225.
Tabaska,J.E.
and Zhang,M.Q.
(1999) Detection of polyadenylation signals in  human DNA sequences.
Gene, 231, 77 86.  van Helden,J.
et al.
(2000) Statistical analysis of yeast genomic downstream se- quences reveals putative polyadenylation signals.
Nucleic Acids Res., 28, 1000 1010.  i325
