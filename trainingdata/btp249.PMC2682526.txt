BIOINFORMATICS ORIGINAL PAPER Vol.
25 no.
11 2009, pages 1412 1418  doi:10.1093/bioinformatics/btp249  Data and text mining MeSH Up: effective MeSH text classiﬁcation for improved document retrieval Dolf Trieschnigg1,2, , Piotr Pezik1, Vivian Lee1, Franciska de Jong2, Wessel Kraaij3 and Dietrich Rebholz-Schuhmann1 1European Bioinformatics Institute, Hinxton, UK, 2HMI, University of Twente, Enschede and 3TNO ICT, Delft, The Netherlands Received on November 20, 2008  revised on April 2, 2009  accepted on April 7, 2009 Advance Access publication April 17, 2009 Associate Editor: Limsoon Wong  ABSTRACT Motivation: Controlled vocabularies such as the Medical Subject Headings (MeSH) thesaurus and the Gene Ontology (GO) provide an efﬁcient way of accessing and organizing biomedical information by reducing the ambiguity inherent to free-text data.
Different methods of automating the assignment of MeSH concepts have been proposed to replace manual annotation, but they are either limited to a small subset of MeSH or have only been compared with a limited number of other systems.
Results: We compare the performance of six MeSH classiﬁcation systems [MetaMap, EAGL, a language and a vector space model- based approach, a K-Nearest Neighbor (KNN) approach and MTI] in terms of reproducing and complementing manual MeSH annotations.
A KNN system clearly outperforms the other published approaches and scales well with large amounts of text using the full MeSH thesaurus.
Our measurements demonstrate to what extent manual MeSH annotations can be reproduced and how they can be complemented by automatic annotations.
We also show that a statistically signiﬁcant improvement can be obtained in information retrieval (IR) when the text of a user s query is automatically annotated with MeSH concepts, compared to using the original textual query alone.
Conclusions: The annotation of biomedical texts using controlled vocabularies such as MeSH can be automated to improve text- only IR.
Furthermore, the automatic MeSH annotation system we propose is highly scalable and it generates improvements in IR comparable with those observed for manual annotations.
Contact: trieschn@ewi.utwente.nl Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Controlled vocabularies play an important role in the integration of large-scale bioinformatics resources and applications.
They are actively used to annotate scientiﬁc literature and experimental data.
Probably the most well-known example is the Medical Subject Headings (MeSH) thesaurus, developed and maintained by the National Library of Medicine, which has been introduced to    To whom correspondence should be addressed.
categorize and search MEDLINE citations.
Similarly, the Gene Ontology (GO) is used for annotating genes and gene product experiments (Gaudan et al., 2008  Lu et al., 2008).
In both cases, concepts, i.e.
distinct entries in a controlled vocabulary, are used for the annotation (also called classiﬁcation and categorization depending on the context) of literature and experiments.
Unsurprisingly, these controlled vocabularies are increasingly used and investigated for representing, searching and summarizing information (e.g.
Ruch, 2006).
Using a controlled vocabulary for representing information is especially useful in the biomedical domain, where a simple text-based representation of information is too ambiguous (Nenadic et al., 2004).
A conceptual representation allows information from different sources, such as databases containing documented experimental data and related literature to be linked in a transparent way, facilitating further data analysis in bioinformatics.
Recently, MeSH and GO concepts have been used to prioritize genes by their relevance to diseases (Yu et al., 2008).
The goal of this work is 2-fold.
First, our goal is to build a system which can annotate an arbitrary piece of text with relevant MeSH terms, similar to the manual classiﬁcation of MEDLINE citations with MeSH terms.
In this work, we extensively compare six systems in terms of their capacity to reproduce manual classiﬁcation.
Several classiﬁers have been proposed in the past (see related work), but either their usefulness or evaluation to other methods has been limited.
We focus our comparison on systems which allow classiﬁcation using the complete set of available MeSH terms.
In addition, we evaluate if manual classiﬁcations are complemented by automatically obtained MeSH terms.
Second, our goal is to use this automatic classiﬁcation method to improve upon biomedical document retrieval.
We compare the usefulness of the six classiﬁers to automatically annotate a textual query with MeSH concepts.
The effectiveness is tested on the Text REtrieval Conference (TREC) Genomics collections (Hersh et al., 2004).
We show that these improvements can be traced back to classiﬁcation performance.
The structure of this article is as follows.
First, we give an overview of related work, followed by an overview of the different MeSH classiﬁers we have tested.
Next, we evaluate the text classiﬁcation performance of these classiﬁers.
After that, we try to use the classiﬁers for the annotation of queries from several TREC Genomics test collections to improve document retrieval.
We ﬁnish with a discussion and conclusion on the results.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:54 14/5/2009 Bioinformatics-btp249.tex]  Page: 1412  1412 1418   2 RELATED WORK Quite a number of researchers have developed MeSH classiﬁcation techniques [see Sohn et al.
(2008) for more related work].
The assignment of MeSH descriptors to text is a large multi-class and multi-label text classiﬁcation problem: one or more of 24 000 MeSH descriptors can be assigned to a piece of text.
Most out-of-the-box text classiﬁers, such as decision trees, rule learning, neural networks and support vector machines (SVMs) are not directly suitable for this task.
SVMs for example, have shown their superiority to Naive Bayes classiﬁers on binary classiﬁcation tasks, but without sophisticated adaptation it is not feasible to train and build a system using SVMs for 24 000 classes.
The related work on MeSH classiﬁcation shows a clear separation between research on sophisticated techniques limited to a subset of the problem and more straightforward techniques which do offer a complete solution.
Several researchers have used the OHSUMED collection and investigated the performance of their classiﬁers on a subset of MeSH descriptors, for example, those in the Heart Disease branch (Lam and Ho, 1998  Ruiz and Srinivasan, 2002), or by only considering generalized descriptors (Rak et al., 2007).
Recently, Sohn et al.
(2008) investigated optimal training sets for Naive Bayes  classiﬁers on a small set of 20 MeSH descriptors.
Despite the reported improvements over the K-Nearest Neighbors (KNN) approach, so far such a classiﬁer has not been proven feasible for all 24 000 MeSH terms.
The systems which do classiﬁcation on all descriptors are usually inspired by information retrieval (IR) techniques and return a ranked list of the most appropriate MeSH terms (e.g.
Lam et al., 1999  Ruch, 2006).
The actual classiﬁcation, i.e.
the binary assignment of a particular term to a piece of text, is achieved by cutting off the list at a particular rank or score.
The well-known Medical Text  Indexer  (MTI)  introduced  by Aronson et al.
(2004) is further discussed in Section 3.4.
We focus this work on systems which can be used for classifying  the full set of MeSH terms.
A fair amount of  research has also been carried out  to incorporate MeSH terms in a retrieval system [see for example, Camous et al.
(2006) for an overview].
During TREC Genomics (http://ir.ohsu.edu/genomics/), a large effort was spent to improve document retrieval using knowledge sources such as UMLS (including MeSH), Entrez Gene and Uniprot.
Extending a user query with appropriate concepts from such knowledge sources showed mixed results.
In fact, Hersh et al.
(2004) note that in comparison to an out-of-the-box full-text search system  approaches that attempted to map to controlled vocabulary terms [such as MeSH] did not fare as well .
In this article, we show the relationship between the quality of this mapping and improvements observed in IR.
Next to mapping query text to concepts using string matching, a common method to obtain a MeSH-based representation of the query is to use relevance feedback: The original query is used to retrieve a set of documents and based on the MeSH terms assigned to these documents, a MeSH- based query is obtained.
Srinivasan (1996) observes improvements in document retrieval using MeSH terms and despite the limited size of the collection concludes that MeSH terms are important for retrieval.
Our work reinvestigates this conclusion in the context of improved document retrieval methods on larger document and query collections.
MeSH Up  3 SYSTEM AND METHODS Four types of approaches are investigated.
First, two classiﬁers which only use the information in the MeSH thesaurus itself (referred to as  Thesaurus- oriented  classiﬁers).
Second, two systems which use training data to build explicit models for each MeSH concept ( Concept-oriented  classiﬁers).
Third, a system which uses the manual annotations of documents similar to the text to classify, to determine suitable concepts ( K-Nearest Neighbor  classiﬁer).
Finally, a hybrid and manually reﬁned system which combines different approaches ( Hybrid  classiﬁer).
One of the Concept-oriented classiﬁers and the KNN classiﬁer are based on IR based on language models, a commonly used retrieval framework which is brieﬂy explained in the online Supplementary Material.
In Sections 3.1 3.4, the investigated systems are explained.
Example output of the different systems can be found in the online Supplementary Material.
In the last section, we discuss the evaluation methodology for the two tasks.
3.1 Thesaurus-oriented classiﬁers The ﬁrst two investigated classiﬁers both rely on information in the MeSH thesaurus only.
The assignment of MeSH terms is based on the match between the information about a particular MeSH term, such as its synonyms and short description, and the text to classify.
3.1.1 MetaMap MetaMap is a major component of the NLM s MTI (see Section 3.4).
Thesaurus concepts are found by ﬁrst parsing the text into simple noun phrases and then by matching a large number of generated variants to the entries in the Uniﬁed Medical Language System (UMLS) metathesaurus.
In our experiments, we ﬁltered the output to concepts which occur in the MeSH thesaurus.
Aronson (2001) describes MetaMap in more detail.
MetaMap assigns a conﬁdence score to each concept found.
These scores are used to rank the list of MeSH terms in descending conﬁdence order.
3.1.2 EAGL Ruch (2006) introduced a retrieval-based system for MeSH classiﬁcation.
For each MeSH term, its synonyms and description are indexed as a single document in a retrieval index.
A piece of text, the query to the retrieval system, is classiﬁed with the best ranked MeSH  documents .
The advantages of this approach are high speed and small index size.
One drawback is that it may return MeSH terms which only share a single word with the text to classify.
The phrase  Breast cancer  could, for example, yield the MeSH term  Breast cancer , but also other MeSH terms containing the word  cancer , such as  Testicular cancer  and  Stomach cancer .
3.2 Concept-oriented classiﬁers The MeSH thesaurus has already been used extensively to classify MEDLINE citations, so it seems obvious to use the available manual assignments of MeSH terms to citations as training data.
For the concept-oriented classiﬁers, we build a model for each MeSH concept ofﬂine, i.e.
before the actual classiﬁcation.
Similar to EAGL, an index is created in which each MeSH term is represented by a special  MeSH document .
This MeSH document is simply created by merging the titles and abstracts of a number of documents assigned with that MeSH term.
Two common retrieval methods are used for retrieving the most relevant MeSH documents, one based on language models (described in the online Supplementary Material) and the other using a vector-based representation, which are described below.
3.2.1 Concept language models For the classiﬁcation system based on language models, a concept language model (CLM) is created for each MeSH term based on the MeSH document introduced before.
This CLM is a probability distribution over words which are associated to a MeSH term.
The parameters of the CLM are a maximum likelihood estimate based on the relative occurrence frequencies of words in the MeSH document.
1413  [11:54 14/5/2009 Bioinformatics-btp249.tex]  Page: 1413  1412 1418   D.Trieschnigg et al.
Formally, the probability of a term t in a CLM is deﬁned as:  P(t M)= (cid:2) D DM  P(t,D M)  (cid:2) D DM  P(t D)P(D M)  where DM is a set of documents assigned to the MeSH term M, P(D M) is the probability a document language model is picked to describe this term (which is assumed uniformly distributed over DM ) and P(t D) is the smoothed document language model of D. A piece of text is classiﬁed by creating a query language model P(t Q) for this text and ranking the CLMs using the negative cross entropy  H:   H(Q,D)= (cid:2)  P(t Q)logP(t D)  This system shows close resemblances to a Naive Bayes classiﬁer,  commonly used for text classiﬁcation (Lewis, 1998).
t  3.2.2 BM25 The Okapi BM25 is a vector space retrieval model which is commonly used as a baseline for retrieval experiments (Robertson et al., 1996).
The MeSH document is indexed as a TF.IDF vector and the text to classify is used as a query Q.
Given a query Q, the BM25 score of a MeSH document is:  score(D,Q)=(cid:2) q Q  IDF(q)  f (q,D) (k1+1) f (q,D)+k1 (1 b+b   ,   D  avgdl )  where IDF(q), is the inverse document frequency of the term q. k1 and b are tuning parameters, and avgdl is the average document length.
3.3 KNN classiﬁer The KNN classiﬁer investigated here is similar to the PubMed related citations algorithm (Lin and Wilbur, 2007).
A piece of text is classiﬁed by looking at the manual classiﬁcation of similar or neighboring documents.
We consider KNN for three reasons.
First, it can be easily scaled up to such a large classiﬁcation task.
Second, it gracefully integrates documents as a link between text and groups of related concepts.
For document classiﬁcation and retrieval such an integration may be preferred over approaches which model separate (rules for) concepts.
The last reason is practical: in many research environments a full-text search system on MEDLINE is already available, making KNN straightforward to implement.
Our KNN classiﬁer relies on a retrieval system based on language models.
Similar to CLM, the parameters of the query language model are estimated on the text to classify.
Next, citations most similar to this query language model are retrieved.
The classiﬁcation is based on the MeSH terms assigned to the top K retrieved documents (based on preceding experiments K =10 was used).
The relevance of a MeSH term is determined by summing the retrieval score of the top documents that have been assigned that term.
3.4 Hybrid classiﬁer: MTI The MTI, provided to registered users by the NLM, incorporates different classiﬁers, including MetaMap, the  Pubmed Related Citations Algorithm  and  Restrict to MeSH .
Different processing steps including clustering and applying (manually deﬁned) rule-based ﬁltering are used in this hybrid system.
Parts of the systems have been evaluated using user questionnaires and in a  machine learning setting  (Aronson et al., 2004  Kim et al., 2001).
An evaluation against other classiﬁcation systems or an assessment of its usefulness for IR is missing however.
Details of the system can be found on the Semantic Knowledge Representation website (http://skr.nlm.nih.gov/).
We treat MTI as a black box system, using the default settings to obtain MeSH classiﬁcations which favors the MeSH term suggested by MetaMap (with weight 7) over the ones from the related citations component (weight 2).
1414  3.5 Evaluation methods 3.5.1 Evaluating text classiﬁcation A commonly used method to evaluate MeSH text classiﬁcation is to see how well a classiﬁer reproduces the manual annotations of MEDLINE citations.
Selected citations of the OHSUMED collection (Hersh et al., 1994) have been used as training and test data, but as Ruiz and Srinivasan (2002) note, different test collections and variable numbers of categories have been used, making comparisons difﬁcult.
Moreover, the OHSUMED collection is not up-to-date anymore.
At the time of its creation, the MeSH thesaurus consisted of around 14 000 MeSH terms.
Currently, the thesaurus contains around 24 000 terms, making an evaluation using OHSUMED not representative for the current state of MeSH.
Similar to Ruch (2006), we therefore take a random sample of a 1000 citations from the MEDLINE 2008 baseline distribution.
Lam et al.
(1999) describe three quality metrics which can be used in this context: document, category and decision perspective metrics.
The document perspective metric evaluates the assignment of MeSH terms at the document level.
Since all our classiﬁcation systems rank the suggested MeSH terms, a summary measure can be used to indicate which system has the ability to rank manually assigned categories higher than others: 10- or 11-point average precision, more commonly known as Mean Average Precision in IR can be used to indicate the performance at a document level.
A more intuitive document perspective metric is Precision at 10 (P10), which indicates how many of the ﬁrst 10 suggested terms correspond to manual annotations.
The category perspective metric calculates the F-measure, Precision and Recall for each MeSH term.
Finally, the decision perspective metric (micro recall, precision and F-measure) looks at the number of correct and incorrect decisions a classiﬁcation system makes, where each possible document and category pair form a decision.
Both the F-measure and micro F-measure require a discrete number of classiﬁcations per instance and our classiﬁers return a ranked list of classes.
Similar to Lam et al.
(1999), we report the measures using the optimal cutoff value (additional measurements are provided in the online Supplementary Material)1.
For more information about these measures, see Lam et al.
(1999).
Despite the fact that manual annotations of MEDLINE are carefully created and on average the most important terms are assigned, we note that using these manual annotations for evaluation is an idealization.
Manual annotators do accidentally assign irrelevant MesH terms or miss relevant terms.
To investigate this issue, an experienced annotator judged some of the false positives, i.e.
automatic annotations which are not in the set of manually assigned terms.
For 50 of the 1000 citations in the test, the annotator judged the three highest ranking false positives from MetaMap, CLM and KNN2 on a 5-point scale.
To test the reliability of our annotator three manual annotations were added to each citation as well.
For each of the 50 citations, the title and abstract were presented with 12 (9 false positives and 3 true positives) randomly ordered MeSH terms.
Each MeSH term was then judged on a 5-point scale ranging from  Strongly irrelevant/Incorrect  to  Strongly relevant  (the scale is discussed in detail in the online Supplementary Material).
This analysis provides additional insights into the performance of the different classiﬁcation systems.
Some of the automatically identiﬁed terms may have been judged as irrelevant (false positives), because they were not included in the original MeSH annotations.
By taking a closer look, however, we may actually ﬁnd them to be highly relevant, i.e.
appropriate to represent the text to classify.
3.5.2 Evaluating document retrieval To determine the added value of using MeSH terms for document retrieval, we carry out a TREC-style evaluation (http://trec.nist.gov).
Given a ﬁxed document collection and a number of queries for information, systems are evaluated for their ability to improve document retrieval.
As a baseline, we only use the textual representation of the queries and documents.
Using the evaluated systems,  1By assuming the number of top classes which gives the highest score.
2Restricted to these systems because of resource limitations.
[11:54 14/5/2009 Bioinformatics-btp249.tex]  Page: 1414  1412 1418   for each textual query we generate a set of MeSH terms, which serve as conceptual queries.
In a ﬁrst experiment, the conceptual queries are matched against the original MeSH annotations provided by MEDLINE (Table 4).
In a second set of experiments, the conceptual queries are matched against document annotations generated automatically using the KNN system (Table 3).
The retrieval model  is again based on unigram language models (described in the online Supplementary Material), which proved to be successful during previous TREC evaluations (Hiemstra and Kraaij, 1999).
To differentiate between the added value of the conceptual and textual representations, separate text and concept indices are created.
In contrast to, for example, Srinivasan (1996), who indexes the words in the MeSH terms, unique identiﬁers are used in the concept index.
The MeSH term  Adult , for example, is indexed as  D000328 .
This prevents matching MeSH terms with overlapping surface forms (e.g.
Mad hatter disease with  Mad cow disease ) and makes concept matching as unambiguous as possible.
Similarly, two query models are created, one textual and one conceptual.
The parameters of the textual query model are based on a maximum likelihood estimate on the query text.
The conceptual query model is based on output of one of the six classiﬁers on the query text.
The parameters of the model are based on relevance scores of suggested MeSH terms:  P(c QC)   s(c,QC) (cid:3) c(cid:6) QC s(c(cid:6),Q)  ,  where s(c,Q) is the classiﬁcation score assigned to MeSH concept c for the query and QC is the set of terms suggested by the system.
As a matching model, we interpolate the query likelihood of both  representations:  P(Q D)= αP(QC DC)+(1 α)P(QT DT ),  (1)  where α deﬁnes the mix between text and concepts.
For the baseline, in which we only use the textual representations for retrieval, α is set to 0.
Since we do not know the optimal value of α we vary this value between 0 and 1 with steps of 0.05, to ﬁnd the optimal mix.
Following the commonly used TREC evaluation criteria, we use mean average precision and precision at 10 as performance indicators.
As suggested by Smucker et al.
(2007), Fisher s randomization test is used to determine the statistical signiﬁcance of the results.
4 MeSH DOCUMENT CLASSIFICATION As an initial test of our selected MeSH classiﬁers, we look at their capability to reproduce manual MEDLINE annotation.
4.1 Experimental setup A 1000 random MEDLINE citations are selected as a test set from the MEDLINE 2008 baseline distribution, with the only requirement that they should have at least one MeSH term assigned to them.
The list of citations can be downloaded for followup research (http://www.ebi.ac.uk/ triesch/meshup/testset_v1.xml).
The test set covers 3951 distinct MeSH terms (9596 assignments).
The remaining citations in the 2008 baseline distribution are used for training: to build an index for the KNN approach and for sampling citations (at most 1000 citations per MeSH term) to assemble the MeSH document for the BM25 and CLM approach.
4.2 Results Table 1 shows the classiﬁcation results of the different systems when presented with the title and abstract of a 1000 random MEDLINE citations.
MTI serves as the baseline against which the other systems are compared.
It shows to perform quite well on the classiﬁcation  MeSH Up  task.
Both thesaurus-oriented classiﬁers (MetaMap and EAGL) and concept-oriented classiﬁers (CLM and BM25) perform worse than MTI on all metrics.
KNN forms a notable exception: it shows 99% improvement in terms of MAP, 41% improved precision at 10 (P10) and 12% improvement in micro F1.
On average, more than three of the top 10 returned terms from MTI correspond to manual annotation, whereas KNN returns more than four matching terms.
In terms of Category F1, KNN performs 10% worse than MTI: when considering one MeSH term, MTI is better in choosing whether to assign it to a citation or not.
Considering the performance from a document perspective, KNN outperforms MTI: given the title and abstract of a citation, KNN ﬁnds more correct/manual MeSH terms and ranks them higher.
MTI shows to be very sensitive to the amount of input provided.
When presented with only the title (and the PMID) of the citation (tables are available in the online Supplementary Material), it performs much worse on all measures (loss between 35% and 43%).
In contrast, KNN only shows a moderate decrease (drops between 4% and 9%), which indicates that it is more robust when less information is presented.
Also the other four systems (BM25, EAGL, MetaMap and CLM) are less sensitive to the length of the input (dropping at most 30%).
Additional investigation (see online Supplementary Material for metrics) shows that MTI and KNN are capable of reproducing both general and speciﬁc MeSH terms.
The other four systems perform relatively well on reproducing speciﬁc MeSH terms, i.e.
terms which are not frequently used for annotation in MEDLINE.
Table 2 shows the results of the annotation process described in Section 3.5.1.
The ﬁrst column of Table 2 shows that in 88% of the cases our annotator judged the original MeSH annotations as (very)  Table 1.
MeSH classiﬁcation performance on 1000 random MEDLINE citations, using title and abstract as input  Document  Category  Decision  Method  MAP  P10  F1  micro F1  MTI BM25 MetaMap CLM EAGL KNN  0.2536 0.0912  64% 0.1623  36% 0.1783  30% 0.1976  22% 0.5052  +99%  0.3200 0.1021  68% 0.1910  40% 0.1748  45% 0.2119  34% 0.4515  +41%  0.4503 0.2251  50% 0.3187  29% 0.3429  24% 0.2987  34% 0.4074  10%  0.4415 0.1972  55% 0.2968  33% 0.2982  32% 0.2977  33% 0.4963  +12%  All differences in MAP and P10 are signiﬁcant with a P   0.005, based on Fisher s randomization test.
Table 2.
Results from the analysis of false positives  Judgment  True  False positives  positives  MetaMap  CLM  KNN  Very relevant Relevant Undecided Irrelevant Incorrect  94 17 12 1 2  75% 13% 10% 1% 2%  40 39 20 33 4  29% 29% 15% 24% 3%  44 26 66 35 16  24% 37 14% 27 35% 49 19% 58 9% 17  20% 14% 26% 31% 9%  1415  [11:54 14/5/2009 Bioinformatics-btp249.tex]  Page: 1415  1412 1418   D.Trieschnigg et al.
relevant.
Using more common inter-annotator agreement measures, such as Cohen s Kappa is not applicable in this case, since we do not know the explicitly negative judgments of the MeSH annotators.
Despite MetaMap s relatively poor performance on reproducing manual annotations, the results show in many cases its terms are useful for representing the text (58% of its false positives are judged as  Relevant  or better).
Only few false positives (3%) are indicated as totally incorrect.
Compared with CLM and KNN, only few terms (14.7%) get labeled  Undecided .
This is because MetaMap requires an almost direct link between words in the text to classify and the MeSH terms it suggests.
As expected, quite a few terms are suggested of which only part can be related to the text to classify.
The largest part of the false positives from the CLM system are judged as  Undecided  (35.5%).
The system returns too many speciﬁc terms and some of the suggestions cannot be directly linked to the text to classify.
For KNN, most of the false positives (31%) are indicated as  irrelevant .
This value can be explained because KNN often returns general terms which are found in similar documents, but are not appropriate to this speciﬁc piece of text.
In general we notice that a fair share of the false positives is judged  relevant  or better (58% for MetaMap, 37% for CLM and 34% for KNN), indicating automatic annotations do contribute relevant terms in addition to manual annotations.
5 IMPROVING DOCUMENT RETRIEVAL USING  MeSH TERMS  Our second series of experiments investigates if any of the automatic classiﬁcation systems described above can improve IR.
5.1 Experimental setup The TREC Genomics collections from 2004 to 2007 are used for retrieval experiments (Hersh et al.
2004 and onwards).
The 2004 and 2005 tasks use a document collection of 4.5 million MEDLINE citations, consisting of a title and optionally an abstract.
The 2006 and 2007 tasks use a collection of 160 000 full-text articles from Highwire Press.
We only consider document retrieval performance for the 2006 and 2007 tasks, which are originally passage retrieval tasks: documents containing a relevant passage are considered relevant.
For the 2004 task, we use the  title  and  narrative  of the topic descriptions as queries.
In total, we have four query sets, consisting of 164 queries, on two document collections.
Table 3.
Retrieval performance on TREC Genomics collections  As explained in Section 3.5.2 for the second set of experiments, we use an automatically obtained MeSH representation of the documents.
Since classifying the whole document collection takes rather long, we only used the KNN classiﬁer on the smaller TREC 2006 collection to obtain an automatic conceptual representation.
5.2 Results Table 3 shows the retrieval performance when using the conceptual query representation obtained from the tested classiﬁers.
Baseline indicates the performance of the retrieval system only using the textual representation.
The percentages in the table indicate the differences from this baseline.
When only the MeSH representation is used (table available in the online Supplementary Material), all classiﬁcation systems perform worse than this baseline (varying from a drop of 32 96% in terms of MAP).
The KNN classiﬁer performs closest to the baseline, but performance is still poor compared with text-only retrieval (between  32% and  53% MAP).
When the textual and conceptual representations are optimally3 mixed, most of the classiﬁers do not show signiﬁcant improvements.
KNN forms the notable exception here, where improvements (up to 15% MAP) are observed for signiﬁcant all query sets.
Despite MTI s hybrid approach, it performs slightly better than its major component MetaMap but worse than KNN.
Although the MeSH thesaurus is not the most appropriate choice for improving Genomics retrieval, we do notice that in some cases searching with MeSH terms only improves searching with only text.
For some topics, the queries can be easily mapped to concepts.
For example,  What is the role of Transforming growth factor- beta1 (TGF-beta1) in cerebral amyloid angiopathy (CAA)   (topic 166), mentions concepts  Transforming Growth Factor beta  and  Cerebral Amyloid Angiopathy .
But in many cases the lack of gene and protein name coverage in MeSH hurts retrieval performance.
The query text speciﬁcally mentions a gene and the representation in MeSH concepts simply misses this key aspect of the query.
The results  show that a mixed textual and conceptual representation only improves retrieval if the classiﬁcation is of high quality.
The KNN system clearly outperformed the other systems in the text classiﬁcation evaluation.
In this retrieval setting it is the only system which shows the added value of using the conceptual  3Using the best performing α, see online Supplementary Material for values.
2004  2005  2006  2007  Method  MAP  P10  MAP  P10  MAP  P10  MAP  P10  KNN+ MTI+ CLM+ BM25+ MetaMap+ EAGL+ Baseline  0.379 0.352 0.345 0.342 0.341 0.341 0.339  b a  +12%  +4%  +2%  +1%  +1%  +1%  +11%  +3%  0.584 0.542 0.520  1% 0.512  3% 0.526 0.520  1% 0.526  0%  a  0.224 0.208 0.199 0.195 0.200 0.198 0.195  +15%  b  +7%  +2%  0%  +2%  +2%  a  +10%  +4%  0.351 0.333 0.298  6% 0.318 0.318 0.312  2% 0.318  0%  0%  a  0.405 0.381 0.364 0.363 0.364 0.365 0.363  a  +11%  +5%  0%  0%  0%  +1%  0%  +2%  0.465 0.442  3% 0.458 0.458 0.442  3% 0.477 0.458  +4%  0%  0.291 0.274 0.267 0.268 0.265 0.268 0.264  +10%  a  +4%  +1%  +1%  0%  +2%  0.469 0.475 0.461 0.467 0.450 0.453 0.450  +4%  +6%  +2%  +4%  0%  +1%   a  and  b  indicate a signiﬁcant difference from the baseline (P   0.05 or 0.005, respectively).
1416  [11:54 14/5/2009 Bioinformatics-btp249.tex]  Page: 1416  1412 1418   Table 4.
Retrieval performance on document index based on KNN  2006  2007  Method  MAP  P10  MAP  P10  KNN+ EAGL+ MetaMap+ MTI+ baseline CLM+ BM25+  +2%  0.411 +13% 0.374 +3% 0.372 0.367 0.363 0.363 0.363  +1%  0%  0%  0%  +1%  0.504 +10% a 0.280 0.273 0.462 0.268 0.458 0.454  1% 0.277 0.264 0.458 0.458 0.264 0.264  0% 0.458  +6% b 0.472 +5% 0.458 +2% +3% 0.456 +1% 0.464 +3% 0.450 0.464 +3% 0.453 +1%  +2%  +5%  0%  0%  0%   a  and  b  indicate a signiﬁcant difference from the baseline (P   0.05 or 0.005, respectively).
representation.
In only a few cases, topics show a modest drop in performance.
In these cases, important words from the query are not represented by concepts from the MeSH thesaurus, leading to query drift.
Table 4 shows the results of using automatic annotation (based on KNN) for the documents in the collection as well.
Again, the query representation based on KNN mixed with the textual representation yields optimal performance, and although not all the improvements are signiﬁcant, using the automatically assigned MeSH terms generated results similar to the ones obtained for the manual MeSH annotations.
6 DISCUSSION The MeSH classiﬁcation experiments clearly show the limitations and advantages of using different methods.
The tested thesaurus-only systems (EAGL and MetaMap) are limited in their capability to produce general MeSH terms or terms which are indirectly related.
The false positive analysis underlines that it is easy for the user to link the suggested concepts to the text through the words that they share.
Advantages of EAGL are its classiﬁcation speed and moderate index size.
Unfortunately, many general terms are missed and incorrect terms are suggested only on the basis of a partial match with the text to classify.
The concept-oriented classiﬁers (CLM and BM25) require a large amount of training data but are straightforward to train.
The BM25 method performs poorly, probably caused by ineffective parameter settings and its limitations to cope with MeSH documents of different lengths.
The CLM system performs on a par with the EAGL system and returns very speciﬁc classiﬁcations.
The false positive analysis conﬁrms that the CLM and BM25 methods return relevant classiﬁcations which can only be related indirectly to the text to classify.
Again these methods fail to produce general MeSH terms.
We expect that a better trade-off between general and speciﬁc MeSH terms can be accomplished by adding a prior to the CLM system.
The classiﬁcation system based on similar documents (KNN) shows the best trade-off between general and speciﬁc MeSH terms.
It strongly outperforms the other classiﬁers in reproducing manual annotations.
Documents related to the text to classify, yield not only relevant speciﬁc MeSH terms, but also very potentially relevant general MeSH terms.
In addition, relevant terms are returned which  MeSH Up  are not explicitly mentioned in the text.
Some of the drawbacks include its classiﬁcation speed (around a second per abstract on a desktop system) and the required index size.
Moreover, the classiﬁer will fail to return MeSH terms which are rarely used.
Finally, quite a few of the false positives are either irrelevant or incorrect, due to general MeSH terms which are appropriate for related documents, but not for a document in particular.
The false positive analysis might be biased in favor of the thesaurus-oriented classiﬁers.
For both KNN and CLM, it was more difﬁcult to judge a false positive if part of the suggested MeSH term did not occur in the text.
This would favor the thesaurus-oriented approaches, since they rely on more explicit overlap.
Moreover, we should note that our annotator did not have access to the same information as the annotators responsible for the MEDLINE annotations  the latter are provided with the full-text of the citation under annotation as well.
The second set of experiments shows a clear relationship between classiﬁcation performance and usefulness for improving IR.
Despite the fact that the MeSH thesaurus was not built speciﬁc for Genomics retrieval, it can still be incorporated to improve state-of-the-art text retrieval if the classiﬁcation performance is of acceptable level.
In our experiments this was only the case for the KNN classiﬁer, which by far outperformed the other four classiﬁers during the classiﬁcation evaluation.
representation results  Using only a conceptual  in poor performance, simply because all query aspects cannot be represented in the conceptual language.
In case a query can be accurately represented in MeSH terms, improved retrieval performance was observed compared with a text-only representation.
This corresponds to earlier results (Schuemie et al., 2007) where a Genomics-speciﬁc thesaurus was used.
A mix between text and concepts however improved retrieval even in cases where the conceptual representation of a query is not complete.
The ranking component based on MeSH terms preselects a large group of documents which are more likely to be relevant.
The ranking based on the text makes sure that the truly relevant documents are favored, resulting in a higher precision.
Surprisingly, MTI, which includes (a variant of) KNN, performed worse than our implementation of KNN on the document retrieval task.
We have three explanations for this.
First, MTI has been built to classify new citations rather than old citations, favoring recently introduced MeSH terms.
Therefore, using its classiﬁcations to ﬁnd older citations might yield poor results.
Second, MTI suggests fewer, but likely conforming better to the NLM s indexing practice, MeSH terms than KNN.
Thus, it might be more useful for suggesting index terms rather than complete search terms.
Finally, the poor classiﬁcation performance of MTI on short input, i.e.
only the title of a citation, might explain why its output on the short Genomics queries could not be used to improve IR.
The KNN classiﬁer can be viewed as a form of pseudo relevance feedback in which the top retrieved documents for a query are used for query reﬁnement.
In the language modeling framework this has been modeled as relevance models.
In this case different representations (text and MeSH) are used, this relates to cross-lingual relevance models in which query and documents are formulated in different languages (Lavrenko et al., 2002).
The difference with ordinary cross-lingual retrieval is that both representations are available and they can jointly be used to improve retrieval.
1417  [11:54 14/5/2009 Bioinformatics-btp249.tex]  Page: 1417  1412 1418   D.Trieschnigg et al.
7 CONCLUSIONS In this work, we tested several MeSH classiﬁers to do text classiﬁcation and its use to improve document retrieval.
Classiﬁers based on only information in the metathesaurus show to perform comparably with a system which models MeSH terms based on a selection of documents assigned to it.
However, a system which automatically annotates text based on the manual annotations of similar documents, strongly outperforms all other approaches.
In fact, it is the only system which is both highly scalable and capable of improving biomedical IR to the degree observed for manual MeSH annotations.
Further experiments are required to ﬁnd out whether having a complete MeSH classiﬁer can be applicable to other tasks, such as generating relevance feedback to users of retrieval systems.
We are also currently applying our approach to enable MeSH-based phenotypic categorization of micro-array experiments available in Gene Atlas (Parkinson et al., 2009).
ACKNOWLEDGEMENTS We would like to thank Stephen Robertson for his insightful comments and suggestions.
Conference on Research and Development in Information Retrieval (SIGIR  94).
Springer, New York, pp.
192 201.
Hersh,W.
et al.
(2004) TREC 2004 genomics track overview.
In Proceedings of the  Thirteenth Text Retrieval Conference (TREC 2004).
MD, USA.
Hiemstra,D.
and Kraaij,W.
(1999) Twenty-One at TREC-7: ad-hoc and cross-language track.
In Proceedings of the Seventh Text Retrieval Conference (TREC  7), pp.
227  238.
Kim,W.
et al.
(2001) Automatic MeSH term assignment and quality assessment.
In  Proceedings of AMIA Symp., pp.
319 323.
Lam,W.
and Ho,C.Y.
(1998) Using a generalized instance set for automatic text categorization.
In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR  98), ACM, New York, pp.
81 89.
Lam,W.
et al.
(1999) Automatic text categorization and its application to text retrieval.
IEEE Trans.
Knowl.
Data Eng., 11, 865 879.
Lavrenko,V.
et al.
(2002) Cross-lingual relevance models.
In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR  02).
ACM, New York, pp.
175 182.
Lewis,D.D.
(1998) Naive (Bayes) at forty: The independence assumption in information In Proceedings of Machine Learning: ECML-98, 10th European  retrieval.
Conference on Machine Learning (ECML  98), pp.
4 15.
Lin,J.
and Wilbur,W.J.
(2007) Pubmed related articles: a probabilistic topic-based model  for content similarity.
BMC Bioinformatics, 8, 423.
Lu,Y.
et al.
(2008) A probabilistic generative model for go enrichment analysis.
Nucleic  Acids Res., 36, e109.
Nenadic,G.
et al.
(2004) Mining biomedical abstracts: what is in a term  In Proceedings  of International Joint Conference on NLP, Sanya, China, pp.
247 254.
Funding: Fellowship granted by the Netherlands Genomics Initiative  the Netherlands Bioinformatics Centre (supported by a BSIK grant through the NGI)  the EC STREP project BOOTStrep (FP6-028099).
the BioRange  programme  of  Parkinson,H.
et al.
(2009) Arrayexpress update from an archive of functional genomics experiments to the atlas of gene expression.
Nucleic Acids Res., 37, D868 D872.
et al.
classiﬁcation categorization of MEDLINE articles into MeSH keywords.
IEEE Eng.
Med.
Biol.
Mag., 26, 47 55.
(2007) Multilabel  associative  Rak,R.
Conﬂict of Interest: none declared.
REFERENCES Aronson,A.R.
(2001) Effective mapping of biomedical text to the UMLS metathesaurus: the MetaMap program.
In Proceedings of AMIA Symp., Washington DC, USA, pp.
17 21.
Aronson,A.R.
et al.
(2004) The NLM Indexing Initiative s Medical Text Indexer.
In  Proceedings of MEDINFO 2004, IOS Press, San Francisco, USA, pp.
268 272.
Camous,F.
et al.
(2006) On combining MeSH and text searches to improve the retrieval of Medline documents.
In Proceedings of the Third Conference en Recherche d Informations et Applications (CORIA).
Lyon, France.
Gaudan,S.
et al.
(2008) Combining evidence, speciﬁcity, and proximity towards the normalization of gene ontology terms in text.
EURASIP J. Bioinform.
Syst.
Biol., 8, 1 9.
Hersh,W.
et al.
(1994) OHSUMED: an interactive retrieval evaluation and new large test collection for research.
In Proceedings of the 17th Annual International ACM-SIGIR  Robertson,S.E.
et al.
(1996) Okapi at TREC-4.
In Proceedings of the Fourth Text  Retrieval Conference (TREC-4 1995), MD, USA.
Ruch,P.
(2006) Automatic assignment of biomedical categories: toward a generic  approach.
Bioinformatics, 22, 658 664.
Ruiz,M.E.
and Srinivasan,P.
(2002) Hierarchical  text categorization using neural  networks.
Inf.
Retr., 5, 87 118.
Schuemie,M.
et al.
(2007) Cross language information retrieval for biomedical  literature.
In Proceddings of the Sixteenth Text Retrieval Conference (TREC 07).
Smucker,M.D.
et al.
(2007) A comparison of statistical signiﬁcance tests for information retrieval evaluation.
In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, Lisbon, Portugal (CIKM  07).
ACM, New York, pp.
623 632.
Sohn,S.
et al.
(2008) Optimal training sets for bayesian prediction of MeSH assignment.
J.
Am.
Med.
Inform.
Assoc., 15, 546 553.
Srinivasan,P.
(1996) Retrieval feedback in medline.
J.
Am.
Med.
Inform.
Assoc., 3,  157 167.
Yu,S.
et al.
(2008) Comparison of vocabularies, representations and ranking algorithms  for gene prioritization by text mining.
Bioinformatics, 24, i119 i125.
1418  [11:54 14/5/2009 Bioinformatics-btp249.tex]  Page: 1418  1412 1418
