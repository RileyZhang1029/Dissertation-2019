BIOINFORMATICS APPLICATIONS NOTE Vol.27 no.7 2011, pages 1032 1033 doi:10.1093/bioinformatics/btr042 Data and text mining GeneTUKit: a software for document-level gene normalization Minlie Huang , Jingchen Liu and Xiaoyan Zhu Advance Access publication February 8, 2011 Department of Computer Science and Technology, Tsinghua University, Beijing, China Associate Editor: Jonathan Wren document-level.
The goal of this software is not to normalize every mention correctly, but to suggest a list of normalized genes given a target document, to assist human annotators.
Most previous systems are normalizing genes at mention-level and only local context surrounding a mention (e.g. the sentence where the mention was recognized) were employed.
However, due to the high ambiguity of gene names, it may be insufﬁcient to use only local context: inter- sentential or document-level context can be helpful in this task.
Second, the software is designed for simultaneously normalizing genes of many different species for full-text articles.
It is not limited to any speciﬁc organism, but rather deals with all species present in a gene database (Entrez Gene in this article). 2 METHODS AND SYSTEM The workﬂow of our software is shown in Figure 1.
The software has four main modules.
The ﬁrst module is for gene mention recognition, the second one for gene ID candidate generation and the third one for gene ID disambiguation.
In the fourth module, the software generates conﬁdence scores for each gene ID, where the conﬁdence score indicates the strength of the association between a gene ID and the document.
We have used three methods for recognizing gene mentions in the ﬁrst module.
The ﬁrst method is a conditional random ﬁeld-based approach, which was trained on the training dataset of BioCreAtIvE II Gene Mention Recognition Task (Smith et al., 2008). The second method is a dictionary- based recognition approach where the dictionary was compiled from Entrez Gene.
The third method is ABNER (Settles, 2005), an open source named entity recognition system for biomedical literature.
The input text is processed by these methods separately, and the resulting mentions are maintained if a mention is recognized by at least two methods.
If two mentions are similar but have different boundaries, the overlapping part is taken as the ﬁnal mention.
The second module generates gene ID candidates for a recognized mention.
In this module, an open-source indexing package, Lucene (http://lucene.apache.org/), was used to index all the genes in Entrez Gene.
Each mention was then queried and top 50 gene IDs were returned as candidates.
The text of mentions and Entrez Gene entries were, respectively, ABSTRACT Motivation: Linking gene mentions in an article to entries of biological databases can facilitate indexing and querying biological literature greatly.
Due to the high ambiguity of gene names, this task is particularly challenging.
Manual annotation for this task is cost expensive, time consuming and labor intensive.
Therefore, providing assistive tools to facilitate the task is of high value.
Results: We developed GeneTUKit, a document-level gene normalization software for full-text articles.
This software employs both local context surrounding gene mentions and global context from the whole full-text document.
It can normalize genes of different species simultaneously.
When participating in BioCreAtIvE III, the system obtained good results among 37 runs: the system was ranked ﬁrst, fourth and seventh in terms of TAP-20, TAP-10 and TAP-5, respectively on the 507 full-text test articles.
Availability and implementation: The software is available at http://www.qanswers.net/GeneTUKit/. Contact: aihuang@tsinghua.edu.cn Received on October 27, 2010 revised on December 21, 2010 accepted on January 10, 2011 1 INTRODUCTION Gene normalization is one of the most challenging tasks in bio-literature mining due to the high ambiguity of gene names as they may refer to orthologous or entirely different genes, may be named after phenotypes and other biomedical terms, or may resemble common names with non-gene entities (Hakenberg et al., 2008). It is time consuming and labor intensive to annotate full-text articles manually.
Therefore, a good assistive tool for this task may facilitate the process greatly.
There has been a large body of work addressing the problem of gene mention normalization.
ProMiner (Hanisch et al., 2005), a strict dictionary-based approach, relies on the quality of its gene dictionaries heavily.
Xu et al. (2007) proposed a method using gene proﬁles generated from PubMed abstracts for gene disambiguation.
GNAT (Hakenberg et al., 2008) is a rule-based and machine learning (ML) based gene normalization system which used extensive background knowledge.
Built from open-source libraries and publicly available resources, GENO (Wermter et al., 2009) employed a carefully crafted suite of symbolic and statistical methods.
Moara (Neves et al., 2010) is a Java library for extracting and normalizing gene and protein mentions, and currently designed for four model organisms.
Our software departs from previous systems in two aspects: ﬁrst, it combines local and global contexts to normalize genes at   To whom correspondence should be addressed.
Fig.1.
The workﬂow of GeneTUKit. Numbers in shaded boxes are gene IDs.
The real-number values in the last box are conﬁdence scores.
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:09 17/3/2011 Bioinformatics-btr042.tex] Page: 1032 1032 1033  processed by the following rules sequentially: (i) removing special characters such as dashes and underscores (ii) removing stop words (iii) changing words such as hBCL into h BCL  (iv) separating digits, Greek and Roman letters from alphabetic letters and (v) converting the text to lowercase letters.
The third module is for disambiguating gene IDs, which is accomplished by a ranking algorithm.
The algorithm was trained on the 32 full-text articles provided by BioCreAtIvE III.
Each article has a list of tuples (gene mention, gene id and species) however, the annotations did not give the positions where a gene mention was recognized.
The training samples were generated as follows: for each gene ID candidate, if the ID appears in the manual annotation list, the candidate is taken as positive, otherwise negative.
For each gene ID candidate and its corresponding mention, we extract features from local and global contexts.
Some local context features are as follows:  The ranking score of the gene ID given by the Lucene index.
Whether the species of the ID is implied by the gene mention, such as hBCL. The edit distance between the mention and the ofﬁcial symbol of the ID.
The minimal edit distance between the mention and all synonyms of the ID.
Whether at least one word indicating gene functions of a gene ID appears in the sentences from which the mention was recognized.
The words indicating gene functions are obtained from the corresponding gene symbols after removing common words (such as protein, gene etc.) and words containing capital letters or digits (e.g. VDR, p65). The document-level, global context features are listed partly as follows:  Whether the species of the gene ID appears in the document.
Whether the species of the ID appears in the title.
Whether the species of the ID is the nearest species in the same paragraph where the mention is recognized.
If the mention has a full (or abbreviated) name through the document, compute the minimal edit distance between synonyms of the ID and the full (or abbreviated) name of the mention.
In constructing these features, we used dictionary-based matching to recognize species as such a simple method can produce fairly good performance.
For ﬁnding full/abbreviated name mappings, we adopted a method from (Schwartz and Hearst, 2003). Once features were obtained, we used a ranking algorithm ListNet (Cao et al., 2007) to rank gene IDs for each mention and the top one ID was maintained for further processing.
The fourth module generates a conﬁdence score for each predicted gene ID to measure the association of the given gene ID and the document using a support vector machine (SVM) classiﬁer. The training examples were constructed similarly as in the third module.
The features were constructed as follows:  The best value of features used in the third module as each ID may correspond to many mentions.
For the edit distance features, best means minimal  for the ranking score feature, best means maximal.
The total number of gene mentions associated with the ID.
The highest rank of the ID among all the mentions associating with the ID.
3 RESULTS We evaluated the system on the BioCreAtIvE III GN corpus (Lu and Wilbur, 2010) in terms of Threshold Average Precision (TAP- k, k=5,10,20, respectively) (Carroll et al., 2010). For training, we used the 32 articles with gold-standard human annotation.
For testing, the ﬁrst dataset has 50 articles, each of which has gold- standard annotation, and the second one has 507 articles whose ground truth was inferred from 37 team submissions (referred GeneTUKit Table 1.
The evaluation results on the BioCreAtIvE III GN corpus Measures 50 articles (gold standard) 507 articles (silver standard) 0.2973 (4/37) 0.3125 (4/37) 0.3248 (4/37) TAP-5 TAP-10 TAP-20 Average precision of TOP k recommendations k=5 k=10 k=20 0.4880 0.4340 0.3231 0.4086 (7/37) 0.4511 (4/37) 0.4648 (1/37) 0.5764 0.4993 0.3984 The number in the bracket is the rank of our score among the 37 submissions.
as silver standard). The 507 articles also include the 50 articles from the ﬁrst dataset. The results presented in Table 1 show the ofﬁcial evaluation results from BioCreAtIvE III.
We have also tested the performance in terms of average precision.
The manual error analysis has revealed that two major error types are (i) wrongly recognized gene mentions, and (ii) wrong species mapping.
The Supplementary Material provide a more detailed analysis at http://www.qanswers.net/GeneTUKit/evaluation.html. 4 CONCLUSION GeneTUKit is a software designed for document-level gene normalization, which employs features from the local context and the global context within the whole full-text article.
It can normalize genes of many different species.
Given a target article, the software outputs a list of normalized genes, and each predicted gene is associated with a conﬁdence score.
Funding: Natural Science Foundation of China (No. 60803075) Chinese 973 project (No. 2007CB311003). Conﬂicts of Interest: none declared.
REFERENCES Cao,Z. et al. (2007) Learning to rank: from pairwise approach to listwise approach.
In Proceedings of the 24th International Conference on Machine Learning, Corvallis, OR.
Carroll,H,D. et al. (2010) Threshold average precision (TAP-k): a measure of retrieval designed for bioinformatics. Bioinformatics, 26, 1708 1713.
Hakenberg,J. et al. (2008) Inter-species normalization of gene mentions with GNAT.
Bioinformatics, 24, i126 i132. Hanisch,D et al. (2005) ProMiner: rule-based protein and gene entity recognition.
BMC Bioinformatics, 6 (Suppl. 1), S14 Lu,Z. and Wilbur,W.J. (2010) Overview of BioCreAtIvE III gene normalization.
In BioCreAtIvE Workshop, Bethesda, MD. Neves,M,L. et al. (2010) Moara: a Java library for extracting and normalizing gene and protein mentions.
BMC Bioinformatics, 11, 157 Schwartz,A.S. and Hearst,M.A. (2003) A simple algorithm for identifying abbreviation deﬁnitions in biomedical text.
In Proceedings of the 8th Paciﬁc Symposium on Biocomputing, 3rd 7th January, World Scientiﬁc Publishing Co. Pte. Ltd, Kauai, Hawaii, pp.
451 462.
Settles,B. (2005) ABNER: an open source tool for automatically tagging genes, proteins, and other entity names in text.
Bioinformatics, 21, 3191 3192.
Smith,L. et al. (2008) Overview of BioCreAtIvE II gene mention recognition.
Genome Biol., 9 (Suppl. 2), S2. Wermter,J. et al. (2009) High-performance gene name normalization with GENO. Bioinformatics, 25, 815 821.
Xu,H. et al. (2007) Gene symbol disambiguation using knowledge-based proﬁles. Bioinformatics, 23, 1015 1022.
1033 [13:09 17/3/2011 Bioinformatics-btr042.tex] Page: 1033 1032 1033
BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.18 2010, pages 2361 2362 doi:10.1093/bioinformatics/btq426 Databases and ontologies RefProtDom: a protein database with improved domain boundaries and homology relationships Mileidy W. Gonzalez1, and William R. Pearson2, 1Department of Biological Sciences, University of Maryland Baltimore County, 1000 Hilltop Circle, Baltimore, MD 21250 and 2Department of Biochemistry and Molecular Genetics, Jordan Hall Box 800733, 1340 Jefferson Park Ave., Charlottesville, VA 22908, USA Associate Editor: Dmitrij Frishman Advance Access publication August 6, 2010 ABSTRACT Summary: RefProtDom provides a set of divergent query domains, originally selected from Pfam, and full-length proteins containing their homologous domains, with diverse architectures, for evaluating pair-wise and iterative sequence similarity searches.
Pfam homology and domain boundary annotations in the target library were supplemented using local and semi-global searches, PSI-BLAST searches, and SCOP and CATH classiﬁcations. Availability: RefProtDom is available from http://faculty.virginia.edu wrpearson/fasta/PUBS/gonzalez09a Contact: miledywgonzalez@gmail.com pearson@virginia.edu Received on January 13, 2010 revised on June 8, 2010 accepted on July 18, 2010 1 INTRODUCTION Evaluation and improvement of protein sequence similarity searches, using algorithms such as BLAST or Smith-Waterman (SSEARCH) and more sophisticated searches such as PSI-BLAST or HMMER (Altschul et al., 1997 Durbin, 1998 Smith and Waterman, 1981), require query sequences and reference sets curated to accurately reﬂect homology relationships.
Because structural similarity is preserved well beyond sequence similarity (Gibrat et al., 1996), protein structures are often the gold standard for annotating homology relationships.
Although both structure-based homology annotations and manually annotated protein sequence relationships can very accurately record homology relationships, they do not reﬂect common practice in protein similarity searching, which is to characterize unknown proteins by searching large, comprehensive protein sets such as RefSeq (Pruitt et al., 2007) and UniProt (Consortium, 2009). To better characterize similarity searching strategies, in particular, PSI-BLAST performance, against comprehensive protein databases, we identiﬁed a set of diverse protein domains from Pfam (Finn et al., 2010) v. 21 to use as queries against a set of real proteins containing those domains.
Our query domain families are taxonomically broad (to provide harder homology detection cases), and have long models (to better simulate full-length protein searches). Although we cannot be certain that all homologs have been found, we believe that statistically signiﬁcant pair-wise alignments are annotated correctly.
To whom correspondence should be addressed.
2 DATABASE ASSEMBLY Evaluation datasets: from 681 initial Pfam (v. 21) families that met criteria for: (i) domain length ( 200 residues) (ii) taxonomic diversity (present in two of bacteria, archaea and eukarya) (iii) family size ( 100 instances) and (iv) available structure, we selected 344 query Pfam families after merging families that belonged to the same clan (Gonzalez and Pearson, 2010). In this initial set, 81 families belonged to distinct clans, while 263 families did not have an associated clan.
This set was reduced to 320 non-homologous domains using information from Pfam (v. 23) (by Pfam v. 24, these domains belonged to 112 distinct clans with 168 families not in clans, for 280 non- homologous domains). The target library was built from 234 505 full-length UniProt proteins (excluding viral sequences) containing Pfam v. 21 homologs to the original 320 Pfam families together with 1627 other domain families.
Two query sets were constructed and the members of these sets evaluated further: (i) a challenging query subset (50 hard) with the lowest family coverage with BLAST and (ii) a randomly sampled representative query set (50 sampled with replacement). Annotation extensions: when the original Pfam v. 21 annotations were used to characterize searches with our hard and sampled queries against the target library, thousands of alignments to very similar UniProt sequences 80, with 95% identity) were annotated as partial homologs (e.g. E() 10 or non-homologs. To correct these conservative annotations, we compared the bare domain query sequences to the target library using SSEARCH and GLSEARCH (a program that produces an alignment that is global in the query sequence but possibly local in the target or library sequence). We identiﬁed all the signiﬁcantly similar sequence regions (E() 0.001) with SSEARCH that were either shorter or unannotated in Pfam v. 21 and calculated the boundaries using GLSEARCH. We extended annotations on 2106 partial domains and added 24 604 domain homology annotations based on SSEARCH alignments, 13 574 of which were included in Pfam v. 24.
RefProtDom describes relationships and alignment boundaries between query domains and the target library homologs according to Pfam v. 21, Pfam v. 24 and the SSEARCH/GLSEARCH alignment boundaries.
40) scores.
We analyzed all signiﬁcant Although SSEARCH/GLSEARCH searches against the target library dramatically reduced the number of apparent false positives with very low E()-values, additional searches with PSI-BLAST using the queries sometimes found unrelated UniProt sequences with signiﬁcant (E() 10 non- homologous alignments found in the ﬁrst three iterations of PSI-BLAST for the 100 queries (94 distinct families). Non-homologous alignments to regions with no annotated Pfam domains were used as queries in reciprocal PSI-BLAST searches for three iterations.
Reciprocal searches that recovered at least 25% of a domain family were annotated as homologous, yielding 375 additional homology annotations across 33/94 families.
Structures of signiﬁcant non-homologs that mapped to unrelated Pfam families were examined in SCOP and CATH if they shared the same SCOP fold or CATH topology they were annotated as homologs. For example, Pfam annotates a (E() 10  4)  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:55 11/8/2010 Bioinformatics-btq426.tex] Page: 2361 2361 2362  M.W.Gonzalez and W.R.Pearson PF00346 domain for residues 295 537 on the Q8ZMJ0_SALTY sequence.
RefProtDom also annotates PF00374 in the same region (390 535) because both domains share the same SCOP fold and superfamily classiﬁcations (e.18.1). This structural annotation is also reported by SCOOP (Bateman and Finn, 2007). We found structural evidence to add 37 additional clans Pfam v. 24 matches 14 of those 37 structural clans.
These additional clans would reduce the number of non-homologous domains in our query set from 94 to 90, but the families were not combined, the cryptic homology was simply annotated.
Structure classiﬁcations yielded 2124 additional homology annotations across 16/94 queries.
3 SUMMARY Iterative similarity searches are usually performed against full- length proteins with complex domain architectures.
Evaluating similarity-searching methods against benchmarking sets with incomplete or missing annotations can introduce dramatic statistical inaccuracies.
RefProtDom s greatest strength is its use of a taxonomically diverse set of full-length, multi-domain, proteins in the target library.
Searches against RefProtDom resemble searches against SwissProt, UniProt or RefSeq (though those databases are much larger). Moreover, the query sequences are evolutionarily independent based on structural comparisons, 90 query domains are non-homologous.
Thus, RefProtDom can simulate searches against comprehensive sequence databases while evaluating success on challenging homologies. resource for Pfam is a powerful identifying homology relationships and domain boundaries, but strategies that use a single hidden Markov model (HMM) to identify every homolog will be challenged by distant sequences at the detection horizon for the model.
For many families, Pfam has addressed this problem by grouping families into clans.
But, sometimes homologs are missed sequences that share strong similarity across the length of a domain to an annotated homolog are surely homologous, even if they do not produce a signiﬁcant score against the HMM model.
The RefProtDom query and target libraries seek to reduce the number of un-annotated homologies with statistically signiﬁcant similarities, and to more accurately estimate homologous domain boundaries.
Although our curation may have missed some homologs, we are conﬁdent in the homologies we annotate.
Homology is annotated for domains that share signiﬁcant pair-wise similarity, show signiﬁcant family coverage after three PSI-BLAST iterations, or when they share structures.
Domain boundaries were revised based on signiﬁcant local or global similarity.
By combining single domain queries with full-length, multi-domain proteins, RefProtDom can highlight alignment errors and evaluate improvements in alignment accuracy.
Accurate boundary annotation has been largely overlooked in pair- wise sequence comparison, because incorrect alignment boundaries rarely detract from the identiﬁcation of homologous proteins.
Pfam s annotations are now generated with HMMER3, which only performs local alignments (Finn et al., 2010). Therefore, one might expect that future Pfam annotations may have an even harder time at identifying complete domains, and thus, should continue to beneﬁt from the extension curation provided by RefProtDom. Nonetheless, HMMER3 compensates with increased sensitivity as a result of better statistics.
In fact, 59% of the domain extensions and 55% of 2362 the missed homologs added to Pfam v. 21 using our protocol were incorporated in Pfam v. 24.
Thus, Pfam v. 24, using HMMER3, has independently addressed many of the missing annotations, validating our approach.
However, we believe that the problems inherent in using a single model for diverse protein family searches will always miss homologs and domain boundaries that can be found with individual domains across the family s phylogenetic tree.
We plan to continue to update the homology relationships and boundary assignments in RefProtDom. For iterative sequence comparison methods, alignment accuracy inaccurate alignments can cause non-homologous is crucial domains to be included in the proﬁles and decrease their speciﬁcity in subsequent iterations.
Using RefProtDom s annotations, we identiﬁed a previously unrecognized alignment overextension error in PSI-BLAST responsible for the corruption of its PSSMs and its poor speciﬁcity (Gonzalez and Pearson, 2010). Additional evaluations with RefProtDom revealed that while JACKHMMER (HMMER3 s susceptible to the same error, it overextends more slowly and, thus, shows better performance than unmodiﬁed PSI-BLAST (M.W.G. and W.R.P., manuscript in preparation). iterative implementation) is Domains are the basic units of protein function and evolution thus, improved homology detection requires improved domain alignment accuracy.
Large-scale automatic annotation of gene function is limited by local alignments incomplete motif matches and fuzzy domain boundaries (Kann et al., 2007). Establishing homology is central to a wide array of bioinformatics methodologies improved domain alignments can improve 3D protein structural predictions that use homology modeling, and also clarify how protein domain networks interact to generate disease phenotypes. RefProtDom provides a comprehensive set of full-length UniProt proteins that can be used to evaluate domain alignment accuracy.
Funding: National Library of Medicine, grant LM04969. Conﬂict of Interest: none declared.
REFERENCES Altschul,S.F. et al. (1997) Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.
Nucleic Acids Res., 25, 3389 3402.
Bateman,A. and Finn,R.D. (2007) SCOOP: a simple method for identiﬁcation of novel protein superfamily relationships.
Bioinformatics, 23, 809 814.
Durbin,R. et al. (1998) Biological sequence analysis: probabilistic models of proteins and nucleic acids.
Cambridge University Press, UK. Finn,R.D. et al. (2010) The Pfam protein families database.
Nucleic Acids Res., 38, D211 D222. Gibrat,J.F. et al. (1996) Surprising similarities in structure comparison.
Curr. Opin. Struct. Biol., 6, 377 385.
Gonzalez,M.W. and Pearson,W.R. (2010) Homologous Over-extension: a challenge for iterative similarity searches.
Nucleic Acids Res., 38, 2177 2189.
Kann,M.G. et al. (2007) The identiﬁcation of complete domains within protein sequences using accurate E-values for semi-global alignment.
Nucleic Acids Res., 35, 4678 4685.
Pruitt,K.D. et al. (2007) NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins.
Nucleic Acids Res., 35, D61 D65. Smith,T.F. and Waterman,M.S. (1981) Identiﬁcation of common molecular subsequences. J. Mol. Biol., 147, 195 197.
UniPort Consortium (2009) The Universal Protein Resource (UniProt) in 2010.
Nucleic Acids Res., 38, D142 D148. [11:55 11/8/2010 Bioinformatics-btq426.tex] Page: 2362 2361 2362
BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.19 2010, pages 2477 2479 doi:10.1093/bioinformatics/btq440 Systems biology PSExplorer: whole parameter space exploration for molecular signaling pathway dynamics Thai Quang Tung and Doheon Lee 1Department of Bio and Brain Engineering, KAIST 373 1 Guseong-dong, Yuseong-gu, Daejeon 305 701, Republic of Korea Associate Editor: Trey Ideker Advance Access publication August 2, 2010 ABSTRACT Motivation: Mathematical models of biological systems often have a large number of parameters whose combinational variations can yield distinct qualitative behaviors.
Since it is intractable to examine all possible combinations of parameters for non-trivial biological pathways, it is required to have a systematic strategy to explore the parameter space in a computational way so that dynamic behaviors of a given pathway are estimated.
Results: We present PSExplorer, a computational tool for exploring qualitative behaviors and key parameters of molecular signaling pathways.
Utilizing the Latin hypercube sampling and a clustering technique in a recursive paradigm, the software enables users to explore the whole parameter space of the models to search for robust qualitative behaviors.
The parameter space is partitioned into sub-regions according to behavioral differences.
Sub-regions showing robust behaviors can be identiﬁed for further analyses.
The partitioning result presents a tree structure from which individual and combinational effects of parameters on model behaviors can be assessed and key factors of the models are readily identiﬁed. Availability: The software, are download http://gto.kaist.ac.kr psexplorer Contact: tqtung@kaist.ac.kr tqtung@gmail.com tutorial manual and test models address: following available the for at Received on April 26, 2010 revised on July 4, 2010 accepted on July 25, 2010 1 INTRODUCTION to understand the Computational modeling is a valuable tool complex biological systems.
Such models often possess complex topological structures with multiple positive and/or negative feedbacks and a large number of parameters so that combinational perturbations on these parameter values can greatly affect qualitative behaviors of the models.
Two important issues in this approach are to understand the scope of behaviors that the model can experience, and to identify key parameters regulating the behaviors.
Bifurcation analysis (Borisuk and Tyson, 1998) is one of well- developed methods for qualitative behavior analysis of dynamical models.
Its application to systems biology is limited, however, since it can examine the effects of only a few parameters, mostly one or two at a time.
Although a tool for the global parametric perturbation analysis of biochemical models is available (Zi et al.. 2008), it is focusing on parameter sensitivity analysis, i.e. a quantiﬁcation of   To whom correspondence should be addressed.
individual effects of parameter variations respecting to quantitative changes in the model outputs.
In this note, we outline PSExplorer, an efﬁcient computational tool to explore high-dimensional parameter space of biochemical models for identifying qualitative behaviors and key parameters.
The software supports input models in SBML format.
It provides a friendly graphical user interface allowing users to vary model parameters and perform time-course simulations at ease.
Various graphical plotting features helps users analyze the model dynamics conveniently.
Its output is a tree structure that encapsulates the parameter space partitioning results in a form that is easy to visualize and provide users with additional information about important parameters and sub-regions with robust behaviors.
2 METHODS An ordinary differential equation (ODE) model of a biochemical network is often represented in the following form: = f (X,θ)  X(t=0)=X0 dX dt (1) where X is a vector of state variables and X0 a vector of their corresponding initial values θ is a vector of kinetic parameters.
To begin with, users are required to specify a set of k parameters P1,P2,...,Pk (Pi can be a kinetic parameter or an initial value, i=1,2,...,k) and their admissible value ranges to be perturbed a set of target variables to represent the model behaviors for convenience, let us say that there is only one target variable Y.
The general framework of PSExplorer has six steps that are performed in a recursive manner.
To help readers grasp main concepts, an illustrative example is provided in Figure 1 complementing to following brief explanations of the steps: (1) Latin hypercube sampling technique (Helton et al., 2005) is used to generate N points in the parameter space (Fig. 1A) N is initially speciﬁed by users.
Each sampling point represents a parameter combination p(i)=[pi1,pi2,...,pik] whose element pij is a randomly generated value of the parameter Pj within its admissible range.
(2) For each p(i), the software solves the set of ODEs in (1) to estimate temporal proﬁle y(i)of the target variable, (Fig. 1B). (3) The simulated temporal proﬁles are grouped into c clusters (Fig. 1C) using a feature-based k-mean clustering algorithm.
Each cluster is regarded as a qualitative behavior of the model.
The software supports a set of features from which users need to select a subset that ﬁts to their analysis purpose.
The number of clusters is speciﬁed by users.
(4) This step is to estimate a vector b(i)=[bi1,bi2,...,bic] for each y(i) where bij(j=1,2,...,c) is the similarity measure between y(i) and j-th cluster center.
For visualizing, we assign each p(i) to a cluster j if bij is the maximum value of b(i), then points in each cluster is plotted with a distinct color in the parameter space as shown in Figure 1D. The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:53 31/8/2010 Bioinformatics-btq440.tex] Page: 2477 2477 2479  T.Q.Tung and D.Lee Fig.1.
An example illustrating a 2-parameter perturbation analysis of a pathway model.
(A) Parameter space is ﬁlled with sampling points.
(B) Plotting of simulation results.
(C) Clustering shows three distinctive clusters representing model qualitative behaviors.
(D) Sampling points is colored respecting to clustering results.
(E) The parameter space is nicely partitioned into sub-regions with different behaviors.
(F) A tree represents the parameter space partitioning.
(G) Parameter ranking estimated from the tree.
(5) Let Bj(j=1,2,...,c) be a variable representing similarity measure between the j-th cluster center and simulated temporal proﬁles. Considering a regression problem where P1,P2,...,Pk are predictors and B1,B2,...,Bc are responses, a multi-response regression (Segal, is constructed using N learning cases p(i), b(i) (i= 1992) 1,2,...,N). Loosely speaking, the goal of this step is to partition the parameter space into sub-regions containing homogenous points (as shown in Fig.1E). The partitioning is represented in a binary tree (Fig. 1F) whose terminal nodes correspond to the sub-regions.
(6) Heterogeneous sub-regions are identiﬁed and additionally sampled with N0 (speciﬁed by users) points.
These sampling points are proportionally distributed to the sub-regions respecting to their heterogeneities. The Steps 2 6 are then repeated iteratively until the total number of generated parameter combinations exceeding a maximum value.
Note that in Step 2, simulations are performed only on a newly generated data and results are combined with the data of previous iterations.
N is then replaced by N+N0 afterward.
Finally, PSExplorer presents the result tree from Step 5 of the ﬁnal iteration in a graphical format where users can intuitively pinpoint regions of robust behaviors for further analyses.
From the tree structure, the software evaluates parameters in terms of their importance and key parameters can be readily to be identiﬁed (Fig. 1G). 3 EXAMPLE An application example of how to use PSExplorer to study system dynamics of computational models is provided in the user s manual of the software.
The Wnt ERK crosstalk model (Kim et al., 2007) is selected as an example model.
Fifty-ﬁve kinetic parameters of the 1 to 101 times of their reference values.
model were varied from 10 PSExplorer has identiﬁed ﬁve distinct qualitative behaviors.
Three among them were mentioned in a previous study explaining co- operatively working modes of the two pathways.
In contrast, the two remaining ones show that ERK and Wnt can behave independently regardless the existence of a hidden positive feedback between them.
Literature information has been found to support the existence of 2478 such behaviors.
We refer readers to the user s manual for a detail description of the example.
4 DISCUSSION It is imperative to employ an in silico analysis to systematically identify the speciﬁc perturbations that have signiﬁcant effects on the system behaviors, especially when numerous experiments on living systems are not practical.
Globally perturbing model parameters to analyze system dynamics was used in previous studies (Hua et al., 2006 Sung et al., 2009). For automating the approach, PSExplorer has been developed from the framework proposed by Hua et al. (2006) with necessary reﬁnements to boost computational performance.
Instead of using an exhaustive sampling approach, PSExplorer utilizes a novel recursive stratiﬁed sampling approach that can help explore high-dimensional spaces efﬁciently. On each recursion step, the Latin hypercube sampling requires less samples to be generated, thanks to its balancing distribution property.
Then, the coupling of a clustering algorithm with a regression tree model provides a mean for a parameter space stratiﬁcation, which allows the sampling process to concentrate on highly heterogeneous regions while ignoring homogeneous ones.
As a result, the number of samples required to properly explore the parameter spaces can signiﬁcantly reduce.
Last but not the least, one unique feature of the software that makes it more interesting than the sensitivity analysis technique is the ability to identify combinatory effects of parameter variations on model behaviors.
The result tree not only identiﬁes sensitive parameters but it also can tell how they combine to produce a speciﬁc behavior of interest.
Funding: WCU (World Class University) program (R32-2008- 000-10218-0) and Korean Systems Biology Research Project (20100002164) of the Ministry of Education, Science and [17:53 31/8/2010 Bioinformatics-btq440.tex] Page: 2478 2477 2479  Technology (MEST) through the National Research Foundation of Korea. Conﬂict of Interest: none declared.
REFERENCES Borisuk,M.T. and Tyson,J.J. (1998) Bifurcation analysis of a model of mitotic control in frog eggs.
J. Theor. Biol., 195, 69 85.
Helton,J.C. et al. (2005) A comparison of uncertainty and sensitivity analysis results obtained with random and Latin hypercube sampling.
Reliab. Eng. Syst Saf., 89, 305 330.
PSExplorer Hua,F. et al. (2006) Integrated mechanistic and data-driven modelling for multivariate analysis of signalling pathways.
J. R. Soc.
Interface, 3, 515 526.
Kim,D. et al. (2007) A hidden oncogenic positive feedback loop caused by crosstalk between Wnt and ERK pathways.
Oncogene, 26, 4571 4579.
Segal,M.R. (1992) Tree-structured methods for longitudinal data.
J.Am.Stat.Assoc., 87, 407 418.
Sung,M.H. et al. (2009) Sustained oscillations of NF-kappaB produce distinct genome scanning and gene expression proﬁles. PLoS One, 4, e7163. Zi,Z. et al. (2008) SBML-SAT: a systems biology markup language (SBML) based sensitivity analysis tool.
BMC Bioinformatics, 9, 342.
[17:53 31/8/2010 Bioinformatics-btq440.tex] Page: 2479 2477 2479 2479
BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btn033 Vol.25 no.7 2009, pages 974 976 Data and text mining MiSearch adaptive pubMed search tool David J. States1,2,*, Alex S. Ade1, Zachary C. Wright1, Aaron V. Bookvich1 and Brian D. Athey1,3 1National Center for Integrative Biomedical Informatics, 2Department of Human Genetics and 3Department of Psychiatry, University of Michigan, Ann Arbor, MI 48109, USA Received on September 7, 2007 revised and accepted on January 22, 2008 Advance Access publication March 6, 2008 Associate Editor: Alfonso Valencia ABSTRACT Summary: MiSearch is an adaptive biomedical literature search tool that ranks citations based on a statistical model for the likelihood that a user will choose to view them.
Citation selections are automatically acquired during browsing and used to dynamically update a likelihood model that includes authorship, journal and PubMed indexing information.
The user can optionally elect to include or exclude specific features and vary the importance of timeliness in the ranking.
Availability: http://misearch.ncibi.org Contact: dstates@umich.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION With the rapidly increasing volume of publications in the biomedical literature, finding relevant work is an ever more difficult challenge.
General solutions to the literature search problem are difficult because biomedical science is very diverse the articles most relevant to one reader may not be relevant to another.
Relevance feedback is a well-established technique to improve performance in information retrieval (Rocchio, 1971 Salton, 1971 Salton and Buckley, 1990). Feedback may be acquired explicitly by asking users to rate retrieval results.
However, many users find this task burdensome.
Even for widely deployed search engines such as Excite, where relevance feedback is available and effective, it is rarely used (Spink et al., 2000). An alternative is to acquire feedback implicitly by observing user behavior (Kelly and Teevan, 2003). MiSearch is an adaptive literature search tool using implicit relevance feedback that helps users rapidly find PubMed citations relevant to their specific interests.
MiSearch auto- matically saves information on citations a reader has viewed during search and browsing, and uses this information to build a statistical profile describing the readers choices.
This profile is used to rank the results of future searches, placing those articles that this reader is most likely to view at the top of the list.
In effect, MiSearch is using query expansion with probabilistic weighting of terms derived from the implicitly *To whom correspondence should be addressed.
defined relevant document set.
Using this implicit feedback approach is effective and improves the relevance ranking of bibliographic search results.
The NCBI Entrez search tool is widely used and alternative interfaces have been developed allowing users to manually vary the weight of different features in determining relevance (Muin et al., 2005) and to reformulate and refine Boolean queries (Bernstam, 2001 Ding et al., 2006), but unlike MiSearch, these tools do not adapt to user behavior.
2 METHODS 2.1 Ranking algorithm MiSearch records the users search history and the history of documents selected for viewing using an HTTP redirect mechanism.
Four domains are considered: authors (Au), journal (Jl), MeSH terms (Me) and substance names (Sn) indexed by NLM (Nelson et al., 2004). Each domain is described using a statistical profile of term use.
The frequency fu(t) of term t occurring in citations that user, u, has selected for viewing is defined as ð fu tð Þ ¼ Nu tð Þ þ fP tð Þ Nu þ 1 Þ where Nu(t) is the count of citations indexed with term t that were viewed by the user,Nu is the total number of citations viewed by the user and fP(t) is the absolute frequency with which papers indexed with term t occur in the entire PubMed database.
The pseudo counts smooth behavior when the profile has few citations and avoid division by zero if a specific term does not occur in the citations selected by a user.
If no feedback is available for the user (Nu¼ 0), then fu(t) is fP(t). When the user has viewed many articles, fu(t) asymptotically approaches Nu(t/Nu). MiSearch uses the PubMed eUtils interface to query the PubMed database and ranks citations based on a log likelihood score, S, X S ¼ SD þ  T  T0 ð Þ D¼Au, Jl, Me, Sn where SD are log likelihood scores for each domain and (T T0) is term weighting the timeliness of an article.
T is the date of publication for a citation, T0 is a reference date (January 1, 2000) and  is an adjustable factor that allows the user to vary the weight given to timeliness in ranking citations.
The score SD for domain D is calculated for each citation as a log likelihood ratio that the term t associated with this citation occur in citations viewed by the user, fu, relative to their frequency in all of PubMed, fP    X SD ¼ log t2Term fu tð Þ fP tð Þ ß 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
MiSearch adaptive pubMed search tool A positive score indicates that a citation is more likely to be viewed by the user, and a negative score indicates that a citation is less likely to be selected for viewing.
Implementation 2.2 MiSearch is implemented in two components, a PHP script running on an Apache web server that generates forms, dispatches search requests to NCBI Entrez and communicates with a local citation datbase, and a relational database server that stores both the PubMed corpus and user search histories.
Ranking is implemented as an SQL stored procedure.
Users can label profiles with a string and can define several different profiles for different search tasks.
3 RESULTS Figure 1 illustrates the effect of adaptive re-ranking of citation searches based on a profile created from the publication of one author (AWL). Dr Lee s research focuses on signal transduc- tion downstream of the CSF-1 receptor (CSF1R). Using a profile based on viewing Dr Lee s publications (top panel), MiSearch ranks two of Dr Lee s publications and a third recent and highly relevant publication at the highest relevance in a PubMed search for CSF1R. In contrast, without adaptive ranking (lower panel), the publications are ranked in reverse chronological order with only one moderately relevant pub- lication highly ranked and that citation is in a journal that Dr Lee does not frequently read.
Because MiSearch ranks citations using a statistical profile, the user does not need to explicitly specify the ranking criteria.
MiSearch thus complements Boolean search strategies.
In Boolean searches, a relevant article may be missed if the user specifies an overly restrictive Boolean filter and the citation uses a synonym for a term not specified in the search query.
Using MiSearch, a broader Boolean query can be performed.
The MiSearch relevance ranking places the citations most likely to be of interest to this user at the top of the list and avoids the need to view large numbers of citations.
Further, a reader may not be aware that all the citations they are viewing contain a common term such as reference to a chemical substance.
The MiSearch statistical profile will automatically capture this information and rank other citations, mentioning this term more highly.
Optionally, the user can request that MiSearch use the results of the query itself to construct the profile.
This results in a ranking where the citations sharing features with the largest number of other citations in the result set are ranked highly.
In this view, citations that are most central to the topic rank highly while citations peripheral to the topic rank lower on the list.
For example, in a search about a gene, citations where the gene is the major focus of the paper will be at the top of the query profile ranking while citations that only mention the gene in passing will rank lower on the list.
Query profile mode is invoked by using query or username query as the username.
3.1 Evaluation To assess the effectiveness of implicit relevance feedback, we use a cross validation approach.
A training profile is constructed by sampling from the citations selected as relevant for viewing by a user.
The test set consists of the remaining citations selected by this user.
Typical results are shown in Figure 2.
Increasing the number of citations in the training set k n a r 0 1 0 2 0 3 0 4 0 5 0 6 Fig.1.
Compares the results of a relevance ranked citation search (top) with the same search ranked in reverse chronological order (bottom). The top three articles in each ranking are shown.
1 2 3 training examples Fig.2.
Shown in the figure are the ranking for a representative search.
The query Xist Tsix that returns 66 articles in PubMed. The user selected four articles from this list related to epigenetic regulation of X inactivation.
Leave one out cross validation of the relevance ranks were computed for training samples containing 1, 2 or 3 of the citations in the user s profile.
The circles on the left show where each article appeared in the PubMed/Entrez ranking.
The þ on the right show the ranking of each article based on the MiSearch algorithm.
975  Response time is an issue, particularly with very large result sets.
The major performance bottleneck is that the system needs to calculate usage frequencies for every term appearing in every document in the result set.
This is done on the fly so that rankings reflect the user s most recent search and retrieval behavior, but the reference term frequencies are pre-computed for all of PubMed. This is a compromise.
For the task of ranking documents a user is likely to select, the reference corpus would ideally be the collection of documents that the users decided not to view among the citations that their queries had retrieve from Entrez. Implementing this would, however, be computationally intensive.
ACKNOWLEDGEMENTS We thank Dr Angel W. Lee for her advice and comments during the course of preparing this manuscript.
This work was supported in part by grants R01 LM008106 and U54 DA021519 from NIH. Conflict of Interest: none declared.
REFERENCES Bernstam,E. (2001) MedlineQBE (Query-by-Example). Proc. AMIA Symp., 47 51.
Ding,J. et al. (2006) PubMed Assistant: a biologist-friendly interface for enhanced PubMed search.
Bioinformatics, 22, 378 380.
Kelly,D. and Teevan,J. (2003) Implicit feedback for inferring user preference: a bibliography.
ACM SIGIR Forum, 37, 18 28.
Muin,M. et al. (2005) SLIM: an alternative Web interface for MEDLINE PubMed searches  a preliminary study.
BMC Med.
Inform.
Decision Making, 5, 37.
Nelson,S.J. et al. (2004) The MeSH translation maintenance system: structure, interface design, and implementation.
Medinfo, 11, 67 69.
Rocchio,J.J.J. (1971) Relevance feedback in information retrieval.
In The Smart System-experiments in Automatic Document Processing.
Prentice Hall Inc., Englewood Cliffs, NJ, pp.
313 323.
Salton,G. (1971) Relevance feedback and the optimization of retrieval effective- ness. In The Smart System-experiments in Automatic Document Processing.
Prentice-Hall Inc., Englewocd Cliffs, NJ, pp.
324 336.
Salton,G. and Buckley,C. (1990) Improving retrieval performance by relevance feedback.
J.Am.Soc.
Inform.
Sci.Technol., 41, 288 97.
Spink,A. et al. (2000) Use of query reformulation and relevance feedback by excite users.
Internet Res.
Electr. Networking Appl. Policy, 10, 317 328.
D.J.States et al. progressively improves the ranking of the test citations.
In Supplementary Data, we compare the performance of MiSearch to relevance feedback using the Entrez related articles /feature. The improved results in cross validation demonstrate that users are consistent in the articles that they select for viewing and that these selections are an effective implicit relevance feedback yielding improved biomedical literature search performance.
source of 4 DISCUSSION Automated collection of implicit relevant feedback information gathered using a click-through mechanism improve biblio- graphic search performance.
Users find this interface intuitive and easy to use.
Relevance feedback is applied by simply rerunning a query periodically during normal browsing.
We find that response time is a critical factor in user acceptance of a relevance feedback system.
While more sophisticated algorithms for classification and ranking based on relevance feedback have been proposed, the likelihood ratios used in MiSearch are effective and easily implemented within an RDBMS. This avoids the need to move large data sets in and out of the database server and improves user response time.
We encountered a number of in implementing MiSearch. Optimizing the performance of the relevance feed- back system to work with small numbers of events is important.
In a typical biomedical literature search task, users often view fewer than a dozen articles.
issues Many author names are not unique.
In the MiSearch formulation, such author names are not resolved, but are expected to occur with higher frequency in the reference corpus and thus provide less information in ranking articles.
Documents vary greatly in the number of authors, MeSH terms and substance names applied to them.
It is thus necessary to rank articles based on variable number of terms in these domains.
We attempt to avoid bias in the formulation of the scores and by using pseudo counts where zero term counts give zero scores.
The MiSearch ranking is necessarily dependent on the NLM indexing processing.
We are developing ways to base retrieval on automatically scored name, substance and MeSH headings, so that we can process documents such as web pages or journal articles that are not indexed by NLM. 976
BIOINFORMATICS Vol.26 ECCB 2010, pages i568 i574 doi:10.1093/bioinformatics/btq383 Utopia documents: linking scholarly literature with research data T. K. Attwood1,2, , D. B. Kell3,4, P. McDermott1,2, J. Marsh3, S. R. Pettifer2 and D. Thorne3 1School of Computer Science, 2Faculty of Life Sciences, 3School of Chemistry, University of Manchester, Oxford Road, Manchester M13 9PL and 4Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK In recent years, ABSTRACT Motivation: the gulf between the mass of accumulating-research data and the massive literature describing and analyzing those data has widened.
The need for intelligent tools to bridge this gap, to rescue the knowledge being systematically isolated in literature and data silos, is now widely acknowledged.
Results: To this end, we have developed Utopia Documents, a novel PDF reader that semantically integrates visualization and data- analysis tools with published research articles.
In a successful pilot with editors of the system has been used to transform static document features into objects that can be linked, annotated, visualized and analyzed interactively (http://www.biochemj.org/bj/424/3/). Utopia Documents is now used routinely by BJ editors to mark up article content prior to publication.
Recent additions include integration of various text- mining and biodatabase plugins, demonstrating the system s ability to seamlessly integrate on-line content with PDF articles.
Availability: http://getutopia.com Contact: teresa.k.attwood@manchester.ac.uk the Biochemical Journal (BJ), 1 INTRODUCTION The typhoon of technological advances witnessed during the last decade has left in its wake a ﬂood of life-science data, and an increasingly impenetrable mass of biomedical literature describing and analysing those data.
Importantly, the modern frenzy to gather more and more information has left us without adequate tools either to mine the rapidly increasing data- and literature-collections efﬁciently, or to extract useful knowledge from them.
To be usable, information needs to be stored and organized in ways that allow us to access, analyze and annotate it, and ultimately to relate it to other information.
Unfortunately, however, much of the data accumulating in databases and documents has not been stored and organized in rigorous, principled ways.
Consequently, ﬁnding what we want and, crucially, pinpointing and understanding what we already know, have become increasingly difﬁcult and costly tasks (Attwood et al., 2009). A group of scientists for whom these problems have become especially troublesome are biocurators, who must routinely inspect thousands of articles and hundreds of related entries in different databases in order to be able to attach sufﬁcient information to a new database entry to make it meaningful.
With something like 25 000 peer-reviewed journals publishing around 2.5 million articles per year, it is simply not possible for curators to keep abreast of developments, to ﬁnd all the relevant papers they need, to locate the most relevant facts within them, and simultaneously to keep  To whom correspondence should be addressed.
pace with the inexorable data deluge from ongoing high-throughput biology projects (i.e. from whole genome sequencing). For example, to put this in context, Bairoch estimates that it has taken 23 years to manually annotate about half of Swiss-Prot s 516 081 entries (Bairoch, 2009 Boeckmann et al., 2003), a painfully small number relative to the size of its parent resource, UniProtKB (The UniProt Consortium, 2009), which currently contains 11 million entries.
Hardly surprising, then, that he should opine, It is quite depressive to think that we are spending millions in grants for people to perform experiments, produce new knowledge, hide this knowledge in a often badly written text and then spend some more millions trying to second guess what the authors really did and found (Bairoch, 2009). The work of curators, and indeed of all researchers, would be far easier if articles could provide seamless access to their underlying research data.
It has been argued that the distinction between an online paper and a database is already diminishing (Bourne, 2005) however, as is evident from the success stories of recent initiatives to access and extract the knowledge embedded in the scholarly literature, there is still work to be done.
Some of these initiatives are outlined below.
The Royal Society of Chemistry (RSC) took pioneering steps towards enriching their published content with data from external resources, creating computer-readable chemistry with their Prospect software (Editorial, 2007). They now offer some of their journal articles in an enhanced HTML form, annotated using Prospect: features that may be marked up include compound names, bio- and chemical-ontology terms, etc.
Marked-up terms provide deﬁnitions from the various ontologies used by the system, together with InChI (IUPAC International Chemical Identiﬁer) codes, lists of other RSC articles that reference these terms, synonym lists, links to structural formulae, patent information and so on.
Articles enriched in this way make navigation to additional information trivial, and signiﬁcantly increase the appeal to readers.
In a related project, the ChemSpider Journal of Chemistry exploits the ChemMantis System to mark up its articles (http://www.chemmantis.com). With the ChemSpider database at its heart, ChemMantis identiﬁes and extracts chemical names, converting them to chemical structures using name-to-structure conversion algorithms and dictionary look-ups it also marks up chemical families, groups and reaction types, and provides links to Wikipedia deﬁnitions where appropriate.
In an initiative more closely related to the life sciences, FEBS Letters ran a pilot study (Ceol et al., 2008) with the curators of the MINT interaction database (Chatr-aryamontri et al., 2007), focusing on integration of published protein protein interaction and post- translational modiﬁcation data with information stored in MINT and UniProtKB. Key to the experiment was the Structured Digital  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:00 28/8/2010 Bioinformatics-btq383.tex] Page: i568 i568 i574  Structured summary: MINT-6173230, MINT-6173253: TSC22 (uniprotkb:Q15714) physically interacts (MI:0218) with fortilin (uniprotkb:P13693) by co-immunoprecipitation (MI:0019) MINT-6173217: TSC22 (uniprotkb:Q15714) binds (MI:0407) fortilin (uni- protkb:P13693) by pull-down (MI:0096) MINT-6173240, MINT-6173270: TSC22 (uniprotkb:Q15714) physically interacts (MI:0218) with fortilin (uniprotkb:P13693) by two-hybrid (MI:0018) Fig.1.
Structured summary for an article in FEBS Letters (Lee et al., 2008). Three interactions are shown, with their links to MINT and UniProtKB. Abstract (SDA), a device for capturing an article s key facts in an XML-coded summary, essentially to make them accessible to text-mining tools (Seringhaus and Gerstein, 2007) these data were collected from authors via a spreadsheet, and structured as shown in Figure 1 while clearly machine-readable, this format has the notable disadvantage of being rather human unfriendly.
A different approach was taken with BioLit (Fink et al., 2008), an open-source system that integrates a subset of papers from PubMed Central with structural data from the Protein Data Bank (PDB) (Kouranov et al., 2006) and terms from biomedical ontologies. The system works by mining the full text for terms of interest, indexing those terms and delivering them as machine-readable XML-based article ﬁles these are rendered human-readable via a web-based viewer, which displays the original text with colored highlights denoting additional context-speciﬁc functionality (e.g. to view a 3D structure image, to retrieve the protein sequence or the PDB entry, to deﬁne the ontology term). A more adventurous approach was taken by Shotton et al. (2009), who targeted an article in PLoS Neglected Tropical Diseases for semantic enhancement.
The enrichments they included were live Digital Object Identiﬁers and hyperlinks mark-up of textual terms (disease, habitat, organism, etc.
), with links to external data resources interactive ﬁgures a re-orderable reference list a document summary, with a study summary, tag cloud and citation analysis mouse-over boxes for displaying the key supporting statements from a cited reference and tag trees for bringing together semantically related terms.
In addition, they provided downloadable spreadsheets containing data from the tables and ﬁgures, enriched with provenance information and examples of mashups with data from other articles and Google Maps.
To stimulate further advances in the way scientiﬁc information is communicated and used, Elsevier offered its Grand Challenge of Knowledge Enhancement in the Life Sciences in 2008.
The contest aimed to develop tools for semantic annotation of journals and text- based databases, and hence to improve access to, and dissemination of, the knowledge contained within them.
The winning software, Reﬂect, focused on the dual need of life scientists to jump from gene or protein names to their molecular sequences and to understand more about particular genes, proteins or small molecules encountered in the literature (Paﬁlis et al., 2009). Drawing on a large, consolidated dictionary that links names and synonyms to source databases, Reﬂect tags such entities when they occur in web pages when clicked on, the tagged items invoke pop-ups displaying Utopia documents brief summaries of entities such as domain and/or small molecule structures, interaction partners and so on, and allow navigation to core biological databases like UniProtKB. All of these initiatives differ slightly in their speciﬁc aims, but nevertheless reﬂect the same aspiration to get more out of digital documents by facilitating access to underlying research data.
As such, it is interesting to see that a number of common themes have emerged: most are HTML- or XML-based, providing hyperlinks to external web sites and term deﬁnitions from relevant ontologies via color-coded textual highlights most seem to ignore PDF as a foundation for semantic enrichment (despite a signiﬁcant proportion of publisher content being offered in this format). The results of these projects are encouraging, each offering valuable insights into what further advances need to be made: clearly, we need to be able to link more than just a single database to a single article, or a single database to several articles, or several databases to a single issue of a single journal.
Although necessary proofs of principle, these are just ﬁrst steps towards more ambitious possibilities, and novel tools are still needed to help realize the goal of fully integrated literature and research data.
In this article, we describe a new software tool, Utopia Documents, which builds on Utopia, a suite of semantically integrated protein sequence/structure visualization and analysis tools (Pettifer et al., 2004, 2009). We describe the unique functionality of Utopia Documents, and its use in semantic mark-up of the Biochemical Journal (BJ). We also outline the development of a number of new plugins, by means of which we have imported additional functionality into the system via web services.
2 SYSTEM AND METHODS Utopia Documents was developed in response to the realization that, in spite of the beneﬁts of enhanced HTML articles online, most papers are still read, and stored by researchers in personal archives, as PDF ﬁles. Several factors likely contribute to this reluctance to move entirely to reading articles online: PDFs can be owned and stored locally, without concerns about web sites disappearing, papers being withdrawn or modiﬁed, or journal subscriptions expiring as self-contained objects, PDFs are easy to read ofﬂine and share with peers (even if the legality of the latter may sometimes be dubious) and, centuries of typographic craft have led to convergence on journal formats that (on paper and in PDF) are familiar, broadly similar, aesthetically pleasing and easy to read.
In its current form, Utopia Documents is a desktop application for reading and exploring papers, and behaves like a familiar PDF reader (Adobe Acrobat, KPDF, OS X Preview, etc.) but its real potential becomes apparent when conﬁgured with appropriate domain-speciﬁc ontologies and plugins.
With these in place, the software transforms PDF versions of articles from static facsimiles of their printed counterparts into dynamic gateways to additional knowledge, linking both explicit and implicit information embedded in the articles to online resources, as well as providing seamless access to auxiliary data and interactive visualization and analysis tools.
The innovation in the software is in implementing these enhancements without compromising the integrity of the PDF ﬁle itself.
Suitably conﬁgured, Utopia Documents is able to inspect the content and structure of an article, and, using a combination of automated and manual mechanisms, augment this content in a variety of ways: 2.1 Adding deﬁnitions Published articles are typically restricted to a deﬁned page count, and are usually written for a speciﬁc audience.
Explanations of terms that might be useful to newcomers to a particular ﬁeld are therefore frequently i569 [11:00 28/8/2010 Bioinformatics-btq383.tex] Page: i569 i568 i574  T.K.Attwood et al. Fig.2.
The architecture of Utopia Documents, showing the relationship between the GUI (top), plugins (middle) and ontology (bottom). omitted.
Utopia Documents allows editors and authors to annotate terms with deﬁnitions from online resources (Wikipedia, UniProtKB, PDB, etc.
), and permits readers to easily ﬁnd deﬁnitions for themselves.
Interactive content and auxiliary data 2.2 Figures and tables in printed form are typically static snapshots of richer data (which are nowadays often available elsewhere online). For example, a table may represent the salient fragment of a much larger experimental dataset, or an image of a protein structure might highlight one speciﬁc feature of that molecule.
Utopia Documents is able to transform such static tables and ﬁgures, in situ, into dynamic, interactive objects, providing richer access to the underlying data.
2.3 Linking references to source articles Most articles published today are made available in electronic form, and substantial efforts are also made by publishers to make their back-catalogues electronically accessible.
Navigating the multitude of online repositories and bibliographic tools, however, is complex.
Utopia Documents simpliﬁes the process of ﬁnding related articles by automatically linking references to their digital online versions.
3 IMPLEMENTATION The software architecture comprises three main components, as shown in Figure 2: the core , providing generic mechanisms for displaying and manipulating articles, both programmatically and interactively  the plugins , which analyze, annotate and visualize document features, either automatically or under the guidance of a user and the ontology , which is used to semantically integrate the other components.
3.1 The core Optimized for interactivity, the multi-threaded core of Utopia Documents is written in C++ using Trolltech/Nokia s Qt toolkit.
The core serves two purposes: (i) it performs the relatively mundane tasks necessary to generate and manage the interactive Graphical User Interface (GUI) and to co-ordinate the behavior of the plugins, which are loaded on demand at run-time (ii) it carries out the low-level analysis of PDF documents, including reading their ﬁle format and converting them into both a visual representation to be displayed on-screen and a hierarchical semantic model for later higher-level analysis and annotation by the plugins.
The analysis performed by the core is generic in nature, and is restricted at this stage to identifying typographical and layout-based features common to scholarly papers from any discipline.
Once this raw structure has been generated, using various heuristics, the system then identiﬁes higher-level typographical constructs (titles, sections, headings, ﬁgures, tables, references, etc.). Annotations identifying these features are assembled, and added to the raw hierarchy to form a semantic model that is then shared with, and further annotated by, the plugins.
From these structural semantics , a ﬁngerprint is created that uniquely identiﬁes the article being read, and allows the annotations to be associated with it.
3.2 The plugins Two broad classes of plugin are deﬁned. Annotators inspect a document s content and semantic structure, then either apply local algorithms or communicate with external services in order to create annotations containing additional content (e.g. deﬁnitions of terms, user comments, links to other resources). Annotator plugins may be conﬁgured to execute automatically when a document is loaded, typically performing document-wide tasks, such as identifying terms of biological or chemical interest alternatively, they may be invoked manually via the GUI in these cases, the plugins have access to the GUI s state, and can generate context-speciﬁc annotations (e.g. associating a highlighted region of text with a speciﬁc comment made by a user, or ﬁnding the deﬁnition of a highlighted concept in an online database.) Visualizers provide various mechanisms for displaying and interacting with annotations: e.g. an annotation containing static images, links and rich text may be displayed using a browser-like visualizer, whereas one containing the structure of a molecule from the PDB might be displayed as an interactive 3D object.
Both types of plugin may be written in C++ or Python, and are executed in their own asynchronous environment, marshalled by the core.
i570 [11:00 28/8/2010 Bioinformatics-btq383.tex] Page: i570 i568 i574  3.3 The ontology Rather than create hard-wired relationships between the system s components, a simple ontology (in its current form, a hierarchical taxonomy) connects the plugins to the core and to one another.
This form of semantic integration allows the components to cooperate ﬂexibly in the analysis of document content and structure, and allows plugins to be developed independently of one another, with sensible relationships and behavior being inferred at run-time rather than being pre-determined: e.g. an annotator plugin may mark content as containing protein structure  a visualizer plugin, encountering this annotation at a later stage, can then decide whether to display this as a 2D static image, an interactive 3D model or as a 1D amino acid sequence.
3.4 Access to remote resources Via its plugins, Utopia Documents has access to a wealth of bioinformatics data.
Each plugin can use whatever client libraries are appropriate to access web-service endpoints (both SOAP- and REST-style), as well as other remotely accessible resources, such as relational databases and RDF stores.
Of particular note here are two substantial linked data initiatives that have proven to be of enormous value to our work.
The ﬁrst of these, the Bio2RDF project (Belleau et al., 2008), combines the content of many of the major life-science databases as a federated linked- data network accessible via SPARQL and REST interfaces.
This both offers a single mechanism via which Utopia Documents can search multiple primary databases, and enforces a consistent naming scheme between sources, allowing results to be interrelated.
The second (and more general), DBPedia, is a machine-readable RDF- based conversion of the popular human-readable Wikipedia (Auer et al., 2007). Although containing much information that is irrelevant to the life sciences, Wikipedia (and thus DBPedia) has evolved to represent a signiﬁcant and mostly authoritative corpus of scientiﬁc knowledge a study performed by the journal Nature concluded that its entries were as accurate (or indeed, as error prone) as those published in Encylopaedia Britannica (Giles, 2005, 2006). The combined application of ontologies and RDF in DBPedia allows queries performed by Utopia Documents to traverse only the portions of the DBPedia network that are semantically related to the life sciences.
Thus, in the context of a paper on enzymatic substrate cleavage, a search initiated via Utopia Documents for the term cleavage returns far more appropriate deﬁnitions than would the same search in a more generic context.
Utopia Documents is freely available via the project web site for Mac OS X (10.4 and later), Microsoft Windows XP and Vista and Ubuntu Linux. We welcome any feedback on the software.
4 RESULTS AND DISCUSSION Utopia Documents was developed in response to the need to achieve tighter coupling between published articles and their underlying data, ultimately to facilitate knowledge discovery.
The tool was designed with two classes of user in mind the reader, as consumer of published material and the journal editor, as curator.
To this end, the software was piloted with Portland Press Limited (PPL) with the goal of rendering the content of BJ electronic publications and supplemental data richer and more accessible.
Utopia documents To achieve this, an editor s version of Utopia Documents, with customized plugins, was integrated with PPL s editorial and document-management workﬂows, allowing BJ editors to mark up article content prior to publication.
In terms of functionality, the editor s version of the software behaves much the same as the reader s, with the additional feature that relationships between concepts in a document and online deﬁnitions/records can be made permanent in order to be shared with readers (Fig. 3g). The role of the editors was therefore to explore each pre-publication PDF, annotating terms and ﬁgures with deﬁnitions and interactive content and then validating them with a stamp of approval (i.e. the BJ icon). With the customized software in-house, article annotation was fairly swift, individual papers taking 10 30 min, depending on their suitability for mark-up.
The launch issue of the Semantic BJ (December 2009 http://www.biochemj.org/bj/424/3/) was primarily handled by two editors since then, the whole editorial team has been involved in the successful mark-up of eight further issues.
Entities relating to protein sequences and structures have been, of necessity, the main targets for mark-up, because this was the functionality built into the original Utopia toolkit.
The kinds of additional mark-up provided by the software include links from the text to external web sites, term deﬁnitions from ontologies and controlled vocabularies, embedded data and materials (images, videos, etc.) and links to interactive tools for sequence alignment and 3D molecular visualization.
three regions (Fig. 3): To allow readers to beneﬁt from these semantic enhancements, a reader s version of the software was made freely available (http://getutopia.com). The tool installs easily on the desktop as an alternative PDF viewer.
Once opened, it displays a window consisting of the main reading pane displays the article itself and supports the pagination, searching, zooming and scrolling features typical of PDF readers.
Below this, thumbnail images give an overview of the document and allow rapid navigation through it.
The sidebar on the right displays the contents of annotations, providing term deﬁnitions and access to auxiliary data as the article is explored.
When no speciﬁc terms are selected, the sidebar defaults to displaying document-wide metadata [including the title, authors, keywords, abbreviations, etc.
(3d)], in addition to the cited references (3e) these are linked, where available, via open-access publishing agreements or institutional or individual subscriptions, to the online versions of the original articles.
Where the PDF version is not available to the reader, clicking on the reference currently launches a Google Scholar search instead.
To avoid cluttering the text with highlighter pen -type marks, the presence of annotations, or availability of auxiliary data, is indicated by discreet colored glyphs in the margin.
Similar marks are added to the corner of the corresponding thumbnail in the pager, to indicate that additional information exists somewhere on that page.
Mousing- over a glyph highlights the nearby terms, or document regions, that contain annotations selecting these areas causes the associated data to be displayed this may involve populating the sidebar with deﬁnitions, or may activate an embedded interactive visualization.
Highlighting any word or phrase in the paper (3a) initiates a context- sensitive search of the online resources to which Utopia Documents is connected, all results again appearing in the sidebar.
At the bottom of the sidebar (3b), a lookup feature allows searches for terms not explicitly mentioned in the paper.
i571 [11:00 28/8/2010 Bioinformatics-btq383.tex] Page: i571 i568 i574  T.K.Attwood et al. Fig.3.
Utopia Documents user interface showing: (a) a selected term in the article (b) manual term lookup (c) resulting deﬁnitions of that term retrieved from Wikipedia (via DBPedia) and the PDB (d) metadata relating to the whole document (shown when no speciﬁc term deﬁnition is selected) (e) live links to articles in the article s bibliography (f) an icon indicating the authority for a particular annotation (here, the BJ) and (g) the panel used by BJ editorial staff to associate terms with annotations (note that this is only available in the editor s version of Utopia Documents). 4.1 Annotations An annotated term or region in a document may be associated with deﬁnitions and/or database records from a variety of sources.
Selecting a term invokes the display of all possible deﬁnitions, allowing the reader (or editor) to select for themselves the most appropriate version.
The provenance of these deﬁnitions is indicated in their headers, as illustrated in Figure 3: the icon on the left (3c) represents the item s origin [e.g., UniprotKB, Wikipedia, KEGG (Kanehisa et al., 2010)], while the presence of an icon on the right-hand side of the header (3f) indicates the person, group or organization who made, and endorsed, the association between a term and this speciﬁc deﬁnition (here, publisher-validated annotations carry the BJ logo). Interactive content 4.2 The current version of Utopia Documents supports three forms of embedded interactive content as with term deﬁnitions, these are indicated by red glyphs in the margins.
Selecting these causes a media player -like panel to appear, which the reader can use to control the behavior of the interactive content.
Activating the triangular play button replaces the static content, in situ, with its interactive version the neighboring pop-up button opens a new window leaving the static page unchanged.
Each type of interactive content has its own functionality: 3D molecules (Fig. 4), for example, can be rotated, zoomed and rendered in a variety of styles (e.g. space-ﬁll, backbone or cartoon) sequences and their associated features can be inspected individually, or edited as multiple alignments and tables of data can be manipulated or converted automatically into scatter-plots or histograms.
Figure 4 illustrates the simple transformation from static images of tables and ﬁgures into semantically annotated, interactive objects.
Utopia Documents provides new ways of reading, of interacting with and ultimately of assimilating the knowledge embodied within research articles.
The approach taken here departs from many initiatives in scholarly publishing in that the focus for enrichment is the hitherto-largely-neglected static PDF ﬁle, rather than HTML- or XML-based ﬁles. The subject of static PDF versus dynamic online articles has been hotly contested in the literature, the general consensus being that PDF is semantically limited by comparison with other online formats and is thus antithetical to the spirit of web publishing (Lynch, 2007 Renear and Palmer, 2009 Shotton et al., 2009 Wilbanks, 2007). We argue that PDFs are merely a mechanism for rendering words and ﬁgures, and are thus no more or less semantic than the HTML used to generate web pages.
Utopia Documents is hence an attempt to provide a semantic bridge that connects the beneﬁts of both the static and the dynamic online incarnations of published texts.
Inevitably, those who prefer to read articles online in a web browser will view the need to download a new, desktop-based PDF reader as a weakness.
Our view is, rather, that Utopia Documents complements browser-based tools, providing a novel mechanism for unleashing knowledge that is i572 [11:00 28/8/2010 Bioinformatics-btq383.tex] Page: i572 i568 i574  Utopia documents Fig.4.
Image sequences showing the transformation of a 2D image (left-hand panel) and of a static table of ﬁgures (right-hand panel) into interactive objects: i.e. a manipulable 3D model (coordinates extracted from the PDB) and a set of live ﬁgures and a customizable semantic graph.
otherwise locked in personal, publisher and/or institutional PDF-ﬁle archives.
In contrast with approaches for creating dynamic (as opposed to semantic ) life-science PDF articles (Kumar et al., 2008 Ruthensteiner and Hess, 2008) that use Adobe Acrobat s support for Universal 3D Data (U3D), Utopia Documents does not insert its augmented content into the PDF ﬁle itself, but instead blends additional visual material into the display process at the ﬁnal stages of rendering.
This mechanism presents a number of beneﬁts over the generic U3D approach: (i) the underlying PDF ﬁle remains small and compact, and does not become bloated by the large polygonal meshes necessary for rendering 3D molecules (ii) rather than the one size ﬁts all U3D approach, Utopia Documents is able to select appropriate rendering and interaction algorithms for different types of artifact (iii) Utopia Documents is able to maintain a semantic relationship between the underlying scholarly article and the object being rendered and importantly (iv) the original PDF, as an object of record , remains unadulterated and its integrity can be veriﬁed by examining it with a conventional PDF viewer.
The philosophy embodied in Utopia Documents is to hide as much of the underlying complexity as possible, to avoid requiring users (whether editors, authors or readers) to change their existing document-reading behaviors, and to present no signiﬁcant extra hurdles to publication.
Like the initiatives in semantic publishing outlined earlier, the Semantic BJ, powered by Utopia Documents, is a pilot, the success of which will depend on various factors, including whether the barriers to adoption are sufﬁciently low, and whether the approach is considered to add sufﬁcient value.
Although it is too early to assess the impact of the pilot on readers of the Semantic BJ, the take-up of the software by the BJ s full editorial team, and it use to mark up every issue since the launch, is a testament to the software s ease-of-use.
Of course, as the project with PPL develops, we will gather relevant usage and usability data in order to provide a more meaningful evaluation.
Many of the projects discussed in this article have exploited fairly traditional text-mining methods, in conjunction with controlled vocabularies and ontologies, to facilitate the launch of relevant external web pages from marked-up entities in documents.
As such, they come with all the limitations in precision of current text-mining tools this brings a signiﬁcant overhead to readers in terms of having to identify errors.
Of course, the difﬁculty for non-experts in any given ﬁeld is to be able to recognize when particular annotations really are errors, and failure to identify them as such leads to the danger of error propagation.
In light of these issues, we took a slightly different approach to entity mark-up in this ﬁrst incarnation of Utopia Documents, taking advantage of linked-data initiatives to facilitate mark-up and add value to published texts.
However, because the functionality of the system is easily customizable via its ﬂexible plugin architecture, any text-mining tool or database that is accessible via web services can be trivially added to the suite.
As a demonstration of the potential of this architecture, in collaboration with their developers, three prototype plugins that link Utopia to other systems have been implemented: Reﬂect: as mentioned earlier, the Reﬂect system is primarily used as a means of augmenting HTML content online, either by accessing a web page via the project s portal, or by installing a browser plugin (http://reﬂect.ws/). Its entity-recognition engine, however, may also be accessed programmatically via a web service, which, given a section of text, identiﬁes objects of biological interest and returns links to the summary pop-ups.
Integration of Reﬂect s functionality with Utopia Documents is therefore a comparatively straightforward task: as a user reads a PDF document, its textual content is extracted and sent to the Reﬂect web service the resulting entities are then highlighted in the PDF article, and linked to the appropriate pop-up, which is displayed when a highlighted term is selected.
A particular advantage of this integration is that it provides the reader with a light- weight mechanism for verifying or cross-checking results returned from multiple sources (e.g. Reﬂect, Bio2RDF, DBpedia/Wikipedia). GPCRDB: this is a specialist database describing sequences, ligand-binding constants and mutations relating to G protein- coupled receptors (http://www.gpcr.org/). Its recently developed web-service interface provides programmatic access to much of its content, enabling Utopia Documents to identify and highlight receptors and their associated mutants when encountered in PDFs. Thus, the presence of a GPCR in an article triggers the creation of a link to a description of that receptor in the database, which is displayed in the sidebar.
The article is then scanned for mutants, which in turn are linked to the relevant mutant records in i573 [11:00 28/8/2010 Bioinformatics-btq383.tex] Page: i573 i568 i574  T.K.Attwood et al. GPCRDB. Having identiﬁed an appropriate receptor, the software then automatically translates between the sequence co-ordinates, allowing equivalent residues to be readily mapped between them.
ACKnowledge Enhancer and the Concept Wiki: the Concept Wiki is a repository of community-editable concepts, currently relating to people and proteins, stored as RDF triples and fronted by a wiki-like interface (http://www.conceptwiki.org). Its associated ACKnowledge Enhancer is an analysis tool that links HTML content to relevant objects in the Concept Wiki and other online sources, exposing these to the user as selectable HTML highlights that, when activated, generate dynamic pop-ups.
As with the Reﬂect plugin, integration with these systems via their web services provides a straightforward way of migrating functionality previously only available for HTML content to scientiﬁc PDF articles.
Videos showing these plugins in use are available at http://getutopia.com. Utopia Documents is at an early stage of development and there is more work to be done.
In the future, as well as opening its APIs to other developers, we plan to extend its scope to systems and chemical biology, and to the medical and health sciences, as many of the requisite chemical, systems biology, biomedical, disease and anatomy ontologies are already in place and accessible via the OBO Foundry (Smith et al., 2007). Furthermore, the growing impetus of institutional repositories as vehicles for collecting and sharing scholarly publications and data, and an increase in the acceptance of open access publishing, together present many interesting possibilities that we are keen to explore.
Another planned extension is to allow readers to append annotations and notes/comments to articles.
There are various scenarios to consider here: (i) a reader might wish to make a note to self in the margin, for future reference (ii) a reviewer might wish to make several marginal notes, possibly to be shared with other reviewers and journal editorial staff (iii) a reader might wish to append notes to be shared with all subsequent readers of the article (e.g., because the paper describes an exciting breakthrough or because it contains an error) these scenarios involve different security issues, and hence we will need to investigate how to establish appropriate webs of trust.
Ultimately, allowing users to append their own annotations (in addition to those endorsed by publishers) should help to involve authors in the manuscript mark-up process.
Utopia Documents brings us a step closer to integrated scholarly literature and research data.
The software is poised to make contributions in a number of areas: for publishers, it offers a mechanism for adding value to oft-neglected PDF archives for scientists whose routine work involves having to attach meaning to raw data from high-throughput biology experiments (database curators, bench biologists, researchers in pharmaceutical companies, etc.
), it provides seamless links between facts published in articles, information deposited in databases and the requisite interactive tools to analyze and verify them for readers in general, it provides both an enhanced reading experience and exciting new opportunities for knowledge discovery and community peer review.
ACKNOWLEDGEMENTS We thank all Portland Press staff for helping to realize the Semantic BJ, and, in particular, Rhonda Oliver and Audrey McCulloch for their courage, patience and positive collaboration.
For their help i574 and guidance in developing interfaces and plugins to their software, we also thank: Gert Vriend and Bas Vroling (GPCRDB) Barend Mons, Jan Velterop, Hailiang Mei (Concept Wiki) and Lars Juhl Jensen and Sean O Donoghue (Reﬂect). project) (Utopia Documents)  Funding: Portland Press Limited (The Semantic Biochemical European Union Journal (EMBRACE, grant LHSG-CT-2004-512092) Biotechnology and Biological Sciences Research Council (Target practice, grant BBE0160651) Engineering and Physical Sciences Research Council (Doctoral Training Account). Conﬂict of Interest: none declared.
REFERENCES Attwood,T.K. et al. (2009) Calling international rescue  knowledge lost in literature and data landslide! Biochem. J., 424, 317 333.
Auer,S. et al. (2007) DBpedia : a nucleus for a web of open data.
In Aberer,K. et al. (eds) The Semantic Web.
Springer, Berlin/Heidelberg, pp.722 735.
Bairoch,A. (2009) The future of annotation/biocuration. Nature Precedings [Epub ahead of print, doi:10.1038/npre.2009.3092.1]. Belleau,F. et al. (2008) Bio2RDF: a semantic web atlas of post genomic knowledge about human and mouse.
In Istrail,S. et al. (eds) Data Integration in the Life Sciences.
Springer, Berlin/Heidelberg, pp.
153 160.
Boeckmann,B. et al. (2003) The SWISS-PROT protein knowledgebase and its supplement TrEMBL in 2003.
Nucleic Acids Res., 31, 365 370.
Bourne,P. (2005) Will a Biological Database Be Different from a Biological Journal  PLoS Comput. Biol., 1, e34. Chatr-aryamontri,A. et al. (2007) MINT: the Molecular INTeraction database.
Nucleic Acids Res., 35, D572 D574. Ceol,A. et al. (2008) Linking entries in protein interaction database to structured text: The FEBS Letters experiment.
FEBS Letters, 582, 1171 1177.
Editorial (2007) ALPSP/Charlesworth Awards 2007.
Learn.
Pub., 20, 317 318.
Fink,J.L. et al. (2008) BioLit: integrating biological literature with databases.
Nucleic Acids Res., 36, W385 W389. Giles,J., (2005) Internet encyclopaedias go head to head.
Nature, 438, 900 901.
Giles,J., (2006) Statistical ﬂaw trips up study of bad stats.
Nature, 443, 379.
Kanehisa,M. et al. (2010) KEGG for representation and analysis of molecular networks involving diseases and drugs.
Nucleic Acids Res., 38, D355 D360. Kouranov,A. et al. (2006) The RCSB PDB information portal for structural genomics.
Nucleic Acids Res., 34, D302 D305. Kumar,P. et al. (2008) Grasping molecular structures through publication-integrated 3D models.
Trends Biochem. Sci., 33, 408 412.
Lee,J.H. et al. (2008) Interaction between fortilin and transforming growth factor-beta stimulated clone-22 (TSC-22) prevents apoptosis via the destabilization of TSC-22. FEBS Letters, 582, 1210 1218.
Lynch,C. (2007) The shape of the scientiﬁc article in developing cyberinfrastructure. CTWatch Q., 3, 5 10.
Paﬁlis,E. et al. (2009) Reﬂect: augmented browsing for the life scientist.
Nat. Biotechnol., 27, 508 510.
Pettifer,S.R. et al. (2004) UTOPIA - User-friendly Tools for OPerating Informatics Applications.
Comp.
Funct. Genom., 5, CFG359. Pettifer,S., et al. (2009) Visualising biological data: a semantic approach to tool and database integration.
BMC Bioinformatics, 10, S19. Renear,A.H. and Palmer,C.L. (2009) Strategic reading, ontologies, and the future of scientiﬁc publishing.
Science, 325, 828 832.
Ruthensteiner,B. and Hess,M. (2008) Embedding 3D models of biological specimens in PDF publications.
Microsc Res Tech., 71, 778 786.
Seringhaus,M.R. and Gerstein,M.B. (2007) Publishing perishing Towards tomorrow s information architecture.
BMC Bioinformatics, 8,17.
Shotton,D. et al. (2009) Adventures in semantic publishing: exemplar semantic enhancements of a research article.
PLoS Comput. Biol., 5, e1000361. Smith,B. et al. (2007) The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration.
Nat. Biotechnol., 25, 1251 1255.
The UniProt Consortium (2009) The Universal Protein Resource (UniProt). Nucleic Acids Res., 37, D169 D174. Wilbanks,J. (2007) Cyberinfrastructure for knowledge sharing.
CTWatch Q., 3, 58 66.
[11:00 28/8/2010 Bioinformatics-btq383.tex] Page: i574 i568 i574 BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btn117 Vol.24 no.11 2008, pages 1410 1412 Data and text mining MedEvi: Retrieving textual evidence of relations between biomedical concepts from Medline Jung-jae Kim*, Piotr Pe zik and Dietrich Rebholz-Schuhmann EMBL-EBI, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK Received on November 8, 2007 revised on January 18, 2008 accepted on March 31, 2008 Advance Access publication April 9, 2008 Associate Editor: Alfonso Valencia ABSTRACT Summary: Search engines running on MEDLINE abstracts have been widely used by biologists to find publications that are related to their research.
The existing search engines such as PubMed, however, have limitations when applied for the task of seeking textual evidence of relations between given concepts.
The limitations are mainly due to the problem that the search engines do not effectively deal with multi-term queries which may imply semantic relations between the terms.
To address this problem, we present MedEvi, a novel search engine that imposes positional restriction on occurrences matching multi-term queries, based on the observation that terms with semantic relations which are explicitly stated in text are not found too far from each other.
MedEvi further identifies additional keywords of biological and statistical significance from local context of matching occurrences in order to help users reformulate their queries for better results.
Availability: http://www.ebi.ac.uk/tc-test/textmining/medevi Contact: kim@ebi.ac.uk authentic usage context (Sinclair, 1991). We believe that a concordancer is a good candidate to meet the above-mentioned tasks of information seeking, since it innately deals with the local context of matching occurrences where the evidence being searched is much more likely found than in other parts of the retrieved documents.
The common limitation of existing concordancers, however, is that they consider only single-term queries.
To deal with multiple-term queries effectively, we implement the positional restriction on top of a concordancer. This feature of MedEvi is similar to the concept of proximity query (Baeza-Yates and Ribeiro-Neto, 1999), for example, as implemented in the proximity search of Lucene queries and the defined adjacency operator of OVID database queries.
The difference between them is that while the latter is explicitly stated, if any, in query strings (e.g. A ADJn B ), the former is compulsorily applied to all queries where the distance between query terms, similar to n of ADJn , can be adjusted by users.
1 INTRODUCTION When exploring biomedical literature for information relevant to our research, we heavily rely on search engines (e.g. PubMed) which deliver us documents that match keyword- based queries.
In the case of a query consisting of multiple keywords or terms, there is a need for restricting positional distance between occurrences of the terms in a document.
If the terms are found too far from each other in the text, it is very likely that the text does not, at least not explicitly by means of the terms given, describe any relationship between concepts denoted by the terms.
We regard this positional restriction as crucial in seeking relational information, for example, when users attempt to find textual evidence of relations between given concepts in the literature.
We provide a novel tool to address this need with a special focus on the biomedical domain.
The tool presented here, named MedEvi, is a search engine that retrieves occurrences matching a given query with their local context.
It is inspired by keywords-in-context (KWIC) concordancers, which have over the last few decades revolu- tionized the field of lexicography where different senses of lexical entries of dictionaries have to be defined in their *To whom correspondence should be addressed.
MedEvi PubMed allows multi-term queries, composed with BOOLEAN operators (e.g. AND, OR). It is different from other existing search engines that also allow multi-term queries [e.g. (http://www.ncbi.nlm.nih.gov/sites/entrez), HubMed (http://www.hubmed.org)]. While the other search engines produce as results a list of MEDLINE abstracts, MedEvi directly browses text fragments that may eventually show semantic relations between given terms.
It is different from other text mining tools that also browse text fragments, mostly sentences [e.g. iHOP (http://www.ihop-net.org/UniPub iHOP/), MEDIE (http://www-tsujii.is.s.u-tokyo.ac.jp/medie/)]. While the text mining tools focus on certain biological entities like proteins (iHOP) (Hoffman and Valencia, 2005) and certain grammatical (MEDIE), MedEvi does not impose any syntactic or semantic restrictions, thus being widely used in any biomedical domains.
We explain the features of MedEvi in the next section.
like subject-verb-object structures Users of MedEvi have found the tool useful to find evidence from the literature, for example, to see whether candidate chemicals are involved in a metabolic pathway, to identify the proteins that regulate given proteins, and to find whether a multi-term ontology concept actually appears in the literature even with a high degree of syntactic variations.
Note that the applications above are generally concerned of semantic rela- tions between biomedical concepts.
Selected example queries can be found on the web page of MedEvi. ß 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
MedEvi Fig.1.
Screen shot of MedEvi result set 2 SOFTWARE FEATURES MedEvi receives a query either through the standard user interface in the entry page or via the advanced user interface available.
It retrieves MEDLINE abstracts relevant to the query by using an Apache Lucene index (http://lucene.apache. org) that covers the whole set of MEDLINE abstracts and is updated on a bi-monthly basis.
It then outputs hypertext that consists of aligned occurrences matching the query with hyperlinks attached to additional candidate keywords.
Figure 1 shows an example output with the top 10 occurrences of the query  (ada OR acrR) AND (activat* OR inhibit*). 2.1 Query syntax The query syntax of MedEvi is based on the Lucene query syntax.
Like Lucene, MedEvi allows both single terms and phrases, concept variables (see Section 2.3 for details), wild- cards (i.e. *, ), BOOLEAN operators (only AND, OR) and escaping of special characters.
It does not support field search and fuzzy search.
Grouping in MedEvi is restricted to OR operators [e.g. (Ada OR acrR) AND (activat* OR inhibit*) ], while grouping for AND operators [e.g. (Ada AND activat*) OR (acrR AND inhibit*) ] is not allowed.
This restriction enables MedEvi to align occurrences of queries by the keywords in the queries, as exemplified in Figure 1.
2.2 Advanced search options If a query string is successfully validated against the syntax, MedEvi searches the Lucene index with the query to retrieve MEDLINE abstracts.
It then filters out abstracts that do not meet the default options or the options set through the advanced user interface.
The options include maximum distance between keywords, range of publication dates of retrieved abstracts, maximum number of retrieved abstracts, criteria for sorting query occurrences.
MedEvi also allows users to limit the search for occurrences of queries within sentence boundaries, as the sentence boundaries are often critical in relation extraction (Ding et al., 2002). Notice, however, that the experimental results of Ding and coworkers also support the necessity of positional restrictions that are narrower than sentence boundaries, for high precision of relation extraction.
The details about the default offset of the search options, which were empirically chosen, are available on the help page of the MedEvi website.
2.3 Support of concept variables and database identifiers as query terms MedEvi provides 10 variables for prevailing types of biomedical entities (e.g. cell, disease, drug, gene) to apply semantic restrictions to the search results.
The functionality is inspired by the question-answering task of the Genomics Track in TREC 2007.
For example of the question What serum [PROTEINS] change expression in association with high disease activity in lupus , we may create a query like serum and [gene] and expression and lupus for MedEvi to collect gene and protein names, which may be the answers of the question, into a column dedicated for the variable (i.e. [gene]). The details of the variables are available on the help page of the MedEvi website.
MedEvi also recognizes query terms that are UniProt accession numbers (e.g. P06134 for Ada ), and it automatically expands them to sets of synonymous terms, so that instead of specifying a set of names denoting a protein, one can use a UniProt accession number to locate strings associated with this accession number.
The estimated precision and recall of the module for recognizing gene/protein names are 91.5% and 94%, respectively, when we accept nested terms as correct matches (Rebholz-Schuhmann et al., 2007). 1411  the source documents, while the citation information can be displayed in a pop-up window if the mouse cursor is placed onto the index column.
3 CONCLUSION MedEvi is supplementary to existing search engines and text mining tools in the biomedical domain.
It shows significant improvements in the presentation of results which offer new information seeking capabilities, by the combination of different search techniques such as concordance, positional restriction, semantic restriction and keyword lookup.
ACKNOWLEDGEMENTS Medline abstracts are provided from the NLM (Bethesda, MD, USA) and PubMed (www.pubmed.org) is the premier Web portal to access the data.
Antonio Jimeno Yepes contributed to the improvement of MedEvi s semantic search capabilities.
This work has been inspired by his contributions to the TREC Genomics Track competition 2007.
Funding: This research was sponsored by the EC STREP project BOOTStrep (FP6-028099, www.bootstrep.org). Conflict of Interest: none declared.
REFERENCES Baeza Yates,R. and Ribeiro Neto,B. Addison-Wesley, Wokingham, UK. (1999) Modern Information Retrieval.
Ding,J. et al. (2002) Mining MEDLINE: abstracts, sentences, or phrases In proceedings of Pacific Symposium on Biocomputing. pp.
326 337, World Scientific Publishing Company, Singapore. Hoffmann,R. and Valencia,A. for navigation of biomedical literature.
Bioinformatics, 21(Suppl. 2), ii252 ii258. Oakes,M.P. (1998) Statistics for Corpus Linguistics.
Edinburgh University Press, (2005) Implementing the iHOP concept Edinburgh, UK. Rebholz Schuhmann, D. et al. (2007) EBIMed: text crunching to gather facts for proteins from Medline. Bioinformatics, 23, e237 e244. Sinclair,J.M. (1991) Corpus, Concordance, and Collocation.
Oxford University Press, Oxford, UK. J.-j.Kim et al. 2.4 Grouping of occurrences of queries In the case of multi-term queries, MedEvi groups their occurrences by the order of query terms in the occurrences.
For the example query of Figure 1, occurrences in the order of ada and activat* (i.e. AB in the display) are displayed before those in order of activat* and ada (i.e. BA). 2.5 Identification of additional candidate keywords MedEvi automatically identifies additional candidate keywords which can be adopted by users for further narrowing the search results.
As candidate keywords, it first recognizes gene and protein names, species names, drug names and Gene Ontology terms in the local context of the query occurrences.
The estimated precision of the modules for the named entity recognition varies between 75% and 95% according to the types of named entities (Rebholz-Schuhmann et al., 2007). It then identifies nouns and verbs in the local contexts and scores them based on their frequencies in the results and in the whole set of MEDLINE abstracts by utilizing keyword extraction statistics (Oakes, 1998). MedEvi provides three links for each additional candidate keyword to help users expand their queries: a link to add the keyword to the old query, another to replace the old query with the new keyword, and the other to show information of the keyword from well-known databases (e.g. UniProt, Gene Ontology). 2.6 Generating output pages MedEvi outputs the result of a query in the form of an aligned hypertext.
In the case of multi-term queries, the hypertext has a section for each permutation of terms.
Each section has one or more rows that correspond to string occurrences matching the query.
The occurrences are sorted by the relevance scores of their source documents, which are generated by the Lucene index according to the given query.
If a document has multiple occurrences, they are displayed in adjacent rows whose index cells are uniformly coloured. The index column has links to PubMed web pages that have actual citation information for 1412
BIOINFORMATICS DISCOVERY NOTE Vol.25 no.22 2009, pages 2891 2896 doi:10.1093/bioinformatics/btp538 Systems biology Phenotypic categorization of genetic skin diseases reveals new relations between phenotypes, genes and pathways Ruslan I. Sadreyev1, Jamison D. Feramisco2, Hensin Tsao3, and Nick V. Grishin1,4, 1Howard Hughes Medical Institute, University of Texas Southwestern Medical Center, 5323 Harry Hines Blvd, Dallas, TX 75390-9050, 2Department of Dermatology, University of California at San Francisco, San Francisco, CA 94115, 3Wellman Center for Photomedicine, Department of Dermatology, Massachusetts General Hospital, 55 Fruit St, Boston, MA 02114, and 4Department of Biochemistry, University of Texas Southwestern Medical Center, 5323 Harry Hines Blvd, Dallas, TX 75390-9050, USA Received on May 11, 2009 revised on August 10, 2009 accepted on September 7, 2009 Advance Access publication September 10, 2009 Associate Editor: Jonathan Wren ABSTRACT Motivation: Systematic analysis of connection between proteins, their cellular function and phenotypic manifestations in disease is a central problem of biological and clinical research.
The solution to this problem requires the development of new approaches to link the rapidly growing dataset of gene disease associations with the many complex and overlapping phenotypes of human disease.
Results: We analyze genetic skin disorders and suggest a manually designed set of elementary phenotypes whose combinations deﬁne diseases as points in a multidimensional space, providing a basis for phenotypic disease clustering.
Placing the known gene disease associations in the context of this space reveals new patterns that suggest previously unknown functional links between proteins, signaling pathways and disease phenotypes. For example, analysis of telangiectasias (spider vein diseases) reveals a previously unrecognized interplay between the TGF-β signaling pathway and pentose phosphate pathway.
This interaction may mediate glucose- dependent regulation of TGF-β signaling, providing a clue to the known association between angiopathies and diabetes and implying new gene candidates for mutational analysis and drug targeting.
Contact: grishin@chop.swmed.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Rigorous quantitative analysis of disease phenotypes is a key problem on our way to understanding systemic effects of human gene mutations.
Such understanding would enable statistical prediction of clinical manifestations for genome abnormalities, inference of causative genes from complex disease phenotypes, as well as deeper into molecular mechanisms of pathophysiology. This tremendous task requires the development of new approaches to link the rapidly growing dataset of gene disease associations with the many complex and overlapping phenotypes of human disease.
insights Previously reported approaches to this problem ranged from considering diseases as individual entities connected through shared  To whom correspondence should be addressed.
causative genes (Goh et al., 2007) or co-occurrence in the same patient (Rzhetsky et al., 2007), to more detailed classiﬁcations involving the comparison of disease phenotypes, usually based on ontologies of phenotypic terms derived from natural-language phenotype descriptions through automated or semi-automated text analysis (Robinson et al., 2008 van Driel et al., 2006). These analyses may include additional high-throughput data on protein associations (Lage et al., 2007 Wu et al., 2008), improving prediction of new connections between diseases and proteins involved.
Here we suggest a different approach to quantitative gene-phenotype analysis.
By focusing on the set of genetic skin disorders, we are able to manually analyze the corresponding descriptions of phenotypic manifestations and design a set of elementary phenotypic features whose combinations deﬁne any given disease as a point in a multidimensional space.
Placing the known gene disease associations in the context of this space reveals new patterns that suggest previously unknown functional links between disease phenotypes, proteins and signaling pathways.
In particular, analysis of telangiectasias (spider vein diseases), reveals a previously unrecognized interplay between the TGF-β signaling cascade and pentose phosphate pathway (PPP), which may mediate glucose-dependent regulation of TGF-β signaling in diabetes.
2 METHODS The database of 560 genodermatoses, with their phenotypic representations, affected organ systems and associated genes (see Supplementary Material) was previously compiled by expert analysis of OMIM database (Feramisco et al. 2009). The elementary phenotypic features were manually selected as general clinical manifestations that can occur independently in different diseases and, when combined, can cover phenotype of any included genodermatosis (Feramisco et al., 2009). Correlation matrix R of elementary phenotypes is calculated based on the set of genodermatoses represented as vectors in the phenotype space: Rij = corr(Pi,Pj), where P = XT is the matrix composed of phenotype vectors Pi that corresponds to the transposed matrix X of disease vectors Xi = xk , k = 1,N (N is the dimension of phenotype space). corr(Pi, Pj) is Pearson s correlation coefﬁcient of two vectors.
Principal components of the set of disease points in the phenotype space are derived as eigenvectors of covariance matrix C = XXT, with largest  The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2891 2891 2896  R.I.Sadreyev et al. eigenvalues corresponding to the dimensions of strongest correlations within the data set X (Jolliffe, 2002). Statistical signiﬁcance of gene phenotype associations is estimated from the observed counts of co-occurrence of a given phenotype with mutations in a given gene (Supplementary Tables S1 S5). Fisher s exact test is used as a more appropriate alternative to chi square association test in the cases where counts in some cells of contingency tables are low ( 5). The signiﬁcance level is adjusted for the multiple testing of all gene phenotype associations using Bonferroni correction: α= α0/n = 1.2 10 6, where α0 = 0.05 and n= n(genes) n(phenotypes). Phenotypic clustering is performed using UPGMA agglomerative method.
3 RESULTS Using the Online Mendelian Inheritance in Man (OMIM) compendium of human mendelian inheritance (Amberger et al., 2009), we previously compiled a database of genetic skin diseases, their phenotypic representations, affected organ systems and associated genes (Feramisco et al., 2009). This database (see Supplementary Material) includes 560 diseases associated with 501 protein-coding genes, with 16% of diseases linked to two or more mutated genes and 18% of genes linked to two or more disease entities.
3.1 Phenotypic categorization of genodermatoses Based on the manual analysis of disease phenotypes, we deﬁne a minimal set of elementary phenotypic features whose combinations cover all included genodermatoses, so that phenotypic manifestation of each skin disease can be represented as a combination of several elementary features.
This set includes 42 elementary dermatologic features forming 18 groups: corniﬁcation phenotypes, pigmentation phenotypes, etc.
and 29 elementary systemic phenotypes forming 17 groups (Feramisco et al., 2009) (see Supplementary Material). For example, the group of pigmentation features includes hyper- and hypo-pigmentation, café au lait, poikiloderma and nevi (birthmarks and moles). Most of the genodermatoses are characterized by more than one elementary phenotype, with the average number of assigned elementary phenotypes being 2.4 per disease (Feramisco et al., 2009). Distributions of the numbers of dermatologic and systemic phenotypes per disease and the numbers of diseases sharing an elementary phenotype are shown in Supplementary Figure S1. As a combination of elementary features, each disease can be represented by a point in multi-dimensional space deﬁned by 71 basis vectors corresponding to the features (Fig. 1). To check for the independence of these features in the phenotype space, we calculate their correlation matrix based on the set of all disease points (see Methods section for details). This matrix (Fig. 2, see also Supplementary Material) does not show major correlation patterns in the incidences of different features in composite disease phenotypes. The highest non-diagonal correlation coefﬁcient is 0.605 for poikiloderma versus alopecia/hypotrichosis phenotypes the second highest is 0.505.
When the distribution of disease points is analyzed in the plane (Supplementary Fig.S4A) or 3D-space (Supplementary Fig.S4B) of highest-correlated phenotypes, every possible combination of these phenotypes is found in a signiﬁcant number of diseases.
In addition, we perform principal component analysis (PCA) of the disease set and visualize the distribution of points along principal components.
Elongated or skewed distribution of points  2892 Fig.1.
Representation of diseases as points in the phenotype space.
As an example, a 3D space of elementary phenotypic features Ph1 Ph3 is shown, with two diseases (D1, D2) deﬁned by the combinations of these features.
The similarity between these composite disease phenotypes is determined by the distance between points D1 and D2. projections onto the subspace of ﬁrst two or three principal components, which correspond to the largest correlations within the dataset, would indicate a major correlation in the occurrence of phenotypic features.
Neither the projection on the plane of top two components (Fig. 2B) nor the projection in the space of top three components (Supplementary Fig.S2) reveal such correlations.
The corresponding projections for the separate dermatologic and systemic phenotype sets, along with phenotype correlation matrices are shown in Supplementary Figure S3. These results suggest that our set of phenotypic features is largely independent.
In phenotypic space, the similarity between two diseases can be determined by the distance between the corresponding points (Fig. 1). We use these distances to group diseases by phenotypes. Several tested distance metrics (Manhattan block, Euclidean distance, etc.) produce similar results, thus we further use Euclidean distances for simplicity.
The distances range from zero for disorders with the same sets of phenotypic features to 3.9 for the most distant disorders that have 15 elementary phenotype differences.
Based on this disease representation, we analyze (i) phenotypes shared among diseases associated with a protein or a group of proteins (Fig. 3A) and (ii) functional links between proteins associated with phenotypically similar diseases (Fig. 3B). This analysis can provide signiﬁcant insights into molecular mechanisms of pathogenesis. First, associations between proteins and disease phenotypes can be dissected in a statistically rigorous manner and previously unknown links can be revealed.
Second, similarity in phenotypic manifestation may suggest common mechanisms of action for different proteins or signaling systems, and point to potential functional interactions.
Finally, inferred phenotypic association of a protein group or pathway provides a set of new protein candidates that may cause similar diseases of yet unknown molecular mechanism (Fig. 1). Our approach is different from previously reported approaches to the representation of disease phenotypes (Lage et al., 2007 Robinson et al., 2008 van Driel et al., 2006) in two essential points.
First, we base our analysis on the data of high quality, with the phenotype descriptions being manually curated by an expert.
Second, as opposed to building ontology of all phenotypic terms [12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2892 2891 2896  Phenotypic categorization of genetic skin diseases Fig.2.
Selected elementary phenotypic features are largely independent.
(A) Correlation matrix of phenotypic features, based on the whole dataset of 560 disease phenotypes. The matrix is shown as a grayscale grid, with each square representing the absolute value of Pearson s correlation coefﬁcient between two phenotypic features (see Methods). The values are in the range between 0.0 (white) and 1.0 (black). The matrix is symmetric, with ones on the diagonal.
(B) Projection of the disease set on the plane of ﬁrst two principal components in phenotypic space does not show any general correlations in the occurrence of elementary phenotypes among the diseases.
Fig.3.
(A) Inference of protein phenotype associations.
Decomposing disease phenotypes into elementary features allows for statistical analysis of co- occurrence between mutations in speciﬁc proteins and resulting elementary phenotypes. Signiﬁcant correlations between defects of a protein (ﬁlled circle) and a manifested phenotypic feature (ﬁlled square) may suggest causation.
(B) Inference of protein and pathway associations.
Diseases are clustered by phenotypic presentation (upper plane) and corresponding groups of disease-associated proteins are considered (lower plane). Left, when a cluster of diseases corresponds to mutations in functionally related proteins (e.g. proteins from the same signaling cascade), other proteins of this functional group (open circles) may be suggested as new potential candidates for the association with the same class of diseases.
Right, new relations between different protein groups may be inferred from their mapping to the same phenotypic cluster of related diseases.
and automatically tracing their relationships through shared parents, we are able to decompose complex phenotypes into elementary unrelated features that can be combined in an independent fashion.
3.2 Protein phenotype associations Decomposition of disease phenotypes into elementary features often reveals that an individual protein or a group of proteins is predominantly associated with a certain phenotypic feature (Fig. 3A). The statistical signiﬁcance of such associations can be estimated from the frequencies of co-occurrence of mutations in a speciﬁc protein with an elementary phenotype.
Our results conﬁrm many previously known associations.
For example, mutations in keratin I and collagens I and VII are signiﬁcantly associated with disease phenotypes of hyperkeratosis (Supplementary Table S1, 6), atrophy/aplasia/fragile Fisher s exact test P-value: P = 1.1 10 8) and bullous skin (Supplementary Table S2, P = 9.2 10 epidermal cohesion (blistering phenotype, Supplementary Table 9), respectively.
In addition, our results suggest S3, P = 1.5 10 previously unknown associations.
For example, the link between mutations in the subunits of laminin 5(332) and mucosal phenotype 7), has been detected.
group (Supplementary Table S4, P = 7.2 10 As a major component of the basement membrane, laminin 5(332) is an essential structural component of the dermal epidermal junction and is involved in cell adhesion and signal transduction. Our analysis suggests that various defects in laminin 5(332) consistently affect the integrity of mucous membranes.
Extending our analysis to groups of functionally related proteins, we ﬁnd statistically signiﬁcant 2893 [12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2893 2891 2896  R.I.Sadreyev et al. the disease (1 17) with the name of Fig.4.
Phenotypic clustering of diseases sharing the phenotype of telangiectasia ( spider veins ) corresponds to a clear functional grouping of associated proteins and suggests the relation between TGF-β signaling cascade and pentose phosphate metabolic pathway.
The cladogram of agglomerate hierarchical clustering is shown, the nodes marked by the numerical code of the associated protein in parentheses.
Filled circles, proteins associated with PPP. Filled squares, proteins involved in TGF-β pathway.
Open triangles, proteins involved in DNA excision repair.
Open circles, other proteins.
The following diseases are shown: 1: arterial tortuosity syndrome 2: transaldolase deﬁciency 3: Osler-Rendu-Weber syndrome 2 4: telangiectasia, hereditary hemorrhagic, of Rendu, Osler and Weber 5: juvenile polyposis/hereditary hemorrhagic telangiectasia syndrome 6: UV-sensitive syndrome 7: xeroderma pigmentosum with normal DNA repair rates 8: xeroderma pigmentosum, complementation group E 9: xeroderma pigmentosum, complementation group D 10: De Sanctis-Cacchione syndrome 11: osteogenesis imperfecta, type IV 12: hypotrichosis-lymphedema-telangiectasia syndrome 13: Kindler syndrome 14: ataxia-telangiectasia 15: Bloom syndrome 16: congenital disorder of glycosylation, type Ie 17: Rothmund-Thomson syndrome.
9). associations for gap junction proteins, connexins, which show a highly signiﬁcant connection to hyperkeratosis (Supplementary Table S5, P = 1.7 10 Unsurprisingly, most of the elementary phenotypic features are associated with a wide range of protein functions.
However, some features are associated with more functionally uniform groups of proteins.
For example, bullous (blistering) phenotype is mainly caused by mutations in structural proteins (collagens, keratins), ﬁlament-associated proteins, or cell adhesion molecules.
3.3 Functional associations between proteins through their phenotypic representation Similarity of disease phenotypes caused by mutations of different proteins may point to functional relationship between these proteins.
In addition to many known protein relations, our dataset allows for the inference of previously unknown functional links.
As an example, Figure 4 shows phenotypic clustering of telangiectasias (spider vein diseases) that suggests an association between the TGF-β cascade and the PPP. The cladogram based 2894 on phenotypic composition reveals two major groups of similar, tightly clustered diseases.
The ﬁrst group is associated exclusively with the proteins of DNA excision repair (ERCC2, ERCC6, DDB2, POLH), whereas the second group is associated with proteins that are connected to two distinct systems: TGF-β cascade (SMAD4, ENG, ACVR1) and PPP (transaldolase TALDO1 and glucose transporter GLUT10). Involvement of TGF-β in vascular anomalies has been reported for several diseases (Coucke et al., 2006 Loeys et al., 2005 Tille and Pepper, 2004). The detected phenotypical similarity to the PPP-associated disorders suggests that PPP is implicated in angiogenesis through the same pathophysiologic mechanism.
In particular, we hypothesize a cross-talk between PPP and TGF- β cascade, which may mediate the connection between glucose metabolism and abnormal vascular development in angiopathies. Among other implications, this hypothesis provides a potential explanation of the known link between diabetes and angiopathies (Miles et al., 2007 Simo et al., 2006), as well as suggests new potential angiopathy-linked proteins.
4 DISCUSSION The role of proteins and their interactions in the living organism is a central focus of molecular and cellular biology, with both fundamental and clinical implications.
In this respect, the catalogued links between protein mutations and their phenotypic manifestations in disease are, in a sense, the results of a grand mutagenesis experiment that may prove useful for ﬁnding new associations and generating new hypotheses.
Here, we present an analysis of phenotypes expressed in genetic skin disorders and the corresponding causative genes.
Although our method involved initial manual curation of elementary phenotypes, it can be readily generalized to other disease sets with available phenotype descriptions.
The construction of the set of elementary phenotypes can start from all phenotypic terms derived from textual annotations of diseases of interest, further ﬁltered by manual expert analysis and/or numerical selection of the maximal subset of independent phenotypic features, based, for example, on the analysis of correlation matrix (Fig. 2A) or on PCA. However, we believe that the set of phenotypes manually curated by an expert provides a more informative categorization, since it (i) involves additional knowledge not reﬂected in the brief disease annotations and (ii) is based on well-deﬁned clinical terms and thus is more accessible and relevant for clinical research community.
As an example, analysis of such a dataset can link genes to speciﬁc clinical manifestations (Fig. 3A) that are easily recognized by medical practitioners, which facilitates data accumulation and hypothesis testing.
Involvement of a protein in the development of a speciﬁc disease phenotype is an important piece of functional information, which may lead to (i) better understanding of protein s function and molecular mechanism of disease (ii) hypotheses about phenotypic effects of mutations in functionally related proteins and (iii) phenotype-based prediction of potential proteins involved in similar diseases with unknown genetic causation.
The presented protein phenotype associations, inferred from a relatively restricted statistical sample of OMIM, can be validated on larger datasets. This validation may be performed in at least two directions.
First, phenotypic effects of mutations in a speciﬁc gene can be analyzed on a wider scale: most directly, in larger [12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2894 2891 2896  clinical studies or in animal models.
Second, for a considerable set of characterized skin diseases, causative genes are still unknown.
With many of these diseases included in OMIM, an interesting further direction would be to test the patients for the mutations in the genes that have statistical associations with the manifested phenotypes. Similarity in phenotypic effects of different proteins may lead to hypotheses about previously unknown functional associations (Fig. 3B). We present a potentially interesting example of such an association, the interaction between PPP and TGF-β pathway suggested by phenotypic clustering of telangiectasias (Fig. 4). Utilizing glucose-6-phosphate as the substrate, PPP produces the major fraction of cell s NADPH, which plays a central role in maintaining intracellular redox potential by serving as a co- factor in the reduction of glutathione (Berg et al., 2001). As an example, PPP enzyme glucose-6-phosphate dehydrogenase is shown to play an important role in oxidative stress (Leopold et al., 2003 Park et al., 2006), which is thought to be the main mechanism of its involvement in cardiovascular disease (Matsui et al., 2005 Rajasekaran et al., 2007 Wiesenfeld et al., 1970). Changing the concentration of reduced glutathione alters redox state and affects, among other systems, TGF-β pathway (Maulik and Das, 2002 Shan et al., 1994). Altered redox state and the resulting oxidative stress are known to stimulate angiogenic response (Maulik and Das, 2002 Ushio-Fukai, 2006), and are shown to be involved in at least one disorder with telangiectasia phenotype (Nicotera et al., 1989). In addition, the effect of GLUT10 deﬁciency on TGF-β signaling in the arterial wall is shown in arterial tortuosity syndrome (Coucke et al., 2006). We hypothesize that (i) defects in PPP may cause abnormal vascular development by altering intracellular redox state and (ii) this effect may be mediated by TGF-β signaling cascade.
This hypothesis has several important implications.
First, the reported effects of glucose concentration on TGF-β cascade (Hua et al., 2003 Isono et al., 2000 Zhu et al., 2007) may be mediated by PPP. Furthermore, our hypothesis may explain the observed connection between microangiopathies and diabetes (Miles et al., 2007 Simo et al., 2006), suggesting that abnormal angiogenesis may be caused by changes in PPP activity due to the disruption of intracellular glucose homeostasis.
Finally, our hypothesis suggests that other proteins of PPP, as well as of TGF-β pathway, may be associated with angiopathies, providing a new set of potential candidates for mutational analysis and drug targeting.
This hypothesis can be readily tested in various experiments, including (i) analysis of redox-state and TGF-beta responses to different levels of glucose concentration in the patients with telangiectasia phenotype carrying TALDO1 and GLUT10 mutations, or in corresponding animal models (ii) analysis of these responses, as well as general phenotypic effects caused by mutations in other PPP proteins (in animal models or clinical studies) (iii) analysis of TGF-β response to expression changes or up- and down-regulation of PPP proteins in vivo and in culture (iv) sequencing genes of PPP and TGF-β pathway in patients with telangiectasias of unknown genetic background and (v) further experimental investigation of the role of PPP activity in the association between angiopathies and diabetes, in diabetes patients and animal models.
In conclusion, our manually curated decomposition of disease phenotypes is based on the set of elementary phenotypic features that serve as basis vectors in a multidimensional space, as opposed Phenotypic categorization of genetic skin diseases to previously reported automated (Lage et al., 2007 van Driel et al., 2006) or semi-automated (Robinson et al., 2008) ontologies of phenotypic terms.
The potential value of this approach is shown by conﬁrming known and revealing previously unknown gene phenotype, gene gene and pathway pathway associations.
ACKNOWLEDGEMENTS The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing high- performance computing resources.
Funding: National Institutes of Health (GM67165 to N.V.G.) Welch Foundation (I1505 to N.V.G.) American Cancer Society (to H.T.). Conﬂict of Interest: none declared.
REFERENCES Amberger,J. et al. (2009) McKusick s Online Mendelian Inheritance in Man (OMIM). Nucleic Acids Res., 37, D793 D796. Berg,J. et al. (2001) Biochemistry.
W.H. Freeman & Co, New York. Coucke,P.J. et al. (2006) Mutations in the facilitative glucose transporter GLUT10 alter angiogenesis and cause arterial tortuosity syndrome.
Nat. Genet., 38, 452 457.
Feramisco,J.D. et al. (2009) Phenotypic and genotypic analyses of genetic skin disease through the online Mendelian inheritance in man (OMIM) database.
J.Invest.
Dermatol., [Epub ahead of print, doi: 10.1038/jid.2009.108] Goh,K.I. et al. (2007) The human disease network.
Proc. Natl Acad. Sci.USA, 104, 8685 8690.
Hua,H. et al. (2003) High glucose-suppressed endothelin-1 Ca2+ signaling via NADPH oxidase and diacylglycerol-sensitive protein kinase C isozymes in mesangial cells.
J. Biol.
Chem., 278, 33951 33962.
Isono,M. et al. (2000) Stimulation of TGF-beta type II receptor by high glucose in mouse mesangial cells and in diabetic kidney.
Am.J. Physiol. Renal Physiol., 278, F830 F838. Jolliffe,I.T. (2002) Principal Component Analysis.
Springer, New York. Lage,K. et al. (2007) A human phenome-interactome network of protein complexes implicated in genetic disorders.
Nat. Biotechnol., 25, 309 316.
Leopold,J.A. et al. (2003) Glucose-6-phosphate dehydrogenase overexpression decreases endothelial cell oxidant stress and increases bioavailable nitric oxide.
Arterioscler. Thromb. Vasc. Biol., 23, 411 417.
Loeys,B.L. et al. (2005) A syndrome of altered cardiovascular, craniofacial, neurocognitive and skeletal development caused by mutations in TGFBR1 or TGFBR2. Nat. Genet., 37, 275 281.
Matsui,R. et al. (2005) Glucose-6 phosphate dehydrogenase deﬁciency decreases the vascular response to angiotensin II.
Circulation, 112, 257 263.
Maulik,N. and Das,D.K. (2002) Redox signaling in vascular angiogenesis. Free Radic. Biol.
Med., 33, 1047 1060.
Miles,P.D. et al. (2007) Impaired insulin secretion in a mouse model of ataxia telangiectasia. Am.J. Physiol. Endocrinol. Metab., 293, E70 E74. Nicotera,T.M. et al. (1989) Elevated superoxide dismutase in Bloom s syndrome: a genetic condition of oxidative stress.
Cancer Res., 49, 5239 5243.
Park,J. et al. (2006) Increase in glucose-6-phosphate dehydrogenase in adipocytes stimulates oxidative stress and inﬂammatory signals.
Diabetes, 55, 2939 2949.
Rajasekaran,N.S. et al. (2007) Human alpha B-crystallin mutation causes oxido- reductive stress and protein aggregation cardiomyopathy in mice.
Cell, 130, 427 439.
Robinson,P.N. et al. (2008) The Human Phenotype Ontology: a tool for annotating and analyzing human hereditary disease.
Am.J. Hum.
Genet., 83, 610 615.
Rzhetsky,A. et al. (2007) Probing genetic overlap among complex human phenotypes. Proc. Natl Acad. Sci.USA, 104, 11694 11699.
Shan,Z. et al. (1994) Intracellular glutathione inﬂuences collagen generation by mesangial cells, Kidney Int, 46, 388 395.
Simo,R. et al. (2006) Angiogenic and antiangiogenic factors in proliferative diabetic retinopathy. Curr. Diabetes Rev., 2, 71 98.
Tille,J.C. and Pepper,M.S. (2004) Hereditary vascular anomalies: new insights into their pathogenesis. Arterioscler. Thromb. Vasc. Biol., 24, 1578 1590.
Ushio-Fukai,M. (2006) Redox signaling in angiogenesis: role of NADPH oxidase.
Cardiovasc Res., 71, 226 235.
2895 [12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2895 2891 2896  R.I.Sadreyev et al. van Driel,M.A. et al. (2006) A text-mining analysis of the human phenome. Eur. J. Wu,X. et al. (2008) Network-based global inference of human disease genes.
Mol. Syst. Hum.
Genet., 14, 535 542.
Biol., 4, 189.
Wiesenfeld,S.L. et al. (1970) Elevated blood pressure, pulse rate and serum creatinine in Negro males deﬁcient in glucose-6-phosphate dehydrogenase.
N. Engl. J.Med., 282, 1001 1002.
Zhu,Y. et al. (2007) Regulation of transforming growth factor beta in diabetic nephropathy: implications for treatment.
Semin. Nephrol., 27, 153 160.
2896 [12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2896 2891 2896
Genomics Proteomics Bioinformatics 13 (2015) 64 68 H O S T E D  BY Genomics Proteomics Bioinformatics www.elsevier.com/locate/gpb www.sciencedirect.com RESOURCE REVIEW Web Resources for Model Organism Studies Bixia Tang 1,2,a, Yanqing Wang 1,b, Junwei Zhu 1,c, Wenming Zhao 1,*,d 1 Core Genomic Facility, Beijing Institute of Genomics, Chinese Academy of Sciences, Beijing 100101, China 2 University of Chinese Academy of Sciences, Beijing 100049, China Received 9 January 2015 revised 22 January 2015 accepted 31 January 2015 Available online 20 February 2015 Handled by Zhang Zhang KEYWORDS Model organism Database Genome Bioinformatics Biology Introduction Abstract An ever-growing number of resources on model organisms have emerged with the continued development of sequencing technologies.
In this paper, we review 13 databases of model organisms, most of which are reported by the National Institutes of Health of the United States (NIH http://www.nih.gov/science/models/). We provide a brief description for each database, as well as detail its data source and types, functions, tools, and availability of access.
In addition, we also provide a quality assessment about these databases.
Signiﬁcantly, the organism databases instituted in the early 1990s such as the Mouse Genome Database (MGD), Saccharomyces Genome Database (SGD), and FlyBase have developed into what are now comprehensive, core authority resources.
Furthermore, all of the databases mentioned here update continually according to user feedback and with advancing technologies.
Model organisms were placed at the forefront of biomedical research by the end of the 20th century [1]. Deﬁning the etymology of the term model organism is relatively difﬁcult however, the development of molecular biology technologies during the 1960s and 1970s led to its materialization.
The key rationale for the study of model organisms in biomedical * Corresponding author.
E-mail: zhaowm@big.ac.cn (Zhao W). a ORCID: 0000-0002-9357-4411. b ORCID: 0000-0002-7985-7941. c ORCID: 0000-0003-4689-3513. d ORCID: 0000-0002-4396-8287.
Peer review under responsibility of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
research is to examine fundamental mechanisms that may be shared by many or all living entities.
Some model organisms such as Drosophila, mouse, and maize have long histories of use, whereas others have been developed more recently.
Since the common conception of a model organism is changing along with technological advances in genome sequencing and editing, it is difﬁcult to provide a complete list of model organisms therefore, here we mainly focus on the canonical set of model organ- isms deﬁned by the National Institutes of Health (NIH) during the 1990s. The available web resources for the 13 covered model organisms are listed in Table S1, and we also provide a quality assessment for each resource (Table 1) based on ﬁve aspects: (1) webpage esthetics, (2) system per- formance, (3) data sources, (4) software and tools, and (5) data availability.
If all aspects are covered, a full score of 5 points is given.
http://dx.doi.org/10.1016/j.gpb.2015.01.003 1672-0229 ª 2015 The Authors.
Production and hosting by Elsevier B.V. on behalf of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Table 1 Major web resources for model organism studies Name MGD RGD SGD Link http://www.informatics.jax.org http://rgd.mcw.edu http://www.yeastgenome.org dictyBase http://dictybase.org WormBase http://www.wormbase.org FlyBase ZFIN Xenbase TAIR http://ﬂybase.org http://zﬁn.org http://www.xenbase.org http://www.arabidopsis.org BeetleBase http://www.beetlebase.org MyMpn http://mympn.crg.eu MaizeGDB http://www.maizegdb.org ASAP https://asap.genetics.wisc.edu/asap/home.php Main features Well-designed webpage standard access speed for data data uploading supported abundant eﬀective tools FTP accepted Well-designed webpage standard access speed for data data uploading supported abundant eﬀective tools FTP accepted Some browser limitation such as Firefox v35 under Win7 system low access speed for data data uploading not supported abundant eﬀective tools web page downloading accepted Well-designed webpage standard access speed for data data uploading not supported abundant eﬀective tools web page downloading accepted Well-designed webpage standard access speed for data data uploading supported abundant eﬀective tools FTP accepted Well-designed webpage standard access speed for data data uploading not supported abundant eﬀective tools FTP accepted Well-designed webpage standard access speed for data data uploading supported abundant eﬀective tools web page downloading accepted Well-designed webpage some problems exist such as wild word search 12* data uploading supported abundant eﬀective tools FTP accepted Well designed webpage standard access speed for data data uploading supported abundant eﬀective tools FTP accepted Well-designed webpage standard access speed for data data uploading supported abundant eﬀective tools FTP accepted Well-designed webpage standard access speed for data data uploading not supported abundant eﬀective tools web page downloading accepted Well-designed webpage standard access speed for data data uploading supported abundant eﬀective tools FTP accepted Well-designed webpage standard access speed for data data uploading supported abundant eﬀective tools web page downloading accepted Rating wwwww wwwww wwwqq wwwwq wwwww wwwwq wwwww wwwwq wwwww wwwwq wwwwq wwwww wwwww Refs.
[2 4] [5] [6,7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] Note: The assessment evidence covers 5 aspects: webpage esthetics, system performance, data sources, software and tools, and data availability.
The details are as follows: whether webpage is well designed whether the accessing speed is acceptable whether data uploading is supported whether there are abundant effective tools, and whether instant downloading is supported.
T a n g B e t a l  W e b R e s o u r c e s f o r M o d e l O r g a n i s m S t u d i e s 6 5  66 Genomics Proteomics Bioinformatics 13 (2015) 64 68 Mouse Genome Informatics The Mouse Genome Informatics (MGI) is a comprehensive bioinformatics resource for the laboratory mouse (Mus muscu- lus) that includes the Mouse Genome Database (MGD), Gene Expression Database (GXD), Mouse Tumor Biology (MTB) Database, Gene Ontology (GO), and MouseMine. Initially designed in 1994, MGD is a key resource of MGI used to curate genome information, as well as to track genome mapping data and to record mouse mutant phenotypes [2]. MGD is mostly suited to inquire about the genetic basis of human disease based on the phenotypic characterization of transgenic mice.
Within the database, users can access genetic, genomic, and phenotypic data either curated from public literature by the MGD group or deposited by researchers from all over the world.
A  Quick Search  function returns a broad range of data from a keyword query, whereas more speciﬁc search forms are also available for users, such as genes, mark- ers and SNPs, among others.
In addition, MGD supports three vocabulary browsers for users to ﬁnd all associated annotated information, the Mammalian Phenotype Ontology (MP), and the Online Mammalian Inheritance in Man (OMIM), as well as the GBrowse and JBrowse graphical browser to view genome interactively.
Furthermore, MGD can provide bulk access to certain kinds of information through the Batch Query tool, or the BioMart [3] and InterMine [4] databases.
MGD undergoes weekly updates, and supports ﬁle transfer protocol (FTP) and direct structure query language (SQL) data retrieval methods.
Overall, MGD has become a researchers prime to obtain biological information on the laboratory mouse.
including GO, international resource for Rat Genome Database The Rat Genome Database (RGD) is a database of biological information for studying the laboratory rat (Rattus norvegicus) that started more than 10 years ago [5]. RGD allows research- ers to investigate mechanisms of human physiology and dis- ease based on experimental research acquired in the rat.
The RGD provides several types of disease-focused data, including genetic and genomic data, as well as information on biological pathways, phenotypes, and individual strains.
To guarantee quality data, the RGD curates a subset of data manually from literature regarding disease models, molecular pathways, and mammalian phenotypes. The database also utilizes controlled vocabularies and ontologies to present data shared with other data sources, including NCBI and Ensembl, and users can access in-source tools to search and view data on hand.
For instance, users can use GBrowse to analyze the genetic or genomic differences among the rat, mouse, and human.
Meanwhile, the Virtual Comparative Map function (VCMap: http://www.animalgenome.org/VCmap/) allows users to com- pare the genomic positions of various genes simultaneously.
Similar to the MGD, RGD users can bulk download data from the FTP site.
Saccharomyces Genome Database The Saccharomyces Genome Database an open-source database for querying the genetics and molecular (SGD) is and cellular biology of Saccharomyces cerevisiae started in 1993 by David Botstein and colleagues at the Stanford University [6]. The SGD primarily sources manually curated scientiﬁc literature to ensure the data quality.
Currently, the yeast genome, genes, proteins, and other encoded features are available for user access.
SGD employs GO and the Ascomycete Phenotype Ontology (APO) to capture gene pro- duct function and mutant phenotypes, respectively.
SGD also offers a number of tools for users to access data.
For example, users can use YeastMine to retrieve and analyze a variety of data types, including chromosomal features, sequences, and protein features, among others whereas the GBrowse and Serial Pattern of Expression Levels Locator (SPELL) [7] tools allow users to explore genomic features and analyze microar- ray gene expression data, respectively.
dictyBase species The recently contrived dictyBase is a genetic database for the social ameba Dictyostelium discoideum [8]. Data provided in dictyBase include the organism s complete nuclear genome sequence, mitochondrial genome sequence, the extrachro- mosomal rRNA genes, expressed sequence tags (ESTs), and annotations information.
dictyBase covers genomes of four including D. discoideum, Dictyostelium different purpureum, Dictyostelium fasciculatum, and Polysphondylium pallidum. The resource collects data from major resource centers and projects such as Swiss-Prot and an international consortium of the Dictyostelium Genome Project and the Japanese cDNA Project.
It also has an integrated central strain repository to facilitate the ordering of Dictyostelium strains and other slime molds, known as the Dicty Stock Center.
Besides maintaining the comprehensive dataset, dictyBase also provides several tools, including a Genome Browser to display information graphically such as genes, RNA-seq, Dicty align- ments, and genome assembly information.
Users can utilize BLAST tools to perform comparison searches for nucleotide and amino acid sequences, and the gene page displays inte- grated information of genes, proteins, gene ontology, and gene orthology. Data can be downloaded directly from the web page and updated on a weekly to monthly basis.
WormBase WormBase is a curated resource for the model organism Caenorhabditis elegans [9]. Available data in WormBase include gene models, allelic variations, mutant phenotypes, anatomy function, expression patterns, gene interactions, and human disease relevance.
WormBase hosts the reference genome (WBcel235) that includes 1402 corrections and loads the annotated reference genomic sequence for over 20 species [9]. Besides maintaining comprehensive data, WormBase also provides several user tools.
For example, users can view graphical data with the GBrowse and BLAST/BLAT tools.
WormMine enables users to query, save, and manipulate these objects, as well as download bulk data.
Full text search also provides the ability to do a customized search and users can freely download data from the FTP site.
WormBase con- tinues to update and has become a dedicated resource for researchers.
Tang B et al  Web Resources for Model Organism Studies 67 FlyBase The Arabidopsis Information Resource FlyBase is a Drosophila genomic database that was founded in 1992 as a resource for collecting and representing related information on the fruit ﬂy (Drosophila melanogaster) [10]. FlyBase curates data from the published scientiﬁc literature accumulated over the past 20 years, including data detailing animal phenotypes, gene expression, interactions, and GO, among others [10]. Besides providing several basic search tools such as Quick Search and BLAST, FlyBase has also developed several novel tools.
These include (1) TermLink, which uses a controlled vocabulary (CV) to classify and annotate queried data (2) RNA-seq Search tool, which pro- vides a reads per kilobase per million (RPKM) value for (3) searching expression patterns of all annotated genes FeatureMapper genomic features such as functional elements, mutant event locations and reagents regions, and QueryBuilder for full text searches.
Users can download the FlyBase data from its FTP site or use SQL to access the database directly.
FlyBase has grown into an integrated and complex database for over 20 years and will continu- ously update to accommodate data and software tools available.
in one or more sequence function speciﬁc search for Zebraﬁsh Information Network The Zebraﬁsh Information Network (ZFIN) is a database for the model organism zebraﬁsh [11], which includes data on genes, mutations, phenotypes, genotypes, gene expression, orthology, and nucleotide and protein sequences.
ZFIN col- lects data from three primary sources: (1) curated scientiﬁc literature (2) collaborations with major resource centers, such as the Sanger Institute, Ensembl, NCBI and UniProt and (3) submissions from individual investigators.
Besides maintaining the comprehensive data, ZFIN also provides several tools to help users to browse data, such as BLAST and GBrowse for alignment searches.
The database also provides search inter- faces for various data types such as genes, markers, clones, and gene expression.
Users can submit their own data to ZFIN directly and download data from either their search results or the web page.
ZFIN updates daily.
Xenbase Xenbase is a genetic database that maintains the genome builds and gene models for two related species: the allote- traploid Xenopus laevis (v7.1) and the diploid Xenopus tropi- calis (v8.0). Xenbase provides eight module entries, including BLAST, GBrowse, gene expression, genes search, information and resources on Xenopus anatomy and development, Xenopus-speciﬁc reagents and protocols, literature, and com- munity [12]. Users can register an account to submit their own data to Xenbase, as well as download data from the Xenbase FTP site.
The database is continually updating.
At present, the current version (v3.3.1) has added a substantial number of relevant resources such as microarray data from the Gene Expression Omnibus (GEO), antibody database, and updated the software components over the previous version released in 2008 (v2.01). The Arabidopsis Information Resource (TAIR) is a genetic and genomic database for Arabidopsis thaliana [13]. Data types include genome sequences, gene structure and annotation, metabolic pathways, gene expression, DNA and seed reserve information, genome maps, genetic and physical markers, and information on ecotypes and natural variation, among others.
TAIR manually curates public literature, integrates data and resources from other sources such as GenBank and the Arabidopsis Biological Resource Center (ABRC), and allows users to upload own data.
TAIR provides several tools for users to query and analyze data, such as Textpresso to extract and process biological literature, N-Browse to view interactive bio- logical networks, and GBrowse that displays multiple genomes to study genome duplication and evolution.
Overall, TAIR serves as a community resource for Arabidopsis researchers and continues to be an essential resource for plant biologists.
Other resources BeetleBase BeetleBase is an integrated resource for the Tribolium research community that hosts several data types, including unmapped scaffolds, FGENESH-predicted genes, BAC-end sequences, genetic markers, mutants, GO, sequence ontology, and links to other databases.
BeetleBase mainly collects sequence data from public databases and the Tribolium research commu- nity including the Human Genome Sequencing Center, NCBI, the Tribolium BAC library, and the Tribolium Mutant Database, as well as annotation information and mapping results from published literature.
BeetleBase also provides sev- eral user tools including BLAST for sequence alignments and a BLAT/GMOD-integrated search engine to enhance querying abilities.
GBrowse, JBrowse, and Cmap are also provided as map viewers.
All the sequences can be downloaded from the BeetleBase FTP site.
MyMpn The MyMpn database is a web resource for the human pathogen Mycoplasma pneumoniae. MyMpn mainly covers the genomics data such as gene, gene essentiality, and operons, transcriptomics data such as microarrays, proteomics data including proteins, Pfam domains, complexes, and peptides, and metabolomics data such as metabolic reactions and growth curves and MyGBrowser to view genome information and enables users to browse pathway information.
Users can download data directly from the MyMpn web page.
[15]. MyMpn supports GBrowse Maize Genetics and Genomics Database First released in 1991, the Maize Genetics and Genomics Database (MaizeGDB) includes a comprehensive dataset encompassing information on genomic regions and loci, allelic variation, markers and probes, gene sequences and products, as well as phenotypic and metabolic information.
MaizeGDB  68 Genomics Proteomics Bioinformatics 13 (2015) 64 68 provides several data visualization tools, such as the Locus Lookup Tool , Bin Viewer , and the MaizeGDB Genome Browser to view relevant data.
Users can register with the website and submit data to data center or download data from either the FTP site or webpage. MaizeGDB updates continually.
Supplementary material Supplementary material associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j. gpb.2015.01.003. A Systematic Annotation Package for Community Analysis of Genomes References A Systematic Annotation Package for Community Analysis of Genomes (ASAP) is a database developed to curate sequence and functional characterization data from the Escherichia coli K-12 MG1655, the best studied genome of enterobacterial fam- ily, as well as 576 other enterobacterial genomes [17]. Data types provided by ASAP include annotations, sequences, expression, experiments, and information on mutant strains.
ASAP is an authority-controlled database where guests can view the published genomic data on genomes and experiments and the full data of K-12 MG1655. However, users must be registered as an annotator to achieve the complete set of fea- tures potentially in a genome and add annotations [17]. ASAP supports data type-speciﬁc, full-text, and BLAST searches.
The MG1655 data can be downloaded directly from the webpage. ASAP is currently under development and updates data continuously.
Concluding remarks The web resources on model organisms have become increas- ingly enriched in both the amount of data and available analysis tools.
The majority of the data surveyed in these databases are manually curated from published scientiﬁc literature.
All sup- port hyperlinks to connect or share data with other databases.
To achieve user ﬂexibility and convenience, these databases also provide several general and dedicated toolsets. Most of the aforementioned databases are open-source and the users can freely download data directly from the resources webpage or FTP site.
Furthermore, the databases above continuously update, aiming to become authority resources for model organ- ism study.
Although many advantages are listed here, there are shortcomings as well.
For instance, different data formats or interface standards are applied in establishing these databases, which bring some limitations, for example data integration, when the users want to access a kind of the data from different databases.
We hope that all these databases will be taken care of by well-organized communities and perform the normative data standardization to enhance data sharing and exchange, while also pushing the applications of the model organism data.
Competing interests The authors declared that there are no competing interests.
Acknowledgements This work was supported by the Strategic Priority Research Program of the Chinese Academy of Sciences of China (Grant No.XDB13040500). [1] Dietrich MR, Ankeny RA, Chen PM.
Publication trends in model organism research.
Genetics 2014 198:787 94.
[2] Blake JA, Bult CJ, Eppig JT, Kadin JA, Richardson JEMouse Genome Database group.
The Mouse Genome Database: integra- tion of and access to knowledge about the laboratory mouse.
Nucleic Acids Res 2014 42:D810 7.
[3] Kasprzyk A. BioMart: driving a paradigm change in biological data management.
Database (Oxford) 2011 2011 [bar049]. [4] Smith RN, Aleksic J, Butano D, Carr A, Contrino S, Hu F, et al. InterMine: a ﬂexible data warehouse system for the integration and analysis of heterogeneous biological data.
Bioinformatics 2012 28:3163 5.
[5] Laulederkind SJ, Hayman GT, Wang SJ, Smith JR, Lowry TF, Nigam R, et al. The rat genome database 2013  data, tools and users.
Brief Bioinformatics 2013 14:520 6.
[6] Skrzypek MS, Hirschman J.Using the Saccharomyces Genome Database (SGD) for analysis of genomic information.
Curr Protoc Bioinformatics 2011:1 3 [Chapter 1: Unit 1.20]. [7] Hibbs MA, Hess DC, Myers CL, Huttenhower C, Li K, Troyanskaya OG. Exploring the functional landscape of gene expression: directed search of large microarray compendia. Bioinformatics 2007 23:2692 9.
[8] Basu S, Fey P, Pandit Y, Dodson R, Kibbe WA, Chisholm RL. DictyBase 2013: integrating multiple Dictyostelid species.
Nucleic Acids Res 2013 41:D676 83.
[9] Harris TW, Baran J, Bieri T, Cabunoc A, Chan J, Chen WJ, et al. WormBase 2014: new views of curated biology.
Nucleic Acids Res 2014 42:D789 93.
[10] St Pierre SE, Ponting L, Stefancsik R, McQuilton P, FlyBase C. FlyBase 102  advanced approaches to interrogating FlyBase. Nucleic Acids Res 2014 42:D780 8.
[11] Howe DG, Bradford YM, Conlin T, Eagle AE, Fashena D, Frazer K, et al. ZFIN, the zebraﬁsh model organism database: increased support for mutants and transgenics. Nucleic Acids Res 2013 41:D854 60.
[12] Karpinka JB, Fortriede James-Zorn C, the Xenopus model Ponferrada VG, Lee J, et al. Xenbase, organism database new virtualized system, data types and genomes.
Nucleic Acids Res 2014 43:D756 63.
JD, Burns KA, [13] Lamesch P, Berardini TZ, Li D, Swarbreck D, Wilks C, Sasidharan R, et al. The Arabidopsis Information Resource (TAIR): improved gene annotation and new tools.
Nucleic Acids Res 2012 40:D1202 10.
[14] Kim HS, Murphy T, Xia J, Caragea D, Park Y, Beeman RW, et al. BeetleBase in 2010: revisions to provide comprehensive genomic information for Tribolium castaneum. Nucleic Acids Res 2010 38:D437 42.
[15] Wodke JA, Alibes A, Cozzuto L, Hermoso A, Yus E, Lluch-Senar M, et al. MyMpn: a database for the systems biology model organism Mycoplasma pneumoniae. Nucleic Acids Res 2014 43:D618 23.
[16] Schaeffer ML, Harper LC, Gardiner JM, Andorf CM, Campbell DA, Cannon EK, et al. MaizeGDB: curation and outreach go hand-in-hand.
Database (Oxford) 2011 2011 [bar022]. [17] Glasner JD. ASAP, a systematic annotation package for community analysis of genomes.
Nucleic Acids Res 2003 31: 147 51.
BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btl650 Vol.23 no.12 2007, pages 1568 1570 Databases and ontologies ProServer: a simple, extensible Perl DAS server Robert D. Finn, James W. Stalker, David K. Jackson, Eugene Kulesha, Jody Clements and Roger Pettett Wellcome Trust Sanger Institute, Wellcome Trust Geome Campus, Hinxton, Cambridge, CB10 1SA, UK   Advance Access publication January 18, 2007 Associate Editor: Alfonso Valencia ABSTRACT Summary: The increasing size and complexity of biological databases has led to a growing trend to federate rather than duplicate them.
In order to share data between federated databases, protocols for the exchange mechanism must be developed.
One such data exchange protocol that is widely used is the Distributed Annotation System (DAS). For example, DAS has enabled small experimental groups to integrate their data into the Ensembl genome browser.
We have developed ProServer, a simple, lightweight, Perl- based DAS server that does not depend on a separate HTTP server.
The ProServer package is easily extensible, allowing data to be served from almost any underlying data model.
Recent additions to the DAS protocol have enabled both structure and alignment (sequence and structural) data to be exchanged.
ProServer allows both of these data types to be served.
Availability: ProServer can be downloaded from http://www. sanger.ac.uk/proserver or CPAN http://search.cpan.org rpettett/. Details on the system requirements and installation of ProServer can be found at http://www.sanger.ac.uk/proserver/. Contact: rmp@sanger.ac.uk Supplementary Materials: DasClientExamples.pdf 1 INTRODUCTION High-throughput projects, such as the sequencing of the human genome, have resulted in a deluge of data.
Thus, a key challenge in modern bioinformatics is to put in place mechanisms for programmatic exchange of and access to large volumes of data from disparate resources.
As biological databases increase in size and complexity, the classical mechanism of data exchange, database duplication, can become impractical.
For example, the underlying database for Ensembl release 41 is over 700GB, containing data on 25 different genomes, spread across hundreds of tables.
An alternative to duplication of databases is to interlink the distributed resources, termed federation.
However, for data exchange to take place between federated databases, a mechanism and standardization of format must be agreed.
One such protocol for data exchange over a network is the Distributed Annotation System (DAS). Briefly, the DAS protocol standardizes queries and responses for DNA or protein sequences, along with their annotations, regardless of the underlying data architecture (Dowell et al., *To whom correspondence should be addressed.
2001). In common with other Web Services, client requests are made via HTTP to servers which process them and return results encapsulated in XML. Since the original DAS specification was developed, DAS has been used extensively for the annotation of genome (DNA) sequences and, more recently, of proteins.
In addition, many extensions to the DAS protocol have been proposed.
Two of these extensions, developed over the past year, allow the exchange of sequence alignments and protein structure data.
These extensions enable annotations to be integrated across DNA, protein sequence and protein structure.
We have developed ProServer, a simple, user-friendly package for making data available using the DAS protocol.
2 PROSERVER ProServer has been available from the Wellcome Trust Sanger Institute since 2003.
We have recently added additional function- ality and documentation to improve usability and robustness.
2.1 Architecture and flexibility ProServer is a standalone, lightweight DAS server, written in Perl and designed to have low system requirements.
The architecture of ProServer is represented in Figure 1.
At the top level, there is a daemon executable which acts as a broker between requests and the code that will handle them.
The server is configured using a.ini -format configuration file holding settings for data sources provided by the server (Fig. 1). The networking requirements of ProServer are handled by the POE package (http://poe.perl.org/), which implements a portable multitasking and networking framework for Perl. The server sits upon a number of source adaptors with each one dedicated to a single underlying data store.
Incoming queries are passed from the daemon to the appropriate source adaptor. The source adaptors handle access to the data store and the conversion of queries into generic DAS response data structures.
Linking source adaptors to their data stores are generic transport helpers which are responsible for handling data acquisition (Fig. 1). Depending on the format of any new data set, it may be necessary to implement a new transport helper (Fig. 1). However, ProServer comes bundled with helpers for some common data stores, for example: flatfile, GFF. MySQL, Oracle and SRS getz. These transport helpers are all simple, command-line or socket-handling modules.
ß 2007 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
A simple, extensible Perl DAS server computation.
Thus, providing the underlying data store is organized optimally (as it would need to be with the alternative servers), then similar performance and scalability should be achievable regardless of the DAS server used.
The simplicity of ProServer means that it can be deployed by users with only basic bioinformatics skills.
Therefore, groups can expose their data set, whether large or small, to the scientific community with little additional overhead to the storage of the data set.
2.3 Examples ProServer was originally developed in conjunction with the Ensembl project (Birney et al., 2006), to display features such as gene predictions on chromosomes.
Since the original release of ProServer, with only features and sequence capabilities, we have added both alignment and structure functionality.
This has enabled the Pfam database (Finn et al., 2006) to provide access to their protein sequence alignments using ProServer to serve data directly from the underlying MySQL database (see http://das.sanger.ac.uk/das/pfamAlign). Other projects outside of the Wellcome Trust Sanger Institute are already using ProServer for the exchange of feature annotations: examples include Gene3D (Yeast et al., 2006), CBS (O lason, 2005) and AnoEST (Kriventseva et al., 2005). The supplementary materials contain a list of some of the most popular clients that provide an insight to the use of DAS within the scientific community.
3 CONCLUSIONS The federation of databases removes the effort involved with their duplication and maintenance and the problem of asynchronous versions.
The relatively simple architecture of ProServer means that even small groups can make their data available via DAS, even if it is stored only as a flatfile, without the overheads of running a web server.
Once available, this resource can be readily integrated into other data sets.
For example, the Ensembl browser (Birney et al., 2006) allows new DAS sources to be added and displayed alongside existing annotation, even if they are only internally available at an institute.
Rare chromosomal abnormality data from the DECIPHER project (http://decipher.sanger.ac.uk/syndromes) are being displayed in a genomic context via the Ensembl browser using ProServer, for example, the 1p36 microdeletion (http://www.sanger.ac.uk/turl/72d). A survey of the DAS registry (Prlic et al., 2006) (http:/ www.dasregistry.org/) demonstrated that 69 of the 103 registered feature servers were using ProServer at the time of writing.
Thus, ProServer is already being widely used to provide data via the DAS protocol.
The recent extensions increase the range of data types available via DAS, allowing the transfer of annotations between aligned objects.
These extensions are leading to the development of exciting new clients that bring together different data types (Prlic A et al., 2005). ProServer s minimal requirements and simplicity, together with the widespread use of DAS in bioinformatics makes ProServer a valuable tool for publishing data in a common, standard programmatic format.
The accessibility and 1569 Fig.1.
A schematic representation of the ProServer architecture.
(See text for details.) How are new data sources made available using ProServer In order to expose different sources of data, a new source adaptor class must be written.
All such source adaptors inherit and extend generic functionality, namely the data retrieval methods that are applicable for the new data set (e.g. transparently handles the transformation of data to XML (Fig. 1). Finally, the details of the new source adaptor are entered into the ProServer configuration file.
sequence). The superclass features or Specific details regarding installation, security (and best practice usage) and scalability can be found in the ProServer README file.
2.2 Other DAS servers for systems creating DAS servers are Two alternative Dazzle (http://www.derkholm.net/thomas/dazzle/) and the Lightweight Distributed Annotation Server (LDAS, http:/ biodas.org/servers/LDAS.html). Dazzle is written in Java, whereas LDAS is written in Perl. Dazzle has comparable functionality to ProServer, whereas LDAS does not have alignment or structure capabilities.
Both of these alternatives require an available web server (e.g. Apache) and time- consuming configuration.
Despite the fact that ProServer does not use a web server such as Apache, our experience indicates that the main bottleneck is input output rather than  REFERENCES Birney,E. et al. (2006) Ensembl 2006.
Nucleic Acids Res., 34, D556 D561. Dowell,R.D. (2001) The distributed annotation system.
BMC al. et Bioinformatics, 2, 7.
Finn,R.D. et al. (2006) Pfam: clans, web tools and services.
Nucleic Acids Res., 34, D247 D251. Kriventseva,E.V. et al. (2005) AnoEST: Toward A. gambiae functional genomics.
Genome Res., 15, 893 899.
Prlic,A. et al. (2005) Adding some SPICE to DAS. Bioinformatics, 21(Suppl. 2), ii40 ii41. Prlic,A. et al. of biological data.
Data Integration Workshop, DILS pp.
195 203.
(2006) The Distributed Annotation System for integration (eds.) International Springer, Berlin/Heidelberg, In Leser U., Naumann F., Eckman B. in 2006.
Proceedings.
Sciences: Third Life the O lason,P.I. (2005) Integrating resources through the Distributed Annotation System.
Nucleic Acids Res., 33, W468 W470. Yeats,C. et al. (2006) Gene3D: modelling protein structure, function and annotation protein evolution.
Nucleic Acids Res., 34, D281 D284. R.D. Finn et al. integration of distributed resources into state of the art tools and websites is accelerating novel scientific discoveries.
4 AVAILABILITY ProServer is available from http://www.sanger.ac.uk/proserver or CPAN http://search.cpan.org rpettett and has been tested on Tru64, Linux and Mac OS X architectures running Perl 5.6.1 and above.
ProServer has also been run under Windows using the Cygwin environment.
ACKNOWLEDGMENTS for The authors thank Andreas Kahari and Stefan Gra f funded by The contributing bug fixes.
This work was Wellcome Trust grant (G0100305). Funding to pay the Open Access publication charges was provided by The Wellcome Trust.
and a MRC (UK) E-science Conflict of Interest: none declared.
1570
BIOINFORMATICS ORIGINAL PAPER Vol.27 no.7 2011, pages 973 979 doi:10.1093/bioinformatics/btr048 Advance Access publication February 4, 2011 Systems biology Model annotation for synthetic biology: automating model to nucleotide sequence conversion Goksel Misirli1, Jennifer S. Hallinan1, Tommy Yu2, James R. Lawson2, Sarala M. Wimalaratne3, Michael T. Cooling2 and Anil Wipat1, 1School of Computing Science, Newcastle University, Newcastle upon Tyne, UK, 2Auckland Bioengineering Institute, The University of Auckland, New Zealand and 3European Bioinformatics Institute, Hinxton, UK Associate Editor: Jonathan Wren ABSTRACT Motivation: The need for the automated computational design of genetic circuits is becoming increasingly apparent with the advent of ever more complex and ambitious synthetic biology projects.
Currently, most circuits are designed through the assembly of models of individual parts such as promoters, ribosome binding sites and coding sequences.
These low level models are combined to produce a dynamic model of a larger device that exhibits a desired behaviour. The larger model then acts as a blueprint for physical implementation at the DNA level.
However, the conversion of models of complex genetic circuits into DNA sequences is a non-trivial undertaking due to the complexity of mapping the model parts to their physical manifestation.
Automating this process is further hampered by the lack of computationally tractable information in most models.
Results: We describe a method for automatically generating DNA sequences from dynamic models implemented in CellML and Systems Biology Markup Language (SBML). We also identify the metadata needed to annotate models to facilitate automated conversion, and propose and demonstrate a method for the markup of these models using RDF. Our algorithm has been implemented in a software tool called MoSeC. Availability: The software is available from the authors web site http://research.ncl.ac.uk/synthetic_biology/downloads.html. Contact: anil.wipat@ncl.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on November 6, 2010 revised on January 18, 2011 accepted on January 21, 2011 1 INTRODUCTION Synthetic biology involves the design and implementation of genetic circuits to enable organisms to perform novel, desirable functions for biotechnology applications.
Such applications include the production of medically relevant biomolecules (Anderson et al., 2006 Ro et al., 2006), environmental bioremediation (Sinha et al., 2010) and biofuel production (Lee et al., 2008). Most genetic systems are currently designed manually by a domain expert with a deep understanding of the system to be engineered.
However, as the aims of synthetic biologists become   To whom correspondence should be addressed.
more ambitious, and designs correspondingly more complex, the manual design of systems at a genetic level becomes more challenging.
Consequently, interest in the computational design of genetic circuits has grown rapidly over the last few years (Andrianantoandro et al., 2006 Bolouri and Davidson, 2002 Endler et al., 2009 Goldbeter, 2002 Hasty et al., 2002 Weiss et al., 2003). Genetic circuits are usually designed and simulated in silico as abstract models.
Computational models of genetic systems, such as Biobricks (Knight, 2003), are valuable because they allow rapid simulation of a system and veriﬁcation of its behaviour under a range of circumstances.
In synthetic biology, in silico models of modular components are typically assembled in a bottom-up fashion to produce a larger computational model of the desired system.
Such models are usually constructed using abstract modelling formalisms such as Systems Biology Markup Language (SBML Hucka et al., 2003) and CellML (Cuellar et al., 2003). Once a suitable model for a system has been designed, the conceptual model must be transformed into a DNA sequence.
This sequence encodes the necessary genetic features required for a designed circuit to be implemented in vitro or in vivo. At ﬁrst glance this transformation appears to be a relatively straightforward task to complete manually: components are selected and their DNA sequences are concatenated.
Appropriate restriction sites can then be added, a cloning vector selected and the entire sequence synthesized or cloned, as appropriate.
In practice, the situation is far more complex, and for large models is time consuming and difﬁcult to complete by hand.
A typical computational model will contain numerous species or components that do not have a physical representation at the DNA sequence level.
Examples include entities representing proteins, protein and RNA degradation, information ﬂow, environmental inputs and chassis- related factors.
In this article, we refer to model components representing biological parts with a DNA sequence as DNA-based parts.
It is not always immediately obvious which entities in the model map directly to the genes and sequence features necessary to encode the system represented by the model.
The fact that the mapping between component abstractions and sequence-based features is not necessarily one-to-one adds additional complexity.
Other factors such as the spacing and ordering of physical features and the genetic elements used to ensure their replication, can also impact on a system s behaviour and must be considered.
As models for computationally designed systems increase in size and complexity, automatically deriving the physical DNA sequence  The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:11 17/3/2011 Bioinformatics-btr048.tex] Page: 973 973 979  G.Misirli et al. necessary to encode a system designed as an in silico model becomes highly desirable, if not essential.
However, carrying out the model- to-sequence conversion process automatically is even more difﬁcult than manual conversion.
The move from an abstract concept such as pSpac promoter or gfp_CDS to a sequence of nucleotide bases involves accessing and integrating theoretical and practical domain knowledge that must be captured and made available in a computationally amenable format.
This mapping is not one-to-one.
A single model component may represent a whole set of biological concepts (e.g. there are numerous processes involved in protein degradation, which is generally modelled as a single entity), while one biological process may require multiple model components (protein production involves transcription, translation and mRNA degradation components). The need for expert domain knowledge to elucidate this mapping constrains the size, complexity and novelty of the systems that can be developed.
When the sequences are manually designed this knowledge comes from a human brain, aided by manual lookup of literature and databases.
When the task is automated such knowledge must be incorporated into the model itself the model requires metadata.
Metadata is data about data that is information about the format, organization and meaning of the fundamental data, which in this case are the model components.
In this article, we present an algorithm for the automated conversion of an abstract computational model into a synthesizable DNA sequence.
In order to perform this task, model components or species must be marked up with metadata.
We therefore also propose a generic method to facilitate this model annotation, incorporating existing standards wherever possible.
The algorithm presented is applicable to models constructed using either of the XML-based modelling languages CellML or SBML. We provide a use case that demonstrates our approach and describe a software application, MoSeC, which implements the model-to-sequence conversion algorithm.
1.1 Synthetic biology models and metadata Computational modelling can be performed in one of two ways: bottom-up or top-down.
Top-down modelling involves the breaking up of a high-level overview of a system into a number of modules, and then further reduction of the modules into submodules until all of the components of the system are speciﬁed at an elemental level.
This approach is valuable for applications where the biological system under study already exists.
Bottom-up modelling, in contrast, involves constructing a model from well-deﬁned, ﬁne-grained components.
In the context of synthetic biology, such components may be, for example, promoters, terminators and coding sequences (CDSs). At a higher level of abstraction, models can be constructed from entire devices with clearly deﬁned and more complex functions.
Recently, an approach to the deﬁnition of ﬁne-grained model components, known as standard virtual parts (SVPs) has been developed.
SVPs are virtual representations of components with clear biological counterparts (Cooling et al., 2008, 2010). There are a number of modelling formalisms in use, but two in particular are rapidly becoming standards in the systems and synthetic biology communities SBML and CellML. SBML has been under development since 2003, and is widely used among 974 the systems biology community.
SBML was the ﬁrst widely used machine-readable format for representing models, and offers a common intermediate format for models developed using different approaches and software.
CellML has been under development since 2000 and also aims to store and facilitate the exchange of computer-based mathematical models (Lloyd et al., 2008). Recently, a CellML repository has been created to provide a repository of SVPs for the composition of synthetic biology models (Cooling et al., 2010). CellML has a modular structure, in which equations and their variables are encapsulated inside components, while SBML uses explicit model elements to describe the reactions, their inputs, modiﬁers and outputs.
Both of these modelling formalisms are encoded in XML. They have different strengths and weaknesses, making them complementary rather than competitive for synthetic biology.
Both are popular enough that we consider it essential to provide model- to-sequence conversion for both formalisms, despite their somewhat different approaches to modelling systems of ordinary differential equations (ODEs). 1.2 Annotation of synthetic biology models Perhaps the greatest challenge in automating the process of converting a dynamic model to the speciﬁcation for a synthesizable DNA sequence is the availability of informative model metadata.
To automate the model-to-sequence conversion process, information about entities and relationships must be represented in the model in a computationally amenable format.
Wherever possible this metadata should draw on existing standards, such as those laid down by the W3C, and employ deﬁned terms that are backed by an ontology.
For CellML and SBML models, appropriate metadata must be added both to the species or components and to their relationships.
Annotations are used to add metadata to models in order to incorporate computationally tractable information about their constituents.
A number of approaches for the annotation of systems biology models have already been described and can be extended for use in synthetic biology (Endler et al., 2009). For example, the MIRIAM project proposes a standard for the minimal information required to annotate systems biology models (Novere et al., 2005). In this work, we annotate models in a similar fashion, embedding ontology terms, or links to ontology terms in the model as fragments of resource description framework (RDF) data.
A number of examples of this approach to model annotation is presented in the Supplementary Material.
1.3 Bottom-up vs top-down model annotations There are large repositories of models in both CellML and SBML. Our annotation approach can be applied to these existing models.
However, there is considerable recent interest in the bottom-up composition of dynamic models using SVPs (Cooling et al., 2010 Marchisio and Stelling, 2008). For bottom-up composition, it is both easier and more efﬁcient to apply the annotation approach to individual SVPs. Therefore, for the work presented here, annotations are applied to virtual parts prior to model assembly, extending the approach deﬁned by Cooling et al. (2010). [13:11 17/3/2011 Bioinformatics-btr048.tex] Page: 974 973 979  2 METHODS A typical dynamic model contains entities such as species and reactions (in the case of SBML), or components and connections (in the case of CellML). The process of converting SBML or CellML dynamic models to a speciﬁcation for a DNA sequence, in the form of a GenBank record for instance, requires that those constituents which represent sequence-based features such as coding sequences, ribosome binding sites and promoters, and the interactions between them, are distinguished from other elements of the model.
These constituents must then be arranged in an appropriate order.
If the sequence construction process is to be carried out automatically then the information to guide the arrangement process must be derived from the model.
The virtual parts used to build models are annotated with additional annotations that are not normally associated with synthetic biology model annotation.
The addition of these annotations, while necessary to facilitate the automation of the model-to-sequence conversion may also be applicable to other model-based problems in the computational design of biological systems.
These speciﬁc annotations and the issues they address are discussed below.
2.1 Genomic context Both cis and trans relationships are an inherent part of any synthetic biology model.
Ultimately, genetic features must be ordered and represented at a physical level in a DNA sequence of a genetic element (chromosome, plasmid, etc.). It is therefore necessary to consider the nature of the interactions between the elements in a model since the type of relationship may constrain the ordering of the physical representations of the model components.
For example, cis interactions must be distinguished from non- cis since cis interactions require sequence-based features to be co-localized on a nucleotide sequence.
The direction of a cis interaction between entities in a model can be used to derive ordering information, for example, specifying that a promoter should be placed upstream of a coding sequence.
Conversely, some non-cis elements represent macromolecules or cellular components that operate in trans, and are not directly relevant to DNA sequence feature organization.
A model also contains abstract elements, and connections between those elements, that allow the model to operate mathematically.
These elements do not directly represent physical biological entities and are therefore neither cis nor trans in their behaviour. In our annotation approach, SVPs contain a term indicating whether they are physical parts (possess a DNA sequence). Interactions between two physical parts are considered to be cis interactions.
2.2 Shims A major confounding factor in both automated and manual model-to- sequence conversion is the need for the insertion of spacer DNA between components.
Spacers may be necessary for biophysical reasons, for example to permit space for protein complexes to bind to the DNA. In this respect, they perform the same function as shims in automotive engineering.
The Compact Oxford English Dictionary deﬁnes a shim as a washer or thin strip of material used to align parts, make them ﬁt or reduce wear  a deﬁnition which appears entirely appropriate to spacer DNA in a biological construct.
We therefore introduce the term shim as a spacer component for the assembly of genetic constructs for synthetic biology.
It has long been recognized that shims are not always passive construction elements mutations in the length and composition of these regions can dramatically affect the dynamics of the system under consideration (Grosschedl and Birnstiel, 1980), and hence the kinetic parameters of the corresponding model.
Shims do not function in isolation  their effect depends upon the components that they link, and model metadata must reﬂect this dependency.
The precise length and sequence of shims is therefore likely to be of critical importance to the performance of a genetic system and therefore shims may have behaviour beyond that of simple inert spacer components.
For example, a shim between a promoter and CDS can have a modifying effect on the transmission of transcriptional ﬂux units from the promoter to the CDS. For Model annotation for synthetic biology these reasons we include shims as annotated virtual parts during the bottom up construction of the models.
However, for the sake of simplicity, in the work presented here, we model shims as inert physical parts that possess a DNA sequence but do not alter the dynamics of the model.
2.3 Transcriptional and translational ﬂux Deriving a DNA speciﬁcation from a model also requires that the ﬂuxes between SVPs in a model are analysed. These ﬂuxes reﬂect the way in which SVPs operate together to shape the transcriptional and translational behaviour of the system.
The BioBricks Foundation has suggested that the standard way to measure the inputs and outputs of a BioBrick should be Polymerases per Second (PoPS). The equivalent metric for the translational activity of an mRNA molecule is Ribosomes per Second (RiPS). We have adopted these standards.
Flux analysis is complicated by a number of factors, which are taken into account by the model-to-sequence conversion algorithm.
In a physical DNA sequence, a CDS upstream in an operon is likely to be exposed to a higher rate of transcription than those further downstream due to occasional premature termination or RNA processing.
In our algorithm, we assume a uniform rate of transcription throughout an operon. The ﬂow of information through parts of the graph corresponding to transcriptional units is therefore split after the promoter, ﬂowing through several coding sequences in parallel.
We also assume that terminators are 100% effective.
While this assumption is valid for the majority of existing models, it would be possible to extend the algorithm to handle terminators with different degrees of read-through.
We add metadata to models to identify and describe the nature of these ﬂuxes in order to direct the ordering of the genetic elements that ultimately encode the virtual system in vitro or in vivo. 2.4 Conversions of CellML and SBML models to graphs In CellML models degradation, activation and production ﬂuxes are connected through model components that act as a common interface.
In SBML each ﬂux is represented as an individual reaction, and these reactions do not need be explicitly connected in the model, since the species serve as a common interface to collect the ﬂuxes. Tracing the path of ﬂuxes through the components of a model is difﬁcult when the model is represented in XML. A more computationally amenable representation is a network, in which the nodes represent model components and the edges interactions between the components.
A network representation also facilitates visual inspection of the model for completeness and topological consistency.
Therefore, the ﬁrst step in our approach to deriving the speciﬁcation for a genetic system from a model is to convert the model to a graph-based representation.
Because of the differences between their XML representations, CellML and SBML models require slightly different approaches to convert the XML representation into a graph.
Also, models of the same process in SBML or CellML may not produce identical graphs.
After the XML to graph conversion step, however, the same algorithms can be applied to graphs derived from both SBML and CellML models.
2.5 An algorithm for graph to sequence conversion The nodes of the graph that contribute to the ﬁnal DNA sequence are ﬁrst identiﬁed. Information on the order and spacing of sequence features must also be derived from the model.
This information is inherent in the structure of the graph.
The model-to-sequence algorithm is based upon an analysis of the ﬂow of information through the model, as represented by its graph.
The ﬁrst step is to identify the starting node that represents the beginning of the ﬂow of information in the model: that component which will be placed ﬁrst in the resulting DNA sequence.
The starting component is identiﬁed via annotation, or it can be derived from the model by analyzing the network topology.
Components which do not have an incoming edge from any other component are candidate starting points.
975 [13:11 17/3/2011 Bioinformatics-btr048.tex] Page: 975 973 979  G.Misirli et al. Fig.1.
Flow of information through the model at both the DNA and RNA/protein level in parallel.
To produce a linear DNA sequence speciﬁcation, models are ﬁrst turned into tree-like structures.
This conversion is achieved by removing non- sequence-based components and any interactions forming loops that make it difﬁcult to disentangle the ﬂow of information through the system.
The analysis of information ﬂow through the model is relatively straightforward for CellML, in which the representation of components as nodes and interactions as edges is inherent.
SBML, however, is more challenging, and the model requires the following preprocessing:  Assignment rules, rate rules, species and reactions are represented as nodes.
If one assignment rule is used in another assignment rule, an edge is created between them.
If a species is used in an assignment rule, an edge is created from the species to the assignment rule.
If an SBML assignment rule is used in a reaction, an edge is created from one to the other.
If a species is a reactant or modiﬁer of a reaction, an edge is created from the species to the reaction.
If a species is an output of a reaction, an edge is created from the reaction to the species.
The model-to-sequence converter algorithm involves the following steps, starting with either a CellML or an SBML ﬁle: (1) Convert the XML representation to a graph representation.
(2) Identify nodes that correspond to DNA-based parts.
(3) Remove all non-cis interactions (interactions between RNA-based components count as cis interactions, and so are not removed). (4) Identify the start node.
(5) Join the DNA-based parts in the direction speciﬁed by the edges.
(The direction of ﬂow of PoPS and the RiPS are derived from the relations of parts in the model.) (6) Identify the subgraphs of physical components joined by mRNAs. Branches from an mRNA entity represent operon structures with more than one gene.
(7) Join the branches to form the operon structures.
(At present, this is done by taking the left branch ﬁrst.) (8) Add terminators to the end of each transcriptional unit.
(9) Concatenate the sequences of the transcriptional units to form a linear DNA structure.
The algorithm uses the models built with SVPs to track the activity at the DNA level (PoPS) and at the RNA/protein level (RiPS) in parallel.
In these models, PoPS are converted into RiPS through mRNA molecules (Fig. 1). 976 Table 1.
Annotations required for DNA-based parts for the model-to- sequence conversion process Attribute Mandatory  VisualName IsDNABased Type Sequence SequenceURI IsDNABasedPartTemplate IsTemplate No Yes If IsDNABased is TRUE If IsDNABased is TRUE and SequenceURI is empty If IsDNABased is TRUE and Sequence is empty Yes No Implementation 3 RESULTS 3.1 The model-to-sequence conversion algorithm has been implemented as an application called MoSeC. MoSeC is written in Java, and so will run on any platform.
MoSeC produces EMBL-Genbank formatted DNA sequences from both CellML and SBML models marked up with RDF according to the guidelines outlined above.
MoSeC works optimally with models composed from sets of virtual parts in CellML or SBML in the format described by Cooling et al. (2010). A standard set of annotations is required for our SVP annotation approach (Table 1). The VisualName attribute holds the name that is used when the model is visualized in MoSeC. If this attribute is not present, the name assigned within the XML ﬁle is used.
The IsDNABased attribute establishes whether an SVP is a physical part that is, whether it has an associated DNA sequence.
If this attribute is TRUE either Sequence or SequenceURI must have a value.
The Sequence attribute holds the actual sequence information if the sequence is long enough to be unwieldy, the SequenceURI may be used instead, to point to the URI at which the sequence data can be found.
The Type attribute must also be set for DNA-based part.
Part types currently supported are Promoter, Shine_Dalgarno_Sequence, CDS, Shim, Signal_Carrier and Chassis.
The ﬁnal two attributes, IsTemplate and IsDNABasedPartTemplate, are used to indicate that a model is used as a template for SVPs. These attributes apply to CellML Version 1.1 only.
A step-by-step guide to the annotation of virtual parts and the automatic model-to-sequence their relationships to support conversion process is presented in the Supplementary Material.
3.2 Application We applied our approach to the derivation of a DNA sequence for the subtilin receiver model described in detail by Cooling et al. (2010). The subtilin receiver model was built from annotated SVPs, assembled into a model and then used to generate a DNA sequence.
The model was based upon that used to design a subtilin receiver device Biobrick by the 2008 Newcastle University iGEM team (part No.BBa_K104001) (Fig. 2). The device encodes a construct which responds to the lantibiotic subtilin by producing green ﬂuorescent protein (GFP). The speciﬁcation for the virtual parts used in the model was derived from the subtilin sensing and regulatory system [13:11 17/3/2011 Bioinformatics-btr048.tex] Page: 976 973 979  Model annotation for synthetic biology Fig.2.
The ﬁnal subtilin receiver device BioBrick. from Bacillus subtilis strain ATCC 6633, which uses subtilin for quorum sensing.
This construct contains a promoter that transcribes an operon containing the CDS for spaR and spaK that encode the response regulator and sensor kinase, respectively, of the subtilin two- component system.
The SpaR regulated promoter, PspaS, is included downstream of the operon separated by a transcription terminator.
Upon sensing the subtilin signal peptide, the SpaK subtilin sensor activates the SpaR regulatory protein by phosphorylation.
The activated SpaR can then activate the PspaS promoter.
The subtilin receiver BioBrick was modelled in CellML Versions 1 and 1.1 and SBML Version 2 (Fig. 3). We annotated the virtual parts used to build this model according to the approach described above and used the model-to-sequence algorithm to automatically derive a DNA sequence speciﬁcation. The essential steps of the model-to- sequence conversion algorithm are illustrated, using this model, in Figure 4.
The annotated SVPs, the complete model and the output of the model-to-sequence conversion process are available in the Supplementary Material accompanying this article.
The nucleotide sequence produced from this model is currently being biologically validated.
4 DISCUSSION Many tools have already been developed to aid biologists with the manual design, simulation and construction of synthetic biological systems (Chandran et al., 2009 Rodrigo et al., 2007a, b). These tools are useful when the system under design is limited in size and the user possesses enough knowledge about the system to provide initial insights into the formulation of a preliminary design.
However, manual, visually guided assembly of representations of sequence- based parts is unlikely to be scalable for large-scale computational design, especially where the detailed structure of the system to be designed is poorly understood.
approaches, on evolutionary computation (EC), are particularly promising for automating large-scale biological system design.
Evolutionary algorithms were developed for applications in complex problem domains where the desired behaviour of the system is known, but the details of the system itself may be poorly understood.
As such, it can produce novel, non-intuitive circuits (Macia and Sole, 2009). Indeed, there is increasing evidence that complex systems constructed manually are less robust than those constructed using an evolutionary approach (Yan et al., 2010). Computational particularly based those In order to have a completely automated circuit design process models must be constructed, their behaviour assessed via simulation, modiﬁed if necessary and then converted to a DNA sequence speciﬁcation without the need for human intervention.
However, the computational assembly of parts that directly represent the molecular entities that comprise biochemical systems is problematical due to Fig.3.
An overview of the complete CellML model of the subtilin receiver BioBrick assembled from virtual parts.
977 [13:11 17/3/2011 Bioinformatics-btr048.tex] Page: 977 973 979  G.Misirli et al. Fig.4.
The model-to-sequence converter algorithm applied to the subtilin receiver device model.
(A) An XML ﬁle encoding the model is converted to a graph representation (B) non-cis interactions between model species or components are removed, together with any entities that lie between these edges (C) DNA-based parts only are retained (D) the ﬁnal sequence.
The virtual parts used to build this model were annotated according to the approach outlined above, facilitating its automatic conversion into a DNA sequence speciﬁcation. the complexities of mapping DNA-based parts to the models that encode them.
Once a system has been successfully designed and simulated, the next logical step is to test the design by implementing it in the target chassis or organism.
The nucleic acid sequences required to encode the system in vivo must be speciﬁed and ordered to give the correct genomic context, a process that is guided by information in the model.
In other words, the model representing the system must be interpreted to identify the nucleic acid molecules necessary to implement the system in vitro or in vivo. two types of information must be present In order to automatically specify the genetic system at a in sequence level, the model: information specifying the nucleic acid sequences necessary to encode the system at a genetic level and connectivity information that allows the correct ordering and spacing of sequences of genetic parts to be interpreted to ensure the correct cis- based interactions between these genetic parts.
Currently, dynamic models generated for systems or synthetic biology in SBML and CellML, are not usually annotated with this information.
In this article, we have described an approach to SBML and CellML model annotation that allows the information to automatically specify genetic systems at the nucleotide sequence level to be derived.
CellML and SBML have proven pedigrees in systems biology, and many complete models of synthetic systems already exist.
It is possible to manually add the annotations necessary for the model- to-sequence conversion process after model composition.
However, our annotation system has been designed with a bottom-up model assembly process in mind.
By marking up the virtual parts prior to model composition, composite models may be directly converted to nucleotide sequences with no further manual intervention.
It is our intention to provide repositories of SVPs already marked up using the MoSeC speciﬁcation as a standard, to support design tools that operate in a bottom up fashion.
978 [13:11 17/3/2011 Bioinformatics-btr048.tex] Page: 978 973 979  Typically, the annotation of virtual parts is done manually by reference to bioinformatics databases.
Although this is a time- consuming step, the fact that SVPs are, by deﬁnition, re-usable (and CellML has templates that can be used to instantiate SVPs) mean that it is a once-off investment for each part.
The practical requirements for annotation reinforces the beneﬁts of using modular parts, rather than annotation of entire models, and of maintaining repositories of already-annotated SVPs. However, in the future it may be possible to automate the annotation process itself through data integration strategies that combine information from remote data sources, such as genome or transcription factor databases, with experimentally derived information about a particular part (Lister et al., 2009). The MoSeC system as described is rich enough to produce the speciﬁcations for complete sequences suitable for synthesis.
However, there are further improvements that could be incorporated into the model-to-sequence conversion process.
We do not currently include information about the physical location of the ﬁnal sequence in the genome of the intended chassis organism.
Furthermore, sequences could be codon optimized for the intended host organism as part of the conversion process.
is an rapidly biology Synthetic becoming iterative, computationally dependent discipline, in which the outputs of models and in vivo implementations feed back to each other to continually improve our ability to understand and predict the behaviour of synthetic genetic systems.
The annotation of designs for synthetic systems will play a critical role in the automation of the bio-design lifecycle. The use of annotations in automating model-to-sequence conversion is a small but vital step towards the computational design of useful, predictable synthetic genetic systems.
ACKNOWLEDGEMENTS We acknowledge Poul F. Nielsen for helpful discussions relating to this work.
Funding: Research Councils UK (to J.S.H.) Engineering and Physical Sciences Research Council/National Science Foundation grant number: EP/H019162/1 (to G.M.) University of Auckland Faculty Research Development Fund New Staff grant (to M.T.C.). Conﬂict of Interest: none declared.
REFERENCES Anderson,J.C. et al. (2006) Environmentally controlled invasion of cancer cells by engineered bacteria.
J. Mol. Biol., 355, 619 627.
Model annotation for synthetic biology Andrianantoandro,E. et al. (2006) Synthetic biology: new engineering rules for an emerging discipline.
Mol. Syst. Biol., 2, 2006.0028.
Bolouri,H. and Davidson,E.H. (2002) Modeling transcriptional regulatory networks.
BioEssays, 24, 1118 1129.
Chandran,D. et al. (2009) Athena: modular CAM/CAD software for synthetic biology.
arXiv:0902.2598v1 [q-bio.QM]. Downloaded 7 May 2010.
Cooling,M.T. et al. (2008) Modelling biological modularity with CellML. IET Syst. Biol., 2, 73 79.
Cooling,M.T. et al. (2010) Standard virtual biological parts: a repository of modular modeling components for synthetic biology.
Bioinformatics, 26, 925 931.
Cuellar,A.A. et al. (2003) An Overview of CellML 1.1, a Biological Model Description Language.
SIMULATION, 79, 740 747.
Endler,L. et al. (2009) Designing and encoding models for synthetic biology.
J. R. Soc.
Interface, 6(Suppl. 4), S405 S417. Goldbeter,A. (2002) Computational approaches to cellular rhythms.
Nature, 420, 238 245.
Grosschedl,R. and Birnstiel,M.L. (1980) Spacer DNA sequences upstream of the T-A-T- A-A-A-T-A sequence are essential for promotion of H2A histone gene transcription in vivo. Proc. Natl Acad. Sci.USA, 77, 7102 7106.
Hasty,J. et al. (2002) Synthetic gene network for entraining and amplifying cellular oscillations.
Phys.
Rev.
Lett., 88, 148101.
Hucka,M. et al. (2003) The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models.
Bioinformatics, 19, 524 531.
Knight,T. (2003) Idempotent vector design for standard assembly of biobricks. Synthetic Biology Working Group Technical Reports, MIT. Available at http://dspace.mit.edu/handle/1721.1/21168 (last accessed date February 15, 2011). Lee,S.K. et al. (2008) Metabolic engineering of microorganisms for biofuels production: from bugs to synthetic biology to fuels.
Curr. Opin. Biotechnol., 19, 556 563.
Lister,A.L. et al. (2009) Saint: a lightweight integration environment for model annotation.
Bioinformatics, 25, 3026 3027.
Lloyd,C.M. et al. (2008) The CellML model repository.
Bioinformatics, 24, 2122 2123.
Macia,J. and Sole,R.V. (2009) Distributed robustness in cellular networks: insights from synthetic evolved circuits.
J. R. Soc.
Interface, 6, 393 400.
Marchisio,M.A. and Stelling,J. (2008) Computational design of synthetic gene circuits with composable parts.
Bioinformatics, 24, 1903 1910.
Novere,N.L. et al. (2005) Minimum information requested in the annotation of biochemical models (MIRIAM). Nat. Biotechnol., 23, 1509 1515.
Ro,D.-K. et al. (2006) Production of the antimalarial drug precursor artemisinic acid in engineered yeast.
Nature, 440, 940 943.
Rodrigo,G. et al. (2007a) Automatic model-based design of genetic circuits.
In Proceedings of the 2007 Conference on Machine Learning in Systems Biology, Evry, France. Rodrigo,G. et al. (2007b) Genetdes: automatic design of transcriptional networks.
Bioinformatics, 23, 1857 1858.
Sinha,J. et al. (2010) Reprogramming bacteria to seek and destroy an herbicide.
Nat. Chem.
Biol., 6, 464 470.
Weiss,R. et al. (2003) Genetic circuit building blocks for cellular computation, communications, and signal processing.
Nat. Comput., 2, 47 84.
Yan,K.-K. et al. (2010) Comparing genomes to computer operating systems in terms of the topology and evolution of their regulatory control networks.
Proc. Natl Acad. Sci.USA, 107, 9186 9191.
[13:11 17/3/2011 Bioinformatics-btr048.tex] Page: 979 973 979 979
Copyedited by: AP MANUSCRIPT CATEGORY: APPLICATIONS NOTE BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.19 2010, pages 2482 2483 doi:10.1093/bioinformatics/btq435 Advance Access publication August 2, 2010 Data and text mining Multi-netclust: an efﬁcient tool for ﬁnding connected clusters in multi-parametric networks Arnold Kuzniar1,2, , Somdutta Dhir3, , Harm Nijveen1,2, Sándor Pongor3,4 and Jack A.M. Leunissen1,2, 1Laboratory of Bioinformatics, Wageningen University and Research Centre, PO Box 569, 6700 AN Wageningen, 2The Netherlands Bioinformatics Centre, PO Box 9101, 6500 HB Nijmegen, The Netherlands, 3Protein Structure and Bioinformatics, International Centre for Genetic Engineering and Biotechnology, Padriciano 99, I-34012 Trieste, Italy and 4Bioinformatics Group, Biological Research Centre, Hungarian Academy of Sciences, Temesvári krt. 62, H-6701 Szeged, Hungary Associate Editor: Jonathan Wren ABSTRACT Summary: Multi-netclust is a simple tool that allows users to extract connected clusters of data represented by different networks given in the form of matrices.
The tool uses user-deﬁned threshold values to combine the matrices, and uses a straightforward, memory-efﬁcient graph algorithm to ﬁnd clusters that are connected in all or in either of the networks.
The tool is written in C/C++ and is available either as a form-based or as a command-line-based program running on Linux platforms.
The algorithm is fast, processing a network of 106 nodes and 108 edges takes only a few minutes on an ordinary computer.
Availability: http://www.bioinformatics.nl/netclust Contact: jack.leunissen@wur.nl Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 18, 2010 revised and accepted on July 22, 2010 1 INTRODUCTION Finding tightly connected clusters in large datasets is a frequent task in many areas of bioinformatics such as the analysis of protein similarity networks, microarray or protein protein interaction data.
Classical clustering algorithms have difﬁculties in handling large datasets used in bioinformatics owing to high demands on computer resources.
Fast heuristic algorithms have been developed for speciﬁc tasks, e.g. BLASTClust (ftp://ftp.ncbi.nih.gov/blast/documents/blastclust.html) from the NCBI BLAST package (Altschul et al., 1990), Tribe-MCL (Enright et al., 2002) or the CD-HIT (Li and Godzik, 2006) can delineate protein or gene families in a large network of sequence similarities (e.g. BLAST E-values). However, there are no apparent tools that could efﬁciently handle large multiple networks, such as those necessary to group proteins using more than one similarity criterion (e.g. based on sequence, structure or function) (Fig. 1A). We developed an efﬁcient, semi-supervised tool that takes the users empirical knowledge of cutoff values into account (below which interactions or similarities can be neglected) to combine multiple data networks using an averaging or kernel fusion method  To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the ﬁrst two authors should be regarded as joint First Authors.
Fig.1.
The principle of Multi-netclust is illustrated on a two-parameter network.
Thick and thin edges correspond to distinct similarity data (A). Dotted lines denote edges that are below the respective threshold, and hence can be omitted from the networks.
Two different aggregation rules are implemented: the weighted arithmetic averaging ( sum rule ) gives clusters that are connected within either of the two networks (B) the weighted geometric averaging ( product rule ) gives clusters that are connected within both networks (C). Mij denotes the value assigned to the edges, w is the weighting factor ( alpha ) of the two matrices (hence n=2) and Mmix refers to the aggregated matrix.
(Kittler et al., 1998). The resulting combined network can then be queried for connected components (clusters) using an efﬁcient implementation of the union-ﬁnd algorithm (Tarjan, 1975). The clusters correspond to groups of nodes that are connected either by any or by all of the constituent networks, depending on the aggregation rule used (Fig. 1B and C, respectively). To adapt this method to large heterogeneous datasets, we combined the thresholding, aggregation as well as the connected component search into a single, memory- and time-efﬁcient tool, Multi-netclust. Multi-netclust is not a new clustering method but an optimized implementation of existing graph algorithms suitable for handling large networks of 106 nodes and 108 edges.
2 MULTI-NETCLUST INPUT AND OUTPUT Multi-netclust uses external memory rather than the in-core approach (Chiang, 1995) for matrix manipulations so that the size of the datasets is not a limiting factor.
The input to Multi-netclust are networks given in sparse matrix format, as well as the aggregation rule, alpha weighting factor, and similarity (or distance) cutoff value(s) associated with a processing step(s). Generally, the product rule results in more reliable connections conﬁrmed by multiple data  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[14:00 28/8/2010 Bioinformatics-btq435.tex] Page: 2482 2482 2483  Copyedited by: AP MANUSCRIPT CATEGORY: APPLICATIONS NOTE Table 1.
Protein classiﬁcation results obtained for the individual and combined similarity networks Correct Incorrect Singletons 910 888 803 316 56 790 36 66 Dataset SW DALI1 (251) BLAST (0.1) DALI2 (0.4) BLAST (0.4) + DALI2 (0.4) SW (251) DALI1 (251) DALI2 (0.4) BLAST (0.4) BLAST (0.1) Numbers in parentheses denote the threshold used.
Symbols   and + refer to the product and sum aggregation rules, respectively.
The results were obtained for alpha weighting factor 0.5.
DALI1, matrix of raw scores DALI2, matrix of diagonally normalized scores correct, proteins connected only to members of the same SCOP superfamily incorrect, proteins connected to members of other SCOP superfamilies. 447 469 85 1041 35 92 1321 190 0 0 469 0 1266 475 0 1101 sources, whereas the sum rule expands the network with new (not necessarily reliable) connections.
Setting the alpha value for each matrix provides means, e.g. to weight the reliability of different data sources or to decide which dataset is more likely to contribute with new (additional) information.
A permissive cutoff value usually results in a few large clusters, while a strict cutoff value tend to produce many small (singleton) clusters.
The data can be entered either via a CGI interface, or from the command line.
The output of Multi-netclust is a list of the connected clusters given in a structured text format.
Multi-netclust is written in the C/C++ language, and the CGI interface is a Perl script.
The source code, sample data, explanations and benchmark results are available on the website http://www.bioinformatics.nl/netclust/. There is also a web-based application suitable to run smaller test-sets.
3 PERFORMANCE The run-time of Multi-netclust subsumes: (i) the time needed for reading-in the data, thresholding and aggregation ( 99.9%) and (ii) ﬁnding the connected components and writing the results ( 0.1%). A benchmark dataset of 1357 proteins, taken from the Protein Classiﬁcation Benchmark database (Sonego et al., 2007) was used to combine sequence similarities calculated by the BLAST and Smith Waterman (Smith and Waterman, 1981) algorithms, and DALI 3D structure similarities (Holm and Sander, 1995). The analysis took 4 s on a 2 GHz processor, the inﬂuence of parameter settings on the purity of connected clusters is apparent from the results (Table 1). An interesting example is the immunoglobulin superfamily (SCOP b.1.1), which has 125 members in the benchmark dataset. Using DALI alone, the b.1.1 proteins clustered together with the E set domains (SCOP b.1.18), grouping proteins related to immunoglobulin and/or ﬁbronectin Type III superfamilies. Using BLAST alone, the b.1.1. proteins clustered Multi-netclust together with a number of other superfamilies. Surprisingly, the combination of both DALI and BLAST datasets made 94% of the group b.1.1 cluster correctly.
The external memory-based, connected component search algorithm is fast and memory-efﬁcient compared to single-linkage- based clustering methods and in-memory graph algorithms used for similar purposes within the bioinformatics community (see Supplementary Material). The strength of Multi-netclust becomes more apparent when we deal with large datasets that cannot be handled with other algorithms.
For example, a network of 2 713 908 nodes and 781 328 458 edges took 5 min on an ordinary computer.
Of the other algorithms tested (see case studies on the website), only BLASTClust was able to handle a dataset of similar size however, its use is limited to BLAST similarity networks (and at greater expense of CPU time and memory required), whereas Multi-netclust is generally applicable.
To conclude, Multi-netclust is an efﬁcient tool that can aid exploratory analyzes of large biological networks using an ordinary computer.
Speciﬁcally, the potential applications include any task where network data of heterogeneous sources, such as sequence and structure similarities, gene expression or protein protein interaction data, are to be combined together, resulting in new and/or improved biologically relevant predictions.
ACKNOWLEDGEMENTS The authors would like to thank Pieter van Beek (SARA Computing and Networking Services) for help in providing additional BLAST data, Anand Gavai for preparing binaries for Mac OS X, Stijn van Dongen for the help with the MCL utilities and the NBIC initiative for using the Dutch Life Science Grid platform.
Funding: Graduate School Experimental Plant Sciences (to A.K.) BioAssist programme of the Netherlands Bioinformatics Centre (to H.N.). Conﬂict of Interest: none declared.
REFERENCES Altschul,SF. et al. (1990) Basic local alignment search tool.
J. Mol. Biol., 215, 403 410.
Chiang,YJ. et al. (1995) External-Memory Graph Algorithms.
In Proceedings of the 6th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 95). Society for Industrial and Applied Mathematics, San Francisco, CA, USA, pp.
139 149.
Enright,AJ. et al. (2002) An efﬁcient algorithm for large-scale detection of protein families.
Nucleic Acids Res., 30, 1575 1584.
Holm,L. and Sander,C. (1995) Dali: a network tool for protein structure comparison.
Trends Biochem. Sci., 20, 478 480.
Kittler,J. et al. (1998) On combining classiﬁers. IEEE Trans.
Pattern Anal.
Mach.
Intell., 20, 226 239.
Li,W and Godzik,A. (2006) Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences.
Bioinformatics, 22, 1658 1659.
Sonego,P. et al. (2007) A Protein Classiﬁcation Benchmark collection for machine learning.
Nucleic Acids Res., 35, D232 D236. Smith,T.F. and Waterman,M.S. (1981) Identiﬁcation of common molecular subsequences. J. Mol. Biol., 141, 195 197.
Tarjan,R.E. (1975) Efﬁciency of a Good But Not Linear Set Union Algorithm.
J. ACM. 22, 215 225.
2483 [14:00 28/8/2010 Bioinformatics-btq435.tex] Page: 2483 2482 2483
Genomics Proteomics Bioinformatics 13 (2015) 69 72 H O S T E D  BY Genomics Proteomics Bioinformatics www.elsevier.com/locate/gpb www.sciencedirect.com RESOURCE REVIEW Web Resources for Microbial Data Qinglan Sun #,a, Li Liu #,b, Linhuan Wu c, Wei Li d, Quanhe Liu e, Jianyuan Zhang f, Di Liu g, Juncai Ma *,h Information Network Center, Institute of Microbiology, Chinese Academy of Sciences, Beijing 100101, China Received 9 January 2015 revised 22 January 2015 accepted 27 January 2015 Available online 23 February 2015 Handled by Fangqing Zhao Abstract There are multitudes of web resources that are quite useful for the microbial scientiﬁc research community.
Here, we provide a brief introduction on some of the most notable microbial web resources and an evaluation of them based upon our own user experience.
KEYWORDS Microorganism Web resource Bioinformatics tools Rating Introduction Microorganisms are found virtually everywhere in the environ- ment and serve as important biological resources.
Some microorganisms have become widely utilized in industrial pro- duction. With the rapid development of high-throughput sequencing technology in recent years, it is considerably easier to obtain whole genome sequence of microorganisms.
To date, thousands of microbes have been sequenced.
Due to the difﬁcul- ties of isolating single microbial species, high-throughput * Corresponding author.
E-mail: ma@im.ac.cn (Ma J). # Equal contribution.
a ORCID: 0000-0002-8451-760X. b ORCID: 0000-0001-6977-1004. c ORCID: 0000-0002-5255-1846. d ORCID: 0000-0003-3364-8690. e ORCID: 0000-0001-8263-9446. f ORCID: 0000-0003-4215-5214. g ORCID: 0000-0003-3693-2726. h ORCID: 0000-0001-6382-8014.
Peer review under responsibility of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
sequencing technology has been applied to mixtures of environ- mental microbes to obtain metagenome sequencing data.
One metagenomics endeavor is the Human Microbiome Project (HMP), which aims to obtain metagenomic sequencing data from a large number of human subjects to enhance our under- standing of the relationship between the microbiome and human health.
In this review, we provide a brief introduction on some of the most useful microbial web resources (Table 1), including information on collection of microbial cultures, spe- cies identiﬁcation, literature, patent resources, microbial geno- mics, and metagenomics, as well as tools for analyzing genomic and metagenomic data.
In addition, we have evaluated their function and provided ratings for each resource based on our own user experience.
Integrated Microbial Genomes The Integrated Microbial Genomes (IMG, http://img.jgi.doe. gov/) system [1] is a combined web resource of microbial gen- ome datasets, including genome sequences and gene annota- tions. The aim of the IMG is to provide free microbial genomic data, together with integrated annotations and com- parative analysis services, to scientists worldwide.
At the end http://dx.doi.org/10.1016/j.gpb.2015.01.008 1672-0229 ª 2015 The Authors.
Production and hosting by Elsevier B.V. on behalf of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). 70 Genomics Proteomics Bioinformatics 13 (2015) 69 72 Table 1 Important web resources for microbial research Name IMG RAST MG-RAST HMP-DACC PATRIC Link http://img.jgi.doe.gov http://img.jgi.doe.gov http://metagenomics.anl.gov http://metagenomics.anl.gov http://atricbrc.org/portal/portal/patric/Home WDCM-CCINFO http://www.wfcc.info/ccinfo GCM ABC http://gcm.wfcc.info http://www.wfcc.info/abc Website for avian ﬂu information GMS-DBD http://www.avian-ﬂu.info http://www.boldmirror.net Note: More stars in the rating column indicate higher importance and usefulness.
Main features The integrated microbial genomes and annotation Gene prediction and annotation tool for microbial genome Analysis and annotation tool for metagenome Data center for the Human Microbiome Project The Integrated pathogen genomes and annotation Worldwide culture collection for microorganisms Global catalog of microorganism resource and storage information Papers and patents of microorganisms Data center of avian ﬂu DNA barcode data for species identiﬁcation Rating wwwww Refs.
[1,2] wwwww [3,4] [5] [6] [7] wwwww wwwww wwwww wwwwq wwwwq [8] wwwwq wwwwq wwwwq [9] [10] functional of 2014, there were over 10,000 microbial genomes in IMG. Approximately 9000 of them were from GenBank, with the remaining ones submitted by registered users or institutes.
Genome sequencing data stored in IMG are annotated using several the Clusters of Orthologous Groups (COG), Pfam, KEGG, TIGRfam, MetaCyc, and the Gene Ontology (GO), providing valuable information for registered users.
In addition to integrating microbial genome data, IMG also provides the tool for analyzing publicly-available environmental microbial samples and metagenomes, which is named IMG/M [2]. references, such as Rapid Annotation using Subsystem Technology The Rapid Annotation using Subsystem Technology (RAST, http://rast.nmpdr.org/) [3,4] is an automated online tool for analyzing microbial genomes.
RAST screens the genome sequence submitted by the registered users and predicts gene- coding sequences, rRNA, and tRNA using glimmer2, search_ for_rnas, and tRNAscan-SE, respectively.
RAST then automatically produces two classes of functional annotations for the predicted gene sequences.
One is a subsystem-based functional annotation that depends on recognition of func- tional variants of subsystems.
The other is a nonsubsystem- based functional annotation, which considers combined evidence from a number of tools.
RAST integrates these two annotations and provides exceptionally strong gene annotation results.
RAST also uses the gene annotation results for meta- bolic pathway reconstruction, which makes the resource useful for comprehensive annotation efforts.
RAST also excels at making collections of functionally related protein families (FIGfams). When a new genome is submitted to RAST and made public by the submitter, annotated genes are compared and added to the FIGfams collection.
The expanding FIGfams collection has proven to be a robust and scalable solution to microbial genome annotation efforts.
To date, over 12,000 users have registered in RAST and have submitted over 60,000 distinct microbial genomes for annotation.
Metagenomics RAST are and processed subsequently The Metagenomics RAST (MG-RAST, http://metagenomics. anl.gov/) server [5] is an open online system for management and comparative analysis of metagenomic data.
Registered users can submit raw sequencing data in FASTA format, along with sampling information into the system.
The uploaded sequences analyzed.
Summaries of the raw data as well as the analyzed results are generated automatically.
The MG-RAST server can man- age many different types of data including phylogenetic and metabolic data.
In addition to analyzing single-sample data- sets, the MG-RAST can also provide comparative analysis for multiple metagenomes and genomes.
The metagenome annotation and comparative analysis systems are designed such that tools and/or new data can be added or replaced at any stage of the analysis process as needed to accommodate new methods.
In order to protect data privacy, user access is controlled.
Registered users retain full control of their data.
Nonetheless, collaboration and sharing of data between multiple users are both possible and encouraged within the MG-RAST server.
HMP  Data Analysis and Coordination Center The Data Analysis and Coordination Center (DACC, http:/ hmpdacc.org/) is the data center for the HMP [6]. It includes sampling information as well as microbial genome and meta- genome sequences.
The HMP was launched in 2008 and funded by the National Institutes of Health (NIH). The main objective of this resource is to produce microbial data, which enhances our understanding of the relationship between the human microbiome and human health.
The HMP has investi- gated the microorganisms from multiple locations on the human body, including the gastrointestinal tract, oral cavity, nasal passages, skin, and urogenital tract.
The Project has pro- duced a signiﬁcant amount of genomic and metagenomic data.
Sun Q et al  Web Resources for Microbial Data 71 Currently, there are more than 1300 human microorganisms that have had their entire genome sequenced.
Additionally, there are more than 1200 microbiome samples collected from various organs of hundreds of human subjects that have been sequenced using whole metagenomic shotgun sequencing tech- nology. All of these data are available on the HMP-DACC. In addition to its role as a data source, the HMP-DACC is an information portal for news and research progress related to human microbiome research.
Pathosystems Resource Integration Center users submit cultured microorganism catalogue information, the GCM extracts the species names and strain numbers from various sources, including GenBank, SwissProt, and PubMed. The GCM then links the species names and strain numbers to the corresponding genomic resource, including nucleotide sequences, protein sequences, and any published reference arti- cles. The data are processed automatically with subsequent manual validation to ensure an accurate microbial resource database.
The GCM provides services for all academic and industrial users.
A number of online tools, including search functions and statistical analysis tools, are integrated into the GCM to facilitate ease of use.
The Pathosystems Resource Integration Center (PATRIC, http://atricbrc.org/portal/portal/patric/Home) [7] is a patho- gen information system with rich data and analysis tools that supports studies on bacteria-based infectious diseases.
PATRIC integrates a variety of data types, including sequence typing data, genomes, transcriptomes, 3D protein structures, and protein protein interactions.
There are currently more than 10,000 genomes and related transcriptomic data available in PATRIC. The genomes in PATRIC are annotated using the RAST server.
Summaries and comparisons of annotations for homologous genes are also available in PATRIC. PATRIC collects and integrates available bacterial genomes and related disease information to minimize time and effort needed for biological analyses.
PATRIC provides for researchers to ﬁnd data quickly and provides a personal work- space to save pertinent data.
Users can also upload their own data into this personal workspace.
Both public and personal data can be analyzed together with the provided suite of tools.
several tools World Data Center for Microorganisms The World Data Center for Microorganisms (WDCM, http:/ www.wfcc.info/ccinfo) was established as a data center of the World Federation for Culture Collections (WFCC). The WFCC collects, authenticates, manages, and distributes cul- tured microorganisms and cells.
The aim of the WFCC is to promote the establishment of connections between microbial culture collections and provide related services to the research community.
The WFCC pioneered the development of an international database for cultured microorganisms and cells, which led to the establishment of the WDCM. The WDCM is currently housed in the Institute of Microbiology, Chinese Academy of Sciences (CAS). The WDCM has integrated infor- mation from 678 culture collections from 71 countries.
The information includes data on organization, cultured microor- ganisms, management, services, and scientiﬁc interests of the collections.
The WDCM is a basic and useful information center for microbiological researchers.
Global Catalogue of Microorganisms The Global Catalogue of Microorganisms (GCM, http://gcm. wfcc.info/) [8] is a reliable and robust microorganism informa- tion system that helps culture collections manage and share data.
The GCM was designed and constructed by the WDCM. Currently, the GCM contains strain information for more than 273,000 strains, covering 43,436 microbial spe- cies from 67 collections from 34 countries and regions.
After Analyzer of Bio-resource Citations The Analyzer of Bio-resource Citations (ABC, http://www. wfcc.info/abc/) is a microbial information system that connects microbial species names and strain numbers to the correspond- ing published reference articles.
Through this system, research- ers can easily identify published articles that have studied a particular microorganism.
This is also useful for cultural col- lections, which can use the citation information when cata- loging the microorganisms in their collection.
According to the WDCM, in 2010, there were about 600 registered culture collections in nearly 70 countries.
Behind this platform, there is a full-text ﬁle system with a bio-resource term indexing and mining engine, which automatically explores the citation information.
ABC also provides an interface for users to access full-text articles (in ﬂash format) and check the citation infor- mation automatically extracted by the mining engine.
Another useful function for researchers is the statistics results generated by the statistics module, which accurately demonstrates the state of bio-resource citation information (automatic or curated results). There is currently citation information for more than 120,000 articles that have been published in 50,307 journals since January of 1953.
The publications include information about 63,000 microbial strains, which belong to 131 culture collections in 50 countries.
Website for Avian Flu Information The Website for Avian Flu Information (http://www.avian-ﬂu. info) [9] was designed and created by the Institute of Microbiology, CAS in 2004.
The website has been maintained and routinely updated for more than 10 years in support of public concerns and research interest that resulted from patho- genic inﬂuenza virus outbreaks.
Information on inﬂuenza infection cases as well as sequencing data have increased signiﬁcantly. The website provides information on outbreak reports, scientiﬁc publications, medicines, and vaccines.
In addition, the website has an integrated inﬂuenza virus sequence database and bioin- formatics tools, to facilitate the analysis of various inﬂuenza viral sequences.
clinical diagnoses, prevention policy, Global Mirror System of DNA Barcode Data The Global Mirror System of DNA Barcode Data (GMS- DBD, http://www.boldmirror.net/) [10] is a web-based system that was designed and built by the Institute of Microbiology,  72 Genomics Proteomics Bioinformatics 13 (2015) 69 72 CAS. The system distributes DNA barcode sequences that are produced by the International Barcode of Life (iBOL) project.
iBOL is the largest cooperation for studying biodiversity geno- mics.
It aims to produce DNA barcode records from 5 million specimens from 500,000 species, including animals, plants, and fungi, to aid in species identiﬁcation. The vast amount of DNA barcoding data that have been generated by collaborative research requires well-organized information system capabili- ties.
In addition to the Barcode of Life Data system (BOLD) established in Canada, the GMS-DBD also plays an important role in the iBOL project.
The GMS-DBD has been established in seven countries and aids the distribution and sharing of DNA barcode information worldwide.
Competing interests The authors declared that there are no competing interests.
Acknowledgements This research was supported by the National High-tech R&D Program of China (863 Program, Grant No.2014AA021501) and the National Scientiﬁc-Basic Special Fund from the Ministry of Science and Technology of China (Grant No.2014FY110500). References [1] Markowitz VM, Chen IM, Palaniappan K, Chu K, Szeto E, Pillay M, et al. IMG 4 version of the integrated microbial genomes comparative analysis system.
Nucleic Acids Res 2014 42:D560 7.
[2] Markowitz VM, Chen IM, Chu K, Szeto E, Palaniappan K, Pillay M, et al. IMG/M 4 version of the integrated metagenome comparative analysis system.
Nucleic Acids Res 2014 42:D568 73.
[3] Aziz RK, Bartels D, Best AA, DeJongh M, Disz T, Edwards RA, et al. The RAST server: rapid annotations using subsystems technology.
BMC Genomics 2008 9:75. [4] Overbeek R, Olson R, Pusch GD, Olsen GJ, Davis JJ, Disz T, et al. The SEED and the rapid annotation of microbial genomes using subsystems technology (RAST). Nucleic Acids Res 2014 42:D206 14.
[5] Meyer F, Paarmann D, D Souza M, Olson R, Glass EM, Kubal M, et al. The metagenomics RAST server a public resource for the automatic phylogenetic and functional analysis of metagen- omes. BMC Bioinformatics 2008 9:386. [6] Wortman J, Giglio M, Creasy H, Chen A, Liolios K. A data analysis and coordination center for the human microbiome project.
Genome Res 2010 11:O13. [7] Snyder EE, Kampanya N, Lu J, Nordberg EK, Karur HR, the VBI pathosystems resource Shukla M, et al. PATRIC: integration center.
Nucleic Acids Res 2007 35:D401 6.
[8] Wu L, Sun Q, Sugawara H, Yang S, Zhou Y, McCluskey K, et al. Global catalogue of microorganisms (gcm): a comprehensive database and information retrieval, analysis, and visualization system for microbial resources.
BMC Genomics 2013 14:933. [9] Liu D, Liu QH, Wu LH, Liu B, Wu J, Lao YM, et al. Website for avian ﬂu information and bioinformatics. Sci China C Lift Sci 2009 52:470 3.
[10] Liu D, Liu L, Guo G, Wang W, Sun Q, Parani M, et al. BOLDMirror: a global mirror system of DNA barcode data.
Mol Ecol Resour 2013 13:991 5.
BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.19 2010, pages 2474 2476 doi:10.1093/bioinformatics/btq452 Advance Access publication August 10, 2010 Genetics and population analysis Genevar: a database and Java application for the analysis and visualization of SNP-gene associations in eQTL studies Tsun-Po Yang1, Claude Beazley1, Stephen B. Montgomery1,2, Antigone S. Dimas1,2,3, Maria Gutierrez-Arcelus2, Barbara E. Stranger1,4, Panos Deloukas1 and Emmanouil T. Dermitzakis1,2, 1Wellcome Trust Sanger Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1HH, UK, 2Department of Genetic Medicine and Development, University of Geneva Medical School, Geneva CH-1211, Switzerland, 3Wellcome Trust Centre for Human Genetics, Oxford OX3 7BN, UK and 4Division of Genetics, Department of Medicine, Harvard Medical School, Brigham and Women s Hospital, Boston, MA 02115, USA Associate Editor: Martin Bishop ABSTRACT Summary: Genevar (GENe Expression VARiation) is a database and Java tool designed to integrate multiple datasets, and provides analysis and visualization of associations between sequence variation and gene expression.
Genevar allows researchers to investigate expression quantitative trait loci (eQTL) associations within a gene locus of interest in real time.
The database and application can be installed on a standard computer in database mode and, in addition, on a server to share discoveries among afﬁliations or the broader community over the Internet via web services protocols.
Availability: http://www.sanger.ac.uk/resources/software/genevar Contact: emmanouil.dermitzakis@unige.ch Received on May 12, 2010 revised on July 22, 2010 accepted on August 4, 2010 1 INTRODUCTION Expression quantitative trait loci (eQTL) mapping, where gene expression proﬁling is treated as a phenotypic trait in genome-wide association studies (GWAS), has successfully been employed to uncover genetic variants that inﬂuence expression variation in recent studies (Dixon et al., 2007 Stranger et al., 2007a). Single-nucleotide polymorphism (SNP) gene associations from eQTL analysis can be investigated in populations (Stranger et al., 2007b) or among tissue types (Dimas et al., 2009 Heinzen et al., 2008). In addition to genome-wide eQTL identiﬁcation, combinations of eQTLs and lead SNPs identiﬁed by GWAS have been provided to interrogate the mechanisms underlying disease susceptibility at speciﬁc loci (Grundberg et al., 2009 Nica et al., 2010 Zeller et al., 2010). However, an analytical and visualization tool, together with a structured repository for multiple datasets, is still needed to facilitate the investigation of loci of interest and to share data publicly and among collaborators.
Here, we present Genevar, a database and Java tool designed to provide: (i) data warehousing (ii) real-time computation of correlation signiﬁcance (iii) visualization of mapping results in a user-friendly interface and (iv) an added web services platform   To whom correspondence should be addressed.
that is implemented as a bridge between the server and multiple users.
Genevar allows published data to be visually accessible in a secure fashion, without the need for users to download raw data.
Through interactive analysis pipelines, researchers are able to rapidly investigate, for instance, cis-acting eQTLs at the locus of interest.
Complementing already available standalone tools (Chen et al., 2009 Ge et al., 2008), a database-centric architecture enables Genevar to perform complex queries on-the-ﬂy and does not have a high memory requirement for prior reading in large-scale datasets. Furthermore, exploiting the convenience of web-based (Wang et al., 2003 Zou et al., 2007) and web-launch (Mueller et al., 2005) tools, a Java interface was developed that connects to both database and web services.
The main advantage of this system design is that users can switch between public services and local data on the same interface.
Default services at the Sanger Institute currently contain gene expression proﬁling and genotypic data from the following two datasets: lymphoblastoid cell lines from eight HapMap3 populations (824 individuals, unpublished data) and three cell types derived from umbilical cords of 75 Geneva GenCord individuals (Dimas et al., 2009). 2 FEATURES Genevar has two main functionalities in cis-eQTL analysis: (i) identifying eQTLs in genes of interest, and (ii) observing SNP gene associations surrounding SNPs of interest (Fig. 1). Additional features include SNP probe association plots and external links to three major genome browsers.
Either cis- or trans- eQTLs can be plotted in the SNP probe association plot module.
Mapping results are listed in tree nodes in a structural manner, and information can be saved as PNG diagrams or exported as tab-delimited lists for further use in presentations or publications.
Genevar is compatible with PLINK (Purcell et al., 2007) genotype data formats and any tab-delimited expression/genotyping ﬁle in our format.
After uploading datasets onto the database, Genevar presents expression proﬁling data and individual genotypes in two cataloged management panels.
Once a group of datasets is selected in the follow-up analysis pipelines, the software automatically prompts available expression genotype pairs for the user to choose from.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:57 28/8/2010 Bioinformatics-btq452.tex] Page: 2474 2474 2476  A B Genevar Fig.1.
Results of Genevar: a scatter plot represents observed eQTLs in a 2 Mb window centering the GBP3 locus in HapMap3 CHB (A), and a line chart illustrates observed SNP gene associations in a 2 Mb region surrounding rs13277113 SNP in eight HapMap3 populations (B). the signiﬁcance of Spearman s rank correlation coefﬁcient is performed to estimate the strength of relationship between alleles and gene expression intensities, linear regression is also used to model the relationship between the two variables.
To test the relationship, a t-statistic is employed with n 2 degrees of freedom for both correlation and regression analysis (Stranger et al., 2007b). The software allows the user to adjust the window size centering on the gene/SNP of interest (e.g. 2 Mb) and user- deﬁned P-value threshold (e.g. P 0.001) for the featured cis-eQTL analysis.
Alternatively, non-parametric permutation P-values are also provided in the subsequent association plot module to further evaluate the signiﬁcance of nominal P-values.
In order to construct a distribution of the test statistic, under the null hypothesis of no SNP probe associations, expression intensities are randomly re-assigned to individuals genotypes, then correlation coefﬁcient and statistical signiﬁcance are re-computed for the relabeled traits, and this procedure is repeated 10 000 times (Stranger et al., 2005). We recommend users to launch Genevar via Java Web Start from our homepage for the most up-to-date version.
After launching, Genevar is initially in web services mode connecting to the Sanger Institute.
The user can then make another services connection to afﬁliated institutes, or switch to database mode connecting directly to user s local database.
Genevar can be run completely ofﬂine in database mode as there is no communication between the Java interface and Sanger server.
Future work will include modiﬁed visualization for displaying next-generation sequence data, e.g. RNA-Seq (Montgomery et al., 2010) and implementation of methylation modules to interrogate epigenomic data.
3 IMPLEMENTATION This approach to relational database design is an attempt to systematically decompose traditional ﬂat ﬁles, which are one record per line and have no structural relationships between the records, into grouped dimension tables and to reduce data redundancy.
A normalized and structured repository is suitable to warehouse all kinds of data format regardless of the ﬁle size and ﬁeld numbers.
Most importantly, the advantage of using database indexing on expression and genotype fact tables highly stabilize retrieval performance with the subsequent but reasonable cost of slower uploads and increased disk space.
The only limitation when the datasets grew would be the storage space as this is a trade-off for query speed.
To maximize the potential of Genevar as a platform shared among afﬁliations, Genevar has been extended to interact with web services protocols to enhance data security the database schema will be deployed behind and protected by the ﬁrewall, whereas only a secure frontend webpage acting as a middle layer will be accessible to the user over the Internet.
Genevar uses Hibernate library (http://www.hibernate.org) to map object-oriented models onto MySQL relational database tables (http://www.mysql.com) in the back-end, and acquires Apache CXF framework (http://cxf.apache.org) to wrap up database queries and business logics into middle-layer services.
Finally, a Tomcat server (http://tomcat.apache.org) is used to provide services in the front-end.
For a standalone database-mode Genevar, only a MySQL database is required to be installed on user s local machine.
Association results are visualized in genomic views by JFreeChart library (http://www.jfree.org/jfreechart/). A gene-centered scatter plot represents observed SNP gene associations around genes of interest, and a SNP-centered line chart illustrates observed eQTLs surrounding SNPs of interest (Fig. 1). Tested on a 1.6 GHz Pentium Centrino laptop with 1 GB of RAM, Genevar was able to upload a 75 23k expression dataset onto the database and built up indexes in 1 min another 23 min were required for the 75 400k genotype ﬁle. Once it is uploaded, Genevar can fetch per SNP probe pairs from these 75 individuals in 0.0257 s from the database, and calculates Spearman s rhos and nominal P-values for 486 SNP probe pairs in 3 s. ACKNOWLEDGEMENTS We thank Guillaume Smits and Johan Rung (EMBL-EBI) for their suggestions on improving the functionalities.
We also thank Richard 2475 [13:57 28/8/2010 Bioinformatics-btq452.tex] Page: 2475 2474 2476  T.-P.Yang et al. Jeffs, James Smith, Paul Bevan (Sanger Webteam) and Andrew Bryant (Database Team) for helpful support on this project.
Funding: Wellcome Trust and Louis-Jeantet Foundation.
Conﬂict of Interest: none declared.
REFERENCES Chen,W. et al. (2009) GWAS GUI: graphical browser for the results of whole- genome association studies with high-dimensional phenotypes. Bioinformatics, 25, 284 285.
Dimas,A.S. et al. (2009) Common regulatory variation impacts gene expression in a cell type-dependent manner.
Science, 325, 1246 1250.
Dixon,A.L. et al. (2007) A genome-wide association study of global gene expression.
Nat. Genet., 39, 1202 1207.
Ge,D. et al. (2008) WGAViewer: software for genomic annotation of whole genome association studies.
Genome Res., 18, 640 643.
Grundberg,E. et al. (2009) Population genomics in a disease targeted primary cell model.
Genome Res., 19, 1942 1952.
Heinzen,E.L. et al. (2008) Tissue-speciﬁc genetic control of splicing: implications for the study of complex traits.
PLoS Biol., 6, 2869 2879.
Montgomery,S.B. et al. (2010) Transcriptome genetics using second generation sequencing in a Caucasian population.
Nature, 464, 773 777.
Mueller,M. et al. (2005) eQTL Explorer: integrated mining of combined genetic linkage and expression experiments.
Bioinformatics, 22, 509 511.
Nica,A.C. et al. (2010) Candidate causal regulatory effects by integration of expression QTLs with complex trait genetic associations.
PLoS Genet., 6, e1000895. Purcell,S. et al. (2007) PLINK: a tool set for whole-genome association and population- based linkage analyses.
Am.J. Hum.
Genet., 81, 559 575.
Stranger,B.E. et al. (2005) Genome-wide associations of gene expression variation in humans.
PLoS Genet., 1, e78. Stranger,B.E. et al. (2007a) Relative impact of nucleotide and copy number variation on gene expression phenotypes. Science, 315, 848 853.
Stranger,B.E. et al. (2007b) Population genomics of human gene expression.
Nat. Genet., 38, 1217 1224.
Wang,J. et al. (2003) WebQTL: web-based complex trait analysis.
Neuroinformatics, 1, 299 308.
Zeller,T. et al. (2010) Genetics and beyond the transcriptome of human monocytes and disease susceptibility.
PLoS One, 5, e10693. Zou,W. et al. (2007) eQTL Viewer: visualizing how sequence variation affects genome- wide transcription.
BMC Bioinformatics, 8, 7 11.
2476 [13:57 28/8/2010 Bioinformatics-btq452.tex] Page: 2476 2474 2476
BIOINFORMATICS Vol.26 ECCB 2010, pages i540 i546 doi:10.1093/bioinformatics/btq391 BioXSD: the common data-exchange format for everyday bioinformatics web services Matúš Kalaš1,2, , Pål Puntervoll1, Alexandre Joseph3, Edita Bartaševi ˇci ut e4, Armin Töpfer1,5, Prabakar Venkataraman1, Steve Pettifer6, Jan Christian Bryne1,2, Jon Ison7, Christophe Blanchet3, Kristoffer Rapacki4 and Inge Jonassen1,2 1Computational Biology Unit, Bergen Center for Computational Science, Uni Research, 5008 Bergen, Norway, 2Department of Informatics, University of Bergen, 5008 Bergen, Norway, 3Université Lyon 1 CNRS, UMR 5086 IBCP, Institut de Biologie et Chimie des Protéines, 69367 Lyon Cedex 07, France, 4Center for Biological Sequence Analysis, Department of Systems Biology, Technical University of Denmark, 2800 Kongens Lyngby, Denmark, 5Institute for Bioinformatics, Center for Biotechnology, Bielefeld University, 33594 Bielefeld, Germany, 6School of Computer Science, The University of Manchester, Manchester, M13 9PL, UK and 7European Bioinformatics Institute, EMBL, Wellcome Trust Genome Campus, Hinxton, Cambridge, CB10 1SD, UK ABSTRACT Motivation: The world-wide community of life scientists has access to a large number of public bioinformatics databases and tools, which are developed and deployed using diverse technologies and designs.
More and more of the resources offer programmatic web-service interface.
However, efﬁcient use of the resources is hampered by the lack of widely used, standard data-exchange formats for the basic, everyday bioinformatics data types.
Results: BioXSD has been developed as a candidate for standard, canonical exchange format for basic bioinformatics data.
BioXSD is represented by a dedicated XML Schema and deﬁnes syntax for biological sequences, sequence annotations, alignments and references to resources.
We have adapted a set of web services to use BioXSD as the input and output format, and implemented a test- case workﬂow. This demonstrates that the approach is feasible and provides smooth interoperability. Semantics for BioXSD is provided by annotation with the EDAM ontology.
We discuss in a separate section how BioXSD relates to other initiatives and approaches, including existing standards and the Semantic Web.
Availability: The BioXSD 1.0 XML Schema is freely available at the Creative Commons BY-ND 3.0 license.
The http://bioxsd.org web page offers documentation, examples of data in BioXSD format, example workﬂows with source codes in common programming languages, an updated list of compatible web services and tools and a repository of feature requests from the community.
Contact: matus.kalas@bccs.uib.no developers@bioxsd.org support@bioxsd.org http://www.bioxsd.org/BioXSD-1.0.xsd under 1 INTRODUCTION The bioinformatics community shares a common feature with a global business corporation, namely the diversity of IT systems.
A global corporation owns a myriad of IT solutions belonging to its smaller or bigger sub-companies, implemented in diverse ways and covering diverse aspects and business areas of the corporation.
To achieve efﬁciency within the corporation or consortium, all the IT systems must be able to work together: to inter-operate.
To whom correspondence should be addressed.
In bioinformatics, we do not have any corporate management to force standards for interoperability. But it is clear that the bioinformatics community would beneﬁt greatly from an IT infrastructure that allows more efﬁcient use of biological data and computational resources, in order to support new exploration and discoveries.
Since the scientiﬁc community lacks a centralized authority, the community must develop its standards within collaborative efforts.
In his visionary comment, Lincoln Stein called for standardization in bioinformatics, suggesting web services (http://www.w3.org standards/webofservices) as the unifying platform for programmatic interfaces to tools and data sources (Stein, 2002). Nowadays, the ELIXIR project chooses SOAP web services for programmatic access to all considered bioinformatics databases and tools (http:/ www.elixir-europe.org/page.php page=wp7). The Web Service Interoperability Organisation (WS-I, http://ws-i.org), supported by the main IT companies, constrains even more strictly the W3C s SOAP-service standards in order to maximize interoperability among the web services and the web-service programmatic libraries.
The EMBRACE project (European Model for Bioinformatics Research and Community Education) has developed guidelines for providing data sources and computational tools that are globally interoperable on the web of services.
The guidelines recommend WS-I compliant web services with document/literal wrapped SOAP binding (Pettifer et al., 2010 Stockinger et al., 2008 technical details in http://www.embracegrid.info/page.php page=tech_documents). Even while following the W3C and WS-I standards, the practical interoperability within the ﬁeld of bioinformatics web services is compromised by the incompatibility or inconsistency of input and output formats of different services (Hull et al., 2006). Standard exchange formats have been identiﬁed as a necessary key to global interoperation by the ELIXIR and EMBRACE projects, and by the BioHackathon jamboree ( http://hackathon.dbcls.jp). Developing an XML Schema of standard data-exchange formats, called a canonical data model, is typical within IT-system integration in industry, business and public administration.
The standard formats of the exchanged data enable web-service developers to use them as the input and output data formats, eliminating the need to deﬁne their own formats and thus saving development and maintenance  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:55 28/8/2010 Bioinformatics-btq391.tex] Page: i540 i540 i546  A B C BioXSD Fig.1.
Three different scenarios of sending data from an output of one web service to an input of another web service.
(A) Plain textual data inside SOAP messages.
Proprietary parsing and serialization, or shims are necessary.
(B) Different XML formats of output of the ﬁrst service and input of the second.
Translation of data is in general easier but still necessary.
(C) Both services using the same standardized exchange format.
No transformation is necessary and data ﬂow smoothly from one service to another.
costs.
This is especially useful when services are developed in a WSDL-centric fashion, as recommended by EMBRACE.
The biggest advantage of the common standard formats is however for the users of services.
The common, canonical data model gives users the ability to easily mix-and-match diverse web services into custom analysis pipelines: workﬂows. Workﬂows are developed using either ordinary programming or scripting languages, or specialized workﬂow tools like Taverna (Hull et al., 2006) or workﬂow languages like BPEL (http://docs.oasis-open.org/wsbpel/2.0/wsbpel-v2.0.html). The use of standard canonical formats diminishes the need to translate data between different formats of the output and input of the various tools.
Such translations normally require custom parsers or development of predeﬁned transformers, referred to as shims (Hull et al., 2004). The various scenarios of mix-and-matching different web services are illustrated in Figure 1.
A standard, canonical format of the exchanged data simpliﬁes workﬂow development, and reduces the effort of developing and tailoring analysis pipelines.
Standard exchange formats aim to save time and resources, and decrease the needs for specialized personnel with advanced programming skills.
Data formats can be deﬁned in several alternative ways.
XML Schema (XSD, http://www.w3.org/standards/techs/xmlschema) is a formal language for deﬁning data types and their formats.
XSD deﬁnes the structure of data objects, restricts the allowed values of atomic data and deﬁnes specialization and inclusion relations on the data types.
XML Schema is primarily used to deﬁne the structure of XML documents, SOAP messages, or, from the web- service perspective, the data objects.
Not limited to deﬁne XML formats, XSD can also be used to deﬁne object data models for object-oriented programming languages.
The source code needed to deﬁne the data objects in the chosen programming language can be automatically generated from the particular Schema using ordinary tools.
When used for so-called marshalling and unmarshalling of objects between their XML-serialized representation and the computer-memory representation, such a framework is called a data binding.
Due to a Schema and the data binding, we can abstract from the textual XML appearance of the exchanged data, and regard it as a medium for directly transferring data objects.
A number of industry-supported tools are available for parsing, validating or translating XML data corresponding to a deﬁned XML Schema.
If a Schema describes the data format at an appropriately detailed level, these widely available frameworks provide us with useful functionality.
This includes automatic validation of the data ensuring that our tools receive only valid inputs, thus improving the security for the providers and eliminating senseless invocations and the need for validating the input in the source code.
Data validation thus lowers the burden for the tool provider and increases the usability for the user.
Detailed Schemas eliminate the need for writing and maintaining special parsers for each type and format of data, making the inputs and outputs parsed on arrival , unmarshalled into the data-binding objects.
Selection and projection, or translation among different formats is simpliﬁed. A detailed XSD makes it possible to semantically annotate details of data formats using the SAWSDL standard (Semantic Annotation for WSDL and XML Schema, http://www.w3.org/standards/techs/sawsdl). The detailed XSD can also help develop or even generate comprehensive graphical user interfaces for editing data and invoking services.
In addition, very efﬁcient compression of data can be achieved by Schema-aware compression tools which use the constraining XML Schema to provide the minimum binary encoding of the data.
Examples of such frameworks are emerging (Augeri et al., 2007), and their adoption will be crucial for the increasingly data- intensive bioinformatics, for instance related to high-throughput sequencing.
Initiated by the EMBRACE project partners, we have developed BioXSD. BioXSD is the candidate for a reasonably lightweight, but formal and detailed, standard XML exchange format of commonly used, everyday bioinformatics data.
BioXSD makes web services i541 [10:55 28/8/2010 Bioinformatics-btq391.tex] Page: i541 i540 i546  M. Kalaš et al. easily interoperable and markedly simpliﬁes the construction of workﬂows. The following section brieﬂy summarizes related work and approaches, and discusses their relationship with BioXSD. Section 3 describes the BioXSD development and summarizes the design principles.
Section 4 describes the developed BioXSD formats and their highlights.
It also presents a feasibility test of BioXSD adoption by web services and a case-study client workﬂow. Section 5 concludes the article.
2 RELATED EFFORTS AND APPROACHES In this section, we introduce previous efforts, related and alternative approaches and discuss their relationship with BioXSD. 2.1 Specialized standard formats Standardized XML data-exchange formats do exist for specialized sub-domains of bioinformatics and the data types in focus.
Examples are SBML for systems-biology models (Hucka et al., 2003), PDBML for structural bioinformatics (Westbrook et al., 2005), The HUPO PSI s Molecular Interaction format ( MIF  Hermjakob et al., 2004), or phyloXML (Han and Zmasek, 2009) and NeXML (http://www.nexml.org) for phylogenetic data.
The Minimum Information standards for different ﬁelds of experimental molecular biology often include an XML Schema of the data-exchange format: for example MAGE-ML in MIAME (Spellman et al., 2002) or GCDML in MIGS/MIMS (Kottmann et al., 2008). The scope of BioXSD is to offer standard exchange formats for the common bioinformatics data not covered by these specialized, mostly heavyweight standards.
We encourage using the big standards for data exchange always when applicable, and BioXSD for the exchange of common, everyday bioinformatics data like sequences, alignments, references and uniﬁed generic sequence annotations.
2.2 XML formats for common bioinformatics DAS (Distributed Annotation System Prli c et al., 2007) and BioMoby (Wilkinson and Links, 2002) are web-based, service- oriented bioinformatics infrastructures, enabling interoperability across distributed resources.
To ensure the interoperability within the infrastructure, they include a set of common XML formats.
In BioMoby, it is an open library of data types and formats, and in DAS it is a set of lightweight formats for sequences and annotation data.
The family of HOBIT XML Schemas developed by the Helmholz Open BioInformatics Technology project (HOBIT) has deﬁned common XML formats of everyday bioinformatics data types.
These are used among web services within a set of German bioinformatics research institutes (Seibel et al., 2006). HOBIT XML deﬁnes a substantial number of types spread over multiple XSDs, which are programmatically accessible using a dedicated Java library (BioDOM). The CBS Common Data Types have been in use among web services at the Center for Biological Sequence analysis (CBS) at the Technical University of Denmark and at EMBL, Heidelberg (http://www.cbs.dtu.dk/ws/doc/datatypes.php). They constitute a couple of including ones corresponding to the FASTA format and a subset of General Feature Format (GFF, http://www.sanger.ac.uk/resources/software/gff). The CBS-EMBL data model has served as one of the starting points for the development of BioXSD. lightweight XML data formats 2.3 Semantic web Semantic web standards offer alternative languages to deﬁne a in a formal way.
For example BioPAX (http:/ data format www.biopax.org), the common exchange format for biological pathways, is deﬁned in OWL (http://www.w3.org/standards/techs owl). Some semantic web-service-oriented architectures propose using a canonical reference ontology instead of a canonical XML Schema, and perform lifting and lowering of the output and input data through data individuals of the reference ontology.
However, to achieve the main goal of BioXSD and EMBRACE, namely interoperable programmatic access, we have opted for a combined approach of a pure XML Schema annotated by a data-type ontology using SAWSDL. This ensures the practical usability by presently mature common tools proven by an extensive industrial use, including the simple interoperability with programming languages using the ordinary SOAP libraries.
For similar reasons, we do not model data annotations and resources using RDF (http:/ www.w3.org/standards/techs/rdf). RDF use will be considered for future versions of BioXSD after evaluating the progressed maturity and user-friendliness of the necessary frameworks.
References to resources in BioXSD are modeled using pure XML-Schema elements, though in a formalized and at the same time versatile way.
Strict formalization of the references enables traceability within the semantic web of data, using URI and preferably dereferenceable URL or REST-service links.
Versatility of representation in BioXSD enables also formal references to resources that are volatile, do not offer stable URIs, or offer data only through a SOAP-service interface.
This way, BioXSD supports formal links to any database, public or private, taxonomies and ontologies, allowing any cross- references and annotations by controlled vocabularies, for example the Sequence (Eilbeck et al., 2005), Gene (Ashburner et al., 2000) or BioSapiens Protein Feature Ontology (Reeves et al., 2008) or any future ontologies. 2.4 Ontology of bioinformatics data types The EMBRACE Data And Methods ontology (EDAM Pettifer et al., 2010, http://edamontology.sourceforge.net), developed within the EMBRACE project, is a comprehensive controlled vocabulary of bioinformatics-speciﬁc data types, computational methods and data sources.
BioXSD is closely coordinated with the EDAM initiative.
BioXSD types and applicable local elements are annotated by EDAM terms using the SAWSDL standard.
Model references to EDAM serve as formal semantics of the syntactic BioXSD types, in addition to the detailed, but only human-understandable semantics in the documentation.
BioXSD thus constitutes a collection of ready- made, semantically annotated building blocks for the web-service interfaces.
The architecture of the EMBRACE standards is shown in Figure 2.
When EDAM is used to annotate multiple XML Schemas deﬁning different formats, it can help in matching and translating, thus enabling interoperability across formats.
In addition to data types, EDAM is used to annotate data and ontology resources and numerical scores within BioXSD, and is recommended to be used to annotate computational methods and other resources within the BioXSD-formatted data.
Using SAWSDL, any BioXSD types or BioXSD-typed variables can be in the same way additionally annotated by any other model references to existing data-type ontologies or future models.
i542 [10:55 28/8/2010 Bioinformatics-btq391.tex] Page: i542 i540 i546  BioXSD Fig.2.
Strategy to reach maximum interoperability, as recommended by EMBRACE.
Strategy comprises the technology for implementing web-service interfaces (WS-I compliant, document/literal wrapped SOAP) the common exchange format for basic bioinformatics data (BioXSD) the semantic annotation format (SAWSDL model reference) and the ontology of bioinformatics-speciﬁc computational methods, data types and resources (EDAM). The common data format (BioXSD) is essential for increasing interoperability within the programmatic access and for construction of workﬂows. The common vocabulary of meanings (EDAM) is essential when discovering and matching services, and doing additional semantic reasoning.
3 METHODS BioXSD has been designed by analyzing the existing approaches, tools and data.
Focused on existing web services and bioinformatics tools, we analyzed their inputs and outputs.
Most important for interoperability, usability and maintenance is precise modeling of data that appears both as output and as input of tools.
BioXSD models the common, everyday-bioinformatics data types, for which no standard exchange formats have been widely adopted.
We have speciﬁed formats for biological sequences, sequence annotations and alignments and references to data and resources.
These common data types cover inputs and outputs of about 2/3 of web services in the EMBRACE Registry (Pettifer et al., 2009). We analyzed legacy textual formats, modern tabular and XML formats, and related life-scientiﬁc ontologies. Some inconveniences of the existing solutions have been taken into account, keeping contact with bench molecular biologists and focusing on the present requirements of their research.
These resulted for example in generalization of some data types, formalized handling of volatile resources and provenance and inclusion of structured optional metadata, including the possibility to annotate the metadata by externally controlled meaning (deﬁned by any external taxonomy or ontology). Technical requirements for BioXSD included WS-I compliance and interoperability with the existing frameworks: the ordinary SOAP and XML/XSD libraries.
Compatibility with such libraries for the main programming languages is crucial for successful provider-side development of web services and for client-side usability.
We have used a pure XML Schema with a constrained subset of its features, tested with a number of the most common SOAP and XML/XSD frameworks.
BioXSD has been designed as a detailed XML Schema, allowing in-depth data validation and semantic annotation.
At the same time, a requirement has been to limit the BioXSD to a lightweight to medium-heavy model, and in particular to ensure that its usage will be as easy as possible.
BioXSD has been developed in an iterative fashion, passing through a number of refactorings. Iterations resulted in prototypes and later beta- versions which were revised and tested by a group of service users and providers representing the stakeholders.
Future versioning and maintenance process is designed as follows: a necessary major rearrangement that demands changes in the providers and clients software will constitute a major release of BioXSD. We hope for as rare major rearrangements as possible, but changes are unavoidable while keeping progress.
A major release is identiﬁed by two major version numbers, and deﬁnes its own namespace and schema location: for example http://bioxsd.org/BioXSD-1.0 and http://bioxsd.org/BioXSD-1.0.xsd. This way, all the services that use the obsolete version will stay fully functional, while their providers will be informed about the new major release.
More frequent, intermediate minor releases (updates) will be identiﬁed by a third version number (1.0.5) and will be designed not to break the interoperability of the canonical model, and thus not to demand any changes in the software for providers and clients.
To prove the feasibility of our effort, we have adapted a number of web services provided by different institutes, so that they use BioXSD for their inputs and outputs.
These services have been developed in Python using the Zolera ZSI library, in Perl using SOAP::Lite and XML::Compile and in Java using Axis2 with XMLBeans data binding.
We implemented a client worﬂow in Java to test the interoperability. As more services will adopt BioXSD, more example client workﬂows in more diverse programming or workﬂow languages will appear on the http://bioxsd.org web page.
BioXSD is annotated with EDAM ontology terms using SAWSDL. Within the web- service and workﬂow implementations, we have tested the feasibility of user support and consulting.
4 RESULTS AND DISCUSSION 4.1 BioXSD 1.0 BioXSD 1.0 is the ﬁrst stable version of the canonical data model for everyday bioinformatics. It deﬁnes exchange formats for the most common data types: biological sequences, sequence alignments, sequence annotation and references to data, resources and vocabularies.
As supporting deﬁnitions, it offers a safe URI format, a recommended set of restricted numeric and string types, formats of the main accession numbers, as well as recommended qualitative values and identifying names, for example for the main public data sources (values for which stable, widely used vocabularies are still missing). In the following paragraphs, we brieﬂy describe features of the most important BioXSD data formats.
BioXSD includes type deﬁnitions for pure 4.1.1 Sequence strings of one-letter-coded sequences, sequence records with additional metadata to identify and describe the sequence and a reference to a sequence in an external resource.
The schema includes i543 [10:55 28/8/2010 Bioinformatics-btq391.tex] Page: i543 i540 i546  M. Kalaš et al. Table 1.
Optional elements for sequence metadata Element species Description Identiﬁes the biological source of the sequence: typically an organism, but possibly also a sample, tissue, cell line, individual, conditions or a geographic location.
The generic species type may formally refer to any taxonomy and supports meta- and individual genomics data customName customNote A name to identify the sequence for a human user A textual note for a human user, if necessary.
(not to formalReference be parsed) Identiﬁes the data source of the sequence: typically a public or private database or data set.
Can contain an accession, database identiﬁcation, provenance data (version, date), isoform. Can also identify a position of the sequence in a super-sequence or a genome.
May include an explicit super-sequence, necessary in special cases translationData Element to hold data for forward or backward translation, if necessary.
May identify for example a genetic code and translational phase of an incomplete coding sequence distinguished types for generic biosequence , nucleotide or amino- acid sequence, unambiguous or general.
The optional metadata of a BioXSD sequence record are listed in Table 1.
Figure 3A D shows a diagram of NucleotideSequenceRecord, a deﬁned restriction of GeneralAminoacidSequence and examples of BioXSD-formatted sequence records.
BioXSD offers a uniﬁed format for 4.1.2 Sequence alignment global and local, pair-wise and multiple alignments.
It enables optimization in case of very long or very many sequences and supports frame shifts and direction.
The aligned sequence records remain intact (no gap characters are inserted into the sequences), and can thus be directly used for further computation.
This is also important for provenance, together with metadata identifying the methods used to construct the alignment.
Any scores can be stated for the alignment and for each single aligned sequence.
4.1.3 Sequence annotation The AnnotatedSequence is a versatile format for describing any kind of feature annotations of a biological sequence or genome.
Nucleotide and protein sequences can be annotated with non-positioned and positioned features.
Features can be spread over multiple segments of a sequence, and can be related to each other.
Wrapping in blocks with local dependencies enables consistent merging of annotations.
Features can contain a wide scale of formalized metadata, including cross-references and inter-feature relations with semantic meaning, sequence variation, alignment, experimental or predictive evidence with generic annotated scores, verdicts, literature citations and reliability.
The format supports controlled ontological meanings of types of features, their relations with cross-referenced data and terms and of types of prediction tools and scores.
Computational methods can additionally contain references to literature and web services.
The BioXSD format for sequence annotations has been designed to fully cover the expressive power of GFF3 (http://www.sequenceontology.org/gff3.shtml) and DAS features.
An additional expressiveness enables loss-less  i544 A B C D Fig.3.
BioXSD format of a sequence record including the sequence and optional metadata.
(A) Diagram showing the structure of the sequence record (example type is specialized towards nucleotides). (B) Restricting pattern of the sequence string (example is a general amino-acid sequence type allowing ambiguous and additional residues: Pyl and Sec). (C) Example of a simple sequence record in BioXSD. (D) Example of a BioXSD sequence record with more metadata.
Figure highlights which metadata elements are textual and focus purely on human understandability, and which are formally structured allowing more automatic usage by computer applications.
exchange of for example UniProt features (The UniProt Consortium, 2010), data from specialized feature databases such as, for example, phiSITE (Klucar et al., 2010) or outputs of feature-prediction and similarity-search tools.
Examples of diverse feature data modeled in BioXSD can be found at http://bioxsd.org, together with an extensive documentation of all deﬁned types.
4.2 Existing implementations A number of web services have been adapted to use the common BioXSD model for the formats of their inputs and outputs.
Their implementation supplied iterations of BioXSD development with [10:55 28/8/2010 Bioinformatics-btq391.tex] Page: i544 i540 i546  Table 2.
BioXSD-compatible web services by the time of article submission Service Provider Function BLAST IBCP, France ClustalW IBCP, France Similarity search (Altschul et al., 1990) Multiple sequence alignment (Thompson et al., 1994) GorIV IBCP, France Prediction of secondary structure of BLAST MaxAlign BCCS, Norway CBS, Denmark Similarity search Optimization of multiple sequence proteins (Garnier et al., 1996) alignment (Gouveia-Oliveira et al., 2007) ProP CBS, Denmark Prediction of pro-peptide cleavage sites NetNES CBS, Denmark Prediction of nuclear export signals (Duckert et al., 2004) (la Cour et al., 2004) Updated list with links to service descriptions is available at http://bioxsd.org. community feedback, and has proven the feasibility of the proposed solution.
Experience has shown that as soon as services deal with a detailed, structured XML, it is easy to adapt the format.
The ﬁrst step of moving from plain text to structured XML, if necessary, is the one requiring considerable effort, but has to be done only once.
Table 2 lists BioXSD-compatible web services by the time of writing this article.
An updated list is maintained on the BioXSD web page.
An example workﬂow, combining diverse web services from different providers, is schematically outlined in Figure 4.
The workﬂow performs a routine bioinformatics analysis.
Given a query sequence, similar sequences are searched and fetched from a database.
These sequences are then brought together in a multiple sequence alignment.
For each of the sequences, annotations with features of interest are predicted or fetched.
Thanks to the common, detailed format of the exchanged data, the interoperability has highly improved resulting in smooth orchestration of the services in the workﬂow. Writing the source code of such a workﬂow in a common programming language has been markedly simpliﬁed. Another advantage when using BioXSD is the easiness with which a service in a workﬂow can be changed to a different service doing the same task (using a different algorithm, database or being hosted elsewhere). In the example, we can change the BLAST service for another BLAST or a different similarity search.
The alignment service can be changed to any other BioXSD-compatible multiple- alignment tool.
We can add any additional feature-prediction tools or data sources and merge the annotations without an effort.
Changing or adding services that have the same programmatic interface in the form of BioXSD, requires only minimal changes in the script or workﬂow. Full working source code of the example workﬂow can be found among example workﬂows on the BioXSD web page (http://bioxsd.org). 5 OUTLOOK BioXSD is a candidate for the standard exchange formats of everyday bioinformatics data.
It enables automatic validation and sustainably decreases the amount of necessary programming both on the side of service users and the service providers.
Users of BioXSD- compatible web services gain most from the smooth interoperability, allowing them to combine services easily and to focus purely on BioXSD Fig.4.
An example bioinformatics workﬂow (analysis pipeline). Blue rectangles are web-service calls, red ovals are data.
Common, standardized BioXSD format of the exchanged data makes sure that there is no additional parsing and transforming of the data necessary between the service calls.
Such web services are smoothly interoperable, allowing users to combine them without any substantial effort.
the scientiﬁc aspects of their analysis.
Providers can use BioXSD directly as the common format whenever applicable, and BioXSD types can be included, extended or restricted as building blocks of other custom or standard data types.
With web services that use and will keep using other formats, BioXSD can serve as a canonical intermediate exchange format, into/from which other formats can be translated.
The tool and database providers are encouraged to include BioXSD as one of the supported formats for input and output data.
Compared to the previous attempts of standardizing everyday bioinformatics formats for web-service data exchange, BioXSD offers more expressive power thanks to the extensive structured metadata including formal annotation by external biological ontologies throughout the format.
It enables provenance, and modeling of data from emerging research ﬁelds like meta- and individual genomics.
BioXSD itself is annotated by the EDAM ontology for discovery and interoperability of web services and data formats.
Of highest importance for practical adoption within i545 [10:55 28/8/2010 Bioinformatics-btq391.tex] Page: i545 i540 i546  M. Kalaš et al. the community, BioXSD is compatible with the widely available, ordinary tools for programmatic access to web services.
The format exchange-data standardization of for basic bioinformatics data types is an initiative coming from within the scientiﬁc community.
It can reach its goal of becoming the standard only with active participation of the community itself.
BioXSD development has been, and should further be done, in form of an open but organized collaboration.
We have been and are further trying to develop a model that is sufﬁciently expressive yet easy to use.
BioXSD is welcoming new features and change requests from the community, for which a submission system has been designed.
If the formats in their current form do not ﬁt some providers or users, they are encouraged to submit their need for a change or addition.
Changes and additions that do not demand rearrangement may be included in an instant update.
Requests that lead to rearrangements of the model will be considered for the next major release.
To successfully establish the World Wide Web of bioinformatics services that use common exchange-data standard is desired by many projects.
Highly important for reaching this goal is to offer service providers the necessary user support and consulting from the consortium responsible for maintenance of the standard.
We offer full user support and consulting, and hope to establish a sustainable consortium which will guarantee the future maintenance of BioXSD. ACKNOWLEDGEMENTS We thank Kjell Petersen, Peter Fischer Hallin and Torbjørn Lium for knowledgeable advice and extensive technical support.
Funding: European Commission within FP6 (grant LHSG-CT-2004- 512092, the EMBRACE project) Research Council of Norway (within eVITA) (grant 178885/V30, eSysbio project, to M.K., P.P., A.T., P.V., I.J.) l Agence Nationale de la Recherche (grant ANR- 06-CIS6-005, HIPCAL project, to A.J.) Villum Foundation (grant for the Center of Disease Systems Biology, to E.B. and K.R.). Conﬂict of Interest: none declared.
REFERENCES Altschul,S.F. et al. (1990) Basic local alignment search tool.
J. Mol. Biol., 215, 403 410.
Ashburner,M. et al. (2000) Gene ontology: tool for the uniﬁcation of biology.
The gene ontology consortium.
Nat. Genet., 25, 25 29.
Augeri,C.J. (2007) An analysis of XML compression efﬁciency. In Proceedings of the 2007 Workshop on Experimental Computer Science.
San Diego, CA, ACM, New York, USA. Duckert,P. et al. (2004) Prediction of proprotein convertase cleavage sites.
Protein Eng. Des. Sel., 17, 107 112.
Eilbeck,K. et al. (2005) The Sequence Ontology: a tool for the uniﬁcation of genome annotations.
Genome Biol., 6, R44. Garnier,J. et al. (1996) GOR secondary structure prediction method version IV.
Meth Enzymol., 266, 540 553 Gouveia-Oliveira,R. et al. (2007) MaxAlign: maximizing usable data in an alignment.
BMC Bioinformatics, 8, 312.
Han,M.V. and Zmasek,C.M. (2009) phyloXML: XML for evolutionary biology and comparative genomics.
BMC Bioinformatics, 10, 356.
Hermjakob,H. et al. (2004) The HUPO PSI s Molecular Interaction format a community standard for the representation of protein interaction data.
Nat. Biotechnol., 22, 177 183.
Hucka,M. et al. (2003) The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models.
Bioinformatics, 19, 524 531.
Hull,D. et al. (2004) Treating shimantic web syndrome with ontologies. In Proceedings of First Advanced Knowledge Technologies Workshop on Semantic Web Services (AKT-SWS04) KMi. The Open University, Milton Keynes,UK. Hull,D. et al. (2006) Taverna: a tool for building and running workﬂows of services.
Nucleic Acids Res., 34, W729 W732. Klucar,L. et al. (2010) phiSITE: database of gene regulation in bacteriophages. Nucleic Acids Res., 38, D366 D370. Kottmann,R. et al. (2008) A standard MIGS/MIMS compliant XML schema: toward the development of the Genomic Contextual Data Markup Language (GCDML). OMICS, 12, 115 121. la Cour,T. et al. (2004) Analysis and prediction of leucine-rich nuclear export signals.
Protein Eng. Des. Sel., 17, 527 536.
Pettifer,S. et al. (2009) An active registry for bioinformatics web services.
Bioinformatics, 25, 2090 2091.
Pettifer,S. et al. (2010) The EMBRACE Web service collection.
Nucleic Acids Res., 38, W683 W688. Prli c,A. et al. (2007) Integrating sequence and structural biology with DAS. BMC Bioinformatics, 8, 333.
Reeves,G.A. et al. (2008) The Protein Feature Ontology: a tool for the uniﬁcation of protein feature annotations.
Bioinformatics, 24, 2767 2772.
Seibel,P.N. et al. (2006) XML schemas for common bioinformatic data types and their application in workﬂow systems.
BMC Bioinformatics, 7, 490.
Spellman,P.T. et al. (2002) Design and implementation of microarray gene expression markup language (MAGE-ML). Genome Biol., 3, research0046.1-0046.9. Stein,L. (2002) Creating a bioinformatics nation.
Nature, 417, 119 120.
Stockinger,H. et al. (2008) Experience using web services for biological sequence analysis.
Brief.
Bioinform., 9, 493 505.
The UniProt Consortium.
(2010) The Universal Protein Resource (UniProt) in 2010.
Nucleic Acids Res., 38, D142 D148. Thompson,J.D. et al. (1994) CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-speciﬁc gap penalties and weight matrix choice.
Nucleic Acids Res., 22(22), 4673 4680.
Westbrook,J. et al. (2005) PDBML: the representation of archival macromolecular structure data in XML. Bioinformatics, 21, 988 992.
Wilkinson,M.D. and Links,M. (2002) BioMOBY: an open source biological web services proposal.
Brief.
Bioinform., 3, 331 341. i546 [10:55 28/8/2010 Bioinformatics-btq391.tex] Page: i546 i540 i546 BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.20 2010, pages 2641 2642 doi:10.1093/bioinformatics/btq437 Systems biology A cell-based simulation software for multi-cellular systems Stefan Hoehme1, and Dirk Drasdo1,2, 1Interdisciplinary Centre for Bioinformatics, University of Leipzig, Germany and 2Institut National de Recherche en Informatique et en Automatique (INRIA) Unit Rocquencourt B.P.105, 78153 Le Chesnay Cedex, France Associate Editor: Trey Ideker Advance Access publication August 13, 2010 ABSTRACT CellSys is a modular software tool for efﬁcient off-lattice simulation of growth and organization processes in multi-cellular systems in 2D and 3D. It implements an agent-based model that approximates cells as isotropic, elastic and adhesive objects.
Cell migration is modeled by an equation of motion for each cell.
The software includes many modules speciﬁcally tailored to support the simulation and analysis of virtual tissues including real-time 3D visualization and VRML 2.0 support.
All cell and environment parameters can be independently varied which facilitates species speciﬁc simulations and allows for detailed analyses of growth dynamics and links between cellular and multi-cellular phenotypes. Availability: CellSys is freely available for non-commercial use at http://msysbio.com/software/cellsys. The current version of CellSys permits the simulation of growing monolayer cultures and avascular tumor spheroids in liquid environment.
Further functionality will be made available ongoing with published papers.
Contact: hoehme@izbi.uni-leipzig.de dirk.drasdo@inria.fr Supplementary information: Supplementary data are available at Bioinformatics online.
Received and revised on June 11, 2010 accepted on July 25, 2010 1 INTRODUCTION Based on the insight that multi-cellular systems are inherently of multi-scale nature, the focus of research in systems biology is currently shifting towards studies of whole cells or populations of cells to complement sequence analysis (Yu et al., 2002), gene expression proﬁles (Missal et al., 2006), research on signal transduction pathways (Swameye et al., 2003) and the analysis of biochemical systems (Calzone et al., 2006). Realistic multi-cellular approaches should permit both, to include models of sub-cellular processes and to simulate up to biologically or clinically relevant population sizes.
For many years, this was hampered by the inherent computational complexity.
However, depending on the level of represented detail the steadily increasing computational power of modern processors (CPUs and GPUs) today permits the simulation of up to several millions of cells by agent-based models (Anderson et al., 2007 Drasdo et al., 2007 and refs.
therein) provided an efﬁcient implementation especially in three dimensions.
Here we present an adaptable software tool (named CellSys) that implements a class of lattice-free agent-based models permitting realistic simulations of tissue growth and organization processes of common experimental settings in vitro, as the growth dynamics in monolayer cultures and multi-cellular spheroids (Drasdo and   To whom correspondence should be addressed.
Hoehme, 2005). It has further recently been used to model liver regeneration (Hoehme et al., 2007, 2010). Therein, the simulation results from CellSys have been directly and quantitatively compared to experimental data, using the same analysis methods for both.
CellSys can be used to predict the experiments that are most informative to identify possible mechanisms on the cell and sub- cell scale to explain a certain multi-cellular phenomenon.
It also has proved useful to guide the development of continuous models where cells are not resolved individually but are represented by locally averaged quantities: Comparing both model types permits identiﬁcation of terms and parameters (Byrne and Drasdo, 2009). 2 SIMULATION MODEL Each individual cell is modeled by an isotropic, elastic and adhesive sphere capable of migration, growth and division.
Cell cell and cell matrix interaction are mimicked by the experimentally validated (Chu et al., 2005) Johnson Kendall Roberts (JKR) model that summarizes deformation, compression and adhesion forces, and displays hysteresis if cells move close to or away from one another.
Model cells are parameterized by the Young modulus, the Poisson ratio, the density of membrane receptors responsible for cell cell and cell substrate adhesion and by its (intrinsic) radius.
Proliferating cells double volume and deform into dumb- bells after a given number of time steps before splitting into two daughter cells.
By assuming that inertia terms can be neglected, cell migration is modeled by an (Langevin-) equation of motion for each cell center summarizing all JKR-forces on that cell, the friction-forces with substrate or extracellular matrix as well as with other cells (parameterized by friction coefﬁcients), and optionally a random force term.
The latter mimics cell micro-motility and is quantiﬁed by the cell diffusion constant.
We solve the equation for diffusion and consumption of nutrients, growth factors, etc.
on a lattice using the Euler forward method while fulﬁlling the Courant Friedrichs Lewy condition (Drasdo and Hoehme, 2005) [for further details, see Supplementary Material (Section 3)]. CellSys also supports the modeling of dynamically varying cell parameters and different cell types as demonstrated for growing cell populations in tissue like environments (submitted for publication). Since the model is completely parameterized in terms of measurable quantities, intracellular control modules can readily be integrated within CellSys by direct coupling the molecular concentrations of intracellular chemicals to the cell-level parameters (Ramis-Conde et al., 2008, 2009). Most recently, we used CellSys to predict a so far unknown mechanism in liver regeneration after toxic damage that could be subsequently validated experimentally (Hoehme et al., 2007, 2010). The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[18:25 20/9/2010 Bioinformatics-btq437.tex] Page: 2641 2641 2642  S.Hoehme and D.Drasdo Fig.1.
(A) User interface of CellSys integrating real-time 3D visualization (upper window) and facilitating the parameterization, observation and visualization of model simulations (two exemplary windows are shown). (B) Simulated multi-cellular tumor grown in an environment of nutrient shortage.
The image was created using the ray tracer interface of CellSys. and observation functionality visualization 3 SOFTWARE CellSys is a software tool that efﬁciently integrates modeling, simulation, for individual-cell (agent)-based models.
It is implemented in portable object-oriented ANSI C++ and thus binaries are available for many operating systems including all recent Windows and Linux variants.
A graphical user interface (GUI) that provides real-time visualization of models in 3D based on OpenGL (Fig. 1A) also serves to access the functionality of the software.
CellSys is organized in encapsulated modules that lay the foundation for its extensibility and maintainability.
Currently, it consists of the following main components: (i) basic classes formulating the major abstractions such as model cells or ﬁnite grid elements (ii) a collection of core algorithms implementing model variants for cellular behavior such as cell-migration or cell-cycle-control (iii) algorithms for modeling the cellular environment (iv) modules for observing and (v) visualizing properties of the cell population and its environment.
During the model simulations, CellSys currently automatically permits taking over 60 different observables. Additionally, CellSys is able to output scene descriptions for example in VRML 2.0 or in the script language used in the popular open-source ray tracer PovRay (povray.org) (Fig. 1B). We use the libraries zlib (zlib.net) and libtiff (libtiff.org) to save screenshots, videos and zip-compressed model states in predeﬁned intervals.
The cross-platform GUI is based on the GLUI library (glui.sourceforge.net) and is used to change parameter settings and control the software components (1 5). It allows users to interactively explore model states and, e.g. mark speciﬁc properties of cells and their environment.
Thereby scalar properties are often mapped to RGB-color vectors as illustrated in Figure 1 and vector-based properties are visualized using 2D or 3D vector ﬁelds. The core algorithms in CellSys are parallelized using OpenMP (openmp.org) to exploit the computational power of modern shared memory multi-core multi-processor machines.
The solution of the stochastic equations of motion (Langevin) is supported by the multithreaded version of SuperLU for shared-memory parallel machines (crd.lbl.gov xiaoye/SuperLU). We ﬁnd the solution of the equations of motion and the neighbor detection to be the major performance bottlenecks of lattice-free individual-cell-based models.
On an Intel Core Duo E8500 system with 8 GB RAM a typical simulation from 1 to 105 cells e.g. takes 44 h (2D, 1 core), 125 h (3D, 1 core) or 41 h (3D, 4 cores). However, runtimes greatly depend on parameter and algorithm choices.
For a more detailed introduction refer to the Supplementary Material.
4 SUMMARY/OUTLOOK CellSys offers a robust and efﬁcient implementation of a lattice-free individual-cell-based model that is parameterized by experimentally measurable quantities.
The current version allows the in silico simulation and analysis of widespread experimental settings in 2D and 3D such as monolayer cultures and multi-cellular tumor spheroids.
Speciﬁc cell lines and culturing settings can be mimicked by changing parameters accordingly.
The results of model simulations can be visualized by a wide variety of virtual stainings and are quantiﬁed by many observables. The modular, object-oriented structure of the software permits extensions like the modeling of the surrounding tissue or vascular networks that will be made available in future versions of CellSys. Funding: German Federal Ministry of Education and Research [HepatoSys 0313081F, LungSys 0315415F and VirtualLiver 0315735, in part] European Union [CancerSys HEALTH-F4-2008- 223188, Passport: 223894, in part]. Conﬂict of Interest: none declared.
REFERENCES Anderson,A.R.A. et al. (2007) Single-Cell-Based Models in Biology and Medicine.
Basel: Birkhäuser. Byrne,H. and Drasdo,D. (2009) Individual-based and continuum models of growing cell populations: a comparison.
J.Math.
Biol., 58, 657 687.
Chu,Y.S. et al. (2005) Johnson-Kendall-Roberts theory applied to living cells.
Phys.
Rev.
Lett., 94, 028102.
Calzone,L. et al. (2006) BIOCHAM: an environment for modeling biological systems and formalizing experimental knowledge.
Bioinformatics, 22, 1805 1807.
Drasdo,D. et al. (2007) On the role of physics in the growth and pattern formation of multi-cellular systems.
J. Stat.Phys., 128, 287 345.
Drasdo,D. and Hoehme, S. (2005) A single-cell-based model of tumor growth in vitro: monolayers and spheroids.
Phys.
Biol., 2, 133 147.
Hoehme,S. et al. (2007) Mathematical modelling of liver regeneration after intoxication with CCl4. Chem.
Biol.
Interact., 168, 74 93.
Hoehme,S. et al. (2010) Prediction and validation of cell alignment along microvessels as order principle to restore tissue architecture in liver regeneration.
Proc. Natl Acad. Sci.USA, 107, 10371 10376.
Missal,K. Ramis-Conde,I. et al. (2006) Gene network inference from incomplete expression data: transcriptional control of hematopoietic commitment.
Bioinformatics, 22, 731 738. the E-cadherin-beta- catenin pathway in cancer cell invasion: a multiscale approach.
Biophys. J., 95, 155 165.
(2008) Modeling the inﬂuence of Ramis-Conde,I. et al. (2009) Multi-scale modelling of cancer cell intravasation: the role of cadherins in metastasis.
Phys Biol., 6, 16008.
Swameye,I. et al. (2003) Identiﬁcation of nucleocytoplasmic cycling as a remote sensor in cellular signaling by databased modeling.
Proc. Natl Acad. Sci.USA, 100, 1028 1033.
Yu,Y.K. et al. (2002) Hybrid alignment: high-performance with universal statistics.
Bioinformatics, 18, 864 872.
2642 [18:25 20/9/2010 Bioinformatics-btq437.tex] Page: 2642 2641 2642
BIOINFORMATICS Vol.26 ECCB 2010, pages i497 i503 doi:10.1093/bioinformatics/btq374 Prototypes of elementary functional loops unravel evolutionary connections between protein functions Alexander Goncearenco1,2 and Igor N. Berezovsky1, 1Computational Biology Unit, Bergen Center for Computational Science and 2Department of Informatics, University of Bergen, N-5008 Norway ABSTRACT Motivation: Earlier studies of protein structure revealed closed loops with a characteristic size 25 30 residues and ring-like shape as a basic universal structural element of globular proteins.
Elementary functional loops (EFLs) have speciﬁc signatures and provide functional residues important for binding/activation and principal chemical transformation steps of the enzymatic reaction.
The goal of this work is to show how these functional loops evolved from pre-domain peptides and to ﬁnd a set of prototypes from which the EFLs of contemporary proteins originated.
Results: This article describes a computational method for deriving prototypes of EFLs based on the sequences of complete genomes.
The procedure comprises the iterative derivation of sequence proﬁles followed by their hierarchical clustering.
The scoring function takes into account information content on proﬁle positions, thus preserving the signature.
The statistical signiﬁcance of scores is evaluated from the empirical distribution of scores of the background model.
A set of prototypes of EFLs from archaeal proteomes is derived.
This set delineates evolutionary connections between major functions and illuminates how folds and functions emerged in pre-domain evolution as a combination of prototypes.
Contact: Igor.Berezovsky@uni.no 1 INTRODUCTION Enzymes are involved in all processes in living organisms.
Well before the ﬁrst protein sequence and structure were determined (Sanger, 1952), the function of enzymes became one of the central questions in biochemical studies.
Despite the wealth of experimental data available nowadays, the functions of the majority of proteins are still uncharacterized (Levitt, 2009). Since the presence of certain biochemical activities is typically sought for, while all other possible activities (e.g. promiscuous functions) are ignored (Furnham et al., 2009), experimental determination of enzymatic function is in most cases conﬁrmative. Besides, biochemical assays are expensive, are subject to in vitro experimental conditions, and they can not be run on a genomic scale.
All the above makes prediction of enzymatic function with computational methods an important alternative approach.
There are several general assumptions on which such methods are based: (i) homologous proteins have similar functions (ii) most of the functional variants emerged as a result of divergence from a common ancestor (iii) structural homologs, so-called fold superfamilies, persist down to 25% of sequence identity (iv) divergence below 25% of sequence identity leads to the emergence of families with different organism, substrate, and/or tissue speciﬁcities (Lo Conte et al., 2000). Though enzymatic  To whom correspondence should be addressed.
function can be inferred by sequence and structure similarity, the relations between sequence, structure and function are far from being completely understood.
Many folds display vast functional diversity.
For example, structurally similar β/α barrels provide scaffolds to a number of biochemical functions (Nagano et al., 2002), while particular biochemical functions can be performed by different protein folds, e.g. hydrolase (Lo Conte et al., 2000). The contemporary evolution of protein structure and function takes place through mutations (Aharoni et al., 2005), recombination and domain swapping and/or interactions (Chothia et al., 2003). However, it is rather obvious that this process was preceded by the emergence of a ﬁrst set of protein domain structures/folds with a limited repertoire of biochemical functions.
These structures emerged from peptides with rudimentary non-speciﬁc catalytic activities in the pre-domain stage of evolution (Lupas et al., 2001). Understanding of this process is important for characterizing the most ancient functions and their connections to the modern proteins.
The difﬁculties in understanding and, more importantly, in predicting protein function, are well reﬂected in the diversity of their descriptions.
Enzymatic reactions are classiﬁed in enzyme nomenclature (EC) by the biochemical transformation and the substrate (Bairoch, 2000). According to MACiE database, there could be different mechanisms employed for the same transformation (Holliday et al., 2007, 2009). Different biochemical reactions can have the same core mechanism, as it is exempliﬁed by mechanistically diverse superfamilies (Glasner et al., 2006). In order to reconcile different approaches and to develop a generic description of enzymatic functions, one has to start from considering their elementary units which provide binding/activation and principal chemical transformation steps of the whole reaction.
Then it should be found out how combinations of these units result in a variety of enzymatic reactions, and how protein folds restrict the possibility of performing a particular biochemical transformation or binding a certain substrate.
The ﬁrst question that arises in this context is what elements of protein folds serve as elementary units of function.
What were the structures of these units in pre-domain evolution, and how did they affect the structures of modern proteins Earlier studies have shown that soluble proteins contain a basic universal element, stemming from the polymer nature of polypeptide chains, namely closed loops or returns of the polypeptide chain backbone with a typical size of 25 30 amino acid residues (Berezovsky and Trifonov, 2001 Berezovsky et al., 2000). Any protein fold can be decomposed into sets of consecutively connected closed loops (Berezovsky, 2003), indicating their independence in the evolutionary past (Trifonov and Berezovsky, 2003). Can we reconstruct the pre-biotic peptides that gave rise to the elementary functional units of modern proteins Our hypothesis is that a functional signature revealing the type of  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i497 i497 i503  A.Goncearenco and I.N.Berezovsky binding/activation or principal chemical transformation of the loop can be obtained from the contemporary proteins.
This signature complements the description of the closed loop, hence it is an elementary functional loop (EFL). Therefore, the ﬁrst goal of this study is to investigate how these functional loops evolved from pre-domain peptides, and to ﬁnd a set of prototypes from which the EFLs of contemporary proteins originate.
In order to draw a picture of folds and functions emerging as combinations of EFLs, prototypes of the EFLs will be derived and the corresponding EFLs will be detected in proteins with known biochemical functions.
The presence of EFLs in distinct folds and functions unravel evolutionary relations between them and can hint on recipes for protein function (re)design. The speciﬁc nature of prototypes calls for developing a new computational procedure for their derivation and characterization.
Indeed, we seek for entities which do not exist in modern proteins, but are represented by their descendants, EFLs. The EFLs themselves presumably have low sequence identity to each therefore, evolutionary connections between them other, and, are not obvious.
In this work, we propose a computational procedure to derive prototypes of EFLs from the sequences of complete proteomes. We expect these prototypes to be of closed- loop size (25 30 residues), ring-like shape and to have distinct functional signatures, where several conserved positions in the proﬁle describe chemically active amino acids which are involved in binding/activation steps and/or take part in principal chemical transformations of the substrate.
We illustrate our approach by reconstructing prototypes from complete archaeal proteomes and analyzing connections between functions and folds found by the reconstructed prototypes.
In particular, we show examples for three characteristic cases: (i) nucleotide triphosphate binding and hydrolyzing loop, called p-loop (Rossmann et al., 1974) (ii) a loop found in functionally diverse proteins having β/α barrel fold (iii) prototypes of two EFLs involved into binding of ADP and glucose which form an enzymatic domain in glycosyltransferases (glycogen synthase). 2 MATERIALS AND METHODS We describe a computational method for deriving prototypes of EFLs based on the sequences of complete genomes.
The procedure comprises the iterative derivation of sequence proﬁles followed by the hierarchical clustering of proﬁles. We propose a scoring function that weights proﬁle positions proportional to the information content on position, allowing to discrimine between matches that carry a speciﬁc signature from non- speciﬁc ones.
The statistical signiﬁcance of the scores is calculated from the empirical distribution of the reshufﬂed proﬁle scores used as a control.
We generalize the proﬁles and remove the remaining redundancies by clustering the proﬁles. The distance measure used in clustering also takes into account the information content on proﬁle positions.
2.1 Derivation of prototypes from complete proteomes Complete sets of protein coding sequences of 68 archaeal organisms (listed in Supplementary Material) are obtained from Genbank (Benson et al., 2009). We use one proteome representing each phylum of the archaeal superkingdom to produce a set of origins for the prototype derivation procedure.
Proteomic sequences contain many sources of biases and redundancies: (i) homologous proteins (ii) domain swapping, recombination and multiplication.
These redundancies have to be removed, as they reﬂect recent events in the evolution of proteins (Chothia et al., 2003). The average i498 domain size is 80 150 residues (Gerstein, 1998 Jones et al., 1998 Svedberg 1929 Wheelan et al., 2000) therefore, in order to remove redundancy originated from domain swapping, we compare 80-residue long sequence segments for identity.
Sequences are clustered with CD-HIT (Li and Godzik, 2006) several times to gradually remove redundancy between domains down to 40% identity.
Low-complexity regions in sequences which contain repeats or have highly biased amino acid composition are masked with SEG (Wootton and Federhen, 1996). Non-redundant domains are cut with 10- residue steps into overlapping 50-residue segments.
These segments contain two 10-residue ﬂanks, which can be adjusted in order to obtain a ﬁnal 30- residue prototype.
Based on the observation that gaps are not distributed uniformly, and multiple sequence alignments of remote homologs (below 25% sequence identity) contain well-aligned blocks without gaps (Kann et al., 2007) we consider that the cost of insertion or deletion in a functional signature is higher compared to at an arbitrary position in the whole protein sequence, therefore we do not allow gaps in proﬁles of EFLs. The procedure starts from the search for sequences in the complete archaeal proteomes that are most closely related to the initial sequence segments (origins) in order to construct the seed alignment with a frequency matrix constituting a proﬁle. The obtained proﬁles are then matched to the complete proteomes again, in order to ﬁnd additional sequence matches and to update the proﬁle. This proﬁle-sequence search is repeated until the proﬁle no longer changes, and, therefore, considered converged (Supplementary Figure S1). The proﬁles represent families of EFLs with speciﬁc signatures.
The iterative procedure allows a proﬁle to gradually expand to more distantly related, but statistically signiﬁcant matches.
Although the procedure resembles PSI-BLAST (Altschul et al., 1997), it has some notable differences originating from the speciﬁc requirements of the prototype derivation task.
These differences are discussed in more detail in Supplementary Material.
2.2 Weighting of proﬁle positions by information We calculate position speciﬁc scoring matrices (PSSM) to score the proﬁles (for details of PSSM calculation see Supplementary Material). The proﬁle- scoring function has to rank the matches according to the similarity of the sequence segment to the signature of the proﬁle. An uneven contribution of positions in the proﬁle should be taken into account.
Degeneration of the proﬁle towards the random compositional background or rare amino acids because of overestimation of pseudocounts (Altschul et al., 2009) should be prevented.
Decrease of the proﬁle sensitivity because of underestimation of pseudocounts should also be avoided.
Therefore, in order to discriminate between matches that carry a speciﬁc signature from non-speciﬁc ones, we weight positions proportional to Kullback Leibler divergence (DKL) (Kullback and Leibler, 1951), which reﬂects the information content on position i relative to the random background: (cid:2) = 20(cid:1) j=1 Di KL fi,j log2 (cid:4)(cid:5) , (cid:3) fi,j cj where f is observed amino acid frequencies on position i and c is proteomic amino acid composition.
The score of a sequence segment q to proﬁle P(n) will become: (cid:6) Score q,P (n ) Di KLmi,qi , (cid:8) (cid:9) i=1,...,n j=1,...,20 is the corresponding PSSM with n positions.
(DKL 1 bit) are not where Proﬁle positions with low-information content contributing to the overall score and are omitted.
mi,j (cid:7) n(cid:1) j=1 = 1 n 2.3 Empirical calculation of the background The signiﬁcance of a score is characterized by an E-value, which is the number of false positive or unrelated matches above this particular score.
The E-value is evaluated by comparing distributions of scores of the proﬁle (s) with scores of the reshufﬂed proﬁle (sR): E(s)= Np= N(1 ecdf(sR)), where p is the P-value, N is the size of the combined proteome and ecdf(sR) [10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i498 i497 i503  Prototypes of elementary functional loops Fig.1.
Scoring and comparison of sequence-proﬁle matches.
(A) Histograms show tails of distributions of proteomic scores for the p-loop prototype (red) and for the same prototype, but with the reshufﬂed positions (blue) where the functional signature is destroyed, while the amino acid composition of the proﬁle is preserved.
Inset shows quantile quantile plot of complete distributions.
(B) Selected matches (28 of 689) of the p-loop prototype (shown as logo). Numbers indicate identity to the consensus sequence (shown below). Sequence label is Genbank description of the protein from which these matches were taken.
(C) Histogram shows pair-wise sequence identity of all the signiﬁcant matches of the p-loop prototype.
is an empirical cumulative distribution function of the reshufﬂed proﬁle scores.
If, instead of the reshufﬂed proﬁle, a randomized proteome is used as control, then all the biases except amino acid composition are lost, resulting in overestimation of signiﬁcance of sequence matches containing non-speciﬁc signals.
Complete positional permutations destroying relative distances between proﬁle positions, similar to the combinatorial problem of pattern-avoiding permutations (Atkinson, 1999), give a robust estimation of the E-values (data not shown). Figure 1A shows an example of the score distributions for a derived proﬁle and the reshufﬂed one.
The difference exists only in the right-most tail of the distributions, showing that there are speciﬁc signatures in the proﬁle, and that they are destroyed by reshufﬂing. All collected proﬁle matches with the E-value below a certain signiﬁcance threshold (e.g. E 1) are used to construct the updated proﬁle. A sample of signiﬁcant matches of the p-loop prototype and the corresponding sequence logo are shown in Figure 1B. Although the matches are found in proteins with different biochemical function, they all possess nucleotide-binding activity, which is presumably an elementary function of this prototype.
It is important to note that some of the proﬁle positions have much higher information content than others, and these positions constitute the signature.
Comparison of all signiﬁcant sequence matches to the consensus shows that the sequence identity is low on average and has a large variance (Fig. 1B and C). Therefore, for proper E-value estimation, i.e. for proper separation of related matches from the unrelated ones, discrimination between informative and non-informative positions is necessary.
2.4 Hierarchical clustering of converged proﬁles The iterative procedure described above results in a set of converged proﬁles that should be analyzed further.
First, there is a redundancy between these proﬁles, caused by the way the origins are obtained: they overlap with a step of 10 residues.
Redundancy also stems from the fact that different origins can actually converge to the same or very similar proﬁles. It means that these origins correspond to evolutionary connected EFLs, but their similarity can only be detected with the help of the proﬁle. We introduce a distance measure that takes into account all possible proﬁle proﬁle alignments in order to hierarchically cluster the proﬁles. Proﬁle proﬁle comparison is more sensitive (Panchenko, 2003) than proﬁle-sequence comparison, thus more distant relations could be detected during proﬁle clustering.
This procedure results in the removal of redundancy and further generalization of the proﬁles. The proﬁles with the most generic signatures represent the functional characteristics of presumably original prototypes.
All (cid:10) (cid:10) possible proﬁle proﬁle alignments without gaps are performed by sliding one 50-residue-long proﬁle [A]50 against the other proﬁle [A]50 and calculating pair-wise positional distances between all possible 30-residue windows [a]30 and [b]30, respectively.
Distances between the pairs of corresponding positions are weighted proportionally to the information at each position (DKL): (cid:12)(cid:13)(cid:13)(cid:14)(cid:6) (cid:11)= 30(cid:1) i=1 +Dbi KL Dai KL (cid:7) 20(cid:1) (cid:10) (cid:11)2. ai,j bi,j j d [a]30 ,[b]30 The distance between two 50-residue proﬁles [A]50 and [B]50 is equal to the minimal distance between all possible sliding windows of size 30: (cid:11)=argmin (cid:8) (cid:10) D [A]50 ,[B]50 d [a]30 ,[b]30 (cid:11)(cid:9) a A b B.
Hierarchical clustering of proﬁles is an iterative procedure where the most similar proﬁles (min[D(A,B)]) are consecutively merged together, resulting in a new, more generic proﬁle (Supplementary Figure S3). 2.5 Characterization of prototypes We characterize prototypes by looking for sequence matches in crystallized enzymes from ASTRAL/SCOP database (Brenner et al., 2000 Lo Conte et al., 2000). These matches describe descendant EFLs that diverged from the prototype.
We assign elementary functions for derived prototypes and determine characteristic positions in their signatures based on the known enzymatic mechanisms in crystallized proteins.
Protein function is typically annotated by homology, although neither 50) high-sequence identity ( 50%), nor low BLAST E-values (below 10 guarantee the conservation of biochemical function (Rost, 1999, 2002). Here we annotate functional units of sub-domain size.
Conventional homology detection methods, which operate on the level of whole proteins or domains, consider connections between SCOP superfamilies as false positives (Gough et al., 2001). It becomes obvious, that analysis of evolutionary relationships on the level of functional closed loops requires a special approach (Andreeva et al., 2007 Fong and Marchler-Bauer, 2009 Xie and Bourne, 2008). Although most of the derived prototypes ( 70%, data not shown) have matches in Pfam, Prosite and CDD, functional annotation can not be directly transferred from the databases deﬁning the function on whole-protein or domain level (Bateman et al., 2004 Lo Conte et al., 2000 Marchler- Bauer, et al., 2009 Sigrist et al., 2010), resulting in ambiguous annotations, and requiring additional manual curation. SCOP superfamilies can be used i499 [10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i499 i497 i503  A.Goncearenco and I.N.Berezovsky Fig.2.
Matches of the nucleotide triphosphate-binding (p-loop) prototype in crystal structures.
Four matches of nucleotide triphosphate-binding prototype are shown.
The fold is displayed in cartoon and the structural loop corresponding to p-loop prototype is highlighted in green.
The structures of the EFLs are also displayed.
The logo of the prototype and the alignment of sequences of the corresponding EFLs highlight the functionally important residues involved in nucleotide triphosphate binding and hydrolysis.
PDB ID, SCOP ID and the coordinates of the sequence segments corresponding to the domains which contain the EFLs on display, are shown in the bottom.
as a reference of the function for crystallized protein domains.
CDD and Swissprot features can be used as more precise indicators of the elementary function of EFLs. Other databases describe enzymatic function on the residue level.
For example, CSA (Gutteridge and Thornton, 2005), and MACiE (Holliday et al., 2007) databases describe experimentally determined roles of functional residues in the biochemical reactions and its mechanisms.
Thus, via sequences of the crystallized structures this annotation can be transferred to the prototype s signature.
2.6 Statistics Non-redundant archaeal proteome has 20 106 sequence segments of length 50.
Starting from 175 458 origins extracted from four archaeal organisms, we end up with 8327 converged proﬁles having 100 matches in the archaeal proteome and containing at least one position with four bits of information in their signature.
These proﬁles were clustered for 120 iterations, which resulted in 138 proﬁles, from which the strongest 43 were selected for further consideration.
The resulting 43 proﬁles are considered to be the most abundant ones and were used in the analysis.
The ASTRAL sequence database based on SCOP release 1.75 contains 16 712 non-redundant domains at 95% sequence identity.
3 RESULTS AND DISCUSSION We developed a computational procedure for deriving prototypes of EFLs, obtained prototypes from the set of archaeal proteomes, considered several prototypes in detail, delineated connections between domains superfamilies using the most abundant prototypes, and exempliﬁed how combinations of EFLs result in speciﬁc enzymatic function.
Figure 2 shows several representatives of the p-loop prototype and exempliﬁes detection of EFLs in different folds and biochemical functions.
The signature of the prototype reads G-X-X-G-X-G-K- [TS] and is known to be the signature of nucleotide triphosphate binding (Rossmann et al., 1974). We show EFLs corresponding to i500 Fig.3.
Matches of the Glycine-rich prototype in crystal structures of β/α-barrels. Four matches of the prototype involved presumably in redox reaction are shown.
All structures have β/α-barrel fold, but perform different biochemical functions and belong, therefore, to different SCOP superfamilies. the prototype in four different proteins representing three different folds (p-loop containing nucleoside hydrolase, PEP carboxykinase- like fold, OsmC-like fold). Though sequence alignment of EFLs reveals high conservation in key sequence positions, the rest of the loop can diverge signiﬁcantly. The degree of divergence of EFLs from the prototype is also indicated in the difference between corresponding structural segments: in the hydrolase and PEP- carboxykinase-like folds the structure resembles β-turn-α, while in OsmC-like fold it resembles β-hairpin. It is important to note, however, that despite the different structures of the EFL, naturally affected by the rest of the fold (Minor and Kim, 1996), the functional signature is always located in the elbow between the two elements of secondary structure and is highly conserved.
EFLs representing this prototype universally provide the elementary function of nucleotide triphosphate binding via interaction with the phosphate groups and with a Mg2+ ion, and also take part in phosphate hydrolysis.
The combination of a speciﬁc sequence signature with its structural location emphasizes the conservation of the closed-loop structure, regardless of the exact secondary structural content of the loop and interaction of this loop with its structural environment.
The diversity of folds containing this loop suggests that in the pre- domain stage of protein evolution the prototype of the p-loop was included into structurally and biochemically different folds, acquiring different elements of secondary structure and mutations in sequences, but preserving the active residues and their relative locations in sequence and space.
The fourth structure in Figure 2 is a protein from V. cholerae with unknown function.
With the help of the prototype it is now possible to hypothesize the function of this protein to a certain extent.
It could be predicted, for example, that this V. cholerae protein has a nucleotide triphosphate binding, and, perhaps, hydrolyzing activity.
The combination with other EFLs detected in this protein can complete description of its possible biochemical function.
Figure 3 shows proteins that share the same β/α-barrel fold, which, in turn, has 30 superfamilies in SCOP. This fold also serves as a scaffold for a variety of functions (Nagano et al., 2002), therefore [10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i500 i497 i503  Prototypes of elementary functional loops Fig.4.
Protein functions connected by prototypes of EFLs. Red diamonds represent prototypes and blue ovals represent protein domain superfamilies with the corresponding SCOP superfamily names.
Edges are matches detected in the non-redundant sequences of protein domains.
Thickness of edges characterizes the number of matches found.
the fold is an important target in protein (re)design experiments (Bershtein and Tawﬁk, 2008 Tokuriki and Tawﬁk, 2009). Based on the derived prototypes, β/α-barrels can be decomposed into a set of β-turn-α subunits, some of these subunits are directly involved in catalysis and, therefore, carry the functional signatures.
The Glycine-rich prototype (Fig. 3) is an illustration of functional connections in an abundant β/α-barrel fold.
The structure of the loops is β-turn-α, and the functionally important residues are located in the turn.
The elementary function of the Glycine-rich pre- domain prototype is related to redox reactions revealing evolutionary connection between β/α barrels with different enzymatic functions.
The biochemical functions of the enzymes where the loop is found are typically various oxidoreductases, dehydrogenases and synthases. A set of abundant prototypes derived on the archaeal proteomes is used to delineate evolutionary connections between functional superfamilies of structural domains.
Figure 4 illustrates connections revealed by a subset of the strongest prototypes (43 prototypes) in form of a graph.
Nodes are prototypes (red diamonds) and protein domain superfamilies (blue ovals) according to SCOP classiﬁcation. The edges represent matches between the superfamilies and the prototypes, where thickness of an edge is proportional the number of matches found in the non-redundant set of protein domains derived from ASTRAL/SCOP database (Brenner et al., 2000 Lo Conte et al., 2000). Since a non-redundant set of protein domains was used in analysis, thickness is a rough indicator of the diversity of proteins in the superfamily. Each prototype is referred to by its number (numbers in red diamonds) and has a functional signature represented in form of a sequence proﬁle. The logos of the corresponding proﬁles are listed in Supplementary Material.
to the logarithm of The functional connections exempliﬁed by the p-loop prototype and Glycine-rich prototype can be seen here in a larger context of archaeal (and homologous to archaeal) domains.
The Glycine- rich prototype (Fig. 3) with the number 8 in the graph has ﬁve connections to folds other than β/α-barrel fold: NAD(P)-binding Rossmann fold, Activating enzymes of the Ubiquitin-like proteins, PreATP-grasp domain, Nucleotide-binding domain, FAD/NAD(P)- binding domain.
Since all these folds have nucleotide phosphate binding in common, these connections suggest that the elementary function of prototype 8 is related to nucleotide phosphate binding.
As a result, functional connections inside the β/α-barrel fold as well as connections between the β/α-barrel and other folds are found, unraveling nucleotide phosphate binding as one of basic elementary functions crucial in the emergence of folds.
Another interesting case is a p-loop containing nucleoside triphosphate hydrolase considered earlier (Fig. 2) which is an example of a fold with different biochemical functions.
The particular biochemical function, in turn, is determined by the unique combination of EFLs, which is reﬂected as a group of prototypes gathered around the superfamily and connected by thick edges.
One of the prototypes around the superfamily is prototype 1603, considered earlier (Fig. 2). The connection between p-loop containing nucleoside triphosphate hydrolase and PEP carboxykinase-like folds via p-loop prototype (Fig. 2) indicates that p-loop as EFL is an essential functional element of enzymes belonging to different superfamilies with different folds.
It also suggests an important role of prototype 1603 in pre-domain evolution of folds and superfamilies. Some prototypes are present in a variety of superfamilies. For example, the Cysteine- rich metal binding loop (number 1845) which corresponds to EFLs forming a nest with cysteines co-ordinating a metal ion (typically Zn2+ ) facilitating nucleic acid binding.
These EFLs are present in various superfamilies mainly related to nucleic acid binding, which i501 [10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i501 i497 i503  A.Goncearenco and I.N.Berezovsky Fig.5.
Two EFLs combine to form the active site in glycosyltransferase. The structure of glycogen synthase (PDB 1rzu) is shown in cartoon representation.
The prototypes are shown as sequence logos, and the corresponding EFLs are highlighted in the structure.
is reﬂected in the graph by edges connecting prototype 1845 to thirteen superfamilies. By representing proteins folds and functions in form of a graph connected by the prototypes one can proceed to explore the emergence of protein functions as combinations of prototypes.
We illustrate how a functional domain emerges as combination of prototypes by the example of Glycosyltransferase superfamily (orange oval in Fig.4). Figure 5 shows glycogen synthase (PDB 1rzu), which catalyzes the elongation of alpha-1,4-glucose backbone.
The enzyme binds ADP-glucose and [Glucose]n 1 as substrates and transfers glycosyl group to form [Glucose]n as product and ADP. One of the enzyme s domains contains the EFLs corresponding to sequence prototypes 3054 (cyan) and 7009 (red). The elementary chemical functions of the prototypes are assigned according to the description of interactions with co-crystallized ligands analogous to the products and the substrates of the enzyme (Buschiazzo et al., 2004 Sheng et al., 2009). The elementary function of prototype 3054 is ADP binding: Arg-299 and Lys-304 interact with the phosphate, Ile-297 (second in position of 3054 s PSSM) with the base and Ser-298 (third in 3054 s PSSM) with the sugar in ADP. Prototype 7009 is also involved in ADP binding, its characteristic elementary function is glucose binding: Glu-376 interacts with phosphate and Thr-381 (third in 7009 s PSSM) with the base of ADP. Besides, residue Glu-376 also plays an important catalytic role in glycosyltransferase activity.
Finally, these two prototypes also interact with each other, forming a stabilizing salt bridge between Lys-304 and Glu-376. This example shows how the emergence of enzymatic function can be explored based on signatures of the prototypes and their elementary chemical functions.
The two-domain nature of glycosyltransferase also points out that analysis of individual folds and their enzymatic functions should be followed by the exploration of recombination events in case of multi-domain proteins.
4 CONCLUSIONS The existence of EFLs in different folds and functions makes it possible to survey subtle evolutionary relations, originating from the pre-domain evolution of protein structure.
It suggests that contemporary enzymatic functions are constructs of different sets i502 and combinations of elementary chemical functions.
It also shows that most of the enzymatic functions are performed by abundant prototypes (e.g. p-loop and Cysteine-containing prototype), which provide common reaction steps of different functions existing in different folds.
An exhaustive description of a protein fold and its enzymatic function as a combination of EFLs illuminates how this fold emerged in pre-domain evolution by fusion of prototype genes.
Therefore, obtaining the full collection of prototypes with elementary functions will make it possible to (i) predict enzymatic functions based on the sequences via determining EFLs corresponding to prototypes and their relative positions revealing structure of the fold (ii) (re)design folds with desired functions by building constructs from necessary EFLs. ACKNOWLEDGEMENTS We thank Simon Mitternacht for helpful comments and suggestions.
Funding: FUGE-II Norwegian functional genomics platform.
Conﬂict of Interest: none declared.
REFERENCES Aharoni,A. et al. (2005) The evolvability of promiscuous protein functions.
Nat. Genet., 37, 73 76.
Altschul,S.F. et al. (2009) PSI-BLAST pseudocounts and the minimum description length principle.
Nucleic Acids Res., 37, 815 824.
Altschul,S.F. et al. (1997) Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.
Nucleic Acids Res., 25, 3389 3402.
Andreeva,A. et al. (2007) SISYPHUS structural alignments for proteins with non- trivial relationships.
Nucleic Acids Res., 35, D253 D259. Atkinson,M.D. (1999) Restricted permutations.
Discrete Math, 195, 27 38.
Bairoch,A. (2000) The ENZYME database in 2000.
Nucleic Acids Res., 28, 304 305.
Bateman,A. et al. (2004) The Pfam protein families database.
Nucleic Acids Res., 32, D138 D141. Benson,D.A. et al. (2009) GenBank. Nucleic Acids Res., 37, D26 D31. Berezovsky,I.N. (2003) Discrete structure of van der Waals domains in globular proteins.
Protein Eng., 16, 161 167.
Berezovsky,I.N. et al. (2000) Closed loops of nearly standard size: common basic element of protein structure.
FEBS Lett., 466, 283 286.
Berezovsky,I.N. and Trifonov,E.N. (2001) Van der Waals locks: loop-n-lock structure of globular proteins.
J. Mol. Biol., 307, 1419 1426.
Bershtein,S. and Tawﬁk,D.S. (2008) Advances in laboratory evolution of enzymes.
Curr. Opin. Chem.
Biol., 12, 151 158.
Brenner,S.E. et al. (2000) The ASTRAL compendium for protein structure and sequence analysis.
Nucleic Acids Res., 28, 254 256.
Buschiazzo,A. et al. (2004) Crystal structure of glycogen synthase: homologous enzymes catalyze glycogen synthesis and degradation.
EMBO J., 23, 3196 3205.
Chothia,C. et al. (2003) Evolution of the Protein Repertoire.
Science, 300, 1701 1703.
Fong,J.H. and Marchler-Bauer,A. (2009) CORAL: aligning conserved core regions across domain families.
Bioinformatics, 25, 1862 1868.
Furnham,N. et al. (2009) Missing in action: enzyme functional annotations in biological databases.
Nat. Chem.
Biol., 5, 521 525.
Gerstein,M. (1998) How representative are the known structures of the proteins in a complete genome A comprehensive structural census.
Fold Des., 3, 497 512.
Glasner,M.E. et al. (2006) Evolution of enzyme superfamilies. Curr. Opin. Chem.
Biol., 10, 492 497.
Gough,J. et al. (2001) Assignment of homology to genome sequences using a library of hidden Markov models that represent all proteins of known structure.
J. Mol. Biol., 313, 903 919.
Gutteridge,A. and Thornton,J.M. (2005) Understanding nature s catalytic toolkit.
Trends Biochem. Sci., 30, 622 629.
Holliday,G.L. et al. (2007) The chemistry of protein catalysis.
J. Mol. Biol., 372, 1261 1277.
Holliday,G.L. et al. (2009) Understanding the functional roles of amino acid residues in enzyme catalysis.
J. Mol. Biol., 390, 560 577.
[10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i502 i497 i503  Prototypes of elementary functional loops Jones,S. et al. (1998) Domain assignment for protein structures using a consensus Rossmann,M.G. et al. (1974) Chemical and biological evolution of nucleotide-binding approach: characterization and analysis.
Protein Sci., 7, 233 242. protein.
Nature, 250, 194 199.
Kann,M.G. et al. (2007) The identiﬁcation of complete domains within protein sequences using accurate E-values for semi-global alignment.
Nucleic Acids Res., 35, 4678 4685.
Rost,B. (1999) Twilight zone of protein sequence alignments.
Protein Eng., 12, 85 94.
Rost,B. (2002) Enzyme function less conserved than anticipated.
J. Mol. Biol., 318, 595 608.
Kullback,S. and Leibler,R.A. (1951) On Information and Sufﬁciency. Ann. Math Stat., Sanger,F. (1952) The arrangement of amino acids in proteins.
Adv.
Protein Chem., 7, 22, 142 143.
1 67.
Levitt,M. (2009) Nature of the protein universe.
Proc. Natl Acad. Sci.USA, 106, 11079 11084.
Li,W. and Godzik,A. (2006) Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences.
Bioinformatics, 22, 1658 1659.
Sheng,F. et al. (2009) The crystal structures of the open and catalytically competent closed conformation of Escherichia coli glycogen synthase. J. Biol.
Chem., 284, 17796 17807.
Sigrist,C.J. et al. (2010) PROSITE, a protein domain database for functional Lo Conte,L. et al. (2000) SCOP: a structural classiﬁcation of proteins database.
Nucleic characterization and annotation.
Nucleic Acids Res., 38, D161 D166. Acids Res., 28, 257 259.
Lupas,A.N. et al. (2001) On the evolution of protein folds: are similar motifs in different protein folds the result of convergence, insertion, or relics of an ancient peptide world J. Struct. Biol., 134, 191 203.
Marchler-Bauer,A. et al. (2009) CDD: speciﬁc functional annotation with the Conserved Svedberg,T. (1929) Mass and Size of Protein Molecules.
Nature, 123, 871.
Tokuriki,N. and Tawﬁk,D.S. (2009) Protein Dynamism and Evolvability. Science, 324, 203 207.
Trifonov,E.N. and Berezovsky,I.N. (2003) Evolutionary aspects of protein structure and folding.
Curr. Opin. Struct. Biol., 13, 110 114.
Domain Database.
Nucleic Acids Res., 37, D205 D210. Wheelan,S.J. et al. (2000) Domain size distributions can predict domain boundaries.
Minor,D.L.,Jr. and Kim,P.S. (1996) Context-dependent secondary structure formation Bioinformatics, 16, 613 618. of a designed protein sequence.
Nature, 380, 730 734.
Wootton,J.C. and Federhen,S. (1996) Analysis of compositionally biased regions in Nagano,N. et al. (2002) One fold with many functions: the evolutionary relationships between TIM barrel families based on their sequences, structures and functions.
J. Mol. Biol., 321, 741 765.
Panchenko,A.R. (2003) Finding weak similarities between proteins by sequence proﬁle sequence databases.
Methods Enzymol., 266, 554 571.
Xie,L. and Bourne,P.E. (2008) Detecting evolutionary relationships across existing fold space, using sequence order-independent proﬁle-proﬁle alignments.
Proc. Natl Acad. Sci.USA, 105, 5441 5446. comparison.
Nucleic Acids Res., 31, 683 689.
[10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i503 i497 i503 i503 BIOINFORMATICS Vol.26 ECCB 2010, pages i632 i637 doi:10.1093/bioinformatics/btq392 Modeling associations between genetic markers using Bayesian networks Edwin Villanueva and Carlos Dias Maciel  Electrical Engineering Department, Sao Carlos School of Engineering, University of Sao Paulo, Sao Carlos, Sao Paulo, Brazil loci is of at different in fundamental ABSTRACT Motivation: Understanding the patterns of association between a population polymorphisms (linkage disequilibrium, LD) importance in various genetic studies.
Many coefﬁcients were proposed for measuring the degree of LD, but they provide only a static view of the current LD structure.
Generative models (GMs) were proposed to go beyond these measures, giving not only a description of the actual LD structure but also a tool to help understanding the process that generated such structure.
GMs based in coalescent theory have been the most appealing because they link LD to evolutionary factors.
Nevertheless, the inference and parameter estimation of such models is still computationally challenging.
Results: We present a more practical method to build GM that describe LD. The method is based on learning weighted Bayesian network structures from haplotype data, extracting equivalence structure classes and using them to model LD. The results obtained in public data from the HapMap database showed that the method is a promising tool for modeling LD. The associations represented by the (cid:2). learned models are correlated with the traditional measure of LD D The method was able to represent LD blocks found by standard tools.
The granularity of the association blocks and the readability of the models can be controlled in the method.
The results suggest that the causality information gained by our method can be useful to tell about the conservability of the genetic markers and to guide the selection of subset of representative markers.
Availability: The implementation of the method is available upon request by email.
Contact: maciel@sc.usp.br 1 INTRODUCTION The detection of linkage disequilibrium (LD), the non-random association of alleles at different loci in a population, and the assessing of its intensity, extent and distribution is a fundamental step in many genetic studies.
In association studies, for example, the search for the locus (or loci) responsible of a particular trait or disease is narrowed to regions of high LD (LD blocks) where genotyped genetic markers were observed to be associated with the studied phenotype (Mueller, 2004). In population genetic, LD patterns has been widely used to study the evolutionary and demographic processes in a variety of animal and plant populations, such as admixtures, migration and natural selection (Tishkoff et al., 1996 Zhang et al., 2004). LD information was also useful to learn more about the architecture of the human genome and its biology of recombination (Pritchard and Przeworski, 2001). To whom correspondence should be addressed.
(cid:2) A variety of coefﬁcients have been proposed to quantify the intensity of LD (see Mueller, 2004 for a review). Pairwise LD measures were the ﬁrst ones reported for this purpose, which measure the overall allele association between two loci.
and r2 (Hedrick, Popular examples of such measures are D 1987). Subsequently, multi-locus LD coefﬁcients were proposed to measure simultaneous allele associations among multiple loci.
Classical examples are the IA index and the coefﬁcient H (Mueller, 2004 Sabatti and Risch, 2002). Recently, information theory was used to develop new LD coefﬁcients. Some examples are: the coefﬁcent ε, based in entropy (Nothnagel et al., 2002) the normalized mutual information (MIR) coefﬁcient (Zhang et al., 2009) and the normalized relative entropy (ER) coefﬁcient (Liu and Lin, 2005). This active search of LD coefﬁcients has been accompanied with the development of various tools that display LD measures in a comprehensive way.
Examples of those tools are: GOLDsurfer (Pettersson et al., 2004), Haploview (Barrett et al., 2005) and LdCompare (Hao et al., 2007). This remarkable interest in developing new measures and tools for studying LD can be explained by the dramatic increase of public genotype data [from the HapMap project (The International HapMap Consortium, 2003), for example], which at the same time enabled association studies in a whole-genome scale that presented new statistical and computational challenges due to the vast quantity of data collected.
Although some of the aforementioned LD measures and tools were useful in characterizing LD at various genomic regions of several populations, they are limited to provide a static view of the LD structure in the studied region.
In an attempt to go beyond these measures some generative models (GMs) were proposed (Hudson, 2001 Li and Stephens, 2003 Maniatis et al., 2002 McVean et al., 2002). GMs are useful because they model the process that generate the observed data, providing the machinery to do inferences and simulations of yet unobserved situations or to help understanding the underlying generative process.
Models based in coalescent theory (Kingman, 2000) have been the most appealing GMs because they relate LD to evolutionary factors, such as recombination rate, migration and mutation.
However, the parameter estimation and inference in such models is still computationally challenging and only applicable to short regions (Nicolas et al., 2006). A more practical alternative to model LD is to learn probabilistic graphical models (PGMs) directly from the observed genotype or haplotype data, as proposed by Thomas and Camp (2004). PGM are a type of GMs that have proven to be useful in modeling a variety of complex real-world problems mainly due to the following reasons: intuitive interpretability of the models, their ability to encode the joint probability distribution with a reduced number of parameters (Heckerman et al., 1995) and the existence of efﬁcient methods to learn PGM from data and to make inferences.
Because the learning  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i632 i632 i637  Modeling associations between genetic markers using Bayesian networks of PGM from data is an empirical approach, the learned models are not intended to describe LD in terms of evolutionary factors, but rather to represent the current LD structure in an accurate, compact and understandable way, which is important in genetic association studies.
In the present article, we propose the use of Bayesian networks (BN) to model LD. BN (Pearl, 1988) are a type of PGM that encode a joint probability distribution via a directed acyclic graph (DAG), where nodes represent random variables and edges represent dependencies between them.
We choose BN because we were interested in learning the causality of the associations, which we found to give additional information about the LD structure.
Our approach is different from the previous works of Thomas and Camp (2004) and Thomas (2005), which propose the use of Markov networks to represent dependences between proximal loci and several runs of simulated annealing algorithm to learn optimal models.
BN models are more naturally interpretable than Markov networks, since the components (factors) of the factorized joint probability distribution are associated to the model nodes instead of cliques.
The learning of the BN structures is based on the K2GA algorithm (Larranaga et al., 1996), which ﬁnds the optimal BN structure(s) through the combination of a global search [using a genetic algorithm] GA on the space of topological orderings and a local search (using the K2 greedy search method) on the subspace delimited by each ordering.
The learning method computes the strength of each edge.
The learned BN structures are grouped into equivalence structure classes (set of BN with the same set of dependence/independence relationships), which are represented by a partially DAG (PDAG). The identiﬁcation of LD associations and LD blocks are performed in these models.
The method was tested in public haplotype data from the HapMap database in three different segments located in ENCODE regions.
The results showed the plausibility of the method in modeling LD, representing associations and LD blocks consistent with standard tools.
The effects of pruning weak edges on the formation of association blocks are studied.
The correlation of the traditional measure D with the associations represented by the PDAGs is also studied.
(cid:2) 2 APPROACH 2.1 Data We consider that we have in hand haplotype data obtained from a given population.
This data are formed by m haplotypes, each one consisting of N marker loci sorted by their physical location in the chromosome.
Let xij denote the allele state of the j-th marker in the haplotype i, which can be one of the allele set (1,2,...,rj), where rj is the number of alleles of marker j.The haplotypes may be experimentally determined or inferred from genotype data by a phasing algorithm [e.g. fastPHASE (Scheet and Stephens, 2006) BEAGLE (Browning and Browning, 2007)]. In the context of BN, the j-th marker is modeled by a random variable Xj. Thus, an haplotype xi=(xi1,...,xiN ) is regarded as a realization of a N-dimensional random variable X=(X1,...,XN ). 2.2 Bayesian networks Bayesian networks (Neapolitan, 2003 Pearl, 1988) are a type of PGM that can represent the conditional dependencies and independencies between a set of random variables X=(X1,...,XN ) via a DAG.
A BN is composed by a model structure S and a set of model parameters θ. The model structure is the qualitative part of a BN and is formed by a DAG S=(V ,E), where V is the set of nodes representing the random variables X and E is the set of edges representing the conditional dependencies between the random variables of X.
The model parameters are the quantitative part of a BN that in conjunction with the DAG deﬁne completely its joint probability distribution.
The model parameters are formed by N parameter vectors θ =(θ1,...,θN ) deﬁning the conditional probability distributions of each variable, i.e. θi deﬁnes the conditional probability distribution of Xi, P(xi pai,θi), where xi represents the instantiation of the variable Xi and pai represents the instantiation of the parent variables of Xi, Pai. All BN have the property that each variable is conditionally independent on its non-descendants given its parent variables (Markov property). For example, the BN a b c and a b c represent the same independences (a and c are independent given b) and, therefore, are indistinguishable.
The BN a b c differs from the previous because it represent diferent independencies (a and c are marginally independent and all other pairs are dependent). This property allows the computation of the joint probability of any instantiation of X, x, as the product of the local conditional probabilities: P(x)= i=1 P(xi pai,θi). (cid:1) N 2.3 Structural learning of BN The induction of BN structures from data is a NP-hard problem (Chickering et al., 2004). We need a heuristic method to ﬁnd the structure(s) that best reﬂect the dependency/independence relations contained in the data.
To this end, we take the idea of the K2GA algorithm (Larranaga et al., 1996) to learn model structures from haplotype data, as described below.
The main idea of K2GA is to ﬁnd optimal BN structures by searching on the space of topological orderings using a combination of global search (using a GA) and a local search [using the greedy search K2 heuristic (Cooper and Herskovits, 1992)]. A topological ordering (TO) is an ordering of the system variables X such that i,j, if Xj comes before Xi in the ordering, namely (...,Xj,...,Xi,...), then Xi can only have node Xj as a parent node.
A TO, therefore, delimits a subspace of structures.
A GA is used to evolve a population of TOs. Each TO in the evolving population is evaluated by the K2 heuristic.
Such evaluation consists in ﬁnding the best structure within the subspace of structures spanned by the TO.
The data provided to K2 are: the TO, a dataset D containing the observations of the variables X, and an upper bound u on the number of parents a node may have.
For each node Xi, (i=1,...,N), K2 iteratively attempts to add up to u edges from Anc(Xi), the ancestors of Xi according to the TO.
In each iteration one node is selected from Anc(Xi) and added to the set of parent nodes of Xi, Pai. The selected parent is one whose addition most increases the structure score.
K2 stops to adding parents to the node Xi when u parents were added or when no more ancestors in Anc(Xi) can increase the structure score.
The posterior probability of the structure given the data (Heckerman and Chickering, 1995), known as BDe metric, is used as the structure score, which is computed as follows: P(S D)= (cid:1) i=1 g(i,Pai), N k=1 (cid:1)ri  (αij) (αij+Nij)  (αijk+Nijk)  (αijk) g(i,Pai)= (cid:1)qi j=1 (1) i633 [11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i633 i632 i637  E.Villanueva and C.D.Maciel where ri is the number of states of Xi, qi is the number of different instantiations that the parents Pai can take Nijk is the number of cases in D where Xi takes its k-th state and its parents Pai take their j-th state Nij is the sum of Nijk over k αijk is a prior on Nijk, calculated as α/riqi when a uniform prior is considered with equivalent sample size α and αij is the sum over k of αijk. When the logarithm of the structure score [Equation (1)] is applied, this reduces to a sum of node scores, log(g(i,Pai)), which only depends on the node Xi and its parents Pai. This modularization of the structure score is advantageous to K2, which selects the set of parents for each node Xi based in the maximization of the node score instead of the maximization of the whole structure score.
The strength of any edge is measured as the increase on the structure score that the edge provokes when it is added to the structure.
For example, if a new parent node Xj is added to the set of parents of Xi, i.e. an edge Xj  Xi, the edge strength is computed as log(g(i,Pai Xj)) log(g(i,Pai)). This strength is equivalent to the logarithm of the Bayes factor.
The GA used to evolve TOs starts with an initial population of λ TOs (called individuals in the context of the GA) generated randomly.
Then, each individual is evaluated with K2, which returns the score of the best structure as the ﬁtness of the individual.
The following operators are then sequentially performed to create the next generation (the evaluated population is referred as the current generation). (i) Selection: ρ pairs of individuals are selected from the current generation to form a mating pool.
The selection is based on the individual s ﬁtness using the roulette wheel algorithm.
(ii) Crossover: produces two new individuals (offspring) for each pair of the mating pool.
The order-based crossover operator OX2 (Larranaga et al., 1996) is used for this purpose.
This operator is only applied when a random number generated uniformly in the interval [0,1] is less than µ, the crossover rate parameter, otherwise the resulting offspring is replicated from the mating individuals.
(iii) Mutation: this is applied to each offspring individual resulting of the previous step.
The displacement mutation (DM) operator (Larranaga et al., 1996) is used for this purpose.
Like crossover, this operator is only applied when a random number generated uniformly in the interval [0,1] is less than ν, the mutation rate parameter, otherwise the offspring individual is left unchanged.
(iv) Scoring: evaluates the resulting offspring individuals after mutation with the K2 heuristic.
(v) Replacement: creates the next generation by replacing the worst α individuals of the current generation with the best α individuals of the offspring population, provided that the replacing individuals are better than the replaced individuals.
The new generation is set as the current generation and a new loop is started.
The algorithm terminates when a predeﬁned number of generations is reached.
2.4 Obtaining BN structure equivalence classes for modeling LD As pointed by Chickering (2002), it is more appropriate to learn equivalence classes of network structures than single structures.
An equivalence class represents a group of structures that encode the same set of dependence/independence relationships.
We took advantage on the ability of the above learning method in generating several optimal BN structures (structures with the same maximum ﬁtness found along the evolutionary process) to get equivalence classes.
All the optimal BN structures are analyzed and i634 Fig.1.
Example illustrating the construction of a PDAG from two optimal DAGs.
The nodes with the same color are associated with each other forming an association block.
grouped according to their topological connection, i.e. structures in one group only differ in edge directions.
Each group is represented by a PDAG constructed by superimposing all the DAGs of the group, as shown in Figure 1.
The resulting PDAGs can serve to characterize LD. Two genetic markers are associated if they are not marginally independent, which imply that the corresponding nodes are connected in a PDAG by a directed path or by a common ancestor node.
In the example of Figure 1 all the pair of nodes with the same color are associated.
Additionally, the pairs (3, 4) and (3, 5) are also associated.
We deﬁne an association block in a PDAG as the set of the largest number of consecutive markers (consecutive in terms of its physical location) that are associated with each other.
In Figure 1, two association blocks can be found, identiﬁed with different colors.
3 RESULTS The described method was implemented in the C++ programming language and tested in a 64-bit 2-core (2.1 GHz) computer with 4 GB of RAM, running a Linux operating system.
The graph drawing was performed using the Graph Visualization Software (www.graphviz.org). To test the ability of the proposed approach in characterizing LD, we used public data registered in the HapMap database (The International HapMap Consortium, 2003). We chose three genomic segment located in ENCODE1 regions of different chromosomes and different populations, as shown in Table 1.
ENCODE regions were chosen due to their high density of genotyped SNPs markers and to the availability of existing studies in such regions (The International HapMap Consortium, 2005). The segments were selected increasing their size and complexity of the LD patterns (Figs 4, 5 and 6), aiming to test the method in different situations.
The datasets were obtained from the HapMap repository, version III release 2, via the Genome Browser web application (http://hapmap.ncbi.nlm.nih.gov) using the option Download Phased Haplotype Data.
The SNP markers with minor allele frequency (MAF)  0.03 were discarded, since they have little polymorphism. The following set of parameters were introduced to the learning method in all tests: population size λ=20, couples in the mating pool ρ=10, crossover rate µ=0.95, mutation rate ν=0.05, number of replacement individuals in each generation α=10, number of generations = 300, maximum number of parents a node may have u=#SNPs 1.
For clarity and space, we ﬁrst detail the results for the ENm010_CHB+JPT dataset and then present summarized results for the other datasets. Figure 2 shows the evolution of the best ﬁtness and average ﬁtness of the BN structures learned from the ENm010_CHB+JPT 1Encyclopedia of DNA Elements (ENCODE Project Consortium, 2004) [11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i634 i632 i637  Modeling associations between genetic markers using Bayesian networks Table 1.
Datasets used to test the method Dataset name ENCODE region Chromosome band Genomic segment (kp) ENm010_CHB+JPT ENr131_CEU ENr321_YRI ENm010 ENr131 ENr321 7p15.2 2q37.1 8q24.11 27 070 27 126 235 065 235 122 118 797 118 895 HapMap population CHB+JPT CEU YRI Number of SNPs (MAF  0.03) Number of haplotypes 15 31 47 340 234 230 These datasets were obtained from the HapMap repository (version III, release 2) using the Genome Browser web application (http://hapmap.ncbi.nlm.nih.gov). Only SNPs with MAF  0.03 were considered.
Fig.2.
Evolution of the maximum and average ﬁtness of BN structures learned from the ENm010_CHB+JPT dataset. dataset. Only the ﬁrst 20 generations are shown, since the curves remain unchanged in subsequent generations.
As can be observed, the ﬁrst optimal BN structure is found at the ﬁfth generation.
Six generations later the entire population converged to only optimal BN structures (the average ﬁtness stabilizes at its maximum value). This can be considered a fast convergence, since it is reached in approximately the ﬁrst 4% of the total number of generations.
We use a predeﬁned high number of generations as a stopping criterion because we were interested in exploring the topological diversity of optimal BN structures to get equivalence classes (PDAGs). All the optimal learned DAGs (a total of 5800 DAGs from generations 11 to 300) were analyzed.
Twelve different DAGs were identiﬁed in that set, which were grouped into four different PDAGs, each having 31 edges.
One of these PDAGs is shown in Figure 3a. The other PDAGs are similar to this ﬁgure, differing only in the location of three edges.
For each of these PDAGs was determined the set of pairwise marker associations.
No difference was found between these four sets of pairwise associations.
As can be observed, two association blocks were identiﬁed in the PDAG, which are consistent with the two LD blocks (Block1 and Block 2) found in the triangular LD plot (Fig. 4) by the Haploview tool using the Strong LD Spine block deﬁnition (Barrett et al., 2005). With the aim to knowing how the associations and association blocks are affected with the elimination of weak edges, we perform two experiments in the PDAG of Figure 3a. In the ﬁrst experiment, the edges with strengths lower than the ﬁrst quartile (the lower quartile) were eliminated.
The resulting PDAG is shown in Figure 3b with 23 edges.
There was no alteration either in the set of pairwise associations or in the association blocks (these blocks are also indicated with line segments at the top of the LD plot of Figure 4). Fig.3.
(a) A PDAG (original) learned from ENm010_CHB+JPT dataset. PDAGs resulting of pruning edges at the ﬁrst quartile (b) and at the second quartile (c) of (a). The node numbers are the codes assigned by the Haploview tool in the segment.
The edge labels are the edge strengths.
i635 [11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i635 i632 i637  E.Villanueva and C.D.Maciel Fig.4.
LD plot for the genomic segment of the ENm010_CHB+JPT dataset. The segmeted lines All, Q1 and Q2 at the top represent the association blocks found in the original PDAG, the PDAG with the ﬁrst quartile of edges removed and the PDAG with the second quartile of edges removed, respectively.
Table 2.
Structural learning results for the three analyzed datasets Dataset Convergence generation # PDAGs Processing time (seg) ENm010_CHB+JPT ENr131_CEU ENr321_YRI 11 43 166 4 4 3 7 143 735 In the second experiment, the edges with strengths lower than the second quartile (the worst half of edges) were eliminated.
The resulting PDAG is shown in Figure 3c. In this case the number of pairwise associations fell from 63 to 49, but the association blocks remained unchanged.
This preservation of the association blocks with the removal of signiﬁcant number of edges can be due to the high compaction of the LD blocks.
A summary of results of executing the method in all datasets can be found in Table 2.
It can be observed that the convergence generation increases with the number of markers, as well as the processing time.
This is an expected result, since the search space is bigger and more complex.
Like the results in the ENm010_CHB+JPT dataset, the four PDAGs obtained from the ENr131_CEU dataset and the three PDAGs obtained from the ENr321_YRI dataset represent the same set of pairwise associations and the same association blocks.
These blocks are illustrated as segmented lines with the name All at the top of Figures 5 and 6, respectively.
As can be noticed, the association blocks represented in the PDAGs considering all edges tend to be generous, grouping large quantity of nodes into few (cid:2) blocks, including in such blocks markers with moderate to low D values.
When the lower quartile of edges is removed, the association blocks are splitted into more compact blocks (line segments Q1). When the second lower quartile of edges is removed the association blocks (line segments Q2) tend to be more segmented and similar to that found by the Strong LD Spine block deﬁnition. It is interesting to know the relationship between the learned associations represented by the PDAGs and the classical measure of LD D.
For this end, Figures 7a, b and c shows the distributions (cid:2) i636 Fig.5.
LD plot for the genomic segment of the ENr131_CEU dataset. Fig.6.
LD plot for the genomic segment of the ENr321_YRI dataset. of the associations with respect to 10 equally spaced intervals of (cid:2) for the three datasets, respectively.
It is possible to observe a D clear trend in the three datasets to represent associations with high (cid:2) values (in the interval [0.9 1.0]). When edges are pruned at ﬁrst D quartile, the distributions of the represented associations are almost unchanged.
When the second quartile of edges are removed the association distributions are moderately altered, predominantly in (cid:2) the lower intervals of D.
This means that weak edges are important (cid:2) components of weak D associations, and vice versa.
The learned PDAGs showed another interesting information: the markers most central in the blocks tend to have more outgoing edges (e.g. markers 4 and 24 in all PDAGs of Fig.3) and the markers on the block boundaries tend to have predominantly incoming edges (markers 1, 13, 19 and 27 in all PDAGs of Fig.3). An explanation for this behavior is that genetic markers near the center of the blocks are highly conserved in the population (had minimal recombination), which is reﬂected in the learned models by their tendency to be more causative than dependent nodes.
This suggests that the causal information gained by BN models can be useful to tell about the conservability of the genetic markers in the analyzed population.
The learned causal information can also be useful to guide the selection of an informative subset of tags markers, since nodes that have predominantly outgoing edges can explain the allele status of their dependent markers, being good candidates as tag markers.
[11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i636 i632 i637  Modeling associations between genetic markers using Bayesian networks Fig.7.
Distributions of the PDAG associations with respect to 10 equally spaced intervals of D ENr131_CEU (c) ENr321_YRI. (cid:2) for the three analyzed datasets: (a) ENm010_CHB+JPT (b) 4 CONCLUSIONS In this article a novel application of Bayesian networks to model associations between genetic markers was proposed.
The method is based on learning optimal BN structures (weighted) from haplotype data, extracting equivalence structure classes (PDAGs) and using them to model LD. The results obtained in public data from the HapMap database showed that our approach is a promising tool for modeling LD. All the association blocks represented in the learned PDAGs were consistent with LD blocks found by standard tools.
It was shown that by pruning weak edges is possible to control the granularity of the association blocks and the clarity and interpretability of the models.
The associations represented by the PDAGs were shown in correlation with the traditional measure of LD D.
It was suggested that the causality information learned by our approach can be useful to infer about the conservability of the genetic markers and to guide the selection of informative tags markers.
Our current developments are in improving the efﬁciency of the method and parallel implementations for extending greatly the number of markers that can be considered up to whole-genome scales.
(cid:2) Funding: Coordenação de Aperfeiçoamento de Pessoal de Nıvel Superior (CAPES), the Brazilian government agency for the development of human resources.
Conﬂict of Interest: none declared.
REFERENCES Barrett,J. et al. (2005) Haploview: analysis and visualization of LD and haplotype maps.
Bioinformatics, 21, 263 265.
Browning,S.R. and Browning,B.L. (2007) Rapid and accurate haplotype phasing and missing-data inference for whole-genome association studies by use of localized haplotype clustering.
Am.J. Hum.
Genet., 81, 1084 1097.
Chickering,D. (2002) Learning equivalence classes of Bayesian-network structures.
J. Mach.
Learn.
Res., 2, 445 498.
Chickering,D. et al. (2004) Large-sample learning of Bayesian networks is NP-Hard. J. Mach.
Learn.
Res., 5, 1287 1330.
Cooper,G.F. and Herskovits,E.A. (1992) A Bayesian method for the induction of probabilistic networks from data.
Mach.
Learn., 9, 309 347.
ENCODE Project Consortium (2004) The ENCODE (ENCyclopedia of DNA elements) Project.
Science, 306, 636 640.
Hao,K. et al. (2007) LdCompare: rapid computation of single- and multiple-marker r(2) and genetic coverage.
Bioinformatics, 23, 252 254.
Heckerman,D. and Chickering,D.M. (1995) Learning Bayesian networks: the combination of knowledge and statistical data.
Mach.
Learn., 20, 197 243.
Heckerman,D. et al. (1995) Real-world applications of Bayesian networks.
Commun. ACM, 38, 24 26.
Hedrick,P. (1987) Gametic disequilibrium measures - proceed with caution.
Genetics, 117, 331 341.
Hudson,R. (2001) Two-locus sampling distributions and their application.
Genetics, 159, 1805 1817.
Kingman,J. (2000) Origins of the coalescent: 1974-1982.
Genetics, 156, 1461 1463.
Larranaga,P. et al. (1996) Learning Bayesian network structures by searching for the best ordering with genetic algorithms.
IEEE Trans.
Syst. Man Cybernet. A Syst. Hum., 26, 487 493.
Li,N. and Stephens,M. (2003) Modeling linkage disequilibrium and identifying recombination hotspots using single-nucleotide polymorphism data.
Genetics, 165, 2213 2233.
Liu,Z. and Lin,S. (2005) Multilocus LD measure and tagging SNP selection with generalized mutual information.
Genet. Epidemiol., 29, 353 364.
Maniatis,N. et al. (2002) The ﬁrst linkage disequilibrium (LD) maps: delineation of hot and cold blocks by diplotype analysis.
Proc. Natl Acad. Sci.USA, 99, 2228 2233.
McVean,G. et al. (2002) A coalescent-based method for detecting and estimating recombination from gene sequences.
Genetics, 160, 1231 1241.
Mueller,J. (2004) Linkage disequilibrium for different scales and applications.
Brief.
Bioinform., 5, 355 364.
Neapolitan,R.E. (2003) Learning Bayesian Networks.
Prentice-Hall, Inc., Upper Saddle River, NJ. Nicolas,P. et al. (2006) A model-based approach to selection of tag SNPs. BMC Bioinformatics, 7, Article 303.
Nothnagel,M. et al. (2002) Entropy as a measure for linkage disequilibrium over multilocus haplotype blocks.
Hum.
Hered., 54, 186 198.
Pearl,J. (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann Publishers Inc., San Francisco, CA.
Pettersson,F. et al. (2004) GOLDsurfer: three dimensional display of linkage disequilibrium.
Bioinformatics, 20, 3241 3243.
Pritchard,J. and Przeworski,M. (2001) Linkage disequilibrium in humans: Models and data.
Am.J. Hum.
Genet., 69, 1 14.
Sabatti,C. and Risch,N. (2002) Homozygosity and linkage disequilibrium.
Genetics, 160, 1707 1719.
Scheet,P. and Stephens,M. (2006) A fast and ﬂexible statistical model for large- scale population genotype data: applications to inferring missing genotypes and haplotypic phase.
Am.J. Hum.
Genet., 78, 629 644.
The International HapMap Consortium (2003) The International HapMap Project.
Nature, 426, 789 796.
The International HapMap Consortium (2005) A haplotype map of the human genome.
Nature, 437, 1299 1320.
Thomas,A. (2005) Characterizing allelic associations from unphased diploid data by graphical modeling.
Genet. Epidemiol., 29, 23 35.
Thomas,A. and Camp,N. (2004) Graphical modeling of the joint distribution of alleles at associated loci.
Am.J. Hum.
Genet., 74, 1088 1101.
Tishkoff,S. et al. (1996) Global patterns of linkage disequilibrium at the CD4 locus and modern human origins.
Science, 271, 1380 1387.
Zhang,W. et al. (2004) Impact of population structure, effective bottleneck time, and allele frequency on linkage disequilibrium maps.
Proc. Natl Acad. Sci.USA, 101, 18075 18080.
Zhang,L. et al. (2009) A multilocus linkage disequilibrium measure based on mutual information theory and its applications.
Genetica, 137, 355 364. i637 [11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i637 i632 i637 BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.18 2010, pages 2347 2348 doi:10.1093/bioinformatics/btq430 Systems biology Cytoscape Web: an interactive web-based network browser Christian T. Lopes, Max Franz, Farzana Kazi, Sylva L. Donaldson, Quaid Morris and Gary D. Bader  Advance Access publication July 23, 2010 Banting and Best Department of Medical Research, Donnelly Centre for Cellular and Biomolecular Research, University of Toronto, 160 College Street, Toronto, ON M5S 3E1, Canada Associate Editor: Joaquin Dopazo ABSTRACT Summary: Cytoscape Web is a web-based network visualization tool modeled after Cytoscape which is open source, interactive, customizable and easily integrated into web sites.
Multiple ﬁle exchange formats can be used to load data into Cytoscape Web, including GraphML, XGMML and SIF. Availability and Implementation: Cytoscape Web is implemented in Flex/ActionScript with a JavaScript API and is freely available at http://cytoscapeweb.cytoscape.org Contact: gary.bader@utoronto.ca Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 3, 2010 revised on July 1, 2010 accepted on July 20, 2010 1 INTRODUCTION Increasing amounts of high-throughput data are being collected, stored, shared and analyzed on the web, highlighting the need for effective web-based data visualization.
Network visualization components are especially valuable to help researchers interpret their data as part of data analysis tools.
However, current web- based network visualization components lack many useful features of their desktop counterparts.
Medusa (Hooper and Bork, 2005) is a Java applet originally used in the STRING database (Jensen et al., 2009) and by many other web sites for network visualization, but lacks advanced features, such as detailed customization of the network view.
jSquid (Klammer et al., 2008) expands Medusa s functionality, but does not provide an easy way for the client web site to change and interact with the network view after it has been rendered.
TouchGraph (http://www.touchgraph.com/navigator.html) is another Java applet for network visualization, but provides only one mode of network interaction designed for exploration and is not easily customizable. yFiles Flex (http://www.yworks.com en/products_yﬁlesﬂex_about.html) is a rich Internet application with a feature-rich user interface, an architecture that balances client/server work and supports efﬁcient data communication.
This commercial software is customizable within the bounds of the code already written, but is not open source.
Cytoscape (http:/ www.cytoscape.org/) is an open source Java network visualization and analysis tool that provides a large array of useful features (Shannon et al., 2003), but is not speciﬁcally designed for use on the web except via Java WebStart or as a library to generate static network images for web display.
The ﬁeld of network visualization is   To whom correspondence should be addressed.
Fig.1.
A Cytoscape Web network with a customized visual style.
lacking an interactive, easily customizable, open source, web-based visualization component.
Cytoscape Web is an interactive, web-based network visualization tool, modeled after the popular Cytoscape software (Fig. 1). Using basic programming skills, Cytoscape Web can be customized and incorporated into any web site.
Cytoscape Web is not intended as a replacement for the Cytoscape desktop application, for example, it contains none of the plugin architecture functionality of Cytoscape instead it is intended as a low overhead tool to add network visualization to a web application.
2 IMPLEMENTATION Cytoscape Web is a client-side component that requires no server- side implementation, and allows developers to choose any server- side technology, if necessary.
The main network display component of Cytoscape Web is implemented in Flex/ActionScript, but a JavaScript application programming interface (API) is provided so all the customization and interaction with the network view can be easily built in JavaScript without needing to change and compile the Flash code.
This architecture has the advantage of using the Flash  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:46 11/8/2010 Bioinformatics-btq430.tex] Page: 2347 2347 2348  C.T.Lopes et al. platform to implement complex and interactive vector images that behave consistently across major browsers, but without requiring the web site to be entirely built with this technology.
In other words, the web site itself can rely on web standards (HTML, CSS and JavaScript) for embedding and interacting with Cytoscape Web.
This design also offers the possibility of migrating the implementation to other technologies such as scalable vector graphics (SVG) and HTML5 in the future without making major API changes.
The choice of Flash rather than Java is motivated by the fact that Java applets can be slow to launch, require the download of the large Java runtime and make it difﬁcult to create custom (non-Swing) user interfaces without writing low-level graphics code.
3 AVAILABLE FEATURES 3.1 Features Similarly to Cytoscape, Cytoscape Web allows the client application to deﬁne a network of nodes and edges and customize their attributes.
Data can be loaded into Cytoscape Web through one of the supported XML-based exchange formats (GraphML or XGMML) or a simple tab-delimited text (Cytoscape s SIF format). The network data can also be exported to any of the above-mentioned formats.
The client can dynamically change node and edge visual styles (e.g. color, size and opacity), using any of the following methods: (i) specifying default visual properties for all elements (ii) mapping node and edge attributes (e.g. name, interaction type and weight) to visual styles and (iii) overriding default or mapped styles by setting a bypass style.
For instance, different types of interactions can be mapped to edge colors (e.g. protein protein to blue, protein DNA to green), and the edge width can be used to represent interaction weight.
Then the developer can use the bypass mechanism to create a ﬁrst neighbors highlight feature, for example, by setting the nodes and edges that belong to the neighbor set to the color red.
When the ﬁrst neighbors bypass is removed, the colors are automatically restored to their default or mapped values.
These three options, combined with more than 20 visual properties for nodes and edges, provide ﬂexibility and enable each Cytoscape Web- based application to deﬁne its own semantics, styles and features.
For example, iRefWeb (http://wodaklab.org/iRefWeb/), an interface to the interaction Reference Index (iRefIndex) database (Razick et al., 2008), uses a basic implementation of Cytoscape Web to display all interactions in which a single query gene participates.
Alternatively, GeneMANIA (http://www.genemania.org Warde- Farley et al., 2010), a gene function prediction tool, uses a more complex implementation of Cytoscape Web to extend a user s input gene list and display interactions among the genes.
Cytoscape Web communicates with the GeneMANIA server, mediated by client- side JavaScript, to display gene or network-speciﬁc highlights and associated information interactively.
Cytoscape Web s API can be used to implement the following features: a ﬁlter for nodes and edges, which temporarily removes the ﬁltered out elements based on attribute data functions for adding and deleting nodes and edges at runtime the ability to export the whole network as an image, either to a PNG (Portable Network Graphics) ﬁle or to a publication quality vector format (PDF). Cytoscape Web provides the ability to pan and zoom the network and choose different network layouts, including force directed.
The layout parameters can be customized, but if none of the available layouts produces the desired results, the web application can run an external layout algorithm in JavaScript or on the server-side, for 2348 instance and pass the results, via node positions, back to Cytoscape Web for visualization.
3.2 Performance Cytoscape Web works best with small- to medium-sized networks, generally with up to a few hundred nodes and edges.
Larger networks can be visualized, but the user interaction can become sluggish around 2000 elements (nodes or edges) 800 nodes and 1200 edges, for example (tested on an Apple laptop computer with 2 GHz dual core CPU and 4 GB RAM). Use of the force-directed layout is the major bottleneck in the initial rendering of a typical network.
However, faster layouts are available and overall performance is dependent upon the client web site implementation and the end user conﬁguration. Additional performance statistics for Cytoscape Web are available in the Supplementary Material.
3.3 Documentation Cytoscape Web is actively developed as an open source project and is freely available at http://cytoscapeweb.cytoscape.org/. This web site includes a tutorial, with ready to use sample code, the API documentation and a showcase of major Cytoscape Web features.
The online examples can be freely used as a template for building web sites containing Cytoscape Web.
3.4 Future directions Future plans include the implementation of custom graphics for nodes and edges, additional network layouts and support for integration with Cytoscape [e.g. importing/exporting networks in Cytoscape (.cys) ﬁle format]. importing and closer ACKNOWLEDGEMENTS Cytoscape is developed through an ongoing collaboration between the University of California at San Diego, the Institute for Systems Biology, Memorial Sloan-Kettering Cancer Center, Institut Pasteur, Agilent Technologies, Unilever, the University of Michigan, the University of California at San Francisco and the University of Toronto. We gratefully acknowledge the contributions of many Cytoscape developers who developed software that Cytoscape Web was based on.
We thank the entire GeneMANIA team for support during the development of Cytoscape Web.
Funding: Genome Canada through the Ontario Genomics Institute (grant number 2007-OGI-TD-05) the U.S. National Institute of General Medical Sciences of the National Institutes of Health (grant number 2R01GM070743-06). Conﬂict of Interest: none declared.
REFERENCES Hooper,S.D. and Bork,P. (2005) Medusa: a simple tool for interaction graph analysis.
Bioinformatics, 21, 4432 4433.
Jensen,L.J. et al. (2009) STRING 8 a global view on proteins and their functional interactions in 630 organisms.
Nucleic Acids Res., 37, D412 D416. Klammer,M. et al. (2008) jSquid: a Java applet for graphical on-line network exploration.
Bioinformatics, 24, 1467 1468.
Razick,S. et al. (2008) iRefIndex: a consolidated protein interactions database with provenance.
BMC Bioinformatics, 9, 405.
Shannon,P. et al. (2003) Cytoscape: a software environment for integrated models of biomolecular interaction networks.
Genome Res., 13, 2498 2504.
Warde-Farley,D. et al. (2010) The GeneMANIA prediction server: biological network integration for gene prioritization and predicting gene function Nucleic Acids Res., 38, W214 W220. [11:46 11/8/2010 Bioinformatics-btq430.tex] Page: 2348 2347 2348
Genomics Proteomics Bioinformatics 13 (2015) 1 3 H O S T E D  BY Genomics Proteomics Bioinformatics www.elsevier.com/locate/gpb www.sciencedirect.com PERSPECTIVE On Bioinformatic Resources Runsheng Chen *,a CAS Key Laboratory of RNA Biology, Institute of Biophysics, Chinese Academy of Sciences, Beijing 100101, China Received 23 January 2015 accepted 15 February 2015 Available online 2 March 2015 A starting point of curating bioinformatic resources for the public is marked by the establishment of the US National Center for Biotechnology Information (NCBI) in 1988 [1]. One of its many purposes is certainly to echo the initiative of the Human Genome Project (HGP)  when two landmark  Mapping and reports were published at the same time: Sequencing the Human Genome  by the National Research [2] and  Mapping Our Genes  The Genome Council Project: How Big, How Fast  by the US Congress [3]. As HGP is prepared to scale up its sequencing operations about 5 years into the Project, a new discipline  bioinformat- ics  became inevitable.
It emerged originally for processing and sharing genome sequences and was thus known as genome informatics.
In the ﬁrst ﬁve-year plan of HGP, it was pointed out that genome informatics is a scientiﬁc discipline that encompasses all aspects of genome information acquisition, processing, storage, distribution, analysis, and interpretation [4]. This insightful statement was footnoted by the launch of GenBank at NCBI in 1992, a database of nucleotide sequences, in collaboration with its international partners at the European Molecular Biology Laboratory (EMBL) and the DNA Data Bank of Japan (DDBJ). Now, some 20 years have passed, not only GenBank still prospers but other data- bases on human genomics and biology have emerged to ﬁll up all data landscapes [5]. Genomics together with many other research ﬁelds of life sciences had entered the Era of Large-scale Data Acquisition in the early 90 s. The Era was led by the fast accumulation * Corresponding author.
E-mail: chenrs@sun5.ibp.ac.cn (Chen R). a ORCID: 0000-0002-1640-6481.
Peer review under responsibility of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
of human genomic sequences and followed by similar data from other large model organisms [6]. As soon as HGP declared the human genome sequenced in 2001 [7], the HapMap Project [8] was announced to sequence hundreds of human genomes in a diverse population background.
Microbial genomics has also been pursued into both metage- nomics and pangenomics [9,10]. The big data collection efforts have been extending all the way into its vertical path  from DNA to RNA to Protein [11,12]. Another expansion of the efforts is horizontally toward biological functions and disease relevance [13 15]. In order to annotate biological functions at different levels of anatomy and physiology  cell, organ, tis- sue, and systems  for biomedical applications, data integra- tion and interpretation become more relevant, and thus the birth of systems biology, which takes account of data from all omics ﬁelds and deciphers them in a context of biological networks.
In recent years, with strong funding and rapid devel- opment, the sequence technology has been advancing signiﬁ- cantly, resulting in dramatic cost reduction and explosion of data accumulation.
Therefore, a large number of bioinformatic resources become obvious and there is demand for some com- prehensive digestions.
There are several data types being curated in bioinformatic resources world-wide.
The ﬁrst of them is classiﬁed as scale-up omics data, which includes genome-wide data from the vertical path as mentioned above: genome, transcriptome, proteome, and metabolome, just to name a few.
Each of these data may have different disease relevance, such as DNA variations for cancer predisposition and protein structures for drug develop- ment. The second type of data resources includes data gener- ated by molecular biologists that tend to look at molecular details, such as transcription factors and signal transduction pathways, where interactions among macromolecules and http://dx.doi.org/10.1016/j.gpb.2015.02.002 1672-0229 ª 2015 The Authors.
Production and hosting by Elsevier B.V. on behalf of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). 2 Genomics Proteomics Bioinformatics 13 (2015) 1 3 structure function relationships are highly valued.
The third type of data resources concerns bioinformatic tools (algorithms and software packages), which include those for sequence assembly and stitching, gene prediction and annotation, pro- tein structure prediction, multiple sequence alignment, phy- logeny, and network analysis.
The fourth type is the primary and secondary literature that contains research papers, mono- graphs, reviews, conference reports, and even personal websites and blogs.
It also includes publically-available educational materials, such as training courses, online tutorials, and e- books.
The last, but not the least, refers to web-based gateways of various institutions, such as research institutes, academic journals, and topic-focusing websites.
All the aforementioned types of information are mainly stored in a variety of bioinfor- matic databases, accessible through the internet.
Another characteristic trend of bioinformatic resources is the increasing bodies of contributors and users from all ﬁelds of life sciences.
First, it calls for more and better user-friendly tools.
In this case, bioinformaticians are held responsible for building better hubs  usually containing databases and toolkits  for users from the diverse research ﬁelds. Second, it forces the contributors and users to start with their own urgent demands and to be innovative for mining data, providing tools, and understanding information imbedded in the data.
In the meantime, the crowdfunding activities often lead to resource allocations and thus generate new ﬁelds or disciplines.
Bioinformatics itself is already a complicated ﬁeld, and its integration and reaching-out to chemistry efforts have given birth to systems biology and synthetic biology, respectively.
Obviously, the demands com- ing from medicine and pharmaceutics are composed of molecular markers for disease diagnostics and prognostics, as well as drug targets and treatment strategies.
The birth of translational medicine represents a ﬁeld of research seek- ing medical applications for genomic data.
The ultimate goal of genomic data and bioinformatics is to pave a way for medicine to be personalized and more precision and thus the new path  precision medicine.
Other extensions include applications in a much broader settings, such as environ- ment, energy, and agriculture.
Third, the complexity of the data and information, as well as the conversion of both into knowledge, are all seeking for more intensive collaborations involving experts and talents from relevant disciplines, other than biologists, such as mathematicians, physicists, chemists, and computer scientists.
I would like to end this perspective by emphasizing a couple of the major issues of bioinformatics, encountered over my career path.
On one hand, we still face the same challenge as the past three decades  best annotating the large number of experimental data one of the examples is non-coding sequences in the genomes, such as the human genome.
There are at most 3% of the human genomes are  gene coding , and the rest is classiﬁed as non-coding, sometimes the  dark matter  without functional deﬁnitions. The full interpretation should include all sequence elements of a genome.
On the other hand, we still do not have enough high-quality data for bio- logical applications, such as in the case of cancer studies, where only thousands of samples have been sequenced so far for the most complicated disease threatening all members of our man- kind on a daily basis.
Most important for the ﬁeld of bioinfor- matics is that as more high-quality data are generated, we apparently are stepping into the  Big Data Era.
Therefore, we need to raise the bar higher for better data and more sophisticated tools.
Genomics, Proteomics & Bioinformatics (GPB) is apparently doing its job by evaluating the existing databases, algorithms, and toolkits (see related articles in this and the upcoming issues). Through summarizing the quotation and unique features of the databases and tools, the authors are trying to provide users clear ideas as to what may be the best to use and tool developers as the right directions for new initia- tives. For instance, biological networks, which are time-depen- dent and non-linear, have to be built with full participation of the relevant parties  not only proteins and enzymes but also RNA molecules that are truly involved in cellular complexes for appropriate functions.
As a ﬁnal note, we would like to see some consolidations where the current bioinformatic resources are organized by applications, ranked based on user feedbacks, and evaluations by all users.
Completing interests The author declared that there is no competing interest.
Acknowledgements This work was supported by the National High-tech R&D Program of China (863 Program Grant Nos: 2012AA020402 and 2012AA02A202). References [1] Smith K. A brief history of NCBI s formation and growth.
In: The NCBI handbook [Internet]. Bethesda: National Center for Biotechnology Information 2013, http://www.ncbi.nlm.nih.gov books/NBK148949/. [2] National Research Council (US) Committee on Mapping and Sequencing the Human Genome.
Mapping and sequencing the human genome.
Washington (DC): National Academies Press 1988, http://www.ncbi.nlm.nih.gov/books/NBK218252/. [3] US Congress, Ofﬁce of Technology Assessment.
Mapping our genes  the Genome Project: how big, how fast.
Washington (DC): US Government Printing Ofﬁce 1988, http://www.ornl. gov/sci/techresources/Human_Genome/publicat/OTAreport.pdf. [4] US Department of Health and Human Services, US Department of Energy.
Understanding our genetic inheritance  the US Human Genome Project: the ﬁrst ﬁve years FY 1991 1995 1990. http://www.ornl.gov/sci/techresources/Human_Genome/project 5yrplan/ﬁrstﬁveyears.pdf. [5] Zou D, Ma L, Yu J, Zhang Z.
Biological databases for human research.
Genomics Proteomics Bioinformatics 2015 13:55 63.
[6] Tang B, Wang Y, Zhu J, Zhao W. Web resources for model organism studies.
Genomics Proteomics Bioinformatics 2015 13: 64 8.
[7] Lander ES, Linton LM, Birren B, Nusbaum C, Zody MC, Baldwin J, et al. Initial sequencing and analysis of the human genome.
Nature 2001 409:860 921.
[8] International HapMap Consortium.
The International HapMap Project.
Nature 2003 426:789 96.
[9] Sun Q, Liu L, Wu L, Li W, Liu Q, Zhang J, et al. Web resources for microbial data.
Genomics Proteomics Bioinformatics 2015 13:69 72.
[10] Xiao J, Zhang Z, Wu J, Yu J.A brief review of software tools for pangenomics. Genomics Proteomics Bioinformatics 2015 13:73 6.
Chen R  On Bioinformatic Resources 3 [11] Chen T, Zhao J, Ma J, Zhu Y.
Web resources for mass Proteomics proteomics. Genomics spectrometry-based Bioinformatics 2015 13:36 9.
[14] Yang Y, Dong X, Xie B, Ding N, Chen J, Li Y, et al. Databases and web tools for cancer genomics study.
Genomics Proteomics Bioinformatics 2015 13:46 50.
[12] Zhao D, Wu J, Zhou Y, Gong W, Xiao J, et al. WikiCell: a uniﬁed resource platform for human transcriptomics research.
Omics 2012 16:357 62.
[15] Wei T, Peng X, Ye L, Wang J, Song F, Bai Z, et al. Web research.
Genomics Proteomics stem cell resources Bioinformatics 2015 13:40 5. for [13] Zhang G, Zhang Y, Ling Y, Jia J.Web resources pharmacogenomics. Genomics 2015 13:51 4.
Proteomics for Bioinformatics BIOINFORMATICS APPLICATIONS NOTE Vol.25 no.23 2009, pages 3187 3188 doi:10.1093/bioinformatics/btp566 Structural bioinformatics VDNA: The virtual DNA plug-in for VMD Thomas C. Bishop Center for Computational Science, Tulane University, Lindy Boggs Center Suite 500, New Orleans, LA 70118, USA Received on July 3, 2009 revised on August 18, 2009 accepted on September 10, 2009 Advance Access publication September 29, 2009 Associate Editor: Anna Tramontano ABSTRACT Summary: The DNA inter base pair step parameters (Tilt, Roll, Twist, Shift, Slide, Rise) are a standard internal coordinate representation of DNA. In the absence of bend and shear, it is relatively easy to mentally visualize how Twist and Rise generate the familiar double helix.
More complex structures do not readily yield to such intuition.
For this reason, we developed a plug-in for VMD that accepts a set of mathematical expressions as input and generates a coarse- grained model of DNA as output.
This feature of VDNA appears to provide a unique approach to DNA modeling.
Predeﬁned expressions include: linear, sheared, bent and circular DNA, and models of the nucleosome superhelix, chromatin, thermal motion and nucleosome unwrapping.
Availability: VDNA is pre-installed in VMD, http://www.ks.uiuc.edu Research/vmd. Updates are at http://dna.ccs.tulane.edu. Contact: bishop@tulane.edu 1 INTRODUCTION The DNA inter base pair step helical parameters (Tilt, Roll, Twist, Shift, Slide, Rise) are widely employed to model the structure, energetics and dynamics of DNA. These parameters describe the relative rotations and translations along a length of DNA needed to move from one base pair to the next.
The parameters are well deﬁned (Dickerson, 1989), and there are a number of programs for extracting these parameters from a Cartesian coordinate representation of DNA or for assembling a model of DNA given a set of helical parameters (El Hassan and Calladine, 1995 Lavery and Sklenar, 1988 Lu and Olson, 2003 Macke and Case, 1998 Tung and Carter, 1994 Vlahovicek and Pongor, 2000). However, none of these enable the user to specify a set of mathematical functions for the helical parameters and rapidly visualize the corresponding 3D model.
This is a major obstacle in development of an understanding of the complex relations between local and global structure.
Only for the most trivial distributions of DNA helical parameters is it easy to mentally visualize the global structure of DNA. For this purpose, we have developed a plug-in for VMD (Humphrey et al., 1996) that allows users to generate models of dsDNA using arbitrarily complex mathematical expressions.
2 METHODS The inter base pair step parameters are a local or internal coordinate representation of DNA that specify how a set of directors attached to the i-th base pair and denoted ( ˆd1, ˆd2, ˆd3) are rotated (Tilt, Roll, Twist) and translated (Shift, Slide, Rise) to achieve the set of directors associated with (cid:1) Roll2 + Tilt2) and local shear the (i+ 1)-th base pair.
The local bend is ( (cid:1) Shift2 + Slide2). In Cartesian coordinates, DNA is a space curve with is ( centerline (cid:1)r(si) and directors ˆdk(si) that results from the accumulation of bend, shear, Twist and Rise as a function of si. We emphasize that the helical parameter description is suitable for any ﬁber-like molecule for which a set of local directors can be embedded in the ﬁber. Thus, while VDNA is intended to represent DNA it is actually much more widely applicable.
The complete set of helical parameters includes intra base pair parameters (Shear, Stretch, Stagger, Buckle, Propeller-Twist, Opening), which describe deformations of the base pairs.
VDNA does not utilize these descriptors.
Here, let (cid:1) =(Tilt, Roll, Twist) and (cid:1) =(Shift, Slide, Rise) represent the DNA helical parameters.
In general (cid:1) and (cid:1) are functions of s and t where s represents the continuum limit of position measured in base pair steps along the DNA and t represents time.
To obtain a 3D representation of DNA as a as a function of s at time t. VDNA simply provides an interface for deﬁning space curve (cid:1)r with local directors ˆdk requires integration of (cid:1) (s,t) and (cid:1) (s,t) (cid:1) (s,t) and (cid:1) (s,t), integrating the expressions and displaying results.
In case (cid:1) and (cid:1) are time dependent, VDNA automatically integrates (cid:1) and (cid:1) from s= 0 to s= maxS for each instance t = 0 to maxT with a time step of 1.
The result is maxT + 1 structures corresponding to t = 0,1,2,... ,maxT. For the purposes of numerical integration VDNA utilizes El Hassan s algorithm (El Hassan and Calladine, 1995). This algorithm has proven to be extremely fast and robust and ensures invariance to the direction of integration, i.e. integration from s= 0 to maxS or s= maxS to 0 gives the same result.
Note the sign of Shift and Tilt must be changed during a reverse integration.
3 USAGE The main VDNA panel, Figure 1, is generated by selecting Extensions  Visualization  Virtual DNA Viewer from the VMD pull-down menus.
This panel provides text boxes for inputing mathematical expressions for the helical parameters.
These strings are used to create the new procedures omega and gamma which are deﬁned as functions of the variables s and t. As the structure at time t is created the variable s varies from 0 to maxS. In case maxT  0, VDNA will generate a total of maxT + 1 structures.
In all cases, the angular deformations are deﬁned in degrees per base pair step, and the translations are given in angström per base pair step.
Any string that forms a valid argument to the tcl command expr can be utilized in deﬁning the helical parameter statements.
(Consult the examples or a tcl language reference source for more details.) VDNA recognizes several deﬁned variables: V1, V2, Nuc and Lk ($Pi is predeﬁned as π to single precision). The procedures omega and/or gamma utilize whatever text is entered for these variables.
Such variable substitution provides a convenient means of introducing arbitrarily complex expressions into more than one helical parameter statement simultaneously.
The text box Cores deﬁnes the procedure  The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:29 3/11/2009 Bioinformatics-btp566.tex] Page: 3187 3187 3188  T.C.Bishop Fig.2.
Examples: VDNA includes predeﬁned mathematical expressions for local untwisting of DNA, circular DNA, Torsion Helix, Shear Helix, chromatin with and without an explicitly bent linker, thermal motion (shown as a 4-point per base pair molecule without and with RMSD ﬁtting) and nucleosome unwrapping.
We have introduced a general purpose tool for creating ﬁber- like models, four-atom-per-base-pair models and all atom models of DNA from mathematical expressions.
Predeﬁned expressions are provided as building blocks to investigate structural, thermal and dynamical properties including: shear (Shift or Slide) non-uniform Twist a Torsion Helix and a Shear Helix (Bishop, 2008) linker DNA in simple models of chromatin thermal variations and time evolutions. VDNA is written entirely in tcl so it can be readily modiﬁed by users with limited programming experience.
ACKNOWLEDGMENTS John Stone assisted in development of this plug in.
Funding: National Institutes of Health (R01GM076356 to T.C.B.). Conﬂict of Interest: none declared.
REFERENCES Bishop,T.C. (2008) Geometry of the nucleosomal DNA superhelix. Biophys J., 95, 1007 1017.
Dickerson,R.E. (1989) Deﬁnitions and nomenclature of nucleic acid structure components.
Nucleic Acids Res., 17, 1797 1803.
El Hassan,M.A. and Calladine,C.R. (1995) The assessment of the geometry of dinucleotide steps in double-helical DNA a new local calculation scheme.
J. Mol. Biol., 251, 648 664.
Humphrey,W. et al. (1996) VMD: visual molecular dynamics.
J. Mol. Graph., 14, 33 38, 27 28.
Lavery,R. and Sklenar,H. (1988) The deﬁnition of generalized helicoidal parameters and of axis curvature for irregular nucleic acids.
J. Biomol. Struct. Dyn., 6, 63 91.
Lavery,R. et al. (2009) A systematic molecular dynamics study of nearest-neighbor effects on base pair and base pair step conformations and ﬂuctuations in B-DNA. Nucleic Acids Res., [Epub ahead of print, doi:10.1093/nar/gkp834, October 22, 2009]. Lu,X.-J. and Olson,W.K. (2003) 3DNA: a software package for the analysis, rebuilding and visualization of three-dimensional nucleic acid structures.
Nucleic Acids Res., 31, 5108 5121.
Macke,T. and Case,D.A (1998) Modeling unusual nucleic acid structures.
In Leontes,N.B. and SantaLucia,J. Jr., eds.
Molecular Modeling of Nucleic Acids, American Chemical Society, Washington, DC, pp.
379 393.
Tung,C S. and Carter,E.S. (1994) Nucleic acid modeling tool (NAMOT): an interactive graphic tool for modeling nucleic acid structures.
Comput. Appl. Biosci., 10, 427 433.
Vlahovicek,K. and Pongor,S. (2000) Model.it: building three dimensional DNA models from sequence data.
Bioinformatics, 16, 1044 1045.
Fig.1.
The main panel in VDNA, left, provides text boxes for inputing mathematical expressions.
The default model, right, is a parameterization of the Shear Helix that mimics the nucleosome core particle with free linker DNA. It is a torsion-free model so all superhelical pitch arises only from shear.
Users can also select from predeﬁned expressions, draw coarse-grained models, plot the helical parameters, save the parameters in a format suitable for input to 3DNA or make a 4-point per base pair molecule.
A 4-point model is automatically loaded into VMD as a molecule with maxT + 1 frames by the Make Molec button, while the Draw It button loads a graphics object into VMD in which all maxT + 1 structures are superimposed.
In all the cases, the base pair at s= 0 is aligned with the origin.
Molecular representations can be further manipulated, e.g. ﬁt, as was done for thermal model in Figure 2.
IsNuc which is called during the main integration loop with the argument s. When IsNuc= 1, VDNA maintains a running average of (cid:1)r(s). When IsNuc switches to zero, VDNA draws a sphere at the accumulated position and resets the average.
This function is useful for indicating proteins docked to the DNA, e.g. the nucleosome s histone core.
Finally, VDNA includes a number of predeﬁned examples from which to choose, see Figure 2.
Some examples are intended to demonstrate geometric relations and to assist in developing intuition rather than model biologic entities.
These models are not pictured, but include: straight, bent, sheared, polygonal, circular and superhelical structures.
Others are designed to be building blocks for modeling biologically relevant structures, Figure 2, but are not parameterized to represent speciﬁc biophysical entities.
The default model, Figure 1, demonstrates how to combine the generic, predeﬁned Shear Helix and Thermal models to accurately represent a speciﬁc biological entity: a Shear Helix model of nucleosome core particle (Bishop, 2008) with DNA linkers that exhibit average B-DNA conformation and ﬂuctuation properties (Lavery et al., 2009). 4 FUTURE PLANS AND CONCLUSION Features to be included in future versions of VDNA include sequence-dependent values for the helical parameters, greater support for conversion between all atom models and helical parameter models using either 3DNA or Curves and the reading of helical parameters from ﬁle. Presently, the parameter ﬁle produced by VDNA can be used to create an all atom model with 3DNA. 3188 [20:29 3/11/2009 Bioinformatics-btp566.tex] Page: 3188 3187 3188
BIOINFORMATICS APPLICATIONS NOTE Vol.25 no.23 2009, pages 3191 3193 doi:10.1093/bioinformatics/btp570 Systems biology W-ChIPMotifs: a web application tool for de novo motif discovery from ChIP-based high-throughput data Victor X. Jin1, , Jeff Apostolos1, Naga Satya Venkateswara Ra Nagisetty2 and Peggy J. Farnham3 1Department of Biomedical Informatics, The Ohio State University, Columbus, OH 43210, 2Bioinformatics Program, The University of Memphis, Memphis, TN 38152 and 3The Genome Center, The University of California, Davis, CA 95616, USA Received on August 3, 2009 Revised on September 28, 2009 Accepted on September 29, 2009 Advance Access publication October 1, 2009 Associate Editor: Martin Bishop ABSTRACT Summary: W-ChIPMotifs is a web application tool that provides a user friendly interface for de novo motif discovery.
The web tool is based on our previous ChIPMotifs program which is a de novo motif ﬁnding tool developed for ChIP-based high-throughput data and incorporated various ab initio motif discovery tools such as MEME, MaMF, Weeder and optimized the signiﬁcance of the detected motifs by using a bootstrap resampling statistic method and a Fisher test.
Use of a randomized statistical model like bootstrap resampling can signiﬁcantly increase the accuracy of the detected motifs.
In our web tool, we have modiﬁed the program in two aspects: (i) we have reﬁned the P-value with a Bonferroni correction (ii) we have incorporated the STAMP tool to infer phylogenetic information and to determine the detected motifs if they are novel and known using the TRANSFAC and JASPAR databases.
A comprehensive result ﬁle is mailed to users.
Availability: http://motif.bmi.ohio-state.edu/ChIPMotifs. Data used in the article may be downloaded from http://motif.bmi.ohio-state.edu/ChIPMotifs/examples.shtml. Contact: victor.jin@osumc.edu 1 INTRODUCTION DNA motifs are short sequences varying from 6 to 25 bp and can be highly variable and degenerated.
Understanding how transcription factors usually selectively bind to these motifs is important for understanding the logic and mechanisms of gene regulation.
One major approach is using position weight matrices (PWMs Stormo et al., 1982) to represent information content of regulatory sites.
However, when used as the sole means of identifying binding sites suffers from the limited amount of training data available (Roulet et al., 1998) and a high rate of false positive predictions (Tompa et al., 2005). Many de novo motif ﬁnding tools have been developed to detect these unknown motifs.
Typical tools include hidden Markov models (Pedersen and Moult, 1996), Gibbs sampling (Lawrence et al., 1993), exhaustive enumeration (i.e. detecting the set of all nucleotide n-mers, then reporting the most frequent or overrepresented e.g. Weeder (Pavesi et al., 2004),   To whom correspondence should be addressed.
greedy alignment algorithms [e.g. CONSENSUS (Hertz and Stormo, 1999)], expectation-maximization (MEME) (Bailey and Elkan, 1995) and probabilistic mixture modeling (NestedMica Down and Hubbard, 2005). ChIP-based high-throughput techniques such as ChIP-chip (Ren et al., 2000 Weinmann et al., 2002), ChIP-seq (Barski et al., 2007 Robertson et al., 2007) and ChIP-PET (Loh et al., 2006) have been used to interrogate protein DNA interactions in intact cells and is well-documented in many comprehensive reviews (Hanlon and Lieb, 2004). The identiﬁed enrichment DNA sequences usually ranging from 150 to 1500 bases from these techniques are currently considered to be highly reliable datasets for detecting the novel motif.
Many computational tools including ours (Ettwiller et al., 2007 Gordon et al., 2005 Hong et al., 2005 Jin et al., 2007) have been recently developed to de novo ﬁnd the motifs for the data generated from these techniques.
There exist many kinds of available computational tools.
However, most of them are platform-dependent stand-alone executable programs, and not easily used by biologists.
In this application, we have built a web-based de novo motif discovery tool for identifying novel motifs for ChIP-based high-throughput techniques.
Although the web tool is based on our previous program, ChIPMotifs, we have signiﬁcantly modiﬁed the program with a reﬁned P-value computation using Bonferroni correction and incorporated a new STAMP tool (Mahony and Benos, 2007) to ﬁnd the phylogenetic information and similar motifs in TRANSFAC (Wingender et al., 2000) and JASPAR (Sandelin et al., 2004) databases.
The web interface is friendly and accessible by this research community.
2 DESCRIPTION OF W-ChIPMotifs Usage of W-ChIPMotifs web service is simple and does not require any knowledge of the underlying software.
The structure of W-ChIPMotifs is shown in Figure 1.
There are three required inputs from the user: the DNA sequence data, contact information and a transcription factor name.
DNA sequences are required to be in the FASTA format.
They can be uploaded either by selecting an existing ﬁle, or by directly copying the data into the form.
Results will be emailed to the address given in the contact information.
The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:32 3/11/2009 Bioinformatics-btp570.tex] Page: 3191 3191 3193  V.X.Jin et al. Input Data Ab initio Motif Discovery  Programs Statistical Methods STAMP Matching Results (cid:127)FASTA file (cid:127)Contact Info (cid:127)Control data (optional) (cid:127)Weeder (cid:127)MaMf (cid:127)MEME (cid:127)Bootstrap re-sampling (cid:127)Fisher test (cid:127)SeqLog (cid:127)PWM (cid:127)P-value (cid:127)Known or novel motifs Fig.1.
A schematic view of W-ChIPMotifs. The transcription factor name is used as a label in the results.
Also, control data can be speciﬁed as an optional input, which is used to infer the statistical signiﬁcance for detected motifs.
In case of no control data input from users, we will use default control datasets where we randomly selected 5000 promoter sequences per run from all human or mouse promoter sequences depending on the user selected species.
After the server validates and retrieves the input, the DNA sequences are processed by a group of existing ab initio motif discovery programs.
This group is currently composed of MEME (Bailey and Elkan, 1995), MaMF (Hon and Jain, 2006) and Weeder (Pavesi et al., 2004). These three are frequently used by the community, and have proven to be relatively accurate in detecting motifs.
The programs are included in a modular fashion which enables the easy addition of other components in the future.
Using these programs, we identiﬁed a set of n candidate motifs (usually 10 motifs), then constructed n PWMs for each candidate motif.
A bootstrap resampling method is then used to infer the optimized PWM scores.
In this method, a new dataset is created by randomizing the user input s sequences of each with 100 times.
This new set no longer corresponds to the original ChIP identiﬁed binding sequences, but shares the same nucleotide frequencies and therefore can be used as a negative control set.
The negative control is used for scanning the identiﬁed motifs at a minimal core score of 0.5 and a minimal PWM score of 0.5.
Then, we retrieve core and PWM scores at the top 0.1, 0.5 and 1% percentiles.
A Fisher test was applied and the P-value was used to deﬁne the signiﬁcant cutoff for these scores.
We also apply the Bonferroni correction by adjusting the P-value multiplying by the number of samples being input.
If the adjusted P-value ended up 1.0, it would be rounded down to 1.0.
To provide users with more ﬂexible and useful information about detected motifs, W-ChIPMotifs also uses the STAMP tool (Mahony and Benos, 2007) to determine if the motifs are known or novel by ﬁnding phylogenetic information and motif similarity matches in the TRANSFAC and JASPAR databases.
Phylogenetic information implemented in STAMP tool is based on two tree- building algorithms: an agglomerative method and a divisive 3192 method.
Both take input motifs PWMs aligned by multiple alignment strategies, and iteratively build tree nodes until reaching each leaf node containing a single PWM. The results from W-ChIPMotifs are composed of two ﬁles. The ﬁrst ﬁle contains detected motifs with their SeqLOGOs, PWMs, core and PWM scores, P-values and Bonferroni correction P-value at different percentile levels.
The second ﬁle contains matched similar motifs from the STAMP tool.
These ﬁles are in PDF format.
In the future, we plan on adding more accurate and efﬁcient motif detecting programs, and optimizing the running time of the statistical methods.
3 IMPLEMENTATION W-ChIPMotifs is written in Perl, and uses a web interface developed with PHP. Multiple scripts are used to produce output from the included motif discovery programs, parse this output and apply statistical techniques.
The sequence logos for the motifs are generated using the WEBLOGO tool (Crooks et al., 2004). The open-source HTMLDOC program is used to convert these logos to PDF format (http://www.htmldoc.org/). A tree from the newicks format is created with the DRAWTREE tool.
The PHPGmailer package is used for sending results to the user from the W-ChIPMotifs email account.
4 SAMPLE TESTS The W-ChIPMotif server is tested with different well-known datasets from the ChIP-seq and ChIP-chip experiments with different sizes of inputs.
Some of such datasets include E2F4, FOXA1, NRSF and OCT4, the test data and results are available online at http://motif. bmi.ohio-state.edu/ChIPMotifs/examples.shtml. ACKNOWLEDGEMENTS We thank the members of our teams for discussion and comments.
Funding: Department of Biomedical Informatics, The Ohio State University.
Conﬂict of Interest: none declared.
REFERENCES Bailey,T.L. and Elkan,C. (1995) The value of prior knowledge in discovering motifs with MEME.
Proc. Int.
Conf. Intell. Syst. Mol. Biol., 3, 21 29.
Barski,A. et al. (2007) High-resolution proﬁling of histone methylations in the human genome.
Cell, 129, 823 837.
Crooks,G. et al. (2004) WebLogo: A sequence logo generator.
Genome Res., 14, 1188 1190.
Down,T.A. and Hubbard,T.J. (2005) NestedMICA: sensitive inference of over- represented motifs in nucleic acid sequence.
Nucleic Acids Res., 33, 1445 1453.
Ettwiller,L. et al. (2007) Trawler: de novo regulatory motif discovery pipeline for chromatin immunoprecipitation. Nat. Methods, 4, 563 565.
Gordon,D.B. et al. (2005) TAMO: a ﬂexible, object-oriented framework for analyzing transcriptional regulation using DNA-sequence motifs.
Bioinformatics, 21, 3164 3165.
Hanlon,S. and Lieb,J. (2004) Progress and challenges in proﬁling the dynamics of chromatin and transcription factor binding with DNA microarrays. Curr. Opin. Genet. Dev., 14, 697 705.
Hertz,G.Z. and Stormo,G.D. (1999) Identifying DNA and protein patterns with statistically signiﬁcant alignments of multiple sequences.
Bioinformatics, 15, 563 577.
[20:32 3/11/2009 Bioinformatics-btp570.tex] Page: 3192 3191 3193  W-ChIPMotifs Hon,L.S. and Jain,A.N. (2006) A deterministic motif ﬁnding algorithm with application Ren,B. et al. (2000) Genome-wide location and function of DNA binding proteins.
to the human genome.
Bioinformatics, 22, 1047 1054.
Science, 290, 2306 2309.
Hong,P. et al. (2005) A boosting approach for motif modeling using ChIP-chip data.
Bioinformatics, 21, 2636 2643.
Jin,V.X. et al. (2007) Identication of cis-regulatory modules for OCT4 using de novo motif discovery and integrated computational genomics approaches.
Genome Res., 17, 807 817.
Robertson,G. et al. (2007) Genome-wide proﬁles of STAT1 DNA association using chromatin immunoprecipitation and massively parallel sequencing.
Nat. Methods, 4, 651 657.
Roulet,E. et al. (1998) Evaluation of computer tools for the prediction of transcription factor binding sites on genomic DNA. In Silico Biol., 1, 21 28.
Lawrence,C. et al. (1993) Detecting subtle sequence signals: a Gibbs sampling strategy Sandelin,A. et al. (2004) JASPAR: an open-access database for eukaryotic transcription for multiple alignment.
Science, 262, 208 214.
Loh,Y.-H. et al. (2006) The Oct4 and Nanog transcription network regulates pluripotency in mouse embryonic stem cells.
Nat. Genet., [Epub ahead of print, March 5, 2006] Mahony,S. and Benos,P.V. (2007) STAMP: a web tool for exploring DNA-binding motif similarities.
Nucleic Acids Res., 35, W253 W258. Pavesi,G. et al. (2004) Weeder web: discovery of transcription factor binding sites in a set of sequences from co-regulated genes.
Nucleic Acids Res., 32, W199 W203. Pedersen,J.T. and Moult.,J. (1996) Genetic algorithms for protein structure prediction.
Curr. Opin. Struct. Biol., 6, 227 231. factor binding proﬁles. Nucleic Acids Res., 32, D91 D94. Stormo,G.D. et al. (1982) Use of the Perceptron algorithm to distinguish translational initiation sites in E. coli. Nucleic Acids Res., 10, 2997 3011.
Tompa,M. et al. (2005) Assessing computational tools for the discovery of transcription factor binding sites.
Nat. Biotechnol., 23, 137 144.
Weinmann,A.S., et al. (2002) Isolating human transcription factor targets by coupling chromatin immunoprecipitation and CpG island microarray analysis.
Genes Dev., 16, 235 244.
Wingender,E. et al. (2000) TRANSFAC: an integrated system for gene expression regulation.
Nucleic Acids Res., 28, 316 319.
[20:32 3/11/2009 Bioinformatics-btp570.tex] Page: 3193 3191 3193 3193
BIOINFORMATICS APPLICATIONS NOTE Vol.27 no.8 2011, pages 1152 1154 doi:10.1093/bioinformatics/btr092 Advance Access publication February 23, 2011 Genome analysis ACT: aggregation and correlation toolbox for analyses of genome tracks Justin Jee1,2, , Joel Rozowsky3, , Kevin Y. Yip3,4, , Lucas Lochovsky1, Robert Bjornson5, Guoneng Zhong3, Zhengdong Zhang3, Yutao Fu6, Jie Wang7, Zhiping Weng7 and Mark Gerstein1,3,5, 1Program in Computational Biology and Bioinformatics, 2Department of Molecular, Cellular and Developmental Biology, 3Department of Molecular Biophysics and Biochemistry, Yale University, New Haven, CT, USA, 4Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, 5Department of Computer Science, Yale University, New Haven, CT, 6Bioinformatics Program, Boston University, Boston and 7Program in Bioinformatics and Integrative Biology, University of Massachusetts Medical School, Worcester, MA, USA Associate Editor: Alfonso Valencia ABSTRACT We have implemented aggregation and correlation toolbox (ACT), an efﬁcient, multifaceted toolbox for analyzing continuous signal and discrete region tracks from high-throughput genomic experiments, such as RNA-seq or ChIP-chip signal proﬁles from the ENCODE and modENCODE projects, or lists of single nucleotide polymorphisms from the 1000 genomes project.
It is able to generate aggregate proﬁles of a given track around a set of speciﬁed anchor points, such as transcription start sites.
It is also able to correlate related tracks and analyze them for saturation i.e. how much of a certain feature is covered with each new succeeding experiment.
The ACT site contains downloadable code in a variety of formats, interactive web servers (for use on small quantities of data), example datasets, documentation and a gallery of outputs.
Here, we explain the components of the toolbox in more detail and apply them in various contexts.
Availability: ACT is available at http://act.gersteinlab.org Contact: pi@gersteinlab.org Received on July 24, 2010 revised on November 2, 2010 accepted on November 23, 2010 1 INTRODUCTION There is now an abundance of genome-sized data from high- throughput genomic experiments.
For instance, there are ChIP-chip, ChIP-seq and RNA-seq experiments from the ENCODE (ENCODE Project Consortium, 2007) and modENCODE (modENCODE consortium, 2009) projects.
There are also genome sequence data that can be used to generate tracks measuring sequence content, such as the densities of single nucleotide polymorphisms (SNPs) from dbSNP (Sharry et al., 2001) and the 1000 genomes project.
In most cases, the representations of these data take the form of either signal tracks that describe a genomic landscape or distinct region tracks that tag portions of the genome as active.
The aggregation and correlation toolbox (ACT) provides a powerful set of programs that  To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the ﬁrst three authors should be regarded as joint First Authors.
can be applied to any experiments producing data in these formats.
The ability to analyze multiple genomic datasets is important, as demonstrated by tools like Galaxy (Giardine et al., 2005). ACT provides a unique set of functionality that complements existing methods of analysis.
2 THE ACT TOOLBOX: OVERVIEW ACT facilitates three main types of analysis: Aggregation: in many scenarios, it is useful to determine the distribution of signals in a signal track relative to certain genomic anchors (Fig. 1, aggregation). For example, it has recently been reported that the contribution of each transcription factor binding site to tissue-speciﬁc gene expression depends on its position relative to the transcription start site (TSS) (MacIssac et al., 2010). It is thus useful to aggregate binding signals of transcription factors at a certain distance from the TSSs of all genes (the anchors). In general, this type of aggregation analyses helps identify proximity correlations and functional relationships between the signals and anchors.
In the ENCODE pilot study (ENCODE Project Consortium, 2007), it has been used to demonstrate positional relationships between chromatin features and TSSs. Correlation: it is also useful to consider how multiple-related signal tracks are correlated with each other.
For example, a previous study (Zhang et al., 2007) demonstrated, using whole-track correlation methods, that there was a consistent relationship among transcription factors as judged by their signal proﬁles across several ChIP-chip experiments.
By providing a means of correlating signal tracks with each other, ACT allows for initial comparison of different experiments to see which are more similar or related than others (Fig. 1, correlation). Saturation: another important type of analysis is determining the number of experimental conditions required to achieve a high genomic coverage of the biological phenomenon under study.
For example, using ChIP-chip or ChIP-seq experiments, one could identify a set of transcription factor binding sites from a human cell line.
When the experiment is repeated using another cell line, some additional binding sites could be identiﬁed. How many cell lines need to be considered in order to reach the point of saturation, so  The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:43 21/3/2011 Bioinformatics-btr092.tex] Page: 1152 1152 1154  ACT data can be especially time intensive, the version of the tools presented here has been designed to run efﬁciently on large datasets. Aggregation: the aggregation component is designed to take a signal track (.sgr or.wig) and an annotation track (.bed) as input, and compute the average signal over a certain number of base pairs upstream and downstream of (i.e. a ﬁxed radius around) the annotations.
In other words, signal values are taken from the region surrounding each annotation, and averaged over the number of annotation anchors provided.
The base pair resolution of the aggregation can be speciﬁed by the number of bins (narrower bins give more data points and therefore ﬁner granularity). Results of such calculation can be plotted as in Figure 1 (aggregation). ACT also provides features such as computing the standard deviation, median and quartiles that can be viewed as a boxplot, as well as scaling aggregation over regions such as areas between transcription start and end sites or within exons so that all of the aggregate signals within those regions fall into a ﬁxed number of bins.
In this case, bin size is dynamically computed for each region so that the same number of bins cover regions of different sizes.
Correlation: the correlation analysis takes a set of active genomic regions (.bed) such as a SNP track or a genomic signal track (.wig). It then divides genomic coordinates into bins and gives each bin a value corresponding to the mean or maximum signal values which fall within the bin, or assigns value based on the number of active regions which fall within the bin.
A ﬁnal correlation matrix is created based on either the Spearman s, Pearson s or normal score correlation between each pair of binned datasets. The results can be visualized as a heatmap or as a phylogenetic tree using programs such as PHYLIP (Felsenstein, 1996). One version of the correlation tool uses parallelization to decrease the pro-gram s overall running time.
This component was written largely in Java.
Examples of correlation output based on SNP tracks and ChIP-chip data are shown in Figure 1 (correlation). Saturation: we provide an efﬁcient implementation of saturation plot generator.
Each input ﬁle corresponds to one dataset (e.g. one new individual, in.bed format), and each line in a ﬁle speciﬁes a genomic location that has the biological phenomenon under study (e.g. tagged SNPs). The saturation plot shows, with each new dataset (x-axis), what percentage of genomic base pairs are covered (y-axis). The program considers the various combinations in which tracks can be added so that the increase in base pair coverage is a range of values based on all the ﬁles in the input.
The resulting plot is output in PDF format (Fig. 1, saturation), in which a series of boxplots depicts increasing base pair coverage, where the boxplot at each position m on the x-axis shows the coverage values of all combinations of m conditions.
Boxplots that approach a horizontal asymptote indicate that the coverage has reached saturation.
Our implementation makes use of special data structures to avoid redundant counting.
It normally takes less than a minute to generate the plot for up to 30 input ﬁles each with a few thousand lines.
To handle more ﬁles and ﬁles with more lines, the tool also provides an option to compute the coverage of a random sample of the input ﬁle combinations.
4 DISCUSSION There are number of additional analyses that can be done to ﬁne- tune the output of ACT.
For instance, it is possible to use the online genomic signal aggregator (GSA), which assigns each genomic position to the nearest anchor in order to reduce the artifacts caused 1153 Fig.1.
Uses of ACT using signal tracks from various sources.
Signal around all TSSs is aggregated to give an average signal proﬁle, for example of Baf155 binding around TSSs (Encode Project) (aggregation). Figure made in Excel (correlation). Multiple signal tracks are correlated to show which tracks are more or less related to each other.
In the selected example, a heatmap of the SNP track correlation between four individuals (dbSNP) leads to a dendogram of their phylogenetic relationship.
Figure made using Web ACT.
Each additional signal track increases the number of base pairs covered (saturation). When the addition of signal tracks is considered in all possible combinations, the average increase in coverage, with error bars, can be visualized by a saturation plot.
In the example, data are taken from individuals from dbSNP [with additional genomes from Ahn et al. (2009), Bentley et al. (2008), Drmanac et al. (2010), Kim et al. (2009)]. In each box plot, the top and bottom pink bars correspond to the maximum and minimum normal values, the top edge, middle line and bottom edge of the box correspond to the top 25 percentile, median and bottom 25 percentile, the black dot is the mean, and red circles are outliers.
Figure made using ACT downloadable saturation program.
that few new binding sites would be identiﬁed by extra experiments ACT produces plots that help answer this type of question.
3 DETAILS AND USE CASES ACT is available as a suite of downloadable scripts corresponding to the aggregation, correlation and saturation components of the toolbox.
The tool is intended for Linux/Unix users with Java and Python.
In addition, it is useful to have R for output visualization for the aggregation and correlation tools.
There is also a compendium of other versions of the tool components written in different languages and with varied functionality.
For some types of analysis, there are web components for demonstration purposes on small datasets with built-in visualization features.
However, because most whole- genome signal tracks are too large to upload via standard Internet connections, users are recommended to download the toolbox and run it locally.
As performing these calculations on whole-genome [11:43 21/3/2011 Bioinformatics-btr092.tex] Page: 1153 1152 1154  J.Jee et al. by the subsets of anchors clustering together, to handle tightly clustered anchors.
Also, aggregation can be used in conjunction with genome structure correction to determine if the enrichments of a given signal with respect to anchor points are signiﬁcantly relative to the non-random positioning of the anchors (ENCODE Project Consortium, 2007). This correction takes into account the fact that a random distribution of anchors on the genome arises from a distinctly non-uniform distribution.
Practically, this could be carried out through ACT by comparing the aggregation over anchors (e.g. TSSs) to that from randomized anchors , where the latter is generated by shifting anchor coordinates along the chromosome or transferring anchor coordinates from a second chromosome to the one of interest.
Finally, ACT can be used as a starting point for other downstream analyses.
In the instance of RNA-seq data tracks, further analysis can be conducted with RseqTools (Habegger et al., 2011) to, for example, determine additional similarities between two or more highly correlated tracks.
The results of correlation analysis, for instance, can also be fed into downstream principal component analysis, allowing for grouping of coregulating factors with their coregulated sites.
This would simply involve diagonalization of the output correlation matrix from ACT.
Saturation analysis can also be used to inform future experimental design.
Funding: National Institute of Health A.L. Williams Professorship funds.
Conﬂict of Interest: none declared.
REFERENCES Ahn,S.M. et al. (2009) The ﬁrst Korean genome sequence and analysis: full genome sequencing for a socio-ethnic group.
Genome Res., 19, 1622 1629.
Bentley,D.R. et al. (2008) Accurate whole human genome sequencing using reversible terminator chemistry.
Nature, 456, 53 59.
Drmanac,R. et al. (2010) Human genome sequencing using unchained base reads on self-assembling DNA nanoarrays. Science, 327, 78 81.
ENCODE Project Consortium (2007) Identiﬁcation and analysis of functional elements in 1% of the human genome by the ENCODE pilot project.
Nature, 447, 799 816.
Felsenstein,J. (1996) Inferring phylogenies from protein sequences by parsimony, distance, and likelihood methods.
Methods Enzymol. 266, 418 427.
Giardine,B. et al. (2005) Galaxy: a platform for interactive large-scale genome analysis.
Genome Res., 15, 1451 1455.
Habegger,L. et al. (2011) RSEQtools: a modular framework to analyze RNA-Seq data using compact, anonymized data summaries.
Bioinformatics, 27, 281 283.
Kim,J.I. et al. (2009) A highly annotated whole-genome sequence of a Korean individual.
Nature, 460, 1011 1015.
MacIssac,K.D. et al. (2010) A quantitative model of transcriptional regulation reveals the inﬂuence of binding location on expression.
PLoS Comput. Biol., 6, e1000773. modENCODE Consortium (2009) Unlocking the secrets of the genome.
Nature, 18, 927 930.
Sharry,S.T. et al. (2001) dbSNP: the NCBI database of genetic variation.
Nucleic Acids Res., 29, 308 311.
Zhang,Z.D. et al. (2007) Statistical analysis of the genomic distribution and correlation of regulatory elements in the ENCODE regions.
Genome Res., 17, 787 797.
1154 [11:43 21/3/2011 Bioinformatics-btr092.tex] Page: 1154 1152 1154
BIOINFORMATICS DISCOVERY NOTE Vol.26 no.17 2010, pages 2071 2075 doi:10.1093/bioinformatics/btq405 Genome analysis Genome-wide histone acetylation data improve prediction of mammalian transcription factor binding sites Stephen A. Ramsey, Theo A. Knijnenburg, Kathleen A. Kennedy, Daniel E. Zak, Mark Gilchrist, Elizabeth S. Gold, Carrie D. Johnson, Aaron E. Lampano, Vladimir Litvak, Garnet Navarro, Tetyana Stolyar, Alan Aderem and Ilya Shmulevich  Advance Access publication July 27, 2010 Institute for Systems Biology, 1441 North 34th Street, Seattle, WA, 98103, USA Associate Editor: Alfonso Valencia (TF) binding in mammalian cells.
ABSTRACT Motivation: Histone acetylation (HAc) is associated with open chromatin, and HAc has been shown to facilitate transcription factor In the innate immune system context, epigenetic studies strongly implicate HAc in the transcriptional response of activated macrophages.
We hypothesized that using data from large-scale sequencing of a HAc chromatin immunoprecipitation assay (ChIP-Seq) would improve the performance of computational prediction of binding locations of TFs mediating the response to a signaling event, namely, macrophage activation.
Results: We tested this hypothesis using a multi-evidence approach for predicting binding sites.
As a training/test dataset, we used ChIP- Seq-derived TF binding site locations for ﬁve TFs in activated murine macrophages.
Our model combined TF binding site motif scanning with evidence from sequence-based sources and from HAc ChIP- Seq data, using a weighted sum of thresholded scores.
We ﬁnd that using HAc data signiﬁcantly improves the performance of motif- based TF binding site prediction.
Furthermore, we ﬁnd that within regions of high HAc, local minima of the HAc ChIP-Seq signal are particularly strongly correlated with TF binding locations.
Our model, using motif scanning and HAc local minima, improves the sensitivity for TF binding site prediction by 50% over a model based on motif scanning alone, at a false positive rate cutoff of 0.01.
Availability: source model http://magnet.systemsbiology.net/hac. Contact: aderem@systemsbiology.org ishmulevich@systemsbiology.org Supplementary information: Supplementary data are available at Bioinformatics online.
for training and validation are freely available online at software code data and The Received on May 27, 2010 revised on June 30, 2010 accepted on July 3, 2010 1 INTRODUCTION Mammalian cells exhibit diverse transcriptional proﬁles across different cell types and conditions, for example, in immune cells activated with different pathogen-associated molecules (Ramsey et al., 2008). To a large extent, these proﬁles are controlled by the arrangement and chromatin accessibility of cis-regulatory   To whom correspondence should be addressed.
elements (Berger, 2007). Transcription factors (TFs) bind speciﬁc sequence elements in chromatin locations of permissive epigenetic or conformational states, leading to activation or repression of transcriptional activity.
For mapping these regulatory interactions, it is particularly promising that the binding of a TF can now be measured genome wide using chromatin immunoprecipitation (IP) with sequence detection (ChIP-Seq, see Johnson et al., 2007). However, antibody and cellular material requirements preclude using ChIP-Seq to screen for all TFs mediating a transcriptional response.
There remains a need for computational approaches that can, in the absence of experimental TF binding data, leverage transcriptional data and genomic information to identify the network of TFs and binding sites that underlies a transcriptional response.
An important tool for predicting mammalian TF binding sites is motif scanning, i.e. searching DNA sequence for matches within a library of sequence motifs reported to be bound by speciﬁc TFs (Lähdesmäki et al., 2008). Such a library enables mapping between a scanning-identiﬁed sequence element and one or more candidate TFs that may bind it.
However, such motifs are often highly uncertain and they can be degenerate, leading to a high frequency of false positive predictions (Hannenhalli, 2008). Furthermore, mammalian cis-regulatory elements can be tens of kilobases from transcription start sites, necessitating searching large sequence regions and further increasing false positives.
These issues undermine the performance of motif scanning as a standalone approach.
Successful motif- based prediction of TF binding depends on identifying the sequence regions, within the relevant cell type, that are likely to contain cis- regulatory elements (Ernst et al., 2010 Wasserman and Sandelin, 2004 Whitington et al., 2009). It has been observed that cis-regulatory elements tend to co- occur with chromatin or sequence features that can be grouped in three categories: (i) chromatin structural features such as DNase I hypersensitive sites (ii) epigenetic marks such as histone acetylation (HAc) and (iii) sequence features such as high GC content and conservation across species.
The HAc mark, which has been associated with active promoters and open chromatin (Vettese- Dadey et al., 1996), is of particular relevance to transcriptional regulation because the modiﬁcation can be placed or removed in response to the cellular state.
These observations have spurred the development of approaches that integrate data for multiple types of chromatin features to improve the accuracy of TF binding site predictions.
Various data integration frameworks for binding site prediction have been used, including the support vector machine (Holloway et al., 2005 Nykter et al., 2009), probabilistic methods  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:29 12/8/2010 Bioinformatics-btq405.tex] Page: 2071 2071 2075  S.A.Ramsey et al. A B Fig.1.
Local minima in the HAc ChIP signal correlate with TF binding.
(A) A 12 kbp region of mouse chromosome 11 including the gene Ccl5 [an LPS- regulated cytokine with multiple NFκB (nuclear factor of kappa light polypeptide gene enhancer in B-cells) sites in its promoter] and its upstream regulatory region, with TF binding data and various feature tracks.
Measured binding locations for the TF NFκB/p50 are shown in orange (top row). Each feature track is displayed in colored vertical bars interspersed every 100 bp: HAc ChIP-Seq signal (magenta)  valley scores (VS) for HAc local minima (blue) normalized NFκB binding site motif match scores (red) DNA sequence GC content (green) nucleosome occupancy score (green) and vertebrate conservation score for genomic sequence (green). The NFκB binding sites correspond to local minima in the HAc ChIP-Seq signal.
Inset: Within a local minimum of the ChIP-Seq signal (magenta curve), the smaller of the maximum signal values on either side of the local minimum is computed, and the entire local minimum region is assigned that value as its VS, and the value zero outside the local minimum region (blue lines) (Supplementary Section S1.6). (B) The distribution of HAc ChIP VS from stimulated cells in TF-bound sites differs substantially from non-TF-bound sites, as shown in the two histograms (note the logarithmic vertical scale). (Beyer et al., 2006 Ernst et al., 2010 Lähdesmäki et al., 2008), and a kernel-based classiﬁer (Wang et al., 2009). Early studies integrating genomic data into binding site prediction were carried out in yeast (Beyer et al., 2006 Holloway et al., 2005), or in mammals using ground-truth datasets that were not cell type-speciﬁc (Lähdesmäki et al., 2008). More recent approaches have used cell type-speciﬁc mammalian epigenetic or transcriptional data to predict binding for a single TF in mammals (Nykter et al., 2009 Wang et al., 2009). Other recent studies have used genome-wide datasets for multiple TFs to develop prediction models without directly incorporating epigenetic data into the model (Won et al., 2009 Zhou et al., 2010). Two recent studies incorporated histone methylation ChIP-Seq data into multi-evidence prediction models, using ChIP-derived ground-truth datasets of 10 and 13 TFs, respectively (Whitington et al., 2009 Won et al., 2010). Whitington et al. found that predictions are improved when the methylation data are derived from the same tissue type from which the TF binding site measurements are derived.
In this study, we investigated the hypothesis that incorporating HAc ChIP-Seq data into a multi-evidence, motif scanning-based model can improve TF binding site predictions.
We further studied whether prediction performance is improved when the HAc data are derived from the same cell condition from which the TF binding data (used for evaluating performance) are derived.
Having observed that TF binding locations frequently occur at local minima of HAc ChIP-Seq signal within regions of high HAc (see Fig.1 and Supplementary Fig.S1), we also studied the predictive utility of valley scores (VS) assigned to local minima of the HAc ChIP- Seq signal.
Following our previous investigation of the regulatory network underlying macrophage activation (Ramsey et al., 2008), this study was carried out using TF binding and HAc measurements in the macrophage, a key cell type of the innate immune system.
When activated by exposure to a pathogen-associated molecule such as lipopolysaccharide (LPS), the macrophage undergoes extensive transcriptional reprogramming that is mediated in part by alterations in HAc (Aung et al., 2006). 2 APPROACH The HAc hypothesis was tested using an integrative TF binding site (TFBS) prediction framework and using an approach designed to estimate the performance that the prediction model would have on a novel TF for which only a binding site motif is available.
As features, the framework used motif scanning data (Supplementary Fig.S2) along with subsets of seven non-TF-speciﬁc features selected for their potential association with TFBSs. As shown in Figure 1A and Supplementary Table S1, the features consisted of HAc (acetylated H4) ChIP-Seq data from activated and non-activated macrophages VS derived from the HAc data and three features based on genomic sequence (GC content, vertebrate species conservation and a nucleosome occupancy prediction score). A peak in the HAc VS signal corresponds to a local minimum in the HAc ChIP-Seq signal.
As a ground-truth TFBS dataset, we used ChIP-Seq data, from activated macrophages, for ﬁve TFs (Supplementary Table S2). In keeping with the study goals, the ChIP-Seq data were not used to improve the motifs, and model performance was tested using binding data for a TF that was not used in the model training.
Performance measurements obtained using such a TF-based cross-validation are, in our view, more relevant to this application (library-based motif 2072 [09:29 12/8/2010 Bioinformatics-btq405.tex] Page: 2072 2071 2075  scanning) than are results from chromosome-based cross-validation.
Importantly, HAc was measured in LPS-stimulated macrophages, consistent with the conditions for TF binding measurements.
TF binding site predictions were made in adjacent 100 bp intervals spanning 10 kb promoter regions of genes that are expressed in murine macrophages.
A value for each TF prediction feature was computed within each 100 bp interval, from the feature s raw data.
A weighted, thresholded linear model class was used to combine the motif scanning feature with zero, one or two additional features to predict binding for the ﬁve TFs. This model class divides the range of each feature s values into three regimes: below minimum threshold (value changes within this regime are not informative), above the maximum threshold (saturated changes are also not informative) and within the linear response range.
Fifteen models, each using a different combination of features (Supplementary Table S3), were trained using the ground-truth dataset. Because the model using HAc ChIP VS performed the best among the two-feature models, the three-feature model analysis was restricted to models with motifs, HAc ChIP VS and one additional feature.
For training, each model s parameters were optimized to maximize the average prediction performance for a set of four TFs, with the performance metric being the area under the sensitivity versus false positive rate (FPR) curve.
The performance of the model, with the best parameter set from the training, was then tested on the ﬁfth TF, and averaged over the leave-one-out cross-validation.
3 METHODS Complete methods are described in Supplementary Material, Section S1. Ground-truth dataset: ChIP-Seq assays were performed for the TFs ATF3, C/EBPδ, IRF1, NFκB/p50 and NFκB/p65 in macrophages activated through treatment with puriﬁed Toll-like receptor agonists for 1 6 h (see Supplementary Table S2 and Section S1.4). Binding locations were identiﬁed from above-threshold locations in the ChIP-Seq signal, as described in Supplementary Section S1.7. Prediction features: TF predictions were made in 100 bp intervals (as used in Won et al., 2010) of transcript-proximal regions comprising 7% of the genome, selected as described in Supplementary Section S1.2. Combinations of eight features, individually listed in Supplementary Table S1 and labeled by index f , were used for TF binding prediction.
Feature f =1, which conferred TF speciﬁcity to the predictions, was based on motif scanning.
For each TF, motif position-weight matrices (PWMs) corresponding to the TF were obtained from TRANSFAC (Supplementary Table S2 and Section S1.3). Sequences were scanned for motif matches using a likelihood- based algorithm (Lähdesmäki et al., 2008), and combined to obtain, within each interval and for each TF, a score representing the strength of the best match for any motif corresponding to that TF, at any position within the interval.
Features 2 5 of Supplementary Table S1 were derived from HAc ChIP-Seq assays of unstimulated macrophages or macrophages stimulated for 1, 4 or 6 h with LPS (Supplementary Sections S1.4 1.5). VS for HAc local minima were computed as described in Supplementary Section S1.6. Features 6 8 were based on genomic sequence, and thus are not macrophage speciﬁc. For the stimulated-cell HAc ChIP-Seq features (Supplementary Table S1, rows 2 and 4), the time point for the HAc dataset that was used was always the same as the time point of the ground-truth dataset for the TF for which predictions were being made.
Prediction model: within each interval i, the model integrates a set F of up to three features (always including the motif feature, f =1) by a weighted sum of thresholded feature values.
Feature values may depend on the TF t, as is the case for motif scanning, or on the cellular condition for which TF binding predictions are being made (as is the case for HAc-derived features). The value for feature f at interval i and TF t is therefore denoted by vﬁt. Ac-Histone data improve binding site prediction The feature value vﬁt is passed through a piecewise-linear function θf that is deﬁned by feature-speciﬁc thresholds λf and µf ,    θf (v)= 0, v  λf , (v λf )/(µf λf ), λf  v µf , v  µf. 1, (1) The prediction score σit that the TF t binds within interval i is obtained by a weighted sum of thresholded contributions, but with a multiplicative factor enforcing a minimum TF-speciﬁc motif match value for a non-zero σit, (cid:4)(cid:5) σit = µ(θ1(v1it)) (cid:6) , f F ωf θf (vﬁt) (2) where the weight vector (cid:5)ω has unit L1 norm (a negative component would represent a feature that is anti-correlated with TF binding), and where µ is deﬁned by µ(x)=0 if x 0 and µ(x)=1 if x 0.
Importantly, a given model instance M, deﬁned by the tuple F,(cid:5)λ,(cid:5)µ,(cid:5)ω , is TF independent.
Performance metric: for a given model M, TF t, and prediction score cutoff σ, the set of intervals (σ,t) for which σit  σ were predicted to contain binding sites for t (remaining intervals were predicted to have no t binding). The set of intervals containing ground-truth binding sites (based on ChIP- Seq) is denoted by (t). Because the typical ChIP-Seq fragment size was 160 bp, some TF binding locations appeared as adjacent intervals in (t) these were counted as single binding sites.
The number of ground-truth binding sites B(t) was counted (Supplementary Table S2), and the fraction of these binding sites that coincided with at least one interval i  (σ,t), was computed as the sensitivity S(σ,t). The FPR E(σ,t) was computed by dividing the number of intervals in the set difference (σ,t)\ (t) by the number of intervals not contained in (t). The cutoff σ was varied and the resulting (E(σ,t),S(σ,t)) function [receiver operating characteristic (ROC) curve] was numerically integrated over the range 0  E 0.01 to obtain the TF-speciﬁc performance score A(t). For model training (Supplementary Section S1.12), the cost function used was C(t)=1 A(t)/0.01. During training, cases where it was not possible to obtain a sufﬁcient number of (S,E) samples were handled using a penalty, as described in Supplementary Section S1.11. Model training: groups of four TFs at a time were selected for model training, and for a given model M, the cost was averaged over the four TFs, C=(cid:7)C(t)(cid:8)t. Model parameters were varied to minimize C subject to constraints on (cid:5)λ, (cid:5)µ and (cid:5)ω, using a two-stage optimization process (Supplementary Section S1.12), to obtain the best parameter set for the model with features F. (cid:9) ) Model testing: for both training and testing purposes, the performance A(t of the model with the best parameter set from the training, was measured on (cid:9) the ﬁfth TF t ) were compared between different feature groups F using a paired t-test, and summarized in terms of the mean and SD (Supplementary Table S3). using leave-one-out cross-validation.
The ﬁve values for A(t (cid:9) the TF speciﬁcity of 4 RESULTS Feature distributions: ﬁrst, the motif scanning was investigated.
Across all ﬁve TFs, the motif scanning score distribution from TFBSs was signiﬁcantly higher than the distribution from non-binding sites (Supplementary Fig.S2). Next, HAc VS representing local minima were computed, and the distributions of VS at TFBSs and non-binding sites were compared.
In LPS-stimulated cells, HAc VS were signiﬁcantly higher at TFBSs than at non-binding sites (Fig. 1B) this motivated the use of HAc ChIP data to improve predictions.
Furthermore, LPS-dependent TFBSs were correlated with LPS-inducible HAc local minima (Supplementary Table S4 and Fig.S3). Model performance: ﬁrst, two-feature models (motifs plus one other feature) were compared with a motifs-only reference model.
Based on the area under the sensitivity versus FPR curve (Fig. 2, Supplementary Fig.S4 and Table S3), the model with HAc VS from 2073 [09:29 12/8/2010 Bioinformatics-btq405.tex] Page: 2073 2071 2075  S.A.Ramsey et al. A B Fig.2.
HAc data improve motif scanning-based TFBS predictions.
(A) Prediction performance (area under the sensitivity versus FPR curve, or ROC curve) for models with motif scanning and one additional feature, and a motifs-only reference model (data for models with three features are shown in Supplementary Fig.S4). Larger bar values correspond to better cross-validation-average performance on the test dataset. The performance for the reference model is shown in the blue bar (and vertical dotted line), and a random model is shown as a negative control (black bar). The motifs-only model outperformed the random model, 27-fold.
Each green bar represents a model that used motif information plus a speciﬁc sequence-based feature (GC content, etc.). Each cyan bar represents a model that used motif information plus a HAc ChIP-Seq-based feature (Supplementary Table S3). Each error bar represents the cross-validation-wide SD of the performance difference between the indicated model and the reference model (Section 3). P 0.001.
For the cyan bars, a dashed border indicates that HAc data are from LPS-stimulated cells a solid border means the HAc data were from unstimulated cells.
In the top two bar labels, VS stands for the valley score for local minima in the HAc ChIP-Seq signal.
(B) ROC curves, for predictions by the models shown in (A) (see Supplementary Fig.S4 for the complete FPR range). The model with HAc VS (from stimulated cells gray curve) outperforms the other models.
ROC curves were obtained by varying the prediction score cutoff (Section 3). The lack of improvement for the nucleosome occupancy-based model is consistent with the very weak association between this feature and TF binding (Supplementary Fig.S6). P 0.05      stimulated cells had the highest performance improvement relative 3). The HAc ChIP- to the reference model (52% increase, P  10 Seq signal also improved prediction performance (by 14%), but the improvement was highly variable from TF to TF (coefﬁcient of variation = 27% see Supplementary Table S5). The model using the stimulated-cell HAc VS also outperformed the unstimulated- cell HAc VS data (by 31%, P 0.01). In contrast to the HAc ChIP-derived datasets, the three genomic features (GC content, conservation and nucleosome occupancy score) did not substantially improve prediction performance.
However, the improvements due to GC content (5% increase) and conservation (3%) were more consistent from TF to TF, and thus in both cases were statistically signiﬁcant (P 0.05). Next, models with motifs plus two other features were compared with the best previous model (motifs + HAc VS). None of the models gave a statistically signiﬁcant improvement over the best two-feature model (Supplementary Fig.S5). These ﬁndings suggest that more TF binding data would be required to discriminate prediction performances of three-feature models.
5 CONCLUSIONS Using cell type-speciﬁc HAc ChIP-Seq data improves motif scanning-based prediction of TFBSs in primary macrophages.
This prediction strategy could be applied to any cell type in which HAc can be globally measured.
Overall, these ﬁndings suggest that within histone-acetylated regions, local minima of HAc ChIP-Seq signal may indicate sites of active transcriptional regulation.
ACKNOWLEDGEMENTS for We thank K. Deutsch, S. Bloom and M. Gundapuneni technical assistance.
H. Lähdesmäki and M. Nykter kindly provided some MATLAB functions.
S.A.R. thanks A. Diercks, E. Fu and V. Thorsson for helpful discussions.
We thank A. Nachman, B. Marzolf, D. Rodriguez and L. Rowen for coordinating the contributions of their groups.
Funding: The National Heart, Lung, and Blood Institute (K25HL098807 to S.A.R.) the National Institute of Allergy and Infectious Diseases (HHSN272200700038C) and the National Institute of General Medical Sciences (R01GM072855 to I.S. and P50GM076547). Conﬂict of Interest: none declared.
REFERENCES Aung,H.T. et al. (2006) LPS regulates proinﬂammatory gene expression in macrophages by altering histone deacetylase expression.
FASEB J., 20, 1315 1327.
Berger,S.L. (2007) The complex language of chromatin regulation during transcription.
Nature, 447, 407 412.
Beyer,A. et al. (2006) Integrated assessment and prediction of transcription factor binding.
PLoS Comput. Biol., 2, e70. Ernst,J. et al. (2010) Integrating multiple evidence sources to predict transcription factor binding in the human genome.
Genome Res., 20, 526 536.
Hannenhalli,S. (2008) Eukaryotic transcription factor binding sites modeling and integrative search methods.
Bioinformatics, 24, 1325 1331.
Holloway,D.T. et al. (2005) Integrating genomic data to predict transcription factor binding.
Genome Inform., 16, 83 94.
2074 [09:29 12/8/2010 Bioinformatics-btq405.tex] Page: 2074 2071 2075  Ac-Histone data improve binding site prediction Johnson,D.S. et al. (2007) Genome-wide mapping of in vivo protein-DNA interactions.
Wasserman,W.W. and Sandelin,A. (2004) Applied bioinformatics for the identiﬁcation Science, 316, 1497 1502.
Lähdesmäki,H. et al. (2008) Probabilistic inference of transcription factor binding from multiple data sources.
PLoS ONE, 3, e1820. Nykter,M. et al. (2009) A data integration framework for prediction of transcription factor targets: a BCL6 case study.
Ann. N. Y. Acad. Sci., 1158, 205 214.
Ramsey,S.A. et al. (2008) Uncovering a macrophage transcriptional program by integrating evidence from motif scanning and expression dynamics.
PLoS Comput. Biol., 4, e1000021. Vettese-Dadey,M. et al. (1996) Acetylation of histone H4 plays a primary role in enhancing transcription factor binding to nucleosomal DNA in vitro. EMBO J., 15, 2508 2518.
Wang,T. et al. (2009) A general integrative genomic feature transcription factor binding site prediction method applied to analysis of USF1 binding in cardiovascular disease.
Hum.
Genomics, 3, 221 235. of regulatory elements.
Nat. Rev.
Genet., 5, 276 287.
Whitington,T. et al. (2009) High-throughput chromatin information enables accurate tissue-speciﬁc prediction of transcription factor binding sites.
Nucleic Acids Res., 37, 14 25.
Won,K.-J. et al. (2009) An integrated approach to identifying cis-regulatory modules in the human genome.
PLoS One, 4, e5501. Won,K.-J. et al. (2010) Genome-wide prediction of transcription factor binding sites using an integrated model.
Genome Biol., 11, R7. Zhou,X. et al. (2010) A systems biology approach to transcription factor binding site prediction.
PLoS One, 5, e9878. [09:29 12/8/2010 Bioinformatics-btq405.tex] Page: 2075 2071 2075 2075
BIOINFORMATICS Vol.24 ISMB 2008, pages i304 i312 doi:10.1093/bioinformatics/btn157 Towards the use of argumentation in bioinformatics: a gene expression case study Kenneth McLeod1, and Albert Burger1,2 1Department of Computer Science, Heriot-Watt University and 2MRC Human Genetics Unit, Edinburgh, UK ABSTRACT Motivation: Due to different experimental setups and various interpretations of results, the data contained in online bioinformatics resources can be inconsistent, therefore, making it more difﬁcult for users of these resources to assess the suitability and correctness of the answers to their queries.
This work investigates the role of argumentation systems to help users evaluate such answers.
More speciﬁcally, it looks closely at a gene expression case study, creating an appropriate representation of the underlying data and series of rules that are used by a third-party argumentation engine to reason over the query results provided by the mouse gene expression database EMAGE. Results: A prototype using the ASPIC argumentation engine has been implemented and a preliminary evaluation carried out.
This evaluation suggested that argumentation can be used to deal with inconsistent data in biological resources.
Availability: The ASPIC argumentation engine is available from http://www.argumentation.org. EMAGE gene expression data can be obtained from http://genex.hgu.mrc.ac.uk. The argumentation rules for the gene expression example are available from the lead author upon request.
Contact: kcm1@hw.ac.uk 1 INTRODUCTION Biologists have access to an ever increasing number and range of online data resources (Bateman, 2007). Many of these resources contain inconsistent data.
This is not surprising as biology is a complex science in which countless parameters affect the outcome of every experiment.
Added to this is the human element that causes two identical results to be evaluated differently by different people.
The consequence is that two seemingly identical experiments can produce contradictory outcomes.
These experiments may be stored in one or more of the online resources that service a particular ﬁeld. If both of these experiments are published by the same resource, it becomes inconsistent.
However, if each experiment is published by a different resource, then the inconsistency is between resources and becomes harder to detect.
Regardless of where it occurs, inconsistency confuses users, forcing them to research further in order to answer their query.
In McLeod and Burger (2007) it was suggested that argumentation could be one solution to this problem.
By using all the resources in a ﬁeld, arguments could be created for and against potential answers to a query.
These arguments could be presented to the user, providing   To whom correspondence should be addressed.
them with a powerful set of knowledge that could be used to identify the most likely solution to the query.
This case study created a prototype using a third-party argumentation engine from ASPIC, Argumentation Services Platform with Integrated Components (www.argumentation.org), to generate arguments for the data held in the EMAGE developmental mouse gene expression database (Davidson et al., 1997). Future work will extend this to include data from a complementary developmental mouse gene expression database, GXD (Ringwald et al., 2001). Section 2 starts with a discussion of argumentation.
It is followed in Section 3 by an examination of the gene expression resources EMAGE and GXD in order to explain the need for argumentation.
In Section 4, the argumentation engine is introduced, and Section 5 describes how the knowledge in the domain was interpreted for use with the ASPIC argumentation engine.
Subsequently, in Section 6, the creation of arguments by ASPIC is discussed.
The study continues with a preliminary evaluation of the prototype in Section 7 and a discussion of the work in Section 8 before the conclusions are presented in Section 9.
2 ARGUMENTATION An argument is a reason to believe that something is true.
Arguments can be used to support or attack statements.
Argumentation (Carbogim et al., 2000 Pollock, 2002) is the use of computers in the process of arguing, either for helping humans to argue or by actually using the computers to conduct the argument.
As an approach argumentation mimics a human process and appears intuitive to human users (Williams and Williamson, 2006). The actual form of an argument will depend on the theory being implemented.
Commonly, an argument is viewed as being a series of inference rules that are chained together in a manner similar to logic programming: there are a number of statements (premises) that if true, imply that the conclusion is true.
A premise of a rule may be satisﬁed by the conclusion of another rule.
So in order for the ﬁrst rule to be satisﬁed, all the premises of the second rule must also be satisﬁed. Eventually, premises will be satisﬁed because they are known to be true: they appear in a knowledge base that holds all currently accepted knowledge for the domain.
As the knowledge changes, new arguments will be formed.
These new arguments may contradict existing arguments, thus creating conﬂict. Conﬂict between arguments is usually represented in two ways.
The ﬁrst is rebuttal, where two arguments have opposite conclusions: e.g. it is raining outside versus it is not raining outside.
Undercut is the second form of conﬂict. It is an attempt to show that another argument is not valid because the premises do not imply the conclusion.
For example, an argument that someone will get wet  2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[19:57 18/6/03 Bioinformatics-btn157.tex] Page: i304 i304 i312  because it is raining outside is undercut by the knowledge that the person has an umbrella.
Argumentation provides a means of resolving this conﬂict. The arguments can be weighed and compared, with the strongest argument(s) winning.
Thus, the conclusion supported by the strongest argument(s) wins.
Argumentation has been used in areas such as medicine, law (Bench-Capon and Prakken, 2006) and practical reasoning (Rahwan and Amgoud, 2006). Medical uses of argumentation vary from systems that provide advice on administering drugs (Hurt et al., 2003), to those that help clinicians plan the management of chronic illness through the provision of decision support (Glasspool et al., 2006). Argumentation has also been used to generate explanations of diagnosis, produced by other computational means, for the beneﬁt of patients (Williams and Williamson, 2006). In contrast to medical-informatics, bioinformatics has produced little work on argumentation, although Jefferys et al. (2006) used argumentation to successfully evaluate the output of a protein prediction tool.
This work showed clearly that argumentation could be applied to bioinformatics tools, but what about bioinformatics data resources  3 ON THE NEED FOR ARGUMENTION IN BIOLOGY Bioinformaticians have access to an ever-increasing range of online resources (Bateman, 2007), many of which publish experimental results for a particular ﬁeld. For example, the results of in situ gene expression experiments for the developmental mouse are published in both EMAGE (Davidson et al., 1997) and GXD (Ringwald et al., 2001). Genes are a set of instructions that tell the body what to build, e.g. a particular set of genes results in the creation of a nose and a different set of genes produces whiskers.
In situ gene expression experiments are designed to identify the genes that are active in a particular anatomical structure.
For that structure, the experiment sets out to determine if the gene is active (expressed) or not active (not expressed). EMAGE and GXD take their knowledge of embryonic anatomical structures from a common anatomy, EMAP (Baldock and Davidson, 2008), though GXD has additional structures for the adult mouse.
An in situ gene expression database, such as EMAGE or GXD, allows its users to ﬁnd the conclusions of gene expression experiments.
These conclusions link a gene to a structure, with a level of expression (i.e. expressed or not expressed). The database also provides provenance data such as: who the research team was, details of where the experiment was published, the images showing the experimental result and details of the mouse experimented on.
When using such a resource, the user will start by asking for the genes (not) expressed in a particular structure, or for the list of structures where a speciﬁc gene is expressed.
The complex nature of biology means that it is possible for experiments to produce conclusions that seem to be contradictory, e.g. one experiment may suggest the gene Hoxb1 is expressed in the Neural Ectoderm (EMAP:151) and a second report that it is not.
There are many reasons why this could be the case.
For example, the experiments though very similar may be slightly different, e.g. using different probes may have produced different results, the results may have been analysed differently, e.g. different interpretations of the original gene expression images generate different experimental Towards the use of argumentation in bioinformatics conclusions, and there is always the possibility of a genuine error, e.g. when entering the data into the database.
In addition to internal inconsistencies, resources covering the same ﬁeld may contradict each other.
For example, although EMAGE and GXD have a high level of duplication (in terms of data), their contents are not identical.
To illustrate this, consider the gene Bmp4 and the structure Future Brain (EMAP:1199). At the time of writing this article, GXD contains only one experiment for this combination, and it suggests that Bmp4 is not expressed.
EMAGE has this experiment, but in addition it contains another three experiments, all of which indicate that Bmp4 is expressed in the Future Brain.
With all the available evidence, the most likely conclusion is that the gene is expressed however, if the user relies on a single resource, in this case GXD, a wrong conclusion may be drawn.
Because these resources are incomplete, it is vital that they are both used, in order to generate as many arguments as possible.
However, this highlights a number of issues, both practical and theoretical, which require consideration.
An example of a practical issue would be in identifying experimental results that are duplicated in the other resource.
If an argument is created from data in EMAGE, there is no point in creating an argument based on the same data in GXD. Theoretical issues include determining whether or not the knowledge used to create arguments for EMAGE can be successfully applied to a similar resource such as GXD. These issues are not considered in this study but will be the subject of future work.
Regardless of location or reason, contradictions are confusing for users, and require them to investigate the data more fully, often to the extent of re-reading the original paper in which the result was published.
It would be useful to conduct an investigation of the data automatically, presenting the ﬁndings to the user in a manner that they could analyse easily.
It is hoped that argumentation may provide a mechanism to achieve this.
4 ARGUMENTATION ENGINE Many different types of argumentation software exist.
Some are used for visualization and explanation of arguments, e.g. Araucaria (Reed and Rowe, 2001), some for decision support (Fox, 2001) and some for collaborative decision support (Gordon and Karacapilidis, 1997). However, an inference tool that generates and evaluates arguments is used in this study.
This case study intended to create a web-based tool that dynamically pulled data on-demand from EMAGE to conduct argumentation.
For this reason, it required a robust inference tool that could be integrated readily into applications.
Few tools are available that meet these requirements.
Many different theories for argumentation have been proposed, but few have a robust implementation that can be integrated freely into another application.
For example, Gordon (1993) produced an implementation of his theory, but it is not available publicly.
Oscar is an implementation from Pollock (2002) which is available for download.
Unfortunately, it is programmed in LISP making it difﬁcult to integrate.
The original argumentation engine concept and theory was produced by Dung (1995), but it did not include an implementation.
ASPIC had the goal of standardizing argumentation theory in Artiﬁcial Intelligence and developing a suite of tools that could be used in standard application areas such as dialogue, decision-making i305 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i305 i304 i312  K.McLeod and A.Burger and machine learning.
The foundation of this implementation is a JAVA tool that generates arguments using inference.
ASPIC s argumentation engine is still in development.
Consequently, it is not as robust as might be desired, though it is reliable enough to work for more complex examples than those presented here.
Crucially, the engine s design ensures that it can be integrated into other projects.
Although the code is not open source, the engine is available publicly.
The theory behind the engine is based on the work of Dung (1995). Dung s system is abstract, in the sense that the notion of an argument was not deﬁned. ASPIC, however, deﬁnes an individual argument in the form of an inference tree: inference rules are chained together to form an argument that is organized in a tree structure (Fig. 1). The sole form of attack in the system is rebuttal.
ASPIC mimics an undercut by allowing the user to assign a name to a rule and then create a second rule that rebuts the name (Fig. 2). This succeeds because the name is automatically treated as a premise to its rule, and therefore the second rule rebuts a premise of the ﬁrst rule.
Input to the engine is: a set of knowledge that models the domain being argued about, a set of rules used to infer new knowledge in the domain (when instantiated a rule forms an argument for the knowledge generated), a set of parameters that conﬁgure the behaviour of the engine, and a query that the user wishes the engine to argue about.
Once a query is submitted, the engine generates arguments that support and attack the query before evaluating them.
Output is the arguments, their status and the relationships between them.
In terms of status, the engine records whether or not an argument is true (w.r.t current knowledge) and for relationships the engine provides a list of which arguments attack which other arguments.
This information can be presented visually, in a graph, or in textual form.
ASPIC s argumentation engine can be used via a supplied Graphical User Interface (GUI), or programmatically through a JAVA Application Programming Interface (API). The engine has a ﬁxed knowledge syntax, so an argument must conform to the Conclusion Premise 1 Premise 3 Rule: Premise 1 - Premise 3 Premise 2 Rule 1: Conclusion - Premise 1 & Premise 2 Fig.1.
Arguments in ASPIC are stored in a tree structure.
The earlier argument has the conclusion Outcome and three contributing subarguments. Rule 1 provides the inference rule used to reach the conclusion.
This inference rule states that Outcome is true if both Premise 1 and Premise 2 are true.
Premise 2 is known to be true.
Premise 1 is the conclusion of another argument, and is only true when Premise 3 is true.
[ID_1] Conclusion - Premise 1 & Premise 2 To undercut this rule: ID_1 - Premise 3 & Premise 4 Fig.2.
Undercutting an argument in ASPIC.
The ﬁrst rule states that Conclusion is true when Premise 1 and Premise 2 are both true.
This rule is assigned the name ID_1. The second rule states that when Premise 3 and Premise 4 are both true, the rule called ID_1 cannot be applied.
i306 speciﬁcation created by the designers.
When using the GUI, input to the engine has the form of ﬁrst-order logic.
The chosen logic is similar to PROLOG (Bratko, 2000) and features weak and strong negation.
The JAVA API is designed around the logic, with the methods reﬂecting the underlying language by using terminology such as Variable, Term, Consequent and Antecedent.
It is the API that the rest of this article deals with.
5 FORMALIZATION OF KNOWLEDGE Argumentation takes place in a particular domain.
That domain could be some everyday area such as planning how to travel to London, or it could be something more specialized such as in situ gene expression.
The argumentation engine is given two forms of knowledge from the domain of gene expression.
The ﬁrst documents the current state of the domain, i.e. what is believed to be true, which in this case is the results of gene expression experiments.
The second type, is the knowledge of how to interpret the ﬁrst. This knowledge came from the EMAGE curator, and was converted into inference rules that the engine uses to infer new arguments.
The domain s state will change continually as new experiments are submitted to EMAGE daily.
However, the knowledge of how to interpret that experimental data changes far less often.
Therefore, it is safe to gather expert knowledge in advance and store it for use later.
Due to the high rate of change, the experimental knowledge must be obtained when it is to be used.
This on-demand creation of knowledge is achieved by pulling data through EMAGE s SOAP- based web service and subsequently converting it.
Knowledge can be strict or defeasible. Strict knowledge is deﬁnitely true, e.g. London is the capital of the UK. Defeasible knowledge may be true, but an element of doubt remains, e.g. it is raining, therefore I will get wet.
Associating knowledge with a real number between 0 and 1 indicates the user s degree of belief in an item of knowledge.
If no degree of belief is speciﬁed, the piece of knowledge is assumed to be strict (have conﬁdence equal to 1). This conﬁdence score is how ASPIC assigns a strength to an argument: the higher the score the stronger the argument.
In addition, each piece of knowledge can be assigned a description: a piece of natural language text that describes the knowledge.
The description can hold a simple explanation of a rule or fact.
It is also possible to assign a description to the conclusion of a rule.
Consequently, an argument can be viewed as a series of logic or natural language statements.
5.1 Expert knowledge to inference rules Inference rules are used by ASPIC to infer new arguments.
They model the inference processes of the domain being investigated.
Once captured the inference processes need to be converted into ASPIC s chosen logic for use in the engine.
The example featured attempts to argue over the accuracy of data stored in EMAGE. As such, new arguments are inferred from the contents of the database according to processes suggested by the EMAGE curation team.
This team is responsible for maintaining the quality of the resource by reviewing the experiments submitted for inclusion in EMAGE. Expert knowledge was gathered in advance during a series of meetings.
These meetings started with informal discussions and [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i306 i304 i312  moved onto using concrete examples to illustrate how the curator processes information.
Although ASPIC s engine uses a ﬁrst-order logic, biologists tend to prefer natural language.
In order to provide a bridge between the two, the notion of an argument schema (Walton, 1996b) is used.
This allowed the expert s reasoning to be captured in a semi-formal way using natural language.
A schema provides a natural language inference rule that documents an inference that can be assumed to be true unless shown otherwise (defeated by a counter-argument). A schema also provides a collection of Critical Questions that highlight exceptions to, and extra conditions on, the use of the inference rule.
All the knowledge needed to create a formal logic inference rule is documented in a manner that can be easily understood by biologists.
For example, when an EMAGE curator evaluates an experiment, they record their conﬁdence in the experiment as a score between 0 and 3, with 3 indicating a high level of conﬁdence. The curator s conﬁdence is made public because a high-quality experiment is more likely to produce a correct result than an experiment the curator has less conﬁdence in.
Intuitively, it can be suggested that if the curator has high conﬁdence in the experiment, the user can have high conﬁdence in the result of the experiment.
This would lead to something like the following schema based on Walton s schema for an Expert (Walton, 1996a): EMAGE is a leading resource on mouse in-situ gene expression EMAGE has C confidence in experiment E suggesting gene G is expressed in structure S Therefore we may be C confident that G is expressed in S Assuming that anyone who uses the system automatically accepts the initial premise that EMAGE is an expert resource, it is possible to simplify the above schema and represent it in a PROLOG-like Table 1.
Some of the rules deﬁned by the EMAGE curator Towards the use of argumentation in bioinformatics syntax (with capital letters indicating variables that unify and lower case letters indicating constants), so the basic rule is: expressed(Gene, Structure) - experiment(Id, Gene, Structure, expressed), confidence(Id, Confidence). The problem with this rule is that the conﬁdence EMAGE has in the experiment is not passed to ASPIC.
There should be a direct link between EMAGE s conﬁdence and the strength of the argument therefore, it is necessary to add a degree of belief.
In the instance of EMAGE having high conﬁdence the argument should be strong and thus have a high degree of belief, for example 0.8.
This can be set when passing the inference rule to ASPIC using its JAVA API. A selection of further rules can be seen in Table 1.
These rules use notions such as Theiler Stages, Spatial Annotation and Textual Annotation.
The Theiler Stages are the 26 developmental phases of a mouse embryo.
Each experiment must be mapped to one of these stages.
The results of gene expression experiments (2D section images) can be described with respect to the EMAP anatomy ontology or spatially mapped into the 3D embryo models (one per Theiler Stage) of EMAP. These are referred to as Textual Annotation and Spatial Annotation, respectively.
Rules 3, 4 and 5 from Table 1 are all variations of the schema discussed earlier in this section.
5.2 State of domain knowledge ASPIC refers to each item of knowledge (or belief) referring to the current state of the domain as a fact like inference rules these can be strict or defeasible. The EMAGE resource provides the setting for this case study, so the facts given to the argumentation engine correspond directly to the data held in EMAGE. The contents of EMAGE can be abstracted to knowledge about an experiment and its conclusion.
The conclusion is literally that a gene was (not) expressed in a particular anatomical structure.
The experimental information states: who performed the experiment, ID 1 2 3 4 5 6 7 8 9 Description If a gene G, is expressed in a structure S, in Theiler Stage (T 1) and also in Stage (T +1), then G is very likely to be expressed in S in Stage T. If the user, after examining the image of the experimental result, is conﬁdent that the gene G, is expressed in the structure S, then G is very likely to be expressed in S. If a spatial annotation SA, suggests a gene G is expressed in structure S, and the curator has high conﬁdence in SA, then we may have high conﬁdence that G is expressed in S. If a textual annotation TA, suggests a gene G is expressed in structure S and the curator has high conﬁdence in TA, then we may have high conﬁdence that G is expressed in S. If a textual annotation TA, suggests a gene G is expressed in structure S and the curator has medium conﬁdence in TA, then we may have medium conﬁdence that G is expressed in S. If the user does not trust the research team that conducted experiment E, then all spatial and textual annotations based on that experiment should have a low level of conﬁdence. If a spatial annotation SA and a textual annotation TA disagree, then always trust TA.
If two experiments disagree on whether, or not, a gene G is expressed in structure S and the user believes the experiments are examining different parts of S, then G is likely to be expressed in part of S. If two experiments disagree on whether, or not, a gene G is expressed in structure S and the user believes the experiments are examining different parts of S, then G is likely to be not expressed in part of S. Rules 1 7 are relatively straightforward.
However, Rules 8 and 9 may require further explanation.
They state that if two experiments are examining different parts of the same structure both results can be correct regardless of their conclusion.
For example, consider two experiments on the human hand.
The ﬁrst experiment may ﬁnd a particular gene expressed in the thumb, and the second conclude that the same gene is not expressed in the index ﬁnger. These experiments show that the gene is both expressed and not expressed in the hand.
i307 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i307 i304 i312  K.McLeod and A.Burger what type of experiment it was, how the gene was detected, what kind of mouse the experiment was performed on (its species, its age, whether it was normal or abnormal) and it provides photographs of the result taken by the researchers.
Consequently, the minimum requirement is to provide ASPIC with knowledge on genes, anatomical structures, the relationship between the two and some details of the experiment that established the relationship.
A fact is treated as a simpliﬁed inference rule, i.e. a rule without premises.
One possible way of saying that an experiment in EMAGE, with the associated identiﬁer EMAGE:772, reported the gene Hoxb1 was expressed in the Neural Ectoderm (EMAP:151) is: experiment( EMAGE:772 , Hoxb1 , EMAP:151 , expressed). Not all of the facts can be generated easily, due to the impossibility of automatically processing the experimental images.
These images are taken by the researchers at the end of the experiment, and are stored in EMAGE as part of an experiment s provenance.
Image analysis is a vital part of evaluating the quality of the result: it is done manually by the EMAGE curator.
Consequently, in this study, the images are presented to the human user and they are asked speciﬁc questions such as: these images are from two experiments that examine the same structure, do they appear to investigate the same area These questions are straightforward for a regular user of EMAGE to answer, but are more challenging for someone with less experience.
6 GENERATING ARGUMENTS Arguments are generated from the contents of the knowledge base, in response to the user posing a query.
The results of the query are returned for the user to examine.
6.1 Query The query is the conclusion that the user wishes ASPIC to argue about.
It will take the form of a fact, and will conform to the earlier discussion in all but one respect: it will not have a degree of belief associated with it.
So in this case, an example would be: expressed( Hoxb1 , EMAP:151 ). Once the query has been created its status is determined by the argumentation engine.
6.2 Evaluating a query ASPIC uses a dialogue game to determine the status of a query (ASPIC, 2004). The knowledge given in the query can be undefeated (true with respect to current knowledge), defeated (false with respect to current knowledge), or unknown.
The game features two computer players, the Proponent (PRO) who attempts to prove the query, and the Opponent (OPP) who tries to stop PRO.
The game starts with PRO creating an argument to support the query (an argument whose conclusion is identical to the query). This process starts by searching for a rule with an appropriate conclusion.
Once found, rules with conclusions that are identical to the premises are sought.
If the premises cannot be satisﬁed in this manner, the facts are examined to determine if they satisfy the premises.
OPP now attempts to defeat PRO s argument.
To succeed, OPP s argument must rebut part of PRO s and have a higher degree of belief.
OPP starts by trying to construct arguments that rebut the conclusion of PRO s argument.
If that cannot be done, OPP attempts to rebut the premises.
If OPP succeeds in defeating PRO s argument, PRO will attempt to counter OPP s argument by defeating it.
This process of attack and counter-attack continues until one player (PRO or OPP) fails to defeat the other s argument.
If PRO is stopped, they try a new line of defence by creating a new argument, to support the conclusion that OPP has defeated, if they fail OPP wins.
However, if OPP fails, they try to defeat one of PRO s previous arguments, if they cannot do so PRO wins.
For an example we shall use a simpliﬁed set of data for Hoxb1 in EMAP:151, ignoring the distinction between textual and spatial annotations (see Section 5.1). EMAGE has two relevant experiments.
The ﬁrst suggests that Hoxb1 is expressed in EMAP:151 and the second that it is not.
The EMAGE curator has medium conﬁdence in the ﬁrst experiment, and a high level of conﬁdence in the second.
In the game, PRO starts by using the ﬁrst experiment to create the argument in Figure 3 (based on a variation of the schema in Section 5.1), which OPP defeats by creating the argument in Figure 4, based on the second experiment (and a different variation of the schema in Section 5.1). The next argument of PRO depends on the information provided by the user.
Since it is impossible for the system to evaluate the images of experimental results the user is asked to help.
They are given a number of questions to answer, for example: are the two experiments dealing with the same part of the structure This question relates to Rules 8 and 9 from Table 1, and is asked because it is possible for a gene to be expressed in one part of an anatomical structure but not expressed in another part of it (e.g. a gene may be expressed in the index ﬁnger but not the thumb, as the index ﬁnger and thumb are two separate parts of the hand, the gene is both expressed and not expressed in the hand). If the user answers the question by suggesting that the experiments are examining different parts of EMAP:151, then it is possible that Hoxb1 is both Hoxb1 is expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is expressed The EMAGE curator has medium confidence in the experiment If the curator has medium confidence in the experiment, then we may   have medium confidence in the experiment and its result Degree of belief = 0.4 Fig.3.
Argument for Hoxb1 being expressed in EMAP:151. i308 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i308 i304 i312  Towards the use of argumentation in bioinformatics Hoxb1 is not expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed The EMAGE curator has high confidence in the experiment If the curator has high confidence in the experiment, then we may have high confidence in the experiment and its result Degree of belief = 0.8 Fig.4.
A counter argument to the argument in Figure 3.
Because the EMAGE curator has more conﬁdence in the experiment used in this argument than the experiment used in Figure 3, this argument has a higher degree of belief and so defeats the argument from Figure 3.
Hoxb1 is expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed EMAGE has an experiment suggesting Hoxb1 is expressed The experiments look at different parts of the same structure If the experiments look at different parts of the same structure, they can both be correct, so the gene is expressed.
Degree of belief = 0.9 Fig.5.
A second argument, based on Rule 8 from Table 1, showing Hoxb1 is expressed in EMAP:151. Hoxb1 is not expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed EMAGE has an experiment suggesting Hoxb1 is expressed The experiments look at different parts of the same structure If the experiments look at different parts of the same structure, they can both be correct, so the gene is not expressed.
Degree of belief = 0.9 Fig.6.
A second argument, based on Rule 9 from Table 1, showing Hoxb1 is not expressed in EMAP:151. expressed and not expressed in EMAP:151. This leads PRO to produce the argument in Figure 5 by using Rule 8 from Table 1.
As this argument defeats OPP s previous argument (based on the EMAGE experiment suggesting Hoxb1 was not expressed) PRO s ﬁrst argument is reinstated because it is no longer attacked.
OPP must counter PRO s argument and does so with the same logic as PRO (Rule 9 from Table 1): the experiments are using different parts of EMAP:151, so Hoxb1 can be both expressed and not expressed, and therefore it is not expressed (Fig. 6). Currently there are two arguments of equal strength that contradict each other (Figs 5 and 6). The outcome of this conﬂict depends on which type of game semantics is used.
ASPIC provides two game semantics, skeptical and credulous, for the user to choose between.
When a skeptical game is played, if there is any doubt about the acceptability of an argument it is rejected.
In this case, there is doubt about the acceptability of both arguments, and so they are both rejected.
The credulous game is implemented in such a way that even if there is doubt about one of PRO s arguments it is accepted, whilst OPP s argument is rejected if there is any doubt.
So here PRO s argument that Hoxb1 is expressed is accepted, with OPP s counter argument being defeated.
It is left to the user to decide which game is most suitable for their situation.
Adopting credulous semantics, PRO s last argument is accepted.
Because of this, both of OPP s arguments are defeated, leaving both of PRO s arguments undefeated.
OPP must try to ﬁnd another argument that defeats one of PRO s two arguments.
However, there are no more arguments available, and so OPP fails.
This game has been won by PRO.
PRO starts a second game with the argument from Figure 5 (as the two experiments are looking at different parts of EMAP:151 Hoxb1 can be both expressed and not expressed, and therefore it is expressed). The same arguments as before are constructed, once again PRO wins.
PRO can construct no more arguments to support the query so the game is over.
The results are given to the programmer to manipulate as they wish.
The results come in two separate parts.
The ﬁrst is a series of yes and no.Each one represents an argument that PRO has constructed to support the query.
As PRO won both games, the results from this example are yes and yes.
The second part of the results is called the proof.
Essentially it is all the arguments used in the game.
If calculated the status of the argument is also recorded.
In this example, the two arguments provided by PRO are undefeated with both of OPP s arguments being defeated.
The programmer can present the arguments to the user in any way they wish.
However, when communicating with biologists it makes more sense to use a natural language form similar to that used in this section (Fig. 4) by using the descriptions attached to rules and facts.
i309 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i309 i304 i312  K.McLeod and A.Burger Fig.7.
Screen shot from the prototype (top-left) with simpliﬁed presentation in a mock prototype (bottom-right). Although this evaluation of a query may seem complicated, it is essentially the type of thought-process naturally deployed by a human user.
The apparent complexity relates primarily to the need to formalize this process for computational purposes.
Fortunately, the details of this formalization need not be communicated to the end user and our initial evaluation (see Section 7) adds weight to our view that the underlying argumentation reasoning is accessible and helpful to biologists.
7 EVALUATION Once the implementation of the above system was completed, a preliminary evaluation was undertaken.
This informal study involved demonstrating the system to the EMAGE curator, and recording the feedback given.
Overall the system was well received.
The tool was deemed easy to use, and the arguments were presented in far less time than the curator had expected.
The arguments made sense to the curator, and they covered the majority of the points the curator wished to see.
The curator felt that the arguments would be enough for most people to evaluate the data from EMAGE, and thus determine whether or not a gene was expressed in a particular structure.
As such the system was a success.
Although feedback from the curator was positive, four issues clearly require to be tackled.
The ﬁrst is the presentation.
The examples discussed above are simpliﬁed in order to improve clarity.
However, the prototype displayed arguments in a rudimentary manner using a slightly amended version of a method ASPIC provided for the task (see top-left of Fig.7). This resulted in a confusing output.
Much of this output was redundant as it restated what had already been given.
For example, in the ﬁrst argument, the ﬁve lines quality_author( EMAGE:772 ) through to You have conﬁdence in the research team said who the research team was twice, and that the user had conﬁdence in this team twice.
Consequently, the test user was presented with a simpliﬁed version of these arguments in a mocked-up prototype (see bottom-right of Fig.7). Feedback from the curator suggested simplifying further the presentation of the arguments.
For example, subarguments were indented to show that they were separate from the main argument but still contributed to it.
However, the curator did not understand the relationship.
Instead, he suggested the information should be presented in a simple paragraph comprising two or three sentences.
i310 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i310 i304 i312  The second key issue highlighted by the evaluation is trust.
When using EMAGE, a user must trust the researcher to have performed and evaluated the experiment correctly, in addition the user must trust that all mistakes were detected and corrected by the journal that published the experiment, and ﬁnally they must trust the curator of EMAGE (or GXD) to have mapped correctly the researcher s ﬁndings to the EMAP ontology for inclusion in the database.
For example, if the research team suggested that Bmp4 was expressed in the presumptive infundibulum, then the curation team needed to map this structure to its equivalent in the EMAP anatomy (infundibular recess of third ventricle). These trust issues should have been made clear to the user by explicitly asking them if they had conﬁdence in each of the above groups.
The system did not do this.
The third issue related to the screen that asked the user for help in processing information.
As mentioned in Section 5.2 the user was asked to analyse some information when the system could not do so.
The curator felt that this screen presented the user with too many tasks to undertake.
One possible solution would be to use the image analysis already undertaken by the authors, journal and EMAGE curation team.
The ﬁnal issue raised by the curator related to GXD. The system worked with data from EMAGE. In real life, the curator would advise anyone with doubts over data in EMAGE to examine GXD (and vice versa), he felt that extending the system to include arguments based on data in GXD was vital.
The goal of this work was to assess the usefulness of argumentation in bioinformatics. Overall it was obvious that much work remained.
However, it was also evident that the current prototype system was the ﬁrst step on the way to a useful and interesting tool.
8 DISCUSSION This work concentrates on two resources publishing in situ developmental mouse gene expression information.
However, other resources that perform this function exist, for example, the Mouse Atlas of Gene Expression, MAGE (http://www.mouseatlas.org). Therefore, it would be beneﬁcial to extend the system to include this and other related resources.
Unfortunately, this is not a simple task.
MAGE uses its own ontology to describe the mouse anatomical structures.
This ontology does not have a mapping to the EMAP ontology used by EMAGE and GXD. One structure in EMAP may correspond to parts of several structures in the MAGE ontology, and vice versa.
Although work is progressing on a cross-linked mammalian ontology that will hopefully link EMAP to MAGE, currently there is no automatic mechanism to do this.
At present this makes it impossible to use these resources together in this system.
If MAGE had used the EMAP ontology, there would be no reason why it could not be included in the system.
Data from MAGE would need to be pulled and converted for use within ASPIC.
Likewise, there would be a need for an evaluation of the current inference rules to determine if they could be applied to MAGE.
It is probable that several extra inference rules would be required.
With the new rules in place ASPIC would be able to argue as before.
However, with futher knowledge at its disposal, it would be possible to create extra arguments and thus have a more complex argumentation process.
Although this would be unlikely to have a signiﬁcant effect in the example discussed here, it is possible that the integration of a large Towards the use of argumentation in bioinformatics number of resources (or resources with a larger number of expert generated inference rules) might cause the argumentation process to run too slowly to be useful.
In such a situation, it might be necessary to balance the inclusion of each resource against the usefulness of the information it provides for the arguments.
Alternatively, it might prove helpful to investigate the other argumentation engines that are beginning to appear e.g. ArgKit (http://www.argkit.org). Of course, in the context of the Internet, the argumentation workload can be distributed across more than one site.
We envision domain-speciﬁc argumentation engines, e.g. one or more sites for in situ gene expression argumentation, that communicate with each other.
Efforts are already underway to develop an Argumentation Interchange Format (Chesñevar et al., 2006) to facilitate such interactions.
In addition, we note that there is a potential issue with scalability in terms of formalizing enough relevant domain knowledge for the purposes of argumentation.
As with most semantics-based applications, it is unrealistic to expect that all relevant domain knowledge will be captured.
However, the experience with the Semantic Web so far shows that even a little semantics goes a long way (Wolstencroft et al., 2005), and we believe that this applies equally to argumentation.
Argumentation has been used within this work to resolve inconsistencies across biological data resources.
A variety of other mechanisms to integrate data and resolve inconsistency exist.
For example, data reconciliation (a.k.a. data fusion) uses a function to turn multiple possible values into a single value, e.g. computing the average of four numbers (Motro and Rakov, 1998). A second possible mechanism would create multiple query plans for the resources, then select the best according to information quality criteria (Naumann, 1996). Our work is not an attempt to replace these mechanisms.
We are not concerned with automatically resolving conﬂict, but instead wish to determine whether or not argumentation can enable biologists to resolve the differences themselves.
9 CONCLUSION This case study explored the usefulness of argumentation in helping biologists work around conﬂicting information presented by an online biological database, in this case a developmental mouse gene expression database called EMAGE. By investigating the reasoning processes of an EMAGE curator, a series of rules for assessing the quality of an experiment were produced.
These rules were used by the ASPIC argumentation engine to generate arguments on the validity of the data provided by EMAGE. This enabled arguments for and against each experimental result to be produced and presented to the user.
Following an implementation of the system, an evaluation was undertaken with the EMAGE curator.
The evaluation showed that the basic concept was correct: arguments could be used to highlight issues and help the user determine if data was valid.
However, it also stressed the importance of presenting the arguments in an appropriate manner, and here further work must be undertaken.
This is not the only work needed, in particular an effort must be made to extend the system so that it can create arguments based on the data held in another developmental mouse gene expression database, GXD. Only then it will be possible to make an accurate assessment of the full worth of argumentation in a bioinformatics setting.
i311 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i311 i304 i312  K.McLeod and A.Burger ACKNOWLEDGEMENT This work could not have been completed without the support of the EMAGE curation team.
Funding: Funding by the EU projects Sealife (FP6-2006-IST- 027269) and REWERSE (FP6-2006-IST- 506779) is acknowledged.
Conﬂict of Interest: none declared.
REFERENCES ASPIC.
(2004) Theoretical framework for argumentation, deliverable d2.1. Baldock,R. and Davidson,D. (2008) In Anatomy Ontologies for Bioinformatics: Principles and Practise. The Edinburgh Mouse Atlas.
Springer, Berlin, Germany. Bateman,A. (2007) Editorial.
Nucleic Acids Research, 35, (Database Issue), D1 D2. Bench-Capon,T. and Prakken,H. (2006) Argumentation.
In Information Technology & Lawyers: Advanced technology in the legal domain, from challenges to daily routine, Springer, Berlin/Heidelberg/New York. pp.
61 80.
Bratko,I. (2000) PROLOG Programming for Artiﬁcial Intelligence.
Addison Wesley, Harlow, UK. Carbogim,D.V. et al. (2000) Argument-based applications to knowledge engineering.
Knowl. Eng. Rev., 15, 119 149.
Chesñevar,C. et al. (2006) Towards an argument interchange format.
Knowl. Eng. Rev., 21, 293 316.
Davidson,D. et al. (1997) The mouse atlas and graphical gene-expression database.
Semin. Cell Dev. Biol., 8, 509 517.
Dung,P.M. (1995) On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games.
Artif. Intell., 77, 321 358.
Fox,J. (2001) Rags: a novel approach to computerized genetic risk assessment and decision support from pedigrees.
Methods Inform.
Med., 4.
Glasspool,D.W. et al. (2006) Argumentation in decision support for medical care planning for patients and clinicians.
In Bickmore,T. and Green,N. (eds), Proceedings of AAAI Spring Symposium Series 2006 (AAAI Technical Report SS-06-01), AAAI Press, California, USA. Gordon,T.F. (1993) The pleadings game: formalizing procedural justice.
In Proceedings of the 4th International Conference on Artiﬁcal Intelligence and Law, ACM Press, New York, NY, USA, pp.
10 19.
Gordon,T.F. and Karacapilidis,N. (1997) The zeno argumentation framework.
In Proceedings of the 6th International Conference on Artiﬁcal Intelligence and Law, Melborne, Australia, ACM Press, pp.
10 18.
Hurt,C. et al. (2003) Computerised advice on drug dosage decisions in childhood leukaemia: a method and a safety strategy.
In Dojat,M., Keravnou,E. and Barahona,P. (eds), Proceedings of the 9th Conference on Artiﬁcial Intelligence in Medicine in Europe. Berlin, Germany, Springer, pp.
158 163.
Jefferys,B.R. et al. (2006) Capturing expert knowledge with argumentation: a case study in bioinformatics. Bioinformatics, 22, 923 933.
McLeod,K. and Burger,A. (2007) Using argumentation to tackle inconsistency and incompleteness in online distributed life science resources.
In Aes,N.G. and Isaís,P. (eds), Proceedings of IADIS International Conference Applied Computing 2007, Salamanca, Spain. IADIS Press, pp.
489 492.
Motro,A. and Rakov,I. (1998) Estimating the quality of databases.
In Proceedings of the 3rd International Conference on Flexible Query Answering Systems, Roskilde, Denmark, Springer, pp.
298 307.
Naumann,F. (1996) Quality-Driven Query Answering for Integrated Information Systems.
Lecture Notes in Computer Science, vol.2261.
Springer, Berlin, Germany. Pollock,J.L. (2002) Defeasible reasoning with variable degrees of justiﬁcation. Artif. Intell., 1 2, 232 282.
Rahwan,I. and Amgoud,L. (2006) An argumentation-based approach for practical reasoning.
In Proceedings of the 5th International Conference on Autonomous Agents and Multiagent Systems, Hakodate, Japan, ACM Press, pp.
347 354.
Reed,C. and Rowe,G. (2001) Araucaria: software for puzzles in argument diagramming and XML. Technical Report, Department of Applied Computing, University of Dundee, Dundee, Scotland, UK. Ringwald,M. et al. (2001) The mouse gene expression database (GXD). Nucleic Acids Res., 29, 98 101.
Walton,D. (1996a) Appeal to Expert Opinion : Arguments from Authority.
Penn State University Press, PA. USA. Walton,D. (1996b) Argumentation Schemes for Presumptive Reasoning (Studies in Argumentation Series). Lawrence Erlbaum Associates, Mawah, NJ, USA. Williams,M. and Williamson,J. (2006) Combining argumentation and Bayesian nets for breast cancer prognosis.
J. Log.
Lang. Inform., 15, 155 178.
Wolstencroft,K. et al. (2005) A little semantic web goes a long way in biology.
In Proceedings of the 4th International Semantic Web Conference.
Galway, Ireland, pp.
786 800. i312 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i312 i304 i312 BIOINFORMATICS APPLICATIONS NOTE Vol.27 no.6 2011, pages 891 893 doi:10.1093/bioinformatics/btr029 Advance Access publication January 22, 2011 Databases and ontologies Automated validation of genetic variants from large databases: ensuring that variant references refer to the same genomic locations Mark Y. Tong1, Christopher A. Cassa2 and Isaac S. Kohane1,2, 1CBMI, Harvard Medical School, 10 Shattuck Street, Boston, MA 02115 and 2Children s Hospital Informatics Program, Children s Hospital Boston, 1 Autumn Street, #721, Boston, MA 02215-5362, USA Associate Editor: Jonathan Wren ABSTRACT Summary: Accurate annotations of genomic variants are necessary to achieve full-genome clinical interpretations that are scientiﬁcally sound and medically relevant.
Many disease associations, especially those reported before the completion of the HGP, are limited in applicability because of potential inconsistencies with our current standards for genomic coordinates, nomenclature and gene structure.
In an effort to validate and link variants from the medical genetics literature to an unambiguous reference for each variant, we developed a software pipeline and reviewed 68 641 single amino acid mutations from Online Mendelian Inheritance in Man (OMIM), Human Gene Mutation Database (HGMD) and dbSNP. The frequency of unresolved mutation annotations varied widely among the databases, ranging from 4 to 23%. A taxonomy of primary causes for unresolved mutations was produced.
Availability: This program is freely available from the web site (http:/ safegene.hms.harvard.edu/aa2nt/). Contact: mt153@hms.harvard.edu mark_tong2009@yahoo.com Supplementary information: Supplementary data are available at Bioinformatics online.
Received on August 19, 2010 revised on December 14, 2010 accepted on January 15, 2011 1 INTRODUCTION Large numbers of genetic variants from medical and genetics publications have been compiled in databases, including the Online Mendelian Inheritance in Man (OMIM), the Human Gene Mutation Database (HGMD), among others.
For example, the HGMD (Stenson et al., 2009) has curated 100 329 disease-associated genetic variants in its current release (March 2010), and OMIM has described 20 068 variants as of June 2010 (Amberger et al., 2009). These disease-associated variants are valuable in the understanding, prevention and diagnosis of human disease.
With the imminent reduction to practice of whole-genome interpretation (Ashley et al., 2010 Ormond et al., 2010), an overview of the accuracy of these databases is important in understanding how much quality improvement work remains to make these prior genome-wide annotations clinically useful.
We focus here on the syntactic accuracy of the annotations which are an important but small step toward assessing their clinical validity (Kohane et al., 2006). To this end, we developed a software module, aa2nt, which provides basic validation of single amino acid changes using information from current databases, derives the corresponding DNA change from an amino acid change and generates Human Genome Variation Society (HGVS)-recommended names (Supplementary Fig.S1). We applied aa2nt to a selected set of variants from three commonly used databases (OMIM, HGMD and dbSNP) to evaluate whether we could correctly resolve the locations of variants in current annotation databases.
We validated 66 638 single nucleotide mutations from OMIM, HGMD and dbSNP and obtained a passing rate ranging from 77 to 96%. 2 METHODS The following algorithm was used to validate and map amino acid substitutions caused by a single nucleotide change: 1.
Required input data: gene symbol, codon number, wild-type amino acid, variant amino acid.
1.1 Replace gene synonyms with ofﬁcial gene symbols by querying data available in Entrez Gene (ftp://ftp.ncbi.nih.gov/gene/DATA gene_info.gz). 1.2 Retrieve available human cDNA sequences from RefSeq (ftp:/ ftp.ncbi.nih.gov/refseq/H_sapiens/mRNA_Prot/human.rna.gbff. gz) for the given gene.
2.
For each cDNA transcript obtained in step 1.2, generates the cDNA codon sequence corresponding to the codon number of amino acid change, and translate it to the corresponding amino acid.
3.
Compare the obtained amino acid to the wild-type reference amino acid at that position.
3.1 If identical, it is validated.
3.2 Otherwise, test if the gene has a signal peptide (http://www. signalpeptide.de/), which could alter the codon numbering.
If yes, adjust the codon number with signal peptide added.
4.
Identify all possible single nucleotide changes from the reference codon sequence to all the possible genetic codons of the variant amino acid.
5.
Generate tuple of HGVS name(s) of DNA and protein changes.
To whom correspondence should be addressed.
A detailed ﬂow chart illustrating this process with sample data can be found in the Supplementary Materials (Supplementary Figure S1). The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:38 19/2/2011 Bioinformatics-btr029.tex] Page: 891 891 893  M.Y.Tong et al. Table 1.
Summary of validated mutation annotationsa,b Database OMIM HGMD (I) HGMD (II) dbSNP Passed Unresolved Total 7722 (76.8%) 47 260 (81.2%) 55 115 (95.8%) 2310 (87.3%) 2332 10 054 2364 57 479 10 922 58 182 336 2646 aTwo sets of codon numbers were used for HGMD data: original (I) and HGVS form (II). bVersions: OMIM: 2010 HGMD: professional version, 2010 (2) dbSNP: 2010 HG18. 3 RESULTS 3.1 Validation of variants by codon number and amino acid substitutions We selected 10 054 single amino acid substitution variants from OMIM (Supplementary Table S1), 58 182 variants from HGMD, 57 479 variants from HGMD where the HGVS codon number is available, 2646 variants from table OmimVarLocusIdSNP in dbSNP (ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606/database organism_data/OmimVarLocusIdSNP.bcp.gz, Supplementary Table S2) as input ﬁles for the validation program.
The results of validations are summarized in Table 1.
3.2 Validating program performance using a gold-standard dataset To evaluate the accuracy of base changes predicted from the speciﬁed amino acid change, we selected 5959 mutations in OMIM which have details about the speciﬁc base change involved in the HGMD. 5113 (86%) of 5959 variants mapped to a single nucleotide change identical to one described in HGMD. The remaining 846 variants mapped to more than one possible codon.
If the highest frequency codon is used based on the frequency table (Nakamura et al., 2000), 5586 (94%) of the predicted codons agree with the mutant codon sequence in HGMD. The aa2nt module does not predict codon change(s) if more than one nucleotide is required to make the prediction.
3.3 Evaluation of major categories of unresolved annotations We analyzed 2332 annotations from the OMIM database which did not achieve proper resolution using the aa2nt test pipeline, and grouped them into the following categories:  Amino acid assignment problems: the annotated amino acid was not present at the described location in any of the known gene product isoforms. Of the 2044 unresolved variants, we checked if the gene products contained a signal peptide.
Of 2044, 950 did have a signal peptide sequence and 528 of the annotations passed a second round validation after re-indexing the codon number with the signal peptide added.
Non-standard gene symbols: 258 variants in 75 genes used gene aliases instead of ofﬁcial gene symbols.
After replacing the alias with an ofﬁcial gene symbol, 219 passed validation.
Codon number greater than protein length: 27 variants belong to this class.
An example is PTEN, which encodes a protein of 403 amino acids.
Therefore, HIS861ASP (OMIM 601728) would be invalid.
892  Genomic coordinates unavailable: there were three examples where genomic coordinates were unavailable: gene ImmunoGlobin Heavy constant Mu (IGHM) is an ofﬁcial gene symbol, but is not in the University of California at Santa Cruz (UCSC) gene annotation database table reﬂink (http:/ hgdownload.cse.ucsc.edu/goldenPath/hg19/database/refLink. txt.gz). Gene OA1 and KCNJ18 (OMIM: 613236) were also missing coordinate information.
Naming of DNA changes: in one mutation, OMIM ID: 600946.0005 (GAA180GAG), the codon change was used to number the cDNA change.
This is inconsistent with the HGVS suggestion, and it is a GAG to GAA change, not GAA to GAG (Berg et al., 1992). In this ﬁrst step of assessing the syntactic validity of the largest publicly available mutation annotation databases, we found that the majority of the annotations were accurate.
Nonetheless, in aggregate there were several thousand mutation annotations that did not pass a simple syntactic veriﬁcation procedure even after allowances were made for isoforms, signal peptide sequence and the use of gene symbol aliases rather than the standard nomenclature.
There are other potential explanations for mis-numbered sequences, including other propeptides that might be cleaved during the post-translational process.
This may explain the difference between the 44% of variants (422 of 950) that did contain a signal peptide in their sequence that still did not pass resolution using aa2nt even when it was considered.
We have made available a list of the variants that did not resolve using aa2nt to enable a community review and manual annotation process.
These data are available using a web application at http:/ safegene.hms.harvard.edu/zak/unresolvedOmimVariants.jsp. Many of these difﬁculties are the residue of early discovery work prior to standardization it is unsurprising that there is difﬁculty in resolving non-synonymous from OMIM, as the database hosts historical discoveries from the literature.
Other variants that did not achieve resolution appear to potentially be the result of some form of data transcription, transfer or copying error.
These syntactic errors fall well short of the clinical requirements for accurate interpretation of human variants.
As we approach whole genome clinical interpretation, it seems that there is an increasing common interest and public good in ensuring that all previous and new annotation data be vetted automatically by a suite of tools such as aa2nt with a standard resolution procedure for those annotations that do not pass this validation process.
Indeed, such a pipeline appears to be an essential component to the Genome Commons that some have envisaged (Brenner, 2007 Field et al., 2009), as well as a valuable addition to the process of mutation ﬁnding through text mining (Horn et al., 2004 Kuipers et al., 2010). ACKNOWLEDGEMENTS We would like to thank Dr Vincent Fusaro and Dr Joon Lee for their assistance in the development of the matching algorithm.
Funding: This research was supported by grant 1-RC1-LM010470- 01 from the National Library of Medicine and by training grant 5T32HD040128 from the National Institute of Child Health and Human Development (Dr Cassa). Conﬂict of Interest: none declared.
[10:38 19/2/2011 Bioinformatics-btr029.tex] Page: 892 891 893  Automated validation of genetic variants REFERENCES Amberger,J. et al. (2009) McKusick s Online Mendelian Inheritance in Man (OMIM). Nucleic Acids Res., 37, D793 D796. Ashley,E.A. et al. (2010) Clinical assessment incorporating a personal genome.
Lancet, 375, 1525 1535.
Berg,M.A. et al. (1992) Mutation creating a new splice site in the growth hormone receptor genes of 37 Ecuadorean patients with Laron syndrome.
Hum.
Mutat., 1, 24 32.
Brenner,S.E. (2007) Common sense for our genomes.
Nature, 449, 1915 1916.
Field,D. et al. (2009) Omics data sharing.
Science, 326, 234.
Horn,F. et al. (2004) Automated extraction of mutation data from the literature: to G protein-coupled receptors and nuclear hormone application of MuteXt receptors.
Bioinformatics, 20, 557 568.
Kent,W.J. et al. (2002) The human genome browser at UCSC. Genome Res., 12, 996 1006.
Kohane,I.S. et al. (2006) The incidentalome: a threat to genomic medicine.
JAMA, 296, 212 215.
Kuipers,R. et al. (2010) Novel tools for extraction and validation of disease-related mutations applied to fabry disease.
Hum.
Mutat., 31, 1026 1032.
Nakamura,Y. et al. (2000) Codon usage tabulated from international DNA sequence databases: status for the year 2000.
Nucleic Acids Res., 28, 292.
Ormond,K.E. et al. (2010) Challenges in the clinical application of whole-genome sequencing.
Lancet, 375, 1749 1751.
Stenson,P.D. et al. (2009) The Human Gene Mutation Database: 2008 update.
Genome Med., 1, 13.
[10:38 19/2/2011 Bioinformatics-btr029.tex] Page: 893 891 893 893
Genomics Proteomics Bioinformatics 13 (2015) 40 45 H O S T E D  BY Genomics Proteomics Bioinformatics www.elsevier.com/locate/gpb www.sciencedirect.com RESOURCE REVIEW Web Resources for Stem Cell Research Ting Wei 1,2,a, Xing Peng 2,3,b, Lili Ye 1,2,c, Jiajia Wang 2,3,d, Fuhai Song 2,3,e, Zhouxian Bai 2,3,f, Guangchun Han 2,g, Fengmin Ji 1,h, Hongxing Lei 2,4,*,i 1 College of Life Sciences and Bioengineering, Beijing Jiaotong University, Beijing 100044, China 2 CAS Key Laboratory of Genome Sciences and Information, Beijing Institute of Genomics, Chinese Academy of Sciences, Beijing 100101, China 3 University of Chinese Academy of Sciences, Beijing 100049, China 4 Center of Alzheimer s Disease, Beijing Institute for Brain Disorders, Beijing 100053, China Received 20 December 2014 revised 11 January 2015 accepted 12 January 2015 Available online 18 February 2015 Handled by Xiangdong Fang In this short review, we have presented a brief overview on major web resources relevant Abstract to stem cell research.
To facilitate more efﬁcient use of these resources, we have provided a pre- liminary rating based on our own user experience of the overall quality for each resource.
We plan to update the information on an annual basis.
KEYWORDS Reprogramming Direct conversion Physical interaction Regulatory interaction Network Introduction Stem cell research is at the frontier of regenerative medicine [1 3]. To avoid the ethical issues related to the use of embryonic stem cell (ESC) or somatic cell nuclear transfer (SCNT) * Corresponding author.
E-mail: leihx@big.ac.cn (Lei H). a ORCID: 0000-0002-3966-7545. b ORCID: 0000-0002-3645-8115. c ORCID: 0000-0001-8471-1480. d ORCID: 0000-0001-5001-8290. e ORCID: 0000-0003-0848-8349. f ORCID: 0000-0001-7071-666X. g ORCID: 0000-0001-9277-2507. h ORCID: 0000-0001-6984-8075. i ORCID: 0000-0003-0496-0386.
Peer review under responsibility of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
technology, induced pluripotent stem cell (iPSC) technology has been developed and matured in recent years [4,5]. Fibrob- last and other types of terminally-differentiated cells can be reprogrammed into iPSCs using deﬁned factors.
iPSCs can be further differentiated into various tissues using tissue-speci- ﬁc inducing factors [6]. Differentiated cells can also be directly converted to other types of differentiated cells (also termed  trans-differentiation ) [7]. To foster the fast development in this ﬁeld, several databases and web servers have been established in the past few years (Figure 1). Relevant literature and high-throughput experimental data have been curated.
Available data analyses range from identiﬁcation of physical interactions and regulatory partners to enrichment analysis and network construction.
Here, we provide a brief overview of these web resources.
Based on our own user experience of the overall quality of the resources, we have provided a preliminary rating for those resources (Table 1). The rating is mainly based on: (1) how many types of data have been includ- ed (2) how many samples or high-throughput experiments have http://dx.doi.org/10.1016/j.gpb.2015.01.001 1672-0229 ª 2015 The Authors.
Production and hosting by Elsevier B.V. on behalf of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Wei T et al  Web Resources for Stem Cell Research 41 Integration of high-throughput data in stem cell research Figure 1 In order to achieve better understanding on reprogramming, direct conversion, self-renewal, and other processes in stem cell biology, genome- wide proﬁling have been conducted at the end points and sometimes during those processes.
Various types of high-throughput data have been collected and integrated in over a dozen of specialized web resources.
The relationship among critical genes can be visualized in a variety of ways.
Shown on the background is a network generated by StemCellNet. been included (3) what kind of online data analysis is available (4) is the web interface user friendly and most importantly, (5) can we gain any novel insight by using the web tool  CellNet Among the available web resources, CellNet is the most practical tool for somatic cell reprogramming and direct conversion [8]. Analyses on the gene regulatory network (GRN) have been con- ducted on 20 mouse cell lines or tissue types and 16 human cell lines or tissue types, and several characteristic GRN modules have been identiﬁed for each cell line or tissue type.
The main aim of CellNet is to facilitate cell engineering, not limited to stem cell biology.
User-uploaded gene expression proﬁles are com- pared with the benchmark proﬁles, and three types of analysis results can be obtained.
The ﬁrst is cell and tissue type classiﬁca- tion, basically indicating how close the engineered cell is to any of the benchmark cells or tissues.
The second is the GRN status, i.e., the evaluation of the establishment of the characteristic GRN modules for intended target cell or tissue.
The third is the network inﬂuence score.
For each of the critical transcrip- tional regulators of the intended target cell or tissue, the distance to the expected expression level will be calculated and the top 50 down-regulated regulators will be highlighted.
Overall, CellNet provides a practical guide to ﬁll the gap between the engineered cell and the intended target.
Although CellNet is not speciﬁcally designed for stem cell research, this unique application on cell engineering is the main reason we gave it a 5-star rating.
LifeMap LifeMap contains a large collection of the literature and gene expression data relevant to stem cell differentiation, embryonic development and regenerative medicine [9]. Information is available for cell types including ESCs, iPSCs, embryonic pro- genitor cells, adult stem cells, primary cells, and fully-differen- tiated somatic cells from human and mouse.
Retrievable information include gene expression, signaling pathways, cell types, developmental stages, anatomical compartments, differ- entiation protocols, diseases, cell therapies, and literature ref- erences. Illustrative and interactive images are provided for better user experience.
LifeMap is more like an encyclopedia for embryonic development and regenerative medicine.
The main highlights include comprehensive curation of both  Table 1 Major web resources for stem cell research Name CellNet LifeMap ESCAPE StemCellNet HSC-explorer SyStemCell CORTECON SCDE StemBase CODEX ESCD Link Main features Rating Refs.
http://cellnet.hms.harvard.edu http://discovery.lifemapsc.com http://www.maayanlab.net/ESCAPE http://stemcellnet.sysbiolab.eu http://mips.helmholtz-muenchen.de/HSC http://lifecenter.sgst.cn/SyStemCell http://cortecon.neuralsci.org http://discovery.hsci.harvard.edu http://www.stembase.ca path= http://codex.stemcells.cam.ac.uk http://biit.cs.ut.ee/escd Cell type classiﬁcation gene regulatory networks reﬁnement of factors for cell engineering Diﬀerentiation, development, and regenerative medicine graphical display of embryonic development ontology tree Multiple data types for human and mouse ESCs network construction, enrichment analysis, lineage prediction Network with physical interaction and regulation interactive visualization of the network online Early stage of hematopoiesis interactive graphical display with many functionalities Clear indication of up or down regulation co-localization analysis for discovery of novel correlation NGS data from in vitro cortical development gene, cluster, disease, KEGG pathway, and GO term Tissue and cancer stem cells curation on experiments enrichment analysis code sharing Detailed curation of experiment information correlation and mutual information analysis NGS data for ESCs and haematopoietic cells ESCs, embryonic carcinoma cells search by GO terms wwwww wwwww wwwww wwwww wwwww wwwwq wwwwq wwwwq wwwwq wwwqq wwqqq [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] Note: Our rating is mainly based on the number of data types included the number of samples or high-throughput experiments included, the kinds of online data analysis available, whether the web interface is user-friendly, and most importantly, whether users can gain any novel insight by using the web tool.
4 2 G e n o m i c s P r o t e o m i c s B i o i n f o r m a t i c s 1 3 ( 2 0 1 5 ) 4 0  4 5  Wei T et al  Web Resources for Stem Cell Research 43 literature and gene expression information, interactive graphi- cal interface of the full development tree, and unique informa- tion on regenerative medicine.
Registration is required for the access of the full features.
ESCAPE The Embryonic Stem Cell Atlas from Pluripotency Evidence (ESCAPE) database is developed based on gene sets from pub- lished experiments on human and mouse ESCs [10]. The curat- ed data types include chromatin immunoprecipitation (ChIP) data for protein DNA interaction, regulatory information from loss-of-function and gain-of-function (Logof) experi- ments, protein protein interaction (PPI) using key factors as baits, miRNA target interactions from popular miRNA web- sites, potential key regulators from RNAi experiments, ESC- or differentiating ESC- speciﬁc proteins, histone modiﬁcations, miRNA expression, and time-course expression.
In addition to the retrieval of the collected information, these gene sets can also be used to construct interaction and regulatory networks, conduct enrichment analysis for user-supplied gene lists, and predict one of the four lineages during ESC differentiation, the latter being a unique feature among the available web tools described in this article.
The network is built upon the input gene list, curated ChIP, PPI, and Logof data.
StemCellNet StemCellNet is mainly a network tool for stem cell biology [11]. The datasets supporting the network construction include physical protein interactions with key regulators, transcrip- tional regulatory interactions from ChIP binding experiments, generic physical and regulatory interactions from public resources, and stemness gene sets from the literature.
The con- structed network can be visualized online or downloaded (as exempliﬁed in Figure 1). The online network display can be reﬁned according to several options.
The node size can be adjusted based on the number of appearances of the speciﬁc gene in the stemness datasets. Users can also evaluate the importance of the nodes based on the number of key stemness neighbors.
In addition, analysis on the signiﬁcance of enrich- ment can be performed on the network for each of the stem- ness gene sets.
The network can also be annotated by incorporating user-uploaded gene expression proﬁles. Trim- ming of the network can be achieved by applying one or sev- eral of the ﬁlters. The network functionality in StemCellNet is the best among the web tools reviewed in this article.
HSC-explorer HSC-explorer is a curated database for hematopoietic stem cells (HSCs) [12]. This database is focused on the early stage of hematopoiesis. At the time of the writing of this manuscript, over 7000 experimentally-validated interactions have been col- lected from 217 publications.
Detailed data statistics is shown on the homepage.
Search results can be displayed as both tables and graphical networks.
The interactions are carefully curated with links to the original publications when necessary.
The graphical network is user-friendly with a variety of functionalities.
The heterogeneous network nodes include gene/protein, SNP, CpG site, drug, pathway, disease, organ- ism, and environment, among others.
The types of directional interactions include increasing, decreasing and affecting the expression, quantity, activity, etc.
of one entity by the other.
Detailed information can be displayed on mouseover at the nodes or edges.
In addition to the retrieval of directly-collected information, several topics with special interest in hematopoi- esis have been curated.
Overall, this database is a good resource for researchers interested in hematopoiesis. SyStemCell SyStemCell collected 285 stem cell related publications at the initial release [13]. The majority of the data is on human and mouse, although a small amount of data is on rat and rhesus macaque.
The data types include mRNA expression, protein expression, DNA methylation and hydromethylation, histone modiﬁcation, miRNA information, and transcription factor (TF) regulation.
The search results are displayed as increase, detected, and decrease with different colors.
Annotations include information from gene ontology (GO), BioCarta, the NCBI BioSystems database, and the database of Differentially Expressed Proteins in human Cancer (dbDEPC). Other func- tionalities include data browsing and co-localization analysis.
The co-localization analysis can be used to discover novel correlation among the selected features.
The last release of SyStemCell was on Feb 10, 2012.
Therefore, data in the past three years may not be available at this website.
CORTECON CORTECON is a neural stem cell (NSC)-speciﬁc resource and a repository for gene expression in the in vitro developing human cortex [14]. The web tool is mainly based on one high- throughput sequencing study by the authors themselves.
The temporal expression data can be retrieved by gene, disease, KEGG pathway, or GO term.
Every gene belongs to one of the clusters according to the temporal expression proﬁle. But a gene may be associated with several diseases or multiple stages of cortical development.
In general, the relationship among gene cluster, disease, KEGG pathway, GO term, and development stage seems to be many-to-many.
Since this is a single study-based web tool, interpretation of the search results shall be cautioned.
SCDE The Stem Cell Discovery Engine (SCDE) is mainly focused on resources for cancer stem cells [15]. Over 53 relevant datasets (1098 assays) have been curated in the database, including samples from blood, intestine, and brain, almost all from human and mouse.
User-speciﬁed gene lists can be compared against the curated datasets. They can also be compared again- st molecular signatures in GeneSigDB, MSigDB, and Wik- iPathway. SCDE has recently evolved into two components, Stem Cell Commons and Galaxy, although both appear to be in the process of further development.
The Galaxy is mainly devoted to data analysis mentioned above.
The Stem Cell Commons (http://stemcellcommons.org/) is being developed into an integrated platform, including browse, search, analysis,  44 Genomics Proteomics Bioinformatics 13 (2015) 40 45 visualization, and code sharing.
Users can also upload data to the Stem Cell Commons.
The main goals are to promote dis- covery and reproducibility in stem cell research.
portion of the data from these projects has already been curat- ed in some of the web tools described above.
StemBase StemBase has curated 62 experiments and 217 samples from mouse, human, and rat [16]. The database can be searched in simple and advanced modes.
A portion of the expression infor- mation can be retrieved by specifying several ﬁelds. The retrieved information can be annotated by GO terms and rele- vant publications.
An additional feature in StemBase is the correlation and mutual information of expression among the speciﬁed genes or probes.
The expression of each probe can also be viewed on the UCSC genome browser, which seems to be a unique feature.
StemBase was originally designed in 2007 without any major update.
Therefore, most of the data collected are not so up-to-date.
CODEX CODEX is devoted to next-generation sequencing (NGS) experiments including ChIP-seq, RNA-seq, and DNase-seq [17]. The datasets are divided by species (human and mouse data). The regulatory information derived from the datasets can also be retrieved.
The CODEX server consists of three sec- tions, i.e., HAEMCODE for haematopoietic cells, ESCODE for embryonic stem cells, and CODEX for all cell types.
Due to the limited NGS data available for stem cell-related experi- ments, CODEX is of limited use at the present time.
ESCD The Embryonic Stem Cell Database (ESCD) has mainly col- lected datasets on key transcription factor binding, RNAi knockdown, and protein overexpression experiments [18]. Data from both human and mouse samples have been includ- ed.
In addition to ESCs, data for embryonic carcinoma cells have also been included.
ESCD can be queried by gene IDs and GO terms.
The major weakness of ESCD is the limited data types and datasets covered.
Other resources Several other resources are available on the web.
StemCellDB (http://stemcells.nih.gov/research/nihresearch/scunit/Pages Default.aspx) is established by the NIH Stem Cell Unit with an aim for direct comparison of human ESC lines, adult stem cells, and iPSCs [19]. PluriNetWork (http://www.ibima. med.uni-rostock.de/IBIMA/PluriNetWork/) has curated 274 pluripotency genes in mouse with 574 interactions (the current data statistics) [20]. The network can be downloaded for fur- ther exploration.
FunGenES was originally designed for mouse ESC differentiation [21]. However, the web server is no longer active.
Additionally, large amount of data is available from some worldwide collaboration projects with broad scope, including ENCODE (http://genome.ucsc.edu ENCODE/), TCGA (https://icgc.org/), and Roadmap Epige- nomics (http://www.roadmapepigenomics.org/). However, a Concluding remarks It is an ongoing effort to develop efﬁcient tools for the better understanding of reprogramming, differentiation, and trans- differentiation.
Some of the web resources are continuously updated or upgraded.
We shall point out that a good portion of the web resources have not been well maintained since the initial publication.
New tools will surely emerge in the future.
The continuous effort on web maintenance should be carefully considered when developing new web tools.
We ourselves are also in the process of developing an integrated web server for stem cell research.
Mere collection of public data will be far from sufﬁcient in the future.
A major effort should be focused on enhancing our fundamental understanding of the mechanism regarding the maintenance of pluripotency and gaining precise control of the reprogramming, differentiation, and direct conversion.
Competing interests The authors declare that there are no conﬂicts of interest.
Acknowledgements This work was supported by the grants from the National Basic Research Program of China (973 Program Grant No.2014CB964901) and the National High-tech R&D Program of China (863 Program Grant No.2015AA020100) awarded to HL by the Ministry of Science and Technology of China.
References [1] Young RA. Control of the embryonic stem cell state.
Cell 2011 144:940 54.
[2] Krupalnik V, Hanna JH. Stem cells: the quest for the perfect reprogrammed cell.
Nature 2014 511:160 2.
[3] Wang H, Zhang Q, Fang X. Transcriptomics and proteomics in stem cell research.
Front Med 2014 8:433 44.
[4] Takahashi K, Yamanaka S. Induction of pluripotent stem cells from mouse embryonic and adult ﬁbroblast cultures by deﬁned factors.
Cell 2006 126:663 76.
[5] Takahashi K, Tanabe K, Ohnuki M, Narita M, Ichisaka T, Tomoda K, et al. Induction of pluripotent stem cells from adult human ﬁbroblasts by deﬁned factors.
Cell 2007 131:861 72.
[6] Hartﬁeld EM, Yamasaki-Mann M, Ribeiro Fernandes HJ, Vowles J, James WS, Cowley SA, et al. Physiological charac- terisation of human iPS-derived dopaminergic neurons.
PLoS One 2014 9:e87388. [7] Xue Y, Ouyang K, Huang J, Zhou Y, Ouyang H, Li H, et al. Direct conversion of ﬁbroblasts to neurons by reprogramming PTB-regulated microRNA circuits.
Cell 2013 152:82 96.
[8] Cahan P, Li H, Morris SA, Lummertz da Rocha E, Daley GQ, Collins JJ. CellNet: network biology applied to stem cell engineering.
Cell 2014 158:903 15.
[9] Edgar R, Mazor Y, Rinon A, Blumenthal J, Golan Y, Buzhor E, et al. LifeMap discovery: the embryonic development, stem cells,  Wei T et al  Web Resources for Stem Cell Research 45 regenerative medicine and 2013 8:e66629. research portal.
PLoS One [10] Xu H, Baroukh C, Dannenfelser R, Chen EY, Tan CM, Kou Y, et al. ESCAPE: database for integrating high-content published data collected from human and mouse embryonic stem cells.
Database (Oxford) 2013 2013:bat045. [11] Pinto JP, Reddy Kalathur RK, Machado RS, Xavier JM, Braganca J, Futschik ME.
StemCellNet: an interactive platform for network-oriented investigations in stem cell biology.
Nucleic Acids Res 2014 42:W154 60.
[12] Montrone C, Kokkaliaris KD, Loefﬂer D, Lechner M, Kasten- muller G, Schroeder T, et al. HSC-explorer: a curated database for hematopoietic stem cells.
PLoS One 2013 8:e70348. [13] Yu J, Xing X, Zeng L, Sun J, Li W, Sun H, et al. SyStemCell: a database populated with multiple levels of experimental data from stem cell differentiation research.
PLoS One 2012 7:e35230. [14] van de Leemput J, Boles NC, Kiehl TR, Corneo B, Lederman P, Menon V, et al. CORTECON: a temporal transcriptome analysis of in vitro human cerebral cortex development from human embryonic stem cells.
Neuron 2014 83:51 68.
[15] Ho Sui SJ, Begley K, Reilly D, Chapman B, McGovern R, Rocca- Sera P, et al. The Stem Cell Discovery Engine: an integrated repository and analysis system for cancer stem cell comparisons.
Nucleic Acids Res 2012 40:D984 91.
[16] Porter CJ, Palidwor GA, Sandie R, Krzyzanowski PM, Muro EM, Perez-Iratxeta C, et al. StemBase: a resource for the analysis of expression data.
Methods Mol Biol 2007 407:137 48. stem cell gene [17] Sanchez-Castillo M, Ruau D, Wilkinson AC, Ng FS, Hannah R, Diamanti E, et al. CODEX: a next-generation sequencing experiment database for the haematopoietic and embryonic stem cell communities.
Nucleic Acids Res 2015 43:D1117 23.
[18] Jung M, Peterson H, Chavez L, Kahlem P, Lehrach H, Vilo J, et al. A data integration approach to mapping OCT4 gene regulatory networks operative in embryonic stem cells and embryonal carcinoma cells.
PLoS One 2010 5:e10709. [19] Mallon BS, Chenoweth JG, Johnson KR, Hamilton RS, Tesar PJ, Yavatkar AS, et al. StemCellDB: the human pluripotent stem cell database at the National Institutes of Health.
Stem Cell Res 2013 10:57 66.
[20] Som A, Harder C, Greber B, Siatkowski M, Paudel Y, Warsow G, et al. The PluriNetWork: an electronic representation of the network underlying pluripotency in mouse, and its applications.
PLoS One 2010 5:e15165. [21] Schulz H, Kolde R, Adler P, Aksoy I, Anastassiadis K, Bader M, et al. The FunGenES database: a genomics resource for mouse differentiation.
PLoS One 2009 4:e6804. stem cell embryonic
BIOINFORMATICS APPLICATIONS NOTE Vol.27 no.8 2011, pages 1159 1161 doi:10.1093/bioinformatics/btr087 Advance Access publication February 23, 2011 Sequence analysis PrimerProspector: de novo design and taxonomic analysis of barcoded polymerase chain reaction primers William A. Walters1, , J. Gregory Caporaso2, , Christian L. Lauber3, Donna Berg-Lyons3, Noah Fierer3,4 and Rob Knight2,5, 1Department of Molecular, Cellular, and Developmental Biology, 2Department of Chemistry and Biochemistry, 3Cooperative Institute for Research in Environmental Sciences, 4Department of Ecology and Evolutionary Biology, University of Colorado at Boulder, Boulder, CO 80309, USA and 5Howard Hughes Medical Institute, Boulder, CO, USA Associate Editor: John Quackenbush ABSTRACT Motivation: PCR ampliﬁcation of DNA is a key preliminary step in many applications of high-throughput sequencing technologies, yet design of novel barcoded primers and taxonomic analysis of novel or existing primers remains a challenging task.
Results: PrimerProspector is an open-source software package that allows researchers to develop new primers from collections of sequences and to evaluate existing primers in the context of taxonomic data.
Availability: PrimerProspector is open-source software available at http://pprospector.sourceforge.net Contact: rob.knight@colorado.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on December 8, 2010 revised and accepted on February 8, 2011 1 INTRODUCTION Using next-generation sequencing methods to characterize hundreds of samples simultaneously in a single sequencing run has revolutionized microbial ecology (Tringe and Hugenholtz, 2008). However, primer design for such studies remains challenging.
The primers must amplify an appropriate region of DNA that is the right length for sequencing and also taxonomically informative (Liu et al., 2008 Wang et al., 2007) a linker that is not complementary to the target in any one of many diverse species must be inserted before the barcode to avoid differential ampliﬁcation (Hamady et al., 2008) and the set of barcodes must be checked to avoid formation of secondary structure within or between primers (i.e. primer- dimers) or between the barcodes and the primers.
Additionally, the techniques need to be generic rather than tied to one taxonomic outline or database, so that many different target genes can be studied.
Here we present PrimerProspector, an open-source software package for primer design and analysis built using the PyCogent toolkit (Knight et al., 2007), that resolves these issues.
We recently  To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the ﬁrst two authors should be regarded as joint First Authors.
applied PrimerProspector to identify the 16S rRNA 515f/806r primer pair as nearly universal to archaea and bacteria, and to optimize this primer pair for increased sensitivity across these domains.
This optimized primer pair, applied successfully in several recent studies (Bates et al., 2010 Caporaso et al., 2010 G.Bergmann et al., manuscript in preparation), has provided novel insight into archaeal and bacterial community membership in soils by allowing for more accurate determination of the abundances of taxa missed by many commonly used canonical primer pairs, e.g. the Verrucomicrobia. (cid:2) No existing tools speciﬁcally address the issues associated with designing barcoded polymerase chain reaction (PCR) primers for community analysis.
Primer design is a large ﬁeld and we cannot survey it comprehensively in this article, but among a selection of related tools, Primer Validator (http://bioinfo.unice.fr/454) allows taxonomic assessment but does not generate de novo primers, or allow a customizable 3 weighted scoring system to predict successful ampliﬁcation of tested primers.
BarCrawl (Frank, 2009) allows design of barcodes for speciﬁed PCR primers but not design of the primers themselves, so is a useful complement to PrimerProspector. RDP s Probe Match (Cole et al., 2005) will report sequences matching a probe, as does Greengenes probe function (DeSantis et al., 2006), but these tools are tied to the respective 16S rRNA databases and do not have support for barcodes. Primrose and OligoCheck (Ashelford et al., 2002) are useful for small numbers of target sequences, but do not scale well to thousands or tens of thousands of sequences, as is necessary when designing universal or near-universal primers, and do not incorporate differential weighting of 5 bases in primer scoring.
Primer BLAST uses Primer3 software to build primers of a speciﬁed length against one target sequence, and then BLASTs the results against other databases to ensure that putative primers do not target BLAST hits.
This functionality is also a useful complement to that provided in PrimerProspector. and 3 (cid:2) (cid:2) While applications of PrimerProspector to date have focused on SSU rRNA primer design, PrimerProspector can be used for any nucleic acid sequences and allows users to design de novo primers based upon arbitrary multiple sequence alignments.
User-speciﬁable design parameters include primer length, degeneracy and targeted regions for generation of primers.
Existing or de novo primers can be analyzed for predicted taxonomic coverage, as shown in Figure 1.
Finally, common pitfalls in primer design can be identiﬁed, such as likely barcode-primer secondary structure, regions susceptible to  The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:58 5/4/2011 Bioinformatics-btr087.tex] Page: 1159 1159 1161  B W.A.Walters et al. A C Fig.1.
Taxonomic coverage summary of the 515f/806r 16S SSU rRNA primer pair at the phylum level for (A) archaea, (B) eukarya and (C) bacteria.
The y-axes represent percent coverage and the value on top of each bar is the total number of reference sequences in each taxon.
In this analysis, the reference sequences were derived from the Silva database, and ﬁltered at 97% sequence identity with uclust (Edgar, 2010). Archaeal and bacterial sequences shorter than 1450 bases, and eukaryotic sequences less that 1800 bases, were excluded from the reference set.
As illustrated, this primer pair is nearly universal for archaeal and bacterial 16S but is generally poor for eukaryotic (notably metazoan) 18S sequences.
This plot and additional PrimerProspector analyses informed the decision to use this primer pair in Caporaso et al. (2010), Bates et al. (2010) and G.Bergmann et al. (2010). Comparisons with the unoptimized primer pair and with an alternative popular pair (27f/338r) are shown as Supplementary Figures S1 and S2, respectively.
primer dimerization and disparate GC content between primer pairs.
Convenient reports show amplicons or simulated reads that cover regions of sequences that are not phylogenetically informative or are of unsuitable lengths for sequencing.
(cid:2) 2 METHODS De novo design of primers is performed by ﬁnding short conserved sequences binding site for in a given multiple sequence alignment to act as a 3 new primers.
Once these sites have been identiﬁed, full-length forward or reverse de novo primers are generated by incorporating the N upstream or downstream bases, where N is 15 by default.
De novo full-length primers can then be sorted according to sensitivity, speciﬁcity or degeneracy, and compared with known primers to ﬁnd matches or signiﬁcant overlap.
Speciﬁcity for particular target groups, such as archaea, can be obtained by supplying an optional alignment of sequences from which to exclude matches.
Primer analyses, including the prediction of taxonomic coverage, rely upon scoring primers against target sequences.
To predict its taxonomic coverage, a primer is locally aligned to full-length target sequences with 1160 (cid:2) (cid:2) known taxonomies, and scored based on gap, 3 mismatch and non-3 is provided in mismatch counts.
An example of the graphical output (cid:2) Supplementary Figure S3. The ﬁnal ﬁve bases are considered to be the 3 region by default, and are considered to be the most important for PCR ampliﬁcation. The scoring scheme is parameterizable. The RDP Classiﬁer (Wang et al., 2007) is used to classify the resulting sequence fragments, and the accuracy is displayed both in terms of which taxa are ampliﬁed and in terms of classiﬁcation level of the resulting fragments.
PrimerProspector supports retraining of the RDP Classiﬁer for taxa coverage analysis based on different reference taxonomies.
Descriptions of the scripts included in PrimerProspector, the various outputs generated by PrimerProspector and an example based on the F515/R806 primer pair are included in the online documentation at http://pprospector.sourceforge.net/. 3 CONCLUSIONS PCR ampliﬁcation continues to be a key step in many high- throughput sequencing applications such as barcoded marker gene- based microbial community analyses.
PrimerProspector represents [11:58 5/4/2011 Bioinformatics-btr087.tex] Page: 1160 1159 1161  a signiﬁcant advance over prior work in this area by providing a single tool to facilitate primer design and analysis, including support for barcodes (and associated linkers). PrimerProspector is a fast and extensible framework for primer design and analysis, and has already been successfully applied to help researchers identify the most relevant and useful primers for their application, starting with multiple sequence alignments for any nucleic acid sequence.
Funding: Bill and Melinda Gates foundation Crohn s and Colitis foundation of America Howard Hughes Medical Institute National Institutes of Health Signaling and Cell Cycle Regulation Training Grant (T32GM008759) in part.
Conﬂict of Interest: none declared.
REFERENCES Ashelford,K.E. et al. (2002) PRIMROSE: a computer program for generating and estimating the phylogenetic range of 16S rRNA oligonucleotide probes and primers in conjunction with the RDP-II database.
Nucleic Acids Res., 30, 3481 3489.
Bates,S.T. et al. (2010) Examining the global distribution of dominant archaeal populations in soil.
ISME J.[Epub ahead of print, doi:10.1038/ismej.2010.171]. PrimerProspector Caporaso,J.G. et al. (2010) Global patterns of 16S rRNA diversity at a depth of millions of sequences per sample.
Proc. Natl Acad. Sci.USA [Epub ahead of print, doi: 10.1073/pnas.1000080107]. Cole, J.R. et al. (2005) The Ribosomal Database Project (RDP-II): sequences and tools for high-throughput rRNA analysis.
Nucleic Acids Res., 33, D294 D296. DeSantis,T.Z. et al. (2006) Greengenes, a chimera-checked 16S rRNA gene database and workbench compatible with ARB. Appl. Environ. Microbiol., 72, 5069 5072.
Edgar,R.C. (2010) Search and clustering orders of magnitude faster than BLAST.
Bioinformatics, 26, 2460 2461.
Frank,D.N. (2009) BARCRAWL and BARTAB: software tools for the design and implementation of barcoded primers for highly multiplexed DNA sequencing.
BMC Bioinformatics, 10, 362.
Hamady,M. et al. (2008) Error-correcting barcoded primers for pyrosequencing hundreds of samples in multiplex.
Nat. Methods, 5, 235 237.
Knight,R. et al. (2007) PyCogent: a toolkit for making sense from sequence.
Genome Biol., 8, R171. Liu,Z. et al. (2008) Accurate taxonomy assignments from 16S rRNA sequences produced by highly parallel pyrosequencers. Nucleic Acids Res., 36, e120. Tringe,S.G. and Hugenholtz,P. (2008) A renaissance for the pioneering 16S rRNA gene.
Curr. Opin. Microbiol., 11, 442 446.
Wang,Q. et al. (2007) Naive Bayesian classiﬁer for rapid assignment of rRNA sequences into the new bacterial taxonomy.
Appl. Environ. Microbiol., 73, 5261 5267.
[11:58 5/4/2011 Bioinformatics-btr087.tex] Page: 1161 1159 1161 1161
BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.18 2010, pages 2354 2356 doi:10.1093/bioinformatics/btq415 Advance Access publication August 2, 2010 Databases and ontologies ISA software suite: supporting standards-compliant experimental annotation and enabling curation at the community level Philippe Rocca-Serra1,2, , Marco Brandizi1, , Eamonn Maguire1,2, , Nataliya Sklyar1, , Chris Taylor1,3, Kimberly Begley4, Dawn Field3,5, Stephen Harris6, , Winston Hide4, Oliver Hofmann4, Steffen Neumann7, Peter Sterk3,5, Weida Tong6, and Susanna-Assunta Sansone1,2, 1The European Bioinformatics Institute, Wellcome Trust Genome Campus, Cambridge, CB10 1SD and 2Oxford e-Research Centre, University of Oxford, 7 Keble Road, Oxford OX1 3QG, 3Natural Environment Research Council, Environmental Bioinformatics Centre, Wallingford CEH, Benson Lane, Mansﬁeld Road, Oxford OX10 8BB, UK, 4Harvard School of Public Health, 677 Huntington Avenue, Boston, MA 02115, USA, 5Genomic Standards Consortium, Wellcome Trust Sanger Institute, Cambridge CB10 1SD, UK, 6US Food and Drug Administration, Center for Bioinformatics, National Center for Toxicological Research, 3900 NCTR Road, Jefferson, AR 72079, USA and 7Leibniz Institute of Plant Biochemistry, Department of Stress and Developmental Biology, Weinberg 3, 06120 Halle, Germany Associate Editor: Jonathan Wren ABSTRACT Summary: The ﬁrst open source software suite for experimentalists and curators that (i) assists in the annotation and local management of experimental metadata from high-throughput studies employing one or a combination of omics and other technologies (ii) empowers users to uptake community-deﬁned checklists and ontologies and (iii) facilitates submission to international public repositories.
Availability and Implementation: Software, documentation, case studies and implementations at http://www.isa-tools.org Contact: isatools@googlegroups.com Received on February 23, 2010 revised on July 7, 2010 accepted on July 8, 2010 1 HIGH-THROUGHPUT OMICS STUDIES The development of high-throughput genomic and post-genomic (hereafter, omics ) technologies entails changes in the handling, processing and sharing of data (Schoﬁeld et al., 2009). Omics datasets are often complex and rich in context.
Studies may run material through several kinds of assay, using both omics and other technologies for example, studying the effect of a compound on rat liver through transcriptome, proteome and metabolome proﬁling (using high-throughput sequencing and two kinds of mass spectrometry, respectively) alongside conventional analyses (e.g. histopathology). Such data must be accompanied by enough contextual information (i.e. metadata sample characteristics, technology and measurement types instrument parameters and sample-to-data relationships) to make datasets comprehensible and reusable if they are to underpin future investigations.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the ﬁrst four authors should be regarded as joint First Authors.
The views presented in this article do not necessarily reﬂect those of the US Food and Drug Administration Many funders and journals require that researchers share data, and encourage the enrichment and standardization of experimental metadata (Field et al., 2009). Consequently, more and richer studies are ﬂowing into public databases.
However, two bottlenecks can signiﬁcantly hamper this process, necessitating urgent solutions.
First, international public repositories for omics data such as GEO (Barrett et al., 2009), ArrayExpress (Parkinson et al., 2009), PRIDE (Vizcaíno et al., 2010), ENA, SRA and DRA (Shumway et al., 2010), have their own submission formats, data models and terminologies, created for speciﬁc types of assay.
This complicates the submission process for researchers producing multi-assay studies (and greatly increases the risk that these datasets become irrevocably fragmented). Secondly, the shortage of curators to check and annotate submissions to public repositories a situation unlikely to change soon necessitates better annotation at source (by experimentalists or community-based efforts Howe et al., 2008). Free software, with automated content validation, is required to facilitate the collection, management and curation of a variety of study inhouse, and to format those data for submission to public repositories.
Such software should support community-deﬁned reporting standards, such as the minimum information checklists listed by the MIBBI Portal (Taylor et al., 2007), and ontologies, (Côté et al., 2006 Smith et al., 2007 Noy et al., 2009). The Investigation/Study/Assay (ISA) infrastructure described here is the ﬁrst general-purpose format and freely available desktop software suite designed to regularize local management of experimental metadata by enabling curation at source, supporting community-deﬁned reporting standards and preparing studies for submission to public repositories.
2 THE ISA FORMAT AND SOFTWARE SUITE The software suite comprises ﬁve platform-independent Java-based software components for local use, including a relational database (Fig. 1), built around the ISA-Tab format.
The components work  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:50 11/8/2010 Bioinformatics-btq415.tex] Page: 2354 2354 2356  ISA software suite ISAcreator: a user-friendly editor 2.2 This desktop application enables users (i.e. experimentalists) to compile experimental metadata sets, and to import and edit existing ISA-Tab formatted ﬁles. It breaks down overall descriptions into relatively simple parts, uses graphical abstraction to enable visualization of the information described and facilitates time-efﬁcient description of experimental steps by remembering prior behaviour (through user proﬁles). ISAcreator s aesthetically pleasing interface makes extensive use of Java Swing and external open source libraries (e.g. Prefuse, http://prefuse.org/). The editor uses a style of form- and spreadsheet-based data entry that is likely to be familiar to researchers, augmenting basic functionality such as auto-ﬁll and undo with advanced features, listed below.
2.2.1 Ontology support A dedicated widget allows ontology terms to be searched for and inserted in real time via the BioPortal (Noy et al., 2009) and the Ontology Lookup Service (Côté et al., 2006). Terms from those sources are imported along with core metadata (identiﬁers, deﬁnitions and ontology version) term selection is facilitated by a search history displaying prior choices (through user proﬁles). 2.2.2 Design wizard An alternative way for users to enter information that leverages common patterns to reduce repetitive tasks by guiding users through a series of questions that elicit information about the design of the Study and associated Assay(s). 2.2.3 Spreadsheet import As a second alternative, this widget enables the mapping and import of information from existing spreadsheets also the reformatting and reannotation of legacy data.
2.2.4 Data ﬁle chooser This widget appends data ﬁles located either local to the operator, or identiﬁed by FTP on a remote system, to an experimental metadata sets.
Upon completion of a valid investigation report, ISAcreator outputs a compressed ISArchive containing the ISA-Tab-formatted metadata and either the actual data ﬁles, or a reference to them, if necessary (e.g. because of their large size), consisting of their address and ﬁle name.
ISAconﬁgurator: standards-compliant templates 2.3 This desktop application allows power users (i.e. community curators) to customize the ﬁelds displayed by ISAcreator, and for example, to meet the requirements of one or more MIBBI minimum information checklists by declaring certain ﬁelds mandatory, or by specifying allowed values (e.g. drawn from a set of ontology terms, or formatted in a speciﬁc manner). Conﬁguration ﬁles from ISAconﬁgurator are read by ISAcreator, which then generates interface components as required.
ISAvalidator: adherence to templates 2.4 This desktop application also reads conﬁguration ﬁles and checks both that completed ISA-Tab ﬁles meet speciﬁed requirements and that associated data ﬁles have been linked.
Whether ISA-Tab ﬁles are created with ISAcreator or another way (e.g. with spreadsheet software), ISAvalidator checks that the document is syntactically correct and internally consistent, and reports on errors (i.e. missing or incorrect values). 2355 Fig.1.
The role of each ISA software component, showing their interrelations, target users and the ﬂow of information through the system.
both as stand-alone applications and as a uniﬁed system to assist in the local management and storage of experimental metadata, and to facilitate data submission to international public repositories.
All components run as desktop applications in addition, the database component features a web-based query interface.
2.1 ISA-Tab: an extensible, cross-domain format Investigation , Study and Assay are the three key entities around which the general-purpose ISA-Tab format for structuring and communicating metadata is built (Sansone et al., 2008). Investigation contains all the information needed to understand the overall goals and means used in an experiment Study is the central unit, containing information on the subject under study, its characteristics and any treatments applied.
Each Study has associated Assay(s), producing qualitative or quantitative data, deﬁned by the type of measurement (i.e. gene expression) and the technology employed (i.e. high-throughput sequencing). The hierarchical structure of ISA-Tab enables the representation of studies employing one or a combination of omics and other technologies, overcoming the fragmentation of the existing submission formats built for speciﬁc types of assay.
To ensure conversion, ISA-Tab has been designed with reference to these existing omics formats (Jones et al., 2007), complementing and extending their work where necessary for example, it shares both syntax and the use of easily- manipulable tab-delimited text ﬁles with ArrayExpress MAGE-Tab (Rayner et al., 2006). Additionally, where omics-based technologies are used in clinical or non-clinical studies, ISA-Tab complements existing biomedical formats such as the Study Data Tabulation Model (http://www.cdisc.org/sdtm), endorsed by the US Food and Drug Administration.
ISA-Tab also complements the XML formats used by the PRIDE, ENA, SRA and DRA repositories, and consequently offers a way to render their experimental metadata documents in a more user-friendly format.
Note though that ISA- Tab is simply a format the decision on how to regulate its use (i.e. enforcing the ﬁlling of required ﬁelds, or the use of ontologies) is left to local administrators use of ISA software components, or the growing number of other systems and groups implementing the format (e.g. Krestyaninova et al., 2009 SysMO-DB http://www.sysmo-db.org/community XperimentR, http://www.imperial.ac.uk/bioinfsupport/resources/data_management more given on the ISA web site). [11:50 11/8/2010 Bioinformatics-btq415.tex] Page: 2355 2354 2356  P.Rocca-Serra et al. 2.5 BioInvestigation Index: local storage An ISArchive provides a simple way to store and share information in a structured manner, but those tasks are better performed by uploading such a ﬁle to an instance of our BioInvestigation Index (BII), or another system that implements ISA-Tab import.
The BII includes a management tool and relational database (tested with Oracle, MySQL and PostgreSQL). The former enables validation and loading of an ISArchive and provides simple permissions functionality to link users (or groups of users) to studies.
The latter manages the storage of experimental metadata, which can be collectively searched and browsed via a query interface or web services the destination for associated data ﬁles, and their protocol for transfer, is custom deﬁned by the local administrator on installation.
As an example, a publicly accessible instance of the BII, maintained by the European Bioinformatics Institute (http://www.ebi.ac.uk/bioinvindex), has proven useful as a curation and storage system for multi-assay studies, and as a mechanism for submitting data ﬁles to ArrayExpress, PRIDE, ENA and SRA. Installation of the BII system requires some knowledge of database management.
However, it is portable enough to be easily installed in individual labs, to maximize the efﬁciency with which high- throughput studies can be managed and shared among users that have been granted access to them.
ISAconverter: submission to public repositories 2.6 ISAconverter recodes the relevant parts of ISArchives as MAGE- Tab, PRIDE XML or SRA-XML (used by ArrayExpress, PRIDE and ENA, SRA and DRA, respectively), enabling combined submission to public omics repositories.
It is readily extensible to support export of other formats, e.g. SOFT required by GEO (Barrett et al., 2009). Mappings for format elements are available in the ISA-Tab speciﬁcation and documentation on the ISA web site.
3 COLLABORATIONS AND CASE STUDIES Developed for the European multi-site CarcinoGENOMICS project (Vinken et al., 2008), the ISA software suite version one was released in early 2009.
The core ISA developers are engaged with an ever-growing number of collaborators: case studies from early implementers already provide evidence of the diverse life science scenarios in which the suite s various components have been successfully tested and are being used with large datasets (details on the ISA web site). The main limitations recorded to date are simply the person hours required to specify the standards and ontologies to be used and to actually curate studies.
Demonstrable acceptance and community engagement has also brought a new funding stream for this project, allowing us to continue the collaborative development of this exemplar system that supports data sharing policies, promotes the uptake of community- deﬁned reporting standards and ontologies and enables curation at source (Field et al., 2009). The ISA components, in particular the BII, have been designed to provide core functionalities.
Inevitably, each collaborator has additional in-house requirements that are too speciﬁc to be included as core functionality.
This may be due to the nature of their studies or their need for one or more ISA software components to be interoperable with existing systems.
To support further collaborative development, 2356 the core ISA developers are setting up an environment for distributed development, and are augmenting the ISA code base with Application Programming Interfaces (APIs). Ongoing collaborative activities include: a module to enable the analysis of ISA-Tab formatted metadata and any associated data, using R integration with other data management and analysis systems (e.g. Fang et al., 2009 MetWare, http://metware.org) and giving assistance to the growing number of projects exploring the tools and underlying format (e.g. Sage http://sagecongress.org/WP/workstreams/Standards Kawaji et al., 2009). Other collaborative activities include an enhanced user authentication system, support for additional formats such as RDF, OWL and SOFT, converters to/from lab equipment-related ﬁle formats (e.g. sampling robots and mass spectrometers) and improved packaging and distribution mechanisms to offer a single download bundle to facilitate installation.
ACKNOWLEDGEMENTS The ISA developers owe debts of gratitude to many collaborators, as listed at: http://isatab.sf.net/people_funding.html. Funding: CarcinoGENOMICS, NuGO, BBSRC (BB/I000917/1, BB/G000638/1, BB/E025080/1), NERC-NEBC and EMBL. Conﬂict of Interest: none declared.
REFERENCES Barrett,T. et al. (2009) NCBI GEO: archive for high-throughput functional genomic data.
Nucleic Acids Res., 37, 885 890.
Côté,R.G. et al. (2006) The ontology lookup service, a lightweight cross-platform tool for controlled vocabulary queries.
BMC Bioinformatics, 7, 97.
Fang,H. et al. (2009) ArrayTrack: an FDA and public genomic tool.
Methods Mol. Biol., 563, 379 398.
Field,D. et al. (2009) Omics Data Sharing.
Science, 9, 234 236.
Howe,D. et al. (2008) Big data: the future of biocuration. Nature, 4, 47 50.
Jones,A.R. et al. (2007) The Functional Genomics Experiment model (FuGE): an extensible framework for standards in functional genomics.
Nat. Biotechnol., 25, 1127 1133.
Kawaji,H. et al. (2009) The FANTOM web resource: from mammalian transcriptional landscape to its dynamic regulation.
Genome Biol., 10, R40. Krestyaninova,M. et al. (2009) A System for Information Management in BioMedical Studies SIMBioMS. Bioinformatics, 25, 2768 2769.
Noy,N.F. et al. (2009) BioPortal: ontologies and integrated data resources at the click of a mouse.
Nucleic Acids Res., 37, W170 W173. Parkinson,H. et al. (2009) ArrayExpress update-from an archive of functional genomics experiments to the atlas of gene expression.
Nucleic Acids Res., 37, 868 872.
Rayner,T.F. et al. (2006) A simple spreadsheet-based, MIAME-supportive format for microarray data: MAGE-TAB.
BMC Bioinformatics, 7, 489.
Sansone,S.A. et al. (2008) The ﬁrst RSBI (ISA-TAB) workshop: can a simple format work for complex studies  OMICS, 12, 143 149.
Schoﬁeld,P.N. et al. (2009) Post-publication sharing of data and tools.
Nature, 10, 171 173.
Shumway,M. et al. (2010) Archiving next generation sequencing data.
Nucleic Acids Res., 38, 870 871.
Smith,B. et al. (2007) The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration.
Nat. Biotechnol., 25, 1251 1255 Taylor,C.F. et al. (2007) MIBBI: a minimum information checklist resource.
Nat. Biotechnol., 26, 889 896.
Vinken,M. et al. (2008) The carcinoGENOMICS project: critical selection of model compounds for the development of omics-based in vitro carcinogenicity screening assays.
Mutat. Res., 659, 202 210.
Vizcaíno,J.A. et al. (2010) The Proteomics Identiﬁcations database: 2010 update.
Nucleic Acids Res., 38, 736 742.
[11:50 11/8/2010 Bioinformatics-btq415.tex] Page: 2356 2354 2356
BIOINFORMATICS Vol.24 ISMB 2008, pages i295 i303 doi:10.1093/bioinformatics/btn156 The EXACT description of biomedical protocols Larisa N. Soldatova , , Wayne Aubrey , Ross D. King and Amanda Clare Department of Computer Science, Aberystwyth University, Penglais, Aberystwyth, SY23 3DB, Wales, UK ABSTRACT Motivation: Many published manuscripts contain experiment protocols which are poorly described or deﬁcient in information.
This means that the published results are very hard or impossible to repeat.
This problem is being made worse by the increasing complexity of high-throughput/automated methods.
There is therefore a growing need to represent experiment protocols in an efﬁcient and unambiguous way.
Results: We have developed the Experiment ACTions (EXACT) ontology as the basis of a method of representing biological laboratory protocols.
We provide example protocols that have been formalized using EXACT, and demonstrate the advantages and opportunities created by using this formalization.
We argue that the use of EXACT will result in the publication of protocols with increased clarity and usefulness to the scientiﬁc community.
Availability: The ontology, examples and code can be downloaded from http://www.aber.ac.uk/compsci/Research/bio/dss/EXACT Contact: Larisa Soldatova lss@aber.ac.uk 1 INTRODUCTION  Everything is vague to a degree you do not realize till you have tried to make it precise.
Bertrand Russell The ability to repeat a published experiment protocol is the foundation stone of laboratory science.
It is widely accepted that for new knowledge to be published in a scientiﬁc journal the protocols used to derive that new knowledge must also be published.
This is essential to validate that the process by which the knowledge was inferred was not ﬂawed in any fundamental way, and to ensure that the result was not caused by some chance event.
In order to repeat a protocol it must necessarily be described in sufﬁcient and unambiguous detail to enable another agent (human or machine) to be able to replicate the original experiment actions.
With the increasing complexity of experiment methods, the description of laboratory protocols is becoming correspondingly more complicated and intricate.
This means that there is a growing technological need to be able to represent experiment protocols in an efﬁcient and unambiguous way.
We propose the Experiment ACTions (EXACT) ontology as the basis of a method of representing biological laboratory protocols.
EXACT provides a model for the description of experiment actions and it can be used for the fully formalized representation of protocols.
It can also be combined with other formalisms for the description of bio-medical investigations.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the ﬁrst two authors should be regarded as joint First Authors.
The rest of this article is organized as follows: Section 2 provides the background to this work, Section 3 provides a detailed description of the proposed ontology of experiment actions and Section 4 demonstrates the application of the ontology to the formalized description of two protocols: for creating competent cells and for compound library replication.
In Section 5 we describe the opportunities for the application/implementation of EXACT and ﬁnally Section 6 provides discussion and conclusion.
2 BACKGROUND 2.1 Current problems The degree of information granularity present in many published protocols is often insufﬁcient to allow the method to be repeated successfully.
An optimization period is then necessary to bridge the gap in knowledge between the published protocol and one which works reliably.
Knowledge of how best to implement an existing method is regarded as a group s intellectual property and is often not included in published manuscripts.
Each and every time research results are published with insufﬁcient information in the materials and methods section this duplication of labour is repeated, adding to inconvenience and cost.
A manuscript published by Akada et al. (2006) illustrates the difﬁculty in repeating another researcher s protocol when not all the necessary information is provided.
The manuscript outlines a novel protocol for gene deletion in Saccharomyces cerevisiae. The protocol focusses on the generation of a deletion cassette through the fusion of two DNA fragments.
The manuscript provides ample information on strains, media, primer sequence and polymerase chain reaction (PCR) conditions.
However, when repeated in our laboratory the deletion cassette could not be generated.
Personal communication with the author revealed that the two DNA fragments require gel puriﬁcation before a successful PCR fusion can occur.
This proved to be a vital step yet was not included in the published manuscript.
The excerpt shown in Table 1 describes the ﬁrst stages of making yeast cells competent and was taken from the High Efﬁciency Transformation of Yeast protocol published in Methods in Yeast Genetics (Amberg et al., 2005), a text book routinely cited in published papers.
The protocol is summarized in point form using natural language.
This can lead to ambiguous statements with unclear objectives.
Point 1: does the word inoculate mean using a single yeast colony from a solid media plate or can the liquid YPAD (Yeast extract, Peptone, Adenine hemisulfate, Dextrose) be inoculated from another previously inoculated YPAD liquid culture The statement incubate with shaking, is this at 20 rpm or 400 rpm Does overnight mean 12 or 24 h Point 2: it states count overnight culture, which only suggests that the person executing the protocol needs to calculate approximately how many cells are present in the overnight culture.
How this estimate is achieved is not stated.
Having statements which  2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[19:52 18/6/03 Bioinformatics-btn156.tex] Page: i295 i295 i303  L.N.Soldatova et al. Table 1.
High-efﬁciency transformation of yeast, methods in yeast genetics   1.
Inoculate 4 ml of liquid YPAD or 10 ml of SC and incubate with shaking overnight at 30 2.
Count overnight culture and inoculate 50 ml of YPAD to a cell density of 5 106/ml culture 3.
... C can be interpreted in different ways introduce inconsistencies in how the protocol is executed.
This can introduce noise and contribute to inaccurate ﬁndings. In this instance the author has nothing to gain from omitting information from the method.
The target audience of this protocol are yeast biologists and therefore the author assumes that the reader has a degree of prior knowledge.
However, this is not always the case.
A researcher new to this ﬁeld may mis-interpret a statement resulting in an inaccurate objective.
It is also possible to envisage information being deliberately left out of published protocols, particularly if omitted information reduces the impact of the ﬁndings. This information could suggest that the ﬁndings cannot be reliably reproduced or could reﬂect poorly on the success rate of a novel technique.
2.2 Existing approaches The requirement for an efﬁcient representation of experiment protocols is recognized as a pressing problem, and several other projects are applying ontologies to the formalization of knowledge about experiment data.
The Microarray Gene Expression Data (MGED)1 ontology is one of the pioneering attempts to use an ontology to record information about experiment data (Whetzel et al., 2006b). The MGED ontology was designed to formalize the descriptors required by the Minimum Information About a Microarray Experiment (MIAME)2 standard for capturing core information about microarray experiments (Brazma et al., 2001). Many journals ( 50 thus far3) require MIAME compliant data as a condition for publishing microarray-based papers.
This is a trend that looks set to continue for other accepted ontological standards.
The Minimum Information About Proteomics Experiment (MIAPE) standard supports proteomic experiments (Taylor et al., 2007). The Metabolomics Standards Initiative (MSI) ontology working group is building an ontology to facilitate the consistent annotation of metabolomic experiments (Sansone et al., 2007). for Biological Minimum Information and Biomedical Investigation (MIBBI) is a web-based resource designed to act as a one-stop-shop for those seeking for or looking to contribute to Minimum Information (MI) checklists.4 According to MIBBI s website these checklists are intended to promote transparency in experiment reporting, enhance accessibility to data and support effective-quality assessment, thereby increasing the value of a body of work.
The MIBBI project maintains a web-based resource for extant checklist projects, complementary data formats, tools, controlled vocabulary and databases.
MIBBI aims to provide guidelines for checklist development, both by increasing 1MGED: http://mged.sourceforge.net/ontologies 2MIAME: http://www.mged.org/Workgroups/MIAME/miame.html 3MIAME journals: http://www.mged.org/Workgroups/MIAME/journals.html 4MIBBI: http://mibbi.sourceforge.net i296 connectivity between MI checklist development projects, and by disseminating best practise both in relation to process (such as open mechanisms to receive and respond to public comment) and presentation (e.g. use of shared language, documentation style and structure, production of user-friendly summaries). Checklists developed using MIBBI guidelines focus largely on capturing the minimum information needed to usefully annotate data generated by biological/biomedical investigations.
Information pertaining to wet-lab processes such as experiment execution(s) is described using natural language with the aid of some controlled vocabulary.
However, the degree of information granularity in these descriptions is thus far at the authors discretion.
These descriptions may be sufﬁcient to make effective use of reported data but are likely to be insufﬁcient to independently repeat the experiment actions used to generate the date.
PRoteomics IDEntiﬁcation (PRIDE)5 is a data repository, supported by a combination of tools, standards and infrastructure for the description of proteomic data (Martens et al., 2005). PRIDE s schema presents a minimum of information about protein identiﬁcations. PRIDE s top level structure contains the part protocol description annotated with key words used to mark the type of method used to generate the proteomic data.
MGED, MIAME, MIAPE and PRIDE are ontologies primarily focused on developing controlled vocabulary and descriptors for high-throughput strategies such as mass spectroscopy and array- based comparative binding assays.
These ontologies are centred around the annotation of data.
Protocol information is only present at a level of detail, which is sufﬁcient to describe the data.
None of these projects provides a detailed enough formalism for the representation of experiment actions.
Investigations6 The Functional Genomics Investigation Ontology (FuGO) project (Whetzel et al., 2006a), and its successor the Ontology for Biomedical (OBI) project, are developing an integrated ontology for the description of biological and medical experiments and investigations.
This ontology aims to model the design of an investigation, including the protocols, instrumentation, materials used and the data generated.
OBI has not yet been released, but it already has the key classes for the description of protocols: OBI: investigator , OBI: instrument , OBI: biomaterial entity.
The generic ontology of scientiﬁc experiments (EXPO) aims to formalize domain-independent knowledge about the organization, execution and analysis of scientiﬁc experiments (Soldatova and King, 2006). This ontology has the class EXPO: experiment action and deﬁnes some of its properties: has Goal, has Object, has Instrument, but there are no subclasses speciﬁed. Our proposal differs from the existing ontology-based approaches for the description of experiment protocols by suggesting a meta-language for the description of experiment actions and their properties.
EXACT provides a formalized representation of the domain that is not sufﬁciently covered by any other ontology.
In theoretical computer science, process algebras have been used to specify and reason about descriptions of processes and actions.
Process algebras are algebraic systems for the manipulation of elements of processes (the individual elements being actions or events). They deﬁne laws governing the sequencing, composition and synchronization of actions.
Leading examples of process 5PRIDE: http://www.ebi.ac.uk/pride 6OBI: http://obi.sourceforge.net [19:52 18/6/03 Bioinformatics-btn156.tex] Page: i296 i295 i303  algebras are the Communicating Sequential Processes (CSP), the Calculus of Communicating Systems (CCS) and the Algebra of Communicating Processes (ACP) (Bergstra and Klop, 1984 Hoare, 1985 Milner, 1980). For example, a process algebra would provide laws stating that: (ﬁlter or centrifuge) then wash is equivalent to the choice of (ﬁlter then wash) or (centrifuge then wash) Process algebras in biology have generally been used as modelling languages for biological systems rather than as a way to specify experiment actions.
The ontology we propose provides much more detail than process algebras are usually designed to give, but could be used together with a suitable process algebra for veriﬁcation and other algebraic reasoning over protocols.
The process algebra for biological protocols would need to represent parameterized actions (e.g. to incubate at 30 C). It would also require the ability to represent state changes (e.g. CSP(cid:4)B, Treharne and Schneider (2002), which combines the CSP representation of processes with the B formal language to represent changes in state). Logics for agency have also considered some of the issues that we deal with in this work.
In particular agents are described by their actions and goals (desires/intentions). There are many logics for agency, each allowing different expressiveness and covering areas such as belief, knowledge, possibility, time, branching, the relationships between actions and goals and the distinction between understanding what must be done and why it must be done.
In EXACT we ﬁrst provide a vocabulary and ontology, and then begin to look at the grammatical aspects of describing experiment actions.
2.3 Our proposed solution To develop EXACT, we ﬁrst analysed protocols from several bio- medical domains, including functional genomics, metabolomics and drug screening, as well as protocols published in Nature Protocols7. We then consulted with biologists, microbiologists, biochemists and chemists with experience in the execution of these protocols to clarify ambiguous statements and to enrich the protocols with as much information as possible.
This helped to capture the precise meaning of each experiment action performed.
General concepts were abstracted from these experiments actions and were used to develop the ontological classes.
The scientiﬁc experts then used these classes to try and represent their own protocols.
After many painstaking rounds of consultation, classes were added and removed or changed in the ontology to help better represent the actions performed in various protocols.
The EXACT hierarchy of experiments is sufﬁcient to formalize many of the protocols used in our labs.
However, as we formalize more and more protocols using EXACT, its class structure will grow and evolve to meet the needs of new methods and techniques.
3 AN ONTOLOGY OF EXACT An ontology of EXACT aims to provide a structured vocabulary of concepts for the description of protocols in bio-medical domains.
7Nature Protocols: http://www.nature.com/nprot EXACT Our ontology intends to be compatible with other formalisms, to share and reuse already formalized knowledge.
For example it reuses classes from the phenotypic qualities ontology PATO,8 OBI and the W3C Time Ontology (OWL-Time).9 EXACT is expressed in OWL DL and was developed using the Protégé ontology editor.10 The main part of EXACT is a hierarchy of experiment actions.
This hierarchy was created using a classiﬁcation based on goals of actions.
The experiment actions are divided into three groups according to their goals: (cid:127) separation (cid:127) transformation (cid:127) combination.
In deﬁning these groups of actions we follow the classes of elementary processes used by Noy (1997). Our approach differs by separating what is done from how it is done.
The same goal can be achieved by many different actions.
For example, the goal separation may be achieved in various ways: by the experiment action centrifuge , by the experiment action ﬁlter or by other actions.
Experiment actions that provide a mode of transformation are classiﬁed into the following subclasses: (cid:127) a mode of property transformation, with such experiment actions as incubate , heat , thaw  (cid:127) a mode of transformation of spatial location, for example  move  (cid:127) a mode of transformation of time, for example wait  (cid:127) a mode of category transformation, with such experiment actions as break , pierce , divide.
Figure 1 shows our classiﬁcation of experiment actions according to their goals.
Many experiment procedures have experiment actions that are executed only if a certain condition is valid.
For example in our experiments with yeast, the optical density (OD) of yeast cells should be between 0.6 and 1.0 before processing.
If the OD is too low, the culture must be incubated for longer.
In order to represent conditions, EXACT deﬁnes the class condition with the sub- classes if-condition , pre-condition , post-condition and store-condition.
Each condition has a boolean expression , a yes-command for execution if the value of the expression is true and a no-command for execution if the value of the expression is false.
Pre-conditions and post-conditions can be used to check whether materials and instruments are ready for execution of experiment actions, whether ﬁnal volumes of solutions are correct, or whether objects are in the correct locations.
Such checks during running of experiments are important to prevent errors.
Store conditions are used to indicate when it is possible to temporarily stop execution of the protocol and put materials in a store under the deﬁned storage requirements.
EXACT also deﬁnes a set of command actions, which control the ﬂow of execution of the protocol: continue , stop , check , store and go.
Continue is the null action that does nothing http://obo.cvs.sourceforge.net/obo/obo/ontology/phenotype 8PATO: quality.obo 9OWL-Time: http://www.w3.org/TR/owl-time 10Protégé: http://protege.stanford.edu i297 [19:52 18/6/03 Bioinformatics-btn156.tex] Page: i297 i295 i303  L.N.Soldatova et al. Fig.1.
EXACT classiﬁcation of experiment actions.
at all (but is useful as the yes-command for pre- and post-conditions, and the no-command for store-conditions). Stop terminates the ﬂow of execution immediately and is used as the no-command for pre- and post-conditions.
Check is used in conjunction with all four conditions described above to test the expression and execute the yes-command or no-command as appropriate.
Store is the action of storage and is used as the yes-command of the Store- condition.
Go is a command that moves the ﬂow of execution to an action elsewhere in the protocol.
The command actions currently deﬁned by EXACT are minimal and are likely to be enhanced in the future by more complex constructs such as loops.
The class role is used in the ontology to describe that some entities can play a certain role.
A location can be a start or end location, a piece of equipment can be a start or end container a location can be a lid location, etc.
Apart from experiment actions that are performed by manipulating dependent variables of an experiment, EXACT deﬁnes the class equipment setup action with instances that have the goal of preparing for experiment actions.
These actions are considered as preliminary to later actions.
EXACT also deﬁnes the class data action with instances for recording measurements and observations that have the goal of preservation of information.
EXACT includes a hierarchy of instructions with the classes warning e.g. ﬂammable and caution e.g. critical step (taken from Nature Protocols) that can be ignored by automated agents executing protocols, but warn human users to take extra care.
EXACT is available in two versions: EXACT/EXPO is compliant with EXPO (Soldatova and King, 2006) and more suitable for i298 automated laboratories EXACT/OBI is more suitable for using within OBO communities.
EXACT/OBI provides an explicit mapping to OBI (the current draft March, 2008). The principal difference between these two versions is in philosophical foundations.
OBO ontologies are based on a philosophy of reality and do not include abstract entities.
This does not put considerable restrictions on the description of the existing protocols as most protocols are designed for execution of experiments in the real physical world by manipulating real physical objects.
The results of our research show (King et al., 2004 Soldatova et al., 2006 Whelan and King, 2008) that the representation of logical and mathematical objects (i.e. sets, relations, facts) and other entities within a computer system as abstract entities provides a clearer description of computational experiments and experiments executed in automated laboratories.
argued about Philosophers have fundamental ontological questions for at least two and a half thousand years, and we do not wish to enter these debates.
What we need to do is to make practical decisions about how best to describe protocols.
We believe that supplying different versions of EXACT is the best way to deal with conﬂicting upper ontologies. Hopefully the two versions of EXACT can be merged when a philosophical solution is found that is suitable for all needs.
EXACT/EXPO has only two abstract entities true value and false value , which are deﬁned as the value of a statement that corresponds/does not correspond to reality.
These classes are used in pre- and post-conditions of actions.
EXACT/EXPO is designed to be compliant with an ontology for automated laboratories, which we [19:52 18/6/03 Bioinformatics-btn156.tex] Page: i298 i295 i303  Table 2.
EXACT/EXPO  EXACT/OBI mapping of the top classes EXACT/EXPO  process  object  proposition  quality  role  abstract entity  EXACT/OBI  BFO: occurent  BFO: continuant  OBI: information entity  BFO: quality  BFO: role  OBI: information entity  are developing at Aberystwyth, UK. In EXACT/OBI these classes are deﬁned as subclasses of the class information entity , which was recently introduced into OBI (January, 2008). The class OBI: information entity is a subclass of the class BFO: generically dependent continuant. Table 2 shows an explicit mapping between the top classes of the two EXACT versions.
EXACT/OBI deﬁnes a mapping of the EXACT/EXPO top classes to the leaf classes of Basic Formal Ontology (BFO)11 (Grenon and Smith, 2004) without considerable loss of semantics and can be reused within OBO ontologies. EXACT aims to follow OBO Foundry principles:12 the ontology is open and available to use by all , is in a common formal language , includes textual deﬁnitions of all terms , uses relations which are unambiguously deﬁned , to OBO ontologies and it follows the naming convention of (Schober et al., 2007). is orthogonal it The current version of EXACT does not yet include axioms.
We are collecting statements about experiment actions and plan to include them in the form of axioms in the next version.
Here are some examples of such statements: For experiment action move : start location(cid:5)=end location For experiment action mix : end location of component 1=end location of component 2 The second statement tells us that in order to mix components, the components must be moved to the same location.
Apart from the well-deﬁned foundational relations is_a and part_of, EXACT includes the relations from the OBO Relational Ontology (RO) (Smith et al., 2005) located_in, has_participant and has_agent, the relations has_role and has_quality that are used in OBI, DOLCE (Gangemi et al., 2003) and HOZO (Kozaki et al., 2002), and a relation has_proposition (or has_information for the EXACT/OBI version). The specialization of BFO in representing real world entities is reﬂected in the set of RO relations.
RO relations are not suitable for linking physical entities to information entities.
The set of RO does not allow to easily represent such knowledge as an experiment action has a goal , an action is conditional , an investigator has a plan.
EXACT includes the relation has_proposition to ﬁll this gap.
The relation allows the connection of an agent of a process with a certain portion of information that is essential for participating in the process.
We deﬁne this relation following the methodology 11BFO: http://www.ifomis.org/bfo 12OBO Foundry: http://ontoworld.org/wiki/OBO_foundry EXACT suggested in Smith et al. (2005). First, we add one more relation to the pain of inﬁnite regress of primitive instance-level relations: a has_proposition i for p at t There is a primitive relation between an agent of a process, a proposition and a time, where a is an agent, p is a process, i is a proposition and t is time.
Second, we deﬁne a class-level relation using this primitive instance-level relation: A has_proposition I := a,a is_instance A   i,t,p such that (i is_instance I) (a has_proposition i) for p at t where A and I are classes of agents and propositions.
We can express an experiment action has a goal with this relation as follows: an agent of an experiment action has_proposition goal.
EXACT is a modular ontology.
It deﬁnes a conceptual scheme for describing experiment actions and their properties.
To represent individual experiment actions it is necessary to import individuals of the classes object , equipment , location and method.
These classes are part of EXACT.
Individuals of these classes are stored in the corresponding knowledge bases.
An example of a protocol with a sequence of particular experiment actions in OWL-DL can be found on the EXACT website: http://www.aber.ac.uk/compsci/Research/bio/dss/EXACT/. in Table 3 from the competent-cells protocol 4 EXAMPLES OF FORMALIZED PROTOCOLS 4.1 Example 1: competent-cells protocol is The excerpt structured as a series of experiment actions explicitly stating what the user must do step by step.
All objects used in the experiment actions for example YPD media bottle, yeast culture ﬂask are deﬁned as instances of the class object.
All locations for example laminar ﬂow hood, cold room are deﬁned as instances of the class location (more precisely as objects playing a role of location ). Each particular instance of an experiment action has to specify values of all parameters.
The action move 12 is an instance of the class move , which is deﬁned in EXACT as an experiment action to change a spatial location of an entity from a start location to an end location.
In order to specify an instance move 12, it is required to specify a start location (= store), end location (= laminar ﬂow hood) and an object of the action the entity that is changing location (= YPD media bottle). In the EXACT formalism laboratory protocols are divided into many operating procedures.
Prerequisite objects for each operating procedure are represented in pre-condition and objects created as a result of executing an operating procedure are represented in post-condition.
In the above operating procedure grow yeast culture, pre-conditions include sealed yeast colonies plate located_in cold room and YPD media bottle located_in cold room, where sealed yeast colonies plate, YPD media bottle are instances of the class object , cold room is an instance of the class location , and located_in is a deﬁned relation.
Therefore in order to execute the operating procedure grow yeast culture the user must have i299 [19:52 18/6/03 Bioinformatics-btn156.tex] Page: i299 i295 i303  L.N.Soldatova et al. Table 3.
EXACT competent-cells protocol (a fragment) Operating procedure: grow yeast culture pre-condition: sealed yeast colonies plate located_in cold room pre-condition: YPD media bottle located_in cold room experiment action: object: start location: end location: experiment action: object: start location: end location: experiment action: object: start location: end location : experiment action: component 1: volume: start container: end container: equipment: experiment action: old name: new name: experiment action: component 1: volume: start container: end container: equipment: experiment action: old name: new name: experiment action: object: start location: end location: experiment action: object: equipment: rpm: temp: time interval: goal: Post condition: move 12 YPD media bottle in store in laminar ﬂow hood move 13 500ml conical ﬂask in store in laminar ﬂow hood move 14 sealed yeast colonies plate in cold room in laminar ﬂow hood add 15 YPD medium 50ml YPD media bottle 500ml conical ﬂask pipette rename 16 500ml conical ﬂask YPD conical ﬂask add 17 single yeast colony small volume sealed yeast single colonies plate YPD conical ﬂask inoculating loop rename 18 YPD conical ﬂask yeast culture ﬂask move 19 yeast culture ﬂask in laminar ﬂow hood in incubator incubate 20 yeast culture ﬂask shaking incubator 200  30 12 24h grow yeast until medium becomes cloudy yeast culture located_in incubator C ﬁrst executed one or more operating procedures where the post- conditions include sealed yeast colonies plate located_in cold room and YPD media bottle located_in cold room.
This provides the protocol user with the knowledge of exactly what he/she needs to have in place before commencing.
The move action ensures that each object is in the correct location.
For example, when adding YPD to a conical ﬂask, ﬁrst both objects are moved to the laminar ﬂow hood.
Similarly, a yeast culture ﬂask cannot be incubated if it is not ﬁrst moved to an incubator.
The action rename was used to represent a change in an object s state.
A 500ml conical ﬂask changes to YPD conical ﬂask i300 when YPD is added to the ﬂask. The rename action was put in place to make the protocol easier to follow when being executed by a human.
It has no signiﬁcance when the protocol is being executed by laboratory robotics.
Figure 2 illustrates the difference between the original text book representation of a portion of this protocol, the detailed EXACT representation, and a basic text representation generated automatically from the EXACT representation.
4.2 Example 2: Formalized protocols for commissioning of equipment EXACT has also been used to formalize protocols to assist with the commission of laboratory-automation equipment.
The Computational Biology group at Aberystwyth (UK) is in the process of purchasing robotic equipment for the automated screening and design of drugs.
As part of this procedure, protocols were created describing the work that the robotic equipment would need to perform.
If a robotic system is to automate a protocol it will need every temperature, every movement and every decision fully speciﬁed. Explicitly describing protocols that were always intended to be automated forced us to be precise and this helped the development of EXACT.
We applied EXACT to deﬁne which experiment actions, with which properties, were necessary to achieve the planned goals.
This enabled us to specify what type of equipment, and what functionality, was required to execute the planned investigations.
The level of detail that can be expressed in EXACT corresponds to the level usually represented by the control software for managing integrated laboratory-automation systems.
This is the level at which the protocols become concrete, well-deﬁned and implementable.
We sent these protocols to companies that sell laboratory automation equipment (such as Tecan, Beckman, Hamilton, FluidX, Matrix and many others), as speciﬁcations of what we wanted to achieve.
Several of these companies then obliged us with demonstrations of how their equipment could meet the protocols.
As an example, one of our compound library replication protocols is available on the EXACT website.
The strict speciﬁcation of protocol elements helped us to recognize inconsistencies and potential problems with equipment.
For example: a lack of space for a lid location.
Equipment demos are often done using plates without lids, causing de-lidding operations to be skipped.
The action of de-lidding must involve a lid location property.
The lid location must be available, reachable by the robot and must not obstruct other operations.
As another example, experiment actions such as discard may not seem important for demos, but inefﬁcient execution can cause serious problems in future investigations.
The strict description of all experiment actions forces one to pay attention to all operations.
Currently there is no standard language for programming the protocols for automated-laboratory systems, no single language that all laboratory equipment understands.
Each device has a proprietary driver, and these are generally linked into an overarching software system by a laboratory integration specialist, who will provide a domain speciﬁc language for end users to represent the protocol they need to run on the system.
Each of these languages provides its own functionality and vocabulary.
EXACT provides a vocabulary at a particular level of detail useful for speciﬁcation: for example we have an experiment action: incubate which has properties [19:52 18/6/03 Bioinformatics-btn156.tex] Page: i300 i295 i303  EXACT Fig.2.
(1) Protocol from Methods in Yeast Genetics.
(2) Protocol represented using EXACT.
(3) Text generated automatically from EXACT representation.
describing the temperature, shaking speed and duration, but does not specify the lower level of which serial commands to use, which location in the incubator should be used, or what to do if the incubator should raise an error.
5 APPLICATION 5.1 Validation EXACT is extremely valuable as a language for the initial speciﬁcation of an automated system, because it forces the removal of ambiguity and can be used for validation.
As an example, we have implemented EXACT in the programming language Haskell. This allows an EXACT speciﬁcation to be executed and tested.
An example of a part of the competent-cells protocol is implemented in Figure 3.
The implementation in Haskell allows actions to be combined with other actions to create an operating procedure which is itself a (complex) action to be combined with others.
Each action may modify the state of the equipment and write a description to a log.
This description can be used as a simple text representation of the formalized protocol.
The state updates and existence of equipment and locations can be validated during the execution of the protocol, and the protocol must typecheck in order to be a valid Haskell program (all necessary properties of actions must be deﬁned). The other beneﬁt of a Haskell implementation of EXACT is as a tool to test the validity of the ontology itself.
The semantics of conditions and command actions can be examined and reﬁned. The type system enforces and makes clear the distinction between materials, equipment and locations, but also demonstrates that some locations are created from equipment, that equipment can contain materials, and that when materials are combined in a container, the container may hold a new material that has been created from the combination.
Other approaches also exist that can assist in the validation of protocols.
The use of agency logics may be able to provide proof by axioms or by model checking that the protocols give the correct results and are achievable.
However we would need efﬁcient and practical implementations of such logic-based reasoning.
The relation between logic theory and practical approaches is still unclear (de Boer et al., 2007). Some logics do not allow the expression of how an action is achieved, only what is achieved.
Several examples of families of logics that may be suitable to enhance EXACT in the future include Belief-Desire-Intention (BDI), Knowledge, Actions, Results and Opportunities (KARO) and Sees To It That (STIT) (Troquard et al., 2006 van der Hoek and Wooldrige, 2003). However, logics for agency have a different emphasis than the work of EXACT, namely that they describe the underlying causes of agent behaviour rather than provide a language for precise description of actions.
5.2 Tools Good tools are vital to the adoption of standards.
If we expect biomedical scientists to unambiguously deﬁne their protocols we must give them tools that are easy to use.
Fully formalized protocols will span many pages of text.
Generating such descriptions by hand is labour-intensive, error-prone and uninspiring.
We need tools for generating, validating, viewing and reasoning with protocols.
Protocol-generation tools should: (cid:127) provide an intuitive graphical user interface (cid:127) automatically enforce the vocabulary of EXACT  i301 [19:52 18/6/03 Bioinformatics-btn156.tex] Page: i301 i295 i303  L.N.Soldatova et al. growYeastCulture :: Action growYeastCulture = do check check (preCondition (isIn Store YPD media bottle )) (preCondition (isIn ColdRoom sealed yeast colonies plate )) move move move add (Object (Start (End (Object (Start (End (Object (Start (End  YPD media bottle ) Store) (In LaminarFlowHood))  500ml conical flask ) Store) (In LaminarFlowHood))  sealed yeastc olonies plate ) ColdRoom) (In LaminarFlowHood)) YPDMedium) 50ml ) (Component (Volume (Container1 YPD media bottle ) (Container2 500ml conical flask ) (Equipment  pipette ) rename (OldName (NewName  500ml conical flask ) YPD conical flask ) add SingleYeastColony) small volume ) (Component (Volume (Container1 sealed yeast colonies plate ) (Container2 YPD conical flask ) (Equipment  inoculating loop ) rename (OldName (NewName  YPD conical flask ) Yeast culture flask ) move (Object (Start (End  Yeast culture flask ) (In LaminarFlowHood)) (In ShakingIncubator)) incubate (Object  Yeast culture flask ) shaking incubator ) 200) (Equipment (RPM (Temperature 30) (TimeInterval 12hours  24hours ) (Goal  to grow yeast so that the medium becomes cloudy ) Fig.3.
An example of part of the competent-cells protocol, implemented in Haskell, using EXACT.
(cid:127) supply default values and allow reuse of existing protocols.
Protocols that are formally deﬁned should be validated before being accepted for publication.
Tools for validation should ensure that: (cid:127) all equipment and objects have deﬁned initial locations and properties  (cid:127) names for equipment and objects are consistently used (cid:127) locations of objects and equipment are consistent (a ﬂask cannot be moved from the bench to the cold store and then from the incubator to the laminar ﬂow hood)  (cid:127) properties of equipment are valid (if you have only one incubator then it cannot be used at two different temperatures at the same time)  (cid:127) biological materials exist and are available (a plate cannot be used as a source of yeast culture if yeast has not been added to it previously)  (cid:127) stated pre-conditions/post-conditions for each subpart of the protocol can be met by the protocol as a whole.
Text generation tools are also needed.
Usually a biologist will not require a full description of a protocol, and will prefer a much higher level summary, but may require clariﬁcation of certain steps.
For this we would like a tool that can translate from a fully speciﬁed EXACT protocol into a summarized human-friendly i302 readable format, with the option of expansion of any instruction for more detailed information.
Given a formalized protocol, useful tools would generate equipment lists and their necessary range of settings, calculate timings and storage points that are friendly to a biologist s working- hours, and compare two or more published protocols and state how and where they differ.
6 DISCUSSION AND CONCLUSION Laboratory-based scientiﬁc experiments must, by deﬁnition, be repeatable.
However, many, perhaps most, scientiﬁc protocols in the literature are so poorly described and deﬁcient in information that their exact repetition is impossible.
Indeed, many experiment protocols bear more resemblance to recipes in cook books than to detailed scientiﬁc methodologies.
And even in bioinformatics, where experiments may be wholly computational, it is often very hard to obtain enough information to fully repeat an experiment.
This unhappy situation is being made worse by the unfortunate trend in scientiﬁc journals to downplay the Methods section, moving it from its traditional place after the introduction to the end, reduce its font, move it into Further information , etc.
This careless/vague description of experiment protocols was perhaps viable when molecular biology focused on qualitative experiments: the correct result being indicated by a band on a gel in the correct place, a colony growing, etc.
Such experiments were routinely executed in batches of 10 or 20.
However, with the ever increasing importance of quantitative methods such as microarrays (where numerical values have to be interpreted as biological observations and tens of thousands of experiments are executed simultaneously) the precise and unambiguous description of experiment protocols is essential.
We propose the EXACT ontology as the basis for the description of protocols.
We followed the current best practice in ontology development by not allowing multiple inheritance, providing deﬁnitions for all classes and relations, using top-level classes (Rosse and Mejino Jr., 2003 Smith et al., 2005 Soldatova and King, 2005). We have demonstrated the utility of EXACT to represent drug screening and functional-genomic protocols.
to formalize many of The EXACT hierarchy of the experiment actions is currently sufﬁcient the protocols used in our Computational Biology Group.
However, more work is required before it is sufﬁciently comprehensive to be able to represent all protocols in laboratory biology.
We are currently working on the development of tools that will make the generation of these protocols easy for biologists.
We also have to deﬁne a consistent language for specifying the ﬂow of execution through the protocols and the relationship of our work to process algebras.
It is intrinsically valuable to describe one s own experiments in a precise and unambiguous way as it provides a clear record of what one has achieved.
However, the value of describing protocols clearly is greatly ampliﬁed by being able to exchange and compare protocols.
Ontologies provide a basis for such a shared understanding.
We therefore envisage developing an EXACT repository as a place where investigators and practitioners can accumulate their knowledge about representing protocol actions.
We invite researchers from all areas to participate in the development of an ontology of experiment actions and to contribute to an Open Source project for the formalized representation of protocols.
[19:52 18/6/03 Bioinformatics-btn156.tex] Page: i302 i295 i303  EXACT ACKNOWLEDGEMENTS We would like to acknowledge RC UK, RAEng/EPSRC, and BBSRC for providing funding to accomplish this work.
Conﬂict of Interest: none declared.
REFERENCES Akada,R. et al. (2006) PCR-mediated seamless gene deletion and marker recycling in Noy,N. (1997) Knowledge Representation for Intelligent Information Retrieval in Experimental Sciences.
Ph.D. thesis, College of Computer Science, Northeastern University, USA. Rosse,C. and Mejino,J.L.V.,Jr (2003) A reference ontology for bioinformatics: the Foundational Model of Anatomy.
J. Biomed. Inform., 36, 478 500.
Sansone,S. et al. (2007) Metabolomics standards initiative  ontology working group.
work in progress.
Metabolomics, 3, 249 256.
Schober,D. et al. (2007) Towards naming conventions for use in controlled vocabulary In Proceedings of BioOntologies SIG, ISMB07, and ontology engineering.
pp.
29 32.
Smith,B. et al. (2005) Relations in biomedical ontologies. Genome Biology, 6:R46, Saccharomyces cerevisiae. Yeast, 15 23, 399 405.
1 15.
Amberg,D.C. et al. (2005) Methods in Yeast Genetics.
Cold Spring Harbor Laboratory Soldatova,L.N. and King,R.D. (2005) Are the current ontologies used in biology good Press.
ontologies Nat. Biotechnol., 9/23, 1096 1098.
Bergstra,J.A. and Klop,J.W. (1984) Process algebra for synchronous communication.
Soldatova,L.N. and King,R.D. (2006) An ontology of scientiﬁc experiments.
J. R. Soc.
Inform.
Control, 60, 109 137.
Interface, 3/11, 795 803.
Brazma,A. et al. (2001) Minimum information about a microarray experiment MIAME- Soldatova,L. et al. (2006) An ontology for a Robot Scientist.
Bioinformatics (Special toward standards for microarray data.
Nat. Genet., 4, 365 371. issue for ISMB), 22/14, e464 e471. de Boer,F.S. et al. (2007) A veriﬁcation framework for agent programming with Taylor,C.F. et al. (2007) The minimum information about a proteomics experiment declarative goals.
J. Appl. Logic, 5, 277 302.
(MIAPE). Nat. Biotechnol., 25, 887 893.
Gangemi,A. et al. (2003) Sweetening ontologies with DOLCE. AI Magazine, 24, Treharne,H.E. and Schneider,S. (2002) Communicating B machines.
In ZB2002: 13 24.
Grenon,P. and Smith,B. (2004) SNAP and SPAN: towards dynamic spatial ontology.
Spat.
Cogn. Comput., 4, 69 103.
Hoare,C.A.R. (1985) Communicating Sequential Processes.
Prentice Hall.
King,R.D. et al. (2004) Functional genomics hypothesis generation by a Robot Scientist.
Nature, 427, 247 252.
Kozaki,K. et al. (2002) Hozo: an environment for building/using ontologies based on a fundamental consideration of role and relationship.
In Knowledge Engineering and Knowledge Management, pp.
213 218.
Martens,L. et al. (2005) Pride: the proteomics identiﬁcations database.
Proteomics, 5, 3537 3545. International Conference of Z and B Users.
Troquard,N. et al. (2006) Towards an ontology of agency and action : from STIT to OntoSTIT+. In International Conference on Formal Ontology in Information Systems (FOIS), Baltimore, Maryland, USA, pp.
179 190. van der Hoek,W. and Wooldrige,M. (2003) Towards a logic of rational agency.
Logic Journal of the IGPL, 11, 133 157.
Whelan,K.E. and King,R.D. (2008) Using a logical model to predict the growth of yeast.
BMC Bioinformatics, 9:97. Whetzel,P.L. et al. (2006a) Development of FuGO: an ontology for functional genomics investigations.
OMICS, 10, 199 204.
Whetzel,P.L. et al. (2006b) The MGED ontology: a resource for semantics-based Milner,R. (1980) A Calculus of Communicating Systems.
Springer Verlag. description of microarray experiments.
Bioinformatics, 7, 866 873.
[19:52 18/6/03 Bioinformatics-btn156.tex] Page: i303 i295 i303 i303 BIOINFORMATICS APPLICATIONS NOTE Vol.27 no.8 2011, pages 1187 1189 doi:10.1093/bioinformatics/btr073 Advance Access publication February 23, 2011 Databases and ontologies Gene List signiﬁcance at-a-glance with GeneValorization Bryan Brancotte1, Anne Biton2,3,4,5, Isabelle Bernard-Pierrot2,3, François Radvanyi2,3, Fabien Reyal2,3,6 and Sarah Cohen-Boulakia1, 1Laboratoire de Recherche en Informatique, CNRS UMR 8623, Université Paris-Sud, F-91405 Orsay Cedex, 2CNRS, UMR 144, Institut Curie, 26 rue d Ulm, F-75248 Paris Cedex 05, 3Institut Curie, Centre de Recherche, Paris, F-75248, 4INSERM, U900, Paris, F-75248, 5Mines ParisTech, Fontainebleau, F-77300 and 6Institut Curie, Departement de Chirurgie, 6 rue d Ulm, F-75005 Paris, France Associate Editor: Dmitrij Frishman ABSTRACT Motivation: High-throughput technologies provide fundamental informations concerning thousands of genes.
Many of the current research laboratories daily use one or more of these technologies and end-up with lists of genes.
Assessing the originality of the results obtained includes being aware of the number of publications available concerning individual or multiple genes and accessing information about these publications.
Faced with the exponential growth of publications avaliable and number of genes involved in a study, this task is becoming particularly difﬁcult to achieve.
Results: We introduce GeneValorization, a web-based tool that gives a clear and handful overview of the bibliography available corresponding to the user input formed by (i) a gene list (expressed by gene names or ids from EntrezGene) and (ii) a context of study (expressed by keywords). From this input, GeneValorization provides a matrix containing the number of publications with co-occurrences of gene names and keywords.
Graphics are automatically generated to assess the relative importance of genes within various contexts.
Links to publications and other databases offering information on genes and keywords are also available.
To illustrate how helpful GeneValorization is, we will consider the gene list of the OncotypeDX prognostic marker test.
Availability: http://bioguide-project.net/gv Contact: cohen@lri.fr Supplementary information: Supplementary data are available at Bioinformatics online.
Received on April 15, 2010 revised on October 12, 2010 accepted on February 4, 2011 1 INTRODUCTION High throughput technologies (e.g. comparative pan-genomic hybridization, gene expression, protein and methylation arrays, high-throughput DNA sequencing) are major, promising and very exciting tools to study biology.
Each of them provide fundamental informations concerning thousands of genes such as their normal functions or speciﬁc alterations (e.g. DNA copy number alteration, loss of heterozygosity, change in expression, promoter methylation, mutation or post-translational modiﬁcation). Many of the current biological research laboratories daily use one or more of these technologies.
Selecting genes of interest to design further experiments is of paramount importance.
In this   To whom correspondence should be addressed.
process, scientists need to access the latest publications concerning individual or multiple genes.
Among the genes they may consider, researchers need to distinguish three categories of genes: (i) genes already clearly shown to be associated with the process they are studying (ii) new and promising genes for a particular research ﬁeld whose interest is well known in other research ﬁelds (iii) genes that have not been studied.
This task is becoming particularly difﬁcult to achieve faced with the many gene lists which are retrieved and the exponential growth of publications available.
In this article, we introduce a web-based tool named GeneValorization. Given a list of gene names and a set of keywords describing the context of the study, GeneValorization provides a matrix with the number of publications cociting each gene name and keyword.
GeneValorization thus gives very quickly a clear and handful overview of the bibliography corresponding to a gene list of interest within a given context of study.
To illustrate how helpful GeneValorization is, we consider here the gene list of the OncotypeDX prognostic marker test (Paik et al., 2004), which is composed of 16 genes and used to determine the individual relapse risk of a breast cancer patient.
2 MAIN FUNCTIONALITIES The main interface of GeneValorization is provided on Figure 1.
Basic queries: GeneValorization takes as input from the user a list of gene names and a list of keywords that we call ﬁlters. Filters are used to describe the context of the study: while the main ﬁlter represents the main context (e.g. Breast cancer), secondary ﬁlters are alternative ways of reﬁning this context (e.g. Proliferation, Migration). Secondary ﬁlters can be as numerous as necessary.
Given this input, GeneValorization provides a matrix of data where each line is dedicated to a gene and each column is dedicated to a ﬁlter. The ﬁrst column considers the main ﬁlter only while the next columns consider both the main ﬁlter and secondary ﬁlters. More precisely, cells (x,y) of the ﬁrst column of the matrix contain the numbers of publications cociting the main ﬁlter (on column y) and the gene name (on line x). Cells from the second column and the next ones contain the number of publications cociting the main ﬁlter and the secondary ﬁlter (on column y) and the gene name (on line x). Columns where a secondary ﬁlter is provided thus allow to subdivide the set of papers considered.
In Figure 1, GeneValorization reported 5138 papers involving PGR in Breast Cancer (main ﬁlter). Among them, 658 papers are also related to Proliferation while 234 mention Apoptosis.
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:22 26/3/2011 Bioinformatics-btr073.tex] Page: 1187 1187 1189  B.Brancotte et al. Fig.1.
Main interface of GeneValorization. Specifying species and using aliases: GeneValorization is able to perform its search not only using the gene name g provided by the user but also using all aliases of g within a given species, by exploiting information from EntrezGene. The default species used is Human, which can be changed by the user.
It is also possible not to consider aliases.
All the menus available to make such choices are described in the manual.
Gene name disambiguation: GeneValorization provides assistance in the process of gene name disambiguation.
Ids from EntrezGene can be directly entered (preﬁxed by #) instead of gene names.
When users provide a gene name g, GeneValorization uses EntrezGene to check which of the three following cases occur: (i) g is an ofﬁcial gene name (ii) g is not an ofﬁcial gene name but appears in the list of synonyms of one or several ofﬁcial gene names (within the same species) and (iii) none of the above.
In case (i), GeneValorization runs normally, the gene name is green to indicate that it is ofﬁcial. In case (ii), the gene name appears in orange and by right clicking on the name, the user can access the list of ofﬁcial gene names having g in their aliases.
Users are provided with links to EntrezGene web pages describing each alternative so that they can then choose one of them to remove any ambiguity (the orange gene name can thus be renamed using one of the alternative ofﬁcial names). In case (iii), the gene name appears in red to indicate that it is not an existing gene name.
By default, GeneValorization will still perform all searches using the value entered by the user.
In case (ii), and if the user allows synonyms to be considered, GeneValorization will choose to consider the ﬁrst ofﬁcial gene name provided by EntrezGene. Visualizing results: clicking on a cell of the main grid allows users to measure the relative importance of genes and keywords.
As an example, clicking on the cell corresponding to PGR and Proliferation generates the graph of Figure 1 (right-hand side) and states that PGR is the third most important gene (according to the number of publications) for the Breast Cancer and Proliferation topics, out of 1188 the 16 genes considered here.
All the papers associated with the keywords are provided and can be accessed.
Interestingly, it is also possible to compare the role of several secondary ﬁlters (several graphs can be displayed at the same time). Accessing information from other sources: links to several databases are also provided.
As for genes, information from PubMed, EntrezGene, GeneCards and DrugBank can be obtained (e.g. Fig.1 shows the DrugBank web page for PGR). GeneValorization also makes calls to the NLM MeSH browser (http://www.nlm.nih.gov mesh/meshhome.html) to match the keywords provided by the user with MeSH terms and places them within the MeSH ontology.
Data import/export: GeneValorization allows users to load and save gene lists and ﬁlters using various formats (e.g. text ﬁles, csv, xml). During the export, EntrezGene ids used to search for the aliases are added to the saved matrix.
Advanced queries: Users can express advanced queries involving wilcards * and AND in gene names or ﬁlters. Considering gene names with or without aliases (from EntrezGene) is possible.
Users may also indicate which part of the PubMed ﬁle should be queried (e.g. abstract, title). It 3 TECHNICAL INFORMATION GeneValorization is a Java webstart application.
is thus multiplatform and can be used without any speciﬁc installation.
GeneValorization follows an on-the-ﬂy querying process: queries are directly sent to portals (Entrez or SRS), and no warehousing is needed.
Loading a cell information may take 1 4 seconds.
It took 10 minutes (but each result is displayed as soon as it is available) to load the entire information associated to the gene list, considering all the aliases available in EntrezGene, of the OncotypeDX prognostic marker test, with the 19 secondary ﬁlters. To deal with long list of genes or ﬁlters, a caching system has been implemented to optimize the response time.
It makes it possible to save previous loaded data [09:22 26/3/2011 Bioinformatics-btr073.tex] Page: 1188 1187 1189  which can be updated on demand later on.
Last, GeneValorization is currently able to run on the Entrez NCBI portal (http://www.ncbi.nlm.nih.gov/Entrez/) or the EBI SRS server (http://srs.ebi.ac.uk/) queries will be sent and interpreted by the respective portals.
Among the differences, MeSH terms are automatically considered during this search when Entrez is used while it is a pure cooccurrence-based search in SRS (more information is available in the Supplementary Material). 4 PROOF OF CONCEPT The OncotypeDX test quantiﬁes the probability of distant recurrence in patients with node negative, estrogen receptor positive breast carcinoma.
It is composed of 16 cancer-related genes, selected after a validation step on independent studies.
This test has been included in the guidelines of the American Society of Clinical Oncology and the National Comprehensive Cancer Network.
We have used GeneValorization to analyze the literature corresponding to these 16 genes.
Two queries have been uploaded with Cancer and Breast Cancer as main ﬁlter. The 19 secondary ﬁlters have been considered corresponding to examples of the most general items to depict the composition of a cancer gene list.
As a ﬁrst result, GeneValorization allowed us to know that each of the genes has been refered in association with the term Cancer and Breast Cancer in 5 to 17 517 and 3 to 10 048 publications.
Second, GeneValorization made it possible to distinguish three sets of genes in the OncotypeDX test.
Five genes (ERBB2, ESR1, PGR, BCL2, MKI67) were highly studied ( 1000 publications associated) while four genes (BAG1, CTSL2, MYBL2, SCUBE2) were not very actively studied (less than 50 publications) and the seven remaining genes (GSTM1, AURKA, BIRC5, CCNB1, CD68, MMP11, GRB7) had an intermediate level.
All the genes of the list are thus not equally known to be involved in breast cancer.
Genes associated to only a few or no publications are then of particular interest since they have been selected to be in the signature while not being studied extensively.
Our study suggests to conduct new experiments on some of these genes to better demonstrate how connected to breast cancer they may be.
Third, GeneValorization underlined that the secondary ﬁlters Proliferation , Apoptosis , Invasion , and Angiogenesis were strongly associated with the gene list while Immunity , Cell- cycle arrest , Epigenetic or microRNA were much less often associated.
This underlines the fact that the OncotypeDX signature is a publication-based molecular signature which contains genes involved in processes commonly known to be linked to breast cancer prognosis but does not contain genes known to be related to new trails in breast cancer studies (as the role of immune response in cancerogenesis). GeneValorization 5 DISCUSSION Mining PubMed abstracts and ranking publications have been of particular interest in the last years.
Approaches are mostly based on Text-Mining techniques [see for instance, Vellay et al. (2009) or Krallinger et al. (2008) and the references therein]. When available, softwares able to analyze genes such as PDQWizard (Grimes et al., 2006), GoGene (Plake et al., 2009) and CoPubMapper (Alako et al., 2005) differ from GeneValorization in several aspects.
From a technical perspective, (i) they make use of data warehouses to store publications, which poses the major problem of updating the local databases and makes it impossible to beneﬁt from all of new publications daily available and (ii) they may not consider simultaneaous requests which makes the response time too long.
From a functionality perspective, (i) they may not be ﬂexible, e.g. providing only predeﬁned lists of keywords and (ii) they may not consider gene aliases.
From a user perspective, they may not provide results in a concise and/or graphical manner and may not allow users to easily store and load their results at any time.
A more complete related work (eight tools compared with GeneValorization) is available in the Supplementary Material.
ACKNOWLEDGEMENT The authors would like to thank the reviewers for their helpful comments to improve the manuscript.
Funding: The CNRS (Brasero project) (to B.B.) the CNRS, the Institut Curie, the Ligue Nationale Contre le Cancer (associated laboratory), and the Drop-Top FP6 European project (LSHB-CT- 2006-037739) (to A.B., I.B.P., F.Ra., and F.Re.) the Institut National du Cancer (to A.B.). Conﬂict of Interest: none declared.
REFERENCES Alako,B.T. et al. (2005) CoPub Mapper: mining MEDLINE based on search term co-publication.
BMC Bioinformatics, 6, 51.
Grimes,G.R. et al. (2006) PDQ Wizard: automated prioritization and characterization literature.
Bioinformatics, 22, of gene and protein lists using biomedical 2055 2057.
Krallinger,M. et al. (2008) Linking genes to literature: text mining, information extraction, and retrieval applications for biology.
Genome Biol., 9 (Suppl. 2), S8. Paik,S. et al. (2004) A multigene assay to predict recurrence of tamoxifen-treated, node-negative breast cancer.
N. Engl. J.Med., 351, 2817 2826.
Plake,C. et al. (2009) GoGene: gene annotation in the fast lane.
Nucleic Acids Res., 37, W300 W304. Vellay,S.G. et al. (2009) Interactive text mining with Pipeline Pilot: a bibliographic web-based tool for PubMed. Infect.
Disorders Drug Targets, 3, 366 374.
[09:22 26/3/2011 Bioinformatics-btr073.tex] Page: 1189 1187 1189 1189
BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.18 2010, pages 2336 2337 doi:10.1093/bioinformatics/btq419 Advance Access publication July 15, 2010 Genome analysis LocusZoom: regional visualization of genome-wide association scan results Randall J. Pruim1, , Ryan P. Welch2,3, , Serena Sanna4, Tanya M. Teslovich2, Peter S. Chines5, Terry P. Gliedt2, Michael Boehnke2, Gonçalo R. Abecasis2 and Cristen J. Willer2, 1Department of Mathematics and Statistics, Calvin College, Grand Rapids, MI 49546, 2Department of Biostatistics and Center for Statistical Genetics, University of Michigan, 3Bioinformatics Graduate Program, The University of Michigan Medical School, Ann Arbor, MI 48109, USA, 4Istituto di Neurogenetica e Neurofarmacologia (INN), Consiglio Nazionale delle Ricerche, c/o Cittadella Universitaria di Monserrato, Monserrato, Cagliari, Italy 09042 and 5National Human Genome Research Institute, National Institutes of Health, Bethesda, MD, USA Associate Editor: Dmitrij Frishman ABSTRACT Summary: Genome-wide association studies (GWAS) have revealed hundreds of loci associated with common human genetic diseases and traits.
We have developed a web-based plotting tool that provides fast visual display of GWAS results in a publication-ready format.
LocusZoom visually displays regional information such as the strength and extent of the association signal relative to genomic position, local linkage disequilibrium (LD) and recombination patterns and the positions of genes in the region.
Availability: LocusZoom can be accessed from a web interface at http://csg.sph.umich.edu/locuszoom. Users may generate a single plot using a web form, or many plots using batch mode.
The software utilizes LD information from HapMap Phase II (CEU, YRI and JPT+CHB) or 1000 Genomes (CEU) and gene information from the UCSC browser, and will accept SNP identiﬁers in dbSNP or 1000 Genomes format.
Single plots are generated in 20 s. Source code and associated databases are available for download and local installation, and full documentation is available online.
Contact: cristen@umich.edu Received on March 2, 2010 revised on July 7, 2010 accepted on July 9, 2010 1 INTRODUCTION Genome-wide association studies (GWAS) have identiﬁed hundreds of loci associated with complex human diseases and traits (Manolio et al., 2009). GWAS test for association with dichotomous or quantitative traits at millions of SNPs across the genome and can identify variants many hundreds of kilobases away from any known gene.
The next challenge in human genetics will be to identify the causal variants and genes responsible for disease association at the many disease-associated loci identiﬁed from GWAS. An associated region may contain only a single strongly associated SNP, or more commonly, a set of SNPs with varying degrees of association due to local linkage disequilibrium (LD) patterns.
When examining results  To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the ﬁrst two authors should be regarded as joint First Authors.
from a GWAS, it is important to visually inspect regions showing association to determine the extent of the association signal and the position relative to nearby genes.
Genes several hundred kb from an associated SNP may be functionally relevant (Loos et al., 2008). We have developed a web-based tool that provides graphical display of locus-speciﬁc association results and gives an overview of the extent of LD and the position relative to nearby genes and local recombination hotspots. 2 IMPLEMENTATION 2.1 Features and functionality The main panel of a LocusZoom plot shows association P-values on the log10 scale on the vertical axis, and the chromosomal position along the horizontal axis (Fig. 1). The user can specify the region to display in one of three ways: (i) an index SNP and a window size, (ii) the chromosome together with start and stop positions or (iii) gene name and size of ﬂanking region.
We allow for the display of a rug above the main panel which gives a tick for any SNP in the results ﬁle, or for all SNPs from HapMap Phase II.
The plots were designed to display 1 Mb windows of the genome, although for regions with several association signals or long-range LD patterns, plots extending further can be drawn.
To identify SNPs that may be potentially causative, LocusZoom plots show not only the magnitude of association for each SNP, but also the pairwise LD pattern with the most strongly associated SNP or another user-speciﬁed SNP. Quick inspection can reveal the extent of the associated region and the location and number of SNPs in strong LD with the index SNP. In addition, a locus may show strongly associated variants that are weakly correlated, suggesting the presence of multiple independent association signals.
Users may choose to display LD (r2 or D ) estimates from HapMap Phase II (CEU, YRI or JPT + CHB) or from the 1000 Genomes Project.
LocusZoom is compatible with 1000 Genomes SNP naming format (chr:position) and will plot association results for novel SNPs identiﬁed by sequencing studies.
(cid:3) We provide an option for the data point symbol to reﬂect genomic annotation (nonsense, non-synonymous, coding, UTR, splice variants, transcription factor binding sites and multi-species  The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:41 11/8/2010 Bioinformatics-btq419.tex] Page: 2336 2336 2337  LocusZoom Full documentation of all features is available on the LocusZoom website.
The LocusZoom webpage comes pre-loaded with genome- wide association results for HDL cholesterol, LDL cholesterol and triglycerides in 20 000 individuals of European ancestry (Kathiresan et al., 2009). 3 CONCLUSIONS We have created a user-friendly tool to generate regional plots of association results in their genomic context.
LocusZoom allows for quick visual inspection of the strength of association evidence, the extent of the association signal and LD, and the position of the associated SNPs relative to genes in the region.
LocusZoom plots provide an option to size the data points relative to sample size and can display functional annotation.
LocusZoom can be accessed from a simple web-based form with drop-down menus or by uploading a speciﬁcation ﬁle to generate many plots at once.
LocusZoom Python application, source code in R, and associated databases are available for download and we provide instruction for users to create custom database tables.
It is anticipated that, in the future, additional publicly available result sets will be available for convenient viewing.
ACKNOWLEDGEMENTS The authors thank Anne Jackson, Karen Mohlke and Laura Scott for ideas to improve LocusZoom. The UCSC Browser can be found at http://genome.ucsc.edu/. LocusZoom can be found at http://csg.sph.umich.edu/locuszoom. Funding: R.J.P. is supported by a Research Fellowship from Calvin College.
M.B. and T.M.T. are supported by grants from the National Institue of Diabetes and Digestive and Kidney Diseases (DK062370, PI M.B.). M.B., T.N.T. and G.R.A. are supported by the National Human Genome Research Institute (HG000376, PI M.B. for T.M.T. HG000040, PI M.B. HG002651, PI G.R.A. and HG005214, PI G.R.A.). G.R.A. is additionally funded by the National Institute of Mental Health (MH084698). C.J.W. is funded by a Pathway to Independence Award from the National Heart, Lung and Blood Institute (K99HL094535, PI C.J.W.). Conﬂict of Interest: none declared.
REFERENCES Bush,W.S. et al. (2010) Visualizing SNP statistics in the context of linkage disequilibrium using LD-Plus. Bioinformatics, 26, 578 579.
Johnson,A.D. et al. (2008) SNAP: a web-based tool for identiﬁcation and annotation of proxy SNPs using HapMap. Bioinformatics, 24, 2938 2939.
Jorgenson,E. et al. (2009) VALID: visualization of association study results and linkage disequilibrium.
Genet. Epidemiol., 33, 599 603.
Kathiresan,S. et al. (2009) Common variants at 30 loci contribute to polygenic dyslipidemia. Nat. Genet., 41, 56 65.
Kent,W.J. et al. (2002) The human genome browser at UCSC. Genome Res., 12, 996 1006.
Loos,R.J. et al. (2008) Common variants near MC4R are associated with fat mass, weight and risk of obesity.
Nat. Genet., 40, 768 775.
Manolio,T.A. et al. (2009) Finding the missing heritability of complex diseases.
Nature, 461, 747 753.
Schmitt,A.O. et al. (2010) CandiSNPer: a web tool for the identiﬁcation of candidate SNPs for causal variants.
Bioinformatics, 26, 969 970.
2337 Fig.1.
An example LocusZoom plot showing the HDL cholesterol- associated region near the MMAB gene (Kathiresan et al., 2009). conservation), which is available for all SNPs in dbSNP or the 1000 Genomes Project (August 2009 release). The size of the data points can optionally reﬂect the square root of the sample size.
The bottom panel of a LocusZoom plot shows the name and location of genes in the UCSC Genome Browser (Kent et al., 2002). Positions of exons are displayed, and the transcribed strand is indicated with an arrow.
This allows the visual comparison of association results relative to coding regions.
Gene names are automatically spaced relative to one another to avoid overlap.
Currently used plotting tools include regional association plotter SNAP (Johnson et al., 2008) and LD-based viewers such as LD-Plus (Bush et al., 2010), CandiSNPer (Schmitt et al., 2010) and VALID (Jorgenson et al., 2009). LocusZoom provides additional features not currently available in any other single tool, such as: (i) the display of 1000 Genomes or novel SNPs from sequencing studies, (ii) functional annotation of SNPs, (iii) exon/intron distinction and automated gene spacing, (iv) ability to plot regions larger than 500 kb, (v) no pre-selection of input ﬁles and (vi) web-based batch mode and availability of source code and databases for download and local installation of LocusZoom. 2.2 Usage LocusZoom was written in R using the grid and lattice graphics packages and runs within a Python wrapper.
SQLite tables with relevant data for recombination rate, SNP position, annotation and gene information can be accessed using Python s built-in SQLite tools.
A simple plot can be generated from the web form by uploading a ﬁle with SNP names and P-values, and specifying the region to be plotted and optional features using drop-down buttons.
Typical run time for a single plot returned to the browser window is 20 s, not including time required to upload a meta-analysis ﬁle, which varies according to the user s internet upload speed and ﬁle size.
To reduce upload time, users may choose to restrict data ﬁles to the region being plotted.
To generate a series of locus plots from the web form, users can submit a speciﬁcation ﬁle where custom speciﬁcations for each plot can be listed.
When a speciﬁcation ﬁle is used to draw many plots, a single pdf containing all generated plots is returned to the user by e-mail.
Finally, users can download our scripts, which require R and Python, and associated databases in SQLite format to enable plot generation on their local unix machine.
[11:41 11/8/2010 Bioinformatics-btq419.tex] Page: 2337 2336 2337
BIOINFORMATICS APPLICATIONS NOTE Vol.27 no.6 2011, pages 889 890 doi:10.1093/bioinformatics/btr020 Databases and ontologies Dalliance: interactive genome viewing on the web Thomas A. Down1, , Matias Piipari2 and Tim J. P. Hubbard2 1Wellcome Trust/CRUK Gurdon Institute, Tennis Court Road, Cambridge CB2 1QN and 2Wellcome Trust Sanger Institute, Hinxton, Cambridge, CB10 1HH, UK Associate Editor: Jonathan Wren Advance Access publication January 19, 2011 ABSTRACT Summary: Dalliance is a new genome viewer which offers a high level of interactivity while running within a web browser.
All data is fetched using the established distributed annotation system (DAS) protocol, making it easy to customize the browser and add extra data.
Availability and Implementation: Dalliance runs entirely within your web browser, and relies on existing DAS server infrastructure.
Browsers for several mammalian genomes are available at http://www.biodalliance.org/, and the use of DAS means you can add your own data to these browsers.
In addition, the source code (Javascript) is available under the BSD license, and is straightforward to install on your own web server and embed within other documents.
Contact: thomas@biodalliance.org Received on September 28, 2010 revised on December 22, 2010 accepted on January 9, 2011 Since the early days of the draft human genome, web-based genome browsers such as Ensembl, GBrowse and the UCSC browser have been popular and important tools for biologists working on datasets large and small (Hubbard et al., 2009 Rhead et al., 2010 Stein et al., 2002). Despite increasing sophistication of data production and analysis methods, the importance of eyeballing data to generate hypotheses or simply check the results of new analyses cannot be understated.
Perhaps surprisingly, while genome browsing tools remain under very active development, the general approach taken by the major browsers has remained constant: a complex piece of server software reads databases, integrates information and creates bitmap image ﬁles that are displayed in the user s web browser window.
This approach is reliable and places low demands on the end user s machine.
However, it imposes serious limits on the level of interactivity, since any change in the display requires a full reload.
There has been some interest in desktop applications, such as IGV (Robinson et al., 2011), which shift more work to the client side and increase the level of interactivity.
Examples such as Apollo (Lewis et al., 2002) and Otterlace/Zmap (Searle et al., 2004) have become important tools to support the specialized activity of genome annotation.
However, given the availability of reasonably functional tools which run in the web browser, the majority of users have been reluctant to install a heavyweight desktop client.
With that said, the web browser is not a static target, and the possibility of writing rich client applications in Javascript that run in web browsers has increased steadily over the last few years.
In particular, making calls back to the server and fetching additional   To whom correspondence should be addressed.
information without fully refreshing the page ( AJAX ) has become a standard part of many web developers toolkits. There have already been several attempts to improve genome browsing using a degree of client side interactivity.
One approach uses large sets of pre- rendered image tiles, analogous to online mapping applications (Yates et al., 2007) while others have used CSS to perform limited drawing within the user s browser (Skinner et al., 2009). However, the former seriously limits the ﬂexibility of the system especially when it comes to adding new data while the latter offers only limited graphical capabilities.
We believe that the time is ripe for a new and uncompromising approach to combine the best features of web-based and desktop genome browsers.
To address the limitations in current browsers, we have developed Dalliance, a new genomics tool which runs within the web browser but uses a number of recent technologies most importantly, the W3C scalable vector graphic model (SVG) to offer a level of interactivity which is competitive with desktop applications.
Dalliance uses the standard distributed annotation system (DAS) protocol (Jenkinson et al., 2008), already used to add extra tracks to the web-based browsers including Ensembl and Gbrowse, to fetch sequence, annotations and alignments from servers around the network, before integrating the data into a smoothly-scrolling vector graphics display (Fig. 1). Taking this approach offers a number of advantages.
Following the DAS model means that researchers wanting to show their own data in a browser can easily do so without hosting their own copies of the reference genome and basic annotation databases, and allows data consumers to combine datasets in novel ways.
Our choice of SVG gives a rich graphics platform comparable with APIs available on desktop platforms: we currently implement all the glyph types from the DAS stylesheet speciﬁcation, and it would be straightforward to add more.
SVG takes a scene graph approach (i.e. the rendering code builds a tree of objects describing what should be drawn, rather than calling rendering primitives directly), which means that smooth scrolling and export of high-quality vector graphics in SVG or, with some straightfoward server support, PDF format for publication or presentation are both straightforward.
Because each track of features is fetched using a separate although usually concurrent network request, and displayed as soon as the data arrives, one slow data source does not hold up the display of the rest of the data.
And by fetching some excess data on each side of what is currently being displayed, the loading time can often be hidden from the user entirely.
In recognition that the reference genome sequences of most species are still moving targets, and that data released a few years ago may still be valuable today, even if it isn t actively maintained, we allow DAS sources targeted to one version of a  The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:37 19/2/2011 Bioinformatics-btr020.tex] Page: 889 889 890  T.A.Down et al. Fig.1.
Typical display from the Dalliance genome browser.
Quantitative data from functional genomics experiments can be displayed in a range of styles, conﬁgured using DAS stylesheets. genome (e.g. human NCBI36/hg18) to be remapped on the ﬂy to another (e.g. GRCh37/hg19). The DAS protocol is used even for this.
We use the standard DAS alignment command although in a somewhat novel way to retrieve the alignment data used for the mapping step, and metadata from the DAS registry tells the client when remapping is necessary.
Dalliance s model of accessing data from multiple sources (via DAS), rather than from a single central server, is also ideally suited to an emerging strategy for the handling of next generation sequencing datasets. The dramatically increased output and decreased costs of sequencing has led to it being used as an assay tool for a wide range of experiment types including genome variation, transcriptional expression, a readout for DNA protein binding.
Traditionally, after mapping to a reference genome, sequence reads are processed and stored in a database in order to provide access for users.
However, the high overhead in maintaining such databases does not scale to the amounts of data now being generated by such experiments.
Kent et al., 2010 have instead proposed processing the output of mapping pipelines for an entire experiment into a single, indexed ﬂat ﬁle, made accessible to users by simply placing it on a local web server.
This is efﬁcient since browsers can be conﬁgured to access portions of these ﬂat ﬁles, only downloading data for the region currently being displayed.
This indexed ﬁle based approach is in the process of being adopted as a submission standard to short read archives at EBI and NCBI using the BAM format implementing Kent s strategy (Li et al., 2009), which will make these ﬁles very widely available.
As part of the Dalliance project, a lightweight BAMMappingSource has been developed to allow Dalliance to access such indexed ﬁles as if they were DAS sources.
Dalliance is written in standard Javascript, using APIs which are being standardized under the HTML5 banner.
It places relatively high demands on the web browser, but is tested regularly and runs smoothly on Mozilla Firefox (3.6 or later), Safari (5.0 or later) and Google Chrome (5.0 or later). Microsoft Internet Explorer does not currently include SVG support, but it is promised for version 9, and we are optimistic that at that point it will be possible to support up-to-date versions of all major web browsers.
The source code is freely available, and is written as a self contained object which can be inserted into almost any HTML page, so it can be combined with blogs, wikis, etc., or added as a browser component to an existing HTML database interface.
Dalliance is a practical genome browser that provides a smooth, interactive, user experience while handling large volumes of data.
Since all data is loaded via DAS, it is straightforward to add additional data, or even a complete new genome dataset. The modern web browser offers a rich platform for data visualization, including complex scientiﬁc datasets, and we expect to see similar technological approaches deployed widely in the future.
ACKNOWLEDGEMENTS Thanks to all the numerous testers who have provided testing and feedback for early versions of the Dalliance software, and to Jonathan Warren for updating the DAS registry to better support Javascript clients.
Funding: Wellcome Trust Research Career Development Fellowship (054523 to T.A.D.). Conﬂict of Interest: none declared.
REFERENCES Hubbard,T.J. et al. (2009) Ensembl 2009.
Nucleic Acids Res., 37, D690 D697. Jenkinson,A.M. et al. (2008) Integrating biological data the distributed annotation system.
BMC Bioinformatics, 9 (Suppl. 8), S3. Kent,W.J. et al. (2010) Bigwig and bigbed: enabling browsing of large distributed datasets. Bioinformatics, 26, 2204 2207.
Lewis,S.E et al. (2002) Apollo: a sequence annotation editor.
Genome Biol., 3, research0082. Li,H. et al. (2009) The sequence alignment/map format and SAMtools. Bioinformatics, 25, 2078 2079.
Rhead,B. et al. (2010) The UCSC genome browser database: update 2010.
Nucleic Acids Res., 38, D613 D619. Robinson,J.T. et al. (2011) Integrative genomics viewer.
Nature Biotech., 29, 24 26.
Searle,S.M.J. et al. (2004) The Otter annotation system.
Genome Res., 14, 936 970.
Skinner,M.E. et al. (2009) Jbrowse: A next-generation genome browser.
Genome Res., 19, 1630 1638.
Stein,L.D. et al. (2002) The generic genome browser: a building block for a model organism system database.
Genome Res., 12, 1599 1610.
Yates,T. et al. (2007) X:Map: annotation and visualization of genome structure for affymetrix exon array analysis.
Nucleic Acids Res., 36, D780 D786. 890 [10:37 19/2/2011 Bioinformatics-btr020.tex] Page: 890 889 890
BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.19 2010, pages 2484 2485 doi:10.1093/bioinformatics/btq457 Data and text mining NoiseMaker: simulated screens for statistical assessment Phoenix Kwan and Amanda Birmingham  Advance Access publication August 11, 2010 Thermo Fisher Scientiﬁc, 2650 Crescent Drive, Lafayette, CO 80026, USA Associate Editor: John Quackenbush ABSTRACT Summary: High-throughput screening (HTS) is a common technique for both drug discovery and basic research, but researchers often struggle with how best to derive hits from HTS data.
While a wide range of hit identiﬁcation techniques exist, little information is available about their sensitivity and speciﬁcity, especially in comparison to each other.
To address this, we have developed the open-source NoiseMaker software tool for generation of realistically noisy virtual screens.
By applying potential hit identiﬁcation methods to NoiseMaker-simulated data and determining how many of the pre- deﬁned true hits are recovered (as well as how many known non-hits are misidentiﬁed as hits), one can draw conclusions about the likely performance of these techniques on real data containing unknown true hits.
Such simulations apply to a range of screens, such as those using small molecules, siRNAs, shRNAs, miRNA mimics or inhibitors, or gene over-expression we demonstrate this utility by using it to explain apparently conﬂicting reports about the performance of the B score hit identiﬁcation method.
Availability and implementation: NoiseMaker is written in C#, an ECMA and ISO standard language with compilers for multiple operating systems.
Source code, a Windows installer and complete unit tests are available at http://sourceforge.net/projects/noisemaker. Full documentation and support are provided via an extensive help ﬁle and tool-tips, and the developers welcome user suggestions.
Contact: amanda.birmingham@thermoﬁsher.com Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 14, 2010 revised on July 23, 2010 accepted on August 5, 2010 1 INTRODUCTION Data analysis and hit identiﬁcation are points of confusion for many screeners (Birmingham et al., 2009). Those asking questions such as Which method identiﬁes the most true hits for my particular screen circumstances  or What will the false positive rate of my chosen method be  are frequently stymied, since answering these requires them to know the identity of the real hits.
However, developing a list of the anticipated real biological hits for any given assay is extremely challenging and is likely to be both noisy and incomplete, especially for medium- to weak-strength effects.
The difﬁculty in assessing the performance of hit identiﬁcation methods can be avoided by moving to in silico-based strategies.
In the computational environment, one can generate a virtual screen containing deﬁned true hits at known locations, and then perturb these true values with varying degrees and types of noise (both   To whom correspondence should be addressed.
systematically biased and random) to simulate the variation inherent in biological screens statistical techniques can then be evaluated based on their ability to identify known true positives and true negatives.
These evaluations will be valid to the extent that the in silico hit distributions and types of noise are congruent with those of the real system.
This approach offers both speed and ﬂexibility, providing the opportunity to proﬁle a method s performance in many different realistic screening scenarios as well as the ability to simulate whole screens within minutes.
To enable such in silico testing, we have developed the NoiseMaker tool for generating simulated high-throughput screening datasets. A NoiseMaker user selects a realistic scenario for his or her simulated screen, including a range of hit properties as well as noise characteristics, derived from previous screens or assay development work (Supplementary Appendix 1) the software then randomly assigns true hits conforming to this scenario and generates noisy replicates of the screen.
The user applies potential analysis approaches to this noisy data, using the known true hits to calculate metrics of interest (such as sensitivity, speciﬁcity or positive predictive value), and selects the most effective method.
2 MAIN FEATURES This simulation software offers two main features: (i) the ability to generate a random set of true hits that conform to expected characteristics and (ii) the ability to apply user-speciﬁed noise to a list of true hits to model realistically messy screening results.
On the tab for generation of true hits (Fig. 1A), the user inputs a tab-delimited plate map ﬁle containing reagent identiﬁers represented by one row per well and a default true value to be assigned to all reagents that are not treated as hits or controls.
Controls are speciﬁed by reagent identiﬁer and assigned a name (such as up-regulating positive control ) and a true value.
The user may specify as many types of controls as desired as long as each has a unique name e.g. an siRNA-based screen might have lipid controls, negative controls for transfection and positive controls for both up- and down-regulation, all with different identiﬁers and different expected values.
All instances of a control s reagent identiﬁer in the plate map will be assigned the value speciﬁed for that control type.
Hits may represent either an increase or a decrease from the default value.
They are speciﬁed by their unique name, strength and frequency the latter number can be either an absolute value (e.g. eight wells) or a percentage of the non-control wells (e.g. 8% of the wells). For each hit type, the NoiseMaker software will randomly select, without replacement, the appropriate number of non-control wells and assign them the value speciﬁed for that hit type.
The Hit Type input can also be used to model random equipment or assay failures that could be mistaken for hits.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[14:01 28/8/2010 Bioinformatics-btq457.tex] Page: 2484 2484 2485  NoiseMaker 3 SAMPLE APPLICATION The B score (Brideau et al., 2003) is a normalization and hit identiﬁcation method employing Tukey s median polish, and has been proposed for use in screens displaying within-plate positional effects such as row and/or column biases.
However, Makarenkov et al. (2007) have reported that it failed to recover correct hits in a scenario with noisy standard normal data with systematic error stemming from row  column interactions which are constant across plates.
To address this apparent inconsistency in the literature and demonstrate how NoiseMaker can be applied in evaluating the performance of statistical techniques, we evaluated the B score in scenarios with different types of row and column positional effects: varying size of SD (Group A), varying size of mean change (Group B) and varying size of both mean change and SD (Group C). After data sets with appropriate noise were created by NoiseMaker, we calculated B scores for all wells and identiﬁed the wells whose scores were in the top 1% as positives.
We found that the true positive rates of datasets in Group A decrease and the false positive rates slightly increase as SD of the row and column noise increases (Supplementary Appendix II). However, the true positive rates and false positive rates of datasets in Group B remain steady regardless of the amount of mean change of the row and column noise, while the true positive rates and false positive rates of data sets in Group C behave similarly to those in Group A.
These results suggest that B score is an appropriate choice for correction of systemic inﬂuences that primarily affect mean rather than variance.
Notably, Makarenkov s work examined simulated data with varying SDs, which is consistent with this ﬁnding. screens 4 CONCLUSION NoiseMaker is simulation software for creating realistic, virtual high-throughput that can be used to evaluate hit identiﬁcation methods and quality criteria.
We establish its power by using it to clarify the utility of the B score under various screening conditions.
This tool will be useful for broader comparisons of available hit identiﬁcation methods, and is freely available for download and use by others interested in modeling screens in silico. ACKNOWLEDGEMENTS The authors thank Gabor Bakos of Trinity College, Dublin, and members of the RNAi Global Initiative for suggestions and helpful discussions, and Kevin Sullivan and John Quinn for code-review.
Conﬂict of Interest: none declared.
REFERENCES Birmingham,A. et al. (2009) Statistical methods for analysis of high-throughput RNA interference screens.
Nat. Methods, 6, 569 575.
Brideau,C. et al. (2003) Improved statistical methods for hit selection in high-throughput screening.
J. Biomol. Screen., 8, 634 647.
Makarenkov,V. et al. (2007) An efﬁcient method for the detection and elimination of systematic error in high-throughput screening.
Bioinformatics, 23, 1648 1657.
2485 Fig.1.
(A) Hit and (B) Noise tabs of the NoiseMaker interface.
The output of the true hit generation is a plate map ﬁle that is annotated with the true values for every well and the type of value for that well (which is either Default or the well s assigned control or hit name). For convenience, this ﬁle s name is automatically copied to the input ﬁeld of the Noise tab (Fig. 1B). Noise can also be added to true values ﬁles not created with NoiseMaker, as long as they conform to the expected format this can be useful for modeling the effect of noise on clumped hit distributions such as those from non-randomly plated screening libraries.
On the Noise tab, the user speciﬁes the plate dimensions and number of replicates and then describes the systematic noise to be applied.
Noise can be applied at the level of several different elements of the screen, including the entire screen, the edges of every plate, an individual plate in the screen, a particular row on every plate, a particular column on every plate and/or a particular well on every plate.
This allows the user to simulate a wide variety of realistic outcomes, from evaporation of reagents in edge wells to a blocked dispensing tip at a single well position.
Noise deﬁnitions are additive e.g. if one is speciﬁed for the entire screen, one for Plate 2 and one for Row 5, then all values in Row 5 of Plate 2 will be permuted with the screen noise, the plate noise, and the row noise combined.
This simulates the convergence of disparate systematic effects in real screens.
All Noise deﬁnitions model noise as a Gaussian perturbance of the true values.
They adjust the well values away from their initial values by approximately the amount assigned to the Noise deﬁnition s mean change value, with the exact amount of adjustment being randomly chosen from a Gaussian distribution centered on the mean change value and with the speciﬁed standard deviation (SD) value.
This ensures that each Noise deﬁnition produces realistically noisy adjustments even as it introduces the intended systematic effects.
Noise can also be limited to a speciﬁc range (such as that simulating an instrument s detection range) using optional ﬂoor and/or ceiling values.
The output ﬁle contains the input true values and one column of noisy values for each simulated replicate.
Currently NoiseMaker is limited to Gaussian noise distributions, additive noise and linear positional effects.
Future development will address non-Gaussian and multiplicative noise, as well as bowl- shaped (non-linear) positional biases.
[14:01 28/8/2010 Bioinformatics-btq457.tex] Page: 2485 2484 2485
Genomics Proteomics Bioinformatics 13 (2015) 36 39 H O S T E D  BY Genomics Proteomics Bioinformatics www.elsevier.com/locate/gpb www.sciencedirect.com RESOURCE REVIEW Web Resources for Mass Spectrometry-based Proteomics Tao Chen 1,a, Jie Zhao 2,b, Jie Ma 1,c, Yunping Zhu 1,*,d 1 State Key Laboratory of Proteomics, Beijing Proteome Research Center, Beijing Institute of Radiation Medicine, Beijing 102206, China 2 Biological Information College, Chongqing University of Posts and Telecommunications, Chongqing 400065, China Received 2 January 2015 revised 22 January 2015 accepted 28 January 2015 Available online 23 February 2015 Handled by Siqi Liu KEYWORDS Mass spectrometry Proteomics Web resources Introduction Abstract With the development of high-resolution and high-throughput mass spectrometry (MS) technology, a large quantum of proteomic data is continually being generated.
Collecting and shar- ing these data are a challenge that requires immense and sustained human effort.
In this report, we provide a classiﬁcation of important web resources for MS-based proteomics and present rating of these web resources, based on whether raw data are stored, whether data submission is supported, and whether data analysis pipelines are provided.
These web resources are important for biologists involved in proteomics research.
The advancement of tandem mass spectrometry (MS) has made it possible to identify hundreds of thousands of proteins in MS-based experiments [1]. With the development of a wide range of methods spectrometry and data analysis, MS-based proteomics has gained popularity in biomedical research.
The vastly-expanding research using tandem MS technology is continually generating large amounts of for * Corresponding author.
E-mail: zhuyunping@gmail.com (Zhu Y). a ORCID: 0000-0001-8191-4135. b ORCID: 0000-0002-6275-7506. c ORCID: 0000-0002-8934-922X. d ORCID: 0000-0002-7320-7411.
Peer review under responsibility of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
proteomics data.
Collecting these datasets is undoubtedly becoming crucial to the research community.
Proteomics data repository contains a proteome with high coverage and sufﬁ- cient data content for statistical analysis, and provides exten- sive observational data for genome annotation projects as well.
However, maintaining such data repository is challenging due to the diversity and quantum of data as well as varying needs of different users.
In this report, we describe web data repositories for MS-based proteomics and rate them based on their score against parameters such as storage of raw data, data submission support, and provision of data analysis pipe- lines.
The main features of these resources are shown in Table 1.
Based on their focus areas within proteomic research, we classiﬁed these resources into 3 categories: general proteomics data repositories, quantitative proteomics data repositories, and proteomics data repositories focusing on protein post-translational modiﬁcations (PTMs). http://dx.doi.org/10.1016/j.gpb.2015.01.004 1672-0229 ª 2015 The Authors.
Production and hosting by Elsevier B.V. on behalf of Beijing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Chen T et al  Web Resources for Proteomics 37 ] 3 [ ] 1 [ ] 4 [ ] 5 [ ] 6 [ ] 7 [ ] 9 , 8 [ ] 0 1 [ ] 1 1 [ ] 2 1 [ ] 4 1 , 3 1 [ ] 6 1 , 5 1 [ w w w w w w w w w w q w w w w w w w w w q q w w w q q w w w q q w w w q w w w w q q w w w q q w w w q w w w w q w w w w q w w w w.
s f e R g n i t a R s t n e m i r e p x e s c i m o e t o r p d e s a b - S M m o r f n o i t a m r o f n i n o i s s e r p x e n i e t o r p s e r o t S g r o.
e r i p s n i e t o r p d e p o m.
  : p t t h s i s y l a n a a t a d d n a , n o i s s i m b u s a t a d , e g a r o t s a t a d w a r s t r o p p u S.
n c.
g r o o p u h.
x o r p i   : p t t h n o i s s i m b u s a t a d d n a e g a r o t s a t a d w a r s t r o p p u S e h c n a r t  g r o.
s n o m m o c e m o e t o r p   : s p t t h s i s y l a n a a t a d s t r o p p u S.
g r o m p g e h t.
w w w   : p t t h h c r a e s e r s c i m o e t o r p r o f e s a b a t a d d n a e t i u s s c i t a m r o f n o b i i d e t a r g e t n i n A u d e.
e l a y.
d e m d e p y   : p t t h.
e h c n a r T B D M P G D E P O M D E P Y X o r P i s i s y l a n a a t a d d n a , n o i s s i m b u s a t a d , e g a r o t s a t a d w a r s t r o p p u S g r o.
s a l t a e d i t p e p w w w.
  : p t t h s a l t A e d i t p e P n o i s s i m b u s a t a d d n a e g a r o t s a t a d w a r s t r o p p u S g r o.
a i d e p n i e t o r p n a m u h w w w.
  : p t t h a i d e p n i e t o r P n a m u H n o i s s i m b u s a t a d d n a e g a r o t s a t a d w a r s t r o p p u S e v i h c r a  e d i r p  k u.
c a.
i b e.
w w w   : p t t h s e r u t a e f n i a M k n i L E D R P I e m a N y r o g e t a C l a r e n e G s e c r u o s e r s c i m o e t o r p d e s a b - S M r o j a m f o t s i L 1 e l b a T e g a r o t s a t a d S M c i m o e t o r p o h p s o h p d n a e g a r o t s a t a d w a r s t r o p p u S m o c.
a d i s o h p w w w.
  : p t t h w t.
u d e.
u t c n.
c b m m t p b d   : p t t h.
A D I S O H P M T P b d s M T P h t i w d e t a i c o s s a s e d i t p e p S M  S M d n a a t a d w a r s e r o t S e g a r o t s a t a d s c i m o e t o r p e v i t a t i t n a u q s t r o p p u S e g a r o t s a t a d S M c i m o e t o r p o h p s o h p s t r o p p u S s e t i s M T P d e t r o p e r - S M d n a a t a d w a r s e r o t S g r o.
e t i s o h p s o h p w w w.
  : p t t h.
g r o u e.
m l e.
o h p s o h p   : p t t h l s u P e t i S o h p s o h P.
M L E o h p s o h P.
g r o b d - x a p   : p t t h b D x a P d e s u c o f - s M T P e v i t a t i t n a u Q  y r t e m o r t c e p s s s a m , S M.
s e n i l e p p i s i s y l a n a a t a d f o n o i s i v o r p d n a , t r o p p u s n o i s s i m b u s a t a d , a t a d w a r f o e g a r o t s s a h c u s s r e t e m a r a p t s n i a g a e r o c s r i e h t n o d e s a b d e t a r e r a s e c r u o s e r b e w e s e h T : e t o N.
n o i t a c ﬁ d o m i l a n o i t a l s n a r t - t s o p , M T P General proteomics data repositories Proteomics IDEntiﬁcations database The Proteomics IDEntiﬁcations (PRIDE) database created by the European Bioinformatics Institute (EBI) is a web resource that collects MS-based proteomics data.
By the end of 2014, PRIDE accumulated data for 41,835 proteins, 269,806 unique peptides, and about 101 million spectra [2]. PRIDE is one of the most popular proteomic data repositories that have played an important role in the nascent Human Proteome Project (HPP) [3]. PeptideAtlas PeptideAtlas is a database that stores various formats of out- put ﬁles and metadata from MS-based experiments [1], it also allows users to submit raw data.
These raw data are periodi- cally analyzed for identiﬁcation and statistical analysis purposes.
The results are made available back to the research- ers by web-based presentation systems.
PeptideAtlas can help plan targeted proteomics improve genome annotation, and support data mining projects [1]. experiments, Human Proteinpedia Human Proteinpedia is a resource to integrate, store, and share proteomic data [4]. It is a platform for collecting human pro- teomic data using a distributed annotation system, which allows the research community to contribute protein annota- tions. By the end of 2014, Human Proteinpedia has covered 15,231 proteins, 1,960,352 peptides, and about 5 million spec- tra [2]. It also provides a panorama of the human proteome. iProX iProX is an integrated proteome resources center based in China, which is built to support the worldwide sharing of proteomics data.
Currently, iProX comprises an experiment data submission system and a proteome database.
The iProX submission system is a public platform that was set up follow- ing the data-sharing policy of the ProteomeXchange consor- tium. Raw data and standardized meta-data from proteomics experiments can be collected and shared by using controlled vocabularies to describe the Minimum Information About a Proteomics Experiment (MIAPE). Registered users can choose to submit their proteomics datasets to iProX via public or pri- vate modes.
Datasets submitted via the public mode are openly accessible, whereas private datasets can only be accessed by the authorized users.
On the other hand, the iProX proteomics database was developed as a structured storage platform for data deposited in the system.
iProX facilitates data analysis and sharing.
Up till now, it has covered 46 projects, 190 subprojects, and 6441 data ﬁles. Tranche Tranche is a data repository targeting storage and sharing of information for proteomics researchers.
It supports re-use and dissemination of both data and software.
To reduce data  38 Genomics Proteomics Bioinformatics 13 (2015) 36 39 redundancy and achieve load balancing, it adopts peer-to-peer networking.
It also uses a client server model to ensure authentication and reliability.
A client tool is required to upload and download datasets. It has several important features including pre-publication encryption, data pedigree, data integrity, immutability, and versioning.
Tranche provides interfaces for PRIDE, Human Proteinpedia, and PeptideAtlas to store and disseminate large MS-based data ﬁles [5]. Global Proteome Machine Database and from primary proteomics data resources such as PRIDE and PeptideAtlas, and then analyzes the actual spectral count [10]. By the end of 2014, it included 10,482 proteins 143,456 peptides, and about 24 million spectra [2]. The launch of PaxDb brings together disparate aspects of biology for high- throughput analysis and supports global comparative analysis across different organism groups.
Proteomics data repositories focusing on protein PTMs The Global Proteome Machine Database (GPMDB) is a resource for collecting diverse tandem mass spectra.
It also includes peptide and protein identiﬁcations that are important for further MS computational research [6]. GPMDB provides a pipeline for reprocessing raw data submitted by users or imported from other repositories, thus generating XML ﬁles that store information about peptide and protein identiﬁca- tion. Speciﬁcally, identiﬁed proteins are organized into separate spreadsheets for each chromosome and mitochondrial DNA. By the end of 2014, GPMDB data spans 136,373 proteins, 1,786,698 peptides, and 1020 million spectra [2]. GPMDB has played an important role in the Chromosome- Centric Human Proteome (C-HPP) Project.
Model Organism Protein Expression Database The Model Organism Protein Expression Database (MOPED) is a proteomics repository that integrates protein expression information from MS-based proteomics experiments on human specimens and that from model organisms [7]. It also provides new estimates of protein abundance and concentra- tion, and statistical summaries from experiments.
Several search and visualization tools are available.
By the end of 2014, MOPED has developed into a repository containing 17,141 proteins, 250,000 unique peptides, and approximately 15 million spectra [2], providing researchers with information on complex biological processes and thus supporting biomedi- cal discovery.
Yale Protein Expression Database The Yale Protein Expression Database (YPED) [8] is an inte- grated bioinformatics suite and database for proteomics research, which was signiﬁcantly improved from the ﬁrst version released in 2007 [9]. YPED supports many kinds of data including those from multiple MS instruments, different search engines, and labeled or label-free quantiﬁcation. YPED is a web-accessible and user-friendly resource, designed to meet data management, archival, and analysis needs of high-throughput MS-based proteomics research.
Quantitative proteomics data repositories PaxDb PaxDb is a meta-database integrating whole-organism data and tissue-resolved data at absolute protein abundance levels for various model organisms.
imports quantitative proteomics data sets exclusively from published experiments It Phospho.ELM Recent advances in MS techniques have enabled more efﬁcient detection of phosphorylated proteins [9]. The Phospho.ELM is a web-based resource aimed at storing phosphorylation data imported from research papers and phosphoproteomic MS analyses.
MS experiments are run on human/mouse cell lines tissues.
Phospho.ELM is used by laboratory scientists and computational biologists to develop public repositories [11]. To date, this web resource covers 42,914 instances, 299 kinases, 3657 references, 11,224 sequences, and 8698 substrates.
PhosphoSitePlus PhosphoSitePlus (PSP) is a comprehensive and manually-cu- rated resource designed to collect the structure and function of PTMs, primarily of human and mouse origin.
PSP supports two kinds of data, including the modiﬁed amino acid and sur- rounding sequences as well as upstream and downstream inter- actions with regard to functional regions of the protein [12]. The majority of PTM sites in PSP were detected using MS. PSP is useful to life scientists and biomedical researchers.
Currently, PSP spans 50,636 proteins, 1,933,888 MS peptides, 438,576 high-throughput MS sites, 20,262 low-throughput sites, and 18,374 curated papers.
dbPTM dbPTM is a resource which collects data on experimentally- validated protein PTMs. This resource imports PTM sites from public resources such as SwissProt, Phospho.ELM, and O-GLYCBASE [13]. It also extracts identiﬁed peptides with PTMs from research papers.
dbPTM is an important resource for researchers working on substrate speciﬁcity of PTM sites [14]. To date, dbPTM has covered 153,113 phosphorylation experimental sites, 23,673 ubiquitylation experimental sites, 10,385 acetylation experimental sites, 15,678 N-linked glycosy- lation experimental sites, and 3711 O-linked glycosylation experimental sites.
Phosphorylation site database The phosphorylation site database (PHOSIDA) is a database with a collection of a large number of high-conﬁdence phos- phorylation sites.
MS-based proteomics is used to identify these sites in various species [15]. To date, the database covers 80,062 N-glycosylated, phosphorylated, or acetylated sites.
Stringent quality criteria based on a very low false positive rate are used to obtain these sites from high-resolution MS data  Chen T et al  Web Resources for Proteomics 39 [16]. PHOSIDA contains PTM sites from human as well as other species, including bacteria.
Concluding remarks In this report, we have covered some important proteomics data repositories that are useful for the research community.
These resources not only provide raw data and identiﬁcation results, but also support prospective, high-throughput pro- teomics research.
In addition, they also act as data providers for large-scale genome annotation efforts.
In the years to come, sharing data and metadata between repositories will become more important.
Thus, proteomics repositories need to focus on developing an integrated approach to data accessibility between repositories.
On the other hand, with the advent of new instruments, new sample preparation tech- niques, and new data analysis methods, new forms of data will be continuously generated.
The amount of data in the repos- itories to be shared at present is just a small fraction of the actually-generated proteomics data that will eventually become available.
In order to attract more researchers to sub- mit data, the resources will have to standardize the process and simplify the interface for data submission.
Competing interests The authors declared that there are no competing interests.
Acknowledgements of China This research was supported by the Ministry of Science and Technology 2013CB910801, 2012AA020201, 2012AA020409, and 2014DFB30010), the National Natural Science Foundation of China (Grant Nos.
21105121, 21475150, and 61303073) and Beijing Municipal Natural Science Foundation of China (Grant No.5122013). (Grant Nos.
References [1] Deutsch EW. The PeptideAtlas project.
Methods Mol Biol 2010 604:285 96.
[2] Perez-Riverol Y, Alpi E, Wang R, Hermjakob H, Vizcaino JA. Making proteomics data accessible and reusable: current state of proteomics databases and repositories.
Proteomics 2014. http:/ dx.doi.org/10.1002/pmic.201400302. [3] Vizcaino JA, Cote RG, Csordas A, Dianes JA, Fabregat A, Foster JM, et al. The PRoteomics IDEntiﬁcations (PRIDE) database and associated tools: in 2013.
Nucleic Acids Res 2013 41:D1063 9. status [4] Kandasamy K, Keerthikumar S, Goel R, Mathivanan S, Patankar N, Shafreen B, et al. Human proteinpedia: a uniﬁed discovery resource for proteomics research.
Nucleic Acids Res 2009 37: D773 81.
[5] Smith BE, Hill JA, Gjukich MA, Andrews PC. Tranche distributed repository and ProteomeCommons.org. Methods Mol Biol 2011 696:123 45.
[6] Craig R, Cortens JP, Beavis RC. Open source system for analyzing, validating, and storing protein identiﬁcation data.
J Proteome Res 2004 3:1234 42.
[7] Kolker E, Higdon R, Haynes W, Welch D, Broomall W, Lancet et al. MOPED: Model Organism Protein Expression D, Database.
Nucleic Acids Res 2012 40:D1093 9.
[8] Colangelo CM, Shifman M, Cheung KH, Stone KL, Carriero NJ, Gulcicek EE, et al. YPED: an integrated bioinformatics suite and database for mass spectrometry based proteomics research.
Genomics Proteomics Bioinformatics 2015 13:25 35.
[9] Shifman MA, Li Y, Colangelo CM, Stone KL, Wu TL, Cheung KH, et al. YPED: a web-accessible database system for protein expression analysis.
J Proteome Res 2007 6:4019 24.
[10] Wang M, Weiss M, Simonovic M, Haertinger G, Schrimpf SP, Hengartner MO, et al. PaxDb, a database of protein abundance averages across all three domains of life.
Mol Cell Proteomics 2012 11:492 500.
[11] Dinkel H, Chica C, Via A, Gould CM, Jensen LJ, Gibson TJ, et al. Phospho.ELM: a database of phosphorylation sites  update 2011.
Nucleic Acids Res 2011 39:D261 7.
[12] Hornbeck PV, Kornhauser JM, Tkachev S, Zhang B, Skrzypek E, Murray B, et al. PhosphoSitePlus: a comprehensive resource for investigating the structure and function of experimentally deter- mined post-translational modiﬁcations in man and mouse.
Nucleic Acids Res 2012 40:D261 70.
[13] Lee TY, Huang HD, Hung JH, Huang HY, Yang YS, Wang TH. DbPTM: an information repository of protein post-translational modiﬁcation. Nucleic Acids Res 2006 34:D622 7.
[14] Lu CT, Huang KY, Su MG, Lee TY, Bretana NA, Chang WC, et al. dbPTM 3.0: an informative resource for investigating substrate site speciﬁcity and functional association of protein post-translational modiﬁcations. Nucleic Acids Res 2013 41: D295 305.
[15] Gnad F, Ren S, Cox J, Olsen JV, Macek B, Oroshi M, et al. PHOSIDA (phosphorylation site database): management, struc- tural and evolutionary investigation, and prediction of phospho- sites.
Genome Biol 2007 8:R250. [16] Gnad F, Gunawardena J, Mann M. PHOSIDA 2011: the post-translational modiﬁcation database.
Nucleic Acids Res 2011 39:D253 60.
BIOINFORMATICS APPLICATIONS NOTE Vol.25 no.22 2009, pages 3038 3039 doi:10.1093/bioinformatics/btp529 Data and text mining Identifying related journals through log analysis Zhiyong Lu , Natalie Xie and W. John Wilbur National Center for Biotechnology Information (NCBI), 8600 Rockville Pike, Bethesda, MD 20852, USA Received on June 16, 2009 revised on August 12, 2009 accepted on August 31, 2009 Advance Access publication September 3, 2009 Associate Editor: Alfonso Valencia ABSTRACT Motivation: With the explosion of biomedical literature and the evolution of online and open access, scientists are reading more articles from a wider variety of journals.
Thus, the list of core journals relevant to their research may be less obvious and may often change over time.
To help researchers quickly identify appropriate journals to read and publish in, we developed a web application for ﬁnding related journals based on the analysis of PubMed log data.
Availability: http://www.ncbi.nlm.nih.gov/IRET/Journals Contact: luzh@ncbi.nlm.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION As a common practice, most of the scientists maintain familiarity with a small number of core journals to keep pace with the state of the art.
Such a list is typically developed through years of personal experience and is highly dependent on an individual s research interests.
In addition, the content of such a list may change over time.
A previous study has shown that scientists are reading more articles on average per year from a wider variety of journals: increasing from 13 journals in 1977 to 33 individual journals by 2005 (Tenopir, 2008). This is no surprise because similar types of articles are now published in a broader range of journals and journals are covering a wider variety of topics and publishing more articles beyond their deﬁned scope.
Not only does this make it difﬁcult for researchers to select journals for reading, it also makes for increasing difﬁculty deciding in which journal(s) to publish their own work (Schuemie and Kors, 2008). Thus, our objective is to suggest for users a list of current important journals related to journals they already know, so that researchers especially scholars who are not yet deeply familiar with an area of research (e.g. junior graduate students) may improve scholarly productivity.
To the best of our knowledge, very few systems/studies attempt to help scientists ﬁnd relevant journals.
The National Library of Medicine s (NLM s) Journals database provides a functionality that allows users to browse journals by discipline via Subject Terms: (cid:1) a set of MeSH journals by subject (e.g. biochemistry). However, Journal Subject Terms are frequently inadequate to ﬁnd relevant journals across discipline boundaries, thus it may fail to meet individual needs.
For instance, although Bioinformatics and Nucleic Acids Research headings designated for indexing MEDLINE (cid:1)   To whom correspondence should be addressed.
are two closely related journals, they are indexed by completely different Journal Subject Terms.
JANE (Journal/Author Name Estimator) is a web server previously developed to help (i) authors ﬁnd appropriate journals and (ii) editors ﬁnd potential reviewers (Schuemie and Kors, 2008). However, by design JANE ﬁnds related journals through the set of MEDLINE citations that share a similar context with the input text, thus a short textual input such as a journal title often cannot yield optimized results (e.g. several top returned journals are not closely related to Bioinformatics when the word is used as input). Similar ideas for ﬁnding related journals can be seen in eTBLAST (Errami et al., 2007). This work is also related to the use of clickthrough data to mine associations between items using techniques like collaborative ﬁltering (Adomavicius and Tuzhilin, 2005). In particular, this is similar to the research on developing recommendation systems for large-scale digital libraries (Smeaton and Callan, 2001). 2 FINDING JOURNALS OF INTEREST 2.1 Browsing through Journal Subject Terms At our web server, users can search for journals by browsing the same set of Journal Subject Terms.
The distinction lies in how the resulting journals are sorted once a speciﬁc Subject Term is clicked.
In addition to displaying journals alphabetically the default order in the NLM s Journals database we also list them by popularity, a measure determined by a journal s past usage.
2.2 Finding related journals Alternatively, users can enter a query in the search box.
Currently, the user can search a journal by its name, abbreviation or ISSN. The web server will return the bibliographic information of the requested journal such as its Publisher.
Furthermore, there is a hyperlink called Related Journals.
When clicked, it will display the 20 most related journals found by our approach (see below). 3 IMPLEMENTATION We collected one month s (March, 2008) worth of the PubMed logs, which include a total of 8 million user sessions (after removing robot sessions) and 51 million citation retrievals.
A citation retrieval is a speciﬁc MEDLINE record being clicked to display its corresponding bibliographic information and abstract text.
For each of the retrieval, we replaced it with its corresponding journal title in the dataset. A total of 15 827 journals (more journals Published by Oxford University Press on behalf of the US Government 2009.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:45 30/10/2009 Bioinformatics-btp529.tex] Page: 3038 3038 3039  than what is currently indexed in the Journals database) were found in the 8 million user sessions.
The usage (number of times a particular journal was accessed) differs signiﬁcantly between journals: some 1010 journals were heavily retrieved (over 10 000 times), while over 8000 journals were rarely viewed (less than 100 times). Our calculation of related journals is based on the existence of a set of user sessions si N i=1 where each user session si consists of a ni set di j=1 of citation retrievals in the form of MEDLINE records that were examined by the user during that session.
If A represents a journal, we will denote by tA(si) the number of click through events that represent articles from journal A.
We set j TA= N(cid:1) i=1 tA (si ) We may then estimate the probability of transitioning from an article in journal A to an article in journal B as ni 1 ni p (B A )= N(cid:1) tB (si ) ni 1 tA (si )tB (si ) (cid:3)(cid:2) (cid:3)(cid:2) tA (si ) TAni (cid:2) (cid:3) (cid:2) (cid:3) TA = N(cid:1) i=1 i=1 ni Here the factor tA(si) represents the probability that a user looking tA at a document from journal A is actually looking at a document tB(si) (ni 1) represents the probability that the in session si. The factor next document (among the ni  1 other documents represented in the session) that the user looks at will be a document from journal B.
Finally, the factor ni 1 represents the probability that the current record from journal A is not the last click through in the session.
In this computation, we have assumed a random order to the clicked records making up the session, as we do not believe the order itself is important.
In support of this assumption we remind the reader that the PubMed search engine retrieves documents in reverse time order of their entry into the database and there is a strong tendency for a user to click on them in that same order.
Also note that in this computation, in addition to journal popularity, we have also taken its topicality (the relation of a journal to a user need) into consideration.
4 USER STUDY AND EVALUTION To gain an understanding of how users respond to the new ranking of journals listed under the same Journal Subject Term (Section 2.1) and how accurately our approach can identify a customized list of related journals (Section 2.2), we conducted a user study by asking users to answer speciﬁc questions with regard to journals and their research interests (details in the Supplementary Material). A total of 29 participants were recruited via email and personal contacts.
They are comprised of graduate students, postdoctoral fellows or faculty members from a wide variety of biomedical subﬁelds (e.g. genetics). They were ﬁrst asked to identify their research ﬁeld(s) and one or more journals they access the most.
Next, they were asked to compare two different sorting strategies on the journal list indexed by Journal Subject Terms.
One is based on usage (popularity), whereas the other is based on the alphabetic order.
Finally, we asked them to evaluate the quality and usefulness of our computed journal list based on the journal they access the most.
Speciﬁcally, participants were asked to identify irrelevant journals Identifying related journals through log analysis as well as other relevant journals missing from our suggestion list.
As a result, both recall and precision can be computed for our journal suggestion lists.
Survey results are: ﬁrst, for the list retrieved by a Subject Term, all users favored results ranked by usage over the alphabetic order, suggesting that popularity is an important factor in users choice of favorite journals.
Second, for the list retrieved by relatedness, the recall and precision are 0.893 ( 0.102) and 0.910 ( 0.086), respectively, suggesting that our computed journal list is closely related to a user s information needs.
Third, all except one user favored our computed list over the list indexed by Subject Terms.
Finally, with regard to the usefulness of such a list of related journals, 40% of our users (12/29) reported that they found at least one journal from the suggestion list to be important to their research but absent from their current checklist.
All except two participants agreed that such a list is helpful, especially for scholars who are not yet deeply familiar with an area of science.
5 CONCLUSIONS AND DISCUSSION We provide a web application for ﬁnding appropriate journals for researchers to read and publish in.
Its unique feature of accurately identifying related journals is beyond the functionality of the NLM s Journals database and is complementary to other text mining tools such as JANE and eTBLAST. Despite high recall and precision, our system failed to satisfy some researchers in the reported user study.
No list of 20 journals could be guaranteed to include all important journals in an area.
We also observed that some journals like Nature were repeatedly present in our results because of their popularity, but they were not always favored (because of their diverse content). The web site is freely accessible and will be regularly updated.
Part of our system has become available in the NLM s Journals database.
The clickthrough data used in this research will be made available upon request after data anonymization, aggregation and transformation in accordance with proper user privacy protection.
ACKNOWLEDGEMENTS We would like to thank all the participants for their contribution to the user survey.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine.
Conﬂict of Interest: none declared.
REFERENCES Adomavicius,G. and Tuzhilin,A. (2005) Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions.
IEEE Trans.
Knowl. Data Eng., 17, 2005.
Errami,M. et al. (2007) eTBLAST: a web server to identify expert reviewers, appropriate journals and similar publications.
Nucleic Acids Res., 35, 2007.
Schuemie,M.J. and Kors,J.A. (2008) Jane: suggesting journals, ﬁnding experts.
Bioinformatics, 24, 727.
Smeaton,A. and Callan,J. (2001) Joint DELOS-NSF workshop on personalisation and recommender systems in digital libraries.
ACM SIGIR Forum, 35, 7 11.
Tenopir,C. (2008) Online Databases: Are E-Journals Good for Sciences.
Available (last accessed date at http://www.libraryjournal.com/article/CA6606485.html September 15, 2009). 3039 [17:45 30/10/2009 Bioinformatics-btp529.tex] Page: 3039 3038 3039
BIOINFORMATICS APPLICATIONS NOTE Vol.25 no.23 2009, pages 3181 3182 doi:10.1093/bioinformatics/btp554 Sequence analysis MOODS: fast search for position weight matrix matches in DNA sequences Janne Korhonen1, , Petri Martinmäki1, Cinzia Pizzi2, Pasi Rastas1 and Esko Ukkonen1 1Department of Computer Science and Helsinki Institute for Information Technology, University of Helsinki, Helsinki, Finland and 2Department of Information Engineering, University of Padova, Padova, Italy Received on July 14, 2009 revised on September 3, 2009 accepted on September 15, 2009 Advance Access publication September 22, 2009 Associate Editor: Alex Bateman ABSTRACT Summary: MOODS (MOtif Occurrence Detection Suite) is a software package for matching position weight matrices against DNA sequences.
MOODS implements state-of-the-art online matching algorithms, achieving considerably faster scanning speed than with a simple brute-force search.
MOODS is written in C++, with bindings for the popular BioPerl and Biopython toolkits. It can easily be adapted for different purposes and integrated into existing workﬂows. It can also be used as a C++ library.
Availability: The package with documentation and examples of usage is available at http://www.cs.helsinki.ﬁ/group/pssmﬁnd. The source code is also available under the terms of a GNU General Public License (GPL). Contact: janne.h.korhonen@helsinki.ﬁ 1 INTRODUCTION Position weight matrices (PWMs), also known as position-speciﬁc scoring matrices or weighted patterns, are a simple, yet important model for signals in biological sequences (Stormo et al., 1982). For example, they are widely used to model transcription factor binding sites in the DNA. Due to the vast amount of biological data, both in PWM and DNA databases, high-performance algorithms for matrix search are needed.
Recent theoretical developments into PWM search algorithms can be roughly categorized into two groups, the index-based algorithms and the online algorithms.
The index-based algorithms preprocess the target sequence into an index structure, typically a sufﬁx tree or a sufﬁx array, and use the index structure to facilitate quick search for matrix matches (Beckstette et al., 2006). The online algorithms, on the other hand, perform a simple sequential search over the target sequence.
Most state-of-the-art algorithms of this type are based on classical string matching algorithms (Liefooghe et al., 2009 Pizzi et al., 2007, 2009 Salmela and Tarhio, 2007 Wu et al., 2000). While index-based algorithms may offer signiﬁcantly faster search times, they also require a large amount of time and space for the construction of the index structure.
For this reason, online algorithms are generally more practical in most situations, as typical DNA databases offer only raw sequence data.
However, the work on advanced online algorithms has so far been mostly of theoretical nature, and no implementation packages intended for end-users   To whom correspondence should be addressed.
have been published.
To ﬁll this gap, we have implemented a suite of efﬁcient algorithms, called Motif Occurrence Detection Suite (MOODS). MOODS implements the algorithms developed in Pizzi et al. (2007, 2009), where also an extensive performance comparison of the new and old algorithms is reported.
MOODS can be used as an extension to various scripting languages popular in bioinformatics. So far we have implemented bindings for the BioPerl (http://www.bioperl.org) and Biopython (http://www.biopython.org Cock et al. 2009) toolkits. 2 ALGORITHMS AND IMPLEMENTATION The core of MOODS is formed by the search algorithms themselves, implemented in C++ and making use of the C++ Standard Template Library.
The package contains the following algorithms described and experimentally compared in detail in Pizzi et al. (2009):  The lookahead ﬁltration algorithm (LF) and its multi-matrix version [multi-matrix lookahead ﬁltration algorithm (MLF)]. For a given input PWM M, these algorithms ﬁrst ﬁnd the statistically most signiﬁcant submatrix (i.e. the most selective submatrix against the background) of ﬁxed length h, called the scanning window of M. Then the target DNA sequence is scanned with a ﬁnite state automaton that ﬁnds subsequences that score well against the scanning window.
The full score against M is calculated only at these sequence positions.
Scanning with the ﬁnite state automaton takes O(n) time, where n is the length of the DNA sequence, leading to nearly linear overall performance.
The memory requirement of the ﬁnite state automaton is limited by the length h of the scanning window.
In the multi-matrix variant, we combine all the automata into a single automaton, making it possible to efﬁciently ﬁnd matches for a large PWM set in just one pass over the sequence.
The naive super-alphabet algorithm (NS), which is as the naive matching algorithm, but uses a large alphabet consisting of tuples of original alphabet symbols.
It works well for very long matrices ( 30 bp). The MLF algorithm is most suitable for PWM search tasks in practice and has the best overall performance out of the algorithms of MOODS.
For completeness, we have also included implementations of the naive algorithm, which directly evaluates the matrix score at all sequence positions, and the permutated lookahead algorithm  The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:24 3/11/2009 Bioinformatics-btp554.tex] Page: 3181 3181 3182  J.Korhonen et al. (Wu et al., 2000). In addition, the package contains the well-known dynamic programming algorithm for converting P-values into score thresholds (Staden, 1989 Wu et al., 2000). MOODS uses the standard scoring model (log-odds against the background distribution) of PWMs, as described, e.g. in Pizzi et al. (2009). A user can specify the pseudocounts for the calculation of log-odds scores from matrices.
This calculation can also account for the background distribution of the alphabet in the DNA sequence, which can be speciﬁed by the user or estimated directly from the sequence.
The scoring thresholds can be speciﬁed via P-values or as absolute thresholds.
The package includes Perl and Python interfaces to the algorithms, making use of the respective bioinformatics toolkits. These interfaces can utilize classes from the existing toolkits as input and return the results as Perl or Python data structures.
We have tested our software on Linux with gcc C++ compiler.
It should be usable on any UNIX-like operating system supported by gcc and either BioPerl or Biopython. 3 DISCUSSION With BioPerl and Biopython interfaces, the MOODS algorithms can easily be included into existing workﬂows. Likewise, scripts can be written to use the implemented algorithms for speciﬁc purposes.
Existing facilities can be used to load sequences from formatted ﬁles or to fetch data from online databases.
The results can then be processed further, for example, to ﬁnd subsequences with statistically signiﬁcant amounts of matches.
On the other hand, the C++ algorithm implementations can also be directly integrated into existing or new software, thanks to the open source licensing.
The MOODS web page (http://cs.helsinki.ﬁ/group/pssmﬁnd) provides several example scripts, as well as a simple C++ program for basic usage and as an example of C++ integration.
To benchmark the performance of our package, we tested the naive algorithm, the permutated lookahead algorithm and the MLF algorithm with real biological data.
We did similar benchmark also for the Motility library (part of the Cartwheel bioinformatics toolkit Brown et al. 2005), TFBS BioPerl extension (Lenhard and Wasserman, 2002) and Biopython s built-in PWM matching algorithm.
These packages all use the naive algorithm.
The test setup was as follows.
We used matrices from the TRANSFAC public database (Matys et al., 2003) as our matrix set, containing a total of 398 matrices.
The target sequences were taken from the human genome.
We matched both the original matrices and their reverse complements against the sequences, in effect searching both strands of the DNA. This means that the MLF algorithm scanned for 796 matrices simultaneously.
We ran the tests on a 3.16 GHz Intel Core 2 Duo desktop computer with 2 GB of main memory, running Linux operating system.
The results of our tests are displayed in Table 1.
The results illustrate the advantages of carefully tuned C++ algorithm implementations and also indicate that more advanced algorithms offer practical beneﬁts. We also tested matching the TRANSFAC matrices against both strands of the whole human genome with 6, using the MLF algorithm.
The total scanning time P-value 10 was about 42.1 min, with the number of matches being 29 354 584.
Overall, implementations perform well even on large datasets. these experiments indicate that our Table 1.
Algorithm benchmarks 600k 6 10 P-value MOODS Naive algorithm Permutated lookahead MLF 6.5 s 3.8 s 0.4 s  4 10 7.3 s 6.3 s 1.1 s Chr20 6 10 689 s 405 s 16.0 s  4 10 782 s 677 s 117 s TFBS Motility Biopython Matches 20.4 s 103 s 42 min 952 53.1 s 103 s 41 min 7.3 104  180 min  1.1 105  181 min  6.7 106 We used two target sequences: 600k is a 600 kb long human DNA fragment, and Chr20 is the 62 Mb long human chromosome 20.
The total scanning times for each algorithm or package are given, with   indicating that the dataset was too large to be processed.
The reported times include the construction of the data structures required in scanning as well as the scanning itself.
The matches row gives the total number of matches found for each P-value.
Funding: Academy of Finland (grant 7523004, Algorithmic Data the European Union s Sixth Framework Programme Analysis) (contract LSHG-CT-2003-503265, BioSapiens Network of Excellence). Conﬂict of interest: none declared.
REFERENCES Beckstette,M. et al. (2006) Fast index based algorithms for matching position speciﬁc scoring matrices.
BMC Bioinformatics, 7, 389.
Brown,C.T. et al. (2005) Paircomp, FamilyRelationsII and Cartwheel: tools for interspeciﬁc sequence comparison.
BMC Bioinformatics, 6, 70.
Cock,P.J.A. et al. (2009) Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics, 25, 1422 1423.
Lenhard,B. and Wasserman,W.W. (2002) TFBS: computational framework for transcription factor binding site analysis.
Bioinformatics, 18, 1135 1136.
Liefooghe,A. et al. (2009) Self-overlapping occurrences and Knuth-Morris-Pratt algorithm for weighted matching.
In Proceedings of Third International Conference on Language and Automata Theory and Applications (LATA), Vol.5457 of Lecture Notes in Computer Science, Springer, Tarragona, Spain, pp.
481 492.
Matys,V. et al. (2003) TRANSFAC(R): transcriptional regulation, from patterns to proﬁles. Nucleic Acids Res., 31, 374 378.
Pizzi,C. et al (2007) Fast search algorithms for position speciﬁc scoring matrices.
In Proceedings of Bioinformatics Research and Development Conference (BIRD), Vol.4414 of Lecture Notes in Bioinformatics. Springer, Berlin, Germany, pp.
239 250.
Pizzi,C. et al. (2009) Finding signiﬁcant matches of position weight matrices in linear time.
IEEE/ACM Trans Comput. Biol.
Bioinform. (in press). Salmela,L. and Tarhio,J. (2007) Algorithms for weighted matching.
In Proceedings of International Symposium on String Processing and Information Retrieval (SPIRE), Vol.4726 of Lecture Notes in Computer Science.
Springer, Santiago, Chile, pp.
276 286.
Staden,R. (1989) Methods for calculating the probabilities of ﬁnding patterns in sequences.
Comput. Appl. Biosci., 5, 89 96.
Stormo,G.D. et al. (1982) Use of the perceptron algorithm to distinguish translational initiation sites in e. coli. Nucleic Acids Res., 10, 2997 3012.
Wu,T.D. et al. (2000) Fast probabilistic analysis of sequence function using scoring matrices.
Bioinformatics, 16, 233 244.
3182 [20:24 3/11/2009 Bioinformatics-btp554.tex] Page: 3182 3181 3182
BIOINFORMATICS APPLICATIONS NOTE Vol.25 no.22 2009, pages 3026 3027 doi:10.1093/bioinformatics/btp523 Systems biology Saint: a lightweight integration environment for model annotation Allyson L. Lister1,2, , Matthew Pocock2, Morgan Taschuk1,2 and Anil Wipat1,2, 1Centre for Integrated Systems Biology of Ageing and Nutrition, Newcastle University and 2School of Computing Science, Newcastle University, Newcastle Upon Tyne, UK Received on June 8, 2009 revised on August 4, 2009 accepted on August 26, 2009 Advance Access publication September 7, 2009 Associate Editor: Alex Bateman ABSTRACT Summary: Saint is a web application which provides a lightweight annotation integration environment for quantitative biological models.
The system enables modellers to rapidly mark up models with biological information derived from a range of data sources.
Availability and Implementation: Saint is freely available for use on the web at http://www.cisban.ac.uk/saint. The web application is implemented in Google Web Toolkit and Tomcat, with all major browsers supported.
The Java source code is freely available for download at http://saint-annotate.sourceforge.net. The Saint web server requires an installation of libSBML and has been tested on Linux (32-bit Ubuntu 8.10 and 9.04). Contact: helpdesk@cisban.ac.uk a.l.lister@ncl.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Quantitative modelling is at the heart of systems biology.
Model description languages such as the Systems Biology Markup Language (SBML Hucka et al., 2003) or CellML (Lloyd et al., 2008) allow the relationships between biological entities to be captured and the dynamics of these interactions to be described mathematically.
Currently, however, many dynamic models include only the mathematical information required to run simulations, and do not explicitly contain the full biological context.
The efﬁcient exchange, reuse and integration of models is aided by the presence of biological information in the model.
Model annotations are necessary to describe how the model has been generated and to deﬁne the meaning of the components that make up the model in a computationally accessible fashion.
If biological information about a model is added consistently and thoroughly, the model becomes useful not just for simulation, but also as an input in other computational tasks and as a reference for researchers.
Without a biological context, models are only easily understandable by their creators.
Interested third parties must rely on extra documentations such as publications or possibly incomplete descriptions of species and reactions included in an SBML element s name or identiﬁer. Without computationally amenable biological annotations, it is difﬁcult to create software tools that determine the biological pathway or reaction being modelled. To whom correspondence should be addressed.
The addition of biological annotations to a model is usually a manual, time-consuming process there is no single resource that encompasses all suitable data sources.
A modeller usually has to visit many web sites, applications and interfaces in order to identify relevant information, and may not be aware of all potentially useful databases.
Because modellers add information manually, it is very difﬁcult to annotate exhaustively.
Whilst annotations are vital for model sharing and reuse, they do not contribute to the mathematical content of a model and are not critical to its successful functioning.
The addition of biological knowledge must be performed quickly and easily in order to make annotation worthwhile to a modeller. A large number of tools1 are available for the construction, manipulation and simulation of models, but there is currently a lack of tools to facilitate rapid and systematic model annotation.
While web sites and applications specializing in integrating disparate data sources exist, such as BioMart (Smedley et al., 2009) and Pathway Commons,2 none are designed to put information directly into a model.
In this article, we describe a lightweight SBML model annotation tool called Saint, speciﬁcally designed to identify and integrate biological information relevant to computational models.
Saint is an application which supports the addition of basic annotation to SBML entities and identiﬁes new reactions which may be valuable for extending a model.
Whilst the addition of biological annotation does not modify the behaviour of the model, the incorporation of new reactions or species adds new features that can later be built upon to potentially change the model s output.
2 IMPLEMENTATION On the client side, Saint is a web application implemented in Google Web Toolkit3 and hosted on a Tomcat4 server, with a query translation service connecting to a number of external web services running on the server side.
New annotation is presented to the user in a single integrated view after retrieval by the server-side queries.
Reactions and associated species are added directly to the SBML model, whereas the majority of the remaining biological annotation is added to Annotation elements according to the Minimal Information Required in the Annotation of Biochemical Models speciﬁcation (Le Novère et al., 2005). (MIRIAM) 1http://sbml.org/SBML_Software_Guide 2http://www.pathwaycommons.org 3http://code.google.com/webtoolkit 4http://tomcat.apache.org  The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:46 26/10/2009 Bioinformatics-btp523.tex] Page: 3026 3026 3027  MIRIAM annotations are resource annotations that are added to SBML in a standardized way which link external resources such as ontologies and data sources to a model.
MIRIAM, among other things, deﬁnes an annotation scheme accessible via web services which speciﬁes the format and set of standard data types which should be used for these URIs (Laibe and Le Novere, 2007). The use of the MIRIAM format provides a standard structure for explicit links between the mathematical and biological aspects of an SBML model.
Saint facilitates the biological annotation of SBML models by using query translation to present an integrated view of data sources and suggested ontological terms.
Data sources include UniProtKB (The UniProt Consortium, 2008), STRING (Jensen et al., 2008) and Pathway Commons.
Supported ontologies and standards include MIRIAM, the Systems Biology Ontology (SBO Le Novère, 2006) and Gene Ontology (GO Ashburner et al., 2000). Query translation within Saint occurs when the query for each species is translated into a set of queries over these resources web services.
Data are matched to a species through syntactic equivalence between the query term and the external data source.
The combined query results are then displayed in the web browser.
If a model is valid, Saint displays the parts of the model available for annotation.
The display is organized around species, which are the main target of annotation.
Saint makes use of the Google Web Toolkit to provide both asynchronous calls to external resources and cross-browser compatibility.
New annotation can be viewed by the modeller, even if the other species are not annotated yet.
The modeller can select or delete annotations as it suits their model, or hide entire species from consideration.
When the modeller is satisﬁed with the new state of the model, it can be converted back to SBML and saved.
Parsing and validation of the models are handled with libSBML (Bornstein et al., 2008). As an example, a Saccharomyces cerevisiae model containing a species with a single, simple identiﬁer of cdc13 is loaded into Saint.
Saint suggests the SBO term macromolecule (SBO:0000245), which is added as an sboTerm attribute of that species element, as the best SBO match to a protein.
This term was suggested both because cdc13 was found within UniProtKB and because the Pathway Commons interaction set identiﬁed the species as a protein.
Saint also suggests the UniProtKB accession P32797, and GO terms including nuclear telomere cap complex (GO:0000783) and single-stranded telomeric DNA binding (GO:0043047) as retrieved from UniProtKB. This information is stored within the model via MIRIAM annotations.
Extensions to the model are also suggested.
For each species, new reactions and their associated species and species references are retrieved from both Pathway Commons and STRING.
More examples and comparisons are available in the Supplementary Material.
3 DISCUSSION To date, there are few tools available for automating the retrieval and integration of data for the annotation of SBML models.
The Saint application was developed as an interactive web tool to annotate Saint models with new MIRIAM resources and reactions, keeping track of data provenance so that the modeller can make an informed decision about the quality of the suggested annotation.
The system makes it easy for modellers to add explicit biological knowledge to their models, increasing a model s usefulness both as a reference for other researchers and as an input for further computational analysis.
A small number of similar tools are available.
SemanticSBML5 provides MIRIAM annotations via a combination of data warehousing and query translation via web services as part of a larger application.
The Java library libAnnotationSBML (Swainston and Mendes, 2009) uses query translation to provide annotation functionality with a minimal user interface.
Unlike libAnnotationSBML, Saint is accessible through an easy-to-use web interface and unlike both tools is unique in its ability to add new reactions and associated species.
Saint is under active development.
Future enhancements will include the addition of new data sources and ontologies, annotation of elements other than species and reactions and support for other modelling formalisms such as CellML. ACKNOWLEDGEMENTS We acknowledge the support of Newcastle University Systems Biology Resource Centre (SBRC) and Newcastle University Bioinformatics Support Unit.
Funding: BBSRC/EPSRC funding for CISBAN (BB/C008200/1 to A.L.L., M.T. and A.W.) BBSRC ONDEX project (BB/F006063/1 to M.P). Conﬂict of Interest: none declared.
REFERENCES Ashburner,M. et al. (2000) Gene ontology: tool for the uniﬁcation of biology.
The Gene Ontology Consortium.
Nat. Genet., 25, 25 29.
Bornstein,B.J. et al. (2008) LibSBML: an API Library for SBML. Bioinformatics, 24, 880 881.
Hucka,M. et al. (2003) The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models.
Bioinformatics, 19, 524 531.
Jensen,L.J. et al. (2008) STRING 8 a global view on proteins and their functional interactions in 630 organisms.
Nucleic Acids Res., 37, D412 D416. Laibe,C. and Le Novere,N. (2007) MIRIAM Resources: tools to generate and resolve robust cross-references in Systems Biology.
BMC Syst. Biol., 1, 58.
Le Novère,N. et al. (2005) Minimum information requested in the annotation of biochemical models (MIRIAM). Nat. Biotechnol., 23, 1509 1515.
Le Novère,N. (2006) Model storage, exchange and integration.
BMC Neurosci., 7(Suppl. 1), S11. Lloyd,C.M.M. et al. (2008) The CellML Model Repository.
Bioinformatics, 24, 2122 2123.
Smedley,D. et al. (2009) Biomart biological queries made easy.
BMC Genomics, 10, 22.
Swainston,N. and Mendes,P. (2009) libAnnotationSBML: a library for exploiting SBML annotations.
Bioinformatics, 25, 2292 2293.
The UniProt Consortium (2008) The universal protein resource (UniProt). Nucleic Acids Res., 36 (Database issue), D190 D195. 5http://sysbio.molgen.mpg.de/semanticsbml 3027 [09:46 26/10/2009 Bioinformatics-btp523.tex] Page: 3027 3026 3027
BIOINFORMATICS APPLICATIONS NOTE doi:10.1093/bioinformatics/btn116 Vol.24 no.10 2008, pages 1310 1312 Structural bioinformatics MTMDAT: Automated analysis and visualization of mass spectrometry data for tertiary and quaternary structure probing of proteins Janosch Hennig1,*, Klaus D. M. Hennig2 and Maria Sunnerhagen1 1Department of Physics, Chemistry, and Biology, Linko ping University, SE-581 83 Linko ping, Sweden and 2Department of Informatics, Gabriele-von-Bu low Gymnasium, DE-13509 Berlin, Germany Received on February 19, 2008 revised on March 14, 2008 accepted on March 29, 2008 Advance Access publication April 3, 2008 Associate Editor: Anna Tramontano ABSTRACT Summary: In structural biology and -genomics, nuclear magnetic resonance (NMR) spectroscopy and crystallography are the methods of choice, but sample requirements can be hard to fulfil. Valuable structural information can also be obtained by using a combination of limited proteolysis and mass spectrometry, providing not only knowledge of how to improve sample conditions for crystallization trials or NMR spectrosopy by gaining insight into subdomain identities but also probing tertiary and quaternary structure, folding and stability, ligand binding, protein interactions and the location of post-translational modifications.
For high-throughput studies and larger proteins, however, this experimentally fast and easy approach produces considerable amounts of data, which until now has made the evaluation exceedingly laborious if at all manually possible.
MTMDAT, equipped with a browser-like graphical user interface, accelerates this evaluation manifold by automated peak picking, assignment, data processing and visualization.
Availability: MTMDAT can be downloaded from the following page: http://www.cms.liu.se/chemistry/molbiotech/maria_sunnerha- gens_group/mtmdat by clicking on the corresponding links (windows- or unix-based) together with the manual and example files.
The program is free for academic/non-commercial purposes only.
Contact: janhe@ifm.liu.se 1 INTRODUCTION Limited proteolysis detected by SDS-page has long been used to gain information on domain identities and quaternary rearrangements in larger proteins (Carey, 2000). The introduc- tion of highly resolved mass spectrometry (MS), especially matrix-assisted laser desorption/ionization (MALDI)-time-of- flight (TOF)-MS, enabled more detailed protein characteriza- tion, including per-residue-mapping of protein DNA (Cohen et al., 1995), protein protein (Lacy et al., 2005) and protein- surface interactions (Lundqvist et al., 2005) by following the degradation of unstable or accumulation of stable protein *To whom correspondence should be addressed.
fragments during a time-course experiment.
Despite these promising results, the richness in detail is limited and larger proteins were hardly accessible using this method due to the amount of data generated.
Recently, we published an algorithm where a residue-wise relative cleavage propensity is calculated from MS spectra (Hennig et al., 2005). The cleavage propensity visualizes the tertiary or quaternary structure probing of proteins and/or their complexes in more detail by localized mapping of proteolytic protection in protein intra- and interactions.
The required data can be rapidly recorded by MALDI-TOF-MS in a matter of hours.
However, the large body of data acquired in such experiments presents a problem for manual evaluation since it increases exponentially with the size of the proteins being studied.
To this end we developed MTMDAT to assist the data evaluation process.
The time needed from recording the data to presenting the information content is thereby reduced from weeks to hours.
In a most recent study (Hennig et al., 2008), we used this method and tool to determine subdomain interaction sites.
The identified residues were used as input for data-driven docking of these subdomains and yielded valuable results.
Thus, this method in conjunction with MTMDAT provides a high-throughput method to screen for protein complex structures without the time-consuming process of sample conditioning and optimiza- tions. In addition, domain identification can be done very quickly, which can facilitate successful crystallization trials in structural genomics, and the identification of inter-domain interactions can picture their arrangement.
The usefulness of MTMDAT can even be exploited in proteomic studies, since it provides the user with the possibility to screen for post-trans- lational modifications and to determine their exact location on the protein.
2 WORKFLOW The general workflow is depicted in Figure 1.
After performing the actual limited proteolysis experiment, each sampled time point is analysed by MS. Spectra need then to be converted to standard text-(ASCII) files for MTMDAT. MTMDAT requires the sequence of the protein to be analysed, which can be uploaded or pasted into the appropriate ß 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
field in one-letter-code, where any other characters than those assigned to the 20 essential amino acids are automatically filtered out.
The protease used can be chosen from a pulldown menu.
All following steps are simply performed by pressing the browser-like forward button five times.
First, your protein will be cleaved virtually, and all possible fragments are listed from the shortest to the uncleaved full-length protein, together with the position in the entire sequence, the average mass, the monoisotopic mass and the sequence of the fragment with highlighted cleavage sites.
It is also possible to include protein modifications, such as thiol groups modified with mercaptoethanol or acrylamide, acetylation and many more.
Modifications not present in the current version can easily be implemented upon request.
The user can now upload MS spectra to be analysed. All information available from the file will be displayed.
A noise filter makes it possible for MTMDAT to pick peaks and distinguish them from noise.
This noise-filter can be applied automatically and manually.
The automatic noise filter is based on common statistical analysis of all data pairs the mass spectrometer generates.
If, for any reason (e.g. spectra without baseline correction), the user chooses to manually apply a noise filter (e.g. raise or lower the noise level) it can be done in a separate parameter window.
Then the peaks are picked and listed in the main window with their corresponding intensity and mass-to-charge ratio.
MTMDAT compares the generated peak list with the virtual fragment list made in the beginning and assigns the experimentally derived peaks to their corres- ponding fragment mass.
A peak assignment window appears, where all assigned peaks are listed.
The user can interactively check the assignment and correct it in case of need.
3 THE OUTPUT AND 3D VISUALIZATION in the assignment the sample and the calculated intensity of For each fragment list, a file will be generated.
The file itself consists of xy-data, namely the time point of the fragments peak relative to all other peaks in the assignment list.
For all subsequently processed spectra of following time points, the xy-data will be saved automatically into the same files given that the user uses the same file name root.
Thus, for each fragment MTMDAT produces files, which can be used in any plotting program to plot the change in relative abundance of this peptide fragment over time.
After evaluating all time-resolved MALDI spectra in a single limited proteolysis experiment, MTMDAT can produce a plot file for 3D visualization of relative cleavage propensities.
Here, MTMDAT takes all the previously produced time-course plot files and calculates the relative cleavage propensity based on the algorithm in Hennig et al. (2005) (an example of such a plot is shown in Fig.1). MTMDAT can produce these plot files in several different arrangements for different plotting programs.
Furthermore, the user can choose if only cleavable residues are shown on the horizontal axis or the entire sequence.
MTMDAT Fig.1.
Workflow scheme of limited proteolysis experiments analysed by MS and MTMDAT. (1) From the reaction tube to the mass spectrometer, evaluated by MTMDAT (2), doing the peak picking and the peak assignment (3) and produces output for plotting in various ways (4). modify all relevant program parameters, such as the noise filter parameters or protein modifications (resulting in different masses in the virtual fragment list, e.g. with -mercaptoethanol modified cysteines, etc). The browser-like usability enables the user to go back a step if one would like to change parameters.
MTMDAT comes as a software written in Java, relying on jre1.6.0 or later.
5 APPLICABILITY Evaluating the easily accessible but overwhelmingly large body of MALDI data from proteolysis experiments has until now been hampered by the lack of appropriate software.
Recently, we successfully applied MTMDAT not only to protein subdomain interactions and protein ligand interactions, but also deter- mined the origin of stability increase due to Zn2þ -binding (Hennig et al., 2008). This provides a high-throughput approach to access limited proteolysis data for larger proteins.
We are convinced that MTMDAT will prove helpful to a wide range of scientists in the structural genomics field.
ACKNOWLEDGEMENTS The authors thank Martin Karlsson for helping in the validation process of MTMDAT. Funding: This work was supported by grants to M. S. from the Swedish Research Council, Swedish Foundation for Strategic Research, and Linko ping University.
Instrumentation was funded by the Wallenberg foundation.
Conflicts of Interest: none declared.
4 GRAPHICAL USER INTERFACE A graphical user interface (GUI) is used to simplify and streamline the setup of an evaluation.
The GUI enables one to REFERENCES Carey,J. (2000) A systematic and general proteolytic method for defining structural and functional domains of proteins.
Methods Enzymol., 328, 499 514.
1311  autoantigenic epitope in Sjgren syndrome, and is an integral and conserved region in TRIM proteins.
J. Mol. Biol., 377, 431 449.
Lacy,E.R. et al. (2005) Molecular basis for the specificity of p27 toward cyclin-dependent kinases that regulate cell division.
J. Mol. Biol., 349, 764 773.
Lundqvist,M. et al. (2005) Proteolytic cleavage reveals interaction patterns between silica nanoparticles and two variants of human carbonic anhydrase. Langmuir, 21, 11903 11906.
J.Hennig et al. Cohen,S.L. et al. (1995) Probing the solution structure of the DNA-binding protein Max by a combination of proteolysis and mass spectrometry. Protein Sci., 4, 1088 1099.
Hennig,J. et al. (2005) Structural organization and Zn2þ -dependent subdomain interactions involving autoantigenic epitopes in the RING-B-box-Coiled-coil (RBCC) region of Ro52. J. Biol.
Chem., 280, 33250 33261.
Hennig,J. et al. (2008) The fellowship of the RING: the RING-B-box linker region 5 (RBL) interacts with the RING in TRIM21/Ro52, contains a native 1312
BIOINFORMATICS APPLICATIONS NOTE Vol.26 no.18 2010, pages 2345 2346 doi:10.1093/bioinformatics/btq423 Systems biology PathwayAccess: CellDesigner plugins for pathway databases John L. Van Hemert1,2 and Julie A. Dickerson1,2, 1Bioinformatics and Computational Biology and 2Electrical and Computer Engineering Department, Iowa State University, Ames, IA, 50011, USA Associate Editor: Alfonso Valencia Advance Access publication July 20, 2010 ABSTRACT Summary: CellDesigner provides a user-friendly interface for graphical biochemical pathway description.
Many pathway databases are not directly exportable to CellDesigner models.
PathwayAccess is an extensible suite of CellDesigner plugins, which connect CellDesigner directly to pathway databases using respective Java application programming interfaces.
The process is streamlined for creating new PathwayAccess plugins for speciﬁc pathway databases.
Three PathwayAccess plugins, MetNetAccess, BioCycAccess and ReactomeAccess, directly connect CellDesigner to the pathway databases MetNetDB, BioCyc and Reactome. PathwayAccess plugins enable CellDesigner users to expose pathway data to analytical CellDesigner functions, curate their pathway databases and visually integrate pathway data from different databases using standard Systems Biology Markup Language and Systems Biology Graphical Notation.
Availability: Implemented in Java, PathwayAccess plugins run with CellDesigner version 4.0.1 and were tested on Ubuntu Linux, Windows XP and 7, and MacOSX. Source code, binaries, documentation and video walkthroughs are freely available at http://vrac.iastate.edu jlv Contact: julied@iastate.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on November 11, 2009 revised on June 18, 2010 accepted on July 16, 2010 1 INTRODUCTION CellDesigner (Funahashi et al., 2008) is a tool for graphically building biochemical pathway models, which integrate model representation by Systems Biology Markup Language (SBML Hucka et al., 2003) with graphical representation by Systems Biology Graphical Notation (SBGN Moodie et al., 2009). There exist many databases providing application programming interface (API) libraries enabling programmatic queries.
These API libraries include many biologically meaningful objects, which carry out intuitive functions.
For example, a pathway object can report the set of reaction objects it contains, a protein complex object can report the monomer objects that contsruct it and a metabolite object might report its SMILES and InChi codes.
The problem is that a Pathway object in one API is not the same as a Pathway object in the API of a different database.
The same biological concept is represented using independently developed in-silico representations, preventing any single application from communicating and integrating across databases.
To whom correspondence should be addressed.
Fig.1.
Dataﬂow for PathwayAccess plugins.
PathwayAccess plugins use respective APIs to communicate with different pathway databases and integrate data in CellDesigner. As indicated by arrows, depending on functionality supported by the datasource, dataﬂow is uni- or bi-directional.
2 FUNCTIONALITY PathwayAccess plugins directly interact with pathway databases so that the user can download one or more pathways to a CellDesigner model and upload (or commit) a CellDesigner model to a database.
Figure 1 shows a dataﬂow diagram for typical use of the PathwayAccess plugins.
The PathwayAccess plugin framework confers three major beneﬁts, depending on whether individual database APIs support data retrieval and modiﬁcation. First, the plugins make pathways stored in remote databases available to the powerful modeling and simulation functionality already provided by CellDesigner. Second, SBGN implemented by CellDesigner provides a standard representation for biologists to curate pathway databases the user can create a pathway model and commit it to the database of his choice.
A user can also download a pathway model from a database, edit it and commit it back to the database, either replacing the original pathway or creating a different version.
Third, CellDesigner can be effective in visually comparing and integrating pathway data from one or many different databases metabolic networks can be downloaded directly into CellDesigner and integrated into custom super-pathways.
CellDesigner can export pathways into ﬁles for loading into other software such as Cytoscape (Shannon et al., 2003), where SBGN is an ancilliary feature to network analysis functions.
Since CellDesigner and most datasources user interfaces provide good automatic layouts, layouts are left to the datasources and CellDesigner independently.
2.1 Pathway integration across databases When PathwayAccess plugins download pathways, they are integrated with the growing model in memory.
CellDesigner is suited to support integration because it uses the XML-based SBML data model not only for ﬁle storage, but also for objects in memory ideal for representing annotations integrated from different sources.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:45 11/8/2010 Bioinformatics-btq423.tex] Page: 2345 2345 2346  J.L.Van Hemert and J.A.Dickerson Among other annotations, PathwayAccess stores synonyms this way, enabling it to match integrated objects in the same subcellular compartment that may be named differently across databases.
To prevent duplicate reactions in integrated pathways, a reaction hashing algorithm calculates a unique integer for every combination of reaction substrates, products and catalysts (see Supplementary Material). Each PathwayAccess plugin has a unique, but editable highlight color, which can be used to color the model objects downloaded using that plugin.
Objects from multiple databases are colored by mixing the colors of the plugins that downloaded them.
2.2 Creating new PathwayAccess plugins The PathwayAccess framework includes a core library plus one or more independent plugins.
A plugin developer can easily create a new CellDesigner plugin that communicates with any pathway database providing a Java API. Simply create a new CellDesigner plugin object using the PathwayAccess library and deﬁne a set of simple database query operations, depending on whether the plugin will support download and/or saving a model to the database.
To create a PathwayAccess plugin that downloads a pathway, deﬁne 18 simple functions such as to get the synonyms of an object (pathway, metabolite, gene, etc). To design a commit feature, deﬁne nine simple functions such as add substrates to a reaction in the database.
With these simple operations deﬁned for communicating with a database, PathwayAccess handles all interaction both with CellDesigner and the database, similarly to Cytoscape s Data Integration Request For Comments (Killcoyne et al., 2009), and provides a way to enrich objects beyond the annotation used for integration.
2.3 Examples Three PathwayAccess plugins, MetNetAccess, BioCycAccess and ReactomeAccess were created.
In addition to representing biological objects differently, each uses a different communication protocol: SQL, Sockets and Web Services, respectively.
2.3.1 BioCycAccess: download and commit to a Pathway Genome Database BioCyc databases are individually deployed for speciﬁc organisms and purposes (http://www.biocyc.org Karp et al., 2005). BioCycAccess uses JavaCycO, our new library wrapped around the JavaCyc API (Krummenacker et al., 2005 Mueller et al., 2005), running in client mode to connect to a BioCyc Pathway Genome Database that is running JavaCycO in server mode.
It supports both downloading and committing pathways.
2.3.2 ReactomeAccess: download from Reactome Reactome is a large repository for pathways (Matthews et al., 2009). ReactomeAccess supports downloading pathways from Reactome directly into CellDesigner models via an API wrapped around Reactome s web services.
download 2.3.3 MetNetAccess: to MetNet MetNetAccess provides CellDesigner access to the pathway database MetNetDB using MetNetAPI (Sucaet et al., 2010), which is wrapped around SQL queries.
It supports both downloading and committing pathways.
MetNetDB is an integrated pathway database commit and that currently includes Arabidopsis thaliana, yeast, soybean and the grapevine.
MetNetAccess has been used to curate many pathways for different organisms in MetNetDB (Wurtele et al., 2007). MetNet allows public downloading of data, but only registered curators may modify data in MetNetDB. 3 IMPACT The PathwayAccess suite of CellDesigner plugins is a powerful tool for researchers who work with metabolic pathway data and wish to take advantage of graphical and computational CellDesigner features.
By directly accessing and publishing to pathway databases, decentralized pathway integration and comparison is made possible over simply saving and loading SBML ﬁles. Although three PathwayAccess plugins have been released, the practical scope of the PathwayAccess library is as wide as the number of databases to which CellDesigner can connect because communication requires a Java API. MetNetAccess, BioCycAccess, ReactomeAccess and future PathwayAccess plugins enable CellDesigner users to expose pathway data to analytical CellDesigner functions as well as visually integrate and curate pathway data from different databases using standard SBGN something that has been previously prevented by disparate in silico representations of biological objects.
ACKNOWLEDGEMENTS We thank Yves Sucaet and Eve Wurtele for developing MetNetAPI and Jesse Walsh for testing JavaCycO. Funding: National Science Foundation (grants DBI 0604755 and EEC 0813570). Conﬂict of Interest: none declared.
REFERENCES Funahashi,A. et al. (2008) Celldesigner 3.5: a versatile modeling tool for biochemical networks.
Proc. IEEE, 96, 1254 1265.
Hucka,M. et al. (2003) The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models.
Bioinformatics, 9, 524 531.
Karp,P.D. et al. (2005) Expansion of the BioCyc collection of pathway/genome databases to 160 genomes.
Nucleic Acids Res., 19, 6083 6089.
Killcoyne,S. et al. (2009) Cytoscape Data Integration RFC. Request for comments, Cytoscape. Available at http://cytoscape.wodaklab.org/wiki/DataIntegration (last accessed date July 28, 2010). Krummenacker,M. et al. (2005) Querying and computing with BioCyc databases.
Bioinformatics, 21, 3452 3455.
Matthews,L. et al. (2009) Reactome knowledgebase of biological pathways and Moodie,S. processes.
Nucleic Acids Res., 37, D619 D622. biology et description http://dx.doi.org/10.1038/npre.2009.3721.1. al. language Systems 1.
Available (2009) level graphical notation: from Nature process Precedings Mueller,L. et al. (2005) The SOL genomics network.
a comparative resource for Solanaceae biology and beyond.
Plant Physiol., 138, 1310 1317.
Shannon,P. et al. (2003) Cytoscape: a software environment for integrated models of biomolecular interaction networks.
Genome Res., 13, 2498 2504.
Sucaet,Y. et al. (2010) MetNetAPI. Available at http://metnet3.vrac.iastate.edu/api (last accessed date July 28, 2010). Wurtele,E.S. et al. (2007) MetNet: Systems Biology Software for Arabidopsis. Concepts in Plant Metabolomics, Springer, pp.
145 158.
2346 [11:45 11/8/2010 Bioinformatics-btq423.tex] Page: 2346 2345 2346
BIOINFORMATICS APPLICATIONS NOTE Vol.27 no.6 2011, pages 863 864 doi:10.1093/bioinformatics/btr026 Sequence analysis Quality control and preprocessing of metagenomic datasets Robert Schmieder1,2, and Robert Edwards1,3, 1Department of Computer Science, 2Computational Science Research Center, San Diego State University, San Diego, CA 92182 and 3Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL 60439, USA Associate Editor: Alex Bateman Advance Access publication January 28, 2011 ABSTRACT Summary: Here, we present PRINSEQ for easy and rapid quality control and data preprocessing of genomic and metagenomic datasets. Summary statistics of FASTA (and QUAL) or FASTQ ﬁles are generated in tabular and graphical form and sequences can be ﬁltered, reformatted and trimmed by a variety of options to improve downstream analysis.
Availability and Implementation: This open-source application was implemented in Perl and can be used as a stand alone version or accessed online through a user-friendly web interface.
The source code, user help and additional information are available at http://prinseq.sourceforge.net/. Contact: rschmied@sciences.sdsu.edu redwards@cs.sdsu.edu Received on November 8, 2010 revised on January 11, 2011 accepted on January 12, 2011 sequencing has 1 INTRODUCTION High-throughput revolutionized microbiology and accelerated genomic and metagenomic analyses however, downstream sequence analysis is compromised by low-quality sequences, contamination, eventually leading to misassembly and erroneous conclusions.
These problems necessitate better tools for quality control and preprocessing of all sequence datasets. and sequence sequence artifacts For most next-generation sequence datasets, the quality control should include the investigation of length, GC content, quality score and sequence complexity distributions sequence duplication contamination artifacts and number of ambiguous bases.
In the preprocessing step, the sequence ends should be trimmed and unwanted sequences should be ﬁltered. Here, we describe an application able to provide graphical guidance and to perform ﬁltering, reformatting and trimming on FASTA (and QUAL) or FASTQ ﬁles. The program is publicly available through a user-friendly web interface and as a stand alone version.
The web interface allows online analysis and data export for subsequent analysis.
2 METHODS 2.1 Sequence complexity The sequence complexity is evaluated as the mean of complexity values using a window of size 64 and a step size of 32.
There are two types of sequence complexity measures implemented in PRINSEQ. Both use  To whom correspondence should be addressed.
overlapping nucleotide triplets as words and are scaled to a maximum value of 100.
The ﬁrst is an adaptation of the DUST algorithm (Morgulis et al., 2006) used as BLAST search preprocessing for masking low complexity regions: ni(ni 1)(w 2)s CD= k(cid:1) i=1 (1) where k=43 is the alphabet size, w is the window size, ni is the number of words i in a window, l 62 is the number of possible words in a window of size 64 and s=100/31 is the scaling factor.
2(l 1)l The second method evaluates the block-entropies of words using the Shannon Wiener method: CE= k(cid:1) i=1 (cid:3) (cid:2) ni l logk (cid:3) (cid:2) ni l (2) where ni is the number of words i in a window of size w, l is the number of possible words in a window and k is the alphabet size.
For windows of size w 66, k= l and otherwise k=43. 2.2 Dinucleotide odds ratio The basic version of the dinucleotide odds ratio calculation (Burge et al., 1992) is used without taking into account the occurrence of ambiguous characters such as N. In addition, the commonly used version that accounts for the complementary antiparallel structure of double-stranded DNA introduces an additional dinucleotide by simply concatenating the sequence with its reverse complement.
To account for this, the odds ratios are calculated using the number nX of nucleotide X and the number nXY of dinucleotide XY only for nucleotides A, C, G and T on the forward strand: ρXY= nXY+nY (cid:3) (cid:3) X (nX+n X)(nY+n (cid:3) (cid:3) Y) 2m2 d (3) where X nucleotides and d is the number of valid dinucleotides in the sequence.
is the complement of nucleotide X, m is the number of valid (cid:3) 2.3 Tag sequence probability Tag sequences are artifacts at the sequence ends such as adapter or barcode sequence.
A k-mer approach is used to calculate the probability of a tag sequence at the 5 -end.
The k-mers are aligned and shifted before calculating the frequencies as described in (Schmieder et al., 2010) to account for sequencing limitations.
(cid:3) - or 3 (cid:3) 2.4 Sequence duplication Sequence replication can occur during different steps of the sequencing protocol, and can therefore generate artiﬁcial duplicates (Gomez-Alvarez et al., 2009). Here, duplicates are categorized into the following groups: (i) exact duplicate, (ii) 5 -end of a longer duplicate, (iv) exact duplicate with the reverse complement sequence), (iii) 3 duplicate with the reverse complement of of another sequence and (v) 5 another sequence.
The duplicates are identiﬁed independently by sorting and preﬁx/sufﬁx matching of the sequences.
duplicate (sequence matches the 5 /3 (cid:3) (cid:3) (cid:3) (cid:3) (cid:3)  The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:34 22/2/2011 Bioinformatics-btr026.tex] Page: 863 863 864  R.Schmieder and R.Edwards 3 FEATURES 3.1 Quality control The summary statistics provided include the number of sequences and number of bases in the FASTA or FASTQ ﬁle, tables with minimum, maximum, range, mean, standard deviation and mode for read length and GC content, charts for read length distribution, GC content distribution, quality scores, sequence complexity, sequence duplicates, occurrence of Ns and poly-A/T tails.
Additionally, the base frequencies at the sequence ends and the probability of tag sequences are provides to the user.
The dinucleotide odds ratios can be used to identify possibly contamination (Willner et al., 2009) and the dinucleotide relative abundance proﬁle can be used to compare the user metagenome to other microbial or viral metagenomes using principal component plots.
The assembly measures such as N50 or N90 are helpful for datasets containing contigs. 3.2 Sequence ﬁltering Sequences can be ﬁltered by their length, quality scores, GC content, number or percentage of ambiguous base N, non-IUPAC characters for nucleic acids, number of sequences, sequence duplicates, sequence complexity (for example, to remove simple repeat sequences such as ATATATATAT), and custom ﬁlters deﬁned by the user given a predeﬁned grammar.
3.3 Sequence trimming The trimming options allow users to trim sequences to a speciﬁc length, trim bases from the 5 -end, trim poly-A/T tails and trim by quality scores with user-deﬁned options.
The trimming of sequences can generate new sequence duplicates and therefore, trimming is performed before most ﬁltering steps.
- and 3 (cid:3) (cid:3) 4 BRIEF SURVEY OF ALTERNATIVE PROGRAMS There are different applications that provide quality control and preprocessing features for sequence datasets. PRINSEQ was compared with three other available programs, each offering various additional features and functions.
Although the programs have been designed to process short read data, they are able to process longer read sequences.
SolexaQA (Cox et al., 2010) is software written in Perl that allows investigation and trimming of sequences by their base quality scores.
The software does not provide additional summary statistics or preprocessing features and requires a working installation of R and Perl modules such as GD to produce graphical outputs.
FastQC (http://www.bioinformatics.bbsrc.ac.uk/projects fastqc/) is software written in Java that provides summary statistics for FASTQ ﬁles. In its current version, FastQC does not provide data preprocessing features.
The FASTX-Toolkit (http://hannonlab.cshl.edu/fastx_toolkit/) is a collection of command line tools that provide preprocessing features and summaries for quality scores and nucleotide distributions.
The tools were recently integrated into the Galaxy platform (Blankenberg et al., 2010). All of these programs are still in active development and new functions will undoubtedly be added over time.
5 CONCLUSION PRINSEQ allows scientists to efﬁciently check and prepare their datasets prior to downstream analysis.
The web interface is simple and user-friendly, and the stand alone version allows ofﬂine analysis and integration into existing data processing pipelines.
The results reveal whether the sequencing experiment has succeeded, whether the correct sample was sequenced and whether the sample contains any contamination from DNA preparation or host.
The tool provides a computational resource able to handle the amount of data that next- generation sequencers are capable of generating and can place the process more within reach of the average research lab.
3.4 Sequence formatting The sequences can be modiﬁed to change them to upper or lower case (for example, to remove soft-masking), convert between RNA and DNA sequences, change the line width in FASTA and QUAL ﬁles, remove sequence headers or rename sequence identiﬁers. Additionally, FASTQ inputs can be converted into FASTA and QUAL format, and vice versa.
ACKNOWLEDGEMENT We thank the PRINSEQ users for comments and suggestions.
Funding: Advances in Bioinformatics from the National Science Foundation (grant DBI 0850356). Conﬂict of Interest: none declared.
3.5 Web interface The web version includes sample datasets to compare and test the program.
All graphics are generated using the Cairo graphics library (http://cairographics.org/). The web interface allows the submission of compressed FASTA (and QUAL) or FASTQ ﬁles to reduce the time of data upload.
Currently, ZIP, GZIP and BZIP2 compression algorithms are supported allowing direct processing of compressed data from the NCBI Sequence Read Archive (http://www.ncbi.nlm.nih.gov/sra). The ﬁlter, trim and reformat options can be exported and imported for similar processing of different datasets. Additionally, the web interface provides predeﬁned option sets to perform different types of preprocessing. Data uploaded using the web interface can be shared or accessed at a later point using unique data identiﬁers. 864 REFERENCES Blankenberg,D. et al. (2010) Manipulation of FASTQ data with Galaxy.
Bioinformatics, 26, 1783 1785.
Burge,C. et al. (1992) Over- and under-representation of short oligonucleotides in DNA sequences.
Proc. Natl Acad. Sci.USA, 89, 1358 1362.
Cox,M.P. et al. (2010) SolexaQA: at-a-glance quality assessment of Illumina second- generation sequencing data.
BMC Bioinformatics, 11, 485.
Gomez-Alvarez,V. et al. (2009) Systematic artifacts in metagenomes from complex microbial communities.
ISME J., 3, 1314 1317.
Morgulis,A. et al. (2006) A fast and symmetric DUST implementation to mask low- complexity DNA sequences.
J. Comput. Biol., 13, 1028.
Schmieder,R. et al. (2010) TagCleaner: identiﬁcation and removal of tag sequences from genomic and metagenomic datasets. BMC Bioinformatics, 11, 341.
Willner,D. et al. (2009) Metagenomic signatures of 86 microbial and viral metagenomes. Envir. Microbiol., 11, 1752 1766.
[11:34 22/2/2011 Bioinformatics-btr026.tex] Page: 864 863 864
