BIOINFORMATICS

Vol. 30 ECCB 2014, pages i379–i385
doi:10.1093/bioinformatics/btu484

Probabilistic single-individual haplotyping
Volodymyr Kuleshov
Department of Computer Science, Stanford University, Stanford, CA 94305, USA

ABSTRACT
Motivation: Accurate haplotyping—determining from which parent
particular portions of the genome are inherited—is still mostly an un-
resolved problem in genomics. This problem has only recently started
to become tractable, thanks to the development of new long read
sequencing technologies. Here, we introduce ProbHap, a haplotyping
algorithm targeted at such technologies. The main algorithmic idea of
ProbHap is a new dynamic programming algorithm that exactly opti-
mizes a likelihood function specified by a probabilistic graphical model
and which generalizes a popular objective called the minimum error
correction. In addition to being accurate, ProbHap also provides con-
fidence scores at phased positions.
Results: On a standard benchmark dataset, ProbHap makes 11%
fewer errors than current state-of-the-art methods. This accuracy
can be further increased by excluding low-confidence positions, at
the cost of a small drop in haplotype completeness.
Availability: Our source code is freely available at: https://github.com/
kuleshov/ProbHap.
Contact: kuleshov@stanford.edu

1 INTRODUCTION

Although modern sequencing technology has led to rapid ad-
vances in genomics over the past decade, it has largely been
unable to resolve an important aspect of human genetics: gen-
omic phase. Each human chromosome comes in two copies: one
inherited from the mother, and one inherited from the father.
Despite the fact that differences between these copies play an
important biological role, until recently, decoding these differ-
ences (a process known as haplotyping or genome phasing) has
been a major technological challenge.

In recent years, however, we have seen an emergence of new
long read technologies (Kaper et al., 2013; Kitzman et al., 2010;
Peter et al., 2012; Voskoboynik et al., 2013) that may one day
enable routine cost-effective haplotyping. Because a long read
comes from a single chromosome copy, it reveals the phase of
all heterozygous genomic positions that it covers. By connecting
long reads at their overlapping heterozygous positions, it is pos-
sible to extend this phase information into haplotype blocks, in a
single-individual haplotyping (SIH)
process
(Browning and Browning, 2011).

referred to as

Although from the molecular biology side, routine haplotyp-
ing seems close to becoming a reality, dealing with long read data
remains non-trivial computationally. Under most formulations
of the problem, it is NP-hard to recover the optimal haplotypes
from noisy sequencing reads (Gusfield, 2001). This has led to a
vast literature on heuristics for dealing with this problem as ac-
curately as possible.

Here, we propose a new algorithm, PROBHAP, which offers an
11% improvement in accuracy over the current leading method,

REFHAP. Unlike most other algorithms, PROBHAP also provides
confidence scores in addition to genomic phase. These scores can
be used to prune low-accuracy positions and further improve
haplotype quality, at the cost of phasing fewer variants.

The main algorithmic ideas of PROBHAP are a new dynamic
programming algorithm and a probabilistic graphical model.
The dynamic programming algorithm determines the haplotypes
that maximize the likelihood function Pðreadsjtrue haplotypesÞ
specified by the probabilistic model as well as the probability
that these haplotypes are correct. It can be seen as a special
case of the well-known variable elimination algorithm (Koller
and Friedman, 2009).

From a theoretical point of view, the likelihood function spe-
cified by our probabilistic model generalizes a well-known ob-
jective called the minimum error correction (MEC). Previously
proposed exact dynamic programming algorithms for the MEC
can be easily derived as special cases of the general variable elim-
ination algorithm within our model. More interestingly, alterna-
tive formulations of this algorithm (corresponding to different
variable orderings) result in novel exact algorithms that are sig-
nificantly faster than previous ones. Thus, our work generalizes
several previous approaches and provides a systematic way of
deriving new haplotyping algorithms.

2 RELATED WORK

Most phasing algorithms solve a formally defined computational
problem called SIH, in which the goal is to minimize an objective
called the MEC (see Section 5). This objective is NP-hard
(Gusfield, 2001); therefore, most early work on the SIH problem
involved simple greedy methods (Geraci, 2010). More recently,
these methods have been superseded by more sophisticated heur-
istics such as RefHap (Duitama et al., 2012) or HapCut (Bansal
and Bafna, 2008) that involve solving a Max-Cut problem as a
subroutine. There is also an exact dynamic programming solu-
tion to the SIH problem; its running time is exponential in the
length of the longest read (He et al., 2010).

Several probabilistic approaches have also been previously
including HASH (Bansal et al., 2008), MixSIH
proposed,
(Matsumoto and Kiryu, 2013) and an algorithm used for recon-
structing the diploid genome of Ciona intestinalis (Kim et al.,
2007). These methods optimize an objective function similar to
that of PROBHAP using heuristics based on Markov chain Monte
Carlo (MCMC). They differ in the way in which they implement
MCMC. In addition, MixSIH (Matsumoto and Kiryu, 2013) is
to our knowledge the only package that also provides confidence
scores at phased positions.

Probabilistic graphical models are widely used in the statistical
phasing literature to determine haplotypes from a panel of indi-
viduals using linkage disequilibrium patterns. However, the vast

ß The Author 2014. Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits
non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com

V.Kuleshov

majority of statistical methods do not use the partial phase in-
formation provided by long reads, and are not applicable to our
setting. A notable exception is a recent method called Hap-Seq
(He et al., 2012); without its statistical component it reduces to
the well-known exact exponential-time algorithm mentioned
above (He et al., 2010).

Also, there exists an extensive literature on the SIH problem
from the perspective of combinatorial optimization (Lippert
et al., 2002). Research in this field is aimed at optimizing com-
binatorial objectives such as minimum fragment removal, min-
imum SNP removal or MEC. This research is of a more
theoretical nature and aims at providing a rigorous theoretical
understanding of the SIH problem (Lippert et al., 2002).

3 RESULTS

3.1 Overview of PROBHAP

PROBHAP is based on a new exact dynamic programming solu-
tion for the SIH problem, which makes it more accurate than
many existing methods. Its main drawback is a higher computa-
tional cost: its worst-case running time increases exponentially
with the read coverage. Fortunately, modern long read technol-
ogies cover the genome at a relatively low depth (Duitama et al.,
2012; Kitzman et al., 2010), making it possible to apply our al-
gorithm to such data. In cases when the coverage is extremely
high, PROBHAP also uses a preprocessing heuristic to merge simi-
lar reads (see Section 4). In our experience, PROBHAP handles
long read coverages of up to 20; however, it is not appropriate
for higher coverage short read datasets.

The output of PROBHAP is a set of haplotype blocks in the
format of RefHap and HapCut. In addition, PROBHAP also pro-
duces at each position three confidence scores that can be used to
identify locations where the phasing results are less accurate. The
posterior score represents the probability of correctly determining
the phase of a SNP with respect to the first SNP in the block. The
transition score represents the probability of correctly determin-
ing the phase of a SNP with respect to the previous one. Finally,
the emission score is often helpful in finding sequencing errors
and other issues with the underlying data.

Whenever the transition score is too low, we suggest breaking
the haplotype block at a position. Whenever the posterior or the
emission scores are low, we suggest
leaving that position
unphased.

3.2 Comparison methodology

We compared PROBHAP to three state-of-the art algorithms—
RefHap (Duitama et al., 2010), FastHare (Panconesi and
Sozio, 2004) and DGS (Panconesi and Sozio, 2004) as well as
to HapCut (Bansal and Bafna, 2008), a historically important
phasing package, and to MixSIH (Matsumoto and Kiryu, 2013),
the only method that we know that produces confidence scores.
Previous studies (Duitama et al., 2012; Geraci, 2010) have iden-
tified the above methods as being the current state-of-the-art in
single-individual haplotype phasing.

Note that we do not compare our method to HapSeq (He
et al., 2012) because this package additionally uses population-
based statistical phasing techniques to improve accuracy. We
also do not consider previously proposed exact dynamic

i380

programming methods (He et al., 2010), as they do not scale
to long reads: their running time increases exponentially in the
number of variants in a read, and some of the reads in our
datasets have 450 variants.

The heuristics we consider work as follows. In brief, FastHare
sorts the input reads, and then traverses this ordering once,
greedily assigning each read to its most probable chromosome
given what has been seen so far. The DGS method is equally
simple: it iterates until convergence between assigning each frag-
ment to its closest chromosome, and recomputing a set of
consensus haplotypes. The RefHap and Hapcut algorithms con-
struct a graph based where each vertex is either associated with a
position (HapCut) or with a sequencing read (RefHap); then, the
algorithms approximately solve a MaxCut problem on this
graph.

We test the above methods on a long read dataset from
HapMap sample NA12878 that was produced using a fosmid-
based technology (Duitama et al., 2012). The long reads have an
average length of 40 kb and cover the genome at a depth of
3. This dataset is a standard benchmark for SIH algorithms
(Duitama et al., 2012; Matsumoto and Kiryu, 2013) in part
because HapMap sample NA12878 has also been phased mul-
tiple times based on the genomes of its parents. In this work, we
take the trio-phased variant calls from the GATK resource
bundle (DePristo et al., 2011); these provide accurate phase at
1 342 091 heterozygous variants that are also present in the long
read dataset.

We measure performance using the concept of a switch error
(Browning and Browning, 2011). A switch error is said to occur
when the true parental provenance of SNPs on a haplotype
changes with respect to the previous position. For example, if
the true SNP origins of a phased block can be written as MMFF,
then we say there is a switch error at the third position. In this
analysis, we differentiate between two types of switches: a long
switch corresponds to an inversion that lasts for more than one
position (e.g. MMFF); a short switch, on the other hand, affects
only a single position (e.g. MMFM). Switch accuracy is defined
as the number of positions without switch errors, divided by the
number of positions at which such errors could be measured.
Long switch accuracy is defined accordingly in terms of long
switch errors. We also measure accuracy in terms of switches
per megabase (Sw./Mb).

Finally, a block N50 length of x signifies that at least 50% of
all phased SNPs were placed within blocks containing x SNPs or
more. The percentage of SNPs phased was defined as the number
of SNPs in blocks of length two or more, divided by the total
number of SNPs.

3.3 Results

Given comparable phasing rates and N50 block lengths,
PROBHAP produced haplotype blocks with more accurate long-
range phase: the long-range switch error of PROBHAP was 11%
lower than that of the second best algorithm, RefHap (Table 1).
In addition, PROBHAP also produced 6% fewer short switch
errors than RefHap.

Note that long switch accuracy is substantially more import-
ant than short switch accuracy, as it drastically changes the
global structure of haplotypes. Short switch errors, on the

Probabilistic single-individual haplotyping

Table 1. Comparison of algorithm performance

Algorithm

Long sw./Mb

Short sw./Mb

% phased

N50

PROBHAP
Refhap
FastHare
DGS
HapCut
MixSIH

1.07
1.20
1.32
1.48
1.61
1.41

3.70
3.91
4.03
4.18
4.93
5.43

91.83
91.75
91.76
91.66
91.61
92.64

227
226
227
227
227
229

Fig. 2. Comparison of the accuracy/completeness trade-off of PROBHAP
and MixSIH. The top panel compares the trade-off between the N50 and
the phasing accuracy; the phasing rate was the same for both algorithms
at each point. Similarly, the bottom panel examines the phasing rate
trade-off

Table 2. Running time of each algorithm on chromosome 22

Refhap

FastHare DGS MixSIH PROBHAP

Running time

3.65 s

1.85 s

1.99 s

274.82 s

58.53 s

Fig. 1. Accuracy/completeness trade-off for PROBHAP

other hand, introduce relatively small amounts of noise in the
data.

3.4 Evaluating confidence scores

In addition to being more accurate, PROBHAP is also one of the
few algorithms which can provide estimates of their accuracy in
the form of confidence scores. As an example of how such scores
might be used, we pruned phased positions that were deemed by
PROBHAP to be uncertain and measured the resulting accuracy.
More specifically, we defined thresholds for each of the three
confidence scores reported by PROBHAP. Whenever the posterior
or emission scores were lower than a threshold, we treated that
position as unphased. Whenever the transition probability was
below a threshold, we split the phased block into two parts at
that position.

Figure 1 shows that after pruning, one obtains phased blocks
that are 30–40% more accurate than the unpruned blocks (recall
that we describe them in Table 1); the price to pay is a drop of
10–25% in N50 and phasing rate. The particular numbers shown
in Figure 1 were achieved by fixing the posterior and transition

cutoffs to 0.6 and 10
cutoff to 10

 5; 10

 4; 10

 5, respectively, and setting the emission

 3; 10

 2, 0.05, 0.1, 0.4 and 0.99.

Next, we compared the pruned regions from PROBHAP to those
of MixSIH, the only other package that allows the user to ex-
clude low-confidence positions. We chose thresholds so as to
keep either the N50 or the phasing rate constant across both
algorithms, and measured how accuracy varied with the remain-
ing non-fixed parameter. We present the results of our experi-
ment in Figure 2.

Overall, we see that given the same level of haplotype com-
pleteness, the pruned blocks of PROBHAP contain 20–30% fewer
switching errors than those from MixSIH.

3.5 Running time

We measured the running times of the algorithms on a laptop
computer (Table 2). We did not include HapCut in this compari-
son, as it is several orders of magnitude slower that the other
methods (Duitama et al., 2012). Although the three heuristics ran
faster than PROBHAP and MixSIH, a major reason for their speed
was due to not having to compute confidence scores. In fact,
PROBHAP spends
running time

roughly two-thirds of

its

i381

V.Kuleshov

Table 3. Example of a 2 4 phasing matrix M, in which two reads cover
three positions each

where

Read 1
Read 2

1

0
–

2

1
1

3

0
0

4

–
0

computing such scores. Nonetheless, it phases chromosome 22 in
just under a minute; the total time for phasing a human genome
was under 30 minutes.

4 METHODS

4.1 Notation
Formally, an instance of the SIH problem is defined by a pair of n m
matrices M, Q, whose columns correspond to heterozygous positions
(indexed by j=1; . . . ; m), and whose rows correspond to reads (indexed
by i=1; . . . ; n). We refer to M as the phasing matrix; its entries take
values in the set f0; 1; g. These values indicate the allele carried by a
read at a given position: for example, Mij = 0 signifies that read i covers
position j and carries allele 0 at j. A value of – indicates that read i did not
cover position j. See Table 3 for an example of a 2 4 phasing matrix.
The n m matrix Q 2 ½0; 1nm is referred to as the q-score matrix; it
encodes the probability of observing a sequencing error at a given pos-
ition in a read. Such scores are available on virtually all sequencing
platforms.
A solution to an instance of the SIH problem consists of a pair of
vectors h 2 f0; 1gm and r 2 f0; 1gn. The former determines the subject’s
haplotypes: at each genomic position j, it specifies an allele hj 2 f0; 1g. We
consider only one haplotype, as the second is always the complement h of
the first. The second vector r 2 f0; 1gn indicates the true provenance
ri 2 f0; 1g of each read i (i.e. whether i was obtained from the ‘maternal’
or the ‘paternal’ copy; because we do not have information to deter-
mine which copy comes from which parent, we refer to them as 0, 1).
We also use

(
hjðriÞ=

hj

hj

if ri=0

if ri=1

to denote alleles on the haplotype from which read i originated.
Next, let PoðiÞ=fjjMij 6¼  g denote the set of positions covered by
read i. Let also Hi=fhjjmin PoðiÞ  j  max PoðiÞg be the set of haplo-
spanned by read i and let Rj=frijmin PoðiÞ 
type variables
j  max PoðiÞg be the set of read provenance variables spanning a pos-
ition j. We will use this notation to simplify several expressions through-
P
out the article. In particular, if position j is spanned by, say, reads 2, 3,
then we will use the notation maxRj fðRjÞ=maxr2;r3 fðr2; r3Þ and

P
fðRjÞ=

Rj

fðr2; r3Þ.

r2;r3

4.2 Probabilistic model
We define the probability Pðr; h; oÞ over haplotypes h 2 f0; 1gm, assign-
ments of reads r 2 f0; 1gn and observed data o 2 f0; 1; gnm to be a
product of factors

Y
Pðr; h; oÞ=

n

Y

Y

n

Y

m

Pðoijjri; hjÞ

j:j2PoðiÞ

PðriÞ

PðhjÞ;

i=1

j=1

i=1

i382

(

Pðoijjri; hjÞ=

Qij
1   Qij

if oij 6¼ hjðriÞ
if oij=hjðriÞ

is the probability of observing the allele on the j-th position in read i, and
the factors PðriÞ and PðhjÞ are priors that we leave as uniform, except for
Pðh1=0Þ=1. This last choice eliminates the ambiguity stemming from
the fact that a solution h can be always replaced with its complement h; it
resolves this ambiguity by always choosing the solution with h1=0.
Finally, note that the r and h variables are hidden, while the o variables
are observed; the observed values are defined by the matrix M.

The dependency structure of P can be represented in terms of a
Bayesian network whose topology mirrors the two-dimensional structure
of the matrix M. See Figure 3 for the Bayesian network associated with
the phasing matrix in Table 3, which we gave earlier as an example.

4.3 Maximum likelihood haplotypes
We determine maximum-likelihood haplotypes h=arg maxh log Pðo=
MjhÞ using the belief propagation algorithm, also known as max-sum
message passing over a junction tree (Koller and Friedman, 2009). In
brief, this algorithm involves groups of variables passing each other in-
formation about their most likely assignment; a well-known special case
of this method is the Viterbi algorithm for hidden Markov models
(HMMs).

4.3.1 Definition of max-sum message passing We start by briefly
defining the max-sum message passing algorithm for graphical models.
Readers familiar with the subject may skip this subsection.

Y
DEFINITION 1. Let P be a probability over a set of
variables
X=fx1; . . . ; xng that is a product of k factors P=
iðXiÞ, with
each factor i being defined over a subset of variables Xi  X. A junction
tree T over P is a tree whose set of nodes is a family of subsets
C=fC1; . . . ; Cmg, with Cj  X and that satisfies the following properties:

i=1

k

(1) For each factor i, there is a cluster c(i) such that Xi  CcðiÞ.
(2) (Running intersection) If x 2 Ci and x 2 Cj, then x 2 Ck for all Ck

on the unique path from Ci to Cj in T.

Given this definition, we now define max-sum message passing. We
restrict our definition to the case when the junction tree T is a path, which
is going to be the case for our model.

DEFINITION 2. Let P be a probability distribution as in Definition 1. Let T
be a junction tree over clusters Cj for j=1; . . . ; m connected into a path and
ordered by j, with Cm serving as the root. The max-sum message from Cj to
Cj+1 is a function Mj defined over the variables in Cj \ Cj+1 as

 

X

i:cðiÞ=j

!
log iðXiÞ+Mj 1ðCj 1 \ CjÞ

;

MjðCj \ Cj+1Þ= max
CjnCj+1

with the additional definition that M0  0.

The max-sum message passing algorithm recursively computes the

above messages and determines that max XPðXÞ is

!

 

X

max
Cm

i:cðiÞ=m

log iðXiÞ+Mm 1ðCm 1 \ CmÞ

:

The actual assignment that maximizes P can be found by storing the
variable assignments that maximize each Mj. Unfortunately, proving
the correctness of this algorithm is beyond the scope of this article. For
a complete discussion that holds for arbitrary junction trees, we refer the
reader to a textbook on graphical models (Koller and Friedman, 2009).

Probabilistic single-individual haplotyping

h1

h2

h3

h4

Table 4. Example of a sequencing error that confounds the long-range
structure of the haplotypes

r1

r2

o11

o21

o31

o22

o32

o42

Read 1
Read 2

1

0
–

2

0
–

3

1
0

4

0
0

5

–
0

Note. If the quality scores are the same at all positions, the haplotypes h = 00000,
h = 00111 have the same probability.

Fig. 3. Bayesian network associated with the problem instance defined in
Table 3. The shaded nodes represent hidden variables; unshaded variables
are observed. Variables belonging to cluster C3 of the associated junction
tree are shown in bold

4.3.2 Applying max-sum message passing to the PROBHAP
model We now define how the max-sum message passing algorithm
is applied to the graphical model we defined in Section 4.2.

DEFINITION 3. Let T be a junction tree for P defined by clusters

Cj=fri; hj; oijjmin PoðiÞ  j  max PoðiÞg

for j=1; . . . ; m connected into a path ordered by j, with Cm serving as the
root.

Each cluster Cj contains hj and all the oij and ri variables associated
with reads that span across position j. For an example of one such cluster,
see Figure 3.

LEMMA 1. The tree T in Definition 3 is a valid junction tree for the distri-
bution P defined in Section 4.2.

PROOF. It is easy to check that the scope of each factor of P is in a unique
cluster. We therefore focus on proving that T has the running intersection
property.

Let Cx, Cy be two clusters in T with x  y, and let Cz be a cluster on
the path between Cx and Cy. Because T is a path, we must have
x  z  y. We need to show that Cy \ Cx  Cz.
Observe that by construction Cy \ Cx can only contain r-variables. Let
rl 2 Cy \ Cx be one such variable. We need to show that rl 2 Cz, i.e. that
min PoðlÞ  z  max PoðlÞ.
From rl 2 Cy \ Cx, we have that PoðlÞ  y  x  max PoðlÞ. Because
we also have x  z  y, our claim follows.

䊏
Now let Rj\j+1=Rj \ Rj+1 and Rjnj+1=RjnRj+1. The interested
reader may verify that the message from cluster j to cluster j + 1
during a run of max-sum message passing with Cm as the root of T
equals for j41,

MjðRj\j+1Þ = max

hj

max
Rjnj+1

i:ri2Rj


log Pðoijjri; hjÞ+Mj 1ðRj 1\jÞ
X

ð1Þ

i:ri2C1

log Pðoi1jri; h1=0Þ

and for j = 1, M1ðR1\2Þ=max R1n2
. Note
that we disregard the priors PðriÞ; PðhjÞ in all messages except the first
because they are uniform.
Intuitively, MðRj\j+1Þ represents the maximum likelihood of the data
at positions 1; . . . ; j assuming that reads spanning both j and j + 1 have
provenances specified by Rj\j+1. The maximum of P is computed using
the recursion

!
log Pðoimjri; hmÞ+Mm 1ðRm 1\mÞ

:

 

X

max
hm

max
Rm

i:ri2Rm

0
@

X

1
A


;

4.3.3 Running time The above algorithm computes one message for
each of m. A message specifies a value for each assignment of variables
in Rj\j+1; this value is the maximum over all assignments to hj and to
Rjnj+1, and for each such assignment, we need to compute
log P
ðoijjri; hjÞ in OðjRjjÞ time. Therefore, computing a message requires jRjj
2  2
jRjj+1 iterations. Thus, the total running
time of the algorithm is Oðm2+1Þ, where =maxjjRjj is the maximal
coverage across all the positions.

jRj\j+1j  2

=jRjj2

P

jRjnj+1j

i:ri2Rj

4.4 Confidence scores
Next, we turn our attention to deriving confidence estimates for genomic
regions. As an example of why such estimates are useful, we show in
Table 4 that, somewhat counter-intuitively, two SNPs may be unphased
even when they are connected by accurate reads.

4.4.1 Motivating example
In Table 4, the data contains sequencing
errors at position 3 or 4. If the error occurs at position 3 (in either row),
then the two reads come from the same haplotype and the correct solu-
tion is h = 00000. If, on the other hand, the error occurs at position 4,
then the two reads come from different chromosomes and the true haplo-
type is h = 00111. If the quality scores are the same at all positions, the
four errors are equally likely, and the haplotypes h = 00000, h = 00111
have the same probability.

Simple optimization-based algorithms would likely produce a single
haplotype in the above example; our probabilistic model, however, would
assign a transition probability of 0.5 to position 3.

4.4.2 Dynamic programming recursion We again perform probabil-
istic inference in our model using belief propagation. Our particular im-
plementation of this method is inspired by the sum-product message
passing algorithm (Koller and Friedman, 2009) over the previously
defined junction tree T. In sum-product message passing, clusters of vari-
ables pass to each other information about their local probability distri-
bution; after two rounds of message passing (referred to as ‘forwards’ and
‘backwards’), the clusters become calibrated and can be queried for vari-
ous probabilities. A well-known special case of this method is the for-
wards–backwards algorithm for HMMs.
More concretely, we compute for each node j two factors, F½hj; Rj and
B½hj; Rj, using the dynamic programming recursions below.

X

X
F½hj; Rj
=

hj 1

X
B½hj; Rj

=

hj+1

Q

F½hj 1; Rj 1PðOjjhj; RjÞPðRjÞPðhjÞ

Rj 1Rj

X

B½hj+1Rj+1PðOj+1jhj+1; Rj+1ÞPðRj+1ÞPðhj+1Þ

Rj+1Rj

The notation RjRj 1 indicates that the ri variables common to both Rj
and Rj 1 have been assigned the same value, and PðOjjhj; RjÞ is short-
Pðoijjri; hjÞ. It follows from our definition of the prior
hand for

i:ri2Rj

ð2Þ

ð3Þ

i383

V.Kuleshov

Pðh1Þ
F½h1=1; R1=0; in addition, B½hm; Rm=1.

values

initial

that

the

equal F½h1=0; R1=PðR1Þ

It is easy to show by induction that

F½hj; Rj=PðO1:j; hj; RjÞ
B½hj; Rj=PðOj+1:mjhj; RjÞ;

where Ok:l=foijjk  j  lg.

4.4.3 Computing confidence probabilities From (4), (5), we can
now easily compute confidence scores. One such score is the posterior
P
probability PðhjjO1:mÞ. It represents the probability that hj was deter-
to h1 and can be computed as
mined correctly with respect
PðhjjO1:mÞ=

Pðhj; Rjjo1:mÞ, where

Rj

Pðhj; RjjO1:mÞ=PðO1:j; hj; RjÞPðOj+1:mjhj; RjÞ=PðO1:mÞ:

Next, the transition probability Pðhjjhj 1; O1:mÞ represents the prob-
ability of consecutive SNPs being phased correctly; it can be used to
detect potential errors like the one shown in Table 4. We compute this
value using the identity Pðhjjhj 1; O1:mÞ=Pðhj; hj 1jO1:mÞ=Pðhj 1jO1:mÞ,
where the denominator is the posterior probability and the numerator
is computed as

P

Pðhj; hj 1jO1:mÞ=

Rj;Rj 1

P

PðO1:mÞ

Pðhj; hj 1; Rj; Rj 1; O1:mÞ

=

where

hj;Rj

PðOj+1:mjhj; RjÞTðhj; Rj; OjÞ
X

PðO1:mÞ

;

hj 1;Rj 1

Tðhj; Rj; OjÞ=

PðOjjhj; RjÞPðhjÞPðRjÞPðO1:j; hj; RjÞ
Q
Additionally, we found that the emission probability PðOjjhjRjÞ was
useful in detecting errors in the data. Computing this value only involves
i:j2PoðiÞ Pðoijjri; hjÞ.
the expression PðOjjhjRjÞ=
Finally, note that in general, one can compute any set of probabilities
Pðhkjhl; O1:mÞ in the model. However, this involves doing potentially up
to a full run of message passing.

 
 




Q
Q

4.5 A merging heuristic
The exact dynamic programming algorithm described above is practical
for coverages of up to 10–12. For deeper or for highly uneven cover-
ages, we propose a simple preprocessing heuristic. The heuristic consists
in reducing the coverage by repeatedly merging reads that are likely to
come from the same haplotypes until there are no reads that we can
confidently merge.

To determine whether to merge reads k, l, we consider the ratio

j2PoðkÞ\PoðlÞ Pðokj; 0; 0ÞPðolj; 1; 0Þ+Pðokj; 1; 0ÞPðolj; 0; 0Þ
j2PoðkÞ\PoðlÞ Pðokj; 0; 0ÞPðolj; 0; 0Þ+Pðokj; 1; 0ÞPðolj; 1; 0Þ

ð6Þ

;

where Pðokj; x; yÞ is shorthand for Pðokj; rk=x; hj=yÞ. Intuitively, the
denominator is associated with the likelihood that the two reads come
from the same haplotype and the numerator is associated with the like-
lihood that the reads’ origins are different. Both terms are estimated by a
heuristic formula that decomposes over each position. If reads k, l are
merged, then position j of the resulting new read is assigned the allele that
has the highest q-score in the initial reads k, l (i.e. arg maxk;lfQkj; Qljg);
the q-score at that position is set to the difference of the initial reads’
q-scores (i.e. jQkj   Qljj).

In practice, one may select a confidence threshold for (6) and only
merge reads that are below this threshold. We found empirically a
value of 1   10

 9 to work well.

i384

and

ð4Þ
ð5Þ

4.6 A post-processing heuristic
In addition, PROBHAP admits an extra post-processing heuristic for ad-
justing the optimal haplotypes h. This heuristic was initially proposed for
the algorithm RefHap; PROBHAP currently uses it by default, although it
can be disabled. The heuristic starts with the optimal read assignments r
and determines at each position j a pair of sets

Sj;0=fijðri=0 \ Mij=0Þ [ ðri=1 \ Mij=1Þg
Sj;1=fijðri=0 \ Mij=1Þ [ ðri=1 \ Mij=0Þg:

It then outputs a new haplotype hnew defined as
if jSj;0j4jSj;1j
if jSj;0j5jSj;1j

hnew
j =

0

1
  otherwise:

8>><
>>:

We found that this heuristic increases the short switch accuracy of
PROBHAP on the NA12878 dataset; the long switch accuracy remains
the same. We suggest using this heuristic in settings where the quality
scores may not be well calibrated.

5 DISCUSSION: THEORETICAL ASPECTS

Interestingly, the probabilistic framework of PROBHAP general-
izes the SIH formalism on which most existing methods are based.
This allows us to easily derive well-known exact dynamic pro-
gramming algorithms as special cases of the variable elimination
algorithm for graphical models. More interestingly, the variable
elimination algorithm with different variable orderings results in
novel exact algorithms that are far more efficient than existing
ones.

5.1 Generalizing the SIH framework

In its standard formulation, the SIH problem consists in finding a
haplotype h that minimizes the MEC criterion:

MECðh; MÞ

X

n

"

X

X

#

=

min

i=1

j:j2PoðiÞ

IðMij=hjÞ;

j:j2PoðiÞ

IðMij=hjÞ

;

where I : fTrue; Falseg ! f0; 1g is the indicator function, and the
remaining notation is the same as defined in the Section 4. The
MEC measures the total number of positions within all the reads
that need to be corrected to make the reads consistent with a
haplotype h.

It is easy to show that the MEC objective can be recovered as a
special case of our framework. Indeed, if we define the factors
ðoij; ri; hjÞ (which we have previously set to Pðoijjri; hjÞ) in a way
that

(
ðoij; ri; hjÞ=

exp ð1Þ
exp ð0Þ

if oij 6¼ hjðriÞ
if oij=hjðriÞ;

then log PðM; r; hÞ equals MEC(h, M), although P is no longer a
probability.

Thus, our dynamic programming algorithms can also produce
exact solutions to the MEC objective, and just as interestingly,
they can produce confidence probabilities associated with the
MEC.

1
A

0
@

X

j:hj2Hi

5.2 Rederiving existing SIH algorithms

the

junction tree defined by n variable

Interestingly, we can easily recover an existing dynamic program-
ming algorithm (He et al., 2010) for the MEC as a special case
of variable elimination in our graphical model. Indeed, con-
sider
clusters
Ci=fri; hj; oijj j 2 PoðiÞg connected into a path ordered by i. If
we assume for simplicity that the data have no contained reads,
then the message from cluster i – 1 to cluster i during a run of
max-sum message passing with Cn as the junction tree root
equals precisely

MðHi\i+1Þ = max

ri

max
Hini+1

log Pðoijjri; hjÞ+MðHi 1\iÞ

ð7Þ
where Hi\i+1=Hi \ Hi+1 and Hini+1=HinHi+1. This is essen-
tially the well-known dynamic programming recursion (He et al.,
2010) we were looking to find.

Unfortunately, the time to compute the above recursion in-
creases exponentially in the length of the reads, which is precisely
the data we want to use for phasing.

5.3 Deriving novel SIH algorithms

for

suitable

that are

Fortunately, as we have seen, we can derive from our framework
exact algorithms
long read data.
Interestingly, these methods are in a sense dual to equation (7):
the structure of the probabilistic model P is entirely symmetric in
r, h. If we reverse h and r in Section 4, we obtain recursion (7).
Potentially, our framework allows deriving other exact algo-
rithms by defining alternative junction trees for the max-sum
message passing algorithm. One way to do this involves using
minimizing their tree-width using some well-known heuristics
(Koller and Friedman, 2009). Because the running time max-
sum message passing is exponential in the tree-width of a junc-
tion tree, this would lead to much faster running times.

6 CONCLUSION

In summary, we have introduced a new single-individual phasing
algorithm, PROBHAP, that offers an 11% improvement in accur-
acy over the current state-of-the-art method, RefHap. In add-
ition, it is one of the only methods to provide the user with
confidence scores at every position; these confidence scores can
be used to prune positions whose phase is uncertain and thus
substantially increase the overall accuracy.

The advances behind PROBHAP are made possible by framing
the phasing problem within a probabilistic graphical models
framework. This framework makes it particularly easy to
reason about the problem; in fact, all our algorithms are special
cases of standard procedures for optimizing graphical models.

On the theoretical side, this work generalizes the MEC criter-
ion used by existing methods. Our approach allows us to obtain
existing algorithms as special cases of well-known optimization

Probabilistic single-individual haplotyping

procedures, and also easily derive new, more efficient algorithms;
it may thus serve as a foundation for further algorithmic insights.

ACKNOWLEDGEMENT

We thank Sivan Berovici for important suggestions regarding the
model definition, as well as Dmitry Pushkarev and Michael
Kertesz for helpful discussions. This research was partly done
at Moleculo Inc.

Funding: This work was partly funded by NIH/NHGRI grant
T32 HG000044.

;

Conflict of Interest: none declared.

REFERENCES

Bansal,V. and Bafna,V. (2008) HapCUT: an efficient and accurate algorithm for the

haplotype assembly problem. Bioinformatics, 24, i153–i159.

Bansal,V. et al. (2008) An MCMC algorithm for haplotype assembly from whole-

genome sequence data. Genome Res., 18, 1336–1346.

Browning,S.R. and Browning,B.L. (2011) Haplotype phasing: existing methods and

new developments. Nat. Rev. Genet., 12, 703–714.

DePristo,M.A. et al. (2011) A framework for variation discovery and genotyping

using next-generation DNA sequencing data. Nat. Genet., 43, 491–498.

Duitama,J. et al. (2010) ReFHap: a reliable and fast algorithm for single individual
haplotyping. In: Proceedings of the First ACM International Conference on
Bioinformatics and Computational Biology. ACM, New York, NY, USA,
pp. 160–169.

Duitama,J. et al. (2012) Fosmid-based whole genome haplotyping of a HapMap
trio child: evaluation of Single Individual Haplotyping techniques. Nucleic Acids
Res., 40, 2041–2053.

Geraci,F. (2010) A comparison of several algorithms for the single individual SNP

haplotyping reconstruction problem. Bioinformatics, 26, 2217–2225.

Gusfield,D. (2001) Inference of haplotypes from samples of diploid populations:

complexity and algorithms. J. Comput. Biol., 8, 305–323.

He,D. et al. (2010) Optimal algorithms for haplotype assembly from whole-genome

sequence data. Bioinformatics, 26, i183–i190.

He,D. et al. (2012) Hap-seq: an optimal algorithm for haplotype phasing with im-
putation using sequencing data. In: RECOMB’12: Proceedings of the 16th
Annual
international conference on Research in Computational Molecular
Biology. Springer-Verlag, Berlin.

Kaper,F. et al. (2013) Whole-genome haplotyping by dilution, amplification, and

sequencing. Proc. Natl Acad. Sci. USA, 110, 5552–5557.

Kim,J.H. et al. (2007) Diploid genome reconstruction of Ciona intestinalis and

comparative analysis with Ciona savignyi. Genome Res., 17, 1101–1110.

Kitzman,J.O. et al. (2010) Haplotype-resolved genome sequencing of a Gujarati

Indian individual. Nat. Biotechnol., 29, 59–63.

Koller,D. and Friedman,N. (2009) Probabilistic Graphical Models: Principles and
Techniques - Adaptive Computation and Machine Learning. The MIT Press,
Cambridge, MA.

Lippert,R. et al. (2002) Algorithmic strategies for the single nucleotide polymorph-

ism haplotype assembly problem. Brief. Bioinformatics, 3, 23–31.

Matsumoto,H. and Kiryu,H. (2013) MixSIH: a mixture model for single individual

haplotyping. BMC Genomics, 14 (Suppl. 2), S5.

Panconesi,A. and Sozio,M. (2004) Fast hare: a fast heuristic for single individual
snp haplotype reconstruction. In: Jonassen,I. and Kim,J. (eds) Algorithms in
Bioinformatics. Springer, Berlin Heidelberg, pp. 266–277.

Peters,B.A. et al. (2012) Accurate whole-genome sequencing and haplotyping from

10 to 20 human cells. Nature, 487, 190–195.

Voskoboynik,A. et al. (2013) The genome sequence of the colonial chordate,

Botryllus schlosseri. eLife, 2, e00569.

i385

