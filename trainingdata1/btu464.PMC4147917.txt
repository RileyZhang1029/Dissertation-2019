BIOINFORMATICS

Vol. 30 ECCB 2014, pages i556â€“i563
doi:10.1093/bioinformatics/btu464

Drug susceptibility prediction against a panel of drugs using
kernelized Bayesian multitask learning
Mehmet G â‚¬onen*,y

y
and Adam A. Margolin

Sage Bionetworks, Seattle, WA 98109, USA

ABSTRACT
Motivation: Human immunodeficiency virus (HIV) and cancer require
personalized therapies owing to their inherent heterogeneous nature.
For both diseases, large-scale pharmacogenomic screens of molecu-
larly characterized samples have been generated with the hope of
identifying genetic predictors of drug susceptibility. Thus, computa-
tional algorithms capable of
inferring robust predictors of drug
responses from genomic information are of great practical importance.
Most of the existing computational studies that consider drug suscep-
tibility prediction against a panel of drugs formulate a separate learning
problem for each drug, which cannot make use of commonalities be-
tween subsets of drugs.
Results: In this study, we propose to solve the problem of drug sus-
ceptibility prediction against a panel of drugs in a multitask learning
framework by formulating a novel Bayesian algorithm that combines
kernel-based non-linear dimensionality reduction and binary classifi-
cation (or regression). The main novelty of our method is the joint
Bayesian formulation of projecting data points into a shared subspace
and learning predictive models for all drugs in this subspace, which
helps us to eliminate off-target effects and drug-specific experimental
noise. Another novelty of our method is the ability of handling missing
phenotype values owing to experimental conditions and quality control
reasons. We demonstrate the performance of our algorithm via cross-
validation experiments on two benchmark drug susceptibility datasets
of HIV and cancer. Our method obtains statistically significantly better
predictive performance on most of the drugs compared with baseline
single-task algorithms that learn drug-specific models. These results
show that predicting drug susceptibility against a panel of drugs
simultaneously within a multitask learning framework improves overall
predictive performance over single-task learning approaches.
Availability and implementation: Our Matlab implementations for
binary classification and regression are available at https://github.
com/mehmetgonen/kbmtl.
Contact: mehmet.gonen@sagebase.org
Supplementary Information: Supplementary data are available at
Bioinformatics online.

1 INTRODUCTION

Human immunodeficiency virus (HIV) and cancer, which are
two major human diseases causing millions of deaths yearly,
require â€˜personalized therapiesâ€™ owing to their inherent heter-
ogenous nature. For both diseases, large-scale pharmacogenomic

*To whom correspondence should be addressed.
y
Present address: Department of Biomedical Engineering, Oregon Health
& Science University, 3303 SW Bond Avenue, Portland, OR 97239, USA

screens have been performed with the hope of discovering asso-
ciations between genetic subtypes of each disease and drug
susceptibility (Barretina et al., 2012; Garnett et al., 2012; Rhee
et al., 2003).

HIV is usually treated with antiretroviral therapies, which
have demonstrated high efficacy. However, the high mutation
rate of HIV helps the virus adapt fast, leading to drug-resistant
viral strains. Thus, selecting the optimal therapeutic regimen for
a given HIV strain requires the ability to predict drug resistance
based on its genomic sequence. To enable this type of discovery,
Rhee et al. (2003) characterize the susceptibility of 41000
genomically sequenced HIV strains to subsets of multiple HIV
therapeutic agents.

Cancer is a collection of genetically diverse diseases, and many
modern cancer therapeutics have demonstrated selective efficacy
in specific matched genetic subtypes (Druker et al., 2001). Thus,
patient selection strategies for personalized cancer therapeutics
require the ability to predict drug sensitivity based on molecular
information about a patientâ€™s tumor. For this purpose, Barretina
et al. (2012) and Garnett et al. (2012) characterize the sensitivity
of 4500 molecularly profiled cancer cell lines to 24 and 138
anticancer compounds, respectively.

For both HIV and cancer, researchers have developed
genomic predictors of drug susceptibility using modern machine
learning techniques for high-dimensional classification or regres-
sion. For example, Rhee et al. (2006) use machine learning algo-
rithms such as decision trees, artificial neural networks, support
vector machines, least squares regression and least angle regres-
sion to predict drug resistance in HIV type 1 (HIV-1) using the
sequence of the viral reverse transcriptase. Barretina et al. (2012)
and Garnett et al. (2012) use a regularized regression method
(elastic net) to predict drug sensitivities based on cancer cell
line molecular profiles, and Neto et al. (2014) formulate a
Bayesian extension of this approach in a recent study. Menden
et al. (2013) combine genomic features of cell lines and chemical
features of drugs for sensitivity prediction using a neural network
approach. Jang et al. (2014) and Papillon-Cavanagh et al. (2013)
compare the performance of various machine learning methods
applied to the cancer cell line datasets.

One potential limitation of these approaches is the formulation
of a separate learning task for each drug. In particular, because
each pharmacogenomic screen profiles multiple drugs with simi-
lar mechanisms of action, leveraging information across multiple
related drugs may yield improved model robustness by reducing
the impact of â€˜off-target effectsâ€™ and drug-specific experimental
noise. Moreover, methods that jointly model sensitivity profiles
across multiple drugs may yield insights into groups of drugs
effecting similar biological processes or infer mechanisms of
action for uncharacterized compounds. For example, Wei et al.
(2012) combine elastic net regression with an expectation

ÃŸ The Author 2014. Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits
non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com

maximization algorithm to simultaneously cluster groups of
similarly behaving compounds and infer a predictive model for
each cluster. Heider et al. (2013) formulate predicting drug re-
sistance against a panel of HIV-1 drugs as a â€˜multilabel learningâ€™
problem (Tsoumakas et al., 2010), which aims to use all available
information by learning models for all drugs simultaneously.
They show that this joint modeling approach is better than in-
dependent modeling in terms of predictive performance.
However, their algorithm has some limitations: (i) It is based
on the classifier chains formulation (i.e. training separate pre-
dictors for all drugs successively linked along a chain) (Read
et al., 2011), which is not sufficient to capture more complex
dependencies between drugs. (ii) It assumes that each data
point have the corresponding drug resistance score for all of
the drugs considered (i.e. no missing output), which limits the
applicability of the proposed method because,
in large-scale
pharmacogenomic assays, there may be many missing values
owing to experimental conditions, quality control reasons, etc.

For predicting drug susceptibility against a panel of drugs, we
propose a novel Bayesian formulation that combines kernel-
based non-linear dimensionality reduction (Sch â‚¬olkopf and
Smola, 2002) and binary classification (or regression) in a â€˜multi-
task learningâ€™ framework (Caruana, 1997), which tries to solve
distinct but related tasks jointly to improve overall generalization
performance. Our proposed method, called â€˜kernelized Bayesian
multitask learningâ€™ (KBMTL), has two key properties: (i) It
maps all data points into a shared subspace and learns predictive
models for all drugs simultaneously in this subspace to capture
commonalities between the drugs. Joint modeling of drugs en-
ables us to eliminate off-target effects and drug-specific experi-
mental noise, leading to a better predictive performance. (ii) It
can handle missing values of drug susceptibility measurements,
which enables us not to discard data points with missing outputs,
leading to larger data collections. As a result, the obtained pre-
dictions become more robust especially for drugs with a large
number of missing phenotype values.

To show the performance gain of our method over standard
modeling approaches, we perform cross-validation experiments
on two benchmark drug susceptibility datasets of HIV and
cancer.

2 MATERIALS

In this study, we use two different drug susceptibility datasets, which we
extract from the following sources: (i) HIV Drug Resistance Database
(HIVDB) (Rhee et al., 2003), (ii) Genomics of Drug Sensitivity in
Cancer (GDSC) (Yang et al., 2013). These two data sources are publicly
available at http://hivdb.stanford.edu and http://www.cancerrxgene.org,
respectively.

2.1 HIV drug resistance database

HIVDB contains phenotype and genotype information about HIV-1 (i.e.
viral reverse transcriptase sequences with corresponding susceptibility re-
sults and amino acid sequences). We extract all reverse transcriptase
sequences originated from subtype B strains, which gives us 970 reverse
transcriptase sequences in total. We use drug susceptibility results mea-
sured using the PhenoSense method for eight nucleoside analogs, namely,
Lamivudine (3TC), Abacavir (ABC), Zidovudine (AZT), Stavudine
(d4T), Zalcitabine (ddC), Didanosine (ddI), Tenofovir (TDF) and

Drug susceptibility prediction against a panel of drugs

Emtricitabine (FTC). Drug susceptibility results are given as fold
change in susceptibility (i.e. standardized measure of HIV drug resist-
ance), which is defined as

IC50 ratio =

IC50 of an isolate

IC50 of a standard wild-type control isolate

where IC50 of a resistant or wild-type control isolate gives its half max-
imal inhibitory concentration. We label reverse transcriptase sequences as
â€˜resistantâ€™ or â€˜susceptibleâ€™ using drug-specific cutoff values as done simi-
larly in the earlier studies (Heider et al., 2013; Rhee et al., 2006). The
cutoff is set to 1.5 for d4T, ddC, ddI and TDF, and to 3.0 for 3TC, ABC,
AZT and FTC. Supplementary Figure S1 shows the drug resistance labels
and the histogram of available IC50 ratios for 970 reverse transcriptase
sequences.

We remove the sequences with no phenotype information (i.e. 48 re-
verse transcriptase sequences with no IC50 ratios), leading to a final
dataset with 922 reverse transcriptase sequences. Table 1 summarizes
the final dataset by listing the drug name, the corresponding analog,
the number of reverse transcriptase sequences with measured IC50
ratio, the IC50 ratio cutoff and the ratio between resistant and susceptible
classes for each drug.

For each reverse transcriptase, genotype information is extracted from
the amino acid sequence of positions 1â€“240. Amino acid differences from
the subtype B consensus wild-type sequence are considered as mutations.
There are 1474 unique mutations for 922 reverse transcriptase sequences
in our dataset, which means each reverse transcriptase sequence can be
represented as a 1474-dimensional binary vector.

2.2 Genomics of drug sensitivity in cancer

GDSC contains phenotype and genotype information about cancer (i.e.
cancer cell lines with corresponding sensitivity results and genomic pro-
files). We use drug sensitivity results measured against 138 anticancer
drugs, which are given in terms of half maximal inhibitory concentration
(IC50) and area under the doseâ€“response curve (AUC) values. We choose
to perform our analysis on AUC values because IC50 values are not
observed before the maximum screening concentration for a significant
proportion of the drug and cell line pairs (i.e. most of the cell lines are
resistant to a given drug within the range of experimental screening con-
centrations). Supplementary Figure S2 shows the AUC values and the
histogram of available doseâ€“response curves for 790 cancer cell lines.

GDSC contains genomic profiles in the forms of copy number vari-
ation, gene expression and mutation profiles. We choose to use only gene
expression, as it is shown to be the most informative data source in earlier
studies (Jang et al., 2014). Gene expression profile is extracted from
hybridized RNA in HT-HGU133A Affymetrix whole genome array.

Table 1. Summary of HIV-1 dataset

Drug name

Analog

Number of
sequences

IC50 ratio
cutoff

Class ratio

3TC
ABC
AZT
d4T
ddC
ddI
TDF
FTC

Cytidine
Guanosine
Thymidine
Thymidine
Pyrimidine
Guanosine
Adenosine
Cytidine

910
743
905
908
472
908
545
165

3.0
3.0
3.0
1.5
1.5
1.5
1.5
3.0

2.487
1.444
1.257
1.147
1.713
1.253
0.622
2.587

Note: Class ratio denotes the ratio between numbers of resistant and susceptible
sequences.

i557

M.G â‚¬onen and A.A.Margolin

There are 12 024 normalized gene expression intensities generated using
the MAS5 algorithm (Hubbell et al., 2002), which means each cell line can
be represented as a 12 024-dimensional real-valued vector.

We remove the cell lines with no phenotype or genotype information,

leading to a final dataset with 664 cell lines and 138 drugs.

3 METHODS

We consider the problem of predicting susceptibility against a panel of
drugs simultaneously for each data point, which is a viral reverse tran-
scriptase for the HIV dataset and a cell line for the cancer dataset. Instead
of training drug-specific models separately, we choose to solve this
problem with a multitask learning formulation by considering each
drug as a distinct task and learning a unified model for all tasks
conjointly. We first discuss our proposed method for binary classification
(i.e. classifying a data point as resistant or susceptible) in detail and then
briefly mention how we extend our method to regression (i.e. predicting
real-valued sensitivity measures such as IC50 or AUC).

3.1 Problem formulation
We assume that there are T related binary classification tasks defined on
the domain X . We are given an independent and identically distributed
sample X = fxi 2 XgN
i=1. For each task, we are given a label vector
yt=fyt;i 2 f 1; +1ggi2I t
, where I t gives the indices of data points with
given class labels in task t. There is a kernel function to define similarities
between the data points, i.e. k : X  X ! R, which is used to calculate
the kernel matrix K=fkÃ°xi; xjÃgN;N

i=1;j=1.

We

extraction using the

first perform feature

Figure 1 illustrates the method we propose to learn a conjoint model
across the tasks; it is composed of two main parts: (i) projecting data
points into a shared subspace using a â€˜kernel-based dimensionality reduc-
tionâ€™ model and (ii) performing â€˜binary classificationâ€™ in this subspace
using the task-specific classification parameters. We first briefly explain
these two parts and introduce the notation used.
input kernel
matrix K 2 RNN and the projection matrix A 2 RNR, where N is the
number of data points and R is the subspace dimensionality. When we
map the data points into a low dimensional latent subspace using the
projection matrix A, we obtain their hidden representations in this shared
>
subspace, i.e. H=A
K. Using a kernel-based formulation has three main
implications: (i) We can apply our method to tasks with high dimensional
representations such as genomic information and small sample size
(i.e. large p, small n). (ii) We can learn better subspaces using non-linear
kernels such as the Gaussian kernel (i.e. kernel trick). (iii) We can use
domain-specific kernels (e.g. graph and tree kernels for structured objects)

task-specific

to better capture the underlying biological processes (Sch â‚¬olkopf et al.,
2004).
the predicted
The
outputs fft=H>
t=1 in the shared subspace using the hidden represen-
tations and the task-specific parameters fwt 2 RRgT
t=1, where Ht contains
only the data points in I t. These predicted outputs are mapped to class
labels by looking at their signs.

classification parts

t wtgT

calculate

3.2 Kernelized Bayesian multitask learning
We formulate a probabilistic model, called KBMTL, for the method
described earlier. We can derive an efficient inference algorithm using
variational approximation because our method combines the kernel-
based dimensionality reduction and task-specific classification parts
with a fully conjugate probabilistic model.

Figure 2 gives the graphical model of KBMTL with hyper-parameters,
priors, latent variables and model parameters. As described earlier, the
main idea can be summarized as (i) finding hidden representations for the
data points by mapping them into a subspace with the help of kernel and
projection matrices and (ii) performing binary classification in this shared
subspace using the task-specific classification parameters.
There are some additions to the notation described earlier: the N R
matrix of priors for the entries of the projection matrix A is denoted by L.
For these priors, f; g are used as hyper-parameters. The standard
deviations for the hidden representations and classification parameters
are given as h and w, respectively. As short-hand notations, the
hyper-parameters of the model are denoted by =f; ; h; w; g, the
priors,
by
=fL; A; H;fwt; ftgT
t=1g. Dependence on is omitted for clarity through-
out the manuscript.

parameters

variables

model

latent

and

The distributional assumptions of the kernel-based dimensionality

reduction part are defined as

s  GÃ°i
i
s  NÃ°ai
sji
ai
ijas; ki  NÃ°hs
hs

s; ; Ã
s; 0;Ã°i
i ; a>

8Ã°i; sÃ
sÃ 1Ã 8Ã°i; sÃ
hÃ 8Ã°s; iÃ;

s ki; 2

where the superscript indexes the rows, and the subscript indexes the
columns. NÃ°; m; SÃ represents the normal distribution with the mean
vector m and the covariance matrix S. GÃ°; ; Ã denotes the gamma
distribution with the shape parameter  and the scale parameter .

The binary classification part has the following distributional assump-

tions:

wt;s  NÃ°wt;s; 0; 2
wÃ

8Ã°t; sÃ

ft;ijhi; wt  NÃ°ft;i; w>
yt;ijft;i  Ã°ft;iyt;i4Ã

t hi; 1Ã 8Ã°t; i 2 I tÃ
8Ã°t; i 2 I tÃ;

where the predicted outputs fftgT
t=1, similar to the discriminant outputs in
support vector machines, are introduced to make the inference proced-
ures efficient (Albert and Chib, 1993). The non-negative margin param-
eter  is introduced to resolve the scaling ambiguity and to place a low-
density region between two classes, similar to the margin idea in support
vector machines, which is generally used for semi-supervised learning
(Lawrence and Jordan, 2005). Ã°Ã represents the Kronecker delta func-
tion that returns 1 if its argument is true and 0 otherwise.

Note that the dimensionality reduction part considers all data points,
whereas the binary classification part considers only the data points with
given labels in each task, leading to the ability of handling missing values.

3.2.1 Inference using variational Bayes To obtain an efficient
inference mechanism, we formulate a deterministic variational approxi-
mation instead of using a Gibbs sampling approach, which is computa-
tionally expensive (Gelfand and Smith, 1990). The variational methods
use a lower bound on the marginal likelihood using an ensemble of

Fig. 1. Flowchart of KBMTL for binary classification. In the kernel-
based dimensionality reduction part, we first calculate the kernel matrix
K using the original data matrix X and then find the hidden representa-
tion matrix H by projecting the kernel matrix into a subspace using the
projection matrix A. In the binary classification part, we first calculate the
predicted outputs fftgT
t=1 over the hidden representations using the task-
specific classification parameters fwtgT
t=1 and then map these outputs into
the class labels fytgT

t=1

i558

Drug susceptibility prediction against a panel of drugs

point x! as

pÃ°h!jk!; K;fyugT

u=1Ã=

Y

R

s=1

NÃ°hs

!; Ã°asÃ>

k!; 2

! SÃ°asÃk!Ã:
>
h+k

found by replacing pÃ°wtjK;fyugT
bution qÃ°wtÃ:

The predictive distribution of the predicted output ft;! can also be
u=1Ã with its approximate posterior distri-
2
4

pÃ°ft;!jh!; K;fyugT

1
A

0
u=1Ã=
@
N ft;!; Ã°wtÃ>

2
4
SÃ°wtÃ

3
5

3
5





1

1

; 1+ 1

h!

h!

;

h!

and the predictive distribution of the class label yt;! can be formulated
using the predicted output distribution:
pÃ°yt;!=+1jft;!; K;fyugT

u=1Ã=Z 1
t;! 

;




Ã°ft;!Ã
SÃ°ft;!Ã

where Zt;! is the normalization coefficient calculated for the test data
point, and Ã°Ã is the standardized normal cumulative distribution
function.

3.3 Baseline algorithms
To show the practical importance of multitask learning, we compare our
method to two baseline algorithms: (i) Bayesian single-task learning and
(ii) kernelized Bayesian single-task learning. The technical details for the
baseline algorithms can be found in the Supplementary Material.

3.3.1 Bayesian single-task learning
Instead of learning a unified
model for all tasks conjointly, we can train a separate model for each
task. For this purpose, we use a Bayesian linear classification algorithm,
which is known as â€˜probit classifierâ€™ (Albert and Chib, 1993). We call this
algorithm â€˜Bayesian probit classifierâ€™ (BPROBIT).

3.3.2 Kernelized Bayesian single-task learning
Instead of training
a linear model, we can also use a kernelized algorithm to obtain non-
linear models. For this purpose, we use a kernelized Bayesian classifica-
tion algorithm, which is known as â€˜relevance vector machineâ€™ (Bishop and
Tipping, 2000; Tipping, 2001). We call this algorithm â€˜Bayesian relevance
vector machineâ€™ (BRVM).

3.4 Extension to regression problems
Our method and two baseline algorithms are defined for the binary clas-
sification scenario but they can easily be extended to regression problems.
The technical details for the regression variant of our method can be
found in the Supplementary Material. We explain the regression variant
of our method in detail, and the regression variants of baseline algorithms
can also be derived similarly.

4 RESULTS AND DISCUSSION

To illustrate the effectiveness of our proposed KBMTL method,
we report its results on two datasets and compare it with two
baseline algorithms, namely, BPROBIT and BRVM. We have
three main reasons for these particular choices:
(i) Both
BPROBIT and BRVM use same type of inference mechanism
with our method. (ii) BPROBIT is from the family of linear and
regularized algorithms, which are considered as the standard ap-
proach for drug susceptibility prediction. (iii) We can see the
effect of multitask formulation by comparing our method to

i559

Fig. 2. Graphical model of KBMTL for binary classification. Small filled
circles: hyper-parameters; large shaded circles: observed variables; other
large circles: random variables

factored posteriors to find the joint parameter distribution (Beal, 2003).
We can write the factorable ensemble approximation of the required
posterior as

pÃ°QjK;fytgT

t=1Ã  qÃ°QÃ=qÃ°LÃqÃ°AÃqÃ°HÃ

Â½qÃ°wtÃqÃ°ftÃÂŠ

Y

T

t=1

and define each factor in the ensemble just like its full conditional distri-
bution:

sÃ; Ã°i
sÃÃ

R

R

N

i=1

s=1

GÃ°i

Y

s; Ã°i

qÃ°AÃ=

Y
qÃ°LÃ=
Y
NÃ°as; Ã°asÃ; SÃ°asÃÃ
Y
NÃ°hi; Ã°hiÃ; SÃ°hiÃÃ
qÃ°HÃ=
Y
qÃ°wtÃ=NÃ°wt; Ã°wtÃ; SÃ°wtÃÃ
qÃ°ftÃ=

s=1

i=1

N

i2I t

T NÃ°ft;i; Ã°ft;iÃ; SÃ°ft;iÃ; Ã°ft;iÃÃ;

where Ã°Ã; Ã°Ã; Ã°Ã and SÃ°Ã denote the shape parameter, the scale
parameter, the mean vector and the covariance matrix for their argu-
ments, respectively. T NÃ°; m; S; Ã°ÃÃ denotes the truncated normal dis-
tribution with the mean vector m, the covariance matrix S and the
truncation rule Ã°Ã such that T NÃ°; m; S; Ã°ÃÃ / NÃ°; m; SÃ if Ã°Ã is
true and T NÃ°; m; S; Ã°ÃÃ=0 otherwise.

We can bound the marginal likelihood using Jensenâ€™s inequality:

log pÃ°fytgT

t=1jKÃ 

EqÃ°QÃÂ½log pÃ°fytgT

t=1; QjKÃÂŠ   EqÃ°QÃÂ½log qÃ°QÃÂŠ

and optimize this bound by maximizing with respect to each factor sep-
arately until convergence. The approximate posterior distribution of a
specific factor  can be found as

qÃ°Ã / exp Ã°EqÃ°QnÃÂ½log pÃ°fytgT

t=1; QjfKtgT

t=1ÃÂŠÃ:

For our proposed model, thanks to the conjugacy, the resulting
approximate posterior distribution of each factor follows the same
distribution as the corresponding factor. The technical details of our
inference mechanism can be found in the Supplementary Material.
3.2.2 Prediction scenario We can replace pÃ°AjK;fyugT
its
the
predictive distribution of the latent representation h! for a new data

u=1Ã with

approximate

distribution

posterior

qÃ°AÃ

obtain

and

M.G â‚¬onen and A.A.Margolin

BRVM, which can also make use of kernel functions for drug-
specific models.

Table 2. Mean and standard deviations of AUROC values
for
BPROBIT, BRVM and KBMTL on HIV-1 drug resistance dataset to-
gether with ranks in parentheses

4.1 Experimental setting and performance metrics

For each dataset, data points are split into five subsets of roughly
equal size. Each subset is then used in turn as the test set, and
training is performed on the remaining four subsets. This pro-
cedure is repeated 10 times (i.e. 10 replications of 5-fold cross-
validation) to obtain robust results.

We use â€˜area under the receiver operating characteristic curveâ€™
(AUROC) to compare classification results. AUROC is used to
summarize the receiver operating characteristic curve, which is a
curve of true positives as a function of false positives while the
threshold to predict labels changes. Larger AUROC values cor-
respond to better performance.

We use â€˜normalized root mean square errorâ€™ (NRMSE) to
compare regression results. NRMSE of drug t can be calculated
as

s
ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ

NRMSEt=

Ã°yt   ^ytÃ>Ã°yt   ^ytÃ
Ã°yt   11
yt=NtÃ>Ã°yt   11
>
>

;

yt=NtÃ

where yt and ^yt denote the measured and predicted output vec-
tors, respectively. Smaller NRMSE values correspond to better
performance.

4.2 Performance comparison on HIVDB



 
ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
p
;
15

three

compare

algorithms,

namely,
On HIVDB, we
BPROBIT, BRVM and KBMTL, in terms of their classification
performances. For BPROBIT, the hyper-parameter values are se-
lected as Ã°; Ã=Ã°1; 1Ã; Ã°; Ã=Ã°1; 1Ã and  = 1. For BRVM,
the hyper-parameter values are selected as Ã°; Ã=Ã°1; 1Ã; Ã°;
Ã=Ã°1; 1Ã and  = 1. For KBMTL, the hyper-parameter values
are selected as Ã°; Ã=Ã°1; 1Ã; h=0:1; w=1 and  = 1. The
shape and scale hyper-parameters of gamma distributed priors
are set to non-informative values not to impose sparsity on the
model parameters. The number of components in the hidden
representation space is selected as R = 10. For all algorithms,
we perform 200 iterations during variational inference.

35

30

;
20

p

p

and

ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ

ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ

To calculate similarity between reverse transcriptase sequences
ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
for BRVM and KBMTL, we use the Gaussian kernel defined as
kGÃ°xi; xjÃ=exp  jjxi   xjjj2
p
p
2=s2
, where the kernel width s is
;
chosen among
using an internal
25
5-fold cross-validation scheme on the training set. We decide to
ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
make a selection from these particular values because the mean
p
of pairwise Euclidean distances between data points, which is
frequently used as the default value for s, is approximately
25
.
Table 2 gives the mean and standard deviation of AUROC
values obtained by BPROBIT, BRVM and KBMTL for each
drug over 50 replications as their performance measures. We see
that KBMTL obtains the highest mean AUROC values for seven
of eight HIV-1 drugs by improving the results from 0.5 (3TC) to
2.3% (TDF) compared with the second highest. For FTC,
BPROBIT obtains the highest mean AUROC value, whereas
KBMTL falls behind by 0.3%. We also report the average
AUROC values over drugs in the last row of Table 2. We see
that KBMTL outperforms BPROBIT and BRVM by 2.1 and
1.7%, respectively. Figure 3 compares the performance of

i560

Drug

BPROBIT

BRVM

KBMTL

3TC
ABC
AZT
d4T
ddC
ddI
TDF
FTC
Average

0.942 0.013 (2)
0.881 0.027 (3)
0.940 0.015 (3)
0.904 0.026 (3)
0.880 0.038 (3)
0.827 0.025 (3)
0.884 0.030 (2)
0.971 0.030 (1)
0.904 0.011 (3)

0.933 0.018 (3)
0.908 0.026 (2)
0.952 0.015 (2)
0.927 0.021 (2)
0.886 0.047 (2)
0.859 0.023 (2)
0.876 0.031 (3)
0.920 0.053 (3)
0.908 0.013 (2)

0.947 0.014 (1)
0.917 0.024 (1)
0.958 0.013 (1)
0.936 0.020 (1)
0.897 0.039 (1)
0.869 0.021 (1)
0.907 0.025 (1)
0.968 0.034 (2)
0.925 0.012 (1)

Note: The best result for each row is marked in bold face if it is statistically signifi-
cantly better than the others according to the paired t-test with P50.01.

BPROBIT, BRVM and KBMTL for each drug using box-and-
whisker plots. It also compares KBMTL and the best baseline
algorithm for each drug using scatterplots. We clearly see that
KBMTL is superior to BPROBIT and BRVM on all drugs
except FTC. The performance differences obtained by
KBMTL over BPROBIT and BRVM on these seven drugs are
statistically significant according to the paired t-test with
P50.01. The increased performance of KBMTL cannot be
explained by the non-linearity introduced owing to the
Gaussian kernel alone because BRVM also uses the Gaussian
kernel and is able to outperform BPROBIT by only 0.4%. The
main reason of this increased performance is the joint modeling
of drugs with multitask learning.

To illustrate the biological relevance of our method, we ana-
lyze the ability to identify drugs with similar mechanisms of
action based on hierarchical clustering of drugs based on the
task-specific classification parameters inferred by KBMTL.
Supplementary Figure S3 compares the clustering results ob-
tained using KBMTL parameters versus clustering based on
similarity of IC50 ratios. We see that the analogs of Cytidine
(3FC and FTC) are clustered together at the bottom level of
the dendogram using both IC50 ratios and KBMTL parameters
for correlation calculation. However, the other drugs with the
same analog are not clustered together at the bottom level based
on IC50 ratios. If we use the task-specific classification param-
eters fwtg8
t=1 found by KBMTL for correlation calculation,
hierarchical clustering is able to find three clusters: (i) analogs
of Cytidine (3TC and FTC), (ii) analogs of Guanosine (ABC and
ddI) and (iii) analogs of Thymidine (AZT and d4T). These re-
sults show that KBMTL is able to reveal underlying biological
similarities between drugs and to make use of this information to
improve predictive performance.

4.3 Performance comparison on GDSC

On GDSC, we compare four algorithms, namely, BRVM with
the linear kernel (BRVM[L]), BRVM with the Gaussian kernel
(BRVM[G]), KBMTL with the linear kernel (KBMTL[L]) and
KBMTL with the Gaussian kernel (KBMTL[G]), in terms of

Lamivudine (3TC)

Abacavir (ABC)

Zidovudine (AZT)

Stavudine (d4T)

Drug susceptibility prediction against a panel of drugs

A
B
C

0.96

0.94

0.92

0.90

A
B
C

0.95

0.90

0.85

0.80

â—

â—

 C 

â—

â—

â—

â—

â—

â—

â—
â—

â—
â—â—
â—
â—
â—

â—

â—

â—
â—
â—
â—
â—
â—
â—
â—â— â—
â—
â—
â—
â—
â—
â—
â—

â—

â—
â—
â—

â—
â—
â—

â—

â—

â—
â—
â—
â—

â—

â—

â—

 C 

A
B
C

0.96
0.94
0.92
0.90
0.88
0.86
0.84
0.82

A
B
C

0.98

0.96

0.94

0.92

â—

 C 

â—

â—

â—

â—
â—

â—

â—

â—
â—
â—

â—
â—

â—
â—

â—

â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—

â—

â—
â—

â—

â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—

â—

â—
â—
â—
â—
â—
â—
â—
â—
â—

â—
â—
â—
â—
â—
â—
â—
â—

â—
â—
â—
â—

â—

â—

â—

â—

â—
â—
â—

A
B
C

0.96
0.94
0.92
0.90
0.88
0.86

â—â—

 C 

â—

â—
â—

â—

â—

â—
â—
â—
â—

â—

â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—

â—

â—â—â—
â—
â—
â—

â—
â—â—
â—
â—
â—
â—
â—
â—

â—â—
â—
â—
â—
â—

â—

pâˆ’value < 1eâˆ’5

 B 

pâˆ’value = 0.00374

â—

 A 

pâˆ’value < 1eâˆ’5

 B 

pâˆ’value < 1eâˆ’5

 B 

0.90

0.94

0.82 0.86 0.90 0.94

0.92 0.94 0.96 0.98

0.86

0.90

0.94

Zalcitabine (ddC)

Didanosine (ddI)

Tenofovir (TDF)

Emtricitabine (FTC)

â—

â—

 C 

â—â—

â—

â—

â—

â—

â—

â—

â—
â—
â—
â—

â—

â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â— â—
â—
â—
â—
â—

â—
â—
â—

â—
â—â—
â—
â—
â—
â—
â—
â—

â—
â—

â—

â—

A
B
C

â—

0.90

 C 

â—
â—â—
â—
â—
â—

0.85

0.80

â—
â—

â—
â—
â—
â—

â—â—
â—
â—
â—
â—
â—â—
â—â—
â—
â—â—
â— â—
â—â—
â—
â— â—
â—
â—
â—
â—

â—
â—

â—

â—
â—
â—
â—
â—
â—
â—
â—â—
â—

â—
â—
â—

â—

â—

â—

A
B
C

0.95

 C 

â—
â—
â—

â—

â—

â—
â—
â—
â—

â—
â—
â—
â—
â—
â—â—
â—
â—â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—

â—

â—

â—
â—
â—
â—
â—
â—

â—
â—

â—

â—

â—

â—

â—
â—
â—
â—

â—

0.90

0.85

0.80

pâˆ’value = 0.00045

 B 

pâˆ’value < 1eâˆ’5

 B 

pâˆ’value < 1eâˆ’5

 A 

 C 

A
B
C

1.00

0.95

0.90

0.85

0.80

â—

â—â—

â—

â—

â— â—â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—

â—

â—

â—

â—
â—
â—

â—

â—

â—

â—

pâˆ’value = 0.33536

 A 

0.80

0.90

0.80

0.85

0.90

0.80

0.85

0.90

0.95

0.80

0.90

1.00

Fig. 3. Performance comparison between (A) BPROBIT, (B) BRVM and (C) KBMTL in terms of AUROC values on HIV-1 drug resistance dataset for
each drug. The box-and-whisker plots compare the AUROC values of the algorithms over 50 replications. The scatterplots give the AUROC values of
the best baseline algorithm and KBMTL for 50 replications on the x- and y-axes, respectively. For comparison, blue: KBMTL is better; red: KBMTL is
worse

their regression performances. For BRVM, the hyper-parameter
Ã°; Ã=Ã°1; 1Ã; Ã°; Ã=Ã°1; 1Ã and
values are selected as
Ã°; Ã=Ã°1; 1Ã. For KBMTL, the hyper-parameter values are
selected as Ã°; Ã=Ã°1; 1Ã; Ã°; Ã=Ã°1; 1Ã; h=0:1 and w=1.
The number of components in the hidden representation space is
selected as R = 100. For all algorithms, we perform 200 iter-
ations during variational inference.

(i)

the

use

linear

The training set is normalized to have zero mean and unit
standard deviation, and the test set is then normalized using
the mean and the standard deviation of the original training
set. To calculate similarity between cell lines for BRVM and
KBMTL, we
as
kLÃ°xi; xjÃ=x>
i xj, where we normalize the kernel matrix to unit
maximum value (i.e. dividing the kernel matrix by its maximum
ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
value) to eliminate scaling issues and (ii) the Gaussian kernel
p
p
;
whose width parameter s is chosen among
20000
;
using an internal 5-fold cross-
25000
30000
validation scheme on the training set. We decide to make a
selection from these particular values because the mean of pair-
wise Euclidean distances between data points is 

p
ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
p
25000
.

ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
p
;
15000

ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ
p
35000

ï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒï¬ƒ

defined

kernel

and

Figure 4 compares the performance of BRVM and KBMTL
with the same kernel in terms of per-drug performance (i.e. 138
mean NRMSE values calculated over 50 replications) and average
performance (i.e. 50 mean NRMSE values calculated over 138

drugs) using box-and-whisker and scatterplots. We see that
KBMTL[L] obtains statistically significantly better results than
BRVM[L] in terms of both per-drug and average performances
according to the paired t-test with P50.01. This result is also
valid when we compare KBMTL[G] and BRVM[G]. Table 3
gives the pairwise comparison results between the four algorithms
over 138 per-drug performance values. For example, KBMTL[L]
obtains better predictive performance than BRVM[L] on 126
of 138 drugs. On 102 of these 126 drugs, KBMTL[L] is statis-
tically significantly better than BRVM[L] according to the
paired t-test with P50.01. If we sort the algorithms in terms
of
following
ordering: KBMTL[G]4KBMTL[L]4BRVM[G]4BRVM[L].
These results show that predicting drug sensitivities with a joint
model obtains superior predictive performance than using drug-
specific models irrespective of the kernel function used.

their predictive performances, we find the

5 CONCLUSION

In this study, we consider the problem of drug susceptibility
prediction based on pharmacogenomic screens against a panel
of drugs. In contrast to earlier studies, we choose to solve this
problem with a multitask learning formulation by considering
each drug as a distinct task and learning a unified model for

i561

M.G â‚¬onen and A.A.Margolin

Perâˆ’drug performance

Average performance

Perâˆ’drug performance

Average performance

A
C

1.4

1.2

1.0

0.8

â—â—

â— â—

â— â—

â—

 C 

pâˆ’value < 1eâˆ’5

â—

â—
â—
â—â—
â—â—
â—
â—
â—
â— â—
â—â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—â—
â—
â—
â—â—
â—â—
â—â—â—â—
â—
â— â—
â—
â—â—
â—â—
â—â—
â—
â—â—
â—â— â—
â—
â—â—â—â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—â—
â—
â—â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—

â—
â—

â—

 A 

A
C

1.04
1.02
1.00
0.98
0.96
0.94
0.92

â—

â—â—

 C 

pâˆ’value < 1eâˆ’5

â—

â—

â—

â—

â—

â—
â—
â—â—
â—
â—
â— â—
â—
â—
â—
â—â—
â—â—
â—
â—â— â—
â— â—â—
â—â—
â—â—
â—â—
â—
â—â—
â—
â—
â—
â—
â—
â—

â—

â—
â—
â—
â—

â—
â—
â—

 A 

B
D

1.4

1.2

1.0

0.8

â—â—

â—â—

â—

â—

â—â— â—

â—

 D 

pâˆ’value < 1eâˆ’5

â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â— â—
â—â— â—â—â—
â—
â—â—â—â—
â—
â—â—
â—â—
â—â—
â—
â—
â—
â—â—â—
â—â—â—
â—
â—
â—â—â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—â—
â—
â— â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—

â—
â—

â—

â—
â—

â—

â—

 B 

B
D

1.04
1.02
1.00
0.98
0.96
0.94
0.92

â—

â—

â—

â—â—

â—

 D 

pâˆ’value < 1eâˆ’5

â—

â—
â—
â—
â—

â—
â—

â—
â—â—
â—
â—â—
â—
â—â—
â—
â—
â—â—â—
â— â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—

â—

 B 

0.8

1.0

1.2

1.4

0.92

0.96

1.00

1.04

0.8

1.0

1.2

1.4

0.92

0.96

1.00

1.04

Fig. 4. Performance comparison between (A) BRVM with the linear kernel, (B) BRVM with the Gaussian kernel, (C) KBMTL with the linear kernel and
(D) KBMTL with the Gaussian kernel in terms of NRMSE values on cancer drug sensitivity dataset. The per-drug and average performance results
compare the algorithms using 138 mean NRMSE values calculated over 50 replications and 50 mean NRMSE values calculated over 138 drugs,
respectively. The box-and-whisker and scatterplots compare the NRMSE values of BRVM (on the x-axis of the scatterplots) and KBMTL (on the
y-axis of the scatterplots) with the same kernel. For comparison, blue: KBMTL is better; red: KBMTL is worse

Table 3. Pairwise comparison of four algorithms in terms of per-drug
performances on cancer drug sensitivity dataset

Algorithm

BRVM[L]

BRVM[G]

KBMTL[L]

KBMTL[G]

BRVM[L]
BRVM[G]
KBMTL[L]
KBMTL[G]

70/93
102/126
114/127

25/45

64/95
98/121

5/12
26/43

84/113

2/11
5/17
4/25

Note: The numbers in each comparison give statistically significant wins according
to the paired t-test with P50.01 and wins according to the direct comparison,
respectively, for the method of the corresponding row.

all tasks conjointly. For this purpose, we propose a novel
Bayesian multitask learning algorithm that combines kernel-
based non-linear dimensionality reduction and binary classifica-
tion to classify data points as resistant or susceptible. We
formulate a deterministic variational approximation inference
scheme, which is more efficient than using a Gibbs sampling
approach in terms of computation time. We then extend our
algorithm to regression to predict real-valued outputs such as
half maximal inhibitory concentration and AUC.

The main novelty of our approach comes from the joint
Bayesian formulation of projecting data points into a shared
subspace and learning predictive models for all drugs in this
subspace, which enables us to capture commonalities between
subsets of drugs to improve predictive performance. The
increased performance is due to elimination of off-target effects
and drug-specific experimental noise that may be present in drug
susceptibility values. Another novelty of our approach comes
from the ability to handle missing drug susceptibility values
owing to experimental conditions and quality control reasons,
which increases the effective sample size, leading to more robust
predictions especially for drugs with a large number of missing
phenotype values.

To demonstrate the performance of our algorithm, called
KBMTL, we perform cross-validation experiments on drug sus-
ceptibility datasets of two major human diseases, namely, HIV
and cancer. For the HIV dataset, we classify viral reverse tran-
scriptase sequences as resistant or susceptible against eight nu-
cleoside analogs using mutation profiles extracted from sequence
information of
the viral genotype. Our multitask learning
method obtains statistically significantly better results on seven
of eight drugs compared with two baseline single-task learning
methods that consider each drug separately. For the cancer data-
set, we predict AUC within the range of experimental screening
concentrations for each cell line against 138 anticancer drugs
using gene expression profiles. Our method with the linear or
Gaussian kernel obtains statistically significantly better results
on 102 or 98 of 138 drugs, respectively, compared to a single-
task learning method with the same kernel function. These re-
sults show that predicting drug susceptibility against a panel of
drugs simultaneously within a multitask learning framework im-
proves overall predictive performance over single-task learning
approaches that learn drug-specific models.

We implement both single-task and multitask learning meth-
ods using efficient variational approximation schemes, where co-
variance calculations are the most time-consuming steps because
of matrix inversions. BRVM has OÃ°N3Ã complexity per iteration,
but we need to train a separate model for each drug, leading to
OÃ°TN3Ã overall complexity. KBMTL learns a unified model for
all drugs conjointly and has OÃ°RN3+NR3+TR3Ã complexity
per iteration, which shows that our algorithm has comparable
computational complexity with single-task learning methods up
to moderate values of R.

We envision several possible extensions of our work in future
pharmacogenomic applications. Based on an analysis over
KBMTL model parameters, we are able to identify groups of
compounds with similar mechanisms of action. As functional
screens are being performed on increasingly large numbers of
compounds or genetic perturbations, often with poorly charac-
terized mechanisms or strong off-target effects, jointly modeling

i562

insights

each compound in the context of the full screening collection
should yield novel
into compound mechanisms.
Moreover, the ability to identify groups of related compounds
with a shared robust molecular predictor should aid drug discov-
ery efforts by improving the interpretability of large screens and
providing multiple lead compounds effecting similar biological
processes. From an algorithmic perspective,
the kernelized
Bayesian framework provides an extensible template for
incorporating prior knowledge. For example, prior information
may be incorporated to encourage similar predictors to be
inferred for compounds known to target proteins in the same
pathway. Importantly, extensions of more complex prior infor-
mation are computationally tractable owing to the highly effi-
cient inference performed by the variational Bayes algorithm. In
summary, we believe that the method presented in this work
contributes to the field of pharmacogenomic analysis by improv-
ing the robustness of drug susceptibility predictions by leveraging
information shared across multiple compounds in a screen, and it
provides an efficient Bayesian inference framework that may be
applied and extended by the community in future applications.

Funding: The Integrative Cancer Biology Program (ICBP) of the
National Cancer Institute (1U54CA149237). Cancer Target
Discovery and Development (CTDD) Network of the National
Cancer Institute (1U01CA176303).

Conflicts of interest: none declared.

REFERENCES

Albert,J.H. and Chib,S. (1993) Bayesian analysis of binary and polychotomous

response data. J. Amer. Statist. Assoc., 88, 669â€“679.

Barretina,J. et al. (2012) The Cancer Cell Line Encyclopedia enables predictive

modelling of anticancer drug sensitivity. Nature, 483, 603â€“607.

Beal,M.J. (2003) Variational algorithms for approximate Bayesian inference. PhD
Thesis, The Gatsby Computational Neuroscience Unit, University College
London.

Bishop,C.M. and Tipping,M.E. (2000) Variational relevance vector machines. In:
Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence.
Stanford, CA, USA, pp. 46â€“53.

Drug susceptibility prediction against a panel of drugs

Caruana,R. (1997) Multitask learning. Mach. Learn., 28, 41â€“75.
Druker,B.J. et al. (2001) Efficacy and safety of a specific inhibitor of the BCR-ABL
tyrosine kinase in chronic myeloid leukemia. N. Engl. J. Med., 344, 1031â€“1037.
Garnett,M.J. et al. (2012) Systematic identification of genomic markers of drug

sensitivity in cancer cells. Nature, 483, 570â€“577.

Gelfand,A.E. and Smith,A.F.M. (1990) Sampling-based approaches to calculating

marginal densities. J. Am. Stat. Assoc., 85, 398â€“409.

Heider,D. et al. (2013) Multilabel classification for exploiting cross-resistance infor-

mation in HIV-1 drug resistance prediction. Bioinformatics, 29, 1946â€“1952.

Hubbell,E. et al. (2002) Robust estimators for expression analysis. Bioinformatics,

18, 1585â€“1592.

Jang,I.S. et al. (2014) Systematic assessment of analytical methods for drug sensi-
tivity prediction from cancer cell line data. Pac. Symp. Biocomput., 19, 63â€“74.
Lawrence,N.D. and Jordan,M.I. (2005) Semi-supervised learning via Gaussian pro-

cesses. Adv. Neural Inf. Process. Syst., 17, 753â€“760.

Menden,M.P. et al. (2013) Machine learning prediction of cancer cell sensitivity to

drugs based on genomic and chemical properties. PLoS One, 8, e61318.

Neto,E.C. et al. (2014) The stream algorithm: computationally efficient ridge-
regression via Bayesian model averaging, and applications to pharmacogenomic
prediction of cancer cell line sensitivity. Pac. Symp. Biocomput., 19, 27â€“38.

Papillon-Cavanagh,S. et al.

(2013) Comparison and validation of genomic
predictors for anticancer drug sensitivity. J. Am. Med. Inform. Assoc., 20,
597â€“602.

Read,J. et al. (2011) Classifier chains for multi-label classification. Mach. Learn., 85,

333â€“359.

Rhee,S.Y. et al. (2003) Human immunodeficiency virus reverse transcriptase and

protease sequence database. Nucleic Acids Res., 31, 298â€“303.

Rhee,S.Y. et al. (2006) Genotypic predictors of human immunodeficiency virus type

1 drug resistance. Proc. Natl Acad. Sci. USA, 103, 17355â€“17360.

Sch â‚¬olkopf,B. and Smola,A.J.

(2002) Learning with Kernels: Support Vector
Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge,
MA.

Sch â‚¬olkopf,B. et al., eds (2004) Kernel Methods in Computational Biology. MIT Press,

Cambridge, MA.

Tipping,M.E. (2001) Sparse Bayesian learning and the relevance vector machine.

J. Mach. Learn. Res., 1, 211â€“244.

Tsoumakas,G. et al. (2010) Mining multi-label data. In: Maimon,O. and Rokach,L.
(eds) Data Mining and Knowledge Discovery Handbook. Springer, New York,
NY, USA, pp. 667â€“685.

Wei,G. et al.

(2012) Chemical genomics

small-molecule MCL1
repressors and BCL-xL as a predictor of MCL1 dependency. Cancer Cell, 21,
547â€“562.

identifies

Yang,W. et al. (2013) Genomics of Drug Sensitivity in Cancer (GDSC): a resource
for therapeutic biomarker discovery in cancer cells. Nucleic Acids Res., 41,
D955â€“D961.

i563

