            Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1193 11931201 BIOINFORMATICS REVIEW Vol.
28 no.
9 2012, pages 11931201doi:10.1093/bioinformatics/bts116 Systems biology Advance Access publication March 14, 2012 Industrial methodology for process verification in research (IMPROVER): toward systems biology verification Pablo Meyer1,, Julia Hoeng2,, J. Jeremy Rice1, Raquel Norel1, Jrg Sprengel3, Katrin Stolle2, Thomas Bonk2, Stephanie Corthesy3, Ajay Royyuru1,, Manuel C. Peitsch2, and Gustavo Stolovitzky1, 1IBM Computational Biology Center, Yorktown Heights, 10598 NY, USA, 2Phillip Morris Products SA, Research and Development, 2000, Neuchtel, Switzerland and 3IBM Life Sciences Division,8802, Zurich, Switzerland Associate Editor: Jonathan Wren ABSTRACT Motivation: Analyses and algorithmic predictions based on highthroughput data are essential for the success of systems biology in academic and industrial settings.
Organizations, such as companies and academic consortia, conduct large multi-year scientific studies that entail the collection and analysis of thousands of individual experiments, often over many physical sites and with internal and outsourced components.
To extract maximum value, the interested parties need to verify the accuracy and reproducibility of data and methods before the initiation of such large multi-year studies.
However, systematic and well-established verification procedures do not exist for automated collection and analysis workflows in systems biology which could lead to inaccurate conclusions.
Results: We present here, a review of the current state of systems biology verification and a detailed methodology to address its shortcomings.
This methodology named Industrial Methodology for Process Verification in Research or IMPROVER, consists on evaluating a research program by dividing a workflow into smaller building blocks that are individually verified.
The verification of each building block can be done internally by members of the research program or externally by crowd-sourcing to an interested community.
www.sbvimprover.com Implementation: This methodology could become the preferred choice to verify systems biology research workflows that are becoming increasingly complex and sophisticated in industrial and academic settings.
Contact: gustavo@us.ibm.com Received on November 16, 2011; revised on February 8, 2012; accepted on March 5, 2012 1 BACKGROUND AND PHILOSOPHY OF SYSTEMS BIOLOGY VERIFICATION 1.1 What is verification?
In the past two decades molecular biology has experienced an increase in the amount and diversity of data that are produced to answer key scientific questions.
Systems biology has emerged as a new paradigm for the integration of experimental and computational To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first three authors should be regarded as joint First Authors.
efforts.
This uses algorithmic analyses to interpret the data and mathematical models are built to predict yet unmeasured states of the biological system.
However, algorithms and models are not unique and the determination of the right algorithm and model leading to the true interpretation of the natural phenomena under study becomes a fundamental question that falls within the realm of the philosophy of science.
Popper postulated (Popper, 1959) that a hypothesis, proposition, theory or in the case of systems biology a model, is scientific only if it is falsifiable.
In Poppers thesis, a theory can be proven wrong by producing evidence that is inconsistent with the theory.
In contrast, a theory cannot be proven correct by evidence because other evidence, yet to be discovered, may exist that will falsify the theory.
Conversely, according to the verificationist school, a scientific statement is significant only if it is a statement of logic (such as a mathematical statement deduced from axioms) or if the statement can be verified by experience (Ayer, 1936).
Statements that do not meet these criteria of being either analytic or empirically verifiable are judged to be non-sensical.
The McGraw-Hill Concise Dictionary of Modern Medicine (2002) defines verification as: The process of evaluating a system, component or other product at the end of its development cycle to determine whether it meets projected performance goals (http://medical-dictionary.thefreedictionary.com/verification).
For systems biology, a fundamental question to address is how to verify the correctness of a model that integrates vast amounts of data into a representation of reality.
These data are not only high-dimensional but noisy given the biological variability, sample preparation inconsistencies and measurement noise inherent to the sensor instrumentation.
While the concept of verification may be applied to different contexts with slightly different meanings, here we always use verification as checking for the truth or correctness of either data (i.e.whether the data represents what we wish to measure) or the correctness of a theorys predictions.
1.2 Crisis in peer-review/slow and low throughput The quality of a scientific prediction or the accuracy of a scientific model is the subject of rigorous scrutiny, usually by the researchers themselves or by colleagues in the peer-review process that is at the heart of scientific publishing (Spier, 2002).
As stated by the editors of the journal Science (Alberts et al., 2008), The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1194 11931201 P.Meyer et al.peer review is under increasing stress [] The growth of scientific publishing is placing a burden on the entire scientific enterprise.
Papers today are more interdisciplinary, use more techniques, and have more authors.
Many have large volumes of data and supplementary material.
The coming of age of systems biology and its computational methods such as data-interpreting algorithms are challenging the peer-review process as large numbers of simultaneous predictions are generated, but only a small minority is tested.
In the best cases, a very small sampling of predictions are verified using sound experimental assays and methods and then are presented as representative confirmation of the soundness of the entire set of predictions.
Typically, this verification method lacks sufficient rigor, objectivity and a clear characterization of the relative strengths and weaknesses of the algorithms (Dougherty, 2010; Jelizarow et al., 2010; Mehta et al., 2004).
The same lack of rigor in verification of model predictions can be found in many areas of science where complex systems are measured, analyzed and modeled.
For example, in systems biology, high-throughput data are collected and analyzed together with insufficient verification.
Specifically, false positive and, equally important, false negative rates, are rarely considered a requisite for verification of the analysis for publication.
Consider that the first experimentally-generated, genome-wide interactomes in yeast (Gavin et al., 2006; Ito et al., 2001; Uetz and Hughes, 2000) showed minimal overlap, generating some concerns within the scientific community that the data and methodologies were unreliable.
Later work showed that high quality interactome maps could be generated by including controls and quality standards in data collection, careful verification of all interacting pairs and validation tests using independent, orthogonal assays (Dreze et al., 2010).
Similarly, Genome-Wide Association Studies (GWAS) generate a high rate of false positives as correlations are found for single nucleotide polymorphisms with no direct effect on the phenotype.
The community responded by defining a quality-control process and software package for analysis (Purcell et al., 2007).
Similar problems are found in other fields including protein structure prediction (Moult et al., 1995), prediction of docking between proteins (Wodak and Mendez, 2004), text mining from scientific literature (Hirschman et al., 2005) and biological network inference (Stolovitzky et al., 2007).
In these cases the response has been to set up community-based efforts, as discussed below.
1.3 Proposed community approaches for science verification The difficulties in verifying complex science with traditional methods is driving changes in the methods of evaluation.
Advances in web technology (called web 2.0) have allowed communities to stay tightly in touch to develop their interests, even when they are geographically dispersed.
The journal Nature developed in 2006 an experiment allowing an online public review of manuscripts that in parallel were undergoing peer-review (http://www.nature.com/nature/peerreview/).
Faculty of 1000 is an annotation service that allows researchers to locate outstanding or influential papers from the whole body available that can completely overwhelm the individual.
Faculty of 1000 has domain experts cull, rate and summarize both the importance of the papers findings and context within the field and hence is a good example of new practices in research evaluation that go far beyond simple indexing and content annotation (as in PubMed, for example).
The journal PLoS ONE and now even mainstream sites like Twitter have become places where manuscripts are publicly criticized (Mandavilli, 2011).
We think that these changes in research evaluation, while valuable, will not have sufficient rigor and consistency for the needs of research workflows verification.
2 COMMUNITY APPROACHES FOR SCIENCE VERIFICATION 2.1 Community consensus as criteria of science done right A natural evolution of allowing community feedback has been the development of crowd-sourcing, a modality of distributed problem-solving.
Challenges are broadcasted to potential interested stakeholders (solvers) in the form of an open call for participation.
Participants submit solutions for the challenges, and the best solutions are typically chosen by the crowd-sourcer (the entity that broadcasted the challenge).
The top performing participants are sometimes rewarded either with monetary awards, prizes, certificates or with recognition.
We think that such directed community approaches could complement and enhance the peerreview process.
Most importantly, we think that these could serve as a tool to verify the scientific results and fulfill the ultimate goal of scientific research that is to advance our understanding of the natural world (Meyer et al., 2011).
Community-based approaches to verify scientific research can be considered a more focused attempt to tap the consensus building that historically occurs in scientific progress.
Kuhn understood progress in science as an eminently social process, in which the scientific worldview is dominated by the paradigm embraced by the scientific community at any given time (Kuhn, 1962).
When the number of anomalies accumulated under the current paradigm generates distrust, the community may adopt a new paradigm that now guides how research is conducted.
In this view, the scientific community, and not just nature itself, needs to be taken into account when considering what is accepted as verified science.
For our purposes, we abbreviate the typical definition of verification given in the first paragraph to: science done right, where the right refers to the accepted best practices of the scientific community or similar criteria.
Accepted best practices means that there is a consensus in the community as to the proper collection and analysis of a data modality.
Obviously, a modality must already be accessible to a wide community for the consensus to form.
For newly developed modalities, crowd-sourcing provides a means to a rapid consensus as to the best collection and analysis methodologies.
2.2 Summary of community approaches for verification in other fields Recent practices involving a new form of research quality control have become well-established during the last decade and a half.
These efforts have merged the need of scientific verification of methods used in research, with the widespread practice of crowd-sourcing, to create a sort of collaboration-by-competition communities.
The practice of this idea has been sufficiently wellestablished to become the business model of for-profit companies.
In this section, we summarize three relevant community-based 1194 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1195 11931201 IMPROVER Table 1.
Additional information for the eight community-based efforts described in the paper.
The last row describes other efforts not discussed in the main text Name Domain and Regularity Website KDD Cup Knowledge discovery and machine learning in various domains.
http://www.sigkdd.org Knowledge Discovery and Data Mining.
Every year since launch in 1997.
InnoCentive The name mixes Innovation and Incentive.
http://www.innocentive.com/ Crowd-sourcing for problems of commercial interest.
Founded in 2001.
New challenges are released on a rolling schedule.
Netflix Prize The name comes from the sponsoring company, Netflix.
http://www.netflixprize.com//index Prediction of user ratings for films, based on previous ratings.
Only challenge so far, released in 2006, lasted 3 years to complete.
CASP Critical Assessment of Techniques for Protein Structure Prediction.
http://predictioncenter.org/ Protein 3D structure prediction assessment.
Every 2 years since 1994.
CAPRI Critical Assessment of PRedicted Interactions.
Assessment of predictions of http://www.ebi.ac.uk/msd-srv/capri proteinprotein docking or protein-DNA interaction from 3D structure.
Goes by Round 22 since 2001.
Starts whenever an experimentalist offers an adequate target.
Predicted structures are submitted 68 weeks later.
DREAM Dialogue for Reverse Engineering Assessments and Methods.
http://www.the-dream-project.org/ Assessment of quantitative modeling in systems biology.
Every year since 2006.
BioCreAtIve Assessment of Information Extraction Systems in Biology.
Evaluating text mining http://www.biocreative.org and information extraction systems applied to the biological literature.
http://biocreative.sourceforge.net Every 2 years beginning in 2004.
FlowCAP Flow Cytometry Critical Assessment of Population Id Methods.
http://flowcap.flowsite.org/ Evaluation of automated analysis of flow cytometry data.
http://groups.google.com/group/flowcap Only one iteration on 2010, second one on planning phase.
Others efforts TunedIT: http://tunedit.org/, RGASP-RNAseq Genome Annotation Assessment Project: www.sanger.ac.uk/PostGenomics/encode/RGASP.html Pittsburgh brain competition: http://pbc.lrdc.pitt.edu/ CAMDA Critical Assessment of Microarray Data Analysis: http://camda.bioinfo.cipf.es/camda2011/ Genome Access Workshop evaluation of statistical genetics approaches: http://www.gaworkshop.
verification approaches with overlapping objectives but different focus areas.
Some relevant details of these efforts are listed in Table 1.
Knowledge Discovery and Data Mining Cup (KDD Cup) is an annual competition organized by the Association for Computing Machinery (ACM) Special Interest Group on Knowledge Discovery and Data Mining, the leading professional organization of data miners (Fayyad, 1996).
KDD goals are to achieve a better understanding and analysis of data in many knowledge domains, such as medical informatics, consumer recommendations, diagnostics from imaging data and Internet user search query categorization.
InnoCentive, a spin-off of Eli Lilly, was founded in 2001 to match problems in need of solutions with problem solvers.
The main entry point of InnoCentive is a web portal where solutions to scientific and business problems are solicited on behalf of organizations seeking innovations.
An example of a recent challenge is Solutions to Respond to Oil Spill in the Gulf of Mexico.
InnoCentive works with seekers to design the challenge, score/judge solutions and manage the intellectual property transfer.
There is usually a cash award to the winning solver.
Netflix Prize was a competition to produce a better algorithm to substantially improve the accuracy of predictions about how much a customer is going to enjoy a movie based on their past movie p
