num = 100
no tasks available 70%
Saved 240 annotations to database SQLite
root@riley-ThinkPad-T470:/home/riley/Documents/fromGit/Dissertation-2019/Prodigy# prodigy ner.batch-train st_ner_100 en_core_web_lg --output software-model --label SOFTWARE --eval-split 0.2 --n-iter 20 --batch-size 8
Using 1 labels: SOFTWARE

Loaded model en_core_web_lg
Using 20% of accept/reject examples (39) for evaluation
Using 100% of remaining examples (157) for training
Dropout: 0.2  Batch size: 8  Iterations: 20  


BEFORE      0.000            
Correct     0  
Incorrect   39
Entities    111              
Unknown     0                

#            LOSS         RIGHT        WRONG        ENTS         SKIP         ACCURACY  
01           2013.403     0            39           24           0            0.000                                                           
precision: 0.0 recall: 0.0 fscore: 0.0
02           1721.152     0            39           31           0            0.000                                                           
precision: 0.0 recall: 0.0 fscore: 0.0
03           1905.032     0            39           31           0            0.000                                                           
precision: 0.088 recall: 0.058 fscore: 0.07
04           1901.833     9            30           35           0            0.231                                                           
precision: 0.408 recall: 0.385 fscore: 0.396
05           1882.890     17           22           46           0            0.436                                                           
precision: 0.452 recall: 0.365 fscore: 0.404
06           1844.333     21           18           44           0            0.538                                                           
precision: 0.511 recall: 0.462 fscore: 0.485
07           1912.412     23           16           46           0            0.590                                                           
precision: 0.529 recall: 0.519 fscore: 0.524
08           1785.492     29           10           54           0            0.744                                                           
precision: 0.549 recall: 0.538 fscore: 0.544
09           1783.159     29           10           55           0            0.744                                                           
precision: 0.571 recall: 0.692 fscore: 0.626
10           1796.913     30           9            57           0            0.769                                                           
precision: 0.571 recall: 0.615 fscore: 0.593
11           1925.615     33           6            60           0            0.846                                                           
precision: 0.607 recall: 0.712 fscore: 0.655
12           1820.924     31           8            56           0            0.795                                                           
precision: 0.6 recall: 0.692 fscore: 0.643
13           1774.278     34           5            62           0            0.872                                                           
precision: 0.621 recall: 0.692 fscore: 0.655
14           1840.251     34           5            59           0            0.872                                                           
precision: 0.597 recall: 0.712 fscore: 0.649
15           1768.142     35           4            61           0            0.897                                                           
precision: 0.587 recall: 0.712 fscore: 0.643
16           1812.940     35           4            61           0            0.897                                                           
precision: 0.608 recall: 0.596 fscore: 0.602
17           1765.619     35           4            60           0            0.897                                                           
precision: 0.613 recall: 0.731 fscore: 0.667
18           1771.087     35           4            59           0            0.897                                                           
precision: 0.649 recall: 0.712 fscore: 0.679
19           1736.328     35           4            57           0            0.897                                                           
precision: 0.614 recall: 0.673 fscore: 0.642
20           1755.828     35           4            58           0            0.897                                                           
precision: 0.692 recall: 0.692 fscore: 0.692

Correct     35
Incorrect   4
Baseline    0.000           
Accuracy    0.897  




num = 200         

