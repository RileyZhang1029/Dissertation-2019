ABSTRACT Motivation: Membrane proteins (MPs) are important drug targets but knowledge of their exact structure is limited to relatively few examples.
Existing homology-based structure prediction methods are designed for globular, water-soluble proteins.
However, we are now beginning to have enough MP structures to justify the development of a homology-based approach specifically for them.
Results: We present a MP-specific homology-based coordinate generation method, MEDELLER, which is optimized to build highly reliable core models.
The method outperforms the popular structure prediction programme Modeller on MPs.
The comparison of the two methods was performed on 616 targettemplate pairs of MPs, which were classified into four test sets by their sequence identity.
Across all targets, MEDELLER gave an average backbone root mean square deviation (RMSD) of 2.62 versus 3.16 for Modeller.
On our easy test set, MEDELLER achieves an average accuracy of 0.93 backbone RMSD versus 1.56 for Modeller.
Availability and Implementation: http://medeller.info; Implemented in Python, Bash and Perl CGI for use on Linux systems; Supplementary data are available at http://www.stats.ox.ac.uk/ proteins/resources.
Contact: kelm@stats.ox.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on July 1, 2010; revised on September 21, 2010; accepted on September 25, 2010 1 INTRODUCTION Membrane proteins (MP) constitute about 30% of all known proteins and are one of the largest classes of drug targets.
They have roles in a multitude of biological processes such as cell recognition and neurotransmitter transport (Mller et al., 2008; Wallin and von Heijne, 1998).
Currently, the Universal Protein Resource (UniProt; Wu et al., 2006) contains more than 11 million protein sequences.
In comparison, the Protein Data Bank (PDB; Berman et al., 2000) contains only about 65 thousand known protein structures.
Both numbers have been following an exponential growth trend.
For MPs, the sequence-structure gap is even larger.
Over 1.7 million UniProt entries contain the word membrane, whereas only 4700 PDB entries match this criterion and only 616 structures are To whom correspondence should be addressed.
specifically annotated as belonging to the SCOP (Murzin et al., 1995) class membrane and cell surface proteins and peptides (the stated numbers were obtained from the respective databases on May 19, 2010).
Physically, MPs differ significantly from water-soluble proteins (Schulz, 2002; Stevens and Arkin, 1999; Ulmschneider and Sansom, 2001).
These differences have been used in various computational methods (Punta et al., 2007), e.g.to identify MPs from sequence alone (Gromiha et al., 2005; Wallin and von Heijne, 1998).
Soluble proteins often adopt a globular conformation, with their hydrophobic residues mainly in the protein core and their polar and charged residues predominantly on the water-exposed surface.
Membrane proteins sit in a lipid bilayer and thus contain stretches of residues that are exposed to the hydrophobic environment at the core of the membrane (Eyre et al., 2004).
These transmembrane (TM) segments usually have one of two structure types:-helices or-strands.
Many protein structure prediction algorithms have been developed in order to close the gap between the number of known sequences and the number of known structures.
Current structure prediction methods can be classified into two main types: templatebased and ab initio modelling.
So far, ab initio methods have only been truly successful for small globular proteins.
Template-based modelling methods, however, have succeeded in producing high accuracy models, if a good template exists, for proteins of almost any type or size (CASP; Moult et al., 2009).
Template-based protein structure prediction for any type of protein can be divided into several steps.
For globular, water-soluble proteins a multitude of programmes are available to perform each of these separate steps (Eswar et al., 2007; Wallner and Elofsson, 2005).
The input to the entire modelling procedure is a single target sequence, whose structure is to be predicted.
The steps of a typical template-based modelling protocol are briefly described below.
The first step is the identification of a template protein of known structure.
Structural databases are scanned for proteins homologous to the target protein.
One or more templates are then selected from the homologues.
By choosing one or more template structures, a 3D-fold is implicitly assigned to the target sequence.
A targettemplate alignment is then generated.
The alignment between target and template is one of the major contributors to the accuracy of the final model.
An incorrect alignment will almost always result in an inaccurate model (Snchez and Sali, 1997).
The final step in modelling is coordinate generation based on the alignment between template protein structures and the target sequence.
The three main approaches to this problem are The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:53 29/10/2010 Bioinformatics-btq554.tex] Page: 2834 28332840 S.Kelm et al.(i) assembly of rigid bodies (Bates et al., 2001; Deane et al., 2001; Koehl and Delarue, 1995; Petrey et al., 2003; Schwede et al., 2003), (ii) segment matching (Levitt, 1992) and (iii) satisfaction of spatial restraints (Sali and Blundell, 1993).
In order to improve the final model, this entire procedure may be iterated.
This has been shown to produce superior results as well as being able to correct errors in the initial alignment between target sequence and template structures (Burke et al., 1999).
The current computational structure prediction methods may not be ideal for TM proteins, designed as they are for water-soluble proteins (Elofsson and von Heijne, 2007).
The physical differences between water-soluble and MPs may mean that many of the steps in structure prediction should be approached differently.
The successful ab initio fragment assembly method ROSETTA has been adapted to specifically predict MP structure (ROSETTA Membrane; Yarov-Yarovoy et al., 2006).
The adapted method included an energy function that modelled a multi-layer artificial membrane environment.
The performance of ROSETTA Membrane was tested on 12 MPs of known structure, producing complete models with backbone root mean square deviations (RMSDs) between 6 and 10.
Sub-sets of each model (between 51 and 145 residues long) achieved RMSDs of 34.
More recently, the method was modified to include experimentally derived constraints, such as known helixhelix contacts (Barth et al., 2009).
The best models selected by the method had RMSDs of 4 , with all test proteins being single chains of at most 230 amino acids.
The authors ventured that for larger proteins, multiple constraints were likely to be required in order to obtain accurate results.
It should also be noted that, being a combinatorial ab initio method, ROSETTA requires large amounts of computing time and is typically run across large (possibly distributed) computing clusters.
Homology modelling methods on the other hand, while relying on the availability of a template, can be run on a single desktop computer.
In this article, we consider the coordinate generation step in a homology modelling framework and show how it can be significantly improved, without the use of experimental information, by considering the different physical environment of MPs.
Wallner and Elofsson (2005) demonstrated that existing coordinate generators, on soluble proteins, showed little difference in terms of overall accuracy and none of the methods consistently produced models that were much closer to the native target structure than the template.
In general, the models were also worse than simply copying the backbone co-ordinates of the template.
They identified Modeller (Sali and Blundell, 1993) as one of the best methods, due to its reliability and consistent model quality.
Modeller attempts to satisfy spatial restraints in order to build a target protein structure.
Its probability density function uses data obtained from the input targettemplate alignment, as well as prior knowledge obtained from a database of structural alignments (Sali and Overington, 1994).
This original database contained 105 families of soluble proteins and no MPs.
In this article, we use the most up-to-date version of Modeller (9v7, at the time of the experiment) as a representative of existing modelling methods and compare it to our own method.
Modellers accuracy for modelling MPs has been tested previously (Forrest et al., 2006).
It was noted that it was possible to build models whose TM region had <2 C-RMSD to the native structure, given a template with >30% sequence identity.
The accuracy for the whole protein was much lower than that of the TM region.
This reflected important local differences in the regions connecting TM segments in MPs with similar topology.
This result indicated that template-based approaches can be successfully applied to MPs.
However, this result also showed that Modeller (as a representative of the current standard in homology modelling software), in its current state, is not ideal for creating complete, accurate models of MPs.
A 2 RMSD in the TM region is surprisingly high, given that the problem of TM protein prediction should theoretically be simplified by the additional physical constraints imposed by the presence of the lipid bilayer.
Modelling errors in the TM region can then propagate to the loops that connect TM segments, thus resulting in even lower accuracy outside the TM region.
Nevertheless, scientists commonly use Modeller, and comparable methods, to predict MP structure (software reviews: Reddy et al., 2006; Saxena et al., 2008; example case studies: Fenosa et al., 2009; Yang et al., 2008).
We present MEDELLER, a new method for coordinate generation specialized for MPs.
The input is a template protein structure and a sequence alignment between the target and template proteins.
This alignment is not altered by MEDELLER.
The most important part of the method is the identification of the reliable core structure shared by the template and target proteins.
First, the template proteins membrane insertion is calculated using iMembrane (Kelm et al., 2009).
The core is initially restricted to the template residues buried in the middle layer of the membrane.
It is then gradually extended using a specialized membrane-specific substitution score.
The model is then completed, as far as possible, using the loop modelling protocols FREAD (Choi and Deane, 2009) and Modeller.
We test the modelling accuracy of our method on four large test sets, containing a total of 616 targettemplate pairs of TM proteins.
Our method, MEDELLER, builds highly reliable core models (which usually correspond to a proteins TM region).
Averaged over all test sets, MEDELLER produces more accurate core models and achieves a core model accuracy of 1.97 RMSD versus 2.57 for Modeller.
With added high-accuracy loops, MEDELLER remains the more accurate method in 65% of test cases and at least as good as Modeller in 77% of test cases, with an average accuracy of 2.62 RMSD versus 3.16 for Modeller.
2 METHODS We have created a homology-based protocol for coordinate generation.
The method is specific to MPs.
The algorithm is outlined below, with detailed explanations of the more complex steps following in separate sub-sections.
Figure 1 gives a flowchart of the algorithm.
All algorithm steps are identical, irrespective of the input proteins structure type (-helical or-barrel), although the substitution score itself is secondary structure dependent and will thus differ for residues in helices or sheets.
2.1 Algorithm overview 2.1.1 User input The input to our coordinate generation method is the target proteins sequence, aligned to one or more homologous template protein sequences, as well as the 3D co-ordinates of those template structures.
2.1.2 Annotation of the sequence alignment iMembrane (Section 2.2; Kelm et al., 2009) and JOY (Mizuguchi et al., 1998) are run on the template structure in order to annotate its membrane insertion and secondary structure.
2834 [09:53 29/10/2010 Bioinformatics-btq554.tex] Page: 2835 28332840 MEDELLER Fig.1.
Algorithm for modelling from a single template.
Progression from the initial user input to the high-accuracy model.
When modelling from multiple templates, steps up to stage 3 are performed for each template separately.
The data are then combined into a single core model (stage 3) and the algorithm proceeds as for a single template.
2.1.3 Building the coremodel The core is built over four phases (for full details, see Supplementary Section 1.1).
In each of these, a specific set of masking rules is in place (Section 2.3), alongside our smoothed fragmentbased environment-specific substitution score (section 2.4).
Once the four core modelling stages are considered complete the remaining gaps, where the target and template are not aligned or where the substitution score is low, are modelled using FREAD (Choi and Deane, 2009).
FREAD is a database search loop prediction method, which selects fragments based on an environment-specific substitution score and anchor RMSD.
FREAD prioritizes accuracy over coverage and thus may not make predictions for all the missing segments.
2.1.4 Prioritizing accuracy or coverage Our core building algorithm and our conservative use of FREAD are designed to give high accuracy coordinates.
However, the model produced may still contain gaps.
For the case where high coverage is required, MEDELLER also produces a highcoverage model, in addition to the default high-accuracy model.
In this case, FREAD is used on a less conservative setting, resulting in a larger number of possibly less accurate loop predictions.
Loops longer than 26 residues or terminal gaps cannot be modelled using FREAD.
MEDELLER thus builds a set of backbone coordinates for the majority of the structure.
For convenience, we include an option to fill any remaining gaps using Modeller, allowing the user to always output a complete all-atom model, including side chains.
2.2 Insertion of proteins into the membrane iMembrane is a method to ascertain a proteins position within the lipid bilayer (Kelm et al., 2009).
It relies on a database of known TM protein structures, which have been simulated in an artificial lipid bilayer using molecular dynamics (Scott et al., 2008).
iMembrane is used here to annotate the template protein structure with regards to its membrane insertion.
Since its original publication, iMembrane has been re-implemented and the output format consolidated.
TM align (Zhang and Skolnick, 2005) now performs the structure superposition and, as a result, all Z-scores have been replaced by TM scores in the programmes output.
The new version of iMembrane is included in the MEDELLER distribution and is available as a web server at http://imembrane.info.
2.3 Alignment column masking During each phase of the core building procedure, masks are used to prevent certain alignment columns from being selected.
Masked columns may be those containing a gap in target or template, those annotated as loop (where masking may be dependent on loop length) or those outside the middle layer of the membrane (where the lipid tails reside).
All rules are active at the beginning of the core building procedure and are then consecutively deactivated during the following phases of the algorithm (Supplementary Section 1.2).
2.4 Substitution score During core building, a smoothed fragment-based environment-specific substitution score Scand is used to determine the order, by which alignment columns are added to the models core.
During the later phases of core extension, a score cut-off is used at each alignment column selection step, in order to decide whether the core building process should be halted.
2.4.1 Environment-specific substitution tables (ESSTs) Using the membrane layer annotation provided by iMembrane (Section 2.2) and the secondary structure annotation provided by JOY, we have created 12 ESSTs specific to various structural environments in MPs (e.g.helix residues in the membrane tail region).
The procedure of creating these ESSTs is similar to SUBST by Kenji Mizuguchi (Shi et al., 2001) and newer methods based on it (Lee and Blundell, 2009).
In our procedure, we count the substitutions between one protein of known structure and many sequence homologues.
This drastically increases the number of observed substitutions in MPs, where few resolved 3D structures are available.
Our programme (JSUBST, written in Java) is available upon request.
For further detail see Supplementary Sections 1.31.4.
2.4.2 Calculating the smoothed fragment-based environment-specific substitution score Scand Our ESSTs are used to assign a raw score Sraw (Supplementary Section 1.4) to every column in the sequence alignment.
Sraw is smoothed over a window (of three residues, by default) to form Ssmoothed (Equation 1).
Scand is the score given to a candidate alignment column.
It is the sum of the smoothed scores Ssmoothed of all alignment columns already in the selected fragment, plus the candidates own smoothed score.
Ssmoothed,i = w j=w Sraw,i+j/ L (1) Scand,i =Ssmoothed,i + Flast j=Ffirst Ssmoothed,j (2) where i, j, Ffirst and Flast are alignment column indices; i is the index of the alignment column, whose score is to be determined; W is a constant dependent on the window size (V ), which is 3, by default; W = (V 1)/2; L is the actual number of scores inside the window (L = W in the normal case, but can be smaller if the window contains alignment gaps or extends past an end of the sequence); Ffirst and Flast are the indices of the first and last columns of the fragment to be extended, respectively.
2835 [09:53 29/10/2010 Bioinformatics-btq554.tex] Page: 2836 28332840 S.Kelm et al.2.4.3 Calculating the substitution score cut-off The substitution score cutoff Scutoff for each core extension phase is dependent on the previous phase and is calculated as follows: Scutoff = 23 Smean,prev + 13 Slast,prev (3) where Smean,prev is the mean score of the previous phase and Slast,prev is the score of the last-added residue in the previous phase.
2.4.4 Using Scand to determine selection order Assume an alignment with several fragments (consecutive stretches of alignment columns) already selected.
During a core extension iteration, one of the many fragments is extended by a single residue.
In order to decide which fragment is extended by which residue, the following steps are taken: (i) identify all possible candidates (unselected alignment columns adjacent to an already selected column); (ii) discard any masked candidates; (iii) calculate the substitution score of each remaining candidate (Equation 2) ; and (iv) discard all but the best-scoring candidatethis alignment column is now selected.
In the later phases of the core extension procedure, if the candidate score Scand is below a defined score cut-off (Equation 3), the candidate is rejected and core extension halted (Section 2.4.5).
2.4.5 Halting core extension When core extension is halted, it regresses (i.e.columns are removed in the reverse order they were added) until a local Scand score maximum is reached.
2.5 Multiple templates MEDELLER allows the user to provide more than one template protein.
In this case, the same algorithm (Section 2.1.3) is run for each template separately, in order to identify the fragments that constitute the common core between the target protein and each template.
Then, the fragments from all templates are pooled and the top-scoring subset of fragments is chosen to build the core model.
Two fragments are allowed to overlap only if (i) the overlapping region is shorter than half the length of the smaller fragment, (ii) there are no alignment gaps in the overlapping region and (iii) the backbone RMSD of the overlap is lower than 1.
Overlapping coordinates are melded (Choi and Deane, 2009).
After assembling the single best-scoring core model, the algorithm proceeds as for a single template (Section 2.1.4).
2.6 Testing the modelling accuracy Modelling accuracy was tested using the all-backbone-atom (C-, N, C, O) RMSD between a model and the native target X-ray structure, as found in the PDB.
In addition, we report GDT_TS (Zemla et al., 1999) scores in the Supplementary Material.
We also analysed the models TM region in terms of tilt angle and rotation angle and shift (as number of residues) relative to the native structure.
The accuracy of the models generated by our method was compared to the topmodel out of 10 equivalent models generated using Modeller with default settings.
In every case, both methods started from an identical ideal sequence alignment, generated from a structure alignment between the template and the native target structure.
The top Modeller model was selected using Modellers own DOPE energy score (Eswar et al., 2007).
To make the comparison fair, we calculated Modellers RMSD using only that sub-set of residues present in the MEDELLER model.
Two models with RMSDs to the native structure that differ by no more than 0.05 were deemed to be equally accurate.
We also conducted identical tests using the best Modeller model, which was selected from the set of 10 as the one with the lowest RMSD to the native target X-ray structure.
2.7 Sequence identity and coverage measures Sequence identity (ID) is calculated as the number of identical residues divided by the total number of alignment columns.
On this scale, a value of 20% identity corresponds to the twilight zone of sequence identity (Rost, 1999).
Target coverage (Cov) is the number of residues, for which the model provides 3D coordinates, divided by the total length of the target sequence.
Target core coverage (CoreCov) is calculated as above, except that the sequence is shortened to exclude any N-or C-terminal stretches of unmodelled residues.
2.8 Creation of the test sets A list of MP structures was compiled by combining data from several publicly available databases: all PDB entries annotated with the SCOP class membrane proteins, the OPM database (Lomize et al., 2006), the PDB TM database (Tusndy et al., 2005) and the CGDB database (Scott et al., 2008).
A list of unique PDB entries was compiled and filtered to include only X-ray structures with a resolution 3.
These structures were split into single protein chains.
The protein sequences were extracted and made non-redundant at a level of 80% sequence identity using CD-HIT (Li and Godzik, 2006).
Thus, none of the target proteins in any of our test sets share >80% sequence identity.
The remaining protein chains were run through iMembrane, in order to identify possible template structures in the CGDB database.
The structure search option was used; this method searches iMembranes database for homologous structures using pairwise structure alignment.
All iMembrane search hits with a TM score (TM align; Zhang and Skolnick, 2005) above 0.50 were kept.
Some target proteins did not receive any iMembrane hits and were thus removed from the dataset.
Pairs where target and template were of the same protein chain were also removed.
The remaining protein pairs were then classified into four test sets of varying sequence identity ranges.
The test set for modelling multiple templates is a subset of the above set and contains targets associated with at least two templates.
3 RESULTS 3.1 Test sets Our complete test set contains 616 pairs of protein chains (target template pairs).
Proteins of both the-helical (413) and the-barrel (203) type are included.
The targettemplate pairs were classified, by their sequence identity, into four test sets representing four different levels of modelling difficulty: easy set: 128 protein pairs, 40100% sequence identity.
medium set: 115 protein pairs, 2040% sequence identity.
hard set: 102 protein pairs, 1020% sequence identity.
hardest set: 271 protein pairs, 010% sequence identity.
A target may be paired with more than one template.
Conversely, a template may also be paired with more than one target.
The full list of targettemplate pairs is given in Supplementary Section 2.1.
Our test set is roughly two-thirds of the size of that used by Wallner and Elofsson (2005; 1036 representative protein pairs from different protein families) for testing co-ordinate generation in soluble proteins, and 17 times larger than the HOMEP set by Forrest et al.(2006; 36 pairs of MPs).
3.2 Modelling from a single template We ran our new coordinate generator, MEDELLER, as well as Modeller on all targettemplate pairs in our test set.
Modelling accuracy and target coverage were compared.
The average accuracy of both methods as well as MEDELLERs coverage are summarized in Table 1.
The distribution of both methods accuracy over all targettemplate pairs in the easy test set is shown in Figure 2.
On average, our method outperforms Modeller on the entire test set, 2836 [09:53 29/10/2010 Bioinformatics-btq554.tex] Page: 2837 28332840 MEDELLER Table 1.
Accuracy of MEDELLERs high-accuracy model versus Modeller Test More Less MED MOD Diff CoreCov Cov set acc (%) acc (%) RMSD () RMSD () RMSD () (%) (%) All 65 23 2.62 3.16 0.54 92 71 Easy 68 13 0.93 1.56 0.63 99 88 Med.
59 23 1.92 2.40 0.48 92 80 Hard 49 39 2.82 2.97 0.15 90 70 Hardest 72 21 3.64 4.32 0.68 88 58 Rows correspond to test sets.
The all test set is the union of all the test sets.
More acc and Less acc are the percentages of test cases where MEDELLER (MED) is more or less accurate, respectively, than Modeller (MOD).
If the two methods accuracies are within 0.05 RMSD of each other, they are deemed equal.
MED rmsd and MOD rmsd are accuracy values measured using the RMSD between the native structure and the MEDELLER or Modeller model, respectively.
Diff rmsd is the difference in accuracy between the two methods.
Cov is the percentage coverage of the target sequence by the MEDELLER model.
CoreCov is equivalent to Cov, but disregards uncovered terminal regions.
All values are averaged over an entire test set.
Fig.2.
Distribution of modelling accuracy for the easy dataset, when modelling from structure-based sequence alignments.
Backbone RMSD to the native structure achieved by MEDELLERs high-accuracy (MED HiAcc) and complete (MED Compl) models and the corresponding coordinates in the Modeller model (MOD HiAcc, MOD Compl).
MEDELLERs accuracy distribution peaks at 0.6 for both high-accuracy and complete models, compared with a 1.0 and a 1.5 peak for Modellers equivalent coordinates to the two model types.
in terms of accuracy, by 0.60 RMSD for the core model and by 0.54 RMSD for the high-accuracy model.
On our easy dataset, the difference in average modelling accuracy between the methods is the most visible, with 0.71 versus 1.46 RMSD for the core model and 0.93 versus 1.56 RMSD for the high-accuracy model.
This can also be seen in terms of GDT_TS with MEDELLER achieving a GDT_TS of 0.94 for high-accuracy models in the easy test set, whereas Modeller achieves 0.85.
GDT_TS scores for all the sets that are given in Supplementary Section 2.2.
The major advantage of MEDELLER lies in its highly reliable core models.
Compared with Modeller, our core models are at least 0.05 more accurate in 66% of test cases and at least as good (0.05 ) in 88% of test cases.
With added high-accuracy loops, MEDELLER remains the more accurate method in 65% of test cases and at least as good as Modeller in 77% of test cases.
The worst core models made by the two methods have respective RMSDs of 5.40 (MEDELLER) and 10.47 (Modeller).
Mostly, these inaccurate models are where target and template have different beta barrel diameters or considerable local changes in helix bundle geometry.
With added high accuracy loops, the worst accuracy values are 13.72 and 13.33 , respectively.
Any errors in the core models are propagated and result in a worse loop accuracy overall.
In terms of loop accuracy, both globally and locally, MEDELLERs high-accuracy models outperform Modeller, with average RMSDs of 5.9 (global) and 1.32 (local).
This relatively high global RMSD is mainly due to a number of loops whose general shape is correct, but which are at a wrong angle to the rest of the model, resulting in a very high RMSD.
Both MEDELLER and Modeller show relatively poor performance if all loops are built (complete model) with global RMSDs of over 11.
In terms of coverage, Modeller always provides a complete model.
However, its loop regions tend to be unreliable.
MEDELLER, on the other hand, provides four models: a core model with only highly conserved regions, a high-accuracy model containing high-confidence loops, a high-coverage model that includes lowconfidence regions and a complete model.
MEDELLERs modelling confidence is shown in the output using an atoms B-factor (Figure 3B).
All our tests were repeated when selecting the best out of the 10 Modeller decoys using the RMSD to the native structure, instead of Modellers DOPE energy score.
This did not change the overall trend of the results.
For further details, see Supplementary Section 2.2.
3.3 Modelling from pure sequence alignments All accuracy values reported in Section 3.2 are achieved when modelling from an ideal alignment, based on a structure superposition between target and template.
Whether such values can be achieved in a real modelling case always depends on the quality of the input alignment.
As a practical worst case scenario, we report accuracy values achieved when modelling from a simple pairwise sequence alignment, generated using MUSCLE (Edgar, 2004).
As expected, the resulting model accuracy was reduced for both MEDELLER and Modeller.
On average MEDELLERs high-accuracy model achieved a backbone accuracy of 0.96 compared with 1.78 for the equivalent Modeller coordinates, on the easy test set.
For further details, see Supplementary Section 2.3.
In practice, users will most likely use a more sophisticated alignment method, such as FUGUE (Shi et al., 2001), yielding somewhat better accuracies.
It should, of course, be understood that, even when using MUSCLE alignments as input, we still base our results on the prior knowledge that the chosen template is adequate for homology modelling (target template TM score >0.50; see Section 2.8).
In the regular modelling case, this knowledge is not available and one has to rely on the accuracy of homology detection methods.
3.4 Comparison to the nave model When modelling from an ideal alignment, the core selected by MEDELLER is on an average smaller but has a far lower RMSD (1.97 for MEDELLER versus 2.77 for the nave model).
The high-accuracy MEDELLER model covers on average a very similar fraction of the model (excluding terminal gaps) to the nave model and the average RMSD (2.62 ) is still lower than the nave model.
The same trend is observed when modelling from a pure sequence alignment (0.93 and 0.96 for MODELLERs core and 2837 [09:53 29/10/2010 Bioinformatics-btq554.tex] Page: 2838 28332840 S.Kelm et al.Fig.3.
Example model of human adenosine A2A receptor (PDB 3EML, chain A).
(A) Progression of model accuracy through the modelling process.
The modelling phases corresponding to the different MEDELLER models [Core, High Accuracy (HiAcc) and High Coverage (HiCov)] are labelled.
(B) MEDELLERs high-coverage model, coloured by modelling confidence in a blue-to-red spectrum, aligned to the native X-ray structure (transparent orange).
(C) MEDELLERs high-coverage model, coloured by membrane insertion using iMembrane [red, middle hydrophobic (tail group) layer; white, peripheral polar (head group) layers; blue, aqueous (non-membrane) layers].
high-accuracy models versus 1.15 for the nave model, on the easy test set).
For further details, see Supplementary Section 2.4.
3.5 Main chain bumps and transmembrane geometry The model quality assessment software WHATCHECK (Hooft et al., 1996) was run on the complete MEDELLER and Modeller models.
On average, both methods had a significantly higher amount of bumps between main chain atoms than the template and target structures.
Modeller produces slightly fewer bumps, thanks to its model refinement function.
For details, see Supplementary Section 2.5.
Using iMembrane, we also compared the accuracy of TM helix/ sheet geometry, relative to the native X-ray structure.
While both methods behaved very similarly, on average MEDELLER produced slightly better geometries.
MEDELLER and Modeller, respectively, had average TM shifts of 1.9 versus 2.3 residues, tilt angle deviations of 2.4 versus 3.4 and rotation angle deviations of 31.2 versus 36.0.
For further details, see Supplementary Section 2.6.
3.6 Modelling from multiple templates We tested both MEDELLER and Modeller on a set of 35 target proteins with at least two templates per target.
On average, MEDELLERs core model achieves an accuracy of 3.24 RMSD, compared with 3.32 for the corresponding Modeller coordinates.
This average is heavily biased by two outliers, where both methods each produced one less bad model but both methods accuracy was worse than 9.
After discarding these two test cases, the two methods average accuracies are 2.20 for MEDELLER and 2.44 for Modeller.
In almost every test case, both MEDELLER and Modeller could create a better core model when given only a single template structure (e.g.the one with the highest sequence identity to the target).
For further details, see Supplementary Section 2.7.
3.7 Example model: human adenosine A2A receptor The human adenosine A2A receptor (PDB 3EML, chain A) is a G-protein coupled receptor (GPCR).
The structure was resolved at a resolution of 2.6 and contains only a single gap in a loop connecting two TM helices.
In 2008, this protein was the subject of a CASP-style blind prediction competition (Michino et al., 2009) in order to assess the current state in GPCR MP structure prediction.
The most direct comparison possible is between values for TM helix accuracy from Michino et al.(2009) and our MEDELLER core models, which are roughly equivalent.
Our test set contains three models of 3EML (all from the hard set), with core accuracies (C-RMSD) of 2.1, 2.3 and 2.5.
The model in Michino et al.(2009) to best predict the TM helices had a C-accuracy of 2.1 (the model submitted by Davis, Barth and Baker).
The average TM helix (C-) accuracy for all submitted models was 2.8 0.5.
This places MEDELLERs models at the top end of the scale.
Of course, this comparison is not entirely fair, as our models are based on a structure alignment to the native structure.
However, Michino et al.(2009) reported that alignment did not seem to be a problem due to the conserved TM sequence patterns.
The high-coverage MEDELLER model with a core (C-) accuracy of 2.5 is shown in Figure 3.
The template structure is Opsin (PDB 3CAP, chain A), another GPCR.
The MEDELLER model is more accurate than the corresponding co-ordinates of the top Modeller model.
MEDELLERs backbone RMSD was 2.64, 2.65 and 3.57 for its core, high-accuracy and high-coverage models, respectively.
Modeller achieved an RMSD of 4.70, 4.83 and 5.11 for the corresponding coordinates.
Loops modelled using FREAD had the right general shape, even in the high-coverage model, whereas Modellers loops tended to be at the wrong angle relative to the core of the protein.
Only one very long loop was not modelled by FREAD.
Modeller produced coordinates for this loop but these were far from the loops position in the X-ray structure.
One particular mistake was made by both methods: failure to predict a helix kink that was present in the native structure but not in the template.
For further details, see Supplementary Section 2.8.
4 DISCUSSION We present MEDELLER, a new template-based coordinate generation protocol for MPs.
First, a common core between target 2838 [09:53 29/10/2010 Bioinformatics-btq554.tex] Page: 2839 28332840 MEDELLER and template proteins is defined, using membrane insertion and secondary structure information.
Then, any gaps in this core model are completed, as far as possible, using FREAD, a database search loop modelling algorithm.
Finally, any remaining gaps are filled using Modeller.
This results in four models with increasing target coverage: the core, high-accuracy, high-coverage and complete models.
In the output, coordinate reliability is indicated using B-factors.
MEDELLERs algorithm speed is comparable to that of Modeller (Supplementary Section 2.9).
MEDELLERs core and high-accuracy models are potentially incomplete.
However, the level of certainty in these coordinates is high.
We have shown that MEDELLERs core models (1.97 RMSD) are consistently more accurate than their corresponding coordinates in the Modeller models (2.57 RMSD).
Even adding high-accuracy loops, MEDELLER still outperforms Modeller in most cases.
MEDELLERs high-coverage model represents a tradeoff between accuracy and coverage.
At this coverage level, the two methods are, on average, approximately equal, with MEDELLER better in the easy set and Modeller better in the hard set.
Where complete coverage is required, we also produce a complete model by filling any remaining gaps using Modeller.
However, the accuracy of such coordinates is unreliable.
Here both methods have average RMSDs of over 10 , mainly due to large regions of the targets not aligned to a template in the hard and hardest test sets.
Only on the easy set, where unaligned regions are short, MEDELLERs complete models have a clear advantage with 2.80 RMSD versus 3.39 for Modeller.
In the medium set, Modellers complete models lead with 5.33 versus 5.84 for MEDELLER.
The fact that MEDELLERs core models are consistently better than Modellers, even though MEDELLER employs no structure optimization methods, suggests that Modellers probability density function, which was created to model soluble proteins, may distort the template structure of a MP.
The high quality of MEDELLERs core models is achieved by reliably selecting parts of the template structure that are similar to the correct target coordinates.
This is made possible by using membrane insertion annotation and an environment-specific substitution score along with the rule-based MEDELLER algorithm (Sections 2.1 2.4).
In terms of loop modelling, MEDELLER is the first software to use the recently revised FREAD algorithm (Choi and Deane, 2009).
For soluble proteins, FREAD guarantees consistent loop quality independent of loop length as long as the anchor structures are correctly modelled.
The high accuracy of our core models allows FREAD, in many cases, to produce accurate loop structures (highaccuracy models).
In our test set, FREAD sometimes does not correctly filter out loops which are similar in shape to the native structure but are attached at a wrong angle to the model core.
We have not observed this phenomenon when modelling loops in soluble proteins.
Future versions of MEDELLER will introduce MP-specific loop selection procedures to deal with such errors.
MEDELLERs loss in accuracy in the higher coverage models, especially at low target-template sequence identity, is due to small local structure differences in the MP core region.
These errors are amplified when adding loops to the model, based on such erroneous anchor structures.
Modeller lessens this dependency using its model refinement method.
This has two opposing effects, however: smaller errors at low target-template identity (which is desirable) and also worse core models overall (which is not).
The obvious way to improve model accuracy would be to either greatly reduce the size of the model core or to create a refinement method that corrects local structural differences such as helix kinks.
Helix kink prediction will allow better core accuracy, even with templates that are locally different from the correct target structure.
Future versions of MEDELLER will include such refinement methods, as well as a more accurate ab initio loop modelling protocol that should allow for better high-coverage and complete models.
ACKNOWLEDGEMENTS The authors thank the Oxford Protein Informatics Group for useful discussion and feedback.
Funding: Biotechnology and Biological Sciences Research Council (to S.K.
); University of Oxford Doctoral Training Centres (to C.M.D.).
Conflict of Interest: none declared.
ABSTRACT Motivation: The inference of pre-mutation immunoglobulin (Ig) rearrangements is essential in the study of the antibody repertoires produced in response to infection, in B-cell neoplasms and in autoimmune disease.
Often, there are several rearrangements that are nearly equivalent as candidates for a given Ig gene, but have different consequences in an analysis.
Our aim in this article is to develop a probabilistic model of the rearrangement process and a Bayesian method for estimating posterior probabilities for the comparison of multiple plausible rearrangements.
Results: We have developed SoDA2, which is based on a Hidden Markov Model and used to compute the posterior probabilities of candidate rearrangements and to find those with the highest values among them.
We validated the software on a set of simulated data, a set of clonally related sequences, and a group of randomly selected Ig heavy chains from Genbank.
In most tests, SoDA2 performed better than other available software for the task.
Furthermore, the output format has been redesigned, in part, to facilitate comparison of multiple solutions.
Availability: SoDA2 is available online at https://hippocrates.duhs.duke.edu/soda.
Simulated sequences are available upon request.
Contact: kepler@duke.edu Received on September 15, 2009; revised on February 3, 2010; accepted on February 4, 2010 1 INTRODUCTION B cells express immunoglobulin (Ig) molecules on their outer surface and secrete them into the extracellular space.
Secreted Ig is known as antibody.
The genes that encode for antibodies are generated by many diversifying mechanisms including combinatorial rearrangement of gene segments, addition of non-templated (n) nucleotides at the junctions, and somatic hypermutation.
This circumstance presents the important challenge of inferring the components of the original rearrangement for any observed Ig gene.
Because point mutations cause loss of information regarding the original rearrangement, there may be multiple plausible solutions.
In this article, we present a Bayesian statistical method based on a Hidden Markov Model (HMM) that allows a complete statistical To whom correspondence should be addressed.
treatment of the problem by providing the posterior probability of each possible rearrangement.
Antibodies serve as effector molecules that neutralize microbes by binding to exposed antigens and targeting them to other components of the immune system, such as phagocytic cells and complement, that effect clearance.
Ig genes generate diversity in two stages: an antigen-independent and an antigen-dependent stage.
Antigenindependent diversity is generated in the bone marrow, where B cells originate, by combinatorial rearrangement of gene segments and junctional diversity.
Combinatorial diversity is created in a number of ways.
First, each antibody molecule comprises one heavy-and one light-chain protein.
Both the light-and heavy-chain genes are encoded by gene segments that are genetically rearranged during a process known as V(D)J recombination (Sakano et al., 1980; Tonegawa, 1983).
Heavy chains are made up of three gene segmentsvariable (VH), diversity (DH) and joining (JH) where as light chains only have a V and J segment.
In humans, there are 50 known functional VH segments, 27 known functional DH segments, and six known functional JH segments (LeFranc, 2001).
This arrangement allows for 8100 combinations in the heavy chain alone.
Humans also have two light-chain loci, (Lorenz et al., 2001) and (Frippiat et al., 1995).
Only one of these loci is expressed per cell so that each antibody either has a light chain or a light chain.
Humans have 44 functional V, 5 J, 33 V genes and 5 J (LeFranc, 2001) resulting in 220 possible chains and 165 possible chains.
Thus this combinatorial rearrangement alone allows for greater than 3 million antibodies.
Junctional diversity is the result of multiple recombination site choices for each recombination event and the addition of n nucleotides.
n nucleotides are sometimes added at the junction by terminal deoxynucleotidyl transferase (TdT) between adjoining gene segments (Desiderio et al., 1984).
Although TdT is believed to be expressed only in pro-B cells, the stage in which the heavy-chain rearrangement takes place (Desiderio et al., 1984), the presence of n nucleotides in light chains has also been seen in a few studies (Bridges, 1998).
Antigen-dependent diversity is generated by somatic hypermutation in the periphery in a manner dependent on activationinduced cytidine deaminase (AID); during this process, mutations in the Ig genes are accumulated at rate of up to 106 times the normal background rate (Muramatsu et al., 2000).
B cells are subsequently selected for enhanced affinity for the eliciting antigen.
It is estimated that these processes of diversification can generate 1012 different antibodies making it challenging to correctly identify the underlying The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
https://hippocrates.duhs[14:32 17/3/2010 Bioinformatics-btq056.tex] Page: 868 867872 S.Munshaw and T.B.Kepler germline gene segments and subsequently the sequences of the complementarity determining regions (CDRs).
The inference of the recombination and mutation events that produced a given Ig gene is of great importance in the study of humoral immunity and has been tackled in many different ways.
The goal of such inference is to identify each of the component gene segments used as well as the recombination sites, point mutations and n nucleotides.
The aligned gene segments usually overlap, which is why alignments of the target gene to the individual gene segments cannot be treated as independent.
Somatic mutations, n nucleotide addition and recombination site choice make this task more challenging.
The short length of the DH gene segment makes it especially difficult to identify the CDR3 region of the heavy chain, which is the most diverse region in the antibody sequence.
This leads to many possible gene segment combinations that can result in a given antibody gene.
Hence, it is necessary to report all such rearrangements and assign a probability to each of the combinations, making it easy to compare all possible rearrangements.
Several algorithms have been developed for inferring Ig gene segment composition.
IMGT/V-QUEST is one of the first and most complete of these tools and has the ability to analyze both Ig and TCR sequences for a variety of organisms including human and mouse (Guidicelli et al., 2004).
V-QUEST, however, is based on the BLAST algorithm; it does not guarantee finding the best alignment of two sequences (Altschul et al., 1990).
Additionally, the implementation of the algorithm only allows for running a maximum of 50 sequences at a time.
Another tool, JOINSOLVER, is based on the identification of conserved motifs in the target gene (Souto-Carneiro et al., 2004).
Both JOINSOLVER and V-QUEST provide multiple gene segment possibilities but the implementation only provides junction analysis for the topmost choice.
Somatic diversification analysis (SoDA) (Volpe et al., 2006) uses a 3D alignment algorithm that allows for insertions and deletions.
The algorithm uses dynamic programming and is an extension of the SmithWaterman local alignment algorithm (Smith and Waterman, 1981).
The 3D alignment allows for a continuous alignment through all the states of the recombination.
SoDAinfers only a single highestscoring alignment, and ignores other solutions that may have equal or nearly equal scores.
SoDAs guarantee of optimality in the inferred rearrangement is obtained at the cost of computational effort; SoDA takes more CPU time than either JOINSOLVER or V-QUEST.
A major shortcoming for all the programs above is that they do not provide a meaningful comparison of the different possible rearrangements.
iHMMune-align (Gaeta et al., 2007) partially solves the problem and provides a probabilistic model using an HMM to infer the rearrangement.
iHMMune-align uses the Viterbi algorithm (Rabiner, 1989) to find the most probable path through the alignment matrix, but does not sum over paths or provide results on suboptimal alignments.
This choice becomes an issue when selecting an appropriate DH gene segment for Ig heavy chains.
The DH gene is the shortest of all gene segments, and is typically the most difficult to align.
We have found Ig genes that present an equally good alignment with different DH genes (see Results section and Figure 5).
iHMMune-align or SoDA gives only the solution with the highest score even if the highest score is not significantly better than the second highest score and so on (iHMMune-align does provide the option of viewing the top 10 VH gene alignments, but not DH).
Among these four methods, only SoDA allows for gaps when performing alignments, although insertions and deletions are known to occur at non-negligible frequencies during somatic hypermutation (Smith et al., 1996), and alignment without gaps when gaps are present leads to dramatically erroneous inferences.
The method we are introducing here is an update of SoDA we call it SoDA2.
It employs a probability mass function-based alignment for determining gene segments and a probabilistic HMM for the inference of CDR3.
The system calculates the posterior probability over all paths using a particular set of gene segments by the forward and backward algorithms.
It then provides the alignment path with the highest posterior probability.
If the sequence does not hold enough information to unambiguously select a gene segment, SoDA2 reports all alignments that do not differ significantly.
We tested this method using a simulated dataset constructed from the statistics of observed rearrangements and compared these results with those obtained using existing methods.
We also used two natural datasets, a set of clonally related Ig genes and a random set of sequences from NCBI.
Each test indicates that SoDA2 provides the most thorough and accurate results among all programs in addition to providing the most statistically complete results.
2 METHODS 2.1 Determining the type of Ig The first step consists of aligning the target sequence with a consensus-like sequence of the VH, V and V families to determine if the input sequence is a heavy, kappa or lambda chain.
These consensus sequences are pre-created by separate alignments of the VH, V and V segments.
We use the AHO numbering scheme (Honegger and Plckthun, 2001) which is based on the spatial alignment of known 3D structures of Ig domains.
The gaps are placed to minimize the average deviation from the averaged structure of the aligned domain so that the position of the CDRs remains consistent.
The consensus is represented by a probability mass function (pmf), a L 5 matrix where L is the length of the V genes in this case (Kepler et al., submitted for publication).
For each nucleotide position in the gene, we determine the frequency of use for each nucleotide state (including gap) at that position in the family.
For the target antibody, we create a similar pmf using the quality scores of the input sequence.
The quality score is proportional to the log probability of the estimated sequencing error and is provided by the users base-calling software (Ewing and Green, 1998).
If quality scores are not provided, we treat the input sequence as well-determined and all mismatches as due to somatic mutation.
The pmf at each position of the target sequence depends on the quality score, which varies at each position, and a mutation frequency which is assumed to be constant over positions.
For each position, we then have the probability of observing the five bases (including a gap) at that position given the quality score and the mutation frequency.
We use the pmf of the target antibody gene and the pmf of the VH, V and V sequences as scores to create a local alignment (Kepler et al., submitted for publication; Smith and Waterman, 1981).
2.2 V and J gene pre-alignment Assume, for example, that our target sequence has been determined to be a heavy chain.
We use the trace-back path generated by aligning the pmf of VH to our target and obtain the likelihood for each member of the VH family.
The mutation frequency is recalculated after observing mismatches in the highest scoring alignment.
All VH segments with equally high likelihood alignments are then submitted to the HMM.
The position of the invariant cysteine is determined.
The target sequence is then aligned past the invariant cysteine with all the appropriate JH segments, using the pmf-based alignment mentioned above.
The JHs with the highest likelihood are selected for submission to the HMM.
The target sequence is further trimmed at the invariant 868 [14:32 17/3/2010 Bioinformatics-btq056.tex] Page: 869 867872 SoDA2: identification of immunoglobulin rearrangements Fig.1.
The basic topology of the HMM for (a) heavy chains and (b) kappa and lambda chains.
The HMM starts at the last base of the invariant cysteine of all high-likelihood V segments, runs through all DH segments in the case of heavy chains, and through all high-likelihood J segments till the first base of the invariant tryptophan or phenylalanine.
tryptophan/phenylalanine, and only the remaining region, CDR3, is used as our target sequence for the HMM.
The 3 ends (post-invariant cysteine) of all significant VH gene segments and the 5 ends (before invariant tryptophan/phenylalanine) of all JH segments from the pre-alignment are also chosen for the HMM.
Since DH segments are most difficult to identify, we submit all DH segments to the HMM.
The mutation frequency of the final trimmed target sequence to be considered for the HMM is set at 1.5 since the CDR3 region is subject to higher mutation than the VH region (Cowell et al., 1999; Gaeta et al., 2007).
Figure 1 shows the basic set-up of the HMM for heavy chains (Fig.1a) and light chains (Fig.1b) with an overview of the states and allowed transitions.
2.3 HMM We implemented a pair HMM with 10 non-silent statesmatch/mistmatch state in V gene, insertion in V gene (Iv), deletion in V gene (Dv), VD junction n nucleotides, match/mismatch in DH gene, insertion in the DH gene (Id), deletion in the DH gene (Dd), DJ junction n nucleotides, match/mismatch in the J gene, insertion in the J gene (Ij) and deletion in the J gene (Dj).
Our HMM must begin in the match/mismatch state of the V gene since the invariant cysteine is encoded by the V. The end state must be match/mismatch in the J gene at the beginning of the invariant tryptophan/phenylalanine.
The emission probabilities in every state are determined by the pmf calculated using the quality scores and the mutation rate.
For target sequences with unknown quality scores, high-quality scores are assumed, making the probability of the observed base depend only on.
Emission probabilities for the n nucleotide states are determined based on empirical data (Basu et al., 1983).
Transition probabilities between states are determined by fitting a negative binomial distribution to the recombination site choice for VH, DH and JH and number of n nucleotides in the junctions as determined in a set of 293 unmutated sequences rearranged sequences (Fig.2, Jackson et al., 2004).
Figure 3 shows a detailed implementation of the HMM with transition probabilities.
2.4 Algorithm Once we have the appropriately trimmed target and germline sequences, we calculate the log of the total probability of a proposed rearrangement using the forward and backward algorithms (Durbin et al., 1998; Majoras, 2007).
We select the gene segments that lead to the highest posterior probabilities, and perform a posterior Viterbi algorithm with traceback (Fariselli et al., 2005) to select the path with the highest posterior probability for each possible rearrangement.
We report the probability of the most probable path for each of the equally probable gene segment sets.
For a heavy chain, a DH gene alignment of <3 nt is flagged as unreliable D alignment.
The functionality of an antibody gene is determined as follows and reported with the results.
A functional Ig chain must have no stop codons and the invariant cysteine Fig.2.
Distributions for (a) VH gene recombination site choice, (b) n nucleotides in the VD junction, (c) 5 DH recombination site choice, (d) 3 DH recombination site choice, (e) n nucleotides in the DJ junction, (f) 5 JH recombination site choice.
All the data is fit to negative binomial distributions with varying parameters derived from Jackson et al.(2004).
These parameters are used for transition probabilities in the HMM.
Fig.3.
Shows a detailed topology of the HMM with all possible transitions.
Each nucleotide in the observed sequence is treated as a separate state.
The transition probabilities are derived from empirical data (Jackson et al., 2004).
The star denotes the start (third position of invariant cystiene) of the HMM and the + denotes the end (first position of invariant tryptophan/phenylalanine).
The I and D in every state stand for insertions and deletions, respectively.
at the start of CDR3 must be in-frame and intact.
For heavy chains, the invariant tryptophan at the end of CDR3 must be in-frame and intact; for light chains, CDR3 must end with an in-frame and intact phenylalanine.
We provide color-coded output in HTML, text and excel formats to allow the user to use the information in ways most convenient to his or her needs.
3 RESULTS 3.1 Simulated datasets We created simulated datasets of 100 sequences each with mutation frequencies of 2.5, 5, 10 and 20%.
Recombination site choice and number of n nucleotides for these simulations were drawn from a negative binomial distribution.
To avoid any bias towards our HMM, the parameters for these simulations were estimated using a set of 662 sequences obtained from Genbank.
Furthermore, rearrangements for these sequences were determined 869 [14:32 17/3/2010 Bioinformatics-btq056.tex] Page: 870 867872 S.Munshaw and T.B.Kepler Table 1.
Number of correct rearrangements (correct gene and allele for VH, DH and JH segments) identified by each software out of 100 sequences tested at each mutation rate 2.50% 5% 10% 20% SoDA2 73 65 47 28 IMGT/V-QUEST 52 47 42 16 JOINSOLVER 59 47 34 11 iHMMune-align 41 31 22 12 SoDA 46 30 31 6 Table 2.
Number of VH, DH and JH genes (with alleles) identified at each mutation rate for the simulated sequences 2.5% 5% 10% 20% V D J V D J V D J V D J SoDA2 97 76 98 94 73 94 87 58 88 85 42 78 JOINSOLVER 90 65 98 83 61 92 81 42 85 72 20 76 IMGT/Vquest 99 52 94 97 49 93 93 45 89 88 23 82 SoDA 79 65 92 77 68 87 76 42 69 69 20 45 iHMMune-align 87 48 90 86 42 87 78 32 86 69 21 61 using IMGT/VQuest (Guidicelli et al., 2004) rather than SoDA or SoDA2.
IMGT Junction Analysis was used to determine empirical data for deriving the distributions (Monod et al., 2004).
Mutations were introduced such that the average mutation frequency across the gene was 2.5, 5, 10 and 20%, and the mutation frequency in the CDRs was 2 than that in the framework.
Each of these datasets was used to test SoDA2, SoDA, IMGT/VQuest, JOINSOLVER and iHMMune-align.
Inverted D segments were omitted from the simulations because IMGT/VQuest and iHMMune-align do not allow for alignments against them.
Table 1 shows the results of running our simulated datasets using the various available software.
The table shows the number of rearrangements (all VH, DH and JH with alleles) identified correctly at each mutation rate by each program out of the 100 sequences tested in each group.
For all our tests, we only compare the highest scoring rearrangement provided by SoDA2 with the highest scoring ones provided by other programs.
For our simulated data, we see that SoDA2 performs better in identifying the complete rearrangement (including correct alleles) than other programs under all mutation rates (Table 1).
In particular, SoDA2 outperforms all other programs in DH segment identification (Table 2).
SoDA2 falls slightly behind IMGT/V-Quest in VH and JH gene identification due to the trade-off between accuracy and efficiency.
We employ a computationally efficient alignment algorithm that aligns the target gene to consensus sequences of alleles, which can lead to the identification of the incorrect allele in a very few cases.
Aligning the target gene to every allele would decrease this error but increase computation time significantly.
Such errors are seen rarely and do not change the overall superior performance of SoDA2 shown in Table 1.
If the score for multiple rearrangements is equal for any of the programs, all rearrangements are considered.
Although SoDA2s performance falls at the 20% mutation rate, it still performs better than other software.
We only report all alignments that are equally probable and leave it up to the user to select and view any number of V, J or complete alignments he or she wants.
For sequences where SoDA2 failed to identify the correct rearrangement as the most probable one, we found a median difference of 0.67 in the natural log of the probability between the highest scoring rearrangement and the correct one at the 5% mutation rate.
Thus, if allowed to include rearrangements with low differences (<1) in the natural log of the probability from the top scoring alignment, SoDA2 would have identified correct rearrangements for 22 additional sequences at the 5% mutation rate, yielding a possible 87% success rate.
3.2 Clonally related datasets In order to test real biological data, we used two clonally related datasets that were used to test iHMMune-align (Gaeta et al., 2007) derived from tonsilar IgD class-switched B cells (Zheng et al., 2004).
Because they are clonally related, sequences within a given set should have identical rearrangements and differ only by somatic mutation.
We analyzed this dataset using VQuest, JOINSOLVER, SoDA and iHMMune-align to determine the number of times each of the programs resulted in the same rearrangement as was done by Gaeta et al.(2007).
We ran the sequences through all the programs and found that iHMMune-align selected 47/57 identical rearrangements for the first group of sequences, while SoDA2 selected 34/57 identical rearrangements.
IMGT/VQuest, JOINSOLVER and SoDA identified 37, 25 and 18 identical rearrangements, respectively.
SoDA2 returned a minority DH gene segment in 17 cases, a minority JH allele in five cases, and a minority VH allele in four cases.
In cases where SoDA2 failed to select the majority VH or JH gene segment, all the other programs, including iHMMune-align also failed to select the majority gene segment.
It can be seen in these cases that mutation had obliterated the information necessary to make the correct inference.
For the 17 cases where SoDA2 did not return the majority DH segment, the DH segment that was returned was typically judged more probable than the majority segment due to the balancing of n-nucleotide use and mutations.
An example of this phenomenon is the inference for AF262199 (Fig.4).
In this case, the mutation frequency in the VH gene segment is 7%.
SoDA2 selects IGHD126*01 requiring three mutations (8.5% mutation frequency in CDR3) and seven n-nucleotides, while IGHD727*01 requires two mutations (5.5% mutation frequency) and 10 n-nucleotides.
For the second dataset of 99 sequences, both iHMMune-align and SoDA2 identified 68/99 identical rearrangements while IMGT/VQuest, JOINSOLVER and SoDA identified 56, 41 and 37 identical rearrangements, respectively.
3.3 Sequences from Genbank We tested a set of 662 sequences collected from Genbank and previously used for testing iHMMune-align and SoDA (Genbank accession nos Z68345-487 and Z80363-770).
Out of 662 sequences, 113 produced inferences on which all five programs agreed.
There was no agreement from any of the programs on 140 sequences.
This means that they either could not infer a rearrangement at all or they all differed in their inference.
From the rest, SoDA2 agreed with the majority of the programs on 300 rearrangements (Table 3).
These did not include those where SoDA and SoDA2 were the only 870 [14:32 17/3/2010 Bioinformatics-btq056.tex] Page: 871 867872 SoDA2: identification of immunoglobulin rearrangements Fig.4.
(a) Top rearrangement as chosen by SoDA2 with a higher mutation frequency than the alternative, shown in (b).
The different rearrangements represent a trade-off between mutation frequency and number of n nucleotides.
two in agreement and the chosen rearrangement was the majority.
SoDA2 performs considerably better than other programs in this test.
We closely examined sequences for which SoDA2 failed to agree with two or more programs.
We found a median difference of 1.05 between the top scoring rearrangement and the majority rearrangement.
We also found that in all cases, SoDA2 selected an alternative rearrangement equally likely as the majority one.
Figure 5 shows an example of one such sequence where SoDA2 selected IGHD221*01 to be the best fitting DH alignment with a score of 785.07 (Fig.5a).
On allowing alignments with a slightly higher Table 3.
Results from 662 sequences from Genbank, showing the performance of the five programs Number of rearrangements All programs agree 113 All programs disagree 140 SoDA2 agrees with two or more programs 300 VQuest agrees with two or more programs 255 iHMMune-align agrees with two or more programs 137 JOINSOLVER agrees with two or more programs 272 SoDA agrees with two or more programs 244 SoDA2 agrees only with SoDA (no other programs agree) 11 If two or more programs displayed the same rearrangement (including the alleles), it was believed to be the majority rearrangement.
probability, we found both the rearrangement chosen by the majority of the programs (VQuest, JOINSOLVER and iHMMune-align, Fig.5b) and also the rearrangement selected by SoDA (Fig.5c).
The difference in the natural log of the probability is 0.63 in the first case and 0.93 in the second.
This shows that allowing rearrangements within a reasonable range of probabilities in SoDA2 would give an accurate and thorough picture of the various rearrangements possible for a given Ig sequence.
It is important to note that SoDA2 considers factors such as recombination site choices for each gene segment and numbers of n nucleotides at both junctions derived from empirical data in inferring rearrangements while alignment algorithms used by SoDA, VQuest and JOINSOLVER base their results on sequence similarity matrices which may not accurately represent the process of V(D)J recombination.
4 CONCLUSION The problem of inferring the correct rearrangement for antigen receptors is difficult due to the stochastic nature of the process, but the task is important for an increased understanding of the population somatic genetics of the immune response.
In this paper we present a method based on an HMM that provides a statistical basis for identifying rearrangements of Ig genes.
In addition to providing the posterior probability of the top rearrangement candidate, SoDA2 also provides the user with an option to see all rearrangements with sufficiently high posterior probabilities, thus giving the user a statistically complete picture of the observed sequences origins.
We tested SoDA2 against simulated datasets that were created using empirically observed recombination site choices for each of the gene segments and numbers of n nucleotides in the junctions.
We also tested it on two clonally-related datasets as well as a set of Ig heavy chains chosen randomly from Genbank.
Our software performed as well as or better than available software on two out of three validation tests.
The one test where SoDA did not outperform all of the others involved a single rearrangement.
On the identical test with a different rearrangement, SoDA did as well as its nearest competitor.
It is important to realize that the key feature of this article is to provide a tool based entirely on a probability model, and that therefore returns results interpretable as posterior probabilities rather than arbitrary scores.
As with other inferential procedures, it is important to not only identify the optimal solution, but to identify near-optimal solutions and have a Fig.5.
The alignment of CDR3H of sequence by 1154693 using IGHD1-21*01 by (a) SoDA2, (b) IMGT/V-QUEST, JOINSOLVER and iHHMune, (c) SoDA.
Rearrangements (b) and (c) were also provided by SoDA2 at a slightly lower probability.
871 [14:32 17/3/2010 Bioinformatics-btq056.tex] Page: 872 867872 S.Munshaw and T.B.Kepler method for the absolute comparison among these alternatives.
This performance and thorough result reporting leads to a substantially longer computation time.
SoDA2 takes 15 s of real user time per set of V and J segment for a given heavy target sequence on a 64 bit machine with a 2.19 GHz processor and 4 GB RAM, but the investment of computational effort seems worthwhile.
ACKNOWLEDGEMENTS Special thanks to William H. Majoros, Institute for Genome Sciences and Policy, Duke University and Todd Wasson, Computational Biology and Bioinformatics Program, Duke University for very helpful discussions on implementing Hidden Markov Models.
Funding: We are grateful for the financial support of the Bill and Melinda Gates Foundation through grant number 38643 to Dr Barton F. Haynes.
Conflict of interest: none declared.
ABSTRACT Motivations: Spreadsheet-like tabular formats are ever more popular in the biomedical field as a mean for experimental reporting.
The problem of converting the graph of an experimental workflow into a table-based representation occurs in many such formats and is not easy to solve.
Results: We describe graph2tab, a library that implements methods to realise such a conversion in a size-optimised way.
Our solution is generic and can be adapted to specific cases of data exporters or data converters that need to be implemented.
Availability and Implementation: The library source code and documentation are available at http://github.com/ISA-tools/ graph2tab.
Contact: brandizi@ebi.ac.uk.
Supplementary Information: A supplementary document describes the theoretical and technical details about the library implementation.
Received on February 24, 2012; revised on April 10, 2012; accepted on April 24, 2012 1 INTRODUCTION Spreadsheet-like tabular formats are popular data exchange means in the biomedical field (Kuchinke et al., 2009; Rayner et al., 2006, 2009; Sansone et al., 2008).
The idea of representing complex structures, such as experimental designs, by means of matrices makes these formats easy to understand by end users, who can use familiar spreadsheet software products or specific applications (Rocca-Serra et al., 2010, Shankar et al., 2010) to manage experiment reporting and other biomedical data (Field et al., 2010).
While working with such formats is relatively easy for software developers too, it is highly desirable to build and share reusable software components for processing them.
In this article we describe graph2tab, a (Java-based) programming library that allows a developer to deal with the specific (yet recurrent) problem of converting the workflow graph of a biomedical experiment into a tabular representation, defined according to a given tabular format, such as ISA-Tab (Sansone et al., 2008) or MAGE-TAB (Rayner et al., 2006).
While the basics of such a conversion procedure are well known (http://www.mged.org/mage-tab/spec1.0.html; Rayner et al., 2006) our experience shows that implementing all the details is not trivial.
Considering this, we have developed the graph2tab library in a generic way, so that it can be adapted to a variety of concrete cases.
A software developer can leverage on it to build tools like data exporters or data format converters.
This can be done To whom correspondence should be addressed.
by adapting the library to a particular combination of an input object model and an output tabular format.
In the following we describe the specific problems related to such conversion and the solutions that graph2tab offers to them.
1.1 Covering a DAG with a minimum path set The input to the problem we deal with is essentially a directed acyclic graph, or DAG (Fig.1a), which represents the execution workflow of a biomedical experiment, such as a microarray study or a clinical trial.
The nodes of such graph can be about a biological product, such as a tissue sample, experiments output, such as a file of geneexpression levels, or an experimental step, such as a reference to a laboratory protocol.
Moreover, usually sets of typed attributes (i.e.header/value pairs), like the organism name a sample is about, are attached to the nodes.
Such a graph can be converted to a headed table (Fig.1b), such that each table row represents a particular path in the workflow, while columns are used to report the nodes/entities and their attributes.
Further rules allow one to represent complex topologies.
For example, in the case of Figure 1b, having sample 1 in two subsequent rows means the table values are about the same node (crossed by two paths).
Also note that the graph in Figure 1a could be reconstructed by means of more than two paths (and hence more than two rows), for example having two paths starting from source 1 and a third starting from source 2.
However, what is usually wanted is a most compact set of paths (and hence number of rows) that allows reconstruction of the original graph.
It can be shown that such solution corresponds to the minimal set of paths that covers all the edges and all the nodes of the graph.
A very convenient way to solve this problem is reducing it to a minimum flow problem, for which well-known efficient algorithms are available (Ciurea and Ciupal, 2004).
We refer the reader to the Supplementary material for the details on how we do the transformation to a minimum flow case and the reasons why we have chosen to exploit the FordFulkerson algorithm (Ford and Fulkerson, 1987) for solving this problem.
1.2 Layering We associate a layer index to each node, which reflects the node arrangement in Figure 1a, where each layer contains nodes of the same type and can be used to report the nodes in a table, using a homogeneous set of columns (Fig.1b).
While the general approach to achieve such a layering is known (http://www.mged.org/ mage-tab/spec1.0.html), a number of details and ambiguous situations have to be dealt with in a concrete implementation.
For example, that the columns about the labelled extract 1 in Figure 1a The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: AA MANUSCRIPT CATEGORY: APPLICATIONS NOTE [19:38 17/5/2012 Bioinformatics-bts258.tex] Page: 1666 16651667 M.Brandizi et al.Fig.1.
(a) an experimental workflow graph and how it is layered.
The node type difference between Protocol 1 and Sample 2 causes the layering gap after Sample 1 (b) the graph converted into a table (assuming nodes expose the attributes reported) Fig.2.
(a) the Node interface, which is the key to make graph2tab generic (b) the node attributes are encoded in the recursive TabValueGroup structure, which makes it possible to prepare a suitable result (c), where values about the same headers are grouped together.
Attributes are nested according to the required output format.
In this example, the term accession is nested within the free-text value attribute about the organism, to reflect the fact that the identifier column has to be reported next to the Organism column.
In turn, the term source column is at the third nesting level, since the corresponding column must appear next to Term Accession come after the sample 2 has to be told to a computer somehow, in order to avoid the opposite column order.
In graph2tab, we allow one to specify this for a particular tabular format by means of the order property in the Node interface (Fig.2a).
A concrete value for such property may be based on a pre-defined list of types, as well as on more sophisticated criteria, such as, for ambiguities involving protocols, the ontology terms about the protocol types (for instance, to establish that a sample treatment protocol comes before a labelling protocol).
Further details about the layering algorithm can be found in the graph2tab user guide (https://github.com/ISA-tools/graph2tab/blob/ master/ graph2tab_intro.pdf) and in the Supplementary Material.
1.3 Node headers and cell values Once the minimum covering path set is computed and the node layers are established, the final table can be built by walking through every path and, for each node, reporting its contribution to the table headers and cell values, where such contribution consists of the node attributes mentioned above.
In order to ease this last step, we require that a graph node exposes a nested structure of header/value pairs (Fig.2a and b).
This way, it is possible to take into account how the logic of the target format requires the columns to be grouped together and to progressively merge the attributes provided by each node into the final result (Fig.2c).
2 USE CASES AND DISCUSSION So far we have used the graph2tab library for realizing two database-to-tabular format exporters, one for the ISA software suite (Rocca-Serra et al., 2010) which exports into the ISA-Tab format (Sansone et al., 2008), and the other for the ArrayExpress repository (Parkinson et al., 2011), which exports experimental data into the MAGE-TAB format (Rayner et al., 2006).
Other cases where we have used the library are: a MAGE-ML-to-MAGE-TAB converter 1666 Copyedited by: AA MANUSCRIPT CATEGORY: APPLICATIONS NOTE [19:38 17/5/2012 Bioinformatics-bts258.tex] Page: 1667 16651667 graph2tab (http://sourceforge.net/projects/mageml2tab/) and a data exporter for the Limpopo library (http://limpopo.sourceforge.net/).
Other tools similar to graph2tab exist.
For instance, the caArray microarray data management software is able to export its data as MAGE-TAB files (https://cabig.nci.nih.gov/community/tools/ caArray).
Similarly, the BASE system (Vallon-Christersson et al.2009), has a plug-in for exporting data into the Tab2MAGE format (http://baseplugins.thep.lu.se/wiki/uk.ac.ebi.Tab2MageExporter), a precursor of MAGE-TAB.
Compared to these tools, graph2tab has the novel features of being a generic implementation of a sizeoptimized graph-to-table conversion, which, as described in this article, embeds the solution to several recurrent problems.
In fact, the library can be adapted to any object model that encodes DAGs and any tabular format that represents DAGs by reporting one path per row and one attribute type per column.
As we describe in detail in the user guide (https://github.com/ISA-tools/graph2tab/ blob/master/graph2tab_intro.pdf) and in the on line code examples, this flexibility is achieved by letting developers to extend the library to accommodate their particular cases, for example a data exporter or data format converter.
The expected input in such scenario is an object model (i.e.a set of Java objects instantiating predefined classes), which needs to be adapted to the Node interface in Figure 2a.
This will include the definition of the getTabValues() method, i.e.of the headers and values to be generated in the output table, according to the particular tabular format that has been chosen as output.
Another benefit of the graph2tab implementation is that it makes available a graph-to-table conversion procedure that has been tested both from the theoretical point of view (see the Supplementary Material) and against real cases.
Using the minimum path coverage of the input graph as a basis for the subsequent table generation corresponds to finding a maximally compact tabular representation of the original graph, a highly desirable feature, especially for the case of complex input workflows with many pooling and splitting operations.
A possible limit of this approach is that, when the converted graph comes from an original submission based on a tabular format, the conversion result (e.g.a re-export needed after data review) is not necessarily identical to the original submission (e.g.if this was redundant and not minimal).
In our experience, usually this is not considered as a problem by end users.
We expect that the graph2tab library will be useful in more similar cases in future, given the importance of experimental workflows in the biomedical field and the increasing popularity of tabular formats.
Moreover, since the biomedical experimental workflows are a particular case of more general provenance models (McCusker and McGuinness, 2010), the library has a potential to be useful in other application domains.
ACKNOWLEDGEMENTS We thank Tony Burdett for the support with ArrayExpress and Limpopo code and Adam Faulconbridge, for having helped in fixing a few code flaws.
Funding: CarcinoGENOMICS (PL037712), EMBL and BBSRC (BB/I000771/1).
Conflict of Interest: none declared.
ABSTRACT Summary: We present a large-scale implementation of the Rankprop protein homology ranking algorithm in the form of an openly accessible web server.
We use the NRDB40 PSI-BLAST all-versusall protein similarity network of 1.1 million proteins to construct the graph for the Rankprop algorithm, whereas previously, results were only reported for a database of 108 000 proteins.
We also describe two algorithmic improvements to the original algorithm, including propagation from multiple homologs of the query and better normalization of ranking scores, that lead to higher accuracy and to scores with a probabilistic interpretation.
Availability: The Rankprop web server and source code are available at http://rankprop.gs.washington.edu Contact: iain@nec-labs.com; noble@gs.washington.edu 1 INTRODUCTION Rankprop (Weston et al., 2004) is a network-based inference algorithm for identifying subtle protein sequence similarities, corresponding to remote homology relationships or to structural similarities.
The algorithm operates on a protein similarity network, a graph in which each node is a protein and each weighted edge connecting two proteins indicates their similarity.
Such a network can be built using existing tools, such as PSI-BLAST (Altschul et al., 1997).
The key idea of the Rankprop algorithm is to extract global information from a protein similarity network by propagating outward from a user-specified query protein.
Effectively, the algorithm sums over all possible paths from the query to each target protein.
Thus, after propagation, the resulting activation score for each node includes global information about that proteins relationship to the query.
Ranking proteins by these scores is analogous to performing a database search using a tool such as PSIBLAST, except that the ranking induced by Rankprop reflects the global topology of the protein similarity network.
In Weston et al.(2004), PSI-BLAST is used to measure sequence similarity, and the unnormalized weight for the edge from node i to node j is Wij =exp(Sj(i)/ ), where Sj(i) is the PSI-BLAST E-value assigned to protein i given query j, and the parameter is a positive constant.
Edges are only included in the network for To whom correspondence should be addressed.
E-values smaller than a fixed threshold.
We obtain a stochastic connectivity matrix M for the protein similarity network by rownormalizing edge weights Wij to obtain transition probabilities: Mij =Wij/ j Wij.
Given such a network and a query sequence q, the Rankprop algorithm is simple to describe.
First, all nodes are assigned initial activation scores that reflect each target proteins similarity to q.
Like the edge weights, these scores are computed from PSI-BLAST E-values using the same equation.
At each iteration of the algorithm, the activation score at a given node is replaced by the weighted sum of the scores from all of its incoming edges.
The update rule includes a diffusion constant that controls the rate of diffusion through the network.
Formally, we define the initial activation scores as P0i =exp(Sq(i)/ ).
Viewing Pt as the column vector of activation levels at iteration t, the algorithm is given by Pt+1i =MPti +P0i if Pi =q and Pt+1i =1 otherwise, where (0,1).
One can show that this iterative procedure converges to a fixed point, which in practice happens in a small number of iterations.
The output of the Rankprop algorithm is a ranking of the nodes in the network according to their final activation values.
Proteins that receive a high activation score are linked to the query via many strongly weighted paths and vice versa.
A multidomain query protein will produce strong matches to any target protein that contains one or more of the query domains.
A single domain query A may connect through a multidomain protein AB to infer a false relationship with B.
However, previous work (Weston et al., 2004) has found that as long as the query sequence is connected to many other proteins, then the true homologs will be mutually reinforcing and receive a higher rank.
In this work, we extend the original Rankprop algorithm in two ways: (1) improving accuracy by propagating simultaneously from proteins that are very closely related to the query, and (2) improving the interpretability of the scores produced by Rankprop by empirically mapping them to probabilities.
The mapped score can be interpreted as the probability that the target protein is a member of the same SCOP superfamily as the query.
We also announce the availability of a free web server that allows individual queries against a protein similarity network derived from the NRDB40, comprising 1.1 million targets.
2 METHODS The Rankprop server uses the PSI-BLAST all-versus-all similarity matrix for NRDB40 provided by the PairsDB website (Heger et al., 2008).
NRDB40 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
I.Melvin et al.is a subset of the non-redundant sequence database, filtered so that no pairs exhibit >40% sequence identity.
We generalize the Rankprop algorithm to accept a set Q of query proteins, rather than a single query protein.
To use this extra information we perform propagation as usual, but we constrain the activation scores for all the query points such that they are highly ranked.
In particular, we choose the set Q to be all the proteins that have a match with the initial query q with a PSI-BLAST E-value <0.001.
We then constrain our algorithm to have Pj = 1Sq(j),jQ.
This modification is useful because, instead of propagating from a single query source node in the graph, we can propagate from several source nodes that all belong to the same family or superfamily that we are searching for.
The original Rankprop algorithm outputs scalar values that are not directly interpretable.
In the new version of the algorithm, we map each Rankprop score to an estimate of the probability that the corresponding query and target proteins belong to the same structural superfamily.
We employ the SCOP database (Murzin et al., 1995) to compute a histogram of empirical frequencies of the activation levels Pi for each protein i.
More specifically, we choose bin centers vk and compute the following quantities: nk , the number of times Pi falls into bin vk , and sn, the number of times that the latter occurs and i is in the same superfamily as the query.
We are interested in the value sk/nk , which can be interpreted as the probability for each activation value bin of the target being in the same superfamily as the query.
We choose the bin centers v= (0,0.01,0.02,...,0.2,0.3,...,1), and we enforce monotonicity in the final output by setting pi/ni =pi1/ni1 if pi/ni < pi1/ni1.
3 RESULTS Table 1 compares our large-scale Rankprop results with PSIBLAST (using NRDB40 and the same blastpgp parameters as PairsDB:-j 10-e 1-h 0.001-b 10000-v 10000 ) and the previously published version of Rankprop (using the SWISSPROT database, 108k proteins).
Rankprop NRDB40 is a straightforward scaling up of the previous Rankprop algorithm to NRDB40.
In addition, Rankprop+homologs NRDB40 employs the extensions described in Section 2.
Accuracy is measured following the methodology given in Weston et al.(2004): SCOP version 1.59 is split into train and test portions, and hyperparameters are chosen by using the training set.
Then, each test protein is treated as a query, and the quality of a methods protein ranking is measured by using the area under the receiver operating characteristic (ROC) curve, up to the first (ROC1) or 50th (ROC50) false positive.
We report results as average ROC1 and ROC50 scores across all 3083 test proteins.
Using a larger network yields improvements across all four performance metrics, and propagating from multiple queries improves the performance still further.
A Wilcoxon signed rank test, corrected for multiple tests, shows that all differences in Table 1 are significant at 0.01, except for the three pairs of methods marked with asterisks.
We also evaluate the performance of Rankprop using a combined ROC curve across all the queries in our test set, following the protocol of Altschul et al.(1997).
Figure 1 shows the combined ROC curves for Rankprop NRDB40 (ranked by activation value), Rankprop+homologs NRDB40 (ranked by probability) and PSIBLAST (ranked by E-value).
Compared with average per-query ROC scores, the combined ROC curve requires that scores are well Table 1.
Ranking accuracy Family Family S-Fam S-Fam Method ROC1 ROC50 ROC1 ROC50 PSI-BLAST 0.833 0.851 0.609 0.628 RankProp SWISSPROT 0.816 0.906 0.592 0.725 RankProp NRDB40 0.872 0.923 0.696 0.779 RankProp+homologs NRDB40 0.884 0.928 0.710 0.775 *Indicate pairs of values that are not different at P < 0.01 (Wilcoxon signed rank).
0 0.2 0.4 0.6 0.8 1 0 500 1000 1500 2000 2500 F ra ct io n of P os iti ve s False positives PSI-BLAST Rankprop SWISSPROT sigma 100 homologs e<1e-3 sigma 100 Fig.1.
Combined ROC curve across multiple queries.
For each method, search results from 3083 queries were sorted into a single list.
The figure plots, for varying thresholds in the ranked list, the fraction of all known homologs (SCOP superfamily members) that fall above the threshold, as a function of the number of non-superfamily members above the threshold.
calibrated from one query to the next.
The figure shows that the mapping of Rankprop scores to probabilities significantly improves the calibration, yielding better performance than PSI-BLAST for all but the first few false positives (across 3083 queries).
The Rankprop web server first looks for an exact match of the query sequence against the sequences in NRDB40.
If such a match is found, the server will retrieve the precomputed PSI-BLAST results from the database and then apply the Rankprop algorithm.
In this case the server takes around 90 s to process a query.
If the sequence is not found in the database, then the server will run PSI-BLAST first, which on average takes an additional 15 min.
Funding: National Institutes of Health award (R01 GM074257).
Conflict of Interest: none declared.
ABSTRACT Summary: YADA can deisotope and decharge high-resolution mass spectra from large peptide molecules, link the precursor monoisotopic peak information to the corresponding tandem mass spectrum, and account for different co-fragmenting ion species (multiplexed spectra).
We describe how YADA enables a pipeline consisting of ProLuCID and DTASelect for analyzing large-scale middle-down proteomics data.
Availability: http://fields.scripps.edu/yada Contact: paulo@pcarvalho.com Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION High-resolution, high-mass-accuracy (<10 p.p.m.
error) mass spectrometry allows highly charged peptide isotopic peaks to be distinguished from one another, thus enabling the calculation of their precise charge states and monoisotopic masses.
This is crucial for confident protein identification, especially when dealing with large molecules, such as the ones usually obtained from transmembrane protein digests.
Other key advantages when analyzing large molecules include an increased identification coverage and the possibility of assessing relationships among multiple modifications in the same molecule (e.g.histone; Chi et al., 2007).
These motivations have given rise to a new proteomics platform, termed middle-down (MD) proteomics, which focuses on large molecules usually obtained through proteases other than trypsin or by modifying the digestion protocols (e.g.short-time trypsin digests; Forbes et al., 2001).
Traditional protein identification search engines [e.g.SEQUEST (Eng et al., 1994) and Mascot (Perkins et al., 1999)] cannot take full advantage of high-resolution mass spectra, especially those of large molecules, mainly because of three reasons: (i) The specific peak in an isotopic envelope being selected for fragmentation can have a significant difference in mass as compared with its monoisotope; this can lead the search engine astray.
(ii) Fragmenting highly charged precursors results in highly charged daughter ions.
If the charge states of the latter are unknown, all charge state hypotheses should be considered for protein identification, resulting in a combinatorial To whom correspondence should be addressed.
explosion that can burden the search engine severely and decrease chances of a successful identification.
(iii) Approximately 10% of all tandem spectra from complex mixtures are composed of different ion species that are co-fragmenting; only one of these ion species is searched (Carvalho et al., 2009).
In order to overcome these limitations, we introduce YADA, a tool that can deconvolute (i.e.deisotope and decharge) high-resolution spectral data of peptide ions having charges up to +18 or masses up to 20 kDa.
Because deconvolution entails the assignment of a charge state and the recalculation of the m/zs as if the charge were +1 (decharging), the combinatorial explosion problem mentioned above is automatically eliminated.
YADA can also update the MS2 to reflect the fragmented precursor monoisotopic mass by locating its corresponding isotopic envelope in the MS1 and replacing its m/z with the monoisotopic m/z.
Accordingly, multiple precursors can be considered when peaks from different isotopic envelopes are found within the precursor isolation bounds; this enables the accounting for multiplexed spectra.
As far as we know, no other freely available deisotoping and decharging tools offer both features.
We describe a freely available pipeline to address large-scale MD studies consisting of YADA; ProLuCID, a protein identification search engine that is ready to efficiently handle deconvoluted spectra (Xu et al., 2006); and DTASelect, which controls and estimates the false discovery rates (Cociorva et al., 2007).
Again, we are aware of no freely available solution for such in the context of MD.
Our pipeline is evaluated on a short-time trypsin digest of a yeast lysate.
The results are compared with those obtained by replacing YADA with Xtract (Thermo, San Jose, CA, USA), a software for decharging based on the THRASH algorithm (Horn et al., 2000), which is also present in a commercial solution for MD analysis [Thermos ProsightPC v2.0 (Boyne et al., 2009)].
2 ALGORITHM 2.1 Peak filtering YADA filters noise peaks and peaks that can be eliminated without compromising isotopic envelope recognition.
Filtering increases speed (by up to 40%), reduces RAM requirements and improves charge assignment (by 5%; data not shown).
It is accomplished in two steps, as follows.
The first step eliminates peaks that fall below an intensity threshold.
The threshold can by default be a user-specified hard The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:09 17/9/2009 Bioinformatics-btp489.tex] Page: 2735 27342736 YADA cutoff, or else be automatically determined for each spectrum (recommended for top-down datasets).
The latter is accomplished by treating each mass spectrum as a two-component probability mixture model.
The premises are that the noise peaks have intensities that follow a normal distribution; and that peptide-derived peaks have considerably higher intensities which, though to a lesser extent than the noise peaks, can also be assumed to follow a normal distribution.
Given these, the well-established expectationmaximization (EM) algorithm is employed to maximize the likelihood of the observed intensity histogram under the assumed bimodal normal distribution.
The two EM seeds (starting points) are chosen by sorting the intensities of all peaks and choosing the ones that, globally, rank at 10% and 90%.
Given the two normal distributions provided by EM, the threshold is equal to that of the optimal Bayesian classifier.
The second step discards peaks that do not contribute to charge determination.
The premise in this case is that the intensities of peaks derived from a peptide ion isotope will monotonically increase until a local maximum is achieved, at which point they will monotonically decrease.
The algorithm proceeds as follows.
Mass spectral peaks are sorted by increasing m/z and an empty result array is created.
Then, for every peak, if its intensity is greater than that of the previous peak and the two differ in m/z by less than a given p.p.m.
tolerance (e.g.30 p.p.m.
), then the current peak replaces the latest peak included in the result array; otherwise the peak is simply included in the result array.
Then the spectral peaks are sorted by decreasing m/z and the process is repeated.
2.2 Detecting and decharging an isotopic envelope All peaks are candidate seeds for an isotopic envelope.
Briefly, for a given peak, first a peak-finding algorithm is employed to integrate peak intensities by stepping a distance of 1.0024 divided by a charge state hypothesis, for several charge state hypotheses (e.g.+1 through +21).
Second, only the charge state hypothesis to have found the greatest number of sequential peaks and having the greatest accumulated intensity is retained.
The observed profile is normalized to 1 and its dot product with a normalized averagine theoretical profile for the estimated mass (obtained using a kernel regressor) is computed.
If the dot product is above a given threshold, the envelope is stored.
The candidate envelope is discarded if verified to be part of an existing envelope of the same charge or one that would produce an overlapping envelope (e.g.+4 and +2).
The algorithm is not subtractive and is capable of identifying overlapping envelopes.
2.3 Decharging, clustering and accounting for multiplexed spectra It is common to observe the same peptide ion species with different charge states in the same spectrum.
After deconvolution, these peptides yield peaks that are very close (e.g.<0.2 Da apart for a given resolution) to one another.
YADA automatically coalesces them by averaging their masses and summing their intensities.
Also, in complex samples, it is common to have more than one ion species in the same isolation window to be fragmented when generating a tandem mass spectrum.
YADA uses isotopic envelope information from the preceding MS1 to assign a monoisotopic precursor mass to the MS2 spectrum and to consider multiple ion species within the isolation window bounds (multiplexed spectra).
Table 1.
Results Xtract Y Y with Corr Number of identified spectra 898 996 1071 Fraction of identified spectra with non-monoisotopic precursor assigned (%) 74 75 1 Y stands for YADA, Corr for monoisotopic correction and multiplexing.
3 RESULTS A 30 min trypsin digest of a yeast lysate was analyzed with a 2 h LC-MS run, acquiring one high-resolution MS1 (60 000 resolution at 400 m/z) followed by three high-resolution ETD-MS2 scans on the Orbitrap (Makarov, 2000) in data-dependent mode (10 071 spectra).
The spectra were extracted using RawExtract (McDonald et al., 2004) and processed by YADA (with and without monoisotopic correction and multiplexing), ProLuCID (protein identification) and DTASelect.
The latter ensures a peptide false discovery rate <1% against a decoy database.
An example of a multiplexed spectrum solved by YADA (Supplementary Fig.S1) and further details regarding the search parameters and false discovery rate estimation are presented in the Supplementary Material.
For evaluation purposes, an in-house script was created to use Xtract to deconvolute the data and replace YADA in our pipeline.
The results are presented in Table 1.
YADA turned out to be 600% faster than the commercial software during deconvolution (YADA: 6 31; Xtract: 42 24; both on a 1 GHz Athlon with 1 GB RAM).
4 FINAL CONSIDERATIONS While previous tools (Chen and Yap, 2008; Horn et al., 2000) are devoted solely to deconvolution, YADAs hallmark is its ability to maximize the results of large-scale experiments by quickly deconvoluting highly charged MD MS2 spectra and accounting for multiple precursors (multiplexed spectra).
We also note that, although it has been shown that assigning monoisotopic precursor masses to MS2s increases protein identification confidence (Mayampurath et al., 2008), MD poses a new challenge.
This is so because large molecules (>12 kDa) often have the monoisotopic peak intensity below the detection sensitivity.
Previous strategies have relied only on detected peaks, but YADA can predict a large molecules undetected monoisotopic peak by considering its three most intense envelope peaks and estimating the monoisotopic mass according to an averagine model.
We have also described a freely available pipeline for analyzing high-resolution tandem mass spectra of large peptide molecules.
The pipeline can be used for datasets containing high-resolution MS1 and MS2 spectra, or only a high-resolution MS1.
In the case of the later, the MS2 cannot be deconvoluted; however, identification results can still be improved by assigning monoisotopic masses.
The key steps are listed in Figure 1.
YADA is coded in C# and requires a PC with Windows XP SP2 or later and.NET 3.5.
It installs under the directory PatternLab for proteomics (Carvalho et al., 2008).
The windows version can be downloaded at our web site (http://fields.scripps.edu).
A commandline version (requires MONO; http://mono-project.com), executable on Windows or Linux, is available upon request.
2735 [15:09 17/9/2009 Bioinformatics-btp489.tex] Page: 2736 27342736 P.C.Carvalho et al.Fig.1.
The key steps of the proposed pipeline for middle-down proteomic analysis.
YADAs current version is not recommended for the analysis of isotopically labeled datasets, since in these cases the isotopic distribution patterns may differ from the theoretically predicted.
Funding: This work was supported by Coordenao de Aperfeioamento de Pessoal de Nvel Superior (CAPES), Conselho Nacional de Desenvolvimento Cientfico e Tecnolgico (CNPq), Fundao Carlos Chagas Filho de Amparo Pesquisa do Estado do Rio de Janeiro (a FAPERJ BBP grant), and the National Institutes of Health [P41RR011823, ROI MH06880].
Conflict of Interest: none declared.
ABSTRACT Motivation: Complex patterns of protein phosphorylation mediate many cellular processes.
Tandem mass spectrometry (MS/MS) is a powerful tool for identifying these post-translational modifications.
In high-throughput experiments, mass spectrometry database search engines, such as MASCOT provide a ranked list of peptide identifications based on hundreds of thousands of MS/MS spectra obtained in a mass spectrometry experiment.
These search results are not in themselves sufficient for confident assignment of phosphorylation sites as identification of characteristic mass differences requires time-consuming manual assessment of the spectra by an experienced analyst.
The time required for manual assessment has previously rendered high-throughput confident assignment of phosphorylation sites challenging.
Results: We have developed a knowledge base of criteria, which replicate expert assessment, allowing more than half of cases to be automatically validated and site assignments verified with a high degree of confidence.
This was assessed by comparing automated spectral interpretation with careful manual examination of the assignments for 501 peptides above the 1% false discovery rate (FDR) threshold corresponding to 259 putative phosphorylation sites in 74 proteins of the Trypanosoma brucei proteome.
Despite this stringent approach, we are able to validate 80 of the 91 phosphorylation sites (88%) positively identified by manual examination of the spectra used for the MASCOT searches with a FDR < 15%.
Conclusions: High-throughput computational analysis can provide a viable second stage validation of primary mass spectrometry database search results.
Such validation gives rapid access to a systems level overview of protein phosphorylation in the experiment under investigation.
Availability: A GPL licensed software implementation in Perl for analysis and spectrum annotation is available in the supplementary material and a web server can be assessed online at http://www.compbio.dundee.ac.uk/prophossi Contact: d.m.a.martin@dundee.ac.uk To whom correspondence should be addressed.
Present address: Wellcome Trust Centre for Stem Cell Research and Cambridge Systems Biology Centre, University of Cambridge, Tennis Court Road, Cambridge, CB2 1QR, UK Present address: Institut de Gnomique Fonctionnelle, Universits de Montpellier, CNRS UMR 5203, F-34094 Montpellier, France Supplementary information: Supplementary data are available at Bioinformatics online.
Received on February 8, 2010; revised on June 7, 2010; accepted on June 22, 2010 1 INTRODUCTION Protein phosphorylation is regarded as a key mechanism for the regulation of many cellular processes including metabolism, cell division and apoptosis (Cohen, 2000).
It has been estimated that >50% of expressed proteins are phosphorylated at some point in their life cycle (Hjerrild and Gammeltoft, 2006) though only a small fraction of the potential phosphorylation sites have been identified.
In recent years, the examination of complex protein mixtures by tandem mass spectrometry (MS/MS) has become feasible through advances in instrumentation and computational methodologies (Cox and Mann, 2007).
Peptide and protein analysis at the cell extract level has become an almost routine procedure as algorithms such as MASCOT (Perkins et al., 1999) or SEQUEST (Yates et al., 1995) among others allow rapid identification of proteins through matching tandem mass spectra to sequence databases.
Statistical methods have been developed to enable the validity of peptide observations to be assessed.
Search strategies such as the inclusion of reversed or scrambled sequences in the database can give an estimate of the likely accuracy, or false discovery rate (FDR), for peptide identifications with respect to the database search engine score for a particular experiment (Elias and Gygi, 2007; Kall et al., 2008).
This allows a reasonably robust description of the protein species in a particular sample.
Further work has been performed to match peptide fragmentation patterns to peptide sequences through machine learning techniques such as hybrid support vector machines/Bayesian networks (Klammer et al., 2008) and decision trees (Elias et al., 2004) but these are not yet readily applicable to peptides containing post-translational modifications (PTMs).
Although protein and peptide analysis is now a mainstream highthroughput technique, reliable identification of PTMs remains a specialist area.
Algorithms designed for peptide database matching can take PTMs into account but are not designed to provide robust identifications.
PTMs are often present only in low stoichiometry and ion signals corresponding to phosphopeptides tend to be suppressed in the presence of non-phosphorylated peptides.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:57 10/8/2010 Bioinformatics-btq341.tex] Page: 2154 21532159 D.M.A.
Martin et al.Fig.1.
An example of misidentification of the correct phosphorylation site.
MASCOT identifies the phosphorylation site as pS16 (peptide score 118), though the expected y5-98 ion is much weaker than the weak y5 ion (blue).
The second ranked hit (pY17, score 102) is preferred by the experienced analyst with a strong y5 ion match (red) giving a continuous y-ion ladder.
The spectrum was annotated with Prophossi and modified.
The threshold for ion inclusion is indicated by a blue bar on the y-axis.
The low stoichiometry of PTMs requires the use of peptide enrichment approaches such as affinity pull down (Gronborg et al., 2002), immobilized metal affinity chromatography (Andersson and Porath, 1986; Stensballe et al., 2001) or titanium dioxide columns (Pinkse et al., 2004) to enrich the phosphopeptide complement of a complex mixture.
The peptide enrichment strategies required for PTM identification in complex mixtures compromise the scoring assumptions upon which generic database search strategies are based, rendering it unreliable to take the database search scores as a surrogate for PTM identification accuracy.
Consequently, confident identification of PTMs requires the manual inspection of the raw MS/MS spectra by a competent mass spectrometrist, a time-consuming step that acts as a major bottleneck in global PTM analyses where many thousands of spectra may require analysis.
This examination requires interpretation of the fragmentation pattern in line with the experience of the scientist, and comparison of multiple interpretations of the data, each of which could be correct if the sample contains multiple isobaric isoforms of the phosphopeptide.
An example is shown in Figure 1 where an experienced mass spectrometrist identifies the phosphoTyrosine (Rank 2 hit) as a better interpretation than the phosphoSerine (top-ranked hit).
Full Prophossi reports for these two peptidespectrum matches (PSMs) are available in the Supplementary Material.
This issue has been approached by two other groups, both of whom apply a post-processing step to standard peptide MS/MS search engine results.
Beausoleil et al.(2006) report a probabilistic method that calculates an Ascore based on the appearance of phosphorylation sites in multiple candidate solutions to the spectrum-database mapping conundrum.
They calculate a probability score based on the appearance of site-determining ions, i.e.those fragment ions that would be specific for a particular phosphopeptide isoform.
The method is dependent on the SEQUEST search engine to identify the two top phospho-isoform hits and it reports quality data only for the best phospho-isoform hit.
It does not consider the possibility of isobaric phospho-isoform mixtures.
The closed source implementation of the software prevents user optimization of the search parameters.
Smith et al.(2007) have taken a different approach which is, in some respects, similar to ours.
They examine the daughter ion spectrum and assign scores based on a limited range of spectral features.
Peptide matches failing to reach a defined score threshold are rejected.
Both of the aforementioned methods have some limitations: The Beausoleil method ignores spectral features beyond the sitedetermining ions and does not consider neutral loss of phosphate from phosphoSerine and phosphoThreonine, resulting in a smaller number of phosphopeptide spectrum matches.
The Smith method assigns scores from a limited range of features, which alone may not be sufficient to specifically locate a phosphorylation site, and has not been tested empirically.
Our methodology, described in this article, incorporates a broader range of spectral features and seeks to identify evidence for the specific localization of the putative phosphorylation sites.
Thus, every PSM, i.e.every hit from a MASCOT or other search engine search, is assessed on its own merit.
The method is, therefore, able to interpret complex spectra derived from multiple isobaric phosphopeptide species.
Opinions from three experienced mass spectrometrists were used to derive a set of chemistry-based criteria that could be applied to tandem mass spectra for selection between, and validation of, the database PSM search hits.
This method does not perform database searches itself but provides a report on how well the observed spectrum fits to the predicted matches, and whether the predicted match passes these analytical criteria.
As such, it can be applied to the results of any such database search.
Typically, relatively few phosphopeptide spectra are observed in proteomics experiments in the absence of specific phosphopeptide enrichment protocols and this low coverage can be treated by hand by an experienced analyst.
However, when such enrichment methods are applied, such as in the experiments described here, the proportion of spectra arising from phosphopeptides rapidly expands to a level where automated processing tools are a practical necessity.
Our aim was to develop tools that automate rapid processing of large numbers of spectra with few falsely identified phosphorylation sites (high selectivity) and a sufficiently good sensitivity to provide significant coverage.
As we are examining proteomes, where little is known about the existing phosphorylation state of the organism, a tool that rapidly and confidently assigns the majority of easy cases is a considerable boost to productivity.
All database hits can be assessed, and positive results reported.
As all the criteria can be explicitly described in English, marginal hits can be also examined rapidly by an experienced analyst with an appropriate visualization tool.
Additionally, a full text report that highlights salient features and annotates the spectrum can be generated.
This approach has been validated through assessment of the automated annotation of the Trypanosoma brucei phosphoproteome (Nett et al., 2009b).
We manually examined all identified hits for a specific family of proteins (the protein kinases) and examined the error rate and bias in our automated processing.
Our method runs rapidly, allows the assessment of more than just the top hit and gives excellent selectivity with good sensitivity.
Output can be via an annotated spectrum and report, produced in HTML or PDF, or via a software application programming interface, allowing integration of the analysis in a high-throughput analysis pipeline.
Several of our predictions of occupied T.brucei phosphoTyrosine sites have been validated experimentally by both western blot and immunofluorescence microscopy experiments using two wellcharacterized anti-phosphoTyrosine antibodies (Nett et al., 2009a).
2154 [11:57 10/8/2010 Bioinformatics-btq341.tex] Page: 2155 21532159 High-throughput phosphoproteomics A public web server has been made available at http://www.compbio.dundee.ac.uk/prophossi for individual researchers to examine their peptide spectra online.
2 METHODS 2.1 Phosphopeptide samples and mass spectra The generation of the T.brucei phosphopeptide mixture and its separation, and analysis have been described previously (Nett et al., 2009b).
In summary, the cytosolic fraction of a T.brucei culture was obtained, digested with trypsin and phosphopeptides enriched though a combination of strong cation exchange and titanium dioxide chromatography as described by Olsen et al.(2006).
Mass spectrometry was performed using a Q-Star XL mass spectrometer (Applied Biosystems) and an LTQ-Orbitrap (Thermo Electron) both equipped with a nanospray ionisation source.
The Q-Star XL mass spectrometer was operated in a data-dependent mode, which consisted of a MS survey scan for 1 s (m/z 4002000) followed by four 2 s MS/MS scans of the four most intense doubly or triply charged ions (m/z 601800) exceeding 10 counts.
In the LTQ-Orbitrap mass spectrometer, a survey scan was performed over a split mass range (m/z 300800 and 8002000) in the Orbitrap analyser each of them triggering five MS2 LTQ acquisitions of the five most intense ions using multistage activation on the neutral loss of 98, 49 and 32.33 Da.
Orbitrap mass spectra were processed with Analyst 1.4 software (Applied Biosystems).
Q-star mass spectra were processed with Analyst QS 1.1 software (Applied Biosystems) and centroided and deisotoped peak list files of the SCXTiO2 experiments were concatenated using the MASCOT daemon engine (Matrix Science, London, UK) for Q-Star XL spectra.
Raw files obtained from the LTQ-Orbitrap were converted to MASCOT generic files using Raw2msm software (gift from Prof. Matthias Mann, Max Planck Institute for Biochemistry, Munich) before merging into a single file.
2.2 Database searches A composite T.brucei database containing all predicted proteins from the genome sequencing projects for T.brucei strains 427 and 497 (downloaded from GeneDB) and all T.brucei peptides in UniProt was created and curated to remove redundant sequences.
A decoy dataset containing the reversed sequence of all remaining sequences was generated and appended to the forward dataset.
MASCOT (version 2.1, Matrix Science, London, UK) searches were performed on a 4-node cluster using a parent/daughter ion mass accuracy of 0.1 Da (Q-Star) or a parent ion mass accuracy of 10 ppm and daughter ion mass accuracy of 1 Da (Orbitrap).
Searches were performed using trypsin as the digestion enzyme, carboxyamidomethylation of cysteine as a fixed modification and with the oxidation of methionine and phosphorylation of serine, threonine or tyrosine classed as variable modifications.
The data were searched against the composite and decoy databases described above.
MASCOT database search results were processed with custom Perl scripts using the MASCOT developers toolkit (Matrix Science, London, UK).
Large results such as those generated by these kinds of experiments are extremely resource-demanding to view through the MASCOT web interface, so an intermediate relational database (MySQL) was created, the MASCOT Large Results Viewer (MLRV), in which the peptide, protein, modification and search hit information could be stored in a readily queryable manner.
MGF files were transformed to DTA files using custom Perl scripts and SEQUEST searches performed on the same sequence database through the TransProteomicsPipeline (TPP) software (Pedrioli, 2010).
A maximum of 10 top hits were retained for each spectrum and imported into a custom PostgreSQL database, the SEQUEST Large Results Viewer (SLRV).
TPP was also used to post-process each search with PeptideProphet, which provides a score for the top hit for each spectrum query.
These data were incorporated into SLRV with custom Perl scripts.
Standard SEQUEST parameters were used with peptide mass tolerance = 10 and peptide mass Table 1.
Analysis Criteria for automated validation of PSMs Prefilter criteria P1 Forward hit Only hits against forward, non-redundant sequences are selected P2 Mass accuracy Only hits within 0.1 Da (or 0.1 + 1 Da) of the parent ion m/z are selected.
P3 Phospho-PTM Only hits containing a putative Phospho PTM are selected P4 Within 20 points Only hits which are within 20 MASCOT score points of the top ranked hit for that query are selected.
P5 Over FDR threshold Only peptides with a MASCOT score over the calculated FDR 1% threshold are selected.
Validation CriteriaPhosphopeptide assignment 1 4 in a row At least four sequential y-or b-series ions are present.
This indicates good coverage of the peptide.
2 5 of 6 5 out of 6 sequential b-or y-series ions are present.
This indicates good coverage of the peptide.
3 3 desphospho ions At least three y-or b-series ions with a phosphate loss are present.
The phosphate ester bond tends to be more labile than the peptide bond.
4 Proline-directed fragmentation The imino bond to the N-terminal side of a proline residue is particularly labile.
If the sequence contains a Proline residue then at least one of the imino bonds should give a fragment ion with at least 50% maximum intensity and much stronger than the relatively weakly cleaved amide bond C-terminal to Proline.
5 6 of top 10 ions 6 of the 10 most intense ions should be assigned to y-or b-series ions.
Validation criteriaphosphosite assignment 6 Phosphate transitions To assign the site specifically, at least one ion unique to that peptide species must be observed.
This is aided by the high rate of phosphate loss from pSer and pThr residues.
7 PhosphoTyrosine Mass differences corresponding to pTyr should be observed between identified peaks.
units = 2.
PSMs were correlated between SEQUEST/Peptide Prophet and MASCOT output with custom Perl scripts to enable comparison of search methods.
2.3 Spectrum analysis criteria definition Three experienced mass spectrometrists were observed and interviewed as they manually assessed peptidespectrum matches.
The processes by which they accumulated evidence were noted and formalized as a set of analytical criteria, an assessment of whether it was always applicable, whether it was only applicable to certain peptides or whether it was only applicable to certain types of spectra.
Criteria of the first two types were identified and coded as Perl modules for inclusion in the data management infrastructure.
It was considered too problematic in this first study to selectively apply criteria of the third type as this would require identifying which spectra these criteria would be applied to.
These, typically more difficult cases, remain at present the domain of the mass spectrometry professional.
The criteria derived are listed in Table 1.
These include an examination of specific proline-directed cleavage products (Breci et al., 2003).
2155 [11:57 10/8/2010 Bioinformatics-btq341.tex] Page: 2156 21532159 D.M.A.
Martin et al.With criteria defined, a system for appropriate Boolean combination of the criteria was devised such that an unambiguous validation could be ascertained.
The quality assessment criteria listed in Table 1 are combined as follows.
All PSMs are subject to a prefilter (a) where they must meet all criteria.
Following ion series matching the criteria in section (b) are applied.
PSMs must match each of the following set of criteria: either four sequential ions or five ions out of a series of six must match; At least three des-phospho ions must be observed for sequences containing phosphoSerine and phosphoThreonine; if the peptide contains proline, then a strong prolinedirected fragmentation ion must be observed; at least 6 of the most intense 10 ions must be positively assigned.
For phosphosite identification in section (c), specific mass transitions corresponding to phosphoTyrosine must be observed, and sufficient ions to unambiguously identify the phosphosite.
Typically, this would require an ion derived from cleavage between any phosphosite candidates, including residues not identified as potentially phosphorylated by MASCOT.
2.4 Automatic hit validation method For each database search, a list of all queries, each corresponding to a specific parent ion and with PSMs meeting the prefilter quality criteria, was obtained from the MLRV database.
For each dataset, the PSMs were prefiltered to exclude matches with an absolute delta mass >0.1 Da (or 10.1 Da for Q-Star data), include only PSMs with phospho modifications and only PSMs that were within 20 MASCOT score points of the highest scoring PSM for that query spectrum.
The proportion of reverse sequences identified with respect to forward sequences was plotted against the MASCOT score threshold and the FDR threshold at n% (FDRn) determined as the lowest MASCOT score where reverse sequences comprise <n% of the total (filtered) peptide species identified.
FDR results for each dataset are summarized in Supplementary Figure S2.
For the purpose of this analysis, we restricted further investigation to peptides with a MASCOT score greater than FDR1.
Each query was then processed as follows: the peak list corresponding to the database search query (parent ion m/z and the related daughter m/z peaks) was read from the peak list data file.
This is referred to as the observed spectrum.
Each PSM for that query that corresponded to predetermined quality standards was retrieved from the MLRV database.
A synthetic spectrum was generated for the peptide in question (the theoretical spectrum) by applying simple fragmentation rules and this was compared to the observed spectrum as a spectrum match.
A customizable threshold was used to exclude peaks that may be due to noise.
In this study, we excluded any peak with an intensity <5% of the most intense peak observed.
This could be examined via a web-based visualization tool with matching ions both labelled on the observed spectrum and tabulated.
This process is described in Figure 2.
Each rule was then applied to the spectrum match.
The results were then combined according to a predetermined Boolean system and the results (pass/fail) stored in a database.
In addition, a second layer of criteria was used to determine whether the specific phosphorylation site(s) could be confirmed from the spectral data.
2.5 Verification of the validation method All 501 PSMs from 74 protein kinase sequences, containing 259 putative phosphorylation sites, were examined manually by an experienced mass spectrometrist to provide a gold standard reference against which the automated validation method could be evaluated.
Detailed statistics for this peptide set are shown in Table 2.
All spectra were classified as being either sufficient or insufficient to verify the presence of the phosphopeptide (i.e.pass/fail, the phosphopeptide validation).
In addition, the ability to unambiguously identify the precise phosphorylation site was recorded (pass/fail, phosphosite validation), allowing the determination of true positive and false positive rates for the automated analysis.
Fig.2.
Workflow for automated annotation of phosphosites.
Experimental LCMS/MS data is gathered (1) and processed using platform specific software (2) to give a generic peak list file (3).
This file, is used as the input to MASCOT (4), which generates a results file (5) containing all the PSMs.
This file is parsed into the MLRV relational database (6) and the FDR for the search determined (7).
A PSM-quality prefilter is applied (8) and suitable PSMs are exported to the TryPP-DB (9) where they are linked to the source peak list file used for the search.
The observed MS/MS spectrum is extracted from the peak list file (10), filtered by an intensity threshold (11) and compared with a calculated fragmentation spectrum (12) for the peptide under examination.
Observed ions are assigned to series (13) allowing the curation rules to be applied (14).
Table 2.
Kinase dataset statistics Site Observations Unique Sites Unique Proteins Peptides Observed All peptides 643 259 74 501 Rank 1 and 2 449 213 72 355 Rank 1 289 159 72 230 The subset of peptides from both Orbitrap and Q-Star experiments that were annotated both manually and by the automated system are described.
An observed peptide is a single PSM.
A PSM may contain more than one observed site.
Each protein may contain many PSMs.
Each site may be observed in many peptide observations.
3 RESULTS 3.1 Comparing search engine hit score, manual and automated validation The relationship between search engine score and manual curation was investigated.
All PSMs with a score over 1% FDR to a protein from the kinase set which had been evaluated manually were ranked by score.
For each search engine score, the false positive rate (PSMs not confirmed after manual curation) and true positive rate (PSMs confirmed after manual curation) were determined.
Similar rates were determined for the subset of matches automatically curated as positive by ProPhosSI.
ProPhosSI-curated peptides correlate significantly better with manual curation than the search engineranked sets (Fig.3).
Performance with the SEQUEST-ranked hits is not as good as with MASCOT, due in part to the differing responses of the search engines and ProPhosSI to phosphate neutral loss.
Neutral loss appears to be dependent on local peptide sequence, though no substantial datasets are yet available to model this appropriately.
SEQUEST matches are, therefore, biased towards sequences where the phosphate is less labile, and ProPhosSI, in this current implementation, towards those where sites are labile.
2156 [11:57 10/8/2010 Bioinformatics-btq341.tex] Page: 2157 21532159 High-throughput phosphoproteomics Fig.3.
All manually curated peptidespectrum matches containing at least one phosphorylated residue were ordered according to their MASCOT (red), SEQUEST (black) or SEQUEST + Peptide Prophet (green) score.
Dotted lines indicate the performance of all matches ordered by search engine score.
Solid lines indicate the performance of the subset of matches positively curated by ProPhosSI.
The increased area under the curve for the solid lines indicates better performance by ProPhosSI.
Table 3.
Automated assignments Dataset Phosphopeptides Phosphosites Pass Fail Pass Fail Orbitrap 1617 1992 557 252 Q-Star 2101 2521 939 456 Automated assignment to the dataset by the methodology described.
A single verified site observation at any peptide rank which met the appropriate quality criteria was considered sufficient to call as a phosphosite.
As SEQUEST only identifies about a third of the phosphopeptides found with MASCOT, and the Ascore software only considers the top SEQUEST hit, it is not possible to rigorously compare Ascore and ProPhosSI.
However, using a set of 20 manually assessed PSMs, ProPhosSI and Ascore gave similar results, with 10 PSMs having an Ascore over 16 positively validated by ProPhosSI and 10 PSMs with an Ascore under 9 not confirmed by ProPhosSI.
When the automated validation rate is extrapolated over the whole peptide set, there only appears to be a relation between validation rate and match score at very high scores for MASCOT (ion score >90) and SEQUEST (Xcorr > 6.5 ), and not at all for Peptide Prophet (data not shown).
Otherwise the validation rate (proportion curated as positive by ProPhosSI) remains constant at around 60% for scores >1% FDR.
3.2 Automated validation Spectra corresponding to 8231 PSMs with MASCOT scores over 1% FDR were assessed.
Of these, 3718 were classified automatically as a validated phosphopeptide, with 1236 distinct validated phosphorylation sites.
These data are shown broken down by machine type in Table 3.
3.3 Verification of validation Of the gold standard manually curated peptide hits, 161/230 (70.0%) were identified with the automated system.
Further, 16.1% Table 4.
Automated versus manual peptide and phosphosite assignments Automated Curation with ProPhosSI Phosphopeptides Phosphosites Pass Fail Pass Fail Orbitrap (all PSM) Manual Curation Pass 60 17 41 5 Fail 12 79 6 13 Q-Star (all PSM) Manual Curation Pass 101 52 69 17 Fail 19 161 17 33 Orbitrap (Rank 1 PSM) Manual Curation Pass 32 5 31 3 Fail 6 37 2 2 Q-star (Rank 1 PSM) Manual curation Pass 53 30 53 15 Fail 9 58 2 2 The results from independent manual curation were compared with results from the automated validation.
Each individual PSM and site observation is considered for each experiment and additionally for the subset of data that only includes top ranked MASCOT PSMs.
of peptide hits automatically annotated as positive were not identified as such by manual curation of the spectra (a selectivity of 83.9%).
A summary of the validation results broken down by instrument type is shown in Table 4 (Orbitrap and Q-Star, all PSM).
Overall the criteria employed appear better suited to the data obtained from the Orbitrap than the Q-Star with a sensitivity of 77.9% versus 66.0% and comparable accuracies with selectivities of 83.3% and 84.2%, respectively.
The overall Matthews correlation coefficient (MCC; Matthews, 1975), a metric which considers all elements of the confusion matrix, for the phosphopeptide assignment is 0.600 (0.652 and 0.576 for the Orbitrap and Q-Star, respectively).
Restricting the analysis to just first rank peptides [Table 4, Orbitrap and Q-Star (Rank 1 PSM)] gives little overall change with 85/118 peptides (sensitivity 70.8%) correctly validated and 15/100 incorrect validations (selectivity of 85%).
The better performance on Orbitrap data is reflected in the comparison of individual phosphosite assignments by the manual curator and by the automated process.
Over all ranked peptides meeting the prefilter quality criteria, the automated process positively validates 47 phosphosite observations in the Orbitrap data [Table 4, Orbitrap (all PSM)].
Manual assignment identifies a further five sites missed by the automated process (a sensitivity of 89.1%) and rejects six validated automatically (selectivity of 87.2%).
Both methods reject 13 site observations for an overall MCC of 0.585.
In the validation of the Q-Star data [Table 4, Q-Star (all PSM)], 86 site observations are validated automatically of which 17 are rejected by the manual curator (selectivity of 80.2%).
The manual curator validates a further 17 sites (sensitivity of 80.2%) and both methods reject 33 site observations giving an MCC of 0.462.
2157 [11:57 10/8/2010 Bioinformatics-btq341.tex] Page: 2158 21532159 D.M.A.
Martin et al.Table 5.
Aggregated phosphosite assignments (by site) Automated assignment Pass Fail All sites (total 129) Manual assignment Pass 80 11 Fail 14 24 Sites with two or more positive automatic assignments (total 60 sites versus 129) Manual assignment Pass 52 39 Fail 8 30 Sites with Rank 2 peptides or higher (109 sites) Manual assignment Pass 72 13 Fail 8 14 Sites with Rank 1 peptides (80 sites) Manual assignment Pass 62 14 Fail 3 1 Each unique site is considered, taking all observations of that site.
If any observation of a phosphosite is validated in any experiment then that site is called as validated.
Restricting the analysis to sites observed only in top-ranked peptides from the MASCOT search improves sensitivity and selectivity in both Orbitrap and Q-Star datasets (Selectivity: 93.9% and 96.4%, respectively; Sensitivity: 91.2% and 77.9%, respectively) though the MCC is considerably reduced to 0.374 and 0.151, respectively [Table 4, Orbitrap and Q-Star (Rank 1 PSM)], primarily due to the small number of true negatives.
Taking both Orbitrap and Q-Star data in aggregate, 94 nonredundant phosphorylation sites from the 74 protein kinases were identified automatically.
From the 445 peptides examined manually, a further 11 sites were identified (a sensitivity of 88%) and 14 were assigned as negative, giving a selectivity of 85.1%.
Twenty-four sites were rejected by both methods giving a MCC of 0.186 (Table 5, all sites).
Confidence in a phosphosite assignment can be improved by requiring that at least two automated validated observations are required for verification.
Sixty sites were identified with at least two validated observations, of which 8 had no manually validated observation, a selectivity of 86.7%, only a small increase over the single observation level but with a decrease of 35% in sensitivity (Table 5, Sites with two or more positive automatic assignments).
Restricting analysis to just those PSMs that are in the top 2 ranks in the MASCOT search results reduces the number of putative phosphosites to 109 from 129.
Of these sites, automated assignment validates 72 of the 85 manually validated sites (sensitivity 84.7%) with 8 assignments, which were not validated by any manual curation (selectivity 90.0%).
Fourteen sites were rejected by both methods giving a MCC of 0.450.
Further restriction to just top-ranked PSMs reduces the number of positively identified sites to 76, 83.5% of those manually curated from all PSMs.
The automated validation identifies 62 correctly (sensitivity of 81.6%) and assigns a further 3 sites with no manual curation (selectivity of 95.4%).
Only 1 site was rejected Fig.4.
A manually verified PSM that ProPhosSI fails to validate.
Many ion labels are not shown for clarity.
Evidence for phosphorylation at S1 arises from the b2 ion (a).
ProPhosSI requires ion transitions over a phosphosite and so requires more than one ion.
Evidence for phosphorylation at S4 arises from the uniquely assigned des-phospho y10 [2+] ion.
ProPhosSI does not consider 2+ ions as they can in many cases be assigned to more than one fragment.
by all automated and manual PSM curations giving a MCC of 0.037.
Taking into account the sites rejected at the phosphopeptide assignment level (80 sites), the MCC is recalculated as 0.792.
This compares favourably with the MCC for all sites (141 rejected at the peptide level) of 0.593 but is almost identical to the MCC for all sites from Rank 2 or better peptides (120 rejected at the peptide level) of 0.793.
We examined a small number of high-scoring PSMs where ProPhosSI fails to validate an assignment made by an experienced analyst.
ProPhosSI appears to be least effective in where the phosphate is not labile, is located on the N-terminal residue or product ions are multiply charged.
An example is shown in Figure 4.
4 CONCLUSIONS Validation of phosphopeptide identifications by the examination of spectra has historically been a bottleneck in high-throughput phosphoproteomics.
It may require many hours of careful crosschecking for each of many thousands of individual PSMs reported by a general database search algorithm such as MASCOT to establish whether it corresponds to a confident identification.
We have demonstrated that the search engine score alone is an insufficient parameter for determining whether a phosphorylation site should be accepted.
With high-throughput proteomics, this bottleneck becomes critical, precluding the use of phosphosite analysis as a routine screening tool.
In this article, we have demonstrated a methodology which, by modelling the analysis and decision process of an experienced scientist, can substantially speed up validation of a large scale phosphoproteomics dataset by processing the data with 8095% confidence in the positive validations and with a high sensitivity.
Not only does it provide a massive speed benefit, allowing the processing of a complete phosphoproteomics experiment overnight, but it provides a report for all considered peptide hits, identifying the features it expects to see and reporting on them.
This processing and visualization then provides a framework for an experienced analyst to manually curate the difficult cases or to re-examine those cases that may be interesting from a biological perspective.
2158 [11:57 10/8/2010 Bioinformatics-btq341.tex] Page: 2159 21532159 High-throughput phosphoproteomics We have been conservative in our application of the analysis criteria.
Even a small decrease in the number of spectra, which must be validated manually, provides a boost to the researcher in terms of time gained.
For this study, we estimate the time saved to be several personmonths work on a typical whole cell phosphoproteome screen.
It is essential, however, to reduce the number of false assignments to a minimum such that researchers making use of these assignments do not waste time chasing false leads.
We have been conservative in our approach, resulting in a low error rate and, including curation at the phosphopeptide level, an exceptionally high MCC.
Taking more stringent criteria, such as requiring more than one positive match to automatically call a site, provide a slight but measurable increase in accuracy with the downside that coverage is reduced.
Combining multiple experiments from multiple machines should provide a wealth of data that can be combined to improve the overall coverage of phosphoproteome identification with minimal degradation in assignment quality.
In this study, we have demonstrated the utility of a methodology for automatically curating large-scale phosphoproteomics experiments.
The principles behind the methods used in the study are simple and easy to comprehend.
Access to this and similar methodologies should assist phosphoproteomics as a routine systems biology tool, allowing a deeper understanding of the essential role of phosphorylation in the function of the cell through rapid, global phosphoproteome analysis.
ACKNOWLEDGEMENTS D.M.A.M.
conceived the automated analysis system, designed and implemented the analysis framework and curation system.
I.R.E.N.
provided the experimental dataset and performed the manual curation of phosphopeptides.
J.D.B.
conceived and constructed the MLRV database.
N.A.M.
provided guidance on construction of the analytical rules and critical assessment of the software.
M.A.J.F.
provided guidance for the project and direction in the definition of the criteria used for assessment.
F.V.
performed the SEQUEST and Peptide Prophet analyses, and provided useful insight into these methods.
This report was prepared by D.M.A.M.
and refined in collaboration with the other authors.
We would like to thank Dougie Lamont and Dr David Campbell for their insights into spectral interpretation, and Dr Tom Walsh for expert systems support.
Funding: Wellcome Trust (programme grant 085622 to M.A.J.F.
; PhD studentship to I.R.E.N.
); the Medical Research Council (to N.A.M.
and F.V.)
and the Biotechnology and Biology research Council IRColl RASOR project (to N.A.M.
and F.V.).
Conflict of Interest: none declared.
ABSTRACT Summary: Insertional mutagenesis is a powerful method for gene discovery.
To identify the location of insertion sites in the genome linker based polymerase chain reaction (PCR) methods (such as splinkerette-PCR) may be employed.
We have developed a web application called iMapper (Insertional Mutagenesis Mapping and Analysis Tool) for the efficient analysis of insertion site sequence reads against vertebrate and invertebrate Ensembl genomes.
Taking linker based sequences as input, iMapper scans and trims the sequence to remove the linker and sequences derived from the insertional mutagen.
The software then identifies and removes contaminating sequences derived from chimeric genomic fragments, vector or the transposon concatamer and then presents the clipped sequence reads to a sequence mapping server which aligns them to an Ensembl genome.
Insertion sites can then be navigated in Ensembl in the context of genomic features such as gene structures.
iMapper also generates test-based format for nucleic acid or protein sequences (FASTA) and generic file format (GFF) files of the clipped sequence reads and provides a graphical overview of the mapped insertion sites against a karyotype.
iMapper is designed for highthroughput applications and can efficiently process thousands of DNA sequence reads.
Availability: iMapper is web based and can be accessed atContact: da1@sanger.ac.uk; iMapper@sanger.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Retroviral-based insertional mutagenesis screens in mice have been a valuable tool for the discovery of oncogenes and tumor suppressors in mice (Mikkers and Berns, 2003), and also for gene discovery in cultured cells (Du et al., 2005).
More recently transposon-based approaches such as the use of the Tc1-family transposon Sleeping Beauty (Collier et al., 2005; Dupuy et al., 2005) and the Trichoplusia-derived transposon Piggybac (Wang et al., 2008) have been developed increasing the repertoire of insertional mutagens available as gene discovery tools in mammals.
To determine where in the genome an insertional To whom correspondence should be addressed.
mutagen has inserted the usual approach is to use a linker based polymerase chain reaction (PCR) method, such as vectorette or splinkerette (Devon et al., 1995).
For any insertional mutagenesis screen to cover a significant proportion of the genome, it is desirable to perform a screen using hundred of mice, and hundreds if not thousands of cell clones.
Thus insertional mutagenesis screens may involve the generation and analysis of tens of thousands of DNA sequence reads from insertion sites.
Although linker based PCR methods are generally specific, non-specific PCR products, chimeric sequences and sequences derived from transposon concatemeric arrays can all represents contaminating sequences within pools of insertion site PCR products.
Thus without careful processing of DNA sequence data, the direct mapping of insertion site sequence reads to the genome may result in the identification of false-positive insertion sites.
To facilitate the analysis of linker mediated insertion site sequences, we have developed a web application called iMapper (Insertional Mutagenesis Mapping and Analysis Tool).
Using linker based PCR sequence reads as input iMapper uses a local sequence alignment algorithm to identify a tag sequence derived from the end of the insertional mutagen (Supplementary Material).
iMapper then scans the downstream sequence for user defined contaminating sequences, processes the sequence to identify the restriction site sequence used for linker ligation during the insertion site PCR, clips out the genomic sequence between the tag and first restriction enzyme cutting site and presents this sequence to a rapid mapping algorithm called sequence search and alignment by hashing algorithm (SSAHA) (Ning et al., 2001).
Output is then generated in various formats.
The main features of iMapper include: (1) Efficient and accurate processing of insertion site sequence data and analysis against Ensembl human, mouse, rat, zebrafish, Drosophila and Saccharomyces cerevisiae genomes.
(2) Output of annotated sequence reads in tabular format with links to Ensembl ContigView so that insertion sites can be viewed in the context of gene structures and other genomic features.
(3) Output of processed sequence data in test-based format for nucleic acid or protein sequences (FASTA) and generic file format (GFF) allowing insertion site sequence data to be analyzed in any sequence analysis package and displayed as a distributed annotation system (DAS) track against an Ensembl genome.
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
J.Kong et al.(4) Output of a graphical chromosome KaryoView showing insertion sites against an ideogram of each chromosome.
2 METHODS 2.1 Architecture The iMapper interface is web-based (Supplementary Material).
The sequence analysis module within iMapper uses Perl and computer generated imagery (CGI), and for comparison to an Ensembl genome Perl scripts run against the Ensembl application programming interface (Ensembl-API).
2.2 Input format Sequence is imported into iMapper in FASTA format.
Sequence data in this format can either be pasted into the text box or imported using the file upload option (Supplementary Material).
2.3 User defined parameters After sequence input, a user can define the species against which they would like their insertion site data analyzed from human, mouse, rat, zebrafish, Drosophila and S. cerevisiae.
The orientation of the tag in the sequence can then be chosen, and the output option selected.
Selecting output good sequenceswill exclude those sequences that do not contain the tag sequence, sequences that do not map to the genome, and sequences that are identified to contain contaminating sequences from all output formats.
The sequence of the mutagen tag sequence can then be specified, or a prevalidated tag sequence can be selected from the drop down menu.
We provide tag sequences for the transposons Sleeping Beauty and Piggybac, and for the U3LTR of the MuLV retrovirus.
The sequence of the restriction site can then be specified.
At this point, the user has defined the boundaries of the sequence which will be mapped to the genome as the sequence between the tag and the restriction site or linker.
Advanced options can then be specified.
These include the tag alignment parameters and the sequence of contaminating sequences, which will be highlighted in the tabular format (Supplementary Material).
It is also possible to specify the parameters used by the SSAHA algorithm for matching the genomic sequence between the tag and the restriction site or linker, to the genome.
Finally, it is possible to specify the criteria for gene overlaps.
By specifying gene overlaps, it is possible to vary the spatial criteria for defining what constitutes an insertion event in or near a gene.
For example, it may be desirable to identify insertion events that mutate in or upstream of a gene, but not downstream.
Sequence processing commences when the Submit Query button is selected.
2.4 Sequence processing The procedure used by the code for sequence processing is shown in the Supplementary Material.
2.5 Tabular format The tabular format is an html-based output of the analyzed sequence data (Fig.1).
Sequences such as the tag sequence, the restriction site and contaminating sequence are highlighted in this view.
Links to Ensembl gene pages and to the Ensembl ContigView are also provided from this page.
2.6 FASTA and GFF formats A FASTA file of the processed traces and a GFF file of the data are provided when the analysis run is complete.
Fig.1.
iMapper maps and analyzes insertion site sequence data.
iMapper generates output in several formats including a tabular format with links to Ensembl ContigView and Ensembl gene pages (upper panel), GFF file format, FASTA format and a KaryoView format (lower panel) which provides a global overview of all insertion site sequences that were mapped to the genome.
Sequence traces displayed in the tabular format are annotated to show the location of the tag sequence (green), the restriction enzyme site (orange) and the mapped genomic sequence (yellow).
An alignment of the tag sequence from the trace and the insertion site mapping location is also shown in the tabular format.
In the KaryoView format, each red triangle indicates an independent insertion site displayed against a genome ideogram.
2.7 KaryoView To obtain a global overview of the sequence data, iMapper has a link to an Ensembl KaryoView providing a graphical view of the data against a chromosomal ideogram (Fig.1).
2.8 Performance The specificity of tag identification depends on the length of the tag sequence entered, and the predefined thresholds specified for sequence tag identification including the percentage alignment threshold, gap penalty, match and mismatch score.
Longer tag sequences, higher alignment percentages and more stringent gap and mismatch scores will result in more accurate tag sequence identification.
We have tested the optimal tag sequence length and percentage threshold using a dataset of 1920 PiggyBac insertion site sequence reads (Wang et al., 2008).
Because PiggyBac integrations invariably occur at TTAA sites a precisely identified tag sequence will always be followed by the sequence TTAA.
As shown in the Supplementary Material, the minimal advisable tag sequence length is 15 bp.
We determined the optimal percentage threshold for sequence tag identification, to be used as the default, and determined this to be 80% (Supplementary Material).
Finally, we optimized the SSAHA sequence mapping parameters to be used as the default finding that for sequences from splinkerette-PCR reactions 2924 A web application for the automated analysis containing genomic junction fragments of on average 200 bp in length the optimal SSAHA score is 35.
This score should be ideal for insertion site sequences generated by capillary read sequencing but may need to be lowered to 20 for shorter reads such as those generated by 454 sequencing.
It is advisable to optimize the SSAHA mapping score for each dataset selecting a score that generates the highest number of uniquely mapped reads.
This is important because the default mapping parameters used by iMapper are stringent and will return only those reads that map to unique unambiguous genomic locations.
We have used iMapper to analyze up to 20 000 DNA sequence traces.
It takes, on average, 12 s for iMapper to analyze each DNA sequence trace (Supplementary Material) and to return the analyzed data in tabular, ContigView, GFF, FASTA and karyoView formats.
3 SUMMARY iMapper is a web-based freely accessible solution for the analysis of insertional mutagenesis datasets and should facilitate the many insertional mutagenesis screens that are ongoing worldwide.
Funding: Cancer Research-UK (C20510/A6997) and the Wellcome Trust (76943 to D.J.A.
); Wellcome Trust Sanger Institute PhD programme (to J.K. and F.Z.).
Conflict of Interest: none declared.
Abstract The development and application of systems strategies to biology and disease are transforming medical research and clinical practice in an unprecedented rate.
In the foreseeable future, clinicians, medical researchers, and ultimately the consumers and patients will be increasingly equipped with a deluge of personal health information, e.g., whole genome sequences, molecular profiling of diseased tissues, and periodic multi-analyte blood testing of biomarker panels for disease and wellness.
The convergence of these practices will enable accurate prediction of disease susceptibility and early diagnosis for actionable preventive schema and personalized treatment regimes tailored to each individual.
It will also entail proactive participation from all major stakeholders in the health care system.
We are at the dawn of predictive, preventive, personalized, and participatory (P4) medicine, the fully implementation of which requires marrying basic and clinical researches through advanced systems thinking and the employment of high-throughput technologies in genomics, proteomics, nanofluidics, single-cell analysis, and computation strategies in a highly-orchestrated discipline we termed translational systems medicine.
Keywords: Systems biology; P4 medicine; Family genome sequencing; Targeted proteomics; Single-cell analysis Introduction Systems biology strives to unravel the enormous complexity of biological systems through a holistic approach in the context of a cross-disciplinary environment.
Since its founding in early 2000, the Institute for Systems Biology (ISB) has been pioneering systems strategies to biology and disease through the development of systems strategies and the application and/or development of cutting-edge high-throughput technologies to the investigation of model organisms and humans with varying degrees of complexity: from single-cell organisms (bacteria and yeast) [13] to experimental animal models (mouse) [47] and to human disorders [810].
Over the last decade, rapid advancements in genomic and proteomic technologies, computational 1672-0229/$-see front matter 2012 Beijing Institute of Genomics, Chinese A Ltd and Science Press.
All rights reserved.
Corresponding authors.
E-mail: Leroy.Hood@systemsbiology.org (Hood L), Qiang.Tian@systemsbiology.org (Tian Q).
strategies and their applications in human diseases have demonstrated promising early success in genomic medicine.
We discuss here our view of how systems approaches to biology and disease and emerging technologies are going to transform the medical practices by shaping up translational systems medicine for early diagnosis, disease progression, patient stratification, predicting recurrence, and therapeutic guidance.
Dealing with disease complexitysystems medicine and its 5 pillars Human phenotypes are specified by two types of biological information: the digital information of the genome, and the environmental information that impinges upon and modifies the digital information.
Two general biological structures connect the genotype and environment to phenotype: (1) biological networks capture, transmit, process and pass on information; these networks organize, cademy of Sciences and Genetics Society of China.
Published by Elsevier 182 Genomics Proteomics Bioinformatics 10 (2012) 181185 integrate and model data to enormously increase the signal to noise; (2) simple and complex molecular machines execute biological functions.
A systems view of disease postulates that disease arises from disease-perturbed networks.
A ramification of this premise entails studies of disease pathogenesis at the network level through a systems approach so that better strategies for early diagnosis and therapeutics targeting these perturbed networks can be devised.
We stipulate five pillars to address disease complexity upholding systems approach as follows.
(1) Viewing biology and consequentially medicine as an informational science is one key to deciphering complexity.
(2) Systems biology infrastructure and strategyholy trinity of biology (i.e., use biology to drive technology and computation development)endorse crossdisciplinary culture and democratization of datageneration and data-analysis tools.
(3) Holistic, systems experimental approaches enable deep insights into disease mechanisms and new approaches to diagnosis and therapy through analyzing the dynamics of disease processes.
(4) Emerging technologies provide large-scale data acquisition and permit exploration of new dimensions of patient data space.
(5) Transforming analytic tools will allow deciphering the billions of data points for each individual sculpting in exquisite detail the wellness and disease landscapes.
These five fundamental principles will allow in-depth interrogation of diseased networks at unprecedented molecular resolution.
Some disease events will occur well before the disease manifestation for early detection, whereas key nodal points amongst perturbed networks can be identified for diagnostic detection or therapeutic interventions.
Both diseased organs/tissues and patient blood constitute excellent specimen reservoirs for systemic assessment of diseased conditions in multiple spatial and temporal measurements.
Whole genome and whole tranTable 1 Clinical assays and emerging technologies for exploring new dimensio Genomics Complete individual genome sequences will be done by sequencing families Complete individual cell genome sequencescancer Complete MHC chromosomal haplotypes in familiesautoimmune disease an 300 Actionable gene variantspharmacogenetics-related and disease-related g Sequence 1000 transcriptomestissues and single cellsstratification disease Analyze aging transcriptome profilestissues and single cellswellness Analyze miRNA profilestissues, single cells and blooddisease diagnosis Proteomics Organ-specific blood SRM protein assays 2500 Blood organ-specific blood proteins from 300 nanoliters of blood in 5 m New protein capture agentsD-amino acid peptides joined to create dimer or Array of 12,000 human proteinsagainst autoimmune or allergic serastrati Single molecule protein analysesblood organ-specific proteins and single cel SWATHe analysesglobal, dynamical analyses scriptome sequencing, targeted proteomics via mass spectrometry and protein chips, single-cell analysis and a variety of targeted nucleic acid detection systems (e.g., next-generation sequencing (NGS), DNA arrays, NanoString n-Counter [11], Fluidigm BioMark, etc.)
will be the workhorse churning out enormous amount of data.
We anticipate that in 10 years each individual will be surrounded by a virtual cloud of billions of data points.
A key challenge is to fully integrate these diverse data type, correlate with distinct clinical phenotypes, extract meaningful biomarker panels for guiding clinical practice.
We enumerate here some of the individual patient information-based assays of the present and future (Table 1).
Family genome sequencing: integrating genetic and genomics Complete human genome sequence is becoming increasingly affordable and will be a fundamental part of ones medical record in 10 years.
While a great deal can be learned regarding ones predisposition to certain diseases from individual genome, sequencing of a family permit one to use the principles of Mendelian genetics to eliminate 70% sequencing error.
This will greatly facilitate better identification of rare variants, determining chromosomal haplotypes and intergenerational mutation rate, and identification of candidate genes for simple Mendelian diseases.
Moreover, knowledge of cis and trans linkage relationships of genes and control elements will be key for understanding biology and disease, and reducing the chromosomal search space for disease genes [9,12].
Recent developments by Complete Genomics Inc (CGI) employing long fragment reads (LFR) have demonstrated whole-genome sequencing from as few as 1020 cells with three striking advances over typical NGS approach.
These advances include (1) high accuracy with a genome error rate of 1 in 10 megabases; (2) assembly of diploid haplotypes from individual genome sequences; and (3) de novo assembly of individual genomes, which enables discovery of structural variations [13].
With this technology, comprehensive genetic studies and diverse clinical applications are within reach.
ns of patient data space predictive health history d allergies enes intwice per year (50 proteins from 50 organs)wellness assessment trimer capture agents fydiseases that kill cells (neurodegenerative) l analyses Hood L and Tian Q/ Translational Systems Medicine 183 Systems approach to blood biomarkers: making blood a window into health and disease Since blood baths all organs and receives their biomarkers, it shall reflect network disease-perturbations either directly or indirectlya molecular fingerprint in the blood reflecting disease pathophysiology.
We stress that organ-specific, cell-type specific or organelle-specific biomarkers are more informative since they inform as to the tissue, cell type or organelle sources of the disease.
Moreover, blood biomarkers may also reflect general cell death or damage (e.g., biomolecules released from nucleus or cytoplasma), secreted protein or membrane perturbations through proteolysis.
Systems blood biomarkers shall include diverse types of biomolecules: proteins, mRNAs, non-coding RNAs (e.g., microRNAs, long intergenic non-coding RNAs), metabolites, etc, while the combination of two or more types increases sensitivity and specificity of assay.
These markers should be multiparameter consisting of many biomolecules of the same type, and even panels of multiple types of molecules so that multiple networks and features may be accessed.
Ideally, blood biomarker panel shall assess all diseases in a given organ simultaneously.
Another important point is that, given the vast individual variation, blood biomarkers should be analyzed in a longitudinal manner so that the individual can be their own control against which change can be measured.
Of note, another information-rich compartment in the blood includes the cellular component, e.g., the peripheral blood mononuclear cells (PBMCs).
These PBMCs contain mainly white blood cells (WBCs) for diagnosing inflammation, immunity and cell death; they also contain rare circulating tumor cells (CTCs) in cancer patients, indicative of tumor progression and recurrence [14,15].
Our method of choice for evaluating blood protein biomarkers is targeted proteomics employing selective reaction monitoring (SRM) mass spectrometry (MS) [3].
This technology allows the analysis of 100200 proteins quantitatively in 1 h. ISB has developed SRM assays for most of the known 20,333 human proteins.
In particular, we have validated SRM assays for 100 brain-specific and 100 liver-specific proteins for human and mouse [16].
These protein panels have been applied in mouse disease models and patient blood samples for successful identification of biomarkers for the diagnosis of liver injury, liver fibrosis/ cirrhosis, prion and other neurological diseases.
For instance, we identified a panel of 15 brain-specific blood proteins that indicate the initiation and progression of disease-perturbation of networks (prion accumulation, glial activation, synaptic degeneration, and neuronal cell death) in a mouse model of prion disease [4].
A panel of three liver-specific proteins successfully stratify liver cirrhosis patients from patients with various degree of liver fibrosis and normal controls [16].
The same strategy is being actively pursued for the identification of brain tumor cell membrane protein biomarker in the blood (unpublished data).
While it is conceivable to set up a SRM-MS infrastructure to provide blood diagnostics to serve clinical needs for a variety of diseased conditions as discussed above, this requires highly-sophisticated expertise in MS instrumentation and supporting informatics capacities.
The company Integrated Diagnostics is pursuing a systems approach to diagnostics for selected disease applications.
An alternative is to develop targeted protein and antibody chips or chips of protein-catalyzed capture (PCC) agents.
The latter demonstrates advantages since it is chemically-stable, low cost, and requires relatively little input of blood samples.
In addition, we are developing a protein Elisa assay on the NanoString n-Counter instrument, in conjunction with their capacity to detect mRNA and miRNA molecules, to generate an assay that combines multiple analytes (mRNA, miRNA, and protein) in a single platform with no loss in sensitivity.
We envision that in a 10-year future, an integrated nanotech/microfluidics platform, consisting of 50 organ-specific blood proteins from each of 50 major human organs, will measure 2500 blood proteins using a fraction of droplet of blood in 5 min at the mid amol level of sensitivity.
The prototype of this nanochip has already been tested in hospitals [17,18].
Single-cell analysis allows interrogation of heterogeneous cell populations at unprecedented resolution Most of the current global molecular profiling studies measure mixed diseased cell populations for averaged signals.
However, there are distinct cell types in any given diseased tissues each with its own distinct perturbed genomic and proteomic profiles.
Although global genome and transcriptome sequencing for single cell is still challenging, early efforts have already revealed important population heterogeneity in tumor cells [19,20].
We envision that more singlecell analysis will be applied clinically.
For instance, one can analyze 10,000 B cells and 10,000 T cells for the functional regions of their immune receptors to inform past and present immune responsiveness, follow vaccinations, and identify autoimmune antibodies.
Single-cell analysis can also be applied concomitantly with various technologies for separating epithelial cells from WBCs in blood, for identifying and monitoring of CTCs.
Single-cell transcriptome analysis can also be applied to quantize cell populations in cancer tissues and differentiating progenies of stem cells.
Systems medicine is transforming healthcare leading to predictive, preventive, personalized and participatory (P4) medicine Systems medicine provides fundamental insights into disease network mechanisms to enable diagnosis, therapy and prevention for the individual patient (Figure 1).
Family genome sequencing reveals disease and wellness genes and actionable genes.
Transforming blood into a window to distinguish health from disease opens up new way for Systems medicine: network of networks Networks organize, integrate and model data to enormously increase the signal to noise Figure 1 Networks organize and integrate information at different levels to create biologically meaningful models Networks formulate hypotheses about biological function and provide temporal and spatial insights into dynamical changes.
184 Genomics Proteomics Bioinformatics 10 (2012) 181185 disease diagnostics, and assessment of drug toxicity and wellness.
Molecular profiling stratifies diseases into their distinct molecular subtypes for impedance match with appropriate drugs.
New approaches to drug target discovery are being devisedre-engineer disease-perturbed networks with drugs for faster and cheaper drug development.
The convergence of the digital revolution and systems medicine leads to deciphering of complexity and P4 medicine (1) Predictive: the probabilistic health history is revealed by DNA sequence and regular multi-parameter (blood) measurements.
(2) Preventive: design of therapeutic and preventive drugs and vaccines via systems approaches; emphasis on wellness.
(3) Personalized: unique individual human genetic variation mandates individual treatment and that patient will be their own control for data analyses.
(4) Participatory: patient-driven social networks for disease and wellness will be a driving force in P4 medicine.
Society must access patient data and make it available to biologists for pioneering predictive medicine of the future.
How does one educate patients, physicians and the healthcare community about P4?
The answer is IT for healthcare.
P4 medicine differs from evidence-based medicine in that it is proactive, individualized, with an emphasis not only on disease, but also on wellness.
It involves generation, mining and integration of enormous amounts of data on individual patients to produce predictive and actionable models of wellness and disease.
Large patient populations will be analyzed at single individual level (not population averages) to generate quantized stratification of patient populations and create the predictive medicine of the future.
It entails patient-driven social networks.
There are several societal implications for P4 medicine.
It forces a revision of business plans of almost every sector of healthcare industry, producing enormous economic opportunity.
Digitalization of medicine for the individual patients is a larger revolution than the digitization of information technologies and communication in that it is patient-driven medicine and wellness.
It turns sharply around escalating costs of healthcaredemocratization of healthcare through (1) early blood diagnosis; (2) benefits of wellnesse.g., survey biannually 2500 blood organ-specific protein measurements (50 from each of the 50 major organs) for global early detection of the transition from health to disease; (3) digital technologies exponentially increasing in measurement potential and decreasing in costsculpt for individuals the dimensions of health/disease while dramatically decreasing measurement costs, e.g., sequencing a human genome cost about $300 million dollars in 2000 but only about $3000 in 2012a 100,000fold decrease in costfor digitalization of medicine.
Eventually, P4 medicine will create significant wealth.
Translational systems medicine should practice proactive P4 medicine A core mission of ISB is to disseminate systems approaches to biology and medicine to the society by and large.
ISB has formed strategic partnerships with Ohio State University, Peace Health, and the State of Luxembourg to promote the practice of P4 medicine.
We propose that any institutions wishing to establish a translational systems medicine program shall adopt the five pillars of a systems approach to disease: an informational view of biology and disease, a cross-disciplinary infrastructure, global experimental systems approaches to capture the dynamics of disease, employ of emerging technologies to search new areas of patient data space and powerful novel analytical tools to handle all the new data generated.
They shall partner with institutions who have systems biology, systems medicine and P4 medicine expertise who can guide, teach and help recruit leadership who understands systems medicine and translational opportunities.
Committed political and scientific leadership at both local and national levels are also indispensible.
Competing interests The authors have declared that no competing interests exist.
Acknowledgements We gratefully acknowledge funding from the Grand Duchy of Luxembourg, NIH/NCI NanoSystems Biology Cancer Center (Grant No.
U54 CA151819A), NIH/NIGMS Hood L and Tian Q/ Translational Systems Medicine 185 Center for Systems Biology (Grant No.
P50GM076547) and NIH/NIAMSD (Grant No.
RC2AR059010).
ABSTRACT Motivation: Homologous protein families share highly conserved sequence and structure regions that are frequent targets for comparative analysis of related proteins and families.
Many protein families, such as the curated domain families in the Conserved Domain Database (CDD), exhibit similar structural cores.
To improve accuracy in aligning such protein families, we propose a profile profile method CORAL that aligns individual core regions as gap-free units.
Results: CORAL computes optimal local alignment of two profiles with heuristics to preserve continuity within core regions.
We benchmarked its performance on curated domains in CDD, which have pre-defined core regions, against COMPASS, HHalign and PSI-BLAST, using structure superpositions and comprehensive curator-optimized alignments as standards of truth.
CORAL improves alignment accuracy on core regions over general profile methods, returning a balanced score of 0.57 for over 80% of all domain families in CDD, compared with the highest balanced score of 0.45 from other methods.
Further, CORAL provides E-values to aid in detecting homologous protein families and, by respecting block boundaries, produces alignments with improved readability that facilitate manual refinement.
Availability: CORAL will be included in future versions of the NCBI Cn3D/CDTree software, which can be downloaded atContact: fongj@ncbi.nlm.nih.gov.
Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Homologous protein families contain core regions that reflect conservation in molecular evolution.
Many protein family alignments in Pfam (Finn et al., 2006), SMART (Letunic et al., 2006) and SUPERFAMILY (Wilson et al., 2007) exhibit conserved regions including blocks, or ungapped regions, within an alignment.
The Conserved Domain Database (CDD) (Marchler-Bauer et al., 2009) models protein domains explicitly as series of blocks.
For NCBIcurated domains, the blocks represent structural core motifs based on structure superpositions as well as conserved sequence regions and motifs.
Comparative analysis of proteins and protein families through sequence alignment is invaluable for grouping homologs, To whom correspondence should be addressed.
subdividing diverse families into sub-families, tracing evolutionary histories and identifying conserved functional sites.
In recent years, alignment methods that compare two profiles, the statistical models that represent protein families, have been shown to improve alignment quality and homolog recognition over sequencesequence methods such as BLAST (Altschul et al., 1997) and sequenceprofile methods such as PSI-BLAST (Altschul et al., 1997; Schaffer et al., 2001).
Numerous profile alignment methods have been assessed in Edgar and Sjolander (2004), Heger and Holm (2001), Ohlson and Elofsson (2005), Ohlson et al.(2004), Panchenko (2003), Rychlewski et al.(2000), Soding (2005), Yona and Levitt (2002) and others.
While many alignment methods focus on detecting remote homologs in order to expand coverage of functional inference, obtaining high-quality alignments remains difficult even for closely-related families.
According to structure superpositions, corresponding core regions in many homologous domains differ by fewer insertions and deletions than inferred by general alignment programs, reflecting the stability of the structural core of the protein family.
To better capture this property, we propose a method CORAL (CORe ALigner) to align core regions from two protein families without indels within blocks, which we will refer to as the core constraint.
CORAL is implemented through a common dynamic programming engine for optimal pair-wise alignment (Needleman and Wunsch, 1970; Smith and Waterman, 1981).
Several other algorithms to align sequence or sequence profiles to core regions have been effective for detecting similarities or assigning domains.
These algorithms include a profileprofile method using Gibbs sampling (Panchenko, 2003), and SALTO (Kann et al., 2005) and GLOBAL (Kann et al., 2007) which employ additional block-based constraints.
SALTO aligns a consecutive subset of complete blocks and GLOBAL aligns a sub-set (including full or empty set) of contiguous columns within every block.
All of these methods disallow indels in alignments of blocks and exclude sequence regions outside blocks.
Additionally, LAMA (Pietrokovski, 1996) and CYRCA (Kunin et al., 2001) were developed to align individual blocks that represent sequence motifs (Henikoff et al., 2000).
Block shift and extension operations have also proved useful to improve multiple sequence alignments (MSAs) through REFINER (Chakrabarti et al., 2006).
Here, we present the CORAL algorithm and benchmark its performance on curated domains in CDD against other widely used profile methods COMPASS (Sadreyev and Grishin, 2003), HHalign (Soding, 2005) and PSI-BLAST.
Reference alignments are inferred from structure superpositions from the VAST database 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1863 18621868 CORAL (Gibrat et al., 1996; Madej et al., 1995) and the SABmark benchmark set (Van Walle et al., 2005), and from a comprehensive set of expert-determined mappings, and homology is defined by CDD relationships.
In particular, CORAL outperforms all other methods in the quality of alignments.
We also discuss the role of profile alignment in modeling protein families.
2 METHODS 2.1 Core regions dataset MSAs representing protein family core regions were taken from the curated domains in CDD.
Sequence regions outside the cores are not aligned in CDD and are not considered in this study.
Here, we use the terms domain and protein family interchangeably.
NCBI-curated domains have been organized into hierarchical domain families.
A superfamily, which indicates common evolutionary descent, contains one or more domain families.
We define related domains with respect to CDD to be those in the same family and unrelated domains to be those in different superfamilies, in order to minimize false positives (FPs).
A set of 100 domains, chosen randomly from different superfamilies, was reserved for parameter optimization (dataset opt100).
Similarity between domains was estimated as the fraction identity of their consensus sequences with pair-wise sequence alignments computed by MUSCLE 3.6 (Edgar, 2004).
The consensus sequences express only columns in the MSA with <50% gap content, including the most conserved columns, and hence report higher similarity values than using full length protein sequences.
2.2 Reference alignments To test alignment accuracy, we construct three benchmark datasets.
The first reference set is based on superpositions of the 3D structures that annotate curated domains in CDD v2.14.
We gather structural neighbors from the VAST database that satisfy the default significance cutoff of P-value < 0.0001, such that folds are described by a continuous sequence region that overlaps the sequence fragment in the domain model by 90%.
To ensure that the structure alignments involve core regions, aligned core positions are required to comprise 80% of all structurally aligned positions and 50% of the respective profiles.
This procedure yields structure alignments for 2385 domain pairs within 91 CDD families.
A second set of structure alignments is taken from the superfamilies set in SABmark, that is, alignments of SCOP domains with a common evolutionary origin.
CDD domains are mapped onto the SCOP domains using RPSBLAST (Marchler-Bauer et al., 2002).
Due to time of testing, a later version of CDD (v 2.16) was used for this benchmark set.
SCOP folds are filtered for live sequences in Entrez and at least 50% overlap with the extent of the domain hit, resulting in structure alignments for 1627 domain pairs in 128 SCOP superfamilies.
The two structural reference sets differ in coverage across and within domain families; classification by CDD versus SCOP; and curator-optimized versus RPSBLAST-computed structure-domain alignments.
Athird benchmark set provides comprehensive coverage over homologous domains in CDD.
In NCBI-curated hierarchies, the MSAs of a parent domain and its sub-family contain overlapping fragments from at least one protein sequence.
The shared sequence identifies aligned columns between the two MSAs and reflects the curators assertion of how the sub-family should be mapped to its parent.
Transitivity over each hierarchy extends the guide alignment to all pair-wise comparisons in multi-domain families.
Guide alignments include 57 786 domain pairs over 212 CDD families.
2.3 Alignment algorithm We describe the profile alignment algorithm with core constraint in terms of required modifications to the canonical algorithm for local alignment (Smith and Waterman, 1981).
The problem is to align profiles A = a1 an and B = b1 bm with n and m columns, respectively, where each profile has been subdivided into blocks.
Let table H contain the maximum similarity score of two profile segments ending in ai and bj in entry Hi,j Scoring functions S(ai, bj) to compute the similarity between profile columns ai and bj are described in the next paragraph.
To prevent gaps within blocks, the affine gap penalty is replaced with a large negative value if the last aligned column before the gap is not a block end.
To ensure that the endpoints of the optimal alignment fall on the N-and C-terminal of some blocks, Hi,j may be re-initialized to S(ai, bj) (replacing initialization to 0) if ai or bj is the first column in its respective block and traceback through Hi,j is required to terminate at that position.
Traceback may begin from the maximum Hi,j such that at least one of ai and bj is the end of its respective block.
These changes preserve the O(nm) running time.
The optimal scores from H are normalized into Z-scores as follows.Alarge set of random alignments was simulated using all curated domains, each aligned with 100 domains from different superfamilies.
Alignment scores were binned by the sum of lengths of the profiles.
Regression curves were fitted for the means and SDs over the bins.
The length-dependent values from the regressions were used to compute Z-score.
2.4 Scoring functions Much of the previous work on profileprofile alignment algorithms sought advances through new scoring functions for comparing profile columns.
Probabilistic methods are believed to be the most effective (Mittelman et al., 2003; von Ohsen et al., 2003) and are applied in state-of-the-art aligners such as prof_sim (Yona and Levitt, 2002), COMPASS and HHsearch.
CORAL uses a symmetrical log-odds function similar to Picasso (Heger and Holm, 2001) and COMPASS (Sadreyev and Grishin, 2003): SLO ( a,b )= k Qak log ( Rbk )+ k Qbk log ( Rak ) To compute similarity between aligned columns a and b, Qa and Qb represent vectors of weighted observed frequencies of amino acids k in the respective columns.
Likewise, R is the vector of the frequency ratios of weighted frequency for each amino acid over the background frequency of the amino acid.
Q and R are defined as for PSI-BLAST (Altschul et al., 1997; Schaffer et al., 2001).
Surveys of scoring functions (Edgar and Sjolander, 2004; Mittelman et al., 2003; Panchenko, 2003) have suggested that probabilistic methods offer incremental improvements over simpler functions such as sum of pairs (Gotoh, 1993), dot product and Pearson correlation coefficient.
Consequently, we also test the symmetrical dot product function: SDP ( a,b )=Qa Rb +Qb Ra In Section 3, the two methods will be denoted as CORAL LO and CORAL DP, respectively.
The public release of CORAL will use the better performing log-odds function.
2.5 Parameter optimization A local alignment requires that the expected column score be negative and some column score(s) be positive.
To satisfy these conditions, a constant shift value is added to each column score.
To initialize the search space for potential shift values, we computed the distributions of column scores for correctly aligned columns in all related domains in CDD and for all pairs of columns in a sampling of unrelated domains.
A second parameter, the gap penalty, is necessary to distinguish significant alignments.
Shift values between the means of each distribution and small gap weights were tested systematically over combinations of both parameters.
Performance was assessed for alignment accuracy and homolog sensitivity following the testing procedures and metrics described in Section 3.
Over the opt100 dataset, performance was fairly robust over a range of parameter values.
We assigned shift values of 0.15 and 6.6 for the two scoring functions, respectively, and gap weights of 0.1 and 0.5, respectively.
1863 [18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1864 18621868 J.H.Fong et al.2.6 Statistical significance To approximate the statistical significance of each alignment, we turn to the extreme value distribution (EVD) which has been shown empirically to fit optimal ungapped alignments of random sequences (Karlin and Altschul, 1990).
It is frequently used with gapped sequences and profile alignments.
Supposing that the alignment scores follow an EVD, the E-value for every alignment can be computed from the alignment score z and parameters and as E =e(z).
To determine and , normalized alignment scores from the random alignments described above were fitted to the cumulative density function,F(x) = exp(exp((x))).
Parameters were computed separately for each scoring function SLO and SDP.
The goodness of fit is illustrated for CORAL LO in Supplementary Figure S1.
3 RESULTS 3.1 Alignment accuracy The quality of CORAL alignments between CDD-curated domains was evaluated against the reference alignments described in Section 2 and compared with alignments from COMPASS 3.0, HHalign 1.5.1.1 and PSI-BLAST.
COMPASS is a high-performance implementation of the standard sum-of-scores optimal local alignment and its comparison with CORAL implies a lower bound in improvement that can be attributed to the core constraint.
COMPASS was run with default parameters and with reduced gap penalties.
To promote longer alignments, the gap open penalty was reduced arbitrarily default from 10 to 3 and the gap extension penalty default from 1 to 0.1.
HHalign was run in local and global modes using one domain alignment as query and the other as template.
To compute probabilities and E-values for HHalign, each HMM was calibrated against the cal.hhm database from the download site.
For every pair of domains, a PSI-BLAST alignment was computed between one domain and each sequence from the MSA of the other domain, and vice versa, using the NCBI Toolkit.
The sequence profile alignment with smallest E-value was used as the PSI-BLAST alignment.
CORAL and COMPASS held a speed advantage over the other methods, requiring than a 10th of a second for most inputs.
HHalign required 510 s, largely because of the calibration step.
The following metrics are used to evaluate alignment accuracy.
To measure extent of reconstructing a reference alignment, we compute Sdev, the ratio of the number of correctly aligned positions to the number of aligned columns in the reference alignment.
Sdev is the same as the developers score of (Sauder et al., 2000).
To measure correctness, we compute Smod, the ratio of the number of correctly aligned positions to the number of aligned columns in the evaluated alignment where at least one of each two aligned columns is present in the reference alignment.
This is analogous to the modelers score (Sauder et al., 2000), modified to include only the profile columns that can be determined to be correct or not.
The two previous measures are summarized through a balanced score, Sbalanced = (Sdev +Smod)/2.
To more directly illustrate the trade-off between alignment accuracy and alignment length, we estimate the latter as Scov, the number of aligned positions divided by the length of the shorter profile.
Results from multiple structure alignments for the same domain pair are averaged over the domain pair.
First, we analyze overall performance over CDD families and SCOP superfamilies, both referred to as families for brevity.
An average Sbalanced for every family is taken over its domain pairs (Fig.1).
CORAL produced high-quality alignments for more families than the other methods: 44% of domain families average Fig.1.
Distribution of balanced scores from three benchmark sets: (A) VAST structure superpositions; (B) SABmark structure alignments; and (C) curator-inferred guide alignments.
SABmark alignments are grouped by SCOP superfamily and the others by CDD family.
The balanced score is an average of accuracy over computed alignment and completeness in reconstructing the reference alignment.
Sbalanced 0.8 compared with 41% by the best non-CORAL method according to the VAST benchmark, 37% versus 28% according to the SABmark benchmark and 57% versus 45% by guide alignments.
In nearly all of these families, the alignments with Sbalanced 0.8 were both accurate and complete.
Under the three highest performing methods (CORAL LO, CORAL DP and HHalign global), over 96% of domain families with Sbalanced 0.8 had both Sdev 0.8 and Smod 0.8 with respect to all benchmark sets.
Comparison of Sbalanced over the domain pairs present in more than one benchmark set reveals high consistency among the reference alignments.
For pair-wise comparison of the reference sets, we identified domain pairs present in both benchmark sets.
Sbalanced scores for the common domain pairs, averaged over domain families, were 0.0260.032 lower according to the different 1864 [18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1865 18621868 CORAL alignment methods in VAST alignments than the corresponding guide alignments and 0.0320.047 lower in SABmark alignments than the corresponding guide alignments.
Approximately 2% of domain pairs had balanced score at least 0.01 higher by VAST structure alignments, 30% of domain pairs had balanced score 0.01 higher for guide alignments and the remaining two-thirds of domain pairs had negligible differences between those two references.
We hypothesize that guide alignments are more accurate because they are the outcome of manual curation that reviews both structure-based alignments and patterns of sequence conservation, where the latter may overrule structure superimposition.
Nearly 19% of the domain pairs evaluated with SABmark were assigned to different CDD superfamilies (but the same SCOP superfamilies).
Average Sbalanced score over these pairings was less than half that from domains in the same CDD superfamily, resulting in a larger fraction of families with low-balanced score than from the other reference alignments for all alignment methods (Fig.1).
CORAL returned highest average Sbalanced score for domains in different CDD superfamilies as well as domains from the same CDD superfamilies.
The higher Sbalanced scores for both CORAL methods over the other methods suggest that the core constraint played a significant role in improving performance for several families.
For some families, including Macro and PDZ, all members benefited from the core constraint.
Domain families that benefited the most and the least using CORAL are listed in Supplementary Table S1.
No correlation was observed between average similarity within families and improvement, or lack thereof, from using CORAL.
Better alignments generally came about because CORAL prevented spurious intra-block gaps and shifted blocks that were misaligned by COMPASS and HHalign into the right positions.
The families with most negative effect from CORAL, phosphofructokinase (PFK) and Rieske, illustrate the case where long blocks must be split to enable a completely correct CORAL alignment.
One example is the alignment of two Rieske domains: nonheme iron oxygenase family/nathphalene 1,2-dioxygenase subfamily (cd03535) and small sub-unit of Arsenite oxidase family (cd03476).
Families such as the kinesin/myosin motor domains contain dissimilar sub-groups such that domains within a sub-group are aligned with much higher accuracy than domains from different sub-groups.
To account for varying difficulty, domain pairs were grouped by sequence identity.
The distribution of sequence identity is shown in Supplementary Figure S2 with mean percent identity 29.6% and SD 10.1%.
We partitioned alignments into four similarity ranges: 020%, 2030%, 3040% and 40%.
Results from guide alignments are provided in Figure 2 and referred to in the remainder of the section; results from VAST and SABmark alignments illustrate similar trends and are provided in Supplementary Figure S3.
Sdev and Smod results within each similarity range are consistent across most alignment methods (Fig.2), pointing to the inherent ease or difficulty of aligning particular domains.
CORAL has highest Sdev over all similarity ranges.
Although HHalign local and COMPASS with default arguments have higher Smod at <30% identity, CORAL yields higher Sbalanced value for every similarity range.
Over the entire dataset, CORAL gives an average balanced score of 0.80 and 0.77 for the log odds and dot product functions, respectively, compared with 0.74 for HHalign and 0.75 for COMPASS.
The shorter alignments correlate with higher alignment accuracy (Smod), Fig.2.
Alignment accuracy in terms of the (A) Sdev; (B) Smod; and (C) Scov metrics based on curator-optimized (guide) reference alignments.
These metrics indicate completeness in reconstructing the reference alignment, accuracy over the computed alignment and the localglobal trade-off in the resulting alignment, respectively.
but are less informative as they exclude more homologous regions.
CORAL and COMPASS parameters may be set to permit near-global alignments using a local alignment algorithm.
PSI-BLAST performance deteriorated rapidly as sequence similarity decreases.
Almost half of all domain pairs from the same family had no significant PSI-BLAST alignment.
Aligning the consensus sequences by pairwise BLAST led to a similar outcome, showing that these families are not as easy to align despite the high-reported sequence identities.
The default significance cut-off for PSI-BLAST is restrictive and many domain pairs may not satisfy the cut-off due to low-sequence similarity or short profile lengths.
Domain pairs with no PSI-BLAST results were assigned value 0 for all metrics (following the regular definitions of Sdev and Scov, and replacing the otherwise undefined Smod term), leading to a large number of families with low Sbalanced score.
1865 [18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1866 18621868 J.H.Fong et al.Fig.3.
Recognizing homologs: ROC curve plotting percentage of TP identified before the n-th FP.
3.2 Homology recognition Next, we evaluated the accuracy of CORAL and its E-values at detecting related domains.
Although we do not propose to identify homologous protein families from core regions alone, given the evolutionary signal present in the more variable loop regions, a scoring system helps to distinguish more similar and better-aligned core regions.
Related and unrelated domains are defined with respect to CDD families/superfamilies, as described in Section 2.
A test set of 100 domains was taken from different superfamilies.
Each domain is aligned with all domains within the same family (with a minimum of two related domains) and with 100 randomly selected unrelated domains, using the alignment methods described in the previous section.
PSI-BLAST was omitted to avoid handling missing data.
The distribution of sequence identity between related domains in this test set is similar to the distribution over the entire CDD (Supplementary Fig.S2).
Figure 3 shows performance measured as the fraction of true relationships (true positive, TP) that score higher than the i-th highest scoring false relationship (FP), averaged over the test set.
Scores refer to the E-values for CORAL and COMPASS and probabilities for HHalign, which performed much better than its E-values.
To assess sensitivity, we measure the area under curve (AUC), ROCn =1/ni=1...n ti, for each sample domain where ti is the fraction of TPs before the i-th FP.
Standard error over ROCn values is computed as SE=/n.
ROC curves and AUC values reveal that the CORAL and HHalign methods detect homologs from core regions at similar rates, and better than COMPASS.
Average ROC100 and SE ranges overlapped for all CORAL and HHalign methods and were: 0.9620.009 for CORAL LO, 0.963 0.008 for CORAL DP, 0.966 0.008 for HHalign local and 0.957 0.011 for HHalign global.
There was a statistically significant difference between the distribution of ROC100 values for HHalign local, the highest curve in Figure 3, from the closest methods HHalign global and CORAL according to the Wilcoxon signed-rank test (P-values 0.010.02), but not between the CORAL methods and HHalign global (all pair-wise P-values > 0.05).
3.3 Alignment in protein family modeling The problem of aligning conserved core regions was conceived by the need to automate domain curation and develop tools for analyzing individual families.
Many domain families contain diverse members that are difficult to align.
Sequence similarity, for example via characteristic motifs, can make it clear that sequence fragments are related by common descent.
More powerful tools are needed to obtain an accurate alignment across the full domain model and to determine domain boundaries.
In defining diverse domain families, two important and interrelated tasks for each domain are step 1: to build a MSA and step 2: to split off sub-families when applicable for increasing functional specificity, starting with a less-diverse sub-set of sequences from the current domain.
These tasks are common to many approaches to subfamily identification (see e.g.Brown et al., 2007), although, here we describe steps in the CDD curation pipeline.
Typically, the higher degree of conservation in child models allows curators to extend blocks and/or define additional blocks beyond the base core structure of their parent.
Aligning the child and parent domains requires the selection of a representative sequence to provide the guide alignment between a new sub-family and its parent domain.
A badly aligned representative compromises the overall alignment of the child with respect to the parent, which may amplify noise present in the parent and misrepresent evolutionary distance and diversity within the superfamily.
Cleaning up the child model by itself further propagates overall error, which may be difficult to detect.
By iterating steps 1 and 2, the child alignment is refined, its core structure may be extended or revised, and realigning the child and parent may help to refine the core structure of the parent as well.
When subfamilies are covered by 3D structure, structure superposition helps to provide high-quality guide alignments.
Profile alignments augment this information and may substitute for superpositions when structures are not known.
The structure alignment may differ markedly from the guide alignment, as in the alignment of the eukaryotic translation factor 5A domain and the Hex1/S1-like RNA-binding domain (Fig.4).
In this case, CORAL validates the structural alignment and extends the aligned region.
A third major step in CDD curation is annotating domain models with function and functional sites following the literature and analysis of 3D structures.
The alignment of related protein families helps to confirm the locations of functional sites, which may be placed at nearby positions in parent and child domains as shown in Figure 4 for RNA-binding sites.
4 DISCUSSION Here, we showed that profileprofile alignment with well-structured alignment constraints can achieve high-alignment accuracy and work well in detecting homologous relationships between conserved core regions of domain families.
The core constraint exploits relationships between profile columns, prohibiting insertions or deletions within blocks, rather than pursuing improvements through refinement of the column scoring function.
Our proposed method is a simple interpretation of a framework in which gap penalties vary according to local conservation, requiring only two different gap penalties.
The core constraint may be incorporated into other alignment algorithms as well.
We benchmarked CORAL on core regions from NCBI-curated domains in CDD.
Blocks in curated domains reflect sequence and structural conservation and approximate the structural core of the family.
However, curators may define blocks to be longer or shorter than in structure alignments, and merge, split or delete the blocks suggested by structure alignments.
They may also introduce 1866 [18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1867 18621868 CORAL Fig.4.
VAST structure, guide and CORAL alignments between the eukaryotic translation factor 5A domain (cd04468; eIF5A) and the Hex1/S1like RNA-binding domain (cd04469; S1_Hex1) are illustrated using sequence fragments from 1X6O and 1KHI and structure superpositions.
The domains share a parent (EF-and S1-like RNA-binding domain) and 28% identity.
Aligned positions in the reference alignments are underlined.
The structure alignment is believed to be the most accurate.
Misaligned regions in the guide and CORAL alignments are colored blue.
RNA-binding sites are highlighted in yellow on both sequence and structure alignments.
The structure superposition is colored red for identical residues, purple for other aligned residues and grey for unaligned residues.
additional blocks to record conserved features and sites outside the structural core, such as binding sites and motifs.
CORAL E-values identify 70% of all domain pairs from the same hierarchy with E-value < 0.05 compared with 3.0% of domain pairs from different superfamilies.
Ranking scores from the same family, as in the homology recognition test, achieves even higher performance.
In general, the CDD superfamily classification used to define homologs is comparable in specificity to SCOP superfamilies, the basis for remote homology in previous benchmark studies (Marchler-Bauer et al., 2009).
Nevertheless, that curated domains in CDD are easier to classify is unsurprising, because many previous studies aligned noisier profiles constructed by PSI-BLAST and the hierarchical organization of CDD families suggests that many domains have similar conserved cores.
Constructing high-quality alignments between well-defined core regions, in contrast, benefits tremendously from the core constraint.
CORAL aligns more families with high-balanced score, produces better alignments with respect to the balanced score than COMPASS or HHalign across all similarity ranges, and returns higher developers score for almost all groups of data.
Possibly even more importantly, by respecting block boundaries, it produces alignments that may be easier to revise.
Automated alignments of sequences or profiles with low similarity often require manual correction to produce optimal results.
Reducing error to a small number of block shifts simplifies manual analysis.
Although the core constraint reduces the space of possible alignment solutions, it does not necessarily constrain the alignment to only one good solution.
Our results demonstrate that weak sequence similarity between corresponding core regions increases errors in all methods.
Additionally, even in the more constrained setting of global alignment, differences in profile and block lengths permit more than one possible alignment between many blocks.
The clear shortcoming of the core constraint is that at some level of divergence, core regions cannot be aligned correctly without insertions or deletions, hence methods without the core constraint are more suited to remote homolog recognition and alignment.
One solution to ameliorate shift errors is to split long blocks into shorter units, randomly or by inspecting the block structure or preliminary alignments of core regions.
The curated domain models already contain breaks within blocks where the sequences naturally split.
In unreported experiments, we have aligned the curated domains using this alternative block definition with similar and slightly worse overall performance.
Further development of this algorithm will allow for cases where additional blocks have been inserted into a sub-family model relative to its parent.
CORAL will be made available to the public as an alignment tool bundled into a future release of the NCBI Cn3D/CDTree software.
This user-friendly implementation will provide fast and accurate alignment of core regions, along with access to protein family alignments from CDD.
While we only tested alignments between pre-computed protein family models, core regions may be inferred from the continuous regions of any protein family alignment.
However, the effective use of CORAL requires high overlap between the conserved regions of two families, for example, in the case of a common structural core, and additional processing may be needed to identify putative conserved core regions.
The core constraint may also be incorporated into profile alignment algorithms with more sophisticated scoring methods to improve on both CORAL and the original method for aligning conserved cores.
ACKNOWLEDGEMENTS We thank Anna Panchenko and John Spouge for helpful discussions.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine.
Conflict of Interest: none declared.
ABSTRACT Motivation: Large-scale methods for inferring gene trees are errorprone.
Correcting gene trees for weakly supported features often results in non-binary trees, i.e.trees with polytomies, thus raising the natural question of refining such polytomies into binary trees.
A feature pointing toward potential errors in gene trees are duplications that are not supported by the presence of multiple gene copies.
Results: We introduce the problem of refining polytomies in a gene tree while minimizing the number of created non-apparent duplications in the resulting tree.
We show that this problem can be described as a graph-theoretical optimization problem.
We provide a bounded heuristic with guaranteed optimality for well-characterized instances.
We apply our algorithm to a set of ray-finned fish gene trees from the Ensembl database to illustrate its ability to correct dubious duplications.
Availability and implementation: The C++ source code for the algorithms and simulations described in the article are available at Contact: lafonman@iro.umontreal.ca or mabrouk@iro.umontreal.ca Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION With the increasing number of completely sequenced genomes, the task of identifying gene counterparts in different organisms becomes more and more important.
This is usually done by clustering genes sharing significant sequence similarity, constructing gene trees and then inferring macro-evolutionary events such as duplications, losses or transfers through reconciliation with the phylogenetic tree of the considered taxa.
The inference of accurate gene trees is an important step in this pipeline.
While gene trees are traditionally constructed solely from sequence alignments (Guidon and Gascuel, 2003; Ronquist and Huelsenbeck, 2003; Saitou and Nei, 1987), recent methods incorporate information from species phylogenies, gene order and other genomic footprint (Akerborg et al., 2009;Boussau et al., 2013; Durand et al., 2003; Rasmussen and Kellis, 2011; Szollosi et al., 2013; Thomas, 2010; Wapinski et al., 2007).
A large number of gene tree databases are now available (Datta et al., 2009;Flicek, 2012; Huerta-Cepas et al., 2011; Mi et al., 2012; Schreiber et al., 2013).
But constructing accurate gene trees is still challenging; for example, a significant number of nodes in the Ensembl gene trees are labelled as dubious (Flicek, 2012).
In a recent study, we have been able to show that 30% of 6241 Ensembl gene trees for the genomes of the fishes Stickleback, Medaka, Tetraodon and Zebrafish exhibit at least one gene order inconsistency and thus are likely to be erroneous (Lafond et al., 2013).
Moreover, owing to various reasons such as insufficient differentiation between gene sequences and alignment ambiguities, it is often difficult to support a single gene tree topology with high confidence.
Several support measures, such as bootstrap values or Bayesian posterior probabilities, have been proposed to detect weakly supported edges.
Recently, intense efforts have been put towards developing tools for gene tree correction (BerglundSonnhammer et al., 2006; Chaudhary et al., 2011; Chen et al., 2000; Doroftei and El-Mabrouk, 2011; Gorecki and Eulenstein, 2011a,b; Nguyen et al., 2013; Swenson et al., 2012; Wu et al., 2012).
A natural approach is to remove a weakly supported edge and collapse its two incident vertices into one (Beiko and Hamilton, 2006), or to remove dubious nodes and join resulting subtrees under a single root (Lafond et al., 2013).
The resulting tree is non-binary with polytomies (multifurcating nodes) representing unresolved parts of the tree.
A natural question is then to select a binary refinement of each polytomy based on appropriate criteria.
This has been the purpose of a few theoretical and algorithmic studies conducted in the past years, most of them based on minimizing the mutation (i.e.duplication and loss) cost of reconciliation (Chang and Eulenstein, 2006; Lafond et al., 2012; Vernot et al., 2009; Zheng et al., 2012).
In the present article, we consider a different reconciliation criterion for refining a polytomy, which consists in minimizing the number of non-apparent duplication (NAD) nodes.
A duplication node x of a gene tree (according to the reconciliation with a given species tree) is a NAD if the genome sets of its two subtrees are disjoint.
In other words, the reason x is a duplication is not the presence of paralogs in the same genome, but rather an inconsistency with the species tree.
Such nodes have been flagged as potential errors in different studies (Chauve and El-Mabrouk, 2009; Flicek, 2012; Scornavacca et al., 2009).
In particular, they correspond to the nodes flagged as dubious in Ensembl gene trees.
We introduce the polytomy refinement problem in Section 2, and we show in Section 3 how it reduces to a clique decomposition problem in a graph representing speciation and duplication relationships between the leaves of a polytomy.
We develop a bounded heuristic in Section 4, with guaranteed optimality in well-characterized instances.
In Section 5 we exhibit a general methodology, using our polytomy refinement algorithm, for correcting NAD nodes of a gene tree.
We then show in Section 6 that this approach is in agreement with the observed corrections of Ensembl gene trees from one release to another.
*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com footprints 1 , 4 , 12 , 31 , 33, 36] [ 10 , 20, 24 , 28] `` '' [14] about , , [22] due [ 3, 8, 30, 37, 32] [2] `` '' [21] l , [ 5, 23, 34, 38] paper [ 29, 14] `` '' XPath error Undefined namespace prefix 2 THE POLYTOMY REFINEMENT PROBLEM Phylogenies and reconciliations.
A phylogeny is a rooted tree that represents the evolutionary relationships of a set of elements (such as species, genes,... ) represented by its nodes: internal nodes are ancestors, leaves are extant elements and edges represent direct descents between parents and children.
We consider two kinds of phylogenies: species trees and gene trees.
A species tree S describes the evolution of a set of related species, from a common ancestor (the root of the tree), through the mechanism of speciation.
For our purpose, species are identified with genomes, and genomes are simply sets of genes.
As for a gene tree, it describes the evolution of a set of genes, through the evolutionary mechanisms of speciation and duplication.
Therefore, each gene g, extant or ancestral, belongs to a species denoted by s(g).
The set of genes in a gene tree is called a gene family.
A leaf-label corresponds to a genome in a species tree, and to a gene belonging to a genome in a gene tree.
Given a phylogeny T, we denote by l(T) the leaf-set and by V(T) the node-set of T. Given a node x of T, we denote by l(x) and call the clade of x, the leaf-set of the subtree of T rooted at x.
We call an ancestor of x any node y on the path from the root of T to the parent of x.
In this case we write y5x.
Two nodes x, y are unrelated if none is an ancestor of the other.
For a leaf subset X of T, lcaTX, the lowest common ancestor (LCA) of X in T, denotes the farthest node from the root of T, which is an ancestor of all the elements of X.
In this article, species trees are assumed to be binary: each internal node has two children, representing its direct descendants (see S in Fig.1).
For an internal node x of a binary tree, we denote by x and xr the two children of x.
DEFINITION 1.
(Reconciliation) A reconciliation between a binary gene tree G and a species tree S consists in mapping each internal or leaf node x of G (representing respect.
an ancestral or extant gene) to the species s(x) corresponding to the LCA in of the set fsl; for all l 2 lxg.
Every internal node x of G is labelled by an event E(x) verifying: Ex= Speciation (S) if s(x) is different from sx and sxr, and Ex= Duplication otherwise.
We define two types of duplication nodes of a gene tree G. A Non-Apparent-Duplication (NAD) is a duplication node x of G such that [x2lxx  \ [y2lxry  =;.
A duplication that is not an NAD is an apparent duplication (AD) node, i.e.a node with the left and right subtrees sharing a common leaf-label.
Therefore, any internal node x of G is of type S, AD or NAD.
The gene trees we consider might be non-binary.
We call polytomy a gene tree with a non-binary root (see F in Fig.1).
DEFINITION 2.
(Binary refinement) A tree HT is a refinement of a tree T if and only if the two trees have the same leaf-set and T can be obtained from HT by contracting some edges.
When HT refines T, each node of T can be mapped to a unique node of HT so that the ancestral relationship is preserved.
HT is a binary refinement of T if and only if HT is binary and is a refinement of T. In this article, as only binary refinements are considered, we omit the term binary from now.
Problem statement.
The general problem we address is the following: Given a non-binary gene tree G and a species tree S, find a refinement of G containing theminimumnumber ofNADswith respect toS.
Such a refinement ofG is called aminimum refinement of G w.r.t.
S. Hence, we aim at refining each non-binary node of G. We first show that each such non-binary node of G can be refined independently of the other non-binary nodes.
THEOREM 1.
Let fGi; for 1 i ng be the set of subtrees of G rooted at the n children fxi; for 1 i ng of the root of G. Let HminGi;S be a minimum refinement of Gi w.r.t.
S. Let G0 be the tree obtained from G by replacing each Gi by HminGi;S. Then a minimum refinement of G is a minimum refinement of G0.
It follows from Theorem 1 that a minimum refinement of G can be obtained by a depth-first procedure iteratively solving each polytomy Gx, for each internal node x of G. In the rest of this paper, we consider G as a polytomy, and we denote by F the forest fG1;G2;...Gng obtained from G by removing the root.
For simplicity, we make no difference between a tree Gi of F and its root.
In particular, sGi corresponds to srootGi, where rootGi is the root of Gi (Fig.1).
We are now ready to define the main optimization problem we consider.
Minimum NAD polytomy refinement (MinNADref) problem: Input: A polytomy G and a species tree S; Output: In the set HG of all refinements of G, a refinement H with the minimum number of NAD nodes.
Such a refinement is called a solution to the MinNADref problem.
3 A GRAPH-THEORETICAL CHARACTERIZATION We show (Theorem 2) that theMinNADref Problem reduces to a clique decomposition problem on a graph that represents the impact, in terms ofNADcreation, of joining pairs of trees fromF.
The join graph of a polytomy.
We first define a graph R based on the notion of join.
A join is an unordered pair fG1;G2g where G1;G2 2 F. The join operation j on fG1;G2g consists in joining the roots of G1 and G2 under a common parent; we denote by (a) ((a,f),(g,h)) (g) (h,l) (b,e) (i,m) (d) ((c,d),(j,k)) (k) F : ((a , f),( g ,h))((c ,d),( j,k)) (a)(d)(g) (k) (b , e) (i , m) (h , l) R :: a b c d e f g h i j k l m (a) (d) (g) (k) (b,e) (i,m)(h,l) ((a,f),(g,h)) ((c,d),(j,k)) S : n H :: ((a , f),( g , h))((c ,d),( j , k)) (a)(d)(g) (k) (b , e) (i , m) (h  , l) Fig.1.
A forest F , a species tree S and the corresponding graph R. Each gene tree G of F is attached to its corresponding node s(G) in S. In R, joins of type AD are represented by green lines.
All other lines are the joins of type S. Non-trivial AD-components (AD-components containing at least two nodes) are represented by green ovals.
Red lines in R represent a vertex-disjoint clique W of RS.
Here, RAD [W has a single connected component, which leads to the binary refinement H of F with no NAD.
After the joins of W are applied (red edges in H), the speciationfree forest can be joined with four joins AD (green vertices in H) i520 M.Lafond et al.which , a paper which , paper G1;2 the resulting join tree.
We call the join type of j=fG1;G2g, and denote by jtG1;G2, the reconciliation label of the node created by joining G1 and G2 (i.e.the root of G1;2), where jtG1;G2 2 fS;AD;NADg, respectively, for speciation, AD and NAD, w.r.t.
the species tree S. We denote by R=V;E the join graph of F , defined as the unoriented complete graph on the set of vertices V=F , where each edge (join) is labelled by the corresponding join type (Fig.1).
We denote by RS and RAD the subgraphs of R defined by the edges of type, respectively, S and AD.
We call a connected component of RAD an AD-component.
Let F0 be the new forest obtained by replacing the two trees G1 and G2 of F by the join tree G1;2.
The rules given below, following directly from the definition of speciation and duplication in reconciliation, are used to update the join type jtG1;2;T for any T 2 FnfG1;G2g.
Ruleset 1 (1) If jtG1;T=AD or jtG2;T=AD, then jtG1;2;T=AD; (2) Otherwise, if jtG1;T=NAD or jtG2;T=NAD, then jtG1;2;T=NAD; (3) Otherwise, if lca(T) is not a descendant of lcaG1;2, then jtG1;2;T=S; (4) Otherwise, jtG1;2;T=NAD.
Clique decomposition of the join graph.
Let a join sequence J=J1; J2;... ; JjJj be an ordered list of joins.
We denote by FJ; i the forest obtained after applying the first i joins of J, starting with F. Note that FJ; 0=F , and that Ji 2 J is a join on FJ; i 1.
Let J denote the set of all possible join sequences of size jF j 1.
Clearly, applying all joins of a sequence J 2 J yields a single binary tree, and there exists a gene tree H 2 HG with d NADs if and only if there exists a join sequence J 2 J with d joins of type NAD.
We refine this property by showing that there is a solution to the MinNADref problem where all duplication nodes are ancestral to all speciation nodes (see the treeH of Fig.1 for an example).
The proof (not shown) makes abundant use of Ruleset 1.
LEMMA 1.
There exists a binary refinement H 2 HG with d NADs if and only if there exists a join sequence J 2 J with d joins of type NAD such that, if Ji 2 J is the first join not of type S in J, then all following joins Jj, for j4i, are of type AD or NAD.
We define a speciation tree as a gene tree in which every internal node is a speciation node.
We deduce from the previous lemma that we can obtain a solutionH to the MinNADref problem by creating a forest of speciation trees first, then successively joining them with joins of type AD or NAD.
As the nodes of R corresponding to the leaves of a given speciation subtree ofH are pairwise joined by speciation edges, they form a clique in RS (in Fig.1 the cliques in red are selected and the corresponding joins are applied to compute refinement H).
The next theorem makes the link between the number of NADs ofH and the cliques of RS.
For a set W of vertex-disjoint cliques of RS, we denote by RAD [W the graph defined by the union of the edges of RAD and W. THEOREM 2.
A solution to the MinNADref Problem has d NADs if and only if, among all graphs RAD [W where W is a set of vertex-disjoint cliques of RS, at least one has d+ 1 connected components and none has less than d+ 1 connected components.
The proof of Theorem 2 is constructive.
Given an optimal set W of vertex-disjoint cliques of RS, it leads to an optimal refinement H. Unfortunately, it can be shown that, given an arbitrary graph with two edge colours AD and S, finding if there exists a set W yielding a given number of connected components is an NP-hard problem (proof not shown).
However, R is constrained by the structure of a species tree, which restricts the space of possible join graphs.
An arbitrary complete graph R with edges labelled on the alphabet {S, AD, NAD} is said to be valid if there exists a species tree and a polytomy whose join graph is R. We characterize below the valid graphs in terms of forbidden induced subgraphs.
The proof is partially based on well-known results on P4-free graphs (Corneil et al.1985).
THEOREM 3.
A graph R is valid if and only if RS is fP4; 2K2g-free, meaning that no four vertices of RS induce a path of length 4, nor two vertex-disjoint edges.
Although we have not been able to find an exact polynomialtime algorithm for the MinNADref problem, this very constrained structure of the R graph yields a bounded heuristic for this problem with good theoretical properties described in the next section.
REMARK 1.
The P4-free property, which was already introduced in relation with reconciliations in (Hellmuth et al., 2013), is of special interest, as many NP-hard problems on graphs have been shown to admit polynomial time solutions when restricted to this class of graphs.
Unfortunately we can prove that, given an arbitrary P4free graph on which we add AD edges, finding an optimal W is still NP-hard (proof not shown).
However, the added 2K2-free restriction imposes a rigid structure on the graph at hand, and we conjecture that there exists a polynomial time algorithm to find an optimal W. 4 A BOUNDED HEURISTIC We first describe a general approach based on the notion of useful speciations, followed by a refinement of this approach with guaranteed optimality criteria.
DEFINITION 3.
Let J=J1;... ; JjJj be a join sequence.
A join Ji= fG1;G2g of J is a useful speciation if jtG1;G2=S and G1, G2 are in two different AD-components of the R graph obtained after applying the J1;... ; Ji1 joins.
Hence, if R has c AD-components, finding a zero NAD solution becomes the problem of finding a join sequence with c 1 useful speciations.
For example, the graph R in Figure 1 has five AD-components (three trivial and two non-trivial), and thus the four useful speciations represented by the red lines lead to a 0 NAD solution (the binary tree H).
In the general case, the problem we face is to select as many useful speciations as possible, as the resulting AD-components will have to be connected by NAD joins.
If we define a speciation-free forest as a forest F such that no edge of its join graph R is a speciation edge, following Lemma 1, we would like to first compute a set of useful speciations that i521 Polytomy refinement , apparent duplication non-apparent duplication ; [9] that [19] 5 3 2 4  results in a speciation-free forest whose join-graph has the least number of AD-components.
DEFINITION 4.
A lowest useful speciation is a useful speciation edge fG1;G2g of RS such that sG1;2 is not the ancestor of any sGi;j, for fGi;Gjg being another useful speciation edge of RS.
Lowest useful speciations fit naturally in the context of bottom-up algorithms where speciations edges that correspond to lower vertices of S are selected before speciations edges corresponding to ancestral species.
The theorem below shows that proceeding along these lines ensures that the resulting join sequence contains at least half of the optimal number of useful speciation.
THEOREM 4.
Let s be the maximum number of useful speciations leading to a solution to the MinNADref problem.
Then any algorithm that creates a speciation-free forest through lowest useful speciations makes at least ds=2e useful speciations.
This theorem implicitly defines a heuristic with approximation ratio 2 on the number of useful speciations that visits S in a bottom-up way, making useful speciations (which would thus be lowest useful speciations) whenever such an edge is available.
We now describe an improved version of this general heuristic principle.
A detailed example is given in Figure 2.
The main idea is to consider a bottom-up traversal of the species tree S, and for each visited vertex s, to find a useful set of speciation edges by finding a matching in a bipartite graph.
More precisely, for a node s 2 VS, we consider the complete bipartite graph B=X [ Y; fxyjx 2 X; y 2 Yg such that the left (respectively right) subset X (respectively Y) contains all the trees Gi of F where sGi is on the left (respectively right) subtree of s. Consider the two partitions ADX and ADY of X and Y, respectively, into AD-components.
The key step of our heuristic is to find a matching M EB of useful speciations between ADX and ADY, called a useful matching.
For example, in Figure 2, the bipartite graph and matching illustrated for Step 3 correspond to node l and that of Step 4 to node m of S. Notice that not all edges of B correspond to useful speciations.
Indeed it is possible that for some x 2 X and some y 2 Y, although {x, y} is a speciation edge, x and y are in the same AD-component of R due to another tree z not in B such that {x, z} and {z, y} are AD-edges.
For example in Figure 1, although fa; gg is a join of type S, the trees (a) and (g) are in the same AD-component ofR due to the tree a; f; g; h. For a vertex x of X (respectively y of Y), denote by AD(x) (respectively AD(y)) the component of ADX (respectively ADY) containing x (respectively y).
We indicate the fact that AD(x) and AD(y) belong to the same AD-component in R by adding two dummy genes b1 in AD(x) and b2 in AD(y), and a bridge fb1; b2g in EB.
Such bridges will be included in every matching, preventing to include non-useful speciation edges.
An instance P of the problem associated with a vertex s of S is denoted by P=X;Y;ADX;ADY;B where X, Y, ADX, ADY are defined as above and B is the set of bridges induced by R. The graph corresponding to P, i.e.the complete bipartite graph on sides X and Y to which we added the bridge edges B, is denoted by BP.
The whole method is summarized in Algorithm 1 MinNADref(F ;S) and illustrated on a simple example in Figure 2.
Algorithm 1: MinNADrefF ;S. for each node s of S in a bottom-up traversal of S do Let P=X;Y;ADX;ADY;B be the problem instance corresponding to s; Find a useful matching M of BP of maximum size (Algorithm MaxMatching below); Apply each speciation of M, and update F end for For each connected component C of RAD, join the trees of C under AD Nodes; If there is more than one tree remaining, join them under NAD nodes.
Finding a useful matching of maximum size can be done in polynomial time by Algorithm 2.
For an instance P=X;Y;ADX;ADY;B, the algorithm progressively increments the set M of speciation edges, eventually leading to a useful matching of maximum size.
At a given step, let GP;M be the graph with vertices X [ Y and edges EP;M=EAD [M, where EAD is the set of AD edges of R connecting vertices of X [ Y.
Components ADXi 2 ADX and ADYj 2 ADY are linked if there is a path in GP;M linking a vertex of ADXi to a vertex of ADYj , and not linked otherwise.
Algorithm 2:MaxMatchingX;Y;XX;ADY;B. D=;; M=B; while D 6 X [ Y do Find C 2 ADX [ ADY of maximum cardinality with vertices not included in D, if any; assume w.l.o.g.
C=ADXi 2 ADX; Fig.2.
A species tree S and a forest F of binary trees forming the polytomy.
The trees of F are placed on S according to their LCA.
The i, k, l and m nodes of S are annotated with the forest obtained after running Algorithm 2 on these nodes.
Their corresponding complete bipartite matching instances are illustrated at the bottom.
AD joins are represented by dotted lines, useful matching are represented by plain lines (we omit drawing all the other edges of the complete bipartite graphs).
Note that there is a bridge induced by M between (F, K) and I at step 4.
In the fourth step, we obtain a single connected component, which allows, in a final step, to connect all the subtrees by AD nodes (final tree is on the top of the figure) i522 M.Lafond et al.prior to , that... due due.
(.
).. , for each x 2 C that is not incident to an edge in M do if there is an y 2 Y such that AD(y) is not linked to C then Find such y with AD(y) of maximum cardinality; Addtheverticesxandy toDandaddthe speciationedge{x,y} toM; end if end for Add remaining vertices of C to D; end while THEOREM 5.
Given an instance P=X;Y;ADX;ADY;B, Algorithm 1 finds a useful matching M of maximum size.
Algorithm 1 is a heuristic, as it may fail to give the optimal solution (refinement with minimum number of NADs), as in Figure 1 for example.
In this example, a bottom-up approach would greedily speciate a and d, which cannot lead to the optimal solution.
However, we prove in Theorem 6 that if transitivity holds for the duplication join type, then Algorithm 1 is an exact algorithm for the MinNADref problem.
The example of Figure 1 does not satisfy this property, as fa; a; f; g; hg is a join of duplication type (AD), fa; f; g; h; gg is a join of duplication type but fa; gg is a join of speciation type.
THEOREM 6.
(1) Let s be the maximum number of useful speciations leading to a solution to the MinNADref problem.
Then, Algorithm 1 makes at least ds=2e useful speciations.
(2) If, for every node s of S the instance P corresponding to s has no bridges, then Algorithm 1 outputs a refinement of the input polytomy with the maximum number of useful speciations.
The following corollary provides an alternative formulation of the optimality result given by the above theorem.
COROLLARY.
Algorithm 1 exactly solves the MinNADref problem for an input F ;S such that each AD-component of the corresponding graph R is free from S edges (i.e.there is no S edge between any two vertices of a given AD-component).
5 GENE TREE CORRECTION The polytomy refinement problem is motivated by the problem of correcting gene trees.
Duplication nodes can be untrusted for many reasons, one of them being the fact that they are NADs, pointing to disagreements with the species tree that are not due to the presence of duplicated genes.
Different observations tend to support the hypothesis that NAD nodes may point at erroneous parts of a gene tree (Chauve and El-Mabrouk, 2009; Swenson et al., 2012).
For example, the Ensembl Compara gene trees (Vilella et al., 2009) have all their NAD nodes labelled as dubious.
In (Chauve and El-Mabrouk, 2009), using simulated datasets based on the species tree of 12 Drosophila species given in (Hahn et al., 2007) and a birth-and-death process, starting from a single ancestral gene, and with different gene gain/loss rates, it has been found that 95% of gene duplications lead to an AD vertex.
Although suspected to be erroneous, some NAD nodes may still be correct, due to a high number of losses.
However, in the context of reconciliation, the additional damage caused by an erroneous NAD node is the fact that it significantly increases the real rearrangement cost of the tree (Swenson et al., 2012).
Therefore, tools for modifying gene trees according to NADs are required.
We show now how Algorithm 1 can be used in this context.
In (Lafond et al., 2013), a method for correcting untrusted duplication nodes has been developed.
The correction of a duplication node x relies on pushing x by multifurcation, which transforms x into a speciation node with two children being the roots of two polytomies.
Figure 3 recalls the pushing by multifurcation procedure.
These polytomies are then refined by using an algorithm developed in (Lafond et al., 2012), which optimizes the mutation cost of reconciliation.
In the context of correcting NADs, we use the same general methodology, but now using AlgorithmMinNADref for refining polytomies.
Removing all NADs of a gene tree can then be done by iteratively applying the above methodology on the highest NAD node of the tree (the closest to the root).
6 RESULTS Simulated data.
Simulations are performed as follows.
For a given integer n, we generate a species tree S with a random number of leaves between 0:5n and 3n.
We then generate a forest F=G1;... ;Gn of cherries by randomly picking, for each cherry Gi 2 F , one node si 2 S and two leaves, one from each of the two subtrees rooted at si.
Any leaf of S is used at most once (possibly by adding leafs to S if required), leading to a set of cherries related through joins of type S or NAD.
Then, for each pair fGi;Gjg with join type NAD, we relate them through AD with probability 1/2 (or do nothing with probability 1/2), by adding a duplicated leaf.
For each pair S;F, we compared the number of NADs found by Algorithm MinNADref with the minimum number of NADs returned by an exact algorithm exploring all possible binary trees that can be constructed from F. We generated a thousand random S and F for each n 4.
We stopped at n=14, as the brute-force algorithm is too time-costly beyond this point.
Over all the explored datasets simulated as described above, Algorithm MinNADref was able to output an optimal solution, i.e.a refinement with the minimum number of NADs.
Therefore, the examples on which the heuristic fails seem to be rare, and the algorithm performs well on polytomies of reasonable size.
We then wanted to assess how the NAD minimization criterion differs from the rearrangement cost minimization criterion.
We generated 960 random instances with forests of sizes ranging between 5 and 100 (10 instances for each 5 n 100).
We a b c d S :G : a1 c3c2 b1 d2d1 x c1 b2 a1 b1 b2 c1 c3c2 d2d1 G* :s sl sr Fig.3.
A gene tree G and a species tree S, from which we obtain G by pushing x by multifurcation.
Here, x is a NAD, and is pushed by taking the forest of maximal subtrees of G that only have genes from species in the sl subtree (green), then another forest for the sr subtree (red) in the same manner.
Both these forests are joined under a polytomy, which are then joined under a common parent, so the root of G is a speciation i523 Polytomy refinement , [ 7, 30 ] [35] `` '' [7] [18] due [30] [21] [23] , compared the output of Algorithm MinNADref with that of Algorithm MinDLref, given in (Lafond et al., 2012), which computes refinement minimizing the duplication+loss (DL) cost of reconciliation with the species tree.
Both algorithms gave the exact same refinement for only 12 instances (1.25%).
As expected, Algorithm MinNADref always yielded a refined tree with a lower or equal number of NADs than the tree given by AlgorithmMinDLref, but always had a higher or equal DL-cost.
However, in many cases, minimizing the DL-score did not minimize the number of NADs, as in 377 instances (39.3%), Algorithm MinNADref yielded strictly less NADs than Algorithm MinDLref.
Ensembl Gene Trees.
Next we tested the relevance of the proposed gene tree correction methodology, by exploring how Ensembl gene trees are corrected from one release to another.
As the Ensembl general protocol for reconstructing gene trees does not change between releases, the observed modifications on gene trees are more likely due to modifications on gene sequences.
We used the Ensembl Genome Browser to collect all available gene trees containing genes from the monophyletic group of rayfinned fishes (Actinopterygii), and filtered each tree to preserve only genes from the taxa of interest (ray-finned fish genomes).
We selected from both Releases 74 (the present one) and 70 the 1096 gene trees that are present in both with exactly the same set of genes from the monophyletic group of fishes, and with less NAD nodes in Release 74.
We wanted to see to what extent our general principle of correcting an NAD by transforming it to a speciation node is observed by comparing Rel.70 to Rel.74.
Such a transformation requires to preserve the clade of the corrected NAD node x of the initial tree, meaning that l(x) should also be the leaf-set of a subtree in the corrected tree.
For490% of these trees (993 trees), the highest NAD node clade was preserved in Rel.74.
Moreover, among all such nodes that were corrected, i.e.were not NAD nodes in Rel.74 (641 trees), almost all were transformed into speciation nodes (630 trees), which strongly supports our correction paradigm.
To evaluate our methodology for correcting NADs, we applied it to the highest NAD node of each of the 1096 aforementioned trees of Rel.70.
Figure 4 illustrates a comparison between the corrected trees (Rel.70C, C standing for Corrected) obtained by our methodology and those of Rel.
74.
Pairwise comparisons are based on the normalized RobinsonFoulds (RF) distance (number of identical clades divided by the total number of clades).
The yellow curve shows a good correlation betweenRel.70C andRel.74, with65% exhibiting480% similar clades between Rel.70C andRel.74.
If we reduce the set of trees to those for which the highest NAD node is also transformed to a speciation node in Rel.74 (630 trees), the correlation is even better (blue curve of Fig.4), with 44% of trees being identical (277 over 630 trees) and 80% exhibiting 480% similar clades between Rel.70C and Rel.74.
Now, to specifically evaluate Algorithm MinNADref, we further restricted the set of trees to those giving rise to a non-trivial polytomy (i.e.polytomy of degree42) after the pushing by multifurcation, which leads to a set of 117 trees.
Overall, the results for these trees (red curve in Fig.4) are close to those observed for all trees (yellow curve) detailed above.
We then wanted to evaluate our correction of the 117 aforementioned trees compared with trees in Rel.74.
Figure 5 provides an evaluation of the corrected trees (yellow curve) compared with those in Rel.
74 (blue curve) based on the normalized RF distance with the initial trees in Rel.70.
Overall, the initial tree is closer to our correction than to the one of Rel.74.
Therefore, even though gene trees of Rel.74 are likely to have stronger statistical support with respect to the gene sequences provided in Rel.74, our correction removes NADs while respecting as much as possible the given tree topology.
Finally, we considered the reconciliation mutation cost as another evaluation criterion.
Among the 117 trees of Rel.70C, 30 are identical to the corresponding trees in Rel.
74, and 60% have a lower mutation cost, which tend to support our correction compared with the tree in Rel.74.
As for the 40% remaining trees, half of them have more NADs than the corresponding tree in Rel.74, which suggests that applying our correction to all NAD, instead of just the highest one, would help to obtain better results.
Fig.4.
Normalized RF-distance between corrected gene trees (by modification of the highest NAD) from Rel.
70 and corresponding gene trees in Rel.
74.
Blue curve: transformation of the highest NAD into a speciation.
Red curve: trees with a non-trivial polytomy after pushing by multifurcation.
Yellow curve: all trees & ' Fig.5.
Normalized RF-distance between corrected trees (yellow curve) and Rel.
74 trees (blue curve) and original Rel.
70 trees i524 M.Lafond et al.[ 23] more than , In order `` '' about more than about more than in order , very to to st to  Finally, we evaluated the effect of NAD correction on the tree likelihood.
For this purpose, we selected the 1891 Ensembl Rel.74 gene trees of the considered monophyletic group containing at least one NAD, and we corrected each NAD individually.
The sequences were aligned using ClustalW (Larkin et al., 2007) and the likelihood values were computed with PhyML (Guidon et al., 2003).
For a tree T and a NAD node x, denote by Tx the tree obtained after correcting x.
For each T and each x, we computed the log-likelihood ratio Lx=logLHT=logLHTx.
Among the 4454 NAD nodes found in the considered set of trees, 95.4% of the L(x) ratios were between 0.98 and 1.02.
Although the correction algorithm is not expected to outperform the Ensembl protocol in terms of likelihood as it ignores sequences, we found that the likelihood of the tree has been improved (L(x)41) after correction for 43.9% of the NAD nodes.
Moreover, 1180 (62.4%) trees contained at least one NAD node improving the likelihood.
7 CONCLUSION The present work is dedicated to the polytomy refinement problem.
While the mutation cost of reconciliation has been used previously as an optimization criterion for choosing an appropriate binary tree, here we use an alternative criterion, which is the minimization of NADs.
The tractability of the MinNADref Problem remains open, as is the problem to select, among all possible solutions, those leading to a minimum reconciliation cost.
Although developing a gene tree correction tool is not the purpose of this article, we show how our algorithm for polytomy refinement can be used in this context, by developing a simple algorithm allowing to correct a single NAD.
This algorithm has been applied to trees of a previous Ensembl release, and the corrected trees have been compared with the trees of the current Ensembl release.
A good correlation between the two sets of trees is observed, which tends to support our correction paradigm.
While minimizing NADs cannot be a sufficient criterion for gene tree correction, it should rather be seen as one among others, such as statistical (Wu et al., 2012), syntenic (Lafond et al., 2013) or based on reconciliation with the species tree (Chaudhary et al., 2011; Lafond et al., 2013; Swenson et al., 2010), that can be integrated in a methodological framework for gene tree correction.
Funding: N.E.-M. and M.L.
are supported by Fonds de recherche du QuebecNature et technologies (FRQNT).
C.C.
and N.E.-M. are supported by the Natural Sciences and Engineering Research Council of Canada (NSERC).
R.D.
is supported by the MIUR PRIN 20102011 grant Automi e Linguaggi Formali: Aspetti Matematici e Applicativi, code H41J12000190001.
Conflict of interest: none declared.
Abstract In the past decades, advances in high-throughput technologies have led to the generation of huge amounts of biological data that require analysis and interpretation.
Recently, nonnegative matrix factorization (NMF) has been introduced as an efficient way to reduce the complexity of data as well as to interpret them, and has been applied to various fields of biological research.
In this paper, we present CloudNMF, a distributed open-source implementation of NMF on a MapReduce framework.
Experimental evaluation demonstrated that CloudNMF is scalable and can be used to deal with huge amounts of data, which may enable various kinds of a high-throughput biological data analysis in the cloud.
CloudNMF is freely accessible at http://admis.fudan.
edu.cn/projects/CloudNMF.html.
Introduction The explosion of biological data brought about by the highthroughput technologies poses a great challenge to bioinformatics research.
In order to learn the hidden structures of these high-dimensional data, nonnegative matrix factorization (NMF) [1] was introduced into biological research.
NMF u S).
eijing Institute of Genomics, tics Society of China.
g by Elsevier jing Institute of Genomics, Chinese A was quickly applied to various fields of biological data analysis, such as capturing expression pattern in microarray data [2], discovery of cancer subtypes [3], clustering of gene expression data [4,5], identification of histone modification modules [6], biological text mining [7,8], etc.
The intrinsic nature of the NMF method makes it very suitable for an integrative analysis of multi-dimensional genomics data [9].
Devarajan presented a comprehensive review of the application of NMF to computational biology [10].
With the increasing dimensionality of biological data, it is foreseeable that the application of NMF to biological research will continue to grow.
For example, sequencing technologies are generating terabytes (TBs) or even petabytes (PBs) of data for a multi-dimensional analysis.
However, current implementations of NMF in the biology area can only deal with matrices of thousands-by-thousands size.
For example, bioNMF [11], cademy of Sciences and Genetics Society of China.
Production and hosting Table 1 Algorithm for CloudNMF Input: nonnegative matrix A, dimension k, iteration number i Output: nonnegative matrices W and H 1: initiate W and H using random nonnegative values 2: for each iteration: 3: calculate X1 =W TA using two MapReduce steps 4: calculate Y1 =W TWH using two MapReduce steps 5: update H with H =H.\X1/Y1 using one MapReduce step 6: calculate X2 = AH T using two MapReduce steps 7: calculate Y2 =WHH T using two MapReduce steps 8: update W with W=W.\X2/Y2 using one MapReduce step 9: output W and H Liao R et al/ CloudNMF: NMF for Large-scale Biological Data 49 an implementation of NMF for bioinformatics analysis, can only handle matrices of 4096 512 (according to the documentation of bioNMF server), and thus would fail to process data with more attributes or samples.
Another implementation using R [12] fails to work when data size reaches gigabytes (GBs) in a standalone machine.
In their original papers, both implementations were used to analyze a microarray dataset represented by a 5000 38 gene expression data matrix [13].
However, a much more scalable implementation will be needed to deal with data of a significantly greater size such as protein protein interaction (PPI) data or sequencing data.
To facilitate biological data analysis in the Big Data era [14], we present CloudNMF, an open-source implementation of NMF in MapReduce framework.
The implementation was developed on the Hadoop platform and can enable the nonnegative factorization of sparse matrices up to millionby-million size.
Furthermore, CloudNMF is provided as a JAR file ready to be deployed anywhere.
In particular, CloudNMF can be easily deployed on Amazon Elastic MapReduce to utilize the power of cloud computing for biological data analysis.
Methods NMF was first introduced by Lee and Seung as a method for learning the substructure of data matrix [1].
It was defined as the factorization of a nonnegative matrix A into the multiplication of two other nonnegative matrices W and H, where A is a m nmatrix,W andH are m k and k nmatrices, where Figure 1 Using CloudNMF w k is the target dimensionality to be reduced to, which is a number smaller than the minimum of m and n. NMF was aimed at minimizing the Euclidian distance between A and WH, and can be used as an effective technique for dimension reduction and unsupervised clustering.
In 2010, Liu et al.proposed an algorithm to perform NMF in the MapReduce framework [15] and showed that the algorithm can be used to factorize huge nonnegative matrices up to millions-by-millions size.
However, this algorithm was aimed at Web applications, and no source code of the algorithm is available for public use.
Our work is the first open-source implementation of NMF in the MapReduce framework, targeted at dealing with the explosion of biological data.
Our work follows the method previously reported [15], which is based on the well-known iterative updating rule of W and H described by Lee and Seung [16].
H H : W TA WTWH 1 W W : AH T WHHT 2 Here,.\ denotes dot product and T denotes transpose of matrix.
Similar to the method used in [15], for each iteration, the updating of H and W are both factorized into five MapReduce steps; the computation of each step can be easily distributed into multiple machines to achieve speedup, please see Table 1 for the details of the algorithm.
The program was implemented using Java and was packaged as a JAR file which can run on local Hadoop clusters (Figure 1).
We offered a command-line interface for the program; the usage of the command-line interface is also provided in our website (http://admis.fudan.edu.cn/projects/CloudNMF.html).
Moreover, Amazon Elastic MapReduce service (http://aws.amazon.com/cn/elasticmapreduce/) offers on-demand computing clusters preinstalled with Hadoop, and provides a web interface to run Hadoop JAR files using only a web browser (see http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CLI_JobFlowUsingCustomJAR.html).
For those inexperienced users who find it hard to build their own Hadoop clusters, it is possible to upload their data and CloudNMF into the cloud and perform their analysis remotely (Figure 2).
ith a local Hadoop cluster 50 Genomics Proteomics Bioinformatics 12 (2014) 4851 Experimental evaluation In order to test the performance of our program, we applied the program to both real data and simulated matrices.
The PPI data matrix from the STRING database [17] was used for performance testing, which includes 108,133,799 protein interactions from 1134 species.
The dataset can be represented by a 1,349,909 1,349,909 matrix, where 1,349,909 is the number of distinct proteins in the dataset.
Since the interactions between proteins are both nonnegative and sparse, the dataset is quite suitable for the application of NMF.
Based on the STRING dataset, three submatrices of different sizes were generated.
The four datasets are described in Table S1 and the performance of CloudNMF for these four datasets is summarized in Table S2.
We also generated three simulated matrices of different sizes but containing the same number of nonzero elements to test the impact of matrix size on the performance of the program (Table S3).
The experiments were performed on an 8-machine Hadoop cluster, and each machine has a Duo Core CPU and 4 GB memory.
From Figure 3 we observed a very interesting feature of CloudNMF: the runtime actually increases in proportion to the number of nonzero elements (the number of PPIs) in the matrix (Figure 3A).
This may be attributed to the MapReduce implementation of the algorithm: only nonzero elements are stored and distributed for computation.
As the size of the maFigure 2 Using CloudNMF w Figure 3 Performance of CloudNMF A.
Performance of CloudNMF on four real datasets shows the linea elements in the matrix.
B.
Performance of CloudNMF on simulated elements shows that the runtime per iteration is linear to the logarithm trix grows, the computation time increases logarithmically (Figure 3B).
These features make the algorithm better to deal with sparse nonnegative matrices in comparison with the traditional implementations.
Discussion CloudNMF is the first open-source implementation of MapReduce-based nonnegative matrix factorization, and is capable of handling significantly a greater size of data than existing NMF implementations in bioinformatics.
Besides being deployed in local Hadoop clusters, CloudNMF can also be easily used on cloud computing platforms such as Amazon Web Services via only a web browser.
Moreover, experimental results show that the algorithm can effectively deal with sparse matrices such as proteinprotein interaction networks.
CloudNMF also has some limitations.
Although the program achieved considerable performance when dealing with large-size matrices, with the high overhead of MapReduce paradigm, it may be less efficient than existing implementations to deal with small-size matrices.
In addition, while bioinformatics analyses using NMF may involve many pre-processing or post-processing steps, we only implemented the basic NMF algorithm.
However, the code of CloudNMF is freely accessible at our website; users can integrate the code into their own pipelines to perform more specific analyses.
ith Amazon Web Services r correlation of runtime per iteration with a number of nonzero matrices of different sizes but with the same number of nonzero of matrix size.
Note that the X-axis is on a logarithmic scale.
Liao R et al/ CloudNMF: NMF for Large-scale Biological Data 51 To sum up, CloudNMF is the first open-source implementation of a MapReduce-based NMF algorithm and can be easily used to process large amounts of data.
With the explosion of biological data and the wide application of NMF to biological research, we expect that CloudNMF will play more important roles in bioinformatics in the upcoming Big Data era.
Authors contributions Ruiqi Liao drafted the manuscript and developed the software.
Yifan Zhang participated in the software development.
Shuigeng Zhou proposed the idea of the software and revised the manuscript.
Jihong Guan revised the manuscript.
All authors have read and approved the final manuscript.
Competing interests The authors have no competing interests to declare.
Acknowledgements This work is financially supported by National High Technology Research and Development Program of China (863 Program; Grant No.
2012AA020403) and National Natural Science Foundation of China (Grant Nos.
61173118 and 61272380).
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.
2013.06.001.
ABSTRACT Motivation: The comprehensive information of small molecules and their biological activities in PubChem brings great opportunities for academic researchers.
However, mining high-throughput screening (HTS) assay data remains a great challenge given the very large data volume and the highly imbalanced nature with only small number of active compounds compared to inactive compounds.
Therefore, there is currently a need for better strategies to work with HTS assay data.
Moreover, as luciferase-based HTS technology is frequently exploited in the assays deposited in PubChem, constructing a computational model to distinguish and filter out potential interference compounds for these assays is another motivation.
Results: We used the granular support vector machines (SVMs) repetitive under sampling method (GSVM-RU) to construct an SVM from luciferase inhibition bioassay data that the imbalance ratio of active/inactive is high (1/377).
The best model recognized the active and inactive compounds at the accuracies of 86.60% and 88.89 with a total accuracy of 87.74%, by cross-validation test and blind test.
These results demonstrate the robustness of the model in handling the intrinsic imbalance problem in HTS data and it can be used as a virtual screening tool to identify potential interference compounds in luciferase-based HTS experiments.
Additionally, this method has also proved computationally efficient by greatly reducing the computational cost and can be easily adopted in the analysis of HTS data for other biological systems.
Availability: Data are publicly available in PubChem with AIDs of 773, 1006 and 1379.
Contact: ywang@ncbi.nlm.nih.gov; bryant@ncbi.nlm.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION PubChem is a public repository for small molecules and their biological properties.
It was created as a component of the Molecular Libraries Roadmap (Zerhouni, 2003) initiated by the National Institutes of Health (NIH), which aims to discover chemical probes through high-throughput screening (HTS) of small molecules to support chemical biology research (Zerhouni, 2003, 2006).
Currently, PubChem contains nearly 40 million unique chemical structure and >50 million biological test results for >600 protein To whom correspondence should be addressed.
targets.
Analyzing the tremendous amount of biological activity data in PubChem (Wang et al., 2009) (http://pubchem.ncbi.nlm.nih.gov) remains a great challenge for chemical biology and cheminformatics researchers.
Until recently, HTS data was largely owned by pharmaceutical industries with only limited access to the public research community.
The comprehensive information of small molecules and their biological activities in PubChem brings great opportunities for academic researchers in the chemical biology, medicinal chemistry and cheminformatics fields (Oprea et al., 2007; Ovaa and van Leeuwen, 2008; Rosania et al., 2007; Southan et al., 2007) to access and utilize large scale biological activity data to meet their research goals.
Several groups have attempted to develop methods to analyze the data in this database (Guha and Schurer, 2008; Han et al., 2008; Hur and Wild, 2008; Nakai et al., 2009; Xie and Chen, 2008).
Xie and Chen (2008) presented a strategy to select a diverse compounds subset from the PubChem database.
Cao et al.(2008) developed a novel maximum common substructure-based algorithm to predict drug-like compounds from PubChem bioassay data.
Rohrer and Baumann (2009) used a refined nearest neighbor analysis method to design benchmark datasets for virtual screening based on information in the PubChem database.
However, working with the bioassay data in PubChem was impeded by the imbalanced nature of HTS data.
The data imbalance problem exists in a broad range of experimental data, but only recently has it attracted close attention from researchers (Barandela et al., 2003; Weiss, 2004).
Data imbalance occurs when one of the classes in a dataset is represented by a very small number of samples compared to the other classes, which is usually of great interest (Barandela et al., 2003; Weiss, 2004).
This issue might skew the prediction accuracy of classification models (Guha and Schurer, 2008; Hsieh et al., 2008), resulting in a weakened performance of machine learning algorithms (Kang and Cho, 2006).
In HTS experiments, usually tens of thousands of compounds are screened, but only a small fraction of tested compounds turns out to be active, while the rest are inactive.
Thus, HTS data is typically imbalanced in general with a small ratio of active compounds to inactive ones.
Although many researchers (Guha and Schurer, 2008; Han et al., 2008; Weis et al., 2008) have noticed this problem when using the data in PubChem, to the best of our knowledge, there has been no method reported to tackle this problem effectively.
Han et al.(2008) suggested that the data imbalance issue hindered the bioactivity classification accuracy.
Guha et al.developed a random forest ensemble model designed to alleviate the imbalance of the The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:00 18/11/2009 Bioinformatics-btp589.tex] Page: 3311 33103316 A novel method for mining screening data dataset in cell toxicity prediction (Guha and Schurer, 2008).
Weis et al.(2008) suggested the assay data be carefully selected from PubChem to attempt to avoid using the imbalanced data in their study.
Recently Tang et al.(2009) conducted an exhaustive comparative study of the currently reported state-of-the-art methods and proposed the granular sampling strategy to rebalance the originally imbalanced data.
In this study, we adopted this method to HTS data analysis and investigate the strategies to mine the highly imbalanced luciferase inhibition bioassay data in PubChem (PubChem AID of 773, 1006 and 1379).
HTS experiments often employ luciferase reporter genes and measure luminescence readouts (Fan and Wood, 2007).
The intrinsic luciferase inhibition property of small molecules has been identified as one of the major screening artifacts (Auld et al., 2009; Fan and Wood, 2007; Inglese et al., 2007) for hits identification in such HTS experiments.
Therefore, we also wanted to develop a virtual screening method to classify luciferase inhibitors, and to filter out potential interference molecules in luciferase-based HTS assays by taking the advantage of large scale luciferase inhibition profiling screening results recently available in the PubChem BioAssay database.
In addition to the data imbalance problem, HTS datasets can be large, containing test results for hundreds of thousands of chemical samples.
It is a time-consuming process to build and optimize statistical models using such HTS data.
Therefore, any improvement to computational efficiency could allow more data to be analyzed in less time.
Moreover, HTS data are also noisy in general (Diller and Hobbs, 2004; Weis et al., 2008), and one needs to be cautious when utilizing and analyzing the data.
Strategies to address these issues are also explored in this study.
2 METHODS 2.1 PubChem fingerprint PubChem fingerprints were used to characterize chemical compounds in this study.
PubChem 2D chemical structure fingerprint is an 881dimension binary (0/1) vector.
Each bit represents a Boolean determination of the absence or presence of a specific element, a type of ring system, atom pairing, or atom environment (nearest neighbors), etc.
A detailed description of this fingerprint system is available at ftp://ftp.ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.txt 2.2 Dataset Three bioassay data entries in PubChem (PubChem BioAssay accession: AID: 773, AID: 1006 and AID: 1379) for screening luciferase inhibitors were used to construct the training dataset.
The HTS experiments were performed by two independent screening laboratories with AID: 1006 and AID: 773 by the Burnham Center for Chemical Genomics, and AID: 1379 by the NIH Chemical Genomics Center (NCGC).
The assay of AID: 773 contained 237 active and 33 inactive compounds out of 270 tested ones; the assay of AID: 1006 contained 2976 active and 192 590 inactive compounds out of 195 566 tested ones; and the assay of AID: 1379 contained 565 active, 197 543 inactive and 982 inconclusive compounds out of 199 080 tested compounds (Table 1).
A majority of the compounds screened by the assay of AID: 773 were also tested by the other two assays with 267 and 270 compounds in common with the assays of AID: 1006 and AID: 1379, respectively.
The compounds screened by assays ofAID: 1006 andAID: 1379 also had a substantial overlap of 187 138 compounds in common.
However, the bioactivity outcomes, i.e.active or inactive, of a single compound, did not completely agree with each other in these three assays, which may be due Table 1.
Three assays of luciferase inhibitors used in this study AIDa Active Inactive Inconclusive Total 773 237 33 0 270 1006 2976 192 590 0 195 566 1379 565 197 543 982 199 080 aPubChem BioAssay accession.
Table 2.
Datasets used in this study Number Type Active compounds Inactive Compounds Imbalance ratioa I Training 390 146 934 1:377 II Blind testing 97 36 733 1:379 aImbalance ratio denotes the ratio of active compounds to inactive ones in the dataset.
to experimental noise or artifacts in the HTS assay (Diller and Hobbs, 2004; Weis et al., 2008).
Therefore, we treated an active compound as a positive sample if at least two assays considered it active.
Similarly, we treated an inactive compound as a negative sample by the confirmation of at least two assays.
Compounds without confirmed bioactivity outcome were excluded in this study.
A total of 487 positive and 183 667 negative compounds were included in the final dataset, which was denoted as (487 + 183 667) in following descriptions.
Other datasets, in this study, was similarly denoted with the first number represents the active compounds, with the second number represents the inactive ones.
The final dataset had an imbalance ratio of 1/377 (active/inactive), or <0.3% active samples were contained in this dataset.
To keep the data imbalanced, we randomly split the whole dataset (487 + 183 667) into training data (dataset I) and blind testing data (dataset II) with the ratio of 80/20 that 80% (390 + 146 934) of the whole dataset (dataset I) was used for constructing model and the other 20% (97 + 36 733) (dataset II) for blind testing (Table 2).
2.3 Modeling with support vector machine We used support vector (SV) machine (SVM), Libsvm (Chang and Lin, 2001), for this study.
According to the statistical learning theory (Corinna and Vapnik, 1995), an optimal hyperplane is drawn by the SVM model to separate active and inactive samples with a maximum distance between the two classes.
However, in an imbalanced situation that the majority class exceeds the minority class by a significant amount, the model likely pushes the ideal hyperplane towards the minority class (Tang and Zhang, 2006; Wu and Chang, 2005), resulting in classifying most observations into the class in which the majority samples belong.
To avoid this distortion, we explored two methodologies in this study: one was to rebalance the dataset with GSVM-RU method (Tang et al., 2009); the other was to evaluate the model performance with vigorous evaluation metrics independent of data distribution (Kubat and Matwin, 1997).
2.4 GSVM-RU undersampling method The GSVM-RU method (Tang and Zhang, 2006; Tang et al., 2009) was used to sample the majority class in order to rebalance the dataset.
As only SVs are important for SVM model classification (Corinna and Vapnik, 1995) that removing non-SV samples does not substantially affect the model performance.
This method provides a mean to extract important samples from the dataset and to eliminate the unimportant ones.
Based on this theory, GSVM-RU extracted the samples from the majority class according to their contributions to the classification in sampling process.
3311 [11:00 18/11/2009 Bioinformatics-btp589.tex] Page: 3312 33103316 Q.Li et al.The GSVM-RU method treated all positive samples as important due to their rarity.
However, for the negative samples, only SVs were considered important.
Because of the large amount of negative samples, it was hard to extract all informative ones by using a single SVM, regardless of parameter tuning.
Despite this, a fraction of informative samples can still be identified and extracted.
These informative inactive samples (SVs) were removed from the original dataset to form a smaller dataset.
A new SVM was then modeled with the reduced dataset to identify another part of the informative negative samples.
This process was repeated as needed.
Finally, the negative samples extracted from the previous sampling runs were discarded, and only the newly extracted inactive samples were retained and aggregated together with the positive samples to produce the training set, on which the final SVM model is built.
Upon the completion of sampling, a final SVM was built and optimized with linear and radius-based function (RBF) kernels.
The flow chart of this strategy is shown in Figure S3 in the Supplementary Material.
2.5 Model evaluation The 5-fold cross-validation method was used to evaluate the performance of the SVM model.
The whole dataset (dataset I) was randomly split into five folds, with four folds used for training and the other fold for testing.
This process was repeated in turn and the final average performance was calculated.
In addition to evaluate the generalization of the final SVM model, a blind test was carried out with dataset II, which was not involved in training process.
To consider the imbalance issue in this study, the receiver operating characteristic (ROC) curve and geometric mean (G-mean) value were used, which were suggested as good indicators dealing with such problems for their independence from the sample distribution between classes (Barandela et al., 2003; Kubat and Matwin, 1997).
In addition, the common metrics of sensitivity, specificity and overall accuracy (Li et al., 2007; Li and Lai, 2007) were also calculated: Sensitivity = TP (TP + FN) (1) Specificity = TN (TN + FP) (2) Overall accuracy = (TP + TN) (TP + FN + TN + FP) (3) where, TP, FP, TN and FN denote true positive, false positive, true negative and false negative, respectively.
G-mean that tries to maximize the accuracy on each of the two classes while keeping them balanced is calculated as following: G-mean = SensitivitySpecificity (4) 3 RESULTS 3.1 Sampling the majority (inactive) data of dataset I The training set (dataset I) contained 390 active compounds and 146 934 inactive compounds, with <0.3% of the data active and 99.7% inactive resulting in a ratio of active to inactive of 1/377.
Due to the rarity of active data, we assumed all 390 active compounds positive in the training set.
For the majority class (inactive), the data was sampled through multiple rounds of sampling as described in Section 2.
The significant compounds after sampling were identified to construct the negative data in the final training set.
Based on the positive and the sampled negative data, the SVM model with linear and RBF kernels were built and optimized.
We used the GSVM-RU method to sample the inactive compounds based on their significance to the classification.
The compounds identified in the previous round of sampling were excluded in the subsequent round.
Considering the efficiency, we Fig.1.
The 5-fold cross-validation results of the training dataset in sampling process.
The squares in the solid line (blue) represent the G-mean values.
The filled circles in the dashed line (green) and the triangles in the dotted line (red) represent specificity and sensitivity, respectively.
used the linear kernel of Libsvm with default parameters and 5-fold cross-validation in this process.
Meanwhile, sensitivity, specificity and G-mean value were calculated to evaluate the performance of each model (Fig.1).
The G-mean value increased from around 60% to over 90% in the sampling process (Fig.1).
Similarly, the sensitivity and specificity improved gradually as well, with the specificity consistently 510% higher than the corresponding sensitivity of each model.
These data indicate that the SVM model performance improved continuously during the sampling process with the discrimination of inactive compounds slightly better than that of active ones.
Moreover, the steady increasing of the model performance confirmed that the negative samples close to positive samples were being extracted step by step.
In addition, we observed a variation of the performance of the SVM model at the beginning of the sampling process.
It is because that there are many inactive compounds close to the active ones, which are hard to be extracted for a single SVM model.
However, as the sampling process is continued, it became relatively stable and the results converged starting from sampling round 67.
At this point, the sample process was considered complete.
The 358 sampled inactive compounds obtained in the sampling process were used together with the 390 active ones to produce the final training set, which was used to build and optimize a SVM model with linear and RBF kernels.
3.2 Optimizing the SVM model We used two kernels of linear and non-linear functions, i.e.RBF kernel, to train the SVM model.
For linear kernel, we tuned the single parameter of cost C to optimize the SVM model; while for RBF kernel, we optimized both cost C and gamma using the grid tool in Libsvm.
The 5-fold cross-validation results of the linear model and the non-linear model are shown in Table 3.
The sensitivities of these two models were similar (88.46%); however, the specificity of 3312 [11:00 18/11/2009 Bioinformatics-btp589.tex] Page: 3313 33103316 A novel method for mining screening data Table 3.
The 5-fold cross-validation results of linear kernel SVM and RBF kernel SVM Kernel type Sensitivity (%) Specificity (%) Accuracy (%) G-mean (%) Linear 88.46 91.34 89.84 89.89 RBF 88.46 94.97 91.58 91.66 Fig.2.
The ROC curves of the SVM with linear and RBF kernels.
The dashed line (blue) and dotted line (red) represent RBF SVM model and linear SVM, respectively.
AUC (the area under the curve) is proportional to the model performance.
the RBF SVM was slightly higher than that of the linear model, and it outperformed the linear model by two percent with the highest G-mean of 91.66%.
For both the linear and the RBF SVMs, the specificity exceeded the sensitivity, which may be due to the plenty sampling of inactive sample space.
Similar results were observed in the ROC curves in Figure 2.
The dotted and dashed lines of the curve represent the performance of the linear SVM and RBF SVM, respectively.
The AUC is proportional to the model performance.
The AUC of the RBF SVM (0.931) was slightly greater than that of the linear one (0.928).
These data suggest that both models performed well in the 5-fold cross-validation tests with the RBF kernel slightly outperforming the linear kernel model.
In summary, both G-mean values and ROC curves show the good performance of the SVM models in the 5-fold cross-validation, with the RBF SVM model consistently producing slightly better results.
3.3 Evaluating models with blind test For a statistical model, it may perform perfectly in training, but poorly in testing due to over-fitting, resulting in a bad generalization.
Here, the generalization of the SVM model was evaluated with a blind test where the testing dataset (dataset II) was excluded from the training process in advance.
This dataset contained 97 active and 36 733 inactive compounds (Table 4).
Both the linear and the RBF SVMs successfully recognized 84 out of 97 active compounds, with an equal accuracy of 86.60%.
The performances for recognizing inactive compounds were similar for both the linear SVM (88.36%) Table 4.
Blind testing resultsa Kernel type Active accuracy (%) Inactive accuracy (%) G-mean (%) Linear 86.60 (84/97) 88.36 (32 457/36 733) 87.48 RBF 86.60 (84/97) 88.89 (32 651/36 733) 87.74 aThe numbers in parentheses show how many samples are correctly recognized out of the total testing ones.
Fig.3.
Distribution of the prediction results of testing samples.
The blue and red bars represent the correctly classified active compounds and inactive compounds, respectively.
While the yellow and green bars denote the misclassified active samples and inactive samples instead, respectively.
and the RBF SVM (88.89%).
The RBF SVM identified 32 651 from 36 733 inactive compounds, exceeding the linear SVM by 200 hits.
The G-mean value shows a similar trend with the accuracies of the linear SVM model and RBF SVM model as 87.48% and 87.74%, respectively.
These data indicate the RBF SVM performs slightly better than the linear SVM.
These blind testing results are very close to those of the 5-fold cross-validation testing, suggesting that both the linear and non-linear SVM have good generalizations.
We observed a clear separation of active and inactive compounds by the optimized SVM model as shown in Figure 3.
3.4 Fingerprint features We used PubChem fingerprint to characterize each of the compounds in this study.
A PubChem fingerprint contains 881 binary bits that indicate the presence or absence of a certain group of chemical features in a compound.
We calculated the weight of each feature using the linear SVM to examine their contributions to the classification.
We found that the contribution to the classification varied among the observed chemical features.
A positive score signifies that the feature is important to positive samples, while a negative score signifies the feature is important to the negative samples.
In general, the absolute value of a feature score is proportional to its contribution to the SVM classification.
About half of the features have a score between 0.1 and 0.1, indicating these features are less important to the classification (Fig.4).
The top 30 fingerprint features with higher weight score for active compounds are listed in Table S1 in the Supplementary Material.
We noticed that some of the high-score features recognized in this study are indeed important structure features observed in the active compounds.
3313 [11:00 18/11/2009 Bioinformatics-btp589.tex] Page: 3314 33103316 Q.Li et al.Fig.4.
The distribution of PubChem fingerprint score of the compounds according to the contribution to the classification.
For example, the feature of C(N)(N) (no.
6 in Table S1 in the Supplementary Material), is part of the scaffold of benzimidazole analogs, one luciferase inhibitor cluster; the features of C:S:C-C and S-C:C-N (no.
24, 30 in Table S1 in the Supplementary Material) are part of the scaffold of 2-Phenylbenzothiazole analogs (Auld et al., 2008; 2009).
Thus, this work may provide useful insights into the structure components critical for luciferase inhibition activity.
4 DISCUSSION One of the main difficulties of working with HTS data arises from the fact that HTS data is highly imbalanced with only a handful of hits identified often out of a huge amount of tested compounds (Fig.S1 in the Supplementary Material).
Most of the HTS assays in PubChem screened a large library of chemical compounds, where only a small fraction of the tested compounds were considered as hits and reported active in PubChem.
A distribution of the imbalance ratios of 258 distinct HTS assays in PubChem deposited by the NIH Chemical Genomics Center (NCGC) is shown in Figure S1 in the Supplementary Material, from which we observed almost ratios are under 1/10.
Here, we investigated an approach for building an SVM model on the highly imbalanced dataset of the luciferase inhibition HTS assay in PubChem (PubChem AID: 773, 1006 and 1379).
The best SVM model successfully distinguished the active compounds at an accuracy of 86.60% and the inactive compounds at an accuracy of 88.89%, with a total accuracy of 87.74% by critical evaluation test.
Ideally, the standard SVM draws an optimal hyperplane to separate positive data from negative data with maximum distance.
However, the SVM model that is constructed from an imbalanced dataset tends to draw the hyperplane away from the ideal place and towards the minority side.
Thus, the model is likely to classify most objects into the majority class, leading to great disparity between specificity and sensitivity, regardless of the parameter optimization.
As a result, the predictability of such a model can be substantially poor.
The GSVM-RU method, which is based on the statistical learning theory (Corinna and Vapnik, 1995), to under sample the majority class and minimize the information loss of the majority class, Fig.5.
Blind test results of each model over the sampling process.
The squares in the solid line (blue) represent the G-mean values.
The circles in the dashed line (green) and the triangles in the dotted line (red) represent specificity and sensitivity, respectively.
achieved good performances on imbalanced HTS data.
To look into the progress of the sampling process, we not only built the final well-optimized RBF SVM model based on the converged sampling results at sampling round 67, but also built a SVM model on the sampled compounds at the end of each round of sampling process.
For each round of the sampling, we tested the performance of the constructed SVM model with the blind test dataset, which contained 97 active compounds and 36 733 inactive compounds that all were excluded from the sampling process (Fig.5).
The initial model was able to recognize the inactive compounds very well at the accuracy of >90%, while it could only correctly classify the active compounds at accuracy of 60% or even less at the beginning.
As the sampling process proceeded, however, the ability of the SVM model to recognize the active compounds increased dramatically from <60% to >85% after certain steps.
These observations agreed well with the results in sampling process.
Meanwhile, the discrimination of inactive compounds only decreased 5%.
These results indicate that the hyperplane of the SVM model moved gradually from the minority class side towards to the majority class side (negative samples), in other words, it was pulled back to the ideal place.
The entire recognition ability (G-mean value) of the SVM model increased from 70 % to >87.74%.
Another concern is the evaluation metrics selection when evaluating statistical models built on HTS data.
Some of the commonly used metrics, such as overall accuracy [equation (3)], can lead to poor predictability when they are used for evaluating models built on imbalanced datasets as the evaluation metrics are dependent on the data distribution.
Indeed, using inappropriate metrics is another reason for prediction distortion when optimizing model on imbalanced data.
For instance, for a dataset with the positive/negative sample ratio of 1/9, although all samples are predicted to be negative, the overall accuracy is still 90%.
For this reason that we used G-mean value and ROC curve, which are 3314 [11:00 18/11/2009 Bioinformatics-btp589.tex] Page: 3315 33103316 A novel method for mining screening data independent of the data distribution, to evaluate the performance of the SVM.
Furthermore, the size of HTS data, which usually contains hundreds of thousands of compounds, presents another challenge.
Training and optimizing a statistical model on such a large dataset can be extremely time-consuming.
The undersampling method in this study was computationally efficient, and it successfully downsized the dataset from more than one hundred thousand compounds to several hundreds of compounds, thus greatly reduced the computational cost in the optimized process and significantly increasing the efficiency of the SVM model.
In other words, the GSVM-RU method provides a strategy for data sampling or data cleaning of a big dataset.
In addition, due to experimental conditions or the complexity of the biological system, there may be a certain amount of false positives and false negative data in HTS assays.
We found that assay bioactivity results for certain compounds conflicted with each other even when they were provided by the same laboratory.
For this reason, we carefully prepared the dataset in our study and used only compounds with bioactivity outcome, i.e.active versus inactive, which were confirmed by at least two assays.
Chemical space coverage is indeed an important issue in cheminformatics research.
The chemical space coverage was estimated based on 2D chemical structure similarity using Tanimoto score of 0.90 as the threshold.
Of the active compounds, 88.30% in the dataset are similar to the active compounds used in the model building, while 1.4% of the inactive compounds in the entire dataset are similar to one or more of the inactive compounds selected by the sampling procedure.
The high coverage of active compounds indicates a strong structureactive relationship (SAR) among this group of compounds.
Unlike the traditional clustering methods, such as k-mean, which finds the most featured representatives of a class, the strategy of the GSVM-RU method used in this study is to find the samples close to the border between two classes, which are used to draw the separating hyperplane.
It means the other samples far away from hyperplane are discarded when a model is built.
The entire training dataset was used in the searching of the optimized hyperplane.
The final coverage (1.4%) for the inactive compounds was calculated using the samples close to the separating hyperplane which were derived at the end of the sampling process.
The confidence of the prediction of a compound is proportional to the distance to the hyperplane.
The distribution of the blind test samples are shown in Figure 3, from which we observed that compounds that were not correctly predicted are basically close to the hyperplane.
In addition, we also investigated the chemical property space coverage and plotted the distribution of rule of five properties (molecular weight, XlogP, hydrogen donor, hydrogen acceptor) for this dataset as shown in Figure S2 in the Supplementary Material, which is coincident with the distribution of the bioactive molecules observed in another work (Frimurer et al., 2000).
We used PubChem 2D fingerprints to characterize chemical compounds.
A PubChem fingerprint indicates the presence/absence of a certain chemical feature in a studied compound.
As the 881 features did not make equal contribution to the classification, we further carried out a feature selection to reduce potential data noise, though no significant improvement of the model performance has been achieved on this regard.
We used the default PubChem fingerprint as the chemical structure descriptor in this study for it is readily available for public usage.
We anticipate that a combination with some other molecular descriptors might further improve the classification performance and help to identify the relationship between a molecular structure and its biological activity.
We plan to evaluate this factor in the future study by combining other molecular descriptors, including 3D molecular descriptors.
Bioluminescent assays are a popular technology used in HTS experiments (Fan and Wood, 2007).
Luciferase inhibition is one of the main sources of interference in such assays, which causes artifacts, and complicates the interpretation of the experimental data and the identification of HTS hits (Auld et al., 2008, 2009).
Development of a computational tool which could facilitate the selection of chemical compounds for HTS screening and assist the interpretation of the resulting bioassay data was another motivation of this study.
Several largescale HTS experimental results of luciferase inhibitor screening have recently been deposited in the PubChem BioAssay database.
The satisfactory performances in both cross-validation and blind test process have demonstrated the good generalization of the SVM model developed in this study, and suggests its potential application in virtual screening for compounds with luciferase inhibition or other types of biological activity.
It needs to be pointed out that the development of the model is a learning process.
Thus, the potential of the developed model is intrinsically limited to the known active compounds and the properties used for training.
With the growth of the additional chemical classes of compounds to be screened, a more robust model may be developed with the availability of biologically interesting small molecules from a diverse compound library.
5 CONCLUSION In this study, we used the GSVM-RU method to construct a SVM model on the extremely imbalanced HTS data [the imbalance ratio of 1:377 (active/inactive)] obtained from several luciferase inhibitor assays in PubChem.
The best model recognized the active and inactive compounds at the accuracies of 86.60 and 88.89%, respectively, with an accuracy of 87.74% in critical evaluation.
These results demonstrate the robustness of the model in handling the intrinsic imbalance problem in HTS data and indicate that the model can be used as a virtual screening tool to identify potential interference compounds in luciferase-based HTS experiments.
Additionally, this method has also proved computationally efficient by greatly reducing the computational cost and can be easily adopted in the analysis and interpretation of HTS data for other biological systems.
ACKNOWLEDGEMENTS We thank the National Institutes of Health Fellows Editorial Board.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine.
Conflict of Interest: none declared.
ABSTRACT Motivation: Metabolic and signaling pathways are an increasingly important part of organizing knowledge in systems biology.
They serve to integrate collective interpretations of facts scattered throughout literature.
Biologists construct a pathway by reading a large number of articles and interpreting them as a consistent network, but most of the models constructed currently lack direct links to those articles.
Biologists who want to check the original articles have to spend substantial amounts of time to collect relevant articles and identify the sections relevant to the pathway.
Furthermore, with the scientific literature expanding by several thousand papers per week, keeping a model relevant requires a continuous curation effort.
In this article, we present a system designed to integrate a pathway visualizer, text mining systems and annotation tools into a seamless environment.
This will enable biologists to freely move between parts of a pathway and relevant sections of articles, as well as identify relevant papers from large text bases.
The system, PathText, is developed by Systems Biology Institute, Okinawa Institute of Science and Technology, National Centre for Text Mining (University of Manchester) and the University of Tokyo, and is being used by groups of biologists from these locations.
Contact: brian@monrovian.com.
1 INTRODUCTION The core of systems biology is the biochemical/signaling network.
Signaling and metabolic pathways are an increasingly important part of organizing knowledge in systems biology and are often represented through collective interpretations of facts scattered throughout literature (Heiner et al., 2004; Kell and Oliver, 2004; Luciano and Stevens, 2007; Ye and Doak, 2009).
Because of the very integrated nature of pathways, they require substantial human effort to construct.
Biologists have to read a large number of published papers, interpret them and construct a pathway (Ananiadou et al., 2006).
The curation of a constructed pathway also requires monitoring of recent publications in order to maintain relevance.
Furthermore, since biologists may have different interpretations of the same set of facts, a biologist wants to read original papers based on which a pathway is constructed to ensure To whom correspondence should be addressed.
it is done in a manner consistent with his or her interpretation.
The biologist would like to see the biological context, stated in original papers, from which the constructed pathway abstracts away (Kell, 2006; Kell and Oliver, 2004).
PathText (http://www.pathtext.org) is an integrated environment for combining standards compliant (Finney and Hucka, 2003; Hucka et al., 2003) biological pathway models and original papers relevant to selected parts of the pathway, through the use of text mining (TM) technology (Ananiadou et al., 2006) and tools to facilitate the creation of manual annotations.
Unlike existing pathway building platforms, such as WikiPathways (Pico et al., 2008), the Edinburgh Pathway Editor (Sorokin et al., 2006), and PathCase (Elliott et al., 2008), PathText brings together the strengths of different TM tools in a unified and extensible framework.
Some of the existing pathway editors offer the functionality of linking parts of a pathway with literature at a very coarse (e.g.PubMed ID) level.
Perhaps one of the most similar to PathText in terms of the richness of TM functionality combined with pathway visualization is Pathway Studio (Nikitin et al., 2003), which is a commercial tool for building and analyzing biological pathways.
It integrates an automated text processing tool called MedScan to extract biological interactions from scientific literature using natural language processing (NLP) technology.
MedScan employs a full syntactic parser to analyze the semantic and lexical structure of an English sentence and finds relations between any types of objects including proteins, small molecules, protein functional classes, cell processes and diseases (Daraselia et al., 2004; Yuryev et al., 2006).
PathText distinguishes itself from other pathway editing tools by providing a seamless combination of advanced TM technologies, including deep syntactic analysis of individual sentences (MEDIE), named entity recognition and disambiguation of acronyms (KLEIO), and real time co-occurrence searches (FACTA).
It provides a flexible interactive environment which allows a biologist to navigate from pathway visualization to TM, to retrieve articles recently published which are potentially relevant, to browse them, and to associate them with relevant parts of pathways.
PathText has been used by biologists to construct signaling pathways of more than 1000 nodes with 400 links.
In the following sections, we provide the overall architecture of PathText, describe the manual annotation and TM components, describe how they are integrated with the overall system, and discuss future directions.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:16 12/5/2010 Bioinformatics-btq221.tex] Page: i375 i374i381 PathText: a text mining integrator Fig.1.
PathText system architecture.
2 OVERALL ARCHITECTURE OF PATHTEXT PathText provides a user friendly interface by which one can freely move from pathways to relevant articles, or to parts of full papers annotated by biologists during the model construction phase.
A detailed illustration of the user interface is provided on the PathText website (http://www.pathtext.org).
The central component of PathText is the Integrator, which coordinates interaction between (i) Payao pathway visualizations, (ii) TM systems, (iii) manual annotations and (iv) the PathText results GUI (see Fig.1).
2.1 Payao pathway visualizations Users of the PathText system access pathway model visualizations through the Payao Web 2.0 (Matsuoka et al., 2008) communitybased collaborative web-service platform for modeling biological networks, a framework designed to enable a community to work on models concurrently.
Payao reads the models in SBML (Systems Biology Markup Language, http://sbml.org) format, displays them with CellDesigner (http://celldesigner.org), a process diagram editor (http://celldesigner.org) which complies with the process description notation defined by SBGN (Systems Biology Graphical Notation, http://sbgn.org) language, and provides an interface for model enrichment (adding tags and comments to models) for the accesscontrolled community members (see Fig.2).
2.2 TM systems The PathText Integrator reads information from a pathway model file in SBML and SBGN, and generates queries for TM systems tailored to the specific requirements of each.
These queries are sent to each of the TM systems through a web service and results from each are stored in the repository of the PathText Integrator.
2.3 Manual annotations PathText includes tools to assist biologists in creating manual annotations linking specific sections of original publications with nodes in a pathway model.
These links are also stored in the repository of the PathText Integrator.
2.4 PathText results GUI A Payao user interacts with the PathText system by selecting document icons overlaid on the pathway visualization.
These links Fig.2.
Payao interface with integrated PathText control panel (far left) and PathText search results displayed as document icons over the corresponding parts of the pathway.
Red and green icons correspond to sets of manually curated articles and ones retrieved by the TM systems.
The shades of the color of these icons reflect the number of articles in the sets.
By clicking one of these colored icons, a user can move another window to see the articles.
are presented not only for individual species (proteins, genes, etc.
), but the reactions linking them as well.
The PathText Integrator presents matching results through a new tab in the users web browser, making PathText feel as if it were a part of Payao.
3 MANUAL ANNOTATION/MODEL CURATION One of the crucial deficiencies in currently constructed biological pathways is their lack of detailed linkage with original articles.
While some models embed the identifiers of articles such as PubMed IDs or their bibliographic information, some do not have any links.
Even for models with uniquely identified papers, biologists who want to check the evidence provided in the original papers spend a significant amount of time retrieving the papers and then identifying the parts (e.g.paragraphs, sentences, tables, figures, etc.)
relevant to the specific nodes or links in a model that they are interested in.
PathText provides a tool for manual annotation of full papers that can be used during the model curation phase.
The tool is web based and assists biologists to create manual annotations, connecting nodes and links in a pathway to arbitrary rectangular regions in papers.
i375 [12:16 12/5/2010 Bioinformatics-btq221.tex] Page: i376 i374i381 B.Kemper et al.Fig.3.
PathText manual annotation tool and search results GUI.
In the top screen, the first page of an article is shown with a yellow rectangular region which is judged relevant by a biologist.
In the right column, a list of annotations in this article with the name of annotator, her/his comment, etc.
is shown.
By clicking one of them, a user can jump to the corresponding part of the article.
In the bottom screen, a list of full articles which are manually annotated.
By clicking one of the articles, a user can see pages of the article in which annotations are made (the bottom half of the screen).
It is used not only at the initial model curation phase, but also for the monitoring phase in later stages of the process.
In Figure 3, we see an example of PathText manual annotation.
First, a curator identifies an initial set of articles by searching through MEDLINE via the automatic search or manual search mode (see Section 6).
Then the curator collects the full-paper version of the retrieved abstracts judged to be relevant and submits them to the PathText Integrator where they are converted (often from PDF) into images and stored in the PathText repository.
The curator then views the publications using PathText to annotate the relevant parts in the model with selected rectangular regions on the images of the full papers.
A rectangular region is stored in the Integrator along with the text inside the region, publication details and the identifier of the node or link in the model annotated by the region in the full paper.
PathText enables biologists to see all the full papers, and the parts judged as relevant to a specific part of a model by other biologists, by simple operations such as clicking an icon attached to links or nodes in a network.
4 TM SYSTEMS In addition to the manual annotation tool for biologists to create annotated links between models and full papers, the PathText Integrator includes a method to retrieve new papers relevant to specific parts of a model through the use of TM systems.
The only way of linking parts of a model with such an implicit set of papers is in the form of queries, by which each of the individual TM systems retrieves a set of text.
Because the results returned by each TM system have their own semantic annotations, the Integrator needs to interpret the annotations in retrieved text to identify the portions of text relevant to the model and visualize them.
We integrate three TM systems (MEDIE, KLEIO and FACTA) in PathText, each of which has different characteristics, its own strengths and weaknesses.
The crucial considerations are how to maximize the effectiveness of the system by exploiting the characteristics of these TM systems, and how to reduce the burden of a user in interaction with these TM systems.
A pathway represents a specific biological context in which species (genes, proteins, enzymes, etc.)
and relations among them (phosphorylation, binding, degradation, etc.)
occur.
When a user clicks a specific node in a pathway, she/he is most likely to be not interested in the gene, protein, enzyme, etc.
in general that the node represents.
Instead, they are interested in the gene, protein or enzyme in the specific context.
Such rich contextual information should be used in query formation to improve the precision.
MEDIE, the query language of which is highly expressive, provides the means by which such rich contextual information is embedded in a query.
The integrator in PathText stores the associations between nodes/relations and complex query formulas.
The complex queries and their associations are established in advance during the model curation phase.
On the other hand, while such fixed queries embedded in a pathway model in advance are effective, a biologist wants to navigate a set of documents rather freely.
The general query format which MEDIE provides is, though expressive, too complex.
Interactive query formation provided by KLEIO, including rich semantic annotations and facet-search based on them, is ideal for such free navigation of document sets.
The other functionality, which we have found extremely useful, is to allow a user to formulate a query on the fly by using visualization of pathway.
That is, she/he clicks an arbitrary number of nodes in a pathway and then, the system returns a set of documents in which the species those nodes represent co-occur, together with distribution statistics of the other species in the pathway.
FACTA provides this functionality with its specialized indexing.
4.1 MEDIE MEDIE (http://www-tsujii.is.s.u-tokyo.ac.jp/medie/search.cgi) (Miyao et al., 2006) is an intelligent search engine to retrieve biomedical relational information from a large textbase created from MEDLINE.
Figure 4, shows MEDIE text mining search results in PathText.
The textbase stores the whole MEDLINE abstracts, with annotations represented by XML-like text markup for both the metadata of the articles provided by NLM (MeSH terms, publication date, etc.
), and analysis results by various NLP modules.
The annotated text is indexed for efficient structure search by extended region algebra (ERA) (Masuda and Tsujii, 2008).
A query with high precision can be formulated by using NLP analyses results and the region algebra.
The NLP modules used for the annotation include (but not limited to) a deep syntactic analyzer, an event expression recognizer (EER) and a term recognizer.
The syntactic analyzer, Enju i376 [12:16 12/5/2010 Bioinformatics-btq221.tex] Page: i377 i374i381 PathText: a text mining integrator Fig.4.
MEDIE text mining search results shown in the PathText GUI.
A sentence which contains a biological event is shown together with the whole abstract in which the sentence appears.
The biological event correspond to a link in a pathway which the user clicks.
The event in a sentence is shown with the verb, the subject and object, each of which is highlighted with different colors.
Fig.5.
A semantic relation expressed in different textual expressions: these two tree structures represent two sentences with different voices (active and passive), which essentially describe the same event.
The identity of the event is captured by the predicate-argument structures (PAS, shown by dotted lines) of these sentences.
Enju computes such predicate-argument structures of sentences, and one can formulate a query based on PAS in the region algebra to retrieve the sentences which contain events of protein A activates protein B. parser (Miyao et al., 2009), produces a syntactic and semantic analysis of the text, based on a linguistic formalism called HPSG (Pollard and Sag, 1994).
A relational concept, such as protein A activates protein B, can be precisely described as a query which specifies the semantic structure given by the Enju parser as constraint (see Figs 5 and 6).
This is the main strength of MEDIE compared to other publicly available TM modules which use Boolean formula of keywords or concepts for query formulation.
Boolean formulas basically specify co-occurrence of concepts or words as constraint for retrieval.
One can only specify co-occurrence of protein A, protein B and the verb to activate in the same textual unit (usually an abstract) as constraint, which results in a large number of false positives.
Units of retrieval in MEDIE are finer than those in other TM modules.
They can be individual sentences in abstracts, or even Fig.6.
An example of an ERA query: >> is a operator.
[R1] >> [R2] means a text span tagged by R1 should contain another text span tagged by R2.
$sbj and $obj are variables.
They are used to express the dotted lines in Figure 5.
For example, $sbj is used to equate the phrase which plays the role of arg1 in a sentence with the phrase which contains the word protein-A.
phrases.
Furthermore, the ERA allows us to specify constraints on context from constraints on units of retrieval.
That is, we can formulate a query for retrieving sentences which contain a specific biological event (e.g.*Protein A* activates *protein B*) and which appear in abstracts with certain keywords or other biological events reported.
This separation of units of retrieval and context is extremely useful for PathText to specify constraints embodied by the neighboring parts of a pathway network.
The semantic representation produced by Enju also works as an intermediate language which bridges the gap between a search query and the textual expression in the article.
That is, a single semantic relation is represented in the same way in the semantic representation level, across various different textual expressions, and hence retrieved with a single query.
See Figure 5 for an example of a semantic relation protein A activates protein B, expressed differently in surface textual form, in which the semantic subject (ARG1) and semantic object (ARG2) of the verb activate is represented in the same way in the semantic representation level (indicated by the dashed arrows).
The EER and the term recognizer further enhance the search capability of MEDIE by introducing another level of abstraction of semantics.
They map surface textual expressions of biological events or technical terms to the corresponding concept identifiers defined in ontologies.
The EER recognizes biological molecular events mentioned in text and map them to identifiers of event types defined in terms of Ontology (Ashburner et al., 2000).
The current version of EER distinguishes 35 event types in GO, which include binding, positive/negative regulation, etc.
Using the annotations by Enju and the EER, we can retrieve sentences in which a biological event of positive regulation of protein A by protein B is reported, even though they may be expressed in diverse surface expressions like A activates B, B is induced by A, and so on.
The domain specific lexical knowledge, like the synonymy of activate and induce in the molecular biology domain, was collected from the GENIA Event corpus (Kim et al., 2008).
The term recognizer detects gene, protein and disease names in the text, and assign unique database IDs to differently expressed entity names (i.e.synonyms).
The gene/protein IDs are taken from a gene/protein meta-DB, Gena (Koike and Takagi, 2004) and the disease IDs are taken from UMLS.
By combining the annotations given by the term recognizer with ones by the parser and the EER, we can recognize a biological event in even when an entity (protein or gene) involved there is mentioned in different names.
The index for MEDIE is based on the ERA.
In the ERA, we can specify a semantic relation encoded as the topological relations (e.g.a text span includes another text span, a text span follows another text span, etc.)
among textual spans and annotations.
Structural relations i377 [12:16 12/5/2010 Bioinformatics-btq221.tex] Page: i378 i374i381 B.Kemper et al.Fig.7.
An ERA query for <subject, verb, object>=<p53, activate, beta-4>.
can also be directly represented by linking variables.
For example, to retrieve the sentences that mention a binding event between protein A and protein B, we formulate a query that has three key phrases: protein A, protein B and bind among which a semantic relation protein A binds to protein B holds.
The query in the ERA is shown in Figure 6.
MEDIE accepts a search query through a WEB-API, in addition to an interactive search UI.
The API takes a tuple of <subject, verb, object> as the input, which describes a biological event/relation, such as <p53, activate, beta-4>, and returns a set of articles in which the event/relation is mentioned.
The tuple is internally translated to an ERA query, using the same gene/protein dictionary and event expression dictionary used in the above-mentioned NLP modules.
For example, the tuple <p53, activate, beta-4> is translated to the following region-algebra query shown in Figure 7.
The WEBAPI thus hides irrelevant details of the backend database from the viewpoint of the users, such as the annotation schemes used in the NLP modules or the dictionary used in developing it.
A specification on the meta-data part such as journal titles can be expressed as additional fields to the subject-verb-object tuple.
4.2 KLEIO KLEIO (http://www.nactem.ac.uk/software/kleio/) (Nobata et al., 2008) uses the results of named entity recognition to provide a range of semantic search functions.
A standard indexing tool, Lucene, is used to generate an index over the terms for proteins, genes, metabolites and medical terms that have been recognized.
This is an index of the concepts that are referred to in the text, rather than individual, or canonical word forms.
This functionality allows us to retrieve documents that refer to a specific concept, although the surface form used may differ in each case, as in the use of orthographic variants, or acronyms instead of their expansions.
In KLEIO, full forms of named entities, including variants, are linked to their acronyms via an acronym recognition and disambiguation process (Okazaki and Ananiadou, 2006).
The system also offers document retrieval based on the unique identifier for a concept, providing a link back to the original databases from which the systems dictionary was generated.
In addition, by further classifying terms into semantic categories the system allows the user to specify a specific concept, by associating a semantic category with a query term.
This can radically reduce the search space.
For example, more than 60 000 documents were returned when the word cat was given as a query, due to its ambiguity.
However, when the query was modified to specify the desired semantic category for cat e.g.PROTEIN, a more focused result is returned.
For the query PROTEIN:cat, 200 documents were returned.
Moreover, the documents returned by the initial query are dynamically organized into semantic facets based on the named entities recognized both in the query and occurring in the same immediate context in each document retrieved.
The user may thus refine the initial query by combining concepts from the offered facets or may pursue the links to the document representations.
The documents themselves are presented with concept markup on all the recognized terms.
As with MEDIE, KLEIO stores the whole set of abstracts from MEDLINE together with metadata provided by the National Library of Medicine and augments these data with rich semantic annotations.
Semantic annotation in KLEIO is much richer in semantic categories of named entities than those of MEDIE, though it does not have syntactic/semantic annotations of sentences.
The normalized identifiers which KLEIO uses are, therefore, not only UniProt identifiers and UMLS identifiers, but also HMDB and DrugBank identifiers for small molecules and metabolites which are crucial for integrating metabolic pathways.
Acronyms, which are pervasive in biological papers, are also disambiguated (Okazaki et al., 2010) and normalized into identifiers if the disambiguated results belong to the semantic categories which KLEIO is able to deal with.
Because of surface word indexing and richer semantic categories, KLEIO is used as a fall-back system when species in a model are not covered by MEDIE.
KLEIO accepts PathText queries through a WEB-API.
The API accepts space separated terms as well as Boolean queries, for example p65 AND beta4.
KLEIO then returns a set of articles relevant to the query in XML containing PubMed IDs and the abstract highlighted with the terms matching the query.
4.3 FACTA FACTA (http://refine1-nactem.mc.man.ac.uk/facta/) (Tsuruoka et al., 2008) is an information retrieval system with a usage very different from MEDIE and KLEIO.
It takes large sets of articles (the whole MEDLINE in the current version of FACTA) to find implicit associations between named entities, by using statistical measures of co-occurrences of entities in the same articles.
It can find and show a biologist a list of genes, for example, which would be relevant to a given disease.
FACTA was originally designed as an interactive system to show the user such a list of entities on the fly, and special care was taken to compute the statistical measures very quickly.
More specifically, it builds a special data structure called inverted index that allows for efficient access to articles in which a particular set of entities appear.
This data structure enables the system to compute co-occurrence statistics on the fly even if the input entities appear in a large number of articles.
For example, FACTA can produce a ranked list of genes and proteins that co-occur with p53, which is mentioned in more than 46 000 articles, in 0.04 s. Combined with the PathText interface, FACTA allows the user to select an arbitrarily subset of the species in the pathway and immediately find the information about which other species co-occur with them in the literature (Fig.8) (see Section 6.2).
It should be noted that such co-occurrence statistics cannot be computed off-line, because the user is allowed to specify any combination of the species as the input.
This is the reason why PathText needed to integrate the functionality of FACTA, which is tuned for real-time uses with a special index structure.
In contrast, retrieval of articles by MEDIE and KLEIO is performed in batch mode (i.e.off-line) and the results are attached to the relevant part of a model in the repository of the Integrator (see Section 6.1).
Unlike MEDIE or KLEIO, FACTA currently uses a simple longest matching algorithm to recognize gene/protein names in i378 [12:16 12/5/2010 Bioinformatics-btq221.tex] Page: i379 i374i381 PathText: a text mining integrator Fig.8.
Payao GUI showing PathText manual search results discovered using FACTA.
The two green colored nodes are the nodes clicked by a user.
FACTA retrieves a set of documents in which these two species co-occur.
The red icons show that these species also occur in the retrieved documents.
The shades of the red icon reflect the numbers of documents in which the three species co-occur.
the literature.
The dictionary was created from BioThesaurus (Liu et al., 2006) with some manual curation efforts including the removal of noisy and highly ambiguous entries.
FACTA runs on a generic Linux server with 2.2 GHz AMD Opteron processors and 16 GB memory, on which all the inverted indexes are stored.
5 QUERYING OF TM SYSTEMS The primary function of the PathText Integrator is to gather information from pathway model files, generate queries that can be interpreted by various TM tools and store the results from these queries in a repository.
In the first step of this process, the PathText Integrator reads an SBML model file and generates tables for the various components in the repository.
This data includes information such as proteins and gene labels, characteristics for these entities such as phosphorylation and ubiquitination, and details for the reactions that link them together.
The Integrator also reads MIRIAM annotations (Le Novre et al., 2005) containing unique identifiers such as UniProt.
This data can then be used to generate queries conforming to the disparate formats of each TM application.
For example, MEDIE accepts queries in a <subject, verb, object> format.
In the case of a reaction contained in the pathway, the Integrator will use a set of rules to determine a valid verb for describing the reaction by interpreting the components of that reaction.
For example, if a reaction starts with a non-phosphorylated protein on one side, and contains the same protein with phosphorylation on the other, then the verb phosphorylates can be generated.
For generating a query for a species, the system uses unique identifiers, such as UniProt if the system supports them, and text labels for that entity in other cases.
PathText also includes a manual override function, where a query for a particular protein or reaction can be manually added to the SBML model file in a PathText Annotation Tag that takes precedence over queries generated by the Integrator.
6 PATHTEXT MODES OF OPERATION PathText provides rich modes of operation for assisting biologists to create and maintain links between biological models and large collections of articles, through the whole life cycle of a model, e.g.curation of articles and model building, model updating, exploration of articles by TM tools through a model, etc.
In addition to the manual annotation tools described above, two distinct methods for interacting with the PathText Integrator and the manual annotations and TM results it contains: automatic and manual Search.
6.1 Automatic search with updating function Once a model is constructed and annotated with relevant papers, curators or biologists want to retrieve other articles which may be relevant to each part of the model.
The queries generated by the Integrator for individual nodes and links are used to retrieve abstracts from MEDLINE.
Since these queries are generated in advance, PathText performs text retrieval regardless of actual user requests and results are attached to corresponding parts of a model.
This off-line renewal of retrieval results is to be performed on a nightly schedule as new set of abstracts are added to the MEDLINE databases of MEDIE and KLEIO on daily basis.
The automatic search mode uses this set of retrieved abstracts.
The mode is called automatic since biologists cannot create new queries on the fly.
They can only specify a date range.
Date rage specification is important for up-dating a model by newly published articles.
The Integrator passes the information on how many abstracts are found for each part of a model to Payao, and Payao then displays document icons overlaid on the pathway visualization over the corresponding entities (proteins, genes, reactions, etc.).
Clicking on any of these document icons will open a new tab in the users browser with the matching PathText results.
This results page contains tabbed sections, one for already curated articles by manual annotation and one for each TM system (MEDIE and KLEIO) that has relevant matches to display.
While one can easily view parts of full papers relevant in the curated articles, the Integrator interprets the annotations in abstracts given by the TM systems and displays them in the same format including article title, authors, publication date, the name of journals and the abstract with the matching terms highlighted.
6.2 Manual search The manual search mode allows a biologist to freely explore new articles by using the TM services, currently FACTA, with their own queries.
The simplest form of a new query is constructed by clicking several nodes (species) in a model.
When receiving a set of identifiers for the selected nodes from Payao, the PathText Integrator invokes FACTA to retrieve a set of abstracts in which the selected species co-occur, and then checks whether these abstracts also contain other species in the model.
The numbers of abstracts which contain other species are sent back to Payao with the identifiers of the species.
Payao displays document icons on the corresponding entities in the network color-coded according to the numbers of abstracts (see Fig.5).
Clicking on any of these document icons will open up a PathText results tab.
7 CONCLUSION AND FUTURE DIRECTIONS PathText integrates three knowledge sources indispensable for systems biology, i.e.(i) external databases such as SwissProt, EntreGene, Flybase, HUGO, etc., (ii) text databases such as i379 [12:16 12/5/2010 Bioinformatics-btq221.tex] Page: i380 i374i381 B.Kemper et al.MEDLINE and full papers, and (iii) pathways as organized interpretations of biological facts.
Since integration of external databases with other knowledge sources has already been attempted and achieved through dictionaries of identifiers of the databases, this article focused on the integration of pathways with literature.
PathText successfully provides integration of text to pathways and is now being used by three biological groups at the Systems Biology Institute, the University of Tokyo (Oda et al., 2008) and the Manchester Centre for Integrative Systems Biology in the UK (Herrgrd et al., 2008).
The implementation of PathText relies on the software being developed by the consortium members.
Payao, which maintains a database of pathway models and provides the software for the pathway visualization interface, is being developed by SBI and OIST.
Information is available on the Payao website (http://www.payaologue.org).
The two TM systems, KLEIO and FACTA, which annotate MEDLINE abstracts by rich semantic annotations of named entities, are provided by the National Centre for Text Mining (http://www.nactem.ac.uk).
The TM system for automatic search, MEDIE, has been developed by the University of Tokyo.
While these subcomponents had their standard WEB-APIs, we added extra functionalities to them for PathText.
These extended APIs will be made available with necessary standardization for other groups that are interested in similar knowledge integration.
There are several issues for future development.
(1) The design decisions of the current PathText are highly dependent on a specific type of pathway, i.e.signaling pathway which is being developed at the University of Tokyo.
In particular, automatic query generation reflects specific characteristics of this pathway.
The biology research group in Manchester, which is engaged in research of metabolic pathways, has significantly different requirements.
We need to impose proper modularization of the current PathText Integrator, in order to meet the differing requirements of users.
(2) The current use of biological context in generating queries is rather straightforward.
As our pilot study shows (Oda et al., 2008), the biological contexts in a pathway affects relevance judgments by biologists.
We need detailed error analyses on the results produced by text retrieval components to generate appropriate queries from a given pathway model.
(3) The three TM subsystems work separately in PathText.
The query generation is designed separately for each of them, and mixing of their results is not done at present.
The results are only presented in the same display format.
In the future, these results will be ranked according to different types of user needs such as displaying contradictory facts extracted from text.
Internal informal evaluation by our collaborating biology teams has demonstrated the usefulness of PathText for their work.
The teams are continuing to use the system.
Their ongoing feedback is being incorporated into a novel user based evaluation framework which will be used for a formal evaluation in conjunction with a wider community.
(4) While the PathText data repository is managed locally, this should be maintained by a global database.
Moreover, future plans include expansion to tackle the analysis of full papers not just abstracts.
Techniques to achieve such analysis are already being developed by the National Centre for Text Mining within the UKPubMed Central project.
However, we note that in the general case, the use of full papers is related with the issue of copyright and access to commercial publishers collections.
PathText is currently only used by biologists who belong to institutions with access rights to full papers.
In order for models in PathText to be freely accessed by a wider community, we need to include a proper authentication method for accessing full papers.
ACKNOWLEDGEMENTS We wish to thank Kanae Oda from Tokyo Medical and Dental University for the development of pathway models and testing of PathText, Chikashi Nobata at the National Centre for Text Mining for his work on the KLEIO system, Rune Saetre from the University of Tokyo for development of the PathText system and Nobuhiro Kikuchi from Mitsui Knowledge Industry for Payao development.
Funding: Biotechnology and Biological Sciences Research Council (BB/E004431/1).
Conflict of Interest: none declared.
ABSTRACT Motivation: Unraveling the structure and behavior of the brain and central nervous system (CNS) has always been a major goal of neuroscience.
Understanding the wiring diagrams of the neuromuscular junction connectomes (full connectivity of nervous system neuronal components) is a starting point for this, as it helps in the study of the organizational and developmental properties of the mammalian CNS.
The phenomenon of synapse elimination during developmental stages of the neuronal circuitry is such an example.
Due to the organizational specificity of the axons in the connectomes, it becomes important to label and extract individual axons for morphological analysis.
Features such as axonal trajectories, their branching patterns, geometric information, the spatial relations of groups of axons, etc.
are of great interests for neurobiologists in the study of wiring diagrams.
However, due to the complexity of spatial structure of the axons, automatically tracking and reconstructing them from microscopy images in 3D is an unresolved problem.
In this article, AXONTRACKER-3D, an interactive 3D axon tracking and labeling tool is built to obtain quantitative information by reconstruction of the axonal structures in the entire innervation field.
The ease of use along with accuracy of results makes AXONTRACKER-3D an attractive tool to obtain valuable quantitative information from axon datasets.
Availability: The software is freely available for download atContact: stwong@tmhs.org 1 INTRODUCTION The modern microscopic image acquisition techniques provide the neuroscientists the visual perception of axons in 3D space.
Of particular interest is the knowledge of connectivity of the neuronal components of the nervous system, the connectome.
Analyzing neuronal structures is instrumental in completely understanding the functional properties of the mammalian central nervous system (White et al., 1986).
One such phenomenon that can be studied is the developmental reorganization of connectivity in the mammalian nervous system, also known as synapse elimination (Purves and Lichtman, 1980).
Pruning of synaptic connections through competitive interactions during neurodevelopment and its effect on the neuronal connectivity is one such example.
A study of connectomes over various time points during brain development can reveal the mysteries about how the neural circuits are established and the way it changes over the lifespan of the animal To whom correspondence should be addressed.
under study.
Moreover, mapping wiring diagram mouse models can help understand how an abnormality in the neural circuits can lead to psychiatric disorders, such as schizophrenia and autism (Lichtman et al., 2008).
Another potential application is in the study of the effect of new drugs on the mammalian connectomes.
Analysis of the connectomes at different time points during the drug-testing could lead to valuable insights into the effectiveness of the new drugs designed to treat disorders related to brain and central nervous system.
The wiring diagram of the neuromuscular circuit is believed to be a good starting point for this, as it might lead to discoveries valuable to this study.
The neuromuscular circuits are well-isolated and accessible.
Its connectome is well defined.
Due to the large size of the adult motor neurons (210 m in diameter), which are separated from neighboring axons by Schwann cells, and the large size of neuromuscular junctions (20 m in length), the neuromuscular connectome is easier to resolve, analyze and can be easily visualized.
Moreover, since the morphological features of the neuromuscular connectome can be directly linked with its functional properties; for example the morphology of individual neuromuscular junction has been found to be directly related to its synaptic strength (Herrera et al., 1985), it renders itself as an attractive platform to conduct connectome studies.
Many questions related to synapse elimination can be answered from the study of mammalian neuromuscular connectome.
As the spatial organization of the axons is very specific in nature, down to the level of individual axons, it becomes important to isolate each axon from the connectomes for extraction of morphological features for these studies.
Studies in neuronal connections are believed to help unravel the functional properties of the nervous system.
The manual effort needed, however, is excessive, as the neurobiologist has to manually track individual axons from a large set of data (3050 GB of volumetric data).
This might amount to a few months of painstaking labor.
Of note are similar researches in the mapping of neural circuits for the study of neuronal connectivity.
A study of potential connectivity among neurons was done based on neuron reconstructions of the cortical circuits in cat primary visual cortex (Stepanyants et al., 2008).
Probabilistic synaptic density maps were derived from the neural circuitry of the olfactory centers of drosophila to predict connectivity (Jefferis et al., 2007).
Drosophila olfactory circuits were mapped and rendered to study the mechanisms of olfactory coding in higher brain centers (Lin et al., 2007).
Significant research efforts have been attempted to extract the geometric structure of axons from microscopic image stacks.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:01 12/5/2010 Bioinformatics-btq179.tex] Page: i65 i64i70 Reconstruction of the neuromuscular junction connectome Some of the algorithms proposed in literature that are directly related to the work in this article are the Kalman filter-based axon tracking (Jurrus et al., 2006), 3D template matching approach to detect neuronal structures (Al-Kofahi et al., 2002), 3D rayburst sampling for detection of neuron structures (Rodriguez et al., 2006), voxel coding for detection of centerlines from segmented neurons (Vasilkoski and Stepanyants, 2009) and use of generalized cylinders for semi-automatic 3D reconstruction of neurons (Schmitt et al., 2004).
As the problems exhibited by our datasets are completely different from the datasets for which these methods were proposed and due to the sheer complexity of the datasets presented in the articl, none of these methods can be directly applied to label individual axons.
The authors labs have also published their research work in this long-standing problem of extraction and visualization of the geometrical features and topological characteristics of axons.
Repulsive force based snake algorithm (Cai et al., 2006) was proposed to solve the close lying axons in a sequence of crosssectional images.
Although this method works for axons that run approximately perpendicular to the cross-sectional plane, it is unsuitable to complicated structures.
Probabilistic region growing algorithm (Srinivasan et al., 2007) was proposed to optimize the time taken to track the axons by utilizing a pseudo-3D approach.
This method tracks the centerlines of the axons in the maximum intensity projection (MIP) image and switches to the cross-sectional domain in case of ambiguity.
This method is suitable for datasets where the axons are somewhat distinguishable in the MIP image and is also unsuitable for use in the datasets presented in this article.
Dynamic programming algorithm (Zhang et al., 2008) introduces an approach to track the axons using constraints for smoothness and continuity in the cross-sectional images along with marker-controlled Watershed algorithm.
This approach also requires the axons to run in bundles and roughly maintain their orientation throughout the dataset, and hence, cannot be directly applied to our case.
Semi-automated reconstruction in serial images (Lu et al., 2009) was introduced as an extension to the already existing software; Reconstruct (Fiala, 2005), to interactively segment the axons in a sequence of cross-sectional images.
This approach requires a re-sampling of the datasets so that majority of the axons appear in the cross-sections.
This is not always achievable, especially in the datasets presented in this article, as the axons do not exhibit a specific orientation pattern.
Freely available image processing software, such as ImageJ (Abramoff et al., 2004) and Reconstruct (Fiala, 2005), either deal with 2D images/sequence of 2D images in a semi-automatic fashion or fail in case of sharp change in axon orientation, which makes it extremely time consuming and infeasible to track complex structures.
On the other hand, commercially available software, such as Imaris (http://www.bitplane.com/go/products/imaris), assumes the dataset to have an excellent contrast and cannot scale up for tracking large neuronal circuitry.
Due to the unpredictable orientation change, connection complexity and low-contrast signal of axons, none of current methods or approaches offer an efficient pipeline for tracking and reconstructing wiring diagram of axons in a large-scale, quantitative neurobiology studies.
In this work, we introduce a new tool, AxonTracker-3D, which does not require any parameter adjustment by the user and can drastically reduce the effort and time required to obtain valuable quantitative information from volumetric axon imagery.
From our study, the AxonTracker-3D, which incorporates a 3D, real-time tracking approach based on diffusion, greatly reduces the time required in processing the large volumes of axon images from months via manual analysis to minutes on personal computers.
2 METHODS The interscutularis muscles were removed from thy-1-YFP-16 transgenic mice (Feng et al., 2000) for diffraction-limited confocal imaging of entire motor axonal connectomes.
For identification of muscle fiber type, muscles were removed and post fixed with 1% PFA for 7 min, frozen and sectioned at 20 m using a Leica Cryostat (Leica, Germany).
Following that, sections were incubated with blocking solution (2% BSA+ 1% goat serum + 0.3% triton) at 25C for 3 h, and incubated with monoclonal antibodies against myosin type I and 2A (mouse anti-myosin I IgG1, 1 : 20, Novocastra, UK; mouse anti-myosin 2A IgG1, 1:10, Iowa Hybridoma Bank, USA) at 4C for 68 h. After several washes in 0.1% PBS-triton, samples were incubated with secondary antibody (Alexa-488 anti-mouse IgG1, 1:1000; Molecular Probes, USA) for 3 h. Finally, muscle sections were rinsed in PBS (25C, 20 min 2) and mounted on slides with Vectashield mounting medium.
The motor neurons expressed yellow fluorescent proteins (YFP) uniformly inside each cell.
The YFP was excited with a 488 nm line of argon laser using a 60 (1.4NA) oil objective and detected with a 520550 nm bandpass emission filter.
The images were sampled at Nyquist limit in the xy direction and oversampled by a factor of 1.5 in the z direction (voxel size = 0.1 m 0.1 m 0.2m) with a 12 bit dynamic range.
In order to obtain the complete connectome (consisting of 200 stacks), the motorized stage controlled by the MutliTimeZ macro, developed by Zeiss, was used.
3 ALGORITHM AND IMPLEMENTATION 3.1 Workflow and user interface Tracking and segmentation are the two major steps in solving the connectome and extracting quantitative information from these datasets.
In this article, AxonTracker-3D, a tool incorporating the tracking and visualization of axons, has been developed to facilitate the connectome function analysis in large-scale, quantitative neurobiology studies.
The datasets are processed oneby-one and are then stitched together to form a collage for further analysis.
A segmentation approach is also proposed, which uses the tracking results from AxonTracker-3D.
The workflow is described in Figure 1.
Fig.1.
Workflow of the proposed method.
i65 [11:01 12/5/2010 Bioinformatics-btq179.tex] Page: i66 i64i70 R.Srinivasan et al.Fig.2.
Tracking interface of AxonTracker-3D: the interactive visualization window is shown with the axons in a dataset.
One of the axons that have been tracked is shown in red.
Provision for manual intervention is provided on the left side of the interface.
The centerline in the visualization window is updated in real-time as the tracking is being done.
The centers can also be seen in the three orthogonal views (shown in Fig.3) in real time.
The user first loads the dataset that needs to be tracked, which is followed by preprocessing for noise removal.
The user then selects starting points for all the axons present in the dataset followed by the initiation of the tracking algorithm.
The initialization of the algorithm consists of computation of the candidate center points for all the axons present in the dataset, which is done automatically.
As the tracking proceeds, the results can be viewed as an overlay on the rendered data in 3D in real time.
In case of a problem in tracking due to low contrast, dramatic change in the orientation of the axons or due to very low voxel intensity, the algorithm stops and prompts the user for correction and guidance.
The interactive tracking has the provision of deletion of wrongly tracked points and addition of new points to the centerline.
After the user guides the algorithm by clicking a few new points along the centerline, the algorithm resumes the tracking process.
Once the tracking is complete, the datasets are stitched together to obtain the big picture (the connectome).
The centerlines can be used to segment the axons and visualize them in 3D space.
The user interface of AxonTracker-3D, which has been developed using C++, ITK, VTK and QT on a PC with 1.86 GHz Intel Core2 CPU and 2 GB of RAM configuration, is shown in Figure 2.
In a few cases when the algorithm fails to find the correct center due to bad contrast between adjacent axons or low intensity, a provision for manual intervention is provided in the software, using which, the user can add or delete the center points of the axons to guide the algorithm.
AxonTracker-3D can display the three orthogonal views of the current location of the centerline along with a visualization of the tracking results (Fig.3).
The user can then add or remove voxels from the centerline, hence guiding the software to accurately track the centerlines of the axons.
After all the axons in the dataset are tracked, segmentation can be performed.
Fig.3.
The orthogonal views of a dataset showing provision for editing the centerlines detected by the algorithm.
3.2 Centerline extraction The raw image datasets obtained are prone to noise and need to be preprocessed.
We use anisotropic diffusion image filtering (Perona and Malik, 1990) to reduce the noise while preserving contrast near the edges of the axons.
In order to extract the structures from the datasets for quantitative analysis, centerlines or the backbones of the objects have to be extracted first, which can then be used for segmentation.
The process of centerline extraction involves the estimation of most likely voxels in the dataset that belong to the medial axis of the axons.
From the initial point (defined manually), we employ a tracking method to extract the centerlines of each axon present in the dataset.
In order to compute the candidate center points of the axons, we define a scoring method for the voxels in the dataset using a combination of the gradient vector flow (GVF) (Xu and Prince, 1997) and object orientation vectors.
In their original work, these forces were generated to guide the initial contour to the boundaries of the objects.
We use the inverse of the approach where the forces help the boundaries to collapse to the center.
To generate these candidate points, we first compute the GVF in all the cross-sectional images along the three orthogonal directions, along the x-, y-and z-axes, respectively.
Each cross-sectional image along a particular axis is then binarized and the boundaries of the structures present in the cross-section are shrunk based on the forces of the GVF.
An example is shown in Figure 4.
As seen in Figure 4c, the resulting scores contain noise around the actual center and hence, non-maxima suppression is used.
Once the computation of possible centers along all three directions is complete, we merge them together using a directional vector-based approach.
The entire dataset is binarized and is first divided into smaller subsets.
In each such portion, the geometrical orientation vector of the binarized objects contained in the small dataset is computed.
The orientation of an object in 3D space can be computed using moment of inertia matrix.
The elements of the matrix are the moment of inertia the mass exhibits in 3D space about different combinations of the three orthogonal axes (x, y and z).
By computing the eigensystem of the matrix and by looking at the eigenvector with the least eigenvalue, we can determine the direction in which the mass exhibits the least amount of inertia.
In other words, this direction is nothing but the orientation of the mass present inside i66 [11:01 12/5/2010 Bioinformatics-btq179.tex] Page: i67 i64i70 Reconstruction of the neuromuscular junction connectome (a) (b) (c) Fig.4.
Identification of candidate centers: (a) raw image, (b) gradient forces shown by vectors, and (c) candidate centers found by collapsing the boundary of the binarized version of (a).
the sub-volume.
We first compute the moment of inertia matrix as: I = Ixx Ixy Ixz Iyx Iyy Iyz Izx Izy Izz  where Ixx = rR V (r ) (d (r )2y +d (r )2z ) and similarly Iyy and Izz are defined; Ixy = Iyx = rR V (r )d (r )xd (r )y and similarly, Iyz = Izy and Izx = Ixz are defined in a symmetrical way; R is the set of all the voxels in the binarized volume under consideration; d(r) = [d(r)x , d(r)y, d(r)z] is the distance of the voxel, r, from the center of mass of the binarized volume; andV (r) is the value of the voxel at r in the binarized volume.
We compute the eigenvalues and the corresponding eigenvectors of the system and select the eigenvector with the lowest absolute eigenvalue as the orientation of the object in the sub-volume.
Intuitively, based on the projection of this vector along the three axes (x, y and z), the three scores along orthogonal directions are combined using: ScoreGVF = txScorexGVF +tyScoreyGVF +tzScorezGVF, where t = [tx,ty,tz ] is the orientation vector of the sub-volume.
This score is an indication of likelihood of a voxel belonging to the axon centerline.
Figure 6bd are examples of candidate center points calculated using GVF along the x, y and z-axes for a dataset.
We can see these candidate center points are very close to actual center points even in a complicated dataset shown in the figure.
Figure 6e is obtained after combining the scores in (bd).
Once the candidate centerlines are computed, the tracking process starts with the manually chosen initial point for each axon.
The centerlines of the axon are traced using a moving sphere whose center always lies on current center of the centerline that has been traced so far.
To compute the next center on the centerline, the directional vector for the sphere (computed using the current center and nine previous centers) is computed.
The radius of the sphere in our study has been set to five voxels.
The search for the next center is performed in a preset angle range (set to 45 in our case) about the directional vector inside the sphere so that the centerline does not trace back itself.
In each step, inside the search region, the voxel with the highest GVF score is chosen as the next center.
If many Fig.5.
An illustration demonstrates the tracking process inside the axon.
voxels are found to have the same value, we resolve them using: Ci+1 =argmin rR { a ( 1Gr )+b(Dr )} , where Ci+1 is the (i+1)-th center (or the new center on the centerline to be determined); R is the set of all the candidate voxels that need to be resolved; Gr is the normalized value of the maximum value of gradient along the path connecting Ci (current center on the centerline) and r; which Dr =ti ti+1 is a smoothness term based on the dot product of the directional vectors (the directional vector of the sphere and the directional vector formed between the current center and the voxel under consideration respectively); a and b are the weights that denote accuracy and smoothness of the centerline.
In our study, these parameters have been set as 0.65 and 0.35, respectively.
An illustration of the tracking algorithm is shown in Figure 5.
The shaded conical region inside the spheres shown in Figure 5 are the search region at different points along the centerline.
3.3 Segmentation of axons In many cases, axons twist and turn, and lie close to each other in the image stack.
The challenges in segmentation include weak boundaries between close-lying axons and large intensity variations in some axons, which cause many segmentation methods to fail.
In order to deal with this problem, the segmentation scheme consists of two steps.
In the first step, local threshold 3D region growing is applied to get the rough segmentation result which will be used as the initial segmentation for the subsequent step.
The local region is defined as spheres with certain radius (set to two to five voxels in our study), and the centers of the spheres are located on these center points.
This radius was determined based on the radius of the thinnest axon present in the datasets.
Once the rough segmentation of axons is obtained, an interactive level set model (Yan et al., 2008) was adopted to detect boundaries in 3D space.
The interaction involves two types of mechanisms: repulsion and competition.
The repulsion term is for separating the touching axons and the competition is for defining the axon boundaries.
These mechanisms were formulated as an energy functional, which is then minimized by using the multiphase level set method.
Figure 6g shows the segmentation result obtained using the centerlines of the axons from Figure 6f.
i67 [11:01 12/5/2010 Bioinformatics-btq179.tex] Page: i68 i64i70 R.Srinivasan et al.(a)        (b)  (c) (d) (e)           (f) (g) Fig.6.
Computation of center point candidates using GVF.
(a) MIP of the original dataset.
(bd) MIP of the GVF dataset computed along the x-, y-and z-axes, respectively.
(e) Visualization (3D) of the combined GVF datasets using vector approach.
(f) Centerlines computed by our tracking algorithm based on the centerline candidates.
(g) 3D rendering of the segmentation results using the centerlines.
3.4 Stitching of datasets After the datasets have been segmented, they are stitched together to form a collage.
As the datasets are contiguous with a variable amount of overlap between them, we process the dataset stack by stack to form a montage by using the normalized cross-correlation measure to determine contiguous stacks and the extent of overlap between them.
The cross-correlation measure between an image, I , and a template, t, is defined as: C (u,v )= x,y [I (x,y ) Iu,v ] [t (xu,yv ) t ] x,y [I (x,y ) Iu,v ]2 x,y [t (xu,yv ) t ]2 , where (u,v) is the coordinate where the template is centered at; I(x,y) is the image region under the moving template; t is mean of the template; and Iu,v is the mean of the image region under the template.
The cross-correlation measure has been applied in xy plane and then in xz plane to complete the alignment in 3D.
This reduces the computational time involved in alignment as opposed to a completely 3D alignment.
As the datasets were down-sampled by a factor of 2, an alignment error of one voxel is unavoidable during alignment.
To record morphological properties of neuronal processes, images were acquired from neonatal mice using laser scanning immunofluorescent confocal microscopes.
The connectome of interscutularis muscle of the mouse was used for this study.
The complete connectome consisted of 200 data stacks.
Adjacent stacks had an overlap of around 10% for alignment to form the montage.
The data acquisition lasted around 2448 h and accumulated 50 GB of data.
The datasets that did not contain any axons were discarded.
The resulting set of datasets contained 154 stacks which amounted to around 35 GB of data.
The volumetric datasets are a large number of 3D axon image stacks, each typically being 1024 1024 (100200) voxels in size (voxel size of 0.1 m 0.1 m 0.2 m), which when put together form a collage of the connectome.
The datasets were down-sampled along the xy direction, yielding datasets that were 512 512 (100 200) voxels in size (voxel size of 0.2 m 0.2 m 0.2 m), which improved the tracking and segmentation speed without losing accuracy and lowered the computer memory requirement.
Figure 7 shows the tracking and segmentation results of 30 contiguous stacks from the collage.
The centerlines were tracked and segmented automatically using AxonTracker-3D.
The datasets were then stitched together using the cross-correlation measure and are visualized together as shown in Figure 7b.
The results show that touching axons and axons with short turns can be accurately extracted by the software.
On an average, each axon takes <30 s to track if no manual intervention/correction is required.
Owing to the complexity of axon, each dataset had 2540% of the axons that needed manual correction at three to four places for tracking.
The average reconstruction time for one image stack is 1015 min.
Once tracked, each dataset in the collage took 5 min for segmentation.
4 DISCUSSION In this article, we presented a novel interactive tool to extract and reconstruct axons obtained from 3D confocal microscopy image stacks.
Compared to the previously proposed methods, our algorithm has two major contributions: firstly, the software can successfully extract centerlines of axons with minimal user intervention.
Rendering of the data in 3D along with overlay of tracking results in real-time is a useful feature offered by the software.
By introducing user interaction, we can see that our algorithm can successfully extract and reconstruct touching axons and axons with dramatic orientation changes.
Secondly, by using local region growing and interactive level set segmentation methods, we can get the actual boundary of each axon which can provide us valuable quantitative information such as the length, radius for axon function and connectivity analysis.
As a continuation of the work presented in this article, we will discuss algorithms to detect and track branches in order to minimize user-intervention needed for AxonTracker-3D.
The following are the planned improvements to the current version of the software.
Axons in the connectome of neuro-muscular junctions tend to branch into complex patterns.
The current version of AxonTracker3D does not have a provision to automatically detect branches.
At branch points, the centerlines were joined together manually to complete the wiring circuitry of the axons.
As an extension to our current work, we plan to design and implement new algorithms to detect branching points along the axons.
Methods such as AdaBoost classifier (Freund and Schapire, 1999) could be a potential solution to this problem.
This consists of three steps: (i) re-slice the axon tubes along its orientation; (ii) extract 2D and 3D features from the slices and spheres rounding the center points; (iii) select samples to train AdaBoost classifier (manual selection of 100 samples for training).
The AdaBoost method can be used to classify positive training examples from negative i68 [11:01 12/5/2010 Bioinformatics-btq179.tex] Page: i69 i64i70 Reconstruction of the neuromuscular junction connectome (a) (b) Fig.7.
(a) An example shows 30 stacks aligned automatically.
(b) Segmentation and labeling using AxonTracker-3D.
examples by selecting a small number of critical features from a huge feature set previously designed and creating a weighted combination of them to use as a strong classifier to detect the branch points along axons.
We will include a provision for manual intervention in branch detection as well, but the need for it will be rather minimal.
AxonTracker-3D has been designed such that the user interface and interaction is kept as simple, intuitive and user-friendly as possible.
Any addition or modification to AxonTracker-3D modules will not affect the basic work-flow as described in Figure 1.
The current version of the software does not make use of multithreading, and hence there is a good scope of increasing the execution speed manifolds by making use of multiple CPUs if available on the PC.
As the size and number of the datasets in the collages are huge, a major improvement to the existing software would be to include a provision for batch processing.
The datasets in the collage could be processed automatically as much as possible to reduce the time and effort needed for tracking the connectome.
Along with this, the existing algorithms will also be improved to increase automation without compromising accuracy of results.
Most importantly, using the quantitative data available from tracking and segmentations of complete connectomes, many biological questions can be generated, such as: (i) the relationship between the number of branches and the length of each segment in a population of axons; and (ii) the spatial distribution of motor neurons or the optimal layout of the motor neurons in a population of axons.
Questions such as whether the spatial distribution of the axons are random in nature or follow a certain pattern can be answered.
We intend to employ data mining and biostatistics methodologies on the datasets and wiring diagrams extracted from AxonTracker-3D to address these questions.
ACKNOWLEDGEMENTS The authors would like to thank various members of the Wong lab at The Methodist hospital Research Institute and of the Lichtman lab at Harvard University for their technical comments over the years on the AxonTracker-3D project.
Funding: This research is funded by grants from Ting Tsung and Wei Fong Chao Center for Bioinformatics Research and Imaging in Neurosciences (BRAIN), The Center for Bioengineering and Informatics at The Methodist Hospital Research Institute, and Harvard Neurodiscovery Center (previously Harvard Center for Neurodegeneration and Repair) (Wong).
Conflict of Interest: none declared.
ABSTRACT Summary: GLay provides Cytoscape users an assorted collection of versatile community structure algorithms and graph layout functions for network clustering and structured visualization.
High performance is achieved by dynamically linking highly optimized C functions to the Cytoscape JAVA program, which makes GLay especially suitable for decomposition, display and exploratory analysis of large biological networks.
Availability: http://brainarray.mbni.med.umich.edu/glay/ Contact: sugang@umich.edu Received on March 30, 2010; revised on October 11, 2010; accepted on October 15, 2010 1 INTRODUCTION With the rapid development in experimental and computational technology, the scale and dimension of accumulated molecular interaction data have increased dramatically.
Many online repositories, such as Michigan molecular interaction (MiMI; Tarcea et al., 2009), have made extensive gene-wise interaction data readily available.
The challenge is then how to systematically explore and visualize such large and complex datasets for biological inferences.
One solution is to decompose such an interaction network into communities of densely interacting nodes and imply functional modules.
A variety of community detection algorithms have been developed to tackle similar challenges in social networks and they have been successfully extended to the biological context (Schwarz et al., 2008; Viana et al., 2009).
Recently, Ruan et al.(2010) proposed an interesting generic method combing association networks with community structure detection algorithms to infer network modules from microarray data.
Cytoscape is a well-established open source software foundation for analysis and visualization of biological networks.
Currently there are several plugins developed for clustering and functional module detection, such as MCode (Bader and Hogue, 2003), NeMo (Rivera et al., 2010) and ClusterMaker (http://www.cgl.ucsf.edu/ cytoscape/cluster/clusterMaker.html).
However, some algorithms in ClusterMaker, such as kmeans or hierarchical, require the network to have numerical attributes to compute a distance matrix for clustering.
MCode and NeMo are engineered to identify small and highly intra-connected clusters in a network, without clustering all the nodes.
For example, when executed on a MiMI human To whom correspondence should be addressed.
interactome network of 11 884 nodes and 88 134 edges using the default parameters, MCODE produced 105 clusters, in which 52 clusters contain less than five nodes.
Therefore, it may not be suitable for global subdividing large networks for exploratory analysis.
In addition, some of these plugins were not tailored for large networks.
For example, NeMo failed when executing on the same MiMI network on a 2.67 GHz Intel Core i7 machine.
So far, no plugin offers a comprehensive collection of highly efficient community detection algorithms, which could profoundly improve cluster analysis if added to Cytoscape.
The increasing size and complexity of networks also bring significant challenges to visualization.
Generating a layout on such a network not only consumes considerable time and computational resources, but also rarely produces any informative outcome.
A typical case is a massive hairball as a result of applying forcebased layout to a large network (>500 nodes) with many edges (Merico et al., 2009).
Visual separation of clusters in a network can be improved by overlaying community structure on a graphic layout addressing specific topology.
We therefore developed this Cytoscape GLay plugin to make commonly used community structure detection algorithms available.
GLay also provides layout algorithms optimized for large networks.
GLay not only supplements existing clustering functions, but also provides structured and informative visualization for more efficient exploration and analysis of large biological networks.
2 IMPLEMENTATION The core of GLay was developed as a Cytoscape plugin with highperformance community analysis and graph layout functions ported from igraph C library (Csardi and Nepusz, 2006).
The bridging is built via Java native access (JNA, https://jna.dev.java.net) interface.
The functions ported from igraph C library are currently only compiled under Windows 32/64 bit platform but will be extended to other platforms in the near future.
Before performing any community analysis, GLay automatically transforms the input network into a simplified model, with edge directionality, duplication and self-looping removed.
Such a network standardization step will make the resultant community structures from different community structure detection algorithms comparable as well as improving performance.
Upon completion of an analysis, the user may browse the resultant community structure with the built-in GLay navigator panel.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:15 23/11/2010 Bioinformatics-btq596.tex] Page: 3136 31353137 G.Su et al.Table 1.
GLay community algorithms Connected components Find connected clusters from a network Edge betweenness (Newman and Girvan, 2004) Optimization of modularity score utilizing edge betweenness score Fast-greedy (Original, HE, HN, HEN) (Clauset et al., 2004; Wakita and Tsurumi, 2007) Greedy optimization of modularity score, with different corrections on edge density and cluster size Label propagation (Raghavan et al., 2007) Determine community membership by iterative neighbor votes Leading eigenvector (Newman, 2006) Find communities using eigenvector of matrices Spin glass (Global, Single) (Reichardt and Bornholdt, 2006) Using spin glass model and simulated annealing.
The single mode allows finding communities only surrounding selected nodes Walk trap (Pons and Latapy, 2005) Determine community membership via short random walks Table 1 summarizes the incorporated community detection algorithms.
Because of the distinct heuristics of algorithms, running speed and the resultant community structures vary.
Some algorithms, such as the leading eigenvector algorithm, works well on a small network of a few hundred nodes but may not be scalable for large networks.
Others are optimized for large datasets but may be less accurate.
For example, the fast greedy algorithm may produce communities with skewed community size distribution because of the greedy optimization of the modularity score (Wakita and Tsurumi, 2007).
Users may test different algorithms and evaluate performance by various benchmarks such as modularity, number of communities and community size distribution.
Table 2 lists GLay layout algorithms.
These algorithms are able to efficiently layout very large networks or generate hierarchical trees.
A key advantage of GLay layout is that it allows the layout calculations of various algorithms to initiate from the current network layout state.
This adds significant flexibility since it enables the user to progressively improve the layout by either finetuning parameters or using different layout algorithms together.
For example, for a very large network, the user may specify a small number of iterations to obtain a draft layout, and then gradually refine the layout by adding more iterations or tuning the parameters.
Once done, the user may superimpose the community structure on the layout to investigate network topology.
For more information, please refer to the plugin homepage and igraph library documentation (Csardi and Nepusz, 2006).
3 RESULTS AND CONCLUSION We have tested GLay on datasets of various size and structure.
GLay demonstrated substantial performance gain in both network decomposition and layout over existing Cytoscape solutions.
For example, using GLay to subdivide the MiMI human Interactome which contains 11 884 nodes and 88 134 edgestakes 0.7 s using the label propagation algorithm and 20 s using the fast greedy algorithm on the same 2.67 GHz Intel Core i7 machine.
MCODE takes 198 s to find clusters.
Generating layout on this network using the Fruchterman Reingold grid algorithm takes about 20 s, Table 2.
GLay layout algorithms Fruchterman Reingold (original, grid) (Fruchterman and Reingold, 1991) Efficient force-based algorithms, with the grid version optimized for large networks graphopt (GraphOPTForce-based algorithm with optimization Kamada kawai Force-based spring layout Large graph layout (Adai et al., 2004) Large graph layout algorithms for connected graphs Multidimensional scaling (MDS) (Brandes and Pich, 2007) Layout based on multidimensional scaling based on shortest distances reingold tilford (hierarchical, circular) (Reingold and Tilford, 1981) Tree-like layout for connected networks, can be hierarchical or circular from any node as root Fig.1.
Fast-greedy community structure superimposed on Frutcherman Reingold grid layout from the largest component of Cytoscape human BIND dataset, consists of 17 961 nodes and 30 156 edges.
Note that nodes belong to the same community tend to aggregate spatially, which resulted in clusters with good visual separation.
The red circle indicates a group of highly interacting immunoglobulins.
whereas the Cytoscape built-in force directed and spring embedded algorithms both reported error during execution both with default setup and 1.5 G heap space.
This demonstrates that Java-C hybrid model has dramatic performance advantage handling large networks in Cytoscape.
GLay also enables easy navigation of clustering results.
Figure 1 shows a screenshot of overlaying fast greedy community structure on Fruchterman Reingold grid layout on the Cytoscape built-in BIND human dataset.
Users may navigate and explore communities of genes with the GLay browser.
For example, clicking the cluster entry in the browser table will select all nodes within a cluster.
The user will then be able to create a new subnetwork or nested network from the selected nodes, extract gene lists from attribute browser or incorporate other experimental data for various research interests.
In addition, GLay can provide qualitative different results from existing solutions.
Figure 2 shows a side-by-side comparison of MCODE at default parameters and GLay using fast greedy algorithm.
It can be seen that by using the default parameters, MCODE produces much smaller clusters than GLay, leaving majority of the nodes unclustered.
Therefore, GLay outperforms 3136 [11:15 23/11/2010 Bioinformatics-btq596.tex] Page: 3137 31353137 GLay Fig.2.
Comparison between clusters produced by MCODE with default parameters (left) and GLay using fast-greedy algorithm (right) on Cytoscape bundled galFiltered (Ideker et al., 2001) dataset.
The node color is determined by the corresponding cluster membership.
Left: MCODE clusters.
The unclustered genes are hidden.
Right: GLay fast-greedy clusters.
(A) A MCODE cluster, in which four out of five genes are associated with MAPK pathway.
The corresponding cluster in GLay contains 25 genes, including more genes in MAPK pathway, cell cycle and ion binding.
(B) A GLay cluster not identifiable by MCODE.
This cluster consists of six genes, with four are related to RNA process.
MCODE in terms of structural partitioning of the original network.
In addition, overall GLay has higher sensitivity than MCODE at the trade-off of specificity, which made it more suitable for functional interpretation.
For example, in Figure 2, one cluster in MCODE contains five genes, with four genes function in MAPK pathway.
The equivalent GLay cluster contains 25 genes.
Submitting these genes to DAVID (Dennis et al., 2003) reveals one enriched functional cluster for the MCODE cluster and nine enriched functional cluster for the GLay cluster.
As some of the genes such as cdc28 and ste12 are involved in multiple regulation processes, the GLay cluster recovered more biological-relevant information than the equivalent MCODE cluster.
In summary, GLay capitalizes on the power of highly optimized C code from several social network analysis and network layout algorithms to improve scalability of Cytoscape for large networks.
We hope GLay can help to address the increasing needs for analysis and visualization of large-scale networks.
We are committed to add cross-platform support for Linux and Mac environments as well as to integrate novel network analysis and layout functions in GLay.
ACKNOWLEDGEMENTS We thank the igraph developers Gabor Csardi and Tamas Nepusz, and the JNA community for enormous help during the development.
We also thank Jing Gao for providing Interactome data from MiMI and user testing.
We appreciate the Google Summer of Code which provided great opportunity for the initial phase of this project, Samad Lotia from Agilent Technologies for helping with building the plugin on Linux platform, and Josh Bucker for proofreading the manuscript.
Funding: This work is supported by National Center for Integrated Biomedical Informatics through National Institutes of Health (grant 1U54DA021519-01A1 to the University of Michigan), also partly supported by a NIH NCRR grant P41-RR01081 to the University of California, San Francisco.
Conflict of Interest: none declared.
ABSTRACT Motivation: Describing biological sample variables with ontologies is complex due to the cross-domain nature of experiments.
Ontologies provide annotation solutions; however, for cross-domain investigations, multiple ontologies are needed to represent the data.
These are subject to rapid change, are often not interoperable and present complexities that are a barrier to biological resource users.
Results: We present the Experimental Factor Ontology, designed to meet cross-domain, application focused use cases for gene expression data.
We describe our methodology and open source tools used to create the ontology.
These include tools for creating ontology mappings, ontology views, detecting ontology changes and using ontologies in interfaces to enhance querying.
The application of reference ontologies to data is a key problem, and this work presents guidelines on how community ontologies can be presented in an application ontology in a data-driven way.
Availability: http://www.ebi.ac.uk/efo Contact: malone@ebi.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on November 24, 2009; revised on February 4, 2010; accepted on March 1, 2010 1 INTRODUCTION The description of experimental variables, even within a single discipline, involves the use of many cross-domain concepts.
For example, describing the characteristics of a single sample in an experiment can use terminology from cell biology, proteomics, transcriptomics, disease, anatomy and environmental science.
This is not a new problem and it is not restricted to bioinformatics.
However, it is pressing within this domain due to the quantity of heterogeneous data available in different formats across multiple resources (Schofield et al., 2009).
The desire to integrate data generated with different experimental technologies and in different biological domains motivates our work.
Experimental descriptions are captured and made available as text within database records, published papers and web site content.
These descriptions contain latent semantic information that is hard to extract and reflects the natural language of the domain.
One solution to this problem is the use of a controlled vocabulary to describe the data.
With this approach, the terminology used in a particular context is restricted to a set of terms that define important aspects of a domain or application.
Ontology adds an extra layer of expressivity by To whom correspondence should be addressed.
structuring this vocabulary into ontological classes and by specifying the sorts of operations that can be performed on them.
Importantly, the ontological models produced from this process are expressed in a language that enables human understanding and computational reasoning over the representation.
Languages such as the W3C recommendation Web Ontology Language (OWL) (Horrocks et al., 2003) aid interoperability by standardizing the syntax across all domains.
Advantageously, validation of this OWL representation can also be performed through the use of description logic reasoners (Sirin et al., 2007).
In bioinformatics, the interest in ontologies to model domain knowledge is apparent from the steadily increasing number of groups developing them.
In an attempt to align these efforts, the OBO Foundry (Smith et al., 2007) provides useful guidance on best practice for developing ontologies in the biomedical domain.
This includes the creation of orthogonal reference ontologies, from which classes are considered defining units of the area they describe.
Although this is a worthwhile longer term aim, the state of the art is that existing ontologies are not orthogonal or interoperable, and many present a focus that is unsuitable for gene expression data.
They can, however, be used to construct application ontologies that focus on describing and structuring a data space for a particular application.
While a vision of full interoperability between ontologies overcomes some of the barriers to integration, there still remain unresolved issues for data-driven applications.
Cross products, i.e.classes composed of two or more existing classes (formally in OWL, the intersection of two or more classes), are required between existing ontologies to more accurately describe omics data.
For example, a cell type in a given tissue or the transcription factors within a pathway activated in a disease state.
Few cross products are available to date partly because many ontologies do not use a common upper level ontology.
Where there are non-orthogonal ontologies, those that best describe a dataset of interest typically do not have the necessary cross products.
Furthermore, combining even ontologies that are interoperable can present problems.
Ontologies such as FMA (Rosse and Mejino, 2003) contain tens of thousands of classes, combined with other ontologies such as Gene Ontology (GO) and Disease Ontology (Osborne et al., 2009), and this presents a large model to consider; this is a particular problem if description logic reasoners are used for consistency checking and inference.
The use of multiple ontologies to annotate experimental data brings with it a considerable overhead.
Consider an annotation example, where a biological user submitting data needs the term lymphoma.
BioPortal (Noy et al., 2009) returns 629 matches from 24 ontologies.
The casual user is not equipped to select from these The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1113 11121118 Experimental Factor Ontology non-orthogonal ontologies and selecting a more specific child term is more problematic; the Disease Ontology alone has 16 subclasses.
In other cases, such as genetic disease, many diseases are not present in the Disease Ontology or SNOMED despite their large size.
Inconsistent use of synonyms also presents a problem, as synonyms are often for a more or less granular term in another ontology.
Another consideration is that ontologies change over time and managing this in the context of annotations is problematic.
Mechanisms are therefore required to help manage this.
Representation of biological ontologies is necessarily complex as they have multiple purposes; explicitly modeling biological relationships, aiding interoperability with other ontologies and facilitating data annotation, to name a few.
This complexity is a barrier to the consumer of ontology annotated data as they may be unfamiliar with the language, constructs and labels used.
Consider the class information content entity from Information Artifact Ontology (IAO) (http://purl.obolibrary.org/obo/iao) defined as an entity that is generically dependent on some artifact and stands in relation of aboutness to some entity.
Such a definition may be incomprehensible to a biologist, yet is an important class in Experimental Factor Ontology (EFO).
A user-friendly view on upper level ontology is thus required.
In this article, we describe our data annotation and query use cases.
We present an application ontology, the EFO, which fulfills the use cases in the context of gene expression data; the methodology and tools that we have developed to produce the ontology are also described and are freely available.
We also illustrate the novel cross-product classes that we create using reference ontologies.
Our application ontology provides a solution for integrating reference ontologies, extracting information from text, applying annotation and visualization of biological data.
1.1 Motivation: the Gene Expression Atlas The Gene Expression Atlas (Kapushesky et al., 2010) provides summaries of gene expression across multiple experimental conditions, called experimental factors.
It also provides a gene level view of experimental data acquired from ArrayExpress (Parkinson et al., 2009).
This data is manually curated to provide an explicit, consistent and homogenous description across a wide range of sample attributes, such as species, developmental stage, disease and tissue type.
Protocol parameters related to the processing of samples, such as application of chemical compounds and sampling times, are also needed.
As of November 2009, there are 40 000 unique annotations of sample or assay properties covering 330 species in datasets suitable for the Gene Expression Atlas.
Given the diverse nature of the annotations, there is a need to support complex queries that contain semantic information.
For example, the query, which genes are under-expressed in brain cancer samples in human or mouse, requires the querying mechanism to understand the term cancer.
Annotations made at the experimental level are necessarily granular in nature; an experiment where the sample is of adenocarcinoma will be annotated with adenocarcinoma rather than more generally cancer.
A database query requiring cancer would therefore not return annotations to adenocarcinoma since this requires additional knowledge.
An alternative solution would be to annotate this sample with adenocarcinoma and cancer and any other intermediate classifications such as carcinoma; however, this has a number of disadvantages.
First, this requires curation, a labor-intensive process.
Second, it embeds the semantics within the database, tightly coupling the data with the domain knowledge.
This makes the approach fragile, since a change or extension to domain knowledge may require a large database update.
It also limits reuse of the knowledge within other resources.
A better solution to this problem is to annotate data using ontologies.
This enables the separation of the formal description of domain knowledge, allowing reuse of these resources and improving interoperability with other data with similar semantic representations.
To annotate the diverse data in the Gene Expression Atlas, classes are required from multiple existing ontologies to capture the cross-domain nature of the data.
Initially, we limited scope to data generated to 12 species including: human, mouse, rat, Arabidopsis, budding yeast, fission yeast, Drosophila melanogaster, Caenorhabditis elegans and zebra fish.
These species have ontologies that describe anatomy and developmental stages, though the limitations of gene expression technology mean that only a subset of tissues or other variables are typically analyzed.
An important use case is the comparability between experiments, for example, where the same tissue, cell type, disease and developmental stage was studied across experiments and species and the data can be potentially combined.
Finally, name value pairs that could be mapped to existing domain ontologies were prioritized as these also cover the most common queries e.g.disease state, cell line, cell type developmental stage, etc.
Data in the gene expression domain are typically not mapped to an ontology at the point of submission, and neither Gene Expression Omnibus nor ArrayExpress use species-specific ontologies in their submission tools.
Requiring use of ontologies at this point is a barrier to data deposition, therefore, the majority of ontology mapping occurs after submission and is based on user-supplied name value pairs e.g.DiseaseState = breast cancer.
An important use case is text mining of data prior to its inclusion in ArrayExpress.
Exploratory analyses of the data prior to the construction of EFO revealed that many terms appear at high frequencies and there is a long tailon the data distribution (Malone et al., 2009).
For example, in the ArrayExpress archive 1350 samples have the annotation heart, 65 ventricle, 14 myocardium and a single annotation for pericardium.
Compare this with the representation of the human heart from the Foundational Model of Anatomy (FMA) (Rosse and Mejino, 2003) where there are >20 terms describing the various parts of the heart.
It is clear that comparatively few terms are needed to describe the data in the Gene Expression domain and that the complexity in FMA is not needed.
For both text mining and query purposes across free text in the data, there is a requirement for synonyms.
This includes local synonyms, e.g.whole brain, to detect user-defined annotation or to deal with alternate spellings.
Our approach for the gene expression domain therefore is analogous to that of the GO (Blake and Harris, 2008), which was initially developed to describe gene products for model organism databases; it has a data-driven motivation, with ontological principles such as use of an upper level ontology applied to provide robustness and to allow interoperability with other ontologies.
2 METHODS The EFO is an application ontologyan ontology engineered for domainspecific use or application focus and whose scope is specified through testable 1113 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1114 11121118 J.Malone et al.use cases and which maps to reference or canonical ontologies.
EFO was developed following the middle-outmethodology first described in Uschold and Grninger (1996) and later by (Gmez-Prez et al., 2004).
Ontologies, like software, should conform to a set of specifications and use cases, and can be tested using competency questions.
Use cases are used to determine the classes we include, and the relations, restrictions and axioms used in our ontology: (1) Data annotationgoal: the primary use case for this application is the annotation of transcriptomics data in the Gene Expression Atlas.
Task: this is a coverage use case, i.e.can we annotate all of the data we wish to associate ontology classes with?
(2) Query supportgoal: to enable querying across hierarchies for which data exists (and is annotated).
Task: enabling queries such as retrieve all cell line data that is derived from epithelial tissue and are associated with cancer.
(3) Data visualization and explorationgoal: to present a tree structure of annotated data within Atlas.
Task: presenting an ontology tree to the user to show which classes have associated data.
(4) Data integrationgoal: to allow integration of data both across experiments in Gene Expression Atlas and externally.
Task: integrating with external resources that use or map to the same ontology class and compare data from these independent sources.
(5) Data summarization and mininggoal: to obtain an analysis of samples, given common conditions of interest.
Task: provide a summary for gene expression data levels for samples treated across same condition, e.g.treated with bacterial toxins.
In addition to use cases, a list of competency questions allows us to evaluate at which point the ontology is able to satisfy the scope of the application (Stevens et al., 2000).
Examples include Which cell lines are derived from epithelial cells?
and which organism parts are parts of the forebrain?
As the ontology will be applied in the context of gene expression data, e.g.which genes in cancerous vs. normal kidney samples in humans show differential expression?, both an ontological query and a data-driven query in the context of an application are needed.
The ontology therefore should represent cancer, kidney and human to resolve this query while the differential expression is determined by the application of the ontology in the context of the data, and this competency question therefore demonstrates the application domain.
One approach to ontology development is the use of a modular methodology using a mixture of generic domain, generic task and application ontologies whose parts are clearly defined so that they can be reused (Stevens et al., 2000).
Our methodology reuses reference ontologies (full list available at http://www.ebi.ac.uk/efo/metadata), where they exist and where they describe classes that are in scope for EFO.
We also enrich these classes with additional axioms e.g.making associations between cell lines and their cell types of origin.
To promote interoperability with the OBO Foundry ontologies, we have selected BFO as an upper ontology; however, we use only a subset of its classes necessary to fulfill our use cases and we provide user-friendly class labels.
An outline of the high-level classes that structure EFO is illustrated in Figure 1.
The five primary axes used are as follows: information, site, process, material and material property.
Our ontology development methodology is as follows (complete process documents can be found at www.ebi.ac.uk/efo): (1) Extract data annotations from the Atlas.
Determine the depth and breadth of these annotations and target the most frequently occurring annotations.
(2) Identify OBO Foundry reference ontologies relevant to an EFO category based on annotation use cases.
(3) Use the query use cases obtained from analysis of query logs to build an appropriate hierarchy.
Fig.1.
EFO upper level structure used to organize the ontology with intermediate node examples.
Fig.2.
Separating the ontology layer (EFO) from the data (ArrayExpress) and the presentation layers (Atlas).
(4) Perform mapping between existing annotations and reference ontologies using the Double Metaphone phonetic matching algorithm.
This produces a list of candidate ontology class matches.
(5) Expert validation of candidate matches, curate and include matched classes into the EFO hierarchy with appropriate intermediate nodes.
Adding classes takes two forms: Where there is no overlap between reference ontologies, import the class directly into EFO [maintaining the original Uniform Resource Identifier (URI)].
Where overlap exists, create a new EFO class (with EFO URI) as a mapping class and add annotation properties with URIs of all mapped classes.
(6) Perform mappings to other reference or application ontologies where these are not provided by the source ontology.
(7) Add structure to EFO to provide an intuitive hierarchy with userfriendly labels and add restrictions to add value e.g.associate cell lines with cell types and tissues of origin.
The strategy of decoupling the data, the presentation layer and the semantic layer is illustrated in Figure 2.
Using EFO as a separate layer in our application means we are able to effect changes to the ontology, such as adding new classes or new class relations, without modifying the underlying data or the presentation layer and manage changes in reference ontologies cleanly.
A further advantage of this approach is that the ontology can be reused without imposing any special requirements on the implementation or on the application presentation layer, thereby enabling EFO to be used in other applications and expanded accordingly.
Our methodology also aims to observe OBO Foundry best practice guidelines.
A set of OWL annotation properties are used to capture metadata 1114 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1115 11121118 Experimental Factor Ontology about the classes; we add human readable labels and use univocal and consistent syntax for class names.
Metadata details for EFO can be found at http://www.ebi.ac.uk/efo/metadata.
We also use the Relation Ontology (RO) (Smith et al., 2005).
There are relations that are not captured by RO (such as those used in OBI), and therefore, we extend RO where necessary.
Our intention is to integrate with future, richer versions of RO when available.
2.1 Detecting external ontology changes Ontologies that are used within biology evolve rapidly due to scientific advances and because the associated computational technologies are themselves rapidly evolving (Smith et al., 2007).
Because we consume from multiple ontologies, class information must be maintained and updated.
This problem is no more severe than if we mapped data annotations to each ontology separately, rather than to EFO that maps to external reference ontologies.
Here we list the changes in external ontologies which affect EFO: (1) An axiom is added to an existing named class.
(2) An axiom is removed from an existing named class.
(3) A new named class is added to the ontology.
(4) A named class is made obsolete.
(5) An annotation property is edited on a named class.
The OWL-API (Horridge et al., 2009) provides a Java-based interface which allows manipulation of OWL ontologies at the axiom level.
Therefore, comparing two different versions of OWL ontologies in an axiom-based approach, as seen in the OWL-API, can be achieved using a set difference operation.
In set theory this is given by a relative complement.
Formally, for sets A and B the relative complement of A in B, that is, the set of elements in B, but not in A, is given as: B\A={xB|x/A} (1) Given two sets of axioms, A and B, and axiom an: A = {a1,a2,a3,a4},B = {a1,a2,a3,a5} B\A = {a1,a2,a3,a5}\{a1,a2,a3,a4} = {a5} For a set of axioms which are equal: A = {a1,a2,a3,a4},B = {a1,a2,a3,a4} B\A = {a1,a2,a3,a4}\{a1,a2,a3,a4} = (2) A=B (3) We can use this information to deduce that no changes have occurred between ontologies and moreover to infer that the classes A and B are logically equivalent.
We have designed a freely available tool, Bubastis, to analyze and report on the five major types of ontology changes we enumerate.
Specifically, we extract the classes mapped to EFO and check for changes.
A log of any changes is created along with relevant time and date stamps and a report generated.
Usefully, if there are no changes the tool will automatically report this too.
This approach allows us to computationally manage the imports and mappings we create within EFO, ensuring they are valid and reducing the overhead on ontology curation.
It also allows us to manage remapping data annotations to EFO which makes the curation process easier.
Importantly, this allows us to maintain a consistent use of external resources ensuring that we do not map to obsolete classes, and erroneous mappings caused by external changes are flagged.
There is still an outstanding issue of how correct the external resources are.
For example, reference ontologies EFO has consumed contain their own mappings which we have further imported to expand interoperability.
On scrutiny, some of these were found to be incorrect.
For example, mappings to EFO class brain structure derived from synonyms in an external ontology Minimal Anatomical Terminology (MAT) included abnormal brain.
Errors of this type are communicated back to the authors of the source ontology.
This represents a useful feature of this methodology; we review how reference bio-ontologies map to one another and how correct these mappings are.
It is clear that synonyms are used in different ways in different contexts and care must be exercised when using these; we now validate synonyms prior to including these and provide feedback both requesting terms and flagging errors when performing mapping.
2.2 Creating an ontology view While an upper level framework can provide structure to the ontology, such high-level classes (cf.
Fig.1) can often appear as abstract and confusing for biological users.
For example, the Basic Formal Ontology (BFO) (Grenon and Smith, 2004) contains the classes continuant and occurent.
Such classes are useful to organize the ontology and to aid interoperability between ontologies, but are less helpful for a biological user.
With this in mind, we use only some of BFO within EFO, and those parts are hidden from users.
First, we create an annotation property, ArrayExpress_label, which we use to indicate a preferential label that is displayed in the Atlas browser which replaces any other label on the class, though such labels may also be synonyms and are supported for queries.
For example, the BFO class processual entity is displayed as process in the Atlas user interface for readability.
Second, we use a further annotation property organizational_class which is given a value of true in any classes we wish to hide from the user (e.g.disposition) which are identified as structural and which are not desired to be visualized in queries.
This allows us to show parts of the ontology relevant to the users, while still using an accepted upper level ontology.
Views generated from EFO are used in both the Atlas and ArrayExpress Archive.
EFO is used to improve searching across textual experimental descriptions and key value pairs used to annotate samples.
When a user enters a keyword that matches an EFO class, synonyms found in alternative_term annotation properties in EFO classes are also used in the search, thereby returning extra matches.
We also provide an option to extend searches with classes related to their query via is_a or part_of ontological relations.
This functionality as deployed in the ArrayExpress Archive is powered by the Apache Lucene as a search engine, and we have packaged the EFOpowered search extension as a separate Java library.
The algorithm consists of two parts.
First, EFO in OWL format is parsed; the ontology tree is traversed and synonyms, all part_of or is_a children for all classes in EFO are extracted and a map structure is built for fast lookup.
Second, the map structure is used with a rewritten input Lucene Query with additional synonyms and children (if the option is selected and if they exist).
For our previous example, query breast carcinomais transformed to (breast carcinoma OR breast cancerOR ductal carcinoma in situ, etc.).
This library is available as stand-alone JAR, Maven artifacts and source code from http://github.com/arrayexpress/aeinterface/tree/master/components/efo-query-expand/.
The Gene Expression Atlas code base is currently under revision to create a stand-alone install anywhere utility, which will also become publicly available and open source in the near future.
2.3 Supporting the linked data vision In addition to using EFO within the Gene Expression Atlas and ArrayExpress Archive, we also embrace the ideas of linked data and integration with external resources.
In the context of the semantic web, linked data describes a method of creating typed links between data (Bizer et al., 2009).
In EFO, we use dereferenceable URIs for all of the classes in the ontology which are assigned EFO URIs.
Such classes are assigned a unique identifier, e.g.http://www.ebi.ac.uk/efo/EFO_0000001, with the number fragment incremented for each new class.
Since each of these identifiers is dereferenceable via the http protocol, they can be requested 1115 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1116 11121118 J.Malone et al.from a web server and information about the class returned as userfriendly content.
These pages contain information such as the Resource Description Framework Schema (RDFS) class label, parent classes, child classes and annotation properties e.g.text definition.
These pages are also machine readable: the source code for each page is actually an EFO Resource Description Framework (RDF) fragment describing a specific class.
Computer agents can therefore interact with the EFO class web pages in a way that is analogous to human user interaction.
Note, this does not apply to those classes in which URIs are imported from external ontologies; for such ontology URIs, the owner of these ontologies would be responsible for creating dereferenceable URIs.
There are two key elements to linking gene expression data.
The first is that parts of the Atlas sample and assay data are annotated with EFO class identifiers.
We associate data elements to our explicit definition of what the data represent, by annotating each experiment with EFO classes.
The second element is the set of cross-ontology mappings that are maintained within EFO.
As EFO is an application ontology, there is an advantage in reusing and importing classes from existing ontologies where possible.
Not only does this reduce the effort in adding new classes to EFO, but it also provides interoperability (via cross references) with other resources that use these existing ontologies.
There are a number of challenges associated with this approach.
One of the most challenging is deciding upon the appropriate ontology to select when attempting to reuse classes.
In the simplest case, where overlap does not exist and there is a clear single authoritative reference ontology, we simply import that class with the original URI maintained, e.g.BFO.
However, there are a limited number of examples for where this is the case; for many terms, there may be multiple classes that can fulfill the required definition.
For example, consider the term hypertension: as of January 2010, there are 12 exact matches for this class label when querying the NCBO BioPortal and most of these ontologies provide definitions consistent with our data annotation use cases.
For this reason, we performed some preliminary dataontology mapping which allowed us to both assess the matching algorithm and the available reference ontologies for coverage on gene expression data (Malone et al., 2009).
Recently, a tool designed to assist with selecting the most suitable ontologies for a given task has become available, and essentially replicate our early work in an extensible framework (Jonquet et al., 2009).
As we require EFO to be cross-referenced to as many external functional genomics datasets as possible, we maximize interoperability and therefore add as many mappings to different ontologies as are valid for our given class and curate these.
The decision to create multiple external mappings to EFO classes clearly presents additional overhead to both the initial set of mappings and the subsequent maintenance of these mappings, as both are labor intensive if performed manually.
We have therefore developed semi-automatic mapping tools.
Our matching approach uses the Metaphone (Phillips, 1990) and Double Metaphone algorithms (Phillips, 2000), which were selected following an empirical study of commonly used matching algorithms and their utility in the biomedical domain (Malone et al., 2008).1 We were particularly interested in algorithms yielding low false positive rates, as we wished to use the same algorithm for semiautomatic annotation of incoming data to the ArrayExpress Archive as a curator aid.
Following the evaluation of several algorithms, a combined strategy was implemented using Metaphone for a first pass and then falling back to Double Metaphone for those terms not matched by Metaphone.
This strategy yields the highest overall number of matches with minimal human intervention (required only for multiple matches).
Verified matched terms identified by this strategy were included as valid mappings in EFO and added to a definition_citation annotation property.
We have developed a speciesspecific ranked list of preferred ontologies with known good coverage when mapping to new terms.
We prefer to use OBO Foundry candidate ontologies when these provide good matches and use general uncurated 1Tools available at http://www.ebi.ac.uk/efo/tools Fig.3.
Added value relations between classes in EFO.
The figure illustrates the existential restrictions (i.e.one or more relationship) placed on some of the subclasses of the classes shown (classes shown in boxes).
resources like Unified Medical Language System (UMLS) only when necessary.
3 RESULTS The diversity of experiments captured in the Gene Expression Atlas and ArrayExpress provides a wide range of experimental variables.
A typical experiment includes factors such as disease, anatomical parts, developmental stage, species and chemical compounds.
Within these experimental factors, there is additional knowledge that we capture to support our use cases.
Consider the query, retrieve all data for cancer cell line samples.
This query requires more than just samples in the database which have been annotated with cancer and with a cell line.
The query is more accurately expressed as cell lines that are derived from some diseased sample.
We therefore add logical relations between classes in the context of EFO; these can serve as OBO Foundry integration use cases.
An example of some of the existential restrictions between classes is shown in Figure 3.
The ability to explicitly express richer statements of knowledge (such as the example above) is one of the major advantages of using ontologies; however, with increased complexity comes increased possibility of contradiction and inconsistent expression.
To help manage this issue, we chose to use the Web Ontology Language (Horrocks et al., 2003), the recommendation for representing knowledge with formally defined meaning.
Using the OWL-DL flavor of the language, we are able to create axiomatic statements about classes, and use the Pellet 1.5.2 description logic reasoner (Sirin et al., 2007) to ensure the ontology is consistent, i.e.class membership is axiomatically correct and there are no contradictions in the model.
OWL also offers the ability to create equivalent classes (often called defined classes) which are useful for inferring hierarchies and managing multiple inheritance.
Considering the previous example of the EFO class cancer cell line, this is defined in OWL as any class which has the bearer_of some cancer axiom.
In other words, any cell line which bears the disease cancer will be inferred to be a subclass of this type.
Here we illustrate some of these examples from our application.
3.1 Querying gene expression data In order to demonstrate that EFO is fit for purpose, we evaluate it against the competency questions and use cases.
One of the primary 1116 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1117 11121118 Experimental Factor Ontology Fig.4.
Gene Expression Atlas Query for genes under-or overexpressed in mammalian craniofacial tissues.
use cases for EFO was to annotate ArrayExpress data (i.e.providing ontological coverage) and to ask meaningful questions of gene expression data.
In a recent review conducted by Jonquet et al.(2009), EFO was assessed for coverage in annotating biological datasets using an NCBO tool, the Open Biomedical Annotator.
Alongside 98 English ontologies in UMLS 2008AA and 92 of the BioPortal ontologies, in total, these resources offer a dictionary of 3 582 434 classes and 7 024 618 textual terms.
Following experimentation with three biological datasets, EFO is reported as fourth best in all three tests.
EFO performs well in these tests due to the data-driven development, cross-domain method we use.
It is also noticeable that the ontologies that finished in the top three were significantly larger than EFO, for example, NCI Thesaurus has 35 000 classes compared with 2600 classes in EFO.
The real return for the user when using an ontology is in the additional relations used for improving queries.
Due to the relations used in EFO, we are able to ask general questions without requiring that every subtype is enumerated in the query.
For example, for experiments about cancer, we want all subtypes of cancer, for example prostate carcinoma, without requiring the user to specifically enumerate these subtypes and we want to return only subtypes for which we have data.
Similarly, we want a user to be able to ask for forebrain and the query to return data that is annotated with forebrain substructures such as hypothalamus.
Finally, for mouse we want to return data annotated to Mus musculus and substrains thereof.
Figure 4 shows the results of a query for gene that is expressed in craniofacial tissues or sub structures.
Within the ontology, relations are made between classes such as those seen in Figure 4.
Specifically here, the query is asking for genes which are over-or underexpressed in assays that are annotated with an organism part that is craniofacial tissue or a sub-structure.
Figure 4 presents the parts of the ontology that satisfy this query at the top of the image.
The tree includes classes such as eye (synonym eye structure), which expands to include its substructures such as retina.
Fig.5.
Ontology-enabled search using EFO, showing query expansion for keyword cancer with breast carcinoma selected.
Subtypes (red), synonyms (green) and matches to the search term (yellow) shown in the ArrayExpress Archive.
As described earlier, EFO has also been used in the ArrayExpress Archive (http://www.ebi.ac.uk/arrayexpress) to enrich querying.
Figure 5 illustrates that in addition to keyword breast carcinoma (yellow), EFO-enabled search returns experiments matching is-a children, e.g.medullary breast cancer.
3.2 Linking data through BioPortal An additional advantage to using an ontology to annotate data in the Atlas is in the use of external ontology tools.
The BioPortal resource at NCBO is an open repository of biomedical ontologies that provides access via web services and web browsers to ontologies developed in OWL, RDF and OBO format (Noy et al., 2009).
It allows the searching of biomedical data resources such as ArrayExpress, through the annotation and indexing of these resources with ontologies that can be accessed through BioPortal.
Since EFO is used to annotate data in ArrayExpress and also provides multiple mappings to other ontologies, it is possible to query data through BioPortal using ontology class names and return annotated data from multiple resources via the BioPortals Resources facility, for example, pathway data from Reactome.
4 DISCUSSION In this article, we present EFO, an application ontology driven by the annotation and query needs of samples in omics datasets.
Our approach to ontology engineering uses the many existing reference bio-ontologies while allowing us to develop a hierarchy that supports our use cases.
EFO enables queries of the data that were not previously possible because we add value to existing ontologies by adding explicit relations and because we have adopted a datadriven methodology.
Furthermore, EFO separates knowledge from the experimental data, is reusable and easy to maintain; when modification to the knowledge is required, modification to the data is not.
We believe the methodology and tools present a reproducible and maintainable strategy to create ontological solutions for a particular application focus.
EFO has also proven to be useful for text mining annotation of gene expression datasets and has been used 1117 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1118 11121118 J.Malone et al.in data mining.
An EFO-R package that facilitates such analysis is currently under development.
Essentially EFO represents a custom view of several domainspecific ontologies.
We believe that use of ontology views will help end users to understand and use ontologies.
An advantage of EFO for ArrayExpress staff is that they do not need specialist domain knowledge of multiple ontologies and are able to apply EFO consistently to data, while users typically do not perform well as annotators.
Ideally, views should contain a subset of the ontology that is still logically consistent containing only classes, instances and properties that are desirable.
The requirements of a view are likely to be driven by particular applications and user communities as described here.
Improved tools that support the creation and use of views will help the users of bioinformatic resources overcome one of the largest obstacles of using ontologies: that the learning curve is extremely steep and the climb is a disincentive to users.
There is a great deal of useful work presently under way within the bio-ontology community.
However, it is impractical and undesirable to import, wholesale, ontologies that touch upon many domains and expect users to apply them consistently.
Guidelines on development of application ontologies and appropriate reuse of existing resources would be useful.
In particular, maintenance and mapping of original ontology identifiers and development of public domain tools are important.
Similarly, there are several important challenges facing reference ontologies.
One of the most challenging is mapping anatomy between multiple species.
This is not in scope for EFO and we look forward to consuming such reference ontologies, but application data should inform some of this work.
Our work with ontologies is focused on enabling us to do novel research with the experimental data we have, such as answer more complex questions and integrate multiple data sources.
In this respect, ontologies are a means to an end; our work here is based on describing experimental data, and we believe this should be the driving force behind ontology development and consumption.
Future work will develop an RDF triple store representation of Atlas and provide federated querying using SPARQL end points.
An ontology-enabled annotation application for functional genomics dataAnnotare (code.google.com/p/annotare/ ) is being collaboratively developed, which allows users and curators to select terms from multiple ontologies, including EFO.
We hope this will expose users to ontologies in a user-friendly way and help provide better annotated datasets.
ACKNOWLEDGEMENTS We thank Eric Neumann, Mlanie Courtot, Frank Gibson, Alan Rector, the Gen2Phen and Engage consortia and P3G colleagues for sharing use cases and data annotations, and the ArrayExpress and Gene Expression Atlas team for their implementation of EFO in the Atlas UI and the 3 anonymous reviewers for their comments.
Funding: European Commision grants FELICS (contract number 021902); EMERALD (project number LSHG-CT-2006-037686); Gen2Phen (contract number 200754); European Molecular Biology Laboratory.
Conflict of Interest: none declared.
ABSTRACT Motivation: The development of the omics technologies such as transcriptomics, proteomics and metabolomics has made possible the realization of systems biology studies where biological systems are interrogated at different levels of biochemical activity (gene expression, protein activity and/or metabolite concentration).
An effective approach to the analysis of these complex datasets is the joined visualization of the disparate biomolecular data on the framework of known biological pathways.
Results: We have developed the Paintomics web server as an easyto-use bioinformatics resource that facilitates the integrated visual analysis of experiments where transcriptomics and metabolomics data have been measured on different conditions for the same samples.
Basically, Paintomics takes complete transcriptomics and metabolomics datasets, together with lists of significant gene or metabolite changes, and paints this information on KEGG pathway maps.
Availability: Paintomics is freely available atContact: aconesa@cipf.es Received on May 7, 2010; revised on October 14, 2010; accepted on October 17, 2010 1 INTRODUCTION Biological research in the post-genomics era has been characterized by the extensive use of omics technologies.
The general availability of transcriptomics, proteomics and metabolomics platforms, together with the development of user-friendly data analysis solutions (Da Wei Huang and Lempicki, 2008; Medina et al., 2010) has boosted the adoption of high-throughput approaches towards the understanding of the relationships between the genome and the phenotype.
Integrated approaches that combine transcriptome, proteome and metabolome profiling have gained popularity and have proven to provide novel insights in the understanding of the biological systems (Cho et al., 2008; Heijne et al., 2005; Kolbe et al., 2006).
A first approach to the interpretation of complex omics experiments is the joined visualization of the data on templates that collect previous knowledge.
Graphical display is an effective tool to assist human reasoning and when different layers of biomolecular activity are presented in the context of the biological pathways they operate, much can be gained at the interpret ability of these large datasets.
To whom correspondence should be addressed.
To date, the number of bioinformatics tools that offer integrated visualization of omics datasets is limited.
KaPPa-View (Tokimatsu et al., 2005) and MapMan (Thimm et al., 2004) are plant-specific tools that display metabolite and transcript levels on predefined pathway blocks.
The MassTRIX software (Suhre and SchmittKopplin, 2008) translates NMR spectra into metabolic compounds and maps them into KEGG pathways together with genome information.
The tool is specially suited for exploring the metabolic repertoire of sequenced genomes but does not incorporates gene expression measurements.
A recent development is ProMeTra (Neuweger et al., 2009) that accepts pre-computed and custommade pathway maps in scalable vector graphics (SVG) format and is able to display dynamics data.
The application is restricted microbial genomes and offers direct access to different omics experimental databases.
Although these tools make an interesting use of visualization strategies, we found that available resources are either restricted to specific biological domains and/or have limitations for representing omics measurements.
We have developed Paintomics to provide a simple but effective resource for integrated visualization in genomics studies where transcriptomics and metabolomics data are generated on the same set of samples.
Basically, the application accepts gene expression and metabolite quantifications and displays data on KEGG maps.
The main distinctive features of Paintomics are: painted KEGG maps supported for a large range of organisms; joint visualization of different types of omics data, displaying both significant and non-significant changes; computation of pathway enrichment based on both transcriptomics and metabolomics data; interactive images with link-outs to KEGG info and experimental values; and easy to download mapped data for further analysis.
Paintomics is available at http://www.paintomics.org.
2 THE PAINTOMICS APPLICATION Paintomics is a platform-independent web application built on Perl and Python scripts running on an Apache web server.
A simple web-form requires users to upload gene expression and metabolite concentration files, optionally provide lists of significant features, and indicate the organism under study.
Paintomics directly supports over 100 top species of different biological kingdoms and offers user the possibility to request any other organism present in the KEGG database.
Once data are submitted, Paintomics parses input files to match gene identifiers and metabolite names to the KEGG database.
Generally, Paintomics will accept EntrezGene ID, although for a number of species different identifiers are supported.
Regarding metabolites, ambiguity frequently exists for the assignment of supplied compound data and KEGG metabolite names.
In this case, The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:40 2/12/2010 Bioinformatics-btq594.tex] Page: 138 137139 F.Garca-Alcalde et al.Fig.1.
Painted citrate cycle map for the Arabidopsis DTT treatment example.
Reduced levels of metabolites are found at the first part of the cycle (blue-colored metabolites), while increased concentrations are found on the second part of the pathway (red-colored metabolites).
Black entry boxes represent significant regulation.
the user is prompted to manually assign compounds to KEGG descriptions or let the application choose the closest name(s).
In the next step, the application will show matching results for the submitted data.
A general summary is presented with the number of genes and metabolites of the selected species present in the current KEGG database version, the number of features matched by the input files and how many of those were labeled as significant.
This summary gives users a broad feeling of the coverage achieved by the submitted data.
Additionally, a per-pathway table is presented with matched and significant figures for both genes and metabolites together with the P-value of pathway enrichment based on the Fishers exact test.
This table is sortable by any of its columns to facilitate browsing of the pathways according to the user main interests.
The user can then select which specific pathways to paint.
Pathways are painted locally at the Paintomics server overlaying the users data on the KEGG image templates using SVG technology.
At each matched feature, a box is painted with as many sections as columns present in the input files, each section colored according to its correponding expression or concentration value.
This aids in the visualization of different samples (conditions or a time course) on the same image.
Significant features are highlighted by a black box and gene names are shown for all proteins present in the indicated organism.
Images retain KEGG link-outs to gene and compound records to fully benefit from up-to-date pathway information.
For browsers supporting SVG technology, additional interactivity is available such as box enlargement and display of actual numerical values on mouse pass-over.
Images can be downloaded in different formats with a simple click, as well as all matching information used to paint each specific pathway.
In order to offer reliable information, Paintomics database is automatically updated on a monthly basis.
3 USE CASE To illustrate the usage and knowledge discover facilitated by Paintomics, we used the tool to analyse a recent study of Arabidopsis thaliana that surveyed metabolomic and transcriptomcis changes on Arabidopsis leaves in response to manipulation of the thiol-disulfide status (Kolbe et al., 2006).
This study aimed at the understanding of the role of redox signals in the regulation of metabolic processes.
Gene expression and metabolic data were downloaded from the publication site.
The dataset contained 6390 named genes and 138 [11:40 2/12/2010 Bioinformatics-btq594.tex] Page: 139 137139 Paintomics 90 metabolites comprising sugars, amino acids and organic acids.
Significant genes were selected as those with at least a 2-fold expression change, while the list significant metabolites was directly obtained from the authors analysis.
Paintomics was fed with data files and run with default parameters, selecting A.thaliana at the species check-box.
A total of 60 pathways where obtained with at least one matching entry.
From the visual analysis of these maps, conclusions on the coregulation of transcripts and metabolites can be drawn.
For example, citric acid map shows the down-regulation of the first part of the pathway (citrate, isocitrate, cis-asconitate and 2-oxoglutarate), while metabolites on the second part (malate, fumarate and succinate) had increased levels (Fig.1).
These changes were accompanied by the significant upregulation of genes on this second half of the cycle such as citrate synthase (CSY3), malate dehydrogenase (PMDH2) and succinate dehydrogenase (SDH2-2), pointing to a coordinated activity of genes and metabolites.
Connections between the tricarboxylic acid cycle (TCA) and the pyruvate metabolism additionally reveal a decrease in pyruvate and phosphoenol pyruvate levels upon DTT treatment, which was accompanied by a significant upregulation of the pyruvate dehydrogenase (IAR4), malate dehydrogenase (IDH2) and phosphoenolpyruvate carboxilase (PCK1) enzymes that catalyze the conversion of these compounds toward acetyl-CoA, oxolacetate and finally malate.
Interestingly, this pattern of metabolite balance at the TCA cycle was also observed by the authors although their transcriptomics analysis did not reveal any significant changes of genes in these pathways.
These results led authors to postulate that changes in fluxes and metabolite concentrations in these pathways were most likely due of post-translational mechanisms.
The integrated visualization offered by Paintomics did reveal the coordinated state of transcript and metabolite levels.
ACKNOWLEDGEMENTS We thank the IT team of the Bioinformatics and Genomics Department of the Centro de Investigaciones Prncipe Felipe for their help.
We also thank the support of the National Institute of Bioinformatics (www.inab.org) and the CIBER de Enfermedades Raras, both initiatives of the ISCIII, MICINN.
Funding: Spanish Ministry of Science and Innovation (MICINN) (grants BIO2008-05266-E BIO2008-04212 and CEN-20081002); GVA-FEDER (PROMETEO/2010/001); Red Tematica de Investigacion Cooperativa en Cancer (RTICC), ISCIII, MICINN (grant RD06/0020/1019, in part).
Conflict of Interest: none declared.
ABSTRACT Motivation: Understanding transcriptional regulation is one of the main challenges in computational biology.
An important problem is the identification of transcription factor (TF) binding sites in promoter regions of potential TF target genes.
It is typically approached by position weight matrix-based motif identification algorithms using Gibbs sampling, or heuristics to extend seed oligos.
Such algorithms succeed in identifying single, relatively well-conserved binding sites, but tend to fail when it comes to the identification of combinations of several degenerate binding sites, as those often found in cisregulatory modules.
Results: We propose a new algorithm that combines the benefits of existing motif finding with the ones of support vector machines (SVMs) to find degenerate motifs in order to improve the modeling of regulatory modules.
In experiments on microarray data from Arabidopsis thaliana, we were able to show that the newly developed strategy significantly improves the recognition of TF targets.
Availability: The python source code (open source-licensed under GPL), the data for the experiments and a Galaxy-based web service are available at http://www.fml.mpg.de/raetsch/suppl/kirmes/ Contact: sebi@tuebingen.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION One of the most important problems in understanding transcriptional regulation is the prediction of transcription factor target genes based on their promoter sequence.
A transcription factor binding site (TFBS) is a short sequence segment (10 bp) located near a genes transcription start site and is recognized by respective transcription factors (TFs) for gene regulation (Gupta and Liu, 2005).
TFBSs recognized by the same TF usually show a conserved pattern, which is often called a TF binding motif (Gupta and Liu, 2005).
Such binding motifs are typically identified through finding overrepresented motifs in promoter sequences of a set of genes that is enriched with targets for a specific TF.
The simplest approaches To whom correspondence should be addressed.
Present Address: Biology Department, Duke University, Box 90338, Durham, NC 27708, USA include the identification of overrepresented oligomers relative to a background model (Bailey and Elkan, 1994).
More sophisticated models include Gibbs sampling methods (Lawrence et al., 1993) that try to identify position weight matrices (PWMs), cf.
e.g.Schneider et al.(1986), which characterize binding sites in the candidate promoter sequences (Stormo, 2000).
Although these methods have been very successful for bacterial and yeast genomes, their success was limited in higher eukaryotes for which TF binding motifs are often degenerate and the search space is considerably larger.
While some recent techniques have improved the state-of-the-art, they all tend to fail if the motif is defined only weakly or found solely in the context of other motifs.
Despite these challenges, there are two possible redeeming factors: (i) many eukaryotic genomes have been or are being sequenced, and comparative genomic analysis can be extremely powerful; and (ii) most eukaryotic genes are controlled by a combination of factors with the corresponding binding sites forming homotypic or heterotypic clusters known as cis-regulatory modules (CRMs) (Gupta and Liu, 2005).
In this work, we want to exploit these redeeming factors and thus have developed novel methods that are able to classify genes as being either targets of the (combination of) TFs being studied or not, based on the presence of motifs and features capable of describing CRMs.
This was implemented as a two-step procedure.
We first used de novo motif finding tools or known motif databases like transfac (Matys et al., 2003) or jaspar (Sandelin et al., 2004) to identify a set of potential motifs.
Then, we used support vector machines (SVMs) employing a newly developed kernel, called the regulatory modules (RM) kernel, that is capable of capturing information about the motifs and their relative location to classify promoter sequences.
Additionally, we demonstrate the potential of our approach to exploit conservation information to improve the classification performance.
Most previous approaches to discover CRMs are based on the identification of motifs and their co-occurrences, e.g.Frith et al.(2008) and Sinha and Tompa (2002).
Other approaches exploit site-clustering information with de novo motif discovery to build rules discriminating modules that preserve the ordering of motifs, e.g.Segal and Sharan (2005).
Finally, Yada et al.(1998) suggested to use hidden Markov models to represent CRMs and Gupta and Liu (2005) developed a Monte Carlo method 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[16:17 14/7/2009 Bioinformatics-btp278.tex] Page: 2127 21262133 KIRMES and dynamic programming approach to screen motif candidates.
The main difference between our approach and most previous approaches is that we use discriminative methods to analyze CRMs, not just an individual TFBS.
In particular, instead of using zerothorder inhomogeneous Markov chains, we use support vector kernels to model higher order sequence information around candidate TFBS.
This manuscript is organized as follows: we start Section 2 by describing the basic methodology of classifying sequences with SVMs using standard sequence kernels.
It is followed by a detailed explanation of the main idea of this work in Sections 2.3 and 2.4, to combine de novo motif finders with state-of-the-art motif modeling.
In Section 3.2, we outline a problem derived from Arabidopsis thaliana microarray expression experiments, with loss or gain of function of certain TFs.
In our experiments, we first illustrate that the straightforward approaches cannot achieve reasonable results, while the newly developed methods are able to drastically improve the target gene recognition performance.
Finally, the article is concluded with a brief discussion in Section 5.
2 APPROACH SVMs are a well-established machine learning method introduced by Boser et al.(1992), to solve classification tasks frequently appearing in computational biology and many other disciplines.
Typical examples are the classification of tumor images or gene expression measurements, the detection of biological signals in DNA, RNA or protein sequences as well as the recognition of hand written digits or faces in images.
SVMs are widely used in computational biology due to their high accuracy, their ability to deal with high-dimensional data, and their flexibility in modeling diverse sources of data (Mller et al., 2001; Noble, 2006; Schlkopf and Smola, 2002; Schlkopf et al., 2004).
The goal of training an SVM is learning to label a dataset just like the training examples (x(i),y(i)),i=1,...,N. Here, x are the examples (sequences in our case), and y the labels.
For a two-class problem, labels are of the form +1 and 1, where the label +1 is assigned to a sequence x if it is part of the positive gene set, and 1 if it is part of the negative set.
After training, the classifier is able to assign such a labeling to any sequence.
If it is regulated by the same combination of TFs as the training examples, a correctly trained classifier will assign +1 to a sequence, otherwise 1.
The domain knowledge inherent in the classification task is captured by defining a suitable kernel function k(x,x), which computes the similarity between two examples x and x.
This strategy has two advantages: the ability to generate non-linear decision boundaries using methods initially designed for linear classifiers; and the possibility to apply a classifier to data that have no obvious vector space representation, for example, DNA/RNA or protein sequences as well as structures (Ben-Hur et al., 2008).
Twoclass SVMs use a maximum-margin hyperplane that separates the classes of the input vectors in the feature space.
We give an introduction to existing SVM kernels that work on sequences, like the weighted degree (WD) kernel.
Subsequently, we extend the WD kernel in two different ways: first, we consider an addition to use conservation information.
Second, given a list of potential motifs, we propose a new kernel that integrates information on the motif sequences with information about their co-occurrence with the aim to characterize RM.
2.1 Spectrum kernel Given two example sequences x and x over the alphabet , a simple way to compute the similarity is to count the number of co-occurring oligomers of fixed length.
This idea is realized in the so-called spectrum kernel that was first proposed for classifying protein sequences by Leslie et al.(2002): kspec (x,x)= spec (x),spec (x) , where || is the number of letters in the alphabet.
spec is a mapping of the sequence x into a ||-dimensional feature-space.
Each dimension corresponds to one of the || possible strings s of length and is the count of the number of occurrences of s in x.
This kernel is well suited to characterize sequence similarity based on oligos that appear in both sequencesindependent of their position.
If the classification of promoter sequences of genes as TF targets was solely based on binding to specific oligos, then the spectrum kernel would be a reasonable choice.
If the motif is less conserved, then allowing for mismatches or gaps can be beneficial (Leslie et al., 2003).
Note that this kernel is (by design) incapable of recognizing positional preferences TFs, and thus TFBSs, might have relative to the transcription start or among each other.
2.2 WD Kernel The so-called WD kernel proposed by Rtsch and Sonnenburg (2004) computes the similarity of sequences of fixed length L by considering the substrings up to length starting at each position l separately: kwd (x,x )= L l=1 d=1 d L I ( x[l:l+d] =x[l:l+d] ) , (1) where d =2d+1 2 + , and x[l:l+d] is the substring of length d of x at position l (Rtsch and Sonnenburg, 2004; Sonnenburg et al., 2007b).
In the WD kernel, only oligos appearing at the same position in the sequence contribute to the similarity of two sequences.
The WD kernel with shifts (Rtsch et al., 2005), or WDS kernel, is an extension of the WD kernel allowing some positional flexibility of matching oligos: kwds,S (x,x )= L l=1 d=1 S s=0 s+il 1 2dL(S+1) (2) ( I ( x[l+s:l+d+s] =x[l:l+d] ) + I ( x[l:l+d] =x[l+s:l+d+s] )) It considers oligomers up to length d, and allows them to be shifted up to S positions, starting from i, in the input sequences.
This kernel is better suited for motifs with indels or at varying positions; see e.g.Rtsch et al.(2005) and Sonnenburg et al.(2007a).
The locality improved and oligo kernels by Zien et al.(2000) and Meinicke et al.(2004), respectively, achieve a similar goal in a slightly different way.
2.3 WD kernel with conservation information To include conservation information, we extended the WDS kernel with a term to multiply the score of the local matches of an 2127 [16:17 14/7/2009 Bioinformatics-btp278.tex] Page: 2128 21262133 S.J.Schultheiss et al.Fig.1.
The idea behind the RM kernel: a motif finder is applied to the regulatory sequences in the input set (long, dark bars), which identifies overrepresented motifs (short, light bars).
The best matching motifs (boxed) in every sequence serve as starting points, where we excise a window of 20 bp around the center of each motif occurrence for the WDSC kernel.
Conservation information for these windows is looked up in a precomputed multiple genome alignment (cf.
Section S.2 of the Supplementary Material for details on conservation data).
Additionally, we construct an input vector for the RBF kernel of the pairwise motif distance, and distance to the transcription start (if available).
oligo of length d at position i with a quantity that depends on its conservation.
We propose to use the average conservation of the oligo in pregenerated alignments of sequences from G other organisms: Ad,i,x =1+ A Gd G g=1 d j=0 I(xi+j =xgi+j), (3) where xg is the sequence of the syntenic regions in the genome of organism g=1,...,G and A>0 is a parameter allowing one to control the importance of the conservation.
We add 1 to not ignore unconserved sequences: ideally, the conservation should only add information by emphasizing the conserved and hence likely functionally important regions of the regulatory sequence.All results shown were obtained with the setting of A=1.
Using this definition of a conservation score, we can now define the WD kernel with shifts and conservation (WDSC): kwdsc,S,A(x,x )= L l=1 d=1 S s=0 s+il d,i,xd,i,x 2d(S+1) (4) ( I ( x[l+s:l+d+s] =x[l:l+d] ) +I ( x[l:l+d] =x[l+s:l+d+s] )) The above kernel, like the WDS kernel, corresponds to a feature space spanned by all possible k-mers at every position.
While the feature value is 1 for the WDS kernel if the k-mer is present at a certain position, for the WDSC kernel the feature value is d,i,x, i.e.it is computed depending on the conservation of the k-mer at this position.
2.4 A kernel for regulatory modules Suppose we are given a set of M motifs Mm, m=1,...,M, which may either come from a database or from a de novo motif detection method.
Such motifs are often represented in a way that one can easily scan a given sequence for occurrences of the motif (e.g.as PWMs).
In a preprocessing step, we compute the best matching position pm,x(i) of each motif Mm in all considered sequences x(i),i=1,...,N. In case of PWMs, the PWM score (Schneider et al., 1986) and in case of oligo-based motifs, the Hamming distance may be used to decide which position in the sequence matches best.
We use MotifScanner from the inclusive package by Thijs et al.(2002) to scan sequences for the best matching occurrence of a PWM; in case of an oligomer, we use a regular expression pattern with decreasing discriminative power to find the closest match.
In case of a tie, the last match in the sequence is selected.
2.4.1 Kernel for multiple motifs For the kernel functions, all input vectors need to be of the same length.
Therefore, we have to choose the same number of matches per sequence for all motifs (1 in our case), regardless of the quality of the matches, as shown in Figure 1.
Biologically, a threshold quality seems more intuitive.
Then, several good matches would be considered, or no match for sequences that do not contain the motif.
However, a soft margin during training allows the algorithm to ignore some mislabeled data points, i.e.sequences that do not contain the motif, without strong effects on generalization.
A soft margin SVM uses a slack variable and can tolerate training data points that, e.g.clearly lie in the space of the class they do not belong to without skewing the separating hyperplane in order to correctly classify such outliers.
This allows the SVM to separate data that would not be linearly separable when using a hard margin, usually achieves a better classification result due to a smoother hyperplane, and makes the SVM more robust against noisy data (Schlkopf and Smola, 2001).
Similar ideas have been proposed and successfully used in image analysis, using kernel methods where motifs correspond to points of interest, e.g.sharp edges (Mikolajczyk et al., 2005; Nowak et al., 2006).
The main idea of the kernel that we propose is to represent an input sequence x by the set of sequences xm :=x[pm,xw,pm,x+w] originating from the region of length 2w around the best motif match pm,x of motif Mm in x.
Each sequence region xm contributes independently to the similarity between two input sequences: k1(x,x)= M m=1k(xm,xm).
This term characterizes the co-occurrence of a collection of motifs in two sequences x and x.
The similarity is highest if all motifs appear in both sequences (in arbitrary order).
We propose to use a position-specific string kernel, for instance the WDSC kernel, to compute the similarity of the regions.
2.4.2 Modeling positional information For the first part of the kernel, the position of the motif does not influence the similarity at all, since the motif windows have been extracted from the input sequence.
In the second part of the kernel, we try to capture the relative position of the best motif matches to each other and to the transcription start site, if available.
This is achieved by computing all pairwise distances 2128 [16:17 14/7/2009 Bioinformatics-btp278.tex] Page: 2129 21262133 KIRMES between match positions of motifs: v(x)=(p1,x ptss,...,pM,x ptss,p1,x p2,x,...,pi,x pj,x,...,pM1,x pM,x ) , for all i = j= 1,...,M, where ptss is the position of the transcription start site in the sequence.
A simple way of computing the similarity between two such vectors is to use the radial basis function (RBF) kernel, e.g.Schlkopf and Smola (2002): krbf (v,v )=exp ( vv2 ) , where is a kernel hyperparameter to be found by model selection.
In the kirmes pipeline (described below), we create one string kernel per motif, with all the windows where the motif occurs as input data and sum them up to a combined kernel, and add an RBF kernel for all pairwise positions.
Having both parts of the kernel defined, the question how to combine them remains.
We propose to simply add both contributions in the RM kernel: krm,S,A, (x,x )=  M m=1 wm kwdsc,S,A(xm,xm) +krbf (v,v) (5) Here, wm are the weights assigned to the subkernels kwdsc, which are all set to 1 per default, and xm is the best match of motif Mm in the sequence x.
Please note that if we add the kernels, it amounts to concatenating the feature spaces.
If one multiplied the contributions of distances and motif-sequence similarity, the kernel would be in some sense similar to the previously proposed oligo kernel (Meinicke et al., 2004).
In our case, this would not be feasible, since we want to inspect the kernel contributions independently and determine the sequence logo that each string kernel uses to discern the two input classes.
Therefore, we want a clear distinction between the contributions of the positional kernel part and the motif part of the kernel.
3 METHODS 3.1 kirmes pipeline Below, we introduce an integrated python pipeline, called kirmes, using the previously described kernels to classify promoter regions of genes as targets of a certain combination of TFs (i.e.as co-regulated).
We outline a use case with a scenario where this pipeline can be applied.
DNA sequences considered in the input sets can come from any part of the euchromatin, e.g.upstream and downstream regions of a gene, intronic and exonic parts, as well as untranslated regions (UTR) in the 3 or 5 direction, each of arbitrary length.
The selection of this region depends on the organism the data stems from; for the use case, we will assume this to be A.thaliana, where good results can be obtained with a combination of upstream, UTR and intronic sequences.
We used 1000 bp upstream of the transcription start; in general, longer sequences introduce more noise.
In organisms with shorter promoters a reduction would be beneficial for the signal-to-noise ratio.
Figure 2 shows an outline of the pipeline for the classification of promoter sequences based on microarray experiments (cf.
Section 3.2).
3.1.1 Initial motif finding In the first step, there is a chocie between several methods to identify candidate motifs.
Initially, we used a common Gibbs sampling algorithm (Lawrence et al., 1993) called MotifSampler from the inclusive package by Thijs et al.(2002) that finds overrepresented motifs Mm, m=1,...,M and creates subkernels for up to M motifs.
To make sure we do not include motifs that are too common, we use several strategies: Fig.2.
A cartoon workflow of the kirmes pipeline: the preprocessing step requires the genomic sequence and a set of regulatory sequences from genes that were determined to be co-expressed in microarray experiments, and ideally a negative set.
kirmes conducts a motif finding step, where it locates the positions of overrepresented motifs in fasta files of the genes regulatory region.
For the classification, we build an input vector with sequence sections of 20 bp, centered around the motif positions obtained during the motif finding step, and optional conservation information from related genome sequences for the WDSC kernel, as described in Section 2.3.
The classifier is trained on the labeled dataset of positives and negatives and can then be applied repeatedly on unlabeled prediction datasets to classify genes as co-regulated by the same mechanism as the input dataset or not.
first, a background model for this organism; second, minimum occurrences were set to 15 % or three genes of the set, whichever is more; third, 1000 random gene sets were generated and searched for motifs of the same length and determinacy.
This was measured through the information content of the position frequency matrix of the motif, an output of the Gibbs sampling program.
Since this last step takes a significant amount of time depending on the length and number of sequences, we searched for alternatives.
We settled on one approach, where we count the occurrence of any oligomer of length six in positive sequences (oligo counting).
We selected a subset of those oligomers that appear in at least 15% of all positive sequences.
This simple strategy certainly leaves room for improvements, but our experiments in Section 3.2 illustrate that it already works rather well.
3.1.2 SVM training We use the large-scale machine learning toolbox shogun at http://www.shogun-toolbox.org (Sonnenburg et al., 2006) through its python interface.
It provides implementations of all kernels described in this work and allows for fast training using several different SVM implementations, e.g.SVMlight (Joachims, 1999).
As described in Section 2.4, we create a kernel for every candidate motif Mm and use a window of 20 bp around each occurrence in every sequence.
We tested different widths 2129 [16:17 14/7/2009 Bioinformatics-btp278.tex] Page: 2130 21262133 S.J.Schultheiss et al.and settled for this length as it gave the highest accurracy for a representative dataset (cf.
Supplement S.4 in the Supplementary Material).
3.1.3 Interpretation of results To find out which of the candidate motifs Mm, m=1,...,M from the initial motif finding step contribute the highest discriminative power of the combined RM kernel krmk, we take a look at all the subkernels, each of which describe exactly one motif.
Every subkernel has an assigned weight wm =1 m=1,...,M, as described in Equation (5).
If we iterate over all motifs Mm and in turn set wm =0,wi =1 i =m once for every motif Mm and re-evaluate the SVM with the combined kernel krm\{m} on the same dataset x, we can obtain a ranked list of all motif kernels: we subtract the new training accuracy from the re-evaluation with one of the weights set to 0 (measured in area under the receiver operating characteristic curve, auROC) from the reference accuracy of the combined kernel with all weights set to 1.
This difference gives us the gain in accuracy for just this motif window.
The subkernels that contribute the largest difference are the most interesting ones for the experiment, since they contain the strongest candidate binding motif.
We then calculate positional oligomer importance matrices developed by Sonnenburg et al.(2008) to obtain a 1mer sequence logo (Schneider and Stephens, 1990) from the kernel that shows the motif that this kernel is attuned to.
This ranked list of motifs, along with the auROC difference, allows for a straightforward interpretation of the kirmes prediction results.
3.2 Microarray expression data We derive sets of co-expressed genes from microarray experiments performed with the commercial Affymetrix GeneChip Arabidopsis ATH1 array.
This chip is designed to measure transcript abundance of more than 20 000 genes of the model organism A.thaliana (Redman et al., 2004).
The sets are obtained through a stringent analysis of expression change using the software GeneSpring (Agilent Technologies).
We labeled genes as co-expressed when they showed a 4-fold change of expression in the experiment as compared with the control, and considered those genes not co-expressed if their levels remain the same, compared with the control, within a margin of 0.2-fold change.
The fold change is computed from the normalized gene expression level p in treatment and respective control, c: n= { c/p if p/c<1 p/c if p/c1 In this case, the direction of the change is represented by the sign of n, positive means up and negative means down relative to the control.
If several replicates were available, the mean after normalization is taken for every gene, for all replicates of p and c, respectively.
We used microarray data from two different experimental setups (cf.
Section S.1 in the Supplementary Material).
The first setup uses leaves from wild type A.thaliana plants exposed to medium at 38C versus leaves exposed to the same medium at room temperature, expression measurement taken 1 h after exposure (Busch et al., 2005).
The second setup uses inducible overexpression of Arabidopsis meristem regulators with the AlcR/AlcA system.
Plants harboring 35S::AlcR/AlcA::GOI (gus control, leafy, shootmeristemless, wuschel) constructs were grown in continuous light for 12 days and induced with 1% ethanol.
After 12 h of EtOH treatment, seedlings were dissected and RNA was processed from the shoot apex and from young leaves.
Affymetrix ATH1 arrays were hybridized in duplicates for each gene construct and condition (Leibfried et al., 2005).
In total, we considered 14 different gene sets to be discriminated by the methods.
3.3 Use case When a researcher has conducted several microarray experiments and has obtained a list of co-expressed genes that react in concert, ideally over several different experiments, and assumes they are targets of a specific combination of TFs, kirmes can be used to find common motifs in this list and in turn in all other known genes.
The experimental design is cruical for the success of our method: kirmes will work best if used on gene sets derived from time-series experiments or from loss or gain of function experiments of a specific gene.
The regulatory sequences of a gene set identified in such a manner have to be available in fasta format.
The regulatory region can be anything from promoters, introns, to even the whole chromatin, of arbitrary length, and can stem from any organism.
To effectively use the positional information of promoter regions, it is a good idea to select the sequences in such a way that the translation start site is at the same position in each of them.
kirmes assumes that the sequences of regulatory regions are given in two sets: a set enriched with TF targets (labeled positive) and a second set containing no or very few targets (labeled negative).
Negative sets could be genes whose expression does not change from the control to the experimental condition, and that are ideally expressed at levels above the microarray detection threshold.
kirmes is available publicly at http://galaxy.fml.mpg.de/, our Galaxy webserver.
Galaxy is an open source, scalable framework for tool and data integration developed by Giardine et al.(2005): users can upload their sequence files; kirmes will classify the input gene set and return the names of the co-regulated genes as well as discriminative motifs in a list.
fasta files of sequences can be uploaded to our Galaxy server, where we use the 6mer oligo-counting strategy and the WDS kernel.
Conservation information is not supported as it depends on the organism from which the sequences were obtained, it may not always be available and would require a significantly larger infrastructure.
There is no upper limit on the amount of input sequences in place, but at least five sequences should be uploaded for cross-validation to work.
In our case, we worked with gene sets from microarray experiments described in Section 3.2.
Here, we were looking for a heat shock element identified by Leibfried et al.(2005) as well as a binding motif for the TF wuschel in A.thaliana.
We made use of the experimental logic to obtain well-suited gene sets (cf.
Section S.1 in the Supplementary Material).
The user working with kirmes can adjust the number of most discriminative motifs to be reported by the program.
These motifs are ranked according to their contribution towards class discrimination and are good starting points for further expression or binding validation experiments.
After training of a kirmes classifier, a further prediction dataset can be uploaded.
This could for instance be comprised of the same regulatory regions, but this time for all of the annotated genes of this organism.
This is especially useful if some newly discovered genes are not yet represented on the microarray platform the expression experiments were performed with, or they were expressed close to the detection threshold and so cannot be readily excluded from the list of genes whose expression was changed in the experiment.
3.4 Experimental setup To train and test the method and compare it against baseline kernels, we first split the data into two parts (80% : 20%).
The first part is used for motif finding and SVM training.
For hyperparameter tuning, we used the first part with 5-fold cross-validation to find the optimal combinations of hyperparameters.
(The SVM and the considered kernels have several hyperparameters to be given in advance.
This includes the regularization parameter C of the SVM, the maximal length of oligomers and the maximal shift S considered in the WDS kernel.)
The second part is used to estimate the generalization performance.
Here, we measure the auROC as the generalization performance (random guessing corresponds to 50% auROC).
The above procedure is repeated five times for different splits of training and test examples (outer cross-validation loop).
As performance measure, we report the average auROC over the five splits.
To compare our method with the priority algorithm, we used the datasets from yeast chromatin immunoprecipitation experiments on tiling array chips from the extensive study by Harbison et al.(2004).
priority was developed by Gordn et al.(2008) and has been applied to this dataset before, but only in comparison to other Gibbs sampling algorithms.
We chose this algorithm 2130 [16:17 14/7/2009 Bioinformatics-btp278.tex] Page: 2131 21262133 KIRMES A B Fig.3.
(A) Accuracy of the spectrum and WDS kernels: the prediction is rarely better than random guessing for these kernels.
The kernels are not well-suited for this particular problem.
The names of the gene sets are derived from the tair7 annotation by the Swarbreck,D.
et al.(2007) and are explained in detail in Section S.1 of the Supplementary Material.
(B) Accuracy of variations of the kirmes approach: this graph shows a comparison of the basic kernels and the conservation kernels (C) combined with two different motif generation approaches: by oligo-counting (Oligo) or by Gibbs sampling (Gibbs).
The average performance () is given for each kernel variant.
The first set is taken from a control experiment, where no overrepresented motifs should occur.
to compare our method with because it was tested on exactly the kind of data we recommend to be used with kirmes.
The comparison to kirmes is in some ways different to the original setup by Gordn et al.(2008): our method needs labeled information as it is a supervised learning technique.
Thus, we employ a 5-fold cross-validation, as described above.
To make the conditions as comparable as possible, the unsupervised priority will also see only the same 80% split of sequences to find an overrepresented motif PWM.
We search for the top-ranking PWM reported on the training set in the remaining 20% of the positive and negative sequences and calculate the auROC from the distance of the best motif occurrences in the sequences from the reported PWM.
This area is compared with the one reported by the cross-validation run of kirmes.
4 RESULTS The goal is to predict the expression change status of potential target genes for overexpressed TFs based on their promoter sequence, with the datasets and the setup described above.
4.1 Comparison on A.thaliana gene sets In a first experiment, we illustrate that simple methods, as for instance SVMs with a spectrum or WDS kernel, cannot easily solve the considered classification problem.
The results are given in Figure 3A.
We can observe that essentially for all gene sets, an SVM with the spectrum kernel fails to identify positive genes (auROC close to 50%).
An SVM with the WDS kernel performs slightly better, but still produces close to random predictions.
In Figure 3B, we present results of the proposed methods in four variants: with motif discovery by Gibbs sampling versus oligo-counting as well as with and without the use of conservation.
We can make the following observations: (i) all four versions show a significantly improved performance relative to the baseline methods.
(ii) Motif finding using oligo-counting seems to work considerably better in combination with SVMs than Gibbs sampling, except on the control.
A possible reason may be that the number of considered oligos M =100,...,200 is higher than the number of motifs generated by the Gibbs sampler, M <50.
(iii) Using conservation as weighting for the WDS kernel considerably improves the recognition performance.
It results in an average performance improvement of 5 percentage points.
4.2 Contribution of vector features To evaluate the contributions of the individual feature types of the input vectors, we used a representative gene set of the A.thaliana experiments for illustration.
We considered different combinations of the feature types of the RM kernel (sequence windows, conservation information and positional information) and observed the classification performance.
The results are shown in Figure 4, where each bar corresponds to the auROC after a 5fold cross-validation with the respective features.
We also analyzed other gene sets (data not shown), and observed that the sequence window feature was consistently the most important feature, while positional information in some cases made a big difference, while in other cases its contribution was neglectable.
Positional preference or lack thereof has been studied for many TFs, e.g.by Smith et al.(2007) for the cyclic-AMP response element.
For this TF, position plays a major role in many reported experiments, but there are cases of functional binding sites that have effects on genes many thousands of base pairs away from the binding site (Smith et al., 2007).
Therefore, it is not surprising that for some sequences positional information adds no discriminative power, even within the same gene set, and for others it adds more than 10 percentage points.
Conservation always boosts the performance by 5 percentage points (cf.
Fig.3).
4.3 Comparison to PRIORITY As described in Section 3.4, we compared kirmes to the state-ofthe-art Gibbs sampler priority by Gordn et al.(2008).
This setup lets us use priority as a classifier: we measure its performance on a testing set that the Gibbs sampler did not use to build its motif PWM.
2131 [16:17 14/7/2009 Bioinformatics-btp278.tex] Page: 2132 21262133 S.J.Schultheiss et al.Fig.4.
Comparison between the contributions of each feature type of the input vector.
A dataset of 42 positive and 1562 negative genes was used from the A.thaliana experiments.
0.4 0.5 0.6 0.7 0.8 0.9 1 PRIORITY KIRMES Box Plot for KIRMES and PRIORITY Cross-validation Runs on 286 Gene Sets from Yeast ChIP-chip Experiments A re a un de r th e R O C c ur ve Quartile 1 Minimum Median Maximum Quartile 3 Fig.5.
Comparison between the Gibbs sampler priority and the kirmes approach for the task of identifying genes regulated by a TF.
The box plot shows the average auROC on the 268 gene sets, giving the minimum, maximum, median, first and third quartile values (cf.
Sections 3.4 and 4.3).
That way, we can use the top-scoring PWM from the training set and find occurrences in the testing set.
The distance to the PWM of each respective best match is scored.
This score is used to compute a ROC curve.
In Figure 5, we show a box plot on the average auROC of the two approaches on 268 gene sets from Harbison et al.(2004), preprocessed by Raluca Gordn.
kirmes is clearly the more accurate classifier, and it can also reveal the motifs it deemed most discriminant.
5 CONCLUSION The results clearly illustrate the power of our approach in exploiting the relationship between motifs as well as conservation to improve the recognition of TF targets.
All four variants significantly improve the performance over nave baseline methods.
The normalization scheme for the multiple genome alignment can be remodeled to take into account evolutionary distances.
The comparison with priority in Figure 5 shows that kirmes is, not surprisingly, a more accurate classifier than this state-ofthe-art Gibbs sampler, because it can make use of several motifs and their positional interdependence.
We are not aware of a more similar classification program that uses the same type of original data, against which we could have compared kirmes in lieu of priority.
We chose the Gibbs sampling program MotifSampler in our study because it performed best on the A.thaliana data among four compared Gibbs samplers.
It was, however, not included in the comparison of samplers by Gordn et al.(2008).
In light of the versatility of kirmes, it would seem pertinent to reinvestigate the performance on a wider selection of data, e.g.by choosing priority as a replacement for the older sampling program or the simplistic oligo-counting method.
For practical purposes, the kirmes algorithm can be applied to any combination of regulatory regions and also any organism.
A researcher may use kirmes to filter gene sets obtained through statistical methods from expression or binding data.
These sets are usually generated when evaluating microarray expression or binding data, e.g.from analyzing a regulatory network around a TF.
Careful experimental design will lead to very concise predictions.
Use of the web service integrated into Galaxy is straightforward and the resulting classification may help to select genes that should be investigated further.
kirmes can visualize the central motifs and the area surrounding them that were most discriminant during classification; this output can serve as a starting point for researchers wanting to investigate the regulatory mechanism that drives the expression changes in the experiments they have conducted.
The output is more valuable than the one shown by a Gibbs sampling algorithm, because the surrounding regions describe the regulatory module to greater detail than the original 6mer would.
Even for experiments where very complex regulatory mechanisms are suspected, kirmes will report at least dominant signatures of the most prevalent mechanism.
Here, careful experimental design and timeseries experiments can help to untangle more complex relationships.
In that sense, using kirmes is not the final solution when trying to understand regulation, but a tool that can be used to direct the design of further validation experiments.
The use by experimentalists will ultimately determine the utility of this approach and govern the direction of further extensions together with technological advances such as next-generation sequencing methods for transcriptome or protein binding data, or its application to other motif-driven biological processes like alternative splicing.
ACKNOWLEDGEMENTS The authors thank the anonymous reviewers for their helpful suggestions that greatly improved the manuscript.
G.R.
would like to thank Gabriele Schweikert for comments on the manuscript.
S.J.S.
is indebted to Raluca Gordn for providing data and very helpful comments on comparing our methods.
We thank Sren Sonnenburg for support with the python interface of the shogun toolbox and Alexander Zien for comments on the RM kernel.
Funding: Max Planck Society; W.B.
is a scholarship holder of the Cusanuswerk; J.U.L.
is an EMBO Young Investigator.
Conflict of Interest: none declared.
ABSTRACT Motivation: For many years, the Unified Medical Language System (UMLS) semantic network (SN) has been used as an upperlevel semantic framework for the categorization of terms from terminological resources in biomedicine.
BioTop has recently been developed as an upper-level ontology for the biomedical domain.
In contrast to the SN, it is founded upon strict ontological principles, using OWL DL as a formal representation language, which has become standard in the semantic Web.
In order to make logic-based reasoning available for the resources annotated or categorized with the SN, a mapping ontology was developed aligning the SN with BioTop.
Methods: The theoretical foundations and the practical realization of the alignment are being described, with a focus on the design decisions taken, the problems encountered and the adaptations of BioTop that became necessary.
For evaluation purposes, UMLS concept pairs obtained from MEDLINE abstracts by a named entity recognition system were tested for possible semantic relationships.
Furthermore, all semantic-type combinations that occur in the UMLS Metathesaurus were checked for satisfiability.
Results: The effort-intensive alignment process required major design changes and enhancements of BioTop and brought up several design errors that could be fixed.
A comparison between a human curator and the ontology yielded only a low agreement.
Ontology reasoning was also used to successfully identify 133 inconsistent semantic-type combinations.
Availability: BioTop, the OWL DL representation of the UMLS SN, and the mapping ontology are available atContact: stschulz@uni-freiburg.de 1 INTRODUCTION As high-throughput experimental methods and advanced information technology have impressively increased the amount of data, the resulting information congestion has well-known consequences such as fragmentation of data and knowledge and duplication of research efforts (Stevens, 2000).
Factual information about proteins, genes, diseases and other relevant biomedical entities are increasingly available in structured databases but their dissemination by unstructured, texts i.e.research articles, still prevails.
It is estimated that as much as 80% of new scientific facts are communicated only in their original journal To whom correspondence should be addressed.
publication (Jelier, 2005), the authors relying on a limited group of curators to manually extract, annotate and transfer these facts into the appropriate databases.
Although the pooling of such facts in databases like UniProt (Mulder, 2008) offers clear advantages over the traditional publication process, it would be of great benefit to concentrate all this information in a structured manner in one centralized repository: ongoing research information, peer-reviewed articles, external, authoritative knowledge bases, together with formalizations of the basic kinds of entities and their interrelations in formal ontologies.
Several projects [e.g.WikiProteins (Mons, 2008)] try to achieve this goal.
Although resource annotation can rely on huge terminological sources as they have evolved in the last decades, automatic reasoning services for tasks including hypothesis generation and knowledge discovery require sound ontologies, whereas they may produce suboptimal results when based on traditional terminological systems.
For this reason, we set out to examine how a formal domain ontology covering the basic kinds of entities in the biomedical domain can replace an informal legacy system.
More precisely, we created a mapping between the UMLS SN (McCray, 2003) and BioTop (Beisswanger, 2008), and assessed through this mapping how each resource contributes to the interpretation of the relation between pairs of co-occurring concepts.
The article is organized as follows: after giving an overview of basic concepts like terminology and ontology (Section 2) we describe the resources used, the mapping approach and the evaluation methodology (Section 3).
Eventually we present our results and discuss them in the context of related work (Sections 4 and 5).
2 BACKGROUND We here introduce the basic concepts underlying our work, viz.
terminology, ontology and description logics.
2.1 Terminology Both text mining and manual annotation require some kind of semantic standard.
Originally, this issue was supposed to be addressed by controlled vocabularies and terminology systems (DeKeizer, 2000a, b; ISO, 2000), a heterogeneous group of mostly language-oriented artefacts that relate the various senses or meanings of linguistic entities to one another (e.g.by assessing the synonymy between Nephroblastoma and Wilms Tumor).
Sets of (quasi-) synonymous terms are commonly referred to 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i70 i69i76 S.Schulz et al.as concepts, and in many terminology systems concepts are furthermore related by informal semantic relationships often following vague natural language predicates (narrower than, associated with, etc.).
Terminology systems are generally built to serve a well-defined purpose such as document retrieval, resource annotation, the recording of mortality and morbidity statistics or billing.
In the medical field, the largest terminological system is the Unified Medical Language System (UMLS) (Bodenreider, 2004; UMLS, 2009) in which synonymous terms from different source vocabularies are clustered into concepts, each of which is categorized using a system of semantic types (STs) (McCray, 1995).
Today, the UMLS comprises 1.9 million concepts and almost 7 million terms from close to 150 sources.
2.2 Ontology In reaction to the language-and purpose-oriented and informal approaches to representing a given domain, there has been a growing interest in using formal methods for precisely describing the invariant and language-independent properties of the entities in a domain.
In biomedicine, the Gene Ontology (GO) (Ashburner, 2000) was the pioneer of moving from a purpose-oriented annotation vocabulary to a more principled resource.
Similarly, collaborative initiatives have emerged such as the Open Biomedical Ontologies (OBO) Foundry (Smith, 2007), the continuing development of SNOMED CT (SNOMED, 2009), which is increasingly challenged and guided by ontological principles, as well as increasing mutual awareness between the Semantic Web and Life Sciences communities (Ruttenberg, 2007; Sagotsky, 2008).
The term ontology stems from analytical philosophy, concerned with the question of what exists?
(Quine, 1948).
It became popular by information sciences, and despite quite contradictory definitions (Kusnierczyk, 2006) it has increasingly been used to refer to domain representation of various kinds.
In order to emphasize the use of a formal language in domain representations, we here subscribe to the concept of formal ontologies (Guarino, 1998) as theories that attempt to give precise representations of the types of entities in reality, of their properties and of the relations among them, using axioms and definitions that support algorithmic reasoning.
2.3 Upper-level ontologies The purpose of upper domain ontologies is to define the foundational kinds and relations relevant to the entire domain.
In the life sciences, this includes classes like gene, protein, cell, tissue, nucleotide, population, organism, diagnostic procedure and biological function, among others.
Upper domain ontologies can either be used alone as a source of basic categories (e.g.for the coarse annotation of resources) or as a common reference for more specialized domain ontologies.
In contrast to domain-specific ontologies such as the GO, upper ontologies propose to trade detail for scope by introducing general categories that are the same across all domains.
Whether or not this is achievable and desirable has been subject of debate.
Nevertheless, several upper-level ontologies have been developed and are being maintained such as BFO1 (Smith, 2007a), DOLCE2 (Gangemi, 1Basic Formal Ontology.
2Descriptive Ontology for Linguistic and Cognitive Engineering.
2002; Masolo, 2003), SUMO3 (Pease, 2008) or GFO4 (Heller, 2004).
More recently, development of application-oriented domain ontologies such as the OBO5 ontologies have led to the proposal of a kind of intermediate-level ontologies, also called top-domain ontologies, such as the Simple Bio Upper Ontology (Rector, 2006), GFO-Bio (Hoehndorf, 2008) or BioTop (Beisswanger, 2008).
In contrast to these recent and more theory-laden resources, the pragmatic UMLS SN6 , developed 15 years ago, can be regarded as the archetype of a biomedical domain upper ontology (McCray, 2003).
Moreover, the SN has already proved its usefulness in providing a consistent categorization of all concepts represented in the UMLS Metathesaurus.
From an upper-level ontology viewpoint, domain upper ontologies play the role of domain ontologies, but from a domain perspective they act as upper ontologies.
For example, the placement of BioTop under BFO or DOLCE could be seen as a domain ontology placed under an upper ontology.
Conversely, BioTop itself may also play the role of an upper ontology when linked to the Cell Ontology (CO) or the GO.
Different upper-level ontologies not only use different formalisms for their representation but also represent the domain in slightly different ways.
As a consequence, the constraints they impose on domain-specific ontologies affect the result of reasoning services based on these upper-level ontologies.
2.4 Description logics Since the 1980s, the application of formal reasoning on ontology structures has led to various formalisms.
Later on, the vision of the Semantic Web (Berners-Lee, 2001) has resulted in a significant standardization of representation languages, formats and reasoning engines.
One of the most noteworthy standards of the Semantic Web was the development of the Web ontology language OWL (Horrocks, 2003) and especially its expressive but still computable subset, OWL description logic (DL).
DLs constitute a family of decidable fragments of first-order logic which have a clean and intuitive syntax (Baader, 2007).
They come in various flavours, ranging from lightweight to highly expressive ones.
The trade-off between expressivity of the logic and computability (and thus, scalability) of its reasoning has to be made in order to properly address the ontology application.
Whereas overly inexpressive DL may lead to underspecifications that imply unintended models of the ontology, highly expensive reasoning makes it infeasible from practical viewpoints.
OWL DL constitutes a compromise between expressiveness and decidability and is supported by DL classifiers like RACER, Fact++ and Pellet (Haarslev, 2003; Tsarkov, 2006; Sirin, 2007).
Description logics are built around the notions of class and relationship and follow model-theoretic semantics.
Classes such as Heart are interpreted as sets of all instances belonging to that class, i.e.here all particular hearts in the domain.
Relationships then are sets of pairs of class instances like hasPart, which extends to all pairs of objects in the domain that are related in terms of parts and wholes.
So are all pairs of heart instances with their 3Suggested Upper-Merged Ontology.
4General Formal Ontology.
5Open Biomedical Ontologies.
6Unified Medical Language System.
i70 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i71 i69i76 Alignment of the UMLS semantic network with BioTop respective mitral valve instances in the extension of hasPart.
We will illustrate DL syntax and semantics through a set of increasingly complex examples, starting with the class Liver, which in our domain extends to all individual livers of all organisms.
Analogously, the class BodilyOrgan then extends to all individual bodily organs.
When those two statements are put together, we can introduce the key concept of taxonomic subsumption: the class BodilyOrgan forms a superclass of the class Liver, i.e.the former subsumes the latter if and only if all particular livers are also instances of the class BodilyOrgan.
In DL notation, this taxonomic subsumption is expressed by the operator, e.g.Liver BodilyOrgan, and is also known as subtype, subclass or is-a relationship.
It is important to stress that this kind of relationship always relates two classes.
In contradistinction to this, the instantiation relationship relates an individual entity to some class, e.g.the particular liver of the first author of this article to the class Liver.
Such simple class statements can then be combined by different operators and quantifiers, e.g.the (and) operator and the existential quantifier (exists).
For example, InflammatoryDisease hasLocation.Liver denotes all instances that belong to the class InflammatoryDisease and are further related through the relationship hasLocation to some instance of the class Liver.
This example actually gives both necessary and sufficient conditions in order to fully define the class Hepatitis: Hepatitis InflammatoryDisease hasLocation.Liver.
The equivalence operator indicates that every instance of hepatitis is necessarily an inflammatory disease that is located in some liver.
But through the equivalence operator, one can go in the other direction as well and say that any inflammatory disease that is located in some liver can be classified as hepatitis.
In practice, the term on the left and the expression on the right are equivalent.
The constructors introduced so far allow for automated classification and the computation of equivalence, but not for satisfiability checking.
This is, however, important, wherever the validity of an assertion is to be assured.
For instance, the assertion Immaterial Object  hasPart.ImmaterialObject restricts the value of the role hasPart by using the universal quantifier (only).
It should therefore reject any assertion that states that an immaterial object (e.g.a space) has a material object as part.
However, a nave use of this construct tends to fail.
The reason of this is the socalled open world assumption: unless otherwise stated, everything is possible.
The following class Strange Object Immaterial Object hasPart.MaterialObject would remain consistent as long as we do not explicitly state that there is nothing that can be both a material and an immaterial object: Immaterial Object MaterialObject (with being the negation operator not).
This means that nothing can be equally an instance of either object, i.e.the two classes are disjoint.
3 MATERIALS AND METHODS 3.1 UMLS SN The provision of an overarching conceptual umbrella over the biomedical domain was the rationale for the development of the UMLS SN (McCray, 2003).
A tree of 135 STs forms the backbone of the SN.
It is partitioned into the branches entity and event, in which nodes are linked by subclass relations.
In addition, the SN contains a hierarchy of 53 associative relationships (e.g.location_of, treats).
These relationships are used to form 612 assertions (e.g.Tissue, location_of, Diagnostic Procedure) from which 6 252 additional assertions can be inferred.
For each semantic relationship, domain and range are specified in terms of one or more STs.
Each concept from the UMLS Metathesaurus is categorized by at least one ST from the SN.
The UMLS SN is a widely used resource in biology and medicine.
However, it suffers from some well-known shortcomings (class descriptions that are ambiguous or vague, relatively low granularity, arbitrary divisions) (Schulze, 2004).
In view of that we wanted to assess these limitations by making them explicit in an OWL DL representation and to explore alternative upper domain ontologies.
3.2 BioTop BioTop (Beisswanger, 2008; Schulz, 2006) originated from a redesign and enrichment of the GENIA ontology.
Like the UMLS SN, its backbone is constituted by a taxonomic tree, consisting of 334 classes.
Its relation hierarchy is populated with 60 relations with domain and range constraints.
The main difference from the UMLS SN is given by its use of OWL DL (see Section 3.1).
BioTop contains 636 logical axioms among which there are subclass, disjointness and equivalence axioms.
The latter (61) enable the computation of additional taxonomic links using DL reasoners.
BioTop exhibits links to the upper-level ontologies DOLCE (Gangemi, 2002; Masolo, 2003), BFO (2007a) and the OBO relation ontology (Smith, 2005).
Furthermore, it provides mappings to OBO Foundry ontologies (e.g.GO, CO, FMA, ChEBI).
3.3 Mapping Our main objective of bridging between the UMLS SN and BioTop was to capitalize on the categorization of the UMLS Metathesaurus with SN types on the one hand, and to benefit from the ontologically sound and computationally more sophisticated architecture of BioTop on the other.
The aim was to represent the totality of the SN knowledge using BioTop, encompassing the SN types and hierarchical organization as well as the semantic relations with their domain and range restrictions.
In order to meet this requirement, an analysis of the UMLS SN semantics in the light of description logics and its transformation into the formalism used by BioTop had to be performed.
Technically, the plan was to use a central mapping file, which imported both UMLS SN and BioTop, and served as a store for class and relation equivalences and restrictions.
In order to provide mappings for each UMLS SN type, we adjusted the coverage of BioTop wherever justified.
3.4 Assessment methodology 3.4.1 Formative evaluation of BioTop: We used the logic-driven knowledge reengineering described by Schulz (2001), which employs an iterative approach.
Each major ontology redesign (including mapping) step is checked by a description logics reasoner, the results of which are then analysed and corrected under two perspectives: first, the classes tagged as inconsistent are identified and the causes are investigated and repaired; second, every time the ontology has reached a consistent state, the logical entailments are analysed for adequacy.
Whenever inadequate entailments are encountered, the causes are investigated and fixed.
3.4.2 Consistency of SN-type combinations: As numerous UMLS Metathesaurus concepts are categorized by more than one ST, their consistency against BioTop should be checked, based on the SN-BioTop map.
On the basis of the assumption that combinations of STs linked to Metathesaurus concepts constitute conjunctions, all occurring combinations are identified and then attached to the ontology.
3.4.3 Named entity co-occurrence: Named entity recognition (NER) is a widely used text mining technique (Park, 2006).
A well-known problem in NER is when the word or phrase to be recognized is ambiguous, i.e.it denotes different things.
The implementation of the UMLS SN in BioTop offers the possibility to check ambiguous named entities for whether the i71 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i72 i69i76 S.Schulz et al.competing referent concepts are compatible with respect to the SN relations allowed for UMLS STs.
We obtained 100 million unique pairs from 15 million PubMed abstracts that had been mined with the state-of-art named entity (NE) recognizer Peregrine (Schuemie, 2007) to recognize UMLS concepts and Uniprot identifiers referred to within the same sentence.
We here consider only the UMLS concept pairs.
The task was to manually assess a sample of 300 UMLS concept pairs.
The curator assessed the plausibility of the linkage between the two concepts in the sentence context.
Each co-occurring pair was first checked against the SRSTRE1 table from the SN and alternatively against the mapping ontology, based on the OWL DL implementation of the BioTop/UMLS SN integration.
4 RESULTS 4.1 Mapping of UMLS STs DL-based ontologies are hierarchies of types (classes) that can be instantiated by particular entities only.
According to (McCray, 2002) we can consider the SN as a hierarchy of upper-level classes (regardless of the naming of some of the types that suggest a meta-level interpretation, e.g.the type Functional Concept).
The categorization relation (that attaches UMLS Metathesaurus concepts to SN types) can therefore be mostly interpreted as a taxonomic subsumption relation (is-a).
Exceptions include geographical locations and a few other true instances, e.g.laws and persons.
In these cases the categorization relation is to be interpreted as an instance-of relation.
The mapping was done as follows.
First of all, the taxonomic tree of the UMLS SN types was remodelled in OWL (SN.OWL) by expressing the taxonomic subsumption (is-a) as OWL subclasses.
No further assumptions were made.
Especially, no partitions were introduced, as the source and its documentation do not make any statements as to whether STs are mutually exclusive.
On the basis of the textual (SN, BioTop) and the formal (BioTop) definitions available we then attempted to map each ST to BioTop.
Lexical mapping criteria were not used.
In cases of doubt, domain experts were consulted.
The mapping was performed in close collaboration among the authors.
At several occasions, problems encountered when accommodating STs in BioTop were discussed in face to face meetings, conference calls and e-mail discussions.
In controversial cases other existing ontologies, e.g.OBI, were consulted.
For the mapping a new OWL-bridging file was created that referenced both resources with owl:imports statements using the Protg 4 ontology editor.7 This allowed us to bring together two resources that were out of our direct control and to introduce new assertions linking them.
Mapping the STs of the SN to BioTop the following cases could be distinguished.
4.1.1 Direct match: The ST is equivalent to a class in BioTop, or the difference is small enough that creating a separate new class alongside an existing one would not be justified; e.g.Animal in BioTop has the exact same meaning as in the SN.
4.1.2 Restriction: No BioTop class is a straight match for the ST, but it can be defined by restricting an existing BioTop class, e.g.AnatomicalAbnormality is mapped to the expression: OrganismPart bearerOf.PathologicalCondition, where OrganismPart and PathologicalCondition are existing BioTop classes and bearerOf is an existing BioTop relation.
7http://www.protege.stanford.edu/.
4.1.3 Union: If the ST cannot be defined by a single class, it corresponds to the union of several classes.
Any combination of the previously described types can participate in the union.
For example, the SN type Gene or Genome was mapped to the disjunction biotop:Gene 	 biotop:Genome.
4.1.4 Out of scope: The ST cannot be expressed using any of the options above; the immediate solution was to create a new class inside the mapping file itself, defined as the subclass of an existing BioTop class and map the ST to this new class.
In the incremental mapping/BioTop redesign process, all ST leaf nodes (but two) introduced this way were recreated in BioTop.
The nonmatching STs (e.g.daily or recreational activity) were mapped to a more general BioTop class.
4.1.5 No match: The ST is regarded meaningless for BioTop in one of the following cases: its definition does not sufficiently differentiate it from its parent, it is too abstract, or it is only included in the SN as a housekeeping node in order to group more meaningful child nodes.
For example, Chemical Viewed Functionally has a meta-class meaning (it groups UMLS concepts, but is useless as a distinguishing criterion for their individuals) which cannot be represented by BioTop.
Leaving the class undefined allows for the existing subsumption hierarchy of the SN to reason up to the nearest parent that does have a mapping, in this case Substance.
Most STs on an upper level have imprecise definitions and do not coincide with any BioTop class, e.g.Idea or concept (An abstract concept, such as a social, religious or philosophical concept.
), the definition of which seems not plausible to its subtypes, e.g.Geographic Area.
The names, textual definitions and the hierarchical context of SN types created mapping difficulties in many cases.
For instance, the ontologically crisp distinction between function and process is mixed up in the SN.
So does the type Phenomenon or Process subsume Pathologic Function, which is a parent of, e.g.Neoplastic Process.
As a result, some upper-level classes were mapped not to a single class in BioTop but to the union of several classes.
An example is Spatial Concept, defined by the union of Body Location or Region, Body Space or Junction, Geographic Area and Molecular Sequence.
Others were mapped to quite complex expressions including disjunctions, value restrictions and exclusions.
4.2 Interpretation and mapping of UMLS semantic relations The treatment of UMLS SN semantic relations turned out to be more complicated thus requiring a two-step approach; they first have to be semantically interpreted and properly built into an OWL DL model before they can be mapped to BioTop.
Their simple interpretation as description logics relations (object properties) is semantically problematic as SN relations range over STs (i.e.instantiable classes) whereas object properties range over individual entities.
Such an interpretation of concept to concept relations in the light of formal logic has been repeatedly discussed in the recent years (Smith, 2005).
For example, five different possible interpretations of SN triples are discussed in Kashyap (2003).
For most UMLS semantic relations there is a quite complex arrangement of domain and range restrictions, in which certain range restrictions are only valid with certain domain restrictions.
For instance, the UMLS SN restricts the domain of the treats relation to i72 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i73 i69i76 Alignment of the UMLS semantic network with BioTop drugs and physicians, and its range to patients and diseases (among others).
However, it does not allow the combination of drug and patient, or health professional and disease.8         Range   Domain Drug Physician Disease allowed disallowed Person disallowed allowed We could, of course, ignore this and take simply the union of the extension of the UMLS concepts as the restriction of new BioTop relations that have to be included into the ontology.
Thus we would have to accept unintended models, e.g.that a drug treats a person.
We discussed and implemented different solutions how to adequately represent these constraints using OWL DL.
As a first solution, we introduced subrelations, in the following style (again simplified): treatsMED treats (domain: Drug, range: Disease) treatsPHY treats (domain: Physician, range: Person).
In this first step, we obtained a total number of 210 relations (OWL object properties).
However, we have to acknowledge that this is a rather cosmetic solution, because such a model is only able to reject unwanted assertions if the specialized relations but not the general ones are used.
Furthermore, by lack of disjointness statements in the class hierarchy it cannot even be rejected that, e.g., something is both a drug and a physician.
This is, however, not a fault of the representation language but an underspecification of the UMLS SN.
As a second solution we discussed the following, as it achieves the desired result without the creation of subrelations.
Drug treats.Disease Physician treats.Person Together with: treats.Disease Drug treats.Person Physician The drawback is here that this solution uses general concept inclusions (GCIs).
Although they are part of the OWL DL specifications, they were not supported by our tools.
Both approaches, however, face a severe problem when it comes to the mapping to BioTop, as the latter includes only a relatively low number of relations.
Enhancing BioTop by the whole array of SN relations would conflict with its design principle to keep the set of relations small but semantically precise, restricting them to those that are needed for BioTop class definitions.
This is not the case with most SN relations: treats, interacts, diagnoses, etc.
Instead, BioTop contains, in its Processual Entity branch, already classes such as Treating, Interacting, etc.
which convey the same meaning and can be regarded as reifications.
TreatingPerson Action  has_agent.
Physician  has_patient.
Person  has_agent.
Physician  has_patient.
Person TreatingDisease Action  has_agent.
Drug has_patient.
Disease  has_agent.
Drug has_patient.
Disease Treating TreatingPerson 	 TreatingDisease 8For the sake of understandability the example is simplified and does not use the lengthy UMLS SN names.
We therefore decided to mapas an alternative approachthe SN relational constraintsexpressed as triplessuch as D1 REL R1, D2 REL R2, D3 REL R3, , Dn REL Rn (Di referring to domain and Ri to range) to an equally uncomplicated DL formula.
As a consequence, we do not need to create new DL relations (which would contradict the DL design principles), but simplify the above formula: REL1 has_domain.
D1  has_range.
R1 REL2 has_domain.
D2  has_range.
R2 REL3 has_domain.
D3  has_range.
R3... RELn has_domain.
Dn  has_range.
Rn REL REL1 	 REL2 	 REL3 	 	 RELn has_domain and has_range are then mapped to biotop: has_agent and biotop:has_patient.
Of course, the agent/patient reading does not make sense with many spatial or temporal relations.
In these cases we extended the map by additional value restrictions.
Finally, there are SN relations that cannot be expressed as relations between particulars because they simply do not relate anything at the level of particulars.
The prototypical example is prevent, such as in the statement contraceptive drugs prevent pregnancy.
On a UMLS concept level it is, without doubt, sensible to express this in a relational form, such as prevents (contraceptive drugs; pregnancy).
Such a close-to-human-language assertion on prevention carries several implicit assumptions that must be made clear before expressing it via an ontology; preventing pregnancy does not exclude the possibility of becoming pregnant but it brings about a strong risk reduction.
Furthermore, there is both a temporal and a dose association between the drug and the risk.
We can therefore rephrase Contraceptive drugs prevent pregnancy as follows: The administration of contraceptive drugs of an adequate dose and regularity to a woman reduces her pregnancy risk within a defined timeframe or more simply: The administration of contraceptive drugs to a woman reduces her pregnancy risk within a defined timeframe.
We could express this as follows: PregnancyRiskReductionBySubstanceIntake Action has_agent.Substance has_agent.Substance  has_patient.
(Risk ( inheres_in.
Organism  inheres_in.
Organism) risk_of.Pregnancy) This digression illustrates the difficulty if not impossibility of an ontologically precise formal reconstruction of seemingly simple close-to-language predicates.
For the semantic relationship mapping we proceeded the following way: all relationships were reified (i.e.expressed as classes) and added as OWL classes using value restrictions on the roles has_agent and has_patient.
Those relationships which had a direct correlate in BioTop (i.e.the SN spatiotemporal relationships) were additionally mapped directly to BioTop relationships (object properties).
In both cases the domain and range-specific subrelations were accounted for by additional subclasses/subrelations (in analogy to the Treating example above).
The reification classes were furthermore provided with so-called covering axioms that assure the enforcement of one of the child classes with their restrictions.
Again, no mappings were performed for some upper-level relationships i73 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i74 i69i76 S.Schulz et al.(and, accordingly, to upper-level reification classes), for the same reasons as explained for the type hierarchy.
The final result of the mapping of each ST to BioTop yielded 132 equivalence and 19 subclass axioms in the mapping ontology.
The OWL reconstruction of the UMLS SN comprised 626 classes and 1530 axioms, and BioTop grew from 200 to 334 classes, 30 to 40 object properties and from 470 to 636 axioms.
4.3 Assessment results The whole mapping exercise constituted an ideal testbed for the ongoing quality assurance and formative evaluation of BioTop.
Because of the constant need of inconsistency checking and resolving, many hidden errors in BioTop were detected, especially faulty disjointness axioms (e.g.Organic Chemical was disjoint from Carbohydrate), unrecognized ambiguities (e.g.Sequence as information entity versus molecular structure) as well as granularity mismatches (e.g.Chromosome as molecule).
The maintenance work was, however, very time consuming, totalling at least one person year, divided among five modellers.
A significant advance for inconsistency checking and resolution was achieved by the use of a new Protg add-in that presents precise explanations of entailments in OWL ontologies (Horridge, 2008).
Runtime performance, however, proved to be a major drawback.
The more axioms are being added (especially negations, disjointness axioms, and inverse properties) the more the performance decreases so that classification time now constitutes a major obstacle in the whole ontology construction and maintenance process.
Nevertheless, it was possible to use the ontology in order to validate an important feature in UMLS, viz.
multiple ST categorization.
In the 2008 Metathesaurus (totalling more than 1.80 million concepts) release there are 397 different combinations of two to four STs, linked by about 220 000 UMLS concepts.
On the basis of the assumption that STY combinations should be interpreted as conjunction, we checked each occurring combination for consistency.
The DL classifier recognized 133 combinations as inconsistent, affecting a total of 6116 UMLS concepts.
The most frequently occurring unsatisfiable type combination was Manufactured Object with Health Care Related Organization (e.g.Hospital as building versus organization).
The preliminary results of the named entity experiment are, however, less encouraging (Table 1).
Because of so many ambiguities, the curator had made a clear assessment of semantic relatedness in only half of the cases.
The comparison of the manual classification to the automated one (into true and false) clearly demonstrates the dilemma.
The checking against the UMLS SN table STSTR1 shows a certain correlation with the curators judgment but still produces many false negatives and false positives.
BioTop via the SN and the mapping ontologyrejects extremely few associations.
In order to correctly interpret these results, we emphasize that the question of whether two UMLS concepts are related is not the same as to ask whether their STs exhibit some allowed relationship.
For instance, the expert rating for the association between Superoxide reductase (ST: Enzyme) and Aldehyde (ST: Organic Chemical) was negative.
Of course, this does not mean that any kind of association between Enzyme and Organic Chemical should de disallowed.
On the contrary, these two STs are closely associated, which is not Table 1.
Named entity co-occurrence results Expert judgement: concepts related Expert judgement: concepts unrelated SN: related 31 22 SN: unrelated 21 71 BioTop: related 52 90 BioTop: unrelated 0 3 changed by the fact that most random combinations of some enzyme with some chemical are irrelevant.
The low rate of rejections by BioTop demonstrates the problem of the so-called open-world semantics (Baader, 2007), i.e.all models are accepted unless they are explicitly falsified.
In the case a description logics ontology is used for this kind of consistency check, the modellers have to be very meticulous in filling the holes.
On the other hand, it must be acknowledged that the OWL reconstruction of the idiosyncratic categorization in SN required many disjunctive statements which resulted in a relaxation of the domain and value restrictions.
In any way, it is known to be difficult to keep an OWL model water-proof in this aspect, and OWL has recently been criticized that it is generally ill-suited for tasks like schema validation (Rajsky, 2008).
However, we argue that this is not an inherent but rather a tooling problem, at least for those description logics dialects that support some kind of negation.
As a consequence, we performed a thorough fault analysis and could identify and fix several underspecifications that gave rise to unintended models.
5 RELATED WORK There are many reports in the literature about the conversion of thesauri, frame knowledge bases and ontologies from various representational formats into description logics.
Examples are Pisanelli (1998) and Schulz (2001) for the UMLS; Beck (2003), Dameron (2005) and Golbreich (2006) for the Foundational Model of Anatomy; Wroe (2003) and Egana (2008) for the GO and Heja (2007) for ICD-10.
What most of these approaches have in common is (i) that the mapping is not straightforward, (ii) it relies on several ontological basic assumptions that are not explicitly stated in the sources, e.g.on disjointness axioms, on the intended meaning and the algebraic properties of relationships and (iii) that not all knowledge conveyed by the sources is expressible in description logics, due to the language constraints.
The UMLS SN was targeted by Kashyap (2003) who concluded that the logical interpretation of the semantic relations in the SN should depend on the application in which the ontology is to be used.
More specifically, ontological aspects of the UMLS SN were discussed by Schulze-Kremer (2004).
The latter authors acknowledge the importance of the SN for the semantic integration of terminology but spot a number of weaknesses future revisions should address.
A major point of criticism is the mixture of concrete with abstract entities, real entities with bauplan entities, objects with their roles, functions and processes.
This mainly coincides with our mapping experiences as described in Sections 4.1 and 4.2. i74 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i75 i69i76 Alignment of the UMLS semantic network with BioTop 6 CONCLUSION We have described the ongoing development and improvement of a semantic resource, the life science ontology BioTop in the light of the mapping to the legacy UMLS SN.
The purpose of this effort is to bring together the large amount of data categorized by the latter with the formal foundation of the former, using emerging standards and tools developed by the Semantic Web community.
Semantic and terminological support is especially important for facilitating an opening of the curation process towards a broader community.
The alignment of a formal ontology with a relatively informal system of hierarchically ordered categories like the UMLS SN challenges the ontology engineer to formally re-interpret the latter and to overcome its ontological shortcomings.
The logical machinery of description logics, implemented in reasoning engines, was an indispensable part of the mapping process, which, ultimately, not only provided a consistent mapping ontology but contributed, by large, to error detection and improvement of BioTop.
We described two assessment experiments.
One of them, aiming at satisfiability checking of SN-type combinations yielded good results that revealed hidden ambiguities of UMLS concepts.
The other, however, generated rather poor results.
It attempted to use the ontology for determining which UMLS concept pairs were closely related to each other.
As a result, the mapping ontology rejected very few models, thus supporting the recent critique on the suitability of OWL for schema verification.
However, this result also challenged the evaluation scenario: judgements on the relatedness of very specific instances can not be necessarily carried over to judgements at the level of STs.
Nevertheless, it was disappointing because the modellers had spent a great effort in partitioning the BioTop ontology in order to antagonize the unwarranted effects of the open-world assumption.
This is an issue where more sophisticated tool support for OWL ontology construction and validation is desperately needed, in order to grant formal ontologies and logic-based reasoning a central place in future high-throughput and high-impact life sciences knowledge management technologies.
ACKNOWLEDGEMENTS The authors thank Martin Boeker (Freiburg) and Holger Stenzhorn (Freiburg) for their BioTop maintenance efforts, as well as Robert Hoehndorf (Leipzig) and Alan Rector (Manchester) for fruitful discussions.
Funding: EC STREP project BOOTStrep (FP6 028099); Intramural Research Program of the National Institutes of Health; National Library of Medicine.
Conflict of Interest: none declared.
ABSTRACT Summary: Community curationharnessing community intelligence in knowledge curation, bears great promise in dealing with the flood of biological knowledge.
To exploit the full potential of the scientific community for knowledge curation, multiple biological wikis (bio-wikis) have been built to date.
However, none of them have achieved a substantial impact on knowledge curation.
One of the major limitations in bio-wikis is insufficient community participation, which is intrinsically because of lack of explicit authorship and thus no credit for community curation.
To increase community curation in bio-wikis, here we develop AuthorReward, an extension to MediaWiki, to reward community-curated efforts in knowledge curation.
AuthorReward quantifies researchers contributions by properly factoring both edit quantity and quality and yields automated explicit authorship according to their quantitative contributions.
AuthorReward provides bio-wikis with an authorship metric, helpful to increase community participation in bio-wikis and to achieve community curation of massive biological knowledge.
Availability: http://cbb.big.ac.cn/software.
Contact: zhangzhang@big.ac.cn Supplementary information: Supplementary data are available at Bioinformatics online.
Received on April 1, 2013; revised on May 8, 2013; accepted on May 13, 2013 1 INTRODUCTION Biological knowledge is generated at ever-faster rates and dispersed among researchers and across literatures.
As each new biological study has become increasingly dependent on the availability of existing knowledge, comprehensive and up-to-date collection of biological knowledge across a wide variety of research fields is of critical significance in life sciences (Clark, 2007).
Traditionally, biological knowledge has been aggregated through expert curation, conducted manually by dedicated experts.
However, with the burgeoning volume of biological data and increasingly diverse densely informative published literatures, expert curation becomes more and more laborious and time consuming, increasingly lagging behind knowledge creation.
Accordingly, community curationharnessing community intelligence for knowledge curationhas gained significant attention as a solution to this issue (Salzberg, 2007; Waldrop, 2008; Zhang et al., 2011).
A successful example that engages community intelligence in knowledge aggregation is Wikipedia that features up-to-date content, huge coverage and low cost for maintenance.
Spirited by the extraordinary success of Wikipedia, multiple biological wikis (bio-wikis) have been built to date (Supplementary Table S1).
However, bio-wikis have not achieved a substantial impact on community curation of biological knowledge (Finn et al., 2012).
One of the major limitations in bio-wikis is insufficient participation from the scientific community, which is intrinsically because of lack of explicit authorship and thus no credit for community-curated contributions (Finn et al., 2012; Howe et al., 2008).
A valuable attempt has been made to motivate community contributions in wikis by means of social rewarding techniques (Hoisl et al., 2007), but it does not provide explicit authorship for any wiki page.
Although authorship has been introduced in a non-MediaWikibased system (Hoffmann, 2008), it only links every sentence to its author but does not provide a quantitative measure of authorship, and most important, it is inapplicable to extant bio-wikis that are largely built on MediaWiki (a free, open source and widely used wiki engine, which is adopted by Wikipedia).
Several initiatives based on semantic web technologies have already emerged for biological knowledge management (Antezana et al., 2009).
However, they do not promise to manage or quantify authorship of the free text in bio-wikis.
To increase community curation in bio-wikis, here we develop AuthorReward, an extension to MediaWiki, to reward community-curated efforts in bio-wikis by contribution quantification and explicit authorship.
2 ALGORITHMS MediaWiki allows anyone to develop customized functionalities by packaging a bunch of codes as MediaWiki extensions.
Thus, AuthorReward is implemented as an extension to MediaWiki.
Although MediaWiki itself includes an infrastructure for individual contributions to be recognized, it only records the revision history and provides no explicit authorship.
*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com A wiki page contains a collection of knowledge on a specific subject, where multiple researchers are most likely to collaboratively provide edits.
AuthorReward aims to provide a viable quantification for researchers contributions in bio-wikis.
A major concern to automated authorship has been ensuring that authorship cannot be manipulated by spurious, short-lived edits (Supplementary Text S1).
For any wiki page p, we assume there are a series of edit versions v0, v1, v2,..., vn, where version v0 is empty and n40.
AuthorReward counts multiple successive versions edited by a researcher as one version.
Thus, any neighboring versions, vi 1 and vi (where 1 i n), are edited by different researchers.
The edit distance between vi and vj, termed as d(vi, vj) (where i5j), is computed by the Levenshtein distance (LD) (Levenshtein, 1966) that measures the minimum number of edit operations (insertions, deletions and substitutions) required to transform one string into the other.
In AuthorReward, the contribution score of version vi, CS(vi), is formulated straightforwardly as CSvi cdvi1, vn dvi, vn, 1 where c is the scale factor, d(vi 1, vn) is the edit distance between vi 1 and vn and d(vi, vn) is the edit distance between vi and vn.
In Equation (1), CS(vi) factors edit quality as well as edit quantity in an implicit manner; the edit quantity of version vi, QTY(vi), amounts to the edit distance between vi and its previous version vi 1, viz., d(vi 1, vi) [Equation (2)], and the edit quality of version vi, QAL(vi), corresponds to whether the edit persists in comparison with the last version vn [Equation (3)].
QTYvi dvi1, vi 2 QALvi dvi1, vn dvi, vn dvi1, vi 3 According to the triangle inequality, QAL(vi) ranges from 1, when the edits were entirely reverted, to 1, indicating that the edits were totally preserved in the last version.
Therefore, QAL(vi), in other words, measures how long the edit lasts in the latest version; a high (or low) quality score is given for version vi, if it is long-lived (or short-lived).
Consequently, CS(vi) can be expressed by QTY(vi) multiplied by QAL(vi), namely, CS(vi)QTY(vi)QAL(vi).
Thus, CS(vi) is not easily gamed, providing a viable quantification for researchers contributions.
Considering that one researcher may provide many discontinuous edits across the evolution of a wiki page, and thereby contribute multiple versions in one wiki page, the contribution score of researcher r in page p, S(r, p), is quantified as the sum over all contributed versions, Sr, p X vi2Er, p CSvi, 4 where E(r, p) is a set of versions contributed by researcher r in page p. As a consequence, the total contribution of researcher r in a bio-wiki is termed as the sum of multiple contribution scores in all participated pages, Sr X p2P Sr, p, 5 where P is a set of pages in which researcher r provides edits.
3 APPLICATION AND FEATURES To test the functionality of AuthorReward, we installed it in RiceWiki (http://ricewiki.big.ac.cn).
For testing purposes, we chose the semi-dwarfing gene (sd1), which is one of the most important genes deployed in modern rice breeding and is also known as the green revolution gene affecting plant height of rice.
There were nine researchers collaboratively annotating the sd1 gene, providing 87 versions as of August 23, 2012 (Supplementary Table S2; http://ricewiki.big.ac.cn/index.php/ Os01g0883800).
As testified on the sd1 gene (Supplementary Fig.S1), AuthorReward is capable of yielding sensible quantitative contributions and providing automated explicit authorship, consistent well with perceptions of all participated contributors.
Moreover, AuthorReward features good compatibility with any MediaWikibased system and simple installation, consequently possessing a broad scope for its application and providing a consistent appearance and functionality as Wikipedia.
4 CONCLUSION AuthorReward provides bio-wikis with an authorship metric, featuring robust contribution quantification and automated explicit authorship.
When contribution is appropriately quantified and authorship is duly rewarded, it is possible to exploit the full potential of the scientific community in knowledge curation.
Although AuthorReward does not contribute directly to the integration of biological knowledge, it provides a standard practice to reward community-curated efforts, which in return can increase community participation in bio-wikis for knowledge curation.
Thus, our intention here is to produce an automated, simple and robust authorship metric and no automated measure will be able to gauge scientific content.
AuthorReward can be used in combination with semantic web technologies, potentially promising a significant advance for harnessing community intelligence for knowledge curation.
In addition, social rewarding techniques (e.g.peer rating) can be used together with AuthorReward for contribution evaluation.
Moreover, it is likely in the long term to integrate community-curated efforts across multiple bio-wikis for each researcher, which accordingly requires close collaborations among bio-wikis and standardized mechanisms for individual identity recognition (e.g.OpenID at AuthorReward provides a standard practice to reward community-curated efforts in bio-wikis, and it is of interest to the scientific community intending to perform knowledge curation collectively and collaboratively in bio-wikis and also other domain wikis.
ACKNOWLEDGEMENTS The authors thank Jun Yu, Lina Ma, Gang Wu, Hao Wu, Chao Xu, Jian Sang and Ang Li for their valuable comments on this work.
Funding: National Programs for High Technology Research and Development (863 Program; 2012AA020409); the 100-Talent Program of Chinese Academy of Sciences (Y1SLXb1365); 1838 L.Dai et al.National Natural Science Foundation of China (60803050, 61132009); USA National Institutes of Health P01 (GM068067).
Conflict of Interest: none declared.
ABSTRACT Motivation: Earlier studies of protein structure revealed closed loops with a characteristic size 2530 residues and ring-like shape as a basic universal structural element of globular proteins.
Elementary functional loops (EFLs) have specific signatures and provide functional residues important for binding/activation and principal chemical transformation steps of the enzymatic reaction.
The goal of this work is to show how these functional loops evolved from pre-domain peptides and to find a set of prototypes from which the EFLs of contemporary proteins originated.
Results: This article describes a computational method for deriving prototypes of EFLs based on the sequences of complete genomes.
The procedure comprises the iterative derivation of sequence profiles followed by their hierarchical clustering.
The scoring function takes into account information content on profile positions, thus preserving the signature.
The statistical significance of scores is evaluated from the empirical distribution of scores of the background model.
A set of prototypes of EFLs from archaeal proteomes is derived.
This set delineates evolutionary connections between major functions and illuminates how folds and functions emerged in pre-domain evolution as a combination of prototypes.
Contact: Igor.Berezovsky@uni.no 1 INTRODUCTION Enzymes are involved in all processes in living organisms.
Well before the first protein sequence and structure were determined (Sanger, 1952), the function of enzymes became one of the central questions in biochemical studies.
Despite the wealth of experimental data available nowadays, the functions of the majority of proteins are still uncharacterized (Levitt, 2009).
Since the presence of certain biochemical activities is typically sought for, while all other possible activities (e.g.promiscuous functions) are ignored (Furnham et al., 2009), experimental determination of enzymatic function is in most cases confirmative.
Besides, biochemical assays are expensive, are subject to in vitro experimental conditions, and they can not be run on a genomic scale.
All the above makes prediction of enzymatic function with computational methods an important alternative approach.
There are several general assumptions on which such methods are based: (i) homologous proteins have similar functions; (ii) most of the functional variants emerged as a result of divergence from a common ancestor; (iii) structural homologs, so-called fold superfamilies, persist down to 25% of sequence identity; (iv) divergence below 25% of sequence identity leads to the emergence of families with different organism, substrate, and/or tissue specificities (Lo Conte et al., 2000).
Though enzymatic To whom correspondence should be addressed.
function can be inferred by sequence and structure similarity, the relations between sequence, structure and function are far from being completely understood.
Many folds display vast functional diversity.
For example, structurally similar/ barrels provide scaffolds to a number of biochemical functions (Nagano et al., 2002), while particular biochemical functions can be performed by different protein folds, e.g.hydrolase (Lo Conte et al., 2000).
The contemporary evolution of protein structure and function takes place through mutations (Aharoni et al., 2005), recombination and domain swapping and/or interactions (Chothia et al., 2003).
However, it is rather obvious that this process was preceded by the emergence of a first set of protein domain structures/folds with a limited repertoire of biochemical functions.
These structures emerged from peptides with rudimentary non-specific catalytic activities in the pre-domain stage of evolution (Lupas et al., 2001).
Understanding of this process is important for characterizing the most ancient functions and their connections to the modern proteins.
The difficulties in understanding and, more importantly, in predicting protein function, are well reflected in the diversity of their descriptions.
Enzymatic reactions are classified in enzyme nomenclature (EC) by the biochemical transformation and the substrate (Bairoch, 2000).
According to MACiE database, there could be different mechanisms employed for the same transformation (Holliday et al., 2007, 2009).
Different biochemical reactions can have the same core mechanism, as it is exemplified by mechanistically diverse superfamilies (Glasner et al., 2006).
In order to reconcile different approaches and to develop a generic description of enzymatic functions, one has to start from considering their elementary units which provide binding/activation and principal chemical transformation steps of the whole reaction.
Then it should be found out how combinations of these units result in a variety of enzymatic reactions, and how protein folds restrict the possibility of performing a particular biochemical transformation or binding a certain substrate.
The first question that arises in this context is what elements of protein folds serve as elementary units of function.
What were the structures of these units in pre-domain evolution, and how did they affect the structures of modern proteins?
Earlier studies have shown that soluble proteins contain a basic universal element, stemming from the polymer nature of polypeptide chains, namely closed loops or returns of the polypeptide chain backbone with a typical size of 2530 amino acid residues (Berezovsky and Trifonov, 2001; Berezovsky et al., 2000).
Any protein fold can be decomposed into sets of consecutively connected closed loops (Berezovsky, 2003), indicating their independence in the evolutionary past (Trifonov and Berezovsky, 2003).
Can we reconstruct the pre-biotic peptides that gave rise to the elementary functional units of modern proteins?
Our hypothesis is that a functional signature revealing the type of The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i498 i497i503 A.Goncearenco and I.N.Berezovsky binding/activation or principal chemical transformation of the loop can be obtained from the contemporary proteins.
This signature complements the description of the closed loop, hence it is an elementary functional loop (EFL).
Therefore, the first goal of this study is to investigate how these functional loops evolved from pre-domain peptides, and to find a set of prototypes from which the EFLs of contemporary proteins originate.
In order to draw a picture of folds and functions emerging as combinations of EFLs, prototypes of the EFLs will be derived and the corresponding EFLs will be detected in proteins with known biochemical functions.
The presence of EFLs in distinct folds and functions unravel evolutionary relations between them and can hint on recipes for protein function (re)design.
The specific nature of prototypes calls for developing a new computational procedure for their derivation and characterization.
Indeed, we seek for entities which do not exist in modern proteins, but are represented by their descendants, EFLs.
The EFLs themselves presumably have low sequence identity to each other, and, therefore, evolutionary connections between them are not obvious.
In this work, we propose a computational procedure to derive prototypes of EFLs from the sequences of complete proteomes.
We expect these prototypes to be of closedloop size (2530 residues), ring-like shape and to have distinct functional signatures, where several conserved positions in the profile describe chemically active amino acids which are involved in binding/activation steps and/or take part in principal chemical transformations of the substrate.
We illustrate our approach by reconstructing prototypes from complete archaeal proteomes and analyzing connections between functions and folds found by the reconstructed prototypes.
In particular, we show examples for three characteristic cases: (i) nucleotidetriphosphate binding and hydrolyzing loop, called p-loop (Rossmann et al., 1974); (ii) a loop found in functionally diverse proteins having/ barrel fold; (iii) prototypes of two EFLs involved into binding of ADP and glucose which form an enzymatic domain in glycosyltransferases (glycogen synthase).
2 MATERIALS AND METHODS We describe a computational method for deriving prototypes of EFLs based on the sequences of complete genomes.
The procedure comprises the iterative derivation of sequence profiles followed by the hierarchical clustering of profiles.
We propose a scoring function that weights profile positions proportional to the information content on position, allowing to discrimine between matches that carry a specific signature from nonspecific ones.
The statistical significance of the scores is calculated from the empirical distribution of the reshuffled profile scores used as a control.
We generalize the profiles and remove the remaining redundancies by clustering the profiles.
The distance measure used in clustering also takes into account the information content on profile positions.
2.1 Derivation of prototypes from complete proteomes Complete sets of protein coding sequences of 68 archaeal organisms (listed in Supplementary Material) are obtained from Genbank (Benson et al., 2009).
We use one proteome representing each phylum of the archaeal superkingdom to produce a set of origins for the prototype derivation procedure.
Proteomic sequences contain many sources of biases and redundancies: (i) homologous proteins; (ii) domain swapping, recombination and multiplication.
These redundancies have to be removed, as they reflect recent events in the evolution of proteins (Chothia et al., 2003).
The average domain size is 80150 residues (Gerstein, 1998; Jones et al., 1998; Svedberg 1929; Wheelan et al., 2000); therefore, in order to remove redundancy originated from domain swapping, we compare 80-residue long sequence segments for identity.
Sequences are clustered with CD-HIT (Li and Godzik, 2006) several times to gradually remove redundancy between domains down to 40% identity.
Low-complexity regions in sequences which contain repeats or have highly biased amino acid composition are masked with SEG (Wootton and Federhen, 1996).
Non-redundant domains are cut with 10residue steps into overlapping 50-residue segments.
These segments contain two 10-residue flanks, which can be adjusted in order to obtain a final 30residue prototype.
Based on the observation that gaps are not distributed uniformly, and multiple sequence alignments of remote homologs (below 25% sequence identity) contain well-aligned blocks without gaps (Kann et al., 2007) we consider that the cost of insertion or deletion in a functional signature is higher compared to at an arbitrary position in the whole protein sequence, therefore we do not allow gaps in profiles of EFLs.
The procedure starts from the search for sequences in the complete archaeal proteomes that are most closely related to the initial sequence segments (origins) in order to construct the seed alignment with a frequency matrix constituting a profile.
The obtained profiles are then matched to the complete proteomes again, in order to find additional sequence matches and to update the profile.
This profile-sequence search is repeated until the profile no longer changes, and, therefore, considered converged (Supplementary Figure S1).
The profiles represent families of EFLs with specific signatures.
The iterative procedure allows a profile to gradually expand to more distantly related, but statistically significant matches.
Although the procedure resembles PSI-BLAST (Altschul et al., 1997), it has some notable differences originating from the specific requirements of the prototype derivation task.
These differences are discussed in more detail in Supplementary Material.
2.2 Weighting of profile positions by information We calculate position specific scoring matrices (PSSM) to score the profiles (for details of PSSM calculation see Supplementary Material).
The profilescoring function has to rank the matches according to the similarity of the sequence segment to the signature of the profile.
An uneven contribution of positions in the profile should be taken into account.
Degeneration of the profile towards the random compositional background or rare amino acids because of overestimation of pseudocounts (Altschul et al., 2009) should be prevented.
Decrease of the profile sensitivity because of underestimation of pseudocounts should also be avoided.
Therefore, in order to discriminate between matches that carry a specific signature from non-specific ones, we weight positions proportional to KullbackLeibler divergence (DKL) (Kullback and Leibler, 1951), which reflects the information content on position i relative to the random background: DiKL= 20 j=1 [ fi,j log2 ( fi,j cj )] , where f is observed amino acid frequencies on position i and c is proteomic amino acid composition.
The score of a sequence segment q to profile P(n) will become: Score ( q,P (n ) ) = 1 n n j=1 DiKLmi,qi , where [ mi,j ] i=1,...,n;j=1,...,20 is the corresponding PSSM with n positions.
Profile positions with low-information content (DKL <1 bit) are not contributing to the overall score and are omitted.
2.3 Empirical calculation of the background The significance of a score is characterized by an E-value, which is the number of false positive or unrelated matches above this particular score.
The E-value is evaluated by comparing distributions of scores of the profile (s) with scores of the reshuffled profile (sR) :E(s)=Np =N(1ecdf(sR)), where p is the P-value, N is the size of the combined proteome and ecdf(sR) i498 [10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i499 i497i503 Prototypes of elementary functional loops Fig.1.
Scoring and comparison of sequence-profile matches.
(A) Histograms show tails of distributions of proteomic scores for the p-loop prototype (red) and for the same prototype, but with the reshuffled positions (blue) where the functional signature is destroyed, while the amino acid composition of the profile is preserved.
Inset shows quantilequantile plot of complete distributions.
(B) Selected matches (28 of 689) of the p-loop prototype (shown as logo).
Numbers indicate identity to the consensus sequence (shown below).
Sequence label is Genbank description of the protein from which these matches were taken.
(C) Histogram shows pair-wise sequence identity of all the significant matches of the p-loop prototype.
is an empirical cumulative distribution function of the reshuffled profile scores.
If, instead of the reshuffled profile, a randomized proteome is used as control, then all the biases except amino acid composition are lost, resulting in overestimation of significance of sequence matches containing non-specific signals.
Complete positional permutations destroying relative distances between profile positions, similar to the combinatorial problem of pattern-avoiding permutations (Atkinson, 1999), give a robust estimation of the E-values (data not shown).
Figure 1A shows an example of the score distributions for a derived profile and the reshuffled one.
The difference exists only in the right-most tail of the distributions, showing that there are specific signatures in the profile, and that they are destroyed by reshuffling.
All collected profile matches with the E-value below a certain significance threshold (e.g.E 1) are used to construct the updated profile.
A sample of significant matches of the p-loop prototype and the corresponding sequence logo are shown in Figure 1B.
Although the matches are found in proteins with different biochemical function, they all possess nucleotide-binding activity, which is presumably an elementary function of this prototype.
It is important to note that some of the profile positions have much higher information content than others, and these positions constitute the signature.
Comparison of all significant sequence matches to the consensus shows that the sequence identity is low on average and has a large variance (Fig.1B and C).
Therefore, for proper E-value estimation, i.e.for proper separation of related matches from the unrelated ones, discrimination between informative and non-informative positions is necessary.
2.4 Hierarchical clustering of converged profiles The iterative procedure described above results in a set of converged profiles that should be analyzed further.
First, there is a redundancy between these profiles, caused by the way the origins are obtained: they overlap with a step of 10 residues.
Redundancy also stems from the fact that different origins can actually converge to the same or very similar profiles.
It means that these origins correspond to evolutionary connected EFLs, but their similarity can only be detected with the help of the profile.
We introduce a distance measure that takes into account all possible profileprofile alignments in order to hierarchically cluster the profiles.
Profileprofile comparison is more sensitive (Panchenko, 2003) than profile-sequence comparison, thus more distant relations could be detected during profile clustering.
This procedure results in the removal of redundancy and further generalization of the profiles.
The profiles with the most generic signatures represent the functional characteristics of presumably original prototypes.All possible profileprofile alignments without gaps are performed by sliding one 50-residue-long profile [A]50 against the other profile [A]50 and calculating pair-wise positional distances between all possible 30-residue windows [a]30 and [b]30, respectively.
Distances between the pairs of corresponding positions are weighted proportionally to the information at each position (DKL): d ( [a]30 ,[b]30 )= 30 i=1 (DaiKL+DbiKL ) 20 j ( ai,j bi,j )2.
The distance between two 50-residue profiles [A]50 and [B]50 is equal to the minimal distance between all possible sliding windows of size 30: D ( [A]50 ,[B]50 )=argmin[d([a]30 ,[b]30)]aA;bB.
Hierarchical clustering of profiles is an iterative procedure where the most similar profiles (min[D(A,B)]) are consecutively merged together, resulting in a new, more generic profile (Supplementary Figure S3).
2.5 Characterization of prototypes We characterize prototypes by looking for sequence matches in crystallized enzymes from ASTRAL/SCOP database (Brenner et al., 2000; Lo Conte et al., 2000).
These matches describe descendant EFLs that diverged from the prototype.
We assign elementary functions for derived prototypes and determine characteristic positions in their signatures based on the known enzymatic mechanisms in crystallized proteins.
Protein function is typically annotated by homology, although neither high-sequence identity (<50%), nor low BLAST E-values (below 1050) guarantee the conservation of biochemical function (Rost, 1999, 2002).
Here we annotate functional units of sub-domain size.
Conventional homology detection methods, which operate on the level of whole proteins or domains, consider connections between SCOP superfamilies as false positives (Gough et al., 2001).
It becomes obvious, that analysis of evolutionary relationships on the level of functional closed loops requires a special approach (Andreeva et al., 2007; Fong and Marchler-Bauer, 2009; Xie and Bourne, 2008).
Although most of the derived prototypes (>70%, data not shown) have matches in Pfam, Prosite and CDD, functional annotation can not be directly transferred from the databases defining the function on whole-protein or domain level (Bateman et al., 2004; Lo Conte et al., 2000; MarchlerBauer, et al., 2009; Sigrist et al., 2010), resulting in ambiguous annotations, and requiring additional manual curation.
SCOP superfamilies can be used i499 [10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i500 i497i503 A.Goncearenco and I.N.Berezovsky Fig.2.
Matches of the nucleotidetriphosphate-binding (p-loop) prototype in crystal structures.
Four matches of nucleotidetriphosphate-binding prototype are shown.
The fold is displayed in cartoon and the structural loop corresponding to p-loop prototype is highlighted in green.
The structures of the EFLs are also displayed.
The logo of the prototype and the alignment of sequences of the corresponding EFLs highlight the functionally important residues involved in nucleotidetriphosphate binding and hydrolysis.
PDB ID, SCOP ID and the coordinates of the sequence segments corresponding to the domains which contain the EFLs on display, are shown in the bottom.
as a reference of the function for crystallized protein domains.
CDD and Swissprot features can be used as more precise indicators of the elementary function of EFLs.
Other databases describe enzymatic function on the residue level.
For example, CSA (Gutteridge and Thornton, 2005), and MACiE (Holliday et al., 2007) databases describe experimentally determined roles of functional residues in the biochemical reactions and its mechanisms.
Thus, via sequences of the crystallized structures this annotation can be transferred to the prototypes signature.
2.6 Statistics Non-redundant archaeal proteome has 20106 sequence segments of length 50.
Starting from 175 458 origins extracted from four archaeal organisms, we end up with 8327 converged profiles having >100 matches in the archaeal proteome and containing at least one position with four bits of information in their signature.
These profiles were clustered for 120 iterations, which resulted in 138 profiles, from which the strongest 43 were selected for further consideration.
The resulting 43 profiles are considered to be the most abundant ones and were used in the analysis.
The ASTRAL sequence database based on SCOP release 1.75 contains 16 712 non-redundant domains at 95% sequence identity.
3 RESULTS AND DISCUSSION We developed a computational procedure for deriving prototypes of EFLs, obtained prototypes from the set of archaeal proteomes, considered several prototypes in detail, delineated connections between domains superfamilies using the most abundant prototypes, and exemplified how combinations of EFLs result in specific enzymatic function.
Figure 2 shows several representatives of the p-loop prototype and exemplifies detection of EFLs in different folds and biochemical functions.
The signature of the prototype reads G-X-X-G-X-G-K[TS] and is known to be the signature of nucleotidetriphosphate binding (Rossmann et al., 1974).
We show EFLs corresponding to Fig.3.
Matches of the Glycine-rich prototype in crystal structures of/-barrels.
Four matches of the prototype involved presumably in redox reaction are shown.
All structures have/-barrel fold, but perform different biochemical functions and belong, therefore, to different SCOP superfamilies.
the prototype in four different proteins representing three different folds (p-loop containing nucleoside hydrolase, PEP carboxykinaselike fold, OsmC-like fold).
Though sequence alignment of EFLs reveals high conservation in key sequence positions, the rest of the loop can diverge significantly.
The degree of divergence of EFLs from the prototype is also indicated in the difference between corresponding structural segments: in the hydrolase and PEPcarboxykinase-like folds the structure resembles-turn-, while in OsmC-like fold it resembles-hairpin.
It is important to note, however, that despite the different structures of the EFL, naturally affected by the rest of the fold (Minor and Kim, 1996), the functional signature is always located in the elbow between the two elements of secondary structure and is highly conserved.
EFLs representing this prototype universally provide the elementary function of nucleotide triphosphate binding via interaction with the phosphate groups and with a Mg2+ ion, and also take part in phosphate hydrolysis.
The combination of a specific sequence signature with its structural location emphasizes the conservation of the closed-loop structure, regardless of the exact secondary structural content of the loop and interaction of this loop with its structural environment.
The diversity of folds containing this loop suggests that in the predomain stage of protein evolution the prototype of the p-loop was included into structurally and biochemically different folds, acquiring different elements of secondary structure and mutations in sequences, but preserving the active residues and their relative locations in sequence and space.
The fourth structure in Figure 2 is a protein from V. cholerae with unknown function.
With the help of the prototype it is now possible to hypothesize the function of this protein to a certain extent.
It could be predicted, for example, that this V. cholerae protein has a nucleotidetriphosphate binding, and, perhaps, hydrolyzing activity.
The combination with other EFLs detected in this protein can complete description of its possible biochemical function.
Figure 3 shows proteins that share the same/-barrel fold, which, in turn, has >30 superfamilies in SCOP.
This fold also serves as a scaffold for a variety of functions (Nagano et al., 2002), therefore i500 [10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i501 i497i503 Prototypes of elementary functional loops Fig.4.
Protein functions connected by prototypes of EFLs.
Red diamonds represent prototypes and blue ovals represent protein domain superfamilies with the corresponding SCOP superfamily names.
Edges are matches detected in the non-redundant sequences of protein domains.
Thickness of edges characterizes the number of matches found.
the fold is an important target in protein (re)design experiments (Bershtein and Tawfik, 2008; Tokuriki and Tawfik, 2009).
Based on the derived prototypes,/-barrels can be decomposed into a set of-turn-subunits, some of these subunits are directly involved in catalysis and, therefore, carry the functional signatures.
The Glycine-rich prototype (Fig.3) is an illustration of functional connections in an abundant/-barrel fold.
The structure of the loops is-turn-, and the functionally important residues are located in the turn.
The elementary function of the Glycine-rich predomain prototype is related to redox reactions revealing evolutionary connection between/ barrels with different enzymatic functions.
The biochemical functions of the enzymes where the loop is found are typically various oxidoreductases, dehydrogenases and synthases.
A set of abundant prototypes derived on the archaeal proteomes is used to delineate evolutionary connections between functional superfamilies of structural domains.
Figure 4 illustrates connections revealed by a subset of the strongest prototypes (43 prototypes) in form of a graph.
Nodes are prototypes (red diamonds) and protein domain superfamilies (blue ovals) according to SCOP classification.
The edges represent matches between the superfamilies and the prototypes, where thickness of an edge is proportional to the logarithm of the number of matches found in the non-redundant set of protein domains derived from ASTRAL/SCOP database (Brenner et al., 2000; Lo Conte et al., 2000).
Since a non-redundant set of protein domains was used in analysis, thickness is a rough indicator of the diversity of proteins in the superfamily.
Each prototype is referred to by its number (numbers in red diamonds) and has a functional signature represented in form of a sequence profile.
The logos of the corresponding profiles are listed in Supplementary Material.
The functional connections exemplified by the p-loop prototype and Glycine-rich prototype can be seen here in a larger context of archaeal (and homologous to archaeal) domains.
The Glycinerich prototype (Fig.3) with the number 8 in the graph has five connections to folds other than/-barrel fold: NAD(P)-binding Rossmann fold, Activating enzymes of the Ubiquitin-like proteins, PreATP-grasp domain, Nucleotide-binding domain, FAD/NAD(P)binding domain.
Since all these folds have nucleotidephosphate binding in common, these connections suggest that the elementary function of prototype 8 is related to nucleotidephosphate binding.
As a result, functional connections inside the/-barrel fold as well as connections between the/-barrel and other folds are found, unraveling nucleotidephosphate binding as one of basic elementary functions crucial in the emergence of folds.
Another interesting case is a p-loop containing nucleoside triphosphate hydrolase considered earlier (Fig.2) which is an example of a fold with different biochemical functions.
The particular biochemical function, in turn, is determined by the unique combination of EFLs, which is reflected as a group of prototypes gathered around the superfamily and connected by thick edges.
One of the prototypes around the superfamily is prototype 1603, considered earlier (Fig.2).
The connection between p-loop containing nucleoside triphosphate hydrolase and PEP carboxykinase-like folds via p-loop prototype (Fig.2) indicates that p-loop as EFL is an essential functional element of enzymes belonging to different superfamilies with different folds.
It also suggests an important role of prototype 1603 in pre-domain evolution of folds and superfamilies.
Some prototypes are present in a variety of superfamilies.
For example, the Cysteinerich metal binding loop (number 1845) which corresponds to EFLs forming a nest with cysteines co-ordinating a metal ion (typically Zn2+) facilitating nucleic acid binding.
These EFLs are present in various superfamilies mainly related to nucleic acid binding, which i501 [10:47 28/8/2010 Bioinformatics-btq374.tex] Page: i502 i497i503 A.Goncearenco and I.N.Berezovsky Fig.5.
Two EFLs combine to form the active site in glycosyltransferase.
The structure of glycogen synthase (PDB 1rzu) is shown in cartoon representation.
The prototypes are shown as sequence logos, and the corresponding EFLs are highlighted in the structure.
is reflected in the graph by edges connecting prototype 1845 to thirteen superfamilies.
By representing proteins folds and functions in form of a graph connected by the prototypes one can proceed to explore the emergence of protein functions as combinations of prototypes.
We illustrate how a functional domain emerges as combination of prototypes by the example of Glycosyltransferase superfamily (orange oval in Fig.4).
Figure 5 shows glycogen synthase (PDB 1rzu), which catalyzes the elongation of alpha-1,4-glucose backbone.
The enzyme binds ADP-glucose and [Glucose]n1 as substrates and transfers glycosyl group to form [Glucose]n as product and ADP.
One of the enzymes domains contains the EFLs corresponding to sequence prototypes 3054 (cyan) and 7009 (red).
The elementary chemical functions of the prototypes are assigned according to the description of interactions with co-crystallized ligands analogous to the products and the substrates of the enzyme (Buschiazzo et al., 2004; Sheng et al., 2009).
The elementary function of prototype 3054 is ADP binding: Arg-299 and Lys-304 interact with the phosphate, Ile-297 (second in position of 3054s PSSM) with the base and Ser-298 (third in 3054s PSSM) with the sugar in ADP.
Prototype 7009 is also involved in ADP binding, its characteristic elementary function is glucose binding: Glu-376 interacts with phosphate and Thr-381 (third in 7009s PSSM) with the base of ADP.
Besides, residue Glu-376 also plays an important catalytic role in glycosyltransferase activity.
Finally, these two prototypes also interact with each other, forming a stabilizing salt bridge between Lys-304 and Glu-376.
This example shows how the emergence of enzymatic function can be explored based on signatures of the prototypes and their elementary chemical functions.
The two-domain nature of glycosyltransferase also points out that analysis of individual folds and their enzymatic functions should be followed by the exploration of recombination events in case of multi-domain proteins.
4 CONCLUSIONS The existence of EFLs in different folds and functions makes it possible to survey subtle evolutionary relations, originating from the pre-domain evolution of protein structure.
It suggests that contemporary enzymatic functions are constructs of different sets and combinations of elementary chemical functions.
It also shows that most of the enzymatic functions are performed by abundant prototypes (e.g.p-loop and Cysteine-containing prototype), which provide common reaction steps of different functions existing in different folds.
An exhaustive description of a protein fold and its enzymatic function as a combination of EFLs illuminates how this fold emerged in pre-domain evolution by fusion of prototype genes.
Therefore, obtaining the full collection of prototypes with elementary functions will make it possible to (i) predict enzymatic functions based on the sequences via determining EFLs corresponding to prototypes and their relative positions revealing structure of the fold; (ii) (re)design folds with desired functions by building constructs from necessary EFLs.
ACKNOWLEDGEMENTS We thank Simon Mitternacht for helpful comments and suggestions.
Funding: FUGE-II Norwegian functional genomics platform.
Conflict of Interest: none declared.
ABSTRACT Motivation: In eukaryotic cells, alternative splicing expands the diversity of RNA transcripts and plays an important role in tissuespecific differentiation, and can be misregulated in disease.
To understand these processes, there is a great need for methods to detect differential transcription between samples.
Our focus is on samples observed using short-read RNA sequencing (RNA-seq).
Methods: We characterize differential transcription between two samples as the difference in the relative abundance of the transcript isoforms present in the samples.
The magnitude of differential transcription of a gene between two samples can be measured by the square root of the Jensen Shannon Divergence (JSD*) between the genes transcript abundance vectors in each sample.
We define a weighted splice-graph representation of RNA-seq data, summarizing in compact form the alignment of RNA-seq reads to a reference genome.
The flow difference metric (FDM) identifies regions of differential RNA transcript expression between pairs of splice graphs, without need for an underlying gene model or catalog of transcripts.
We present a novel non-parametric statistical test between splice graphs to assess the significance of differential transcription, and extend it to group-wise comparison incorporating sample replicates.
Results: Using simulated RNA-seq data consisting of four technical replicates of two samples with varying transcription between genes, we show that (i) the FDM is highly correlated with JSD* (r =0.82) when average RNA-seq coverage of the transcripts is sufficiently deep; and (ii) the FDM is able to identify 90% of genes with differential transcription when JSD* >0.28 and coverage >7.
This represents higher sensitivity than Cufflinks (without annotations) and rDiff (MMD), which respectively identified 69 and 49% of the genes in this region as differential transcribed.
Using annotations identifying the transcripts, Cufflinks was able to identify 86% of the genes in this region as differentially transcribed.
Using experimental data consisting of four replicates each for two cancer cell lines (MCF7 and SUM102), FDM identified 1425 genes as significantly different in transcription.
Subsequent study of the samples using quantitative real time polymerase chain reaction (qRT-PCR) of several differential transcription sites identified by FDM, confirmed significant differences at these sites.
Availability: http://csbio-linux001.cs.unc.edu/nextgen/software/FDM Contact: darshan@email.unc.edu To whom correspondence should be addressed.
Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 6, 2011; revised on July 12, 2011; accepted on August 1, 2011 1 INTRODUCTION The transcriptome is a key vantage point for a molecular biologists study of phenotypic differences between cells that result from environmental factors, cell specialization or disease.
Classically, this study has been conducted largely by observing differential gene expression levels using microarrays or high-throughput RNA sequencing technologies.
However, detailed analysis of the transcriptome has shown that significant variation is also encoded in the diversity and relative abundance of a genes constituent transcripts (Kwan et al., 2008; Sultan et al., 2008; Wang et al., 2008).
Consequently, beyond measuring differences in overall expression of genes between samples, there is a need to measure differences in expression at the transcript level.
We define differential transcription of a gene between samples as a difference in the relative abundance of the genes transcript isoforms in the samples.
In this manner, differential transcription is independent of the overall gene expression in the samples.
Short-read RNA sequencing technologies (RNA-seq) have evolved rapidly to sample the transcriptome at increasing depth and accuracy (Wang et al., 2009).
Using RNA-seq datasets obtained from samples, the locus and depth of coverage by reads aligned to a reference genome provide the starting point for the detection of differential transcription (Pan et al., 2008).
Recently, two approaches have emerged to detect differential transcription between samples.
The first approach is based on transcript inference and abundance estimation of the transcripts, as performed by tools like Cufflinks (Trapnell et al., 2010), rQuant (Bohnert and Rtsch, 2010), Trans-Abyss (Robertson et al., 2010) and Scripture (Guttman et al., 2010).
Applying these methods to each of two samples, differential transcription can be determined directly for each gene using the estimated relative abundances of the genes transcripts in the two samples.
However, transcript inference algorithms rely on heuristics to resolve the transcript structure, because the inference problem is, in general, underdetermined.
As a result, some transcripts may be missed or inferred incorrectly.
Abundance estimation, in turn, is not able to correctly explicate The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:24 8/9/2011 Bioinformatics-btr458.tex] Page: 2634 26332640 D.Singh et al.the observed distribution of read alignments when starting from an incomplete or incorrect transcript model.
Thus, differential transcription measured in this fashion may be inaccurate.
The second approach to detect differential transcription is based on observing loci in the reference genome at which reads from the two datasets align with different depth of coverage (after appropriate normalization for differing gene expression).
The idea is that differential transcription should be revealed by different utilization of some exons.
Stegle et al.(2010) describe two methods along these lines.
The first is based on a priori analysis of annotated transcripts to identify regions that could reveal differential transcription.
In each region, a Poisson statistical test is applied.
The second method is without dependence on known transcript structure, and uses a non-parametric kernel-based statistical test called maximum mean discrepancy.
Using synthetic data, both methods are shown by Stegle et al.(2010) to give accurate detection of differential transcription.
In this article, we introduce an approach that does not depend on annotations and instead leverages the splicing structure of a gene uncovered by spliced read alignments using tools like TopHat (Trapnell et al., 2009), MapSplice (Wang et al., 2010) or PALMapper (Jean et al., 2010).
Using the read alignments from these tools, a splice graph is constructed with edges corresponding to transcribed intervals or splices, weighted by read coverage.
We introduce the flow difference metric (FDM) to measure the difference between two graphs in the relative utilization of edges at splicing points.
Using synthetic samples, for which we know the transcripts and their relative abundances, we show the FDM between two samples is highly correlated with the JSD*, provided coverage of the edges is sufficient.
Hence, the FDM can serve as a metric of differential transcription, without need to infer the underlying transcripts or need for any annotation.
To interpret the significance of the FDM, we define a permutation test that can be efficiently implemented on the splice graph representation of the RNA-seq data.
Since pairwise comparison of two samples is often insufficient to draw robust conclusions about differential transcription between two biological conditions, we extend the statistical test to incorporate replicates in each condition, when they are available.
The test identifies differential transcription that is significant between conditions more often than it is significant within replicates.
2 METHODS 2.1 JensenShannon divergence as a measure of differential transcription Let G be a gene with n different transcripts.
In a given sample, the transcript abundance vector for G gives the relative abundance of each transcript isoform, i.e.the fraction of each isoform among all isoforms of G. One measure of differential transcription between two samples A and B, with transcript abundance vectors VA and VB, is the JensenShannon Divergence JSD(VA,VB)=H ( VA +VB 2 ) H(VA)+H(VB) 2 where H(V ) is the Shannon entropy.
The JSD itself is not a metric, but JSD* =JSD does satisfy the properties of a metric.
We adopt JSD* to measure differential transcription in this article, because it defines an objective measure of difference in transcript populations that is independent of the computational methods we examine.
It has also been used to report differential transcription in other methods, e.g.CuffDiff (Trapnell et al., 2010).
2.2 Aligned cumulative transcript graph The alignment of RNA-seq reads to a reference genome provides (i) the genomic coordinates of transcribed bases and (ii) the start and end coordinates of splices.
As a consequence of alternative splicing, transcribed bases and splices may be part of multiple RNA transcripts and hence their coverage by aligned reads reflects their total utilization by all transcripts.
In the literature, transcripts have been mostly represented as paths in an acyclic directed graph with exons as nodes and splices as edges, e.g.Heber et al.(2002) and Sammeths Flux Capacitor (http://flux.sammeth.net/capacitor.html).
Analyzing the read coverage information with this data structure has limitations.
First, this representation can only be used if all exons are known beforehand, which is usually not the case.
Second, if two or more exons overlap in a region (e.g.in the case of alternative 5 donor sites or 3 acceptor sites), the read coverage needs to be determined separately for each of those exons.
Our graph representation addresses these limitations.
The Aligned Cumulative Transcript Graph (ACT-Graph) is a weighted directed acyclic multigraph in which nodes correspond to genomic coordinates of splice start or end sites or to transcription start or end sites.
Edges correspond to transcribed intervals (exonic edge) or to spliced-out intervals (splice edge).
The weight of an exonic edge is its average coverage over the genomic interval it spans, and the weight of a splice edge is the number of reads that include the splice.
The direction of the edges is the direction of transcription.
Each exonic edge is transcribed as whole, i.e.it is included in its entirety in a transcript or not at all.
In principle, an ACT-Graph is the sum of weighted paths (flows), each of which is a transcript with some specific abundance.
Therefore, we named the graph the ACT-Graph.
Figure 1 shows an example of ACT-Graph.
In practice, since reads are sampled non-uniformly from transcripts due to various biases, we use average coverage as an approximation of the total abundance.
2.2.1 ACT-Graph construction The following describes the step-by-step construction of an ACT-Graph from RNA-seq data: (1) Spliced Alignment: RNA-seq reads are aligned to the reference genome using a gapped aligner such as MapSplice (Wang et al., 2010).
(2) ACT-Graph nodes: the ACT-Graph nodes are created using one of the following(i) Splices: genomic coordinates of splice start and end locations are obtained from spliced alignments; (ii) interpreting start and end sites of transcripts: we can use inference or annotations to identify these sites.
We can infer the start of a transcript based on the observation that the first (1) bases following the start coordinate, where is the RNA-seq read length, show a characteristic ramp of increasing coverage as there are increasingly many ways for a read to sample bases further away from the start of a transcript.Atranscription end site is inferred similarly.
Alternatively transcript start and end coordinates can be taken from gene annotations, if available.
Nodes introduced in this fashion are not harmful if the transcripts happen not to be expressed.
(3) ACT-Graph edges and weights: (i) a splice edge is inferred from a spliced alignment.
The weight of the splice edge is the number of reads that support the splice.
The direction of the edge is inferred from the orientation of the flanking bases in the intron for canonical splices or it can be inferred from the direction of other splices in the gene.
(ii) An exonic edge connects two adjacent nodes (from the sorted list of nodes) if the genomic interval is fully covered or nearly fully covered and has an average coverage above threshold.
We use a threshold of 1.
The weight of an exonic edge is the average coverage of that genomic region.
Averaging over the genomic region gives a better estimate of the number of transcripts that use that genomic region.
(4) ACT-Graph genes and transcribed regions: a transcribed region is a connected component in the ACT-Graph when edges are considered as undirected, and typically would correspond to genes.
If gene 2634 [17:24 8/9/2011 Bioinformatics-btr458.tex] Page: 2635 26332640 FDM: differential transcription analysis Fig.1.
ACT-Graph: the nodes are genome coordinates.
A solid (blue) edge represents an exon or part of an exon labeled with the average depth of read coverage along the interval.
A dashed (green) edge is a splice edge and is labeled by the number of reads that include the splice.
Alternative splicing features such as mutually exclusive exons, a retained intron and a skipped exon are illustrated.
Nodes drawn as boxes, circles and hexagons, respectively, represent annotated-only positions, novel-only splice positions and both annotated and novel positions.
annotations are available, the regions can be restricted to known genes.
The coverage of a gene is defined as average base coverage over all the bases of the exonic regions in the gene.
2.2.2 ACT-Graph compressed representation The ACT-Graph is stored in the standard GFF format.
The field TYPE tells if the line describes a node, a splice edge or an exonic edge.
The field SCORE is used for weight of the edges.
The ACT-Graph format is a concise summary of alignments, and is powerful representation for quantitative analysis of alternative splicing.
Figure 2 shows the compression achieved by the ACT-Graph representation as a function of the number of reads.
The ACT-Graph is typically two to three orders of magnitude smaller than the SAM file or the raw reads, depending on the number of reads in the dataset and can be used for a number of downstream analyses, such as differential transcription.
Fig.2.
ACT-Graph Compression (Section 2.2.2): plot of file sizes of ACTGraph (ACTG), FastQ file (FASTQ) and the alignment file (SAM).
As the number of reads increases, the storage used by ACT-Graph increases orders of magnitude more slowly than other representations.
2.2.3 Alternative splicing features in ACT-Graph The ACT-Graph can be used to identify various alternative splicing features in a gene.
Each alternative splicing feature can be represented by a subgraph which can be searched in the ACT-Graph.
Figure 1 shows examples of various such features in a gene.
2.3 FDM In this section, we describe the FDM, which uses the ACT-Graph to find genes with differential transcription.
As stated earlier, the ACT-Graph can be viewed as the sum of weighted paths or flows, each of which corresponds to a transcript with some abundance.
ACT-Graph nodes that have m>1 incoming or outgoing edges indicate that at least m transcripts use that node.
These nodes are called divergence nodes.
Divergence nodes imply alternative splicing.
The m incoming/outgoing edges are called the divergence edges.
The weights of divergence edges signify the relative abundances of alternative transcripts passing through the divergence node.
The normalized weights of all the divergence edges of a node are grouped together in a vector called the flow vector for the node.
The difference between flow vectors in ACT-Graphs constructed from different samples indicates the magnitude of differential transcription between the two samples.
We measure the difference in flow vectors using a metric called the FDM which is defined as follows.
Assume an ACT-Graph has n divergence nodes.
The flow vector for divergence node i of sample A is defined as VAi =[e(a,i)1,...,e(a,i)m] where m is the number of edges at node i and e(a,i)j is the normalized coverage at edge j, such that m j=1 e(a,i)j =1.
The flow difference between samples A and B at divergence node i is FDi(A,B)= m j=1 e(a,i)j e(b,i)j The FDM is computed as FDM(A,B)= 1 2n n i=1 (FDi(A,B)) as illustrated in Figure 3.
It is important that ACT-Graphs of both samples have identical nodes and edges.
If a node or edge is present in only one ACT-Graph, it is added to the other one with weight zero.
The weights of exonic edges split by added nodes are recomputed using the alignments.
2635 [17:24 8/9/2011 Bioinformatics-btr458.tex] Page: 2636 26332640 D.Singh et al.Fig.3.
FDM and JSD illustration: an example for a gene in two samples A and B is shown.
The gene has two transcripts with expression ratio of 1:4 and 4:1 in the two samples, respectively.
The FDM is computed using the two ACT-Graphs.
The ACT-Graphs have two divergence nodes: node n2 has outdegree 2 and node n5 has indegree 2.
FDM(A,B) = 12n (FDn2(A,B)+FDn5(A,B)) = 122 ((|0.80.2|+|0.20.8|)+(|0.80.2|+|0.20.8|))=0.6.
The JSD is computed using the ground truth knowledge of the transcript abundance vectors.
VA = [0.2,0.8] and VB = [0.8,0.2].
JSD(VA,VB)=H ( VA+VB 2 ) H(VA)+H(VB)2 = 0.28.
Thus, JSD*= 0.53 is the magnitude of differential transcription representing ground truth.
2.3.1 FDM properties Lemma: the FDM is between 0 and 1 Lemma: FDM is a metric (1) FDM(A,B) 0 (2) FDM(A,B) = 0 if and only if A = B (3) FDM(A,B) = FDM(B,A) (4) FDM(A,B) FDM(A,C) + FDM(B,C) The proofs of both lemmas are provided in the Supplementary Materials.
2.3.2 FDM usage FDM may be applied between ACT-Graphs without need for normalization by the number of reads or read length, because the FDM is based on ratios of coverage, and these factors scale coverages linearly.
Using synthetic data, we show that FDM has a high correlation with JSD*.
The details of this are in Section 3.1.
Since we do not know the transcripts or their relative abundance, we use the FDM as a metric for differential transcription.
2.4 Statistical tests for differential transcription 2.4.1 Statistical test to find genes with significant differential transcription.
We use the FDM as a test statistic to find genes with significant differential transcription between two samples.
The ACT-Graph of each gene is different, so the range of FDM values differs from one gene to another.
Thus, the FDM value for a gene is in itself not sufficient to tell if the differential transcription is significant.
Instead, we devised a non-parametric test to determine whether differential transcription is significant.
We create the null distribution of FDM for a gene, and test if the FDM value for the two samples has a significant P-value.
The null hypothesis is that the gene has no differential transcription in two samples.
The process of creating the FDM null distribution is illustrated in Figure 6 in the Supplementary Materials.
Assume that there are N aligned reads in both the sample datasets.
Create ACT-Graphs for the two samples such that nodes and edges are identical.
The reads are partitioned into p equal-sized groups in both samples, and an ACT-Graph is created from the alignments of each group of N/p reads.
Thus, for each sample we have p ACT-Graphs.
The 2p ACT-Graphs are randomly shuffled into two groups of p partitions each and a composite ACT-Graph for each group is created by simply adding the edge weights of the p ACTGraphs in the group.
Now the FDM is computed between ACT-Graphs of these two groups.
This gives a value of the random variable which follows the null FDM distribution.
By shuffling partitions a sufficient number of times, we get a null distribution of the FDM.
In this fashion, the FDM null distribution is created for each gene, and the P-value for the specific partition that corresponds to the reads of the two samples can be computed.
Section 1.5 in the Supplementary Materials provides details on sensitivity to the choice of p and the number of permutations.
2.4.2 Statistical test for multiple replicates A single pairwise comparison is often insufficient to draw robust conclusions about differential transcription.
Due to several uncontrolled factors, RNA-seq replicates may vary considerably more than predicted from sampling error alone.
Thus, pairwise comparison between replicates may yield false positives.
If we have multiple replicates of the two samples, we can apply one more level of permutation test to further filter the false positives.
Let us assume that there are r replicates for each of the two samples.
Replicates from first sample are called Group 1, and replicates from other sample are called Group 2.
The FDM pairwise statistical test can be applied to all (2r 2 ) pairs.
Out of those, r2 pairs are between replicates in different groups, and the rest are between replicates in the same group.
Now, if a gene has significant differential transcription between groups more often than within groups, it is likely to be true positive.
The difference between groups and within groups is used as the test statistic.
By permuting the group label of the replicates, we get the null distribution of the test statistic.
The P-value of the statistic is computed for the original labeling and tested for significance.
3 RESULTS 3.1 Experiments with simulated data In biological data, we typically do not know the exact set of transcripts and their relative abundance in a sample, using which we could calculate the 2636 [17:24 8/9/2011 Bioinformatics-btr458.tex] Page: 2637 26332640 FDM: differential transcription analysis A B Fig.4.
Sensitivity and specificity of the FDM as a function of read coverage (Section 3.1.1 & 3.1.2) : Synthetic data of three sample pairs of 1500 genes each is analyzed.
The first sample pairs have low gene coverage ( coverage = [0,5]), the second sample pairs have medium gene coverage (coverage = [10,15]), and the third sample pairs have high gene coverage (coverage of 20 or higher).
(A) JSD*-FDM Correlation: The points in the scatter plots correspond to (JSD*, FDM) values for a gene, where JSD* is ground truth and FDM is computed from ACT-Graphs.
When the average gene coverage is high, the correlation between JSD* and FDM is high.
For average coverage higher than 20, the correlation is 0.819.
(B) FDM as a classifier for JSD*: a gene is marked positive for differential transcription if JSD* is more than 0.22 and negative otherwise.
FDM is used to classify genes as positive or negative.
Thus for each value of FDM, we get some true positives and some false positives.
By varying FDM, the complete curve is plotted.
The FDM values of (0.01,0.02,0.04,0.08,0.16,0.32.0.64) are marked on the curve.
With coverage of 20 or higher, 90% of true positives can be identified with 10% false positives.
JSD*.
Hence ,we use synthetic data, for which we know the exact transcript expression vectors for each gene, to determine (i) the correlation of the FDM and the JSD* metrics; (ii) the power of the FDM method when used as a classifier for a particular value of JSD* under various levels of read coverage; and (iii) the advantage of the groupwise significance test.
The RNA-seq dataset is simulated as follows.
We use the annotated transcripts for human genome as a reference.
Genes which have at least two transcripts are selected.
Each of the genes is assigned an expression level sampled from an empirical distribution of observed expression levels in human genes.
The individual transcripts of the genes are each assigned a relative abundance so that their sum is 1.
The vector of relative abundances is called the transcript expression vector.
For example, a gene with two transcripts T1 and T2 and a transcript expression vector of [p1, p2] indicates that p1% of transcripts are T1 and p2% are T2.Aread of size from a transcript is a random segment of size taken from the transcript sequence generated using the reference DNA.
The number of reads generated from a transcript is proportional to the product of gene coverage, transcript expression and the length of the transcript.
The alignment for every read is known, and hence the input SAM datasets consist of reads that are uniquely and perfectly aligned.
Additional details on the datasets created can be found in the Supplementary Materials.
3.1.1 FDM correlation with JSD* We create three pairs of simulated RNA-seq datasets each with different gene coverages.
The three pairs of datasets have 1500 genes each.
They are generated by varying gene coverages over three ranges [0,5] , [1015] and 20 or higher.
The JSD* for the genes is varied over the range 0.01.0.
The ACT-Graph is created for all the genes for both the samples in the pair.
The FDM is computed for each gene in the pair.
From the transcript expression vectors of the genes, the JSD*, which represents the ground truth of differential transcription, is computed.
In Figure 4, we see that the correlation of FDM and JSD* increases as read coverage of the gene increases.
This is as expected; when gene coverage is lower, the ACT-Graph edges will have lower weights.
Since ratios are used, a small change in edge weight caused by random effects would affect the FDM considerably.
3.1.2 FDM as a classifier for JSD* We tested if FDM can classify genes as high JSD* genes and low JSD* genes.
We call a gene positive for high JSD* if the JSD* is >0.22 and negative otherwise.
This threshold is arbitrary; we obtained similar results for other values.
For each gene, we create ACT-Graphs for two samples and compute the FDM.
For a constant c, if FDM >c, we classify the gene as positive.
Some of the positives are true positives (using JSD definition) and some false positives.
For each c, we get true positives and false positives.
By varying c from 0.01 to 0.99 over a step of 0.01, we get the complete receiver operating characteristic curve (ROC).
Figure 4 shows that with high coverage, 90% of true positives can be identified with 10% of false positives.
3.1.3 FDM method over synthetic replicates We created two synthetic tissues over 2100 genes with at least two transcripts.
The JSD* between genes in the two tissues varies randomly over the range 0.011.00.
The distribution of JSD* and log(Coverage) are given in Figure 1e and f, respectively, in Supplementary Materials.
Four replicates were created for each of the tissues resulting in eight samples.
FDM method was applied over all the (8 2 ) pairs of which 16 pairs were between group and 12 were within group comparisons.
We used P 0.05 as significant.
For creating FDM null distribution, the number of partitions we used was 30 and the number of permutations was 2637 [17:24 8/9/2011 Bioinformatics-btr458.tex] Page: 2638 26332640 D.Singh et al.(c) (d) (a) (b) Fig.5.
Detection of differential transcription by different methods.
The circles in scatterplots (ad) represent 2100 genes in two samples with varying differential transcription (measured by JSD*) and varying depth of RNA-seq sampling (measured by the average coverage per transcribed nucleotide).
Filled circles correspond to genes with significant differential transcription according to each of the methods.
(a) FDM consistently identifies differential transcription when coverage is high or JSD* is high.
For example, for genes with JSD* >0.28 and log(coverage) >0.85 (coverage >7), FDM was able to identify 90% of the genes as differentially transcribed.
Two other methods not using annotations, (c) Cuffdiff (Trapnell et al.(2010) without annotations) and (d) rDiff (MMD) Stegle et al.(2010), had lower sensitivity, identifying differential transcription in 68 and 49% of the genes in this region, respectively.
(b) For comparison, we also ran Cuffdiff with gene annotations, which identified differential transcription in 86% of the genes in this region.
1000.
Section 1.5 in the Supplementary Materials shows that increasing the number of partitions and permutations has little effect on the results.
The method finds 90% of the genes which have JSD* >0.28 and coverage >7 as significant.
3.1.4 Comparison with other methods In Figure 5, the results of FDM are compared against other methods not using annotations, namely Cuffdiff (without annotations) and rDiff (MMD), using synthetic RNA-seq datasets defined in the previous section.
We ran Cuffdiff as included in release 1.0.3 of the Cufflinks software.
Since the data are synthetic and without sampling bias, we deactivated the bias correction module.
We used the upper quartile normalization option in order to improve the accuracy of the abundance estimation.
All genes with P 0.05 were marked as significant.
We ran rDiff.web as provided in http://galaxy.tuebingen.mpg.de/.
The only option available for the software is which method to use: we used the MMDbased method.
All the genes with P 0.05 were marked as significant.
The scatter plots in 5 show the results.
For genes with JSD* >0.28 and coverage >7, FDM was able to identify 90% of the genes as differentially transcribed.
This represents higher sensitivity than Cuffdiff (without annotation) and rDiff (MMD), which identified differential transcription between 68% and 49% of the genes in this region, respectively.
For comparison, we also ran Cuffdiff with gene annotations, which identified differential transcription in 86% of the genes in this region.
3.2 Experiments with biological data We used RNA-seq data for four replicates each of the cancer cell lines MCF7 and SUM102.
Each dataset has 80 million single-ended reads of length 100 nt.
We used the FDM method to find genes with differential transcription between SUM102 and MCF7.
We used MapSplice to align the RNA-seq datasets.
Using these alignments, we created ACT-Graphs for all the known genes.
We applied the FDM statistical test to all the (8 2 ) pairs of replicates.
Out of these 28 pairs, 6 pairs were of MCF7-MCF7, another 6 for SUM102SUM102 and 16 were MCF7-SUM102.
The number of significantly different genes in single pair comparison are as follows: MCF7-MCF7 : 1949 (average over six pairs).
SUM102-SUM102: 1966 (average over six pairs).
MCF7-SUM102: 2727 (average over 16 pairs).
Next we applied the statistical test for replicates to get the most significant genes.
After applying the replicates statistical test, 1425 genes were judged to have significant differential transcription between MCF7 and SUM102.
CD46 is one of the genes found to be significantly different.
The UCSC browser bedgraph tracks for gene CD46 (Fig.6) shows that the middle exon has a different skipping ratio in MCF7 and SUM102.
Additional examples can be found in the Supplementary Materials.
2638 [17:24 8/9/2011 Bioinformatics-btr458.tex] Page: 2639 26332640 FDM: differential transcription analysis Fig.6.
UCSC browser: Gene CD46 in MCF7 and SUM102 (Section 3.2).
The first four samples are from MCF7 and next four samples are from SUM102.
This gene was identified as a gene with differential expression using FDM methodology.
Note that the middle exon is skipped in different ratios in MCF7 and SUM102.
This result was verifed by qRT-PCR.
Additional figures are provided in Supplementary Material.
We performed quantitative real time polymerase chain reaction (qRTPCR) on three genes to validate the FDM results.
Details for the method can be found in Section 1.4 of the Supplementary Materials.
For CD46, the skipped exon (chr1:207963598-207963690) was found to be expressed >2fold higher in SUM102 than in MCF7 as measured by qRT-PCR.
Working from the ACT-Graphs, average skipping ratios in the MCF7 samples were 0.16 and in the SUM102 samples were 0.5 predicting an average 3.1-fold change.
For NPC2 (shown in the Supplementary Materials), the retained intron (chr14:74946991-74947405) was expressed at least 10-fold more in MCF7 than in SUM102 as measured by qRT-PCR.
Working from the ACTGraphs, an average fold change of 25 was predicted.
Both experimental results were in congruence with the FDM results.
Using Cuffdiff with annotations on our dataset, NPC2 was judged to have significant differential transcription, but the test for CD46 failed and thus was inconclusive.
A third gene ZNF408 (shown in the Supplementary Materials) gave a different result in the biological experiment than predicted by the FDM method.
We directly resequenced cDNA derived from the mRNA from both cell lines and genomic DNA from both cell lines.
The region of interest (chr11:46724721-46724734) has a high number of mutations in MCF7 compared with the reference genome, a common observation for cancer cell lines and cell lines that have been propagated extensively.
This caused reads from a region of MCF7 to not align to the reference genome, and present a difference in the ACT-Graphs.
Thus, the incorrect result is due to alignment limitations, rather than to FDM.
4 DISCUSSION 4.1 FDM-JSD* correlation Although Figure 4 shows a high correlation between FDM and JSD*, there still are genes with high FDM and low JSD*.
These genes are artifacts of low coverage at some divergence nodes and could be filtered out.
Since FDM uses ratios, a variance in small edge weights can cause high variance in the flow difference.
There are also some genes with high JSD* but low FDM.
These can be due to complex gene models with many transcripts giving rise to many divergence nodes.
When most transcripts have low abundance and are unchanged between samples and just a few similar transcripts have larger abundance changes, then JSD* can be large, yet only a few divergence nodes observe large flow changes, and these are attenuated by the remaining unchanged nodes to create an FDM value that is not exceptional under permutation testing.
Focusing on divergence nodes with flow differences could improve detection of these cases.
4.2 FDM and sequencing bias Sample preparation protocols can introduce significant deviations from the assumption of uniform sampling of reads along transcript isoforms, in ways which are not fully understood.
It is useful to consider how such sampling bias would affect FDM.
(Roberts et al., 2011) cite two types of sampling bias.
Sequence-specific bias (Hansen et al., 2010) is related to the underlying sequence of nucleotides in a transcript, resulting in preferential locations for read starts.
Sequence-specific bias affects the count of reads whose alignment starts within an exonic edge in the ACT-Graph the same way for all transcripts utilizing the exonic edge.
Associating average coverage with such an edge both smooths local variation due to sequence-specific bias, and is independent of the underlying transcripts involved.
In effect, sequence-specific bias is minimized in this fashion.
Position-specific bias (Bohnert and Rtsch, 2010) is related to position in the transcript, and results in increased sampling at transcript starts and ends.
Position-specific bias affects both exonic and spliced edge coverage according to the specific transcript utilizing the edge, and this will change as the relative abundance of transcripts changes, which will alter the magnitude of the flow difference in a divergence node.
However, we have indicated that the magnitude of a genes FDM signal varies by gene, and for this reason a non-parametric test is used to determine significance.
Thus, we believe the effect of position-specific bias will not substantially affect the determination of significance.
In summary, while further investigation and validation is needed, we expect FDM to be largely insensitive to sequence-specific and position-specific sampling bias.
4.3 FDM and read length The FDM method is specifically designed to detect differential transcription with short reads (35100 nt), for which transcript reconstruction can be unreliable and, we would argue, is not needed.
As we increase read length, read alignments become more accurate and the coverage on ACT-Graph edges increases, both of which improve the accuracy of the method.
At the same time, if increased read length comes at the expense of deep sampling (under a fixed throughput assumption), then sensitivity would be expected to decrease.
Paired-end reads can improve FDM accuracy depending on the operation of the underlying RNA-seq aligner.
At the least, paired-end reads yield higher quality alignments, because of the extra constraints on mate pair distance and alignment orientation.
MapSplice aligns paired-end reads using these constraints and also incorporates a maximum likelihood method operating on the splice 2639 [17:24 8/9/2011 Bioinformatics-btr458.tex] Page: 2640 26332640 D.Singh et al.graph to infer the alignment of the complete insert, including the unsequenced fragment, given the distribution of insert lengths (Hu et al., 2010).
This results in an effective increase in read length and coverage and hence can improve the accuracy of FDM.
5 CONCLUSION While splice graphs were introduced nearly a decade ago (Heber et al., 2002), our definition is intended to record RNA-seq read coverage in such a graph (this is also the approach taken in the Flux Capacitor).
To make such graphs efficient to analyze, we choose a specific representation that differs from classic splice graphs.
Nodes are labeled with genomic coordinates which are unique and help address the ambiguities caused by overlapping exons and unannotated genomic regions.
The node labels are also well suited for computing the union of graphs from which the edge set for comparison of coverages is easy to determine.
The ACTGraph representation can dramatically decrease the data storage requirement for RNA-seq data.
It is not a lossless compression as the underlying reads cannot be recovered from the ACT-Graph, but it does suffice for the analysis of differential expression and transcription.
The FSM captures the signal of differential transcription directly from a pair of ACT-Graphs, without knowledge or inference of the underlying transcripts or need for normalization.
The FDM has high correlation with JSD*, which is an independent measure of differential transcription.
We showed that FDM can be used as classifier for differential transcription.
We presented a statistical method using a permutation test on ACT-Graphs to find genes with significant differential transcription between pairs of samples or between groups of replicates.
ACKNOWLEDGEMENTS We thank the referees for their insightful questions and comments, and thank Charles Perou for RNA samples from the MCF-7 and SUM-102 cell lines and Anais Monroy for qRT-PCR validation.
Funding: National Science Foundation (ABI/EF grant number 0850237 to J.L.
and J.F.P.
); National Institutes of Health: NCI TCGA (grant number CA143848 to Charles Perou); NCRR Idea (INBRE Grant P20RR016481 to N. Cooper); NCI GI SPORE Developmental Project Award (P50CA106991 to D.Y.C.
); Alfred P. Sloan Foundation fellowship (to D.Y.C.).
Conflict of Interest: none declared.
ABSTRACT Summary: Genomes undergo large structural changes that alter their organization.
The chromosomal regions affected by these rearrangements are called breakpoints, while those which have not been rearranged are called synteny blocks.
Lemaitre et al.presented a new method to precisely delimit rearrangement breakpoints in a genome by comparison with the genome of a related species.
Receiving as input a list of one2one orthologous genes found in the genomes of two species, the method builds a set of reliable and non-overlapping synteny blocks and refines the regions that are not contained into them.
Through the alignment of each breakpoint sequence against its specific orthologous sequences in the other species, we can look for weak similarities inside the breakpoint, thus extending the synteny blocks and narrowing the breakpoints.
The identification of the narrowed breakpoints relies on a segmentation algorithm and is statistically assessed.
Here, we present the package Cassis that implements this method of precise detection of genomic rearrangement breakpoints.
Availability: Perl and R scripts are freely available for download at http://pbil.univ-lyon1.fr/software/Cassis/.
Documentation with methodological background, technical aspects, download and setup instructions, as well as examples of applications are available together with the package.
The package was tested on Linux and Mac OS environments and is distributed under the GNU GPL License.
Contact: Marie-France.Sagot@inria.fr Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 3, 2010; revised on May 10, 2010; accepted on June 3, 2010 1 INTRODUCTION Large scale modifications of the genome, such as inversions or transpositions of DNA segments, translocations between nonhomologous chromosomes, fusions or fissions of chromosomes and deletions or duplications of small or large portions are called rearrangements.
They are further involved in evolution, speciation and also in cancer.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First authors.
One crucial step before analysing the rearrangements and their possible relation with other genomic features is to locate these events on a genome.
In the case of two genomes, it is possible to identify conserved regions, also known as synteny blocks, by comparing the order and orientation of orthologous markers along their chromosome sequences.
A region located between two consecutive synteny blocks on one genome, whose orthologous blocks are rearranged in the other genome (not consecutive or not in the same relative orientations), is called breakpoint.
As far as we know, current methods for detecting breakpoints [Grimm-synteny (Pevzner and Tesler, 2003) Mauve (Darling et al., 2004), for example] are in fact strategies for detecting synteny blocks: they provide the coordinates of the breakpoint regions only as a byproduct, simply by returning regions that are not found in a conserved synteny.
Lemaitre et al.(2008) developed a formal method that aims to go one step further and to extend the synteny blocks by focusing on the breakpoints themselves.
This method was shown to improve significantly the precision of breakpoint locations on mammalian genomes and enables to better characterize breakpoint sequences and distributions (Lemaitre et al., 2008, 2009) (see also datasets and comparisons available together with the package).
The first step of the method is to process a list of orthologous genes to identify synteny blocks between the genomes of two related species (a reference genome Gr and a second genome Go).
This step outputs a list of ordered and non-intersecting synteny blocks that are used to identify the breakpoints.
For each breakpoint on the genome Gr , we can define three sequences: the breakpoint sequence Sr , and its two orthologous sequences on the second genome Go, SoA and SoB (Fig.1).
In a second step, the method aligns the breakpoint sequence Sr against SoA and SoB and the information provided by the hits of the alignments is coded along Sr as a sequence of discrete values.
A segmentation algorithm calculates the best segmentation of this sequence of discrete values into at most three segments: a segment related with SoA, a segment related with SoB and a central segment which will represent the refined breakpoint.
2 CASSIS Cassis is a package which contains the implementation in Perl and R of the methods developed by Lemaitre et al.(2008).
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:23 16/7/2010 Bioinformatics-btq301.tex] Page: 1898 18971898 C.Baudet et al.Gr BrAr sequence SoA CoAo Go extended sequence SoA sequence Sr extended sequence Sr sequence SoB BoDo Go extended sequence SoB Fig.1.
Sequence Sr is defined by the boundaries of two consecutive synteny blocks Ar and Br on the genome Gr.
SoA (SoB) is defined by the boundaries of the orthologous block Ao (Bo) and of the previous/next synteny block (according to the orientation of the blocks) in the genome Go.
To perform the segmentation, the package considers the extended version of the sequences Sr , SoA and, SoB which includes the first/last genes of the synteny blocks.
The package receives as input data a list of pairs of one2one orthologous genes which can be found in the genomes Gr and Go.
First, all pairs of intersecting genes which have same order and direction in both genomes are merged.
Overlapping genes that do not respect this criterium are discarded.
After that, the list of genes is used to create synteny blocks according to the algorithm described by Lemaitre et al.using k =2.
Basically, the parameter k controls for the flexibility degree of the method.
With k =2, the algorithm enables individual isolated genes to be out of order without disrupting a synteny block, and all synteny blocks must contain at least two genes.
For each breakpoint on the genome Gr , we define the boundaries of the sequences Sr , SoA and, SoB according to the synteny blocks.
We perform the alignment with LASTZ (Harris, 2007) of the sequences Sr against SoA and Sr against SoB.
LASTZ was chosen because it was shown to be more sensitive in the alignment of intergenic sequences.
To obtain better results in the segmentation step, we align the extended version of the sequences Sr , SoA and, SoB.
This includes the genes that are on the boundaries of the blocks that define the sequence (Fig.1).
If at least one of the alignments (Sr against SoA or Sr against SoB) leads to a hit, the breakpoint sequence can be refined.
The segmentation algorithm is applied to the breakpoint and the refined coordinates can thus be obtained.
During this step, we perform a statistical test that verifies if the breakpoint region is actually structured into three segments to validate the obtained results.
The package Cassis also works with lists of orthologous synteny blocks.
In this case, the steps of overlapping identification and synteny blocks definition are not executed and the input data is directly submitted to the breakpoint identification step.
As we do not have information about the genes that are inside of the synteny blocks that are given by the user, to build the extended sequences we add on each side of the sequence a fragment of length L. If the resulting extended sequence has length smaller than Lmin, it means that we have a considerable overlap between consecutive blocks.
Thus, we cannot properly define the sequence and the corresponding breakpoint is not refined.
The default values of the parameters L and Lmin are 50 kbp.
This was chosen because it is close to the average size of a gene.
The package contains a main script which controls the whole process of breakpoint identification and refinement.
The script is very simple to use and receives the following parameters: Input table: tab separated values file that contains the orthology information.
It can be a list of pairs of one2one ortologous genes or a list of pairs of orthologous synteny blocks, which can be found on the genomes Gr and Go; Input type: flag that indicates the type of the given input table: G for genes and B for synteny blocks; Directory Gr (Go): directory where the script can find the sequences of the chromosomes of the genome Gr (Go); Output directory: directory which will receive the results; and Other optional parameters including a stringency level for the LASTZ alignments and the values for sequences extensions (L and Lmin).
The script generates a table that contains, for each breakpoint, the chromosome of the genome Gr where the breakpoint is located, the coordinates of the breakpoint before and after the segmentation process and a flag that can have the following values: 1, 0 and 1.
The value 1 denotes that it was impossible to execute the segmentation because the alignments output no hit.
The values zero/one denote, respectively, that the segmentation failed/passed on the statistical test.
The package also produces, for each breakpoint, a plot with the graphical representation of the segmentation.
We recommend the use of chromosome sequences whose repeats have been masked.
The alignment of masked sequences results in more relevant hits and, consequently, on better segmentation results.
The package contains a main script which controls the execution of a set of scripts that performs atomic tasks.
The modularization of the implementation answers to the needs of advanced users who may desire to create their own pipelines of breakpoint refinement.
Funding: Coordenao de Aperfeioamento de Pessoal de Nvel Superior (4676/08-4 to C.B.
); Conselho Nacional de Desenvolvimento Cientfico e Tecnolgico (472504/2007-0, 479207/2007-0 and 483177/2009-1 to Z.D., partial); French project ANR (MIRI BLAN08-1335497); French-UK project ANR-BBSRC (MetNet4SysBio ANR-07-BSYS 003 02); Project ERC Advanced Grant Sisyphe.
Conflict of Interest: none declared.
ABSTRACT Motivation: The nucleosome is the basic repeating unit of chromatin.
It contains two copies each of the four core histones H2A, H2B, H3 and H4 and about 147 bp of DNA.
The residues of the histone proteins are subject to numerous post-translational modifications, such as methylation or acetylation.
Chromatin immunoprecipitiation followed by sequencing (ChIP-seq) is a technique that provides genome-wide occupancy data of these modified histone proteins, and it requires appropriate computational methods.
Results: We present NucHunter, an algorithm that uses the data from ChIP-seq experiments directed against many histone modifications to infer positioned nucleosomes.
NucHunter annotates each of these nucleosomes with the intensities of the histone modifications.
We demonstrate that these annotations can be used to infer nucleosomal states with distinct correlations to underlying genomic features and chromatin-related processes, such as transcriptional start sites, enhancers, elongation by RNA polymerase II and chromatin-mediated repression.
Thus, NucHunter is a versatile tool that can be used to predict positioned nucleosomes from a panel of histone modification ChIP-seq experiments and infer distinct histone modification patterns associated to different chromatin states.
Availability: The software is available at http://epigen.molgen.mpg.de/ nuchunter/.
Contact: chung@molgen.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 20, 2013; revised on June 24, 2013; accepted on August 1, 2013 1 INTRODUCTION The genome of eukaryotes is packaged into a macromolecular structure called chromatin.
The basic repeating unit of chromatin is the nucleosome, which contains two copies each of the four core histones H2A, H2B, H3 and H4, around which a 147-bp stretch of DNA is wrapped in a flat left-handed superhelix (Luger and Richmond, 1998).
Nucleosomes form approximately every 200 bp along the genome to package the underlying DNA.
Apart from the packaging function, nucleosomes may serve as a signaling module (Turner, 2012) that is integrated into biological processes acting with and on chromatin.
This signaling function depends on post-translational modifications of the histone proteins, such as acetylation and methylation of lysine residues.
These histone modifications may serve as a binding platform for non-histone proteins, whose activities change chromatin structure and function.
When nucleosomes tend to form at the same or nearby genomic positions in different cells, they are called (well) positioned.
Positioned nucleosomes are important for the hypothesis that nucleosomes constitute a signaling module, because gross movements of modified nucleosomes along the chromatin fibers may lead to a loss of coherence between the modifications and the genomic features and/or functions.
The binding locations of modified histone proteins can be determined by a technique called chromatin immunoprecipitation followed by sequencing [ChIP-seq; Johnson et al.(2007)].
The immunoprecipitation step enriches for chromatin fragments containing a histone modification of interest, whereas the sequencing step is used to quantify the abundance of the underlying DNA.
Because the core histone proteins are part of a stable protein DNA complex, it is natural to assume that the localization of modified histone proteins corresponds to the position of the nucleosomes.
This suggests that histone modification ChIP-seq data can be used to infer nucleosome positions.
However, this is far from being a trivial task for a number of reasons: (i) histone binding does not seem to be as sequence-specific as for many transcription factors; (ii) nucleosome positions can change considerably with time and across cells; and (iii) the data are affected by sparse sampling and high noise.
Nucleosome calling algorithms, such as the one presented here, aim at detecting positioned nucleosomes.
To obtain a comprehensive and reliable set of predictions, one should combine the information contained in as many ChIP-seq experiments as possible and allow for some plasticity in the shape of the signal.
However, modified histones tend to be mixed-source factors (Landt et al., 2012), which means that the degree of positioning can vary considerably across the genome.
In regions where nucleosomes occupy different positions in different cells (e.g.within the body of actively transcribed genes), nucleosome calling algorithms are less suitable than segmentation approaches [Song and Smith (2011); Zang et al.(2009), to mention a few], which aim at detecting domains of high nucleosome abundance.
A number of tools for the inference of nucleosome positions have already been developed.
Most of them apply signal processing techniques, such as Fourier transforms (Flores and Orozco, 2011), wavelet decomposition (Zhang et al., 2008a) and ad hoc filters (Albert et al., 2007; Weiner et al., 2010), to smooth the enrichment profile, followed by the detection of local maxima.
Others are based on Bayesian modeling of the nucleosome*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/bync/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com bas e pair s base pairs ( ) ile Since very is for example, ) in order  enrichment pattern (Zhang et al., 2012).
These methods do not allow one to control for systematic biases by comparing the nucleosome calls with data from control experiments.
Furthermore, they cannot integrate data from multiple histone marks in a straightforward manner.
Finally, because of the large size of the problem, e.g.the human genome, and the potentially high number of histone modifications, the runtime and memory consumption of these tools may limit their applicability.
Our tool can use information from a control sample to correct for systematic biases inherent in this high-throughput technology.
It is designed to integrate multiple histone marks to broaden the range of nucleosome positions that can be detected.
It annotates each identified nucleosome with the contributing histone modifications.
We will demonstrate that these annotations can be used to cluster nucleosomes by their histone modification patterns.
This clustering yields patterns of modifications that can be correlated to the function of the chromatin, such as transcriptional start sites and enhancers, or to the underlying process, such as transcriptional elongation by RNA polymerase II.
These results support the assumption that nucleosomes serve as signal modules for biological process and that the corresponding histone modification patterns are a reflection of the signaling taking place on these modules.
2 METHODS The algorithm performs three major steps: (i) a preprocessing step, where each file containing the chromosomal positions of mapped reads is turned into a numerical signal, (ii) a shape detection step, where candidate positions for nucleosome formation sites are detected and (iii) a filtering step, where these candidates are filtered and scored accounting for a number of possible sources of bias.
In the following, we will refer to the enrichment profile on the positive or negative strand as the signal that counts for each location the number of positive or negative reads whose 50 end maps there, and they will be denoted P(p) and N(p), respectively, where p is the chromosomal position.
2.1 Preprocessing A well-positioned nucleosome typically exhibits the enrichment profile shown in Figure 1: a peak of positive strand reads upstream of the nucleosome location, and one of negative strand reads downstream.
To obtain a consensus signal, which will be called the input signal I, the enrichment profile on the positive strand P is shifted to the right, the one on the negative strandN is shifted to the left and the sum of the two is considered.
Denoting with F the average length of a fragment in the DNA library, the amount of this shift is about F=2, which yields the input signal: Ip Pp F=2 Np F=2: In case of single-end sequencing data, usually the average fragment length needs to be estimated from the data itself.
This estimation can be carried out by several available tools [such as Zhang et al.(2008b)].
However, because of the mixed source nature of the data, we found the available methods unsatisfactory when applied to histone marks, and therefore, as part of NucHunter, we also provide a method for estimating the average fragment length (described in Section 2.4).
2.2 Peak detection In the peak detection step (see Fig.2), a suitable filter is applied to the input signal, followed by the detection of local maxima in the filtered signal and the analysis of the statistical significance of these maxima.
A filter (more precisely a linear time-invariant filter) is characterized by a discrete signal K(p) called impulse response.
Given an input signal I(p), the filter output O(p) is the result of the following operation, called convolution: Op I Kp X j Ip j K j , where the index j ranges over positions where K(j) is not 0.
The impulse response in our approach has been chosen according to the following two criteria: first, it must separate sharp peaks from more spread out read distributions or non-enriched regions; second, it must have good smoothing properties, so that the convoluted signal contains a limited number of local maxima (Rice, 1944) and, therefore, the algorithm returns fewer false positives.
We chose as impulse response the second derivative of a Gaussian density function, also known as the Mexican hat wavelet (see Fig.3): Ki 1 i 2 2 e i2 22 : TheMexican hat wavelet removes from the Fourier spectrum of the input signal both high-and low-frequency components (band-pass filter), which is appropriate if we interpret high frequencies as random oscillations Fig.1.
Preprocessing: from mapped reads to consensus signal.
Positive and negative reads generate a strand specific enrichment profile which counts at each position the amount of reads whose 50 end maps there.
The consensus signal is obtained by shifting the strand specific enrichment profiles F/2 bases downstream, where F is the average fragment size, and summing them up Fig.2.
Peak detection from the consensus input signal.
The input signal is smoothed using a filter with a certain impulse response, then the maxima of the resulting signal are detected and non-significant local maxima are filtered out 2548 A.Mammana et al.due to which P , ' , ( ) due to , zero , ly because of noise or insufficient coverage, and low frequencies as broad ambiguous peaks coming from a mixture of nucleosome positions, or as local biases such as GC content or open chromatin.
The wavelet is parametrized by the scale parameter.
In our studies, we chose a default value of 50 for because, in general, it corresponds to a good compromise between calling too many peaks and merging closely spaced ones.
The parameter can also be fitted to the dataset under consideration using the method outlined in Section 2.4.
Obtaining the convoluted signal for large genomes poses computational problems.
In fact, a long signal as impulse response results in a slow convolution operation.
In NucHunter the convolution has been implemented using recursive filters, an efficient signal-processing technique (Hale, 2006).
Once local maxima are extracted from the filter output, their statistical significance is assessed.
To this end, we model the noise by assuming that values of the input signal within a certain region are independent identically distributed random variables (rvs).
Using this assumption, we derive the mean and standard deviation of the convoluted signal, and we assign a z-score to each local maximum.
If I(p) denotes the input signal, K(p) the impulse response and O(p) the convoluted signal at position p, let m(p) and std(p) denote, respectively, the mean and standard deviation of the input signal in a large region R that contains position p, then the z-score is given by: mp X k2R Ik jRj , stdp ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiX k2R Ik mp2 jRj 1 vuut , z-scorep Op mp P i2N Ki stdp ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiP i2N Ki2 q : The detected peaks are all those local maxima with a z-score above a certain threshold.
This z-score represents the strength of a peak, and a user-defined threshold, whose default value is 3, specifies how many standard deviations above average the peaks strength must be.
Additionally, the peaks are assigned a fuzziness score that represents the degree of uncertainty about the peak position, given by the formula: fuzziness p ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 2 Op O0 0p s , where O0 0p denotes the second discrete derivative of the filter output.
2.3 Filtering and scoring After a set of putative peaks has been derived, additional filtering steps are carried out when a control sample is available.
They are all based on the enrichment level of a peak, defined as the total number of reads that contribute to the input signal in a window of a certain size (by default 147bp) centered around the peak.
Peaks are filtered in a similar manner as in Zhang et al.(2008b): the enrichment level is modeled as a Poisson rv whose parameter is estimated from both a global and a local average of the control sample, which is rescaled so that the number of sequenced reads in the two samples matches.
From this model a P-value is obtained, and peaks can be filtered based on a user-defined threshold (which defaults to 105).
In more detail, let G denote the genome length.
The noise level in the control sample is rescaled according to the total coverage ratio  P k2N IkP k2N Ck , so that the local and global noise estimates W and tot can be expressed, respectively, as PW kW Cpk 2W1 and P k2N Ck G , and the final noise estimate is chosen as the maximum between the two (W defaults to 1000).
Finally, the null model for the read counts in a window of a fixed radius R is given by: XF kF Ip k Poiss2F 1: The next filtering step consists in controlling the relative amount of positive and negative reads in the enrichment level, as highly unbalanced contributions from the two strands are likely to arise from mapping biases.
Following the approach of Zhang et al.(2008a), we filter out peaks where the ratio between the two contributions is not contained in the interval r, 1=r, where r defaults to 4.
A final step takes place when the sample is obtained from multiple ChIP-seq experiments.
In this case, the consensus input signals from the different samples are added together, and the above steps are carried out as if the signal came from a single experiment.
After that, however, the enrichment level at each peak is decomposed into the contributions of the different experiments, and each of them is tested independently to assess whether a certain histone modification is present or not (see Fig.4).
The tests are carried out using the same noise model and formulas shown above, where now I corresponds to the consensus signal derived from the single histone modifications.
Finally, for each nucleosome call the algorithm provides, along with the genomic coordinates, the following statistics: (i) the z-score (peak strength), (ii) the input signal enrichment level (sum of the raw read Fig.4.
Integration of multiple histone modification experiments.
First, peak detection is performed on the sum of the input signals, then the signal is decomposed into the contributions of the single histone modifications and then a statistical test is performed for each of them to asses whether their contribution is significant or not Fig.3.
The Mexican hat wavelet for  50 (on the right) and its frequency spectrum (on the left) 2549 NucHunter due to , , that s base pairs random variable p since counts in a window of 147bp around the peak), (iii) the control signal enrichment level (sum of the smoothed read counts in the same window), (iv) a P-value derived from the comparison between input and control enrichment levels (significance of the enrichment) and (v) the fuzziness score for the peak position.
In case multiple samples are simultaneously analyzed, it is also provided, for each input sample and each nucleosome call, the contribution to the total enrichment level in terms of raw reads and the result of the statistical test as an on/off flag.
2.4 Inferring the average fragment length The average fragment length F is typically inferred based on the strand cross-correlation function, defined as: CCk X p PpNp k, where the index p spans all genomic positions.
For point-source factors and low noise levels the cross-correlation function usually has a peak at position F (the fragment peak), as shown in Figure 5, which yields a straightforward method for the estimation of F. However, for many histone marks the cross-correlation plot is harder to interpret because of the presence of a so-called phantom-peak (Landt et al., 2012) and other systematic biases, which can sometimes completely obscure the fragment peak (see Fig.5b).
To account for these biases, we introduce a modified cross-correlation function that we call peak cross-correlation (pcc):dCCk X p PpNp k: The signals N and P are a dense representation of the peaks obtained applying a peak detection algorithm to the strand-specific signals N and P. More specifically, N and P are binary signals whose only non-zero entries are ones occurring at the peaks locations.
The peak detection technique presented in Section 2.2 applied to the consensus signal I is applied to the signals N and P. The pcc function, which is used to infer F, can also indicate how appropriate the choice of is, where is the parameter used for peak detection (see Section 2.2).
If N and P are assumed to be two replicates of the same signal with a systematic shift of F base pairs in the nucleosome peaks, a good choice of should result in a strong peak in the pcc function around position F, whereas a bad choice should lead to almost independent peaks in the two strands and a flat pcc function.
After the pcc function is computed, a clustering technique is applied to interpret it, which yields an estimate for F and a quality score for.
We assume that the plot is generated by sampling from a mixture of three rvs: a uniform rv to model the background noise, a Gaussian rv to model the phantom peak and another Gaussian rv to model the fragment peak, as shown in Figure 5c.
The parameters of these distributions are inferred using an expectation-maximization algorithm, and the mean of the Gaussian rv corresponding to the fragment peak is used as an estimate for the average fragment length.
The quality score, which is derived from the likelihood of the inferred model, can be computed for different values of and yields a score curve (see Section 1 in Supplementary Material for more details).
3 RESULTS 3.1 Comparison to other available tools We have developed NucHunter to identify nucleosome positions using histone modification ChIP-seq data.
To test the predictive power of our algorithm and to compare it with other available tools, we ran NucHunter and two other tools [Nucleosome Positioning from Sequencing (NPS) from Zhang et al.(2008a); Template Filter from Weiner et al.(2010)] on a H3K9ac dataset from yeast (Weinberger et al., 2012).
Some tools had to be (a) (b) (c) Fig.5.
Strand cross-correlation analysis for some ChIP-seq experiments in human K562 cells.
(a) Histone modification H3K4me3, a point-source or mixed-source factor.
The phantom peak and the fragment peak are clear.
(b) Histone modification H3K9me3, a broad-source factor.
The fragment peak is almost not visible, in contrast with the phantom peak.
(c) The pcc for the histone modification H3K9me3.
Now also the fragment peak is visible, and it is possible to infer the average fragment length with an EM algorithm 2550 A.Mammana et al.s p , `` '' due to ` `` ' '' ( ) that random variables ( ) g g ( ) g In order t to ( ) excluded from the comparison either because they were not able to deal with the large amount of data or because the results obtained using default parameters were unsatisfactory.
We chose yeast because we wanted to compare the predictions to a base pair resolution map of nucleosome positions in yeast (Brogaard et al., 2012).
This map has been obtained with a technique that, even if it has not been tested widely yet, is independent from ChIP-seq, and it is claimed to be more accurate.
In line with previous studies (Chung and Vingron, 2009), to compare the nucleosome predictions with the nucleosome map, we used the (normalized) area under the cumulative error curve (AUC) as a performance measure.
The AUCwas obtained applying the following procedure (see also Supplementary Material): (1) we consider the set of all distances between nucleosome predictions and nucleosomes in the map573bp, (2) we obtain a cumulative error curve.
In such a curve, a point x, y means that a fraction y of the distances is less than x base pairs (see Supplementary Fig.S5), (3) we compute the AUC and we normalize it, so that a set of perfect predictions has an AUC of 1 and a random set of genomic positions has an expected AUC of 0.5.
Along with the AUC, we also computed the sensitivity and the specificity, defined, respectively, as the fraction of nucleosomes in the map that are closer than 20bp to a nucleosome prediction and the fraction of nucleosome predictions that are closer than 20bp to a nucleosome in the map.
Moreover, to account for the great variability in the number of predictions returned by each tool, we repeated the performance measurements for different score thresholds.
The results from Figure 6 show that NucHunter makes more accurate predictions compared with the other tools.
Considering the default score thresholds, NucHunter andNPS return a similar number of predictions but the former has an higher AUC than the second, whereas Template Filter returns many more predictions and of lower quality.
When the score threshold is increased, the AUC difference between NucHunter and NPS becomes much more pronounced.
This suggests that the nucleosome predictions with highest score from NucHunter are, in general, much more precise compared with those from the other tools.
All the tools suffer from low sensitivity in this dataset, in particularNucHunter and NPS when the default score thresholds are used.
The reasons for unidentified nucleosomes or incorrect predictions can be many.
In the first place, the experimental procedures used for the ChIP-seq experiment and that used for the nucleosome map are different.
Roughly 5.6% of the nucleosomes in the map, for instance, are located in low-mappability regions and are not covered by any read.
Moreover, the ChIP-seq experiment targeted only acetylated nucleosomes, as opposed to the nucleosome map.
A more general problem is the identification of fuzzily positioned nucleosomes.
If the nucleosome positioning varies extensively from cell to cell, the assumptions made by the algorithms are violated and nucleosomes are hard to identify.
Lastly, both specificity and sensitivity are affected from high noise levels, insufficient sequencing coverage and sequencing biases.
In addition to the yeast dataset, we also tested the algorithms on a simulated dataset and on different histone modification ChIP-seq files in human K562 cells.
In the simulated dataset, the nucleosome map is randomly generated and the reads are generated accordingly.
Because there is no nucleosome map for the human dataset, we used pairs of replicate experiments and pairs of different histone modifications as gold standard-predictions pairs.
The details of the simulation and the performance evaluations are reported in Supplementary Material in Sections 3 and 4.
In general, the results are in agreement with those shown previously.
In the Supplementary Material, it is also shown that NucHunter runs faster and requires less memory than the other two algorithms (see Supplementary Material Section 5).
Fig.6.
Accuracy assessment of different tools on the yeast dataset.
The performance measures (AUC, sensitivity and specificity) are computed for every possible score threshold, which results in an AUC number of calls curve (left) and a specificitysensitivity curve (right).
The circles indicate the performance of the algorithms using the default thresholds 2551 NucHunter to less than s base pairs in Supplementary Material area under the cumulative error curve s base pairs s base pairs to to very for the human dataset as well as above 3.2 Clustering of nucleosomes based on histone marks We ran NucHunter on a composite dataset from a human leukemia cell line [K562, Myers et al.(2011)] consisting of a control experiment and 12 ChIP-seq experiments for different histone modifications.
For each detected nucleosome and for each experiment the algorithm returned, along with other statistics, the raw read count within a window of a specified width (which defaults to 147) around the inferred nucleosome location (see Methods).
We used these read counts for an exploratory analysis of the chromatin landscape.
After a normalization procedure that corrects the read counts taking into account the control sample, the different sequencing depths of the datasets and the nucleosome abundance at each locus, we obtained a joint histone modification level distribution and we applied the k-means clustering algorithm on it (see Supplementary Material for more details).
Given a parameter k, this unsupervised learning method aims at partitioning the data points into k different families (clusters) such that elements in the same cluster are as similar to each other as possible.
Each cluster is characterized by its centroid, which is, in our case, a prototypical histone modification pattern.
We found that with k equals 6 the results are robust, whereas for higher values of the parameter the clusters tend to change depending on the initialization (see Supplementary Material).
Moreover and most importantly, we found that such a partitioning, derived solely from the histone modification patterns, can also capture biologically meaningful positional features of the nucleosomes.
We assigned labels to each cluster based on the histone modification pattern and genomic localization.
The labeled centroids are shown in Figure 7.
We studied the genomic localization of nucleosomes from the different clusters using the RefSeq annotation dataset as well as publicly available data from cap analysis of gene expression (CAGE) and DNase I hypersensitivity sequencing experiments (Myers et al., 2011).
We performed the following analyses (further discussed in Supplementary Material): (i) we derived a consensus nucleosome profile along genes by considering a large set of annotated genes, by rescaling their nucleosome profiles to the same length and by adding them up (Fig.8a); (ii) we analyzed the nucleosome positioning around promoters of active genes by considering the distribution of distances between CAGE tags and nucleosomes (Fig.8b); and (iii) we obtained the average DNase I hypersensitivity profile around nucleosomes for each class (Fig.8c).
Overall these data give a clear picture of the nucleosome landscape and recapitulate previous knowledge (see Fig.7).
The nucleosomes in the first family are characterized by a strong enrichment of H3K4me2/3 and H3K9ac, and they tend to reside in the 50 portion of a gene near the transcriptional start site (TSS; Fig.8a).
Thus, we labeled them promoter nucleosomes.
In proximity of promoters of active genes, these nucleosomes exhibit a strikingly regular pattern (Fig.8b), whose main features are a nucleosome-depleted region right upstream the TSS and a well-positioned nucleosome 170bp downstream (the 1 nucleosome).
The second and third clusters show an enrichment of H3K4me1 and H2AZ as well as a general enrichment of active marks, whereas TSS-associated histone marks, such as H3K4me2/3 and H3K9ac, are less enriched compared with the promoter cluster.
These features, together with the high levels of DNase I hypersensitivity that we observe (Fig.8c), suggest that these nucleosomes may flank enhancer sequences.
Thus, we labeled them as enhancer 2 and enhancer 1 nucleosomes.
The fourth centroid is enriched in H3K79me2 and H4K20me1, whereas the fifth centroid is enriched in H3K36me3 and H3K9me1, which are all histone marks related to elongation of RNA polymerase II (Vavouri and Lehner, 2012).
The localization of these two classes of elongation nucleosomes along the gene body, shown in Figure 8a, suggests that the 5th centroid is enriched toward the 30 end of a gene, whereas the 4th centroid is enriched more to the 50 end.
Thus, we termed them elongation early and elongation late nucleosomes, respectively.
The last centroid is characterized by an enrichment of H3K9me3 and H3K27me3, suggesting that it represents chromatin-repressed genomic regions (Margueron and Reinberg, 2011).
Thus, we termed it repressed.
Lastly, we explored the relation between the different clusters that we obtained and a previously published study (Ernst and Kellis, 2010) that aimed at classifying the chromatin landscape into discrete states.
Even though the last method uses a more complex model, different data sources and positional relations between histone modification patterns, we found that the overall results are comparable (see Supplementary Material).
We believe that the joint analysis of histone modification patterns and nucleosome positioning that NucHunter allows for provides complementary information and offers a greater potential than histone modification studies based on arbitrary binning schemes of the genome.
Fig.7.
Using the k-means algorithm, 422547 nucleosomes called by NucHunter were clustered into six clusters: promoter (20.4%), enhancer 1 (19.8%), enhancer 2 (14.4%), elongation early (16.4%), elongation late (14.7%) and repressed (14.3%).
The rows of the heatmap represent the centroids of the clusters and the columns represent the histone modifications.
The labels have been assigned based on prior biological knowledge 2552 A.Mammana et al.( ) ile l ( ) ( ) ( ) is s s very ' ( ) l `` '' ( ) very s ile to ( ) l `` '' `` '' ile ( ) s ' ile ' `` '' `` '' `` '' which 4 DISCUSSION The fast-paced development of chromatin immunoprecipitationbased techniques is heading toward an increased spatial resolution for DNAprotein interactions.
In line with this trend, we developed NucHunter, a software for base pair resolution nucleosome identification in ChIP-seq experiments.
The innovative aspects of this tool reside in a more accurate and efficient signal processing, an improved statistical analysis of the peaks, the possibility of integrating data from a control sample and to consider multiple histone modifications at once.
We put forward a nucleosome-centric view, because if we view the modifications (either sequentially or in a combinatorial pattern) as a reflection of a signaling activity then nucleosomes can be viewed as signaling modules (Turner, 2012).
In agreement with this idea, we found that nucleosomes can be clustered into distinct subgroups.
These subgroups either mark certain functional regions of the genome, such as promoters and enhancers, or are related to biological processes, such as elongation or chromatin-mediated repression.
Although this is not a new finding [see Ernst and Kellis (2010)], we think that our approach has the benefit of assigning the data to a physical entity that carries the information: the nucleosome.
Thus, separation of different histone modification patterns into distinct subgroups becomes much more meaningful than by arbitrarily binning the genome into non-overlapping windows (Ernst and Kellis, 2010), where two nucleosomes with different modification patterns could be present.
(a) (b) (b) 5000 0 5000 10000 15000 20000 25000 0 5 10 15 20 25 30 distance from TSS (bp) co un t TSS TES elongation early elongation late enhancer 1 enhancer 2 promoter repressed Fig.8.
Genomic localization of the different nucleosome classes in human K562 cells.
(a) Occupancy of nucleosomes from the different classes along the gene body.
The nucleosome occupancy profiles from a subset of genes in RefSeq have been rescaled to the same length and summed up.
(b) Nucleosome distribution at promoters of active genes.
The profile has been obtained by computing the distribution of distances between CAGE tags and nucleosomes.
(c) DNase I hypersensitivity levels in relation to nucleosomes.
The profile for each nucleosome class is the average DNase I hypersensitivity profile of all nucleosomes from that class 2553 NucHunter `` '' ( ) In summary, we developed a new tool called NucHunter that is able to identify positioned nucleosomes along the genome using ChIP-seq data of histone modifications and annotates each nucleosome with (i) a flag indicating presence or absence of a certain histone modification and (ii) the number of contributing reads (if one is interested in a more quantitative view).
We demonstrated that NucHunter performs better than currently available tools and has some features not present in any of them.
By focusing on the nucleosome as information carrier, charting the epigenome will become much more meaningful and will in the long run allow for unraveling novel chromatin-mediated mechanisms.
ACKNOWLEDGEMENTS A special thank goes to Johannes Helmuth and Matthew Huska for critical reading of the manuscript.
Funding: This work was supported by the International Max Planck Research School for Computational Biology and Scientific Computing; and the Bundesministerium fur Bildung und Forschung for the Deutsches Epigenom Programm (DEEP) [01KU1216C].
Conflict of Interest: none declared.
ABSTRACT Motivation: Reverse-phase protein arrays (RPPAs) allow sensitive quantification of relative protein abundance in thousands of samples in parallel.
Typical challenges involved in this technology are antibody selection, sample preparation and optimization of staining conditions.
The issue of combining effective sample management and data analysis, however, has been widely neglected.
Results: This motivated us to develop MIRACLE, a comprehensive and user-friendly web application bridging the gap between spotting and array analysis by conveniently keeping track of sample information.
Data processing includes correction of staining bias, estimation of protein concentration from response curves, normalization for total protein amount per sample and statistical evaluation.
Established analysis methods have been integrated with MIRACLE, offering experimental scientists an end-to-end solution for sample management and for carrying out data analysis.
In addition, experienced users have the possibility to export data to R for more complex analyses.
MIRACLE thus has the potential to further spread utilization of RPPAs as an emerging technology for high-throughput protein analysis.
Availability: Project URL: http://www.nanocan.org/miracle/ Contact: mlist@health.sdu.dk Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Reverse-phase protein arrays are typically nitrocellulose-covered glass slides on which crude lysates of tissue samples or treated cell lines are spotted.
Each single slide can carry several thousand spots.
Only small amounts of lysate in the range of a few cell equivalents are required for each spot.
Consequently, several hundred slide copies can be created at minimal sample consumption, each of which can be interrogated with a different proteinspecific antibody.
This allows high-throughput measurement of the relative abundance of proteins in up to several thousand samples.
Parallel processing of large sample numbers discerns RPPAs from forward phase arrays, where probes are immobilized on a slide, and mass spectrometry, which are suitable for analysis of many proteins in small sample numbers.
The field of RPPAs has shown steady growth since its introduction in 2001 (Paweletz et al., 2001) (Fig.1).
Several studies applied this technology successfully to protein and signaling pathway analyses in cancer (Leivonen et al., 2009; Sevecka and MacBeath, 2006; Uhlmann et al., 2012; York et al., 2012), as well as for cancer subtype classification and prognosis of disease progression (Gonzalez-Angulo et al., 2011; Sonntag et al., 2014; Wiegand et al., 2014).
The relevance of RPPA data for multiOMICS and high-throughput projects is also highlighted by its inclusion into the Cancer Genome Atlas, where, for instance, measurements of 171 antibodies for 4400 samples of breast cancer patients are available (Atlas, 2012).
1.1 Challenges Experimental challenges involved in this technology, such as antibody selection, sample preparation and optimization of staining conditions, have been addressed successfully in the past (Hennessy et al., 2010; Mannsperger et al., 2010b; Mircean et al., 2005; Spurrier et al., 2008).
The subsequent image analysis can be handled by established methods and software developed for traditional microarrays, such as the commercial tool MicroVigene2.
RPPA-specific challenges arise in the further processing of the quantified signal, which, in a first step, should be corrected for bias introduced through uneven staining (Neeley et al., 2012).
Another concern is the dynamic range of signal detection, which can be described as a sigmoidal curve due to limitations in sensitivity in the lower range and signal saturation in the upper range (Tabus et al., 2006).
Through adjusting each sample for the total protein amount a priori, measurement is possible in the linear range of this curve.
However, this is often not feasible for high-throughput experiments, due to the tradeoff between large sample numbers, feasibility and costs.
To overcome this problem, samples are typically spotted multiple times in a dilution series to cover a broad dynamic range of protein concentrations, where each sample gives rise to a response curve.
In a process called quantification, an estimate of relative protein abundance is created by merging these values (Supplementary Fig.S1).
The resulting relative concentration estimates still need to be normalized for the total amount of protein, before they can be evaluated statistically.
Statistical analysis comprises assessing significance of relative differences in protein concentration estimates, as well as their correlation between slides and *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Reverse ; Leivonen etal., 2009 ; Uhlmann etal., 2012 more than ; Mannsperger etal., 2010b; Hennessy etal., 2010 In order tXPath error Undefined namespace prefix other types of readout.
Finally, the large number of samples involved in each experiment poses a significant challenge in terms of sample management.
1.2 Quantification Mircean et al.(2005) proposed a linear model for merging the signal originating from individual response curves.
One drawback of this approach is that a linear model cannot deal with samples close to saturation or close to the detection limit.
Consequently, Tabus et al.(2006) compared a variety of parametric models and found that a logistic model was most suitable to reflect the sigmoidal shape of the response curve.
Furthermore, a joint response curve based on all samples increased the confidence of the model fit, as similar chemistry can be assumed for all samples.
Hu et al.(2007) showed that a more flexible non-parametric model yields more accurate estimates at the cost of robustness.
Finally, Zhang et al.(2009) proposed a simplified robust parametric model called serial dilution curve based on the Sips model for DNA binding.
In contrast to other parametric models, this method is based on meaningful and intuitive parameters like the detection limit, the dilution factor and the saturation level.
1.3 Normalization If the total protein amount of each sample is not determined a priori, protein levels have to be normalized, to guarantee a meaningful comparison between samples.
This can be achieved by either normalizing to a slide stained for total protein with, for instance, Sypro Ruby (Leivonen et al., 2009) or Fast Green (Loebke et al., 2007), or by using additional antibody stainings.
For the latter approach, one can rely on either a selection of housekeeping proteins that are assumed to be constantly expressed or on incorporating the entire panel of antibody-generated signals, where all antibodies are first centred and scaled before the median value is used for normalization.
This so-called median loading normalization has been improved by Neeley et al.(2009) in a method called variable slope normalization.
Here, a correction factor is included to take into account that additional bias arises due to independent slide measurements.
Finally, Neeley et al.(2012) also proposed an additional normalization step called surface adjustment that is applied to the raw data.
As customary for all microarray data, the background signal is determined for each spot and subtracted from the foreground signal.
This approach, however, does not correct for signal bias due to uneven antibody staining, which is an issue specific to RPPA technology.
Positive control spots on the slide can be utilized for creating a smoothing surface mirroring the staining bias.
A correction factor can then be calculated from a generalized linear model for each individual spot.
1.4 Sample tracking A single RPPA experiment may comprise thousands of samples distributed over large slide sets.
Precise sample tracking is a challenge that grows with the number, size and complexity of the RPPA experiments.
To date, the only documented solution to address this critical issue is an integrated platform called RIMS (Stanislaus et al., 2008), which provides features for uploading and annotating sample information, data visualization, correlation and pathway analysis.
Notably, the authors also propose a XML standard called RPPAML to overcome the lack of a data exchange format for RPPA data and a standardized annotation.
Unfortunately, however, none of the project URLs are accessible (last access attempt March 20, 2014), indicating that the project is no longer under active development and has not been adapted by the community.
Being implemented for the commercial softwareMATLAB, RIMS also lacks integration of R methods commonly used for RPPA analyses.
Finally, RIMS only supports sample tracking at the slide level and not at the level of the plate formats that form the basis for all experimentation.
This leaves the most difficult step of sample tracking to the user: Samples are taken up by an extraction head configured to generate a slide in multiple extractions, thereby producing a complex spotting pattern that does not permit researchers to locate their samples in a straight-forward fashion.
1.5 Existing solutions Implementations for both, parametric and non-parametric methods are available through the R packages SuperCurve (Hu et al., 2007) and RPPanalyzer (Mannsperger et al., 2010a).
A major challenge in analysing RPPA data is, however, that end users are often not familiar with R. SuperCurve overcomes this problem partly by offering an tcl/tk-based graphical user interface, making both analysis and experimental design more accessible.
Sample management on a larger scale, however, is neglected.
RIMS addresses some of these issues, but does not cover more complex data analysis (see Table 1).
1.6 Microarray R-based analysis of complex lysate experiments (MIRACLE) This motivated us to develop MIRACLE, a comprehensive and user-friendly open-source web application providing an end-to-end solution covering experimental design, sample tracking, data processing, normalization, as well as visualization and statistical analysis of the results.
MIRACLE conveniently keeps 0 20 40 60 2002 2004 2006 2008 2010 2012 2014 Year N um be r of p ub lic at io ns Fig.1.
Number of RPPA publications per year.
Data were extracted for 20012013 from PubMed, using the search term reverse phase protein OR reverse-phase protein i632 M.List et al.(Mircean etal., 2005) since  (Tabus etal., 2006) (Hu etal., 2007) (Zhang etal., 2009) in order '' '' e (Neeley etal., 2009) (Neeley etal., 2012)/ 03/ z track of sample information, starting with the source plates, throughout array generation and down to the signal data, in a process called virtual spotting.
MIRACLE allows biological researchers without any knowledge of R to process and analyse RPPA data efficiently, grasping back to established methods by interacting directly with R in the background.
This interface will also allow future methods to be added in a straight-forward fashion.
Results are directly visualized and can be investigated interactively with regards to statistical significance, as well as to correlation to primary plate based readout data.
MIRACLE is designed with user approachability in mind, but also supports R data analysts by offering a convenient data export/import interface with R. While the data analysis part of MIRACLE is particularly laid out for handling RPPA data, the sample management functionality is suitable for any kind of customized array design.
With its deep integration of sample management and data analysis, MIRACLE separates itself from existing solutions that only cover parts of the RPPA work-flow shown in Figure 2.
See Table 1 for an overview of existing solutions and MIRACLE.
2 SYSTEM AND METHODS 2.1 Sample management 2.1.1 Plate and slide layouts In a typical RPPA experiment, lysate samples are stored in 96-well or 384-well microtiter plate, before they are subjected to microarray generation using a bioarrayer,-printer or-spotter.
Already at this stage, MIRACLE supports experimental design by offering an interactive web interface for creating so-called plate layouts.
To avoid cryptic and long sample names, several layers of information can be included, for instance regarding cell material, treatments, applied compounds, lysis conditions, etc.
(Fig.3).
A lot of this information is shared by samples and is therefore redundant.
MIRACLE stores information in relational databases, where each layer corresponds to a single property, keeping the data concise through use of ids and mapping tables (Supplementary Fig.S2).
By relying on linked tables of a relational database, changes of layout properties are immediately available to all samples and experiments, thereby ensuring data consistency and comparability in the analyses.
Similar to plate layouts, users can also define slide layouts, in which sample properties for each individual spot of the slide can be edited using the aforementioned sample layers.
The format of the slide is determined through specifying the number of rows and columns, as well as blocks where applicable, e.g.when using spotters, where each pin of the extraction head gives rise to a different block.
2.1.2 Virtual spotting As previously mentioned, it is not always trivial to determine the location of a sample on the slide, since a large number of samples originate from different plates.
Furthermore, repetitive spotting of samples with various dilutions and varying number of depositions per spot has to be considered, as well as the format of the extraction head.
MIRACLE addresses this issue in a feature called virtual spotting, where previously created plate layouts are combined with information about the operation mode of the spotter, such as format of the extraction head, column or row-wise extraction, top-to-bottom or left-to-right spotting, to determine the final layout.
The selection and order of the plates can be manipulated via dragand-drop and for each plate individual extractions can be excluded.
If a so-called deposition pattern is used, in which Fig.2.
Schematic exemplary work-flow of RPPA construction and analysis via MIRACLE.
(A) Larger sample sets are stored in multiple source plates, with individual and partly complex sample information.
Optionally primary plate readout data can be included.
(B) The individual sample lysates are diluted (indicated by the colour gradient) in a first reformatting step and subsequently spotted onto slides in a customized pattern.
(C) Multiple array copies are generated and stained with different antibodies, adding to the sample tracking demands.
(D) An array scanner yields signal intensities for all spots, which need to be further processed to obtain the final results.MIRACLE offers a user interface for data analysis under automated mapping of samples on the RPPAs to plate-and slide-based data Table 1.
Features of MIRACLE and other open-source solutions for processing RPPA data Feature SuperCurve RPPanalyzer RIMS MIRACLE Platform R, tcl/tk R MATLAB, PHP R, Grails GUIa   Plate layouts   Plate readouts   Slide layouts   Virtual spotting   Visualization   Surface adjustment   Quantification   Normalization   Significance   Correlation   Timecourse analysis   Network analysis   aGraphical user interface.
i633 MIRACLE several adjacent spots originate from the same sample, but are spotted with varying depositions, the layout can be simplified.
Because these samples are otherwise identical, the respective columns of the layout can be merged.
2.1.3 Projects and experiments MIRACLE offers a quick search field to locate sample information quickly using full text search.
However, to keep experimental data organized, projects and experiments can be created, linked to layouts and subsequently be used for filtering.
2.2 Data processing 2.2.1 Slides Slide layouts can be linked to an arbitrary number of slides, which correspond to the copies created during spotting.
For each slide, additional information such as a barcode, the antibody that was used for staining and scanner settings, such as the wavelength of the readout can be specified.
Three types of files can be uploaded, including the output file from the scanner containing signal intensities, an image of the slide and an experimental protocol.
2.2.2 Supported file formats The experimental protocol can be of any file type (e.g.doc, pdf or txt), while for images the most common file types, such as jpg, png and tiff, are supported.
MIRACLE processes each image into a zoomable format for visual detection of quality issues, such as clogged tips, scratches or uneven stainings.
With regards to the array scanner output, MIRACLE is not limited to certain file types, but has a flexible system supporting import of comma, semicolon, tab-separated or Microsoft Excel 2 files without requiring a specifc format.
2.2.3 Processing raw data After successfully reading the scanner file, MIRACLE will offer to add all spots to the database.
During this process, the signal of each spot is linked to the sample information stored in the slide layout.
Subsequently, users can create heatmaps to visualize the data to detect quality problems.
One example are block shifts introduced by the scanner software that can then be corrected for (Supplementary Fig.S3).
2.2.4 Plates and readouts Similar to how slides can be added to slide layouts, plates can be added to plate layouts, where Fig.3.
Sample management of plate layouts illustrated for an exemplary 96-well plate layout.
Several layers of information are accessible i634 M.List et al.Since additional information, such as plate and well type, barcode and replicate number are stored.
If a readout is performed before plates are subjected to spotting, for example, fluorescencebased measurement of cell viability or colourimetric analysis of total protein amount, MIRACLE allows for adding these results for each plate, utilizing the aforementioned file upload mechanism.
2.3 Data analysis When sample management and data processing are complete, users can begin with data analysis.
After selecting a slide layout, the user is presented with a list of all slides linked to this layout.
In case the slide layout was created through virtual spotting, readouts linked to the source plates are also shown.
By starting the analysis, the user will be forwarded to an R-based web application called Rmiracle, which will automatically begin to fetch the selected slides and readouts from the database.
2.3.1 Processing of raw signal Rmiracle offers data analysis in several steps (Fig.4): A heatmap for visual inspection and correction of block shifts (Supplementary Fig.S3).
Positive control spots can be used to correct for uneven staining using the method proposed by Neeley et al.(2012).
If a dilution series has been spotted, a quantification method can be selected for merging these samples.
Rmiracle currently supports SuperCurve, as well as implementations of a logistic model (Tabus et al., 2006), serial dilution curve (Zhang et al., 2009) and a non-parametric model (Hu et al., 2007).
Slides can be normalized for total protein amount by selecting between median loading, variable slope (Neeley et al., 2009) or housekeeping normalization.
For the latter, one or several of the slides have to be marked as loading controls.
Significance of relative sample differences can be assessed by selecting a sample reference for performing Dunnetts test (Hothorn et al., 2008).
2.3.2 Protein concentration estimates and sample grouping With the above settings, Rmiracle computes protein concentration estimates that allow assessment of relative differences between samples.
To this end, we grasp back to the multi-layer sample information model of MIRACLE to group samples.
Users can select horizontal and vertical grouping categories, for example cell-lines tested or treatments applied, which will then be reflected by different facets of a bar plot.
Users can also select an additional category called fill for separating bars by colour to achieve a third grouping dimension to compare, for instance, replicates with different numbers of depositions.
The results are also shown in tabular form, including a download option, and are further accompanied by diagnostic plots specific for the selected quantification method.
Figure 5 depicts the user interface of the Rmiracle analysis.
Beyond data visualization and computation of protein concentration estimates, the analysis comprises additional features introduced below.
2.3.3 Comparison across slides and readouts The comparison tab provides a bar plot (Fig.5F), in which an average is calculated for the previously selected colour fill category, since colours are here reserved for comparing protein concentration estimates across slides.
It is also possible to include plate readouts.
2.3.4 Correlation An important aspect of quality control is signal correlation.
In the correlation tab (Fig.5G), all pairwise Pearson correlation coefficients are calculated for both, protein concentration estimates and raw signal intensities, and presented as a heatmap.
Correlation is also shown between slides and plate readouts.
This can be an important factor, e.g.in case a platebased readout provides information about the total protein amount and should therefore correlate with the RPPA signal used for normalization.
Fig.4.
Processing of raw data in Rmiracle: Signal intensities are displayed in heatmaps (A) for visual inspection.
Subsequently, a surface adjustment based on control spots may be performed to correct for uneven staining (B).
In case of serial sample dilutions, quantification can be applied (C) to obtain a single protein concentration estimate (D).
Furthermore, data can be normalized to negative controls (A_NC1) and total protein amount, e.g.using protein data from separate slides, which enables the identification of effector samples (E, depicted by arrows).
Significance of the sample differences is finally assessed in comparison to a selected negative control by applying Dunnetts test (F) i635 MIRACLE 2.3.5 Significance The significance of relative differences between samples or between samples and a control is of great interest for experimental researchers.
Traditionally, t-tests are used to obtain the necessary P-values, often neglecting multiple comparisons correction and issues arising from low replicate numbers.
To address these issues, MIRACLE applies Dunnetts test (Fig.5H), which is a t-statistic based multiple comparison method comparing each sample with a pre-defined control.
In contrast to other methods, the variance is pooled across all samples, thereby dealing with low replicate numbers (Hothorn et al., 2008).
2.3.6 Import and export Convenient import functions allow experienced R users to download RPPA data directly from MIRACLE by specifying ids or barcodes, respectively.
R methods to process or visualize these data are directly available, allowing data analysts to perform deeper analysis not covered by the proposed work-flow.
Each slide, as well as the resulting protein concentration estimates can be downloaded as tab-or comma-separated file, in which all layout information is included.
3 IMPLEMENTATION 3.1 MIRACLE web application MIRACLE was built using Grails (http://grails.org/), a Groovy/ Java based web application framework that allows for rapid development with a convention over configuration approach.
Grails provides web application critical functionality through industry-proven projects and plug-ins.
This includes, for instance, Hibernate for abstracting data access by modelling database tables through java classes and Apache Lucene (http://lucene.apache.org/core/) for efficient database search.
Using hibernate allows MIRACLE to operate with any JDBC compatible SQL database, such as Microsoft 2 SQL Server or Oracle 2 MySQL.
Data export to R is realized through a web service, in which efficient conversion between database content and JSON objects is facilitated using Jackson (https://github.
com/FasterXML/jackson).
Furthermore, the SpringSecurity project (http://projects.spring.io/spring-security/) limits access with a role-based user model.
In case data should be accessible to users without an account, MIRACLE provides an alternative access (A) (F) (H) (G) (E) (D)(C) (B) Fig.5.
Analysis using Rmiracle: A heatmap visualizes different slide properties, such as signal intensities (A).
Users can change various parameters, for example inclusion of surface adjustment, selection of methods for quantification and normalization for total protein amounts.
Specific samples can be selected and grouped based on different properties of the data set (B).
Depending on the quantification method, diagnostic plots are shown (C).
The results are displayed in an interactive table and in a bar plot (D and E).
A global overview of protein concentration estimates is available for all slides and plate-based readouts (F).
Pearson correlation coefficients are calculated for all slides, as well as for plate-based readouts (G).
Significance is assessed by comparing sample groups to negative controls through Dunnetts test (H) i636 M.List et al.model through universal unique identifiers called security tokens.
These are generated automatically for each slide and plate readout.
In the current version (v. 0.8), all data are accessible to all users.
With the next release, we will change this such that data are private for each user unless selected otherwise.
To efficiently deal with large image files in MIRACLE, we created the Grails OpenSeaDragon plug-in (http://grails.org/plugin/open-seadragon) for generating and displaying pyramide representations of slide images in the Microsoft 2 deep zoom format.
3.2 Rmiracle R package All R functions have been wrapped in the R open-source package Rmiracle.
This includes user and security token based authentication for downloading data from MIRACLE, methods for RPPA data processing, e.g.surface adjustment, various methods for quantification and normalization, as well as methods for visualization and statistical analysis using functionality implemented in the R package multcomp.
In addition, all of these features are accessible through two web applications developed directly on top of R utilizing Shiny (http://www.rstudio.com/shiny/).
Both Shiny applications are included in the Rmiracle package and can be used independent of MIRACLE via uploading files in a MIRACLE compliant format (see Suppl.
File 1 for an example).
4 DISCUSSION RPPAs are a promising technology that finds growing application in both, basic and clinical research.
While many of the challenges of this technology are similar to those of traditional microarrays, RPPA-specific challenges arise and have to be addressed.
In our efforts to adapt this technology as secondary readout to high throughput genome-wide RNAi screens, we identified a lack of a comprehensive tool incorporating all necessary tasks, such as experimental design, sample tracking and data analysis.
To fill this gap, we developed MIRACLE, a web-based tool with deep integration of R for efficient data analysis.
Using a database-driven web application, such as MIRACLE, for sample management offers a number of advantages.
Due to relational tables, all data are kept in a concise and consistent format, where changes and updates are automatically propagated.
In contrast to file-based storage solutions, no experimental information is lost upon turnover of laboratory staff and no problems arise from cryptic and inconsistent sample terminology.
Collaboratively creating experimental data is significantly more convenient in web-based applications, as concurrency issues, such as file locks, can be avoided.
In addition, all information can be located quickly, using full text search, which additionally increases efficiency.
With regards to sample management, both the SuperCurve package and RIMS provide a graphical user interface for specifying slide layouts, but it does not address sample tracking from plate to slide level and does not allow for multiple levels of sample information.
Moreover, they lack the virtual spotting and layout editing features ofMIRACLE that enable researchers to enter all sample-related information already on the plate level and before the complexity of the layout is increased by the array generation.
This saves a significant amount of time and effectively avoids mistakes due to manual data processing.
The R packages SuperCurve and RPPanalyzer provide experienced R users with a wealth of options to analyse RPPA data.
The results of these methods are relative protein concentration estimates.
A logical next step could be to investigate how significant relative differences in protein levels are and how well results correlate, e.g.between slides used for normalization or between individual slides and plate-based readout.
Only RPPanalyzer reports on slide to slide correlation (Mannsperger et al., 2010a).
Significance analysis is not part of any existing solution.
Moreover, replicates are typically merged during data processing to increase confidence of the results, but thereby making them unavailable for subsequent significance analysis.
In contrast, Rmiracle processes replicates individually and offers a comprehensive evaluation of significance and correlation.
A number of analysis methods, such as serial dilution curve (Zhang et al., 2009) have been published as R code, but have not been adapted to a user-friendly format, thereby limiting their application for experimental researchers.
Experienced R users, on the other hand, need the flexibility of the R environment to perform deeper analysis of the data.
Rmiracle strives to serve both target groups by incorporating a broad number of published methods on RPPA data analysis in both, command line and web interface.
Additionally, Rmiracle can be used completely independently of the MIRACLE web application, requiring only a local installation of R. Notably, the web application RIMS followed similar goals as MIRACLE, but did not include scenarios where more sophisticated data processing, e.g.quantification and normalization of the signal intensities, is necessary (Neeley et al., 2009).
Moreover, RIMS is not actively developed or available at the moment, stressing the need for a solution like MIRACLE.
4.1 Outlook While MIRACLE has been designed to handle RPPA data, the sample tracking issues addressed here are in general common for researchers constructing customized microarrays, allowing adaption of MIRACLE to serve other array formats.
RPPA data are particularly suited for unraveling complex protein signaling mechanisms.
Therefore, we plan to integrate MIRACLE with suitable tools for network and pathway analysis.
The minimum information about a micorarray experiment (MIAME) standard (Brazma et al., 2001) and platforms like the gene expression omnibus (GEO) (Edgar et al., 2002) offer an effective method for standardized data exchange of gene expression array data.
In GEO, users can export data to R or utilize a web-based application called GEO2R (https://www.ncbi.
nlm.nih.gov/geo/geo2r/) for basic analysis.
MIRACLE has the potential to deliver similar features to the RPPA community.
We therefore intend to continue development towards an RPPA web portal for published data.
In addition, we envision that MIRACLE could serve as a framework for comparing the performance of various methods for RPPA data processing.
i637 MIRACLE is is In order tsince is https://www.ncbi.nlm.nih.gov/geo/geo2r/ https://www.ncbi.nlm.nih.gov/geo/geo2r/ 5 CONCLUSION In recent years, the application of RPPA technology has matured considerably.
Along with this progress, suitable computational methods have been developed to address issues in data processing.
To further promote acceptance of this technology, fully integrated tools like MIRACLE are indispensable.
Furthermore, it can be expected that standardization of RPPA data in a common framework can substantially aid the development of novel algorithms and allow better integration at the level of network biology and other multi-OMICS data.
It is our hope that MIRACLE will attract contributions from both, users and developers, which will help to strengthen the entire field.
To this end, we have established a github repository (https://github.com/NanoCAN/MIRACLE) and established a demo application (http://www.nanocan.org/miracle/demo) containing biological sample data.
Further documentation and a stepby-step user guide are available online (Supplementary File 2).
ACKNOWLEDGEMENT We would like to thank Prof. Torben A. Kruse for valuable advice.
Funding: This work was supported by the Lundbeckfonden grant for the NanoCAN Center of Excellence in Nanomedicine, the Region Syddanmarks ph.d.-pulje and Forskningspulje, the Fonden Til Lgevidenskabens Fremme and co-financed by the INTERREG 4 A-program Syddanmark-Schleswig-K.E.R.N.
with funds from The European Regional Development Fund.
Conflict of Interest: none declared.
ABSTRACT Motivation: Shotgun sequence read data derived from xenograft material contains a mixture of reads arising from the host and reads arising from the graft.
Classifying the read mixture to separate the two allows for more precise analysis to be performed.
Results: We present a technique, with an associated tool Xenome, which performs fast, accurate and specific classification of xenograft-derived sequence read data.
We have evaluated it on RNA-Seq data from human, mouse and human-in-mouse xenograft datasets.
Availability: Xenome is available for non-commercial use fromContact: tom.conway@nicta.com.au 1 INTRODUCTION Xenograft models are an important tool for many areas of biomedical research, including oncology, immunology and HIV pathology.
A typical scenario, drawn from oncology research, is that of a human prostate cancer grown in an immunocompromised mouse model.
Doing so allows researchers to investigate aspects of the cancer that are not necessarily preserved in cell lines, and it allows investigations into the interactions between the cancer and the surrounding stromal tissue.
The mouse may be biopsied or harvested and samples of cancer and/or stroma collected at various time points during an experiment.
Difficulties arise, when sequencing the genome or transcriptome of the samples because host (mouse) material (i.e.DNA/RNA) will inevitably comingle with the graft (human) material.
If a sufficiently careful section is taken, it has been generally assumed that the level of host contamination is low enough that it may be ignored.
This may be a dangerous assumption, however, since the level of gene expression is non-uniform.
If the overall level of host contamination in a graft sample is measured to be 10% overall, it may still be the case for a given gene that the host homologue accounts for most or all of the expression.
Contamination may be minimized by physical or biochemical techniques such as conservative sectioning, cell sorting or laser capture micro-dissection, but these techniques can be a significant source of technical bias, or in some cases may require infeasibly large amounts of starting material.
Further, in the case of transcriptomic investigation, classifying host and graft in vitro may fail to adequately capture the interactions between them.
An alternative strategy is to sequence an acknowledged mixture of host and graft, then use in silico methods to classify the individual sequence reads.
This is the approach discussed here.
We demonstrate a simple technique, based on an analysis of sequence To whom correspondence should be addressed.
reads using Tophat, and a more precise technique based on a k-mer decomposition of the host and graft reference sequences, Xenome.
In both cases, the primary goal of the analysis is to classify reads into four classes: reads attributable to the host, reads attributable to the graft, reads which could be attributed to both and reads which are attributable to neither.
To the best of our knowledge, there are no results in the literature examining the classification of high-throughput sequencing short reads from xenograft models.
The studies we know of are concerned with microarray expression profiles or alternative methods for estimating the amount of host material or cell types in the samples.
For example, Lin et al.(2010) investigate the use of species-specific variation in gene length and a multiplex PCR to ascertain the relative amount of mouse and human DNA.
Wang et al.(2010) use microarray gene profiling data and in silico techniques to estimate the quantity of various tissue components.
In Samuels et al.(2010), there is an analysis of a mouse xenograft model using microarray data.
They conclude that if there is more than 90% human DNA then the expression profiles are not unduly skewed.
They also describe an experimental method for removing homologous genes based on cross-hybridization analysis of the probes.
Ding et al.(2010) use short read sequencing to study a cancer genome and identify mutations/deletions.
They estimate tumour cellularity using pathological assessment, and state that their xenograft is 90% tumour cells.
They also map NOD/SCID (mouse) genomic data to human and mouse genomes, reporting 3.17% and 95.85% mapping rates, respectively, and so apply no correction for the murine cells.
We note that in the context of non-uniform RNASeq data ignoring the contribution of the murine expression can lead to biases.
Tools such as Tophat serve a different purpose than that of Xenome.
The former aligns reads to a reference, and we can use those alignments for a variety of purposes, including the classification task we present here.
In contrast, Xenome only performs the classification task itself.
This is an important distinction, since an alignment must assign the read to zero or more positions in the genome; the classification merely has to decide if the read was more likely to arise from the genome than not.
For the remainder of the article, we will assume, unless otherwise stated, that sequence reads arise from RNA-Seq.
However, the techniques we present are applicable to genomic DNA sequences (including ChIP-Seq and MeDIP-Seq) and also to other mixtures of DNA species.
2 METHODS Under the assumption that a graft sample has only a low level of host material contamination, the simplest analysis is to use a regular mappingbased RNA-Seq analysis tool, such as Tophat and assume that either the observed expression is dominated by the graft, which has the greatest number The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i173 i172i178 Xenome Fig.1.
A Venn diagram showing the different classes that a given k-mer may belong to.
The marginal host (and marginal graft) partitions are for those host (and graft) k-mers that are Hamming distance 1 from a k-mer in the graft (and host) reference of input cells, or that the homology between the host species and graft species is such that reads arising from host material will tend to map poorly, and the resultant inferred level of gene expression will be negligible.
In some cases, these assumptions may be true, but in the case of human cancer xenografts in mice, for example, the second assumption is false for many transcripts, and a more precise technique is desirable.
Therefore, we have developed two techniquesone based on the existing RNA-Seq resequencing tool Tophat (Trapnell et al., 2009), and one based on k-mer decompositions of the host and graft references.
For genomic DNA, another resequencing/alignment tool could just as well be used.
2.1 A Tophat-based method A more precise analysis may be performed by using Tophat (Trapnell et al., 2009) to analyse the reads.
First, Tophat is used to process the read set with the graft genome as reference.
Secondly, Tophat is used to process the read set with the host genome as reference.
Lastly, the accepted alignments from the Tophat mappings are post-processed to partition the reads into four classes: host, graft, both or neither.
Tophat provides mapping quality scores in its output, but they only reflect whether or not the read mapped to multiple locations.
If the quality scores reflected a measure of certainty that the read maps to the given location, a more sophisticated approach would be to extend the classification to assign reads that map with high certainty to one genome and low certainty to the other to the appropriate specific class rather than both.
We do not pursue this further here.
An implementation of this method may be achieved easily with Tophat, SAMtools (Li et al., 2009) and some simple scripts.
As will be apparent in the results presented in Section 3, although very few reads are misclassified (i.e.classified as host instead of graft, or vice versa), a significant proportion of the reads, even in a pure graft or pure host sample, fall into the both class.
If these ambiguous reads were uniformly distributed in their origin across the genome, this would have only a small impact, but as we will elucidate in Section 4, the ambiguous reads are non-uniformly distributed.
As a result, a significant number of genes cannot have their expression unambiguously pinned to the host or the graft, though at least compared with a single analysis, the set-based analysis makes clear which reads may be clearly associated with the host or the graft, and does not assume that all gene expression in the sample is explained by the graft.
2.2 A k-mer-based method Our method proceeds in two phases: constructing a reference data structure, then classifying reads with respect to that reference data structure.
The reference structure is built from the sets of k-mers in a pair of reference sequences, which we will refer to as the host and the graft.
2.2.1 Definitions Since in most sequencing protocols the reads are a mixture of forward and reverse complements with respect to the reference sequence, we cannot assume the orientation of k-mers drawn from reads will match the orientation of k-mers drawn from the reference.
We could always consider both orientations, but that would entail a lot of double handling of information, so instead we normalize or canonicalize k-mers.
Definition 1.
(k-mer canonicalization).
Consider a k-mer x.
We denote its reverse complement by x.
A canonical k-mer x is defined by a choice function C giving a deterministic choice between x and x: x : x=C(x)=C(x) In principle, we can choose any such function: min or max being obvious candidates, and the results of our method are identical for all choices.
In Section 4, we will present our specific choice which has important performance ramifications.
This definition can be extended to a set of k-mers S in the obvious way: S ={x :xS} Definition 2.
(Marginal inclusion).
Consider a set of canonical k-mers S. We say that a k-mer x is has marginal membership of S if x does not exist in S, but has a Hamming distance 1 neighbour y such that y is a member of S. To aid our computation of marginal inclusion, we define the function M: M(x,S)={y :yHam1(x)} S where Ham1(x) is the set of Hamming distance 1 neighbours of x.
Note that{ y :yHam1(x) }={y :yHam1(x)} 2.2.2 Reference construction For both host and graft reference sequences, we construct the set of canonical k-mers (H and G respectively).
From these we compute a complete set of canonical reference k-mers S = H G. Note that x S,x= x.
The sets of canonical k-mers tend to be large: with k =25, there are 2.4 billion in the human genome, 2.1 billion in the mouse genome, and 4.5 billion in the uniononly 12 million are shared.
A naive representation (2 bits per base, packed), would use 50 bits per 25-mer, or about 26 GB.
As discussed in our previous work (Conway and Bromage, 2011), information theory gives a lower bound for the memory usage.
For a domain of 4k possible k-mers, the minimum number of bits required to represent a set of n k-mers is log2 ( 4k n ) In the case of the union S above, this is about 10 GB, or less than half what is required for the naive representation.
As also discussed in our previous work, succinct data structures have been developed to give concrete representations that approach this theoretical lower bound.
The one we use, due to (Okanohara and Sadakane, 2006), works very well, requiring about 13 GB.
For each reference k-mer, we determine whether it occurs in the host, the graft, both, or if it occurs in one and has a marginal occurrence in the other.
These classes are denoted h, g, b or m, respectively (Fig.1).
More formally, we compute the function K for each x S: K(x)=  b if x G x H g if x G x/ H (H M(x,S)=) h if x/ G x H (GM(x,S)=) m if ( x G x/ H (H M(x,S) 	=) ) ( x/ G x H (GM(x,S) 	=) ) K may be extended to project not just k-mers to classes, but also a set of k-mers Q to a set of classes in the obvious way: K(Q)={K(x) : x Q} These classes are pre-computed and stored as a sequence of 2-bit values corresponding to the k-mers in the succinctly store reference set.
i173 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i174 i172i178 T.Conway et al.The class m denotes that the k-mer exists in H and has marginal membership of G or vice versa.
That is, they are marginally distinctive, in the sense that a single polymorphism or sequencing error may cause a k-mer in that class to change from being marginally host to being marginally graft.
Since the marginal set is symmetric (every marginal host k-mer has a corresponding marginal graft k-mer and vice versa), and represents k-mers that are not very discriminating, we combine both marginal sets into a single marginal set with the class m. 2.2.3 Classification Classification proceeds by taking each read r and constructing the set of canonical k-mers that may be derived from the read Qr.
We then map the set of k-mers to a set of classes by the function K described above.
k-mers that do not occur in the reference, S, are ignored.
The k-mer classes that occur for a given read determine the classification of the read as a whole according to the following function: C(Qr)=  graft if gK(Qr)h/K(Qr) host if g/K(Qr)hK(Qr) ambiguous if gK(Qr)hK(Qr) both if K(Qr){g,h}=K(Qr) 	= neither if K(Qr)= The read classes graft and host denote cases where there is at least one k-mer which unambiguously comes from that k-mer class, and there are no contradictory k-mers (i.e.unambiguously from the opposing reference).
The ambiguous class corresponds to the case where there are k-mers which appear to be contradictoryunambiguously host, and unambiguously graft.
The both class represents cases where there are only k-mers which are either unambiguously common to both the host and graft or k-mers which may belong to either if they contain a single polymorphism or sequencing error.
The last class, neither, represents those cases where there were no matching k-mers.
2.2.4 Implementation concerns In Section 2.2, we introduced an abstract canonicalization function C. The most commonly used concrete function is min (or max), which selects the lexicographically/numerically smaller of the k-mers x and x: Cmin(x)=min(x,x) We use the following definition, assuming some reasonable hash function f : Chash(x)= x if f(x)< f(x) min(x,x) if f(x)= f(x) x if f(x)> f(x) Assuming f is a reasonable hash function, this definition effectively makes a random, but deterministic choice between x and x.
We use this definition, rather than the more common lexicographic one, because lexicographic canonicalization leads to the set of canonical k-mers being non-uniformly distributed across the set of all possible k-mers.
There are two ways in which a more uniform distribution of k-mers improves performance of Xenome.
The first is that the succinct bitmap representation that we use (due to Okanohara and Sadakane, 2006) performs better on a uniform distribution of bits (that is, a uniform distribution of k-mers).
The second is that it improves the performance of our intermediate hash table, from which the succinct bitmap data structure is built.
The way the hash table is used is that as the input sequences are read, they are decomposed into k-mers which are stored in the hash table, which is of a fixed (controlled by a command line parameter) size.
When the hash table fills (that is, unresolvable collisions arise), it is sorted and written out to disk.
When all source k-mers have been read, the sorted runs are merged and the main succinct bitmap is constructed.
The specific representation used is a succinct cuckoo hash table, broadly similar to (Arbitman et al., 2010).
The succinct representation relies on the fact that the location of the slot where a key x (in this case, a k-mer) is stored contains some of the information present in the key.
Consider an idealized hash table with load factor 1 (i.e.the number of values stored in the table equals the number of slots in the hash table), with no collisions.
If the width of the hash table is 2J , then J bits of the key are implied by the slot chosen.
For keys of N bits, therefore, the entries of the hash table need store only N J bits.
The simplest way this may be realized would be to just use J bits of the key as the slot number, and store the remainder in that slot, but of course, this is likely to have an unacceptable number of collisions.
Instead, we use an invertible hash function [based on a single-stage Feistel network, see Luby and Rackoff (1988)] to turn a key x into a slot number s and a stored component v in such a way that given s and v we can recover x (assuming a hash function f ): Ff (x)= ( x mod 2J )f ( x 2J ), x 2J F1f (s,v)=sf (v)+v2J The size of the key data stored in the hash table is, then, 2J (N J ) bits.
If 2J 2N , this is within (1+o(1))log2(2N2J ) bits, and hence succinct.
[Of course, idealized hash tables are not possible if the set of keys is not known in advance.
Hence we use cuckoo hashing (Pagh and Rodler, 2004), in which collisions are resolved (as far as possible) using multiple hash functions.]
To make the sorting of keys more efficient, our hash function preserves the N J most significant bits of the keys, which are then stored in the hash table (along with a few bits to determine which hash function was used).
This means that we can perform an initial bucketing without inverting the hash functions; the buckets can then be processed independently, using multiple threads if required.
Now consider a set of random k-mers.
For hash-based canonicalization, given a good hash function, there is an equal probability of observing any base in the most significant position in x.
For lexicographic canonicalization, there is a probability of 410 that it is a, 3 10 that it is c, 2 10 that it is g, and only 1 10 that it is t. For a set of N random k-mers, the expected entropy at the most significant base is 2.0 bits and 1.85 bits, respectively.
This matters, because hash functions are frequently vulnerable to poor behaviour in the presence of highly correlated sets of keys.
In real, biologically derived sets of k-mers, the set of k-mers is non-random, so there is likely to be less entropy to begin with, and therefore loss of entropy due to canonicalization is exacerbated.
This is especially problematic in the case of the above family of invertible hash functions, since it is precisely the most-significant bits of the key which are passed to the underlying hash function f. The upshot of this is that Cmin leads to highly correlated k-mers, which in turn lead to a high probability of unresolvable collisions even when the hash table is nearly empty, resulting in a large number of short runs.
Since the set of canonical k-mers that result from Chash are less correlated, the hash table is unlikely to encounter unresolvable collisions until it is almost full (8085% in practice).
2.3 Xenomeusage and workflow To use the xenome tool, first it must be invoked to construct the reference data structures.
A typical invocation will give the FASTA filenames for the host and graft genomes, a filename prefix for the index files, and optionally, the amount of working RAM to use (in Giga Bytes), and the number of threads to use: $ xenome index-M 24-T 8-P idx \-H mouse.fa-G human.fa This will run for some timeon the human/mouse references, around 46 h on an 8-core server.
This need only be done once for a given pair of references, and a given k-mer size.
The k-mer size defaults to 25, which seems to work well.
With the reference data structures built, read data may be segregated.
To make the output files easier to identify, command line flags can be used to i174 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i175 i172i178 Xenome name the host and graft files.
If paired data is being classified, the pairs flag should be givenpairs are classified by computing K(Q) over all the k-mers in the pair.
$ xenome classify-T 8-P idx pairs \ graft-name human host-name mouse \ output-filename-prefix XYZ-i XYZ_1.fastq-i XYZ_2.fastq This yields the following set of files: XYZ_ambiguous_1.fastq XYZ_ambiguous_2.fastq XYZ_both_1.fastq XYZ_both_2.fastq XYZ_human_1.fastq XYZ_human_2.fastq XYZ_mouse_1.fastq XYZ_mouse_2.fastq XYZ_neither_1.fastq XYZ_neither_2.fastq If the total size of the index files is larger than RAM,xenomewill perform poorly, but the flag-M can be supplied with the maximum desired working set size, and the classification will be done in multiple passes each using less memory.
On a single server with 8 AMD Opteron cores running at 2 GHz and with 32 GB of RAM Xenome processes 15 000 read pairs per second.
Having classified the reads, typical usage would be to then run Tophat and Cufflinks to perform intron-aware gapped alignments and compute gene expression.
Instead of running Tophat on all of the reads, having separated the reads according to their origin, we can run it with the human genome just against the human fraction of the reads, and against the mouse genome on the mouse fraction of the reads.
It may well be desirable to combine the both and ambiguous fractions with the human fraction to run against the human genome, and also with the mouse fraction to run against the mouse genome.
If this is done, attention should be paid to homologous genes, since it is possible that the human and mouse homologues may be represented by the same reads.
In some cases, the correct attribution of the gene expression may be apparent from the nature of the experiment.
For example, a sample from a human prostate cancer grown in mouse may show ambiguous expression in MYH genes which would be reasonably attributed to the stromal mouse tissue.
3 RESULTS Our analysis examines two questions: whether or not it is technically feasible to separate host and graft reads in silico; and whether or not the fast technique we have proposed (Xenome) yields a worthwhile improvement over the mapping (Tophat) based technique.
In the first experiment, we take a sample of human cDNA sequence data (SRR342886), and a sample of mouse cDNA sequence data (SRR037689) and analyse them both with Tophat and Xenome, and compare the results.
This allows us to evaluate the degree to which sequences are misclassified (assigned to human rather than mouse or vice versa), and the specificity of the classificationthe proportion of sequences which are not classified as both.
The use of pure human or mouse cDNA gives an experiment where the correct assignment of reads is known.
The second experiment runs the same analysis on sequence data from a human prostate cancer xenograft growing in a mouse host [BM18, see McCulloch et al.(2005)].
In this case, however, we not only classify the reads, but use the Tophat mappings to compute approximate levels of gene expression [measured in fragments per thousand bases of transcript per million mapped reads, or FPKM Trapnell et al.(2010)] and use human species-specific quantitative RT-PCR (qRT-PCR) on selected genes to validate the results.
In this instance, we have no gold standard by which we can judge the Fig.2.
Summary of the results with Human cDNA.
Each of the classes of reads is divided into those reads assigned to the class only by Xenome (Xenome), only by the Tophat analysis (Tophat) or by both Xenome and the Tophat analysis (Concordant) results, but the qRT-PCR will give some degree of validation, and known aspects of the biology of the cancer can give some qualitative corroboration.
Tophat uses a global analysis, combining the results of all the read mappings to locate exons, junctions and so on.
In contrast, Xenome performs pre-computation on the two reference genomes, then classifies each read independently.
Therefore, for each of the three sets of reads, we ran Tophat with the human reference genome and again with the mouse reference genome, then, as described in Section 2.1, the mappings were post-processed to determine which reads belonged to each of the four classes.
Each of the four partitions of the sets of reads was then partitioned with Xenome to allow us to easily determine which reads were classified as the same by both procedures, and which were classified differently.
Figures 2, 3 and 4 summarize the results.
For the three samples, the proportion of reads receiving the same classification were 82, 87 and 84%, respectively.
As can be seen from the human and mouse only figures, both techniques are accurate, in as much as they misclassify only a small proportion of the reads (the worst case being the Tophat-based analysis of the human cDNA which misclassified 1.2% of the readsall the other analyses misclassified 0.20.3%).
The main difference between the Tophat-based and Xenome analyses is that the latter yields better specificitythe fraction of reads classed as both is significantly smaller in the Xenome analysis.
To check for false positives for the human cDNA dataset, we also used BLAT (Kent, 2002) to map the 1.1 million reads which Xenome classed as human but which were not mapped to either genome by Tophat.
Most of them were successfully mapped with high quality to the human reference by BLAT (about 90%).
BLAT also mapped about 18% of them, with very variable quality, to the mouse reference.
This supports our confidence in the accuracy of the Xenome algorithm.
From this we can conclude that the in silico classification of sequences is feasible and accurate.
i175 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i176 i172i178 T.Conway et al.Fig.3.
Summary of the results with Murine cDNA Fig.4.
Summary of the results with BM18 xenograft cDNA The second experiment, using RNA-Seq data from a prostate cancer xenograft into a mouse demonstrates that the classification works on a real mixture.
As described above, we performed the same process to partition the reads into classes, then we used the refGene genome coordinates as calculated by Tophat to assign reads to genes, from which we computed an expression level using the fragments per kilobase of transcript per million mapped reads formula (Trapnell et al., 2010): FPKM= f 10 9 zN where f is the number of fragments (reads, for single ended data or pairs for paired data), z is the combined length of the exons Fig.5.
Validation of the in silico classification of xenograft RNA-Seq data with qRT-PCR.
The horizontal axis shows log10 FPKM for the Xenomederived gene expression for the 18 test genes.
The vertical axis shows the Ct value for each gene relative to the Ct of actin.
There were two RNA-Seq samples processed (biological replicates), and four replicates of the qRTPCR.
For each gene, an ellipse is shown centered on the mean log10 FPKM in the x-axis, and on the mean relative Ct in the y-axis.
The horizontal and vertical radii show the variance in the samples of the gene and N is the total number of mapped fragments.
Although this quantification is peripheral to the technique we are presenting, we have computed expression levels for the purposes of comparing with some qRT-PCR data for the same biological data.
The qRT-PCR data were available for 18 genes:ABCG2,ALDH1A1, CD177, DLL1, DLL3, GLI1, GLI2, HES1, JAG1, JAG2, LGR5, NANOG, NOTCH1, NOTCH2, NOTCH3, PTCH1, PTCH2 and SMO.
Figure 5 shows the log10FPKM versus the difference of the Ct for each target gene and the Ct for actin (which was used as a housekeeping gene).
With the exception of NANOG, the two methods correlate reasonably well (the Pearsons correlation coefficient is 0.80).
We have investigated the NANOG data, and cannot explain the low FPKM.
Whether this is a sequencing issue or a biological variation in the mice is unknown, but the low level of expression does not appear to be related to the behaviour of Xenome or Tophat.
4 DISCUSSION We have presented a simple read classification method based on Tophat, and our refined classification approach, Xenome.
Xenome can be used to efficiently and effectively partition the read set for subsequent processing by tools such as Tophat.
What is not apparent from the results above is the relative behaviour at the level of a single gene.
It should be expected that the distribution of ambiguously mapped reads (classed as both) should be non-uniform, since some genes in the two genomes are more highly conserved than others.
i176 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i177 i172i178 Xenome Fig.6.
An in silico analysis showing the degree of ambiguity in HG19 refGene, according to the k-mer based analysis used by Xenome.
In this analysis, k =25 The first result we present in Figure 6 on this point is an in silico analysis showing the proportion of each human gene (ignoring introns) covered by k-mers that are not classed as human.
It is clear that the vast majority of genes contain few or no k-mers that are not classed as human.
The fraction of k-mers which are ambiguous gives a worst-case view of how Xenome might be expected to perform.
In order for a read to be classified as both, all of its k-mers must be of the both class, or there must be at least one k-mer from each of the two genomes (which happens less than 2% of the time in the samples we have tried).
Conversely, a single host or graft k-mer is sufficient to classify the read into the respective class.
Therefore for a read to be classified as both, the reference must contain a sufficiently long run of consecutive k-mers of class both and/or SNPs and sequencing errors must eliminate all the distinctively host or graft k-mers.
The second result we report on this point, presented in Figure 7 is the relative proportion of reads which are classified as both on a per-gene basis.
What is evident in this figure is that although there are many genes for which the proportion of both reads is tightly correlated between Tophat and Xenome, there are a large number of genes for which the Tophat-based analysis has significantly more both reads.
There are 15 591 genes for which there were at least 20 mapped reads in the BM18 xenograft sample.
Of these, there were 65 for which Xenome assigned both or ambiguous to at least half the reads mapping to the gene; there were 498 for which the Tophatbased analysis assigned both to at least half the reads mapped to the gene.
For the most highly conserved genes, there is not much that can be done with this data directlyfurther signal processing or other data would be required to determine the relative expression in the host and graft.
While we have developed Xenome with RNA-Seq on human/ mouse xenografts in mind, we anticipate it will be an effective tool for other similar mixtures.
For example, capturing the differential methylation around genes between host and graft using MeDIP-Seq may shed light on the interraction between the two.
Fig.7.
A plot showing the distribution of human genes with respect to the proportion of xenograft reads which are classed as both by the Tophat-based analysis and the Xenome analysis.
The reads considered are only those mapped by Tophat since Xenome does not yield mappings, so cannot be used to assign reads to genes.
Only genes for which at least 20 reads mapped were considered.
The horizontal axis corresponds to the number of reads classified as both or ambiguous by Xenome as a proportion of all the reads that might possibly be human (i.e.both, ambiguous or human).
The vertical axis corresponds to the number of reads classified as both by the Tophatbased analysis, once again, as a proportion of all the reads that might possibly be human The need for our technique is substantially motivated by the fact that for a xenograft to be viable there must be very strong homology between the host and graft organisms.
This leads to a situation where there is a high probability that a read may map to either genome, and it is this problem that Xenome specifically addresses.
A side benefit is that the classification is done independently for each read, and results in groups of reads in each class; each group may then be processed independently with further tools [such as Tophat, Cufflinks (Roberts et al., 2011) or others].
Given that many kinds of analysis require global processing of the input data, being able to process a coherent subset of the data can lead to a time/space gain.
This benefit extends beyond the sphere of xenografts.
For example, there are situations where a parasite or pathogen cannot be cultured independently (for example Chlamydia, and some fungi), so samples will generally contain a mixture of host and pathogen.
In some examples, although the pathogen can be cultured independently, there are phenotypic differences between organisms growing in culture and those growing in a host.
In both cases, there are benefits to being able to classify the two groups of reads, even though straight mapping based approaches will be less sensitive to cross-talk than xenograft data.
Precise alignment and alignment-free methods represent different points along a spectrum of possible classification techniques.
Fundamentally, both rely on establishing homology between a read and the host and/or graft references.
By substituting different algorithms for establishing homology (e.g.various alignment algorithms, k-mer spectrum methods, etc.
), different sensitivity and specificity might be achieved.
Although our current technique is built on simple set-based classification, there is clearly scope to develop statistical models i177 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i178 i172i178 T.Conway et al.which allow for a more subtle classification procedure.
These could, for example, be based on the frequency of k-mers of different classes, giving rise to the computation of the likelihood that a read originated from either the host or graft genomes.
Indeed, if such likelihoods were used from read alignments (Tophat unfortunately does not produce such scores), a unified model allowing either our current k-mer based model or alignments could be used.
A further extension could take into consideration the established homology between genes in the host and graft organisms.
Where reads are ambiguous, non-ambiguous reads associated with the same gene homologues could be used to help disambiguate the classification.
EM-based methods such as those described in Newkirk et al.(2011); Chung et al.(2011); Hormozdiari et al.(2010) would be a good basis for such an extension.
We note however, that this would require significant conceptual changes to Xenome since it requires relatively more precise alignments.
It is instructive to consider a specific example of a gene where the Tophat based and Xenome analyses are very different.
For the gene MYH3, in the human cDNA dataset, there are 29 reads classed as human by the Tophat-based analysis and 2 713 classed as human by Xenome.
All the reads that are thus assigned by Xenome but not by the Tophat-based analysis were classed as both by the latter.
This is because there is a high level of conservation in this gene between the two species, and the reads therefore aligned to both.
Were Tophat to yield meaningful alignment quality scores, a statistical approach of the kind hinted at above may perform similarly to the k-mer based approach.
ACKNOWLEDGEMENT National ICT Australia (NICTA) is funded by the Australian Governments Department of Communications; Information Technology and the Arts; Australian Research Council through Backing Australias Ability; ICT Centre of Excellence programs.
Funding: Prostate Cancer Foundation of Australia (EDW) and the Victorian Governments Operational Infrastructure Support Program.
EDW is supported by an Australian NHMRC Career Development Award [#519539] in part.
Conflict of Interest: none declared.
ABSTRACT Summary: Here, we present riboPicker, a robust framework for the rapid, automated identification and removal of ribosomal RNA sequences from metatranscriptomic datasets.
The results can be exported for subsequent analysis, and the databases used for the web-based version are updated on a regular basis.
riboPicker categorizes rRNA-like sequences and provides graphical visualizations and tabular outputs of ribosomal coverage, alignment results and taxonomic classifications.
Availability and implementation: This open-source application was implemented in Perl and can be used as stand-alone version or accessed online through a user-friendly web interface.
The source code, user help and additional information is available atContact: rschmied@sciences.sdsu.edu; redwards@cs.sdsu.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 5, 2011; revised on November 28, 2011; accepted on November 29, 2011 1 INTRODUCTION Metatranscriptomic approaches are drastically improving our understanding of metabolism and gene expression in microbial communities.
By investigating all functional mRNA transcripts isolated from an environmental sample, metatranscriptomic analyses provide insights into the metabolic pathways important for a community at the time of sampling.
Although metatranscriptomes are used to investigate metabolic activities, the majority of RNA recovered in metatranscriptomic studies is ribosomal RNA (rRNA), often exceeding 90% of the total reads (Stewart et al., 2010).
Even after various treatments prior to sequencing, the observed rRNA content decreases only slightly (He et al., 2010) and metatranscriptomes still contain significant amounts of rRNA.
Although rRNA-like sequences are occasionally removed from metatranscriptomes, the removal is performed only with a subset of the publicly available rRNA sequences.
Failure to remove all rRNA sequences can lead to misclassifications and erroneous conclusions during the downstream analysis.
It is estimated that misannotations of rRNA as proteins may cause up to 90% false positive matches of rRNA-like sequences in metatranscriptomic studies (Tripp et al., 2011).
The potential for false positives arrises To whom correspondence should be addressed.
from a failure to completely remove all rRNA prior to translating the putative rRNA and querying a protein database.
The rRNA operons in Bacteria and Archaea are not known to contain expressed protein coding regions that at the same time code for rRNA and therefore, annotations of proteins in rRNA coding regions should be presumed to be misannotations (Aziz et al., 2008).
Metagenomic sequence data generated to asses the metabolic potential of a community will also be affected by false positive matches of rRNA sequences when querying a protein database.
Therefore, transcript analysis should only proceed after it has been verified that all rRNA-like sequences have been found and removed from the dataset to allow accurate identification of the transcribed functional content.
The high-throughput nature of community sequencing efforts necessitates better tools for the automated preprocessing of sequence datasets.
Here, we describe an application able to provide graphical guidance and to perform identification, classification and removal of rRNA-like sequences on metatranscriptomic data.
The application incorporates a modified version of the BWASW program (http://bioinformatics.oxfordjournals.org/citmgr?gca =bioinfo;26/5/589), and is publicly available through a user-friendly web interface and as stand-alone version.
The web interface allows online analysis using rRNA sequences from public databases and provides data export for subsequent analysis.
2 METHODS 2.1 Implementation and computational platform The riboPicker application was implemented as stand-alone and web-based version in Perl.
The web application is currently running on a web server with Ubuntu Linux using an Apache HTTP server to support the web services.
The alignments are computed on a connected computing cluster with 10 working nodes (each with 8 CPUs and 16 GB RAM) running the Oracle Grid Engine version 6.2.
All graphics are generated using the Cairo graphics library (http://cairographics.org/).
2.2 Identification of rRNA-like sequences The identification of rRNA-like sequences is based on sequence alignments using a modified version of the BWA-SW program.
The modifications do not change the default behavior of the algorithm and include parameter forced changes in the alignment of ambiguous bases and the generation of an alternative output.
The documentation provides a detailed list of changes and is available on the program website.
riboPicker uses query sequence coverage, alignment identity and minimum alignment length thresholds to determine if an input sequence The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:50 31/12/2011 Bioinformatics-btr669.tex] Page: 434 433435 R.Schmieder et al.is an rRNA-like sequence or not.
This approach is based on the idea that looking for similar regions consists of grouping sequences that share some minimum sequence similarity over a specified minimum length.
Threshold percentage values are rounded toward the lower integer and should not be set to 100% if errors are expected in the input sequences.
The results for multiple databases are automatically joined before generating any outputs.
Using simulated datasets, we evaluated the classification of rRNA-like sequences and showed that riboPicker performed with high accuracy comparable to the latest version of meta_rna (Huang et al., 2009) and BLASTn (Supplementary Material).
A comparison on real metatranscriptomic data showed that riboPicker processes data more than twice as fast as Hidden Markov Model (HMM)-based programs and >100 times faster than BLASTn (Supplementary Material).
2.3 Reference databases The web-based version offers preprocessed databases for 5S/5.8S,16S/18S and 23S/28S rRNAsequences from a variety of resources, currently including SILVA (Pruesse et al., 2007), RDP (Cole et al., 2009), Greengenes (DeSantis et al., 2006), Rfam (Gardner et al., 2011), NCBI (Sayers et al., 2011) and HMP DACC (The NIH HMP Working Group et al., 2009).
To reduce the number of possibly misannotated entries, sequences were filtered by length to remove very short and long sequences and by genomic location to remove overlapping rRNA misannotations.
The remaining sequences were then converted into DNA sequences (if required) and filtered for read duplicates to reduce redundancy in the sequence data.
Detailed information for each reference database is provided on the website.
Taxonomic information was either retrieved with the sequence data from the resources or was added based on the NCBI Taxonomy.
The databases are automatically updated on a regular basis and can be requested from the authors for offline analysis.
A non-redundant database is made available for the stand-alone version on the program website.
3 WEB-INTERFACE 3.1 Inputs The web interface allows the submission of compressed FASTA or FASTQ files to reduce the time of data upload.
Uploaded data can be shared or accessed at a later point using unique data identifiers.
It should be noted at this point that the input datasets should only contain quality-controlled, preprocessed sequences to ensure accurate results (Schmieder and Edwards, 2011).
In addition to the sequence data, the rRNA reference databases have to be selected from the list of available databases.
Unlike the stand-alone version, the web-based program allows the user to define threshold parameters based on the results after the data are processed.
This does not require an a priori knowledge of the best parameters for a given dataset and the parameter choice can be guided by the graphical visualizations.
3.2 Outputs Users can download the results in FASTA or FASTQ (if provided as input) format or its compressed version.
Results will be stored for the time selected by the user (either 1 day or 1 week), if not otherwise requested, on the web server using a unique identifier displayed during data processing and on the result page.
This identifier additionally allows users to share the result with other researchers.
The current implementation offers several graphical and tabular outputs in addition to the processed sequence data.
The Coverage versus Identity plot shows the number of matching reads for different coverage and identity threshold values.
The coverage plots show where the metatranscriptomic sequences aligned to the rRNA reference sequences and provide an easy way to check for possible bias in the alignment or the rRNA-removal prior to sequencing.
The coverage data for each database sequence is available for download.
The taxonomic classifications of rRNA-like sequences are presented as bar charts for each selected database.
The summary report includes information about the input data, selected databases and thresholds, and rRNA-like sequence classifications by database, domain and phyla.
4 BRIEF SURVEY OF ALTERNATIVE PROGRAMS There are different applications that can identify rRNA-like sequences in metatranscriptomic datasets.
The command line program meta_rna (Huang et al., 2009) is written in Python and identifies rRNA sequences based on HMMs using the HMMER package (Eddy, 2009).
Another program based on HMMER is rRNASelector (Lee et al., 2011), which is written in Java and can only be used through its graphical interface.
The web-based MG-RAST (Meyer et al., 2008) uses the BLASTn program, identifying rRNA-like sequences based on sequence similarity.
The HMM-based programs currently allow identification of bacterial and archaeal rRNAs.
The sequence similarity-based programs make it easy to assign sequences to taxonomic groups.
5 CONCLUSION riboPicker allows scientists to efficiently remove rRNA-like sequences from their metatranscriptomic datasets prior to downstream analysis.
The web interface is simple and user-friendly, and the stand-alone version allows offline analysis and integration into existing data processing pipelines.
The tool provides a computational resource able to handle the amount of data that next-generation sequencers are capable of generating and can place the process more within reach of the average research lab.
ACKNOWLEDGEMENT We thank Matthew Haynes and Ramy Aziz for comments and suggestions.
We thank the HMP DACC for making reference genomes data from the NIH Human Microbiome Project publicly available.
Funding: National Science Foundation Advances in Bioinformatics grant (DBI 0850356 to R.E.).
Conflict of Interest: none declared.
ABSTRACT Summary: Computational methods designed to discover transcription factor binding sites in DNA sequences often have a tendency to make a lot of false predictions.
One way to improve accuracy in motif discovery is to rely on positional priors to focus the search to parts of a sequence that are considered more likely to contain functional binding sites.
We present here a program called PriorsEditor that can be used to create such positional priors tracks based on a combination of several features, including phylogenetic conservation, nucleosome occupancy, histone modifications, physical properties of the DNA helix and many more.
Availability: PriorsEditor is available as a web start application and downloadable archive from http://tare.medisin.ntnu.no/priorseditor (requires Java 1.6).
The web site also provides tutorials, screenshots and example protocol scripts.
Contact: kjetil.klepper@ntnu.no Received on April 21, 2010; revised on June 17, 2010; accepted on June 30, 2010 1 INTRODUCTION Computational discovery of transcription factor binding sites in DNA sequences is a challenging problem that has attracted a lot of research in the bioinformatics community.
So far more than a hundred methods have been proposed to target this problem (Sandve and Drabls, 2006) and the number of publications on the topic is steadily increasing.
There are two general approaches for discovering potential transcription factor binding sites with computational tools.
One is to examine regulatory regions associated with a group of genes that are believed to be regulated by the same factors and search for patterns that occur in all or most of these sequences.
This approach, often referred to as de novo motif discovery, can be used when we have no prior expectations as to what the binding motifs might look like.
One concern with this approach, however, is that it might be necessary to consider rather long sequence regions to ensure that the target sites are indeed covered.
Since binding motifs for transcription factors are usually short and often allow for some degeneracy, the resulting signal-to-noise ratio can be quite low, making it difficult to properly discriminate motifs from background.
Another problematic issue is that DNA sequences inherently contain a lot of repeating patterns, such as tandem repeats and transposable elements, which To whom correspondence should be addressed.
can draw focus away from the target binding motifs when searching for similarities between sequences.
The other general motif discovery approach, called motif scanning, searches for sequence matches to previously defined models of binding motifs, for instance in the form of position weight matrices (PWMs; Stormo, 2000).
The main drawback with motif scanning is that it tends to result in an overwhelming number of false positive predictions.
According to the futility theorem put forward by Wasserman and Sandelin (2004), a genome-wide scan with a typical PWM could incur in the order of 1000 false hits per functional binding site, which would make such an approach practically infeasible for accurate determination of binding sites.
The problem here lies not so much in the predicted binding patterns themselves, since many of these would readily be bound by transcription factors in vitro.
In vivo, however, most such binding sites would be nonfunctional, perhaps because the chromatin conformation around the sites precludes access to the DNA (Segal et al., 2006) or because the target factors require the cooperative binding of additional factors nearby to properly exert their regulatory function (Ravasi et al., 2010).
One way to improve accuracy in motif discovery is to try to narrow down the sequence search space as much as possible beforehand, for instance, by masking out portions of the sequences that resemble known repeats or considering only sequence regions that are conserved between related species (Duret and Bucher, 1997).
Kolbe et al.(2004) introduced a measure they called Regulatory Potential which combines phylogenetic conservation with distinctive hexamer frequency profiles to identify possible regulatory regions.
This measure calculates a score for each position along the sequence, and regions receiving higher scores are deemed more likely to have a regulatory role.
Regulatory Potential can be considered as an example of a positional prior since each position is associated with an a priori probability of possessing some specific property.
Positional priors can be used as an aid in motif discovery by assigning high prior values to regions that we consider more likely to contain functional binding sites and then focus the search on these regions.
Besides conservation and oligonucleotide frequencies, other features that can be relevant for assigning prior values include: localized physical properties of the DNA double helix, distance from transcription start site or other binding sites, ChIP-chip and ChIP-seq data, and potentially tissuespecific epigenetic factors such as the presence of nucleosomes and associated histone modifications.
Many of the aforementioned features have previously been applied and shown to improve the performance of motif discovery by themselves (see e.g.Bellora The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:32 30/7/2010 Bioinformatics-btq357.tex] Page: 2196 21952197 K.Klepper and F.Drabls Fig.1.
The top left panel in this screenshot shows examples of some of the features that can be used as a basis to create positional priors.
These features are visualized as data tracks in the main panel for a selected set of sequences.
The bottom-most track contains predicted matches to TRANSFAC and JASPAR motifs in regions with non-zero RegulatoryPotential7X scores.
et al., 2007; Segal et al., 2006; Whitington et al., 2009), and it has also been demonstrated that further gain can be achieved by integrating information about multiple features (see e.g.Ernst et al., 2010; Lhdesmki et al., 2008).
We present here a program called PriorsEditor, which allows users to easily construct positional priors tracks by combining various types of information and utilize these priors to potentially improve the motif discovery process (Fig.1).
2 SOFTWARE DESCRIPTION The first step in constructing a priors track with PriorsEditor is to specify the genomic coordinates for a set of sequences one wishes to analyze.
Next, data for various features can be imported to annotate these genomic segments.
PriorsEditor supports three types of feature data.
The first type, numeric data, associates a numeric value with each position in the sequence and can be used to represent features such as phylogenetic conservation scores, DNA melting temperatures and nucleosome-positioning preferences.
Numeric data tracks are also used to hold the final positional priors.
The second feature type, region data, can be used to refer to continuous stretches of the DNA sequence that share some unifying properties which distinguish them from the surrounding sequence.
Different regions are allowed to overlap, and regions can also be assigned values for various attributes, including type designations, score values and strand orientations.
Features best represented as regions include genes, exons, repeat regions, CpG-islands and transcription factor binding sites.
The last feature type, DNA sequence data, represents the DNA sequence itself in single-letter code.
DNA sequence data can be passed on to motif discovery programs for further analysis, and it can also be used to estimate various physical properties of the DNA double helix, such as GC content, bendability and duplex-free energy.
Additional feature data can be obtained from web servers such as the UCSC Genome Browser (Rhead et al., 2010) or be loaded from local files.
Once the data for the desired features have been loaded, the data tracks can be manipulated, compared and combined to create a priors track using a selection of available operations.
These include operations to extend regions by a number of bases upstream and/or downstream, merge overlapping regions or regions within close proximity, filter out regions, normalize data tracks, smooth numeric data with sliding window functions, interpolate sparsely sampled 2196 [15:32 30/7/2010 Bioinformatics-btq357.tex] Page: 2197 21952197 PriorsEditor data, weight numeric data tracks by a constant value or positionwise by another track, combine several numeric tracks into one using either the sum or the minimum or maximum value of all the tracks at each position and several more.
It is also possible to specify conditions for the operations so that they are only applied to positions or regions that satisfy the condition.
For example, to design a priors track that will focus the search toward conserved regions within close proximity of other binding sites, one could start off with a phylogenetic conservation track, then load a track containing previously verified binding sites from the ORegAnno database (Griffith et al., 2008), extend these sites by a number of bases on either side and lower the prior values outside these extended sites.
After a priors track has been constructed, there are several ways to make use of this new data.
The most straightforward way is to provide it as input to a motif discovery program that supports such additional information, for instance, PRIORITY (Narlikar et al., 2006) or MEME version 4.2+ (Bailey et al., 2010).
Unfortunately, not many motif discovery programs are able to incorporate priors directly, so an alternative is to mask sequence regions that have low priors by replacing the original base letters with Xs or Ns since most motif discovery tools will simply ignore positions containing unknown bases when searching for motifs.
Apart from being used to narrow down the sequence search space, priors information can also be applied to post-process results after motif discovery has been carried out, for instance, by filtering out predicted binding sites that lie in areas with low priors or adjusting the prediction scores of these sites based on the priors they overlap.
Positional priors tracks and masked sequences can be exported for use with external tools, but it is also possible to perform motif discovery from within PriorsEditor itself by using operations to launch locally installed programs.
To facilitate motif scanning, PWM collections from TRANSFAC Public (Matys et al., 2006) and JASPAR (Portales-Casamar et al., 2010) have been included, and users can also import their own PWMs or define new collections based on subsets of the available PWMs.
Constructing priors tracks and performing motif discovery analyses can be tedious, especially when it involves many datasets and requires several steps to complete.
If a user discovers a good combination of features to use for priors, it may be desirable to repeat the same procedure to analyze other sequence sets as well.
PriorsEditor allows such repetitive tasks to be automatized through the use of protocol scripts.
Protocol scripts describe a list of operations to be performed along with any specific parameter settings that apply for these operations.
They can be programmed manually in a simple command language or be constructed using a macro recording function which logs all operations the user carries out while in recording mode.
With protocol scripts these same series of operations can be automatically applied to new sequence sets simply by the click of a button.
These scripts can also be set up so that users can provide values for certain settings during the course of an execution, enabling users to select for instance a different background model or PWM threshold value to use in the new analysis.
By providing a protocol script describing the operations to be performed along with a file specifying the target sequences, it is possible to run PriorsEditor from a command-line interface instead of starting up the normal graphical interface.
This allows the construction and use of positional priors to be incorporated into a batch-processing pipeline.
Funding: The National Programme for Research in Functional Genomics in Norway (FUGE) in The Research Council of Norway.
Conflict of Interest: none declared.
ABSTRACT Summary: LinkinPath is a pathway mapping and analysis tool that enables users to explore and visualize the list of gene/protein sequences through various Flash-driven interactive web interfaces including KEGG pathway maps, functional composition maps (TreeMaps), molecular interaction/reaction networks and pathwayto-pathway networks.
Users can submit single or multiple datasets of gene/protein sequences to LinkinPath to (i) determine the cooccurrence and co-absence of genes/proteins on animated KEGG pathway maps; (ii) compare functional compositions within and among the datasets using TreeMaps; (iii) analyze the statistically enriched pathways across the datasets; (iv) build the pathwayto-pathway networks for each dataset; (v) explore potential interaction/reaction paths between pathways; and (vi) identify common pathway-to-pathway networks across the datasets.
Availability: LinkinPath is freely available to all interested users atContact: supawadee@biotec.or.th Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 21, 2011; revised on May 9, 2011; accepted on May 25, 2011 1 INTRODUCTION Pathways are functional units resulted from the interplay of interacting genes, RNAs, proteins and small molecules.
Mapping genes or proteins into the context of pathways can help gain more insights into their functions and interactions in an organism.
Although sequence similarity-based methods [e.g.NCBI BLAST (Altschul et al., 1990)] have been commonly used for identification of pathways to genes/proteins based on their orthologous genes/proteins annotated in the well-characterized pathways, these methods have limitations such that some best hits (such as those annotated as hypothetical or unknown genes/proteins) may not necessarily be annotated in any pathway.
The incorporation of additional data such as proteinprotein interactions and enzymatic reactions can help infer the pathways and their interconnection and uncover the biological function of genes/proteins.
However, the information regarding the connections between pathways through molecular interactions and reactions are not included and adequately represented to support the exploratory analysis in most pathway mapping tools (reviewed in Gehlenborg et al., 2010), for example, GenMAPP (Salomonis et al., 2007), Pathway Explorer (Mlecnik et al., 2005) and Pathway Projector (Kono et al., 2009).
To overcome To whom correspondence should be addressed.
these limitations, a web-based interactive tool, so-called LinkinPath, was developed to analyze, map and visualize the gene/protein lists in the context of interconnected pathways, which provides a valuable resource for not only comprehensive studies of genegene interaction, but also functional genomics in virtually all organisms.
2 METHOD LinkinPath has been developed as a web-based interactive exploration tool for pathway analysis.
Users can upload the datasets of DNA or protein sequences in FASTA format and submit to the LinkinPaths job queue (Supplementary Fig.S1), which could support the analyses of genomescale inputs.
Although, many jobs may be submitted at the same time, the LinkinPath web server accepts a total maximum of 5000 sequences for each job.
When jobs are finished, users can retrieve the result using the bookmarked URL during the submission or the web link from the notification email.
LinkinPath processes each job in five main steps: (i) sequence and protein domain search; (ii) pathway mapping and annotation; (iii) identification and comparison of enriched pathways; (iv) construction of interconnected pathway networks and (v) identification of common pathway subnetworks (Supplementary Fig.S2).
First, input sequences are searched against KEGG (Kanehisa and Goto, 2000), NR (Benson et al., 2007), PFAM (Finn et al., 2008) and RFAM (Griffiths-Jones et al., 2003) databases.
Second, if significant similarities are found to match with an enzyme class (EC) and/or KEGG Orthology (KO), genes/proteins will be annotated with the corresponding pathways.
Third, to support comparison of functional composition, Treemaps and statistical methods are employed to examine the pathways enriched in the dataset.
Fourth, information of molecular interactions and reactions from BIND (Bader et al., 2003) and KEGG will be used for inference of pathways and networks.
In case that an input sequence cannot be annotated in any pathway, LinkinPath will use its interacting partners to infer its related pathways.
The pathways will thus be linked together via interaction or reaction paths to form the network in this step.
Lastly, to identify the commonalities across multiple datasets, LinkinPath includes an algorithm to extract the frequent subnetworks from the pathway networks built in previous step.
3 RESULT Using the method described above, LinkinPath automatically maps and annotates genes/proteins in the datasets into the context of interconnected pathways and presents the results to users via a Flash-driven interactive web interfaces.
The results are organized into five analysis steps (see following subsections), which can easily be browsed, searched and downloaded in any of supported formats.
Summary charts and an interactive Venn diagram are also provided to illustrate of how a dataset differs from others according to their annotated sequences with EC numbers, interactions and pathways (Supplementary Fig.S3).
The annotation results of input sequences can be interactively explored in different visualization perspectives The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:20 22/6/2011 Bioinformatics-btr328.tex] Page: 2016 20152017 S.Ingsriswang et al.including KEGG-pathway maps, TreeMaps, pathway-to-pathway networks and interaction and reaction paths.
3.1 Detecting changes in animated pathway maps In most tools, the KEGG pathway image is statically displayed with positioned genes/proteins in the pathway.
To enable the dynamicity, LinkinPath allows users to run the pathway map animation to depict changes of expressed genes/proteins annotated in each pathway over multiple and time-series datasets (Supplementary Figs 4 and 5).
Detecting changes between the same pathway maps taken from different times/experiments/organisms can help reveal the functional characterization of genes/proteins expressed under different conditions.
In addition, the co-occurrence and co-absence of genes/proteins in a series of pathways can suggest the functional relationships between these molecules.
Alternatively, users can interactively explore the annotated pathways across datasets on KEGG Atlas, in which connected paths are colored according to the datasets.
3.2 Visualizing functional composition via TreeMaps Many classification schemes such as enzyme classification, KO and KEGG pathways have tree-based structures that are difficult to simultaneously display and compare the information contained in the trees.
To circumvent this problem, Treemaps are included in LinkinPath to facilitate the visualization and comparison of those hierarchical data via a set of nested rectangular maps.
A base rectangle represents the root of the hierarchy and is divided into rectangular subareas proportional to data size and colored by data type.
As a result, Treemaps enable users to compare sizes of nodes and subtrees, and are helpful in revealing patterns.
To disclose the functional composition among datasets, LinkinPath provides users with three different Treemaps: (i) the enzyme compositions using top-level EC numbers; (ii) the pathway compositions using KEGG pathway classification; and (iii) and KO compositions using KO numbers (Supplementary Fig.S6).
The Treemap of enzyme composition, for example, helps users visually examine enzyme enrichment in lists of genes/proteins, where each rectangle represents the top-level EC numbers with different colors and indicates the proportion of genes/proteins annotated with the EC group.
Alternatively, LinkinPath allows users to compare and examine the enrichments of the EC and KO annotations of entire input data across pathways.
3.3 Identification of statistically enriched pathways Since pathways that are highly enriched with the list of annotated genes/proteins are more likely to be biologically relevant, LinkinPath employs KOBAS (Wu et al., 2006) to help identify significantly enriched pathways in a dataset.
KOBAS uses KO number to link genes/proteins to KEGG pathways and calculates the statistical significance of each pathway in a queried dataset against all pathways in the referenced datasets.
There are three statistical tests available including binominal, chi-square and hyper-geometric distribution tests to assess the enrichment of the found pathways.
For each pathway found in the input datasets, LinkinPath calculates these statistics by comparing the number of sequences involved in the pathway for each dataset with the total number of sequences involved in the same pathway for all datasets.
3.4 Inferring pathway-to-pathway interconnections Despite their complexity, the interconnections between pathways can help unravel novel regulation mechanisms including metabolism and signaling in organisms.
LinkinPath infers the pathway-topathway connections using molecular interactions and reactions.
Pathways will be connected to each other and represented in a form of interactive networks.
Each dataset could have several pathway networks with different sizes.
Each node represents either a pathway or an input sequence.
The pathway node contains the information on the number of input sequences annotated in that pathway.
An edge between nodes indicates the pathway connection types with different colors and line styles.
Two pathway nodes are connected with a solid line if a path between them exists in KEGG database.
The dash line indicates an inferred path from an input sequence or a query node to a known pathway.
With its interactive network browser, LinkinPath allows users to browse and access the network characterization and the associated information such as the number of input sequences and interactions involved in a pathway node and the list of input sequences that appear in single or multiple pathways.
Exploring putative functions of genes/proteins: LinkinPath utilizes molecular interactions to infer the pathway and putative function of an un-annotated sequence on the basis of its interacting partners function.
The shortest interaction paths connecting from a query node to a pathway node in the network are identified using all paths breadth first search.
In addition, the putative function of an input sequence might be inferred by its proximity to functionally annotated genes/proteins within the context of interconnected pathways.
In the network browser, users can traverse the interaction paths to the inferred pathways of the nodes with dashed edge (Supplementary Fig.S7).
Exploring reactions between metabolic pathways: LinkinPath searches the reaction paths from an input sequence mapped in a metabolic pathway to the compounds linking to other metabolic pathways.
On a solid blue edge in the pathway-to-pathway network, if the reaction paths exist, users can explore what enzymes and compounds are essential to a metabolic process via the reaction paths between two pathways (Supplementary Fig.S8).
3.5 Discovering the common pathway subnetworks LinkinPath extracts the frequently occurred subgraphs/subnetworks from the pathway networks to discover the commonalities across the datasets (Supplementary Fig.S9).
Users can navigate and visualize frequent subnetworks of varied sizes that occur in a number of different datasets.
4 CONCLUSION LinkinPath is a web-based tool that was applied the state of the art visualization techniques for original aspects of pathway mapping and analyses.
Its novel contributions such as functional composition using Treemaps, pathway-to-pathway interconnections and the global metabolic map with highlighting mechanisms add value over comparable existing tools.
ACKNOWLEDGEMENTS The authors would like to thank Dr. Anan Jongkaewwattana for critically reading the manuscript and a former ISL staff, Eakasit 2016 [09:20 22/6/2011 Bioinformatics-btr328.tex] Page: 2017 20152017 LinkinPath Pachawongsakda, who helped in programming during the initial stage of the project.
Funding: National Center for Genetic Engineering and Biotechnology (BIOTEC), Thailand.
Conflict of Interest: none declared.
ABSTRACT Motivation: Identification of post-translationally modified proteins has become one of the central issues of current proteomics.
Spectral library search is a new and promising computational approach to mass spectrometry-based protein identification.
However, its potential in identification of unanticipated post-translational modifications has rarely been explored.
The existing spectral library search tools are designed to match the query spectrum to the reference library spectra with the same peptide mass.
Thus, spectra of peptides with unanticipated modifications cannot be identified.
Results: In this article, we present an open spectral library search tool, named pMatch.
It extends the existing library search algorithms in at least three aspects to support the identification of unanticipated modifications.
First, the spectra in library are optimized with the full peptide sequence information to better tolerate the peptide fragmentation pattern variations caused by some modification(s).
Second, a new scoring system is devised, which uses charge-dependent mass shifts for peak matching and combines a probability-based model with the general spectral dot-product for scoring.
Third, a target-decoy strategy is used for false discovery rate control.
To demonstrate the effectiveness of pMatch, a library search experiment was conducted on a public dataset with over 40 000 spectra in comparison with SpectraST, the most popular library search engine.
Additional validations were done on four published datasets including over 150 000 spectra.
The results showed that pMatch can effectively identify unanticipated modifications and significantly increase spectral identification rate.
Availability: http://pfind.ict.ac.cn/pmatch/ Contact: yfu@ict.ac.cn; rxsun@ict.ac.cn Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Liquid chromatography coupled with tandem mass spectrometry (LC-MS/MS) is the key experimental method for large-scale protein identification.
In this method, proteins are digested into peptides, which are then ionized and dissociated in a mass spectrometer.
The mass-to-charge ratios (m/z) and the intensities of the resulting product ions are measured to produce MS/MS spectra.
To identify the peptides and proteins, sequence database search has achieved great success in the past years, and a variety of search tools have been developed, e.g.SEQUEST (Eng et al., 1994), Mascot (Perkins et al., To whom correspondence should be addressed.
1999) and pFind (Fu et al., 2004).
Such an approach is implemented by comparing the similarities between the experimental spectra and the theoretical spectra predicted from peptide sequences in a database.
Unfortunately, due to insufficient understanding of the factors that determine peptide fragmentation, most current search tools employ simplified fragmentation models, such as the uniform backbone dissociation model, leading to many unidentified or misidentified spectra.
In recent years, with the availability of millions of confidently identified MS/MS spectra, an alternative as well as complementary approach called spectral library search has emerged.
Its essential idea is to build a library of experimental reference spectra rather than theoretically predicted ones.
Since this approach was first introduced to the field of protein identification by Yates et al.(1998), the last decade has witnessed a group of mass spectral library search tools, such as SpectraST (Lam et al., 2007, 2008), NIST MSPepSearch (http://peptide.nist.gov/), BiblioSpec (Frewen et al., 2006), X!Hunter (Craig et al., 2006), ProMEX (Hummel et al., 2007), HMMatch (Wu et al., 2007) and MSDash (Wu et al., 2008).
Compared to the sequence database search, the spectral library search takes advantage of the previously obtained knowledge and has three obvious merits.
First, improved sensitivity.
Spectral library search takes into account the fragmentation pattern individually for each experimental spectrum.
It yields more discriminative match scores than does the sequence database search.
Second, high search speed.
Experiments show that in shotgun proteomics some peptides are detected all the time while some are never (Lam et al., 2008).
Thus a well-organized spectral library consisting of empirically observed experimental spectra permits a smaller and more accurate search space.
Third, convenient identification of extraordinary spectra, such as those produced from peptides with unusual posttranslational modifications (PTMs).
These spectra are big challenges to sequence database search engines, but could be identified as easily as the ordinary ones by spectral library search (Craig et al., 2006; Hummel et al., 2007; Lam et al., 2007; Wu et al., 2007).
Apparently, the above merits are based on reliable and comprehensive spectral libraries.
One of the main obstacles is library coverage (Lam et al., 2007; Yates et al., 1998).
Many efforts have been made on library constructions, such as NIST (http://peptide.nist.gov/) and PeptideAtlas (http://www.peptideatlas.org/speclib/).
However, it remains difficult considering that PTMs may generate substantial modified forms of a peptide.
Note that there have been hundreds of known modifications (e.g.512 entries recorded in the RESID modification database by February 26, 2010) and only a few of them, e.g.phosphorylation, were extensively studied in the past.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:21 12/5/2010 Bioinformatics-btq185.tex] Page: i400 i399i406 D.Ye et al.In fact, PTM mapping has become the central issue of current proteomics.
The conventional sequence database search approach meets inevitable difficulties in PTM-centric data analysis, since the PTM types have to be explicitly specified by users.
In this case, not only are some possible unanticipated PTMs missed, but also the number of the PTMs considered has to be restricted to avoid combinatorial explosion of theoretical peptides in all possible modified forms.
To solve these problems, the mode of open search has been proposed, in which the peptide precursor ion mass tolerance is largely expanded and one or more modification masses are inferred to compensate for the peptide mass difference (Chen et al., 2009; Tsur et al., 2005).
Such an approach does not require specifying PTM types and is able to identify spectra from peptides with unanticipated PTMs, though it still has some defects to overcome (e.g.low search speed).
Also, Bandeira et al.(2007) developed a databaseindependent algorithm, named Spectral-Networks, to detect spectral pairs produced from modified and unmodified versions of the same peptide and identify the unanticipated modifications by propagating spectral annotations in the networks of related spectral pairs.
However, the potential of applying the same idea to the spectral library search had not been explored until very recently.
Ahrne et al.(2009) proposed a workflow to combine open library search with sequence database search to increase spectral identification rate, but the library search engine they used was not deliberately designed for the open search mode.
Besides, a spectral matching algorithm Bonanza is sometimes considered as an open library search tool (Falkner et al., 2008; Menschaert et al., 2009), but it was actually devised in a clustering framework and it is unknown whether the methods in it are directly applicable to general library search, such as the method for false discovery rate (FDR) control.
There are three key issues that have to be addressed when designing an open library search tool.
The first one is the shifted m/z values of the product ions carrying PTMs.
One solution to this issue lies in the proper use of precursor ion mass differences between the spectral pairs to be matched; that is, the mass differences should be considered as the potential PTM masses, as done by some open sequence database search engines, e.g.PTMap (Chen et al., 2009).
However, none of the current library search algorithms has considered it.
Although Bonanza does allow a mass shift equal to the mass difference when matching product ion peaks, the mass shift value is roughly determined without considering the charge states of product ions.
The second issue is how to use the sequence information behind library spectra.
Although some of the current library search algorithms have tried some ways to use the sequence information by annotating the explained peaks in library spectra, they do not make the best of it, especially for scoring.
Usually, only a proportion of the theoretical product ions are observed in an experimental spectrum.
However, the omitted proportion may also be valuable, in particular for the open search where the changes of peptide fragmentation patterns caused by some unanticipated PTM(s) should be considered.
The third issue is FDR control of search results.
The FDR control methods used in current library search engines are not as mature as those used in sequence database search, e.g.the widely adopted target-decoy database search strategy (Elias and Gygi, 2007).
In this article, we present a dedicated open spectral library search tool, named pMatch, to identify unanticipated PTMs from MS/MS data.
It is the first time, to our knowledge, that the issues mentioned above are comprehensively addressed.
First, the library is constructed with spectra optimized by the full peptide sequence information to better tolerate the peptide fragmentation pattern variations caused by some PTMs.
Second, a new scoring system is devised, which uses charge-dependent mass shifts for peak matching and combines a probability-based model with the general spectral dot-product for scoring.
Third, a target-decoy strategy is used for FDR control.
To demonstrate the effectiveness of pMatch, a library search experiment was conducted on a public dataset of standard proteins with over 40 000 spectra.
Since no open library search tool is currently available, comparison was made with SpectraST, the most popular library search engine.
As expected, pMatch significantly outperformed SpectraST in detecting unanticipated PTMs and increasing the number of identified spectra.
Additional validations were done on four published datasets including over 150 000 spectra; a variety of PTMs were found and the spectral identification rates were increased to a large extent.
2 METHODS As an integrated library search engine, pMatch supports an entire workflow including library construction, spectral matching and result evaluation.
2.1 Library construction pMatch enrolls the identified raw spectra and makes full use of their corresponding sequence information to construct the library of optimized consensus spectra.
At the beginning, consensus spectra are generated from duplicate spectra for redundancy removal.
Here, the credibly identified raw spectra with the same peptide sequence, charge and modification states are assumed as duplicate spectra.
To produce a consensus spectrum, the peaks from each raw spectrum have their intensities normalized such that the top intensity value is one.
The common peaks (peaks from different spectra but with small differences in m/z according to the instrument precision, e.g.0.5 Th for ion trap) in duplicate spectra are combined into a consensus peak, with the averaged m/z and intensity values.
Only those consensus peaks occurring in the majority of the duplicate spectra are retained.
All the peak intensities are then rescaled by taking their square roots.
This strategy has been demonstrated to lead to better performance in spectral similarity comparison (Liu et al., 2007; Stein and Scott, 1994).
Next, consensus spectra are optimized by incorporating the peptide sequence information to make theoretical peaks bud (including those unobserved ones).As is shown in Figure 1, for each consensus (experimental) spectrum, a theoretical spectrum is generated with theoretical ion peaks (the b/y series product ions for collision-induced dissociation (CID) in this study) in the observed m/z range, with a uniform intensity value one.
In Fig.1.
An optimized spectrum holds the duality of experimental and theoretical spectra.
The parameter spanning from 0 to 1 can be considered as the tendency towards the theoretical spectrum.
The optimized spectrum equals the experimental spectrum when is 0, and is shaped the same as the theoretical spectrum when reaches 1. i400 [12:21 12/5/2010 Bioinformatics-btq185.tex] Page: i401 i399i406 Open MS/MS spectral library search each consensus spectrum, peak intensities are normalized making the top intensity value be one.
Then, the intensities of the peaks in the theoretical and consensus spectra are, respectively, multiplied by the factor of (0  1) and 1, and the two spectra are merged by superimposing their common peaks.
Thus, the optimized consensus spectra are generated, with each explained peak annotated by its ion type, fragmentation position and charge state.
This budding strategy regains a part of sequence information that was lost in the experimental spectra.
The optimized spectra emerge as a theoretical and experimental duality and are expected to tolerate the variations in peptide fragmentation patterns introduced by some PTMs.
The last procedure is to generate a group of decoy spectra with the same volume as the optimized consensus spectra, since pMatch uses a targetdecoy strategy to evaluate its search results.
The details of decoy spectrum generation scheme will be described later in this article.
2.2 Spectral matching Given a query spectrum, those library spectra with their precursor ion mass differences within a user-set tolerance and with the same charge state are selected as candidates for comparison.
The precursor ion mass tolerance may be very large for the open search, e.g.300 Da.
Finally, the candidate spectrum with the highest match score is assigned as the identification result of the query one.
2.2.1 Preprocessing Before matching, each query spectrum undergoes a simple preprocessing procedure.
Isotopic peaks are removed and the peak intensities are rescaled by taking their square roots.
At most the top 6 peaks per 100 Th are reserved for later matching.
2.2.2 Peak hit determination To determine peak hits when matching two spectra, the precursor ion mass difference (which we call M in the following parts of this article) is used to compute the allowed mass shifts for peak matching.
Since the charge states of the explained peaks in library spectra are already known, the mass shifts could be accurately determined.
The specific rules to find out peak hits are exhibited as follows.
Peaks from the query spectrum are examined in the descending order of their intensities.
If the query peak being examined has its m/z value mQ, and the user-set product ion m/z tolerance is Tp, then two sets of library peaks are selected: S1 ={library peak with m/z value mL : |mQ mL | < Tp}, S2 ={explained library peak with m/z value mL and charge state chrL : |mQ mL M/chrL |<Tp}.
The peaks from either S1 or S2 are chosen as candidate peaks if the M is big enough to cause a PTM (say beyond 0.5 Da); otherwise, only peaks from S1 are chosen.
The most intensive candidate peak is finally determined as the hit peak to the query peak.
Each peak can only be hit at most once.
2.2.3 Similarity scoring As for spectral similarity scoring, pMatch employs two sub-scores: a spectral dot-product score and a probability-based score.
The spectral dot-product score (SDP_Score) is calculated as: SDP_Score= peak_hits IQ IL query_peaks I 2 Q  library_peaks I 2 Q , (1) where IL and IQ denote the intensities of the library peak and the query peak, respectively.
For a query spectrum, there are usually several candidate library spectra (here we let the number be W ).
To determine whether one match stands out from the remaining candidates, we use a probability-based score.
A peak in a query spectrum is defined as a capital peak if its intensity is no less than 5% of the most intensive peak and is ranked in the top 10 in this spectrum.
A hit between a capital peak and an explained library peak is called a mighty hit.
Let n be the number of the capital peaks in the query spectrum, ki be the number of mighty hits in the match between the query and the i-th candidate spectrum, and mi be the number of explained peaks of the i-th candidate (the value of mi is doubled if mass shifts are triggered in the i-th match).
Then the global average probability (p) that a capital query peak and an explained library peak make a peak hit can be calculated as follows: p= W i=1 ki/nW i=1 mi.
(2) For each capital peak in the query spectrum, the probability (P) that a mighty hit occurs by chance between it and one of the explained peaks in the i-th candidate library spectrum is: P=1(1p)mi =1[1C1mi p++Cmimi (p)mi ]pmi.
(3) The probability (P_value) that ki or more mighty hits occur by chance between the query and the i-th candidate library spectrum is: P_value= n j=ki Cjn Pj (1P)nj.
(4) The probability-based score, denoted by P_Score, is then calculated according to Equation (5).
It evaluates the significance of a certain match on the basis of the statistic background of all candidate matches.
P_Score=log(P_value).
(5) The final score of a match between a library spectrum and the query spectrum, as we call pMatch_Score, is the product of SDP_Score and P_Score: pMatch_Score=SDP_ScoreP_Score (6) 2.2.4 PTM locating After the library spectrum with the highest pMatch_Score is found, the location of the PTM on the peptide is assigned as follows.
Each amino acid residue is assumed as the PTM site and a theoretical spectrum is predicted from the peptide with the PTM-containing product ion peaks shifted accordingly.
Then this series of theoretical spectra are scored against the query spectrum using the common spectral dot-product.
The highest scored site is accepted as the PTM location.
2.3 Control of false discovery rate If a large set of query spectra are searched, then the control of FDR is necessary.
Since the target-decoy search strategy has been the leading way to estimate the FDR of the sequence database search results, a natural idea is to extend it to the spectral library search.
Yen et al.(2009) and Lam et al.(2010) have demonstrated the feasibility of using decoy spectra for FDR estimation in the spectral library search.
Here, we extend this idea into the open search mode, and employ a similar approach for decoy spectra generation.
For each optimized consensus spectrum in the library, a decoy spectrum is generated with the same precursor ion mass and charge state.
Since the amino acid sequence is already known, a pseudo-reversed (Elias and Gygi, 2007) sequence is made from the original peptide sequence; that is, the sequence of all the amino acid residues is reversed except the C-term one, by which means the enzyme digestion feature is reserved.
Then, the corresponding decoy spectrum is born with explained peaks moved to the new m/z positions determined by their annotations and the pseudo-revered sequence.
pMatch filters search results by their pMatch_Scores and estimates FDR using the formula FDR = FP/TP, where FP and TP represent the numbers of matches to the decoy and original spectra, respectively.
Importantly, for the open search mode, an issue that could produce considerable impact on spectral identification rate is the result filtration rule.
The normal rule is to rank the whole result list by score and then calculate the estimated FDR.
Because of the mass shifting strategy used in the open search, however, a pair of spectra with significant M (where mass shifting works) raises the chance of peak hits, and thus are likely to produce a higher score compared to the pairs with insignificant M. Obviously, the false positive identifications i401 [12:21 12/5/2010 Bioinformatics-btq185.tex] Page: i402 i399i406 D.Ye et al.with significant M would have higher chance to pass a uniform score cutoff.
Therefore, a more reasonable filtration rule that we advocate is to group all the results into two lists according to their M values, i.e.those with insignificant M and those with significant M. Afterwards, the results in the two groups are ranked separately for FDR estimation.
This separate filtration rule is expected to increase the spectral identification rate compared to the normal rule.
3 RESULTS In this study, a comparison experiment on a public dataset was carried out with detailed analysis between pMatch and SpectraST in both the conventional and the open search modes.
To further validate pMatch, four additional published datasets were analyzed in the open search mode.
The five datasets including 200 000 spectra in total were all engaged in the same experimental workflow.
3.1 Datasets and library construction The five published datasets chosen in this study were from different species.
The MS spectra were derived from high-or mediumprecision instruments, as the use of high-precision instruments becomes the trend of proteomics development (Mann and Kelleher, 2008), and it is practical to gain more accurate PTM masses determined by the precursor ion mass differences.
The brief summaries of the datasets are given below: ISB-18mix is designed deliberately for the purpose of testing peptide and protein identification software tools (Klimek et al., 2008).
Eighteen purified recombinant proteins were mixed and digested by trypsin into peptide mixtures, which were then analyzed by LC-MS/MS on diverse mass spectrometers under various conditions.
In this study, the Mixture 3 on a LTQ-FT mass spectrometer with all 10 LC-MS/MS runs was chosen for our experiment.
We focused on the analysis of this data for comparison between pMatch and SpectraST.
TAP-PSD95 refers to the samples from the mice with proteins in gene-targeted TAP tagging.
The samples were purified in four replicates, which were then analyzed by LTQ-FT mass spectrometer (Fernandez et al., 2009).
Replicate_2 was randomly chosen for our experiment.
HUPO-14 is from a study in which 20 highly purified recombinant human proteins were distributed to 27 laboratories for mass spectrometry-based analysis (Bell et al., 2009).
The data from Lab 14 was chosen in our study and the instrument they used was LTQ-FT. Haas-Data refers to a yeast sample digested by trypsin, from which the collision activated dissociation (CAD) MS/MS spectra were produced by different mass spectrometers (Haas et al., 2006).
Only the dataset from the LTQ-FT instrument was used here.
Gygi-Qstar refers to the yeast proteome digested by trypsin which was analyzed by LC-MS/MS using a Q-STAR mass spectrometer (Elias et al., 2005).
The way to construct the spectral libraries is similar to that proposed by Ahrne et al.(2009).
This way has been demonstrated to be very effective in increasing the spectral identification rate of a dataset.
First, the spectra in a dataset are searched against a protein sequence database.
Then, the credibly identified spectra are accumulated to construct a spectral library, against which the remaining spectra are afterwards searched.
3.2 Results on the ISB-18mix dataset To identify some of the spectra for library construction, the pFind search engine (Fu et al., 2004; Li et al., 2005; Wang et al., 2007) (version 2.3) was used to search a target-decoy sequence database including the standard, pollution and background proteins (see Supplementary Data for the detailed description of the database).
During searching, the precursor ion mass tolerance was set to 50 ppm, and the product ion m/z tolerance was 0.5 Th.
Full tryptic specificity was applied, allowing up to two missed cleavage sites.
Carbamidomethylation of cysteine was specified as a fixed modification, and oxidation of methionine as a variable one.
After sequence database search, we observed that most of the identified spectra with high confidence had their precursor ion mass biases of around +2 ppm.
The search results were then filtered with precursor ion mass deviation from 2 to +6 ppm at 1% FDR.
Additionally, only those spectra from the proteins containing at least two unique detected peptides were reserved.
Finally, a total of 12 032 identified spectra, including 577 unique peptides (with distinct amino acid sequences and PTMs) and 963 unique precursor ions (with distinct sequences, PTMs and charge states), were obtained and used to construct a spectral library.
In terms of the library search, pMatch (version 1.0) and SpectraST (version 3.1) were engaged with the same spectral source for library constructions and searches, and both the conventional and the open searches were carried out.
For SpectraST, the precursor ion m/z tolerances were set to 2 and 150 Th, respectively, for the conventional and the open searches.
The parameter to control the production m/z tolerance was 1 bin/Th (equal to 0.5 Th).
The search results were post-processed by PeptideProphet (Keller et al., 2002) for FDR estimation, as suggested by Lam et al.(2007, 2008).
While for pMatch, in library construction, the in the budding step was set to zero for conventional search to reduce spectral distortion and was set to 0.2 for open search to increase the robustness of the library.
Given that the lowest charge state of the spectra in this dataset was 2+, the precursor ion mass tolerances were set to 4 and 300 Da, respectively, for the conventional and the open searches.
The product ion m/z tolerance was 0.5 Th.
The FDRs of the search results were controlled by the target-decoy strategy with the normal filtration rule (not the separate filtration rule for a fair comparison).
The number of identified spectra from both engines at different FDR cutoffs are illustrated in Figure 2.
It can be seen that compared to the conventional search, the open search significantly increased the number of identified spectra for both search engines.
pMatch and SpectraST comparably performed in the conventional search.
When it comes to the open search, however, pMatch identified nearly twice as many spectra as SpectraST through the whole FDR range considered.
In order to explore the differences between the two engines, a careful analysis was conducted on the results under 1% FDR.
In the conventional search, as shown in Figure 3a, there were 2462 spectra identified by both engines, among which 2451 had agreeable matches and 11 conflicted.
After manually validating the 11 spectra by taking a close-up view of their MS/MS spectra and tracing back to the corresponding MS spectra, we found all of the 11 query spectra i402 [12:21 12/5/2010 Bioinformatics-btq185.tex] Page: i403 i399i406 Open MS/MS spectral library search were co-eluted spectra and their identifications by the two engines caught different components.
Supplementary Figure S1 gives a typical example of a co-eluted spectrum.
Unlike the conventional search where the two engines showed over 80% overlap between their results, in the open search, as revealed by Figure 3b, only <40% of the pMatchs identifications could be found in SpectraSTs results, although the 13 disagreements all came from co-eluted spectra also.
Fig.2.
FDR curves for pMatch (solid lines) and SpectraST (dashed lines) search engines.
The x-axis denotes the FDR value and the y-axis denotes the number of identified spectra.
The thin and thick lines represent the results of the conventional and the open searches, respectively.
Fig.3.
Venn diagrams of the number of identified spectra at 1% FDR from pMatch and SpectraST in the conventional (a) and open (b) search modes.
The ashen regions denote the spectra with inconsistent identifications from the two engines.
As is discussed previously, each identification has the precursor ion M as the potential PTM mass in the open search.
The histograms of the M values detected by the two engines are exhibited in Figure 4.
As shown, some intensive M detected by pMatch were not or rarely detected by SpectraST, for example, 128 Da (lysine loss), 22 Da (sodium adduct), 38 Da (calcium adduct) and 152 Da (carbamidomethylDTT).
The crucial reasons should be that some modified spectra have a considerable percent of the observed peaks with their m/z values shifted and that some special PTMs might largely vary the fragmentation pattern of a peptide (see Supplementary Figure S2 for a spectrum with a sodium adduct and Supplementary Figure S3 for the influence of the budding strategy on PTM detecting).
However, neither did SpectraST consider the mass shifts caused by unanticipated PTMs during peak matching, nor made use of the sequence information to tolerate the peptide fragmentation pattern variations.
On the contrary, SpectraST identified more spectra with very small absolute M values (within 5 Da), which mainly resulted from duplicate spectra, co-eluted spectra and spectra from deamidated peptides.
Then, we concentrated on the abundant M (with 20 spectra for either engine) and manually validated some representative spectra.
Nearly all of the abundant M were explained (see Supplementary Table S1 for their frequencies and explanations).
Among these M, many PTMs were found (shown in Table 1); for example, a disulfide bridge was detected (shown in Figure 5).
Additionally, some M were caused by amino acid substitutions, or missed cleavages, or semi-digestions, while some corresponded to the combinations of two or more other M values.
Only two M were not explained using our current knowledge.
One of them had evidence supporting that there was indeed something happened on the peptides (see Supplementary Figure S4), while the other one might be a false positive.
In addition to those abundant M, low-abundance ones also provided a wealth of information.
Some of them corresponded to important PTMs, such as phosphorylation.
pMatch and SpectraST identified 13 and eight spectra, respectively, with M of 79.97 Da.
These spectra are supposed to be derived from phosphorylated peptides.
Figure 6 gives an example of such spectra.
Fig.4.
Histograms of M detected by pMatch (top) and SpectraST (bottom).
The intensive peaks are annotated by their M values in integer accuracy.
i403 [12:21 12/5/2010 Bioinformatics-btq185.tex] Page: i404 i399i406 D.Ye et al.Table 1.
The open search results of pMatch on all datasets Dataset Total MS/MS Identified spectra Identification rate raised by Spec Lib Abundant modifications (Da) Seq DB Spec Lib ISB-18mix 40 376 12 032 +8025 29.80% 49.68% 116 (a disulfide bridge); 18 (dehydration); 17 (ammonia loss); 16 (ammonia loss and deamidation); 1 (deamidation); 2 (two deamidations); 16 (oxidation); 22 (sodium); 23 (sodium and deamidation); 26 (acetaldehyde +26); 38 (calcium); 39 (calcium and deamidation); 152 (carbamidomethylDTT); 153 (carbamidomethylDTT and deamidatoin); 174 (carbamidomethylDTT and sodium) TAP-PSD95 36 387 3575 +1882 9.82% 15.00% 18 (dehydration); 17 (ammonia loss); 1 (deamidation); 14 (methylation); 16 (oxidation); 22 (sodium); 26 (acetaldehyde +26); 28 (formylation); 32 (dioxidation); 42 (acetylation); 54 (acetaldehyde +26 and formylation); 70 (formylation and acetylation); 80 (phosphorylation) HUPO-14 15 221 7281 +2418 47.84% 63.72% 17 (ammonia loss); 1 (deamidation); 12 (formaldehyde induced modification); 71 (propionamide); 26 (acetaldehyde +26); 42 (acetylation) Haas-Data 56 599 9172 +2558 16.21% 20.74% 17 (ammonia loss); 1 (deamidation); 43 (carbamylation); 171 (carbamylation and lysine added) Gygi-Qstar 46 195 9255 +4357 20.03% 29.40% 1 (deamidation); 12 (formaldehyde induced modification); 22 (sodium); 28 (formylation) Fig.5.
An example of a disulfide bridge.
(a and b) are the tandem mass spectra of a same peptide sequence IVSNASCTTNCLAPLAK, but the former one is with two carbamidomethylated cysteines, while the latter one has a disulfide bridge across the two cysteines.
The spectrum in (a) has several product ions indicating the CID fragmentations between the two cysteines, while in (b) no noticeable ions supporting such fragmentations can be found in the query spectrum identified with the M of 116.06 Da.
Most of intensive peaks are explained with low m/z errors.
i404 [12:21 12/5/2010 Bioinformatics-btq185.tex] Page: i405 i399i406 Open MS/MS spectral library search Fig.6.
A spectrum from a phosphorylated peptide.
This triply charged spectrum was identified to have the peptide sequence of TGKPDYVTDSAASATAWSTGVK, with a M of 79.97 Da that implies a phosphorylation.
The modified site is the 10th amino acid residue (the first serine) from the N-term.
The neutral loss peaks of precursor ions by masses of 98 and 116 Da are obvious, and there are also many neutral loss peaks of product ions by 98 Da.
These features are typical for spectra of phosphorylated peptides.
Most of intensive peaks are explained with low m/z errors.
3.3 Results on four additional datasets For further validations, four additional published datasets were analyzed by pMatch in the open search mode, obeying the same workflow as above.
The detailed search parameters are listed in Supplementary Table S2.
To explore how much in the end pMatch could help to increase the spectral identification rate, here we used the separate filtration rule for FDR estimation.
Table 1 shows the analysis results.
For completeness, the result of the ISB-18mix dataset is also listed.
We can see that the spectral identification rates significantly grew after library search and some interesting modifications were detected.
For example, the M of 12 Da detected in two datasets all occurred on peptide N-terms or basic amino acids.
This modification is induced by formaldehyde (Toews et al., 2008), and has been recently detected in other datasets (Menschaert et al., 2009).
Other detected PTMs include formylation (28 Da), acetylation (42 Da), methylation (14 Da), etc.
Interestingly, in the Gygi-Qstar dataset, a number of spectra are identified with M distributed from 20 to 3 Da.
Many of them show no mass shift in product ions, compared with their matched library spectra (see Supplementary Figure S5), indicating that their precursor ion masses might have been incorrectly judged.
4 CONCLUSION We have presented a novel spectral library search tool, pMatch, deliberately designed for the open search mode.
Its ability to identify spectra with unanticipated PTMs was demonstrated on several datasets.
In cooperation with traditional sequence database search, pMatch is able to push up the spectral identification rate to a large extent.
The key points to contributing the success of this method lie in three aspects: the consideration of accurate mass shifts for peak matching; the use of full peptide sequence information for consensus spectral optimization; a new scoring function that combines the general intensity-based dot-product with a probabilistic model of peak matching.
ACKNOWLEDGEMENTS The authors thank Dr Wilhelm Haas (Harvard Medical School) for providing the dataset Haas-Data in RAW format, and thank Dr Henry H.N.
Lam (Department of Chemical and Biomolecular Engineering, HKUST) and Dr Stephen E. Stein (National Institute of Standards and Technology) for valuable discussions.
Funding: This study was supported by the National Natural Science Foundation of China under grant no.
30900262; the CAS Knowledge Innovation Program under grant no.
KGGX1-YW-13; the National Key Basic Research and Development Program (973) of China under grant no.
2010CB912701; and the National High Technology Research and Development Program (863) of China under grant nos.
2007AA02Z315, 2008AA02Z309.
Conflict of Interest: none declared.
ABSTRACT Motivation: Molecular association of phenotypic responses is an important step in hypothesis generation and for initiating design of new experiments.
Current practices for associating gene expression data with multidimensional phenotypic data are typically (i) performed one-to-one, i.e.each gene is examined independently with a phenotypic index and (ii) tested with one stress condition at a time, i.e.different perturbations are analyzed separately.
As a result, the complex coordination among the genes responsible for a phenotypic profile is potentially lost.
More importantly, univariate analysis can potentially hide new insights into common mechanism of response.
Results: In this article, we propose a sparse, multitask regression model together with co-clustering analysis to explore the intrinsic grouping in associating the gene expression with phenotypic signatures.
The global structure of association is captured by learning an intrinsic template that is shared among experimental conditions, with local perturbations introduced to integrate effects of therapeutic agents.
We demonstrate the performance of our approach on both synthetic and experimental data.
Synthetic data reveal that the multitask regression has a superior reduction in the regression error when compared with traditional L1-and L2-regularized regression.
On the other hand, experiments with cell cycle inhibitors over a panel of 14 breast cancer cell lines demonstrate the relevance of the computed molecular predictors with the cell cycle machinery, as well as the identification of hidden variables that are not captured by the baseline regression analysis.
Accordingly, the system has identified CLCA2 as a hidden transcript and as a common mechanism of response for two therapeutic agents of CI-1040 and Iressa, which are currently in clinical use.
Contact: b_parvin@lbl.gov 1 INTRODUCTION Genome-wide association studies of expression and phenotypic data are becoming a routine methodology for identifying potential biomarkers.
While the literature is rich with supervised or unsupervised clustering of genomic information, methods for studying the relationships between genomic and phenotypic data remain relatively limited.
Existing association methods are typically based on the univariate correlation analysis, which either correlates a single gene to the resultant phenotype(s) or vice versa.
This is known as the gene-and phenotype-based approaches, respectively (Dryja, 1997).
More recently, (Yi et al., 2008) quantized large number of transcript data through clustering, and associated them with physiological responses or clinical metadata.
In contrast, another group of researchers have taken a new direction by first clustering morphometric data and then associating with the transcript data (Han To whom correspondence should be addressed.
et al., 2010).
However, in both cases, correlation is based on the independent, pairwise univariate analysis.
Pairwise univariate correlation analysis can quickly provide important association information, as well as candidates for further screening.
However, it treats the genes and the phenotypes as independent and isolated units, therefore the underlying interacting relationships between the units might be lost.
It is well-known that some transcripts act as regulatory nodes, driving other transcripts in a coordinated manner to determine the phenotypic profile.
Additionally, incubation with each therapeutic reagent simultaneously interferes with a subset of genes.
Here, we hypothesized that simultaneous incorporation of genomewide expression data coupled with phenotypic data computed from multiple perturbation conditions, each targeting a different molecular region, can elucidate a common mechanism of response that may be hidden otherwise.
In fact, perturbation and molecular diversity of the model system have shown to be capable of reducing the samples needed for biological inference, thus enhancing robustness of biological conclusion (Ideker et al., 2001; Sachs et al., 2005; Tegnr et al., 2003).
Thus, we ask the following questions.
How can traditional univariate associations be modeled simultaneously and in the absence of a correlation threshold?
How can the inherent sparsity of association be formalized within an optimization framework?
How can one compensate for the lack of replicates due to the high experimental cost associated with gene expression profiling?
To address these issues, we have developed an integrated platform that simultaneously and systematically takes into account an ensemble of gene and phenotypic signatures.
Such an enterprise must incorporate an experimental design with sufficient degree of molecular diversity for increased computational robustness.
In this context, molecular diversity is achieved by using a panel of breast cancer cell lines that are well-characterized and readily available through American Type Culture Collection.
Our computational framework consists of two major steps.
First, a vector-valued, multitask regression formulation is adopted to model the relationships between transcripts and phenotypes under multiple experimental conditions.
In particular, the regression coefficients are factorized into two parts.
One part is a shared template that suggests a common mechanism of action under various treatments.
The second part is related to the perturbation that is induced locally in the transcript network under individual perturbation.
The regression has to be sparse, because only a subset of genes is typically involved in a specific phenotypic response.
Sparsity is enforced through L1-norm regularization, which inherently removes outliers and irrelevant associations.
The end result is a sparse regression matrix that captures intrinsic properties of genephenotype association.
This matrix is reordered for improved visualization of the gene phenotype grouping, where the reordering aims at an optimum permutation of rows and columns of the regression matrix such that The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i98 i97i105 K.Zhang et al.the underlying saliency becomes apparent.
In this context, reordering reveals dominant association between subsets of genes (with the similar expression profile) and subset of phenotypic indices (with the similar measurements).
We have demonstrated the efficacy of our method with synthetic and experimental data, where the main purpose of synthetic data is to profile the robustness and precision of the proposed method.
Experimental data consist of baseline gene expression data for a panel of breast cancer cell lines, which are associated with cell-cycle inhibitor data.
The proposed method can be used as a complementary tool besides baseline regression techniques, to provide a richer and a more promising list of candidate molecular predictors for further biological verifications.
Section 2 presents our computational model and detailed optimization procedures.
Section 3 provides results on synthetic and experimental data.
Section 4 concludes with a discussion on the molecular predictors and system performance.
2 MODELS 2.1 Description of basic computational models In this section, we introduce our basic computational models for exploring the associations between genes and phenotypic responses.
To reduce excessive costs associated with the collection of gene expression data, we assumed that the gene expression were collected under a baseline (unperturbed) condition, as denoted by X0 RCN.
Here, C is the number of cell lines and N is the number of genes.
On the phenotypic side, assume that we obtained measurements Yd RCM s for d =0,1,2,....,D, where M is the number of phenotypic features, d =0 denotes the controlled, baseline condition and d = 1,2,...,D corresponds to the drug-perturbed conditions.
We used the linear regression model to measure the dependency between genes and phenotypes, as illustrated in Figure 1.
The design matrix X0 was mapped to the phenotype responses Yd RCM via a regressing matrix Td RNM , as X0Td Yd.
(1) The coefficient matrices Td s reflect the dependency (or correlation) between the genes and the phenotypes of interest, i.e.its ij-th entry is the weight associated with the i-th gene in reconstructing the j-th feature in the phenotypic profile under the d-th condition.
There are a number of complexities in estimating T. These complexities originate from low sample size, high dimensionality of the data and coupling between different perturbation conditions.
However, majority of the transcript data can be considered as Fig.1.
The linear regression model used to compute the sparse association between baseline gene expression data and phenotypic responses.
noisy background, as it believed that only a subset of genes are involved in each specific cellular process.
To address these issues, we propose a sparse, regularized multitask regression framework with co-clustering.
The novelty of our method involves: (i) leveraging the locality of the molecular interactions as a result of treatment with therapeutic agents, and modeling multiple treatments simultaneously; (ii) coupling it with a L1-regularized solution that enforces sparsity and simultaneously compensates for small sample size; and (iii) grouping associations with co-clustering.
First, a multitask regression framework is used to model the molecular interactions under multiple conditions in a systematic way.
The Multitask learning (Caruana, 1997; Lee et al., 2007; Xiong et al., 2007) is aimed at information sharing among learners from a set of different but related tasks, with the hope to boost the overall performance.
In this context, regression (1) under each experimental condition is deemed as a task.
As phenotypic profiles arise from the original gene regulatory network and its local perturbation, we can assume that phenotypic responses are triggered by different experimental conditions are lying on the same low-dimensional space, i.e.Td =TPd for d =0,1,2,...,D. (2) In other words, task relatedness is enforced by requiring that Td s associated with each task are local perturbations of a shared subspace T. Here, TRNK represents the shared structure (related to the gene regulatory network), Pd RKM compensates for the perturbation of different experimental conditions and K is the dimension of the latent space in which the phenotypic responses are supposed to reside.
In our formulation, K is set to be equal to M for practical reasons, and Pd s are diagonal matrices.
The actual structure of Pd is an open problem at this point, and it is possible that a non-diagonal matrix can produce a better reconstruction result.
The structure of Pd and the choice of K is one of the topics for our continued research.
Nevertheless, the shared template matrix T has the potential to summarize association descriptor between N genes and M phenotypes.
An advantage of decomposing the Td matrices is a significant reduction in the number of variables for estimation.
Second, the L1 regularization technique is used to mathematically guarantee the robustness of the system against irrelevant genes.
The L1 regularization typically leads to sparse learning models, and has been independently discovered in several research areas such as regression shrinkage and variable selection (Tibshirani, 1996), basis pursuit (Donoho et al., 2001), compressive sensing (Donoho, 2006) and feature vector machine (Li et al., 2005).
By penalizing the L1norm of the variables, part of the regression coefficients will be driven to zero with the level of sparsity controlled by the strength of regularization.
This is a desirable property considering the highly localized functionalities of genes as they relate to specific phenotypic signatures.
By combining the multitask learning frame with the L1 regularization, we established sparse multitask regression as follows: min TRNM PdRMM f =Dd=0X0TPd Yd2F +T1.
(3) s.t.
PdF =1, for d =1,2,...,D. Here, F is the matrix Frobenius norm and 1 is the matrix L1-norm.
The first term enforces a fit between the gene expression i98 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i99 i97i105 Sparse multitask regression (b)(a) Fig.2.
The co-clustering procedure transforms a randomly displayed association table (a) of 50 genes and 40 phenotypes to a organized partition (b).
and the phenotypic signature under each condition, while the second term enforces sparsity on the shared template T. The constraints Pd=1 are used to prevent trivial solutions (i.e.T approaches zero and Pd s approach infinity).
Alternatively, this can be achieved by penalizing PdF with a extra regularization parameter.
More recently, a heterogeneous multitask learning framework that considers both continuous (regression) and discrete (classification) variables was successfully used to discover genetic markers that jointly influence multiple correlated traits (Yang et al., 2009).
In comparison, our method considers pure regression setting only, where the phenotypic measurements are continuous.
Formulation (3) allows us to obtain condition-specific regression matrices Td s based on a common template T. Note that for each Td , its non-zero rows signify important genes under the d-th condition.
Therefore, template T, which is shared among multiple Td s, defines a combined list of genes that are important to the phenotypes studied under these conditions.
In other words, T is an integrated association descriptor that summarizes correlating relations between genes and phenotypes under multiple conditions; and we want to read out useful structures (such as the grouped correlation between subsets of genes and subsets of phenotypes) encoded in T. To achieve this goal, we performed co-clustering analysis (Hartigan, 1972) on T. Co-clustering analysis has been used to find clusters in various tabulated data such as the co-occurrence of documents/words (Dhillon, 2001), or the expression of genes under various conditions (Ding, 2003; Kluger et al., 2003; Tanay et al., 2002), by simultaneously grouping rows and columns of the association table.
However, it has rarely been applied to interpret associations between genes and phenotypes, where the association table is not directly available from raw data but instead has to be learned.
In fact, co-clustering can reorganize regression coefficients in a perceptually meaningful manner to bring more insights into our analysis.
This is illustrated by synthetic data, as shown in Figure 2.
For example, assume we have learned an association table of 50 rows (e.g.genes) and 40 columns (e.g.phenotypes) where it is difficult to observe any meaningful structures.
However, if we permute the rows and columns of the table by co-clustering (Dhillon, 2001), we will discover four dominant correlation groups, as shown in the Figure 2B.
Such a grouping can be regarded as a distinctive watermark of the genephenotypic association.
Furthermore, rows (genes) grouped into the same block are more likely to participate together in affecting corresponding columns (phenotype responses).
In summary, the sparse multitask regression has three advantages: (i) it allows us to reduce the number of variables from O(MND) to O(NM +DM2); (ii) the sparsity of T easily transfers to those of Td s due to the simple linear relation Td =TPd ; and (iii) as we shall see, the template matrix T is a platform from which explorative analysis can be carried out in identifying important, grouped correspondences between genes and phenotypic signatures.
2.2 Optimization procedures Formulation (3) is a vector-valued regression with intrinsic T and perturbation-specific Pd s. It can be solved by an alternating optimization strategy, i.e.iteratively fixing Pd s and solving T, and then fixing T and solving Pd s. We will show that both T and Pd s subproblems are convex.
Thus a locally optimal solution of the problem (3) can always be guaranteed.
In the following, we present details on the alternating optimization (Parts I, II and III) and the co-clustering procedure (Part IV).
(I) Fix {Pd}Dd=0 and solve T: We will show that when Pd s are fixed, T can be solved through quadratic programming.
First, use the operator vec() :Rpq Rpq1 to denote the mapping that transforms a pq matrix into a pq1 vector via concatenating the columns in the matrix, and let ivec() be the inverse mapping.
Let t=vec(T)RMN1.
Then define a 3D matrix Ad RCMMN for d =0,1,2,...,D, such that Ad (i,j,:)=vec ( X0(i,:)Pd (:,j) ).
(4) Here, X0(i,:) is the i-th row in X0, Pd (:,j) the j-th column in Pd and each (i,j)-pair locates an MN 1 vector denoted by Ad (i,j,:).
Now, computing T is equivalent to the following quadratic program min tRMN1 tQt2bt+t1 (5) where Q= D d=0 C i=1 M j=1 Ad (i,j,:)Ad (i,j,:) (6) b= D d=0 C i=1 M j=1 Yd (i,j)Ad (i,j,:).
(7) It can be easily verified that the residual termD d=0X0TPd Yd2F in (3) is identical to tQt2bt up to a constant that is independent of the optimization variables.
Note that the Hessian of the above quadratic programming problem is positive semi-definite: for any xRMN1 we have xQx = D d=0 C i=1 M j=1 xAd (i,j,:)Ad (i,j,:)x = D d=0 C i=1 M j=1 ( Ad (i,j,:)x )2 0.
On the other hand, the L1 regularization term t1 is a convex function.
Therefore, the problem is convex, and there exists a unique, globally optimal solution for the subproblem (5).
The main computational barrier is that the Hessian matrix Q is MN-by-MN , which can be very large and does not fit in a modern desktop computer.
However, this matrix is symmetric, positivedefinite Hessian matrix Q and has very low rank in practice, i.e.its eigen-spectrum decays very quickly to zero.
This is shown in Figure 3, where we chose N =1210 genes and M =3 phenotypes to construct the matrix Q (6) with size 36303630.
It is clear that i99 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i100 i97i105 K.Zhang et al.Fig.3.
Spectrum of a 36303630 matrix Q, computed from our experimental data, indicates that only the largest 48 eigenvalues are strictly positive and the rest are insignificant.
The spectrum clearly reflects the lowrank nature of the matrix Q and the feasibility of low-rank approximation.
the spectrum of Q decays rapidly, with only the top 48 eigenvalues being strictly non-zero, thus substantiating the low-rank nature of the Q matrix.
As a result, the Hessian matrix can be represented by the low-rank approximation to alleviate prohibitive computational requirements.
To do this, we searched for a rank-R matrix L that best represents the Q matrix in a least square sense, minLRMNR Q LL2F , where RNM, LRMNR is a rectangular matrix with low row-rank and LL is called the rank-R approximation of Q.
This approximation QLL dramatically reduces memory usage from O(N2M2) to O(NMR).
Mathematically, the optimal rank-R matrix L is given by the eigenvectors of Q (Golub and Loan, 1996), which is computationally expensive.
We therefore pursued an approximate solution by adopting the sampling-based low-rank approximation scheme, known as the Nystrm method, which originated from the numerical treatment of integral equations of the second type (Baker, 1997).
The basic idea of the Nystrm method is to randomly sample R columns from the Q matrix, which, due to its symmetry, also corresponds to R rows.
Let E and E denote the sampled columns and its transpose, respectively, where E RMNR.
Let W RRR be the intersection of the selected rows and columns.
Then Q can be decomposed as QEW1E.
In our specific context, Q is represented as the sum of multiple outer products (6).
By utilizing this property, E and W can be computed efficiently as follows: E(p,q)= D d=0 C i=1 M j=1 Ad (i,j,p)Ad (i,j,q), W =E(I,I), 1pMN, q I, where I ={1,2,...,MN}R is the index of selected columns.
Given W and E, the low-rank approximation of Q is then expressed as QLL, where L=EW 12.
(8) As W is a positive semi-definite (PSD) matrix, there exists theoretically a real square root of W. In practice, we could encounter diminishing eigenvalues.
A robust way is to first perform the eigenvalue decomposition W =UU, remove those diminishing eigenvalues and then let W 1 2 =U 12 U.
The low-rank decomposition (8) allows us to rewrite the L1regularized quadratic programming problem (5) into a standard least square problem (with L1 regularization), min tRMN1 Ltq2 +t1.
(9) Here, qRR1 can be determined by expanding the quadratic term in (9), comparing it with (3) and requiring Lq=b.
Formulation of (9) is a good approximation to the original problem (5) and it has been widely examined in statistics, optimization and machine learning.
We use the l1-ls solver (Kim et al., 2007) for large-scale L1regularized least square problems, which are based on the truncated Newton interior-point method.
Empirically, it can solve large sparse problems with a million variables with high accuracy in a few tens of minutes on a modern desktop computer.
(II) Fix T and solve {Pd}Dd=1: By fixing T, entries of Pd s can be computed using simple scalar equations.
Let the i-th column of the matrix XT be denoted by XT(:,i) and the i-th column in Yd be Yd (:,i).
Its easy to verify that the i-th diagonal entry in Pd can be solved easily as Pd (i,i)=XT(:,i)XT(:,i)/Yd (:,i)22.
To guarantee that Pd s all have Norm 1, we will normalize them by Pd =Pd/PdF.
This can be deemed as iteratively projecting the solutions on the feasible region PdF =1.
Note that rescaling both T and Pd s with 1 does not affect the prediction performance of the multitask regression, but will reverse the signs of associations learned in T. To solve this problem, we require that the signs of the resultant matrix T should be maximally correlated with those of the standard correlation coefficients on the same set of genes.
From a practical standpoint, because Pd s are initialized with identity matrices, we have always observed that they continue to be PSD during the optimization procedure.
Empirically, our method converges rapidly in about 5 to 10 iterations on our current datasets.
(III) Initialization and parameter selection: By fixing one of the two groups of variables, T or Pd s (d =1,2,...,D), the other can be computed.
Here, we choose to initialize Pd s as identity matrices for d =1,2,...,D. Note that initialization of the Td s is usually much easier than that of T , where degrees of freedom are M2D and MN , respectively.
We used leave-one-out cross-validation to choose the hyperparameter since the sample size is very small.
This involves selecting one sample as a testing sample and the rest as training.
We repeated this process for each sample and computed the averaged predictor error on the testing sample at each grid point {103,102,101,1,10}.
(IV) Co-clustering: Template T is an intrinsic regression coefficient matrix linking the gene expression and phenotypic signature under the multiple conditions studied: the ij-th entry signifies the strength of the relationship between the i-th gene and the j-th phenotype.
To reveal the clustered structure in these associations, we used co-clustering to permute the rows and columns of T, so that the underlying saliency becomes apparent and can be visualized.
We have adopted the bipartite spectral clustering (Dhillon, 2001) for simultaneously clustering the genes and phenotypes.
Bipartite spectral clustering uses a bipartite graph where vertices are divided into two types, each from one dimension of the given contingency table (T).
In our case they are genes and phenotypes, denoted by G and P , respectively, and the number of vertices will be M +N.
The edge weights are determined by Wij = { |T(i,j)| vi G,vj P, 0 vi,vj G orvi,vj P. In other words, edges only exist between a gene vertex and a phenotype vertex.
By applying i100 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i101 i97i105 Sparse multitask regression spectral clustering on this bipartite graph, simultaneous groupings on gene and phenotype vertices can be computed.
Mathematically, we need to compute the singular value decomposition of the degreenormalized association matrix, S=D 1 2 l TD 12 r , where Dl is an N N diagonal degree matrix whose i-th entry is the summation of the i-th row in T, and Dr is a M M diagonal degree matrix whose i-th diagonal entry is the summation of the i-th column of T. Interestingly, the left and right singular vectors of S (corresponding to the second largest singular value) not only provide a partitioning of the rows and columns of T, but also provide a natural ordering (embedding) of the required row and column permutations.
3 RESULTS Our proposed method has been tested with both synthetic and experimental data.
The synthetic data is used for method validation and profiling against other known techniques.
Our studies with experimental data identified molecular predictors of cell cycle data from baseline gene expression data.
3.1 Evaluation with synthetic data In the synthetic case: (i) a data matrix X0 R50300 was created from the Gaussian distribution; (ii) a sparse intrinsic template TR3005 with 50 non-zero rows and a small set of randomly generated perturbation matrices Pd R55 were created for each d =1,2,...,D task; and (iii) the responses (e.g.target values) were then determined by Yd =X0TPd +, where is the noise term.
We examined how well the system recovers Td s, and compared the proposed method with (i) independent L1-regularized regression, and (ii) independent L2-regularized regression, also known as regularized least squares (RLS).
First, we set D=10 and selected one of the tasks to visualize the regression qualities against the competing methods.
Reconstruction results are shown in Figure 4.
Notice that the L1 and L2 regressions (Fig.4c and d) contaminated the true regression coefficients.
In practical association analysis, this can lead to a number of false predictions.
In contrast, multitask regression (Fig.4b) reliably recovered the regression coefficients.
Second, we varied D from 1 to 50 and quantified the average per-task-error for each of the three methods, as shown in Figure 5.
It is clear that the error in multitask regression decreases monotonically with the number of tasks, while the errors in pure L1 and L2 regressions remain stationary.
Although this experiment demonstrates an improved error profile for multitask learning, we have not yet designed a synthetic experiment that maintains a correlation between transcripts.
3.2 Experimental design and quantification of biological endpoints We applied our method to a set of publicly available gene expression data for a panel of breast cancer cell lines collected with Affymetrix HG-U133A (Neve et al., 2006).
We used the following 14 cell lines: MCF12A, HCC38, HCC1428, AU5650, MDAMB415, SUM185PE, ZR75B, MCF7, MDAMB361, LY2, T47D, MDAMB436, MDAMB468 and ZR751.
From the original N =22215 probe sets, we chose 5706 by removing those with a variance of <0.3.
This is slightly above the noise level of the Affymetrix U133 platform.
Notice that the gene expression data were collected under baseline (e.g.unperturbed) condition.
Our main (a) (b) (c) (d) Fig.4.
Reconstruction of the regression coefficient matrix indicates that multitask learning is more accurate when compared with L1-and L2regularized regressions.
Td is a 300-by-5 matrix and each column is represented by a unique color.
(a) Ground-truth solution, (b) Multitask regression, (c) standard L1 regression and (d) regularized least square regression.
0 10 20 30 40 50 60 70 1.2 1 0.8 0.6 0.4 0.2 0 0.2 0.4 number of tasks er ro r/ pe r ta sk ( lo g) independent L2reg independent L1reg Multitask L1reg Fig.5.
Multitask learning has an improved error rate profile as the number of tasks is increased.
hurdle has been the prohibitive cost of collecting necessary data (e.g.three conditions, 14 lines, and at least three biological replicates).
Thus, we assumed that perturbed expression data would be linearly predictable from the control data.
Cell cycle data where collected for cells exposed to three conditions: control condition (e.g.DMSO solvent alone), the MEK inhibitor CI1040 and the tyrosine kinase inhibitor Iressa.
Both these inhibitors induce cell cycle arrest, but through different mechanisms.
Each cell line was plated in triplicate and incubated for 48 h with CI1040 and Iressa at 5.6 and 4.0 M, respectively.
Subsequently, samples were fixed and stained with Hoechst and BrdU, and 25 fields of view were imaged using the Celomics high-throughput system.
These images were uploaded into the BioSig imaging bioinformatics system (Parvin et al., 2003), and then analyzed for their morphometric and BrdU incorporation on a cell-by-cell basis (Raman et al., 2007; Wen et al., 2009).
Figure 6 shows a sample of images that have been registered with the BioSig and one segmented image.
Each segmented nucleus is represented using a multidimensional feature (Han et al., 2010) and stored in the database.
In our experiment, the pertinent features are total BrdU and i101 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i102 i97i105 K.Zhang et al.(a) (b) Fig.6.
(a) Biological images are registered with BioSig and (b) each nucleus is segmented to quantify total DNA and BrdU incorporation on a cell-by-cell basis.
DNA content on a cell-by-cell basis.
By aggregating these features, within each well, percentages of cells being G1, S and G2 Phase can be quantified as a function of their treatment, as shown in Figure 7.
The main advantage of microscopy for evaluating cell cycle arrest is a significant reduction in the number of required cells.
Finally, outliers were removed.
Summary results are shown in Figure 8.
3.3 Evaluation with therapeutic agents First, we examined associations of gene expression and cell cycle data using independent L1-regularized regression that learns the regressing coefficients Td s separately for each experimental condition.
The results enabled us to contrast traditional L1 regression with multitask learning.
Predicted results are shown in Figure 9, where each subfigure corresponds to the regression matrix Td under one condition.
Here, zero rows in the regression matrix were removed, and the rows and columns of Td s have been reordered by the co-clustering procedure.
The positive and negative association Fig.7.
By aggregating total DNA and BrdU, on a cell-by-cell basis for all images in each well, the percentages of cells in G1, S, and G2 phase are quantified.
0 20 40 60 80 100 Cancel Cell Lines P er ce nt ag e of G 1 A rr es t M D A M B 436 H C C 1937 M C F 12A M D A M B 468 A U 565 LY 2 M D A M B 415 Z R 751 M C F 7 S U M 85P E Z R 75B M D A M B 361 H C C 38 H C C 1428 PBS CI1040 IRESSA Fig.8.
Percentage of each cell line being arrested in G1 phase with DMSO, CI1040, and Iressa treatment conditions.
between each genephenotype pair is encoded by green and red blocks, respectively.
Second, we applied the proposed multitask regression to learn a common template of correlation between genes and cell cycle data for the two inhibitors (e.g.CI1040 and Iressa), as shown in Figure 10.
Again, we assumed that each therapeutic reagent would perturb a small molecular region in the cell cycle progression.
In this experiment, both CI1040 and Iressa induced cell cycle arrest by targeting different molecular moieties.
However, if there is a common mechanism of action, then we would like to infer that.
We observed that the genes identified by multitask regression (Fig.10) contained subset of genes that were identified separately by independent L1 regression, shown in Figures 9b and c. i102 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i103 i97i105 Sparse multitask regression (a) (b) (c) Fig.9.
The regression matrices (a) T0, (DMSO) (b)T1 (C11040), and (c)T2 (Iressa) learned by the independent regression using 14 cell lines and reordered by co-clustering.
Fig.10.
The intrinsic template T learned by the multitask regression using 14 cell lines and the two drug conditions (CI1040 and Iressa) and reordered by co-clustering.
However, there are certain genes that can only be predicted through the multitask regression.
These are hidden markers that are relevant to the effect of the therapeutic reagent and provide potential new hypothesis for further studies.
The total computation time on a modern desktop computer is approximately 6500 s. 4 DISCUSSION Our experiments with synthetic data have clearly demonstrated that multitask learning offers the following advantages over independent L1 regression: (i) regression is less noisy; (ii) regression error is reduced as a function of the number of tasks; and (iii) hidden variables are revealed since traditional L1 regression can push non-zero coefficients to zero and vice versa.
Therefore, the bulk of the discussion in this section is devoted to the experimental data by focusing on a few important genes and their independent analysis through Ingenuity Pathway Analysis (IPA) and Pathway Studio.
(I) CLCA2 is a hidden variable that has been identified through multitask regression and is shown to be negatively associated with the S phase.
We hypothesized that CLCA2 is a common mechanism of response for inhibitors CI1040 and Iressa.
This gene is known to be downregulated in breast cancer cell lines.
In addition to being a p53 client (Gruber and Pauli, 1999), its knockdown leads to increased invasiveness (Walia et al., 2009), and it is epigenetically regulated (Li et al., 2004).
It is also a tumor suppressor gene that may be a potential target for therapy.
It is likely that CLCA2 acts as a common molecular switch to inhibit DNA synthesis and initiate apoptosis as a result of treatment with either therapeutic agent.
Therefore, it not only serves as a therapeutic target, but can also be used in combination with other therapeutic targets used today for improved lethality.
(II) NLRP2 is regulated by NFB and is shown to be expressed in MDA-MB-436 and MCF-7 (Bruey et al., 2004) breast cancer cell lines.
This particular gene appears in both independent and multitask regression.
Furthermore, the Gene Ontology annotation indicates that NLRP2 is in involved in caspase activities and apoptosis.
We hypothesized that strong G1 arrest and complementary negative correlation with cells being in S is the result of treatment with the therapeutic agent.
This particular gene is reflected in multitask regression and independent regression analysis corresponding to CI1040 and Iressa.
It is also a potential common mechanism of response for further analysis.
(III) CDKN2A (also known as p16) expression is positively associated with G1 arrest in normal cells and tissues, but is negatively associated with the S phase in our analysis of the human i103 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i104 i97i105 K.Zhang et al.Fig.11.
Interaction of CSTA with JUN and FOS curated through IPA.
breast tumor cell lines (in both the independent regression of Fig.9b and the multitask regression of Fig.10).
This discrepancy is likely explained by the fact that most of the malignant cell lines in the panel have aberrations in downstream effectors of the product of this gene.
The aberrations result in continued proliferation in the presence of p16 expression that ordinarily would yield cell cycle arrest and senescence (Gauthier et al., 2007).
(IV) CSTA is involved in apoptosis and differentiation, and is normally regulated by JUN and FOS (Takahashi et al., 1998), whose gene products together constitute the AP1 transcription factor.
AP1 drives the expression of a number of genes that are necessary for cell cycle progression.
The relationships between these proteinprotein interactions are shown in Figure 11.
This gene appears in multitask and one of the independent regression analysis.
(V) CA2 is an example of the gene that is reported by both independent association of gene expression data with CI1040 (Fig.9b) and the multitask regression analysis (Fig.10).
CA2 is ordinarily involved in differentiation and apoptosis, overexpressed in MCF7 and MDA-MB-231 and negatively correlated with the S phase in the drug-treated cells.
SiRNA-mediated interference with human CA2 gene expression has been shown to decrease survival of MDA-MB-231 cell lines (Mallory et al., 2005).
Finally, we performed an independent analysis by using Ingenuity Pathway Analysis and Pathway Studio, scientific software that helps researchers more effectively search, explore, visualize, and analyze biological and chemical findings related to genes, proteins and small molecules.
We selected the set of genes that was correlated with the S phase, and uploaded them into IPA and Pathway Studio.
The IPA analysis indicated that this group of genes is largely involved in (i) cell cycle and signaling networks and (ii) cancer.
The net result is a more substantial support for gene-by-gene analysis.
Similar results have been obtained from Pathway Studio, which provides gene set enrichment analysis (GSEA) and identifies common regulators with the user-defined number of neighbors.
Gene enrichment analysis revealed that predicted gene groups are involved in response to toxin, drug, negative regulation of cell proliferation, negative regulation of peptidase activity where S phase is one of them and apoptosis among top-ranked groups.
Furthermore, a number of common regulators with high P-values were also inferred that are associated with the cell cycle machinery.
Figure 12 shows three regulators of MAPK, Jun/Fos, and GF, and their target entities.
Fig.12.
Three common regulators that have been inferred from a subset of genes associated with the S phase.
In summary, multitask learning has the potential to summarize a vast amount of data, compute biologically relevant markers and identify hidden variables that traditional regressors may fail to capture.
Although the technique is currently applied for integration of gene expression data with cell cycle data, it can also be used for other integrative biology applications.
ACKNOWLEDGEMENTS The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products or organizations imply endorsement by the U.S. Government.
Funding: U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research (contract DE-AC0205CH11231); the National Institutes of Health (grants U54 CA112970 and CA58207).
Conflict of Interest: none declared.
ABSTRACT Motivation: There has recently been a notable shift in biomedical information extraction (IE) from relation models toward the more expressive event model, facilitated by the maturation of basic tools for biomedical text analysis and the availability of manually annotated resources.
The event model allows detailed representation of complex natural language statements and can support a number of advanced text mining applications ranging from semantic search to pathway extraction.
A recent collaborative evaluation demonstrated the potential of event extraction systems, yet there have so far been no studies of the generalization ability of the systems nor the feasibility of large-scale extraction.
Results: This study considers event-based IE at PubMed scale.
We introduce a system combining publicly available, state-of-theart methods for domain parsing, named entity recognition and event extraction, and test the system on a representative 1% sample of all PubMed citations.
We present the first evaluation of the generalization performance of event extraction systems to this scale and show that despite its computational complexity, event extraction from the entire PubMed is feasible.
We further illustrate the value of the extraction approach through a number of analyses of the extracted information.
Availability: The event detection system and extracted data are open source licensed and available at http://bionlp.utu.fi/.
Contact: jari.bjorne@utu.fi 1 INTRODUCTION In response to the explosive growth of biomedical scientific literature, there has recently been significant interest in the development of automatic methods for analyzing domain texts (Chapman and Cohen, 2009).
In the previous decade of work on automatic information extraction (IE) from biomedical publications, efforts have focused in particular on the basic task of recognizing entity mentions in text, such as gene, protein or disease names (Kim et al., 2004; Smith et al., 2008; Yeh et al., 2005) and on the extraction of simple relations of these entities, such as statements of protein protein interactions (PPI; Krallinger et al., 2008; Ndellec, 2005).
State-of-the-art IE methods frequently rely on a detailed analysis of sentence structure (parsing) (Airola et al., 2008; Miwa et al., 2009), and several studies have addressed the development and adaptation of parsing methods to biomedical domain texts (Hara et al., 2007; Lease and Charniak, 2005; McClosky, 2009; Rimell and Clark, 2009).
The research focus on biomedical text analysis has brought forth notable advances in many areas.
Automatic protein and gene Named Entity Recognition (NER) with performance exceeding 90% To whom correspondence should be addressed.
F-score1 was demonstrated to be feasible in the recent BioCreative community evaluation (Smith et al., 2008).
Similarly, significant improvement has been made in PPI extraction (Airola et al., 2008; Chowdhary et al., 2009; Miwa et al., 2009) and there is an active collaboration between database curators and method developers to integrate PPI methods into curation pipelines (Chatr-aryamontri et al., 2008).
Finally, methods for a variety of biomedical text processing tasks ranging from sentence splitting (Tomanek et al., 2007) to full parsing (McClosky, 2009) have been introduced with performance approaching or matching the performance of similar methods on general English texts.
Building on such text analysis methods, several IE systems and services have been created for retrieving interaction information from PubMed (http://www.pubmed.com).
Varying levels of parsing and other NLP methods have been used to detect biological entities of interest and their relationships.
Most previous efforts have focused on pairwise PPI, with extracted pairs often represented as merged interaction networks.
The MEDIE and InfoPubmed systems (Ohta et al., 2006) offer access to deep syntactic analysis and entity relation extraction results from the entire PubMed through subject-verb-object search patterns.
The Chilibot system looks for pairwise relationships based on co-occurrence and uses the presence of interaction keywords to type them (Chen and Sharp, 2004).
The TextMed system, based on the LYDIA project (Lloyd et al., 2005), uses shallow parsing and co-occurrence information to generate pairwise entity relationship networks from PubMed citations.
The Ali Baba system likewise visualizes relationships from PubMed abstracts as graphs (Palaga et al., 2009).
IHOP hyperlinks PubMed abstracts together through shared protein and gene mentions (Hoffmann and Valencia, 2004).
Finally, GoPubMed uses Gene Ontology (http://www.geneontology.org/) and medical subject headings (MeSH) (http://www.nlm.nih.gov/mesh/) to provide a knowledge-based search for relevant citations (Doms and Schroeder, 2005).
Supported in part by the maturation of basic technologies for biomedical text analysis and the availability of richly annotated text corpora (Kim et al., 2008; Pyysalo et al., 2007), there has recently been notable movement in the biomedical IE community toward more detailed and expressive representations of extracted information.
In particular, event representations that can capture different types of associations of arbitrary numbers of entities and events in varying roles have been applied as an alternative to the simple relation representation.
While the applicability of the results produced by IE methods employing relation representations is closely tied to the specific relation type targeted (i.e.PPI or genedisease), event representations have wider potential in supporting applications ranging from PPI to semantic search and 1F-score is the harmonic mean of (p)recision and (r)ecall, i.e.F = 2prp+r.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:17 12/5/2010 Bioinformatics-btq180.tex] Page: i383 i382i390 PubMed event extraction pathway extraction (Kim et al., 2009).
Showing wide interest in the approach, 24 teams participated in the first communitywide competitive evaluation of event extraction methods, the BioNLP09 Shared Task on Event Extraction (http://wwwtsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask/; Kim et al., 2009).
The number is comparable to the 26 teams participating in the PPI task of the established BioCreative II community evaluation (Krallinger et al., 2008).
In this study, building on the best performing system in the BioNLP09 shared task (Bjrne et al., 2009) and J.Bjrne et al.(submitted for publication) which remains competitive with even the most recent advances (Miwa et al., 2010), we join together stateof-the-art methods for biomedical text parsing, protein/gene name recognition and IE into a unified system capable of event extraction from unannotated text.
We apply this system to a random 1% sample of citations from the PubMed literature database, providing the first estimate of the results of event extraction from the entire PubMed data.
We further analyze the performance of the key components of the system, thus providing the first evaluation of the ability of state-of-the-art event extraction systems to generalize to PubMed scale.
2 SYSTEM AND METHODS The event extraction system presented in this article follows the model of the BioNLP09 Shared Task on event extraction in its representation of extracted information.
The BioNLP09 Shared Task was the first large-scale evaluation of biomedical event detection systems (Kim et al., 2009).
The task introduced an event representation and extraction task based on the GENIA event corpus annotation (Kim et al., 2008).
The primary extraction targets in the defined task are nine fundamental biomolecular event types (Table 1) and the participants in these events.
In this article, the term event refers to events as defined by the Shared Task annotation scheme.
Several aspects of the event representation differentiate the event extraction task from the body of domain IE studies targeting, e.g.PPI and genedisease relations, including previous domain shared tasks (Krallinger et al., 2008; Ndellec, 2005).
While domain IE has largely focused on a relation model representing extracted information as entity pairs, the event model allows for a more expressive way of capturing statement semantics.
Events can have an arbitrary number of participants whose roles in the Table 1.
Targeted event types with brief example statements expressing an event of each type Event type Example Gene expression 5-LOX is expressed in leukocytes Transcription promoter associated with IL-4 gene transcription Localization phosphorylation and nuclear translocation of STAT6 Protein catabolism I kappa B-alpha proteolysis by phosphorylation.
Phosphorylation BCL-2 was phosphorylated at the G(2)/M phase Binding Bcl-w forms complexes with Bax and Bak Regulation c-Met expression is regulated by Mitf Positive regulation IL-12 induced STAT4 binding Negative regulation DN-Rac suppressed NFAT activation In the examples, the word or words marked as triggering the presence of the event are shown in italics and event participants underlined.
The event types are grouped by event participants, with the first five types taking one theme, binding events multiple themes and the regulation types theme and cause participants.
event (e.g.theme or cause) are specified, making it possible to capture n-ary associations and statements where some participants occur in varying roles or are only occasionally mentioned.
Further, events are modeled as primary objects of annotation and bound to specific statements in text (triggers), allowing events to participate in other events and facilitating further analysis such as the identification of events stated in a negated or speculative context.
Finally, events following the Shared Task model are given GENIA Event ontology types drawn from the community-standard Gene Ontology, giving each event well-defined semantics.
Using events to represent information contained in natural language sentences, for the first time it is now possible to accurately describe in a formal fashion the multitude of different biological phenomena depicted in research articles.
In the rest of this section, we describe the steps of the event extraction pipeline (Fig.1).
2.1 Named entity recognition NER is a fundamental requirement for IE: the analysis of IE systems normally takes the form of associations between references to entities, and most applications require the references to be sufficiently specific to identify particular real-world entities, i.e.entity names.
Consequently, NER is a well-studied subtask in IE.
Also in the biomedical domain NER has been A B C D E Fig.1.
Event extraction.
A multiphased system is used to generate an event graph, a formal representation for the semantic content of the sentence.
Before event detection, sentences are parsed (A) to generate a suitable syntactic graph to be used in detecting semantic relationships.
Event detection starts with identification of named entities (B) with BANNER (parses are not used at this step).
Once named entities have been identified, the trigger detector (C) uses them and the parse for predicting triggers, words which define potential events.
The edge detector (D) predicts relationship edges (event arguments) between triggers and named entities.
Finally, the resulting semantic graph is divided into individual events by (E) duplicating trigger nodes and regrouping argument edges.
i383 [12:17 12/5/2010 Bioinformatics-btq180.tex] Page: i384 i382i390 J.Bjrne et al.Table 2.
NER system performance System Corpus F-score
ABSTRACT Motivation: Histone acetylation (HAc) is associated with open chromatin, and HAc has been shown to facilitate transcription factor (TF) binding in mammalian cells.
In the innate immune system context, epigenetic studies strongly implicate HAc in the transcriptional response of activated macrophages.
We hypothesized that using data from large-scale sequencing of a HAc chromatin immunoprecipitation assay (ChIP-Seq) would improve the performance of computational prediction of binding locations of TFs mediating the response to a signaling event, namely, macrophage activation.
Results: We tested this hypothesis using a multi-evidence approach for predicting binding sites.
As a training/test dataset, we used ChIPSeq-derived TF binding site locations for five TFs in activated murine macrophages.
Our model combined TF binding site motif scanning with evidence from sequence-based sources and from HAc ChIPSeq data, using a weighted sum of thresholded scores.
We find that using HAc data significantly improves the performance of motifbased TF binding site prediction.
Furthermore, we find that within regions of high HAc, local minima of the HAc ChIP-Seq signal are particularly strongly correlated with TF binding locations.
Our model, using motif scanning and HAc local minima, improves the sensitivity for TF binding site prediction by 50% over a model based on motif scanning alone, at a false positive rate cutoff of 0.01.
Availability: The data and software source code for model training and validation are freely available online atContact: aderem@systemsbiology.org; ishmulevich@systemsbiology.org Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 27, 2010; revised on June 30, 2010; accepted on July 3, 2010 1 INTRODUCTION Mammalian cells exhibit diverse transcriptional profiles across different cell types and conditions, for example, in immune cells activated with different pathogen-associated molecules (Ramsey et al., 2008).
To a large extent, these profiles are controlled by the arrangement and chromatin accessibility of cis-regulatory To whom correspondence should be addressed.
elements (Berger, 2007).
Transcription factors (TFs) bind specific sequence elements in chromatin locations of permissive epigenetic or conformational states, leading to activation or repression of transcriptional activity.
For mapping these regulatory interactions, it is particularly promising that the binding of a TF can now be measured genome wide using chromatin immunoprecipitation (IP) with sequence detection (ChIP-Seq, see Johnson et al., 2007).
However, antibody and cellular material requirements preclude using ChIP-Seq to screen for all TFs mediating a transcriptional response.
There remains a need for computational approaches that can, in the absence of experimental TF binding data, leverage transcriptional data and genomic information to identify the network of TFs and binding sites that underlies a transcriptional response.
An important tool for predicting mammalian TF binding sites is motif scanning, i.e.searching DNA sequence for matches within a library of sequence motifs reported to be bound by specific TFs (Lhdesmki et al., 2008).
Such a library enables mapping between a scanning-identified sequence element and one or more candidate TFs that may bind it.
However, such motifs are often highly uncertain and they can be degenerate, leading to a high frequency of false positive predictions (Hannenhalli, 2008).
Furthermore, mammalian cis-regulatory elements can be tens of kilobases from transcription start sites, necessitating searching large sequence regions and further increasing false positives.
These issues undermine the performance of motif scanning as a standalone approach.
Successful motifbased prediction of TF binding depends on identifying the sequence regions, within the relevant cell type, that are likely to contain cisregulatory elements (Ernst et al., 2010; Wasserman and Sandelin, 2004; Whitington et al., 2009).
It has been observed that cis-regulatory elements tend to cooccur with chromatin or sequence features that can be grouped in three categories: (i) chromatin structural features such as DNase I hypersensitive sites; (ii) epigenetic marks such as histone acetylation (HAc); and (iii) sequence features such as high GC content and conservation across species.
The HAc mark, which has been associated with active promoters and open chromatin (VetteseDadey et al., 1996), is of particular relevance to transcriptional regulation because the modification can be placed or removed in response to the cellular state.
These observations have spurred the development of approaches that integrate data for multiple types of chromatin features to improve the accuracy of TF binding site predictions.
Various data integration frameworks for binding site prediction have been used, including the support vector machine (Holloway et al., 2005; Nykter et al., 2009), probabilistic methods The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:29 12/8/2010 Bioinformatics-btq405.tex] Page: 2072 20712075 S.A.Ramsey et al.A B Fig.1.
Local minima in the HAc ChIP signal correlate with TF binding.
(A) A 12 kbp region of mouse chromosome 11 including the gene Ccl5 [an LPSregulated cytokine with multiple NFB (nuclear factor of kappa light polypeptide gene enhancer in B-cells) sites in its promoter] and its upstream regulatory region, with TF binding data and various feature tracks.
Measured binding locations for the TF NFB/p50 are shown in orange (top row).
Each feature track is displayed in colored vertical bars interspersed every 100 bp: HAc ChIP-Seq signal (magenta); valley scores (VS) for HAc local minima (blue); normalized NFB binding site motif match scores (red); DNA sequence GC content (green); nucleosome occupancy score (green); and vertebrate conservation score for genomic sequence (green).
The NFB binding sites correspond to local minima in the HAc ChIP-Seq signal.
Inset: Within a local minimum of the ChIP-Seq signal (magenta curve), the smaller of the maximum signal values on either side of the local minimum is computed, and the entire local minimum region is assigned that value as its VS, and the value zero outside the local minimum region (blue lines) (Supplementary Section S1.6).
(B) The distribution of HAc ChIP VS from stimulated cells in TF-bound sites differs substantially from non-TF-bound sites, as shown in the two histograms (note the logarithmic vertical scale).
(Beyer et al., 2006; Ernst et al., 2010; Lhdesmki et al., 2008), and a kernel-based classifier (Wang et al., 2009).
Early studies integrating genomic data into binding site prediction were carried out in yeast (Beyer et al., 2006; Holloway et al., 2005), or in mammals using ground-truth datasets that were not cell type-specific (Lhdesmki et al., 2008).
More recent approaches have used cell type-specific mammalian epigenetic or transcriptional data to predict binding for a single TF in mammals (Nykter et al., 2009; Wang et al., 2009).
Other recent studies have used genome-wide datasets for multiple TFs to develop prediction models without directly incorporating epigenetic data into the model (Won et al., 2009; Zhou et al., 2010).
Two recent studies incorporated histone methylation ChIP-Seq data into multi-evidence prediction models, using ChIP-derived ground-truth datasets of 10 and 13 TFs, respectively (Whitington et al., 2009; Won et al., 2010).
Whitington et al.found that predictions are improved when the methylation data are derived from the same tissue type from which the TF binding site measurements are derived.
In this study, we investigated the hypothesis that incorporating HAc ChIP-Seq data into a multi-evidence, motif scanning-based model can improve TF binding site predictions.
We further studied whether prediction performance is improved when the HAc data are derived from the same cell condition from which the TF binding data (used for evaluating performance) are derived.
Having observed that TF binding locations frequently occur at local minima of HAc ChIP-Seq signal within regions of high HAc (see Fig.1 and Supplementary Fig.S1), we also studied the predictive utility of valley scores (VS) assigned to local minima of the HAc ChIPSeq signal.
Following our previous investigation of the regulatory network underlying macrophage activation (Ramsey et al., 2008), this study was carried out using TF binding and HAc measurements in the macrophage, a key cell type of the innate immune system.
When activated by exposure to a pathogen-associated molecule such as lipopolysaccharide (LPS), the macrophage undergoes extensive transcriptional reprogramming that is mediated in part by alterations in HAc (Aung et al., 2006).
2 APPROACH The HAc hypothesis was tested using an integrative TF binding site (TFBS) prediction framework and using an approach designed to estimate the performance that the prediction model would have on a novel TF for which only a binding site motif is available.
As features, the framework used motif scanning data (Supplementary Fig.S2) along with subsets of seven non-TF-specific features selected for their potential association with TFBSs.
As shown in Figure 1A and Supplementary Table S1, the features consisted of HAc (acetylated H4) ChIP-Seq data from activated and non-activated macrophages; VS derived from the HAc data; and three features based on genomic sequence (GC content, vertebrate species conservation and a nucleosome occupancy prediction score).
A peak in the HAc VS signal corresponds to a local minimum in the HAc ChIP-Seq signal.
As a ground-truth TFBS dataset, we used ChIP-Seq data, from activated macrophages, for five TFs (Supplementary Table S2).
In keeping with the study goals, the ChIP-Seq data were not used to improve the motifs, and model performance was tested using binding data for a TF that was not used in the model training.
Performance measurements obtained using such a TF-based cross-validation are, in our view, more relevant to this application (library-based motif 2072 [09:29 12/8/2010 Bioinformatics-btq405.tex] Page: 2073 20712075 Ac-Histone data improve binding site prediction scanning) than are results from chromosome-based cross-validation.
Importantly, HAc was measured in LPS-stimulated macrophages, consistent with the conditions for TF binding measurements.
TF binding site predictions were made in adjacent 100 bp intervals spanning 10 kb promoter regions of genes that are expressed in murine macrophages.
A value for each TF prediction feature was computed within each 100 bp interval, from the features raw data.
A weighted, thresholded linear model class was used to combine the motif scanning feature with zero, one or two additional features to predict binding for the five TFs.
This model class divides the range of each features values into three regimes: below minimum threshold (value changes within this regime are not informative), above the maximum threshold (saturated; changes are also not informative) and within the linear response range.
Fifteen models, each using a different combination of features (Supplementary Table S3), were trained using the ground-truth dataset.
Because the model using HAc ChIP VS performed the best among the two-feature models, the three-feature model analysis was restricted to models with motifs, HAc ChIP VS and one additional feature.
For training, each models parameters were optimized to maximize the average prediction performance for a set of four TFs, with the performance metric being the area under the sensitivity versus false positive rate (FPR) curve.
The performance of the model, with the best parameter set from the training, was then tested on the fifth TF, and averaged over the leave-one-out cross-validation.
3 METHODS Complete methods are described in Supplementary Material, Section S1.
Ground-truth dataset: ChIP-Seq assays were performed for the TFs ATF3, C/EBP, IRF1, NFB/p50 and NFB/p65 in macrophages activated through treatment with purified Toll-like receptor agonists for 16 h (see Supplementary Table S2 and Section S1.4).
Binding locations were identified from above-threshold locations in the ChIP-Seq signal, as described in Supplementary Section S1.7.
Prediction features: TF predictions were made in 100 bp intervals (as used in Won et al., 2010) of transcript-proximal regions comprising 7% of the genome, selected as described in Supplementary Section S1.2.
Combinations of eight features, individually listed in Supplementary Table S1 and labeled by index f , were used for TF binding prediction.
Feature f =1, which conferred TF specificity to the predictions, was based on motif scanning.
For each TF, motif position-weight matrices (PWMs) corresponding to the TF were obtained from TRANSFAC (Supplementary Table S2 and Section S1.3).
Sequences were scanned for motif matches using a likelihoodbased algorithm (Lhdesmki et al., 2008), and combined to obtain, within each interval and for each TF, a score representing the strength of the best match for any motif corresponding to that TF, at any position within the interval.
Features 25 of Supplementary Table S1 were derived from HAc ChIP-Seq assays of unstimulated macrophages or macrophages stimulated for 1, 4 or 6 h with LPS (Supplementary Sections S1.41.5).
VS for HAc local minima were computed as described in Supplementary Section S1.6.
Features 68 were based on genomic sequence, and thus are not macrophage specific.
For the stimulated-cell HAc ChIP-Seq features (Supplementary Table S1, rows 2 and 4), the time point for the HAc dataset that was used was always the same as the time point of the ground-truth dataset for the TF for which predictions were being made.
Prediction model: within each interval i, the model integrates a set F of up to three features (always including the motif feature, f =1) by a weighted sum of thresholded feature values.
Feature values may depend on the TF t, as is the case for motif scanning, or on the cellular condition for which TF binding predictions are being made (as is the case for HAc-derived features).
The value for feature f at interval i and TF t is therefore denoted by vfit.
The feature value vfit is passed through a piecewise-linear function f that is defined by feature-specific thresholds f and f , f (v)= 0, v<f , (vf )/(f f ), f vf , 1, v>f.
(1) The prediction score it that the TF t binds within interval i is obtained by a weighted sum of thresholded contributions, but with a multiplicative factor enforcing a minimum TF-specific motif match value for a non-zero it , it =(1(v1it)) ( f F f f (vfit) ) , (2) where the weight vector has unit L1 norm (a negative component would represent a feature that is anti-correlated with TF binding), and where is defined by (x)=0 if x0 and (x)=1 if x>0.
Importantly, a given model instance M, defined by the tuple {F,, , }, is TF independent.
Performance metric: for a given model M, TF t, and prediction score cutoff , the set of intervals (,t) for which it were predicted to contain binding sites for t (remaining intervals were predicted to have no t binding).
The set of intervals containing ground-truth binding sites (based on ChIPSeq) is denoted by (t).
Because the typical ChIP-Seq fragment size was 160 bp, some TF binding locations appeared as adjacent intervals in (t); these were counted as single binding sites.
The number of ground-truth binding sites B(t) was counted (Supplementary Table S2), and the fraction of these binding sites that coincided with at least one interval i(,t), was computed as the sensitivity S(,t).
The FPR E(,t) was computed by dividing the number of intervals in the set difference (,t)\(t) by the number of intervals not contained in (t).
The cutoff was varied and the resulting (E(,t),S(,t)) function [receiver operating characteristic (ROC) curve] was numerically integrated over the range 0<E 0.01 to obtain the TF-specific performance score A(t).
For model training (Supplementary Section S1.12), the cost function used was C(t)=1A(t)/0.01.
During training, cases where it was not possible to obtain a sufficient number of (S,E) samples were handled using a penalty, as described in Supplementary Section S1.11.
Model training: groups of four TFs at a time were selected for model training, and for a given model M, the cost was averaged over the four TFs, C =C(t)t. Model parameters were varied to minimize C subject to constraints on , and , using a two-stage optimization process (Supplementary Section S1.12), to obtain the best parameter set for the model with features F. Model testing: for both training and testing purposes, the performance A(t) of the model with the best parameter set from the training, was measured on the fifth TF t using leave-one-out cross-validation.
The five values for A(t) were compared between different feature groups F using a paired t-test, and summarized in terms of the mean and SD (Supplementary Table S3).
4 RESULTS Feature distributions: first, the TF specificity of the motif scanning was investigated.
Across all five TFs, the motif scanning score distribution from TFBSs was significantly higher than the distribution from non-binding sites (Supplementary Fig.S2).
Next, HAc VS representing local minima were computed, and the distributions of VS at TFBSs and non-binding sites were compared.
In LPS-stimulated cells, HAc VS were significantly higher at TFBSs than at non-binding sites (Fig.1B); this motivated the use of HAc ChIP data to improve predictions.
Furthermore, LPS-dependent TFBSs were correlated with LPS-inducible HAc local minima (Supplementary Table S4 and Fig.S3).
Model performance: first, two-feature models (motifs plus one other feature) were compared with a motifs-only reference model.
Based on the area under the sensitivity versus FPR curve (Fig.2, Supplementary Fig.S4 and Table S3), the model with HAc VS from 2073 [09:29 12/8/2010 Bioinformatics-btq405.tex] Page: 2074 20712075 S.A.Ramsey et al.A B Fig.2.
HAc data improve motif scanning-based TFBS predictions.
(A) Prediction performance (area under the sensitivity versus FPR curve, or ROC curve) for models with motif scanning and one additional feature, and a motifs-only reference model (data for models with three features are shown in Supplementary Fig.S4).
Larger bar values correspond to better cross-validation-average performance on the test dataset.
The performance for the reference model is shown in the blue bar (and vertical dotted line), and a random model is shown as a negative control (black bar).
The motifs-only model outperformed the random model, 27-fold.
Each green bar represents a model that used motif information plus a specific sequence-based feature (GC content, etc.).
Each cyan bar represents a model that used motif information plus a HAc ChIP-Seq-based feature (Supplementary Table S3).
Each error bar represents the cross-validation-wide SD of the performance difference between the indicated model and the reference model (Section 3).
P<0.05; P<0.001.
For the cyan bars, a dashed border indicates that HAc data are from LPS-stimulated cells; a solid border means the HAc data were from unstimulated cells.
In the top two bar labels, VS stands for the valley score for local minima in the HAc ChIP-Seq signal.
(B) ROC curves, for predictions by the models shown in (A) (see Supplementary Fig.S4 for the complete FPR range).
The model with HAc VS (from stimulated cells; gray curve) outperforms the other models.
ROC curves were obtained by varying the prediction score cutoff (Section 3).
The lack of improvement for the nucleosome occupancy-based model is consistent with the very weak association between this feature and TF binding (Supplementary Fig.S6).
stimulated cells had the highest performance improvement relative to the reference model (52% increase, P < 103).
The HAc ChIPSeq signal also improved prediction performance (by 14%), but the improvement was highly variable from TF to TF (coefficient of variation = 27%; see Supplementary Table S5).
The model using the stimulated-cell HAc VS also outperformed the unstimulatedcell HAc VS data (by 31%, P<0.01).
In contrast to the HAc ChIP-derived datasets, the three genomic features (GC content, conservation and nucleosome occupancy score) did not substantially improve prediction performance.
However, the improvements due to GC content (5% increase) and conservation (3%) were more consistent from TF to TF, and thus in both cases were statistically significant (P<0.05).
Next, models with motifs plus two other features were compared with the best previous model (motifs + HAc VS).
None of the models gave a statistically significant improvement over the best two-feature model (Supplementary Fig.S5).
These findings suggest that more TF binding data would be required to discriminate prediction performances of three-feature models.
5 CONCLUSIONS Using cell type-specific HAc ChIP-Seq data improves motif scanning-based prediction of TFBSs in primary macrophages.
This prediction strategy could be applied to any cell type in which HAc can be globally measured.
Overall, these findings suggest that within histone-acetylated regions, local minima of HAc ChIP-Seq signal may indicate sites of active transcriptional regulation.
ACKNOWLEDGEMENTS We thank K. Deutsch, S. Bloom and M. Gundapuneni for technical assistance.
H. Lhdesmki and M. Nykter kindly provided some MATLAB functions.
S.A.R.
thanks A. Diercks, E. Fu and V. Thorsson for helpful discussions.
We thank A. Nachman, B. Marzolf, D. Rodriguez and L. Rowen for coordinating the contributions of their groups.
Funding: The National Heart, Lung, and Blood Institute (K25HL098807 to S.A.R.
); the National Institute of Allergy and Infectious Diseases (HHSN272200700038C); and the National Institute of General Medical Sciences (R01GM072855 to I.S.
and P50GM076547).
Conflict of Interest: none declared.
ABSTRACT Motivation: The prediction and annotation of the genomic regions involved in gene expression has been largely explored.
Most of the energy has been devoted to the development of approaches that detect transcription start sites, leaving the identification of regulatory regions and their functional transcription factor binding sites (TFBSs) largely unexplored and with important quantitative and qualitative methodological gaps.
Results: We have developed ReLA (for REgulatory region Local Alignment tool), a unique tool optimized with the SmithWaterman algorithm that allows local searches of conserved TFBS clusters and the detection of regulatory regions proximal to genes and enhancer regions.
ReLAs performance shows specificities of 81 and 50% when tested on experimentally validated proximal regulatory regions and enhancers, respectively.
Availability: The source code of ReLAs is freely available and can be remotely used through our web server under http://www.bsc.es/ cg/rela.
Contact: david.torrents@bsc.es Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 28, 2011; revised on December 19, 2011; accepted on January 9, 2012 1 INTRODUCTION The identification of the genomic regions that control the transcription of genes still remains a challenge despite the recent and continuous development of new experimental and computational methodologies (Tompa et al., 2005).
Multiple automatic approaches have been proposed, ranging from those that search for phylogenetic conservation of sequence or transcription factor binding motifs in non-transcribed DNA regions (Blanchette and Tompa, 2003; Van Loo and Marynen, 2009) to the analysis of DNA physical properties characteristic of regions expected to bind transcription factors To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
(Abeel et al., 2008; Goi et al., 2007).
However, the incorporation of novel biological knowledge into these programs is not necessarily improving the quality of their predictions, which still contain a substantial fraction of false positives.
Currently, methods that de novo detect and characterize proximal regulatory regions show specificity levels <50% (Van Loo and Marynen, 2009).
Even though phylogenetic footprinting using prealigned homologous regulatory regions offers promising results in the identification of Conserved Regulatory Modules (CRMs) of transcription factor binding sites (TFBSs) (Blanchette and Tompa, 2003; Blanco et al., 2007; Pavesi et al., 2007; Sebestyen et al., 2009; Tokovenko et al., 2009; Tonon et al., 2010), they cannot define the regulatory region itself in most real scenarios because they are based on global alignment strategies and require that all matching binding sites across all compared sequences are located in the same (or similar) position within each sequence, i.e.they require predefined and pre-aligned regulatory regions.
As a result, in spite of the existing methodologies, still the most common and reliable way to identify proximal regulatory regions in genomes is the analysis of a few nucleotides (typically up to 1000) immediately upstream of annotated transcription start sites (TSSs), which likely constitutes the proximal promoter.
But the annotation of gene starts is still unsolved, particularly for non-human species.
For example, a simple search in the ENSEMBL database (Hubbard et al., 2009) identified substantial fractions of vertebrate genes without annotated 5UTR (from 17% in mouse to 91% in opossum, 42% for human).
This result is even more dramatic within invertebrates.
Other problems that constitute a barrier for the automatic inference of promoters (even in human or mouse) are the abundance and overprediction of alternative transcripts.
Taken together, most computational methods that detect or align promoters strongly depend on or are coupled with the annotation of untranslated gene regions, which is generally insufficient for this purpose (Guigo et al., 2006).
On the other hand, the computational identification of enhancers is even more complex.
These regulatory regions that work in cooperation with promoters throughout multiple structural constraints are, apparently, delocalized relative to the genes that are controlling (Arnosti and Kulkarni, 2005).
Therefore, their identification through computational methods requires strategies The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
MANUSCRIPT CATEGORY: ORIGINAL PAPER [18:56 25/2/2012 Bioinformatics-bts024.tex] Page: 764 763770 S.Gonzlez et al.based on local alignments.
Some existing methods, like rVISTA, look for conserved TFBS clusters between regions that have been pre-aligned with local alignment tools, such as BLASTz in rVISTA (Loots and Ovcharenko, 2004), while others use directly local alignment-based search strategies, like the Enhancer Element Locator (EEL) that uses the SmithWaterman algorithm (Palin et al., 2006).
These tools have shown good prediction rates on enhancers, and also show important limitations regarding the number of species that they can analyse, the required parametrization and the accuracy of the prediction.
To overcome these limitations, and to provide novel and improved solutions to the prediction of regulatory regions, we have developed ReLA, a public local-based alignment tool that is capable of detecting promoters and enhancers by identifying clusters of regulatory motifs conserved in any position within large homologous DNA regions (i.e.independently of gene annotation).
Considering the wide range of potential users of this tool, we have also developed a user-friendly web server for remote predictions.
The source code of ReLA is distributed also as a standalone program that can be used locally in Unix-based computational platforms.
2 MATERIALS AND METHODS 2.1 From DNA to TFBS sequences The first step of our method consists on the mapping of putative TFBSs sequences along the input sequences according to a certain catalogue of position frequency matrices (PFMs) (in the documentation included with the program and in the web server, we provide guidance for selecting a set of homologous sequences).
Users locally running ReLA should provide their own PFM files (information about accepted file formats is also provided with the program and in the web server).
It has been shown that the selection of particular collections of matrices yields slightly different results (Blanco et al., 2006b).
After evaluating different options (data not shown), we obtained the optimal results by using PFMs from TRANSFAC (Matys et al., 2006).
We classified this collection of models into three subsets: whole collection of TRANSFAC PFM, the first 600 and the first 400 PFMs ranked by their information content.
These three sets are included as the default option in the web server.
The identification of potential TFBSs is performed using the MATSCAN software (Blanco et al., 2006b) at high levels of stringency: 80% of similarity threshold.
For this study, we calculate the similarity score using SS = [(current min)/(max min)], where, current is the actual matching score, min is the minimum possible matching score and max, the maximum possible matching score of a particular PFM, as described elsewhere (Kel et al., 2003).
Since the next SmithWaterman step requires a single sequence of TFBSs, and because MATSCAN results usually contain PFM that overlap in all possible ways, we next simplify this output.
We remove this overlap by sliding a window of n bp (n=3 or 5, both conditions are included in the global search, see below).
For each overlapping PFM starting at the first position of each of the 3 or 5 nt sliding window, we systematically kept the most informative one, maximizing the overall information content of the sequence.
To preserve the relative distance between motifs during the comparison, we insert a spacer box every n consecutive nucleotides in regions free of predictions.
In summary, we convert each input DNA sequence into a sequence of highly informative non-overlapping TFBS, which is used next in the comparative searches.
2.2 Comparative searches For our searches, we have modified the classical local alignment Smith Waterman algorithm (Smith and Waterman, 1981) to deal with sequences of TFBSs (associating a unique three-letter combination to each TFBS) and to provide the best scoring local alignment (i.e with the highest density of conserved TFBSs) for each of the comparisons performed between the reference sequence with each of the others (see pseudocode in Supplementary Material).
The overall stringency of the searches, the reliability of the resulting predictions and the conservation of the TFBSs between the input sequences can produce different predictions.
Instead of selecting a universal and fixed set of parameters for each of the searches, which would yield one unique prediction, we chose to run recursively each pairwise comparison (reference sequence against each of the other input sequences) with a different set of parameters generating a collection of preliminary predictions.
A set of posterior selection filters (see below) is then applied on these preliminary predictions to come up with a final prediction of the promoter region.
Each of these pair-wise comparisons is carried out in two different scoring scenarios (10/1 and 20/1 match/mismatch scores, both with an open and extension gap penalties of 2 and with two overlapping thresholds to remove redundant sites (using a window of three or five nucleotides, see above); i.e.a total of four comparisons are performed on each pair of sequences and each set of matrices defined.
These specific combinations of parameters were determined by monitoring and maximizing the relationship between sensitivity and specificity using a collection of 10 known promoters of the ABS database (Blanco et al., 2006a) (see Supplementary Material and www.bsc.es/cg/rela/downloads).
These 10 regions were excluded from the performance evaluation.
We also observed that the best predictions obtained during the benchmarking were those with sizes between 200 and 600 nt.
Preliminary predictions covering shorter regions usually involved too few conserved TFBSs, while larger predictions tend to connect distant and, apparently, unrelated binding sites.
For this reason, preliminary predictions outside this range of sizes are not considered during the generation of the final prediction.
2.3 Output generation As part of the results, ReLA generates a graph showing the distribution of all accepted preliminary predictions on the reference sequence.
From the analysis of the overlap among these preliminary predictions, we generate the final prediction by selecting the region, between 200 and 1000 nt long that contains the highest number of preliminary candidate predictions.
Together with the final prediction on the reference sequence, additional consensus regions are also defined in each of the other sequences, which correspond to those (if any) that match the final predicted promoter region.
We also provide the list of all conserved TFBSs.
From this list, a subset of the most conserved TFBSs (specifically, those within the top 10% of conservation) is selected and used to generate a multiple alignment in graphical format.
2.4 Web server We have implemented ReLA as a web server that can be accessed at http://www.bsc.es/cg/rela.
The underlying search engine is distributed also as a standalone program.
We have designed the ReLA website to meet the requirements of non-expert users.
The web version provides a graphical representation of the putative promoter region predicted in all the input sequences (Fig.5) and a plain text description of the results.
As many as two suboptimal solutions can be provided through the web on each set of input sequences to potentially predict alternative promoters.
2.5 Selection of databases for evaluation To validate the results obtained and to compare our method with other existing programs in similar searching conditions, three different working subsets or reported regulatory regions were generated from three different databases: Annotated regulatory Binding Sites database [ABS; (Blanco et al., 2006a)], Eukaryotic Promoter Database [EPD; (Schmid et al., 2006)] and Vista Enhancer Database Browser [VISTA; (Visel et al., 2007)] To follow common criteria and to be consistent with the annotation of ABS (as 500 nt promoters), we transformed these TSS into regions by considering as promoter region 500 bp upstream from the EPD TSS.
VISTA Enhancer 764 MANUSCRIPT CATEGORY: ORIGINAL PAPER [18:56 25/2/2012 Bioinformatics-bts024.tex] Page: 765 763770 ReLA Browser is a database of regions containing experimentally validated human and mouse enhancers tested in transgenic mice.
The working subsets were generated according to three different filters to facilitate the automation of the validation process and to ensure reliability of the evaluation protocol: (i) genes associated to selected regulatory regions must have at least three orthologous one-to-one genes according to ENSEMBL orthology data; (ii) the promoter fragments selected should not overlap with coding regions; and (iii) they have to be in our scanned region: as described in the Section 3, for ABS and EPD it is 500 bp upstream of the first codon of the gene, and for Vista, 50000 bp upstream of the closest gene (see below).
Applying these three filters we obtained 75 human and mouse promoters from ABS, and 740 from EPD.
In both cases, 5000 bp upstream from the first methionine were used to check and compare the accuracy of the method.
From VISTA Enhancer Browser, we ended up with 44 enhancers laying in the 5000 bp upstream of a known gene.
2.6 Evaluation protocol For the evaluation of the promoter prediction programs, we followed a modified version of the Distance-based validation evaluation protocol from Abeel et al.(2009).
Taking into account that we were evaluating promoter genes and we were considering distances, we calculated recall and precision values as Precision=correct predictions/total predictions Recall=discovered genes/total genes For those programs that provide single positions as outputs (TSS predictors, ARTS, Eponine and Promoter Explorer), we considered the sequence 500 from TSS for the evaluation.
In the case of TFM, we obtained conserved binding sites as result and considered the fragment between the first and the last one for evaluation.
A correct prediction was considered if there was an overlap between the prediction and the 500 bp upstream of the defined TSS.
For all programs, we considered the unique or the best prediction, except for Promoter Explorer that does not rank the results.
Since we were using already filtered promoter genes instead of big DNA fragments, we did not discarded any prediction further of 500 nt from the TSS as it is done elsewhere (Abeel et al., 2009).
For all programs of our evaluation, we used default settings defined by the corresponding developers.
For EEL runs, we used the mouse and human sequences for each of the orthologous groups and applied the parameters described for this species pair elsewhere (Palin et al., 2006).
All the data used for the validation procedure are available at www.bsc.es/cg/rela/downloads.
3 RESULTS 3.1 Rationale and underlying search strategy of ReLA The goal of this study is to develop a novel methodology that would overcome the current limitations mentioned above by focusing on: (i) the detection of conserved TFBS; (ii) the use of local search strategies; (iii) simplicity of use; and (iv) a low computational cost to perform genome-wide searches.
For this, we decided to use the same strategies that have been used for fast local sequence comparisons of protein sequences.
In particular, we adapted the SmithWaterman algorithm (Smith and Waterman, 1981) to make it capable of comparing and detecting the best optimal local alignment of regions with similar sequences of TFBSs.
In our procedure, each TFBS is internally transformed into symbols of an arbitrary alphabet, as if they were amino acid or nucleotides in traditional proteinprotein and DNADNA comparative searches.
This search algorithm is the core of a pipeline, referred from now on as ReLA (for REgulatory region Local Alignment tool).
The complete procedure can be divided into three major steps.
First, input DNA sequences are transformed into sequences of TFBSs by mapping with the MATSCAN software (Blanco et al., 2006b) all the PFMs provided; secondly, the resulting TFBS sequences are compared with each other to identify conserved groups of TFBSs using the modified SmithWaterman algorithm under different scoring scenarios.
Finally, all the resulting preliminary alignments are evaluated to produce the final prediction of the promoter region.
3.2 Evaluation of ReLAs performance We first applied ReLA to a collection of experimentally validated promoter and enhancer regions, both to define its internal search parameters and to evaluate its performance.
Despite ongoing efforts of acquiring experimental data, on functional TFBS, still the vast majority of detailed and reliable data can only be retrieved from the literature.
In this direction, the ABS database (Blanco et al., 2006a) is the result of one of the few initiatives to gather, from the literature, promoters with two or more of their TFBSs experimentally validated.
For this reason we used this database for ReLAs evaluation.
We selected the subset of 73 (35 human and 38 mouse) promoters from this database that showed, in ENSEMBL (Hubbard et al., 2009), one-to-one orthologous relationship with at least three out of seven chosen vertebrate species (human or mouse, rat, horse, dog, cow, opossum and chicken).
By reproducing a common and realistic search scenario, where the TSS and 5UTRs of query homologous regions are not known, we collected the putative upstream region of these genes and their corresponding orthologous regions.
These upstream regions comprise 5000 bp upstream DNA, from the first annotated codon according to the encoded ENSEMBL protein.
From the measurement of the length of 5UTRs regions of known ENSEMBL genes, we previously had estimated that this selection of 5000 bp is sufficient to capture the proximal promoters for >85% of known vertebrate genes (data not shown).
In addition, we have also compared the resulting performance of ReLA with the prediction ratio of other reported TFBS-based search tools: TFM (Tonon et al., 2010) and rVISTA (Loots and Ovcharenko, 2004), as well as with that of TSS predictors: ARTS (Sonnenburg et al., 2006); Eponine (Down and Hubbard, 2002) and PromoterExplorer (Xie et al., 2006), all of them run on the same regions (Table 1).
Table 1.
Performance results on ABS promoters ReLA TFM rVISTA(1) PromoterExplorer Eponine ARTS Recall 0.81 0.6 0.37 0.51 0.2 0.14 Precision 0.81 0.61 0.46 0.69 0.21 0.14 Prediction type Defined regions with conserved TFBS Conserved TFBS TSS 765 MANUSCRIPT CATEGORY: ORIGINAL PAPER [18:56 25/2/2012 Bioinformatics-bts024.tex] Page: 766 763770 S.Gonzlez et al.Table 2.
Performance results on EPD TSSs ReLA TFM PromoterExplorer Eponine ARTS Recall 0.56 0.49 0.78 0.23 0.17 Precision 0.56 0.51 0.67 0.27 0.17 Following the same strategy, we alternatively evaluated ReLA using 740 regions derived from the Eukaryotic Promoter Database [EPD; (Schmid et al., 2006)], which, despite not being ideal for this purpose, sets our tool into the context of previous evaluations of these other existing search strategies: ARTS, Eponine and PromoterExplorer against which we have also compared ReLAs predictions (Table 2).
From all resulting predictions, we calculated different performance parameters, such as recall and precision by adapting an evaluation protocol used for the comparison of a large number of TSSs predicting methods (Abeel et al., 2009).
This adaptation is necessary because the different methods we used provide different type of outputs, e.g.ARTS, Eponine, PromoterExplorer provide single TSS positions, TFM and rVISTA lists of conserved TFBS, and ReLA delimited regions of conserved TFBSs.
From the results of this evaluation, we observe that the overall performance is different between the different methods and databases: while ReLAs performance was the best on ABS entries, PromoterExplorer on EPD regions outperformed it.
Interestingly, ReLAs precision values for EPD regions are lower than those shown with ABS.
To discard a possible bias in the performance of ReLA towards specific promoter types that are more abundant in the ABS database, we divided all EPD and ABS regions in different promoter classes (Section 2) and calculated the same precision values for each promoter type and each prediction method separately (Supplementary Tables S1 and S2).
This analysis has shown that, despite ABS appears to be enriched in TATA-box containing promoters (a 42% versus a 20% in EPD), ReLAs performance is not affected by this, as precision values are similar among most of the promoter types present in ABS and EPD.
It is also worth noticing that predictors based on TFBSs show a better performance on the ABS, which is also based on TFBS, than with the TSS-based EPD entries, where TSS predictors tend to do better.
We did not find any significant difference when comparing performance values for Human or Mouse entries (data not shown).
In order to have a sense of the TFBS conservation levels, upon which ReLA is able to build predictions, we have also analysed the distribution of the number of conserved boxes detected within all ABS and EPD results.
This analysis shows that, indeed, there is a wide range of TFBS conservation, both in number and in composition (Supplementary Fig.S1).
Similarly, from the analysis of the contribution of each of the species in ReLAs performance, we observe that all the other vertebrates used in this study contribute substantially to the final prediction in human: for example, cow and dog contribute to 60% of the predictions whereas opossum and chicken to 36 and 32%, respectively (Supplementary Table S3).
A detailed inspection of ReLAs results on ABS entries uncovered some interesting features.
Often, the promoters that we identified on each of the species present different locations within the input 5000 bp region (Fig.1).
This typical scenario, which must be necessarily solved with local-based comparative approaches, is Fig.1.
Prediction of the proximal regulatory region of the Cyclin E1 (CCNE1) gene in seven vertebrate species.
Typical search scenario where the TSS for most of the species compared is not known or missannotated: no TSS information was available for cow, dog and opossum, whereas in chicken is wrongly placed.
The predicted promoter for these species appears in different location within the input sequence.
Dashed lines show the distribution of all primary predictions along these regions.
Consensus predictions are delimited by the first and the last coloured box (each box corresponds to a conserved TFBS).
Red horizontal lines indicate the experimentally characterized regulatory region.
Initial fragments of each transcript are shown on the right: non-coding regions in blue, and coding exons in green.
The numbers indicate the position of the coding exons in the human mRNA.
ENSEMBL TSSs are indicated with arrows.
observed when the annotation of orthologous gene 5UTRs and first exons is practically absent, as occurs for most of the available genomes.
These results highlight the potential of using ReLA for the systematic identification and annotation of regulatory regions in non-model organisms, such as chicken, cow, dog, opossum and any other that has incomplete gene and cDNA data.
3.3 Prediction of multiple alternative promoters During the evaluation of ReLA, we also observed that, in some cases, the distribution of preliminary predictions along the reference sequence highlighted two regions with similar scores, which could correspond to alternative promoters.
From these two options, ReLA selects the one that generated more preliminary predictions (Section 2).
Suboptimal solutions, i.e.potential alternative promoters, can be obtained by simply masking the previously predicted regions in the reference sequence and running ReLA again.
For a number of such cases, we confirmed the presence of two TSSs through the analysis of ESTs or known alternatively transcribed full-length mRNAs.
For this reason, we have implemented this option in the web server, where the user can launch a second search run to find suboptimal solutions.
Figure 2 shows the best two predictions of regulatory regions located upstream from SLC7A7, an amino acid transporter gene, which has been experimentally proven to have two alternative promoters (Puomila et al., 2007).
766 MANUSCRIPT CATEGORY: ORIGINAL PAPER [18:56 25/2/2012 Bioinformatics-bts024.tex] Page: 767 763770 ReLA Fig.2.
Prediction of alternative known promoter regions of the solute carrier family 7 member 7 (SLC7A7).
Searches were done on 15000 bp region upstream of the first amino acid annotated in the ENSEMBL database for the human SLC7A7 gene.
Dashed lines show the distribution of preliminary predictions in the first and second run.
Final first and second predictions are enclosed in a box and delimited by the first and the last conserved TFBS (each designed by a coloured box).
Initial fragments of each transcript are shown on the right: non-coding regions in blue, and coding exons in green.
The numbers indicate the position of the coding exons in the mRNA.
ENSEMBL TSSs are indicated with arrows.
Distances are not drawn at real scale.
The finding of two high scoring regions in any ReLA prediction could suggest, instead of the presence of an alternative promoter, the existence of highly conserved coding exons, which would constitute a false positive prediction.
Thus, the identification of regulatory regions with ReLA would be based only on the level of sequence conservation and the presence of highly conserved non-regulatory DNA, like coding exons, could negatively influence the results.
To discard this, we studied how this scenario can affect ReLA predictions.
The example in Figure 3 shows a positive promoter prediction when all seven orthologous input sequences include the complete region of the E2F1 gene and the additional upstream untranslated regions (a total of 15000 nt each).
In this case, the distribution of hits along the human sequence shows two high scoring regions that appear to share similar conservation levels of nucleotides.
One of these fragments constitutes the third exon of this gene, while the other matches the 5UTR, the TSS and the core promoter.
ReLA is able to successfully discriminate the correct promoter region, including sites that have been experimentally proven (Blanco et al., 2006a).
In particular, the two TFBS that ReLA scores highest in conservation among input sequences are precisely described in the ABS database as two E2F1 factor binding sites necessary for self-regulation during the transition from G1 to S phase in the cell cycle (Johnson et al., 1994).
Interestingly, the third TFBS following the conservation ranking corresponds to ADF1, which was located within the 5UTR and is known to bind the same motifs recognized by the E2F1 factor in mice (Hsiao et al., 1994) Despite these results, we cannot discard the possibility that exons are wrongly predicted as promoters in certain situations.
Therefore, we recommend performing preliminary evaluations of the coding potential within the input sequences, for example, by comparing them against protein sequence databases with BLASTX (Altschul et al., 1997).
Putative coding regions should be preferentially masked from the input sequences to ensure the correct prediction.
3.4 Identification of enhancers The local nature of the underlying search engine and the capacity to compare large DNA sequences makes ReLA a suitable tool for Fig.3.
Prediction of the proximal regulatory region of the E2F transcription factor 1 (E2F1) using the sequence of the whole gene.
Predicted regulatory region along a highly conserved region of 15 722 bp that includes the E2F1 complete gene transcript and 5000 bp upstream of the first amino acid annotated in ENSEMBL.
Dashed line shows the distribution of all preliminary predictions along this region.
A schematic representation of the structure of this gene is shown: non-coding regions are in blue, and coding exons in green.
The numbers designate the position of the coding exons in the mRNA, according to human.
Data related to DNA conservation from UCSC are also included [http://genome.ucsc.edu; (Kent et al., 2002)].
the identification of enhancers, which are often located distant from other functional elements.
In order to test ReLAs capabilities in enhancer detection we have gathered a collection of experimentally validated human enhancers from the VISTA database with activity assessed on transgenic mice (Visel et al., 2007).
To test ReLA, we selected 44 enhancers that are located within the first 50000 bp upstream of a known gene.
In order to search for each of these distal regulatory regions we extracted up to 50000 bp from the most upstream TSS annotated for the closest gene in human and from each of the corresponding one-to-one orthologous genes in mouse, rat, horse, dog, cow, opossum and chicken.
The first run of ReLA on these 44 regions generated 40 predictions, from which 11 (28%) overlapped with the annotated enhancer.
Considering that the regions selected for the search theoretically contain other unknown regulatory regions (promoters, for instance) that could match with the first prediction, we performed a second run on the remaining 29 cases, which yielded nine other positive hits.
In total, with two iterative runs, ReLA showed a positive predictive value of 50% of the screened subset of annotated human enhancers.
A similar prediction rate (49%) is obtained over the same enhancer benchmark set when using a specific enhancer locator tool, EEL (Palin et al., 2006) that also relies on local search strategies (EEL searches implied only human and mouse sequences, as it does not accept more than two sequences per search).
It is worth mentioning that an important difference between both methods is that ReLA provides more precise results, as the regions predicted are shorter (up to 750 nt long, with an average of 485 nt) than those coming from EEL (up to 11563 nt, with an average of 2644 nt).
These results indicate that ReLA is capable of searching large genomic DNA fragments and identifying multiple proximal and distal regulatory regions, which makes this tool suitable for genome-wide screenings and across several genomes (see an example in Fig.4).
To further exemplify this feature, we also performed a genomewide analysis on a 109 kb long ENCODE region [ENm011; chr11:1858751-1968592; (Birney et al., 2007)] that includes six genes coding for, at least, 11 transcripts, with their corresponding intergenic regions.
By using SYT8 and MRPL23 flaking genes 767 MANUSCRIPT CATEGORY: ORIGINAL PAPER [18:56 25/2/2012 Bioinformatics-bts024.tex] Page: 768 763770 S.Gonzlez et al.Fig.4.
Prediction of the SALL1 enhancer in six vertebrate genomes.
The upper panel shows the SALL1 gene and its corresponding 50 kb upstream region for six vertebrate species.
Coordinates and strand of these regions are: Chicken (chr11: 61480986213605, ), Mouse (chr8: 9155114391618061, +), Rat (chr19: 1922733719293298, ), Dog (chr2: 6708005667144522, ), Cow (chr18: 186886711875278, +) and Human (chr16: 51169886 51235181, +).
In each line, we display the structure of the gene (green boxes are coding exons, while blue are untranslated).
Known and predicted TSSs are also shown.
ReLAs predictions are shown for each species as groups of colored boxes.
Please, note that these regions are not drown to scale.
ReLAs predicted enhancer regions expanded from 223 and 233 bp for dog and mouse, 301 bp for cow, to 359, 360 and 366 bp for rat, human and chicken, respectively.
The locations of the experimentally proven regions (as shown in rVISTA db) are displayed as green boxes.
The bottom line of this panel shows the sequence conservation profile (according to human coordinates; http://genome.ucsc.edu).
In the bottom panel, we display the alignment of the conserved TFBS detected within each of the predicted enhancer regions.
TFBS are labelled (in TRANSFAC format) and differentiated using arbitrary shapes and colours.
Coordinates shown here indicate the position of the predicted enhancer within the 50 kb input sequence.
as anchors, we identified and characterized the corresponding orthologous regions for mouse, rat, dog, cow and chicken.
The complete analysis consisted in 10 iterative ReLA searches and implied the screening of TFBS sequences in >600 kb of genomic DNA.
In order to obtain an estimation of the performance on these genome-wide conditions, we have taken as positive predictions those that match ChIP-Seq transcription evidences (Birney et al., 2007), as well as those falling immediately upstream of annotated gene starts.
This count shows that 8 out of 10 predictions have evidence of expression or regulation (Supplementary Fig.S2).
Please note that we cannot discard that additional runs would provide other overlooked regulatory regions and, at some point, also false positives.
4 DISCUSSION Taking into account the available methods to in silico recognize gene regulatory regions, a substantial improvement is necessary to accurately annotate genes and promoter sequences in most genomes.
Here we report the development of ReLA, a computational tool to identify such regulatory regions using genome-wide comparisons.
ReLA is distributed as a standalone program and as a web server.
Our approach is mostly based in an adaptation of the popular Smith Waterman algorithm that is able to rapidly identify coincidences of TFBSs between two sequences (conceptually similar to traditional proteinprotein comparisons).
ReLA is able to efficiently process long sequences in standard computational platforms (e.g.less than a minute to obtain the results shown in Fig.5).
We have evaluated the accuracy of ReLA, first in a dataset of experimentally validated human and mouse promoters, on an extensive collection of validated TSS from the Eukaryotic Promoter Database, as well as on an experimentally validated collection of rVISTA enhancers.
We have reached maximums of 0.81 of recall and precision levels on ABS sequences On the other hand, and surprisingly, ReLAs performance results lower when using EPD TSS entries.
A possible explanation for this observation could be that ReLA performs better on certain types of promoter regions.
But, after we classified all ABS and EPD entries into different promoter types according to their composition and evaluated their associated performance obtained with all the methods used here for the validation, we observed that ReLAs accuracy is similar among most of the identified promoter types.
We cannot discard though that other uncontrolled biases present either inside the underlying search methodology of each of the protocols used here or in the used databases could actually explain the different behaviour observed.
It is worth noticing that overall, TFBS-based prediction methods perform better on the TFBS-based ABS database than on the TSS-based EPD, where TSS predictors are doing better.
In any case, the levels of precision and recall obtained with ReLA are sufficient to provide reliable predictions that guide posterior experimental validation.
This study also demonstrates the benefits of using the SmithWaterman algorithm to directly search for conserved binding sites, as it outperforms other methods like rVISTA and TFM, that are based on pre-aligned DNA and global search strategies.
See the example in Figure 1, which cannot be solved using global alignment approaches.
Please note that other methods based on similar strategies could not be included in the comparison, as they did not provide results on our benchmark set because of limitations in the size (MMETA) and on the number of sequences [Conreal (Berezikov et al., 2005)].
Furthermore, we also show that ReLA is able of predicting alternative promoters and even enhancer regions, dealing with multiple suboptimal solutions in most cases.
Our approach is suitable for integrating a computational annotation pipeline in which other predictive methods such as homology searches (e.g.BLAST against protein databases) can assist in the improvement of the final predictions.
In summary, we believe that the development of ReLA constitutes a significant step forward in the field of the prediction of regulatory regions, as it shows the highest predictive power reported so far.
ReLA is able to locally compare multiple large genomic regions and identify non-alignable conservation events across different genomes.
This is relevant if we consider that the limited information regarding regulatory regions in eukaryotes is restricted to human 768 MANUSCRIPT CATEGORY: ORIGINAL PAPER [18:56 25/2/2012 Bioinformatics-bts024.tex] Page: 769 763770 ReLA Fig.5.
Snapshot of the ReLA web server output.
Graphical representation of the putative promoters and alignments of TFBSs of the human E2F1 gene, as well as the lists of predicted regions and conserved binding motifs.
See Sections 2 and 3 for a complete interpretation of each of the results provided.
and mouse, e.g.from 2540 vertebrate entries in the Eukaryotic Promoter Database (Schmid et al., 2006), 2067 (81%) belong to these two species.
Thus, with this tool in hand we can now, not only fill missing gaps in the annotation of the genomes of model organisms, mostly with the identification of enhancers and alternative promoters, but also to start a reliable and consistent annotation of conserved promoters throughout the rest of genomes that have little or no information regarding 5UTRs and often first coding exons (Fig.1).
Beyond the current performance of ReLA and, as we are planning a genome-wide search of regulatory regions across sequenced vertebrate genomes, we are actively searching for ways of improving further its predictive power by, for example, applying more sophisticated scoring systems and accepting even larger DNA regions with low additional computational costs.
ACKNOWLEDGEMENTS We thank Mar Alb, Steven Laurie and our entire group for constructive feedback during the development of this work and during the writing of the manuscript.
We also thank Jan and Aina Sagrist for designing ReLAs logo.
Funding: Ministerio Espaol de Ciencia e Innovacin (BIO200615036).
Conflict of Interest: none declared.
ABSTRACT Sporulation in low-G+C gram-positive bacteria (Firmicutes) is an important survival mechanism that involves up to 150 genes, acting in a highly regulated manner.
Many sporulation genes have close homologs in non-sporulating bacteria, including cyanobacteria, proteobacteria and spirochaetes, indicating that their products play a wider biological role.
Most of them have been characterized as regulatory proteins or enzymes of peptidoglycan turnover; functions of others remain unknown but they are likely to have a general role in cell division and/or development.
We have compiled a list of such widely conserved sporulation and germination proteins with poorly characterized functions, ranked them by the width of their phylogenetic distribution, and performed detailed sequence analysis and, where possible, structural modeling aimed at estimating their potential functions.
Here we report the results of sequence analysis of Bacillus subtilis spore germination protein GerM, suggesting that it is a widespread cell development protein, whose function might involve binding to peptidoglycan.
GerM consists of two tandem copies of a new domain (designated the GERMN domain) that forms phylum-specific fusions with two other newly described domains, GERMN-associated domains 1 and 2 (GMAD1 and GMAD2).
Fold recognition reveals a-propeller fold for GMAD1, while ab initio modeling suggests that GMAD2 adopts a fibronectin type III fold.
SpoVS is predicted to adopt the AlbA archaeal chromatin protein fold, which suggests that it is a DNA-binding protein, most likely a novel transcriptional regulator.
Contact: drigden@liverpool.ac.uk Supplementary information: Supplementary data are available at ftp://ftp.ncbi.nih.gov/pub/galperin/Sporulation.html 1 INTRODUCTION Cell division remains one of the least understood processes in the life of the bacterial cell.
In stark contrast to the significant progress in the understanding of microbial metabolism and signal transduction brought about by the complete genome sequences, the contribution of genomics to the studies of cell division has been relatively modest.
Some cell division proteins are still poorly characterized and the importance of the presence or absence of a certain gene in a given genome cannot be readily interpreted as it is being done for the metabolic enzymes.
In addition, cell division involves To whom correspondence should be addressed.
numerous proteinprotein interactions, so mutant phenotypes are fairly complex, making their formal description (e.g.using the Gene Ontology system) almost impossible.
Finally, preliminary characterization of such genes in Escherichia coli, Bacillus subtilis or other bacteria usually results in assigning them stable names, e.g.cell division protein FtsN, which often creates an illusion of at least some understanding and obscures the fact that the functions of these proteins remain enigmatic.
Sporulation in B.subtilis and other low-G+C gram-positive bacteria (Firmicutes) is an important survival mechanism that is related to cell division and involves up to 150 genes, acting in a highly regulated manner (Piggot and Losick, 2002).
Mutational analyses and transcriptional profiling revealed the timing of action of each sporulation gene and suggested functions for most of them.
In some cases, deduced functions have been verified by structural analysis or direct biochemical experiments, mostly on proteins from B.subtilis.
Phylogenetic distribution of B.subtilis sporulation genes is quite complex (Onyenwoke et al., 2004; Wu et al., 2005).
Many of them have regulatory roles and appear to be non-essential for spore formation.
Accordingly, such genes may be missing in certain bacillar and clostridial genomes.
On the other hand, close homologs of several sporulation genes can be found in the genomes of non-sporulating microorganisms (Onyenwoke et al., 2004).
Such genes typically encode cell division proteins, e.g.SpoVD (FtsI) or SpoVE (FtsW), enzymes of peptidoglycan turnover, or components of bacterial signaling systems, such as sensory histidine kinases, response regulators, alternative sigma factors and other transcriptional regulators (Piggot and Losick, 2002).
However, function of many sporulation genes remains unknown, including some that have wide phylogenetic distribution and are found in a variety of non-sporulating bacteria.
It would be reasonable to assume that widespread functionally uncharacterized sporulation genes encode additional components of bacterial division or signal transduction machinery.
We have set out to identify such genes and analyze their likely functions using a combination of sequence and structure analysis tools.
Here we report the results of domain analysis and structural modeling of two widespread proteins from this group, GerM and SpoVS.
2 METHODS The initial list of B.subtilis sporulation proteins was compiled from published sources (Errington, 2003; Onyenwoke et al., 2004; Piggot and Losick, 2002).
Phylogenetic distribution of each protein was judged based on the species 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
ftp://ftp.ncbi.nih.gov/pub/galperin/Sporulation.html[15:59 8/8/03 Bioinformatics-btn314.tex] Page: 1794 17931797 D.J.Rigden and M.Y.Galperin lists in the Pfam (Finn et al., 2008), CDD (Marchler-Bauer et al., 2007) and COG (Tatusov et al., 2000) databases, where available, and verified using PSI-BLAST (Altschul et al., 1997) searches, employing an e-value of 0.01, filtered to exclude hits from the Firmicutes.
The search results were sorted using the Taxonomy reportsoption in BLAST outputs.
Domain composition of the retrieved sequences was analyzed by comparing them against Pfam, CDD and COGs with e-values of below 0.01 taken to represent significant hits.
Possible templates for modeling were sought at the META server, a portal to the several fold recognition methods (Bujnicki et al., 2001) and to the 3D-Jury consensus method, by which scores of over 50 are taken as highly confident (Ginalski et al., 2003).
Distant homologies were also sought with HHpred (Soding et al., 2005) using a cut-off of e < 0.01.
Secondary structures were predicted using PSI-PRED (Jones, 1999).
Template-based modeling was carried out with MODELLER (Sali and Blundell, 1993).
Five variant models were constructed after an initial coordinate randomization step.
PROCHECK (Laskowski et al., 1993) was used for their stereochemical evaluation and VERIFY_3D (Lthy et al., 1992) and PROSA II (Sippl, 1993) employed for model ranking by solvent exposure and residueresidue contacts.
ROSETTA was used for ab initio model building using default protocols: 2000 individual models were constructed from 3-and 9-residue segments using Monte Carlo substitution and optimization protocols and clustered based on RMSD calculations (Simons et al., 1997, 1999).
I-TASSER (Lee and Skolnick, 2007) ab initio models were obtained from the web server (http://zhang.bioinformatics.ku.edu/I-TASSER/).
I-TASSER models and coordinates for centre models of each ROSETTA cluster were submitted to the DALI server (http://www.ebi.ac.uk/dali/; Holm and Sander, 1993) for structural comparison to the Protein Data Bank (PDB).
By DALI, Z-scores of <2 are insignificant.
Side chains were added to the best ROSETTA model using SCWRL (Canutescu et al., 2003).
PYMOL (http://pymol.sourceforge.net) was used for structure manipulation and visualization as well as display of electrostatic potential surfaces calculated with Adaptive Poisson-Boltzmann Solver (APBS) (Baker et al., 2001).
Structural relationships were browsed in the SCOP database (Andreeva et al., 2008).
3 RESULTS 3.1 Sporulation genes in non-sporulating bacteria Earlier studies identified homologs of B.subtilis sporulation genes in a variety of non-sporulating bacteria both within and outside of the phylum Firmicutes (Onyenwoke et al., 2004; Wu et al., 2005).
In order to identify widely conserved sporulation genes, we have looked only for those that have close homologs outside the Firmicute phylum.
The resulting list was winnowed down by manually removing known transcriptional regulators, signal transduction proteins, sigma subunits of the RNA polymerase, antisigma and anti-anti-sigma factors, enzymes involved in synthesis or hydrolysis of peptidoglycan and previously characterized cell division proteins.
Further, detailed analysis of the remaining proteins showed that for some of them functional assignments either had been made or could be made very easily, based on recent experimental data or convincing sequence similarity to experimentally characterized proteins.
After these proteins were removed, the final list (Supplementary Table 1) included 20 previously uncharacterized sporulation and germination proteins that had close homologs in more than three different bacterial phyla.
We applied a battery of bioinformatics analyses to yield insights into possible functions and present here results for GerM and SpoVS.
3.2 GerM protein and associated domains B.subtilis protein GerM has been implicated in both sporulation and spore germination (Sammons et al., 1987; Slynn et al., 1994), suggesting an important role in cell development.
According to the COG database, orthologs of GerM (COG5401) are encoded in spirochaetes and cyanobacteria.
Indeed, a PSI-BLAST search of complete genome sequences using GERM_BACSU protein (SwissProt accession P39072) as a query and run to convergence retrieved more than 150 proteins (last searched April 1, 2008), encoded in Firmicutes and members of other bacterial phyla, including Actinobacteria, Cyanobacteria, Proteobacteria and Deinococcus Thermus group.
These searches revealed that GerM from B.subtilis and other bacilli contains tandem copies of a 100 amino acidlong domain, hereafter called the GERMN domain (Fig.1 and Supplementary Fig.1).
In other firmicutes, this domain was found mostly in a stand-alone form, whereas Moorella thermoacetica and several other clostridia encoded both a stand-alone and a duplicated version.
In a single species in the current database, Halothermothrix orenii, GERMN is fused to the amidase (PF01520) domain which cleaves amide bonds in cell wall peptidoglycans.
In addition, the GERMN domain was found fused, in a phylum-specific fashion, to two further novel domains (Fig.1 and Supplementary Figs 2 and 3) named GERMN-associated domains 1 and 2 (GMAD1 and GMAD2).
GERMN, GMAD1 and GMAD2 have been deposited with the Pfam database, receiving accession numbers PF10646, PF10647 and PF10648, respectively.
The GERMN entry replaces and extends a PfamB entry PB005693 (in Pfam 22.0), which only covered the first two domain architectures shown on Figure 1.
In Firmicutes, orthologs of GerM protein, containing duplicated GERMN domain are encoded in spore-forming clostridia and bacilli, but not in non-sporogenous lactobacilli, staphylococci or streptococci.
Every sequenced genome of Actinobacteria encodes a GERMN-GMAD1 domain fusion that is referred to as putative lipoprotein LpqB although the origin of this name seems obscure.
The lpqB gene, nearly always found adjacent to the genes encoding the two-component system MtrAB, has Fig.1.
Domain architectures formed by the GERMN (blue oblongs), GMAD1 (yellow hexagon) and GMAD2 (red pentagon) domains, drawn approximately to scale.
The purple diamond is the amidase domain (PF01520) while the green ellipses represent LysM (PF01476) domains.
The proteins are listed by their Uniprot identifiers or NCBI gene index (gi) numbers and genome locus tags.
1794 [15:59 8/8/03 Bioinformatics-btn314.tex] Page: 1795 17931797 Sequence analysis of GerM and SpoVS been proposed to modulate the function of these signaling proteins (Hoskisson and Hutchings, 2006).
Deletion of MtrA and MtrB in Corynebacterium glutamicum leads to increased susceptibility to cell wall targeting vancomycin and lysozyme as well as to cell elongation (Moker et al., 2004).
Thus the MtrAB double mutant appears to have cell envelope and cell division defects.
The function of this pair is intriguingly similar to that of YycFG (VicRK) system in the Firmicutes (Aravind et al., 2003).
The importance of the GERMN domain is underscored by the fact that it is encoded in all completely sequenced genomes of representatives of such phyla as Spirochaetes and Deinococcus Thermus, including the relatively small genomes of obligate parasites Borrelia burgdorferi and Treponema pallidum.
The GERMN domain is also encoded in many cyanobacterial genomes.
Screening GERMN-containing proteins with PROSITE (Hulo et al., 2008) prokaryotic lipoprotein (PS51257) motif revealed their strong association with this predicted post-translational modification.
Single GERMN sequences were predicted lipoproteins in 48/120 cases.
The figure for twin GERMN sequences (e.g.GerM) is 33/39, for GERMN-GMAD1 it is 30/36 and for GERMN-GMAD2 it is 6/11.
We then considered whether modeling, comparative or ab initio, or domain context could shed light on the functions of these domains.
None of the fold recognition methods implemented at the METAserver (Bujnicki et al., 2001) gave significant results for either the GERMN or the GMAD2 domains.
In the case of GERMN, taking a consensus view of secondary structure predictions for different proteins, the fold seems to contain the following principal elements of regular secondary structure.
The lack of significant hits suggests that GERMN has a novel + type fold.
Ab initio modeling was unsuccessful for GERMN.
The GMAD2-predicted secondary structure indicates an all--fold containing approximately nine-strands.
Several methods rated the immunoglobulin-type fold most highly but with scores not high enough for confident fold assignment.
Ab initio modeling with ROSETTA was also unsuccessful, but, suggestively, all five models obtained by ab initio modeling at the I-TASSER server shared the same fold, matched significantly by DALI (Z-scores up to 7.5) to immunoglobulin-type structures.
Among these the top scores were for matches to fibronectin type III (FnIII) domains.
These have structural roles in animals, but are also present in bacteria, where they are often associated with glycoside hydrolase enzymes (Bork and Doolittle, 1992; Little et al., 1994).
Browsing of the current Pfam database (release 22.0) confirms the strong association between this domain and carbohydrate metabolism, as shown by its presence, with few exceptions, alongside catalytic domains that clearly act on carbohydrates.
At a molecular level, bacterial FnIII domains act as spacers between catalytic and substrate-binding domains (Toratani et al., 2006) while structurally similar bacterial-sandwich domains have a direct carbohydrate-binding function (Jee et al., 2002).
The GMAD2 domain is found combined with LysM domains in two distinct architectures (Fig.1).
LysM domains were first associated with peptidoglycan binding (Steen et al., 2003), although recent data show that some examples bind chitin or lipochitin (Spaink, 2004; Onaga and Taira, 2008).
Specificity for carbohydrate ligand therefore seems to vary between LysM domains.
The GMAD1 domain gave strong hits to-propeller folds by fold recognition methods at the META server, consistent with earlier data (Hoskisson and Hutchings, 2006).
Interestingly, WD40-type 7-bladed propellers consistently achieved the top scores (up to 96 by 3D-Jury).
However, the WD motifs themselves are absent and such propellers are, in any case, rare in bacteria and generally contain more blades than the five or six predicted for GMAD1 (Neer et al., 1994).
Unfortunately, the-propeller fold assignment provides few clues as to the function of the GMAD1 domain since-propellers are involved in a wide variety of binding and catalytic functions.
A certain tendency towards sugar binding and metabolism is evident in the functions of smaller 5-and 6-bladed-propellers of known structure; 2 of the 3 superfamilies of 5-bladed propellers and 4 of the 11 superfamilies of 6-bladed propellers have these functions (Andreeva et al., 2008).
However,-propellers are also known as proteinprotein interaction domains, so the interaction mentioned above between LpqB, a GerMNGMAD1 domain fusion protein and the MtrAB system could be mediated by the GMAD1 propeller.
The presence of only a single conserved hydrophilic residue, an Arg, in an alignment of GMAD1 domains, would be more consistent with a passive binding role than with a catalytic function (Koonin and Galperin, 2002).
Intriguingly, a direct connection can be made with PSI-BLAST between a GERMN query and Streptococcus pneumoniae Wzd, a component of the capsule polysaccharide export machinery (Aanensen et al., 2007).
The match appears in the third iteration in the form of an ungapped 56 residue alignment with a bit score of 43 and an e-value of 0.006.
As yet, no Wzd structure exists with which to assess the significance of this match.
3.3 SpoVS protein Mutations in B.subtilis spoVS gene block sporulation at Stage V but allow the spoIIBspoVG double mutant to bypass the sporulation block at Stage II (Resnekov et al., 1995).
According to the Pfam database, proteins with the SpoVS domain (PF04232), in addition to firmicutes, are encoded in members of the bacterial phyla Chloroflexi, Thermotogae and DeinococcusThermus.
This is consistent with results of PSI-BLAST searches, which detect orthologs of B.subtilis SpoVS (SP5S_BACSU, Swiss-Prot accession P45693) in every completely sequenced genome of these phyla.
In firmicutes, SpoVS is found in sporulating bacilli and clostridia, but not in non-sporulating lactobacilli, listeria, staphylococci or streptococci.
PSI-BLAST searches failed to reveal any distant homologs of SpoVS.
At the META server, the single match scoring above the 3D-Jury confidence threshold of 50 (Ginalski et al., 2003) was between Thermotoga maritima SpoVS and Sulfolobus solfataricus Alba (PDB code 1h0x; Wardleworth et al., 2002) with a score of 53.
An excellent match between predicted secondary structure for SpoVS and the actual secondary structure of Alba was seen (Supplementary Fig.4).
In order to assess the compatibility of the SpoVS sequences and the Alba fold in more detail, modeling was carried out.
We modeled the T.maritima protein TM1059 which, among those we tested, achieved the best 3D-Jury score.
The S.solfataricus Alba structure was used as template.
Although there are no experimental data regarding the SpoVS oligomeric state we supposed that it might exist, like Alba and likely its most closely related families of relatives (Aravind et al., 2003), as a dimer, a hypothesis supported by the presence in the dimer model of a large hydrophobic interface between the subunits.
The modeling showed that indels between SpoVS andAlba could be readily accommodated 1795 [15:59 8/8/03 Bioinformatics-btn314.tex] Page: 1796 17931797 D.J.Rigden and M.Y.Galperin and the final model has favorable VERIFY_3D and PROSA II profiles and an optimal pG value (Sanchez and Sali, 1998) of 1.0.
Ab initio modeling of B.subtilis SpoVS lent further support for its structural correspondence with Alba.
The top cluster of 2000 ROSETTA models contained many more models, 268, than the next most populated cluster (81) indicative of likely success.
Indeed, the top cluster centre gave a highly significant Z-score of 9.3 by DALI, far in excess of the 3.35.6 achieved by other cluster centres.
The top cluster centre matched S.solfataricus Alba and corresponded to an alignment of 81 residues with a C RMSD of 2.2.
For the top I-TASSER model the corresponding figures were Z-score of 13.2 for an alignment of 84 residues with a C RMSD of 1.5.
As shown in Figure 2, the top ROSETTA and I-TASSER models of B.subtilis SpoVS and the homology model of T.maritima TM1059 are remarkably similar.
We interpret the ability of ab initio modeling to produce the Alba fold as strong evidence that this is indeed the correct fold assignment for the SpoVS family.
The Alba fold is strongly linked with the broad function of nucleic acid binding.
Although Alba itself binds DNA, homologs recognizable by iterative database searching include several families of RNA binding proteins (Aravind et al., 2003).
Furthermore, Albas IF3-C fold is found in many other families lacking sequence similarity but recognizable at the structural level.
As reported (Aravind et al., 2003), and verified in the current SCOP database, this fold is strongly associated with nucleic acid binding.
The comparative SpoVS model has the significant dipole characteristic of nucleic acid binding proteins (Szilagyi and Skolnick, 2006).
The positively charged face (see Supplementary Fig.5), also seen for the ab initio models (data not shown) corresponds to a similarly positively charged surface in the Alba dimer (Wardleworth et al., 2002) that has been convincingly modeled as forming its interface with duplex DNA.
4 CONCLUSIONS Our census (Supplementary Table 1) and the work of others (Errington, 2003; Onyenwoke et al., 2004; Piggot and Losick, 2002); show that many sporulation and germination proteins are in fact of broad phyletic distribution.
Furthermore, their annotation as being involved in these processes often obscures a real lack of knowledge of their molecular functions.
Here, we have attempted indepth sequence analysis of some of them.
Three novel domains are described which will contribute to extending Pfam coverage towards its upper limit (Sammut et al., 2008).
The strong theme emerging from domain context analysis of GERMN (Fig.1) and the available structure predictions is carbohydrate binding.
We therefore propose that the GERMN domain interacts with bacterial carbohydrate.
With the separate implication of the GERMN domain in cell envelope and division for GerM (Sammons et al., 1987; Slynn et al., 1994) and LpqB (Moker et al., 2004), likely carbohydrate targets would be peptidoglycan and/or capsule polysaccharide.
The former is supported by associations with the amidase and LysM domains (Fig.1), while the PSI-BLAST link from GERMN to Wzd hints at the latter possibility.
Experimental data regarding SpoVS are apparently limited to two reports (Resnekov et al., 1995; Perez et al., 2006).
In the first, mutation of the spoVS gene halted sporulation at Stage V and reduced expression of two K-directed genes, cotA and gerE Fig.2.
Comparison of comparative (blue to red, N-to C-terminus), ROSETTA (grey) and I-TASSER (black) models of SpoVS.
(Resnekov et al., 1995).
On the other hand, spoVS mutation increases D-directed gene expression, cell separation and autolysis (Perez et al., 2006).
These sigma factors direct RNA polymerase to transcribe certain sets of genes and, along with other DNA-binding regulators such as GerE and SpoIIID, form complex networks that control sporulation (Eichenberger et al., 2004).
Along with our structurebased prediction, these data suggest that SpoVS is a further member of these networks and, by binding to specific DNA sites, influences the transcriptional profile of the cell during the onset of sporulation.
ACKNOWLEDGEMENTS Funding: M.Y.G.
was supported by the Intramural Research Program of the National Library of Medicine at the National Institutes of Health.
Funding to pay the Open Access publication charges for this article was provided by the NIH Intramural Research Program.
Conflict of Interest: none declared.
ABSTRACT Summary: MASS is a command-line program to perform meta-analysis of sequencing studies by combining the score statistics from multiple studies.
It implements three types of multivariate tests that encompass all commonly used association tests for rare variants.
The input files can be generated from the accompanying software SCORESeq.
This bundle of programs allows analysis of large sequencing studies in a time and memory efficient manner.
Availability and implementation: MASS and SCORE-Seq, including documentations and executables, are available at http://dlin.web.unc.
edu/software/.
Contact: lin@bios.unc.edu Received on March 19, 2013; revised on April 24, 2013; accepted on May 10, 2013 1 INTRODUCTION Meta-analysis of genome-wide association studies (GWAS) has led to the discoveries of common genetic variants for virtually every complex human disease.
Recent advances in sequencing technologies have made it possible to extend association studies to rare variants.
Because larger sample sizes are required to detect rare variants than common variants (with similar effect sizes), combining evidence from many sources is necessary for sequencing studies.
For ethical and logistical reasons, it is strongly preferable to gather summary statistics than collecting original data.
For association testing with rare variants, it is customary to aggregate information across several variant sites within a gene to enrich association signals and to reduce the penalty of multiple testing.
The simplest approach is the burden test, which creates a burden score for each subject (by taking a weighted linear combination of the mutation counts within a gene or indicating whether there is any mutation within a gene) (Li and Leal, 2008; Lin and Tang, 2011; Madsen and Browning, 2009; Price et al., 2010).
A second approach is the variable threshold (VT) test, which performs a burden test for variants with minor allele frequencies (MAFs) below a certain threshold and minimizes the P-value over the observed MAF thresholds (Lin and Tang, 2011; Price et al., 2010).
A third approach is the variance-component test, which is aimed at detecting variants with opposite effects within a gene (Neale et al., 2011; Tzeng and Zhang, 2007; Wu et al., 2011).
After creating a burden score for each subject, one may carry out the burden test by performing a standard regression analysis and combine the Wald statistics of multiple studies through the inverse-variance formula.
However, this strategy is problematic for rare variants because the effect estimates may be unstable or even undefined when only a small number of subjects carry any mutation.
Indeed, the log odds ratio is undefined if the burden scores are zero for all cases or all controls.
We recommend to use score statistics, which are statistically more accurate and numerically more stable than Wald and likelihood ratio statistics, especially for binary traits (Lin and Tang, 2011).
Currently, there is no meta-analysis software for combining score statistics.
Thus, we developed the Meta-Analysis of Score Statistics (MASS) software, which performs an overall burden test by combining the score statistics of multiple studies.
In addition, MASS performs an overall VT test for multiple studies based on score statistics.
Finally, MASS performs an overall variancecomponent test also based on score statistics.
The meta-analysis performed byMASS is statistically as efficient as joint analysis of individual-level data.
The software is extremely easy to use.
Because score statistics are not available in existing software packages, we developed SCORE-Seq, which takes the standard input format of sequencing data and outputs score statistics, which can then be meta-analyzed in MASS.
2 METHODS Suppose that we are interested in d genetic variables.
For the burden and VT tests, the genetic variables pertain to the burden scores; for the variant-component test, the genetic variables pertain to the genotypes of individual variants; for the CMC test (Li and Leal, 2008), the genetic variables contain the genotypes of common variants and the burden scores of rare variants.
Suppose that there are K independent studies.
For the kth study, we calculate the (multivariate) score statistic Uk for testing the null hypothesis H0 that none of the d genetic variables have any effect on the trait of interest, and we also calculate the corresponding information matrix Vk.
Note that Uk is a d 1 vector and Vk is a d d matrix.
If a genetic variable is absent in the kth study, then we set the corresponding entries in Uk and Vk to zero.
Given the input of Uk and Vk k 1,... ,K, MASS calculates U PK k1 U k and V PK k1 V k. UnderH0, the random vectorU is (asymptotically) multivariate normal with mean 0 and covariance matrix V. It can be shown that U is the score statistic for testingH0 from the joint likelihood for the original data of the K studies allowing nuisance parameters (e.g., intercepts and error variances) to be different among the K studies (Lin and Zeng, 2010).
Thus, association testing based on U and V is equivalent to the joint analysis of the original data.
After calculating U and V, MASS can perform three types of multivariate tests, which encompass all commonly used rare-variant tests.
1.
Quadratic statistic: Q UTV1U: Under H0, the test statistic Q is distributed as 2 d. If d 1 and the genetic variable pertains to a specific burden score (based on aMAF threshold or*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com the MadsenBrowning weighting), then Q is a burden test with 1 degree of freedom.
If the genetic variables consist of the genotypes of common SNPs and the burden score of rare variants, then Q is the CMC test.
2.
Maximum statistic: Tmax max j1,..., d U2j =Vj, where Uj is the jth component of U, and Vj is the jth diagonal element of V. The P-value of Tmax is determined by the multivariate normal distribution of U (Lin and Tang, 2011).
If the d genetic variables are the burden scores at d MAF thresholds, then Tmax yields the VT test.
If the genetic variables pertain to different types of burden scores, such as T1, T5, and MadsenBrowning, then Tmax can be used to adjust for multiple testing with those burden tests.
3.
Weighted quadratic statistic: Qw UTWU, where W is a weight matrix.
The null distribution of Qw is determined by Pd j1 lj 2 1, j, where lj is the jth eigenvalue of V 1=2WV1=2, and 21, 1,... , 2 1, d are independent 2 1 random variables.
If the genetic variables are the genotypes of individual SNPs, then Qw becomes the SKAT or C-alpha test.
For SKAT, W is a diagonal matrix that depends on the MAFs through a beta function; for C-alpha, W is an identity matrix.
3 RESULTS MASS is a freely available C program that runs on Unix and Linux systems.
The basic command line is MASS [-method method][-sfile script][-ofile outfile][options].
The option-method selects one of the three test statistics: quadratic, maximum or weighted quadratic.
The option-sfile specifies a script file that describes the input files from multiple studies.
For the weighted quadratic statistic, the option-weight can be used to specify a file with a weight for each component of U.
MASS can filter out genetic variables based on minor allele counts (MACs).
Full documentation is available at http://dlin.web.unc.edu/software/.
The summary statistics for individual studies can be obtained from SCORE-Seq, which inputs the sequencing data with a quantitative or binary trait and outputs the score statistics and information matrices for all commonly used rare-variant tests.
The basic command line is SCORE-Seq [-pfile phenofile] [-gfile genofile] [-mfile mapfile] [-ofile outfile] [-vtlog vtlog] [-snplog snplog] [options].
There are three input files: phenofile contains the phenotype and covariates; genofile contains the genotypes; mapfile provides the geneSNP mapping and SNP annotation.
The output files outfile, vtlog and snplog contain the score statistics and information matrices for the burden test, VT and SKAT.
In the output files, each row corresponds to a component of the score statistic and each column of the score statistic is followed by the corresponding information matrix.
The SCORE-Seq output files for different studies can be input directly into MASS.
We recently applied MASS to the NHLBI Exome Sequencing Project.
The meta-analysis involved 11 studies and 15 404 genes, with an average of 7 genetic variables per test.
In three of the studies, subjects were selected for sequencing because they had extreme values of a quantitative trait.
Thus, we developed a special program called SCORE-SeqTDS to perform quantitative trait analysis under trait-dependent sampling.
We obtained the summary statistics from SCORE-Seq or SCORE-SeqTDS, dependent on the study design.
The total size of the input files for MASS was 172 MB.
We ran the three types of tests on an IBM HS22 machine: the quadratic statistic took 510 seconds and 1 MB memory; the maximum statistic took 5280 seconds and 33 MB memory; and the weighted quadratic statistic took 5200 seconds and 33 MB memory.
Figure 1 is a flow chart for performing the VT test.
The top panel shows the SCORE-Seq input and output files for the first two studies.
The first gene, ABHD8, has three MAF thresholds.
Thus, the score vector for this gene is provided in three rows, and the information matrix is a 3 3 matrix, whose upper diagonal elements are redundant and thus set to zero.
The 11 output files generated by SCORE-Seq and SCORE-SeqTDS are input into Fig.1.
Pipeline for performing the VT test in the NHLBI Exome Sequencing Project 1804 Z.-Z.Tang and D.-Y.
Lin MASS via the script file shown in the bottom panel of Figure 1.
The output file of MASS provides the maximum test statistic and P-value for each gene.
4 DISCUSSION Protection of human subjects and other study policies make it difficult to share individual-level data, even in well-organized consortia.
Thus, meta-analysis is strongly preferable to joint analysis.
The MASS software enables one to perform meta-analysis of score statistics for sequencing studies, which is statistically as efficient as and indeed numerically equivalent to joint analysis of individual-level data.
This software can also be used to combine results from other types of genetic studies as well as non-genetic studies.
In meta-analysis of sequencing data, the participating studies should use the same annotation file so that the summary statistics are generated in a consistent manner across studies.
This does not mean that the variants have to be the same among studies.
If a genetic variable is missing in one study, then zero can be used as a placeholder in the summary statistics andMASS will combine all available information.
The calculation of the burden scores requires specification of the MAFs.
TheMAFs may be estimated separately in each study or jointly across all studies; they may also be determined from an external source.
We recommend that the same MAFs be used by all participating studies.
In this way, the same variants are included in the calculations of the burden scores among the studies, and the MAF thresholds for the VT test are consistent across the studies.
For studies that use different exome capturing kits or studies in which some have exome sequencing while others have exome chip data, the input variants are different.
If the genetic variable pertains to the genotype of a variant and that variant is not measured in the kth study, then we simply set the corresponding entries in Uk and Vk to zero, so that MASS will combine all available data.
If the genetic variable pertains to a burden score, then a variant that is absent in a study will not contribute to the calculation of the burden score for that study.
The score statistics from such studies can still be combined in MASS, although the results need to be interpreted with extra care.
If the burden score is a (weighted) linear combination of the genotype values (e.g.the total number of mutations or a weighted sum of the mutation counts), then the score statistics for the burden and VT tests are (weighted) linear combinations of the score statistic for testing the null hypothesis that the genotypes of individual variants are not associated with the trait of interest.
In that case, it would be sufficient to input only the score vector and information matrix for individual variants because they can be used to create the score statistics and information matrices for the burden, VT and SKAT tests.
We did not take this approach because it requires the burden scores to be calculated under the additive mode of inheritance.
Indeed, it has become a common practice to define the burden score as the presence or absence of any mutation within a gene rather than the total number or weighted linear combination of the mutations.
SCORE-Seq allows burden scores to be calculated under the additive, dominant or recessive mode of inheritance.
By asking the user to input the score statistics and information matrices for each specific test, MASS can accommodate any mode of inheritance.
MASS adopts the fixed-effect model, which assumes that the genetic effects are the same among the participating studies.
This approach will have reasonable power as long as the effects are in the same direction across studies.
An alternative approach is the random-effect model, under which the effects in different studies follow a normal distribution.
The random-effect model tends to be less powerful than the fixed-effect model even when the effects are heterogeneous and thus has rarely been used in genetic association studies.
ACKNOWLEDGEMENTS We thank the Associate Editor and three referees for reviewing our work and providing helpful comments.
Funding: National Institutes of Health awards R01CA082659, P01CA142538 and R37GM047845.
Conflict of Interest: none declared.
ABSTRACT Motivation: Phenotypic information is important for the analysis of the molecular mechanisms underlying disease.
A formal ontological representation of phenotypic information can help to identify, interpret and infer phenotypic traits based on experimental findings.
The methods that are currently used to represent data and information about phenotypes fail to make the semantics of the phenotypic trait explicit and do not interoperate with ontologies of anatomy and other domains.
Therefore, valuable resources for the analysis of phenotype studies remain unconnected and inaccessible to automated analysis and reasoning.
Results: We provide a framework to formalize phenotypic descriptions and make their semantics explicit.
Based on this formalization, we provide the means to integrate phenotypic descriptions with ontologies of other domains, in particular anatomy and physiology.
We demonstrate how our framework leads to the capability to represent disease phenotypes, perform powerful queries that were not possible before and infer additional knowledge.
Availability: http://bioonto.de/pmwiki.php/Main/PheneOntology Contact: rh497@cam.ac.uk Received on July 23, 2010; revised on September 13, 2010; accepted on October 9, 2010 1 INTRODUCTION Large-scale genomics research increasingly includes the collection of phenotypic information to infer disease states from genetic conditions.
Similarly, evolutionary studies heavily rely on phenotypic descriptions across species.
Several biomedical databases collect and organize phenotypic information (Bult et al., 2008; Firth et al., 2009; The Flybase consortium, 1999).
To integrate this information across different domains and databases and to communicate the data to the research community, phenotype ontologies were developed which formalize the meaning of terms used to characterize phenotypes (Gkoutos et al., 2004b; Robinson et al., 2008; Smith et al., 2004).
Ontologies are specifications of a conceptualization of a domain (Gruber, 1993) and are used to make the meaning of terms in a vocabulary explicit (Guarino, 1998) such that they can be used for consistency verification, information retrieval and knowledge discovery.
At least two kinds of phenotype ontologies can be distinguished: ontologies in which each term contains one specific phenotypic trait (Robinson et al., 2008; Smith et al., 2004), and ontologies and methods that permit the composition of a term To whom correspondence should be addressed.
through the combination of an entity and a quality (Gkoutos et al., 2004a, b; Mungall et al., 2010).
Each of these approaches describes a phenotype through qualities that are attributes of an entity.
For example, a size of an arm would be described as a quality Size which is the quality of Arm.
In addition to these qualities, phenotype ontologies contain classes describing absence and presence of parts, functions, dispositions and processes, as well as abnormality.
Currently, these features are also represented as qualities and rarely further analyzed.
In particular, the attribution of qualities like Absent or Dysfunctional does not yet enable inferences about the parts or functions of an entity.
Consequently, these approaches fail to interoperate with anatomy or physiology ontologies.
If, however, the meaning of these qualities would be made explicit and classes like Absent or Dysfunctional characterized in terms of the has-part and has-function relations, information flow between phenotype and anatomy or physiology ontologies would be possible, thereby leading to a semantic integration of these ontologies and the capability for expressive queries.
As anatomy ontologies often represent canonical, prototypical organisms, inconsistencies may arise when they are combined with phenotype ontologies (Hoehndorf et al., 2007).
For example, an anatomy ontology may contain a statement that asserts that every human has an appendix as part, while a phenotypic description of a human may assert that this human has no appendix as part.
Because inconsistencies prevent query answering and semantic interoperability, a framework for phenotypic descriptions must accommodate statements about deviations from reference models without leading to inconsistencies.
We provide a method to make the intended meaning of phenotypic descriptions explicit and interoperable with anatomy and physiology ontologies.
For this purpose, we first provide the means to formalize phenotypic traits while reusing classes and relations from other ontologies.
Based on this formalization, we describe how to consistently integrate phenotypic descriptions with canonical ontologies and demonstrate how our method leads to expressive and flexible descriptions of disease phenotypes as well as the possibility for automated inference and knowledge discovery.
We applied our method to examples taken from phenotype ontologies and provide an example ontology which is available from our project web site.
Our framework for formalizing phenotypic descriptions is based on the Web Ontology Language (OWL; Motik et al., 2009).
OWL is based on an expressive, decidable fragment of first-order logic and provides the foundation for the Semantic Web.
To maintain compatibility with currently established methods for representing phenotypes and reuse the data that has been annotated with it, we also demonstrate an implementation in the OBO Flatfile Format.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:39 18/11/2010 Bioinformatics-btq578.tex] Page: 3113 31123118 Phenotype and anatomy ontologies Phene Process phene Object phene Structural phene Qualitative phene Dispositional phene Participatory phene Has-part phene Part-of phene Lacks-part phene Lacks-part-of phene Quality-has-value phene Quality-has-value-in phene Processual-role phene Fig.1.
The first distinction is drawn between phenes of objects and phenes of processes.
We primarily classify phenes of objects into four main categories: structural, functional, qualitative and participatory phenes.
Under the structural phenes, we show possible further classifications based on the relations we use in our method.
Qualitative phenes can be further distinguished into those where only the quality is relevant and those where the qualitys value is considered.
2 SYSTEMS AND METHODS 2.1 Overview An organism is composed of organs, cells and other structural components.
The variability of the structural components can be observed and described with the EntityQuality (EQ) representation based on the Phenotypic Attribute and Trait Ontology (PATO; Gkoutos et al., 2004a; Mungall et al., 2010).
Beyond the structure of organisms and its variability, we can observe processes in which the organism and its anatomical parts may participate.
These processes may lead to changes in the structure or its variability in response to the organisms requirements.
Developmental processes and functions such as growth and synaptic plasticity serve to change the organismsstructure and consequently lead to new dispositions.
Furthermore, all living beings have to adapt to their environment and therefore expose responses in terms of changes to their structure and functioning.
Structure and function form dual components (Hoehndorf et al., 2010a; Wright, 1973) in a system that describes the complexity of the phenotypes.
Many aspects of this complex network of interdependencies can currently be captured using biomedical ontologies.
For example, the components and parts of organisms are described in anatomy ontologies such as the Foundational Model of Anatomy (FMA; Rosse and Mejino, 2003) or the cellular component branch of the Gene Ontology (GO; Ashburner et al., 2000).
Organismal, cellular and molecular processes are described in the biological process branch of the GO, developmental anatomy ontologies are available for various species (Bult et al., 2008; Tweedie et al., 2009) and environmental features can be characterized with the Environment Ontology (Environment Ontology Consortium, 2008).
An ontological framework for phenotypes should enable the description of this variability and complexity, and interoperate with the existing ontologies and resources that were developed for it.
In particular, a framework for representing phenotypes should enable the inference of consistent dependencies between different aspects in this network of dependencies, and provide for the detection of inconsistencies in the phenotypic description of an organism.
We first define a uniform framework for characterizing phenotypic properties of entities.
For this purpose, we introduce the concept of phene that allows to make the semantics of an entitys features explicit and accessible to automated reasoning.
As phenotypic characterizations are usually done comparatively with respect to the features of another reference entity, we show how to use the phene concept to facilitate interoperability of phenotypic descriptions with reference ontologies in biomedicine.
In particular, we demonstrate how to formally represent and infer normality and abnormality.
Based on the development of our formal framework for representing phenotypic information, we demonstrate how to apply it to knowledge discovery and querying across the diversity of knowledge resources containing phenotypic information.
2.2 Formalizing phenotypic traits We define phene as a basic characteristics possessed by an organism.
Phenes are attributive entities which are existentially dependent on a bearer, and phenes characterize the properties of their bearer.
Therefore, we formally define phenes as the properties that are possessed by entities which are Y , and express Y as class-membership in description logic, or as a unary predicates in first-order logic.
We call Y the defining property of a phene.
The defining property of a phene characterizes the phenes bearer, and therefore we can distinguish different kinds of phenes based on the relations that are necessary to formulate this characteristic.
For example, a class of phenes, such as producing endorphines, refer to participation in processes of a certain kind.
A large portion of phenes refer to ontological qualities: being cyanotic, being 7.5m in size.
Further phenes are structural, such as having or not having certain parts, refer to topology (being connected in a certain way) or to dispositions (being able to hear).
To analyze and distinguish these differences, we derive a top-level classification of phenes based on the relations that are necessary to formulate their defining properties.
Figure 1 gives an overview of this top-level classification.
2.2.1 Structural phenes Structural phenes are based on the mereological relation part-of.
The defining property of structural phenes is expressed using the part-of or has-part relation.
Examples of structural phenes are present spleen and absent spleen which are defined as phenes of things which have (or do not have) a spleen as part: Present_spleen equivalentTo: phene-of some (has-part some Spleen) Absent_spleen equivalentTo: phene-of some (not has-part some Spleen) The defining property is has-part some Spleen and not has-part some Spleen, and the instances of the defining property are entities that either have or do not have a spleen as part.
Because our definition of these phenes uses the has-part relation, we can combine phenotypic information with anatomical information, thereby establishing an informative link between both.
2.2.2 Qualitative phenes Another group of phenes represents ontological qualities and we call them qualitative phenes.
Qualities are related to their bearer by the inheres-in relation and its inverse has-quality.
We can express qualitative phenes using our definition pattern in a similar way as structural phenes: as phenes of things in which a certain quality inheres.
For example, the phene being cyanotic is represented as a phene of things which have the quality (has-quality) cyanotic.
We can further refine this by distinguishing qualities from their values and relate both used the value-of and has-value relations.
Then, being cyanotic is represented as a phene of things that have a color within a certain value range.
2.2.3 Function and disposition phenes Another group of phenes is related to functions and disposition of entities, and can be expressed using the has-function, function-of, has-disposition and disposition-of relations.
We call these dispositional phenes.
Intuitively, functions of biological entities establish the reason (or cause) that an entity exists (Wright, 1973) while their dispositions determine their capabilities and potentials (Lewis, 1997).
For example, the endocrine pancreatic cells have a function to produce insulin, and normally have a disposition to produce insulin.
3113 [12:39 18/11/2010 Bioinformatics-btq578.tex] Page: 3114 31123118 R.Hoehndorf et al.In many cases, neither functions nor dispositions are explicitly included in ontologies, but rather the processes that realize these functions or dispositions.
Therefore, dispositional phenes will often be represented using the has-function-realized-by or has-disposition-realized-by design patterns (Hoehndorf et al., 2010a).
Using these, a relation between a class C and a class of processes P is established such that every instance of C has a function (or disposition) that is only realized by processes of the kind P. For example, the endocrine pancreatic cells have a function to produce insulin which is realized through insulin production processes.
The processes that result from the realizations of functions are called functionings (Johansson, 2004).
2.2.4 Functionings and processes An organism or anatomical structure may participate in certain processes, for example in physiological, metabolic or developmental processes, or in the organisms activities.
To participate in these processes is a participatory phene.
Participatory phenes are represented with the participates-in and has-participant relations.
Furthermore, we can distinguish different modes of participation for a phene of a process participant and thus determine how an entity participates in a process.
We represent these phenes with the relation plays-processual-role (Loebe, 2005).
For example, the sinoatrial nodes participate in blood pumping processes and play the processual role of an initiator of the rhythmic excitation of the heart muscle.
Apart from the phenes of process participants, we can distinguish a second kind of phene which represents characteristics of the processes themselves.
We call this kind the process phenes, and they include characteristics of physiological processes, metabolic processes, biological pathways, chemical reactions and their parts.
These processes can have attributes such as duration or a heart beat rate, they can have parts or participants.Although some aspects of our classification of phenes can also be applied to process phenes, we leave a detailed classification of process phenes as subject to future work.
2.2.5 The logic of phenes Axioms for classes and relations: phenes are related to their bearers using the phene-of and has-phene relations.
Because phenes are existentially dependent on their bearer, phenes are always the phene-of some entity.
Furthermore, phene-of is functional, i.e.a phene belongs to at most one entity.
Therefore, the following restriction holds for the class Phene: Phene subClassOf: phene-of exactly 1 Thing The domain of the phene-of relation is the class Phene, while the range is owl:Thing, i.e.the class of all things.
The relation has-phene is the inverse of the phene-of relation, i.e.whenever an individual x is the pheneof an individual y, then y stands in the has-phene relation to x.
Because phene-of is functional, and has-phene is inverse functional.
Applying phenes to entities.
a particular class of phenes P is defined with respect to a defining property Y of its bearers.
Therefore, the definition pattern for phenes is P subClassOf: phene-of some Y or P equivalentTo: phene-of some Y Because the phene-of relation is functional and the has-phene relation inverse functional, the following is true: if an individual i is the bearer of a phene of the kind P, i is an instance of Y.
For example, we have defined the phene Absent spleen as the phene of things that have no Spleen as part.
Based on this phene, we can define a class of Human without spleen as: Human_without_spleen equivalentTo: Human and has-phene some Absent_spleen Based on the inference rules for OWL, we can show that this definition is equivalent to Human_without_spleen equivalentTo: Human and has-phene some (phene-of some (not has-part some Spleen)) and due to the functionality of phene-of and the inverse functionality of has-phene, this is equivalent to Human_without_spleen equivalentTo: Human and (not has-part some Spleen) Composing complex phenes: besides applying phenes to entities, we can introduce intersections and unions of phenes to form complex phenes.
Due to the axioms for the phene-of and has-phene relations, intersections and unions of phenes correspond to intersections and unions of the defining properties of these phenes: when the class of phenes P1 is based on the defining property Y1, and P2 based on Y2, then the complex phene P1 and P2 is based on the defining property Y1 and Y2.
For example, in addition to the Absent spleen phene we define Absent kidney similar to Absent spleen.
Then, we can define a complex phene Absent spleen and absent kidney (ASAK): ASAK equivalentTo: Absent_spleen and Absent_kidney Through inference, we can show that this definition is equivalent to ASAK equivalentTo: phene-of some (not has-part some Spleen) and phene-of some (not has-part some Kidney) and due to the functionality of the phene-of relation, this is equivalent to ASAK equivalentTo: phene-of some ((not has-part some Spleen) and (not has-part some Kidney)) The defining property of ASAK is not defined based on either of the relations in our ontology of phenes, and therefore it is not a primitive phene.
However, through inference we can show thatASAK becomes a subclass both of Absent spleen and Absent kidney.
Reasoning with phenes: phenes can be used to infer additional knowledge and verify consistency.
The simplest case is a contradictory application of phenes, e.g.an individual organism with both the Absent spleen and Present spleen phene.
Due to the definition of both phenes, such an assertion would be inconsistent and can be automatically detected using automated inference.
More importantly, phenes can make use of the information in ontologies to infer that other phenes must hold as well.
For example, when integrated with an anatomy ontology like the FMA, the phene Absent spleen entails the Absent serosa of spleen phene, because Serosa of spleen (FMA:15848) must be a part-of some Spleen according to the FMA.
Similar entailments hold for relations between anatomical entities and their functions.
For example, if endocrine pancreatic cells are the only cells with a function to secrete insulin, their absence can entail the absence of insulin secretion.
2.3 Phenes and comparative descriptions Phenes are properties of entities and phenes whose defining property involves negation can be attributes of a large number of entities.
Therefore, phenes alone should rarely be used to describe the set of phenotypic characteristics of an organism.
As phenotypic descriptions are often comparative statements with respect to a reference model, either a model of normality, a control group, another organism or similar, we can exploit these descriptions to record phenes that characterize deviations from such a reference model.
In particular, we show how to integrate phenes with canonical ontologies, although other artifacts can serve as reference models as well.
3114 [12:39 18/11/2010 Bioinformatics-btq578.tex] Page: 3115 31123118 Phenotype and anatomy ontologies 2.3.1 Interoperability with canonical ontologies Canonical ontologies are those that serve as reference models within their domain and characterize prototypical, idealized entities (Smith and Rosse, 2004).
Phenotypic descriptions and representations should interoperate with such resources and use them to infer knowledge and verify consistency.
When phenes are combined with canonical ontologies, inconsistencies and unsatisfiable classes may arise (Hoehndorf et al., 2007).
For example, the FMA states that every Human has a Spleen as part.
Characterizing an instance of Human h with the Absent spleen phene will lead to an inconsistency: as an instance of Human, h must stand in the has-part relation to some instance of Spleen; because h has a phene of the Absent spleen type, h must not have an instance of Spleen as part.
For the purpose of integrating canonical ontologies and the phene method, we restructure canonical ontologies to explicitly state that they describe only canonical entities.
For example, instead of stating that every human has a spleen as part, we restructure the corresponding anatomy ontology to state that every canonical human has some canonical spleen as part.
For phenotypic descriptions, we need to describe non-canonicity in a flexible way, and we have at least two choices: the non-canonicity of the spleen could either be the absence of a canonical spleen or the presence of a non-canonical spleen.
The first choice is more general, as it allows both for the presence of a non-canonical spleen as well as for the absence of the spleen.
Therefore, we adopt this option and define a non-canonicity of the spleen as a phene of things that have no canonical spleen as part.
A noncanonicity is different from an absence of the spleen, which is a phene of things which have no spleen as part, whether canonical or not.
An absence of the spleen automatically becomes a subcategory of a non-canonicity of the spleen according to this definition.
Formally, we introduce two new unary predicates called C and NC (for canonical and non-canonical, respectively).
In description logics, both C and NC are classes.
C and NC are disjoint and exhaustive, i.e.everything is either C or NC but not both.
Based on these classes, we restructure canonical ontologies such that all occurrences of class symbols X are replaced with X and C. Biomedical ontologies consist to a large portion of statements of the kind X subClassOf: R some Y (Horrocks, 2007).
Assertions of this kind are consequently replaced with X and C subClassOf: R some (Y and C).
For example, the FMA statement that all Humans have a Spleen as part (Human subClassOf: has-part some Spleen) is replaced by all canonical humans have a canonical spleen as part (Human and C subClassOf: has-part some (C and Spleen)).
This replacement can be performed automatically using the OWLDEF method (Hoehndorf et al., 2010b).
Based on these definitions, we can formally define a non-canonicity of the spleen (NCOS): NCOS EquivalentTo: phene-of some (not has-part some (Spleen and C)) Following our pattern, we can define a human with an NCOS as NCOS-Human EquivalentTo: Human and hasPhene some NCOS from which we derive, using deductive reasoning in OWL, that an NCOSHuman has no canonical spleen as part.
We can also prove that a NCOSHuman is a subclass of a non-canonical human, i.e.of Human and NC, because canonical humans must have a canonical spleen as part.
We demonstrate these features in an OWL ontology that formalizes abnormality and absence of the appendix, liver and-cells.
The demonstration ontology is available from our project website.
3 IMPLEMENTATION A large portion of our method is based on description logic and the Web Ontology Language (OWL) (W3C OWL Working Group, 2009).
Using OWL has the advantage that the myriad of software tools, methods and libraries that have been developed for the Semantic Web can be reused with the method.
In particular, OWL has an explicit semantics that makes it amenable for automated reasoning.
However, many biomedical ontologies are developed using the OBO Flatfile Format (Horrocks, 2007).
For our method to be successful, we provide an implementation in the OBO Flatfile Format that is compatible with the description logic treatment of phenes and phenotypes put forward in this manuscript.
For this purpose, we use the OWLDEF method (Hoehndorf et al., 2010b) to provide OWL definitions for relations in the OBO Flatfile Format.
These relations follow our top-level ontology of phenes and are listed in Table 1.
4 DISCUSSION 4.1 Towards canonical disease phenotypes We envision one application of our method to provide a means for representing canonical disease states.
For this purpose, we introduce the notion of canonical disease phenotype: a complex phene which is the combination of those phenes that constitute the idealized or canonical form of a disease.
Some of the phenes in a disease phenotype are dispositional, while others relate to qualities, absence or presence of body parts and participation in physiological processes.
To characterize disease phenotypes further and utilize the existing ontologies for automated inferences and knowledge discovery, several domains have yet to be covered by canonical ontologies so that they can serve as a bridge that links existing resources.
Table 2 provides an overview of available and missing resources and their relation to our method.
To infer and reason over developmental abnormalities, a developmental anatomy must be available.
There are developmental anatomies for several model organisms (Segerdell et al., 2008; Tweedie et al., 2009).
To describe human developmental defects, a human developmental anatomy ontology integrated with the FMA would be beneficial as reference model on which deviations can be based.
Similarly, to describe missing, abnormal or additional dispositions and functions, a functional anatomy ontology should be developed.
Although there are approaches to construct such a resource (Hoehndorf et al., 2010a; Johansson et al., 2005), no comprehensive ontological resource for anatomical functions has been developed so far.
Similarly, almost no ontologies for canonical physiology are currently available.
To describe qualitative phenes such as those relating to color or size, qualitative descriptions can be added to anatomy ontologies.
For this purpose, it is important to use ranges of values for qualities, because values for qualities are often highly variable.
Using the phene method, it would further be beneficial to describe the qualities of the wild type or control group in genetic experiments.
This has the additional benefit of providing additional documentation to an experiment.
Such a documentation can increase the interoperability with ontologies of investigations such as the Ontology of Biomedical Investigations (OBI; Courtot et al., 2008).
The method we propose can be used to represent and infer dependencies between phenotypic traits and verify the consistency of descriptions of phenotypes.
For example, the absence of liver cells entails that they cannot have qualities or functions, because both 3115 [12:39 18/11/2010 Bioinformatics-btq578.tex] Page: 3116 31123118 R.Hoehndorf et al.Table 1.
List of relations defined using the OWLDEF method Relation OWLDEF Example CC-pheneOf-has-part ?X subClassOf: Phene and pheneOf some (has-part some ?Y) Having an appendix as part CC-pheneOf-lacks-part ?X subClassOf: Phene and pheneOf some not (has-part some ?Y) Not having an appendix as part CC-pheneOf-part-of ?X subClassOf: Phene and pheneOf some (part-of some ?Y) Being part of an appendix CC-pheneOf-not-part-of ?X subClassOf: Phene and pheneOf some not (part-of some ?Y) Not being part of an appendix CC-pheneOf-has-quality ?X subClassOf: Phene and pheneOf some (has-quality some ?Y) Having a color CC-pheneOf-lacks-quality ?X subClassOf: Phene and pheneOf some not (has-quality some ?Y) Not having a size CC-pheneOf-has-quality-value-of ?X subClassOf: Phene and pheneOf some (has-quality some (?Y and has-value some ?Z)) Having color #4F1A33 (in RGB color space) CC-pheneOf-lacks-quality-value-of ?X subClassOf: Phene and pheneOf some (has-quality some (?Y and not (has-value some ?Z))) Not having a mass of 0.12g CC-pheneOf-has-quality-value-in ?X subClassOf: Phene and pheneOf some (has-quality some (?Y and has-value-in some ?Z)) Being between 1.2 and 1.7 m in height CC-pheneOf-lacks-quality-value-in ?X subClassOf: Phene and pheneOf some (has-quality some (?Y and not (has-value-in some ?Z))) Not having between 13 and 18 gm/dl hemoglobin concentration CC-pheneOf-has-disposition ?X subClassOf: Phene and pheneOf some (has-disposition some ?Y) Being able to hear CC-pheneOf-lacks-disposition ?X subClassOf: Phene and pheneOf some not (has-disposition some ?Y) Not being able to hear CC-pheneOf-has-disposition-realized-by ?X subClassOf: Phene and pheneOf some (has-disposition some (realized-by only ?Y)) Being able to hear CC-pheneOf-lacks-disposition-realizedby ?X subClassOf: Phene and pheneOf some not (has-disposition some (realized-by only ?Y)) Not being able to hear CC-pheneOf-plays-p-role ?X subClassOf: Phene and pheneOf some (plays-p-role some ?Y) Playing the role of catalyst within some process Table 2.
The table lists dependencies between different kinds of phenes, and resources which are necessary to formally represent them Type Provides Dependencies Relevant Missing Example 1: Example 2: resources resources Diabetes Coagulation Structural Components composing the organism, both macroscopic and microscopic.
Topology of structures.
Structures can be part of larger structures and are the result of developmental processes FMA, GO-CC Developmental anatomy (human)-cells in the pancreas Liver cells Quality Attributes of the structures, observables pertaining to the variability of the structures Qualities are existentially dependent on their bearers.
PATO, HPO, MPO Qualities of anatomical entities Reduced amount of-cells in the pancreas Liver cells are reduced and increased in size, increased fatty acid vacuoles Functional and dispositional Capabilities of the structures Functions and dispositions are existentially dependent on a bearer GO-MF Anatomical functions Function of-cells to produce insulin Function to produce coagulation factors Process Functionings of the structures, changes in the structures and the organism caused through functionings; physiology Processes require structures as participants, and result from functionings of anatomical structures GO-BP, FMA, functional systems Physiology, metabolism Import of glucose into muscle cells, reduction of lipid catabolism Coagulation qualities and functions are dependent on a bearer.
Consequently, physiological processes that rely on liver cells functionings will be impaired.
Similarly, the absence of insulin producing cells will prevent them from exerting their function, leading to a disruption in glucose metabolism and consequently an increased concentration of glucose in the blood.
Because phenes make the semantics behind phenotypic traits explicit, these interrelations can be asserted or inferred, depending on the information present in the corresponding canonical ontologies.
In addition, if cells are absent, they may not be increased in size or have dispositions, and making such an assertion would lead to an inconsistency that can be automatically detected.
3116 [12:39 18/11/2010 Bioinformatics-btq578.tex] Page: 3117 31123118 Phenotype and anatomy ontologies Although the phene method intends to provide a semantic framework for representing phenotypic information, it can also be applied to other domains such as representing clinical information.
In particular, canonical disease phenotypes are similar to canonical conceptual graphs which are used in the Canon Groups model to represent prototypical clinical findings (Friedman et al., 1995).
Consequently, the phene method is currently being applied to represent the canonical disease phenotypes of primary immunodeficiency diseases in the PID Ontology (Adams et al., 2010).
4.2 Phenes and the EQ formalism The phene method can provide a semantics for the Entity-Quality (EQ) formalism and make it amenable to automated reasoning.
EQ is currently widely used to annotate and formalize phenotypes and phenotypic descriptions (Mungall et al., 2010), and its formalization states that an EQ statement is equivalent to a Quality (Q) that inheres-in an Entity (E) (Mungall, 2007).
This approach is strictly weaker than our method and corresponds to the use of the relation CC-pheneOf-has-quality (Table 1) in our method.
The currently employed semantics for EQ has several shortcomings.
First, it is based on the assumption that phenotypic characters are qualities.
Qualities do not allow to infer further information about other kinds of entities such as parts, functions and processes, and therefore the EQ semantics limits interoperability with other ontologies.
For example, having a quality Lacks all parts of type (PATO:0002000) or Lacks function of type (PATO:0001641) formally conveys no information about the parts or functions of an entity.
Even more problematic is the use of the towards relation to specify the kind of entity that is absent: in its currently used semantics (Horrocks, 2007), Absent spleen is interpreted as a Lacks all parts of type quality that is directed towards some Spleen.
In this statement, towards is used in an existential restriction over Spleen, thereby leading to the inference that an instance of Spleen must exist in order to be absent.
The second shortcoming of EQ relates to the combination of qualities, which is important to describe complex phenotypic characteristics or disease phenotypes.
In EQ, qualities from PATO are characteristics such as color or length, and intersections of color and length would be qualities which are both a color and a length at the same time.
Such qualities do not exist in the domain of phenotypes.
However, combined qualities such as Absent spleen and absent kidney are intended to be the qualities of organisms that have both an absent spleen and an absent kidney.
The method we propose overcomes these shortcomings by explicitly defining phenotypic characteristics using relations and classes from other ontologies.
In addition, we demonstrated how phenes can be combined through intersections to form complex phenes.
4.3 Normality and abnormality Being normal and being canonical are sometimes used synonymously, but we make a distinction between both because canonicity does not always coincide with normality.
For example, although the canonical human body represented by the FMA is both male and female (in virtue of having both ovaries and testes as part), normal humans are either male or female, but not both.
Being both male and female would instead be considered to be abnormal.
Additionally, in experiments, normal is defined with respect to a control group of organisms, and not directly based on a canonical ontologies.
Also, normal affects all phenes, while canonicity is often restricted to structure, function and physiology.
Consequently, a tail size of a mouse may be within a certain normal range within a population of mice, abnormal within another population, and not considered in a canonical ontology at all.
The reason is that normal and abnormal are statistical, empirical measures, while canonical and non-canonical are based on prototypical idealizations constructed by humans.
Our method is neutral with respect to these distinctions.
We could introduce another set of predicates, Normal and Abnormal, both of which are neither a sub-nor a super-class of C and NC.
Being Normal would, for example, entail being either male or female, but not both.
In an ontology of Normal humans, sex would either not be considered (when the ontology is open or incomplete in this aspect), or two kinds of humans would be distinguished based on their sex.
An important application is the specification of Normal phenes of organisms within the context of an experiment.
These phenes can include qualities such as fur color, tail size or similar, i.e.the phenes that are measured within an experiment protocol.
Then, an organism or anatomical part of the organism is Abnormal if it lies outside the range of values that is considered normal.
As for canonical and noncanonical entities, a phene P is an abnormality with respect to the Normal phene Q if having the phene P implies not having Q.
4.4 Future research To integrate phenotype and canonical ontologies, we rely on introducing explicit classes for canonicity and non-canonicity.
This permits a consistent integration of these ontologies, but has some drawbacks.
In particular, when an organism has a single deviation from its canonical form, the organism is non-canonical and inferences from assertions in canonical ontologies cannot be drawn anymore.
To prevent this issue, more distinctions than canonical versus non-canonical can be introduced (Rector, 2004).
In the future, we plan to integrate our method of representing phenotypes in a framework that supports default reasoning (Hoehndorf et al., 2007).
4.5 Conclusions We developed a method to represent phenotypic information in ontologies such that the semantics is made explicit.
For this purpose, we introduce a category of phenes.
Phenes are basic observable features of organisms which are defined with respect to a defining property.
Defining properties can include absence or presence of parts or functions.
This defining property is the feature of phenes that allows to make their semantics explicit and facilitate information flow with other ontologies based on the use of common relations.
Using our method to represent phenotypic information, we suggest a new top-level classification of phenotypic characteristics, based on the relations used in the defining properties of the phenes.
The main distinction is between phenes of processes and phenes of objects.
Those of objects are further distinguished into structural, qualitative, functional and participatory phenes.
The first kind pertain to presence or absence of parts, the second to having qualities or values of qualities within certain ranges.
Functional phenes characterize the functionality or dysfunctionality of organisms or their parts.
Participatory phenes characterize the participation and the mode of participation of organisms and their parts in processes.
3117 [12:39 18/11/2010 Bioinformatics-btq578.tex] Page: 3118 31123118 R.Hoehndorf et al.The formal representation of phenotypic information permits the integration of canonical ontologies with phenotype ontologies.
For this purpose, canonical ontologies must be re-interpreted as explicitly referring to canonical entities, while phenotype ontologies can refer to either the canonical, the non-canonical or both kinds of entities.
This simple restructuring, which can be hidden from ontology users, together with our proposed method enables the representation of canonical disease phenotypes as a means for characterizing disease states.
Altogether, we provide an explicit and formal framework for representing phenotypes that is applicable within biomedical ontologies for reasoning, answering queries and integrating knowledge about domains ranging from molecular biology to medicine.
This enables the use of knowledge-based methods to infer, structure, classify and query information about disease and phenotype, thereby facilitating translational research and medicine.
ACKNOWLEDGEMENTS We thank Nico Adams and George Gkoutos for valuable discussions about phenotypes and their representations.
Funding: This research was funded by the European Bioinformatics Institute.
Conflict of Interest: none declared.
Abstract Acinetobacter baumannii is an important opportunistic pathogen in hospital, and the multidrug-resistant isolates of A. baumannii have been increasingly reported in recent years.
A number of different mechanisms of resistance have been reported, some of which are associated with plasmid-mediated acquisition of genes.
Therefore, studies on plasmids in A. baumannii have been a hot issue lately.
We have performed complete genome sequencing of A. baumannii MDR-TJ, which is a multidrug-resistant isolate.
Finalizing the remaining large scaffold of the previous assembly, we found a new plasmid pABTJ2, which carries many phage-like elements.
The plasmid pABTJ2 is a circular double-stranded DNA molecule, which is 110,967 bp in length.
We annotated 125 CDSs from pABTJ2 using IMG ER and ZCURVE_V, accounting for 88.28% of the whole plasmid sequence.
Many phage-like elements and a tRNA-coding gene were detected in pABTJ2, which is rarely reported among A. baumannii.
The tRNA gene is specific for asparagine codon GTT, which may be a small chromosomal sequence picked up through incorrect excision during plasmid formation.
The phage-like elements may have been acquired during the integration process, as the GC content of the region carrying phage-like elements was higher than that of the adjacent regions.
The finding of phage-like elements and tRNA-coding gene in pABTJ2 may provide a novel insight into the study of A. baumannii pan-plasmidome.
Introduction Acinetobacter baumannii is an important opportunistic pathogen, which causes serious nosocomial infections in hospital, especially in the intensive care unit (ICU) [1].
Unfortunately, the multidrug-resistant isolates of A. baumannii have been nces and Huang H et al/ Complete Genome Sequence of pABTJ2 173 increasingly reported in recent years, which largely limited the performance of antibiotics against its infection [2].
Studies on plasmids in A. baumannii have been a hot issue lately, as the presence of some plasmids was closely related with drug resistance [3].
Most clinically significant resistance to carbapenem in this species has been associated with plasmid-mediated acquisition of genes encoding either class B metallo-b-lactamases or carbapenem-hydrolyzing class D OXA-type b-lactamases (CHDLs) [4].
Plasmids are extrachromosomal DNA molecules that are capable of autonomous replication, and they can be inherited both vertically and horizontally in a prokaryotic community [5].
As a mobile element, plasmid may accelerate the dissemination of genetic determinants, such as the spread of resistance genes [6].
We previously reported the complete genome sequence of the multidrug-resistant A. baumannii strain MDR-TJ, which was collected in Tianjin, China in 2012 [7].
The genome of A. baumannii MDR-TJ consists of a circular chromosome and a circular plasmid pABTJ1.
Using bioinformatics tools, we further finalized the remaining large scaffold of the previous assembly [8].
Interestingly, we found that this newlyassembled circular DNA carried many phage-like elements, which was suspected to be a prophage.
This circular DNA molecule was not described when we submitted the previous draft genome.
Further scrutinization of the experimental procedures showed that no cellular lysis was performed following mitomycin C or UV radiation treatment [9], suggesting that this DNA was not likely to be an active prophage.
Moreover, elements related to plasmid replication and stabilization were identified in this new circular DNA.
Therefore, we consider this DNA as a new circular plasmid and name it as pABTJ2.
Here we report the complete nucleotide sequence of pABTJ2 and describe its structure briefly.
Results and discussions The plasmid pABTJ2 was determined to be a 110,967 bp circular double-stranded plasmid with an average GC content of 41.61%.
We annotated 125 coding sequences (CDSs) in pABTJ2, representing 88.28% of the complete sequence.
Notably, a tRNAAsnGTT gene was also found in this plasmid.
Among the protein-coding genes, only 44 CDSs were assigned to a known function.
The map of pABTJ2 is shown in Figure 1.
Replication modules The putative origin of replication (oriV) region contained four imperfectly-conserved 22 bp direct repeats, which may be termed as iterons [10].
The rep gene lies next to the oriV region, encoding a 373-amino acid replicase (also known as the replication initiation protein).
This replicase belongs to the Rep-3 superfamily (pfam0151), similar as the replicase of most A. baumannii plasmids [3].
A. baumannii PCR-based replicon typing (AB-PBRT) [3] is one of the most commonmethods used for plasmid typing inA.
baumannii.
The rep gene in pABTJ2was compared with representatives of each group (GR) categorized by AB-PBRT using BLASTN to identify its closest matches.
However, the rep gene in pABTJ2 showed low similarities with each of these groups.
Therefore, we failed to categorize pABTJ2 into any identified GRs using this method.
These data indicate that pABTJ2 would possess a novel replicon type.
The pABTJ2 also contained many genes that are potentially related to DNA metabolism and replication, such as dnaN, polA, nrdA and nrdB.
These genes are normally located on the chromosome of bacteria or bacteriophages but rarely found in the plasmid.
In bacteriophage T4, NrdA and NrdB, which are part of the primase replication complex, contribute to rapid phage DNA biosynthesis [11].
We speculate that the genes in pABTJ2 may also help improve the replication functions under stressful conditions, such as the high antibiotic environment or in the host [11].
tRNA gene A gene encoding tRNA was identified in pABTJ2, which is uncommon.
This tRNA gene is specific for the second most frequently used asparagine codon, GTT.
tRNA genes are often observed in large DNA viruses or in bacterial genomes [12].
For viruses, tRNA may serve as a recruitment element to neutralize the compositional differences between the phage and the host genome, so as to adjust the translation capacity during infection [13].
However, this may not be the case for pABTJ2, since there are four copies of the tRNAAsnGTT present in the A. baumannii chromosome, suggesting that this tRNAmay be dispensable for inheritance of the plasmid.
tRNA loci commonly serve as insertion sites for mobile elements in the prokaryotic chromosomes, as they are highly conserved between bacteria and thus allow for a promiscuous process [13].
Therefore, the asn tRNA gene carried by pABTJ2 could also be utilized as the recognition site for recombination.
As genetic elements could be probably exchanged between plasmids and the host chromosome, we did a BLASTN analysis between the tRNAAsnGTT gene in pABTJ2 and the ones on the chromosome to uncover the potential origin of the tRNAAsnGTT gene.
However, the nucleotide sequence of the tRNAAsnGTT gene in pABTJ2 shares no significant similarity with the ones on the chromosome, suggesting that this gene may not originate from the host chromosome ofA.
baumanniiMDR-TJ.
The mechanism underlying the presence of tRNA gene in the plasmid remains obscure.
One possibility is that plasmids may be able to integrate into the bacterial chromosome and pick up the chromosomal sequences, such as the tRNA gene, in the course of incorrect excision [14].
For instance, the asn tRNA gene in the plasmid pHCM2 was hypothesized to be acquired through this mechanism [14].
Phage-like elements A cluster of genes encoding phage proteins were detected, which were scattered in a 30 kb region at the end of pABTJ2.
Compared to larger cryptic prophages, these genes are isolated, which were thus proposed to be phage remnants.
Nine phage-like genes were annotated (Table 1), which encode proteins related to packing/morphogenesis and host lysis (muraminidase).
Among them, 7 genes share similarities with those of the Salmonella phage SSU5.
However, compared to Salmonella phage SSU5, pABTJ2 lacks the genes required for the phage life cycle (i.e., receptor-recognizing phage tail fiber adhesion, superinfection exclusion protein and phage repressor) [15].
Instead, pABTJ2 mainly harbors some phage structural genes, which corresponds to the non-inducibility of pABTJ2 as mentioned above.
Since many ORFs included in this region are annotated as hypothetical proteins, it is currently difficult Figure 1 Graphical circular map of plasmid pABTJ2 DNA replication-related or phage-like CDSs (in blue) and the tRNA gene (in pink) were indicated in the forward (Circle 1) and reverse (Circle 4) strands, respectively; whereas the open reading frames (ORFs) were indicated in the forward (Circle 2) and reverse (Circle 3) strands, respectively.
Circles are numbered 1 to 4 from the outside inward.
Note that only ORFs containing more than 100 codons are shown.
The locations of seven nucleotides that are different between pABTJ2 and pOIFC189-111 are put in brown and indicated by arrows, and the corresponding nucleotides on the pOIFC189-111 are marked in parentheses.
Three out of these seven nucleotides are also different between pABTJ2 and pABUH4-111, which are marked with triangles.
The origin of replication (oriV) is marked in green.
Table 1 Characteristics of phage-like elements in pABTJ2 ORF Putative function Length Amino acid similarity (%) ABTJ_00102 Phage terminase large subunit 414 59.97 ABTJ_00105 Phage capsid family 300 58.61 ABTJ_00115 Phage tail tape measure protein 1862 10.86 ABTJ_00116 Phage minor tail 108 31.25 ABTJ_00117 Phage minor tail 231 46.81 ABTJ_00118 Cell wall-associated hydrolases 249 36.80 ABTJ_00119 Phage tail component 192 34.83 ABTJ_00120 Phage tail component 3727 18.27 ABTJ_00126 Phage-related lysozyme 212 42.40 Note: Length of protein is indicated by number of amino acid residues comprised; amino acid similarity (%) of individual ORFs is calculated against those in Salmonella phage SSU5, except for ABTJ_00115 (against Enterobacterial phage mEp390) and ABTJ_00126 (against Acinetobacter phage ZZ1).
174 Genomics Proteomics Bioinformatics 12 (2014) 172177 to predict whether these phage-like elements would confer any fitness advantage to the strain.
The mechanism underlying the presence of phage-like elements in the plasmid is not clear.
According to the GC-Profile [16] (http://tubic.tju.edu.cn/GC-Profile/), the GC content of this region (70,050 nt 106,632 nt) (44.90%) is higher than that of the adjacent regions (38.44% and 39.06%), indicating that the phage-like elements may be acquired through A.baumannii OIFC189 A.baumannii UH2107 A.baumannii MDR-TJ A.baumannii BJAB07104 A.baumannii MDR-ZJ06 A.baumannii BJAB0868 A.baumannii TCDC-AB0715 A.baumannii TYTH-1 A.baumannii 1656-2 A.baumannii ACICU A.baumannii SDF A.baumannii ATCC 17978 A.baumannii D1279779 A.baumannii BJAB0715 A.baumannii ZW85-1 A.baumannii AB0057 A.baumannii AB307-0294 A.baumannii AYE100 100 99 74 52 100 100 96 100 100 100 100100 88 100 0.05 Figure 2 SNP-based phylogenetic tree of A. baumannii genomes 83,560 SNPs were identified on the basis of whole-genome alignment of 18 sequenced A. baumannii strains, which were used to construct a maximum-likelihood tree with 100 bootstrap replicates.
The bar indicates 0.05 substitutions per nucleotide position.
Bootstrap values >50 are shown on the branches.
Huang H et al/ Complete Genome Sequence of pABTJ2 175 horizontal transfer.
Genetic exchanges between plasmids and their host chromosome are common [5].
Considering that chromosome usually harbors prophages, the phage-like elements in pABTJ2 may be acquired from the chromosome via homologous recombination [5].
Other site-specific integration processes may have been functional in the acquisition of mobile genetic elements as well [17].
The predicted asn tRNA gene in pABTJ2 may be used as an insertion site for integration of these phage-like elements.
Generally, the site-specific integration of genetic elements through tRNA could generate two direct repeat sequences on both ends, namely attL and attR [18].
However, no direct repeat portions of the tRNA gene are found elsewhere to suggest that this region functions as an excisable island.
Through an NCBI BLASTN analysis, we found that two plasmids, pOIFC189-111 and pABUH4-111, shared 99% nucleotide sequence identity with pABTJ2.
pOIFC189-111 was extracted from A. baumannii strain OIFC189 that was isolated from combat casualties in Iraq.
It is 110,967 bp long with only seven nucleotides different from pABTJ2.
On the other hand, 111,007 bp pABUH4-111 was extracted from A. baumannii strain UH2107, which was isolated from sputum sample of an inpatient in a US hospital [19].
Compared to pABTJ2, there exist 40-bp stretches of nucleotides in pABUH4-111, which turned out to be duplication located at both the head and the tail of the sequence.
Given the distinct geographical origins, the presence of highly-identical plasmids is intriguing.
It should be noted that, pABTJ2 is neither a conjugative plasmid, as it lacks the required transfer machinery, nor it is a mobilizable plasmid, as it lacks the mobilization regions either and it is much larger than the co-resident conjugative plasmid pABTJ1 [7,20].
Therefore, pABTJ2 should not be able to transfer independently.
Natural transformation or phage transduction may be the alternative mechanism through which transmission is achieved [21].
However, whether these A. baumannii strains are able to engage in natural transformation remains unknown, and the presence of endogenous transducing phages in A. baumannii has not been demonstrated yet.
Besides, due to the high similarity in the plasmid sequences, the genetic relationships of their hosts capture our interest.
We thus constructed a single nucleotide polymorphism (SNP)-based phylogenetic tree using the genomes of 16 completely-sequenced A. baumannii strains and the draft of the A. baumannii OIFC189 and UH2107 published in the NCBI.
It is shown that the A. baumannii MDR-TJ, A. baumannii UH2107 and A. baumannii OIFC189 are very closely related (Figure 2).
Therefore, the close relationship among the bacterial strains may explain in part the high level of similarity observed among the plasmids they host.
The presence of nearly-identical plasmids and closely-related bacterial strains isolated from geographically-widespread areas indicates the global dissemination of these organisms and elements.
Conclusion In this article, we reported the complete sequence of pABTJ2, a newly-identified plasmid, in A. baumannii strain MDR-TJ.
To the best of our knowledge, the existence of the tRNA gene and phage-like elements has rarely been reported in other A. baumannii plasmids.
The tRNA gene is specific for asparagine codon, GTT, and this tRNA gene may be a small chromosomal sequence picked up through incorrect excision during plasmid formation.
The phage-like elements are mainly phage 176 Genomics Proteomics Bioinformatics 12 (2014) 172177 structural genes, which may have been acquired during the integration process.
The exact mechanism of the presence of tRNA gene and phage-like elements has yet to be clarified.
pABTJ2 also encodes many genes that are potentially related to DNA metabolism and replication, which are speculated to improve replication function under stressful conditions.
The presence of nearly-identical plasmids may indicate the global dissemination of plasmids similar to pABTJ2.
Although underlying mechanisms need to be further explored, the finding of pABTJ2 may provide a novel insight into the research of A. baumannii pan-plasmidome.
Materials and methods Genome annotation and bioinformatics analysis The nucleotide sequence of pABTJ2 was obtained from the whole-genome shotgun sequencing of A. baumannii MDR-TJ [7].
Genome annotation was performed using IMG ER [22] and ZCURVE_V [23].
Phage-like elements were predicted according to the annotation results.
tRNA genes were predicted using tRNAscan-SE [24].
The origin of replication (oriV) was identified using Ori-Finder [25].
A circular map of plasmid pABTJ2 was created using the CGView Server [26].
Protein sequences of phage-like elements of pABTJ2 were aligned using DNAMAN v6 software (Lynnon Biosoft, Quebec, Canada).
Nucleotide differences between pABTJ2, pOIFC189-111 and pABUH4-111 were determined using BLASTN.
The GenBank accession numbers for these three plasmid sequences were listed in Table S1.
Phylogenetic analysis Whole genome sequences of 18 A. baumannii strains were obtained from NCBI.
The whole-genome alignment was performed by using MUMmer and 83,560 SNPs were identified, which were used to construct a maximum-likelihood tree with 100 bootstrap replicates [27].
The SNP-based phylogenetic tree was constructed using snpTree [27].
The names of the 18 A. baumannii strains and GenBank accession numbers of their genome sequences were listed in Table S1.
Authors contributions HH and FG conceived the idea and planned the project.
DY and ZLY performed experiments and analyzed the sequence of pABTJ2.
HL and XZ participated in bioinformatics analysis.
DY drafted the manuscript.
All authors edited the manuscript and approved the final manuscript.
Competing interests The authors have declared that no competing interests exist.
Acknowledgements We would like to thank Professor Chun-Ting Zhang for invaluable assistance and inspiring discussion.
The present work was partially supported by the National Natural Science Foundation of China (Grant Nos.
90408028, 31171238, 30800642 and 10747150), the Program for New Century Excellent Talents in University of China (Grant No.
NCET-120396) and Tianjin Municipal Natural Science Foundation of China (Grant No.
09JCZDJC17100).
Supplementary material Supplementary material associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/ j.gpb.2014.05.001.
ABSTRACT Motivation: For the biologist, running bioinformatics analyses involves a time-consuming management of data and tools.
Users need support to organize their work, retrieve parameters and reproduce their analyses.
They also need to be able to combine their analytic tools using a safe data flow software mechanism.
Finally, given that scientific tools can be difficult to install, it is particularly helpful for biologists to be able to use these tools through a web user interface.
However, providing a web interface for a set of tools raises the problem that a single web portal cannot offer all the existing and possible services: it is the user, again, who has to cope with data copy among a number of different services.
A framework enabling portal administrators to build a network of cooperating services would therefore clearly be beneficial.
Results: We have designed a system, Mobyle, to provide a flexible and usable Web environment for defining and running bioinformatics analyses.
It embeds simple yet powerful data management features that allow the user to reproduce analyses and to combine tools using a hierarchical typing system.
Mobyle offers invocation of services distributed over remote Mobyle servers, thus enabling a federated network of curated bioinformatics portals without the user having to learn complex concepts or to install sophisticated software.
While being focused on the end user, the Mobyle system also addresses the need, for the bioinfomatician, to automate remote services execution: PlayMOBY is a companion tool that automates the publication of BioMOBY web services, using Mobyle program definitions.
Availability: The Mobyle system is distributed under the terms of the GNU GPLv2 on the project web site (http://bioweb2.pasteur.fr/ projects/mobyle/).
It is already deployed on three servers: http://mobyle.pasteur.fr, http://mobyle.rpbs.univ-paris-diderot.fr and http://lipm-bioinfo.toulouse.inra.fr/Mobyle.The PlayMOBY companion is distributed under the terms of the CeCILL license, and is available atContact: mobyle-support@pasteur.fr; mobyle-support@rpbs.univ-paris-diderot.fr; letondal@pasteur.fr Supplementary information: Supplementary data are available at Bioinformatics online.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
1 INTRODUCTION Over the last 10 years, an increasing number of bioinformatics tools, covering a growing spectrum of applications, including aspects of genomics, systems and structural biology have been made available to the community.
The complexity of analyses undertaken in these domains makes the publication, integration and interconnection of these tools particularly challenging.
Several systems propose a solution to automate the access to bioinformatics resources.
Web-based user interfaces, such as the Biology Workbench (Subramaniam, 1998), PISE (Letondal, 2001), wEMBOSS (Sarachu and Colet, 2005), Galaxy (Giardine et al., 2005), GenePattern (Reich et al., 2006), MOWServ (Navas-Delgado et al., 2006), the New Generation Biology Workbench (Rifaieh et al., 2007), BioManager (Cattley and Arthur, 2007) and BioExtract (Lushbough et al., 2008), simplify the access to powerful computer resources by providing a familiar graphical-based environment for inexperienced users and by saving them from installing software on their own computer.
In contrast with these tools which focus on the execution of programs, and where the users can often interactively chain analyses, GBrowse MOBY (Wilkinson, 2006) and Seahawk (Gordon and Sensen, 2007b) propose data-centric solutions where the user can explore a set of BioMOBY (Wilkinson, 2004) services to analyse a set of given data, with edition, navigation and visualization components which fully exploit the composite nature of BioMOBY objects.
Standalone workflow systems such as Taverna (Oinn et al., 2004), Kepler (Altintas et al., 2004) or BioSide (Hallard et al., 2004) enable to combine tools within desktop applications, using graphically specified workflows.
It is hence possible, for bioinformaticians, to organize and automate complex data processing.
Such possibilities have also been offered on web interfaces, either in dedicated systems such as Remora (Carrere and Gouzy, 2006), or within some of the web-based systems cited above.
PISE, Galaxy and BioExtract also offer the possibility to save interactively designed protocols.
Many of these systems now offer the possibility of accessing distributed resources, often using dedicated web-service solutions such as BioMOBY and SoapLab (Senger et al., 2003).
While SoapLab offers a system to automate the distribution of asynchronous analytical services, the BioMOBY protocol provides in addition more detailed descriptions, including semantic metadata and a registration system to facilitate the discovery of relevant resources.
A number of tools facilitates the publication and The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:28 4/11/2009 Bioinformatics-btp493.tex] Page: 3006 30053011 B.Nron et al.management of BioMOBY services, such as MoSeS (http://biomoby.open-bio.org/CVS_CONTENT/moby-live/Java/docs/ Moses-generators.html) and the BioMOBY dashboard (http://biomoby.open-bio.org/CVS_CONTENT/moby-live/Java/docs/ Dashboard.html).
Many of the existing user interfaces rely on the descriptions provided by these systems to generate user interfaces: Ajax Command Definition from EMBOSS (Rice et al., 2000), BioMOBY or even pure WSDL definitions.
Mobyle is a generic web-based framework.
While including advanced technologies such as web services, remote execution and dataflow mechanisms, it addresses major end-user concerns arising from the use of sophisticated bioinformatics systems.
A recent study (Gordon and Sensen, 2007a) shows how simple issues in the user interface can impede the use of a system by scientists, causing them to waste time and even preventing them from using it further.
To address this issue, the design of Mobyles interface is user-centered, to provide a usable yet customizable access to a large panel of services, from genome analysis to structural bioinformatics.
The description language it uses to generate user interfaces is sufficiently extensive and flexible for rich user interfaces to be generated.
Its core partly relies on concepts previously embedded in PISE and the RPBS portal (Alland et al., 2005).
Moreover, different Mobyle servers can be federated to integrate services distributed over various sites.
This functionality enables the federation of a network of curated portals, combining the skills of each of them.
The Mobyle program description language also enables to define web services, as shown by the PlayMOBY Mobyle companion tool, which automates the publication of BioMOBY web services, and has monitoring capabilities that provide the bases for service quality monitoring.
2 SYSTEM DESIGN The Mobyle design process was based on a user-centered design process (Javahery et al., 2004; Letondal and Amanatian, 2004; Shachak et al., 2007), involving numerous interviews and a number of users workshops (see Section 4.1).
We identified the major end-user concerns that needed to be addressed: (1) An integrated bioinformatics framework needs to provide end users with several important capabilities: (i) reuse of data and results without the burden of unsafe copyand-paste operations; (ii) management of scientific studies (e.g.retrieving parameters of a job, comparing results or relaunching an analysis using different data); and (iii) access to a wide range of local or remote services within a unique user interface.
(2) A bioinformatics framework should enable scientists to share knowledge: (i) on setting up analyses, through tutorials based on curated examples; (ii) by accessing protocols designed and validated by experts in the domain; and (iii) through the use of a confidence network, allowing the interconnection of distributed resources, taking advantages of local skills to improve service quality, instead of favouring centralized platforms.
Technical requirements for such criteria to be satisfied include: (i) a well-designed web user interface, enabling easy navigation and data management within a user workspace; (ii) a distributed architecture, enabling access to and combined use of local and remote services; (iii) workflow authoring and enacting tools to support protocols; (iv) sound software architecture and APIs, to simplify system maintenance, configuration and extension; (v) a flexible description language to enable domain-specific adaptation needs, including visualization of results using special-purpose graphics components.
3 SYSTEM OVERVIEW In this section, we provide an overview of Mobyle.
We describe the underlying concepts of the system, the design of the web user interface, the server components and the distributed architecture.
3.1 Concepts 3.1.1 Homogeneous user interfaces to heterogeneous programs The steep learning curve involved in the use of command-line tools invocation is problematic for inexperienced users (Shneiderman, 1983).
A classical solution involves wrapping each application in a custom CGI script and providing a web interface to reduce the burden of remembering the syntax of the command line and the name of the parameters.
Given the sheer number of bioinformatics software programs available, we chose to automate the generation of web interfaces using an abstract definition of a programs parameters.
This solution provides a homogeneous interface to all the programs, minimizing the syntactic complexity.
It is also helpful in file management issues and program chaining.
3.1.2 Persistent user workspaces One of the shortcomings of the PISE environment is its lack of support for a persistent user workspace (Gilbert, 2002).
The new system, while still allowing a fast and temporary guest access to the programs, also gives to the users the possibility to create registered accounts, which allows user data and jobs to be maintained and managed across multiple work sessions.
3.1.3 XML description of interfaces Based on feedback from PISE server administrators, as well as the need to extend the capacity of functionalities such as web service wrapping or workflow management, we modified the schema that describes the system.
All Mobyle data, including program descriptions, job definitions and user workspaces are stored in XML documents.
The main element of the system, the program description, is inherited from the PISE system.
It describes various aspects, in a language that targets simplicity as much as possible (experience shows that the authoring and maintenance of program descriptions is a complex and tedious task).
Aspects covered include: Program/service documentation: describes the accomplished task and guides users through the process.
It also provides authoring and version informations.
Data typing: characterizes the parameters and results of the different programs.
Further details are provided in Section 3.3.
Wrapping instructions: translates a user request into a valid execution of the program (e.g.in the case of a unix program call, the construction of the command line will be defined by rules included in the XML file).
User interface layout: the layout of the program invocation form and job result pages are by default automatically generated from the XML definition, but can be customized.
3006 [09:28 4/11/2009 Bioinformatics-btp493.tex] Page: 3007 30053011 Mobyle Fig.1.
Web user interface: portal overview, displaying the multiple alignment CLUSTALW submission form.
The form in the main part displays a control, the databox, where the user can either paste data, enter a database ID or upload a file.
The user can also select a previously uploaded file from the File bookmarks menu.
When available, results from previous jobs are also provided through a Results menu.
BioMoby integration: the XML files can be enriched with metadata such as BioMoby parameter types, which permits the publication of a program as a BioMoby service using PlayMOBY (see Section 3.6).
A formal description of the XML grammar used to describe Mobyle programs is available on the project webpage (see http://bioweb2.pasteur.fr/projects/mobyle/downloads.html), using Relax NG (Clark et al., 2001).
3.1.4 Network-enabled bioinformatics tools The Mobyle system aims at acting as a hub for a set of programs of interest.
A program integrated in Mobyle can not only be local but also remote, using the Mobyle Net functionality (further described in Section 3.5).
Additionally, current and future developments aim at providing gateways to web-service-based systems such as BioMOBY (see Section 3.6).
The interest is to provide a unified framework for bioinformatics platforms maintainers, who have to publish and integrate various local or remote programs for both biologists and bioinformaticians.
3.2 User interface design Mobyle provides the scientist with a global and integrated view of all the elements needed to perform his or her analyses.
At one glance, the user can see which programs are available and which analyses have already been run.
The portal is organized in three main parts (Fig.1): a left navigation panel, a central panel displaying a selection of elements of the current work, such as forms and results, and horizontal tabs enabling the user to navigate between these elements.
3.2.1 Program search Available programs are classified so that they can be searched, using program and parameter names, prompts and documentation, or bibliographic references.
The page that displays the results of a given analysis also provides the user with the list of programs that can be run for further analysis.
This list guides users, restricting them to a view of programs that are compatible with a given result file.
3.2.2 User workspace At any time, the user can navigate between previously used forms and details of individual analyses, for instance to compare results.
This view persists over separate uses of the portal during a given amount of time.
This navigation model gives a flat multi-directional access to several forms, which was not previously possible with the classical browser history mechanism.
A list of bookmarked data is available: this provides the user with reuse-based access and information on previously submitted data.
3.2.3 Forms and reusability Program forms are classical HTML forms.
They contain a specific control elementthe databox designed to facilitate data reuse by biologists.
This control allows users to set parameter values for biological data (e.g.DNA sequence or 3D protein structure) using various methods (pasting, file upload, databank entry retrieval, bookmark reuse).
3.2.4 Results and interactive chaining The job status and results page provides a preview of the job data and metadata.
It also includes an advanced control elementthe resultboxwhich lets users access results in plain browser windows, download them to their workstation, bookmark them or pipe them to a new form, i.e.display a new form that is preloaded with the user data.
This ability to pipe data into a new form, together with databox bookmark reuse, permits interactive program chainings within the portal.
3.2.5 Flexible layout design The portal, given a program definition, can automatically generate a form and a result page without any need for layout information.
However, this process can be overridden to generate program-specific forms and job result pages with particular layouts.
This possibility can prove extremely useful, for instance to fulfil custom program requirements, or to define a particular layout required for optimal use of a given program 3007 [09:28 4/11/2009 Bioinformatics-btp493.tex] Page: 3008 30053011 B.Nron et al.Fig.2.
Web user interface: example of customised interface, showing the simultaneous display of a program output in both a text box and in a Jmol applet.
(see Fig.2).
This functionality relies on via specific layout tags or even HTML snippets embedded in the XML file, which can include inline javascript or Java applets.
3.3 Data typing and helpers A major aspect of Mobyle is its capacity to facilitates automated data conversion and formatting for service integration, thus saving the user from tedious and non-scientific data manipulation tasks.
The Mobyle typing system describes program parameters using a typing mechanism that aims to help users in such tasks.
It modifies the interface display, the controls for user values (the input parameters), the possibility of chaining between programs and data reusability.
Similarly to SWAMI (Rifaieh et al., 2007), data and parameter characterization is multidimensional: the biotype describes the biological object (e.g.nucleic acid, protein or drug); the datatype describes the data structure (e.g.sequence, alignment and matrix, but also more basic types for non-bio parameters such as string or integer); the format (blast html report, fasta sequence, phylip distance matrix).
The type of a data item or a parameter is used to: detect, check and convert the format of the data provided by the user, based on external ancillary tools; filter the available data sources (banks) that can be used to load data directly into a given tool; filter the data bookmarks that can be reused in a new analysis; filter the programs that a given result can be piped to; offer specific visualization options in the interface (such as Java applets) (currently in development).
In contrast with other typing systems, such as the one offered by BioMOBY, the parameter compatibility between two tasks is based on a more abstract description.
Since the Mobyle system can convert the data to the format which is accepted by the next task, there is no need to have service interoperability rely on a bioinformatics format description.
For instance, in the Mobyle Pasteur server, Fig.3.
Mobyle components overview.
the user can align a set of protein sequences using the clustalwmultialign interface, then pipe the clustalw-formatted result to Phylip protdist: although this program only accepts Phylip-formatted multiple alignments, the squizz program, which handles sequence and alignment conversions, will automatically detect and convert the multiple alignment into the accepted format.
This approach simplifies the construction of the protocol: the user does not have to specify data transformation tasks, allowing optimal handling of the data (the format conversion information is directly accessible in the job result page).
This typing system is configurable and relies on a best-effort approach.
Tasks such as format detection or conversion are handled by the system only if they are compatible with the local configuration.
This feature underlies the difference in what is considered to be a service between Mobyle and BioMOBY: in Mobyle, data format conversions should be, as far as possible, considered as mere connectors in the analysis dataflow, whereas in BioMOBY they are considered to be on the same level as other analysis programs.
Furthermore, since some programs accept multiple data formats as inputs, publishing them in BioMOBY involves registering multiple services on the registry (one per accepted format), whereas they are published as a single interface in Mobyle.
3.4 Components The Web portal provides a unified access to the services available on the server which is briefly described on Figure 3.
We use Ajax (Garrett, 2005) to coordinate required information and to enable the user to explore various functionalities on a single page.
For instance, a user action such as a job submission triggers an update of the users job list and the data bookmarks list.
This event triggers an update of the history data available in the program forms, without the page having to be completely reloaded.
The Mobyle server, based on a set of python modules, handles the various aspects of job, data and session managements.
The most important of these, job submission, includes: security validation of parameters and semantic validation (data types, range of value, compatibility between options); command-line construction; submission to an execution system, which could be local or on a cluster managed by SGE or torque; job cancelling or status querying; data management: all data needed to replay a job are stored (inputs, outputs, secondary parameters, etc.
), ensuring the traceability of each job.
3008 [09:28 4/11/2009 Bioinformatics-btp493.tex] Page: 3009 30053011 Mobyle Fig.4.
Mobyle network architecture.
The Mobyle server can invoke either local or remote programs, integrating them in a single work environment.
The server also has a substantial capacity for configuration, such as setting program access permissions based on request address, user limit for space size and handling user account types (registered and/or guest accounts).
All of these features are based on server administrator needs.
3.5 The Mobyle network The Mobyle network allows the execution of programs that are available on different Mobyle servers from a single portal.
This feature facilitates user access to services physically distributed over different Mobyle servers, within an integrated environment enabling data reuse and program chaining.
As opposed to BioMOBY, Mobyle does not provide any central repository to register new services, relying instead on a distributed registration: each Mobyle server administrator chooses to publish a number of services, which can be upon his choice exported to or imported from remote Mobyle servers.
Remote program invocation relies on the import of the remote program XML descriptions.
Upon submission, this allows to generate a call to the remote server instead of generating and submitting a local batch call.
In such a configuration, the user workspace, as well as the local jobs, remain stored on the initial server, but remotely invoked jobs are directly linked from their execution servers (Fig.4).
We demonstrate the use of this facility for the easy comparative modelling of the structure of a protein identified from analysis of the sunflower genome.
A step by step tutorial is available on the MobyleNet help pages (http://mobyle.rpbs.univ-paris-diderot.fr/ help/MobyleTutorials_network.html).
This task can be broken down into several successive steps: inferring protein sequence from the genomic information, performing a search for a template structure, fitting the query sequence to the template and then generating a 3D model.
Protein sequence inference can currently be performed through the LIPM Mobyle server, using the heliagene resource dedicated to sunflower genomic information.
The next steps can be achieved through the RPBS Mobyle server using a two pass blast (PDBblast service) to identify candidate template structures from the query sequence.
Sequence alignment can be performed through the Institut Pasteur Mobyle server using the clustalw service.
Finally, the 3D modelling and model visualization/analysis can be performed through the RPBS site (Fig.2).
The Mobyle network allows efficient organization of the complete sequence of tasks as a pipeline, without having to leave the RPBS Mobyle portal, but making use of the resources and services at the three sites.
3.6 Deploying BioMOBY services from Mobyle program descriptions, using PlayMOBY PlayMOBY is an external Mobyle companion tool allows the publication of BioMOBY-compliant web services, using Mobyle program descriptions.
From the same XML description, bioinformatics programs can thus be both published on the Mobyle network and used by the code generator of PlayMOBY in order to implement, register and validate BioMOBY web services automatically.
PlayMOBY also integrates a perl library to generate the corresponding Mobyle XML description file from program parameters.
Thus, PlayMOBY reduces the overheads of publishing BioMOBY web services.
To date, PlayMOBY handles more than 100 BioMOBY web services for providing generic sequence analysis tools and ensuring the interoperability of specialized databases (http://www.legoo.org, http://www.heliagene.org and http://narcisse.toulouse.inra.fr).
In addition to web service deployment, PlayMOBY provides daily monitoring tools for the deployed web services.
During the Mobyle to PlayMOBY format conversion step, a service developer can provide a test dataset for each service input.
These data are used to test web services availability and consistency by the provider himself.
We made the choice to not allow web services users to launch these tests, which are instead automatically launched.
The tests consist in verifying accessibility to the web service and consistency and stability of the results on test data.
Their results are published as XML reports and RSS feeds.
The PlayMOBY description for the QoS presently follows the specifications stated by the BioWorkflow group (Wessner et al., 2008).
However, it could easily be extended to other frameworks such as the BioCatalogue project (Belhajjame et al., 2008).
In the case of failures or unexpected results, alerts are sent by e-mail to the developers.
Thus, PlayMOBY also provides the framework to evaluate Mobyle service reliability and curation.
This problem is particularly important in bioinformatics, given the volatility of tools (especially web-based), impairing the reliability of biological data processing.
4 DISCUSSION AND PERSPECTIVES 4.1 System design: a user-centered approach Over a period of 4 years leading up to Mobyles first release, a series of about 30 user interviews and participatory design workshops involving brainstorming sessions and video prototyping were conducted.
These were used to gain a deeper understanding of how a web portal could facilitate the use of bioinformatics tools by biologists.
These studies are described in detail elsewhere (Letondal and Amanatian, 2004).
The results showed the principal needs of biologists to be: (1) a stable and predictable set of known tools integrated in a portal designed for inexperienced users; (2) an overview of the analyses and careful organization of results; (3) reusability features to re-execute previous commands; and (4) userdefined and ready-to-use analysis pipelines, similar to benchmarked protocols.
We also concluded that caution should be taken regarding advanced features.
Indeed, (5) biologists are often sceptical about sophisticated tools that are difficult to understand: they have to be able to anticipate the benefits that such tools may provide.
Before opening the portal, we invited users to participate in test sessions.
We selected 16 users among the users of the previous portal, ranging from beginners to frequent users, to participate in test 3009 [09:28 4/11/2009 Bioinformatics-btp493.tex] Page: 3010 30053011 B.Nron et al.sessions.
We used the think-aloud method (Nielsen, 1994), involving users working in pair.
Participants were told they could bring their own data.
Most of them tried 2D structure analysis, alignment and phylogenetic programs.
Several users perceived the new system as more complex, but more complete than the current one.
They appreciated being able to retrieve jobs results, particularly from the databox menus.
Most of the concerns, which were consequently solved, were related to (i) having to navigate back and forth between forms and results; (ii) user accounting; (iii) not fully understanding some English terms (such as pairwise alignment) used in program classification; and (iv) ambiguities concerning data storage.
The Institut Pasteur, RPBS and LIPM Mobyle servers have been publicly available since January 2008, October 2008 and January 2009, respectively.
The Pasteur server provides access to a wide range of bioinformatics tools mainly focused on sequence analysis and phylogeny; the LIPM server offers access to tools related to plant genomics; and the RPBS server publishes programs that mainly concern structural bioinformatics.
Over 400 000 jobs have already been submitted by over 40 000 different users throughout the world on these servers.
4.2 Original contributions The Mobyle project arose from the natural evolution of software such as PISE, offering a simple way to publish and to share bioinformatics software on the Web.
Program chaining is limited to the most common usage patterns that we identified, and does not provide any of the advanced workflow patterns included in previously described systems (Taverna, Kepler).
It is not either intended to handle analyses of very large quantities of data, data transfer over the network being too costly for web-based applications.
Several aspects make this approach novel: the simplicity of the interface, aiming to provide advanced functionalities without imposing the steep learning curve generally associated with complex systems.
For instance, data can be directly uploaded onto the program form (without having to navigate to a separate upload interface) and the data format is automatically detected and validated.
This simplicity is in the continuity of the PISE approach, with the addition of new features such as a persistent workspace and multiple functionalities to facilitate data reuse.
flexibility in the design of the program user interfaces (submission forms and job results pages, as described in Section 3.2.5) and in the configuration of the system.
the Mobyle network enables the integration of local software with remote services, reducing the costs associated with the local maintenance of an exhaustive list of programs.
Application of this strategy over several Mobyle sites should help to connect providers, generating a framework that emcompasses a wide spectrum of applications and covers complementary aspects of bioinformatics.
This strategy needs to be tested for a large number of sites; however, we anticipate the generation of a confidence network combining specific tools offered by each platform and thus promoting quality management of the services.
deployment of BioMOBY web services from bioinformatics programs is made available through PlayMOBY.
It thus benefits from the conceptual strength of the BioMOBY architecture, while avoiding the duplication of wrapper authoring effort, using a common format for both Mobyle and BioMOBY publications.
Additionally, it encourages good practices such as services monitoring by integrating the design of tests into the publication process.
4.3 Current developments Workflows and protocols: Mobyle already embeds a prototype dataflow-oriented workflow engine, enabling the chaining of successive or parallel tasks to be automated.
It runs on top of the Mobyle core library, exploiting the Mobyle network to execute tasks distributed between local and remote servers, and orchestrates the tasks to synchronize their execution with the availability of all their input data.
Mobyle workflows modelize a set of tasks, but in contrast with existing workflow languages such as SCUFL in Taverna (Oinn et al., 2004) or MoML in Kepler (Altintas et al., 2004), do not specify explicitly some low-level data format detection and transformation tasks.
Authors are, whenever possible according to local configuration, not asked to specify these steps.
Hence, the storage of workflow definitions is planned to be based on an extension of the current Mobyle XML language, as they are based on a higher level perspective on the analyses they describe.
An interesting outcome is the possibility to save an interactively designed program chaining as a reusable composite service.
This perspective provides a gateway towards the publication of useracknowledged protocols on a community-based platform such as myexperiment.org (De Roure et al., 2007).
Future end-user access to the workflow definitions and workflow engine will be integrated to the current web interface.
Web services: the PlayMOBY system already allows the automated publication of BioMOBY web services using Mobyle program definitions.
Future work also includes enabling the use of other approaches such as custom SOAP or REST interfaces, and SoapLab.
Enabling the integration of such services as remote programs, for which the Mobyle Web Portal acts as a client will also be considered.
Didactic tutorials: further developments will encompass the design of tutorials.
As explained by Cattley and Arthur (2007), integrated web portals are a very efficient learning and teaching tool.
Mobyle additionally provides technical support for implementing interactive tutorials.
A major issue to be incorporated in its design is to make more interactive components available: as shown by user studies, workflows should be available as semi-automatic protocols, with interactive components enabling the biologist to customize the visualization of the result from an analysis (e.g.a 3D molecule or an alignment).
5 CONCLUSIONS The design of Mobyle, which provides an effective way to make a large panel of curated bioinformatics tools available in a homogeneous environment, has been driven with the concern to meet the requirements of different audiencesbiologists and bioinformaticians, mostly.
Our objective was thus to facilitate the access to complex features and advanced technologies by a design 3010 [09:28 4/11/2009 Bioinformatics-btp493.tex] Page: 3011 30053011 Mobyle that suits the work of biologists, which we were able to observe and understand during a number of interviews and workshops.
Funding: the Agence Nationale de la Recherche (GPLA06026G ANR Genoplante, to PlayMOBY project ).
Conflict of Interest: none declared.
ABSTRACT Motivation: Tumors acquire many chromosomal amplifications, and those acquired early in the lifespan of the tumor may be not only important for tumor growth but also can be used for diagnostic purposes.
Many methods infer the order of the accumulation of abnormalities based on their occurrence in a large cohort of patients.
Recently, Durinck et al.(2011) and Greenman et al.(2012) developed methods to order a single tumors chromosomal amplifications based on the patterns of mutations accumulated within those regions.
This method offers an unprecedented opportunity to assess the etiology of a single tumor sample, but has not been widely evaluated.
Results: We show that the model for timing chromosomal amplifications is limited in scope, particularly for regions with high levels of amplification.
We also show that the estimation of the order of events can be sensitive for events that occur early in the progression of the tumor and that the partial maximum likelihood method of Greenman et al.(2012) can give biased estimates, particularly for moderate read coverage or normal contamination.
We propose a maximum-likelihood estimation procedure that fully accounts for sequencing variability and show that it outperforms the partial maximum-likelihood estimation method.
We also propose a Bayesian estimation procedure that stabilizes the estimates in certain settings.
We implement these methods on a small number of ovarian tumors, and the results suggest possible differences in how the tumors acquired amplifications.
Availability and implementation: We provide implementation of these methods in an R package cancerTiming, which is available from the Comprehensive R Archive Network (CRAN) at http://CRAN.Rproject.org/.
Contact: epurdom@stat.Berkeley.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on August 29, 2012; revised on April 26, 2013; accepted on September 18, 2013 1 INTRODUCTION Tumors accumulate large numbers of mutations and other chromosomal abnormalities due to defects in the genomic repair mechanisms of tumor cells.
Not all of these abnormalities are believed to be crucial for tumor growth and progression, and a question of great importance is to try to identify the critical abnormalities.
One possible indicator of the importance of an abnormality is when it occurred, relative to other abnormalities.
The most straightforward approach for determining the progression of these abnormalities is to evaluate multiple samples from the same individual, such as primary and metastatic samples (Frumkin et al., 2008; Gerlinger et al., 2012; Nishizaki et al., 1997; Sasatomi et al., 2002), sub-clonal populations (Campbell et al., 2008), or different portions of the same tumor (Navin and Hicks, 2010; Siegmund et al., 2009).
It is usually difficult to have data on multiple time points in the progression of an individual tumor; rather it is more common to have cross-sectional data with a single time point from multiple individuals.
In this case, we cannot directly observe the accumulation of genomic abnormalities and must infer it.
There has been a great deal of interest in identifying driver mutations and events based on the frequency of their occurrence across patients (e.g.Beroukhim et al., 2007; Cancer Genome Atlas Research Network, 2008, 2011; Taylor et al., 2008).
Many statistical methodologies rigorously analyze frequencies of aberrations to determine those that are significantly represented in the population, with specific statistical methods developed for mutations, copynumber abnormalities and others types of genomic profiles (Beroukhim et al., 2007; Brodeur et al., 1982; Huang et al., 2007; Newton and Lee, 2000; Newton et al., 1994, 1998; Taylor et al., 2008).
Yet, these techniques do not explicitly attempt to estimate the order of occurrence.
Many methods do explicitly estimate a common temporal ordering among samples based on the co-occurrence across patients.
Fearon and Vogelstein (1990) first proposed a temporal ordering of mutations based on the mutations in colorectal tumors from different stages.
Since then, a great deal of methodological work has formalized this work.
For example, oncogenetic tree models (Desper et al., 2000) cast this notion in a probabilistic setting, which later work extended and generalized (Beerenwinkel et al., 2005a, b, 2006; Gerstung et al., 2009; Hjelm et al., 2006; Liu et al., 2009; Newton, 2002; Rahnenfuhrer et al., 2005; Simon et al., 2000).
Bilke (2005) modeled the critical elements of tumor progression in neuroblastoma by analyzing the sets of shared mutations between the stages of a tumor and finding the most likely model of progression between stages of aberrations.
Other approaches rely on stochastic models of cellular*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
All rights reserved.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Frumkin et al., 2008; Gerlinger et al., 2012 ; Navin and Hicks, 2010 ) ) copy , ; Newton, 2002; Beerenwinkel et al., 2005a,b; Rahnenf&uuml;hrer et al., 2005; Hjelm et al., 2006; Beerenwinkel et al., 2006; Gerstung et al., 2009; Liu et al., 2009 growth, such as the algorithm RESIC (Attolini et al., 2010), which models the overall accumulation of abnormalities in a population of tumors based on a probabilistic model of cell division and fitness of mutations.
This is not an exhaustive review, as many other approaches to this problem exist, but a common feature is estimating a common temporal ordering by comparing across many samples.
The simulation study of Sprouffske et al.(2011) compares estimates of tumor progression that use multiple samples from a single tumor/patient with those obtained from cross-sectional samples from many independent tumors and finds that the crosssectional estimates can be quite misleading due to the heterogeneity of paths to tumorigenesis seen in tumors.
In Durinck et al.(2011), we introduced a novel approach for temporal ordering of genomic abnormalities that instead focused on assessing from a single sample of a single individual the internal ordering of chromosomal abnormalities.
In that work, we dealt with the narrow case of ordering regions of copy-neutral loss-of-heterozygosity (CNLOH), when one copy of the chromosome is deleted and replaced by the other copy.
Greenman et al.(2012) gave a generalization to general chromosomal amplifications, and in Nik-Zainal et al.(2012) applied the technique to 21 breast cancer samples.
The model proposed by Durinck et al.(2011) and Greenman et al.(2012) is general in principle and offers a remarkable ability to analyze the history of a single tumor.
However, there has been little examination of the performance of the estimates of the temporal orderings.
We show that with higher levels of amplification, most events result in non-identifiable models, meaning that most regions with high levels of amplifications cannot be timed in this way.
Furthermore, differences in estimation procedures can have important effects on the quality of the estimate, particularly for events that occur early in tumorigenesis, and therefore are of particular biological interest.
The method of Greenman et al.(2012) uses a partial maximum-likelihood estimation (MLE) technique, which can perform badly for early (and late) events.
We introduce a full MLE procedure that accounts for sequencing variation, which we show performs better for estimating early events, particularly for samples with moderate read coverage.
We also introduce a Bayesian estimation procedure, which can, in some situations, stabilize estimates for early events when there are low numbers of mutations in a region.
The implementation of these methods on ovarian tumors, which contain a large number of amplifications, allows for the examination of the general pattern of amplification in ovarian tumors and suggests that there might be two distinct patterns of amplification present: steady accumulation of amplifications over time versus whole-genome amplification.
Traditional copy-number analysis examines regions of the normal genome as to whether they are amplified in the tumor, and as such is largely our focus; indeed, this is the only alternative in the case of exon sequencing.
If limited to this approach, only regions with one of three types of allelic copy number are viable candidates for the temporal analysis.
However, an amplification and insertion of one region of the genome into another creates a different genome with connections between regions that do not exist in the normal genome.
In the case of whole-genome sequencing, reads that span such breakpoints will be sequenced and algorithms have been proposed to use these breakpoints to estimate the relationships between these separate regions (Greenman et al., 2012).
The additional information regarding the connections between regions, when available, has the potential to make specific estimates of timing more feasible.
2 METHODS We consider regions that have chromosomal copy-number changes, in other words a region in the genome that has been amplified or deleted a known number of times resulting in S copies of the region.
What we observe as a single region with abnormal copy number is generally the culmination of a series of K events resulting in the final observed copy number.
This results in K 1 stages in the life of the tumor where the regions copy number is stable, and the goal is to estimate the proportion of the lifetime of the tumor spent in each stage.
As we make clear later, we are generally only able to consider regions that have a history of only amplifications, but for now we will keep the terminology general.
At each stage, individual point mutations could have been introduced into one of the existing copies.
The amount of time for which the tumor rested in a particular stage will determine the probability of a mutation accumulating in that region during that time, as will the mutation rate of the tumor at that time.
More precisely, let the vector  0,... ,K consist of the probabilities that a mutation originated in the corresponding stage of the tumor progression.
Comparing vectors calculated for different regions allows for precise comparison of the temporal order of aberrations in different regions.
Of particular interest might be 0, the proportion of time before any chromosomal change.
2.1 Model We now describe a basic probablistic model for linking the vector to the observed set of N mutations, as proposed by Durinck et al.(2011) for CNLOH events and generalized by Greenman et al.(2012).
Assume there are N total mutations in the region.
Let Pi be the allele frequency for a mutated location i, defined as the proportion of the sequenced copies that are mutated in that location.
We can only consider locations that have been mutated and have Pi40; locations that have been mutated in the past but have Pi 0 at the time of observing the tumor cannot be distinguished from locations that were never mutated.
We do not assume that we know which of the S copies hold the mutation or from which of the original copies (maternal or paternal) the mutation is descended.
The set of possible values for of Pi are given by f1=S,... ,S=Sg for a pure tumor sample.
For generality, we will denote the set of them as fa1,... , aSg to handle possible contamination in our sample (see Supplementary Appendix 3).
Depending on the types of copy-number changes that have occurred, only a subset of the set fa1,... , aSg may actually be possible.
For example, if only histories with amplifications events are considered, then S/S is not possible.
Then Pi is a multinomial random variable defined by a probability vector q qa1,... , qaST that gives the probability of a randomly acquired mutation having allele frequency for each aj, qaj PPi ajjPi40.
The values of the vector q depends on the random process of mutagenesis over the life of the tumor.
Specifically, Pi is completely determined by two random events: (i) the stage in which mutation i occurred and (ii) which copy in existence during that stage was mutated.
We can formulate a probability model that links q with the parameter of interest, , by making the following assumptions: (1) Each location was mutated once in the history of the tumor (2) If a mutation occurred in stage k, it is equally likely to be on any of the copies in existence during stage k (3) The probability of a mutation occurring in a stage k is assumed proportional to k and the number of copies of the region in existence during the stage k. As we are concerned only with locations 3114 E.Purdom et al.to quite  very maximum very whole in fact Note that w s n denoted in order copy 1 2 formulated.
Since  i with Pi40, we can use Bayes rule and assumption 2, previously mentioned, to obtain Pmutation at i originated in stage kjPi40 Skk=c where Sk is the number of copies at stage k that survive to the end point at which the tumor is observed, and c is a normalizing constant equal to PK k0 Skk Let Ckj be the set of copies in existence in stage k that lead to an allele frequency aj in the observed tumor.
All mutations originating from stage k on the same copy s will have the same allele frequency.
We notate that allele frequency as pk, s; pk, s will be the proportion of the existing S copies that trace their descent from the parental copy s that existed in stage k. Conditioning we can write that qaj XK k0 X s2Ckj Pmutation on copy sjmutated in kth stage,Pi40 Pmutated in kth stagejPi40 XK k0 X s2Ckj 1 Sk Skk c 1 c XK k0 Ajkk;where Ajk f#copies in stage k that result in allele frequency ajg We can collect the elements Ajk into an S K 1 matrix A and write the model simply as q A=c: As noted earlier, only a subset of the set f1=S,... ,S=Sg may actually be possible.
When this is the case, the corresponding row of A will be all zeros, and can be removed.
2.2 Identifiability To formulate the model, we must assume that we can determine the number of copies in stage k that could have resulted in allele frequency aj to construct the matrix A.
This requires precise knowledge of the history of amplifications.
If we have three copies of the maternal copy (M) and two of the paternal copy (P), these could have been acquired in a variety of ways.
For example, theM copy could be duplicated, then the P copy and then one of the existing copies of M. This will result in a different matrix A than if the P copy were duplicated, and then the M copy was duplicated two times (see Supplementary Table S2).
If there is only one event (K 1), then the only possible event histories are a deletion, a gain, or a CNLOH event where one of the copies deletes and replaces the other copy simultaneously (a deletion would not result in an identifiable model, see later in text).
These events can generally be distinguished from each other based on estimating the total copy number S as well as the allelic copy number (the number of copies of the maternal and paternal alleles).
Similarly, for caseS 4, if we assumeK-2i.e.two events where at each event a single copy of the region was amplifiedthe allelic copy number is sufficient to distinguish the two possible historymatricesA corresponding to (1,3) and (2,2) allelic copy numbers.
When there is42 events in the history of the region (S44), even if they are simple amplifications, then there aremultiple event histories that can lead to the same allelic copy number but different histories A, and thus different resulting probabilities, q, of observed allele frequencies.
If only allelic copy number is available, as with exome sequencing, this means that only these five cases where K equals 1 or 2 can we know the matrix A.
With whole-genome sequencing, algorithms have been proposed to use reads spanning the breakpoints to reconstruct the event history A (Greenman et al., 2012), though not all amplified regions will have a unique construction.
With exome sequencing, however, the event histories of large amplification regions will not be distinguishable.
Even if the event history matrix A is completely known, the question remains as to whether is identifiable, i.e.does complete knowledge of the probability distribution of the data, q, allow for reconstruction of the parameter ?
It is clear that is only identifiable if the matrix A has rank K 1.
For this reason, we can only estimate in cases where there have been no deletions (Greenman et al., 2012), so only regions with a history of pure amplification can be considered.
An exception is the setting of CNLOH, where the assumption is that one of the copies deletes and replaces the other copy simultaneously so the time for the stage corresponding to a deletion is zero.
We show that in the case of sequential amplification (where each event is the addition of only one copy of the region) the sequence of events for which this will be true is limited: for a total number of copies equal to S, there is always exactly one history such that A is invertible and it is a history where all of the gains are on a single line of descent.
This necessitates that the minor copy must have copy number 1, but this is not a sufficient condition if S44.
When S44, even if the minor copy number is equal to 1, there can still be multiple histories associated with it and only one of these histories will be identifiable (see Fig.1).
This implies that of the five cases where the A matrix can be identified based solely on allelic copy number, only three of them are identifiable: CNLOH (2,0), single gain (1,2), and unbalanced two gains (1,3).
For the two tumor types we analyzedovarian and skinthis was around 40% of the amplified regions, see Supplementary Table S3.
In the sequential amplification setting, the only identifiable matrix A has a simple form and its inverse has the same simple form, 0 0  0 1 0 0  1 0......... 0 1  0 0 1 0  0 0 0 BBBB@ 1 CCCCA ke1xT; 1 where e1 is the unit vector.
For A, k 1 and x 1; 2;  ; S 1T, and for A1, k 1=S and x S 1;  , 2, 1T.
See Supplementary Appendix 2 for the proof.
Not all amplifications are the result of a single copy gain at each event.
For example, if two copies are adjacent to one another, a further duplication event can replicate both copies simultaneously.
We can consider that at each event a random set of the existing copies are chosen to be duplicated.
In this setting, we have explored the possible histories via simulation (see Supplementary Appendix 6 for details).
Generally, the histories that result in identifiable models are a small proportion of all models simulated, though the histories generated by our simulations may not be representative of likely biological scenarios.
There is no obvious condition that appears to guarantee identifiability, but the simulations continue to suggest that identifiability is loosely a property of having a concentration of duplications along a small number of lineages.
2.3 Modeling sequencing variability With sequencing data, we will not observe the true allele frequencies Pi, but rather the mi sequenced fragments that overlap the location i, of (a) (b) (c) Fig.1.
Example of three histories of amplification that can lead to copy number S 5: starting at the top with normal copy-number state of one allele from the maternal (M) and one from the paternal (P), at each time point k there is an amplification of an existing copy resulting in five copies at the bottom, which represents the point at which the tumor sample was removed.
Only (a) is identifiable because all amplification occurs on one lineage 3115 Timing chromosomal abnormalities above In order t in order ,more than oneSequencing Variability which Xi of them are mutated.
Greenman et al.(2012) estimate Pi by first classifying each mutated location as one of the possible fajg based on which aj results in the largest likelihood, and then by finding the MLE of the model given in Section 2.1 using the estimated Pi as the true Pi.
This will ignore variability in the estimates of Pi, which can have an important impact if mi is moderate.
For example, normal contamination will result in a set of alleles that encompass a smaller range, making it harder to infer the true Pi from the Xi without greater sequencing depth.
We propose directly taking into account the variability in the Xi by modeling the distribution of Xi.
These adjustments allow us to reliably include mutations with lower values of mi, increasing the total number of mutations and thus the power.
This distribution allows us to account for sequencing error, as well as the fact that we only consider locations where Xi40.
However, in practice this will also make little difference in most settings unless the coverage is very low.
We model Xi as Binomialmi, ~Pi, where ~Pi is the expected allele frequency after accounting for normal contamination and sequencing error (see Supplementary Appendix 3).
In general the sequencing error will be fairly small (12%), and will only affect small values of Pi (50:2), for example, if there is a large amount of normal contamination or large copy number.
A larger concern are sub-clonal populations, where some of the mutations, or even the entire region, are not variant in all tumor cells.
Mild levels of sub-clonal populations that do not contain the region will not necessarily affect the results badly if there is a large read depth and the number of events K is small, but in more difficult cases can severely bias the results, see Supplementary Appendix 3.
2.4 Estimating from tumors We can then estimate using maximum-likelihood techniques; unlike Greenman et al.(2012), we expand our likelihood to include the sequencing variability and sequencing error described earlier in text.
For large amount of sequencing depth, there is likely to be little difference in the two methods, but for lower levels of sequencing, explicitly accounting for the sequencing variability brings improved stability.
We assume in what follows that A defines an identifiable model.
As q/ A, and both q and must sum to 1, q lies in a constrained set.
If A is square, we can write this as  fq : A1q 0, 1Tq 1:g. Then by the invariance property of the MLE it is sufficient to find the MLE of q, with the constraint that q 2 and then solve for.
We use an EM algorithm to estimate q from the data X1,... ,XN, where the M-step involves a constrained maximization (see Supplementary Appendix 4 for details).
The most important factor in the ability to estimate the timing of events is the true value of the vector.
The allele frequencies of mutated locations follow a multinomial distribution with the number of categories equal to the number of alleles.
When one of the alleles has a low probability of occurrence, as given by the parameter q, then the estimates of become more unstable.
Mutations acquired in every mutational stage contribute to the allele frequency 1/S (or its corresponding allele frequency after adjusting for normal contamination and sequencing error).
The corresponding element of q, notated as q1, absorbs much of the probability; indeed, when the history is sequential amplification, it is easy to show that q1 is guaranteed to be largest element of the vector q, meaning that the most likely allele frequency must always be 1/S.
As probabilities in q are far from balanced regardless of the value of , then when has small values the probabilities in q are even smaller; this will lead to instability in the estimates.
Furthermore, as the total copy number of a region grows, the number of possible alleles does as well, making the estimation problem even more difficult.
To observe all the possible alleles with a high probability requires a large number of mutationsN as the copy number grows or a value of that is small (see Supplementary Fig.S1).
Some of the most important events to time accurately are those that are early (with small 0), and therefore to counteract this instability, we introduce a Bayesian model for estimating.
Specifically, we assume that follows a Dirichlet distribution that puts uniform probability on the K-simplex where lies.
This is commonly done in the case of simple multinomial estimation, where a Dirichlet prior is equivalent to adding pseudo-counts to the data to stabilize the estimates.
The Dirichlet distribution is not a conjugate prior for our distribution, and therefore we sample from the posterior distribution of using sampling importance resampling to calculate the posterior mean and credible intervals (see Supplementary Appendix 5 for details).
3 RESULTS 3.1 Simulation data We simulated mutation data for different histories using the model described earlier in text: calculating the q vector of multinomial probabilities for a given and A, generating Pi from a MultinomialN, q, and then generating mutation counts using a Binomialm, ~Pi.
We varied the parameter , the read depth, numbers of mutationsN and the normal contamination to evaluate the performance of our estimation procedures; no sequencing error was comprehensively simulated because the effect was so small.
In all situations, estimation of 0 when 050:1 is highly variable with few mutations, and furthermore the MLE is a biased estimate, underestimating 0, until N becomes large.
Even in the simplest example of K 1 (CNLOH or single gain), if 0 0:01 values of N as high as 200 or 300 are needed to remove this bias (Supplementary Fig.S2).
For 0 0:01, the MLE estimate 0 equals zero for almost all simulations, reflecting the low probability of observing an allele identified with the earliest stage.
For larger values of 0, the estimates are unbiased starting around N 50, with continually greater precision for larger N (Supplementary Fig.S4).
The read depth has much less effect on the estimation, particularly if the read depth is430; even for read depth as low as 10, the loss of precision due to low read depth is not nearly as striking as that due to reducing the number of mutations (assuming that the mutations are correctly identified as mutated, which is problematic with only 10 coverage).
This implies that including more mutations with lower read depth will increase N and lead to greater precision in the estimate of despite the lower precision of each individual location.
We also note that the 95% bootstrap confidence intervals are slightly biased, with coverage probability somewhat 595% even with large N (Supplementary Fig.S6).
Therefore, in evaluating our proposed procedures, we concentrate on two different contexts, 0 0:1 and 040:1, and assume that we have at least 50 mutations in a region.
We focus on the estimation of 0 as being of the greatest biological interest.
Full Maximum Likelihood We expect that the difference between the partial MLE method of Greenman et al.(2012) and our full MLE method will be the largest when the question of classifying mutated locations to a particular allele frequency has the greatest uncertainty: lower read coverage and/or higher levels of normal contamination.
Simulation results show that with no normal contamination, the partial MLE method can be biased even in the relatively simple case of the single-gain case with read coverage as high as 30 (Fig.2).
By 75 coverage the two methods are indistinguishable for low numbers of events, but for larger K, the partial MLE still remains biased even with 75 coverage, see Supplementary Figure S8.
In particular, the 3116 E.Purdom et al.very maximum above.
Since Since (SIR) in order greater than x less than single x x xpartial MLE method overestimates 0 for small 0, and conversely for large 0.
Even where the full MLE tends to be biased and underestimates 0, the partial MLE goes the other direction and overestimates 0 by a larger margin (and conversely for large 0), resulting in worse average error, Figure 3.
When normal contamination increases, the partial MLE does worse, so that even for 75 coverage and K 1, estimation of moderately low values of 0 (e.g.0 0:1) is noticeably still biased (Figs 2 and 3).
For large K, where the allele frequencies are closer together and harder to distinguish, the problems are magnified across a wide spectrum of 0 and larger coverage is required before the bias disappears, see Supplementary Figures S6.
We note that the difference between the partial and full estimation methods also depends on the complexity of the problem.
For CNLOH, where there is a direct identification between allele frequency and the stage in which the mutation occurred, there is less difference in the methodsonly when normal contamination reaches 0.3 is there a difference if the read depth is 30.
This is likely due to the fact that with CNLOH there is no constraint on the space in which the vector q lies.
With more complex models (i.e.larger K), small variations in the estimation of q result in larger perturbations of the vector (see Supplementary Fig.S9).
Bayesian Estimation From a frequentist perspective, Bayesian estimates will be on average biased, but can offer less variability and thus less overall error.
Simulations definitely reflect this bias, with Bayesian estimates on average underestimating 0 across the board for CNLOH regions and generally for single-gains shrinking the estimates toward 0 0:5.
The Bayesian estimates for gains generally are similar to that of the MLE, with the overall error similar for most values of 0.
For extreme 0 (0 0:01 or 0.99), the Bayesian estimates have worse overall error than the MLE, so that they do not improve those estimates as was hoped (Supplementary Fig.S10).
However, the Bayesian interval estimates have a better coverage probability, particularly in extreme values of 0, than the bootstrap CI.
For CNLOH, however, the Bayesian estimates have a different behavior than the MLE.
For small 0 ( 0:1), the Bayesian estimates have a better error rate as well as better coverage probability; indeed for extreme 0, the MLE bootstrap confidence intervals are bad, often giving extremely small or zero-width intervals.
The Bayesian estimates have a much worse error for 040:1 because they are extremely biased downward (Fig.3).
3.2 Cancer data We use the exome sequencing data from six of the eight tumors that we previously analyzed in Durinck et al.(2011).
In that work, we analyzed only CNLOH events and found that the CNLOH event on chromosome 17 covering the tumor suppressor gene TP53 to be an early event.
Here we analyze both CNLOH and single copy gains, and compare the performance of estimates of (no events with S4 were observed).
We note that for these tumors, there is a high mutation rate, and many CNLOH and single-gain abnormalities, with few higher level amplifications.
We also evaluated the timing of regions for five ovarian tumors with WGS available through the TCGA project (Cancer Genome Atlas Research Network, 2011).
The ovarian tumors have many more rearrangements than the skin cancers and a much lower mutation rate.
See Supplementary Appendix 1 and Supplementary Table S3 for more details regarding the datasets.
We first observe that the estimate of 0 for the two skin tumors containing a CNLOH event on chromosome 17 continues to imply 17p CNLOH is an early event (Fig.4a), even after the addition of the single-gain regions, with estimates of 0 on the order of 0.05 [additional CNLOH events were found by Durinck et al.(2011) in samples that we did not analyze, see Supplementary Appendix 1].
CNLOH events involving the region containing TP53 are present in four of the five ovarian samples, and TP53 mutations are found in all four of these regions.
Three of these TP53 mutations clearly occurred before the CNLOH event (i.e.homozygous) with the remaining mutation (a) (b) Fig.3.
Comparison of Bayesian and MLE estimates for CNLOH, see Supplementary Figures S10 and S11 for the single-gain case.
(a) Plots of relative mean squared error on simulated data of CNLOH for three different estimates.
Relative MSE is the MSE scaled by the value of 01 0 to reflect the size of the MSE relative to the size of 0.
(b) Comparison of CI coverage for MLE and Bayesian on simulated data.
For each possible 0, CI coverage was calculated as the percentage of CIs from simulated data that covered 0; a color scale indicates the CI coverage, with red indicated 95% coverage and magenta indicated 9095% coverage.
The solid points indicate the true value (circle for MLE and triangle for Bayesian).
Shown are the results for when the true 0 is small (0.01, 0.05) and the Bayesian estimates are not extremely biased; see Supplementary Figure S12 for all values of 0 (a) (b) Fig.2.
Boxplots of 0 based on simulated data for values of 0 0:1, 0:5 in the single-gain case.
(a) Read depth of 30 and no normal contamination.
(b) Read depth of 75 and 30% normal contamination.
The number of mutations, N, was fixed at 125 3117 Timing chromosomal abnormalities x and S6  around xsingle very very very quite Data single very single (ambiguous (see Supplementary Table 4 and Supplementary Fig.S15).
However, only one sample (131411) has an estimate of 0 for the region containing TP53 that is as early in the life of the tumor as seen in the skin cancers.
The large number of aberrations present in ovarian tumors allows us to observe the general trajectory of chromosomal amplifications (Supplemental Fig.S15).
The five tumors present different general profiles.
Two tumors (13-0890 and 13-1411) have events that are clearly separated through time and span the range of (relative) time, whereas events from other three tumors are estimated to have occurred over a small range of time, suggesting a short duration of rapid copy-number change.
This suggests the possibility of two different biological mechanisms in use, with some tumors starting with a whole-genome duplication whereas other tumors steadily accumulate copy-number changes.
Comparison with the partial MLE We have seen in simulations that the partial MLE implemented in Greenman et al.(2012) tends to overestimate 0, particularly for small values of 0.
For the skin data, the estimates of 0 for the two early events on chromosome 17p given by the partial MLE method are much larger (Fig.4a).
For sample M01, the 95% confidence intervals based on the partial MLE for the CNLOH of chromosome 17p overlaps that of chromosome 2, making it ambiguous whether 17p is the first event.
The early events in the ovarian tumors show an even larger difference between the two estimates.
For these early events of interest, the difference in estimation can be important and accounting for sequencing variability identifies early events more conclusively.
Aside from the early events, we see that the estimates for the full and partial MLE methods are generally equivalent for the skin tumors.
For the ovarian tumors, however, the differences are more striking even for events that have only moderate estimates of 0, see Figure 4b.
This is due to the fact that the ovarian samples are sequenced at 35 coverage compared with 100 coverage for the skin tumors (Supplementary Table S3); furthermore, gains are more heavily represented for the ovarian tumors, and the gains show much greater differences between the estimates.
(a) (b) Fig.4.
Estimates of 0 with bootstrap confidence intervals for the (a) skin tumors and (b) a single ovarian tumor (131411).
The full MLE (black symbol), partial MLE (gray symbol) and Bayesian estimates (white symbol) with their corresponding confidence intervals are shown side by side for each region.
Regions of CNLOH are marked with a circle, and those from a single gain are marked with a triangle.
Regions from different samples are separated by dashed lines.
Each region is labeled by the chromosome and the arm that contain the region.
Above each region, the number of mutations (N) identified in the region is indicated.
The CNLOH of 17p is shown in red 3118 E.Purdom et al.-while copy whole while copy to quite very around x to xBayesian Estimation For regions where the MLEs are approaching the boundary of the parameter space (estimates that are essentially 0 or 1) the Bayesian estimates, as expected, shrink the estimates away from the boundary and toward the priormean.
Particularly for CNLOH regions, the bootstrap confidence intervals for the MLE estimates often do not sufficiently capture the variation in the estimates.
One example is the CNLOH event in 8pq in the skin tumorM01, where only 11mutations are observed, but all of them are heterozygous.
Bootstrap confidence intervals give great confidence to the parameter estimate of 0 1 even though N 11; the Bayesian analysis, as hoped, modulates these estimates, both decreasing the estimate in accordance with the prior distribution and giving greater levels of uncertainty corresponding to the small sample size.
The Bayesian estimates, as seen in simulation, also generally give lower estimates of all 0 for CNLOH and estimates closer to 0.5 for extreme values of 0 for single gains, which is also reflected in both cancer datasets.
However, the difference in estimates is not large relative to the confidence intervals of the estimates, and is probably offset by the advantage of increased accuracy of the confidence intervals, particularly for early events.
4 CONCLUSION Precise timing of chromosomal abnormalities provides a wonderfully detailed glimpse of the etiology of a single tumor.
However, we have demonstrated that there are limitations to this technique.
In particular, we have shown that for high-level amplifications, most of the possible combinations of events that result in large amounts of amplification will not retain enough information in the allele frequencies to be able to estimate the ordering.
Only regions where the amplification follows one single lineage can be timed using this model.
This may result in a biased impression of the etiology, as this type of amplification may be predominant for the promotion of certain types of abnormalities and may miss many other types of oncogenes.
As we note in the introduction, our focus has been the traditional one of copy-number analysis, where each region in the normal genome is analyzed separately as to its behavior in the tumor.
With exome sequencing, this traditional viewpoint is still the only one available.
With whole-genome sequencing, as we noted, one can analyze the relationships between the regions and order the events using the information from other regions.
In this case, a single region A can share an event with another region B if the amplification brought the two into proximity to each other through an insertion.
Then there are additional constraints on estimating A and B jointly, as that event must occur at the same moment for both.
This implies that with reasonably deep whole-genome sequencing such that these relationships are reliably determined, there will be a larger percentage of histories that are identifiable.
In addition, early events, which are of particular biological significance, are sensitive to estimation procedures and large numbers of mutations are necessary to be able have stable estimates of the time of occurrence.
Of even greater difficulty is the ordering of a collection of early events.
Even with whole-genome sequencing, some regions will not have the hundred or more mutations that our simulations show are necessary to distinguish early events, particularly in tumors with low mutation rates.
However, we have also shown that differences in estimation techniques can help provide better estimates and confidence intervals for temporal estimates.
We have introduced a full MLE to handle sequencing variability due to lower read coverage, as well as a Bayesian estimation technique.
We have shown the full MLE can provide improvement with read depths as large as 30, and even up to 75 or higher if there is normal contamination or early events.
The Bayesian estimates have a varying performance for different values of the parameter space, but can provide increased stability, particularly in their estimates of confidence intervals for the estimates.
Ultimately, the ability to successfully estimate also relies on intrinsic properties of the cancer.
In the skin tumors, only half of the samples had CNLOH over the tumor suppressor gene TP53 (not all of which were examined in this work); in the other samples, both copies of TP53 were also inactivated but through multiple mutations, not a chromosomal abnormality.
Other important regions may be too small in a particular sample to have sufficient mutationsthe regions we ordered were large, sometimes entire chromosomal arms.
Some tumors, such as the ovarian, have low mutation rates so that even with whole-genome sequencing many regions will have few mutations or not enough to confidently distinguish between events.
While 3060% of the abnormal regions could theoretically be timed in our sample, the percentage that had enough mutations was generally 2030%.
Therefore, timing of the chromosomal abnormalities of a single sample remains extremely fragmentary, and an insight into tumor etiology will still ultimately be gained by comparing the temporal ordering of many tumors.
ACKNOWLEDGEMENTS The authors thank the anonymous reviews for their helpful suggestions.
The results published here are in whole or part based on data generated by The Cancer Genome Atlas pilot project established by the NCI and NHGRI.
Information about TCGA and the investigators and institutions that constitute the TCGA research network can be found at http://cancergenome.nih.gov/.
Funding: National Institute of Health TCGA grant (U24 CA143799); the Anna Fuller fund; the Dermatology Foundation; the American Skin Association; and National Science Foundation grants (DMS-0636667, DMS-1026441).
Conflict of Interest: none declared.
ABSTRACT Summary: The current methods available to detect chromosomal abnormalities from DNA microarray expression data are cumbersome and inflexible.
CAFE has been developed to alleviate these issues.
It is implemented as an R package that analyzes Affymetrix *.CEL files and comes with flexible plotting functions, easing visualization of chromosomal abnormalities.
Availability and implementation: CAFE is available from https://bit bucket.org/cob87icW6z/cafe/ as both source and compiled packages for Linux and Windows.
It is released under the GPL version 3 license.
CAFE will also be freely available from Bioconductor.
Contact: sander.h.bollen@gmail.com or nancy.mah@mdc-berlin.de Supplementary information: Supplementary data are available at Bioinformatics online.
Received on August 8, 2013; revised on January 10, 2014; accepted on January 14, 2014 1 INTRODUCTION Gross chromosomal abnormalities are a hallmark of cancers (Hanahan and Weinberg, 2011) and are frequently acquired by cultured cells as an adaptation to cell culture conditions (Baker et al., 2007).
Recently, it has been recognized that induced pluripotent stem cells often feature gross chromosomal duplications or deletions (Laurent et al., 2011).
Various methods exist to detect chromosomal gains or losses.
Traditional karyotyping relies on careful examination of Giemsa-stained metaphase chromosomes.
Newer techniques like spectral karyotyping have increased ease of analysis but nevertheless feature low resolution.
For high-throughput and high resolution analysis of gross chromosomal abnormalities, array-based Comparative Genomic Hybridization (a-CGH) is often used.
This a-CGH approach is based on the detection of a quantitative difference of DNA content.
Whole-genome and SNP-based sequencing approaches have also been developed.
Although not initially developed for this purpose, it is possible to use gene expression microarray data for the detection of copy number abnormalities.
This approach is not based on the measurements of DNA content but rather on mRNA expression levels.
A protocol to use expression microarrays to karyotype samples was recently published by Benvenisty and coworkers but requires the manual use of different tools (Ben-David et al., 2013).
Here, we present CAFEChromosomal Aberration Finder in Expression dataas an R package for the detection of gross chromosomal gains and losses from expression microarrays, with a resolution up to cytoband level.
CAFE follows the expression-based karyotyping workflow (e-karyotyping) and greatly simplifies and speeds up the detection analysis of chromosomal aberrations from expression DNA microarrays.
2 FEATURES AND METHODS The starting point of a CAFE analysis is a set of gene expression microarrays from samples whose e-karyotype will be computed and another (larger) set of microarrays representing controls.
The controls define a normal e-karyotype against which the altered e-karyotype will be defined.
We recommend choosing a dataset of at least 10 controls for 23 test samples.
More controls may be required depending on the particular case.
CAFE is implemented as an R package and relies on several Bioconductor packages.
It runs on version 2.10 or newer of R. The analyses are performed using Affymetrix *.CEL files as input.
Using the ProcessCels() function, a list object is created from these *.CEL files and returned to the user.
This object contains normalized and relative expression levels, along with several mappings of probesets to chromosomes, chromosomal arms, cytobands and chromosomal locations.
The output can be further filtered so as to exclude multiple probesets that map to the same gene or to the same location.
CAFE can then be used to perform several enrichment tests for the detection of duplications or deletions of chromosomes, chromosomal arms and cytobands.
Furthermore, several plot functions are available to visualize any detected aberrations.
2.1 Enrichment testing CAFE contains three statistical functions that determine enrichment or depletion of a given chromosome/chromosomal region.
One function exists for each of the three resolutions: chromosomeStats(), armStats() and bandStats(), corresponding to chromosomes, arms and cytobands, respectively.
The ability of CAFE to detect aberrations within chromosome, arm or cytoband is dependent on the density of the microarray probesets within these areas.
Areas that are gene-poor are not likely to be detected, as expression microarrays are designed to detect transcribed genes.
The user defines two thresholds as a ratio of median expression values, over and under, for which probesets are called overand under-expressed.
The threshold for genomic DNA hybridized onto a comparative genomic hybridization array would be log2(2) ratio to detect a chromosome gain or loss, respectively.
Because CAFE uses mRNA expression as a surrogate for DNA copy number and because the levels of mRNA expression are variable and do not strictly reflect*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com https://bitbucket.org/cob87icW6z/cafe/ https://bitbucket.org/cob87icW6z/cafe/ mailto:sander.h.bollen@gmail.com mailto:nancy.mah@mdc-berlin.de (iPSCs) , While , f ,--, , since `` '' `` '' +/-, since DNA copy number, a less restrictive default threshold is recommended as a starting point for analysis: log2(1.5) ratio.
Using these thresholds, enrichment of under-and over-expressed probes in chromosomes or chromosomal regions is computed using a Fishers exact test or a 2 test.
P-values are Bonferroni-corrected by default.
2.2 Graphics For graphics, CAFE uses the ggplot2 plotting system (Wickham, 2009).
There are four different plotting functions available: (i) rawPlot(): the raw log-transformed relative expression values are plotted along the chromosome of interest; (ii) slidPlot(): a moving average smoother is applied to the log-transformed relative expression values before plotting the values along the chromosome of interest; (iii) discontPlot(): a discontinuous smoother is applied to the log-transformed relative expression values and the values are plotted along the chromosome of interest; (iv) facetPlot(): all chromosomes are plotted in one horizontally aligned graph, with relative expression values along each chromosome.
This function can be used in conjunction with a moving average smoother.
For all plot functions except facetPlot(), it is possible to add a chromosome idiogram over the chromosome plot.
This allows easy visualization of chromosomal abnormalities.
See Figure 1 for example plots.
All plots are printed to the file system and returned as ggplot2 objects.
Plot parameters, such as labels and scales, can be modified to the users liking by altering the ggplot2 object.
2.3 Comparison with other tools To the best of our knowledge, there are currently no other Bioconductor packages that are designed to identify chromosomal copy number abnormalities from mRNA expression microarray data.
However, there are other packages that perform similar functions, such as processing comparative genomic hybridization arrays (aCGH, snapCGH) or identifying differentially regulated regions on a chromosome from expression microarrays (MACAT).
The initial choice of datasets for analysis is critical and must be completed by hand, regardless of the package used.
Once this is done, CAFE is able to normalize and preprocess the *.CEL files in one step.
Although aCGH and snapCGH were designed to analyze CGH arrays, one can use CAFE to preprocess the *.CEL files and then subsequently reformat the preprocessed data for input into these packages.
Both aCGH and snapCGH use hidden Markov models to predict state changes (i.e.changes in chromosomal copy number).
Plotting functions then show the course of state changes over the chromosome.
MACAT uses a modified t-statistic and permutation to score regions of the chromosome that are differentially regulated.
The scores for a selected chromosome are shown as a static html page.
To compare the performance, CAFE and the other three packages were used to analyze two test datasets with known chromosomal aberrations (see Supplementary Data).
CAFE was able to detect copy number abnormalities just as well as or better than the other packages.
3 DISCUSSION Karyotyping by expression microarrays, as described by BenDavid et al.(2013), extends the utility of expression microarray data by providing some limited information on the status of chromosomal aberrations in a sample.
However, the original e-karyotyping method is a tedious process, using four different programs and requiring an estimated 15 h to analyze only 15 samples.
At the time of writing, there are no Bioconductor packages to specifically carry out e-karyotyping from raw microarray data.
The CAFE package simplifies the e-karyotyping protocol.
Starting from the *.CEL files, CAFE can do the same analysis in minutes and requires no more than basic R knowledge.
Bioconductor packages for CGH analysis can be used to perform an e-karyotyping analysis, but the data preprocessing steps must be carried out manually and the resulting graphs are static and not user-configurable.
In contrast, CAFE processes the expression data from *.CEL files and all plotting functions return a ggplot2 object that can be modified by the end-user to his or her specific needs.
CAFE will become a Bioconductor package, to be freely available for anyone to use, distribute, modify and easily integrate into an R-workflow for automatic analysis.
Currently, only Affymetrix *.CEL files from 3IVT arrays can be seamlessly preprocessed in CAFE; functions for processing raw microarray data from other platforms may be added in the future if there is demand for this feature.
Data import from other platforms is still possible, as CAFE data are represented by a simple R list structure.
CAFE analysis currently works with three different resolutions: whole chromosome, chromosomal arm and cytoband.
Therefore, it is not suited for smaller deletions or duplications.
In addition, it is not readily possible to detect insertions or translocations using this technique, as the probesets will be mapped to the original chromosome or location.
Therefore, CAFE is most suited for the detection of numerical abnormalities, rather than structural abnormalities.
CAFE has not been designed to replace existing karyotyping techniques but rather to gain information when data from more specific approaches is not yet available.
Funding: German Research Foundation Priority Programme 1463 (Project No.
AN 735/2-1) (to N.M.).
Conflict of Interest: none declared.
ABSTRACT Motivation: High-throughput sequencing has made the analysis of new model organisms more affordable.
Although assembling a new genome can still be costly and difficult, it is possible to use RNAseq to sequence mRNA.
In the absence of a known genome, it is necessary to assemble these sequences de novo, taking into account possible alternative isoforms and the dynamic range of expression values.
Results: We present a software package named Oases designed to heuristically assemble RNA-seq reads in the absence of a reference genome, across a broad spectrum of expression values and in presence of alternative isoforms.
It achieves this by using an array of hash lengths, a dynamic filtering of noise, a robust resolution of alternative splicing events and the efficient merging of multiple assemblies.
It was tested on human and mouse RNA-seq data and is shown to improve significantly on the transABySS and Trinity de novo transcriptome assemblers.
Availability and implementation: Oases is freely available under the GPL license at www.ebi.ac.uk/zerbino/oases/ Contact: dzerbino@ucsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on December 2, 2011; revised on January 20, 2012; accepted on February 17, 2012 1 INTRODUCTION Next-generation sequencing of expressed mRNAs (RNA-seq) is gradually transforming the field of transcriptomics (Blencowe et al., 2009; Wang et al., 2009).
The first attempts to discover expressed gene isoforms relied on mapping the RNA-seq reads onto the exons and exonexon junctions of a known annotation (Jiang and Wong 2009; Mortazavi et al., 2008; Richard et al., 2010; Sultan et al., 2008; Wang et al., 2008).
Consequently, reference-based ab initio methods have been developed to assemble a transcriptome from RNA-seq data using read alignments alone, inferring the underlying annotation (Denoeud et al., 2008; Guttman et al., 2010; Trapnell et al., 2010; Yassour et al., 2009).
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
Unfortunately, the use of a reference genome is not always possible.
Despite the drop in the cost of sequencing reagents, the complete study of a genome, from sampling to finishing the assembly is still costly and difficult.
Sometimes, the model being studied is sufficiently different from the reference because it comes from a different strain or line such that the mappings are not altogether reliable.
For these cases, de novo genome assemblers have been employed to create transcript assemblies, or transfrags, from the RNA-seq reads in the absence of a reference genome (Birol et al., 2009; Collins et al., 2008; Jackson et al., 2009; Wakaguri et al, 2009).
However, these short read genomic assemblers, based mainly on de Bruijn graph genomic assemblers (Zerbino and Birney, 2008; Simpson et al., 2009), make implicit assumptions regarding the evenness of the coverage and the colinearity of the sequence.
Indeed, the coverage depth fluctuates significantly between transcripts, isoforms and regions of the transcript, therefore it cannot be used to determine the uniqueness of regions or to isolate erroneous sequence.
In addition, these tools are geared to produce long linear contigs from the given sequence, not to detect the overlapping sequences presented by isoforms of a single gene.
This affects a number of steps, including error correction, repeat detection and read pair usage.
These methods are therefore not necessarily suited to process transcriptome data which does not conform to either of these assumptions.
More recently, transcriptome assembly pipelines were developed to post-process the output of de novo genome assemblers: Velvet and ABySS (Martin et al., 2010; Robertson et al., 2010; SurgetGroba and Montoya-Burgos, 2010).
The common idea shared by these pipelines is to run an assembler at different k-mer lengths and to merge these assemblies into one.
The rationale behind this approach is to merge more sensitive (lower values of k) and more specific assemblies (higher values of k).
The pipeline presented by Robertson et al.(2010), transABySS, also handles alternative splicing variants.
It detects them by searching for connected groups of contigs such that they are connected in a characteristic bubble and one of the contigs has a length of exactly (2k2).
These bubbles are first removed, then added to the final assemblies, to reconstruct alternate variants.
A variety of algorithmic researchers have used splicing graphs to represent alternative splicing which have a direct relationship to de Bruijn graphs, as pointed out by Heber et al.(2002).
This homology The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
dzerbino@ucsc.edu Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1087 10861092 Oases de novo RNA-seq assembly between data structures opens the possibility of a de novo short read transcriptome assembler, as illustrated by the Trinity algorithm (Yassour et al., 2011).
Trinity starts by extending contigs greedily, connecting them into a de Bruijn graph, then extracting sufficiently covered paths through this graph.
Trinity is designed to reconstruct highly expressed transcripts to full length using only one k-mer length.
We present Oases, a de novo transcriptome assembler that combines these advances.
Oases merges the use of multiple k-mers presented in (Robertson et al., 2010; Surget-Groba and MontoyaBurgos, 2010) with a topological analysis similar to that presented by Yassour et al.(2011).
It uses dynamic error removal adapted to RNA-seq data and implements a robust method to predict full length transfrags, even in cases where noise perturbs the topology of the graph.
Single k assemblies are merged to cover genes at different expression levels without redundancy.
We tested the latest version of Oases (0.2.01) on experimental datasets and found that Oases produces longer assemblies than previous de novo RNA-seq assemblers.
Oases was compared with a reference-based ab initio algorithm, Cufflinks (Trapnell et al., 2010).
The latter approach has a considerable advantage in low expression genes, as it can join otherwise disjoint reads by virtue of their genomic positions, but at high read coverage, Oases sensitivity approaches that of reference-based ab initio algorithms.
We also examined the effect of coverage depth, hash length, alternative splicing and assembly merging on the quality of assemblies.
2 METHODS 2.1 Overview The Oases assembly process, explained in detail below and illustrated in Figure 1, consists of independent assemblies, which vary by one important parameter, the hash (or k-mer) length.
In each of the assemblies, the reads are used to build a de Bruijn graph, which is then simplified for errors, organized into a scaffold, divided into loci and finally analyzed to extract transcript assemblies or transfrags.
Once all of the individual k-mer assemblies are finished, they are merged into a final assembly.
2.2 Contig assembly The Oases pipeline receives as input a preliminary assembly produced by the Velvet assembler (Zerbino and Birney, 2008) which was designed to produce scaffolds from genomic readsets.
Its initial stages, namely hashing and graph construction can be used indifferently on transcriptome data.
We only run these stages of Velvet to produce a preliminary fragmented assembly, containing the mapping of the reads onto a set of contigs.
However, the later stage algorithms, Pebble and Rock Band, which resolve repeats in Velvet, are not used because they rely on assumptions related to genomic sequencing (Zerbino et al., 2009).
Namely, the coverage distribution should be roughly uniform across the genome and the genome should not contain any branching point.
These conditions prevent those algorithms from being reliable and efficient on RNA-seq data.
2.3 Contig correction After reading the contigs produced by Velvet, Oases proceeds to correct them again with a set of dynamic and static filters.
The first dynamic correction is a slightly modified version of Velvets error correction algorithm, TourBus.
TourBus searches through the graph for parallel paths that have the same starting and end node.
If their sequences are similar enough, the path with lower coverage is merged into the path with higher coverage, irrespective of their absolute coverage.
In this sense, Fig.1.
Schematic overview of the Oases pipeline: (1) Individual reads are sequenced from an RNA sample; (2) Contigs are built from those reads, some of them are labeled as long (clear), others short (dark); (3) Long contigs, connected by single reads or read-pairs are grouped into connected components called loci; (4) Short contigs are attached to the loci; and (5) The loci are transitively reduced.
Tranfrags are then extracted from the loci.
The loci are divided into four categories: (A) chains, (B) bubbles, (C) forks and (D) complex (i.e.all the loci which did not fit into the previous categories).
the TourBus algorithm is adapted to RNA-seq data and fluctuating coverage depths.
However, for performance issues, the Velvet version of TourBus only visits each node once, meaning that it does not exhaustively compare all possible pairs of paths.
Given the high coverage of certain genes, and the complexity of the corresponding graphs, with numerous false positive paths, it is necessary for Oases to exhaustively examine the graph, visiting nodes several times if necessary.
In addition to this correction, Oases includes a local edge removal.
For each node, an outgoing edge is removed if its coverage represents <10% of the sum of coverages of outgoing edges from that same node.
This approach, similar to the one presented by Yassour et al.(2011), is based on the assumption that on high coverage regions, spurious errors are likely to reoccur more often.
Finally, all contigs with less than a static coverage cutoff (by default 3) are removed from the assembly.
The rationale for this filter is that any transcript with such a low coverage cannot be properly assembled in the first place, so it is expedient to remove them from the assembly, along with many low coverage contigs created by spurious errors.
2.4 Scaffold construction The distance information between the contigs is then summarized into a set of distance estimates called a scaffold, as described in (Zerbino et al., 2009).
Because a read in a de Bruijn graph can be split between several contigs, the distance estimate for a connection between two contigs can be supported by both spanning single reads or paired-end reads.
The total number of spanning reads and pair-end reads confirming a connection is called its support.
A connection which is supported by at least one spanning read is called direct, otherwise, it is indirect.
Connections are assigned a total weight.
It is calculated by adding 1 for each supporting spanning read and a probabilistic weight for each spanning pair, proportional to the likelihood of observing the paired reads at their observed positions on the contigs given the estimated distance between the contigs and assuming a normal insert length distribution model.
2.5 Scaffold filtering Much like the contig correction phase, several filters are applied to the scaffold: static coverage thresholds for the very low coverage sequences and a dynamic coverage threshold that adapts to the local coverage depth.
Because coverage is no longer indicative of the uniqueness of a sequence, contig length is used as an indicator.
Based on the decreasing likelihood of high identity conservation as a function of sequence length (Whiteford et al., 1087 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1088 10861092 M.H.Schulz et al.2005), contigs longer than a given threshold [by default (50+k1) bp] are labeled as long and treated as if unique and the other nodes are labeled as short.
Connections with a low support (by default 3 or lower) or with a weight <0.1 are first removed.
Two short contigs can only be joined by a direct connection with no intermediate gap.
A short and a long contig can only be connected by a direct connection.
Finally, connections between long contigs are tested against a modified version of the statistic presented in (Zerbino et al., 2009), which estimates how many read pairs should connect two contigs given their respective coverages and the estimated distance separating them (see Supplementary Material).
Indirect connections with a support lower than a given threshold (by default 10% of this expected count) are thus eliminated.
2.6 Locus construction Oases then organizes the contigs into clusters called loci, as illustrated in Figure 1.
This terminology stems from the fact that in the ideal case, where no gap in coverage or overlap with exterior sequences complicate matters, all the transcripts from one gene should be assembled into a connected component of contigs.
Unfortunately, in experimental conditions, this equivalence between components and genes cannot be guaranteed.
It is to be expected that loci sometimes represent fragments of genes or clusters of homologous sequences.
Scaffold construction takes place in two stages similarly to the approach described by Butler et al.(2008).
Long contigs are first clustered into connected components.
These long nodes have a higher likelihood of being unique, therefore it is assumed that two contigs which belong to the same component also belong to the same gene.
To each locus are added the short nodes which are connected to one of the long nodes in the cluster.
2.7 Transitive reduction of the loci For the following analyses to function properly, it is necessary to remove redundant long distance connections, and retain only connections between immediate neighbors, as seen in Figure 1.
For example, it is common that two contigs which are not consecutive in a locus are connected by a paired-end read.
A connection is considered redundant if it connects two nodes that are connected by a distinct path of connections such that the connection and the two paths have comparable lengths.
The transitive reduction implemented in Oases is inspired from the one described in (Myers, 2005) but had to be adapted to the conditions of short read data.
In particular, short contigs can be repeated or even inverted within a single transcript and form loops in the connection graph.
Because of this, occasional situations arise where every connection coming out of a node can be transitively reduced by another one, thus removing all of them, and breaking the connectivity of the locus.
To avoid this, a limit is imposed on the number of removed connections.
If two connections have the capacity to reduce each other, the shortest one is preserved.
2.8 Extracting transcript assemblies The sequence information of the transcripts is now contained in the loci.
These loci can be fragmented because of alternative splicing events which cause the de Bruijn graph to have a branch.
Oases, therefore, analyses the topology of the loci to extract full length isoform assemblies.
In many cases, the loci present a simple topology which can be trivially and uniquely decomposed as one or two transcripts.
We define three categories of trivial locus topologies (Fig.1): chains, forks and bubbles, which if isolated from any other branching point, are straightforward to resolve.
These three topologies are easily identifiable using the degrees of the nodes.
Oases, therefore, detects all the trivial loci and enumerates the possible transcripts for each of them.
Because the above exact method only applies to specific cases, an additional robust heuristic method is applied to the remaining loci, referred to as complex loci.
Oases uses a reimplementation of the algorithm described in (Lee, 2003), which efficiently produces a parsimonious set of putative highly expressed transcripts, assuming independence of the alternative splicing events.
This extension of the algorithm is quite intuitive, since there is a direct analogy between the de Bruijn graph built from the transcripts of a gene and its splicing graph, as noted by Heber et al.(2002).
Using dynamic programming, it enumerates heavily weighted paths through the locus graph in decreasing order of coverage, until either all the contigs of the locus are covered, or a specified number of transcripts is produced (by default 10).
As in the transitive reduction phase, this algorithm had to be slightly modified to allow for loops in the putative splicing graph of the locus.
Loops are problematic because their presence can prevent the propagation of the dynamic programming algorithm to all the contigs of a locus.
When a loop is detected, it is broken at a contig which connects the loop to the rest of the locus, so as to leave a minimum number of branch points, as described in the Supplementary Material.
2.9 Merging assemblies with Oases-M De Bruijn graph assemblers are very sensitive to the setting of the hash length k. For transcriptome data, this optimization is more complex as transcript expression levels and coverage depths are distributed over a wide range.
A way to avoid the dependence on the parameter k is to produce a merged transcriptome assembly of previously generated transfrags from Oases.
Oases is run for a set of [kMIN, ,kMAX] values and the output transfrags are stored.
All predicted transfrags from runs in the interval are then fed into the second stage of the pipeline, Oases-M, with a user selected kMERGE.
A de Bruijn graph for kMERGE is built from these transfrags.
After removing small variants with the Tourbus algorithm, any transfrag in the graph that is identical or included in another transfrag is removed.
The final assembly is constructed by following the remaining transfrags through the merged graph.
3 RESULTS 3.1 Datasets Two datasets were retrieved from the Nucleotide Archive (http://www.ebi.ac.uk/ena/).
A human dataset was produced in a study by Heap et al.(2010), where poly(A)-selected RNAs from human primary CD4(+) T cells were sequenced.
Paired-end reads of length 45 bp with an insert size of 200 bp from one human individual (studyID SRX011545) were downloaded.
A mouse dataset was taken from the study of Trapnell et al.(2010).
In a timeseries experiment of C2C12 myoblast mouse cells, paired-end reads of length 75 bp with an insert size of 300 bp were sequenced.
Read data from the 24 h timepoint (study id SRX017794) was used.
To reduce the amount of erroneous bases, both paired-end datasets were processed by (i) removing Ns from both ends, (ii) clipping bases with a Sanger quality 10 and (iii) removing reads with more than six bases with Sanger quality 10 after steps (i) and (ii), leading to a total of 30 940 088 and 64 441 708 reads for human and mouse, respectively.
3.2 Assemblies and alignments All experiments were run with Oases version 0.2.01, and Velvet 1.1.06 and the coverage cutoff and the minimum support for connections were set to 3.
TransABySS 1.2.0 was run with ABySS 1.2.5 through the first two stages of transABySS (assembly and merging, before mapping to a reference genome is required).
Instead of just running with 1088 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1089 10861092 Oases de novo RNA-seq assembly Table 1.
Comparison of Velvet and Oases assemblies on the human RNAseq dataset k-mer Method Tfrags >100 bp Sens.
(%) Spec.
(%) Full Lgth.
80% lgth.
19 Velvet 89 789 12.45 83.58 42 78 Oases 67 319 17.23 92.55 828 7437 25 Velvet 88 042 16.13 89.62 92 516 Oases 53 504 14.97 93.0 754 6882 31 Velvet 55 986 12.78 93.16 213 1986 Oases 47 878 10.55 94.63 429 3751 35 Velvet 36 507 7.9 94.81 107 1660 Oases 34 012 6.67 95.99 196 1885 The total number of transfrags longer that 100 bp (Tfrags), nucleotide sensitivity and specificity, as well as the number of full length or 80% length reconstructed Ensembl transcripts are shown.
the default parameters, we tested an array of parameters and chose the best for those datasets, namely n = 10, c = 3 and ABYSS with the options-E0 (Supplementary Material).
Trinity (ver.
2011-08-20) was run with the default parameters.
In particular, the k-mer length of 25 could not be modified.
Potential poly-A tails after assembly were removed using the trimEST program from the EMBOSS package (Rice et al., 2000) before alignment.
Subsequently, predicted transfrags of the methods were aligned against the genome using Blat (Kent, 2002).
The Cufflinks assemblies are those published by its authors.
Reads per kilobase of exon model per million mapped reads (RPKM), as defined by Mortazavi et al.(2008) expression values for annotated genes have been computed by aligning reads against annotated Ensembl 57 transcripts with RazerS (Weese et al., 2009), (see Supplementary Material).
3.3 Metrics In all the following experiments, we focused on a simple set of metrics as used in (Robertson, 2010; Yassour, 2011): nucleotide sensitivity, nucleotide specificity, percentage of transcripts assembled to 100% of their length and percentage of transcripts assembled to 80% of their length.
The Blat mappings of the assemblies were compared with the Ensembl annotations of the corresponding species.
3.4 Comparing Oases to Velvet To evaluate the added value of the topology resolution within each loci, we compared the Oases contigs from the Velvet assemblies which they are built from.
Table 1 shows how the Oases assemblies significantly improve on the Velvet assemblies.
This confirms the intuition that in the presence of alternative splicing and dynamic expression levels, the assembly is broken by breaks in the graph, which can be resolved by topological analysis and adapted error correction as described in the Methods section.
As an example, the percentage cutoff for local edge removal was modulated (see Supplementary Table S1).
These results show how dynamic filters improve the quality of the assembly.
3.5 Impact of k-mer lengths One of the major parameters in de Bruijn graph assemblers is the hash length, or k-mer length.
Comparing single-k assemblies 20 40 60 80 100 0 20 0 40 0 60 0 80 0 Expression Quantiles R ec on st ru ct ed to a t l ea st  8 0 % Merged 19 35 k=19 k=21 k=27 k=31 k=35 Fig.2.
Comparison of single k-mer Oases assemblies and the merged assembly from kMIN=19 to kMAX=35 by Oases-M, on the human dataset.
The total number of Ensembl transcripts assembled to 80% of their length is provided by RPKM gene expression quantiles of 1464 genes each.
performed by Oases, it is possible to observe that this parameter is crucial in RNA-seq assembly.
Figure 2 shows how the k-mer length is closely related to the expression level of the transcripts being assembled.
As expected, the assemblies with longer k-values perform best on high expression genes, but poorly on low expression genes.
However, short k-mer assemblies have the disadvantage of introducing misassemblies, as shown in Supplementary Table S7.
3.6 Impact of merging assemblies In addition, Figure 2 shows the same statistics for the merged assembly by Oases-M, which is significantly superior to each of the individual values.
This result illustrates how the different assemblies do not completely overlap.
Further, Supplementary Figure S2 shows how each single k-mer assembly resolved transcripts at different expression levels.
We compared merging different intervals of k-mers (see Supplementary Material).
The wider the interval, the better the results.
To determine bounds on this interval we arbitrarily bounded on the low values with 19, on the assumption that smaller k-mers are very likely to be unspecific for mammalian genomes (Whiteford et al., 2005).
In theory, on the upper end, all the k-mer values (up to read length) could be used.
To avoid wasting resources, we measured the added value of each new assembly (see Supplementary Material).
As expected, marginal gains progressively diminish and this metric could be used to determine how large a spectrum of k-mers to use.
We also investigated which kMERGE should be used and we found that kMERGE= 27 works well with little difference for higher values (see Supplementary Table S4) and is therefore used for all analyses in the article.
3.7 Comparing Oases to other RNA-seq de novo assemblers Oases-M was compared with existing RNA-seq de novo assemblers, transABySS (Robertson et al., 2010) and Trinity (Yassour et al., 2011).
The previous human dataset and a mouse dataset were used 1089 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1090 10861092 M.H.Schulz et al.Fig.3.
Reconstruction efficiency of Ensembl transcripts for different RNA-seq de novo assembly methods (Oases-M, Trinity, and transABySS) on human and mouse datasets.
Reference-based assembly results using Cufflinks are provided on the mouse dataset.
All annotated genes have been grouped into quantiles by RPKM expression values of 1464 (resp.
1078) genes for the human data (resp.
mouse).
for the comparison.
The datasets have different read lengths and sequencing depth, as detailed in Methods.
Both transABySS and Oases were run for k-mer length 1935 bp on the human dataset.
Because the mouse reads are longer, these two assemblers were run for k-mers 2135 on that dataset.
The highest value of k was determined by an approach similar to that used on the human data (see Supplementary Material for data).
Trinity is fixed by implementation at k =25 bp.
Figure 3 shows the number of reconstructed Ensembl transcripts for each assembler on both datasets separated by expression quantiles.
The main observation is that all assemblers do not behave equally with respect to expression level.
Trinity appears to perform best on high expression genes, whereas transABySS performs best on low expression genes.
Oases performs comparatively well throughout the spectrum of expression levels, hence the greater overall success (Table 2).
Regarding correctness, we computed the number of misassemblies and the qualities of the different assemblers are comparable (see Supplementary Material).
Transfrags mapped with high confidence to the genome occasionally differ from the known annotation.
For example, Oases produced 237 (resp.
390) transfrags longer than 300 bp which mapped to the reference genome, but did not overlap with the human (resp.
mouse) annotation.
In Figure 4, the overlap of full length mouse transcripts reconstructed by the three methods is shown.
It is interesting to note that although the results greatly overlap, the different assemblers succeeded in assembling different transcripts.
3.8 Comparing de novo and reference-based assemblers Oases and the other de novo assemblers were finally compared on the mouse data to a reference-based assembly algorithm, Cufflinks (Trapnell et al., 2010), on the mouse dataset.
As could be expected, Cufflinks generally outperforms the de novo assembly algorithms, 1090 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1091 10861092 Oases de novo RNA-seq assembly Table 2.
Overall comparison of the different RNA-seq assembly methods on human and mouse datasets Data Method Tfrags >100 bp Sens.
(%) Spec.
(%) Full lgth 80% lgth Human Oases-M 174 469 21.44 92.35 1463 11 169 tABySS 100 127 19.65 92.16 1358 10 992 Trinity 76 232 19.99 88.63 953 7129 Mouse Oases-M 175 914 30.83 89.08 1324 9880 tABySS 174 744 30.66 92.79 1149 9376 Trinity 92 810 31.57 87.14 1085 7028 Cufflinks 63 207 48.13 75.29 4369 21 222 The number of transfrags longer than 100 bp produced (Tfrags) and nucleotide sensitivity and specificity, as well as the number of full length or 80% length reconstructed Ensembl transcripts are shown.
Fig.4.
Venn Diagramm that compares mouse Ensembl transcripts reconstructed to full length by Trinity, Trans-AbySS and Oases-M for the mouse RNA-seq data.
as it benefits from using the reference genome to anchor its assemblies (Fig.3).
Nonetheless, it is interesting to note that as expression level and therefore coverage depth go up, the gap narrows.
Beyond assembling more transcripts, it is also important to recover multiple isoforms for each gene.
For each assembled transcript, the average number of additionally assembled transcripts from the same gene are, respectively, 1.21, 1.25, 1.01 and 1.56 for Oases, transABySS, Trinity and Cufflinks.
Cufflinks performs better in that respect, whereas Trinity is less sensitive.
3.9 Runtime and memory Ade novo transcriptome assembly is a resource intensive task.
Velvet uses multithreading but Oases currently does not.
The complete merged assembly for human took 3.2 h and 6.1 GB of peak memory on a 48 core AMD Opteron machine with 265 GB RAM.
The merged assembly for mouse took 10.3 h and 15.1 GB peak memory.
4 DISCUSSION We have shown that merging different single k assemblies is beneficial, in concordance with previous work (Surget-Groba and Montoya-Burgos, 2010; Robertson et al., 2010).
Oases employs dynamic cutoffs, where possible, to allow for a robust reconstruction with different k-values.
However, detailed parameter optimization for Oases and trans-ABySS may lead to further improvements.
Overall, the de novo methods produced large numbers of misassemblies.
Given the dynamic ranges involved, the exact parameter settings of these programs define a trade-off between sensitivity and accuracy.
In these experiments, Oases tends to be more sensitive, Trinity more accurate.
The correlation of small k-mer assemblies and misassembly rates suggests that homologies between genes are the main source of errors.As reads get longer, and coverage depths greater, sensitivity will only increase and users will probably avoid the shorter k-mer lengths for greater accuracy.
Short k-mers will only be necessary to retrieve the very rare transcripts.
An independent but significant factor to these assemblies is read preprocessing, as read error removal has already been shown to have a significant impact in the context of de novo genome assembly (Smeds and Knster, 2011).
Interestingly, the comparison of reconstructed transcripts for the three de novo methods in Figure 4 reveals that each method outperforms the others on a separate set of transcripts.
These differences in performance are probably due to the different strategies employed to remove errors.
A more aggressive method, which discards more data, would presumably end up with many gaps on low expression data, whereas a more lenient algorithm would leave too many ambiguities at high coverage.
In particular, it appears that the performance of all the assemblers sometimes drops at very high coverage depths.
This is probably linked to increased noise.
Indeed, this drop is especially marked for transABySS, which, to our knowledge, is the only of the three de novo assemblers not to integrate dynamic filters which adapt with coverage depth.
Intriguingly, transABySS outperformed Trinity in our experiments, contrary to the observation of Yassour et al., (2011).
This could not be due to the parameterization of Trinity, which cannot be parameterized apart from the insert length.
Instead, the larger k range used for transABySS and the lower sequencing depth in our analyzed data sets may explain this discrepancy, as transABySS was shown to perform especially well for low to medium expressed genes.
Similarly, our experiments on mouse data show a bigger gap between Cufflinks and the de novo assemblers than observed by Yassour et al.(2011).
In their work, the comparison was focused on the set of oracle transcripts, which show sufficient coverage of exact k-mers in the reads.
However, no such restriction was applied here and Cufflinks surpasses the de novo methods for low to medium expression ranges, where coverage is sparse.
In this study, we did not analyze strand-specific RNA-seq datasets.
However, as these datasets become more available (Levin, 2010) Oases already supports this data.
During the hashing phase, reverse complement sequences can be stored separately instead of being joined as the two strands of the same sequence.
5 CONCLUSION Oases provides users with a robust pipeline to assemble unmapped RNA-seq reads into full length transcripts.
Oases was designed to deal with the conditions of RNA-seq, namely uneven coverage and alternative splicing events.
Our results show how crucial it is to explore and understand the relevant conditions.
Alternative splicing can significantly confound 1091 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1092 10861092 M.H.Schulz et al.the assembly and has to be specifically addressed.
Gene expression levels are a major factor determining the sensitivity of an algorithm.
High coverage genes require more selective methods, whereas low coverage genes favor more sensitive algorithms.
This is why exploring a range of k-mer lengths is key to success.
In the light of these results, Oases was designed to perform well overall by adapting to these varying conditions and succeeded in obtaining superior overall results compared to previously published RNA-seq de novo assemblers.
Nonetheless, it also appears that merging assemblies from a diversity of algorithms could be beneficial.
This is probably due to the dynamic range of all the variables, which prevent any single method from being systematically superior.
Finally, we examined the difference between de novo and reference-assisted assembly.
In the presence of a well-assembled genome (typically human or mouse), the latter methods are generally at a significant advantage.
Nonetheless, this gap reduces at high expression levels.
This shows that the absence of an assembled genome can be largely compensated for provided sufficient read coverage.
ACKNOWLEDGEMENTS Thank you to Cole Trapnell, Ali Mortazavi and Diane Trout for their help in obtaining the C2C12 dataset.
Many thanks to Brian Haas for his patient testing of Oases, and to all the other users for their feedback.
Thank you to Saket Navlakha and Benedict Paten for their proofreading and commentaries.
Funding: M.H.S.
was supported by the International Max Planck Research School for Computational Biology and Scientific Computing.
D.R.Z.
and E.B.
were funded by EMBL central funds.
D.R.Z.
is also funded by ENCODE grant 1U41HG004568-01, the NHGRI GENCODE subcontract (Prime: 1U54HG004555-01, Subaward: 0244-03) and the NHGRI ENCODE DAC grant (Prime: NHGRI 1U01HG004695-01, Subcontract: European Bioinformatics Institute).
Conflict of Interest: none declared.
ABSTRACT Motivation: The growth of sequence data has been accompanied by an increasing need to analyze data on distributed computer clusters.
The use of these systems for routine analysis requires scalable and robust software for data management of large datasets.
Software is also needed to simplify data management and make large-scale bioinformatics analysis accessible and reproducible to a wide class of target users.
Results: We have developed a workflow management system named Ergatis that enables users to build, execute and monitor pipelines for computational analysis of genomics data.
Ergatis contains preconfigured components and template pipelines for a number of common bioinformatics tasks such as prokaryotic genome annotation and genome comparisons.
Outputs from many of these components can be loaded into a Chado relational database.
Ergatis was designed to be accessible to a broad class of users and provides a user friendly, web-based interface.
Ergatis supports highthroughput batch processing on distributed compute clusters and has been used for data management in a number of genome annotation and comparative genomics projects.
Availability: Ergatis is an open-source project and is freely available at http://ergatis.sourceforge.net Contact: jorvis@users.sourceforge.net Received on 15 February 2010; revised on 9 April 2010; accepted on 13 April 2010 1 INTRODUCTION Workflow management systems (WMS) include software systems that execute and manage computational pipelines.
These have become important tools in bioinformatics because they enable researchers to analyze the massive quantities of data generated by modern laboratory equipment.
There are a number of WMS targeted to bioinformatics that differ in scope and approach for construction To whom correspondence should be addressed.
and execution of workflows (Romano, 2008; Tiwari and Sekhar 2007).
One class of WMS enables biologists to manipulate data in a manner that would normally require some level of scripting ability or the use of a collection of local tools and web forms.
The scope of this class usually includes querying preexisting datasets and transforming results.
Operations include retrieving sequences from public collections, extraction of subsequences, converting among file formats and performing set operations on collections of results.
Applications in this class include ISYS, a local application and development framework (Siepel et al., 2001), and Galaxy, a comprehensive web-based interface designed for tool and database integration (Giardine et al., 2005).
Galaxy, in particular, excels at enabling users to gather data from diverse sources and execute a set of queries.
Pipelines are represented as a history of user actions within the system that can be reused.
BioMOBY (Wilkinson and Links, 2002) and Taverna (Oinn et al., 2004) are notable instances of a class of WMS that organize and integrate a disparate collection of web service providers.
In this model, data are exchanged using a common format and protocol, usually XML and SOAP, respectively, between the host and any number of providers during the course of a pipeline execution.
Users of these systems do not need extensive local hardware resources since all computes are performed at each providers site.
Indeed, the number of providers available for common and overlapping services has grown so large that new strategies have been developed to assist in managing and ranking them (DiBernardo et al., 2008).
For data-intensive pipelines, web service approaches can be limited by network performance, service availability and I/O compatibility between providers.
Other WMS typically assume direct access to component executables and manage the execution of pipelines either locally or on a compute cluster.
Wildfire (Tang et al., 2005) and Pegasys (Shah et al., 2004) aim to enable construction of components into a pipeline using local graphical user interfaces, representing these pipelines in a form that can be executed on distributed resources.
Ergatis is a WMS that fits into this last class and is targeted toward the analysis of genome sequence data.
Ergatis is designed to be accessible to bioinformaticians and biologists alike.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:43 21/5/2010 Bioinformatics-btq167.tex] Page: 1489 14881492 Ergatis Fig.1.
Architecture diagram showing process and data flow from pipeline creation in Ergatis, processing of wXML by Workflow Engine, job scheduling on a computational grid by SGE and finally optional data loading into a Chado relational database instance.
Using an intuitive web-based interface, biologists can use the suite of integrated components to construct new analysis pipelines or reuse existing pipeline templates.
These pipelines can be executed on a single desktop or distributed across large compute clusters.
Their output can be converted into common formats or loaded into a relational database.
The underlying workflow system provides structured pipeline representation, monitoring, audit capability and task execution on managed compute resources such as a grid or compute cluster.
Our system allows construction of pipelines from common bioinformatics analysis tools as well as providing an architecture within which these pipelines can be reused and applied to new datasets.
Ergatis is the only WMS among these to provide these features and contain prepackaged pipelines for whole-genome annotation, comparative genomics and pan-genome diversity studies.
2 ARCHITECTURE AND METHODS The architecture of Ergatis is shown in Figure 1.
Ergatis consists of a web interface for building workflows, a processing engine for execution and reusable components and templates that incorporate a suite of bioinformatics tools and tasks.
It has a web interface that allows users to create a pipeline description in XML in a simple point and click manner.
The steps of a pipeline are described in XML that is read by the processing engine.
The processing engine uses Sun Grid Engine (SGE) to schedule and manage jobs on a compute cluster.
2.1 Pipeline components and templates Ergatis allows users to build pipelines out of modular analysis components.
A component is a series of steps where each step is a script or binary executable.
Most components consist of a bioinformatics package, such as a sequence alignment or gene prediction program along with pre-and postprocessing Perl scripts.
A component is described by an XML definition file and a configuration file that, together, define the steps of the component and its configurable parameters.
A set of components constitutes a pipeline.
Components can be combined in series or in parallel to form any desired order of execution.
Pipelines can be executed locally or distributed across a compute grid.
They can also be saved as an XML file for future execution or sharing with other users.
2.2 Inputs and outputs Ergatis was designed to support batch processing of sequences in a highthroughput environment using compute grids.
Parallelism is achieved by splitting inputs into groups and distributing each group to a node of the compute cluster.
Support for generating groups is included in each component.
The automatic input grouping described above helps to alleviate load on job schedulers and avoid file system limitations using an output directory.
Each component in Ergatis has a defined set of required input files.
Input files include traditional FASTAformatted or Bioinformatic Sequence Markup Language (BSML) files (LabBook).
BSML is an XML format that provides a simple encoding for sequences, annotation and search results.
Our current Ergatis release includes scripts for parsing and generating BSML from common input formats such as GFF3 and the GenBank flat file format and most components in Ergatis include scripts to also transform the native tools output to BSML.
Using BSML as a common output format has the added advantage of regularizing data by using terms from controlled vocabularies such as the Sequence Ontology (Eilbeck et al., 2005) and Gene Ontology (Ashburner et al., 2000).
Ergatis also includes support for the Chado relational database schema.
Chado is a community-supported schema for biological data that relies heavily on the use of ontologies for typing data (Mungall and Emmert, 2007).
Ergatis includes a component (initdb) that can initialize a Chado database with a set of ontologies described in OBO file format (Day-Richter).
Genome sequences, annotation and search evidence that are encoded in BSML can be written to and read from a Chado database instance using the bsml2chado and chado2bsml Ergatis components.
The Ergatis database components support multiple database vendors, including PostgreSQL, MySQL and Sybase.
2.3 Workflow processing and grid support A scientific workflow can be imagined as a directed acyclic graph (DAG) where the nodes of the graphs are scientific processes and the edges represent the path for data and process flow.
Ergatis uses a processing engine called Workflow that has a simple XML format for describing steps in a pipeline called wXML.
The wXML represents the procedural specification of such scientific workflows in a machine-readable language that describes the nodes of the DAG as Command or CommandSet elements.
Command elements represent a single atomic process while CommandSet elements represent a collection of Command and CommandSet elements, allowing the capability to nest such elements and construct complex, hierarchical 1489 [15:43 21/5/2010 Bioinformatics-btq167.tex] Page: 1490 14881492 J.Orvis et al.Table 1.
Selected Ergatis components by classification Component type Count Examples Gene prediction 14 fgenesh, glimmer3, genscan, RNAmmer HMM alignment 4 hmmpfam, panther Sequence masking 2 repeatmasker, seg Functional prediction 12 SignalP, tmhmm, pFunc Phylogeny/binning 3 RDP, stap Multiple alignment 3 clustalw, MUSCLE Pairwise alignment 14 NCBI blast suite, WU-BLAST, BER Ergatis release v2.r12 currently contains 162 components that can be used to form complex bioinformatics analysis pipelines.
workflows.
Additionally, the CommandSet construct permits the ability to iterate sequentially or concurrently over a subset of the elements of a workflow encapsulated in a CommandSet.
The workflow engine is written in Java and is multithreaded to support multiprocessors on a local machine or computational grid.
Workflow Engine supports SGE and Condor through use of the Distributed Resource Management Application API.
The supporting hardware architecture includes a distributed compute cluster and a shared file system, such as Network File System.
The workflow engine executes the wXML, distributing jobs on a local server or a compute cluster.
The engine tracks the execution of the workflow steps and maintains detailed audit information for each command in the wXML.
Workflow engine has the ability to recover from errors and resume execution from the last point of failure or roll back execution to a userdefined arbitrary location in the workflow and resume execution from that point.
2.4 Pipeline build/monitor interface Ergatis provides a web interface to build a wXML description of a pipeline.
The pipeline wXML is built from reusable modular components that come prepackaged with Ergatis.
The pipeline wXML contains a complete description of the pipeline including start and end times, error messages, return values, execution host, logging and execution strings of every command in the pipeline.
The Ergatis web interface reads the pipeline wXML to provide status information to the user.
The Ergatis interface allows users to build pipelines by easily adding components in a pipeline and dynamically arranging them to execute either serially, in parallel or in any nested combination of these.
A subset of Ergatis components is listed in Table 1.
While monitoring a pipeline the interface periodically updates the status of each component, with progress bars color coded by status.
Command counts and a label of the current execution step are displayed.
Not limited to this pipeline-level abstract view, users can click through to see details on each component and down to the level of each individual command, allowing any individual command line string to be copied and run within a terminal.
2.5 Collaborative pipeline development Important features of a WMS include pipeline reuse and exportability.
Pipelines built with Ergatis can be saved as a pipeline template and reused within the system or exported to other users.
The template contains the pipeline layout and each components configuration options, allowing the pipeline to be exactly reproduced.
Importing a pipeline requires that it be placed inside of directory in the Ergatis installation.
The templates are also small enough to be sent via e-mailthe 47-component prokaryotic annotation pipeline described below is only 28 kb.
We encourage developers in the bioinformatics community to utilize the public Subversion repository, located on the Ergatis SourceForge project site, to build and contribute new components.
3 PIPELINES Ergatis has served as a data management tool in bioinformatics cores with compute grids of up to 600 CPU cores at both the J. Craig Venter Institute and the Institute for Genome Sciences, University of Maryland School of Medicine.
It has been used to build and run analysis pipelines that have been incorporated into numerous published individual genome (Nene et al., 2007; Ouyang et al., 2007; Carlton et al., 2007) and comparative genomic studies (Hotopp et al., 2006; El-Sayed et al., 2005).
Ergatis contains several components for gene/RNA prediction, repeat masking, BLAST, HMM searching, subcellular localization prediction and more (Table 1).
Ergatis also includes a number of multicomponent analysis pipelines.
These include pipelines for bacterial genome annotation, an orthology identification pipeline and a pan-genome analysis pipeline.
3.1 Genome annotation 3.1.1 Bacterial genome annotation Ergatis includes a pipeline template for automated prokaryotic genome annotation that is composed of 36 analysis components.
These include gene structure prediction and evaluation, RNA analysis, homology searches, frameshift analysis and functional prediction.
Predicted transcripts are assigned functional names, Enzyme Commission numbers, gene symbols and GO terms where possible.
The final output is a BSML file that can be loaded into a Chado database instance.
Execution of this pipeline on a Pyrobaculum species, with a genome size of 2.2 megabases, yielded 2863 predicted genes.
The pipeline had 202 704 commands overall and ran in 216 CPU hours, or 3.8 actual hours when distributed on our compute grid.
Ergatis and this pipeline template are used to support the IGS annotation engine (Giglio).
3.1.2 Eukaryotic genome annotation The complexity of eukaryotic gene structures makes purely automated prediction with a single pipeline difficult.
Ergatis contains components for over a dozen different gene prediction programs, such as GeneWise (Birney et al., 2004), GeneMark (Besemer and Borodovsky, 2005) and FGENESH (SoftBerry), as well as RNA prediction.
Gene models can be used as inputs to other components for functional annotation using sequence searches, signal sequence prediction using SignalP and protein subcellular localization with components such as TargetP and TMHMM (Emanuelsson et al., 2007).
Ergatis has been the primary data management tool for several eukaryotic genome projects, including Aedes aegypti (Nene et al., 2007) and Oryza sativa (Ouyang et al., 2007).
3.2 Comparative genomics The decreasing cost of genome sequencing has provided data for the comparative analysis of related whole genomes.
Ergatis provides a pipeline template to identify putative paralogs and orthologs within a collection of organisms.
The pipeline is based on all-vsall BLASTP searches and a reciprocal best BLAST clustering of proteins.
Putative paralogs are flagged from BLAST hits that span at least 80% of sequence length at least 80% identity (Crabtree et al., 2007).
The pipeline template consists of eight components and provides default cutoffs for the BLAST and clustering steps.
The output is set of gene clusters encoded in a BSML file that can be loaded into Chado and visualized using Sybil (Crabtree et al., 2007).
1490 [15:43 21/5/2010 Bioinformatics-btq167.tex] Page: 1491 14881492 Ergatis This comparative pipeline was used to build gene clusters for the Pathema resource center (Brinkac et al., 2010).
The analysis contained an all-vs-all protein BLAST of the 131 008 polypeptide sequences in this organism set.
The BLAST search ran in 172 CPU hours and the complete pipeline ran in 381 compute hours, which included output validation and format conversion steps to a tab-delimited format and BSML.
Distributed across a 60-node compute cluster, this component took just 8.2 h to complete.
The Ergatis comparative pipeline was also used to build gene clusters for the Strepneumo website (Tettelin) with a group of 32 Streptococcal strains to identify putative ortholog and paralog gene clusters.
This collection of organisms contained 72 101 coding genes, which were predicted to form 1543 paralogous clusters and 3342 orthologous clusters.
This pipeline contained 79 106 commands that were executed in 87 CPU hours which, when distributed across a 100-node computation cluster, had a wall-clock runtime of 1 h and 53 min.
Ergatis also includes a pipeline template to summarize genomic diversity in a pan-genome (Tettelin et al., 2008).
The pipeline classifies genes in the pan-genome as core (conserved across all input genomes), shared (conserved across a subset of input genomes) and unique (present in a single genome) used the method described in Tettelin et al.(2008).
The results are plotted and a regression is fit to determine whether the data suggest more sequenced genomes would uncover more new genes (an open pan-genome) or not (a closed pan-genome).
A related plot estimates size of the entire gene repertoire (or pan-genome) for the species being sampled.
The Ergatis pan-genome pipeline was used on the analysis of 14 Yersinia pestis genomes in Eppinger et al.(2010).
The input of 4844 unique protein-coding genes ran for 16 h when distributed across 150 compute nodes of our cluster consuming 519 compute hours.
4 DISCUSSION The Ergatis uses a modular, scalable and extensible approach to pipeline creation and management on local or distributed compute resources.
It provides a wide array of analysis components and pre-configured pipeline templates with which users can build their own custom pipelines.
While flexible and extensible, this modular approach is not necessarily the most efficient way to execute some pipelines.
Each modular component accepts a set of input types and creates one or more output types.
Because of this, Ergatis pipelines may involve a series of format conversion steps that can incur extra computational overhead.
We believe that the benefits of the component abstraction layer, such as modular construction and pipeline reuse, outweigh drawbacks such as this.
We believe this approach is preferable over webservice-based systems for institutions who possess adequate computational hardware and who wish to ensure maximal resource availability and customization.
Pipelines can be executed in Ergatis without concern for service availability or data transfer limitations to remote service sites.
Though initially intended as a local pipeline management tool, Ergatis has been extended and employed by some users to drive publicly available computational web resources.
One example of this is the Integrative Services for Genomics Analysis, a webbased prokaryotic annotation server that uses Ergatis as its back-end (Hemmerich et al., 2010).
Ergatis has also been extended to serve as the pipeline framework for CloVR, a virtual appliance that integrates genomics tools on cloud computing platforms for viral, prokaryotic, metagenomic and eukaryotic sequencing projects (Fricke).
Current and future work includes improvement to the web interface, training documentation and addition of new components and pipeline templates to Ergatis for analysis of metagenomics and transcriptomics data.
This work will also enable Ergatis to serve as one of the access portals for the Data Intensive Academic Grid, a publicly available 1000+ core computational infrastructure currently under development.
The Ergatis software is open source and freely available at http://ergatis.sf.net.
ACKNOWLEDGEMENTS We would like to thank both the developers on the project and the user community who continue to suggest new features and improve the project.
Funding: National Institute of Allergy and Infectious Diseases Microbial Sequencing Contract (NIH-N01-AI-30071); National Institutes of Health BRC contract (NIH-N01-AI-30071) in part.
Conflict of Interest: none declared.
Abstract Motivation: With rapid accumulation of sequence data on several species, extracting rational and systematic information from multiple sequence alignments (MSAs) is becoming increasingly important.
Currently, there is a plethora of computational methods for investigating coupled evolutionary changes in pairs of positions along the amino acid sequence, and making inferences on structure and function.
Yet, the significance of coevolution signals remains to be established.
Also, a large number of false positives (FPs) arise from insufficient MSA size, phylogenetic background and indirect couplings.
Results: Here, a set of 16 pairs of non-interacting proteins is thoroughly examined to assess the effectiveness and limitations of different methods.
The analysis shows that recent computationally expensive methods designed to remove biases from indirect couplings outperform others in detecting tertiary structural contacts as well as eliminating intermolecular FPs; whereas traditional methods such as mutual information benefit from refinements such as shuffling, while being highly efficient.
Computations repeated with 2,330 pairs of protein families from the Negatome database corroborated these results.
Finally, using a training dataset of 162 families of proteins, we propose a combined method that outperforms existing individual methods.
Overall, the study provides simple guidelines towards the choice of suitable methods and strategies based on available MSA size and computing resources.
Availability and implementation: Software is freely available through the Evol component of ProDy API.
Contact: bahar@pitt.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 Introduction With sequence data being generated at an ever increasing rate in the post-genomic era, it is becoming crucially important to develop efficient and accurate methods at the interface between evolutionary biology, computational biology and molecular biophysics to learn and make inferences from sequence data (Liberles et al., 2012).
Structural and functional properties of proteins go hand-in-hand with their evolutionary properties.
For instance, maintaining protein stability usually involves interactions between conserved residues at the core of the structure.
Likewise, biochemical activities such as catalysis involve conserved residues.
Recognition sites, on the other VC The Author 2015.
Published by Oxford University Press.
1929 This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Bioinformatics, 31(12), 2015, 19291937 doi: 10.1093/bioinformatics/btv103 Advance Access Publication Date: 19 February 2015 Original Paper hand, may show correlated mutations that maintain the balance between specificity and adaptability (Tokuriki and Tawfik, 2009a, b).
Recent studies also highlight how sequence evolution correlates with structural dynamics (Liu et al., 2010; Liu and Bahar, 2012).
Coevolution patterns derived from multiple sequence alignments (MSAs) provide valuable constraints that assist in structure prediction (Marks et al., 2011, 2012; Morcos et al., 2011; Weigt et al., 2009).
The idea of inferring inter-residue contacts for structure prediction, using sequence correlation data indeed goes back to the early 1990s (e.g.Gobel et al., 1994).
Such applications may be particularly useful in the case of membrane proteins that can often be difficult to resolve (Hopf et al., 2012).
Detection of correlated mutation sites could also assist in identifying hot spots, and provide guidance for protein design and engineering.
In line with increased sequence data and, thereby, increased opportunity for detecting and interpreting sequence correlations across the members of protein families, a broad range of theory and methods have been introduced for correlated mutations analysis (CMA) in the last decade.
Mutual information (MI) (Gloor et al., 2005) from information theory was one of the first rigorous metrics adopted for quantifying the extent of cross-correlations between amino acid substitutions in proteins.
A corrected version, MIp, where the background noise and phylogenetic effects were largely eliminated by subtracting an average product correction (APC) (Dunn et al., 2008) proved to enhance signals associated with amino acids that are proximal in the structure.
Also, non-MI-based methods have been shown to help identify correlated mutations, such as the observedminus-expected-squared (OMES) method (Kass and Horovitz, 2002) and the statistical coupling analysis (SCA) (Halabi et al., 2009; Lockless and Ranganathan, 1999).
More recently, advanced approaches that require more expensive computations have been introduced, focused on removing indirect (or transitive) couplings that may obscure the detection of direct correlations between sequence positions.
Such methods include direct coupling analysis (DCA or DI for direct information) (Morcos et al., 2011; Weigt et al., 2009), Protein Sparse Inverse COVariance (PSICOV) (Jones et al., 2012), a Bayesian network algorithm for disentangling direct from indirect dependencies between residues (Burger and van Nimwegen, 2010), the pseudolikelihood maximization DCA (plmDCA) method due to Ekeberg et al.(2013), Gremlins pseudo-likelihood method (Kamisetty et al., 2013) and a network deconvolution approach based on spectral decomposition of the correlation matrix (Feizi et al., 2013).
These studies have shown success in detecting correlations that relate to contacts in the three-dimensional (3D) structure, and in reverse engineering the 3D structure from correlations.
In a control study, Horovitz and coworkers (Noivirt et al., 2005) demonstrated that CMA methods may erroneously yield coevolutionary signals even between non-interacting proteins.
This study performed for a set of 16 non-interacting protein pairs (Supplementary Table S1) further showed that shuffling algorithms could be adopted to improve signal-to-noise ratio and reduce these false positives (FPs).
Of interest is to see if methods developed for improving the detection of 3D contact-making residues are equally effective in eliminating intermolecular FPs.
In a broader context, it is not often clear which method might be most suitable for a given set of data, or what are their limits of applicability.
Which fraction of signals outputted by these methods can be reliably used for making structural or functional inferences?
How does the size of the MSA affect the results?
Can we estimate the minimum size of the MSA to achieve a certain level of accuracy?
Can we design hybrid approaches, or combined methods, that take advantage of the strengths of different methods to outperform individual methods?
In the present study, we present a critical assessment of the performance of nine methods/approaches developed for predicting pairwise correlations from MSAs.
Proteins in Supplementary Table S1 (see also Supplementary Information (SI), Supplementary Table S2) are adopted as a benchmark dataset for a detailed analysis, which is further consolidated by extending the analysis to a dataset of 2330 structurally resolved protein pairs extracted from Negatome 2.0 database (Blohm et al., 2014) of non-interacting proteins.
Two basic performance criteria are considered: first, does the method correctly filter out intermolecular correlations (FPs) if the analyzed pairs of proteins are known to be non-interacting?
Second, if one focuses on intramolecular signals, does the method detect the pairs that make tertiary contacts in the 3D structure (termed intramolecular true positives, TPs)?
The study shows that the abilities of the existing methods to discriminate intermolecular FPs are comparable, but their abilities to identify intramolecular TPs vary, with DI and PSICOV outperforming others.
We also analyse the relationship between the size of MSAs and the effectiveness of shuffling algorithm.
We examine the similarities/dissimilarities, or the level of consistency, between the outputs from different methods, and provide simple guidelines for estimating how accuracy varies with coverage.
Finally, using a nave Bayesian approach with a training dataset of 162 families of proteins (SI, Supplementary Table S3), we propose a combined method of PSICOV and DI that provides the highest levels of accuracy.
Overall, the study provides a clear understanding of the capabilities and deficiencies of existing methods to help users select optimal methods for their purposes.
2 Materials and methods 2.1 Dataset We used two datasets for our computations: Dataset I, comprised of 16 pairs of non-interacting proteins (Supplementary Table S1) introduced by Horovitz and coworkers as a benchmarking set for CMA (Noivirt et al., 2005) and Dataset II derived from the Negatome 2.0 database of non-interacting proteins/domains (Blohm et al., 2014).
Dataset I contained 15 distinctive families of proteins, the properties of which are detailed in the SI, Supplementary Table S2.
We present in Supplementary Table S1 the numbers of sequences/rows (m) as well as the number of columns (N) for each of the 16 MSAs generated for Dataset I.
Supplementary Table S2 lists the corresponding Pfam (Punta et al., 2012) domain names, representative UNIPROT (UniProt Consortium, 2014) identifiers and Protein Data Bank (PDB) (Bernstein et al., 1977) structures, along with the MSA sizes (m and N) used for analyzing separately the intramolecular coevolutionary properties of the individual proteins.
About half of the proteins in this set contained more than one Pfam domain (Supplementary Table S2).
Only those domains that appeared in more than 80% of the sequences were considered for further analysis.
For those domains, full MSAs (except for PF00005; see Supplementary Table S2) and representative structures were obtained from Pfam (Supplementary Table S2).
Dataset II comprised 2330 pairs (formed by 453 distinctive Pfam proteins/domains).
These were selected from the Negatome 2.0 PDB-stringent dataset of 4161 pairs upon removing all pairs that involved multidomain proteins.
The three panels in Supplementary Figure S1 display the histograms for (a) the number of columns, (b) the number of rows and (c) the average sequence identities between all pairs of rows, for the MSAs corresponding to Dataset II.
Note that Dataset II contains two orders of magnitude larger data (2330 versus 16 pairs of proteins) compared with Dataset I, but the corresponding MSAs contained fewer sequences (rows) and smaller 1930 W.Mao et al.; Tokuriki and Tawfik, 2009b).
; Liu etal., 2010 ; Marks etal., 2011.,.
Direct Coupling Analysis Direct Information ' 3).
2,330 vs to proteins (columns).
The respective averages for the two sets were <N>I495 and <N>II230, and <m>I1681 and <m>II334.
We used Dataset I for a detailed analysis and Dataset II for further validation of major results.
The following filters were applied in refining the MSAs: All sequences having less than 80% row occupancy (sequences having >20% gaps) were removed using ProDy (Bakan et al., 2014).
The refined MSAs for individual proteins in Dataset I were concatenated whenever a protein was composed of more than one domain.
Likewise, for each protein family pair, we concatenated the sequences from the same species to form a combined MSA.
The sequence with the lowest average sequence identity with respect to all others in a given MSA was removed until the average sequence identity was above 25%.
No upper sequence identity threshold was adopted for Dataset I, as the average sequence identities (last column in Supplementary Table S1) varied between 31% and 58%; and even in the case of the MSA containing the highest proportion of similar sequences, those pairs with more than 85% sequence identity were 3 standard deviations apart from the mean.
Dataset II showed a broader distribution, depicted in Supplementary Figure S1 (c).
In this case, the pairs sharing more than or equal to 99% sequence identity amounted to 0.75% of the data, yielding on the average two to three such pairs per MSA.
The effect of this small subset of highly similar paralogs can thus be expected to be negligible.
We also confirmed the above by repeating calculations for Dataset II with 95% upper sequence identity cutoff (data not shown).
The results showed that the effect of this small subset of highly similar paralogs was negligibly small.
Finally, columns whose occupancy was lower than 90% (positions with >10% gaps) and those fully conserved were removed for coevolution analysis.
2.2 Methods for sequence coevolution analysis The methods we used in our comparative study are MI (Gloor et al., 2005), MIp (Dunn et al., 2008), OMES (Kass and Horovitz, 2002), SCA (Halabi et al., 2009; Lockless and Ranganathan, 1999), PSICOV (Jones et Al., 2012) and DI (Morcos et al., 2011; Weigt et al., 2009).
A summary of the methods included in our comparative study is presented in SI.
Details may be found in the original studies.
In each case, we evaluated the NN sequence covariance matrix; the off-diagonal elements of which represent the degree of coevolution between pairs of amino acids.
MI, MIp, OMES and SCA matrices were calculated using the Evol module of ProDy (Bakan et al., 2014), PSICOV by the code listed online (Jones et al., 2012) and DI by the code provided by Morcos et al.(2011).
2.3 Shuffling algorithm The shuffling algorithm introduced earlier (Noivirt et al., 2005) was adopted here.
Accordingly, for a given MSA of m sequences and N residues/columns, we shuffle the m elements within each column (e.g.column k) randomly while the other columns are kept unchanged.
A new correlation matrix (MI, MIp or OMES) is calculated for each shuffling procedure.
This process is repeated P10 000 times for each column (1kN); and because each position is evaluated twice on either position shuffling, we obtain a total of 20 000 shuffled results for each pair.
The new random correlation value is compared with its original counterpart and we assign a P-value.
For instance, if we observe a shuffled value more than or equal to original value in 200 times out of 20 000 iterations for a given pair, the P-value for the corresponding (original) covariance value is assigned as 200/20 0000.01.
We set the P-value significance threshold to 0.005, i.e.only those pairs with P-values<0.005 were considered to be statistically significant.
The newly generated covariance matrices are designated as MI(S), MIp(S) or OMES(S).
The shuffling algorithm can be practically implemented for these three methods among the six listed above.
This is because DI and PSICOV require the inversion of the entire C at each iterative step, and repeating this task approximately 104 times for each column is prohibitively expensive.
Likewise, SCA does not lend itself to efficient iterative re-evaluation, and hence was not subjected to shuffling refinement.
3 Results 3.1 Rationale We assessed the performance of MI, MI(S), MIp, MIp(S), OMES, OMES(S), SCA, PSICOV and DI based on two criteria: exclusion of intermolecular FPs, and ability to capture intramolecular contactmaking pairs (TPs).
The former criterion is assessed by examining the protein pairs that are known to be non-interacting (Datasets I and II; see Supplementary Table S1).
We construct MSAs by juxtaposing the sequences of such pairs of proteins, e.g.A and B, each row corresponding to a given species/organism.
The resulting covariance matrix is composed of four blocks/sub-matrices, two describing the intramolecular (AA and BB) correlations, and two, off-diagonal, associated with intermolecular (AB or BA) correlations (Fig.1a).
In principle, the latter two sub-matrices should not contain any signals as they are for non-interacting proteins, or the observed signals are FPs.
The most accurate method is, therefore, the one where these FPs are negligible if not totally eliminated.
The second criterion, referred to as accurate detection of intramolecular contacts is assessed by examining if the coevolving pairs Fig.1.
Two criteria for assessing the performance of different methods: (I) exclusion of intermolecular FPs and (II) detection of residue pairs that make intramolecular contacts.
(a) and (b) The MIp and MIp(S) matrices obtained for a pair of proteins [in this case, porphobilinogen deaminase (protein A) and ribosomal 50S L1 protein (protein B)] (Supplementary Table S1).
Residue pairs yielding the top-ranking 1% signals are displayed by dots.
Shuffling reduces the percentage of intermolecular signals (FPs) from 9.57 to 6.69%.
(c) and (d) The individual proteins are separately analyzed and the physical distance between coevolving pairs is evaluated by examining the corresponding structure in the PDB Methods for detecting sequence coevolution 1931 &equals;  &equals;  &equals; 1,681 &equals; ,Sequence Coevolution Analysis.... a. ,.
x ); (.,  &equals; , &leq;  &leq; , `` '' o p >&equals; , p , &equals; p p < &sim;make inter-residue contacts in the 3D structure of the protein.
Two residues are considered to make 3D contacts if at least one pair of atoms (belonging to the respective residues) is separated by a distance smaller than 8A.
Previous detailed examination of the coordination geometry of non-bonded residues in PDB structures has shown that this distance range includes all pairs within a first coordination shell (Bahar and Jernigan, 1996).
A threshold of 8.0 A (for CaCa pairs) has been adopted in similar studies for defining inter-residue contacts (Burger and van Nimwegen, 2010; Kamisetty et al., 2013).
The occurrence of a 3D contact is strong evidence for the biological or physical significance of the detected covariation.
Methods that identify a larger number of such pairs (among the top-ranking coevolving pairs) are deemed to perform better.
3.2 Illustrations for selected pairs Figure 1 illustrates the above two criteria for porphobilinogen deaminase and ribosomal 50S L1 protein (pair 11 in Supplementary Table S1), designated as proteins A and B, analyzed by MIp(S).
Panel (a) displays the MI map calculated after subtracting the APC, MIp.
For clarity, only the strongest 1% signals are shown by dots.
Among them, 90.43% lie in the lower-left and upper-right diagonal blocks, corresponding to the respective intramolecular signals within A and within B (AA and BB groups); and 9.57% lie in the other two blocks corresponding to intermolecular correlations (AB or BA; the matrix is symmetric).
The latter subset constitutes the FPs in view of the lack of known physical interaction between these two proteins.
Panel (b) shows that the application of shuffling algorithm to MIp to generate MIp(S) reduces the percentage of FPs to 6.69%.
Panels (c) and (d) illustrate the screening of the results for individual proteins against their PDB structures to identify the fraction of intramolecular signals that correspond to 3D contact-making pairs.
In this example, 26.37% of residue pairs, shown by the orange dots, make physical (atomatom) contacts.
Figure 2 illustrates the analysis of the intramolecular signals obtained for c-glutamyl phosphate reductase and pantetheine phosphate adenylyl transferase (pair 2 in Supplementary Table S1).
Panel a compares the relative ability of the nine different methods to detect contact-making pairs of residues.
Results are displayed for a range of signal strengths (or covariance scores), from top-ranking 0.120%.
Clearly, the fraction of accurately predicted contacts drops as larger subsets are considered, but the results also show a strong dependency on the selected method.
SCA and MI show the weakest performance: contact-making residue pairs amount to less than one-third of the identified pairs in either case, even when the strongest 0.1% signals are considered.
On the other hand, at the same signal strength, a large majority (>85%) of residue pairs predicted by PSICOV make contacts in the 3D structures.
PSICOV is closely followed by DI.
Of note is the high performance of MIp(S) in the range 520%, indicating little decrease with coverage compared with other methods.
The improvement in MIp upon implementation of the shuffling algorithm is remarkable; whereas MI and OMES hardly change upon shuffling.
Panels (b) and (c) display the locations of residue pairs that are accurately detected by at least seven methods within the respective proteins.
3.3 Results for the complete Dataset I Results obtained for the complete Dataset I are presented in Figure 3 and SI, Supplementary Figure S2.
First, we compare the ability of the nine methods [SCA, MI, OMES, MIp, PSICOV and DI (solid colored curves) and MI(S), OMES (S) and MIp(S) (dashed colored curves)] to detect coevolving pairs that make intramolecular contacts (Fig.3a and Supplementary Fig.S2b).
To this aim, we examined the location of the top-ranking signals in the PDB structure of each investigated protein (Supplementary Table S2) and evaluated the percentage of 3D-contact-forming pairs (see Supplementary Fig.S3).
The results are shown (z-axis) for increasingly larger subsets of predictions, starting from the strongest 0.1% coevolution signals, up to 20%.
Results for individual proteins are displayed as a bundle of gray dashed curves.
The averages over all proteins yielded the colored curves as a function of signal strength.
A broad range of performance is observed.
PSICOV and DI exhibit the highest performance; 8788% of coevolving pairs predicted by these two methods that rank in the top 0.1% subset make 3D contacts.
These are TPs whose coevolutionary behaviour may be rationalized by their physical interactions.
The performance of these two methods drops with coverage, e.g.to 5254% when the top 1% predictions are considered.
In contrast, MI, MI(S) and SCA exhibit the poorest performance; the corresponding fractions of TPs are 3034% and 1920% for the respective subsets.
The lower panel in Figure 3b provides a clear comparison of these results obtained by DI, PSICOV, SCA and MI(S), OMES(S) and MIp(S) averaged over all proteins and their standard deviations (see also Supplementary Fig.S2b).
The two best performing methods, DI and PSICOV, are followed by MIp(S), and then OMES, in the range less than 1%.
Notably, MIp(S) outperforms all others when a higher fraction of predictions (e.g.top 20%) is considered, as will be further discussed below.
Most methods were found to successfully eliminate intermolecular FPs.
The upper panel in Figure 3b shows that the percentage of intermolecular signals (FPs) is approximately 530% (or that of intramolecular signals 7095%) in general, with a small dependence on the method and overall decrease with increasing coverage (see also SI, Supplementary Fig.S2a).
PSICOV and DI practically have no FPs among the top 0.5% coevolving pairs; and MIp, MIp(S), OMES and OMES(S) show equally good performance.
In all these six cases, the fraction of FPs (intermolecular signals) remains smaller Fig.2.
Comparison of the performance of different methods.
The ability of the methods to detect residue pairs that make 3D contacts is illustrated for the pair 2 in Supplementary Table S1.
Panel (a) displays the percentage of TPs among intramolecular predictions (based on subsets of different size, from top 0.1% to top 20%), TPs being defined as residue pairs that make contacts in the 3D structure.
Panels (b) and (c) show the residue pairs (blue stick representation) within c-glutamyl phosphate reductase (top) and pantetheine phosphate adenylyl transferase (bottom) predicted among the top 1% signals by all nine methods (red lines), or eight methods (orange lines) or seven methods (yellow lines) 1932 W.Mao et al.&amp; Selected Pairsmutual information Figure 1.
Two criteria for assessing the performance of different methods: (I) exclusion of intermolecular FPs and (II) detection of residue pairs that make intramolecular contacts.
(a) and (b) Shown are the MIp and MIp(S) matrices obtained for a pair of proteins (in this case, porphobilinogen deaminase (protein A) and ribosomal 50S L1 protein (protein B) (Table S1).
Residue pairs yielding the top-ranking 1&percnt; signals are displayed by dots.
Shuffling reduces the percentage of intermolecular signals (FPs) from 9.57 to 6.69&percnt;.
(c) and (d) The individual proteins are separately analyzed and the physical distance between coevolving pairs is evaluated by examining the corresponding structure in the PDB.
to C 1/3 toco-evolutionary behavior Figurethan 8% among the top-ranking 1% signals; whereas in the case of MI(S) and SCA, the same fraction increases to 2025%.
Notably, the performance of MIp(S) shows the least deterioration with increasing coverage, as already noted in the above illustrative case.
As an additional test, we examined the ability of these methods to predict not only contact-making pairs, but those pairs that are not nearest neighbours along the sequence.
These will be termed non-local contacts (they are localized in space, but not along the sequence).
The horizontal lines on the bars in Figure 3b (lower panel) indicate the proportions of contacts of different orders, starting from order 1 (bottom), then orders 2, 3 and finally more than or equal to 4 (top portion) which are viewed as non-local.
A contact of order k means a contact made between residues i and ik.
In principle, it is conceivable that some of the neighbouring residues coevolve, compensating for some properties on a local scale.
More interesting are the non-local couplings, which can serve as constraints for structure prediction.
PSICOV yields the highest proportion of non-local contacts, followed by DI, again demonstrating the superior performance of these two methods.
3.4 Validation with Dataset II As a further validation, we repeated the same analysis with Dataset II of 2330 protein pairs extracted from the Negatome database.
Supplementary Figure S4 shows that the results obtained for Dataset II closely reproduced those obtained with Dataset I.
The major difference was the larger variances in this case (shown by error bars), which resulted from the broader distribution of chain lengths (N) as well as the relatively small size of some of the MSAs included in Dataset II (see Supplementary Fig.S1).
Note that the outputs here correspond to the MI, MIp and OMES in the absence of shuffling (which does not lend itself to high-throughput evaluation of thousands of MSAs).
This mainly affects the performance of MIp at around 20% as can be seen in the figure.
This further set of computations confirmed the robustness of the results presented in Figure 3, and firmly established the significantly higher ability of DI and PSICOV to detect residue pairs making 3D contacts.
3.5 Dependence on MSA size and efficacy of shuffling algorithm The above computations indicated an improved performance upon implementation of shuffling algorithms in the case of MIp, while the effects on MI and OMES were negligible on average.
However, by looking closely at individual cases, we found that shuffling may be very effective for particular pairs (e.g.pairs 1 and 2) whose MSAs comprise fewer sequences.
We speculated that the effectiveness of the shuffling algorithm correlates with the size of the MSA; those MSA containing fewer sequences benefiting more from this type of refinement.
A systematic examination indeed showed that the level of improvement upon shuffling strongly depends on the size m of the MSAs.
Figure 4 demonstrates the above observation.
In order to obtain those results, we generated a series of MSAs with varying sizes in the range [50m2000] by choosing random subsets of concatenated sequences from the MSAs generated for Dataset I, as summarized in SI, Supplementary Table S4; and computations were performed for these test MSAs, using the three methods that lend themselves to shuffling, MI, MIp and OMES.
As can be clearly seen in Figure 4, upon implementation of the shuffling algorithm, all methods exhibit some improvement in their ability to eliminate intermolecular FPs (panels ac) and their ability to detect pairs supported by physical interactions in the 3D structures (panels df).
The improvements are more pronounced when the input MSAs are smaller.
Furthermore, shuffling helps when larger subsets of predictions (e.g.top 20%) are considered.
In summary, shuffling emerges as a useful tool in the absence of a sufficiently large number of sequences that can be used in the MSA, and/or for alleviating the decrease in accuracy with increasing coverage.
As a further assessment, we repeated the calculations for all nine methods and examined their ability to detect coevolving pairs that make contacts in the 3D structure as a function of MSA size.
The results, based on the strongest 1% coevolution signals are presented in Figure 5.
Their counterparts for the 0.1% and 10% subsets are presented in the respective panels a and b of Supplementary Figure S5.
Notably, if the MSA size is of the order of a few hundreds of sequences (as opposed to a few thousands), MIp(S) emerges as the Fig.3.
Comparative analysis of the performance of different methods.
(a) Ability to detect residue pairs that make contacts in the 3D structure.
The fraction of contact-making pairs is plotted for increasingly larger subsets of pairs predicted to be coevolving (between the strongest 0.1% and 20% signals obtained by the indicated methods).
DI and PSICOV outperform all other methods.
(b) Results from two tests: elimination of intermolecular signals for non-interacting pairs (top) and detection of intramolecular contact-making pairs (bottom) displayed for six methods as a function of coverage.
See more details in SI, Supplementary Figure S2.
The bars in the lower plot are broken down into four pieces corresponding to contacts of various orders (1, 2, 3, and 4, starting from bottom) permitting us to distinguish between local (near-neighbours along the sequence) and non-local (spatially close but sequentially distant) contacts.
Top-ranking predictions made by PSICOV contain the largest proportion of non-local contacts Methods for detecting sequence coevolution 1933 nonlocal &geq; nonlocal.
&plus; nonlocal Figure 2.
Comparison of the performance of different methods.
The ability of the methods to detect residue pairs that make 3D-contacts is illustrated for the pair 2 in Table S1.
Panel a displays the percentage of TPs among intramolecular predictions (based on subsets of different size, from top 0.1&percnt; to top 20&percnt;), TPs being defined as residue pairs that make contacts in the 3D structure.
Panels b and c show the residue pairs (blue stick representation) within &gamma;-glutamyl phosphate reductase (top) and pantetheine phosphate adenylyl transferase (bottom) predicted among the top 1&percnt; signals by all nine methods (red lines), or 8 methods (orange lines) or seven methods (yellow lines).
the 2,330the Shuffling Algorithm &leq;  &leq; Figure 3.
Comparative analysis of the performance of different methods.
(a) Ability to detect residue pairs that make contacts in the 3D structure.
The fraction of contact-making pairs is plotted for increasingly larger subsets of pairs predicted to be coevolving (between the strongest 0.1 and 20&percnt; signals obtained by the indicated methods).
DI and PSICOV outperform all other methods (b) Results from two tests: elimination of intermolecular signals for non-interacting pairs (top) and detection of intramolecular contact-making pairs (bottom) displayed for 6 methods as a function of coverage.
See more details in SI Figure S2.
The bars in the lower plot are broken down into 4 pieces corresponding to contacts of various orders (1, 2, 3 and &geq; 4, starting from bottom) permitting us to distinguish between local (near-neighbors along the sequence) and nonlocal (spatially close but sequentially distant) contacts.
Top-ranking predictions made by PSICOV contain the largest proportion of non-local contacts.
Figure 4.
Effectiveness of shuffling algorithm as a function of MSA size and coverage the labels in the figure are not clear.
The performance of three methods before (lower surface) and after (upper surface) implementation of shuffling algorithm is compared, with respect to their ability to eliminate intermolecular FPs (a-c) and to identify evolutionarily correlated pairs that make direct contacts in the 3D structure (d-f).
Shuffling algorithm partially compensates for the loss in accuracy that originates from the use of smaller size MSAs (containing for example a few hundreds of sequences) as well as that occurring with increasing coveragemethod of choice: it allows for the detection of the highest proportion of contact-making pairs.
This distinctive feature is particularly striking when the MSA contains 50100 sequences (Figure 5), or when a larger coverage (of potentially contact-making residue) is of interest (see Supplementary Fig.S5b).
3.6 Development and validation of a hybrid method The above analysis exposes the different strengths of various methods in detecting of contact-making residue pairs, in discriminating intermolecular FPs and in dealing with small MSAs or providing more coverage at a relatively small loss in accuracy.
Of interest is to examine the consistency of the predictions, i.e.to see whether the different methods are detecting different subsets of correlated pairs.
Such an assessment of the overlap between predictions would also help in designing a hybrid method that takes advantage of the strengths of different methods.
To this aim, we calculated the average correlation coefficients, s(a, b), between the top 20% predictions from each pair of methods (a, b).
The results are shown in Figure 6.
This analysis reveals that the DI and PSICOV yield consistent results with correlation coefficient s(DI, PSICOV)0.67, which may be attributed to the fact that both methods use a global optimization scheme that retrieves direct contacts.
Likewise, MI and OMES (and their shuffled versions) show some overlap.
MI and OMES are based on different formulations, but they both measure the observed departure from the expected results, which may explain their correlation of s(MI, OMES)0.48.
MIp shows moderate correlations with all methods (except OMES), which vary between s(MIp, MI)0.39 and s(MIp, PSICOV)0.51.
In contrast, SCA yields weak correlations (<0.26) with all methods, except with MIp (s0.44).
The above analysis suggests that one might combine methods that exhibit different strengths to devise hybrid methods that may potentially outperform the individual methods.
The construction of a model based on two methods has been successfully accomplished by Eloffson and coworkers (Skwark et al., 2013), by combining plmDCA and PSICOV to build the PconsC method.
Recent application of PConsC (Michel et al., 2014) was found to improve protein models by improving contact predictions.
Towards this goal, we focused first on PSICOV and DI as they exhibit superior performance (see Fig.3 and Supplementary Fig.S2).
We designed a combined nave Bayes classifier utilizing these two methods (Fig.7).
162 Pfam families were utilized as training set, the properties of which are detailed in SI, Supplementary Table S3, along with the criteria for their selection from the entire dataset of Pfam families.
PSICOV and DI matrices were calculated for all the 162 families, and each residue pair was classified as positive () (within interatomic distance range of 8A in the 3D structure) or negative () (if otherwise).
The density distributions of the positive and negative classes were modeled by kernel density estimation based on PSICOV and DI values (Fig.7b).
The kernel width was determined by Silvermans rule (Silverman, 1986).
For a given Fig.4.
Effectiveness of shuffling algorithm as a function of MSA size and coverage.
The performance of three methods before (lower surface) and after (upper surface) implementation of shuffling algorithm is compared, with respect to their ability to eliminate intermolecular FPs (ac) and to identify evolutionarily correlated pairs that make direct contacts in the 3D structure (df).
Shuffling algorithm partially compensates for the loss in accuracy that originates from the use of smaller size MSAs (containing for example a few hundreds of sequences) as well as that occurring with increasing coverage Fig.5.
Dependence of the performance of different methods on the size of the MSA.
The abscissa shows the number m of sequences included in the MSAs.
The ordinate shows the percentage of 3D contact-making pairs among the most strongly coevolving (top 1%) pairs of residues predicted by different methods.
PSICOV and DI show a strong dependence on m. MIp(S) is distinguished by its superior performance when the number of sequences is as low as 50.
See also the results for top 0.1% and 10% covarying residues in SI, Supplementary Figure S5.
The latter case further exposes the distinctive effectiveness of MIp(S) for identifying 3D contact-making pairs Fig.6.
Correlation between the predictions of different methods.
The entries represent the correlation coefficients calculated for the top 20% predictions made by the different methods, averaged over all proteins 1934 W.Mao et al.Figure 5 Dependence of the performance of different methods on the size of the MSA.
The abscissa shows the number m of sequences included in the MSAs.
The ordinate shows the percentage of 3D-contact making pairs among the most strongly coevolving (top1&percnt;) pairs of residues predicted by different methods.
PSICOV and DI show a strong dependence on m. MIp(S) is distinguished by its superior performance when the number of sequences is as low as 50.
See also the results for top 0.1&percnt; and 10&percnt; co-varying residues in SI Figure S5.
The latter case further exposes the distinctive effectiveness of MIp(S) for identifying 3D contact-making pairs.
V Hybrid Method ) &equals; ) &equals; ) &equals; ) &equals;  &equals; uresthe combination of DI and PSICOV scores, the combined method provides the posterior probability for positive as PjDI;PSICOV PDI;PSICOVjP PDI;PSICOVjPPDI;PSICOVjP (1) Application of this classifier to our dataset showed that an improvement, albeit incremental (e.g.4.12% with respect to PSICOV for the subset of top 0.5% predictions), can be achieved over either method in so far as the prediction of contact-making pairs is concerned (Fig.7d).
We note that for PconsC, on average nearly three quarters of the top N predictions seemed to be correct (Michel et al., 2014).
This means, for a protein of 200 residues for example, the top 200 predictions, i.e.the top 1% (i.e.f0.1% using fN(N1)/2N for N200), and this fraction will be N-dependent.
The performance of 75% of PconsC is thus achieved in our case if f<0.4%, which would correspond to a protein length of N>500.
In the case of smaller proteins, e.g.N300, the fraction of contact-making residues drops to 65%.
The hybrid method at that level of coverage shows an improvement of about 24% above either of the individual (DI and PSICOV) methods.
We also checked whether the combined method can also eliminate intermolecular FPs as efficiently as PSICOV (which showed the best performance), and although the method was not trained on these properties, a performance comparable to that of PSICOV was obtained (Fig.7c).
Finally, we examined whether one might obtain more accurate results upon selecting the intersection of the best methods.
Examination of the intersection of PSICOV and DI did not provide an improvement over the individual methods when the same level of coverage was aimed, i.e.the top-ranking 1000 overlapping results from DI and PSICOV picked up entries ranking lower in the output list, which contained negative results.
On the other hand, given the consistency of MIp with a broad range of methods, we examined the consensus predictions (or intersection) from MIp, DI and PSICOV.
At the same level of coverage, the intersection led to a considerable improvement (e.g.6.5% compared with DI, at top 2% signals) in eliminating intermolecular FPs, as depicted by the green curve in Figure 7c, but not in identifying 3D contact-making pairs (Fig.7d).
4 Conclusion The above comparative analysis led to the following conclusions summarized below in the context of three groups of outputs/regimes, colored light green, yellow and pink in Supplementary Figs.
S2 and S7: strong coevolution signals (ranked in the top 0.5% subset), intermediate signals (0.55%) and relatively weak signals (520%).
First, among all studied methods, PSICOV and DI yielded the best performance in the strong signal regime.
Both methods were successful in accurately detecting coevolving pairs of residues that Fig.7.
Development of hybrid methods.
(a) Assessment of prior probability of 3D contact, P(), by a regression analysis of a training set of 162 structurally known protein sequences.
(b) Density distributions of positive and negative signals, P(DI, PSICOVj) and P(DI, PSICOVj) (see Equation 1), modelled by kernel density estimation.
(c and d) Comparative performance of the individual methods DI (gray) and PSICOV (red), and the combined nave Bayes classifier method (Equation 1) (black), based on the fraction of intramolecular signals (c) and fraction of 3D contact-making pairs (d).
The predictions based on the intersection of MIp, DI and PSICOV are shown by the green curve Methods for detecting sequence coevolution 1935 ure Figure 6.
Correlation between the predictions of different methods.
The entries represent the correlation coefficients calculated for the top 20&percnt; predictions made by the different methods, averaged over all proteins.
&equals;  &equals;  &equals;  <  >  &equals; ure 1,000 to ure 3 to Figure 7.
Development of hybrid methods.
(a) Assessment of prior probability of 3D contact, P(&plus;), by a regression analysis of a training set of 162 structurally known protein sequences.
(b) Density distributions of positive and negative signals, P(DI,PSICOV&boxv;&plus;) and P(DI,PSICOV&boxv;-) (see Eq 20), modeled by kernel density estimation.
(c-d) Comparative performance of the individual methods DI (gray) and PSICOV (red), and the combined na&iuml;ve Bayes classifier method (Eq 20) (black), based on the fraction of intramolecular signals (c) and fraction of 3D contact making pairs (d).
The predictions based on the intersection of MIp, DI and PSICOV are shown by the green curve.
make contacts in the 3D structure (Fig.3a and b and Supplementary Figs.
S2b and S4) including non-local contacts, or in eliminating the intermolecular FPs (Fig.3b and Supplementary Fig.S2a).
Their performance was particularly impressive when the strongest coevolutionary signals (top 0.1%) were considered.
For a protein of N300 residues, 0.1% means 0.001N(N1)/245 pairs.
Thirty-nine of them predicted by these methods were, on average, observed to form inter-residue contacts in the structure; likewise, among the top 0.5% signals, 157 pairs (out of 224) would make contacts.
The predictions thus help not only in elucidating evolutionarily relationships, but also in assisting in structure prediction.
These methods are therefore uniquely useful in cases where no suitable template structures are available.
DI indeed showed remarkable success in predicting the structures of membrane proteins (Hopf et al., 2012).
Second, in the intermediate regime, while the proportion of contacts among coevolving pairs predicted by PSICOV and DI remains high, we note that the discriminatory ability of OMES and MIp (and their shuffled versions) between intermolecular and intramolecular interactions start to pick up and outperform that of DI.
Notably, MIp(S) exhibits the highest performance in the relatively weak (but high coverage) regime, both in terms of elimination of FPs and identification of 3D contact-making TPs.
This superior performance of MIp in situations where DI and PSICOV start to underperform is noteworthy.
Two such situations are: (i) the search for a large number of predictions (or higher coverage) albeit at lower accuracy, and (ii) the search for coevolving pairs that potentially make 3D contacts, in the absence of a sufficient number of sequences (see Figs.
5 and Supplementary Fig.S5).
MIp(S) emerges as the method of choice in those situations.
For example, if one is interested in exploring coevolutionary patterns within a small (sub)family of 50200 sequences, one-third of predictions made by MIp(S) would be, on the average, making contacts in the 3D structure among the top 10% signals; see Supplementary Fig.S5b).
This subset of signals contains 4500 pairs for N300, of which 1500 would be physically interacting.
This is a large majority of native contacts, based on inter-residue coordination number of z12 within 10 A.
Third, the study highlights how the size m of MSA, a parameter known to be an important determinant of the statistical significance of results, affects different methods.
It is well known that larger MSAs usually give better results, and some methods have specified lower bounds for m: 100 sequences for SCA, 250 for sensitive results from DI, and 1000 for full DI performance (Morcos et al., 2011).
PSICOV doesnt specify a lower bound, but there is a clear correlation between performance and MSA size (Jones et al., 2012).
However, the present study further shows that the deficiency arising from small MSAs can be partially offset by the shuffling algorithm (Fig.4).
Shuffled MIp(S) in particular emerges as a better choice than DI and PSICOV when dealing with small MSAs.
Generally speaking, we need more than m250 sequences to justify the use of the computationally expensive DI and PSICOV methods; otherwise, MIp might be preferred together with a shuffling algorithm (Fig.5 and Supplementary Fig.S5).
On a practical side, both PSICOV and DI involve the inversion of a covariance matrix and/or global optimization algorithms which may take hours, even days, depending on the size of the MSA.
Specifically, PSICOV and DI need each about 2.5 GB memories to analyse a 400-residue MSA.
The memory requirement increases quadratically with sequence size, and this O(N2) dependence may become prohibitively expensive for large proteins.
The computing time for inverting the DI covariance matrix scales between N2.373 (Williams, 2012) and N3 depending on the algorithm and parameters.
MI, MIp and OMES, on the other hand, are very fast.
As such, they lend themselves to high-throughput analysis, thus allowing for statistical inferences about sequence-structuredynamics-function relations (see e.g.Liu and Bahar, 2012).
Even though shuffling is time-consuming, it needs very small memory and we could speed up the calculation by adjusting the number k of shuffles because the computing time scales linearly with k, as O(kN2m).
So, vis-a-vis the tradeoff between accuracy and efficiency, MIp(S) could serve as an optimal approach, especially for MSAs of large proteins containing a small number of sequences.
Finally, our analysis permitted us to develop a hybrid method that takes advantage of the strengths of DI and PSICOV.
The improvement in performance is incremental due to an already high overlap of 0.68 between the predictions of DI and PSICOV.
Yet, one may advantageously adopt this hybrid method to maximize the fraction of contact-making predictions, especially in the intermediate coverage regime.
Another useful recipe for case studies is to select the intersection of DI, PSICOV and MIp, which appears to be particularly useful for eliminating FPs.
All methods are accessible via the Evol extension of ProDy (Bakan et al., 2014).
Acknowledgements Scholarship to W.M.
awarded by China Scholarship Council is gratefully acknowledged.
The authors benefited from useful discussions with Drs Ahmet Bakan and Lila Gierasch.
Funding Funding from the National Institutes of Health grants (5P41 GM103712 and 5R01 GM099738 to I.B.)
is gratefully acknowledged.
Conflict of Interest: none declared.
ABSTRACT Motivation: G-quadruplexes are stable four-stranded guanine-rich structures that can form in DNA and RNA.
They are an important component of human telomeres and play a role in the regulation of transcription and translation.
The biological significance of a G-quadruplex is crucially linked with its thermodynamic stability.
Hence the prediction of G-quadruplex stability is of vital interest.
Results: In this article, we present a novel Bayesian prediction framework based on Gaussian process regression to determine the thermodynamic stability of previously unmeasured G-quadruplexes from the sequence information alone.
We benchmark our approach on a large G-quadruplex dataset and compare our method to alternative approaches.
Furthermore, we propose an active learning procedure which can be used to iteratively acquire data in an optimal fashion.
Lastly, we demonstrate the usefulness of our procedure on a genome-wide study of quadruplexes in the human genome.
Availability: A data table with the training sequences is available as supplementary material.
Source code is available online at http://www.inference.phy.cam.ac.uk/os252/projects/quadruplexes Contact: os252@cam.ac.uk; jlh29@cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Understanding biological sequences and predicting the functional elements they determine are widely studied themes in computational biology.
Examples of well-established problems are gene finding and the prediction of protein structure from its amino acid sequence.
Computational methods addressing such challenges helped to gain insights into interesting biological phenomenon.
However, other information encoded in the DNA sequence remains to be explored.
Recently, it has been found that particular G-rich DNA (and RNA) sequences are capable of forming stable four-stranded structures known as G-quadruplexes (Burge et al., 2006; Huppert, 2008; Neidle and Balasubramanian, 2006).
G-quadruplexes have been shown to be relevant in a number of biological processes (Patel et al., 2007).
They are an important component of human telomeres (Oganesian and Bryan, 2007), and play a role in regulation of transcription (Qin and Hurley, 2008; Siddiqui-Jain et al., 2002) as well as translation (Kumari et al., 2007).
Structurally, intramolecular G-quadruplexes consist of a square arrangement of four guanines (a tetrad) in a planar hydrogen bonded form.
At the centre of the tetrads is a monovalent cation, e.g.K+, that further stabilizes the structure.
The core guanines are linked by three nucleic acid To whom correspondence should be addressed.
N1 N N N O N2 H H H N N N N7 O 6 N H H H N N N N O N H H H N N N N O N H H H K+ Loop 1 Loop 3 Loop 2(a) (b) Fig.1.
(a) Hydrogen bond pattern in a G-tetrad.
A monovalent cation occupies the central position.
(b) Schematic diagram of an intramolecular G-quadruplex, with three G-stacks.
sequences (loops) of varying composition and topology.
Figure 1 shows a schematic picture of a G-quadruplex together with the hydrogen bond pattern.
An obvious challenge is to predict which sequences will form these G-quadruplexes.
A necessary condition for G-quadruplex formation is the presence of core guanines and loop sequences.
These basic requirements can be used to identify putative G-quadruplexes using a simple pattern-based rule, matching sequences of the form d(GNG N1NL  L1 GNG N1NL  L2 GNG N1NL  L3 GNG ), (1) where GNG are the guanine cores that can occur with different numbers of G-stacks, NG =2,3,4.
The symbol N denotes any nucleotide.
The loop sequences (L1, L2, L3) have varying length, where NL =7 is a typical choice for the maximum length.
For very long loops, G-quadruplexes are unlikely to form as their stability decays with the total sequence length (Bugaut and Balasubramanian, 2008; Hazel et al., 2004).
Similar rules have been widely used in the literature (e.g.Huppert and Balasubramanian, 2005) and demonstrated to work well in practice.
However, they are not exhaustive, for example some structures with much longer loops can be formed (Bourdoncle et al., 2006).
The most important limitation of pattern-based sequence rules is that they do not predict the thermodynamic stability, a key property of the G-quadruplex.
In order for the G-quadruplex to have a biologically meaningful role, it needs to be stable enough to form a structure at body temperature.
Furthermore, it has been speculated that G-quadruplexes that are metastable at body temperature carry the most significant role, as their influence on transcriptional processes can be active or inactive depending on other factors.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i375 i374i382 Predicting the stability of G-quadruplexes This motivates the problem of predicting the G-quadruplex melting temperature as a proxy for stability from its sequence alone.
In contrast to simpler systems such as DNA duplexes (SantaLucia, 1998), sequence differences in G-quadruplexes affect thermodynamic stability in a non-linear fashion, hence rendering this prediction task challenging.
The nearest neighbour approaches that have been so successful for predicting duplex stability, such as from (SantaLucia, 1998), are not applicable to folded-back structures such as G-quadruplexes.
It is relatively straightforward to experimentally determine the thermodynamic stability for specific G-quadruplexes using ultraviolet (UV) melting (Mergny et al., 1998).
In a UV melting experiment, the absorbance of a guanine-rich oligonucleotide is recorded as a function of the temperature.
This allows the melting temperature of the G-quadruplex to be deduced.
However, no one has managed to extrapolate generalized energy parameters to each component of the structure.
Instead, empirical rules and intuition have been built up based on small-scale studies with a few dozen G-quadruplex sequences.
Various details have been discovered, establishing the importance, in particular, of the loops that join the core guanines together (Bugaut and Balasubramanian, 2008; Hazel et al., 2004; Lane et al., 2008).
Although it is still in the early days of our understanding of G-quadruplex stability, it is clear that both loop length and loop composition are important.
The stability of G-quadruplexes is also strongly influenced by the surrounding solution providing the monovalent cation that sits inside the structure, typically between the G-tetrad stacks (Fig.1).
For instance, K+ is strongly favoured over Na+ or Li+ and hence leads to more stable structures.
In this work, we propose a computational prediction method for the stability of G-quadruplexes based on Gaussian process (GP) regression.
This includes a special purpose covariance function that allows sequence features potentially affecting the G-quadruplex stability to be flexibly incorporated.
The inference procedure automatically determines the relevance of sequence features and yields predictions with error bars.
Using a heavy-tailed likelihood, our model gains additional robustness with respect to outliers.
The presented framework can also handle experimental data that merely set a maximum or minimum range on the melting temperature rather than an explicit value.
This situation occurs if a structure is found to be stable at all experimentally accessible temperatures.
We demonstrate the accuracy of the prediction method on previously unseen sequences and compare it to alternative methods.
Finally, we consider an active learning procedure and apply the methodology to assess the stability of G-quadruplexes in gene promoters, comparing them to other G-quadruplexes.
2 QUADRUPLEX PREDICTIONS USING GP The prediction of G-quadruplex stability can be cast as a regression problem.
For a given training dataset with observed G-quadruplexes, D={xn,tn}Nn=1, the task is to infer a latent function f :x t, mapping from a G-quadruplex input x to its melting temperature t. The main determinant of G-quadruplex stability is the sequence information.
However, the cation nature and salt concentration also have an effect on the stability of the resulting G-quadruplex.
Our G-quadruplexes were measured at different concentration levels, which must be taken into account when making predictions.
Fig.2.
Bayesian network representation of a GP regression model.
The model relates observed independent input/output pairs {xn,tn}Nn=1.
The thick lines couple the latent function value {fn}, illustrating the smoothness assumptions introduced by the GP prior.
The parameters K and L denote hyperparameters of the kernel and likelihood, respectively.
We assume that inputs x={s,c} consist of the quadruplex sequence s and a vector of log-salt concentrations c. To apply the GP machinery, all we need is a positive definite covariance function defined (kernel) between pairs of G-quadruplex inputs.
Given a training dataset D the posterior distribution over latent function values f is P(f |HGP ,D,K,L)N (f |0,KX,X(K)) N n=1 pL(tn |fn,L), (2) where K and L are hyperparameters of the kernel (K) and the likelihood (L), respectively.
We use X to denote the set of all training inputs, X={x1,...,xN }.
The covariance matrix KX,X(K) is derived from the covariance function k(x,x |K) which specifies how function values at two inputs x,x covary.
The noise model pL(tn |fn,L) relates function values fn and the corresponding noisy observations tn.
For simplicity let us first assume standard Gaussian noise, pL(tn |fn,L)=N (tn |fn, 2) with noise level.
In this case, the predictive distribution for an unseen input x is a Gaussian again (Rasmussen and Williams, 2006), where t N (,v) and =K,X [ KX,X(K)+ 2I ]1 t (3) v =K,K,X(K) [ KX,X(K)+ 2I ]1 KX,X (K).
A Bayesian network representation of this model is shown in Figure 2.
A comprehensive introduction to GPs can be found in Rasmussen and Williams (2006).
Hyperparameters: a GP is a non-parametric model.
The only explicit parameters of the model are hyperparameters L and K, all other parameters can be integrated out and are not represented explicitly.
In a GP model the posterior probability of the hyperparameters is P(K,L|HGP,D)P(t|HGP,X,K,L)P(K,L).
(4) The log of the first term, L(K,L) (marginal likelihood), can again be computed in closed form for a Gaussian-noise model (Rasmussen and Williams, 2006).
Gradient-based optimizers can be used to then determine the most probable setting of the hyperparameters {K,L}=argmax K, L (L( K, L)+logP( K, L)).
(5) i375 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i376 i374i382 O.Stegle et al.2.1 Covariance function and hyperpriors An important design choice for using a GP is a suitable covariance function.
We use a product of covariance functions to combine kernels evaluated on the sequence s and solution concentrations c k({s,c},{s,c})=kc(c,c)ks(s,s), (6) where ks is the sequence kernel and kc the concentration kernel.
The product expresses the belief that both kernels must assign high similarities for covariation of function values.
The squared exponential concentration kernel decays exponentially with log-concentration difference kc(c,c)=A2c exp ( 1 2 i ( ci ci )2 lic 2 ) , (7) where Ac determines the typical amplitude of deviations from the mean and {ci} are log salt concentrations in mM of Na+, K+, NH+4 and Mg2+, respectively.
These are the four most common stabilizing cations for G-quadruplexes; the nature of the anion does not seem to play a role.
The lengthscale parameters lc determine the significance of the associated concentration parameters where large lengthscales correspond to less relevant parameters and short length scales to more relevant ones.
To make the lengthscale comparable, the individual input dimensions are linearly rescaled such that observed training inputs fall into a set range, here 5 to 5.
The sequence kernel, ks, is a sum of two covariance functions.
The first covariance is designed to specifically incorporate existing beliefs about characteristic sequence features that are likely to determine the stability of the G-quadruplex (Lane et al., 2008).
For flexibility, we consider G-quadruplexes that contain either two, three or four stacked tetrads and hence have the equivalent number of guanines in each run.
From the raw sequence information of a G-quadruplex with the form d(GNG N1NL  L1 GNG N1NL  L2 GNG N1NL  L3 GNG ), (8) a set of features f is extracted: Ltotal total length of the sequence (in bases) NG number of G-tetrad stacks (2, 3 or 4) L1 length of the first loop (from the 5 end, in bases) L2 length of the second loop L3 length of the third loop FA relative frequency of adenine in the sequence FC relative frequency of thymine FT relative frequency of cytosine The loop lengths determine the number of bases between the guanine stacks, N1NL.
The relative frequency of the adenine, thymine and cytosine are calculated as FA = NALtotal , where NA denotes the total number of adenines in the sequence (similarly for thymine and cytosine).
Again, a squared exponential kernel is used to combine these features kf(f,f )=A2f exp 1 2 i ( fi f i )2 lif 2 , (9) where fi denotes the i-th of the eight sequence features.
The parameters have the same interpretation as for the concentration kernel.
As before, input dimensions are rescaled and the lengthscale parameters lf was adjust the relevance of the sequence features.
The second sequence covariance function is ignorant to the biological meaning of the G-quadruplex sequence and merely treats it as character string.
We can construct a spectrum kernel (Leslie et al., 2002), that is sensitive to common k-mers present in two sequences s and s ks(s,s)=A2s k(s)k(s), (10) where k(s) maps the sequence s to a vector of counts with the number of occurances for each k-mer in s. The number of possible k-mers in a nucleotide sequence scales as 4k and hence only small orders k are practical.
In experiment,1 we consider k-mers up to an order of k =4.
Due to this low order of k, this spectrum kernel is local in that it is not sensitive to long common substrings.
In contrast, the feature kernel captures global sequence characteristics and hence both sequence kernels complement each other.
Finally, all three kernels are combined in k({s,c},{s,c})=kc(c,c) [ kf(f,f )+ks(s,s) ].
(11) The relative weights of the individual kernels are controlled by the amplitude parameters Ac,Af and As.
Hyperpriors: priors on all kernel-and likelihood-hyperparameters {K,L} are Gamma distributed.
The prior on the expected amplitudes of the squared exponential kernels Af and Ac is (2,10) with an expected value of 20.
The amplitude of the string kernel has a prior As (2,0.5).
The prior on the noise level is (2,0.5), which corresponds to an a priori uncertainty of 1C about the measured G-quadruplexes melting temperatures.
The lengthscale parameters of the feature and concentration kernels have a prior of (4,10), which favours long lengthscales (mean 40) encouraging irrelevant features to be switched off.
2.2 Robust likelihood The presentation of the GP model so far makes the simplifying assumption that observation noise is Gaussian.
For our full model, we use a heavy-tailed noise model which acknowledges that a small fraction of the data points can be extremely noisy (outliers) while others are measured with considerably more precision.
The two model (Jaynes and Bretthorst, 2003) reflects this belief, pL(tn |fn,L)=0N (tn |fn, 2)+(10)N (tn |fn, 2inf ).
(12) Here, 0 represents the probability that a datum is a regular observation and (10) is the probability of an outlier observation.
The variance of the outlier component, 2inf , is much larger than for regular observations, 2, which allows the model to effectively discard outlier observations.
When using this likelihood model, the posterior in Equation (2) is no longer computable in closed form.
To overcome this problem, we use Expectation Propagation (EP) (Minka, 2005) for approximate inference.
The goal of EP is to approximate the exact posterior with 1Source code for the mapping from strings to k-mer count vectors is taken from the Shogun toolbox (Sonnenburg et al., 2006).
i376 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i377 i374i382 Predicting the stability of G-quadruplexes a tractable alternative of the form Q(f |D,K,L)N (f |0,KX,X (K)) N n=1 gn ( fn |Cn,n,n ) , (13) where gn ( fn |Cn,n,n ) denote approximate factors.
Following Rasmussen and Williams (2006) we choose unnormalized Gaussians gn ( fn |Cn,n,n )=Cn exp( 1 22n (fn n)2 ) , (14) which results in a GP for the approximate distribution again.
The idea of EP is to iteratively update one approximate factor at a time, leaving all other factors fixed.
This is achieved by minimizing the KullbackLeibler (KL) divergence, a distance measure for distributions (Kullback and Leibler, 1951).
The update for the i-th approximate factor is performed by minimizing KL [ N (f |0,KX,X(K)) n =i gn(fn |Cn,n,n) exact factor  pL(ti |fi,L) N (f |0,KX,X(K)) n =i gn(fn |Cn,n,n)gi ( fi |Ci,i,i )  approximation ] (15) with respect to the i-th factors parameters i,i and Ci.
This is done by matching the moments between the two arguments of the KL divergence which can then be translated back into an update for factor parameters.
There is no convergence guarantee for EP but in practice it is found to converge for the likelihood model we consider (see also Kuss et al., 2005).
The fact that the mixture of Gaussian likelihood is not log-concave represents a problem as it may cause invalid EP updates, leading to a covariance matrix that is not positive definite.
We avoid this problem by damping the updates as suggested by Kuss et al.(2005) and Seeger (2005).
EP also yields an approximation of the log marginal likelihood which can be used to determine the setting of hyperparameters L(K,L) ln df N (f |0,KX,X(K)) N n=1 gn(fn) = 1 2 N n=1 ( ln2n +lnCn ) 1 2 ln KX,X(K)+ 1 2 tT ( KX,X(K)+ ) t, (16) where =diag({n}Nn=1).
In addition to the noise level (Section 2.1), the robust likelihood includes a parameter inf and the mixing proportion 0.
The parameter 0 is optimized together with the remaining hyperparameters.
The noise level of outliers, inf , is set to 10 4.
After convergence of EP, we obtain a GP as approximate posterior distribution (Equation 13).
Predictions from this model follow analogous to the standard GP (Equation 3).
A comprehensive overview on EP approximations for GP models can be found in Rasmussen and Williams (2006); robust GP regression has been previously applied to biological time series in Stegle et al.(2008).
2.3 Constrained likelihood In addition to normal observations of sequence/temperature pairs, our G-quadruplex measurements also include a small fraction of sequences where only a bound on the melting temperature was determined.
For example, if a G-quadruplex is so stable that it does not complete its melting transition within the experimentally accessible range (typically 1085C), one can only deduce that the melting temperature is larger than this threshold value.
Such observations can be included using a theta likelihood function.
For instance, for an observed lower bound tn pL(tn |fn,L)	(fn tn), (17) where 	(x)= { 1 x>0 0 x<0.
These non-Gaussian likelihood terms can be dealt with using an EP approximation similar to the one used in (12), where exact likelihood terms are approximated by Gaussian approximate site functions.
2.4 Active learning In addition to predicting G-quadruplex melting temperatures, it is possible to use the GP framework for experimental design, i.e.to choose which of a set of candidates to measure.
Suppose that we would like to optimally expand a training dataset D, such that we can make most informative predictions about a test set Dtest.
A naive approach would be to randomly draw a subset of the sequences in Dtest, measure their melting temperatures and use them as additional training data.
Alternatively we can consider active learning, choosing this set using an information criterion as proposed by MacKay (1992), or in the context of GP discussed by Seo et al.(2000).Apractical objective function is the mean marginal information gain over the set of interest, here Dtest ={xm,tm}Mm=1.
If the predictions are Gaussian, the mean marginal entropy is entirely determined by the predicted variances 2tm SM = 1 2 M m=1 log 2tm.
(18) To decide which sequence to measure and add to the training data, we iterate through all candidate test inputs xm Dtest, choosing the one which minimizes SM.
The mean entropy SM can be efficiently evaluated as predictive uncertainties of a GP, 2tm , only depend on the training inputs (Equation 3) and hence candidate sequences can be scored before knowing their melting temperature (Seo et al., 2000).
Once a measurement has been taken, the new input/target pair {x, t} is added to the training dataset and hyperparameters are optimized again.
3 EXPERIMENTS To evaluate the proposed method, we applied the GP predictor to a meta dataset summarizing major G-quadruplex experiment data available as of today.
In total, this dataset consists of 260 G-quadruplex structures which have been experimentally tested with varying salt concentrations.
All of the considered sequences were of the form described by the pattern in Equation (8).
Hence the covariance function as introduced in Section 2.1 was applicable.
i377 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i378 i374i382 O.Stegle et al.Fig.3.
Accuracy of GP predictions for a representative 50:50 training/test split (260 total measurements).
(a) True measured melting temperatures (green) and marginal GP predictions with 2 SDs error bars (blue).
(b) Prediction errors.
(c) Z-Scores for the predicted values, | |.
3.1 Predictive performance on observed data To assess the accuracy of the GP method, the model was trained on subsets of all 260 G-quadruplexes.
Subsequently, the trained model was used to predict melting temperatures of G-quadruplexes in the remaining test set, and predictions were compared with the true observed melting temperatures.
This predictive test was repeated for different training/split ratios and multiple random splits.
3.1.1 Mean prediction We first investigated how well we were able to predict real data using our model.
Figure 3a shows marginal GP test predictions versus the true melting temperatures for a representative 50:50 training/test split.
The plot illustrates that the GP has estimated appropriately sized error bars.
A histogram view of the differences of the true melting temperatures and the predictions is shown in Figure 3b.
The results show that most of the experimental data was predicted within a 5C error margin, a reasonable standard of accuracy.
Indeed, across 100 random 50:50 training/test splits, on average 80% of the predictions were within 5C of the experimentally determined values.
We then compared the performance of our model with alternative methods.
This comparison includes the proposed GP model (GP robust), a simpler variant of the model without the robust and constrained likelihood (GP standard), Bayesian linear regression on the sequence features f (Linear regression, Bishop 2006) and a support vector machine (SVM, Fan et al., 2005).
The SVM was applied with the same kernel as used in the GP models.
For the standard GP, linear regression and the SVM, sequences where the data only supplied an upper or lower bound on the melting temperature (i.e.the sequence was too stable to measure under these conditions) had to be excluded.
In total, this reduced the size of the training dataset from 260 to 256 sequences.
Figure 4a, shows the root mean squared error on the test dataset for different algorithms as a function of the relative test set size.
As expected, the performance of all algorithms decreased with growing test set and therefore shrinking training set sizes.
The GP methods outperformed the SVM, and linear regression.
Our robust GP model performed marginally but consistently better than the standard GP.
3.1.2 Variance prediction As a second criterion, we assessed the mean log probability of the test data under the predictive distribution given by different models.
Bigger predictive probability indicates that a method not only is accurate in estimating the mean but also yields appropriately sized error bars.
For this analysis, the results from the support vector machine had to be excluded as the method does not yield a predicted uncertainty.
The results in Figure 4b mirror the comparison of the root mean squared errors.
However, using this probabilistic performance measure, the robust GP performed significantly better than the standard GP variant.
This suggests that the robust likelihood model helps to ensure appropriate predictive uncertainties.
The quality of these error bars is also supported by Figure 3c, which shows Z-scores of test predictions for a 50:50 training/test split.
The number of data points within a 2 SDs margin is in line with the expected number hence showing that the robust GP model knows what it knows.
This is an important and powerful feature for making useful predictions, and will be relevant in the genome-wide G-quadruplex study in Section 4.
3.2 Determining causal features of the G-quadruplex sequence To understand the mechanisms of G-quadruplex stability it is useful to be able to analyse which sequence features play a role in determining the stability of a G-quadruplex.
Such insights can be gained from observing the optimized hyperparameters of the feature kernel kf.
As the lengthscale parameter l i f indicates the relevance of a particular feature i, this can be regarded as a form of feature selection.
A related approach has been described by Chu et al.(2005) who used GP for biomarker discovery in microarray experiments.
The string covariance function ks(s,s) explains part of the sequence similarity and thus makes the relevances of the sequence feature kernel difficult to interpret.
Hence the string covariance was excluded for this evaluation.
Figure 5 shows the inverse lengthscale parameters of the sequence kernel optimized on the full Gquadruplex dataset.
The results were averaged over 100 independent optimizations with random starting points.
The results show that the relevance of features varied significantly.
The most important features were the length of the middle sequence (L2), the total loop length (Ltotal) and the number of guanine stacks (NG).
Among the parameters for base composition frequency, the adenine frequency appeared to be most important.
Both observations are in line with previously observed characteristics of G-quadruplexes (Lane et al., 2008).
However, it had been expected that L1 and L3 would also have a large effect.
In this context, it is interesting to note the strong fluctuation of the significances of the outer loop lengths L1 and L3 as indicated by the error bars in Figure 5.
A possible explanation for this effect is that there are dependencies between these parameters such that either one or the other feature is needed to explain G-quadruplex stability.
Obviously, there is an underlying relationship between Ltotal, NG and L1...3.
As a result of this interaction, independent i378 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i379 i374i382 Predicting the stability of G-quadruplexes Fig.4.
Comparative predictive performance of different algorithms evaluated as a function of the relative test-set size (260 total measurements).
(a) Root mean squared error on the test set.
(b) Mean log probability of the test data under the predictive distribution.
Error bars show 1SD estimated from 100 random training/test splits.
Fig.5.
Optimized inverse lengthscale hyperparameters.
The plot shows empirically estimated means and 1 SD error bars estimated from 100 restarts of the optimization procedure.
Larger bars indicate more important parameters.
restarts might then explore different modes of the hyperparameters posterior distribution.
To better understand the posterior over hyperparameters, we employed a Hamiltonian Monte Carlo sampler (e.g.MacKay, 2003) to draw samples from this distribution.
Figure 6 shows the correlations between hyperparameters of the feature kernel as a Hinton diagram.
The correlation coefficients have been calculated from 500 MCMC samples (500 burn-in).
This figure shows that the relevances of L1 and L3 were indeed anti-correlated.
This observed anti-correlation can be explained by positive correlations between the corresponding features in the training dataset, causing that either L1 or L3 is sufficient to predict the melting temperature.
A strong positive correlation of hyperparameters was observed between the loop length L2 and the number of G-stacks NG.
4 GENOME-WIDE ANALYSIS OF G-QUADRUPLEX CANDIDATES We applied the GP predictor to human genome-wide G-quadruplex candidates downloaded from the quadruplex.org database (Wong Fig.6.
Correlations between inferred hyperparameters illustrated as Hinton diagram.
Correlation coefficients were estimated from 500 Monte Carlo sample.
The size of the squares denote the strength of the correlation, where white squares indicate positive correlation and black squares negative correlation.
et al., 2008).
The database contains candidate structures extracted from sequence information using the pattern-based rule from Equation (8), considering quadruplexes with three or more G-stacks (NG 3).
Using this rule a total of 359 548 G-quadruplex candidates with precisely 3 loops have been identified genome-wide, from a total of 373 k predicted sequences, some of which contain several possible G-quadruplexes, and hence cannot be predicted with the available data.
Following Huppert and Balasubramanian (2007), we also extracted those G-quadruplexes found in the promoters of human genes, looking at the 200 bp upstream of the transcription start site.
Again restricting to 3-loop G-quadruplexes there were 10 987 quadruplexes in human promoter regions.
All computational predictions for these G-quadruplexes were made for a solution containing 100 mM K+, which roughly approximates physiological conditions and has become something of a standard for experimentation.
i379 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i380 i374i382 O.Stegle et al.Fig.7.
Average predictive uncertainty for promoter G-quadruplexes as a function of the number of additional measurements.
Compared are two random measurement sequences (black) and the active learning strategy (red).
The red and black cross indicate the average predictive uncertainty after physically measuring 10 actively (red) or randomly (black) chosen G-quadruplexes.
4.1 Active learning for promoter G-quadruplexes Given the large number of genomic sequences and the relatively small number of data points, it is necessary to be efficient with data collection, so as to maximize the information derived from each new experiment.
We therefore developed a method of active learning such that we can predict which experimental data (i.e.melting temperatures of sequences) would be most useful to collect.
As a preliminary case study of the usefulness of active learning, we considered the set of promoter G-quadruplexes and applied the active learning strategy outlined in Section 2.4.
Given the training dataset, we selected the subset of the 10 most informative G-quadruplexes in promoter regions, assessed by the marginal information gain.
The melting temperatures of the corresponding sequences were experimentally determined and added to the training set.
As an alternative, we did the exact same experiment but selected 10 randomly chosen sequences instead.
Again the sequences were experimentally characterized and added to the training set.
In each case, the sequences were prepared at 4 M concentration in a TrisHCl buffer at pH 7.4 with 100 mM KCl.
A Varian Cary 300 spectrophotometer was used to measure the absorbance at 295 nm over repeated slow heating/cooling cycles (Mergny et al., 1998).
Melting temperatures were determined by the derivative method.
Figure 7 shows the average predictive uncertainty for all promoter quadruplexes as a function of the number of additional measurements.
Results for the physical measurements are indicated as red and black crosses.
Lines show the expected uncertainties obtained from the model without conducting any physical measurement.
It is apparent that very few additional measurements can significantly reduce the predictive uncertainty.
This observation can be explained by the sequence homology present in the G-quadruplexes found across the genome (Huppert and Balasubramanian, 2005; Todd et al., 2005).
The active selection performed significant better than the randomly selected sequences.
Active learning allows a feedback cycle to be developed, where after each set of data is added, new learning can be performed to optimize the next data collection, resulting in efficient experimentation.
Fig.8.
Predictive uncertainty for genome-wide G-quadruplex candidates shown in standard deviations in degree celsius.
The average uncertainties resulting after real measurements were higher than the model expectations.
This discrepancy is because the theoretical calculations are approximations based on fixed hyperparameters, whereas for the physical measurements the hyperparameters were re-optimized (Section 2.4).
However, we did clearly observe a substantial reduction in uncertainty using the experimental data.
These results are supportive and encouraging that active learning in the context of G-quadruplex structures is a helpful tool, although clearly more than 10 further data points are required to make a substantial difference to the predictive power of the model.
4.2 Study of genome-wide G-quadruplex candidates We also performed predictions on all 360 k G-quadruplexes genome-wide.
The predictive uncertainty for those G-quadruplexes varied significantly.
Figure 8 shows a histogram of the predictive uncertainty in SD for the entire set of all G-quadruplex sequences.
For 90% of the sequences this uncertainty was <14C.
At a more stringent cut off level, still 63% of the sequences could be determined within 10C and 6% within 5C.
This highlights the need for further data collection and the active learning methodology previously described, as well as highlighting the usefulness of predictive uncertainties.
4.2.1 Quadruplexes in promoters Previous analysis of G-quadruplexes suggests that G-quadruplexes are likely to play a widespread regulatory role, supporting experimental demonstrations.
It has been shown that G-quadruplexes are over-represented inside promoter regions compared to elsewhere in the genome, by about an order of magnitude (Huppert and Balasubramanian, 2007).
However, so far it has not been possible to assess whether these quadruplex structures have different stabilities.
Here, we use the developed GP predictor to investigate whether there are systematic differences of G-quadruplex stability inside and outside of promoter regions.
Figure 9 directly compares the predictive mean melting temperature for G-quadruplex structures inside promoter regions with G-quadruplexes elsewhere in the genome.
For this analysis, we restricted the considered sequences to those that could be predicted with at most a 5C standard deviation error margin yielding a total of 17 006 G-quadruplexes out of which 235 were in promoter regions.
The plots suggest that the statistics of melting temperature might indeed be different for promoter G-quadruplexes.
The significance of the difference i380 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i381 i374i382 Predicting the stability of G-quadruplexes Fig.9.
Mean predictions of the melting temperature in 100 mM KCl for genome-wide G-quadruplex candidates with a predicted uncertainty <5C.
(a) Histograms for promoter and non-promoter quadruplexes.
(b) Cumulative distribution functions.
between the two distributions, melting temperatures of promoter G-quadruplexes and non-promoter quadruplexes, was assessed by a KolmogorovSmirnov test.
A two-sided test on the predicted mean temperatures for promoter and non-promoter G-quadruplexes found the difference was significant (P = 4.05105).
This result suggests that G-quadruplexes found in gene promoters are likely to be more stable than those found in the bulk of the genome.
5 DISCUSSION AND CONCLUSION We have here presented a robust and sensitive method for inferring the stability of G-quadruplexes from the sequence information.
Our approach is robust with respect to outliers, allows constraints to be incorporated as observations and automatically determines relevant sequence features.
We have further demonstrated how active learning can be used to perform experimental design to guide the choice which sequences of a set of candidates to measure.
We demonstrated as proof of principle that we can apply this approach to determine features of biologically important Gquadruplexes, selecting as our example G-quadruplexes found in the 200 bp region upstream of known human gene transcription start sites, a region containing much promoter activity.
We have shown previously that G-quadruplexes are concentrated in this region (Huppert and Balasubramanian, 2007), and a number of individual studies have confirmed that these can have transcriptional regulatory ability (Qin and Hurley, 2008).
From the results shown here, we can now conclude that the G-quadruplexes in promoters are likely to be more stable than in the genome as a whole, further supporting the hypothesis that they play an important general role in transcriptional control.
The precise mechanistic details of how G-quadruplexes regulate transcription are not entirely clear, but the current model is that their formation disrupts the binding of the normal transcriptional machinery (Qin and Hurley, 2008).
This approach can be further extended to other regions where G-quadruplexes are found to investigate other functional roles.
Several interesting and fruitful extensions to our proposed method could be considered.
The sizes of currently available G-quadruplex datasets is very limited.
As more data becomes available it would be possible to apply more general sequence kernels characterizing similarity of the loop sequences.
Such an approach might yield novel insights into how the sequence composition influences the stability of G-quadruplex structures.
We are currently in the process of scaling available G-quadruplex data to significantly larger datasets using the active learning approach proposed in this work to efficiently explore the phase space available.
Once the amount of available data goes beyond 1000 examples, it would be helpful to explore sparse approximations to the proposed GP scheme (for instance Snelson and Ghahramani, 2006).
We will also arrange a data store for other researchers to contribute experimental data they have collected.
We plan to have discussions with other researchers to establish a standard for experimental measurements, as well as standards for the quality and style of data provided, which should include measurements of G(37C), H and S as well as the melting temperature.
This would allow us to predict these parameters in addition to the melting temperature alone.
We intend to provide a web-enabled version of these predictions.
Links to these resources, source code and Supplementary Material are available online.2 The field of Gquadruplexes has grown rapidly in recent years, and we anticipate that the ability to predict their thermodynamic properties will be useful to many in the field, and accelerate the rate of discovery of new functional roles for these fascinating structures.
Conflict of Interest: none declared
ABSTRACT Motivation: Analysis of expression quantitative trait loci (eQTL) significantly contributes to the determination of gene regulation programs.
However, the discovery and analysis of associations of gene expression levels and their underlying sequence polymorphisms continue to pose many challenges.
Methods are limited in their ability to illuminate the full structure of the eQTL data.
Most rely on an exhaustive, genome scale search that considers all possible locusgene pairs and tests the linkage between each locus and gene.
Result: To analyze eQTLs in a more comprehensive and efficient way, we developed the Graph based eQTL Decomposition method (GeD) that allows us to model genotype and expression data using an eQTL association graph.
Through graph-based heuristics, GeD identifies dense subgraphs in the eQTL association graph.
By identifying eQTL association cliques that expose the hidden structure of genotype and expression data, GeD effectively filters out most locusgene pairs that are unlikely to have significant linkage.
We apply GeD on eQTL data from Plasmodium falciparum, the human malaria parasite, and show that GeD reveals the structure of the relationship between all loci and all genes on a whole genome level.
Furthermore, GeD allows us to uncover additional eQTLs with lower FDR, providing an important complement to traditional eQTL analysis methods.
Contact: przytyck@ncbi.nlm.nih.gov 1 INTRODUCTION The development of methods that allow us to uncover mechanisms of gene regulation and reconstruct gene regulatory networks is an important open problem in molecular biology.
The advancement of high-throughput genotyping and gene expression platforms supports the analysis of expression quantitative trait loci (eQTL) as a tool to elucidate gene regulation.
eQTL analysis considers expression of each gene as a quantitative trait and maps it to a genomic locus or marker.
The genotype associated with a genes expression level highlights the genome region carrying the DNA polymorphism impacting the expression.
The polymorphism may reside in the genes coding region or in a transcription factor binding site and could affect the expression level of its own or other genes in an inheritable way (Brem and Kruglyak, 2005; Monks et al., 2004; To whom correspondence should be addressed.
Present address: NOB, NCI, NIH, 37 Convent Drive 1142E, Bethesda, MD 20892, USA.
Petretto et al., 2006).
Hence, a significant statistical linkage between a locus and a genes expression suggests that the gene in question is regulated by the locus, which may hold a regulatory element or a regulator gene.
Since the early work of Jansen and Nap (Jansen and Nap, 2001), eQTL has become a widespread technique to identify such regulatory associations and has been applied to several species including yeast (Brem and Kruglyak, 2005; Yvert et al., 2003), mouse (Bystrykh et al., 2005; Chesler et al., 2005) and human (Cheung et al., 2005; Stranger et al., 2005).
Typically, these studies use genome-wide association studies (GWAS), considering loci spanning the genome and expression profiles of all genes in the organism.
As a major advantage, simultaneous monitoring of thousands of gene expression traits provides unique and unbiased data and opens the possibility of constructing a global view of the underlying regulation machinery.
Despite the valuable insights that can be gained, current attempts to elucidate the structure of eQTLs still face many challenges.
Only a few methods are available that model complete eQTL data to discover broader eQTL structure.
The complex dependence of the variations of gene expression regulation on phenotypic differences nurtures the expectation that important information can be gained from considering more subtle relationships between genotype and expression.
The large number of gene expression traits and genomic loci poses challenges for both computational efficiency and statistical power.
Traditionally, an eQTL study tests the linkage between all genes expression and all loci, adding up to millions of single statistical tests.
For example, (Stranger et al., 2007) used 2 million single-nucleotide polymorphism (SNP) and more than 13 000 transcripts, leading to more than 1010 tests for all possible associations, a number that causes a serious multiple testing issue (i.e.the chance of false positives in a family of multiple hypothesis tests is higher than that of a single test).
Consequently, that study was restricted to consider mainly cis-regulationassociations between SNPs and genes within 1 M bp of the SNP in questionwhich reduced the number of tests dramatically.
While more complex regulation programs are of increasing interest (Storey et al., 2005), the combinatorial nature of such problems and the large number of loci call for improved methods that allow discovery of more complex regulation programs involving more than one locus and one gene.
To address such problems, we propose a novel method, GeD (Graph based eQTL Decomposition), to analyze eQTL data.
Our method models the genotype, progeny and expression data as an eQTL association graph, a threepartite graph which is the union of two bipartite graphs.
By simultaneously exploring two bipartite graphs, GeD discovers sets of dense subgraphs, called 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:50 15/5/2009 Bioinformatics-btp189.tex] Page: i16 i15i20 Y.Huang et al.eQTL association cliques, each containing a set of loci, a set of progenies and a set of genes.
The progenies provide evidence that the set of loci may be associated with the set of genes.
Such eQTL association cliques give a succinct representation of structures among loci, progenies and genes on a genome-wide scale.
More importantly, each locus, progeny and gene can appear in more than one association clique, which depicts a complete picture of eQTL data.
To find eQTL association cliques, GeD employs an efficient bipartite clique enumeration algorithm initially designed for building a concept lattice (Farach-Colton and Huang, 2008).
The set of association cliques helps to select a small set of locusgene pairs that are expected to have significant linkage; subsequently, statistical tests, including corrections for multiple testing, are performed for theses selected locusgene pairs.
Testing GeD, we reanalyzed data from a recent eQTL study of the human malaria parasite Plasmodium falciparum (Gonzales et al., 2008).
While understanding regulatory programs of this parasite is of fundamental importance, successes in identifying specific transcription factors in P.falciparum have been limited.
Gene expression of various P.falciparum strains does not vary significantly in response to perturbation (Rockman and Kruglyak, 2006); however, ubiquitous heritable expression patterns likely exist, although the association between loci and gene expression might be weak.
Due to the difficulty of breeding and growing P.falciparum strains, only 34 progeny strains were used, a small number of strains that aggravates the detection of eQTL associations and increases the need for the more discerning methodology outlined here.
Despite these challenges, Gonzales et al.successfully identified 1063 eQTLs with a FDR 0.24 in a genome-wide association study and showed several eQTL hotspots (Gonzales et al., 2008).
Using GeD, we find that the size of eQTL association cliques is significantly different from random association cliques, and loci on different chromosomes tend to co-occur in some eQTL association cliques.
In addition, by using eQTL association cliques, we detected 1327 eQTLs in P.falciparum with a FDR less than 0.05 without testing all possible locusgene pairs, and new eQTL hotspots identified by GeD show several interesting biological characteristics.
2 MATERIALS AND METHODS First we introduce the basic rationale of the GeD approach and present a detailed description of GeD.
Finally, we describe the P.falciparum eQTL data we used to test our method.
2.1 Problem definition In an eQTL experiment, we consider a set of progeny strains, often obtained from a cross between two parental strains with different genetic and phenotypic background.
In our case, we only consider two possible genotypes (0, 1) for each locus and assign each locus lj , to the parental strain the locus was inherited from.
All strains can be partitioned into two groups by the genotype of a given locus, and we discretize the expression levels of each gene as being either up-regulated, unchanged or down-regulated (Fig.1a).
To represent the above relationship between loci and genes, we define an eQTL association graph (GSL,E) as follows: The graph contains three sets of vertices (G,S,L), where L represents the genotypes of loci, S represents progeny strains, and G represents up-or down-regulated gene expression.
Vertices giu and gid indicate a gene gis up-or down-regulation and lj0 and lj1 represent the genotype at locus lj as either 0 or 1.
An edge Fig.1.
(a) Each genomic locus l is assigned the genotype of the strain it was inherited from (0/1).
In all strains s, we discretize expression levels of genes g as being either up, unchanged or down.
Considering the genotypes of l, we observe different gene expression patterns, indicating different expression mechanisms in different strains.
For example, the expression of gi is upregulated in strain sh1, sh2, sh4 and sh6, and down-regulated in strain sk2, sk3, sk5 and sk6.
In (b) we show the corresponding eQTL association graph.
Specifically, we find an association clique including genes gsd and giu, strains sh1, sh2, sh4, sh6 and loci lr0 and lj0, shown in light grey.
The edges of the association clique are drawn with wider lines.
between giu/gid and a progeny strain sk indicates gis expression is up-or down-regulated in strain sk.
An edge between lj0/lj1 and a progeny strain sk indicates the genotype of lj is 0/1 in strain sk.
Note, that there are no edges between genes G and loci L. Our eQTL association graph can be viewed as a three-partite graph which is the union of two bipartite graphs BG1(LS,E1) and BG2(GS,E2).
The corresponding eQTL association graph in Figure 1a is shown in Figure 1b, where we consider the subgraph induced by gsd , giu, sh1, sh2, sh4, sh6, lr0 and lj0.
We call such a subgraph an eQTL-association-clique, defined as p = (Gp Sp Lp,Ep), giu/d Gp,sk Sp,(giu/d ,sk)Ep and lj0/1 Lp,sk Sp,(lj0/1,sk)Ep.
In other words, we require that Gp and Sp, and Lp and Sp are fully connected.
Additionally, no such association clique q = (Gq Sq Lq,Eq) exists, where Gq and Sq are fully connected, Lq and Sq are fully connected, and Gq Sq Lq Gp Sp Lp.
In other words, each eQTL association clique is a maximal subgraph that cannot be extended further, maintaining full connectivity.
Similarly, an eQTL association clique can be viewed as the union of two dense bipartite subgraphs formed by Gp Sp, and Lp Sp, respectively.
As defined, please note that in each eQTL association clique, |Gp|1 and |Lp|1.
Furthermore, opposing loci lj0 and lj1, or gene expression states giu and gid can not appear in the same association clique.
It is easy to see that there can be four cases where a locusgene pair (lj , gi) can appear in an association clique: The first case is that an up-regulated gene giu and a 0-genotyped locus lj0 are in an association clique while in the second case a down-regulated gene gid and a 1-genotyped locus lj1 are in an association clique.
We call these cases P1.
In a third case an up-regulated gene giu and a 1-genotyped locus lj1 are in an association clique while in the last case a down-regulated gene gid and a 0-genotyped locus lj0 are in an association clique, cases we call P2.
We call the first two cases compatible since they both suggest that gis expression pattern is different in two groups defined by ljs genotype-and vice versa.
Intuitively, a locus and a gene that co-appear in an association clique that has a large subset of strains are expected to be more closely associated.
Therefore, we define the size of the progeny strain set in a subgraph of the association graph as support, sp.
For a locusgene pair (lj , gi) and an association clique with support sp, if giu and lj0 co-appear in the clique, we define the support provided by the clique spu0ij.
Similarly, we define i16 [09:50 15/5/2009 Bioinformatics-btp189.tex] Page: i17 i15i20 Graph theoretical approach to study eQTL spd1ij , sp u1 ij and sp d0 ij.
Using these definitions, the support for pattern P1 for (lj , gi) is sp P1 ij =max(spu0ij )+max(spd1ij ), over all eQTL association cliques.
Analogously, the support for P2 is sp P2 ij =max(spu1ij )+max(spd0ij ).
Since P1 and P2 are opposites if we consider the linkage between lj and gi, we define spij =|spP1ij spP2ij | as a rough measurement of the net support for the expectation that significant linkage between lj and gi exists.
2.2 Method Based on these important heuristics, GeD performs the following steps to identify eQTL association cliques and to detect eQTL: (i) Discretize (see below) gene expression levels and build an eQTL association graph (GSL,E), a union of bipartite graphs BG1(LS,E1) and BG2(GS,E2).
(ii) Find all maximal bipartite cliques in BG2(GS,E2).
(iii) For each maximal bipartite clique BC(Ga Sa,Ea), find all maximal bipartite cliques BC(Lai Sai,Eai) in the bipartite graph induced by Sa in BG1(LS,E1).
(iv) Identify sets Gai where each vertex is connected to each vertex in Sai appearing in BG2(GS,E2).
If the subgraph (Gai Sai Lai,Eai) has not been generated yet, output this graph as an eQTL association clique.
(v) For each locusgene pair (lj , gi) appearing in one eQTL association clique, select the pair if its support value max(spP1ij ,sp P2 ij ) and spij meet criteria described below.
(vi) Among selected locusgene pairs compute p-values of their association (adjusted for multiple testing).
In both steps (ii) and (iii), it is essential to enumerate bipartite cliques from a large bipartite graph efficiently.
We apply an algorithm for building a concept lattice, which can be considered a hierarchical structure for organizing all bipartite cliques given a bipartite graph.
Such structures have been used to compare gene expression matrices (Huang and Farach-Colton, 2007).
The delay-time complexitythe time spent to compute each bipartite cliqueof the algorithm is O(n1n2), where n1 and n2 are the size of two sets of vertices in the bipartite graph.
Here, we assume that the number of bipartite cliques in BG2(GS,E2) is lower than in BG1(GS,E1).
If this is not the case, GeD starts from BG1(LS,E1) in step (ii); steps (iii) and (iv) are changed accordingly.
To obtain corrected p-value in the last step of GeD, we apply the method of Churchill and Doerge (Churchill and Doerge, 1994).
For each gene gi in a selected locusgene pair from step (v), we maintain a locus list (lj1, lj2, , ljd ), where each locus in the list appears with gi in one of selected locusgene pairs.
We randomly permute gis expression and compute the nominal p-value for the linkage between the random expression and a locus in the list and retain the smallest p-value.
After repeating the process 1000 times, we use all retained p-values to approximate a null distribution.
By comparing the nominal p-value from real data to the null distribution, we obtain the corrected p-value.
While numerous ways to discretize gene expression data (Becquet et al., 2002) transcription patterns of most genes in several major P.falciparum strains are very similar (Llinas et al., 2006).
Therefore, we used a simple method (Quackenbush, 2002) that can be readily applied to our case.
We computed the mean m and standard deviation stdev for each probe and define genes with expression levels > m+bstdev as up-regulated and < mbstdev as down-regulated.
Specifically, we set b to 1, allowing us to detect more variation in the gene expression.
Another advantage of b = 1 is that each probe will be represented by at least one vertex in the association graph.
In the worst case, the number of bipartite cliques in a bipartite graph is min(2n1, 2n2)2, where n1 and n2 are the sizes of the two vertex sets of the bipartite graphs.
Since thousands of vertices in G and L in the eQTL association graph exert extreme computational costs we only allow bipartite cliques with at least five progeny strains in step (ii) and (iii).
2.3 Materials Utilizing P.falciparum eQTL data from the reference (Gonzales et al., 2008), we used 34 progeny strains obtained from a HB3xDd2 cross.
Each progeny was genotyped at 329 microsatellite markers along 14 chromosomes.
Expression levels were measured 18 h after the parasite invades human erythrocytes (RBCs), by 7665 probes representing 5150 ORFs.
3 RESULTS As previously mentioned, eQTL association cliques allow us to determine the structure inherent in eQTL data.
We first show the difference between association cliques obtained from the underlying eQTL data as well as from randomized data.
Subsequently, we report eQTLs we determine in eQTL association cliques.
3.1 Size Distribution of eQTL Association Cliques Applying GeD, we obtain 135 044 eQTL association cliques with support sp5.
Overall, the support in eQTL association cliques ranges from 5 to 10.
To generate random eQTL association cliques, we permuted the expression vector of each probe and applied GeD with the same parameters on the random data 100 times.
We find 40 773, 20 393 and 5396 association cliques with support of 5, 6 and 7 in the real data.
In random data, we find on average more association cliques (sp = 5: 77 200; sp = 6: 28 019; sp = 7: 5809).
Applying a one-sample t-test between the number of association cliques in real and random data yielded p < 1011 in all three cases.
Considering association cliques with sp 8, 9 and 10, we find 872, 84 and 5 in the real data.
Compared to the random data, we analogously find significant differences.
Specifically, we find on average 807, 74 and 4 random association cliques with the same supports in the randomized data with p < 1011 in the cases of support 8 and 9, and p < 1010 in the case of support 10.
Subsequently, we compared the number of association cliques in real and random data that have the same support sp and |G|, the number of probes.
The number of random association cliques was significantly smaller except when sp = 5 or 6 or 7, |G| = 1, and sp = 5, |G| = 2.
In Figure 2a we show the number of real association cliques and the average number of random association cliques with sp = 6 and several different number of probes |G|.
Specifically, the largest eQTL association clique with sp = 6 has 87 probes.
A closer look revealed that, given support sp, |G| and |L|, the number of association cliques in the random data was significantly larger than in the real data only when |G| is small.
For example, when sp = 6 and |L| = 7, the number of association cliques in the random data was larger only if |G| = 1.
For a given a support value sp and the number of loci |L|, the number of association cliques in the random data was significantly larger than in the real data for most cases when sp = 5 or 6.We find similar results when sp = 7, |L| < 12, sp = 8, |L| < 7, and sp = 9, |L| < 5.
Specifically, we show the number of random association cliques with sp = 6 and different numbers of loci |L| in Figure 2b.
Since genotypes of adjacent loci are more similar than others, we expect that many loci in the same eQTL association clique are adjacent.
Though this is frequently the case, we also find many co-appearing loci although they are on different chromosomes.
For example, loci 2_0 on chromosome 2 and 12_45.8 on chromosome 12 co-appear with six loci on chromosome 3 in an eQTL association clique with support 10.
We did not find such a result in the random data, suggesting that these loci tend to co-segregate and indicating i17 [09:50 15/5/2009 Bioinformatics-btp189.tex] Page: i18 i15i20 Y.Huang et al.Fig.2.
Number of association cliques from real and random data with support 6.
Numbers we obtained from randomized data were averaged over 100 runs.
In (a) we perform the analysis varying |G|, the number of probes, while we show the analogous results with changing number of loci |L| in (b).
that a closer examination of their relation might be interesting with linkage disequilibrium based methods for P.falciparum (Su and Wootton, 2004).
3.2 eQTL detection We have shown that a locusgene pair appearing in two eQTL association cliques in a compatible way is more likely to have a significant linkage than those pairs that do not.
Hence, we could use eQTL association cliques to select a small number of locusgene pairs to be tested for linkage, many of which we expect to yield significant p-value.
To this end, we used as criteria max(spP1ij ,sp P2 ij )12 and spij 6 in step (v) to select locusgene pairs (lj , gi).
Please note that each association clique has at least five progeny strains because we generate maximal bipartite cliques with at least five progeny strains.
If we assume a locusgene pair (lj , gi) with pattern P1, then the minimum value for sp P1 ij is 10 since spP1ij =max(spd1ij )+max(spu0ij ), where spu0ij 5 and spd1ij 5.
Note, that if we set this threshold too high, we potentially remove Fig.3.
Histogram of nominal P-values for all possible locusgene pairs and pairs we selected from eQTL association cliques.
locusgene pairs having significant linkage.
In total, we selected 6232 locusgene pairs.
Figure 3 shows the histogram of nominal p-value computed by a two-sided T-test for the linkage of these selected pairs and all possible locusgene pairs.
We observe that selected locusgene pairs from association cliques yield significantly lower linkage p-value.
Correcting p-values (see Methods section) and calculating FDRs (Storey and Tibshirani, 2003) we identified 2853 eQTLs (p < 0.05, FDR < 0.04).
Identifying the most significant eQTLs, we used our set of association cliques we found in randomized data.
With the same criteria, we obtained a list of locusgene pairs from each set of randomized association cliques and applied a T-test to obtain p-values for these pairs.
In this way, we obtained 100 groups of pvalues from random data, allowing us to estimate an empirical null distribution, which is often more stringent than the null distribution obtained individually for each gene (Churchill and Doerge 1994).
We required that each reported eQTL has a nominal p-value smaller than 90% of p-values in the empirical null distribution.
Following this protocol, we found 1327 eQTLs for 513 probes (482 genes) and 231 loci.
Previously, Gonzales et al.(Gonzales et al., 2008) identified a set of 1063 eQTLs with FDR < 0.24 using standard GWAS.
In Figure 4, we show the distributions of eQTLs identified by Gonzales et al.and 1327 eQTLs we obtained with GeD.
We observe that the distribution of eQTLs detected by GeD is similar to the distribution of previously identified eQTLs, which were obtained by considering all possible locusgene pairs.
We also find 251 (25%) eQTLs that appear in both sets.
Although the overlap is considerable the two sets are quite different, an observation that can be attributed to fundamental differences in the methods (see Discussion section).
Both analyses show that there are several eQTL hotspots on chromosome 3, 5 and 7.
Gonzales et al., (Gonzales et al., 2008) called a locus eQTL hotspot if there existed at least 14 linked probes at a particular locus.
Analogously, we found 17 eQTL hotspots and discovered two/three new eQTL hotspots in the right/left subtelomeric region on chromosome 3.
While two weak eQTL hotspots on chromosome 9 and 12 detected by Gonzales et al.did not appear in our result, we detected two new eQTL hotspots on chromosome 5 and 7.
Note that the definition of a hotspots used in both studies does not differentiate between cis-and trans-links, and i18 [09:50 15/5/2009 Bioinformatics-btp189.tex] Page: i19 i15i20 Graph theoretical approach to study eQTL Fig.4.
In (a), we show genome-wide eQTL distributions in P.falciparum by testing all possible combination of loci and genes (Gonzales et al., 2008).
In (b), we find similar, yet enriched patterns of eQTLs we detected with GeD.
the reported hotspots represent the combined effect of both types of regulation as well as that of the pattern of linkage disequilibrium.
Both subtelomeric regions on chromosome 3 are enriched with highly polymorphic surface antigen genes such as cytoadherence linked asexual genes (CLAG), stevor genes, and var genes (Gardner et al., 2002).
While compelling, it remains to be experimentally determined if such polymorphic antigen genes are indeed regulated by eQTL hotspots we identified in the same region.
Interestingly, it has been reported that the right telomere of chromosome 3 has an extended region of similarity with the right telomere of chromosome 2, and some pseudogene sequences in the regions were also preserved (Bowman et al., 1999).
Such preservation in these rapidly evolving regions may imply that these subtelomeric regions are biologically significant (Bowman, et al., 1999), suggesting that the detection of additional eQTL hotspots in these regions provided more evidence for their importance in regulating the host-parasite interface.
We also performed Gene Ontology term enrichment analysis for the target genes of newly detected eQTL hotspots using GOTermFinder (Boyle et al., 2004).
We found that two hotspots show enriched GO terms referring to drug interaction and parasitehuman invasion.
The GO annotation of target genes of eQTL at locus 5_25.8 on chromosome 5 was enriched for drug binding (p < 0.001) and cis-trans isomerase activity (p < 0.002).
The GO annotation of target genes of eQTLs at locus 3_14.3 on chromosome 3 was enriched for cytoadherence to microvasculature mediated by parasite protein and interaction with the host (p < 0.03).
4 DISCUSSION We introduced a novel methodGeDthat integrates genotype, expression and progeny data, providing an analytical framework for the determination of gene regulation programs.
In an eQTL association clique, vertices representing a locus genotype are fully connected with vertices that represent progeny strains.
Such a structure refers to the case that loci have the same genotype when restricted to these progeny strains.
Analogously, vertices that represent genes are fully connected with vertices representing progeny strains, indicating that the corresponding progeny strains share the same gene expression patterns.
As such, eQTL association cliques allow the determination of associations of loci, progeny strains and genes in a simple way.
In addition, the number of progeny strains supports the linkage between loci and genes in the same association clique, which can help to detect eQTLs.
In this article we focused on the application of the eQTL association cliques to enhance eQTL discovery.
However, eQTL association cliques have the potential to answer other questions as well.
For example, loci that are not in linkage disequilibrium and co-occur in a highly supported clique might indicate functionally important co-segregation.
Note that while loci that are in the same clique and are genomic neighbors are likely to be in linkage disequilibrium.
However, the opposite case is not necessarily true.
This observation should be useful in elucidating non-random properties of linkage disequilibrium.
Additionally, eQTL association cliques may help the identification of loci and genes that are related in a certain phenotype.
If the phenotype of progeny strains in an association clique is different from remaining progeny strains, the loci and genes in the corresponding association clique are the prime candidates that affect the phenotype in question.
Using eQTL association cliques might also help to uncover multiple locus linkage.
For example, consider loci lj and lr and gene gi, and four eQTL association cliques, where lj0 and lr0 appear with giu in one clique, lj0 and lr1 appear with gid in another clique, lj1 and lr1 appear with giu in the third clique and lj1 and lr0 appear with gid in the last clique.
It is unlikely that lj or lr are associated with gi individually because the genotype 0/1 of lj is associated with both up-and down-regulated expression of gi.
The same rational holds for locus lr.
But since the joint genotype 00 and 11 of lj and lr is associated with up-regulation of gis expression, and joint genotype 01 and 10 of lj and lr is associated with down-regulation of gis expression, the two loci can have a significant epistatic interaction effect on gi.
By restricting our attention on loci in the same association clique, we can select a small set of triplets (lj , lr , gi), which fit the above scenario, by simply counting association cliques.
Testing the selected triples for epistatic effects reduces the number of statistical tests, O(|L|2|G|), required by an exhaustive search, where L is the locus set and G is the gene set.
In our method, we modeled underlying data using certain choices.
First, discretizing expression data, a gene was considered differentially regulated if its expression level was at least one standard deviation away from its mean expression.
This choice was dictated by its relative simplicity and applicability of that method to the data where differences in the expression levels are not expected i19 [09:50 15/5/2009 Bioinformatics-btp189.tex] Page: i20 i15i20 Y.Huang et al.to be very large.
Other methods of discretizing expression data will be considered in the future improvement of the method.
Next, we chose to look at maximal cliques rather than other densely, yet not completely, connected subgraphs, allowing us to avoid the introduction of additional density parameter.
Furthermore, such an approach also allowed us to easily generate such clique-structures utilizing the efficient bipartite clique enumeration method (FarachColton and Huang, 2008).
While bipartite cliques can potentially be replaced with bi-clusters, the best heuristic for the identification of such overlapping bi-clusters remains to be found.
We conclude that our choices might potentially influence our ability to detect potential eQTLs.
However, we made our choices as simple as possible and highlight the usability of our novel method.
We applied GeD to progeny data of P.falciparum and found that eQTL association cliques have very different structures and distributions compared to random association cliques.
Using eQTL association cliques to select a small set of locusgene pairs, we corroborated previously identified eQTLs, and significantly increased their number, including new eQTL hotspots.
Preliminary analysis of the possible functional relevance of these new eQTL hotspots showed that some harbor important antigen genes while others include target genes involved in drug and parasite-host interactions.
Compared to previous results, we conclude that GeD bolsters traditional eQTL analysis methods and provides new opportunities for the discovery of critical biological functions in P.falciparum.
Approximately 25% of eQTLs in the two eQTL sets identified by GeD and Gonzales et al.(Gonzales et al., 2008) overlap, a difference that can be caused by several factors.
First, Gonzales et al.applied an interval mapping method based on a complex Bayesian model for QTL detection (Sen and Churchill, 2001).
Assuming each marker is the potential eQTL location, we in turn applied a two-sided T-test to determine linkage between markers and gene expression.
To a certain extent, GeD may lose some information and consequently detection sensitivity due to the discretization of gene expression values and focus on relatively large eQTL association cliques.
In contrast, the GWAS used by Gonzales et al.is likely to miss more subtle associations detected by our method because only the most significant eQTLs can pass multiple testing correction performed for all possible locusgene pairs.
Our current implementation of GeD is designed for the analysis of the large data set of P.falciparum.
However, the number of eQTL association cliques can increase exponentially with the number of loci and genes in the worst case.
Therefore, the scalability of GeD to larger eQTL data sets containing thousands or even millions of loci remains to be tested.
Specifically, in human studies where we have to deal with huge amount of expression and genomic data we expect strongly increasing computational costs, prompting the development of further heuristics and improved computational techniques that will allow us to tackle more challenging GWAS problems.
ACKNOWLEDGEMENTS The authors thank John Wootton (NIH/NCBI) for stimulating discussions.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine; National Institutes of Health (AI071121 and AI055035 to M. T. F.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Pangenome arrays contain DNA oligomers targeting several sequenced reference genomes from the same species.
In microbiology, these can be employed to investigate the often high genetic variability within a species by comparative genome hybridization (CGH).
The biological interpretation of pangenome CGH data depends on the ability to compare strains at a functional level, particularly by comparing the presence or absence of orthologous genes.
Due to the high genetic variability, available genotype-calling algorithms can not be applied to pangenome CGH data.
Results: We have developed the algorithm PanCGH that incorporates orthology information about genes to predict the presence or absence of orthologous genes in a query organism using CGH arrays that target the genomes of sequenced representatives of a group of microorganisms.
PanCGH was tested and applied in the analysis of genetic diversity among 39 Lactococcus lactis strains from three different subspecies (lactis, cremoris, hordniae) and isolated from two different niches (dairy and plant).
Clustering of these strains using the presence/absence data of gene orthologs revealed a clear separation between different subspecies and reflected the niche of the strains.
Contact: J.Bayjanov@cmbi.ru.nl Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Detection of genomic variation between related organisms can elucidate relations between genotypic and phenotypic traits of organisms, for example, those related to diseases with a genetic origin (Inazawa et al., 2004; Kallioniemi et al., 1992) or to functional traits of microorganisms (Pretzer et al., 2005).
Comparative Genomic Hybridization (CGH) microarrays allow the detection of variation between a reference genome, whose sequences are targeted by the probes, and query genomes.
The type of genetic variations that can be detected depends on the array design and the sequence similarity of reference and query genomes.
Using short oligonucleotides, single nucleotide polymorphisms (SNPs) To whom correspondence should be addressed.
may be detected between highly similar genomes, like those of different human individuals.
However, bacterial strains belonging to the same species often display extensive sequence variations (Lan and Reeves, 2000; Medini et al., 2005).
In these cases, CGH microarrays generally only allow the detection of deletions, insertions and amplifications of relatively large pieces of DNA, like entire genes.
Nevertheless, even this coarse-grained information can be very helpful in understanding the genetic basis of functional differences between strains of the same bacterial species.
CGH data were used to show that highly variant parts of genomes of 20 Lactobacillus plantarum strains encode proteins that have a major role in the adaptation of these strains to different environments (Molenaar et al., 2005).
CGH arrays can also be used to provide insight into evolutionary processes by analyzing the diversity among strains of the same species (Earl et al., 2007; Rasmussen et al., 2008) or different species (Fukiya et al., 2004).
Current microarray chips can contain several hundreds of thousands of probes, and make it possible to design an array from genomes of several reference strains of the same species at high probe density.
These microbial species-level pangenome arrays overcome the limited variability that is detectable with arrays based on a single reference genome.
Several genotype-calling algorithms (Hua et al., 2007; Plagnol et al., 2007; Teo et al., 2007; Xiao et al., 2007) have been proposed for the interpretation of these data.
However, these algorithms are mainly suited for detecting SNPs or other genomic variations between closely related organisms.
The biological interpretation of pangenome microarrays in terms of the presence and absence of genetic functionalities in strains with unknown sequences poses a problem, because the probes target different homologous genes with various degrees of sequence similarity.
To solve this problem, we have devised the genotypecalling algorithm PanCGH that combines orthology (Fitch, 1970) information about genes with species-level pangenome array data to determine the presence or absence of orthologous genes in bacterial strains.
In this study, we test and apply PanCGH to CGH data of 39 Lactococcus lactis strains to investigate their genotypic variation.
To our knowledge, PanCGH is the first algorithm addressing the problem of deducing gene content from data obtained with CGH microarrays that target the pangenome of a group of relatively diverse microorganisms.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
J.R.Bayjanov et al.2 METHODS 2.1 DNA preparation DNA was prepared from L. lactis strains (Supplementary Table 1) using the QiaAmp DNA Mini Kit (Qiagen GmbH, Hilden, Germany) according to the manufacturers protocol for the isolation of genomic DNA from Grampositive bacteria.
2.2 Microarray design and hybridization data acquisition All genomic, plasmid and single gene or operon DNA sequences (1988 sequences in July 2005, constituting 10.7 Mb) of L. lactis were collected from the NCBI CoreNucleotide database and were deposited in a local database.
This included complete genome sequences of L. lactis strain IL1403 (2.35 Mb, accession number AE005176) and fragments of the genome of strain SK11 (2.43 Mb, Genbank record GI:62464763).
Additionally, draft genome sequences consisting of 547 contigs (2.3 Mb) of L. lactis ssp.
lactis strain KF147 (NIZOB2230) and 961 contigs (2.6 Mb) of L. lactis ssp.
lactis KF282 (NIZOB2244W) were added to this database.
Redundant stretches of DNA were removed from the database, where a stretch of DNA was defined as redundant if it differed from another piece of DNA by at most 2 nt over a window of 100 nt.
For the remaining non-redundant 7 Mb of DNA, a 32-mer tiling design was defined by starting an oligomer approximately every 19 nt, resulting in a total of 386 298 probes.
We also designed 3181 random probes with their sequence absent in the non-redundant 7 Mb of DNA and they were randomly located on the array.
Description of the platform with probe information has been deposited in the Gene Expression Omnibus (www.ncbi.nlm.nih.gov/geo) with an accession number GPL7231.
Array production and DNA hybridization, using fragmented DNA, were performed by NimbleGen Systems Inc. (Madison, WI, USA).
The raw hybridization data, as well as annotations of the sequences, were stored in a custom relational database.
Additionally, raw and normalized hybridization data of 39 L. lactis strains have been deposited in the GEO database with an accession number GSE12638.
The annotations (gene definitions and putative protein function descriptions) were, in case of publicly available sequences, extracted from the GenBank files.
For the draft sequences of L. lactis strains KF147 and KF282, GLIMMER (Salzberg et al., 1998) was used to define the genes and InterProScan (Zdobnov and Apweiler, 2001) was used to generate protein function descriptions.
2.3 Normalization of CGH microarray data Many of the available normalization techniques do not take positional information of probes into account, yet spatial artifacts do contaminate array data.
Such artifacts can be minimized by incorporating positional information of probes into normalization (Khojasteh et al., 2005; Neuvial et al., 2006; Yuan and Irizarry, 2006).
Since a multiplicative noise model works better to minimize spatial artifacts than the additive noise model (Sasik et al., 2002), the normalization process is carried out on a logarithmic scale.
We tested both the loess (Cleveland et al., 1992) and the fields (Fields Development Team, 2006) algorithms to normalize array data in two dimensions (R Development Core Team, 2007).
Both methods fit a smooth 3D surface to the data.
The height of this surface at a specific position represents the local average signal.
For each individual spot, the height of the surface at the position of that spot is then subtracted from its raw signal intensity value.
In order to avoid negative values the overall mean of the smooth fit is added to all signal intensities.
We compared normalized data of both methods and concluded that the fields algorithm was faster and yielded better results.
Therefore, we used the fields algorithm with its default NadarayaWatson kernel for spatial normalization of the array data.
Although this normalization minimizes within-array spatial biases, there is still a difference in overall signal intensity between arrays, which makes it difficult to compare them.
Therefore, after spatial normalization, signal intensities in each array were divided by the median of their distribution.
2.4 The genotype-calling algorithmPanCGH The purpose of this genotype-calling algorithm is to facilitate the biological interpretation of pangenome CGH data by inferring the presence of a gene in a query strain using signal intensities of probes matching an orthologous gene of a reference strain.
Since in a pangenome array several orthologs from different reference strains are represented on the array, the question is generalized to whether the query strain contains a member from a group of orthologous genes.
Therefore, our algorithm also requires ortholog groups as input.
Each ortholog group gi contains the gene identifiers of a single gene or of several orthologous genes from the reference strains (reference orthologs).
The set of all ortholog groups is represented as G = {g1, g2, , gk}.
To predict the presence or absence of a member gene from ortholog group gi in the query strain, one cannot generally simply use the average signal from the set Pi of all probes targeting all genes from gi as an indicator.
Since short 32-mers are used, only probes that almost perfectly match the query gene will display a high fluorescence.
Generally, these are a subset of Pi targeting the most similar reference ortholog.
Therefore, the PanCGH algorithm uses these subsets of probes, and calculates the presence score from that subset of which the largest majority of probes has a high fluorescence (see Fig.1 for an illustration of this principle).
The output of the algorithm is a prediction of the presence or absence in the query strain of a member gene for each of the ortholog groups from the set G. In addition, if it predicts a gene to be Fig.1.
Schematic representation of the PanCGH algorithm for a CGH experiment.
The left panel shows the fluorescence of a query strain to a set of probes (p1 to pn) targeting different reference orthologs (homologous genes from reference strains A, B and C) of an ortholog group gi.
Some probes target several reference orthologs, as shown by the overlap between the probe sets targeting the reference orthologs from strains A and B.
In the right panel, a schematic representation of the calculation of the presence score is shown.
For each reference ortholog, the mode (indicated with a star) is calculated from the distribution of (log) signals of the corresponding probes.
The presence score is the highest of these mode values.
In this case, the presence score is above the threshold and equals the mode of the signals targeting the reference ortholog from strain B.
310 PanCGH: a genotype-calling algorithm present it also predicts which of the reference orthologs is most similar to the gene in the query strain.
The algorithm proceeds as follows.
For each group of orthologs gi in the set G perform Steps 1 to 4.
Step 1: For the set of reference strains {A, , X}, get the sets of probes PA, , PX that match a sequence of a gene in the ortholog group gi.
Construct the union set i= PA PB  PX.
Step 2: Construct the set S of ordered pairs (pk , sk), where pk i and sk is the normalized fluorescence intensity value of probe pk from the CGH array of the query strain.
Step 3: Calculate the presence score Si and the reference strain strainY with the closest homolog for a group gi in the query genome as follows.
For each reference strain Y in {A, , X} compute the mode value mY over signals sk in the sets {(pk , sk)| pk PY } (see below how the mode is calculated).
Define Si as the maximum of the modes, Si = max (mA, , mX ), or if all modes are undefined then Si = NA.
If there is only one strain Y which has a mode mY = Si, then this is the strain with the closest homolog.
Else, if there is more than one strain and only one of them has a mode Si then mY = Si and the strain with the closest homolog is strainY = {Y | my = Si}.
Else, if there is more than one strain and more than one of them has a mode Si then mY = Si and the strain with the closest homolog is strainY = {Y | mY = Si, nY = max (nA, , nX )}, where nY = |PY | is number of probes in a set PY.
Step 4: Assign presence or absence of an ortholog in a query strain for the gene with closest similarity to that of strainY in group gi using the following criteria.
If Si = NA (undefined) then the presence or absence of a member of gi in the query strain cannot be decided from the data, hence presence = NA.
Else, if Si > threshold, the query strain has a gene in an ortholog group gi, hence presence = 1.
The most similar reference ortholog is found in strainY.
(See the results section for a determination of the optimal threshold presence score.)
Else, if Si < threshold, the query strain possesses no gene in ortholog group gi, hence presence = 0.
The mode over the signals sk of a set of ordered pairs {(pk , sk)| pk PY } is calculated as follows.
(1) Define nY as the number of probes in the set PY.
(2) If nY < 10 then the mode is undefined: mode = NA.
(3) Else, bin the signals log(sk) into B = round (log2 (nY +1)) equal sized intervals on the logarithmic scale.
Calculate the frequencies {fj |j {1, , B}} as the number of signals log(sk) in each bin and define mode as the mean of lower and upper limits of the bin with the highest associated frequency.
In the case study of L. lactis CGH arrays, the majority of genes in the falsepositive group had <10 matching probes.
Therefore, a minimum of 10 probes was required.
This is not a strict requirement, and it might differ, depending on the probe size and the size of genes.
The binning procedure in Step 3 of the calculation of the mode is recommended by Sturges (1926).
2.5 Defining orthologous groups of L. lactis genes In order to predict orthology among genes, the genome sequence of three fully sequenced public L. lactis strains (ssp.
lactis IL1403, ssp.
cremoris SK11 and ssp.
cremoris MG1363, accession numbers AE005176, CP000425 and AM406671, respectively) and incomplete genome sequences of two L. lactis strains (ssp.
lactis KF147, ssp.
lactis KF282) isolated from plants were used (Siezen et al., 2008).
The orthology prediction program InParanoid (Remm et al., 2001) was run with default settings to find orthologous genes among the three completely sequenced genomes.
All possible pairwise comparisons between the three genomes were performed.
In cases where inconsistencies regarding bidirectionality of the ortholog relation were found between the pairwise InParanoid predictions, genes were regarded as not being orthologous and each treated as single genes in an orthologous group of size 1.
As incomplete genomes are not suited for bidirectional bestBLAST analyses like InParanoid, the genes of the two incomplete genomes were added by performing a pairwise BLAST analysis of the genes from the incomplete genomes against the three complete genomes.
If a gene in the incomplete genome had a best-BLAST hit with a member of one of the ortholog groups derived from the completely sequenced genomes, this gene was added to that ortholog group.
In cases where best-BLAST hits referred to different ortholog groups, the gene was assigned to a new ortholog group, unless the difference in E-value of the BLAST searches was larger than 1010.
In those cases, the gene was added to the ortholog group of the gene with the hit having the lowest E-value.
We found a total of 4571 ortholog groups of which 1389 groups had a gene in all five L. lactis reference strains.
3 RESULTS 3.1 Microarray design and data normalization Species-level pangenome CGH arrays containing oligonucleotides that target, among others, sequences of four reference strains L. lactis ssp.
lactis IL1403, L. lactis ssp.
cremoris SK11, L. lactis ssp.
lactis KF147 and L. lactis ssp.
lactis KF282 were designed.
During the course of our work, the complete sequences of L. lactis ssp.
cremoris strains SK11 and MG1363 were published (Makarova et al., 2006; Wegmann et al., 2007), and we remapped the probe targets of the existing design on these genomes.
The availability of the complete MG1363 genome sequence also allowed us to use this strain as a test case (query strain) for the PanCGH algorithm.
We analyzed genomic DNA isolated from 39 different L. lactis strains, including the reference strains.
The raw data from the hybridization experiments was biased.
In particular, spatial artifacts on the microarrays were apparent.
Hence, we applied a spatial normalization method to improve the data set.
Visual inspection of the corrected data indicated that the spatial bias was minimized.
To confirm the correctness of this procedure, a hierarchical clustering of strains using either raw or normalized signal intensities of all probes was carried out.
Using the normalized signals, all except one ssp.
cremoris strain clustered together and all ssp.
lactis strains made another cluster, whereas strains from different subspecies clustered together when raw signals were used for clustering.
This shows that, normalized microarray data correspond better with independent experimental criteria, namely those used for subspecies determination.
3.2 Determination of a presence score threshold for the genotype-calling algorithm The pangenome microarray for L. lactis used in this work contains probes for several representatives of orthologous genes in different reference strains (reference orthologs).
To predict whether a representative gene from a group of orthologous genes is present in a query strain with unknown sequence, a presence score for that group is calculated from the normalized fluorescence signals of probes that target the different reference orthologs (Fig.1).
A target sequence is predicted to be present when the presence score lies above a 311 J.R.Bayjanov et al.threshold value.
To define this threshold value, we used CGH data from the reference strains SK11 and IL1403 and calculated presence scores for sets of ortholog groups known to be either present or absent in SK11.
An ideal threshold score value should separate all present from all absent genes.
Supplementary Figure 1 shows that there is a clear separation between present and absent genes, although there is some overlap of the distributions.
The PanCGH algorithm was also applied to strain MG1363.
This is an ideal test strain for the procedure, because its gene content is known from the genome sequence, but just like any of the other query strains, its genome was not used for the design of the array.
The distribution of presence scores was also bimodal for this strain, clearly separating present and absent genes.
To determine the best threshold value, Table 1.
True-positive rate (sensitivity) and true-negative rate (specificity) of the PanCGH genotype-calling algorithm for three L. lactis strains Strain True-positive rate (%) True-negative rate (%) SK11 97.6 90.5 IL1403 97.9 86.2 MG1363 95.4 96.4 we tested all possible threshold values between the minimum and maximum presence score.
As the best possible threshold, we defined the value at which the total error rate (false-positive + false-negative) was minimal.
Supplementary Figure 2 shows that the position of the best possible threshold is 5.5 in an ROC curve (Hanley and McNeil, 1982) for SK11, IL1403 and MG1363.
We estimated the accuracy (Table 1) of the algorithm using the gene annotation of the genome of strain MG1363 at the same threshold.
Ortholog groups predicted as absent in MG1363 separated clearly from the groups predicted to be present.
3.3 Applying the PanCGH algorithm The PanCGH algorithm was applied to hybridization data from 39 L. lactis strains to assign corresponding genotypes to each strain.
Strains were hierarchically clustered based on the presence or absence of genes of ortholog groups in these strains (Fig.2).
The observed clustering is in agreement with a number of independent genotypic and phenotypic observations on the strains (Rademaker et al., 2007; see Supplementary Table 1) supporting the robustness of the method developed in this article.
Most strains group in either of the two large subclusters representing the two different subspecies: L. lactis ssp.
lactis genotype (bottom subcluster) and L. lactis ssp.
cremoris genotype (top subcluster).
In the dendrogram, strains cr em or is g en ot yp e la ct is g en ot yp e Fig.2.
Hierarchical clustering of L. lactis strains based on presence/absence predictions of representatives of 4571 ortholog groups of L. lactis.
The pairwise binary distance was used as a distance metric and clustering was performed using the average linkage agglomeration method (Hastie et al., 2001).
The cluster of strains at the top represents the subspecies cremoris genotype, while the large cluster at the bottom, excluding strains P7266 and P7304, contains strains of subspecies lactis genotype and one strain (LMG8520) of subspecies hordniae phenotype.
In these two clusters 1341 groups from the total of 4571 ortholog groups are present in all strains.
Though strains P7266 and P7304 have subspecies lactis phenotype, they are far apart from other subspecies lactis strains (see explanation in text).
Branches with a solid rectangle are dairy isolates and other strains are isolated from plants.
312 PanCGH: a genotype-calling algorithm P7266 and P7304 formed two distinct branches.
Although these two strains have a L. lactis ssp.
lactis phenotype, they have been shown to be highly different in genotype compared to the L. lactis ssp.
lactis and the L. lactis ssp.
cremoris genotypes (Rademaker et al., 2007).
Further divisions within these two subclusters also reflect functional differences among strains.
For instance, the top subcluster (cremoris genotype) is divided into three branches with 1, 4 and 5 strains; the latter branch contains five strains with both cremoris genotype and phenotype, whereas the other two branches contain strains having a cremoris genotype but displaying a lactis phenotype (Supplementary Table 1).
The large subcluster at the bottom (lactis genotype) is divided into different branches, of which the largest contains 17 strains isolated from plants, while the next largest branch contains mostly strains of dairy origin.
4 DISCUSSION The predictions of the PanCGH algorithm on L. lactis strains show a high true-positive rate (sensitivity) and low to moderate false-positive rate, as shown by tests of the algorithm with CGH data from sequenced strains (Table 1).
Two types of sources that increase total error rate (false-positive + false-negative) can be distinguished: those that are inherent to the CGH method, like noise and limitations of the array platform, and those that are due to external factors.
To the first type belong, for example, errors due to low sequence similarity (leading to poor hybridization) or due to the small size of some genes, as it is difficult to determine the presence or absence of small genes with low numbers of targeting probes.
Errors due to low sequence similarity can be avoided by basing the array design on reference genomes from strains in different branches of the phylogenetic tree of a species.
Errors due to external factors mainly originate from inconsistencies in the ORF calling and annotation of the reference strains or the InParanoid orthology prediction.
A large part of the false-positive and false-negative predictions are due to the latter type of errors.
For example, analysis of the genomes and genome annotations of strains MG1363 and SK11 showed that ORF-calling criteria differ between the two annotations.
Many of the small ORFs defined only in strain SK11 were found by us to be also present in MG1363, but they were not identified as such in the original annotation.
This caused positive gene calls by PanCGH in strain MG1363 for those ORFs that are not identified in the original annotation, but whose sequences are nonetheless present in this strain.
These appear as false calls in the test of PanCGH, but are in fact correct.
Imperfections in the orthology prediction also caused errors.
In particular, for genes with many paralogs, it is difficult to correctly assign orthology relations using automated prediction methods that rely only on gene sequence information (Koonin, 2005; Notebaart et al., 2005).
For example, in strain MG1363, we found that almost half of the apparent false-positive calls concerned hypothetical proteins.
The remaining false-positive calls concerned mainly transporters and transposases, which often have many paralogs (Table 2).
Despite these sources of errors, the PanCGH algorithm has a high accuracy, which shows the robustness of the method.
In order to avoid the errors originating from inconsistencies in ORF-calling and annotation, the same ORF-calling algorithm and definitions should be applied to all reference genomes.
The orthology grouping can also be improved by including additional sources of information Table 2.
Functional categories in ortholog groups with frequent false calls in test strain L. lactis MG1363 Functional category False-positivea (%) False-negativea (%) Hypothetical genes 49.9 60 Transposases 29.2 0 Related to transporters 5.3 7.2 aAs a percentage of the total number of false cells.
from e.g.phylogenetic trees and 3D structures (Francke et al., 2008; Golding and Dean, 1998).
In summary, we have developed a novel genotype-calling algorithmPanCGHfor the biological interpretation of specieslevel pangenome CGH arrays.
In contrast to conventional CGH arrays, these pangenome arrays allow the comparison of strains that are relatively diverse in terms of genome sequence.
Information obtained from sequenced reference strains was incorporated to compare strains not only by signal intensities of individual probes, but also at the level of the inferred genotype, or more specifically, the presence and absence of members of ortholog groups.
The results show that our genotype-calling algorithm predicts a genotype with high accuracy from a species-level pangenome CGH array data, which enables the extraction of relevant biological information for unsequenced strains.
Since the threshold is determined from training data, the PanCGH algorithm can be applied to arrays that target the pangenome of any microorganism.
Currently we are working on biological interpretation of the PanCGH analysis of L. lactis diversity (G.Felis et al., unpublished data).
ACKNOWLEDGEMENTS We thank Giovanna Felis for useful discussions.
Funding: BSIK grant [through the Netherlands Genomics Initiative (NGI)]; BioRange programme [as part of, the Netherlands Bioinformatics Centre (NBIC)]; NGI (as part of the Kluyver Centre for Genomics of Industrial Fermentation).
Conflict of Interest: none declared.
Abstract Exosomes are 40100 nm nano-sized vesicles that are released from many cell types into the extracellular space.
Such vesicles are widely distributed in various body fluids.
Recently, mRNAs and microRNAs (miRNAs) have been identified in exosomes, which can be taken up by neighboring or distant cells and subsequently modulate recipient cells.
This suggests an active sorting mechanism of exosomal miRNAs, since the miRNA profiles of exosomes may differ from those of the parent cells.
Exosomal miRNAs play an important role in disease progression, and can stimulate angiogenesis and facilitate metastasis in cancers.
In this review, we will introduce the origin and the trafficking of exosomes between cells, display current research on the sorting mechanism of exosomal miRNAs, and briefly describe how exosomes and their miRNAs function in recipient cells.
Finally, we will discuss the potential applications of these miRNA-containing vesicles in clinical settings.
Introduction Exosomes, membrane-bound vesicles of 40100 nm in diameter, are present in almost all biological fluids [13].
They are released from most cell types into the extracellular space after fusion with the plasma membrane [13].
Lipids and proteins are the main components of exosome membranes, which are enriched with lipid rafts [13].
In addition to the proteins, various nucleic acids have recently been identified in the exosomal nces and 18 Genomics Proteomics Bioinformatics 13 (2015) 1724 lumen, including mRNAs, microRNAs (miRNAs), and other non-coding RNAs (ncRNAs) [4].
These exosomal RNAs can be taken up by neighboring cells or distant cells when exosomes circulate, and they subsequently modulate recipient cells.
The discovery of their function in genetic exchange between cells has brought increasing attention to exosomes.
MicroRNAs are a class of 1724 nt small, noncoding RNAs, which mediate post-transcriptional gene silencing by binding to the 30-untranslated region (UTR) or open reading frame (ORF) region of target mRNAs [5].
The involvement of miRNAs in many biological activities has been well documented, including cell proliferation, cell differentiation, cell migration, disease initiation, and disease progression [610].
Accumulating evidence has shown that miRNAs can stably exist in body fluids, including saliva [11,12], urine [13], breast milk [14], and blood [11,15,16].
In addition to being packed into exosomes or microvesicles, extracellular miRNAs can be loaded into high-density lipoprotein (HDL) [17,18], or bound by AGO2 protein outside of vesicles [16].
All these three modes of action protect miRNAs from degradation and guarantee their stability.
Given the transportability of vesicles, the role of miRNAs in exosomes is gaining increasing attention.
Conveying information via circulating vesicles is deemed to be the third way of intercellular communication that is as essential as the cell-to-cell contact-dependent signaling and signaling via transfer of soluble molecules [19,20].
Formation and secretion of exosomes require enzymes [21,22] and ATP [23], and the miRNA and mRNA profiles of exosomes differ from those of the parent cells [24].
Therefore, cells may possess an active selecting mechanism for exosomes and their cargos.
Besides, functions of the transferred exosomal molecular constituents in the recipient cells are under investigation.
Hereby, this review will concisely introduce the origin and trafficking of exosomes and discuss the sorting mechanism and function of exosomal miRNAs.
Formation and secretion of exosomes Exosomes were first discovered by Pan and Johnstone in 1983 [25].
They reported that the release of transferrin receptors into the extracellular space during the maturation of sheep reticulocytes was associated with a type of small vesicle [23,25].
In 1989, Johnstone defined such functional vesicles as exosomes [26].
To date, a series of extracellular vesicles have been described [27].
However, in the last three decades, no unified terminology for extracellular vesicles has been presented.
The definition for such extracellular vesicles named as microvesicles, exosomes, and microparticles remains confusing among different reports [2830].
Now, according to the way of vesicular secretion from cells, extracellular vesicles can be grouped into two general classes.
One of these classes is known as microvesicles, which are directly shed from the cell membrane.
The other is known as exosomes, which are released by exocytosis when multivesicular bodies (MVBs) fuse with the plasma membrane [31].
Here, we mainly focus on the second group of vesicles, i.e., exosomes.
Exosomes can be revealed using transmission microscopy, possessing a cup-shaped morphology after negative staining [13].
These vesicles can be concentrated in the 1.101.21 g/ml section of a sucrose density gradient [13].
They can also be identified by the presence of proteins common to most exosomes, such as the tetraspanin proteins CD63, CD9, and CD81 [13].
As mentioned above, exosomes are originally formed by endocytosis.
First, the cell membrane is internalized to produce endosomes.
Subsequently, many small vesicles are formed inside the endosome by invaginating parts of the endosome membranes.
Such endosomes are called MVBs.
Finally, the MVBs fuse with the cell membrane and release the intraluminal endosomal vesicles into the extracellular space to become exosomes [32].
The regulatory molecules involved in the release of exosomes were identified by Ostrowski and colleagues, who observed that Rab27a and Rab27b were associated with exosome secretion.
Knockdown of Rab27 or their effectors, SYTL4 and EXPH5, could inhibit secretion of exosomes in HeLa cells [33].
Moreover, Yu et al.discovered that both the tumor repressor protein p53 and its downstream effector TSAP6 could enhance exosome production [34].
Baietti et al.found that syndecan-syntenin interacted directly with ALIX protein via Leu-Tyr-Pro-X(n)-Leu motif to support the intraluminal budding of endosomal membranes, which is an important step in exosome formation [35].
All of these studies indicate that a set of molecules act as a regulatory network and are responsible for the formation and secretion of exosomes in parent cells.
The trafficking of exosomes Exosomes present in body fluids play an important role in exchanging information between cells.
In general, there are three mechanisms of interaction between exosomes and their recipient cells.
First, the transmembrane proteins of exosomes directly interact with the signaling receptors of target cells [36].
Second, the exosomes fuse with the plasma membrane of recipient cells and deliver their content into the cytosol [37].
Third, the exosomes are internalized into the recipient cells and have two fates.
In one case, some engulfed exosomes may merge into endosomes and undergo transcytosis, which will move exosomes across the recipient cells and release them into neighboring cells.
In the other case, endosomes fused from engulfed exosomes will mature into lysosomes and undergo degradation [37,38].
Some recent studies have reported the factors influencing internalization of exosomes in recipient cells.
Koumangoye et al.observed that disruption of exosomal lipid rafts resulted in the inhibition of internalization of exosomes and that annexins, which are related to cell adhesion and growth, were essential for the uptake of exosomes in the breast carcinoma cell line BT-549 [39].
Escrevente et al.described a decrease in exosome uptake after the ovarian carcinoma cell line SKOV3 and its derived exosomes were treated with protease K, which indicated that the proteins mediating exosome internalization are presented on the surface of both the cells and the exosomes [40].
However, the detailed mechanism of exosome internalization is still not well understood.
The function of exosomes Exosomes can be released from many cell types, such as blood cells, endothelial cells, immunocytes, platelets, and smooth muscle cells [4143].
It is believed that exosomes can regulate Zhang J et al/ Trafficking, Sorting, and Function of Exosomes/miRNAs 19 the bioactivities of recipient cells by the transportation of lipids, proteins, and nucleic acids while circulating in the extracellular space.
Several reports have shown that exosomes play important roles in immune response, tumor progression, and neurodegenerative disorders.
Esther et al.reported that activated T cells could recruit dendritic cell (DC)-derived exosomes that contain major histocompatibility complex (MHC) class II to down-regulate the immune response during interaction of T cells and DCs [44].
Exosomes derived from platelets that were treated with thrombin and collagen stimulated proliferation and increased chemoinvasion in the lung adenocarcinoma cell line A549 [45].
Exosomes derived from SGC7901 promoted the proliferation of SGC7901 and another gastric cancer cell line, BGC823 [46].
In addition, CD147-positive exosomes derived from epithelial ovarian cancer cells promoted angiogenesis in endothelial cells in vitro [47].
Interestingly, Webber et al.incubated exosomes derived from a mesothelioma cell line, a prostate cancer cell line, a bladder cancer cell line, a colorectal cancer cell line, and a breast cancer cell line with primary fibroblasts in vitro, and found that fibroblasts could be transformed into myofibroblasts [48].
A similar phenomenon was also observed by Cho et al., who described that tumor-derived exosomes converted mesenchymal stem cells within the stroma of the tumor tissue into cancer-associated myofibroblasts [49].
Although the function of exosomes has been documented in the aforementioned studies, it remains an open question which specific class of molecules contained in exosomes influences the recipient cells.
The sorting mechanism for exosomal miRNAs As described above, a wide variety of molecules are contained in exosomes, including proteins, lipids, DNAs, mRNAs, and miRNAs, which are recorded in the ExoCarta database [50].
Among these molecules, miRNAs have attracted most attention, due to their regulatory roles in gene expression.
Goldie et al.demonstrated that, among small RNAs, the proportion of miRNA is higher in exosomes than in their parent cells [51].
As some profiling studies have shown, miRNAs are not randomly incorporated into exosomes.
Guduric-Fuchs et al.analyzed miRNA expression levels in a variety of cell lines and their respective derived exosomes, and found that a subset of miRNAs (e.g., miR-150, miR-142-3p, and miR-451) preferentially enter exosomes [52].
Similarly, Ohshima et al.compared the expression levels of let-7 miRNA family members in exosomes derived from the gastric cancer cell line AZ-P7a with those from other cancer cell lines, including the lung cancer cell line SBC-3/DMS-35/NCI-H69, the colorectal cancer cell line SW480/SW620, and the stomach cancer cell line AZ-521.
As a result, they found that members of the let7 miRNA family are abundant in exosomes derived from AZ-P7a, but are less abundant in exosomes derived from other cancer cells [53].
Moreover, some reports have shown that exosomal miRNA expression levels are altered under different physiological conditions.
The level of miR-21 was lower in exosomes from the serum of healthy donors than those glioblastoma patients [29].
Levels of let-7f, miR-20b, and miR-30e-3p were lower in vesicles from the plasma of nonsmall-cell lung carcinoma patients than normal controls [30].
Different levels of eight exosomal miRNAs, including miR21 and miR141, were also found between benign tumors and ovarian cancers [54].
All these studies show that parent cells possess a sorting mechanism that guides specific intracellular miRNAs to enter exosomes.
According to previous studies, there exists a class of miRNAs that are preferentially sorted into exosomes, such as miR-320 and miR-150.
Members of the miR-320 family are widely distributed in exosomes derived from normal tissue and tumors [29,41,52,55,56].
miR-150 is highly expressed in exosomes derived from the HEK293T cell line, peripheral blood of tumor patients, colony-stimulating factor 1 (CSF1)-induced bone marrow-derived macrophages, and the serum of colon cancer patients [52,54,55,57,58].
In addition, some miRNAs, miR-451 for example, are highly expressed in exosomes derived from normal cells, such as the HMC-1 cell line, the HEK293T cell line, primary T lymphocytes, and Epstein Barr virus-transformed lymphoblastoid B-cells [52,5961].
Other miRNAs, such as miR-214 and miR-155, are enriched in exosomes derived from tumor cell lines or peripheral blood from cancer patients [54,58,62].
Based on current research, there are four potential modes for sorting of miRNAs into exosomes, although the underlying mechanisms remain largely unclear.
These include: 1) The neural sphingomyelinase 2 (nSMase2)-dependent pathway.
nSMase2 is the first molecule reported to be related to miRNA secretion into exosomes.
Kosaka et al.found that overexpression of nSMase2 increased the number of exosomal miRNAs, and conversely inhibition of nSMase2 expression reduced the number of exosomal miRNAs [22].
2) The miRNA motif and sumoylated heterogeneous nuclear ribonucleoproteins (hnRNPs)-dependent pathway.
Villarroya-Beltri et al.discovered that sumoylated hnRNPA2B1 could recognize the GGAG motif in the 30 portion of miRNA sequences and cause specific miRNAs to be packed into exosomes [59].
Similarly, another two hnRNP family proteins, hnRNPA1 and hnRNPC, can also bind to exosomal miRNAs, suggesting that they might be candidates for miRNA sorting as well.
However, no binding motifs have been identified yet [59].
3) The 30-end of the miRNA sequence-dependent pathway.
Koppers-Lalic et al.discovered that the 30 ends of uridylated endogenous miRNAs were mainly presented in exosomes derived from B cells or urine, whereas the 30 ends of adenylated endogenous miRNAs were mainly presented in B cells [60].
The above two selection modes commonly indicate that the 30 portion or the 30 end of the miRNA sequence contains a critical sorting signal.
4) The miRNA induced silencing complex (miRISC)-related pathway.
It is well known that mature miRNAs can interact with assembly proteins to form a complex called miRISC.
The main components of miRISC include miRNA, miRNA-repressible mRNA, GW182, and AGO2.
The AGO2 protein in humans, which prefers to bind to U or A at the 50 end of miRNAs, plays an important role in mediating mRNA:miRNA formation and the consequent translational repression or degradation of the mRNA molecule [63].
Recent studies recognized a possible correlation between AGO2 and exosomal miRNA sorting.
In exosomal protein analyses, AGO2 has sometimes been identified by using mass spectrometry (MS) or Western blotting [51,64].
GuduricFuchs et al.discovered that knockout of AGO2 could decrease the types or abundance of the preferentially-exported miRNAs, such as miR-451, miR-150, and miR-142-3p, in HEK293T-derived exosomes [52].
Other evidence has also supported a relationship between miRISC and exosomal miRNA 20 Genomics Proteomics Bioinformatics 13 (2015) 1724 sorting.
First, the main components of miRISC were found to be co-localized with MVBs [65].
Second, blockage of the turnover of MVBs into lysosomes could lead to the over-accumulation of miRISCs, whereas blockage of the formation of MVBs resulted in the loss of miRISCs [66].
Third, the changes in miRNA-repressible targets levels that occur in response to cell activation may cause miRNA sorting to exosomes, partially by differentially engaging them at the sites of miRNA activity (miRISCs) and exosome biogenesis (MVBs) [55].
In summary, specific sequences present in certain miRNAs may guide their incorporation into exosomes, whereas some enzymes or other proteins may control sorting of exosomal miRNAs as well, in a miRNA sequence-independent fashion (Figure 1).
The function of exosomal miRNAs Since Valadi et al.[24] described that miRNAs could be transferred between cells via exosomes, more similar observations AAA 7m G DNA AA pri-miRNA pre-miRNA pre-miRNA Drosha complex exp ort in5 Dicer complex Multivesicular body Nucleus nSMase2 Donor cell ?
(U > A) 2 3 1 4  hnRNP ?
Figure 1 The sorting mechanism for exosomal microRNAs In animals, microRNA (miRNA) genes are transcribed into primary m form precursor miRNAs (pre-miRNAs), which are exported into the digestion by the Dicer complex to become mature miRNAs.
Mature nSMase2-dependent pathway; (2) miRNA motif and sumoylated hnR recognizes the GGAG motif in the 30 portion of the miRNA sequenc miRNA sequence-dependent pathway; miRNAs that are preferentiall end.
(4) The miRISC-related pathway.
miRISCs co-localize with th components, such as AGO2 protein and miRNA-targeted mRNA, ar have been reported [6769].
The miRNAs in cell-released exosomes can circulate with the associated vehicles to reach neighboring cells and distant cells.
After being delivered into acceptor cells, exosomal miRNAs play functional roles.
Although it is difficult to completely exclude the effects of other exosomal cargos on recipient cells, miRNAs are considered the key functional elements.
The functions of exosomal miRNAs can be generally classified into two types.
One is the conventional function, i.e., miRNAs perform negative regulation and confer characteristic changes in the expression levels of target genes.
For example, exosomal miR-105 released from the breast cancer cell lines MCF-10A and MDA-MB-231 reduced ZO-1 gene expression in endothelial cells and promoted metastases to the lung and brain [70].
Exosomal miR214, derived from the human microvascular endothelial cell line HMEC-1, stimulated migration and angiogenesis in neighboring HMEC-1 cells [71].
Exosomal miR-92a, derived from K562 cells, significantly reduced the expression of integrin a5 in the human umbilical vein endothelial (HUVEC) cells and enhanced endothelial cell migration and tube formation [72].
A Ago2 GW182 GW182 Ago2 Recipient cell ?
?
iRNAs (pri-miRNAs), and processed by the Drosha complex to cytoplasm by the exportin5 complex.
The pre-miRNAs undergo miRNAs are sorted into exosomes via four potential modes: (1) NPs-dependent pathway; The sumoylated hnRNP family protein e and guides specific miRNAs to be packed into exosomes.
(3) 30 y sorted into exosomes have more poly(U) than poly(A) at the 30 e sites of exosome biogenesis (multivesicular bodies) and their e correlated with sorting of miRNAs into exosomes.
Zhang J et al/ Trafficking, Sorting, and Function of Exosomes/miRNAs 21 The other one is a novel function that has been identified in some miRNAs when they are studied as exosomal miRNAs rather than intracellular miRNAs.
Exosomal miR-21 and miR-29a, in addition to the classic role of targeting mRNA, were first discovered to have the capacity to act as ligands that bind to toll-like receptors (TLRs) and activate immune cells [73].
This study uncovered an entirely new function of miRNAs.
To further understand this novel function of miRNAs, more investigations are worthwhile.
Notably, current functional studies of exosomal miRNAs have some limitations.
First, diverse methods are used for exosome isolation.
Exosomes can be enriched from cell culture media by ultracentrifugation, density gradient separation, immunoaffinity capture, size exclusion chromatography, and ExoQuick Precipitation (System Biosciences, USA).
Use of different exosome purification strategies could slightly affect exosomal contents, including proteins and miRNAs [7476].
Second, the large number of variable miRNAs carried by exosomes may regulate many different signaling pathways, and will generate integral effects on recipient cells.
Therefore, it is difficult to gain a thorough understanding of the functions of exosomal miRNAs.
According to studies of miRNA sorting mechanisms, certain miRNAs may be classified by portions of their sequences, and the functions of each group may be elucidated separately.
Third, it is difficult to identify exosomal miRNAs in a single exosome or to measure the amount of a given miRNA carried by an exosome when it is present in low abundance.
Chevillet et al.quantified the number of exosomes by a NanoSight instrument (Malvern, UK) and the Table 1 Exosomal microRNAs capable of distinguishing different path Sample description Isolation strategy Tumor cells from glioblastoma patients at passage 115; serum from glioblastoma patients and controls Ultracentrifugation Plasma from NSCLC patients (n= 28 for test, n= 78 for validation); plasma from controls (n= 20 for test, n= 48 for validation) Immunobead (EpCAM) Serum from malignant tumor patients (n= 50); serum from benign tumor patients (n= 10); serum from controls (n= 10).
Immunobead (EpCAM) and ultracentrifugation Plasma from lung adenocarcinoma (n= 27); plasma from control (n= 9).
Size exclusion chromatography and immunobead (EpCAM) Note: NSCLC, non-small-cell lung carcinoma; EpCAM, epithelial cell adh number of miRNA molecules in an exosome collection using a real-time PCR-based absolute quantification method.
They found that, on average, most exosomes did not harbor many copies of miRNA molecule [77].
According to this study, accumulation of exosomal miRNAs in recipient cells is necessary for miRNA-based communication.
More sophisticated techniques and methods need to be developed to enrich the subpopulation of miRNA-rich exosomes, and functionally sufficient quantities of exosomal miRNAs need to be determined.
Applications of exosomes and exosomal miRNAs Exosomal miRNAs can stably exist in the blood, urine, and other body fluids of patients, and exosomes can reflect their tissue or cell of origin by the presence of specific surface proteins [13].
Furthermore, the amount and composition of exosomal miRNAs differ between patients with disease and healthy individuals.
Thus, exosomal miRNAs show potential for use as noninvasive biomarkers to indicate disease states.
Several previous studies have profiled exosomal miRNAs in different samples.
It is of note that some exosomal miRNAs can be used to aid in clinical diagnosis (Table 1) [29,30,54,62].
For example, a set of exosomal miRNAs, including let-7a, miR-1229, miR-1246, miR-150, miR-21, miR-223, and miR-23a, can be used as the diagnostic biomarker of colorectal cancer [57].
Another set, miR-1290 and miR-375, can be used as the prognostic marker in castration-resistant prostate cancer [56].
ological conditions in patients Quantification method Findings Ref.
Quantitative PCR 11 miRNAs (miR-15b, miR-16, miR196, miR-21, miR-26a, miR-27a, miR-92, miR-93, miR-320, miR-20, and let-7a) were known to be abundant in gliomas, able to be detected in their derived microvesicles; the level of exosomal miR-21 was elevated in serum microvesicles compared with controls [29] Quantitative PCR The levels of exosomal let-7f and/or miR-30e-3p in NSCLC patients can distinguish patients with resectable tumors from those with nonresectable tumors [30] Microarray The levels of 8 exosomal miRNAs (miR-21, miR141, miR-200a, miR200b, miR-200c, miR-203, miR-205, and miR-214) from malignant tumor are significantly distinct from those observed in benign tumor; exosomal miRNAs could not be detected in normal controls [54] Microarray The levels of 12 exosomal miRNAs (miR-17-3p, miR-21, miR-106a, miR146, miR155, miR-191, miR-192, miR-203, miR-205, miR-210, miR212, and miR-214) are significantly different between patients and controls [62] esion molecule.
22 Genomics Proteomics Bioinformatics 13 (2015) 1724 Besides the endogenous miRNAs, exogenous miRNAs can also be sorted into exosomes, which has been experimentally confirmed by Pegtel et al.[78] and Meckes et al.[79], who observed that human tumor viruses can exploit exosomes as delivery vectors to transfer their exogenous miRNAs to other non-infected cells [78,79].
Hence, exogenous small RNAs have also been transferred by exosomes by mimicking the molecular mechanism of endogenous miRNAs transportation.
RNA interference (RNAi) has been applied to gene therapy [80,81].
The findings on the employment of exosomes by the exogenous miRNAs suggest that combination of exosomes with RNAi technology is a promising method for gene therapy and this idea has been supported by several lines of evidence.
For instance, Wahlgren et al.used plasma exosomes as gene delivery platforms to transfer exogenous siRNAs to monocytes and lymphocytes, which resulted in the silencing of the target MAPK gene [82].
In addition, Shtam et al.introduced exogenous siRNA into exosomes derived from HeLa cells, and used these transfected exosomes to knock down the target gene RAD51 in the recipient cells [83].
Moreover, the effect of exosome-siRNA gene silencing has also been validated in a mouse model [84].
It is therefore possible to use exosomes to modulate target genes for therapeutic purposes, but a great deal of additional research will be required to develop these therapies for clinical use.
Perspective Although exosomes were first identified in the 1980s, studies on exosomes have been increasing remarkably during the last five years, especially following the discovery of functional mRNAs and miRNAs in exosomes.
Exosomes play a key role in the process of cell-to-cell communication and influence the phenotype of recipient cells.
However, exosome study is still in its infancy.
People may want to know whether other noncoding RNAs such as long non coding RNAs could be present in exosomes, and whether they get involved in target gene regulation in recipient cells.
With the discovery that exosomal miRNAs can function as ligands, a new field in exosome study has been opened up.
It remains controversial whether the mechanism for packing of bioactive molecules into exosomes and secreting them into the extracellular space is an active or a passive process.
More investigations on this matter will be warranted.
Nonetheless, the most exciting but challenging application will be to utilize exosomes and their cargo as a clinical tool to diagnose and monitor disease, perhaps even for gene therapy, but much work remains to achieve this goal.
Competing interests The authors declared that there are no competing interests.
Acknowledgments This work was supported by the Projects of International Cooperation and Exchanges from the National Natural Science Foundation of China (Grant No.
31161120358), the National Basic Research Program from the Ministry of Science and Technology of China (973 program; Grant Nos.
20111CB510106 and 2015CB910603), the Open Project of State Key Laboratory of Biomembrane and Membrane Biotechnology, and the Scientific Research Foundation for Returned Scholars from the Ministry of Education of China.
ML was supported by National Natural Science Foundation of China (Grant No.
31400741).
ABSTRACT Motivation: Statistical assessment of cis-regulatory modules (CRMs) is a crucial task in computational biology.
Usually, one concludes from exceptional co-occurrences of DNA motifs that the corresponding transcription factors (TFs) are cooperative.
However, similar DNA motifs tend to co-occur in random sequences due to high probability of overlapping occurrences.
Therefore, it is important to consider similarity of DNA motifs in the statistical assessment.
Results: Based on previous work, we propose to adjust the window size for co-occurrence detection.
Using the derived approximation, one obtains different window sizes for different sets of DNA motifs depending on their similarities.
This ensures that the probability of co-occurrences in random sequences are equal.
Applying the approach to selected similar and dissimilar DNA motifs from human TFs shows the necessity of adjustment and confirms the accuracy of the approximation by comparison to simulated data.
Furthermore, it becomes clear that approaches ignoring similarities strongly underestimate P-values for cooperativity of TFs with similar DNA motifs.
In addition, the approach is extended to deal with overlapping windows.
We derive ChenStein error bounds for the approximation.
Comparing the error bounds for similar and dissimilar DNA motifs shows that the approximation for similar DNA motifs yields large bounds.
Hence, one has to be careful using overlapping windows.
Based on the error bounds, one can precompute the approximation errors and select an appropriate overlap scheme before running the analysis.
Availability: Software to perform the calculation for pairs of position frequency matrices (PFMs) is available at http://mosta.molgen.mpg.
de as well as C++ source code for downloading.
Contact: utz.pape@molgen.mpg.de 1 INTRODUCTION An important goal in computational biology is to decipher the transcriptional regulation of genes.
Interaction of nearby transcription factors (TFs) initiate or inhibit transcription of a gene (Arnone and Davidson, 1997; Fickett, 1996; Yuh et al., 1998).
They mainly bind to DNA upstream of genes by recognizing TF-specific sequences which can be summarized by a DNA motif.
TFs which combinatorially regulate genes are called cooperative.
To whom correspondence should be addressed.
Such TFs are assumed to have exceptionally many DNA motif occurrences in proximity to each other.
Thus, a significant number of co-occurrences of the corresponding DNA motifs can be used to assess the strength of cooperativity.
The set of DNA motif occurrences upstream of a gene is called a cis-regulatory module (CRM; Berman et al., 2002).
A CRM is a sequence region with dense clusters of DNA motif occurrences as demonstrated experimentally (Clyde et al., 2003; Harbison et al., 2004) and computationally (Lifanov et al., 2003; Wagner, 1999).
In general, they can be divided into CRMs bound by the same TF, homotypic CRMs, and heterotypic CRMs bound by different TFs (Brown et al., 2002; Wagner, 1997).
Homotypic CRMs are often detected using a scoring function (Papatsenko et al., 2002; Wagner, 1999), e.g.FLYENHANCER (Markstein et al., 2002), SCORE (Rebeiz et al., 2002) and CLUSTER (Lifanov et al., 2003).
Common programs to find heterotypic CRMs are ClusterDraw (Papatsenko, 2007), ModuleSearcher (Aerts et al., 2003), MCAST (Bailey and Noble, 2003), eCISANALYST (Berman et al., 2004), Cister (Frith et al., 2001), Cluster-Buster (Frith et al., 2003) and TargetExplorer (Sosinsky et al., 2003).
CRMs can be detected using ab initio discovery of new (e.g.Gupta and Liu, 2005; Zhou and Wong, 2004) or based on known DNA motifs.
We assume that the DNA motifs are known.
Many approaches have been proposed integrating different kinds of data for improving CRM prediction (Manke et al., 2005; Pilpel et al., 2001; Yu et al., 2006).
Since the main characteristic of CRMs is their high local density of DNA motif occurrences, one essential data source is always the DNA sequence annotated with DNA motif occurrences.
Here, we focus on DNA motifs represented by position frequency matrices (PFMs; Stormo, 2000).
Other approaches compute the cooperative binding energy of multiple sites of TFs (Frith et al., 2004; GuhaThakurta and Stormo, 2001) using thermodynamical models.
Based on the PFM representation, GuhaThakurta (2006) classifies the approaches to find CRMs into hidden Markov models (Crowley et al., 1997; Frith et al., 2001) and occurrence-based approaches.
We further divide the occurrence-based approaches into two categories (Fig.1): (i) relying on small distances between DNA motif occurrences (Klingenhoff et al., 1999; Wagner, 1999; Wasserman and Fickett, 1998) and (ii) based on co-occurrences of DNA motifs in a small window (Berman et al., 2002; Bleser et al., 2007; Frith et al., 2002; Hannenhalli and Levy, 2002; Klein and Vingron, 2007).
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:17 4/8/2009 Bioinformatics-btp143.tex] Page: 2104 21032109 U.J.Pape et al.Fig.1.
Two different approaches to detect CRMs: upper panel illustrates approaches which are based on short distances between DNA motif occurrences.
Lower panel visualizes detection of CRM considering occurrences in windows.
The method to compute statistical significance is a difficult problem (Krivan, 2004) and can be solved by: (i) assuming position independence of occurrences (Frith et al., 2002; Wagner, 1999; Wasserman and Fickett, 1998), (ii) employing randomizations (Bleser et al., 2007; Hannenhalli and Levy, 2002) or (iii) exact calculation (Boeva et al., 2007).
The position independence of binding site occurrences is strongly violated for (self-)similar PFMs (Pape et al., 2008a; Wagner, 1999).
The significance calculation based on randomization also encounters problems for similar PFMs, hence, they are usually removed from the analysis (Hannenhalli and Levy, 2002).
In addition, incorporating the complementary strand introduces further dependencies and worsens the results.
The exact calculation (Boeva et al., 2007) based on an AhoCorasick automaton (Aho and Corasick, 1975) has high computational complexity such that solutions for longer PFMs are hard to obtain.
In Pape and Vingron (2008), we propose a fast and accurate approximation for the significance calculation of CRMs circumventing the position independence assumption, incorporating similarity between PFMs, and incorporating the complementary strand.
We define a CRM to be a sequence region, which we call a window, of defined length where all DNA motifs of a given set have at least one occurrence.
This is called the co-occurrence event.
Thus, we assume that TFs only interact if their motifs occur within the window size.
Although long-range interactions are reported, especially in higher organisms (e.g.Yoshida et al., 1999), it is impossible to predict such interactions on the sequence level due to high stochastic noise.
In fact, the larger the window the higher the probability for the co-occurrence event to be in a random sequence.
Hence, the length of the window has to be small to get statistically significant CRMs.
Using TransCompel (Matys et al., 2006) to get a first idea of a good choice for the window size shows that 98% of the 375 known vertebrate composite elements have a distance of less than 100 bp (Klein and Vingron, 2007).
We compute the probability of a CRM which is the probability of the co-occurrence event in a random sequence given a window length.
Considering the overlap probabilities between the occurrences of the TF binding sites, we capture the (self-)similarities of the PFMs and most of the dependencies introduced by the complementary strand.
In this article, we extend the approach such that one can compute the length of the window for a specific set of DNA motifs by defining the probability of the co-occurrence event as parameter.
We focus on pairs of DNA motifs.
Intuitively, the results show that for similar PFMs the length of the window is smaller than for dissimilar PFMs given the same probability.
Due to this computation, one can adjust the window size based on the similarity of the PFMs.
Hence, by using different window sizes for sets of PFMs sharing different degrees of 1.
2.
4 of 7 windows with co-occurrences 3.
0.3 0.2 0.1 0.0 1 2 3 4 5 6 70 Number of Co-Occurrences P ro ba bi lit y Fig.2.
Proposed algorithm to compute cooperativity of a pair of TFs: first, divide sequence into windows.
Second, count windows containing at least one hit of each TF.
Compute corresponding count distribution under random sequence model to obtain P-value for cooperativity.
similarity between their PFMs, one can obtain equal co-occurrence probabilities for all sets.
Therefore, follow-up analyses do not have to consider the similarity between PFMs anymore.
Otherwise, similar PFMs would yield more co-occurrence events than dissimilar PFMs just due to their similarity.
This would generally bias statistics based on the number of co-occurrence events.
Hence, window size adjustment by considering the similarity of PFMs is necessary.
We provide strong evidence for this by comparing our approach with an approach ignoring similarities based on simulated data.
Furthermore, one is interested in whether specific TFs are generally involved in the same CRMs.
We call this cooperativity of TFs.
In Pape and Vingron (2008), we also show how to compute the significance of cooperativity.
The sequence is divided into equal-sized non-overlapping windows covering the whole sequence (Fig.2).
Based on the count distribution, we compute a P-value for the number of observed CRMs (windows with the co-occurrence event).
In case of non-overlapping windows the count distribution is exact except for the approximations in the calculation of the co-occurrence event.
The accuracy of the approximation is shown by comparison with a simulation study (Pape and Vingron, 2008).
In contrast, overlapping windows introduce further dependencies.
Therefore, we show in this article how to compute error bounds using the ChenStein method.
Applying these error bounds to selected sets of PFMs show that similar PFMs retrieve high approximation errors due to stronger dependencies between overlapping windows.
Again, these results are supported by a simulation.
In the next section, we first show that the approach can generally be extended to sets of PFMs.
Afterwards, we focus on pairs of PFMs for simplicity.
There, we derive formulae for the window length and explicitly state the ChenStein error bounds.
Furthermore, we introduce the independence approach ignoring similarities and describe the dataset of human TFs and how the PFMs are selected.
Section 3 applies the formulae for window length and the Chen Stein error bounds to selected pairs of TFs and compares the new approach with the independence approach based on simulated data.
2 METHODS We assume that each TF is given by a PFM.
For each position j of a sequence, we have an indicator random variable Yj(A) which is 1 if the summed score at this position reaches the threshold.
We denote the random variables for the complementary strand by a prime, e.g.Y j (A).
The threshold can be controlled by the type I error A :=P(Yj(A)=1)=P(Y j (A)=1) in a random sequence.
The model for the random sequence is assumed to be an i.i.d.
sequence 2104 [13:17 4/8/2009 Bioinformatics-btp143.tex] Page: 2105 21032109 Cooperativity of PFMs defined by the GC content.
We assume this simple background model, since it causes the distribution of hits on both strands to be equal.
As stated before, a CRM is a window of given length w with at least one hit for TF A and one hit of TF B.
We split up the calculation of this co-occurrence event into three parts: Let Nw(A)=wj=1(Yj(A)+Y j (A)) denote the random variable for the number of hits of TF A in a random sequence of length w where we allow hits overlapping the boundary of the window.
Now, we can state the probability p(w) of a CRM in a given window of length w by p(w) :=P(Nw(A)>0,Nw(B)>0).
Calculation using the inclusionexclusion formula results in p(w) = 1P(Nw(A)=0)P(Nw(B)=0) +P(Nw(A)=0,Nw(B)=0).
(1) Applying transformations as described in Pape and Vingron (2008) yields for the probability of the co-occurrence event p(w)1erAw erB w + erAB w where rA and rB correspond to rates for the occurrence of TF A and B, respectively, and rAB contains the joint rate of A and B considering overlaps.
2.1 Sets of PFMs So far, we derived formulae to compute the co-occurrence probability for pairs of PFMs.
Here, we briefly extend the approach to deal with a set T of PFMs with size |T |.
Equation (1) reduces the calculation of the co-occurrence probability to compute the (joint) events of zero counts of the PFMs.
For a set of TFs, we apply the inclusionexclusion formula on the count variables of all PFMs: P(min TT Nw(T )>0) = 1 TT P(Nw(T )=0) + TT UT \T P(Nw(T )+Nw(U)=0) Hence, one only has to compute the probabilities for zero counts for all subsets U of the power set of T. Calculation of these probabilities is straightforward using the same technique as described in Pape and Vingron (2008) and are given in Pape (2008).
2.2 Calculate window size From now on, we only consider pairs of PFMs although extension to sets of PFMs is possible.
In practice, the probability for the co-occurrence event is given as parameter and the window size has to be computed.
In this case, we have to find the roots of 1erAw erB w +erAB w p. Using the Newton approach, we obtain following recursion starting from a chosen initial value w0: wi+1 =wi 1e rAwi erB wi +erAB wi p rAerAwi +rBerB wi rABerAB wi.
In case one requires a closed formula, one can also apply a Taylor expansion to the formula for the co-occurrence probability.
For example, the formula for a second-order expansion which already gives accurate results for small p is given with a=rAB rA rB and b=r2AB r2A r2B by w(p)= a b + (a b )2 + 2p b.
2.3 P-value for cooperativity Previously, we have shown how to compute the co-occurrence probability p(w) in a given window.
To compute cooperativity, we suggest to decompose the sequence into non-overlapping windows of equal size and count the number x of CRMs (windows with the co-occurrence event).
We define for each window i a Bernoulli random variable Wi which is 1 if the corresponding window contains a co-occurrence event and otherwise 0.
Denoting the number of windows by m=n/w with sequence length equal to n, we define W :=mi=1 Wi.
The number W of windows with co-occurrence events is distributed as Poisson P() with =p(w)m if p(w)0 and m. 2.4 Bounds for overlapping windows Considering overlapping windows necessitate the step size s as parameter, the number m of windows becomes m=n/sw+1.
We assume that n,s,w are chosen such that m,n,s,w are positive integers and s<w< 12 n. Obviously, overlapping windows are dependent on each other.
In this case, we can still use a Binomial or Poisson distribution but the dependencies lead to an error in the approximation.
Using the ChenStein method (Chen, 1975), the error can be quantified.
The quantification is done in terms of the total variation distance.
Let U and V be any two random processes with values in the same space E, then the total variation distance between their distributions [denoted by L()] is dTV ( L(U),L(V ) )= sup DE |P(U D)P(V D)| where D is assumed to be measurable.
Here, we focus on the Poisson approximation since it obtains slightly better error bounds.
Thus, we calculate the bound for dTV(L(W ),P()).
Let I :={i :0< im} denote the index set of the Bernoulli variables.
The main idea is to define for each Bernoulli variable Wi a neighborhood set Bi I of random variables which have strong dependencies with Wi.
We also require iBi.
In our case, there are only local dependencies since only overlapping windows are dependent on each other.
Therefore, we capture all dependencies in the sets Bi which means that for each window i the set Bi contains the index i and the indices of overlapping windows to the left and to the right.
Hence, we obtain the bound derived from Theorem 1 in Arratia et al.(1990) using an improved bound (Barbour et al., 1992) dTV(L(W ),P())1(1e )(b1 +b2) with b1 := iI jBi E[Wi]E[Wj], b2 := iI jBi,j =i E[Wi Wj].
The bound b1 is straightforward to compute as it only contains the first moment.
We have to consider the fact that the Bis for the first and last few windows contain less dependent variables than windows in the middle of the sequence.
Let r =w/s, then for example, the first window has r1 overlapping windows, thus, |B1|=r since we also include index 1 in the set.
The second window additionally overlaps with the first window, thus, |B2|=|B1|+1.
The set size is incremented by 1 until the (r+1)-th window as this window has equal number of overlaps to the left and to the right.
At the end of the sequence, the set size is decremented in the same way.
Hence, we obtain b1 =p(w)2 ( r(1r+2m)m).
The second bound b2 is more complicated to calculate because it contains the second moment.
Since we consider Bernoulli variables, the second moment is the probability that both variables are equal to one: E[WiWi+k]= P(Wi =1,Wi+k =1).
Considering only two PFMs A and B, we can write this probability in terms of the count random variables by decomposing it into four disjoint events as illustrated in Figure 3.
Denoting the size of each non-overlapping part by d =k s while the overlapping part has a length of v=wd, we obtain for the second moment: E[WiWi+k] = p(v)+ ( 1edrA )2[ 1evrB p(v)] + ( 1edrB )2[ 1evrA p(v)] +p(d)2evrAB.
To compute the bound, we observe that E[WiWi+k] is independent of i since all Wis are identically distributed and have the same pairwise dependencies.
Therefore, we clarify notation by defining k :=E[WiWi+k].
For the same reason, we also obtain k =E[WiWik].
Using the further 2105 [13:17 4/8/2009 Bioinformatics-btp143.tex] Page: 2106 21032109 U.J.Pape et al.Fig.3.
The four disjoint events for two windows where the dark gray area indicates the overlap.
Regions containing an A or B must necessarily contain at least one hit of the corresponding PFM, while A and B label regions where the respective PFM must not occur.
In blank regions, any PFM and combinations of PFMs might be present.
definition of =r1k=1k , we yield for bound b2 applying the same logic as above: b2 = 2 r i=1 [ + i1 k=1 k ] +2(m2r) = 2 ( m r + r i=1 i1 k=1 k ).
Here, we assume that the empty sum ( i1 k=1k for i=1) is equal to 0.
2.5 Alternative independence approach To assess the necessity to incorporate dependencies into the calculation, we compare the results with an approach ignoring dependencies.
For the probability of no hits, we obtain P(Nw(A)=0) ( 1A )2w , P(Nw(B)=0) ( 1B )2w , P(Nw(A)+Nw(B)=0) [( 1A )(1B)]2w.
Since we also consider the complementary strand, we have to double the window size w. For the rates, we obtain rA 2A 2A, rB 2B 2B, rAB 2[1(1A) (1B)][1(1A)(1B)]2.
The factor 2 and the substraction of the squared probability is necessary to incorporate the complementary strand.
Eventually, we obtain for the co-occurrence probability p(w) in a sequence of length w p(w)=1erAw erB w +erAB w. Obviously, the approach does not incorporate similarities between PFMs A and B.
2.6 Data The PFM set used here is the vertebrate_non_redundant_minFP set from the TRANSFAC database (v. 11.3) (Matys et al., 2003).
Since, despite the name, the set contains more than one PFM per TF (214 in total), we only select the first PFM per TF and obtain a set of 142 PFMs.
Hence, we are left with a set of one PFM per TF.
However, the remaining similarities between PFMs in this set are not negligible.
To show this, we measure the similarity between all pairs of PFMs by the limiting covariance (Pape et al., 2008b).
Then, we select the pair of PFMs with highest similarity (0.0002): S8 (V$S8_01) and CHX10 (V$CHX10_01).
We use this pair for our analysis.
To assess the influence of similarity, we also select a very dissimilar pair of PFMs.
Given S8, the most dissimilar PFM is HIC (V$HIC1_02) with a similarity Fig.4.
Logos (Crooks et al., 2004) of the selected PFMs CHX10, S8 and HIC.
The first two motifs share the motif AATTAand, therefore, are similar.
The third PFM has no similarity to other PFMs.
of 0.000004.
The similarity between CHX and HIC is higher with a value of 0.000003.
Hence, we define a pair of similar PFMs S8 and CHX10 and two pairs of dissimilar PFMs S8 and HIC as well as CHX and HIC (Fig.4).
All analyses regarding PFMs are performed based on a balanced type I error () in a sequence of length 500 controlled at a level of 10% [see Pape et al.(2006) for details].
In a step called regularization, we add pseudo-counts to the position-specific distributions of the PFM according to the information content of the position (Rahmann, 2003).
Simulated sequences are generated i.i.d.
with 50% GC content.
3 RESULTS In this section, we analyze the influence of the similarity between PFMs on the co-occurrence probabilities.
First, we determine the window size for each pair such that the co-occurrence probability is 1%.
Next, we confirm the approximated window size by a simulation.
Based on these results, we compare the approximated cooperativity distributions for all pairs with the corresponding empirical distributions and the results from the independence approach.
Finally, we apply the approach to overlapping windows and report the accuracy of the approximation.
3.1 Co-occurrence probability First, we apply the formulae for the window size given a cooccurrence probability of P=0.01 to all pairs of PFMs.
The pair of similar PFMs S8:CHX10 yields a window size of 54 bp for both Newton iteration and Taylor expansion.
Computing the co-occurrence probability for the window size 54 bp yields exactly 0.01.
Hence, both approximations are very accurate.
The most dissimilar pair S8:HIC yields for the same given co-occurrence probability a window size of 297 bp using Newton iteration and 281 bp using Taylor expansion.
The corresponding co-occurrence probabilities are 0.01 and 0.009.
Hence, the Newton iteration is slightly more accurate than the Taylor expansion.
The dissimilar pair CHX:HIC yields a window size of 266 bp using Newton iteration and a slightly smaller window of 252 bp using Taylor expansion.
Again, the window size derived from the Newton iteration is exact such that it leads to a co-occurrence probability of 0.01, while the Taylor extension yields 0.009.
In comparison to the similar pair, one obtains an 5-fold larger window size for the dissimilar pairs.
Since similar PFMs tend to have overlapping hits, their probability of co-occurrence which includes overlapping hits is high.
Therefore, an occurrence of one PFM increases the probability of an occurrence of the other PFM.
In contrast, dissimilar PFMs cannot overlap.
Thus, presence of one PFM decreases the probability of an (overlapping) occurrence of the other PFM.
Due to the big difference in the window sizes, it is very important to consider the similarity between PFMs.
The presented approach shows that one can simply adjust the window size.
2106 [13:17 4/8/2009 Bioinformatics-btp143.tex] Page: 2107 21032109 Cooperativity of PFMs A B C Fig.5.
Histograms of empirical co-occurrence probabilities for (A) the most similar pair S8 and CHX10 with window size 54 bp, for (B) the most dissimilar pair S8 and HIC with window size 297 bp and for (C) the dissimilar pair CHX and HIC with window size 266 bp.
Hence, one would use a window size of 54 bp for the similar pair and of 297 bp and 266 bp, respectively, for the dissimilar pairs.
Then, all pairs have almost equal co-occurrence probabilities.
We verify this prediction by a simulation study.
After annotating 100 random sequences each of length 1 000 000 bp with the corresponding PFMs, we count the number of co-occurrence events given above window sizes.
The histograms for all three pairs are shown in Figure 5.
The left panel contains the histogram for the similar pair S8:CHX.
The distribution has a mean of 0.007 and a SD of 0.0006.
Hence, the approximated co-occurrence probability of 0.01 is slightly biased towards lower probabilities.
The reason is that the approximation of the co-occurrence probability only considers first-order dependencies between occurrences.
This means overlaps between more than two occurrences are ignored.
The center panel of Figure 5 shows the histogram for the most dissimilar pair S8:HIC.
The mean is 0.012 with SD 0.002.
Thus, the empirical probability is slightly higher than our approximation but the difference is still within one SD of the mean.
The right panel contains the dissimilar pair CHX:HIC.
The distribution has a mean of 0.009 and an SD of 0.002.
Therefore, our approximation slightly overestimates the co-occurrence probability.
Anyhow, the approximation of the co-occurrence probability is very accurate.
Since dissimilar PFMs do not strongly overlap, the corresponding first-order approximations yield more accurate results.
In contrast, applying the window size of one of the dissimilar pairs (e.g.297 bp) to the similar pair would yield a co-occurrence probability of around 0.04 (retrieved by simulation).
Hence, by adjusting the window size the difference between co-occurrence probabilities decreases from almost 3-to 4-fold to quite comparable co-occurrence probabilities.
As we will see next, such small differences already have strong influence on the cooperativity P-values.
3.2 Cooperativity Based on the co-occurrence probabilities and the window sizes, one can compute P-values for cooperativity.
This is done by counting the number of windows with a co-occurrence event.
The P-value is the probability for at least as many co-occurrence events as observed.
A simulation with 10 000 sequences of length 100 000 bp is used as reference.
In each sequence, we count the number of co-occurrence events.
The frequencies of the counts are the empirical distribution.
Fig.6.
Comparison of the log10 P-values of the approximation (y-axis) and the simulation (x-axis).
Red cross indicates the new approach while green circles correspond to the independence approach.
Upper panels are based on non-overlapping windows with size such that co-occurrence probability is 1%.
Left panels show the most similar pair S8:CHX10 with window size 54 bp, center panels contain the most dissimilar pair S8:HIC with window size 297 bp, and the right panels belong to the dissimilar pair CHX:HIC with window size 266 bp.
Lower panel considers overlapping windows where two neighboring windows overlap by 10%, yellow area indicates ChenStein bounds.
Figure 6 compares the log10 P-values of two approximations and the simulation.
The left panel shows the computations for the similar pair S8:CHX.
The approximation of the independence approach strongly underestimates the P-values, while the new approach yields P-values differing only by around one order of magnitude from the empirical values.
The reason for the huge underestimation is the high-overlap probability of the PFMs.
Therefore, the co-occurrence probabilities are underestimated leading to the underestimation of the cooperativity P-values.
The new approach considers overlap probabilities and, therefore, corrects against similarity.
Using overlapping windows (lower panel, overlap of 10%) yields similar results.
Since the ChenStein error bound is 0.21, it is not possible to obtain P-values smaller than this value.
Hence, with such an overlapping window scheme, it is impossible to obtain significant P-values.
The center panels of Figure 6 contain the comparisons for the most dissimilar pair S8:HIC.
The independence approach overestimates the P-values by one order of magnitude, while the new approach underestimates the values by around half an order of magnitude.
The underestimation can be explained using the results of the last section: the new approach underestimates the co-occurrence probabilities.
Thus, fewer windows with a co-occurrence event are expected, therefore, the probabilities are lower.
The results for overlapping windows (lower panel) are very similar, again.
The ChenStein error bound has a value of 0.07.
Again, such a high approximation error makes it difficult to obtain significant P-values.
The dissimilar pair CHX:HIC is compared in the right panels of Figure 6.
The independence approach slightly overestimates the P-values, as well as the new approach.
However, the new approach is more accurate.
The overestimation can be explained by the overestimation of the co-occurrence probabilities.
For overlapping windows, the results are similar except for the smallest P-values.
However, the smaller the P-values the more simulations are needed.
Thus, the smallest P-values have weakest support.
Since they are outliers, we do not consider them.
The ChenStein error is also 0.07.
In summary, we can state that the independence approach works for dissimilar pairs of PFMs while it cannot be used for similar pairs.
2107 [13:17 4/8/2009 Bioinformatics-btp143.tex] Page: 2108 21032109 U.J.Pape et al.In contrast, the new approach incorporates the similarity and returns accurate approximations for all pairs of PFMs independent of the shared similarity.
Furthermore, overlapping windows lead to high approximation errors such that overlapping windows should be used carefully.
However, using the new approach one can compute the approximation error before performing the analysis.
Based on this, one can ensure that the overlapping scheme can yield significant P-values at least theoretically.
Here, the analysis is done for sequences of length 100 000 bp.
The ChenStein bounds implicitly depend on the sequence length because the number of windows is considered.
Therefore, we also analyze the bounds for smaller sequences in the next section.
3.3 Overlapping windows for small sequences Assuming a sequence length of 1000 bp, we compute ChenStein error bounds for the cooperativity P-values.
Using 54 bp long windows which overlap by 10% yields an error bound of 0.04 for the similar pair S8:CHX10.
Hence, it will still be difficult to obtain significant results since one cannot obtain P-values less than 0.04.
In general, similar PFMs have a high approximation error for overlapping windows since overlapping occurrences induce high dependencies between two windows.
In contrast, the dissimilar pairs S8:HIC and CHX:HIC have error bounds of 0.002 and 0.003 for window sizes of 297 and 266 bp, respectively.
The bounds are smaller for two reasons: first, the windows are larger and thus fewer windows are used for the sequence.
Second, dependencies between overlapping windows are smaller since dissimilar PFMs have smaller overlap probabilities.
Hence, in case of dissimilar PFMs one can use overlapping windows and still obtain significant cooperativity.
4 DISCUSSION In conclusion, we can state that detection of significant co-occurrences and cooperativity based on PFM occurrences is a difficult problem due to strong dependencies induced by similarity between PFMs.
We show a reasonable approximation to adjust the window size such that co-occurrence and cooperativity probabilities are comparable between similar and dissimilar PFMs.
Therefore, statistical followup analyses can ignore the similarity issue.
Instead, the interpretation of cooperativity changes slightly: the window size defines the longest distance between two motifs such that the corresponding TFs are assumed to interact.
Therefore, similar pairs of interacting TFs are required to have smaller distances between occurrences than dissimilar pairs of TFs.
This is due to the fact that interaction over longer distances cannot be predicted with sufficient statistical support for similar TF pairs.
Furthermore, we propose a new approximation for cooperativity using overlapping windows.
Using the ChenStein technique, we can bound the approximation error.
Results show that similar PFMs imply strong dependencies between overlapping windows.
This leads to high approximation errors.
In contrast, dissimilar PFMs yield low approximation errors.
Based on our error bounds, one can precompute the approximation errors and select an appropriate overlap scheme before running the analysis.
We give strong evidence for the accuracy of our approach and the necessity of incorporating similarities by comparison with the empirical distribution and the independence approach.
Our results underline the difficulty in applying overlapping windows especially for similar motifs.
However, it is important to use overlapping windows, otherwise, a motif occurring at the end of one window with another occurring at the beginning of the next window would not be counted as a co-occurrence event although the distance between them might only be a few base pairs.
Hence, one could derive statistics for the distances between motifs instead of using windows (see Fig.1).
The distance between two successive occurrences of the same motif follows an exponential distribution with the Poisson rate as a parameter assuming independence between the occurrences (Wagner, 1999).
As shown in Pape et al.(2008a), the independence assumption does not generally hold.
This makes derivation of the distance distribution already complicated for only one TF.
Extension to more than one TF is even more difficult since the order of overlapping motifs has to be considered.
The main shortcoming of the approach is the limitation to an i.i.d.
background model.
Extension to a Markov model is not straightforward since calculation of co-occurrence probabilities rely on the independencies between sequence positions.
In addition, we require the distribution of occurrences on both strands to be equal.
This can be justified by Chargaffs second law (Chargaff et al., 1951).
Furthermore, in contrast to coding sequence, there is no motivation to handle both strands in the upstream region differently.
Therefore, modeling of CpG islands and other higher order sequence features cannot be done by using a more elaborate sequence model.
However, one can circumvent this problem by using different window sizes for different sequences incorporating the respective GC content.
Another strategy could use a mixture Poisson distribution based on different rate parameters incorporating variable GC content as approximation.
ACKNOWLEDGEMENTS We thank the organizers of the GCB 2008 for the opportunity to present this work at the conference.
Furthermore, discussions with Hugues Richard helped to improve the manuscript.
Funding: International Research Training Group-Genomics and Systems Biology of Molecular Networks (to H.K.).
Conflict of Interest: none declared.
ABSTRACT Motivation: As the use of microarrays in human studies continues to increase, stringent quality assurance is necessary to ensure accurate experimental interpretation.
We present a formal approach for microarray quality assessment that is based on dimension reduction of established measures of signal and noise components of expression followed by parametric multivariate outlier testing.
Results: We applied our approach to several data resources.
First, as a negative control, we found that the Affymetrix and Illumina contributions to MAQC data were free from outliers at a nominal outlier flagging rate of = 0.01.
Second, we created a tunable framework for artificially corrupting intensity data from the Affymetrix Latin Square spike-in experiment to allow investigation of sensitivity and specificity of quality assurance (QA) criteria.
Third, we applied the procedure to 507 Affymetrix microarray GeneChips processed with RNA from human peripheral blood samples.
We show that exclusion of arrays by this approach substantially increases inferential power, or the ability to detect differential expression, in large clinical studies.
Availability: http://bioconductor.org/packages/2.3/bioc/html/array Mvout.html and http://bioconductor.org/packages/2.3/bioc/html/ affyContam.html affyContam (credentials: readonly/readonly) Contact: aasare@immunetolerance.org; stvjc@channing.harvard.edu 1 INTRODUCTION Recent successes with microarrays for the identification of gene patterns that correlate with disease states has resulted in their increased use in human studies.As this approach moves from smaller scale efforts into biomarker discovery efforts in clinical trials, rapid and reliable quality assurance approaches are necessary, both from the perspective of ensuring accurate data for inclusion as clinical trial secondary endpoints, and as a measure to contain costs (Group, 2004).
As a large clinical trial consortium, we have processed over 1500 Affymetrix Gene Chips from eight different clinical trials, with these numbers growing yearly.
Thus, we required a streamlined and accurate method for defining which arrays are of high quality To whom correspondence should be addressed.
The authors wish to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
and to determine the overall success rate of arrays processed in our central laboratory.
Many of the currently used microarray quality assessments are manufacturer-based recommendations that rely on a limited number of parameters with imprecise specifications for acceptable results.
In this report, we review definitions of quality assessment criteria for Affymetrix and Illumina expression arrays, and describe algorithms for formally identifying aberrant arrays with a specified false positive rate.
Our procedure involves parametric multivariate outlier testing using a multivariate Gaussian model, which we apply to principal components of the quality measure matrix.
2 METHODS 2.1 Definitions of quality metrics For Affymetrix Gene Chips, quality criteria include the actin (HSACO7) and GAPDH 3/5 ratios, the percent present calls according to the MAS5 algorithm, the array-specific scale factor and average background [see Hubbell et al.(2002) and the Affymetrix Statistical Algorithm Reference Guide].
The actin and GAPDH ratios indirectly reflect the efficiency of reverse transcription of the total RNA template, as high 3 to 5 ratios indicate poor transcription from the 3-end, resulting in small fragments of cDNA/cRNA available for hybridization.
The other indicators reflect hybridization efficiency as they indicate the number of probes hybridized from the total (percent present call) and the amount of background noise.
Bolstad et al.(2003) define probe-level modeling (PLM) for Affymetrix arrays as a flexible generalization of the established robust multiarray preprocessing procedure (RMA) due to Irizarry et al.(2003).
PLM computes two quantities of particular interest for quality assessment.
The first, relative log expression (RLE), examines point estimates of expression values at the probe set level.
Suppose, here and in the sequel, that there are N arrays and G probe sets.
PLM computes eij , i = 1,...,G, j=1,...,N , on the basis of a robust regression model allowing general probe and sample effects.
The quantity ei, is the median log expression of the i-th probe set.
RLEij = eij ei,, computed for all probes, all arrays.
Denote by RLEj the G-vector of RLE measures for the j-th array.
Features of the distribution of RLEj are informative on array quality: medians should be close to zero for all arrays, and variances should be similar across arrays.
Departures from these conditions are usually associated with quality defects.
Another quantity derived from PLM is the normalized unscaled standard error (NUSE).
Here, the focus is on variability of estimation of expression.
PLM computes standard errors of all expression measures, and these are standardized, gene by gene, across arrays, so that the median standard error across arrays is 1.
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Parametric array outlier testing The G-vector of NUSE for each array should have median 1 and common variance across arrays.
Another indicator provided by the affy package of Bioconductor/R (Gautier et al., 2004) is the RNA degradation slope, which assesses the 3/5 ratio for all genes on the array.
Probes are ordered within probe sets from 5-to 3-most and the intensity gradient along this sequence is qualitatively estimated pointwise for each array.
Arrays with quality problems can possess aberrant gradients, typically assessed visually.
For Illumina arrays, the Bioconductor lumi package (Du et al., 2008) includes the lumiQ function which computes a four-dimensional quality feature vector for each array in a lumiBatch.
The quality measures are average intensity and intensity SD, average detection rate as reported by the BeadStudio preprocessor and average Euclidean distance of probe-specific intensities to their means over all samples.
2.2 Array quality feature-vector and dimension reduction For Affymetrix arrays, the metrics described above lead to nine quantitative measures of array quality computed on each array: ABG (average background), SF (scale factor), PP (percent present), AR (actin 3/5 ratio), GR (GAPDH 3/5 ratio), median NUSE, median RLE, RLE-IQR (interquartile range of IQR per array, to measure variability in RLE) and RNAS (slope of RNA degradation measure).
This sequence of numbers is regarded as the array-specific quality feature vector.
A principal components transformation is conducted using R function prcomp to obtain linear combinations of the original features that are mutually orthogonal and that capture substantial fractions of variation among samples.
The use of linear combinations also aids in procuring a multivariate representation of the quality information that may, under a null hypothesis of equivalent quality, be reasonably approximated by a multivariate Gaussian probability model.
We denote by Q the N 9 feature matrix (with arrays defining rows and quality measures defining qualities).
The matrix consisting of the user-selected m>1 initial principal components of Q as columns is denoted as P. For Illumina arrays, we denote by Q the N 4 feature matrix of lumibased quality measures, to which principal components transformations may be applied.
2.3 Multivariate outlier detection algorithms If there are r > N/2 non-outlying rows of the N m matrix, P are regarded as a collection of r samples from the m-dimensional Gaussian model Nm(,) with m-dimensional mean and mm covariance matrix , an algorithm of Caroni and Prescott (1992) can be used to identify outlying observations among the N data rows with a fixed small probability of incorrectly labeling a non-outlying observation as an outlier.
Let S denote the m m sample covariance matrix of the m first principal components of Q [taking the m columns of P as variables and the N rows of P as observations (denoted Pi, i = 1,...,N)].
Wilks scatter ratios are the quantities Wl =|S(l)|/|S|, l=1,...,N, where S(l) denotes the sample covariance matrix of P computed with row l removed.
Wilks scatter ratio is the likelihood ratio test statistic of H0 :Pi Nm(,), i=1,...,N against H1 :Pi Nm(,), i = j, and Pj Nm(+aj,) where the outlier index j, the m-dimensional slippage parameter aj and the variance parameter are all unknown.
The scatter ratio may be re-expressed as a function of the Mahalanobis distance: Wl =1 N N 1 (Pi ) tS1(Pi ).
Caroni and Prescott implement an inward peeling followed by outward testing procedure following a univariate procedure due to Rosner (1983).
Define D1 =minj(Wj), and D2,...,DNr as the sequence of scatter ratios formed from successive eliminations of rows of P posessing the smallest scatter ratios at each stage.
Caroni and Prescott observe that the distribution of Wj for given j is Beta([np1]/2, p/2) and derive Bonferroni bounds using Rosners arguments to obtain approximations to the distributions of Ds, s=1,...,N r. Critical values are obtained using these approximations and the quantities DNr ,...,D1 are each tested at level.
If DNq is smaller than the associated critical value, then the associated sample and all samples associated with minimal scatter ratios computed at stages earlier than N q are declared outlying.
This procedure mislabels non-outlying samples as outliers with overall error rate whether or not any outliers are actually present in P, and is implemented in R source code in the arrayMvout package.
In tables below, we refer to the results of using this procedure as PMVO (for Parametric MultiVariate Outlier labeling).
An important recent contribution to array outlier assessment is the methodology of Cohen Freue et al.(2007) in which robust Mahalanobis distances are used to identify aberrant arrays.
The standard Mahalanobis distance can be robustified by substituting for the sample covariance a covariance estimator based on the minimum covariance determinant, minimum volume ellipsoid or a specific S-estimator.
Parametric or simulation-based critical values for outlier labeling are available.
In comparative tabulations below this algorithm is denoted MDQC.
2.4 Implementation The arrayMvout package includes a function ArrayOutliers that accepts an AffyBatch (imported intensity data structure derived from CEL files) or lumiBatch instance, specification of and a vector indicating which principal components are to be used to define P.ArrayOutliers returns an instance of the arrOutStruct class, for which a simple reporting method is defined, showing the indices and feature values of outlying arrays; a plot method returns the principal components biplot of Q.
The object quietly maintains results of all quality metric computations, such as the fitPLM result, facilitating more detailed diagnosis should such be required.
2.5 Power estimation PowerAtlas, a power and sample size estimation tool for microarray study, was adopted to investigate impact of aberrant arrays on statistical power for identification of differentially expressed genes (Page et al., 2006).
The PowerAtlas tool is developed based on a mixture model approach for estimation of power and sample size of high-dimensional data (Gadbury et al., 2004).
A list of per-gene P-values, generated from a pair-wise comparison using LIMMA package, was used as input for PowerAtlas for power estimation of the comparison.
3 RESULTS 3.1 Negative control: application to MAQC arrays The Affymetrix contribution to the MAQC expression study consists of 120 Affymetrix hgu133plus2 arrays collected from three MAQC labs with two replicates on each of four sample types.
These arrays were produced under a strict protocol for the MAQC cross-platform comparison (Shi et al., 2006) and so are expected to be of very high quality.
Our outlier algorithm was applied with m=3 and =0.01,0.05 and 0.10 with two approaches to feature representation.
As requested by two referees, we applied outlier detection to the raw quality features without principal components re-expression.
The table entry under PMVO-raw shows that relatively large numbers of outliers are flagged with this approach.
When principal components re-expression is applied, no outliers are detected with PMVO at = 0.01,0.05 or 0.10.
49 A.L.Asare et al.Table 1.
Application of multivariate outlier detection to negative and positive controls derived from MAQC and Affymetrix spike-in series, the latter with digital contamination Negative controls Source No.
of chips No.
of chips flagged PMVO-raw PVMO-PC MDQC-PC Affy.
MAQC 120 (34,34,23) (0,0,0) (9,3,1) Illu.
MAQC 19 (0,0,0) (0,0,0) (3,1,0) Digitally contaminated arrays Source No.
of chips Contaminated Chips flagged PMVO-PC MDQC Affy.
spike-in 12 none 2,8,10 1 1 1,8 1,2 1,2,8 1,2 1,2,11 1,2,8,11 8,10 For negative controls, table cells give number of arrays flagged at =0.10,0.05,0.01.
For positive controls, cell entries give indices of arrays contaminated or identified by various algorithms.
Method labels are: PMVO-raw, for parametric multivariate outlier detection applied to raw QA features; PMVO-PC, for PMVO applied with dimension reduction to first three principal components; MDQC, for Mahalanobis distance-based algorithm of Cohen Freue et al.(2007) with the MCD estimator of covariance, applied to raw QC features; and MDQC-PC, for MDQC with the S-estimator of covariance applied on PC1PC3 of QC features.
We obtained the raw MAQC data contributions from Illumina, Inc. (Le.
Shi, personal communication), and created raw reads of 19 arrays using the lumiR procedure of the lumi package of Bioconductor (Du et al., 2008), and then computed the quality measures via the lumiQ procedure.
Table 1 shows that PMVO finds no outliers, while MDQC finds a small number as long as 0.05.
We conclude that PMVO has reasonable specificity when used in conjunction with principal components re-expression for arrays produced in good quality conditions.
3.2 Sensitivity to specific contamination events Figure 1 displays a digitally contaminated CEL file from the Affymetrix spike-in experiment archive.
The readily visible artifacts are created by altering intensity levels in specific regions of the chip either by fixing them to constant value or by rescaling them to achieve altered intensity variance.
For conciseness, the figure shows a chip on which four contamination events occur simultaneously; in our data analyses these were applied separately to different subseries of chips as shown in Table 1.
The base series is the first 12 chips in the U133A spike-in subset.
Each of four types of artifact were introduced to chip 1, then chips 1 and 2, and then chips 1, 2 and 11, to understand the effects of artifact type and number on sensitivity of outlier detection algorithms.
We see that when there are no artifacts, PMVO does not declare any array to be outlying, but MDQC (run with default settings) declares three arrays to be outliers.
Results for PMVO and MDQC were invariant to the type of artifact when only 1 or 2 chips were contaminated, so contamination type is not recorded in Table 1.
When three chips were contaminated (1, 2 and 11), Fig.1.
Composite of four types of digital contamination applied to raw Affymetrix intensity datathe three circular subregions are, counterclockwise from upper left, low constant, variable and high constant blobs, and the rectangular region on the right has inflated variance.
PMVO always flagged chips 1, 2, 8 and 11, regardless of the type of contamination.
MDQC was sensitive to type; in the table we show that it flagged chips 8 and 10, as was true for low constant and increased variability blobs; with the high constant blob MDQC flagged 10 and 11, and for rectangle of increased variability, MDQC flagged 1 and 8.
Further study of differential sensitivity to artifact type will be warranted.
3.3 Large-scale applications We applied this method to identify aberrant arrays from a pool of 507 microarrays from multiple clinical studies conducted by the Immune Tolerance Network (ITN).
This dataset was generated from clinical human studies using peripheral blood samples, representing a very different experimental setting from the MAQC study.
There is higher variability and little to no ability to perform technical replicates due to cost and sample limitations.
In the human peripheral blood RNA microarray sample set, 18 microarrays, or 3.5%, were detected as outliers at =0.01 (Fig.2).
We confirmed this approach to outlier detection by plotting the distribution of arrays by individual QA indicator (Fig.1) with the outlier arrays highlighted in red.
As shown, blue traces correspond to arrays of high quality that fall within acceptable QA ranges as previously described (GeneChip Expression Analysis Technical Manual, www.affymetrix.com).
Arrays that the approach flagged as outliers (Fig.2) do not fall within the acceptable range for at least one if not more of the QA parameters.
To demonstrate that our outlier detection method improves the ability to determine meaningful changes in gene expression (differential gene expression), we assessed the impact of our outliers on gene expression estimates (absolute measures that are averaged across all probes for a particular gene).
Our assessment focused on two trials where we expected differential expression based on clinical phenotype or treatment regimen.
In the first case, we used a set of 204 arrays from a ragweed allergy clinical trial.
Our method identified five outliers (Fig.3A, shown in red and designated as AE) that are coincident with five points outside the major grouping 50 Parametric array outlier testing Fig.2.
Parallel coordinate plots are a common way of visualizing multivariate data with different scales to facilitate detection of outliers.
Applying our QA approach, 18 of the 507 microarrays were flagged as aberrant (highlighted in red).
As shown, our approach to QA has selected samples as problematic where one or more indicators appear as an outlier based on reduction of the dimensionality of the data via PCA and applying a sequential Wilkss multivariate outlier test at an = 0.01.
Our approach provides greater consistency in designating problematic arrays through a statistical framework that does not rely on arbitrary cutoffs for any individual indicator.
of samples on plots of PCA gene expression estimates.
For outliers ABC, this was due to poor (high) NUSE values, with only array A having poor (low) percent present calls.
Outliers D and E had normal percent present calls and NUSE values, but poor GAPDH , actin (HSACO7) and RNA slope values.
Our second case study used a set of 42 microarrays from a kidney transplant trial, in which three patient cohorts were compared.
Here, we detected three outliers using our method.
These outliers also correspond to arrays which fall outside the primary cluster on PCA gene expression estimate plots (Fig.3B Points FH).
In this case, all three points falling out of the cluster had poor GAPDH, actin (HSACO7) ratio, RNA slope, and NUSE values.
Array F had a poor percent present call, while G and H did not.
While the outlier arrays detected by our method correspond to arrays that are outside the cluster of gene expression estimate PCA plots, our approach provides improved discriminatory power in that it permits discrimination of arrays that are outliers due to poor array quality compared with those that are likely to reflect actual biological variance.
3.4 Improvement of statistical power The benefit of excluding poor quality arrays in differential expression analyses was further demonstrated through statistical power calculations for these two clinical trials.
Statistical power is a major limiting factor in clinical microarray studies due to limited sample size and lack of technical replicates.
Power calculations were performed, using PowerAtlas (Gadbury et al., 2004), both with and without the two array outliers (Fig.3A, Points B and C) identified by our method.
These two microarrays correspond to data points that were collected to assess treatment effect.
The other three outliers in Fig.3A addressed seasonal effects of the study.
Gadburys procedure fits a two-component 110 83 56 29 2 25 51 78 105 132 160 670 579 489 399 309 220 129 39 50 140 230 PC #1 21.9% P C # 3 4.
34 % A A B B C D E 210 172 134 96 58 20 17 55 93 131 170 690 608 527 446 365 285 203 122 41 39 120 PC #1 P C # 2 11.5 % F G H Fig.3.
PCA was applied to gene expression estimates for all genes in two clinical trials.
(A) The outlier detection approach described was applied to 204 arrays from a ragweed allergy study and identified five samples.
These microarrays are highlighted in red in the PCA 1 versus PCA 3 plot for gene expression to show the relationship of outlier samples detected by the system to actual gene expression estimates per array.
Points A, B and C have problematic NUSE values.
Points D and E have abnormally high GAPDH and HSAC07 ratios.
The location of these arrays based on gene expression PCA suggests that QA problems may contribute to deterioration of overall expression.
(B) A kidney transplant trial with 42 arrays where three were detected as outliers.
The three arrays are highlighted in red in the PCA 1 versus PCA 2 gene expression plot.
Points F, G and H have abnormal NUSE, GAPDH and HSAC07 ratios.
Again, the samples flagged by the QA approach appear to have gene expression estimates that differ from the majority of other arrays.
The arrayMvout package includes a map fig3map from records in the ITN QA metrics matrix to samples labeled AH in these figures.
mixture of Beta variates to the distribution of P-values of genespecific differential expression tests.
The two components of the Beta model for P-values correspond to (i) the distribution of P for non-differentially expressed genes (which will be uniform), and 51 A.L.Asare et al.A B Fig.4.
Statistical power calculations.
(A) Ragweed allergy study showing improved EDR upon removal of arrays flagged as Points B and C in Figure 3A comparing two time points of interest.
(B) Kidney transplant study showing removal of two arrays flagged as F and G in Figure 3B in a differential expression comparison of two treatment cohorts.
(ii) the distribution of P for differentially expressed genes (which will have mass predominantly near zero).
Parametric bootstrapping is then used in conjunction with the mixture model fit to relate the sample size of an experiment to the operating characteristics of tests for differential expression.
Gadbury defines the expected discovery rate (EDR) as the expected proportion of genes that are truly differentially expressed that will be declared to be differentially expressed under a given design.
If differential expression is present, but tests of differential expression are impaired by the presence of poor quality arrays, the P-values obtained will not be readily resolved into two components, and power will be diminished.
With the arrays of low quality included, Gadburys estimate of the EDR [with false discovery rate (FDR) = 0.05] was 24.2% at a sample size of 10, whereas the EDR improved to 73.3% for the same sample size, same FDR, through exclusion of the poor quality arrays (Fig.4A).
Similarly, in our kidney transplantation case study, we compared two of the three cohorts; each of these two cohorts had one outlier (Fig.3B, Points F and G).
Power calculations with outlier arrays both included and excluded showed that the EDR reached 14.1% and 74.2%, respectively for a sample size of 30 (Fig.4B).
4 CONCLUSION While microarray post-hybridization quality indicators are readily generated via standard output from analysis software packages, cutoffs used to identify problematic arrays have typically been subjective and arbitrary in nature.
These indicators, by themselves, do not always give the level of discrimination needed to distinguish microarrays that are poor in quality.
We have proposed a three-step procedure for decision-making about array quality.
First, choose a collection of quantitative quality metrics.
For Affymetrix expression arrays, we have identified nine metrics that appear to have reasonable utility, and for Illumina expression arrays we use four metrics that are routinely computed by open source software.
These metrics can be supplemented or restricted as desired by users.
Second, compute the principal components re-expression of the metrics and reduce the quality data to a modest number of components.
This step pursues parsimonious integration of the various metrics and yields a multivariate quality representation that should be reasonably approximated by the multivariate Gaussian model.
Third, apply calibrated parametric multivariate outlier detection to a subset of the resulting quality principal components.
We propose Caroni and Prescotts generalization of Rosners GESD procedure, show that it has reasonable specificity and sensitivity in several contexts, and indicate that its use leads to increased inferential power in an important clinical application.
Despite the attractive features identified above, prospective users of statistical outlier labeling in microarray contexts need to be cautious in their application of these methods.
It is a commonplace that the outliers are often the most interesting records in any given database.
Because we are studying outlyingness with respect to average quality, the outlying arrays may be informative about important events or discrepancies in the overall processing workflow.
It is also inevitable that the procedure described here has several components conferring flexibility, and, consequently, manipulability.
There is no basis at present for objective choice of base quality metrics, for the choice of number of principal components to use in reduction, or for the choice of null outlier labeling rate that will lead to greatest confidence that arrays labeled as outliers are truly aberrant and that unlabeled arrays are of adequate quality.
Thus, it is possible that two users may obtain different decisions on identical data.
We have designed the ArrayOutliers tools so that they are reasonably selfdocumenting, so that all applications can be audited.
There are various avenues along which the work described here should be extended.
First, by working with larger numbers of arrays that have been independently classified into acceptable and unacceptable states, it should be possible to analyze the contributions of different quality metrics to probabilities of class membership.
Second, when large numbers of arrays that are known to be of good quality are available, the outlier detection process can be supplemented by a reference database.
Specifically, parametric outlier testing can be conducted on the basis of a fixed null mean and covariance for quality features derived from a family of arrays known to be of high quality.
The arrayMvout package includes matrices of quality measures for the Affymetrix and Illumina MAQC contributions, which can support exploration of this reference-based testing concept.
52 Parametric array outlier testing ACKNOWLEDGEMENTS We acknowledge the constructive comments of referees and an associate editor.
Funding: Subcontract from the Immune Tolerance Network; a project of the National Institute of Allergy and Infectious Diseases; The National Institute for Diabetes, Digestive and Kidney Diseases; Juvenile Diabetes Research Foundation.
National Institute of Health (P41 HG004059-01 to V.J.C.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Revealing the subcellular localization of proteins within membrane-bound compartments is of a major importance for inferring protein function.
Though current high-throughput localization experiments provide valuable data, they are costly and time-consuming, and due to technical difficulties not readily applicable for many Eukaryotes.
Physical characteristics of proteins, such as sequence targeting signals and amino acid composition are commonly used to predict subcellular localizations using computational approaches.
Recently it was shown that protein protein interaction (PPI) networks can be used to significantly improve the prediction accuracy of protein subcellular localization.
However, as high-throughput PPI data depend on costly highthroughput experiments and are currently available for only a few organisms, the scope of such methods is yet limited.
Results: This study presents a novel constraint-based method for predicting subcellular localization of enzymes based on their embedding metabolic network, relying on a parsimony principle of a minimal number of cross-membrane metabolite transporters.
In a cross-validation test of predicting known subcellular localization of yeast enzymes, the method is shown to be markedly robust, providing accurate localization predictions even when only 20% of the known enzyme localizations are given as input.
It is shown to outperform pathway enrichment-based methods both in terms of prediction accuracy and in its ability to predict the subcellular localization of entire metabolic pathways when no a-priori pathway-specific localization data is available (and hence enrichment methods are bound to fail).
With the number of available metabolic networks already reaching more than 600 and growing fast, the new method may significantly contribute to the identification of enzyme localizations in many different organisms.
Contact: shira.mintz@weizmann.ac.il; tomersh@cs.technion.ac.il 1 INTRODUCTION Eukaryotic cells contain several membrane-bound compartments called organelles that perform specialized biological functions.
Subcellular compartmentalization allows the cell to maintain different environments that bring enzymes and substrates into physical proximity, participating in compartment-specific processes.
Revealing the subcellular localization of proteins is of a major importance for inferring protein functions (Huh et al., 2003) and for the discovery of drug targets (as some compartments are more easily accessible than others for drug molecules).
Systematic protein To whom correspondence should be addressed.
localization experiments based on green fluorescent protein (GFP) tagging have been performed for several microbial species (Kumar et al., 2002; Matsuyama et al., 2006).
Such large-scale experiments are both costly and time-consuming, and due to technical difficulties are commonly not applicable to higher Eukaryotes.
Current limitations of experimental procedures for identifying protein subcellular localization have given rise to ongoing development of computational methods for predicting localization data (Bhasin and Raghava, 2004; Emanuelsson et al., 2000; Nakai and Horton, 1999; Scott et al., 2004; Shatkay et al., 2007).
Such methods rely on lists of features that characterize a protein, such as its amino acid composition, their physio-chemical and structural properties, codon-bias, protein motifs and targeting signals (short stretches of amino-acid residues predominantly located at the N-terminus).
The various localization methods apply different supervised classification approaches (e.g.artificial neural networks, nearest neighbor, SVM, etc.)
to predict protein subcellular localization based on training data of experimentally determined protein localization.
The performance of these methods significantly varies between different organisms and compartments, requiring specific calibration in each context.
Recent studies have investigated a complementary approach for predicting subcellular localization, utilizing large-scale proteinprotein interaction (PPI) networks (Lee et al., 2008; Scott et al., 2005).
These methods are based on the assumption that two proteins should be localized within the same or adjacent compartments in order to interact.
The work of Lee et al.has shown that in some cases, utilizing a PPI network may provide accurate localization predictions even without relying on common protein characteristics such as those described above.
This study presents the first method that predicts the subcellular localization of enzymes based on a metabolic network.
Relying on a metabolic network rather than a PPI network is highly advantageous as metabolic networks are readily available for hundreds of species based on cross-species enzyme sequence homology (Kanehisa and Goto, 2000), while large-scale PPI networks depend on costly highthroughput experiments that are currently available for only a few organisms.
Metabolic enzymes are also less likely to yield PPI interactions (Uetz et al., 2000); e.g.the probability of a PPI between metabolic enzymes in yeast is less than half of the probability of an interaction between other non metabolic proteins (based on data extracted from the DIP database (Salwinski et al., 2004)).
Thus, PPI-based localization methods have far less data to bootstrap upon the localization of metabolic enzymes.
Yet, constraint-based modeling of metabolic networks was previously shown to predict strong functional associations between enzymes (Notebaart et al., 2008; Rokhlenko et al., 2007), and to successfully predict various 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:02 15/5/2009 Bioinformatics-btp209.tex] Page: i248 i247i252 S.Mintz-Oron et al.metabolic phenotypes in microorganisms [see Price et al.(2004) for a review] and recently in human (Duarte et al., 2007; Shlomi et al., 2008).
Considering a metabolic network view of metabolic processes, the potential activity of an enzyme in a certain compartment depends on the activity of many other enzymes in the compartment synthesizing and degrading its substrate metabolites and on the activity of membrane transporters that move metabolites between compartments.
Indeed, metabolic processes tend to be clustered within various compartments without extensive usage of crossmembrane metabolite transporters.
For example, 36% and 47% of the metabolic pathways in yeast are confined within only one or two compartments, respectively (based on pathway annotation data in the model of (Duarte et al., 2004).
This may be explained by the fact that the cross-membrane exchange of many of the metabolites depends on transporter proteins, imposing an energetic cost (e.g.demanding an ATP or GTP molecule per each metabolite translocation (Palmieri et al., 2000) or requiring the maintenance of a membrane potential (Wada et al., 1987).
Accordingly, our method predicts the subcellular localization of enzymes relying on a parsimony principle of a minimal number of cross-membrane metabolite transports.
The utility of the method is demonstrated via a cross-validation test of predicting known subcellular localization of yeast enzymes.
2 METHODS We present a new constraint-based modeling (CBM) method for systematically predicting subcellular localization of enzymes in a metabolic network, based on a-priori localization data for a subset of the enzymes, relying on a parsimony principle of minimal number of cross-membrane metabolite exchange.
A schematic representation of the method is presented in Figure 1.
The input data for this method is a metabolic network, representing a set of enzyme-catalyzed reactions, and the known localization of a subset of the enzymes (Fig.1a).
Enzymatic reactions whose subcellular localization is given as input are referred to as localized reactions.
We refer to the remaining reactions as non-localized reactions.
The first step of our method involves the integration of the given metabolic network and the known localization data to construct an initial compartmentalized network (Fig.1b).
This network consists of several compartments, in which localized reactions may be activated in the corresponding compartments, and non-localized reactions are duplicated to be present in all compartments.
Next, we apply a Mixed Integer Linear Programming (MILP) method to compute a localization score for each pair of non-localized reaction and compartment, reflecting the likelihood of this reaction to be present in that compartment (Fig.1c).
These scores are used to determine the localization of each non-localized reaction, which is the output of the method (Fig.1d).
Next, we provide a brief overview on constraint-based modeling, followed by a detailed description of the various steps of our compartment prediction method.
2.1 Constraint-based modeling of metabolic networks A metabolic network consisting of n metabolites and m reactions can be represented by a stoichiometric matrix, denoted S, in which the Si,j represents the stoichiometric coefficient of metabolite i in reaction j (Price et al., 2004).
A steady-state flux distribution (i.e.an assignment of flux rates to all reactions in the network), denoted v, should satisfy the following mass-balance constraint: S v=0 The exchange of metabolites with the environment is represented as a set of exchange reactions, enabling for a pre-defined set of metabolites to Fig.1.
A schematic representation of the enzyme subcellular localization prediction method.
(a) The input data is a metabolic network, representing a set of enzyme-catalyzed reactions, and the known localization data for a subset of enzymes.
(b) Integrating the given network and localization data yields an initial compartmentalized network, consisting of several compartments.
Localized reactions appear in the corresponding compartments while the non-localized reactions are duplicated to all compartments.
(c) Mixed Integer Linear Programming (MILP) is applied for each pair of non-localized reaction and compartment to calculate a localization score, reflecting the likelihood of this reaction to be present in that compartment.
(d) Enzymes are predicted to be localized in compartments achieving the highest localization scores.
be either taken-up or secreted from the growth media.
Available metabolite localization data can be incorporated within S, by considering each row as an instance of a metabolite in a specific compartment, and each column as a reaction that involves metabolites in specific compartments (Duarte et al., 2004).
In this case, additional transport reactions (incorporated as additional columns in S) are used to represent metabolite translocation across compartments.
In addition to mass-balance, a-priori data on reaction directionality can be used to enforce flux rates to have positive values for i248 [10:02 15/5/2009 Bioinformatics-btp209.tex] Page: i249 i247i252 Network-based prediction of metabolic enzymes subcellular localization non-reversible reactions.
In some cases, additional flux capacity constraints are available and can be further applied.
The stoichiometric mass-balance, directionality and capacity constraints give rise to a space of feasible flux distributions that has been analyzed by various optimization methods (Price et al., 2004).
2.2 Constructing an initial compartmentalized network We construct an initial compartmentalized network model with k compartments, in which (i) all metabolites are present in all k compartments; (ii) transport reactions enable metabolite exchange between cytoplasm and all k compartments.
We denote by ti the transport reactions between the cytoplasm and compartment i; (iii) all reactions are present in all k compartments.
(iv) localized reactions can be activated (i.e.carry non-zero flux) only in their associated compartments, while non-localized reactions can be activated in all compartments.
We denote the set of compartments in which a localized reaction i can be activated by Ci.
In this model, a steadystate flux distribution, v= (v1,v2,...,vk,), should enforce the following stoichiometric mass-balance, flux directionality and capacity constraints: S v1 = k i=2 ti + e t1 (1) S vi =ti, i=2...k (2) vmin vi vmax, i=1...k (3) vij, i,j|Ci|>0 and j/ Ci (4) where, v1 denotes the flux distribution in the cytoplasm, and e denotes the set of enabled exchange reactions (i.e.all zero, except for ones for exchange reactions) that move cytosolic metabolites across the model boundaries.
Equation (1) enforces mass-balance in the cytosol, where all metabolites may be transported to all other compartments, and a subset of them exchanged with the growth environment.
To enable the activation of a significant fraction of the reactions in the network, we considered a rich media in which all exchange reactions in the network model are enabled.
Equation (2) enforces mass-balance for each compartment i.
Equation (3) enforces flux directionality and capacity constraints, once available in the model.
Equation (4) restricts flux activity of localized reactions only to their known compartments (for which the set of associated compartments Ci consists of at least a single compartment, i.e.|Ci|>0).
2.3 A MILP optimization for predicting enzyme localization scores To predict a subcellular localization of non-localized reactions, we employ MILP to predict a feasible flux distribution v in the initial compartmentalized network, in which: (i) a maximal number of localized reactions are activated (i.e.carry non-zero flux) in their known compartment, and (ii) a minimal number of transport reactions are activated.
The predicted flux distribution is used to infer the localization score for each reaction and compartment pair, as described below.
To enable counting of the number of localized reactions that are correctly activated in the right compartments, for each localized reaction i and its known compartment j, we define Boolean auxiliary variables, yi,+j ,y i, j , representing whether reaction i indeed carries non-zero flux in either the forward or backward directions in compartment j, respectively [following (Shlomi et al., 2008)]: vij +yi,+j (vmin,j )vmin,j,|Ci|>0 (5) vij +yi,j (vmax,j )vmax,j,|Ci|>0 (6) where denotes a minimal flux rate in which a reaction is considered as active.
We used =0.1, following previous studies (Shlomi et al., 2008), though other choices provided qualitatively similar results.
Using a similar method to count the number of activated transport reactions is problematic as it requires k n additional Boolean auxiliary variables, which is computationally intractable.
Instead, we employ a relaxation based on an L1 metric (Shlomi et al., 2005), minimizing the absolute value of fluxes rate through transport reactions (which requires no additional Boolean variables).
The resulting objective function of our MILP formulation is hence: maxvi ,y i,+,yi,, ti m j=1 (yi,+j +yi,j ) n j=1 p|tij | (7) where p denotes penalty per unit of flux through a transport reaction.
We choose a penalty value satisfying p << 1/(k n) (where k n equals the total number of transport reactions), such that the optimization criteria of maximizing the activity of localized reactions is given a higher priority over the optimization criteria of minimizing the activity of transport reactions.
Using this parameter, the optimization problem is equivalent to first, maximizing the number of localized reactions that are activated and only then, minimizing the flux through transport reactions.
The transport of currency metabolites that participate in a high number of reactions in the network (above 20; e.g.ATP, NADH, H2O) was not associated with a penalty score, as these metabolites are assumed to be present in all compartments.
The commercial CPLEX solver was used for solving MILP problems, on a Pentium-4 machine running Linux in dozens of seconds per problem.
Once an optimal flux distribution is computed via our MILP formulation, the localization of enzymes can be predicted based on their flux activity in various compartments.
However, due to the existence of alternative pathways in the network, alternative optimal flux distributions may exist, resulting in different localization predictions in each case.
To handle such alternative MILP solutions we employ a method similar to flux variability analysis (FVA) (Mahadevan and Schilling, 2003).
Specifically, for each non-localized reaction i and compartment j, we compute an optimal flux distribution using the above MILP method, while forcing the reaction to have non-zero flux specifically in that compartment.
The resulting optimization value is referred to as the localization score.
In the cross-validation tests described below, we predict for each non-localized enzyme either one or two compartments (based on the actual number of compartments the reaction is known to be localized to), by considering the compartments achieving the highest localization scores.
In case of an enzyme having the same optimal localization score for both cytoplasm and another compartment, the enzyme is predicted to be localized in the other compartment (to avoid bias due to the large size of the cytoplasm in which reaction activation is a-priori more likely to require a lower number of transporters).
When the same localization score is obtained for more than two compartments, we provide no localization prediction for that enzyme (lowering the prediction coverage).
2.4 An illustrative example of enzyme subcellular localization predictions An illustrative example of the prediction method is depicted in Figure 2.
In this example, the metabolic network contains 11 metabolites, 8 enzymatic reactions and 4 exchange reactions.
The initial compartmentalized network is shown in the figure, depicting three compartments (cytoplasm, compartmentA, compartment-B), and transport reactions for metabolites between cytoplasm and the two other compartments.
Localized reactions, given as input, are marked with thick edges (R2, R4, R6 and R8), while non-localized reactions (R1, R3, R5 and R7), whose localizations we wish to predict, appear in all three compartments.
We applied our MILP method to predict an optimal flux distribution for this example, and show the derived flux distribution within the network (i.e.marking activated, non-zero flux, reactions).
As shown, the flux distribution spans all three compartments, starting from the uptake of metabolite M1 and M10 from the growth environment, and ending with the secretion of metabolites M5 and M9.
Reactions R1 and R5 are predicted to be localized to compartment-A, as the transport of a single metabolite M1 into the compartment enables the production of metabolites M2 and M7, required for activating reactions R2 and R6 in this compartment.
Reaction R7 is predicted i249 [10:02 15/5/2009 Bioinformatics-btp209.tex] Page: i250 i247i252 S.Mintz-Oron et al.Fig.2.
An illustrative example of our method for enzyme subcellular compartment prediction.
The initial compartmentalized network consists of three compartments, with instances of 11 metabolites and 8 reactions in each compartment.
Thin edges connecting different instances of a metabolite in various compartments represent transport reactions that move metabolites across membrane boundaries.
Wide arrows represent localized reactions whose known localization is given as input to the prediction method.
Solid arrows represent reactions that are predicted to have non-zero flux by our method reflecting their predicted localization.
Dashed arrows represent reactions predicted to have zero flux.
to be localized in the cytoplasm, as its substrate M11 is produced by R8 solely in the cytoplasm, and hence activating R7 in a different compartment would require an unnecessary transport of M11 out of the cytoplasm.
An example in which the method cannot uniquely predict the localization of a certain reaction is in the case of reaction R3 that produces metabolite M4 from M3.
Activation of the localized reaction R4 in compartment-B requires that its substrate metabolite M4 would be present in this compartment.
Metabolite M3 is produced solely in compartment-A, and hence reaction R3 can be activated in compartment-A (with M4 being transported via two transporters to compartment-B), or M3 can be transported to the cytoplasm or to compartment-B and R3 activated in the cytoplasm or compartment-B, respectively.
In all three cases, the activation of R4 in compartment-B would have the same total cost of activating two transport reactions.
3 RESULTS 3.1 Validating the localization prediction via a metabolic network of Saccharomyces cerevisiae To evaluate the performance of our method, we applied it to predict enzyme localization for metabolic enzymes in the yeast S.cerevisiae.
Both the metabolic network and subcellular localization data for S.cerevisiae are available within the genome-scale, fully compartmentalized metabolic network model of (Duarte et al., 2004).
This network model accounts for 750 genes, 1062 metabolites and 1149 reactions, acting in seven compartments: cytosol, mitochondrion, peroxisome, nucleus, endoplasmic reticulum, golgi apparatus, and vacuole.
The cytosol is the largest compartment, consisting of 65% of the total metabolic reactions, followed by the mitochondrion, consisting of 16% of the reactions.
Fig.3.
Accuracy (a) and coverage (b) of enzyme subcellular localization predictions in a cross-validation test in the yeast S.cerevisiae.
The average and standard error of the accuracy and coverage measures were calculated based on 10 applications of the prediction methods over randomly sampled sets of localized enzymes of similar size that are used as input.
To evaluate our method, we first removed all existing localization data from the network model of Duarte et al., forming a new stochiometric matrix with a single merged compartment that can be given as input to our prediction method.
Then, we applied a cross-validation test, in which we randomly partitioned the enzymes to localized and non-localized sets and applied our method to predict the localization of the non-localized enzymes, given the localized enzymes and the metabolic network.
For the non-localized enzymes, we assumed that prior knowledge (obtained for example, via sequence-based prediction methods such as those described above) narrows down the list of potential localizations of enzymes to one out of four compartments, and hence restricts the activity of non-localized reactions in the model to three randomly chosen compartments in addition to the correct compartment.
The specific choice of restricting non-localized reactions to four compartments was made based on a comprehensive analysis of ten prediction methods applied to Arabidopsis thaliana, which showed that over 90% of its enzymes are predicted to be localized to no more than four compartments (Heazlewood et al., 2007).
To further evaluate the performance of our method, we compare it with an enrichment-based method that predicts subcellular localization based on an assumption that pathways are coherently localized in various compartments.
Specifically, for each non-localized reaction participating in a certain pathway we compute a hyper-geometric p-value reflecting the pathways enrichment with enzymes localized within each compartment.
The localization of the reaction is predicted based on the compartment that yields the lowest p-value.
The metabolic pathway data was also obtained from the metabolic network model of Duarte et al.The accuracy and coverage of our method in comparison with the enrichment-based method, for various fractions of localized enzymes input sets, are shown in Figure 3.
Since the known distribution of enzyme compartment localization is significantly skewed towards the cytoplasm, we present the accuracy and coverage statistics separately for cytosolic reactions and noncytosolic reactions (showing that our method correctly predicts localization in both cases).
The accuracy of our method is markedly i250 [10:02 15/5/2009 Bioinformatics-btp209.tex] Page: i251 i247i252 Network-based prediction of metabolic enzymes subcellular localization robust, remaining above 78% for both cytosolic and non-cytosolic reactions when the percentage of given localized reactions is as low as 20%.
The coverage shows a moderate decline from 88% and 79% for cytosolic and non-cytosolic reactions, respectively, given the 80% localized reactions as input, towards 78% and 63% in the case of 20% localized reactions.
This decline in coverage is quite expected, as when the set of localized reactions used as input is small, many reactions have the same likelihood of being active in various compartments.
The pathway enrichment-based method constantly achieves a markedly lower accuracy (except for the case in which only 20% localized reactions are used as input in which its coverage is minute), especially for the prediction of cytosolic enzymes.
The coverage of this method shows a slight advantage over our network-based approach for high fractions of localized reactions used as input.
However, the pathway enrichment-based method is shown to be highly non-robust when the fraction of given localized reactions decreaseswhen it reaches 20% its coverage significantly drops to below 10%.
The failure of the pathway enrichment-based method to match our network-based approach is somewhat expected, considering that many metabolic pathways do cross compartmental boundaries (as discussed above) and as it does not account for pathways intersection as shown below.
3.2 An example of predicting the subcellular localization of complete pathways We further tested our method in predicting subcellular localization for a set of enzymes with unknown localization that covers two complete pathways.
In such cases, a-priori localization data is not available for any enzyme in a certain pathway, and hence pathway enrichment-based methods are bound to fail, requiring a networkbased view of pathways connectivity.
Specifically, we aim to predict the localization of a group of enzymes composing the TCA cycle and the glyoxylate cycle, given the known localization all of the remaining enzymes in the network (Fig.4).
The set of enzymes to predict consist of 12 reactions: three ethanol fermentation reactions (with isozymes localized to both cytoplasm and mitochondria), four TCA cycle reactions (only in mitochondria; referred to as mitochondria-specific TCA cycle reactions), two glyoxylate cycle reactions [one in both peroxisome and cytosol and another only in cytosol) and three reactions involving both gloxylate and TCA (with isozymes localized in all three compartments; (Regev-Rudzki et al., 2005)].
The three reactions involved in the ethanol oxidation pathway are correctly predicted to be localized both in the cytosol and mitochondria.
The four mitochondria-specific TCA cycle reactions are correctly predicted to be localized in mitochondria and the three reactions involving both gloxylate and TCA pathways are correctly predicted to be localized in all three compartments.
The glyoxylate reaction localized to both peroxisome and cytosol is correctly predicted to be localized in the cytosol, though its second most likely localization is falsely predicted to be mitochondria, while only its third predicted localization is peroxisome.
A similar problem arises with the localization prediction of the glyoxylate reaction that is known to be localized only in the cytosol, where the method predicts a most likely mitochondrial localization.
The two false predictions for these reactions result from the utilization of their substrate metabolites also in mitochondria.
However, in both cases, the next most likely localization prediction given by our method is the correct one.
Fig.4.
Enzyme subcellular localization prediction of two complete metabolic pathways, including the TCA cycle (black rectangles) and glyoxylate cycle (grey rectangles), and a subset of the ethanol oxidation pathway (white rectangles), given localization data for enzymes in other connected pathways (white ellipses) as input.
Transport reactions are marked by dotted arrows.
3.3 Validating emergent subcellular localization predictions via GO annotation Following the cross-validation test, we turned to predict novel subcellular localizations of enzymes in the metabolic network.
Towards this goal, we re-ran our method on the same network in a leave-one-out cross validation setup, in which localization data for all reactions but one was used to predict the localization of that single reaction.
We found that our method predicts a noncytosolic localization for 22 reactions in the model although they are localized in the model to the cytosol.
Inspecting the GO cellular localization annotation for these reactions revealed that the localization of 10 out of the 22 was correctly predicted.
This prediction accuracy is statistically significant (p < 0.05) compared with a random assignment of genes to compartments (with an assignment probability for each compartment relative to its size in GO).
An example of such emergent localization predictions that are not accounted for in the model is the case of enzymes SUR2 (dihydrosphingosine C-4 hydroxylase), and TSC10 (3-ketosphinganine reductase), which catalyze consecutive reactions in the ceramide biosynthesis pathway.
Ceramides are formed as the key intermediates in the biosynthesis of sphingolipids, essential components of the plasma membrane.
This pathway is known to be accomplished by ER enzymes, some of which can also be localized to the cytosol (Natter et al., 2005).
SUR2 is known to be localized exclusively to the ER, although mistakenly it is localized strictly to the cytosol in the model.
TSC10 is experimentally localized to both the ER and the cytosol, though again, mistakenly it is localized in the model only to the cytosol.
Our method predicts the correct localization of both enzymes in the ER.
4 DISCUSSION This study presents a novel constraint-based modeling method for predicting subcellular localization of enzymes embedded in a i251 [10:02 15/5/2009 Bioinformatics-btp209.tex] Page: i252 i247i252 S.Mintz-Oron et al.metabolic network.
While constraint-based modeling was previously employed to predict many different metabolic phenotypes, this is the first application of this approach to predict subcellular localization.
The method is based on a parsimonious principle of minimal number of metabolite transports across compartmentsa novel concept in the context of metabolic network analysis, which enables the prediction of plausible flux distributions that are clustered within various compartments, but does not strictly enforce such a clustering in a hard-wired manner.
While previous methods have used PPI networks to improve localization predictions, relying on metabolic networks is advantageous as metabolic networks are readily available for hundreds of species.
Another advantage is in regard to the prediction of metabolic enzyme localization, as metabolic enzymes are less likely to yield PPIs.
However, an evaluation of the performance of PPI-based localization prediction methods when applied strictly to metabolic enzymes has yet to be performed.An inherent limitation of metabolic network-based localization prediction is that it is strictly limited to metabolic enzymes.
Furthermore, we note that our method is computationally more demanding than common methods that rely on standard supervised classification approaches, requiring multiple mixed integer optimizations.
We intend to apply this method to predict subcellular localizations in other organisms in which the true localization is unknown.
For example, in the plant model organism A.thaliana for which experimentally determined localization data is available for only 50% of the metabolic enzymes (Heazlewood et al., 2007).
Different aspects of proteins localization can be explored using this method, such as the dependency of localization on various growth media (John et al., 2006).
While here we rely on this principle of minimal metabolite exchange to partition a network to its subcellular compartments, future research may employ the same principle to partition metabolic networks into different subsystems.
For example, with the availability of a genome-scale human metabolic network model, a similar approach may be used to partition it to various tissue-specific subsystems.
Or in the realm of meta-genomics, similar approaches may be used to partition a metabolic network of population of species to species-specific subsystems.
Funding: Israeli Science Foundation grant (to E.R.
); A.A. is the incumbent of the Adolpho and Evelyn Blum Career Development Chair of Cancer Research; the work in the Aharoni laboratory was supported by Mrs Louise Gartner, Dallas, TX and Mr and Mrs Mordechai Segal, Israel.
Conflict of Interest: none declared.
ABSTRACT Motivation: Analysis of multiple genomes requires sophisticated tools that provide search, visualization, interactivity and data export.
Comparative genomics datasets tend to be large and complex, making development of these tools difficult.
In addition to scalability, comparative genomics tools must also provide userfriendly interfaces such that the research scientist can explore complex data with minimal technical expertise.
Results: We describe a new version of the Sybil software package and its application to the important human pathogen Streptococcus pneumoniae.
This new software provides a feature-rich set of comparative genomics tools for inspection of multiple genome structures, mining of orthologous gene families and identification of potential vaccine candidates.
Availability: The S.pneumoniae resource is online at http://strepneumo-sybil.igs.umaryland.edu.
The software, database and website are available for download as a portable virtual machine and from http://sourceforge.net/projects/sybil.
Contact: driley@som.umaryland.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on October 4, 2011; revised on November 10, 2011; accepted on November 20, 2011 1 INTRODUCTION The increasing throughput and reduced cost of second-and thirdgeneration DNA sequencing technologies has resulted in a deluge of genome sequence data in the public domain.
These require efficient bioinformatics tools enabling visualization and mining of multigenome comparisons.
Such tools are difficult to develop because of the size and complexity of such large-scale comparisons.
The challenge is to develop a tool that can be flexible, extensible and usable while providing the following four basic features: search, visualization, interactivity and export.
1.1 Search The Internet and the ubiquity of Google, Yahoo and Bing have proven that when datasets become extremely large, search becomes a critical entry point.
Internet searching is typically done based on unstructured keyword searches with some structured filters such as To whom correspondence should be addressed.
date or country of origin.
In the case of biological data, structured and unstructured searches are both important features.
Unstructured keyword searching gives users the power to find items of interest based on free text that matches against fields like gene name or gene function in the same way one might search the Internet.
Structured search parameters include filtering for source organism, feature length, presence of a particular motif and overlap with another feature of interest.
Both these search strategies combined can form the basis of a powerful tool with the ability to find a reasonable number of features of interest in an otherwise vast and inaccessible dataset.
BioMart (Smedley et al., 2009) is a widely used tool that integrates large datasets and provides advanced structured and unstructured search capability.
While not specifically designed for comparative genomics, BioMarts power includes its flexibility in handling data of many different types and formats.
It is in this spirit that search capability should be implemented into a full-featured comparative genomics tool.
1.2 Visualization Summarizing large datasets in pictures can reveal important patterns and results in the data and can serve as a valuable entry point for data querying.
Illustrating a small-scale genomic rearrangement or deletion is typically the most effective way to describe the event.
Existing comparative genomics visualization tools include linear genome browsers, dot-plot viewers and circular genome viewers (Nielsen et al., 2010).
Linear browsers draw pictograms of the genes in the genome with links between related genes or regions.
For example, SynView (Wang et al., 2006), Gbrowse-syn (McKay et al., 2010) and SynBrowse (Pan et al., 2005) are linear browsers useful for comparing genomic regions between a few genomes.
The UCSC genome browser (Kent et al., 2002) and VISTA (Frazer et al., 2004) are linear representations providing conservation plots that indicate regions where the genomes share conserved primary sequence.
Such conservation plots can provide a high-level visualization of whole genome alignments.
This type of view is most useful when looking at large, distantly related genomes.
Dot-plot viewers are an alternative way of visualizing whole genome alignment data with a particular emphasis on large-scale changes in synteny and indel detection.
Dot-plots are limited in that they can only compare a limited number of genomes at once and typically do not display annotated feature information.
The whole genome aligner Mauve has a built-in viewer that color-codes locally syntenic regions across several genomes (Darling et al., 2004).
GMAJ is another viewer that specializes in displaying whole genome alignment data as percent The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:14 24/12/2011 Bioinformatics-btr652.tex] Page: 161 160166 Sybil identity plots and dot-plots (Blanchette et al., 2004).
These tools are particularly useful when looking for large-scale rearrangements, insertions and deletions.
Circular representations like those from the Circos package (Krzywinski et al., 2009) draw genomes in a circle and draw arcs between regions of conservation.
These displays can capture much of the information of the linear browser with the added benefit of being compact and having fewer line crossings.
Circular browsers can also work at the whole-genome scale.
The challenge for comparative genomics visualization methods is including the ability to navigate seamlessly from a whole genome view that accurately depicts feature content and conservation information to a localized view that provides detailed imagery of genomic features and comparisons.
1.3 Interactivity While search and visualization can provide valuable entry points to large datasets, the ability to interact with these results turns a simple tool into a powerful application.
The power to sort results by any field, further refine results and navigate to visualizations makes search a jump-off point to more in-depth data mining.
A more difficult feature to implement is interactivity of visualizations.
Giving the user the power to obtain detailed information about drawn features, download sequence data and navigate to other views for a different perspective make an otherwise static graphic into another powerful entry point.
Gbrowse-based (Stein et al., 2002) tools have the ability to leverage mouseover events, which give these tools the power to hide detailed information from the visualization until requested.
Clicking on gene features in these tools can bring the user to a report page with additional details about the gene.
Similarly, the UCSC genome browser provides several links to more detailed information about the different features it can display.
These features allow the user to begin at a particular search result or graphic and investigate features of interest without having the impression of reaching a dead end.
1.4 Data export Export is essential in allowing users the flexibility to take data and results out of the system for publication and further inspection.
In large comparative genomics datasets, interactive search and visualization capabilities are used to identify a set of genes, gene clusters or genomic regions of particular interest.
Export of these data allows for additional analyses outside of the tool.
Data export can include the ability to capture a vector graphics-based publication quality image, a table or a FASTA file for use in downstream bioinformatics analyses.
1.5 Sybil With Sybil, we sought to combine interactive search, visualization and data export in a dynamic web-based comparative genomics software package that provides the freedom to analyze many data types in a comparative genomics context (Crabtree et al., 2007).
The Sybil suite of feature-rich web tools allows users to search and visualize several genomes based on clusters of orthologous genes.
The views in Sybil are interactive allowing users to click on each drawn feature to retrieve more detailed information.
Navigation between views is supported and provides Sybil with the unique ability to explore from the whole genome level to the protein and nucleotide multiple sequence alignment level without leaving the application.
Export of tab-delimited tables, sequence data, and SVG and PDF publication-quality figures is possible from many of the views.
The Strepneumo Sybil-based website http://strepneumo-sybil.igs.umaryland.edu was developed as a comparative genomics resource to house all published, annotated genome sequences of the human pathogen Streptococcus pneumoniae.
The aim of the project focused primarily on combining annotated genomics data in a comparative context with several additional data types related to vaccine candidate identification.
Infections caused by S.pneumoniae continue to be a major cause of human morbidity and mortality worldwide (OBrien et al., 2009).
This is a result of the pathogens extreme adaptability that leads to antibiotic resistance and escape from the current polysaccharide capsulebased vaccines (Mera et al., 2008).
There is a crucial need for novel candidates for the development of efficacious protein-based vaccines.
Efficient mining of the wealth of genome sequence data available for S.pneumoniae will open novel avenues for vaccine development.
2 METHODS 2.1 Clustering Multigenome homologous gene clusters are the primary comparative data type in Sybil.
These clusters must be pre-computed and loaded into a relational database that supports the Sybil software.
Gene clusters are treated as generic objects and alternative clustering methods can be used.
For the Strepneumo site, the clusters were generated using a modified form of reciprocal best BLAST (Altschul et al., 1990) match corrected for paralogs as described in Crabtree et al.(2007).
In brief, an all-vs.-all BLASTP with an e-value cutoff of 1e-05, a percent identity cutoff of 80% and a percent coverage cutoff of 70% results in a hit graph.
The hit graph is used to first identify homologs within a single genome, termed paralogs.
Paralogs are grouped by computing a Jaccard similarity coefficient (Jaccard, 1908) for each pair of proteins and applying a cutoff of 0.6.
The resulting groups of paralogs and singleton genes are used as input to identify clusters of genes across genomes or orthologs.
In the most basic example, two genes are grouped in an ortholog cluster if they are from different genomes and are each others best hit, or reciprocal best BLAST match.
When orthology involves Jaccard paralog clusters, all the genes in the cluster are treated together such that any member that is a best hit can cluster a paralog cluster with a singleton gene or another paralog cluster.
For example, if genes A + B are paralogs in genome X and genes C + D are paralogs in genome Y.
If gene As best hit in genome Y is C but Cs best hit in X is B these four genes will end up in an ortholog cluster together despite not having an individual reciprocal best hit.
For Strepneumo, this pipeline is implemented in an instance of Ergatis (Orvis et al., 2010) and results are loaded into a relational database that supports Sybil.
2.2 Data types In addition to comparative computes, Sybil supports a number of additional data types that can be visualized and used as search criteria.
These data types fall into two major categories: genome data and protein/gene data.
Genome data is the result of a search or analysis that finds features located on the raw genome sequence and therefore has genome coordinates.
In the case of Strepneumo, a simple repeat library was used along with Repeatmasker (Smit et al., 1996) to predict simple repeats, Phobos (Mayer, 20062011) was used to identify tandem repeats, HMMER (Eddy, 1998) was used to identify several genomic repeats particular to S.pneumoniae and genomic islands were identified using IslandPath (Hsiao et al., 2003).
Protein/genebased data are associated or located on genes and are the result of an analysis 161 [10:14 24/12/2011 Bioinformatics-btr652.tex] Page: 162 160166 D.R.Riley et al.Table 1.
Feature types loaded into the Strepneumo Sybil system Feature name Tool No.
of features found Genome features Simple repeat Repeatmasker 3470 Homopolymeric tract Custom Perl 7964 Tandem repeat Phobos 17223 BoxA repeat HMMER 4368 BoxB repeat HMMER 6111 BoxC repeat HMMER 4057 RUP repeat HMMER 3555 Genomic island IslandPath 162 Protein features Signal peptide SignalP 13899 B-cell epitope BepiPred 10240 Antigenic region EMBOSS antigenic 9668 Lipoprotein Custom Perl 1647 LPxTG HMMER 1264 Membrane/surface motif HMMER 6398 Bacteriocin motif HMMER 195 Fibronectin motif HMMER 34 Transmembrane region tmhmm 79543 Subcellular localization psortb NA These features are divided into Genome features, found and located on genomic coordinates, and Protein features, found and located on polypeptide sequences.
Detailed information on each can be found in the Supplementary Table S1.
that operates on protein or nucleotide gene sequences.
In this case, the analysis can yield either a feature with gene coordinates or it can predict or identify a particular property of the target gene.
Strepneumo contains signal peptides from SignalP (Bendtsen et al., 2004), B-cell epitope predictions from BepiPred (Larsen et al., 2006), transmembrane regions from tmhmm (Krogh et al., 2001) and subcellular localizations from psortb (Yu et al., 2010).
The psortb subcellular localizations exemplify an analysis that gives a gene a particular property but does not predict features with coordinates.
The EMBOSS tool antigenic (Kolaskar and Tongaonkar, 1990) was used to predict antigenic regions.
In addition, several protein motifs were predicted using hmmpfam (Eddy, 1998) including LPxTG Gram-positive cell wall anchors, as well as bacteriocin and fibronectin binding sites.
Any data type with either genomic or gene coordinates can be used in the system.
New data types must be loaded into the underlying database and configured in the Sybil site configuration file.
A complete list of the data types provided in the Strepneumo Sybil site is found in Table 1 with more details in Supplementary Table S1.
2.3 Data storage All data are stored in a Chado (Mungall and Emmert, 2007) relational database using PostgreSQL.
The results of the protein and genomic feature searches are localized to the relevant protein and genome sequences, respectively.
Clusters are stored as match features with member proteins linking to this central match.
The Chado schema is designed with a high degree of abstraction and normalization resulting in reduced performance with very large datasets.
To address this, a data mart was developed called ChadoMart.
This mart consists of three tables: cm_proteins, cm_clusters and cm_cluster_members.
The cm_proteins table contains de-normalized information about each protein while the cm_clusters table includes summary statistics on all protein clusters.
The cm_cluster_members table links the cm_clusters table with the cm_proteins table, mapping all the proteins to their respective clusters and vice versa.
These three tables contain the data most frequently queried by Sybil.
Therefore, using ChadoMart significantly improves the website performance.
In addition to the ChadoMart, server-side query caching was implemented to further improve performance.
For overlap or ranged queries an interval tree is used.
Using an interval tree was found to be much faster than doing large table joins in the relational database.
The dataset can be quite large with the all vs. all BLAST results constituting a majority of the data.
In the most recent version of the Strepneumo data, the BLAST results accounted for 10.5 million of the 11 million rows in the central feature table.
Therefore, for distribution of the data for local use, the BLAST results can be left out of the database, allowing more modest hardware to run the system successfully.
2.4 Visualization/user interface The Sybil website is composed of a combination of traditional standardbased HTML/CSS and JavaScript along with elements from the JavaScript framework ExtJS.
HTML/CSS and JavaScript follow the HTML 4.01 standard and the implementation of the site is designed to work in most modern web browsers.
The use of asynchronous JavaScript server requests (AJAX) gives Sybil the feel of an application rather than a website.
Complementing the traditional HTML/CSS/JavaScript is the use of ExtJS, a JavaScript library that speeds development of feature-rich web applications.
ExtJS is used exclusively on the Protein/Cluster Search Pages and the Genomic Comparative View.
The ExtJS grid on the search pages provides useful functionality like sorting, column reordering and pagination.
ExtJS windows are used as informational popups when rendered features are clicked and ExtJS ToolTips are used to display help information when a user mouses over a box with a question mark (?)
as shown in Figure 1.
Visualization is central to the Sybil system and is accomplished using two different methods.
The first leverages the BioPerl (Stajich et al., 2002) Bio::Graphics Perl package.
An additional Sybil-specific glyph allows visualization of the cluster relationships across genomes (Fig.2).
Bio::Graphics is used for the Genomic Comparative View, Cluster Report and Protein Report pages.
In general, this package is good at displaying comparative data for up to 100 kb of bacterial sequence.
The second method is a custom module called Sybil::Graphics::GenomeImage developed as part of the new implementation of Sybil.
GenomeImage currently leverages SVG natively.
This package is used for drawing whole-genome scale images such as the Gradient View (Fig.3) and the Whole-Genome Display (Fig.4).
GenomeImage allows for the drawing of large amounts of data with less detail than Bio::Graphics while still retaining an interactive image map for web display.
3 RESULTS 3.1 Data mining and visualization A number of interactive visualization and querying tools were developed to aid interactive analysis of the S.pneumoniae data.
Effort was primarily focused on providing tools that are responsive and informative for comparisons of many genomes using a web browser.
Particular attention was paid to supporting navigation between the various Sybil views.
Providing this capability gives Sybil the feel of a fully integrated application rather than a set of independent web pages.
3.1.1 Search Sybil provides extensive search capability, and the results are interactive and exportable.
Gene queries can be initiated using locus identifiers, database accessions, gene names and protein functions.
Results can be filtered by taxonomy, length, subcellular location, protein motifs, overlap with genomic features, membership in orthologous clusters and BLAST matches.
The orthologous gene clusters are also searchable based on the same criteria as gene searches with the addition of taxonomic profile and cluster size.
The search results are displayed in a paginated table in the web interface as shown in Figure 1.
The data can be sorted by any column, 162 [10:14 24/12/2011 Bioinformatics-btr652.tex] Page: 163 160166 Sybil Fig.1.
The Sybil search page is made up of two panels: a search form on the left and a table of results on the right.
The form on the left provides a variety of search options including free text, length filters, overlapping feature filters and more.
The table on the right shows the results of the search in a paginated form.
The table includes links to other Sybil pages for more detailed information and visualization.
The data can be exported as a tab-delimited or multi-FASTA file.
Fig.2.
The Genomic Comparative View provides the ability to visually compare genomic regions.
Genes in each region of each genome are drawn with arrowheads indicating their orientation.
Genes that share a common cluster are connected via a shaded polygon.
Additional genome/protein feature types are drawn as different shapes in different colors in their respective positions.
This particular image shows the deletion of several genes in the top genome.
and links are provided to gene and cluster report pages.
Results are also downloadable as a tab-delimited text file or FASTA file for downstream analysis.
3.1.2 Cluster report page/genomic comparative display Visualizing genomic comparisons is central to Sybils power.
Both the cluster report page and the genomic comparative page allow the user to view genes and other genomic features drawn in a logical linear fashion (Fig.2).
Gene cluster relationships are easily visible providing clear illustration of insertions, deletions, rearrangements and annotation/sequencing anomalies.
On the cluster report page, the cluster of interest is drawn in the center with pink shading.
The genomic comparative display supports two drawing modes: search mode and edit mode.
Search mode allows the user to specify a reference region and search query genomes for similar regions.
Edit mode allows the user to manually define all viewable regions and can also be used to perfect a view to create publication ready figures.
Clicking on genes, gene clusters or any protein or genomic feature will generate a popup with data pertinent to the clicked feature and links to other views/pages.
This allows Sybil displays to forego displaying labels for all features, decluttering complex images.
Clicking on a gene or gene cluster will provide several pieces of data and additionally provide a link to center on this gene or cluster in the Genomic Comparative View.
Several links are provided allowing the user to center on the gene or cluster in the Genomic Comparative View with a variety of base pair pads and search other genomes in the dataset for homologous regions.
These links give the user the power to explore regions of interest more fully without leaving the system.
Multi-FASTA files of protein or nucleotide sequences can be exported for external analyses and publication quality figures can be exported in SVG or PDF format using buttons along the bottom of the page.
3.1.3 Whole genome display/synteny gradient display The synteny gradient display enables the visualization of changes in synteny relative to a reference (Fig.3).
In this view, a reference genomes genes are colored from yellow to blue on a gradient from left to right.
If a query genome shares a cluster with a reference gene, then it is drawn above the matching reference gene but using a color that corresponds to the query genes position in its native genome.
The resulting figure reveals conservation of the color gradient in syntenic regions while shared genes located in rearrangements will 163 [10:14 24/12/2011 Bioinformatics-btr652.tex] Page: 164 160166 D.R.Riley et al.Fig.3.
The synteny gradient display is a unique representation of conserved gene content/order between several genomes that can be used to identify rearrangements as well as large-scale insertions/deletions.
The view is based on a reference sequence (in this case S.pneumoniae TIGR4) that is drawn in the bottom panel.
Genes in the reference genome are colored yellow to blue from left to right.
White in the reference denotes a region with no gene annotation.
Matching genes in each of the 33 query sequences are drawn atop their reference match, i.e.not in the order in which they appear in their respective genomes.
To highlight rearrangements and conserved synteny, the matching genes are colored based on the relative position in their respective genomes (yellow for the beginning and blue for the end).
Genes shown in black are part of a paralogous cluster in their respective genome and therefore do not have a single native location.
GC-skew, %GC and atypical nucleotide composition are plotted for the reference genome.
Fig.4.
This whole genome display illustrates Sybils data type flexibility.
In this figure, all the genes from an entire S.pneumoniae genome are drawn at the top.
Below the gene track are several additional annotations listed in Table 1.
Any feature that has a genomic or gene coordinate can be drawn.
For signal peptides and trans-membrane regions, the density of features is drawn.
Lastly, % GC, GC-skew and atypical nucleotide composition plots are drawn.
show up as color mismatches between the reference and query genomes.
Gaps in query genomes represent genes that are present in the reference but not the query.
Query genes with paralogs will show up as either colorless or black depending on the users preference.
Multiple drawings can be made at once with each genome serving as the reference if the user chooses.
Additionally, the genes in the reference are clickable.
This generates a popup with information about the gene, its clusters and links to other Sybil views or pages.
Images can be exported as high-quality SVG or PDF files for publication or other uses.
As the name implies, the whole genome display provides visualization of an entire genome or chromosome (Fig.4).
This 164 [10:14 24/12/2011 Bioinformatics-wwwaspgd.org] Page: 165 160166 Sybil display draws an entire genome in a linear and compact fashion with genes drawn as vertical bars.
The bars are clickable and more information is provided in popups including links to the genomic comparative display, and cluster and protein report pages.
The power of this display is the ability to draw additional genomic data types and graphs of feature densities.
This view also allows filtering the displayed genes by gene function keyword.
Like the synteny display, images are exportable as SVG or PDF.
3.2 Strepneumo Sybil The Sybil system has been leveraged to explore the genomes of the human pathogen S.pneumoniae.
Currently, the Strepneumo public website houses 34 streptococcal genomes (33 S.pneumoniae and 1 close relative, S.mitis) comprising 76 539 genes with a total of 3407 ortholog clusters.
In addition to the cluster data, several other data types are present, including genome features (e.g.genomic islands, repeats) and protein features (trans-membrane spans, cell wall anchors) (Table 1).
These data types can be used to search for genes and gene clusters and can also be visualized in all the Sybil views.
3.2.1 Potential vaccine candidates Many of the data types used in Strepneumo were inspired by the need to identify potential vaccine candidate genes for the purpose of reverse vaccinology (Sette and Rappuoli, 2010).
These features and properties have been used in combination to find genes that are exposed on the cell surface and could therefore be accessible to the immune system.
Antigenicity and epitope prediction are also used to identify protein regions that have the greatest chance of interacting with the immune system.
The goal is to improve reverse vaccinology by reducing the number of predicted vaccine candidates such that downstream experimental confirmation is less burdensome.
No single analysis is a good predictor for whether a gene will make a good vaccine candidate.
However, combining a variety of properties and visualizing them across the different strains can identify a reduced set of likely candidate genes.
3.2.2 Strepneumo-Sybil virtual machine While Strepneumo is freely available to everyone with access to an Internet connection, some situations like travel or unreliable Internet connections may necessitate a local copy of the tool.
The Sybil package requires considerable expertise to install, therefore distribution of mirrors and local copies of the Strepneumo Sybil system was accomplished via virtual machines (VM).
The Strepneumo VM leverages the CloVR (Angiuoli et al., 2011b) architecture, contains an install of Ubuntu Linux and comes pre-loaded with all the dependencies required for running Sybil and the Strepneumo website.
The virtual machine is 1.7 GB in size when compressed and is available for download from the Strepneumo website.
Any computer that can run the 64-bit VMWare player, has at least 15 GB of disk space and 2 GB of RAM can run the Strepneumo Sybil website locally.
3.3 Sybil screencasts One often-overlooked aspect of Bioinformatics software development is documentation and training.
For Sybil it was decided that documentation would be part of the tool.
To accomplish this goal, descriptions of the various panels and their use are available by hovering the cursor over the ?
boxes at the top of each panel.
In the case of complex features like the protein/cluster search page, cluster and protein reports, and Genomic Comparative View, video screencasts were recorded and uploaded to YouTube (www.youtube.com/SybilScreencasts).
Each screencast is 10 min in length and describes each of the basic features of the tool.
The screencasts are narrated and the watcher observes the tool in use.
These screencasts can be accessed by going directly to the SybilScreencasts YouTube page or by hovering over the ?
icon in one of the tools where a screencast is available.
4 DISCUSSION 4.1 Sybil VM In order to provide a system that is portable, deployable and scalable, the Strepneumo Sybil website was deployed using the CloVR virtual machine.
The VM proved to be invaluable in setting up the system in two remote sites: the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) in India and the International Livestock Research Institute (ILRI) in Kenya.
These two sites provided mirrors of the Strepneumo Sybil instance by running a VM based on the CloVR project.
These installations did not require travel and were frequently updated in a matter of a few days with minimal effort.
The computes that form the basis of the Sybil Strepneumo system are complex, long running and require significant computational resources.
For example, the computes for Strepneumo data version 15 required 136 CPU hours.
Planned integration of these compute pipelines into the CloVR VM will allow users with limited resources to set up Sybil websites with their own data by allowing them to run their computes in a cloud environment with minimal expertise.
4.2 Genomic data The Sybil system is designed to work with genomic data from any type of organism.
In addition to numerous instances involving bacterial genomes, including the public Strepneumo site described here, the system has been employed for the eukaryotic Aspergillus database AspDB at www.aspgd.org (Arnaud et al., 2010) to provide comparative genomics capabilities.
Internally, the system has been used to compare several protozoan parasites.
These capabilities can be as easily leveraged for archaea, other eukaryotes, or viruses.
Whole genome shotgun second-and third-generation sequencing approaches result in large numbers of draft or unfinished genome sequences being used as the final product.
Sybil accepts unfinished genome data, but it does not attempt to connect together any separate contigs.
This can cause issues when looking for regions of synteny as in the gradient display and the genomic comparative display.
Currently, this can be addressed by building pseudomolecules, where contigs are ordered and oriented based on a reference genome and made into a single FASTA entry representing one prediction of the genome.
However, this can incorrectly be perceived as the true order of the genome when the information is actually lacking.
One way to address this issue is incorporation of whole genome alignment data.
Generation of gene clusters based on data from the whole genome aligner Mugsy (Angiuoli and Salzberg, 2011) has already been accomplished and extension of this work to dynamically order previously unordered contigs will give Sybil 165 www.aspgd.org [10:14 24/12/2011 Bioinformatics-wwwaspgd.org] Page: 166 160166 D.R.Riley et al.the power to better visualize draft genome data (Angiuoli et al., 2011a).
4.3 Optimizations and scalability As datasets grow, the Sybil system will need to implement more aggressive optimizations to maintain acceptable performance.
The Strepneumo Sybil system contains 11 million rows in the feature table with 22 million feature locations and 43 million feature properties.
The normalized structure of the Chado database can require numerous table joins, making some queries slow.
Optimizations like the ChadoMart and query caching have allowed the system to scale to near 50 bacterial genomes, but other strategies are needed for larger data.
A prototype NoSQL document-based database was benchmarked using MongoDB.
This method of data storage does away with the relational schema and opts for a schemaless data model where every gene is a conceptual document.
Using MongoDB as a primary data store allowed the system to scale to >100 56 MB bacterial genomes.
The bottleneck at this level becomes the visualization strategies used since graphic drawing in Sybil is done on demand.
Future iterations of the system will likely require a refactored drawing strategy to provide the flexibility and interactivity that has become standard in the current Sybil system.
5 CONCLUSION As is typical in bioinformatics, the biological problem defines the needed software.
Development of Sybil has been focused on manual data mining efforts, which give the user the power to navigate countable numbers of genomes in search of potentially important biological features.
As datasets become larger and less manageable, the comparative genomics tools of the future will need to adapt to provide meaningful information without overwhelming the user.
Sybil has addressed this problem by transitioning from a system capable of drawing a handful of genomes to one that is able to handle >100 genomes.
ACKNOWLEDGEMENTS We thank Etienne de Villiers at ILRI and Trushar Shah at ICRISAT for their efforts in setting up the remote mirrors.
We also thank Dave Kemeza and IGS IT for their work maintaining the IGS grid compute, database and web-hosting infrastructure.
Funding: Platform for Appropriate Technology in Health (PATH) Contracts (GAT.0782-01990-COA and 003024).
Conflict of Interest: none declared.
ABSTRACT Summary: CD-HIT is a widely used program for clustering and comparing large biological sequence datasets.
In order to further assist the CD-HIT users, we significantly improved this program with more functions and better accuracy, scalability and flexibility.
Most importantly, we developed a new web server, CD-HIT Suite, for clustering a user-uploaded sequence dataset or comparing it to another dataset at different identity levels.
Users can now interactively explore the clusters within web browsers.
We also provide downloadable clusters for several public databases (NCBI NR, Swissprot and PDB) at different identity levels.
Availability: Free access at http://cd-hit.org Contact: liwz@sdsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 17, 2009; revised on December 7, 2009; accepted on January 2, 2010 1 INTRODUCTION The size of the biological sequence databases is rapidly growing due to large-scale genome projects and the emerging field of metagenomics (Yooseph et al., 2007).
New sequencing technologies are now producing sequence data at a very high rate, and this has created a greater need for bioinformatics tools to effectively organize and analyze the data.
Fortunately, biological sequences are related and may share homology, and thus clustering these sequences into groups and finding a representative or a consensus for each group are practical ways to solve the sequence analysis problems.
Our previous works (Li and Godzik, 2006; Li et al., 2001; Li et al., 2002) introduced CD-HIT based on short word filtering and a greedy incremental clustering algorithm to cluster and compare large biological sequence datasets.
One advantage of CD-HIT is its ultrahigh speed and the ability to handle large datasets.
Since its release, CD-HIT has been widely used by many groups in various fields, including UniRef (Suzek et al., 2007), SMART (Letunic et al., 2009) and metagenome data analyses (Turnbaugh et al., 2009; Yooseph et al., 2008).
In the last few years, we have been continuously improving this program with more functions and better accuracy, scalability and flexibility.
We also implemented a new web server to allow To whom correspondence should be addressed.
Present address: Department of Medicine, University of California San Diego, La Jolla, CA, USA.
users to cluster or compare sequences without installing and executing the command-line version of CD-HIT locally.
The server provides interactive interface and additional visualization tools.
It also provides precalculated and regularly updated sequence clusters for several widely used databases, including NCBI NR, Swissprot and PDB.
2 METHODS AND IMPLEMENTATION The detailed algorithms and benchmark results for CD-HIT can be found from our previous works (Li and Godzik, 2006; Li et al., 2001; Li et al., 2002).
Here, we highlight the novel features and functions.
2.1 Improved clustering algorithm The original CD-HIT uses a fast greedy incremental clustering process.
Briefly, sequences are first sorted by decreasing length.
The longest one becomes the representative of the first cluster.
Then, each remaining sequence is compared with the existing representatives.
If the identity with any representative is above a given threshold, it is grouped into that cluster without comparing it to other representatives.
Otherwise, it becomes the representative of a new cluster.
In the updated CD-HIT, we added a refined greedy incremental clustering process that produces more accurate clusters.
In this process, a sequence is grouped into the most similar cluster instead of the first similar cluster.
The refined process does not change the representative sequences.
CD-HIT uses a short word filter to avoid unnecessary alignments.
In short, the minimum number of identical short words (k-mers) shared by two sequences depends on their sequence identity and can be calculated analytically or statistically.
Without an actual alignment, we can still determine that the identity of two sequences is below a given threshold by counting short words.
A short word filter performs much better with a higher identity threshold.
Clustering in the refined process is implemented with a dynamic short word filter.
For each sequence to be clustered, the initial filter matches the user-defined identity threshold.
But during the clustering procedure, if this sequence hits any cluster with better identity, the filter is reset to match this better identity to increase the performance of the filter.
With the dynamic short word filter, although the refined clustering process needs to evaluate the similarities of a sequence and all the existing representatives, it only requires about 1.53 CPU time of the original process.
2.2 Improved clustering control The original CD-HIT uses global sequence identities.
The improved CDHIT also works with local identities.
Users can finely control the clustering behavior by including more criteria besides sequence identity cutoffs.
We include alignment length, unaligned length and alignment coverage for both aligned sequences as new clustering parameters into the current CD-HIT.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:16 12/2/2010 Bioinformatics-btq003.tex] Page: 681 680682 CD-HIT Suite For example, users can make clusters of sequences of similar length by specifying that the alignments must cover both sequences at similar coverage.
2.3 Clustering at low identity thresholds The performance of the native short word filter drops significantly with a lower identity threshold; therefore, the original CD-HIT does not provide protein clustering under 40% identity.
However, clustering at low identities has been frequently requested by CD-HIT users.
We implemented a script, called PSI-CD-HIT, to perform protein sequence clustering at a low identity threshold such as 30%.
It uses the similar greedy incremental clustering strategy, but it uses BLAST to calculate the similarities.
So users can also specify an expect-value cutoff.
PSI-CD-HIT runs on a stand-alone computer or a LINUX cluster.
It can cluster a PDB-sized dataset in 20 min.
2.4 Hierarchical clustering In the hierarchical clustering process, the program first performs clustering on the original input dataset at a high identity threshold, and the representatives of each previous clustering step will be the input of the following clustering run at a lower identity threshold.
The whole process iteratively joins the similar sequences into families and therefore produces a hierarchical structure.
For protein sequences, the last step is performed with PSI-CDHIT if the final identity threshold is <40%.
This strategy can maximize the computational efficiency and the quality of clustering.
We have applied such strategy in a protein family analysis of a large metagenomic dataset with 17 million sequences (Li et al., 2008).
2.5 Annotation enrichment of sequence cluster We provide an option for joint analysis of sequence clustering and annotation information.
Users can place annotation terms (Gene Ontology, protein family, etc.)
in the definition lines of input FASTA files.
For each annotation term A and each cluster C, we use the following numbers: NAC = number of sequences with A in C; NC = number of sequences in C; NAI = number of sequences with A in the input; NI = number of the input sequences.
A P-value is calculated using the one-tailed Fishers exact test to assess whether NAC/NC >NAI/NI and annotation term A is enriched in cluster C. Such functionality is very useful to check the cluster quality at different identity levels and also for function assignment of proteins with unknown function.
2.6 Web server All basic functions of CD-HIT are provided through tab-based interfaces in our web server.
We provided CD-HIT (CD-HIT-EST) to cluster a protein (DNA/RNA) dataset.
Users can upload a FASTA file and select a desired sequence identity level and other parameters.
CD-HIT-2D (CD-HIT-EST-2D) can compare two databases uploaded by users.
H-CD-HIT and H-CD-HITEST in our server performs hierarchical clustering up to three steps.
After submitting a clustering or comparison job, a unique identifier will be assigned.
A user can use the identifier to track the status of the job.
After the job is finished, we provide the raw outputs generated by the command-line CD-HIT.
Additionally, we provide tools to visualize the clustering results with cluster explorer and cluster distribution plots.
Cluster explorer uses a tree structure to represent the clustering results Figure 1a.
Each cluster is represented by a clickable text object on the web page, and users can click on a representative sequence to retrieve information of the sequences belong to the cluster.
This option is most useful for investigating the results Fig.1.
Screenshots of CD-HIT Suite.
(a) Cluster Explorer for investigating clusters.
(b) A cluster distribution plot to explore the global structure of a whole dataset.
from hierarchical clustering.
In this situation, each sequence could be a representative sequence from the previous clustering step, and users can click it to explore the results from the previous clustering.
Cluster distribution plots are scatter plots where the X-axis is the cluster size (number of sequences in a cluster), and then the Y-axis represents the number of clusters of at least this size and the number of corresponding sequences Figure 1b.
This tool is very useful to observe the global structure of a sequence database.
3 CONCLUSION CD-HIT has been significantly improved from our previous work.
CD-HIT Suite provides users with a friendly web interface to perform biological sequence clustering and comparison with additional visualization tools.
It also provides precalculated clusters for several public sequence databases which are regularly updated.
ACKNOWLEDGEMENTS We thank Mr Michael Chiu for his excellent editorial assistance.
Funding: National Institutes of Health (1R01RR025030) from National Center for Research Resources.
Conflict of Interest: none declared.
ABSTRACT Motivation: Efficient and fast next-generation sequencing (NGS) algorithms are essential to analyze the terabytes of data generated by the NGS machines.
A serious bottleneck can be the design of such algorithms, as they require sophisticated data structures and advanced hardware implementation.
Results: We propose an open-source library dedicated to genome assembly and analysis to fasten the process of developing efficient software.
The library is based on a recent optimized de-Bruijn graph implementation allowing complex genomes to be processed on desktop computers using fast algorithms with low memory footprints.
Availability and implementation: The GATB library is written in C++ and is available at the following Web site http://gatb.inria.fr under the A-GPL license.
Contact: lavenier@irisa.fr Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 25, 2014; revised and accepted on June 20, 2014 1 INTRODUCTION The analysis of next-generation sequencing (NGS) data remains a time-and space-consuming task.
Many efforts have been made to provide efficient data structures for indexing the terabytes of data generated by the fast sequencing machines (Suffix Array, BurrowsWheeler transform, Bloom filter, etc.).
Genome assemblers such as Velvet (Zerbino and Birney, 2008), ABySS (Simpson et al., 2009), SOAPdenovo2 (Luo et al., 2012), SPAdes (Bankevich et al., 2012) or mappers such as BWA (Li and Durbin, 2009) or variant detection such as CRAC (Philippe et al., 2013) for instance make an intensive use of these data structures to keep their memory footprint as low as possible.
At the same time, parallelism has been largely investigated to reduce execution time.
Many strategies such as GPU implementation (Liu et al., 2012), cloud deployment (Zhao et al., 2013), algorithm vectorization (Rizk and Lavenier, 2010), multithreading, etc., have demonstrated high potentiality on NGS processing.
The overall efficiency of NGS software depends on a smart combination of data representation and use of the available processing units.
Developing such software is thus a real challenge, as it requires a large spectrum of competence from high-level data structure and algorithm concepts to tiny details of implementation.
The GATB library aims to ease the design of NGS algorithms.
It offers a panel of high-level optimized building blocks to speedup the development of NGS tools related to genome assembly and/or genome analysis.
The underlying data structure is a memory efficient de-Bruijn graph (Compeau et al., 2011), and the general parallelism model is multithreading.
The GATB library targets standard computing resources such as current multicore processor (laptop computer, small server) with a few gigabytes of memory.
Hence, from the high-level C++ functions available in the GATB library, NGS programing designers can rapidly elaborate their own software based on state-of-the-art algorithms and data structures of the domain.
Based on the same idea, other bioinformatics libraries exist, from which domain-specific tools can be elaborated.
The NGS++ library (Markovits et al., 2013) is specifically tailored for developing applications that work with genomic regions and features, such as epigenomics marks, gene features and data that are associated with BED type files.
The SeqAn library (Doring et al., 2008) is a general-purpose library targeting standard sequence processing.
Advanced data structures such as de-Bruijn graphs are not included in SeqAn.
Khmer (Crusoe et al., 2014) is a library and toolkit for doing k-mer-based NGS dataset analysis.
As with GATB, most of khmer relies on an underlying probabilistic data structure (Bloom filter).
The khmer library can be used in various k-mer processing such as abundance filtering, error trimming, graph size filtering or partitioning.
2 METHODS One of the main concerns of the GATB-core library is to provide computing modules able to run on standard machines, i.e.computers not requiring large amount of main memory.
The central data structure is a de-Bruijn graph from which numerous actions can be performed as shown Figure 1: data error correction, assembly, biological motif detection [e.g.single nucleotide polymorphism (SNP)], etc.
The graph is constructed by extracting and by counting all the different k-mers from one or several sequencing datasets.
This timeand space-consuming task is conducted by a disk streaming algorithm, DSK (Rizk et al., 2013), which adapts its memory requirement according to the available computer memory.
Trade-off between execution time and memory occupancy can be set up: the larger the computer memory, shorter the computation time (reduced disk access).
The de-Bruijn graph memory footprint is kept low thanks to an optimized Bloom filter representation (Chikhi and Rish, 2012; Salikhov et al., 2014).
Only vertices of the de-Bruijn graph are memorized.
Edges are*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com mailto: lavenier@irisa.frvery , deduced by querying the Bloom filter.
False positives (owing to the probabilistic behavior of the Bloom filter) are suppressed by adding an extra data structure enumerating critical vertices.
This efficient de-Bruijn graph representation fits, for example, a complete mammal genome in 4GB.
3 IMPLEMENTATION The GATB library is composed of five main packages: system, tools, bank, kmer and de-Bruijn packages.
The system package holds all the operations related to the operating system (OS): file management, memory management and thread management.
Using such an abstraction allows client code to be independent from the OS, thus suppressing compilation directive inside the code or improving some OS accesses by hiding specific OS optimization.
The supported operating systems are Linux, Mac and Windows.
The tools package offers generic operations used throughout the user application but not specific to genomics area.
For example, this package includes design pattern tools (such as iterators, observers, smart pointers, etc.)
and object collections (such as containers, bags, iterables, etc.).
It also optimizes the way GATB data structures are saved.
The HDF5 file format is currently used (HDF5, 2012).
This powerful technology is extremely well suited for large and complex data collection such as those handled in the GATB library.
The bank package provides operations related to standard genomic sequence dataset management.
All the main sequence file formats are supported, and high-level interfaces allow sequences to be easily iterated regardless of the input format.
In other words, algorithms are written independently of the input formats.
The kmer package is dedicated to fine-grained manipulation of k-mers.
Optimized routines are provided to perform k-mer counting from large sequence datasets, to find k-mer neighborhood or to select k-mers based on different criteria.
Finally, the de-Bruijn package provides high-level functions to manipulate a static de-Bruijn graph data structure: creation from a set of k-mers, iteration through different nature of nodes (simple k-mers, branching k-mers, etc.
), extraction of neighbor nodes, etc.
Additional information (e.g.k-mer coverage, markers of visited nodes) is stored in the graph branching nodes.
From this abstraction level, developing new tools based on de-Buijn graphs is fast, and does not require programmers to delve into low-level details.
The GATB library takes benefit of the parallel nature of todays multicore architecture of microprocessors.
When possible, time-consuming parts of the code are multithreaded to provide fast runtime execution.
The GATB library is developed in C++ under the A-GPL license and is available from the following Web site: http://gatb.
inria.fr.
An extensive documentation with tutorials is available to guide designers in the process of developing new NGS tools from the GATB building blocks: http://gatb-core.gforge.inria.fr (see also Supplementary File 2 for technical implementation details).
4 RESULTS To demonstrate the efficiency of the GATB library, a few software implemented from GATB are briefly presented.
The idea is to give a quick overview of the application spectrum of the GATB library and some performance numbers.
Minia (Chikhi and Rish, 2012) is a short-read de-Bruijn assembler capable of assembling large and complex genomes into contigs on a desktop computer.
The assembler produces contigs of similar length and accuracy to other de-Bruijn assemblers e.g.Velvet (Zerbino and Birney, 2008).
As an example, a Boa constrictor constrictor (1.6Gb) dataset (Illumina 2 120bp reads, 125 coverage) from Assemblathon 2 (Bradnam et al., 2013) can be processed in 45h and 3GB of memory on a standard computer (3.4GHz 8-core processor) using a single core, yielding a contig N50 of 3.6 kb (prior to scaffolding and gap-filling).
Bloocoo is a k-mer spectrum-based read error corrector, designed to correct large datasets with low memory footprints.
It uses the disk streaming k-mer counting algorithm contained in the GATB library and inserts solid k-mers in a Bloom filter.
The correction procedure is similar to the Musket multistage approach (Liu et al., 2013).
Bloocoo yields similar results while requiring far less memory: for example, it can correct whole human genome re-sequencing reads at 70 coverage with 54GB of memory (see Supplementary file 1 for extra information on Bloocoo).
DiscoSNP aims to discover Single Nucleotide Polymorphism from non-assembled reads and without a reference genome.
From one or several datasets a global de-Bruijn graph is constructed, then scanned to locate specific SNP graph patterns (Uricaru et al., 2014).
A coverage analysis on these particular locations can finally be performed to validate and assign scores to detected biological elements.
Applied on a mouse dataset (2.88Gb, 100 bp Illumina reads), DiscoSnp takes 34 h and requires 4.5GB RAM.
In the same spirit, the TakeABreak software discovers inversion variants from non-assembled reads.
It directly finds particular patterns in the de-Bruijn graph and provides execution performances similar to DiscoSNP (Lemaitre et al., 2014).
Funding: ANR (French National Research Agency) (ANR-12EMMA-0019-01).
Conflict of interest: none declared.
Fig.1.
Schematic view of the GATB organization 2960 E.Drezen et al.due very about 9 10 , b wpproximately ours K p very , b x less than (SNP) p ours G , This work is supported by the ,
ABSTRACT Motivation: Characterizing and comparing temporal geneexpression responses is an important computational task for answering a variety of questions in biological studies.
Algorithms for aligning time series represent a valuable approach for such analyses.
However, previous approaches to aligning gene-expression time series have assumed that all genes should share the same alignment.
Our work is motivated by the need for methods that identify sets of genes that differ in similar ways between two time series, even when their expression profiles are quite different.
Results: We present a novel algorithm that calculates clustered alignments; the method finds clusters of genes such that the genes within a cluster share a common alignment, but each cluster is aligned independently of the others.
We also present an efficient new segment-based alignment algorithm for time series called SCOW (shorting correlation-optimized warping).
We evaluate our methods by assessing the accuracy of alignments computed with sparse time series from a toxicogenomics dataset.
The results of our evaluation indicate that our clustered alignment approach and SCOW provide more accurate alignments than previous approaches.
Additionally, we apply our clustered alignment approach to characterize the effects of a conditional Mop3 knockout in mouse liver.
Availability: Source code is available at http://www.biostat.wisc.
edu/aasmith/catcode.
Contact: aasmith@cs.wisc.edu 1 INTRODUCTION Characterizing and comparing temporal gene-expression responses is an important computational task for answering a variety of questions in biological studies.
In previous work (Smith and Craven, 2008; Smith et al., 2008), we have introduced methods for answering similarity queries about gene-expression profiles after exposure to some chemical or treatment.
These methods have been motivated by the task of quickly and accurately characterizing the potential toxicity of chemicals.
A fundamental step in comparing two time series is with temporally align the series using a method such as dynamic time warping (Sakoe and Chiba, 1978; Sankoff and Kruskal, 1983).
Previous approaches to aligning gene-expression time series have assumed that all genes should be aligned in lockstep with one another.
In other words, these methods assume that the transformation that specifies how one series relates to another is the same for all genes.
Here, we present a novel approach that finds clusters of genes such that the genes within a cluster share a common alignment, but each cluster is aligned independently of the others.
Our method is similar to k-means clustering (Duda et al., 2000) in that it alternates between assigning genes to clusters and recomputing the alignment for each cluster using the genes assigned To whom correspondence should be addressed.
Fig.1.
The time-series similarity task.
Given a gene-expression time series as a query, we want to find the time series in the database which are most similar to the query.
Shaded areas represent strong matches to the given query.
Notice that for both Treatments B and C, the best alignment to the query does not account for the entire extent of the treatments.
Also notice that with Treatment B, all genes can be aligned together, whereas with Treatment C the second gene should be aligned separately.
to it.
We also present a novel multi-segment alignment algorithm that computes more accurate alignments for sparse gene-expression time series than previous methods.
One application for time-series alignment that we consider is the task of answering similarity queries as illustrated in Figure 1.
Given an expression profile as a query, we want to identify treatments in a database that have expression profiles most similar to the query.
When the query and/or some of the database treatments are time series, we assess similarity by determining the temporal correspondence between the query and treatments in the database.
In our toxicogenomics application, we might be trying to determine if an uncharacterized chemical induces an expression response similar to any known toxicants.
The figure shows a simple case in which our database consists of expression profiles from four different treatments, and each expression profile characterizes only three genes.
Figure 1 illustrates two important issues that arise in this task.
Sometimes (as with Treatment B) all genes should be aligned (i.e.warped) together to find the best correspondence.
But, it may also happen that some genes need to be warped separately from the others, as with Treatment C. A second issue is that often the best alignment does not account for the complete extent of both time series.
Therefore, we want to allow a type of local alignment in which the end of one series is unaligned.
We refer to this case as shorting the alignment.
The two main contributions of this work are algorithms that are designed to address both of these issues when computing time-series alignments.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i120 i119i127 A.A.Smith et al.Fig.2.
Alignment space example.
The multi-segment alignment path characterizes correspondences between two series, as shown by the dotted lines.
Knots are the points of discontinuity in the path.
Figure 2 shows the alignment of two time series in alignment space, using a multi-segment alignment method.
The alignment path determines which points in the two series are mapped to one another.
For a given point in the path, the coordinate in the first time series directly below it and the coordinate in the second time series directly to its left correspond to one another.
A multi-segment alignment can take into account that the nature of the relationship between the two series may vary in different segments.
For example, it may be the case that the later part of the expression response occurs more slowly in one treatment than in a similar treatment.
We refer to the points of discontinuity that define the segment boundaries as knots.
The alignment in Figure 2 also illustrates the concept of shorting.
Here, Time Series A seems to have advanced more quickly than Time Series B, which has not started to increase at the end.
An alignment path that represents shorting ends in the top row or the right column of the alignment space diagram, but not in the top-right cell.
Note that we do not allow an alignment to short both series; all of one or the other must be mapped to some point in its mate.
In previous work (Smith et al., 2008), we described a novel multisegment alignment method and empirically demonstrated that it classifies and aligns our toxicogenomics data better than several competing methods, including dynamic time warping, several parametric methods (such as linear alignment) and another multisegment method called correlation-optimized warping, or COW (Nielsen et al., 1998).
Parametric methods, which constrain the warping path to a simple functional form, often are not expressive enough to capture the most appropriate warping.
In contrast, dynamic time warping can often be too expressive, finding highscoring alignments of unrelated series.
A multi-segment method provides a balance between these two methods.
The accuracy advantage of our previous multi-segment method over COW was slight.
COW is a global alignment method that cannot short.
On closer inspection, we found that our method discovered more accurate alignments in cases that required shorting, whereas COW dominated those trials that did not.
Here, we present a modified version of COW that allows shorted alignments.
We call the method SCOW, for shorting COW.
Our algorithm for computing clustered alignments uses SCOW as its base alignment method.
Aach and Church (2001) were the first to apply the method of dynamic time warping (Sakoe and Chiba, 1978) to gene-expression profiles, and other groups have followed with this warping method (Criel and Tsiporkova, 2006; Liu and Mller, 2003) and others (BarJoseph et al., 2003).
Importantly, they have all done their warping on all genes together, whereas we compute clustered alignments.
Also, our approach differs in that it compute multi-segment alignments and considers local alignments via shorting.
Other studies have investigated clustering gene-expression time series (Bar-Joseph et al., 2003; Eisen et al., 1998; Leng and Mller, 2006; Liu and Mller, 2003).
The important differences between these approaches and ours are 2-fold: the goals of the clustering process and the notion of similarity used.
Whereas these previous methods have focused on identifying clusters of genes that have similar expression profiles, our approach, in contrast, is focused on identifying clusters in which the genes have similar warpings.
The genes in one of our clusters may have very different expression profiles, but they are similar in how they should be warped across the two time series being compared.
Listgarten et al.(2005) have developed a method for multiple alignment of time series data that has some similarities to our approach.
Their method, however, computes a single alignment of multiple time series, whereas our method computes a clustered alignment of a pair of time series.
We are not the first group to develop algorithms for computing shorted alignments.
Keogh (2003) devised a two-step shorting method that first finds the appropriate end points of an alignment before calculating a global alignment up to these points.
Our approach to shorting is different in that the shorting decision is not decoupled from the computation of the alignment; the dynamic programming method considers shorted as well as non-shorted alignments.
2 METHODS In this section, we detail two novel techniques that we have developed.
The first is SCOW, which is a method for computing multi-segment alignments of two time series and assessing their similarity.
The second is an algorithm which computes clustered alignments in which the genes within a cluster share a common alignment, but each cluster is aligned independently of the others.
2.1 SCOW We start by describing COW (Nielsen et al., 1998), which is a dynamic programming algorithm designed to find an optimal alignment between two series with multiple channels of information (such as genes).
We then describe SCOW, which is our extension to COW.
COW was developed to align chromatography time-series data.
Briefly, it aligns and scores two given time series based on their similarity.
Here, we refer to the two series as q (for query series) and d (for database series).
For each possible alignment, the series are partitioned into m segments, in which the i-th segments of the two series correspond to each other.
The score of a given alignment is the sum of correlations between corresponding segments.
As shown in Figure 3A, COW searches for good segment boundaries in only a limited area of alignment space.
The segments are assumed to be of constant length in q, and variable in d. The vector K contains the coordinates of the knots (segment endpoints) in q.
These are usually evenly spaced.
COW works by filling a zero-indexed matrix , which is of dimensions m+1 by |d|+1.
The element k,x contains the score of the best alignment of d from zero to x and q from zero to Kk (the k-th element of K) using k segments.
i120 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i121 i119i127 Clustered alignments Fig.3.
COW and SCOW in alignment space.
Both perform searches to find the best set of knots, or points of discontinuity, for a multi-segment alignment.
(A) COW, which assumes no shorting and searches for good knots only in a single dimension, along the dotted lines.
(B) The first step of SCOW, which searches independently in both dimensions.
Subsequent steps are numbered in (C), as SCOW alternates horizontal and vertical movement of each knot until it converges.
It is filled using the following recurrence relations: 0,x= { 0 if x=0 otherwise (1) k,x= max ypred(x,k) [ k1,y+cor ( d(y,x),q(Kk1,Kk) )] (2) where cor is the Pearson correlation, q(a,b) represents a subseries of q from a to b and d(a,b) is defined likewise.
The predecessor function lists valid starting locations in d for segments ending at x: pred(x,k)= x |q||d| (KkKk1)t,..., x |q||d| (KkKk1)+t , (3) with t being a user-defined slack parameter that controls the size of the search space.
The best alignment, and its resulting score, is represented by the element of that corresponds to the end of the global alignment: BestScore()=m,|d|.
(4) Note that COW can be used to align a one-channel time series, such as the expression profile of a single gene, or a multi-channel time series, such as the expression profile of a set of genes.
The only difference between these two cases is in how the correlations are calculated.
A limitation of COW is that it forces the entirety of both series to be aligned to each other; it cannot short the alignment.
Also, COW is apt to align segments which differ greatly in magnitude because it scores by correlation.
Further, the computation in Equation (2) may sometimes return to an undefined value if the input segments do not have a defined correlation (as when both segments consist of all zeros).
Our SCOW is designed to rectify these problems.
As shown in Figure 3B, SCOW searches for optimal knots in both dimensions.
It first finds optimal knots with respect to q using evenly spaced knots in d, and with respect to d using evenly spaced knots in q.
It uses the better alignment from these two passes as the starting point for an iterative process.
From then on it alternates, which dimensions knot coordinates it holds constant, using the coordinates found by the previous pass as the constant knots in the next one.
This iterative process is illustrated in Figure 3C, and Table 1 provides pseudocode describing the SCOW algorithm.
There are two different recurrence relations used in SCOWs dynamic programming formulation: q k,x= maxypred(x,k) [ q k1,y+score ( d(Kdk1,K d k ),q(y,x) )] , (5) Table 1.
The pseudocode for SCOW procedure SCOWAlign(series d, series q, set of genes G)://initial passes//Kqevenly spaced integers from 0 to |q| Kdevenly spaced integers from 0 to |d| calculate q,d using G if (BestScore(q)>BestScore(d )): q else: d KTraceback()//main loop//repeat: swap-dimension calculate using G calculate BestScore() KTraceback() until Kq,Kd converge Knots are recalculated at least three times.
The Traceback function extracts the best knots found from the previous pass to use in the next one.
dk,x= max ypred(x,k) [ dk1,y+score ( d(y,x),q(Kqk1,K q k ) )].
(6) The matrix q is calculated when the algorithm searches for knots with respect to q and holds them constant with respect to d, while d is calculated during the opposite case.
The vectors Kq and Kd represent the coordinates of the knots in each dimension.
The predecessor function is altered so as to not center around the line with slope |q|/|d| but instead to enable a cone-shaped search space (as illustrated in Figure 3B) since we want to consider shorted alignments: pred(x,k)= max [ x(KkKk1), 1 Kk1) ] ,..., min [ x 1 (KkKk1),Kk1) ] , (7) where is the maximum slope allowed the aligning path in alignment space.
In addition, SCOW does not assume a global alignment, but searches the last i121 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i122 i119i127 A.A.Smith et al.row of the matrix for the best scoring alignment using m segments: BestScore()=  m,|d| if other dimension shorted max j m,j otherwise.
(8) This allows SCOW to short in the current dimension, if the other dimension is not already shorted.
Thus the alignment found cannot short both q and d. The effect on the search can be seen in Figure 3C: the last knot cannot move down during the first step, because doing so would short both dimensions.
In addition to a different search procedure, SCOW also differs somewhat from COW in the function it uses to score alignments.
The scoring function presented here is similar to one we used in previous work (Smith et al., 2008).
In particular, the scoring function includes terms that incur penalties for segments that involve stretching and significant differences in amplitude.
We use the term stretching to refer to distortions in the rate of some expression response, and the term amplitude to refer to distortions in the magnitude of the response.
Consider, for example, the alignment shown in Figure 2.
The first segment in this alignment involves a noticeable amplitude difference (Time Series B has a higher amplitude than Time Series A), and the last segment involves significant stretching (this part of the response in Time Series B happens more slowly than the corresponding part of Time Series A).
We define the score of an alignment segment to be: score(qi,di)=cor(qi,di) log 2 si 2 2s log 2 ai 2 2a (9) Here, qi and di denote the i-th segments of series q and d, respectively, si is the amount of stretching in the alignment of the i-th segments, ai is the amplitude difference, and cor is the Pearson correlation.
The stretching si is defined as the ratio of lengths between qi and di, and ai is the amplitude ratio between the two as determined by a weighted least squares fitting procedure.
The form of the stretching and amplitude terms comes from a generative, probabilistic model we developed in earlier work (Smith et al., 2008).
This previous approach uses probability distributions over possible stretching and amplitude values that have the following form: p(v)= e 2 2  2 e log2 v 22.
(10) The key property of this distribution is that it is symmetrical around 1 such that P(x)=P(1/x).
Thus stretching, and amplitude values that deviate from 1 are penalized, and the penalty is the same regardless of which series, q or d, is considered to have the distortion.
For all of our experiments with COW and SCOW, we calculate correlations in the following way.
We first use B-splines (Rogers and Adams, 1989) to interpolate between the observations in our time series (which are typically sparsely sampled).
To calculate correlations between segments qi and di, we resample their spline approximations to the same predetermined number of values for the two segments.
We also alternately add and subtract a tiny value to values in qi and di, so that correlation is always defined and two segments with constant values will have a correlation of one.
Like COW, SCOW operates with a time complexity of O(n3), where n is the length of the interpolated series to be aligned.
Further, many of the calculations in successive passes of SCOW are the same, and may be cached.
In contrast, the segment-based method from our previous work took O(n5) time to do an exhaustive search for the best segments to align the series.
The speed-up is dramatic: what took the old method an hour to calculate takes SCOW only a few seconds.
2.2 Clustered alignments Now we describe the algorithm we have developed for computing clustered alignments.
The goal of this algorithm is to find sets of genes that would have very similar alignments if they were aligned independently.
The alignment Table 2.
The pseudocode for our clustered alignment algorithm procedure ClusterAlignments(series d, series q, # clusters k)://initialize cluster centroids//centroid[1]null alignment for all (genes g): possible[g]ScoreGene(q,d,g,Align(q,d,{g})) best[g]ScoreGene(q,d,g,centroid[1]) for (i2 to k): worstargming(best[g]possible[g]) centroid[i]Align(q,d,{worst}) for all (genes g): best[g] max(best[g],ScoreGene(q,d,g,centroid[i])) repeat://assignment step//for all (centroids c): set[c] for all (genes g): sargmaxc(ScoreGene(q,d,g,c)) set[s]set[s]g//update step//for all (centroids c): cAlign(d,q,set[c]) until sets converge represented by each cluster may be quite different from the alignments that the other clusters represent.
This approach is motivated by the fact that the relationship between two similar time series may differ depending on which subset of genes we consider.
The algorithm we have devised is a variant of traditional k-means clustering (Duda et al., 2000).
In k-means, each cluster is represented by a centroid and the clustering process involves iteratively refining the locations of these centroids.
For example, if we were clustering points in Rn, each centroid would be represented by a point in Rn.
In our clustered alignment method, each centroid is represented by an alignment (e.g.such as the one illustrated in Fig.2).
In our algorithm, as in standard k-means, the number of clusters is determined by a parameter k that is provided as an input.
We reiterate that, in contrast to previous methods which have focused on identifying clusters of genes that have similar expression profiles, our algorithm is focused on identifying clusters in which the genes have similar warpings.
The genes in one of our clusters may have very different expression profiles.
Table 2 shows the pseudocode for our alignment clustering method.
It takes as input two series, termed d and q, and the number of clusters k. It relies on the subroutines Align, which returns the best alignment between two series based on a given set of genes, and ScoreGene, which returns the score of two series when aligned using a given alignment and a specified gene.
We use SCOW to perform these functions, using SCOWAlign for Align while using Equation (8) for ScoreGene.
However, we could substitute any other alignment algorithm for this purpose.
The first step in the method is to assign the initial alignment centroids.
We use a greedy method, similar to that used by Ernst et al.(2005) to select a representative set of gene alignments as the centroids.
The first centroid is taken to be the null alignment, which represents no warping.
For each gene, we record a best possible score (when the alignment is based solely on that gene), and the best score seen so far for that gene using one of the current centroids.
Each additional centroid is initialized by finding the gene with the largest difference between its best score so far and its possible high score.
The new centroid is the alignment calculated using this selected gene alone.
After each new centroid is determined, the best scores for all the genes i122 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i123 i119i127 Clustered alignments are modified to take the new centroid into account.
We proceed until all k centroids are defined.
Now we perform the assignment step and the update step in turn until convergence.
For the assignment step, we score every gene with every clusters centroid and assign the gene to the cluster with the highest score.
For the update step, we set each centroid to the alignment calculated by aligning q and d using just the set of genes assigned to the cluster.
We continue iterating until the cluster assignments do not change.
Because SCOW performs a heuristic search, however, it is possible that the process will not converge.
In practice, this is seldom a problem.
We can simply stop iterating after a large number of iterations, or when infinite loop conditions are detected by retaining a short history of cluster assignments.
Alternatively, we can guarantee convergence by using an alignment algorithm that is exact.
3 RESULTS AND DISCUSSION In this section, we describe a set of computational experiments that are designed to (i) evaluate the alignment accuracy of SCOW and our clustered alignment method, and (ii) assess how well the clustered alignment algorithm is able to uncover sets of genes that share similar alignments across two time series.
3.1 SCOW experiments In our first set of experiments, we are interested in testing the ability of the SCOW method to find accurate alignments.
We do this in the context of the task illustrated in Figure 1.
Here, we are given an expression profile as a query, and we want to identify the treatment in the database that has the expression profile most similar to the query.
We construct queries for which we know the correct matching database treatments and their correct alignments.
The data we use comes from the EDGE toxicology database (Hayes et al., 2005), and can be downloaded from http://edge.oncology.wisc.edu/.
Our dataset consists of 216 unique observations of microarray data, each of which represents the expression values for 1600 different genes.1 Each of these expression values is calculated by taking the average expression level from four treated animals, divided by the average level measured in four control animals.
The data are then converted to a logarithmic scale, so that an expression value of 0.0 corresponds to the average basal level observed in the control animals.
Each observation is associated with a treatment and a time point.
The treatment refers to the chemical to which the animals were exposed and its dosage.
The time point indicates the number of hours elapsed since exposure occurred.
Times range from 6 h up to 96 h. The data used in our computational experiments span 11 different treatments, and for each treatment there are observations taken from at least three different time points.
Additionally we can assume that for all treatments, there exists an implicit observation at time zero.
This is the time at which the treatment was applied, so all expression values are assumed to be at the basal level.
We assemble queries by randomly sub-sampling time series in our dataset.
We assemble 10 such queries from each treatment.
We build each query by first selecting the number of observations to be in it, then choosing which time points will be represented, and finally picking an observation for each of these time points.
The query sizes are chosen from a uniform distribution that ranges from 1Technically, the expression measurements correspond to clones selected from liver-derived EST and full-length cDNAs.
These clones represent products for 1600 unique genes.
one up to the number of observed times in the given treatment.
The maximum size of a query is eight, although most consist of four or fewer observations.
The time points are chosen uniformly as are the observations for each chosen time.
To test the ability of our approach to find accurate alignments in situations that require warping, we also assemble cases in which we distort the query time series temporally.
We use three different distortions.
The first one doubles all times in the first 48 h (i.e.it stretches the first part of the series), and then halves all times (plus an offset for the doubling) for the next 24 h. The second distortion halves for the first 36 h and then doubles for 60 h. The third one triples for the first 60 h and then thirds for another 20 h. It should be noted that not all the treatment observations extend this long in time.
The short ones (e.g.those for which we only have measurements up to 24 or 48 h) will thus not be distorted as much as the long ones.
We then classify and align the query using all the other observations as the database.
We preprocess both the query and the 11 database treatments using B-splines (Rogers and Adams, 1989) to reconstruct pseudo-observations at every 4 h (starting at time zero, when all expression values are at the basal level).
We then align the query against all 11 treatments using our method.
We return the database treatment with the highest scoring alignment, as defined by Equation (8).
Because the alignment also maps each query time to a database treatment time, we can find the temporal error for any query time point.
We then measure how accurately we are able to (i) identify the treatment from which each query series was extracted, and (ii) align the query points to their actual time points in the treatment.
We refer to the former as treatment accuracy and the latter as alignment accuracy.
We consider several other alignment methods as baselines.
The first is COW (Nielsen et al., 1998), as described in Section 2.
The second is a generative method we previously developed (Smith et al., 2008), which we refer to as Generative Multisegment.
Like SCOW, it finds alignments which consist of multiple segments each of which can have different warping parameters.
However, the Generative Multisegment scoring function is based on a generative, probabilistic model, rather than correlation.
Further it performs a complete search for the best segments to use, rather than using the heuristic search of SCOW.
The next baseline we consider is traditional Euclidean dynamic time warping (Sakoe and Chiba, 1978; Sankoff and Kruskal, 1983).
Briefly, this method computes alignments by creating a matrix with elements defined recursively as i,j=D(di,qj)+min [ predDTW(i,j) ] (11) where D(di,qj) is the Euclidean distance between points di and qj in the two series and predDTW(i,j) refers to the matrix elements adjacent to i,j with both indices less than or equal to i and j, respectively.
The first element 0,0 is just the Euclidean distance at time 0, and each other element i,j is the score of warping d from times 0 to i and q from 0 to j.
We then create a normalized score matrix where i,j=i,j/ |i|2+| j|2.
(12) This makes it reasonable to compare warpings with different treatments, where one or the other dimension has been shorted.
i123 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i124 i119i127 A.A.Smith et al.Fig.4.
Treatment and alignment accuracies when there is no temporal distortion (A), and when there is (B).
The top lines represent treatment accuracy, while the bottom two lines add the criterion that the predicted times are within 24 h and 12 h, respectively, of the actual time, on average.
For each alignment method, we show results when splines of various orders are used to interpolate the time series before alignments are calculated.
Highlights represent cases in which there is a significant difference in accuracy from the corresponding SCOW case (P0.05 with McNemars 2-test).
Finally, we consider linear parametric warping.
This is similar to the method explored by Bar-Joseph et al.(2003), except that we make the assumption that the series are aligned at time zero.
To find an alignment, we search possible slopes of the alignment line, and return the slope that results in the least average Euclidean distance between the query and the given database treatment.
For these experiments, SCOW, COW and Generative Multisegment use three segments in their alignments, and we set s and a = 10.
Using more segments and setting s and a to other values yields substantially similar results.
The results of this experiment are shown in Figure 4.
Figure 4A and B shows results for the queries without distortion and results for the distorted queries, respectively.
For each method, the top line represents treatment accuracy with different orders of splines, the middle line represents alignment accuracy by adding the criterion that the average time error in the mapping is less than or equal to 24 h, and the bottom line shows alignment accuracy where this tolerance is decreased to 12 h. Highlighted boxes denote points that are significantly different from the corresponding SCOW point, as determined by McNemars 2-test.
There are several interesting conclusions we can draw from these results.
First, it is clear that the multi-segment alignments computed by SCOW, COW and Generative Multisegment are superior to the alignments determined by ordinary dynamic time warping and the linear alignment method.
Second, SCOW finds more accurate alignments than the other two multi-segment algorithms, COW and Generative Multisegment.
Based on these results, we conclude that SCOW is a state-of-the-art alignment method for gene-expression time series, and we therefore use it as the core alignment method for our clustered alignment approach.
3.2 Clustered alignment experiments In our second set of experiments, we are interested in testing the ability of our clustered alignment algorithm to identify sets of genes that should share a common alignment.
We first conduct an experiment designed to determine if our clustered alignment method Fig.5.
Treatment and alignment accuracies, varying by the number of clusters when using SCOW.
In the final case (1600), we warp every gene separately.
Highlighted points are significantly different from the unclustered case, (P 0.05 under McNemars 2-test).
is able to find more accurate alignments when there are sets of genes that have different, known correct alignments.
This experiment is similar to the one in the previous sectionwe use the same data and substantially the same methodology.
The difference is that we simultaneously apply five different temporal distortions to every query: each one is applied to 1/5 of the genes.
We then run our clustered alignment method, in conjunction with SCOW, on the data, allowing the number of clusters k to range from one (i.e.unclustered, ordinary SCOW) to 10.
We also run the experiment with k=1600, which warps every gene separately.
The results for queries containing three or more observations are shown in Figure 5.
These results show the value of the clustered alignment approach with this dataset.
The accuracy of the alignments i124 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i125 i119i127 Clustered alignments increases as k increases, until about k=4.
After this point, there is a slight degradation in accuracy.
For almost all values of k tested, however, the treatment and the 24 h alignment accuracies are greater with the clustered alignment method than with ordinary SCOW.
With queries containing fewer than three observations, the clustered alignment method actually results in somewhat less accurate alignments than the non-clustered method (i.e.ordinary SCOW).
These results can be explained by a bias-variance tradeoff (Geman et al., 1992).
The gene-expression data we use (like most expression time series) is sparse in time, and prone to noise (because of both technical limitations and biological variability among the animals).
The sparsity and noise mean that it is difficult to compute accurate single-gene alignments.
Aggregating genes into clusters has a regularization effect as this alignment error is averaged out (Bar-Joseph et al., 2003).
The more genes there are in a cluster, the greater the regularization effect.
Thus we want to find the ideal trade-off between the high-bias approach of few clusters (or one cluster, in the limit), and the high-variance approach of many clusters.
The variance component of the error is more significant in the case when the queries are short.
We can conclude, however, that the clustered alignment approach demonstrates good predictive value for moderately sized queries and a range of values of k. In our second experiment, we are interested in identifying sets of genes that are distorted in similar ways in a knockout experiment focusing on circadian rhythms.
Mop3 is a transcription factor in hepatocytes (Bunger et al., 2000, 2005) that is a positive regulator of circadian rhythm and activates the transcription of genes such as Per1 and Tim.
There are two sets of mice in this experiment.
The control group has a functional Mop3 gene, while the knockout group does not.
This is a time-course study based on Zt which stands for Zeitgeiber timethe number of hours after exposure to light begins.
Before Zt0, the mice are kept in darkness for a period.
At Zt0 the lights turn on, and at Zt12 they turn off again.
At intervals of 4 h from Zt0 to Zt20, three mice from each group are sacrificed, and microarrays are derived from pooled RNA samples from the livers of each set of mice.
In all, 27 962 genes are measured.
We interpolate the series with B-splines so that we can sample measurements every 2 h. When aligning the control and knockout time series, we want to allow phase shifting.
That is, we want to allow alignments of the two time series are not necessarily aligned at Zt0.
In our previous experiment, it was reasonable to assume that the expression responses were all identical at time zero.
We cannot make that assumption in this case, however.
We modify SCOW to allow phase shifting by first concatenating the control time series with itself, to obtain 2 days worth of data.
When computing alignments, we allow the control series to short at both ends by redefining the initialization [Equation (1)] of d : 0,x=0.
(13) However, we disallow the alignment from shorting the knockout series, at either end, by using Equation (4) to score q.
Thus, all knockout series times must be mapped to some time in the control series, but the zero times need not correspond.
Figure 6 shows alignments for several genes in each cluster, as determined by our clustered alignment algorithm.
Here, we set the number of clusters k=5.
Each panel represents one of the clusters, and within each one we show the three genes with the highest relative scores for that cluster.
The white alignment path in each plot represents the consensus alignment, when all genes are warped as a unit.
The black alignment path represents the clusters individual alignment.
Note that we only show 1 day in the control dimension rather than 2 days.
The alignments in panels C, D and E, all extend into 2 days.
This is shown by a break in the black alignment path, as it wraps back to the left side and the beginning of the second day.
The clustered alignment allows us to uncover sets of genes that are disrupted in a similar manner by the knockout, even when their expression profiles are quite different.
It is clear that the clustered alignments align the series better than the consensus alignment.
Peaks and valleys in the expression data line up well for the black cluster alignment paths, whereas they often do not for the white consensus ones.
For example, the genes in panel E have undergone a large phase shift.
The consensus path often matches segments with quite different expression profiles, whereas the cluster path shifts the starting point by 12 h and achieves good agreement.
In panel D, the genes appear to be acting more quickly in the knockout mice, while the consensus alignment would indicate they are acting more slowly.
It should also be noted that often the genes within a cluster have very different expression profiles.
Consider panel D, in which the profiles for the three genes are all quite different, but the mapping between control and knockout is similar.
This effect illustrates the advantage of clustering alignments in contrast to clustering the expression profiles directly.
4 CONCLUSION Alignment algorithms provide a valuable approach for gaining biological understanding from gene-expression time series.Avariety of methods have been employed for such analyses, including dynamic time warping, linear alignment algorithms and multisegment alignment methods.
We have presented new methods which advance the state of the art in two ways.
Most importantly, we have developed an algorithm which is able to compute clustered alignments.
This algorithm relaxes the assumption, common to previous work in expression time-series alignment, that all genes should be warped in the same way.
Instead, our method identifies sets of genes that share a common alignment.
It does this by simultaneously clustering genes and computing a shared alignment for the genes in each cluster.
The second contribution introduced here is a new multisegment alignment method, called SCOW, that features the ability to calculate shorted alignments, a correlation-based scoring function, and an efficient dynamic programming algorithm for computing alignments.
The results of our empirical evaluation indicate that both the clustered alignment approach and SCOW improve the accuracy of alignments computed with sparse time series from a toxicogenomics dataset.
Additionally, we applied our clustered alignment approach to a dataset involving a conditional Mop3 knockout in mouse liver.
This analysis illustrates the power of the clustered alignment approach to find sets of genes that share similar temporal distortions.
Funding: National Institutes of Health/NIEHS (grant R01ES012752); National Institutes of Health/NLM (grant R01LM07050); National Institutes of Health/NCI (grants P30CA014520 and T32-CA009135).
i125 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i126 i119i127 A.A.Smith et al.Fig.6.
Alignment clusters found by our method for the Mop3-knockout circadian data.
Each panel shows the top three genes for a different cluster.
The white alignment paths represent the consensus alignment for all the genes, while the black paths represent the cluster-specific alignments.
i126 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i127 i119i127 Clustered alignments Conflict of Interest: none declared.
ABSTRACT Motivation: The assessment of protein structure prediction techniques requires objective criteria to measure the similarity between a computational model and the experimentally determined reference structure.
Conventional similarity measures based on a global superposition of carbon atoms are strongly influenced by domain motions and do not assess the accuracy of local atomic details in the model.
Results: The Local Distance Difference Test (lDDT) is a superpositionfree score that evaluates local distance differences of all atoms in a model, including validation of stereochemical plausibility.
The reference can be a single structure, or an ensemble of equivalent structures.
We demonstrate that lDDT is well suited to assess local model quality, even in the presence of domain movements, while maintaining good correlation with global measures.
These properties make lDDT a robust tool for the automated assessment of structure prediction servers without manual intervention.
Availability and implementation: Source code, binaries for Linux and MacOSX, and an interactive web server are available at http://swiss model.expasy.org/lddt Contact: torsten.schwede@unibas.ch Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 12, 2013; revised on August 5, 2013; accepted on August 9, 2013 1 INTRODUCTION The knowledge of a proteins 3D structure enables a wide spectrum of techniques in molecular biology, ranging from rational design of mutagenesis experiments for the elucidation of a proteins function to drug design.
While the rapid development of DNA sequencing techniques has been providing researchers with a wealth of genomic data, experimental structure determination techniques require substantially more effort, and consequently the gap between the number of known protein sequences and the number of known protein structures has been growing continuously.
To fill this gap, various computational approaches have been developed to predict a proteins structure starting from its amino-acid sequence (Guex et al., 2009; Moult, 2005; Schwede et al., 2009).
Despite remarkable progress in structure prediction methods, computational models often fall short in accuracy compared with experimental structures.
The biannual CASP experiment (Critical Assessment of techniques for protein Structure Prediction) provides an independent blind retrospective assessment of the performance of different modeling methods based on the same set of target proteins (Moult et al., 2011).
One of the main challenges for the CASP assessors is to define appropriate numerical measures to quantify the accuracy with which a prediction approximates the experimentally determined structure.
In the course of the CASP experiment, model comparison techniques have evolved to reflect the current state of the art of prediction techniques: In the first installments of CASP, root-mean-square deviation (RMSD) between a prediction and the superposed reference structures was used in various forms as the main evaluation criterion (Hubbard, 1999; Jones and Kleywegt, 1999; Martin et al., 1997; Mosimann et al., 1995).
However, RMSD has several characteristics that limit its usefulness for structure prediction assessment: the score is dominated by outliers in poorly predicted regions while at the same time it is insensitive to missing parts of the model, and it strongly depends on the superposition of the model with the reference structure.
To overcome some of the limitations of RMSD in the context of CASP, the Global Distance Test (GDT) was introduced in CASP4 (Zemla, 2003; Zemla et al., 2001).
In contrast to RMSD, the GDT is an agreement-based measure, quantifying the number of corresponding atoms in the model that can be superposed within a set of predefined tolerance thresholds to the reference structure.
For each threshold, different superpositions are evaluated and the one giving the highest number is selected.
The final GDT score is then calculated as the average fraction of atoms that can be superposed over a set of predefined thresholds (0.5, 1, 2 and 4 A for GDT-HA and 1, 2, 4 and 8 A for GDT-TS, respectively).
One of the advantages of GDT is that strongly deviating atoms do not considerably influence the score.
At the same time, missing segments in the predictions lead to lower scores.
Besides GDT, several other scores for model comparison have been developed to overcome the limitations of RMSD (Olechnovic et al., 2013; Siew et al., 2000; Sippl, 2008; Zhang and Skolnick, 2004).
One of the main limitations of measures based on global superposition becomes evident when applied to flexible proteins composed of several domains, which can change their relative orientation naturally with respect to each other (Fig.1).
Typically in those cases, the global rigid-body superposition is dominated by the largest domain, and as a consequence, the smaller domains are not correctly matched, resulting in artificially unfavorable scores.
In CASP, the effects of domain *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com In order to to , which In order to , of 2012 movement are reduced by splitting the target into the so-called assessment units (AUs), that are evaluated separately.
The definition of AUs is carried out by visual inspection, and is therefore time-consuming.
Furthermore, the criteria used to define the AU are often subjective (Clarke, et al., 2007; Kinch et al., 2011).
Grishin et al.have proposed an approach to numerically support this decision by analyzing the variability among the predictions for a specific target (Kinch et al., 2011).
Local superposition-free measures based on rotation-invariant properties of a structure are an attractive alternative to overcome several of the shortcomings outlined before.
For example, dRMSDthe distance-based equivalent of RMSDis used in chemoinformatics to assess differences in ligand poses in binding sites (Bordogna et al., 2011).
In CASP9, the local Distance Difference Test (lDDT) score was introduced, assessing how well local atomic interactions in the reference protein structure are reproduced in the prediction (Mariani et al., 2011).
More recently, other nonsuperposition-based scores have been proposed, e.g.CAD score based on residueresidue contact areas (Olechnovic et al., 2013), measures using residue contact similarity (Rodrigues et al., 2012) or the recall, precision, F-measure (RPF)/DP score, which was initially developed to evaluate the quality of nuclear magnetic resonance (NMR) structures (Huang et al., 2012).
Also, the SphereGrinder score (Kryshtafovych et al., 2013) was used for the assessment of local accuracy of refinement targets in CASP9 (MacCallum et al., 2011).
Initially, most of the scores used in structure prediction assessment aimed at the evaluation of the protein backbone or fold, thereby focusing on carbon (C) atom positions.
However, with increasing accuracy of prediction methods for templatebased models, the focus of the assessment has shifted to the evaluation of the atomic details of a model.
In CASP7, the first scores based on local atomic interactions were introduced in the form of HBscore, which quantifies the fraction of hydrogen bond interactions in the target protein correctly reproduced in the model (Battey et al., 2007; Kopp et al., 2007).
In CASP8, several scores for assessing the local modeling quality were introduced (main chain reality score, hydrogen bond correctness, rotamer correctness and side-chain positioning) (Keedy et al., 2009), as well as an evaluation of the stereochemical realism and plausibility of models using the MolProbity score (Chen et al., 2010).
In this article, we expand the initial concept of lDDT.
Because the lDDT score considers all atoms of a prediction including all side-chain atoms, it is able to capture the accuracy of, e.g.the local geometry in a binding site, or the correct packing of a proteins core.
We discuss its properties with respect to its low sensitivity to domain movements, and the significance that can be assigned to the absolute score values.
Furthermore, we introduce the concept of using multiple reference structures simultaneously, and incorporate stereochemical quality checks in its calculation.
We finally illustrate how lDDT can be used to highlight regions of low model quality, even in models of multidomain proteins where domain movements are present.
2 METHODS 2.1 The lDDT lDDT measures how well the environment in a reference structure is reproduced in a protein model.
It is computed over all pairs of atoms in the reference structure at a distance closer than a predefined threshold Ro (called inclusion radius), and not belonging to the same residue.
These atom pairs define a set of local distances L. A distance is considered preserved in the model M if it is, within a certain tolerance threshold, the same as the corresponding distance in L. If one or both the atoms defining a distance in the set are not present in M, the distance is considered non-preserved.
For a given threshold, the fraction of preserved distances is calculated.
The final lDDT score is the average of four fractions computed using the thresholds 0.5 A, 1 A, 2 A and 4 A, the same ones used to compute the GDT-HA score (Battey et al., 2007).
For partially symmetric residues, where the naming of chemically equivalent atoms can be ambiguous (glutamic acid, aspartic acid, valine, tyrosine, leucine, phenylalaine and arginine), two lDDTs, one for each of the two possible naming schemes, are computed using all non-ambiguous atoms in M in the reference.
The naming convention giving the higher score in each case is used for the calculation of the final structure-wide lDDT score.
The lDDT score can be computed using all atoms in the prediction (the default choice), but also using only distances between C atoms, or between backbone atoms.
Interactions between adjacent residues can be excluded by specifying a minimum sequence separation parameter.
Unless explicitly specified, the calculation of the lDDT scores for all experiments described in this article has been performed using default parameters, i.e.Ro 15 A, using all atoms at zero sequence separation.
2.2 Multireference lDDT The lDDT can be computed simultaneously against multiple reference structures of the same protein at the same time.
The set of reference distances L includes all pairs of corresponding atoms, which, in all reference structures, lie at a distance closer than the reference threshold Ro.
For each atom pair, the minimum and the maximum distances observed across all the reference structures are compared with the distance between the corresponding atoms in the modelM being evaluated.
The distance is Fig.1.
Comparison of predicted protein structure model with its reference structure for CASP target T0542.
The target structure (shown in gray) consists of two domains.
In (A), a predicted model (TS236, in color) is shown in full length, with the first domain superposed to the target.
For graphical illustration, (B) shows the two domains in the prediction separated according to CASP AUs and superposed individually to the target structure.
In both panels, the model is colored according to full-length lDDT scores following a traffic-light-like red-yellow-green gradient, with red corresponding to low values of the lDDT, green to high values and yellow to average values.
As superposition-free method, lDDT is insensitive to relative domain orientation and correctly identifies segments in the full-length model deviating from the reference structure 2723 lDDT: a local superposition-free score for comparing protein structures and models ` ' assessment units and coworkers --for example 2012 , template , manuscript local Distance Difference Test ( ) Since for example, Local Distance Difference Test The local Distance Test ( ) <inlinemediaobject><imageobject><imagedata fileref= , G A A A V T L P A local Distance Difference Test carbon manuscript Local Distance Difference Test local Distance Difference Test  considered preserved if it falls within the interval defined by the minimum and the maximum reference distances or if it lies outside of the interval by less than the predefined length threshold.
The fraction of preserved distances is computed like in the single reference case.
2.3 Stereochemical quality checks To account for stereochemical quality and physical plausibility of the model being evaluated, the calculation of the lDDT can take violations of structure quality parameters into account.
Here, stereochemical violations in the model are defined as bond lengths and angles with values that diverge from the respective average reference value derived from highresolution experimental structures (Engh and Huber, 1991, 2006) by more than a predefined number of standard deviations (12 by default; see Supplementary Material).
Interatomic distances between pairs of nonbonded atoms in the model are considered clashing if the distance between them is smaller than the sum of their corresponding atomic van der Waals radii (Allen, 2002), within a predefined tolerance threshold (by default 1.5 A).
Tolerance thresholds can be defined for each pair of atomic elements independently.
In case where the side-chain atoms of a residue show stereochemical violations or steric clashes, all distances that include any side-chain atom of this residue are considered as not preserved for the lDDT calculation.
In case the back-bone atoms are involved in stereochemical violations or steric clashes, all distances that include any atom of the residue are treated as not preserved.
2.4 Determination of the optimal inclusion radius Ro To determine the optimum value of the inclusion radius parameter Ro for lDDT, an analysis of predictions of all multidomain targets evaluated during the CASP9 experiment (Kinch et al., 2011; Mariani et al., 2011) was carried out (see Supplementary Table S1 for a complete list).
GDCall scores for predictions covering450% of the target protein sequence were computed based on the AUs definitions by the CASP9 assessors (Kinch et al., 2011).
A weighted whole target GDC-all score was computed for each target as the average GDC-all scores of its AUs weighted by the AU size.
GDC-all scores are an all-atom version of GDT with thresholds from 0.5 to 10 in steps of 0.5 A. GDC-all scores were computed using LGA version 5/2009 (Zemla, 2003), using a 4A cut-off for the sequence-dependent superposition.
lDDT scores were calculated for the whole targets by including all residues that are covered by any AU, and in an AU-based form using the same weighting scheme already applied to GDC-all scores.
The inclusion radius parameter was varied in the range from 2 to 40 A, and the correlation R2 score between the distribution of weighted averaged GDC-all scores and the distribution of lDDT scores was computed and plotted against the value of the inclusion radius (Figs.
2 and 3).
2.5 Validation of baseline scores for different folds To analyze the influence of the protein fold of the assessed structure on the lDDT score, pseudorandom models were created for different architectures in the CATH Protein Structure Classification system (Cuff et al., 2011) using the following procedure: representative domains longer than 50 residues were selected as evenly as possible among the topologies of the CATH classification.
For each domain, side-chain coordinates were removed and then rebuilt using the SCWRL software package (with default parameters) (Krivov et al., 2009).
Pseudorandom models representing threading errors were then generated by shifting all residues by one alignment position in a backbone-only model, and rebuilding the sidechains with SCWRL4, and computing the corresponding lDDT score.
The procedure was repeated iteratively until a threading error of 50 residue positions was reached.
This method is loosely based on the approach described in Shi et al.(2009).
In Figure 4, we show the results for CATH Architecture entries 1.25 (Alpha Horseshoe) and 2.40 (Beta-barrel), each represented by 60 example structures.
For estimating lDDT scores of random protein pairs, 200 protein models with wrong fold were generated by selecting pairs of structures with different CATH topologies, generating models by rebuilding side chains on the backbone of the other protein, and computing lDDT scores for these decoy models.
The median of the resulting distribution was 0.20, with a 0.04 mean absolute deviation.
2.6 Implementation and availability lDDT has been implemented using the OpenStructure framework (Biasini et al., 2010).
Source code, standalone binaries for Linux and Mac OSX, as well as an interactive web server are available at http://swissmodel.
expasy.org/lddt/.
The web server has been implemented using the Python Django and JavaScript jQuery frameworks; it supports all the major browsers.
Fig.3.
Correlation between whole structure GDC-all and lDDT scores and domain-based weight-averaged GDC-all scores.
For CASP9 predictions of multidomain targets, GDC-all scores (red dots) and lDDT scores (blue dots) were computed against the whole unsplit target structures.
For the lDDT scores, the default value of 15 A for the inclusion radius was used Fig.2.
Determination of the optimal inclusion radius parameter Ro.
Pearson correlation (R2) between whole target lDDT scores (solid line) and domain-based weight-averaged lDDT score (dashed line) versus domain-based weight-averaged GDC-all scores for different values of the inclusion radius parameter Ro were computed over all CASP9 predictions for multidomain targets 2724 V.Mariani et al.In order to local Distance Difference test which ; Engh and Huber, 2006 sin supplementary materials more than Assessment Units ( ) which &Aring; , Fig.3 R st A3 RESULTS AND DISCUSSION We have developed the lDDT as a new superposition-free measure for the evaluation of protein structure models with respect to a reference structure.
In the following, we will discuss the choice of the optimal inclusion radius parameter Ro to achieve low sensitivity to domain movements, and analyze baseline scores for lDDT for different fold architectures.
We will discuss the application of lDDT for assessing local correctness of models, including stereochemical plausibility.
Finally, we will present an approach for assessing a model simultaneously against several reference structures, e.g.a structural ensemble from NMR.
3.1 Optimal choice of the inclusion radius parameter R0 makes lDDT largely insensitive to domain movements 3.1.1 Determination of the optimal inclusion radius The nature of the lDDT score is ultimately determined by the choice of the inclusion radius parameter Ro.
For low values of the inclusion radius, only short-range distances are assessed, and the accuracy of local interactions has a major impact on the final value of the lDDT score.
On the other hand, when the value of the inclusion radius parameter is high, the evaluation of long-range atomic interactions gains a bigger contribution in the final score, and the final lDDT score turns into a representation of the global model architecture quality.
For assessing the accuracy of protein models, the inclusion radius should be high enough to give a realistic assessment of the overall quality of the model, but at the same time, the lDDT score should not lose its ability to evaluate the modeling quality of local environments.
Especially, scores should not be influenced by changes of domain orientation between the model and the target structures.
The optimal value of the inclusion radius parameter Ro has been determined on a dataset comprising all CASP9 predictions for multidomain targets, and the corresponding assignment of AUs as defined by the CASP9 assessors.
Weighted GDC-all scores were calculated as weighted averages of the AU-based scores (see Materials and Methods for details).
Hence, the weighted GDC-all scores can be considered to be largely devoid of the influence of domain movements.
lDDT scores were then computed using both the full target structures and, in a weight-averaged AU-based form, for a range of Ro, values from 1 to 40 A.
For each threshold, we calculated the correlation with the weight-averaged GDC-all scores for the same predictions.
We used GDC-all (and not the more common C-based GDT) score to compare two all-atoms measures on the same set of data.
The results are shown in Figure 2.
The conclusions presented in this article, however, also hold when using GDT as reference measure.
For small values of the Ro parameter, the two types of lDDT scores essentially reduce to a contact map overlap measure (Vendruscolo et al., 1999) and the correlation with global scores such as GDC-all is rather low.
As the inclusion radius increases, longer-range interactions are being evaluated and the correlation shows a steep increase as the lDDT score starts to reflect the global quality of the model.
For large values of Ro, where inter-domain relationships start playing a more significant role and domain movements start to influence the whole-target lDDT score, its correlation begins to decrease slowly.
However, the slow decrease in correlation for values of the inclusion radius 424 A (Fig.2) shows the stability of the whole-target lDDT score with respect to the influence of domain movements.
Even including all inter-atomic distances in the calculation (Ro1), which maximizes the effect of domain movement, does not significantly lower the correlation with domain-based GDC-all scores (R2 0.82).
Based on this analysis, we selected a default value of 15 A for the inclusion radius Ro.
This allows the lDDT score to avoid the drawbacks that affect measures based only on very local characteristics, e.g.contact map overlap.
3.1.2 Sensitivity analysis versus relative domain movements Proteins consisting of multiple domains can exhibit flexibility between their domains, which can often be experimentally observed in the form of structures with different relative orientations of otherwise rigid domains.
In many cases, these relative movements play a functional role.
From a modeling assessment perspective, however, the analysis of the relative orientation of the domains must therefore be separated from the assessment of the modeling accuracy of the individual domains.
The insensitivity to relative domain movement makes the lDDT score a good choice for the unsupervised evaluation of predictions of multidomain structures, in contrast to scores based on global superposition.
To illustrate this behavior, Figure 3 shows lDDT and GDC-all scores computed on fulllength structures as a function of the AU-based weight-averaged GDC-all scores (x-axis).
As expected, the correlation between the two types of GDC-all scores is rather poor (R2 0.58), whereas the correlation between the AU-based GDC-all scores and the lDDT scores is good (R2 0.89).
The hybrid nature of the lDDT score allows it to be global enough to evaluate the modeling quality of the protein domains, but local enough to be only marginally affected by their relative orientations in the compared structures.
When using the lDDT score to evaluate predictions, Fig.4.
Baseline lDDT scores for models with simulated threading errors.
lDDT scores of pseudo-models with threading errors for two examples of different CATH Architectures are shown: Alpha Horseshoe (left) and Beta Barrel (right).
The lDDT score is plotted as a function of the introduced threading error (top).
The histograms (bottom) show the distribution of these baseline scores for threading error offset415 residues for the two architectures.
The structure inlays show an example structure of the respective CATH Architecture.
Peaks at large off-sets indicate repetitive structural elements with locally correct arrangement 2725 lDDT: a local superposition-free score for comparing protein structures and models local Distance Difference Test ( ) superposition Ro assessment units ( ) both i n order paper higher than s vs. anaylsis while very  it is not necessary to split the target structure in separate domains, whose identification can be a complex and timeconsuming procedure.
The absolute lDDT score values show a dependency on the structural architecture of the protein being modeled (see Section 3.2 later in the text).
For example, a small group of predictions off-diagonal (GDC-all between 0.2 and 0.35, lDDT between 0.4 and 0.6) belonging to target T0629 show a high correlation within the group, but the slope is different from other targets.
The elongated trimeric structure of T0629 has relatively few intra-chain contacts and is mainly stabilized by interactions between chains.
Thus, local interactions within a chain are mainly limited to trivial nearest neighbor contacts that are easily satisfied in predictions, which explain the higher lDDT scores.
For reference, the correlation between the lDDT and GDC-all scores for single-domain CASP9 targets is shown in Supplementary Fig.S2).
3.2 Validation of lDDT score baselines for different protein folds Because lDDT scores express the percentage of inter-atomic distances present in the target structure that are also preserved in the model, a value of 0 corresponds to 0 conserved distances, and 1 to a perfect model.
However, these extreme values are in practice rarely observed, even in extremely high and low-quality models.
At the high-accuracy end, fluctuations in surface side chain conformations will result in values 51.
For very low accuracy models, still some local inter-atomic distances will be preserved if the model has at least a stereochemically plausible structure and features some secondary structure elements.
In the context of protein model assessment, two types of baseline values are of interest: the expected score when comparing two random structures, and scores for models with correct folds but including threading errors.
In principle, the first value could be estimated using Flory Huggins polymer solution theory (Flory, 1969; Huggins, 1958), e.g.as done for the determination of RPF/DP values for NMR structures (Huang et al., 2012).
However, because protein structures are rich in rigid structural elements like-helices and-sheets, where the relative local positions are restricted, they show in general a higher number of preserved local distances than random polymers.
Based on these considerations, we decided to empirically derive lDDT baseline scores by comparing a reference structure with a set of well-defined decoy models.
A comparison of the FloryHuggins and decoy-based analysis can be found in the SupplementaryMaterials.
The average lDDT score when comparing random structures, i.e.protein models with different architectures (see Materials and Methods), is 0.20 (0.04).
For estimating the effect of alignment shifts in models with otherwise correct fold and stereochemistry, we created pseudomodels starting from the original protein structure and introducing threading errors of increasing magnitude for different representative structure architectures from CATH (Cuff et al., 2011).
We then compared the pseudomodels with the original structure, computing their lDDT scores against it.
Here, we show the results for CATH architecture entries 1.25 (Alpha Horseshoe) as example for proteins rich in-helices, and 2.40 (Beta-barrel) as representative for a-sheet protein (Fig.4).
The plots at the top of each panel show the value of the lDDT scores (on the y-axis) for 60 pseudomodels as a function of the magnitude of the threading error (residue offset) on the x-axis.
For large threading errors, the lDDT scores converge to a baseline range of scores, which appear to be largely independent of the threading error magnitude.
We considered scores in this range to be typical lDDT scores for a low-quality model with the same architecture as the target structure.
For models in the Alpha Horseshoe architecture, the average baseline lDDT score is 0.28, whereas for the Beta barrel class, the value of 0.22 is lower, illustrating the influence of the architecture of the protein.
This indicates that the lower boundary of the lDDT score can vary as a function of the architecture of the target protein, which influences the comparison of absolute raw scores of models for different folds, but not of models of the same architecture.
This is a common behavior of most structure comparison measures.
One interesting feature in Figure 4 is the presence of several peaks at larger threading errors (e.g.around residue 34) in the Alpha Horseshoe architecture example.
These peaks correspond to internal repeats in the structure, which give rise to locally correct models when the threading shift coincides with the size of the repeat.
3.3 Local model accuracy assessment Modeling errors are typically not homogenously distributed over the model, but are localized, e.g.in template-based models often in segments that had to be remodeled de novo.
Residue-based lDDT scores quantify the model quality on the level of a residues environment.
The low sensitivity of lDDT to relative domain movements also applies to per-residue scores.
As shown in Figure 1, local lDDT scores are not dominated by different domain orientations between the target and the model structures, but correctly reflect the accuracy of the local atomic environment surrounding the residue under investigation in the model.
Figure 1 shows a superposition of the structure of target T0542 (in light gray) with prediction by group TS236 (colored according to the full-length lDDT score).
The models represent each of the two individual domains with high accuracy, but their relative domain orientation does not correspond to the target structure.
Superposition-based scores would assign a high score to one of the domains but not to the other, or require scoring based on isolated domain.
As illustrated on the right panel (Fig.1), residues with low lDDT score correspond to regions of large local structural divergence between the two domain structures, irrespective of the domain movement between them.
As expected, low local scores can also be detected at the interface between the two domains where the interactions cannot be modeled correctly without knowing their relative orientation in the target.
3.4 Stereochemical realism assessment Although validation of the stereochemical plausibility of protein models is a routine procedure for experimental structure determination, e.g.in X-ray crystallography (Read et al., 2011), this is not a common practice in theoretical modeling.
Depending on the applied method, models generated in silico may reveal rather unrealistic stereochemical properties.
Typically, numerical scores applied in retrospective model assessment compute a measure for the average atomic dislocation between the reference structure and the model, without considering the stereochemical quality of 2726 V.Mariani et al.below whichlower than since s+/-to s about while are template which grey While the latter.
Consequently, two models with similar average atomic displacements may nevertheless differ significantly in their stereochemical plausibility, and some models might include atomic arrangements that are physically impossible.
To address this question, lDDT incorporates a stereochemical plausibility check, which assesses two aspects of model quality: the lengths of chemical bonds and the widths of angles in the model structure.
Bond and angle measurements are compared with a set of standard parameters derived from high-resolution crystal structures (Engh and Huber, 2006).
A stereochemical violation is defined as a parameter deviating from the expected values by more than a specified number of standard deviations (default: 12; see Supplementary Material).
Inter-atomic distances between non-bonded atoms in the model are compared with the sum of their Van der Waals radii (Allen, 2002), and a violation (clash) is assigned if two atoms are closer than the sum of the Van der Waals radii, allowing a certain tolerance (default: 1.5 A).
When calculating the lDDT score, all distances involving side-chain atoms of a residue involved in any type of stereochemical violations in the model are considered as non-preserved.
In cases where backbone atoms are involved in stereochemical violations, all distances involving this residue are considered nonpreserved.
This approach leads to the lowering of the final lDDT score of a model according to the extent of the structures stereochemical problems (Fig.5).
As an example, Figure 5 shows the CASP9 prediction TS276_1 for target T0570-D1.
The backbone of the prediction can be superposed accurately to the backbone of the target structure (left panel), and the prediction has indeed a high GDT-HA score (0.814).
Displacement-based all-atom scores do not immediately reveal the problems, with a GDC-all score of 0.705 and an lDDT score without stereochemical checks of 0.682.
However, when the lDDT score includes stereochemical check, the lDDT score drops to 0.571.
Panel B shows a close-up of the region around residue alanine 21, where several stereochemical violations are evident.
3.5 Multireference structure comparison The typical situation for protein structure prediction assessment is to compare a model against a single reference structure.
There are, however, cases where several equivalent reference structures are available, e.g.structural ensembles generated by NMR, crystal structures with multiple copies of the protein in the asymmetric unit (non-crystallographic symmetry) (e.g.target T603 in CASP9), or independently determined X-ray structures for the same protein at different experimental conditions.
In these cases, no structure can be considered more reliable than any other.
However, owing to the choice of different templates, models often have a higher similarity to one or the other reference structure, and the choice of reference for the evaluation score can lead to very different results for models of equal quality.
In case of the lDDT, the following approach allows to evaluate a model simultaneously against an ensemble of reference structures: for each pair of atoms, we define an acceptable distance range by taking the minimal and maximal distance observed across all references where the atoms are present.
If, in any of the reference structures, the distance is longer than the inclusion radius Ro, this distance is considered a long-range interaction, and is ignored.
For the assessment, the corresponding distance in the model is considered preserved, when it falls inside the acceptable range or outside of it by less than a predefined threshold offset.
One obvious application of the multi-reference lDDT score is the evaluation of models against NMR structure ensembles.
For example, in the case of CASP9 target T0559 (PDBID: 2L01), an ensemble of 20 NMR structures has been experimentally determined.
Selecting one single chain from the ensemble as reference to evaluate prediction models would be an arbitrary decision, artificially favoring some models that are closer to that specific structure.
To estimate the effect of selecting a single reference structure, all structures in the ensemble were in turn used as a model and evaluated against all the others.
Using traditional pairwise comparison with GDC-all scores (Fig.6, striped bars), fluctuations of almost 12 GDC points around an overall low Fig.6.
Comparing a model against an ensemble of reference structures.
The experimental reference structure for CASP target T0559 (human protein BC008182, PDBID:2L01) is an ensemble of NMR structures.
The graph shows the effect of selecting a single structure as reference (GDC-all values as striped bars) in contrast to the multireference lDDT implementation (dotted bars).
For this example, each structure within the ensemble was selected in turn as reference and compared with the other members Fig.5.
Assessing stereochemical plausibility.
This example illustrates the stereochemical quality checks on lDDT score for a model (TS276, left side as ribbon representation) for target T0570-D1 with unrealistic stereochemistry (close-up, right).
Residues with too short (1) or too long (2) chemical bonds, as well as those with close atomic interactions (3) or impossible bond angles (4), result in lower scores during the lDDT computation 2727 lDDT: a local superposition-free score for comparing protein structures and models which to sDisplacement does close for example , due local distance difference test which value of 0.77 are observed.
To avoid this, variable regions of the ensemble are often excluded from the assessment (Clarke et al., 2007; Kinch et al., 2011; Mao et al., 2011).
Ideally, this situation should be avoided, and a prediction should not be rewarded or penalized for being more similar to one member of the ensemble than to another.
The multireference version of the lDDT score has been developed to overcome this problem by sampling the conformational space covered by the ensemble and compensating for its variability.
Using the same example, the multireference lDDT score, which uses one chain as a model and all the others together as multireferences, shows a spread of 51% (Fig.6, dotted bars), indicating its robustness when scoring a model against an ensemble of equivalent reference structures.
Recently, methods using elastic network models have been proposed to computationally explore the intrinsic flexibility landscape for a single reference protein (Perez et al., 2012).
4 CONCLUSION In this article, we describe the lDDT score, which combines an agreement-based model quality measure with (optional) stereochemical plausibility checks.
We have demonstrated its low sensitivity with respect to domain movements in case of multidomain target proteins, which allows for automated assessment without the need for manually splitting targets into AUs.
We also have shown that local atomic interactions are well captured and local lDDT scores faithfully reflect the modeling quality of sub-regions of the prediction.
In addition, we present an approach to compare models against multiple reference structures simultaneously without arbitrarily selecting one reference structure for the target, or removing parts that show variability.
Additionally, as an agreement-based score, lDDT is robust with respect to outliers.
One disadvantage of the lDDT score is that it does not fulfill the mathematical criteria to be a metric.
However, the same is true for most scores commonly applied for structure comparison such as GDT, or RSMD based on iterative superposition when comparing models with different number of atoms.
We consider lDDT particularly suited for the evaluation of predictions for the same target protein, e.g.in the context of the CASP and CAMEO (www.cameo3d.org) experiments.
For these kind of applications, unlike, e.g.for clustering protein structures, we do not see the lack of metric properties as a significant limitation.
ACKNOWLEDGEMENT The authors gratefully acknowledge the financial support by the SIB Swiss Institute of Bioinformatics.
Conflict of Interest: none declared.
ABSTRACT Summary: NAViGaTOR is a powerful graphing application for the 2D and 3D visualization of biological networks.
NAViGaTOR includes a rich suite of visual mark-up tools for manual and automated annotation, fast and scalable layout algorithms and OpenGL hardware acceleration to facilitate the visualization of large graphs.
Publication-quality images can be rendered through SVG graphics export.
NAViGaTOR supports community-developed data formats (PSI-XML, BioPax and GML), is platform-independent and is extensible through a plug-in architecture.
Availability: NAViGaTOR is freely available to the research community from http://ophid.utoronto.ca/navigator/.
Installers and documentation are provided for 32-and 64-bit Windows, Mac, Linux and Unix.
Contact: juris@ai.utoronto.ca Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION The availability of proteinprotein interaction (PPI) data is increasing rapidly through literature-derived databases (Bader et al., 2003; Breitkreutz et al., 2002; Hermjakob et al., 2004a; Peri et al., 2004; Xenarios et al., 2000; Zanzoni et al., 2002), high-throughput detection methods (Barrios-Rodiles et al., 2005; Rual et al., 2005) and computational predictions (Brown and Jurisica, 2005; Persico et al., 2005).
These data, collectively referred to as the interactome, are critical to our understanding of both normal cellular processes and disease mechanisms.
Visualizing the interactome, along with integrating orthogonal data types, may aid in the understanding of cell function, help elucidate hidden relationships within the data and help prioritize functional studies.
Several biological graph visualization tools are currently available, implementing a diverse range of approaches and algorithms (Breitkreutz et al., 2003; Chin et al., 2008; Hu et al., 2004; Ju and Han, 2003; Macpherson et al., 2009; Paananen and Wong, 2009).
Cytoscape (Shannon et al., 2003), in particular, has been widely adopted by the biological community for its ease of use and extensibility through open source plug-in development.
To whom correspondence should be addressed.
While many of these tools are effective and widely used, there are several critical areas where these applications require improvement (reviewed in Suderman and Hallett, 2007).
Scalability is essential to visualize the tens of thousands of known PPI, which is a challenge for current layout algorithms and software.
Biological graph drawing software must also be able to handle richly annotated data, including genomic and proteomic profiles, KEGG pathways (Kanehisa and Goto, 2000), Gene Ontology (GO) annotations, data in PSI-MI (Hermjakob et al., 2004b) and BioPAX formats (http://www.biopax.org/), in addition to the vast quantity of microarray data that is currently available.
NAViGaTOR builds upon these earlier efforts, addressing known issues in existing software.
NAViGaTOR uses a combination of hardware-based graphics acceleration and highly optimized layout algorithms to enable interactive visualization of large networks.
It supports community-based data interchange formats, such as PSIMI, BioPAX and GML, facilitating interoperability with existing software tools.
Additionally, NAViGaTOR includes a rich suite of built-in analysis and visualization functions, which can be extended through an application programming interface (API).
Here, we describe the implementation of NAViGaTOR, and highlight how this tool improves upon existing network visualization packages.
2 SOFTWARE 2.1 Implementation NAViGaTOR has been implemented in Java (v1.6), providing platform-independence, and uses JOGL (https://jogl.dev.java.net/) to enable OpenGL hardware-accelerated graphics rendering.
At present, the core code-base is closed-source to ensure stability, but future enhancements will extend the plug-in API to an OSGicompliant (http://www.osgi.org/Main/HomePage) framework that enables community-driven extensibility.
2.2 Features NAViGaTOR enables interactive visualization and analysis of complex graphs that are typical in systems biology studies.
Graphs can be loaded from PSI-MI XML, BioPax, GML and tab-delimited text files, or through online databases such as I2D (http://ophid.utoronto.ca/i2d) and cPATH (http://cbio.mskcc.org/cpath/).
Both The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[14:42 10/11/2009 Bioinformatics-btp595.tex] Page: 3328 33273329 K.R.Brown et al.Fig.1.
Screen capture of the NAViGaTOR user interface.
Labels indicate the various tools and descriptive regions of the interface.
A graph is shown in the Graph Panel, with edges adjusted automatically by Edge Filters.
Filters can be used to automatically control visual attributes of both nodes and edges.
2D and 3D network views are fully interactive, allowing the user to manually manipulate the graph, or to use automated layouts such as circular, linear or concentric circular on subsets of nodes or entire graphs.
A spreadsheet view supports selecting and deselecting nodes, edges and paths based on any attributes.
Nodes and edges can be grouped into subsets, which can be collapsed or expanded to simplify views, or manipulated through set operations.
Network analysis tools provide information about node and edge connectivity, shortest paths, identify hubs, cliques and articulation points and summarize network statistics.
NAViGaTOR can also use a multithreaded implementation to efficiently generate random networks for enrichment analyses.
Fully annotated graphs can be exported to six different graphics formats, including PDF and SVG.
In summary, NAViGaTOR provides a network analysis platform that is rich in the features essential to many biological applications, and yet is extensible through a plug-in interface to include additional features when required.
See Figure 1 and the Supplementary Materials for examples of the NAViGaTOR interface and rendered networks.
2.3 Advances NAViGaTORs ability to handle larger datasets is facilitated through optimized layout algorithms, hardware-based graphics acceleration and a reduced memory footprint relative to other software.
NAViGaTOR performs an initial layout using Graph Drawing with Intelligent Placement (GRIP; Gajer and Kobourov, 2002), which performs network layout in near linear time, and then continuously updates the layout of the graph using a multi-threaded grid-variant (Fruchterman and Reingold, 1991) of a force-directed layout algorithm.
When benchmarked against the force-directed algorithms in Cytoscape and VisANT, NAViGaTOR consistently provided graphs rendered in significantly shorter time (Fig.2; Supplementary Fig.3.3).
Only the yFiles Organic plug-in for Cytoscape rendered in similar time to NAViGaTOR, although the resulting graph was poorly optimized (compare Supplementary Fig.3.5C to Supplementary Fig.3.4C).
OpenGL enables NAViGaTOR to take advantage of hardwarebased acceleration to render larger graphs in both 2D and 3D.
Additionally, the data structures within NAViGaTOR were designed to maintain a small memory footprint in order to provide greater scalability for large datasets.
When compared against Cytoscape and VisANT, NAViGaTOR had a memory footprint approximately half that of Cytoscape, although a 1238% larger footprint than VisANT (Supplementary Fig.5.1).
The NAViGaTOR user interface includes unique tools to help simplify the hairball, which is a common challenge in many PPI 3328 [14:42 10/11/2009 Bioinformatics-btp595.tex] Page: 3329 33273329 NAViGaTOR 0 500 1000 1500 2000 2500 3000 yF ile s O rg an ic Fo rc edir ec te d RS FD P Fo rc edir ec te d Un do O N Un do O FF Fo rc edir ec te d Re lax ed e leg an t Fo rc edir ec te d Layout Algorithm/Status T o ta l V is u al iz at io n T im e (s ) Rendering Loading Cytoscape Interviewer3 NAViGaTOR VisANT Osprey Fig.2.
Performance comparisons between applications.
The Reactome Caenorhabditis elegans BioPax file was used to test the performance of several graph visualization applications in loading and visualizing the graph.
Only Cytoscape and NAViGaTOR were able to load the BioPax file directly; Interviewer3 required a GML export from NAViGaTOR, VisANT required a PSI-MI XML v1.0 file, and Osprey required a tab-delimited text file.
Stacked bars were used to show the cumulative loading and rendering time, or the total time to view a graph.
networks.Alpha blending is a technique to deemphasize unimportant areas of the network and focus on important areas by fading out selected nodes and edges.
Automated filters let users map node and edge properties, such as color, size, shape and opacity to any numeric or text attribute.
For instance, nodes can be scaled by degree or betweenness centrality, and colors can be mapped to GO ontology categories.
Rectangle and lasso selection, and the unique ability to (de)select a connected neighborhood of nodes by dragging out its radius in edges, allow users to easily select specific subsets of nodes and edges, while other tools allow the selected subset to be rotated, scaled or laid out along a line or circle.
Combined with pan/zoom navigation, users can quickly explore or simplify complicated networks.
3 FUTURE DEVELOPMENT NAViGaTOR has evolved from an in-house visualization tool to a more versatile, comprehensive platform.
While the current version of NAViGaTOR includes a plug-in API, NAViGaTOR 3.0 will adopt a more formal open plug-in interface compliant with the OSGi framework.
This framework will allow for communitydriven development through small, tightly coupled bundles while protecting the core code-base of the application.
NAViGaTOR will also serve as a platform to explore novel ways for biologists to interact with graphs, as well as new ways to encode and display information in biological networks.
ACKNOWLEDGEMENTS We would like to acknowledge Richard Lu and Frederic Breard for supporting the I2D database, which provides PPI data and annotations for NAViGaTOR, Rick Valenzano for researching and implementing the GRIP layout, and Uzma Khan for helping with updating and improving the NAViGaTOR web site (http://ophid.utoronto.ca/navigator/).
Funding: Genome Canada via the Ontario Genomics Institute; Canada Foundation for Innovation (grant nos 12301 and 203383); Canada Research Chair Program in part; Ontario Research Fund Research Excellence.
Conflict of Interest: none declared
ABSTRACT Motivation: Recent advances in high-throughput experimental techniques have yielded a large amount of data on proteinprotein interactions (PPIs).
Since these interactions can be organized into networks, and since separate PPI networks can be constructed for different species, a natural research direction is the comparative analysis of such networks across species in order to detect conserved functional modules.
This is the task of network alignment.
Results: Most conventional network alignment algorithms adopt a node-then-edge-alignment paradigm: they first identify homologous proteins across networks and then consider interactions among them to construct network alignments.
In this study, we propose an alternative direct-edge-alignment paradigm.
Specifically, instead of explicit identification of homologous proteins, we directly infer plausibly alignable PPIs across species by comparing conservation of their constituent domain interactions.
We apply our approach to detect conserved protein complexes in yeastfly and yeastworm PPI networks, and show that our approach outperforms two recent approaches in most alignment performance metrics.
Availability: Supplementary material and source code can be found at http://www.cs.duke.edu/amink/.
Contact: xinguo@cs.duke.edu; amink@cs.duke.edu 1 INTRODUCTION Understanding complicated networks of interacting proteins is a major challenge in systems biology.
Recently, with the rapid progress of high-throughput experimental techniques, protein protein interaction (PPI) databases have rapidly increased in size, allowing for comparative analysis of PPI networks from which conserved modules can be identified across PPI networks of different species (Sharan and Ideker, 2006; Srinivasan et al., 2007).
By analogy to sequence alignment, this problem is called PPI network alignment.
Typically, PPI network alignment algorithms compare PPI networks of two or more species and identify conserved modules, e.g.pathways or protein complexes.
Often a PPI network is represented as an undirected graph in which nodes indicate proteins and edges indicate interactions.
Hence, the network alignment problem can also be viewed as a graph isomorphism problem.
Many network alignment algorithms have been proposed in recent years and most of them focus on the pairwise alignment of PPI networks.
As an early approach, PathBLAST (Kelley et al., 2003) proposed a likelihood-based scoring scheme to search for conserved pathways.
Sharan et al.(2005a) extended PathBLAST to employ a greedy heuristic to detect conserved To whom correspondence should be addressed.
protein complexes across species.
NetworkBLAST-E (Hirsh and Sharan, 2007) introduced an evolutionary model of networks into the alignment scoring function to extract conserved complexes.
MaWISh (Koyutrk et al., 2006) merged pairwise interaction networks into a single alignment graph and treated network alignment as a maximum weight induced subgraph problem.
MNAligner (Li et al., 2007) described an integer quadratic programming (IQP) model to identify conserved substructures.
Recently, several network alignment algorithms have been developed that can align more than two species.
Graemlin (Flannick et al., 2006) is capable of aligning at least 10 microbial networks at once.
NetworkBLAST (Sharan et al., 2005b), another extension of PathBLAST, can align networks of up to three species, and its later version, NetworkBLAST-M (Kalaev et al., 2008), can align 10 networks with tens of thousands of proteins in minutes.
In addition, Singh et al.(2008) described a method inspired by Googles PageRank to detect global alignments from five eukaryotic PPI networks.
All these network alignment algorithms follow a node-thenedge-alignment paradigm.
That is, they generally first need to identify homologous proteins across species before they can exploit protein interaction and network topology information to detect conserved subnetworks.
The node alignment step essentially acts as a filter, artificially constraining the search space of conserved modules to putatively homologous protein pairs.
However, proteins rarely act alone.
They interact with each other to carry out their activities, and these interacting proteins are likely to evolve with high correlation during the evolution of species (Goh et al., 2000; Mintseris and Weng, 2005; Pazos et al., 1997).
Furthermore, it has been shown recently that such co-evolution is more evident if we focus our attention on interacting domains that are responsible for PPIs (Itzhaki et al., 2006; Jothi et al., 2006; SchusterBckler and Bateman, 2007).
Based on these observations, we present DOMAIN, an algorithm for domain-oriented alignment of interaction networks, that follows an alternative direct-edgealignment paradigm.
DOMAIN does not explicitly restrict its attention to putatively homologous proteins.
Instead, it directly aligns PPIs across species by decomposing PPIs in terms of their constituent domaindomain interactions (DDIs) and looking for conservation of these DDIs.
We apply DOMAIN to detect conserved protein complexes in yeastfly and yeastworm PPI networks, and demonstrate that it achieves better results than two previous techniques in most performance metrics.
The article is organized as follows: Section 2 presents the details of DOMAIN.
Section 3 describes the quality assessment measures, as well as the experimental results of DOMAIN compared with two extant methods.
In Section 4, we discuss implications of the results, along with further directions.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:58 15/5/2009 Bioinformatics-btp202.tex] Page: i241 i240i246 DOMAIN: Domain-oriented alignment of interaction networks Fig.1.
Method overview.
(1) Constructing APEs.
The input of DOMAIN includes two PPI networks and the constituent domains of the proteins.
Using this information, DOMAIN calculates species-specific DDI probabilities, and then identifies a set of APEs across networks.
(2) Building an APE graph.
An APE graph is a merged representation of the PPI networks, in which each node represents an APE and each edge represents one of four network connectivities connecting two APEs: (a) alignment extension, (b) node duplication, (c) edge indel (insertion/deletion), or (d) edge jump.
The details of these connectivities are given in Section 2.2.
(3) Searching for high-scoring non-redundant subgraphs within the APE graph.
We use a greedy heuristic to carry out this task.
2 METHODS As illustrated in Figure 1, DOMAIN consists of three stages: (1) it constructs a complete set of alignable pairs of edges (APEs); (2) it builds an APE graph; and (3) it employs a heuristic search to identify conserved protein complexes across species.
The three subsections that follow elaborate upon these three stages.
2.1 Constructing and scoring APEs Domains are structural and functional units of proteins.
Many studies (Bernard et al., 2007; Deng et al., 2002; Riley et al., 2005) have revealed that direct PPIs are often mediated by interactions between the constituent domains of the two interacting proteins.
These studies have made two particular assumptions that we adopt as well: (1) DDIs are independent of each other, and (2) two proteins interact if at least one pair of domains from two proteins interact.
These assumptions allow us to formulate the probability of an interaction between two proteins in terms of a noisy-or over the DDIs that might possibly mediate the interaction between those two proteins.
In our network alignment scenario where we seek to align edges directly, we additionally assume that a pair of cross-species PPIs can be aligned to one other only if they are plausibly mediated by at least one common DDI.
We represent the input PPI networks from two species as undirected graphs G1(V1,E1) and G2(V2,E2), where nodes indicate proteins and edges indicate the observed PPIs.
We first wish to construct a complete set of APEs.
We say that a pair of edges, e1E1 and e2E2, is alignable if there exists a DDI that can plausibly mediate the two PPIs represented by that pair of edges.
We say that a DDI can plausibly mediate a PPI if the corresponding interaction probability between the two domains is above some value >0.
Using a non-zero value for allows us to filter out domains between which there is negligible evidence of a DDI.
For an edge eE1 or E2, we define D(e) to be all the possible interactions between the constituent domains of the two proteins.
Given the speciesspecific probabilities of DDIs that mediate PPIs, we can then write the score of an APE c= (e1,e2) using a noisy-or formulation: f (c)=Pr(e1,e2|1,2)=1 d,D(e1) D(e2) (1g(1, ,2, )) where d, denotes an interaction between domains and , and , = Pr(d, ), and ={, }.
The function g(1, ,2, ) measures the probability of aligning the PPI e1 to the PPI e2 mediated by interactions between domains and.
In this work, we have chosen to set g(1, , 2 , )= (1, 2, )1/2.
As previous authors have also done, to estimate the species-specific DDI probabilities , we applied the EM (expectationmaximization) algorithm of Deng et al.(2002) for each given network.
2.2 Building an APE graph The APE graph is motivated by the evolutionary model of PPI networks suggested by Berg et al.(2004).
The model indicates that PPI networks are shaped primarily by two kinds of evolutionary events, link dynamics and gene duplication.
Link dynamics events are primarily caused by sequence mutations of a gene and affect the connectivities of the protein whose coding sequence undergoes mutations.
Gene duplication, the second kind of evolutionary event, is often followed by either silencing of one of the duplicated genes or by functional divergence of the duplicates.
From the perspective of protein domains, a link dynamics event may result from switching a constituent domain of a protein to another, or a change in a domains interaction partners; a gene duplication event consists of duplication of one protein, followed by a domain switching or being removed in one or both of the duplicates, or followed by progressive small changes from point mutations that cause a change in domain interaction partners.
With this motivation in place, we define an APE graph to be an undirected weighted graph, where nodes correspond to the APEs identified above, and edges correspond to one of four evolutionary relationships that we consider between two APEs, as illustrated in Figure 2 and as listed below: (a) Alignment extension: two APEs are connected if they share two proteins, one per species.
i241 [09:58 15/5/2009 Bioinformatics-btp202.tex] Page: i242 i240i246 X.Guo and A.J.Hartemink Fig.2.
Four connectivities in an APE graph.
The details of these connectivities are given in the text, and the legend is the same as in Figure 1.
(b) Node duplication: two APEs are connected if they share a protein in one species and a PPI in the other.
(c) Edge indel (insertion/deletion): two APEs are connected if they share a protein in one species and the graph distance between the two PPIs in the other network is 1.
(d) Edge jump: in this case, all proteins within the two APEs are distinct, but for each species, the graph distance between the two PPIs in their corresponding network is 1.
We consider this case because our current knowledge of both PPIs and DDIs is noisy and incomplete.
Thus, if there exists a pair of PPIs that can make two APEs connected in each network, we treat the pair as a potential APE.
Note that some insignificant DDIs (probabilities of DDIs <) are shared in such potential APEs.
Given this definition of an APE graph, we note that every subgraph in an APE graph corresponds to a network alignment.
Each node in an APE graph contributes the score f (c) of its corresponding APE, and each edge is scored by a positive number according to its connection relationship.
Using these edge scores, we want to reward alignment extension and penalize both node duplication and edge indel.
Let a, b, c and d be the edge scores of alignment extension, node duplication, edge indel and edge jump, respectively.
We thus need to assign a>1 and b,c<1.
Because we neither wish to reward nor penalize an edge jump, we simply assign d =1.
For a subgraph Gs(Vs,Es) in an APE graph, the overall score for its corresponding network alignment is calculated as S(Gs)= eEs (e)  cVs f (c) where (e) is the edge score for eEs, and f (c) is the score of the APE cVs.
2.3 Detecting protein complexes Network alignment methods generally require a search algorithm to detect high-scoring subgraphs from a single or several weighted graphs.
Such tasks are computationally difficult, so a number of search heuristics have been proposed: for example, PathBLAST uses randomized dynamic programming to search for conserved pathways across networks, while NetworkBLAST-E implements a greedy heuristic to search for conserved protein complexes.
As many pairwise network methods aim to identify conserved protein complexes, for comparative purposes, we devise a greedy heuristic for finding conserved protein complexes across species.
The heuristic aims to identify high-scoring non-redundant subgraphs from the resultant APE graph.
Specifically, exhaustively starting from each APE, we iteratively expand the subgraph by introducing a new APE that increases the alignment score the most, until any of the following empirical stopping conditions occur: (i) the number of proteins in either species exceeds an upper limit (we used 15); (ii) the score of the next expanding APE is smaller than a threshold (we used 102); (iii) the overall alignment score of the subgraph is smaller than a threshold (we used 103); or (iv) the graph distance of the next expanding APE exceeds an upper limit (we used 4).
At the end, small and redundant subgraphs are removed if the number of proteins in a subgraph is less than four, or if there exists a higher scoring subgraph overlapping > 80% of proteins in either species.
3 RESULTS 3.1 Experimental setup We compare our method to two extant pairwise network alignment algorithms, NetworkBLAST and MaWISh.
We do not include NetworkBLAST-M and Graemlin in our comparisons because they mainly focus on alignment of multiple networks, and because Graemlin requires the unavailable in-house SRINI algorithm (Srinivasan et al., 2006) to assign weights to PPIs.
The ISOrank algorithm aims at resolving a different problem of aligning networks globally, while NetworkBLAST-E performs similarly to NetworkBLAST and is not available online.
We thus exclude these methods from the comparisons as well.
We apply DOMAIN on yeastfly and yeastworm PPI networks taken from DIP (Database of Interacting Proteins, Oct 2008) (Xenarios et al., 2002), as they were widely used in pairwise network alignment studies as benchmarks.
The protein-to-domain mappings are taken from Pfam (Pfam 23.0) (Finn et al., 2008), and we only consider high-quality Pfam-A entries.
Because not all proteins contain significant Pfam domains, we generate a so-called backbone network, a subnetwork of DIP in which all proteins contain at least one Pfam-A domain.
As summarized in Table 1, 78.2% of MIPS annotated proteins and over 70% of GO annotated proteins are contained in backbone networks.
To simplify the setting of the four parameters, we reduced the parameter space to one dimension by insisting that a =k, b =c =1/k and d =1, for some value of k>1.
We found that DOMAIN was not sensitive to changes in k. In the results that follow, we used k=10.
3.2 Experimental results We employ three measures to evaluate the biological significance of the alignments: sensitivity/specificity, MIPS purity and GO enrichment.
These measures are also suggested in several other network alignment studies (Dutkowski and Tiuryn, 2007; Hirsh and Sharan, 2007; Kalaev et al., 2008).
The first two measures use the known yeast protein complexes cataloged in MIPS (May 2006) (Mewes et al., 2002) as a gold standard.
We exclude category 550 (obtained from highthroughput experiments) and only use complexes at level 3 or lower.
In consequence, there exist 122 MIPS complexes spanning 519 yeast proteins in the yeast backbone network, 62 of which contain at i242 [09:58 15/5/2009 Bioinformatics-btp202.tex] Page: i243 i240i246 DOMAIN: Domain-oriented alignment of interaction networks Table 1.
Summary of backbone networks DIP Backbone DIP Yeast Fly Worm Yeast Fly Worm Number of PPIs 17 528 22 381 4038 11 426 11 013 2213 Number of proteins 4928 7446 2644 3300 4500 1620 Number of GO annotated proteinsa 4625 4477 1566 3280 3253 1145 Number of MIPS annotated proteinsb 1100  860  aWith respect to the biological process annotation of Gene Ontology.
bExcluding MIPS category 550. least three proteins and span 438 proteins.
For each identified yeast alignment, we try to find a complex from MIPS that maximizes the hypergeometric score and calculate an empirical enrichment P-value.
The significance level is obtained from sampling 10 000 random sets of proteins of the same size, and the P-values are corrected for multiple testing using the false discovery rate (FDR) (Benjamini and Hochberg, 1995).
Then, the specificity is defined as the percent of yeast alignments that have a significant match in MIPS (P<0.05), and the sensitivity is defined as the percent of MIPS alignments that have significant matches in the resulting alignments.
Moreover, an alignment is called a pure alignment if it satisfies two conditions: (i) it contains at least three MIPS annotated proteins and (ii) there exists a complex in MIPS that covers >75% of its MIPS annotated proteins.
We report purity, calculated by the number of pure alignments divided by the total number of alignments with at least three MIPS annotated proteins, as an alternative measure of the sensitive identification of specific complexes.
GO enrichment measures the functional coherence of the proteins in an identified alignment with respect to the biological process annotation of GO, for each species separately.
We use the tool GO TermFinder (Boyle et al., 2004) to compute empirical enrichment P-values, and correct for multiple testing using FDR.
For each species, we report the fraction of process-coherent alignments with P-value <0.05 (considering only the alignments with at least one GO annotated protein).
We chose to set the probability threshold of DDIs to the low but non-zero value of 1020 so as to take into account as much DDI information as possible.
For yeastfly alignment, DOMAIN generated an APE graph consisting of 6918 APEs with 47 964 alignment extension links, 24 549 node duplication links, 5573 edge indel links and 1149 edge jump links; for yeastworm alignment, it returned a 1410-node APE graph with 4230 alignment extension links, 4087 node duplication links, 140 edge indel links and 37 edge jump links.
For accurate comparison, we applied NetworkBLAST and MaWISh on backbone networks with their suggested parameter settings [see Koyutrk et al.(2006) and Sharan et al.(2005b) for details].
As summarized in Tables 2 and 3, DOMAIN identified more significant non-redundant alignments than NetworkBLAST and MaWISh in both alignmentsexplaining the good scores on the sensitivity metricbut also managed to outperform the other methods on the specificity and purity metrics.
Indeed, it achieved the highest performance on almost every evaluation metric, and in the instances in which it was bested, the difference is slight.
The running time of DOMAIN is comparable with MaWISh and NetworkBLAST.
DOMAIN is currently implemented in Perl, and its running time on yeastfly and yeastworm backbone networks is <1 min (Intel Core 2 CPU 6600@2.4 GHz, 2 GB RAM).
Because the running time is so small, we were able to exhaustively expand from all APEs.
If for some reason we needed to further reduce computational complexity, we could instead consider an alternative expansion strategy where we would expand only from seedAPEs.
The idea would be that if a protein complex is conserved in many species, the PPIs in this complex are likely to be conserved as well, and therefore the corresponding subgraph in the APE graph should contain many alignment extension links.
With this in mind, we could rank the APEs by counting the number of their surrounding alignment extension links and select, say, the top 25% as seeds for expansion.
We tested this, and the results were nearly identical to those listed in Tables 2 and 3, but the running time for yeastfly and yeastworm alignments reduces to 30 and 15 s, respectively.
In our case, the running time was not a problem, but it is reassuring that a seed-based expansion strategy seems to be effective at reducing the running time without affecting the results.
3.3 Case studies DOMAIN is sensitive at detecting small network alignments that might be deemed by other algorithms to be topologically insignificant.
For example, DOMAIN reported a network alignment between the yeast NEF1 complex and the fly proteins mei-9, Ercc1 and Xpac with high confidence (Fig.3).
The GO process coherence of these three fly proteins is significant: nucleotide excision repair (P 108), DNA repair (P 106), cellular response to DNA damage stimulus (P 106), etc.
However, neither MaWISh nor NetworkBLAST reports any alignment involving the yeast NEF1 complex.
They are likely to miss such alignments because (i) the sequence similarity between Rad10 and Ercc1 is insignificant (BLAST E-value 108) and may be ignored if using a restrictive BLAST E-value threshold [e.g.1010 suggested in Hirsh and Sharan (2007)], and (ii) this alignment consists of only three matched proteins and two conserved interactions, so it may not be sufficiently topologically significant for some aligners to detect.
On the other hand, the DDIs within this alignment are well-conserved across i243 [09:58 15/5/2009 Bioinformatics-btp202.tex] Page: i244 i240i246 X.Guo and A.J.Hartemink Table 2.
Performance comparisons of DOMAIN with NetworkBLAST and MaWISh on yeastfly backbone networks Method No.
of complexes No.
of proteins Specificity (%) Sensitivity (%) MIPS purity (%) GO enrichment Yeast Fly Yeast (%) Fly (%) DOMAIN 100 338 313 34.0 9.0 66.7 89.0 78.0 NetworkBLAST 82 299 213 31.7 7.4 40.6 87.8 79.3 MaWISh 54 193 142 18.5 4.1 30.0 75.9 66.7 The largest value in each column is indicated in bold.
Table 3.
Performance comparisons of DOMAIN with NetworkBLAST and MaWISh on yeastworm backbone networks Method No.
of complexes No.
of proteins Specificity Sensitivity(%) MIPS purity GO enrichment Yeast Worm Yeast (%) Worm (%) DOMAIN 21 84 63 36.4 3.3 75.0 90.5 9.5 NetworkBLAST 19 82 51 7.7 0.8 60.0 89.5 10.5 MaWISh 11 42 32 11.1 1.6 42.8 63.6 9.1 The largest value in each column is indicated in bold.
Fig.3.
DOMAIN reports a network alignment between the yeast NEF1 complex (MIPS category 510.180.10.10) and the fly proteins mei-9, Ercc1 and Xpac.
The object to the right of the double arrow depicts the corresponding subgraph of this alignment in the APE graph.
species (the DDI probabilities of ERCC4-Rad10 are 1.00 in both species; the DDI probabilities of Rad10-XPA_C are 1.00 and 0.54 in yeast and fly, respectively).
Another advantage of DOMAIN is that often it provides a more comprehensive means of interpreting the identified network alignments, because protein domains are directly relevant to function in many cases.
For instance, Rad14 and Xpac may play a similar role in the biological process of nucleotide excision repair, as they share a common XPA_C domain.
Furthermore, although the XPA_N domain is not reported as a significant domain for Rad14 in Pfam (Evalue =0.023), the alignment of yeast Rad14 to fly Xpac suggests that XPA_N is potentially an important functional domain in Rad14.
Identifying conserved biological pathways across species is another important application of network alignment.
Figure 4a demonstrates an example of alignment reported by DOMAIN Fig.4.
(a) DOMAIN reports an alignment between 10 yeast proteins and 3 worm proteins that significantly matches the pathway of SNARE interactions in vesicular transport in KEGG.
(b) An example of improving network alignment by combining several cross-species pairwise alignments.
(Green, yeast proteins; blue, fly proteins; orange, worm proteins.)
between 10 yeast proteins and three worm proteins, in which nine of the yeast proteins (all except Nyv1) and all three worm proteins are known to be involved in the pathway of SNARE interactions in vesicular transport in KEGG (Kanehisa and Goto, 2000).
i244 [09:58 15/5/2009 Bioinformatics-btp202.tex] Page: i245 i240i246 DOMAIN: Domain-oriented alignment of interaction networks Alignment performance may further be improved by combining several cross-species pairwise network alignments.
Figure 4b shows an example of combining three alignments taken from yeastfly, yeastworm and flyworm network alignments, respectively.
By aligning yeast and fly networks, DOMAIN detects an alignment between three fly proteins (CG8142, RfC3, and RfC40) and seven yeast proteins, and four of them (Rfc1-4) are involved in the replication factor C complex (MIPS: 410.40.30).
As the yeast replication factor C complex contains five proteins (Rfc1-5), the F-score1 is 0.67.
Further, we see that two worm proteins (F44B9.8 and Rfc-2) are aligned to all these three fly proteins in flyworm alignment and three of these seven yeast proteins (Rfc2-4) in yeastworm alignment.
This three-way alignment suggests that the alignment between fly proteins CG8142, RfC3 and RfC40 and yeast proteins Rfc2-4 are of high confidence, and the F-score is increased to 0.75.
4 CONCLUSIONS In this study, we described DOMAIN, a domain-oriented pairwise network alignment framework.
To our knowledge, DOMAIN is the first algorithm to introduce protein domains into the network alignment problem.
Also, DOMAIN uses a novel directedge-alignment paradigm to directly detect equivalent PPI pairs across species and suggests a new graph representation to merge these equivalent PPI pairs and their network evolutionarybased relationships into one graph.
We tested DOMAIN to identify conserved protein complexes in the yeastfly and yeast worm protein interaction networks, and the experimental results show that DOMAIN exhibits better performance than two recent pairwise network alignment methods in most performance metrics.
Although DOMAIN can be applied only to the subset of proteins with domain mappings, we notice that most functionally annotated proteins contain domain structures and remain in this subset.
To overcome this restriction, we may employ a larger domain database, e.g.CDD (Marchler-Bauer et al., 2007), or combine DOMAIN with other network aligners.
In addition, as the set of defined domains expands and is refined over time, this will gradually become less of a restriction.
Further directions for research include extending this approach to multiple network alignment and to network querying.
Since multiple network alignment requires more than two networks by definition, we would simply need to devise an appropriate scoring scheme that can handle more than a pair of alignable PPIs at once, and then extend the notion of the APE graph accordingly.
The goal of network querying is to identify subnetworks in a given network that are similar to the query.
Typically, the query is a hypothetical or known functional module.
We may simply treat the query as a small input network and apply our DOMAIN method directly on it.
A more sophisticated approach would be to devise a sequence-profile-like structure to describe the DDI contents of the network query, as well as perhaps constructing such structures for the full network as a one-time expense for many successive queries.
1F-score is defined as F =2(precisionrecall)/(precision+recall).
ACKNOWLEDGEMENTS The authors would like to thank Bruce Donald and anonymous reviewers for helpful discussions and comments on this article, and Michael Mayhew for suggesting the name DOMAIN.
Funding: Duke Graduate School Fellowship (to X.G.
); a National Science Foundation CAREER award (NSF 0347801 to A.J.H.
); Alfred P. Sloan Research Fellowship (to A.J.H.
); National Institutes of Health (P50-GM081883-01 and R01-ES015165-01 to A.J.H.
); DARPA (HR0011-08-1-0023 to A.J.H.).
Conflict of Interest: none declared.
Abstract Proteinpeptide interactions, where one partner is a globular protein (domain) and the other is a flexible linear peptide, are key components of cellular processes predominantly in signaling and regulatory networks, hence are prime targets for drug design.
To derive the details of the proteinpeptide interaction mechanism is often a cumbersome task, though it can be made easier with the availability of specific databases and tools.
The Peptide Binding Protein Database (PepBind) is a curated and searchable repository of the structures, sequences and experimental observations of 3100 proteinpeptide complexes.
The web interface contains a computational tool, protein inter-chain interaction (PICI), for computing several types of weak or strong interactions at the proteinpeptide interaction interface and visualizing the identified interactions between residues in Jmol viewer.
This initial database release focuses on providing proteinpeptide interface information along with structure and sequence information for proteinpeptide complexes deposited in the Protein Data Bank (PDB).
Structures in PepBind are classified based on their cellular activity.
More than 40% of the structures in the database are found to be involved in different regulatory pathways and nearly 20% in the immune system.
These data indicate the importance of protein peptide complexes in the regulation of cellular processes.
PepBind is freely accessible at http://pepbind.bicpu.edu.in/.
hur PP).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A Introduction Functional analyses of proteins involve the exploration of their interactions with other molecules, which plays vital roles in different pathways.
Nearly 60% of the interaction pathways such as signal transduction, apoptotic, immune system and other pathways contain domains with bound peptides [1].
These interactions are prevalent in Src homology 2 (SH2) domain, major histocompatibility complex (MHC), antibodies, proteases, calmodulin, PapD chaperone and OppA (oligopeptide permease cademy of Sciences and Genetics Society of China.
Production and hosting 242 Genomics Proteomics Bioinformatics 11 (2013) 241246 A) structures, with variable sequence specificity and binding affinity [2].
Proteinpeptide interactions require only a small interface and can occur in many interaction networks.
Hence, these are attractive drug targets both for small molecules and inhibitory peptides [35].
This implies that synthetic peptides can be designed to alter specific interactions in disease or other pathways [1,6,7].
Out of the structures deposited in the Protein Data Bank (PDB) [8], every month around 20 new entries are shown to exhibit interactions with small peptides.
As the number of new and interesting proteinpeptide complex structures continue to expand, our understanding of these proteinpeptide recognition events should improve.
To understand and analyze the proteinpeptide interactionmechanisms, a reliable database of proteinpeptide complexes is necessary.
A number of sequence-based proteinpeptide interaction databases are available, such as ELM [9], PhosphoELM [10], DOMINO [11], SCANSITE [12], PepBank [13], APD [14], ASPD [15] and BIOPEP [16].
Structural data are also available on proteinpeptide complex structures in peptiDB [17] and PepX [18].
While peptiDB is a set of 103 curated PDB files for non-redundant proteinpeptide complexes, PepX contains 1431 non-redundant X-ray structures clustered based on their binding interfaces and backbone variations.
Previous studies report heterogeneity of domains or proteins to bind multiple peptides (e.g., at least 13 different types of peptides have been reported to bind to SH3domains [19]).
For detailed analysis of interactions of similar proteins with different peptides, an enormous amount of data concerning proteinpeptide complex structures are needed.
To address this problem, we have created the Peptide Binding Protein Database (PepBind), which contains 3100 available protein Figure 1 Interactions between the complex of Apopain with the tetrap Hydrogen bonds (A), hydrophobic interaction (B) and ionic interact colored in brown.
structures from the PDB, irrespective of the structure determination methods and similarity in their protein backbone.
Different kinds of interactions have been noted in the stabilization of proteinpeptide binding.
Analyses of various interacting interfaces between linear peptide and protein domains help us in distinguishing transient and permanent complexes [20 22].
It has been demonstrated that protein-peptide interfaces contain more hydrogen bonds per 100 A2 solvent accessible surface area (ASA) (i.e., 50% more than proteinprotein interactions and 100% more than intrinsically-unstructured regions to protein interactions) [17].
The importance of other interactions such as interactions between nonpolar hydrophobic amino acid residues and ionic interactions in the structure and function of proteins is also well known [23,24].
Knowing the importance of proteinpeptide interface hydrogen bonds and other kinds of interactions, we developed and integrated a web-based interaction tool, protein inter-chain interaction (PICI), which calculates all the interface hydrogen bonds along with other interactions (such as disulfide bonds, hydrophobic interactions and ionic interactions) in tertiary structures of proteinpeptide complexes and can be visualized with an integrated Jmol [25] viewer.
Although a similar tool, Protein Interaction Calculator (PIC) [26], has been available, this tool calculates interface interactions specific for the peptide chain of a proteinpeptide complex structure and visualizes them in a single web page along with highlighted interacting residues on sequences.We have also developed a binding prediction server built in PepBind (http://pepbind.bicpu.edu.in/PepBind_prediction_beta.php) to predict the possible protein domains in the PepBind database that may bind the user-defined peptide sequence.
eptide inhibitor ACE-DVA-ASK (PDB ID: 1CP3) ions (C) were identified by PICI server.
Interacting residues are Das AA et al/ PepBind: Proteinpeptide Database with PICI Tool 243 Results The PepBind database provides researchers with residue and atomic-level information about sequences and structures of proteinpeptide complexes and their interfaces, helping in the analysis of proteinpeptide interactions by computing various interface interactions and by providing structural information both interactively on screen and in a text format (Figure 1).
The PepBind database also maintains a repository of structure coordinate files, PDBML [27] data files and proteinpeptide interaction files generated by PICI tool.
The database is updated on a regular basis to serve as a resource for structural, functional and proteinpeptide interaction studies of peptidebinding proteins.
Researchers can also submit proteinpeptide complexes to the database, which will be uploaded to PepBind after manual verification.
Database statistics As shown in Table 1, current version of PepBind contains structural information for a total of 3100 proteinpeptide complexes.
Based on cellular activity, 1745 complexes of all the 3100 proteins (56.3%) are involved in regulatory pathways, along with inhibitory complexes.
Our study shows 1278 structures (41.2%) in the database play major roles in hormonal activity, gene regulation, transcription and signal transduction pathways along with transferases.
Furthermore, 600 structures (19.3%) in the database are found to function in the immune system.
It has been found that 252 proteins (8.1%) are structural, contractile and membrane proteins involved mainly in transport (5.2%) and cell adhesion (1.9%).
In addition, 953 (30.7%) structures have protease or other hydrolase activities, Table 1 Contents of the PepBind database Cellular activity No.
of complexes (%) Cell cycle 90 (2.9) Structural proteins 126 (4.0) Cell adhesion 59 (1.9) Transporta 163 (5.2) Calmodulin (CaM) 42 (1.3) Apoptosis 125 (4.0) Signaling 626 (20.2) Hormones 84 (2.7) Transferasesb 415 (12.7) Transcription 268 (8.6) Gene regulation 38 (1.2) Inhibitory complex 663 (21.4) MHC 340 (10.9) Immunoglobulin (Ig) 250 (8.0) Antibiotics 15 (0.5) Other immune system proteins 98 (3.1) Proteases 687 (22.1) Other hydrolases 266 (8.5) Others 326 (10.5) Note: There are totally 3100 proteinpeptide complexes in PepBind.
Since categories.
a Transporters, channels and pumps; b Transferases along with while 10.5% structures in the database are associated with proteins involved in other cellular activities.
Web interface The user interface has been developed for browsing through all the contents of the database as a list or by different categories (Figure 2).
For the ease of users to search and access data, we have integrated many search tools (Figure 2A) into the web interface.
Using the simple search function, users can retrieve information about proteinpeptide complexes using their PDB ID or protein name.
Our keyword search tool scans all the fields of all the tables in PepBind for the matched word and returns a list of all protein structures related to the query.
Using the advanced search function, users can filter search based on peptide length, cellular activity of proteins, structure determination methods (e.g., X-ray diffraction, nuclear magnetic resonance and electron microscopy) and authors contributing to solving protein structure.
All these search options with their parameters are joined by AND operator for an intensive search.
Additionally, to find any protein sequences homologous to the sequence submitted, we provide BLAST searching [28] against PepBind/PDB/SwissProt.
The web interface for the output result has been designed to show all the chains present in the protein structure (Figure 2B).
Each chain is linked to the PICI web tool for analyzing its interactions with other chains of the protein.
This tool shows the interaction details by highlighting the corresponding interacting residues in the displayed sequence along with the Jmol visualization tool for the identified interactions between the residues (Figure 2C).
Different tab viewers have been designed for various types of interactions.
The protein detail page shows information about protein complex on a single web page under Functional category No.
of complexes (%) Structural, contractile and membrane proteins 252 (8.1) Regulatory proteins 1278 (41.2) Inhibitory complexes 663 (21.4) Immune system 600 (19.3) Proteases and other hydrolases 953 (30.7) Others 326 (10.5) some proteins are multi-functional, there are overlaps among different kinase, phosphomutase, transaldolase and transketolase.
Figure 2 Snapshots of PepBind output A.
Search page with search parameters.
B.
Result summary page showing all the chains with their sequence.
C. Jmol showing protein peptide interface and sequence viewer showing protein chains with identified residues highlighted.
D. Detailed result page displaying summary of the protein and other tab options.
244 Genomics Proteomics Bioinformatics 11 (2013) 241246 different tabs (Figure 2D), such as summary, sequence and source, gene ontology, methodology, Ramachandran plot, citation and external links.
While the sequence and source tab displays amino acid sequence in different colors as per their biochemical properties along with source organism data, the Ramachandran plot tab shows the Ramachandran plot image developed by the MolProbity [29] server, and the Gene Ontology tab shows GO functional annotation [30].
For a structure similarity search, we take advantage of the web service of PDB, which employs the FATCAT algorithm [31] to recognize homologous domains available at PepBind, SCOP [32] and PDP [33].
Discussion Proteinpeptide interactions are the key components of cellular processes such as signal transduction, protein trafficking, defense mechanisms and enzyme regulation.
Various databases are available on protein interactions.
They can be grouped as protein-small molecule, protein-nucleic acid and protein protein interaction databases.
However, the retrieval of structural and functional information of proteinpeptide interactions in biological processes is tedious due to the lack of specific databases to provide such details.
The establishment of the PepX database has resolved the difficulty of unavailability of a proteinpeptide interaction database, whereby authors have classified the proteins based on backbone variations and binding interfaces.
While in PepX, grouping is based solely on 3D similarity, PepBind complements PepX by providing interface information for both the peptide and protein chains of the complexes along with their cellular functions and options for sequence and structure similarity searches.
PepBind is integrated with the Jmol viewer to visualize the interface residues along with the interaction files generated by the PICI tool.
Furthermore, PepBind provides BLAST search and structure similarity search for protein chains.
It also provides a prediction service for binding of user-given peptides to possible protein domains present in the PepBind database.
Links to other related databases and servers for the queried protein are provided for further analysis of the structures.
These resources include PDB [8], PDBsum [34], Pfam [35], Das AA et al/ PepBind: Proteinpeptide Database with PICI Tool 245 CASTp [36], OCA Browser (http://bip.weizmann.ac.il/oca/), PSI/KB (http://sbkb.org/kb/), SRS [37], MMDB [38], PQS [39], SCOP [32], CATH [40], Proteopedia [41], Jena Library [42] and UniProt [43].
Currently our interaction tool PICI is capable of analyzing inter-chain interactions like hydrogen bond, disulfide bridge, hydrophobic interaction and ionic interaction.
Keeping in view the importance of other weak interactions in stabilizing the protein structure, we plan to improve our tool to study interactions such as aromatic-aromatic interactions [44], cation-pi interactions [45] and aromatic-sulfur interactions [46].
In addition, the current interaction tool capabilities will be extended to user-submitted structures, allowing for examination of interfaces in complexes currently not present in the PepBind.
Methods Data collection and curation Files for atomic coordinate (pdb files version 3.30), sequences (fasta files) and other data (pdbml files version 4.0) of 3100 proteinpeptide complexes in the PDB were downloaded following a thorough manual screening of all the available structures in the PDB.
Because PepBind intends to be a comprehensive collection of proteinpeptide complexes from the PDB, the database contains all the available proteinpeptide complexes, irrespective of their sequence or structure redundancy.
Classification of all the collected structure data was done in three steps: (I) an automated program to scan the amino acid sequences and classify them based on length of the bound peptide, (II) manual curation for the cellular activity of the complexes through study of the literature and (III) an automated program to read the data file and group the complexes as per their structure determination methods.
Functionality has been analyzed through literature studies and classified as proteins involved in different cellular activities and grouped in 19 categories.
Database schema and implementation The PepBind database consists of a series of server-side scripts written in the PHP programming language with HTML and JavaScript for user interface functions, which runs on the Apache 2.2 web server, using MySQL 5.1 as a database back-end.
Atomic coordinate information from the PDB and other related information from other remote databases and web servers were mined through an automated program and stored in a file repository for further processing.
We developed sets of PHP scripts for operating with the available data and process them for easy integration in the database and frontend user interface.
The first set of scripts reads the PDBML files [27], extracts the data, and inserts them into the database tables; the second set sorts these data with respect to each attribute and the third set generates web pages with specific information about individual complexes.
Utilities and tools The PICI tool for depicting potential hydrogen bonds and other interactions between the short peptide and core protein was developed and integrated into PepBind.
This tool parses the structure coordinate files, removes the hetero atoms and water molecules, and predicts the interaction based on coordinate distance between atoms of amino acid residues of small peptide and the protein.
For structures determined by NMR, the first model in the file is taken for calculation by PICI tool.
For the two atoms A(x1, y1, z1) and B(x2, y2, z2), linear distance D is calculated as per the Euclidean distance equation D(A, B) = p {(x1 x2)2 + (y1 y2)2 + (z1 z2)2}.
Various potential interactions are calculated based on standard and published criteria.
The hydrogen bond is detected if the distance between oxygen or nitrogen atoms of the peptide and the protein domain is 63.5 A [47].
Interactions between hydrophobic residues (such as alanine, valine, leucine, isoleucine, methionine, phenylalanine, tryptophan, proline and tyrosine) [48] have been predicted if they fall within 5 A range.
Apart from these interactions, ionic residue (arginine, lysine, histidine, aspartic acid and glutamic acid) pairs falling within 6 A contribute to ionic interactions.
The tool with integrated Jmol viewer shows various interactions between the peptide and the amino acid residues of the interacting protein chains.
Moreover, it highlights the positions of interacting amino acid residues on the displayed sequence (Figure 2D).
This tool also generates an interaction file for each type of interactions.
A sequence modification tool has been developed and incorporated into the result page, which can read the protein sequence file and color the amino acid sequence (using single letter code) of protein according to their biochemical properties (such as green for non-polar hydrophobic amino acids, yellow for uncharged polar amino acids, blue for positively charged amino acids, red for negatively charged amino acids and black for non standard amino acids).
A web-based prediction server has been provided to find the protein domains present in the database that likely bind to the user-given peptide.
The sequence search tool present in the web interface allows users to BLAST search the queried sequence in the database using various parameters.
All data related to structure, sequence and interface interactions currently in the PepBind database have been made available for further analysis.
These files along with the complete list of the PepBind dataset can be downloaded freely from our database.
A reporting tool has been integrated to generate the result in a printer-friendly PDF file.
Authors contributions PPM, RK and MSK conceived and designed the project.
AAD collected the data, developed the database, developed the tools and designed the website.
OPS developed the BLAST search script.
AAD and OPS wrote the manuscript.
All authors read and approved the final manuscript.
Competing interests The authors have no competing interests to declare.
Acknowledgements This work is supported by the Department of Biotechnology (Grant No.
BT/BI/03/015/2002) and Department of 246 Genomics Proteomics Bioinformatics 11 (2013) 241246 Information Technology (Grant No.
DIT/R&D/15 (9)2007), Government of India.
ABSTRACT Motivation: We propose an efficient method to infer combinatorial association logic networks from multiple genome-wide measurements from the same sample.
We demonstrate our method on a genetical genomics dataset, in which we search for Boolean combinations of multiple genetic loci that associate with transcript levels.
Results: Our method provably finds the global solution and is very efficient with runtimes of up to four orders of magnitude faster than the exhaustive search.
This enables permutation procedures for determining accurate false positive rates and allows selection of the most parsimonious model.
When applied to transcript levels measured in myeloid cells from 24 genotyped recombinant inbred mouse strains, we discovered that nine gene clusters are putatively modulated by a logical combination of trait loci rather than a single locus.
A literature survey supports and further elucidates one of these findings.
Due to our approach, optimal solutions for multi-locus logic models and accurate estimates of the associated false discovery rates become feasible.
Our algorithm, therefore, offers a valuable alternative to approaches employing complex, albeit suboptimal optimization strategies to identify complex models.
Availability: The MATLAB code of the prototype implementation is available on: http://bioinformatics.tudelft.nl/ or http://bioinformatics.
nki.nl/ Contact: m.j.t.reinders@tudelft.nl; l.wessels@nki.nl 1 INTRODUCTION To explain complex biological phenomena it is of vital importance to measurein the same sampleall relevant (complementary) biological variables, and to measure these at a genome-wide scale.
For this reason, many multimodal screens have been performed that have complemented transcriptional profiling with, among others, copy number variation measurements, transcription factor binding assays, methylation status profiling or genotype calls (Bystrykh, 2005; Pollack et al., 2002; Shames et al., 2006; Visel et al., 2009).
A common aim in analyzing these multimodal datasets is to find associations between the biological variables measured to infer their regulatory role.
Consider, for instance, a study in which expression profiles and genome-wide genotype data were obtained in hematopoietic cells from a panel of fully homozygous recombinant inbred mouse strains (Fig.1A).
This genetical genomics approach To whom correspondence should be addressed.
enables the determination of expression quantitative trait loci (eQTLs) characterized by strong associations between the genotype and the observed expression levels (Jansen and Nap, 2001; Schadt et al., 2003).
In the absence of a strong direct association between the genotype and gene expression, real multi-locus interactions may still be present, due to epistatic interaction (Frankel and Schork, 1996; Michaelson et al., 2009).
Such interactions may not be detectable as (marginal) direct associations between the genotype and gene expression (Fig.1B).
To alleviate this, approaches which evaluate the joint association of multiple loci and a phenotype of interest are required.
Several approaches have been proposed to attack this problem.
These approaches differ mostly regarding the way the associations are modeled and the strategy employed to solve the combinatorial optimization problem.
Some approaches (Manichaikul et al., 2009; Wongseree et al., 2009) follow what could be loosely termed a two-stage approach, where all two-locus models are first evaluated, which, in stage two, are used in a greedy search to yield multilocus models.
Approaches employing more advanced strategies to traverse the space of possible models are represented by a genetic programming approach (Nunkesser et al., 2007) and Markov Chain Monte Carlo (MCMC) approaches associated with Bayesian analyses (Mukherjee et al., 2009; Zhang and Liu, 2007).
Since twostage approaches have been demonstrated to be suboptimal (Evans et al., 2006) and advanced search strategies such as MCMC are very sensitive to their implementation and parameter settings, and are not guaranteed to be optimal, an approach that finds a provably global solution to a selected model within reasonable time is highly desirable.
Of particular interest is the method of Ljungberg et al.(2004) which is used for the pair-scan analysis that is available on the GeneNetwork on http://genenetwork.org.
Ljungberg et al.(2004) stress the importance of performing a global search rather than relying on greedy searches by (pre)selecting markers based on their marginal effects.
To deal with the computational complexity associated with such an optimization problem, the authors present a method to find global optima of a linear regression problem for up to three predictors that is fast enough to be employed in permutation procedures.
In contrast to the class of additive models employed by Ljungberg et al.(2004) (and many other approaches), we follow others (Kooperberg and Ruczinski, 2004; Mukherjee et al., 2009; Nunkesser et al., 2007) and employ Boolean combinatorial logic to explicitly incorporate interactions in the eQTL inference.
To this end, we infer combinatorial association logic (CAL) networks that combine the observed genotypes through and (), or () and xor The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i150 i149i157 J.de Ridder et al.A B C Fig.1.
Schematic overview of data and association inference.
(A) A panel of BXD mice that is densely genotyped and expression profiled.
The genotype data can be considered as binary vectors by choosing a binary encoding of the alleles (in the figure D=true and B=false) and putting thresholds that divide the genome into loci such that each locus differs in at least one element from its neighbors.
The cartoon shows that good association is obtained between Locus 5 and Gene 7 because elevated expression is consistently observed in conjunction with the D allele of Locus 5.
(B) Interaction among genetic features may destroy direct associations between individual loci and genes.
The cartoon shows that configurations exist in which the gene expression can only be predicted by considering two loci simultaneously (using Boolean xor logic).
(C) By inferring CAL networks, interaction among genetic features is taken into account in the association inference.
Inferring CAL networks is achieved by selecting the input loci with the selection function S and combining these with the appropriate Boolean function B, such that the association (as measured by a scoring function f ) between the network output and the gene of interest is maximized.
() functions by searching for associations between the result of the Boolean operation and the gene expression.
The Boolean and function can be used if altered expression is consistently observed in combination with a particular combination of two alleles (which do not necessarily have to be equal), but remains unchanged in all other genotype configurations.
An example of a situation in which this may be observed is the case of two parallel pathways that only promote transcription of their downstream target when the genes in these pathways have specific alleles.
Conversely, we may also consistently observe differential transcription in the strains for which either one of two loci is of a certain genotype.
This may, for instance, be observed in case of a cascaded signaling pathway: a silencing mutation in one of the alleles can repress the entire pathway, regardless of which gene in the cascade contained this mutation.
Boolean or () and xor () are capable of capturing this behavior (Fig.1B).
Like the search for optimal predictors in the additive model, inferring optimal predictors of a Boolean function is a challenging computational problem, especially considering that more complex combinations of these functions are also possible.
Moreover, we noted that the objective function that needs to be optimized is highly discontinuous and nonlinear so that standard optimization techniques, such as genetic algorithms, simulated annealing and MCMC do not provide an optimal solution.
Nevertheless, an efficient andmost importantlyglobal solution is highly desirable, since this allows permutation procedures with which significance estimates of the discovered associations can be realized (Ljungberg et al., 2004).
In the following, we will mathematically prove that, under reasonable conditions, CAL network inference provides an efficient way to obtain globally optimal multi-locus models that associate multiple genomic loci with the expression of target genes.
We illustrate our approach on the genetical genomics dataset from Gerrits et al.2009, and using these data show that 100% accuracy is achieved at runtimes that are a fraction of those required for exhaustive search.
Furthermore, we observe that using this approach complex associations are revealed that otherwise would have gone unnoticed.
As such, our approach offers a useful alternative to the commonly used additive models and suboptimal search strategies.
2 METHODS 2.1 CAL network search The construction of a CAL network that predicts the expression profile from a set of binary predictors can be formulated as an optimization problem.
Interesting logic networks are those for which maximal association between the network output and the gene expression is obtained.
Let g be the (T 1) vector, with T the number of samples, containing the expression values of a gene and L the (T L) matrix of binary predictors, e.g.the genotypes, where L is the number of predictors.
A CAL network L is defined in terms of S(L;n) :BL BN , a selection function that selects N columns from L and B(I) :BN B, a Boolean logic function that specifies the network topology.
In the latter, (T N) matrix I is a concatenation of the columns selected by S, i.e.I= (in(1),...,in(N)), where n is a (N 1) vector containing the indices of the selected columns.
Consequently, CAL network L maps the genotype matrix L to a (T 1) output vector y as follows: y=L(L;B,n)=B(S (L;n)).
(1) The association between g and y is quantified with an association measure f (g,y) f (g,y)=  |x0x1| (n01)s20+(n11)s21 n0+n12 ( 1 n0 + 1n1 ) if (n0 >)(n1 >) 0 otherwise.
(2) For notational convenience, we used x0 ={g() :y()=0, (1,...,T )} and x1 ={g() :y()=1, (1,...,T )}, i.e.vector g is split into x0 and x1 according to the Boolean values in y.
Furthermore, x0 (x1), s20 (s 2 1) and n0 (n1) are defined as the sample mean, the sample variance and the number of elements in x0 (x1), respectively.
Note that Equation (2) is equal to the absolute value of the t-statistic, except when n0 or n1 becomes too small, which ensures high f-values are only obtained in Case x0 and x1 have at least elements.
The inference of CAL networks is a computationally challenging problem.
Primarily, because the feature selection problem, i.e.finding the optimal vector n, critically depends on the number of features that are considered.
i150 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i151 i149i157 Combinatorial association logic networks In the case of genetic markers, this easily runs in the several hundreds to thousands.
Moreover, the optimal subset of markers is heavily dependent on how these markers are combined, i.e.dependent on the optimal Boolean function B.All together, one frequently has to rely on greedy search strategies that easily get stuck in local optima or near exhaustive searches that are computationally too expensive, especially when employed in permutation procedures required to assess statistical significance.
Our solution to this problem hinges upon two observations.
First, in most practical datasets the sample size is relatively small, especially when compared to the number of features.
This means that we can limit ourselves to considering only small CAL networks with few inputs, since larger networks are prone to overfitting, which makes them less informative.
For this reason, and because most networks have many equivalent topologies that do not need to be evaluated due to symmetry, the set containing all unique and meaningful network topologies {Bj : j=1,2,} is relatively small (in the order of 10100, depending on the desired topology).
Consequently, the set of optimal input vectors {nj : j=1,2,}, associated with each Bj , can be found by fixing Bj and maximizing for each Bj separately nj =argmax n { f ( g,Bj(S(L;n)) )}.
(3) Second, we observe that Equation (3) still represents a complex optimization problem that can be significantly simplified by employing an approximation to the association measure, denoted by f. In the following, we show that maximizing f is equivalent to maximizing f , but the maximization of the former can be very efficiently realized by using a branch and bound search.
Before defining f , we define the Boolean vector yopt as the solution for which f reaches a global maximum independent of the network topology, i.e.yopt =argmaxy f (g,y).
Note that yopt can be easily determined by sorting the gene expression vector g and evaluating all positions for a threshold t that splits g into x0 and x1 (Fig.2A).
For f , we use the weighted Hamming similarity between yopt and the network output y f (yopt,y)= w()I(yopt()=y()) (4) A B C Fig.2.
Association versus approximated association.
(A) Example gene expression vector (circles) split in x0 and x1 according to yopt.
The magenta line denotes the association measure f , defined in Equation (2), as a function of a threshold t that splits the expression vector in x0 and x1.
The blue triangles indicate the error weights w() that result after optimizing them.
(B and C) 500 random samples that are generated by introducing up to seven bit-flips in yopt to show the relation between f and f. The red dot indicates f and f values for yopt.
(B) shows the samples in case the weights are assumed equal.
Although the trend of the data is monotonically increasing, a large spread around this trend is observed.
(C) shows the same samples in case the weights are optimized, resulting in a near one-to-one relation between f and f. where w()>0 denotes the weight for sample , and I() the indicator function, evaluating to 1 if the-th element of vectors yopt and y are equal.
For an example gene expression vector, Figure 2B shows 500 random samples of (f ,f ) pairs, in case all weights are equal to one.
Although the trend of this distribution is monotonically increasing, the spread around the trend is substantial.
This is undesirable because a maximum in f is only guaranteed to correspond to a maximum in f in case there is a direct one-toone relation between them.
Clearly, this is not the case in Figure 2B, since each value of f corresponds to many values of f. However, by optimizing the weights such that the difference between f and f is minimal, a near one-to-one relation can be obtained, as exemplified by Figure 2C.
With the proper adjustments, detailed below, it is thus ensured that maximizing f is equivalent to maximizing f. The major advantage of maximizing f instead of f is that in the former each sample has an independent contribution to the association measure.
This can be readily exploited using a branch and bound search, so that it is possible to avoid the expensive evaluation of the association measure.
2.2 Optimizing Equation (3) Here, we show that optimizing Equation (3) can be achieved by first determining f =maxn(f ), where f was defined in Equation (4).
After this the search for f =maxn f is readily solved by searching in the neighborhood of f. For a single sample , let V () be the set of input combinations such that y()=yopt()nV (), where y=L(L;B,n).1 Figure 3AC shows how V () can be inferred from L and the truth table of B.
For a set of samples C, the input combinations nV (C) for which all C reach the optimal output yopt are found by taking the intersection of all the individual sets of input combinations, i.e.V (C) =C V ().
Note that, under the assumption that each sample has at least one non-zero locus, V () 	=.
In other words, for individual samples there always exists a combination of inputs for which the network can reach the desired optimal output yopt.
However, for an arbitrary combination of samples this is clearly not the case.
If we observe that V (C) =, this means that for the collection of samples in C there does not exist a valid combination of inputs.
Moreover, if V (C) =, all supersets of C will also result in the empty set.
Finally we note that, by choosing a convenient binary encoding, V () and V (C) can be computed very efficiently by means of bitwise xnor and and operations, respectively (see Fig.3D and the Supplementary Fig.S1 for details).
With these definitions in mind, we propose the following lemma: Lemma 1. f =max C C w() subject to: V (C) 	= (5) Proof.
Let C =argmaxC C w(), i.e.C is the set of solutions for which f is obtained.
Since it is required that V (C) 	=, there must be at least one solution n such that yopt()=y() C. Since for C the optimum in f is obtained, it must also hold that yopt() 	=y()/C.
This means that Equation (4) can be rewritten as follows:  w()I(yopt()=y())= C w(), proving the statement in this lemma.
As argued by Lemma 1, Equation 4 is thus maximized by having as many samples in C as possible, while taking into account their respective weights w().
Before we will show that Equation (5) fits a branch and bound framework, we first make the observation that for the relation between f and f the following holds: (f (yopt,y1)< f (yopt,y2)) (f (g,y1)< f (g,y2)), (6) 1Since we optimize Equation 3 for each Bj separately, we omit its subscript if its meaning is inconsequential.
i151 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i152 i149i157 J.de Ridder et al.A B C D Fig.3.
Computation of solution sets for each sample.
(A) Example data from Figure 1A.
(B) The topology and the truth table of the Boolean function B under investigation.
(C) Explanation by example of the calculation of V (), the set of all possible input combinations to B such that yopt()=y().
This panel shows how V (1) is determined.
Since yopt(1)=1, the rows from the truth table for which y=1 are applicable, i.e, r ={2,4,6,7}.
According to r =2, the desired output for =1 is obtained by selecting any of the loci that are 0 for inputs i1 and i2, and loci that are 1 for input i3.
Accordingly, for i1 we may select from the set: {l1,l2,l4}.
This can be efficiently calculated by taking the xnor (evaluates to 1 when both inputs are equal) between row =1 from the data matrix and the row r =2 from the truth table, as shown in (C).
Observe that the result is an efficient encoding of all the possible input combinations that satisfy yopt(1) while using r =2 from the truth table.
In general, we denote this set by V ()r , and its binary encoding by V ()r. To determine the complete set of valid input combinations for =1, rows 4, 6 and 7 need to be considered in a similar fashion.
V (1) is now determined by taking the union of the subsets, i.e.V (1) =V (1)2 V (1)4 V (1)6 V (1)7 , which, in binary form, may be represented by a concatenation of V (1)2 , V (1)4 , V (1)6 and V (1)7.
(D) This panel shows the valid input combinations for =1 and =3 in binary representation (i.e.V (1) and V (3)).
For any set of samples C the input combinations for which the output equals yopt can be obtained by taking the intersection of the individual sets.
In binary representation, this is equivalent to taking the row-wise cartesian product (row-wise product of all combinations of rows), as is shown in the panel.
where y1 and y2 are two Boolean vectors.
Note that, for =0, Equation (6) reduces to the requirement for strict monotonicity, and that for larger >0 this requirement is increasingly relaxed.
Even though this seems trivial, the value of this relation becomes clear by considering that if there exists a strong positive correlation between f and f , there may in fact exist a small for which Equation (6) is true.
Based on Lemma 1 and Equation (6), we observe that solutions that are suboptimal in terms of f may still be optimal in terms of f , since can be non-zero.
In the following, let {yi : i=1,2,} and {Ci : i=1,2,} be all the network outputs and the sample sets for the solutions for which holds that f  f (yopt,yi) f *, respectively.
Finally, let be chosen such that Equation (6) holds.
Our main theorem can now be formulated as follows: Theorem 2. n  Ci V (Ci) (7) Proof.
First, assume that Equation (6) holds for =0, and thus f (yopt,yi)= f i.
Furthermore, from Equation (6) it follows that in this case there exists a direct one-to-one relation between f and f. Consequently, a maximum in f is guaranteed to correspond to a maximum in f and V (Ci) must contain n. This is true because from Lemma 1 it follows that V (Ci) 	=.
For non-zero values of , the one-to-one relation does not hold.
However, from Equation (6), it follows that all values of f for which the corresponding f lies outside the interval [f , f ] are strictly smaller than the value of f corresponding to f. Thus, it must be the case that the maximum of f is constrained to solutions for which f lies in the interval [f , f ].
Therefore, the union of the sets of solutions that lie in this interval will contain n. From Theorem 2 it naturally follows that: Corollary 3. n =argmax n f (g,L(L,B,n))nV (Q), (8) where V (Q) =Ci V (Ci).
Notably, if there exists a small for which Equation (6) holds, the number of solutions in V (Q) is limited, and hence n is easily determined by an exhaustive search over all possible solutions in V (Q).
In the following, we show that in practice the set V (Q) is small by choosing w such that is small.
2.2.1 Estimating the weights Ideally, vector w is chosen such that is minimal.
For practical purposes, it is sufficient to choose w so that is small, which can be realized by minimizing the difference between f and f. For this purpose, we sample the (f ,f ) relation by generating N random instances yn by introducing up to m random bit-flips in yopt (shown in Fig.2B and C).
The N corresponding association measures fn and Hamming similarities are collected in vector f =[f (g,y1),f (g,y2),]T and matrix F=[(yopty1)T ,(yopt y2)T ,]T , respectively.
In the latter, denotes the xnor operation, which evaluates to 1 in case its arguments are equal.
Notably, m (the number of bit-flips) should be chosen such that the region of interest of the distribution of f is sampled.
Since we are interested only in network outputs that associate well with the gene expression, we can choose m rather small to focus only on the right tail for which a good fit between f and f is obtained.
We found that smaller residuals were obtained by converting log-transformed f-values to z-scores, i.e.f =z(lnf).
Furthermore, to deal with the intercept, the matrix F is mean centered, denoted by F. Using the vector f and matrix F we can find the weights w by constraint linear least squares minimization w=argmin w ||f Fw||2, subject to: w()w (9) where w > 0 is a small scalar that ensures each sample receives a non-zero weight.
Figure 2 illustrates a typical example showing that the trend of the relation is monotonically increasing, and the spread around the trend is marginal, indicating that Equation (6) indeed holds for a small.
2.2.2 Estimating The parameter can be estimated by randomly resampling the (f ,f ) relation using the obtained weights and measuring the spread around the trend in the data in the f direction (Fig.2C illustrates this schematically).
To this end, lowess smoothing was performed to obtain the the trend in the data (Cleveland, 1979).
Subsequently, the spread around i152 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i153 i149i157 Combinatorial association logic networks this trend was obtained by applying a sliding window in the f direction and defining as the maximum spread across all window positions.
2.2.3 Branch and bound search tree Equation (5) naturally fits a branch and bound framework with a backtracking search tree, in which each node corresponds to a particular set of samples C (shown in Supplementary Fig.S2).
Although this tree exhaustively represents all possible sample sets C, the search is very efficient since most nodes can be pruned from the search tree.
First of all, if V (C) becomes equal to the empty set, all child nodes of node C can be discarded because these will also result in the empty set.
Second, as a result of the search tree topology, for each node C we can define an upper bound f (C)up and lower bound f (C) low.
The upper bound f (C) up is defined as the value of f that would be obtained assuming all its subnodes do not result in the empty set (best case scenario) f (C)up = C w()+ Csub w(), (10) where Csub denotes the collection of all samples in the subnodes of C. The lower bound f (C)low is defined as the value of f that would be obtained assuming all subnodes will result in the empty set (worst-case scenario) f (C)low = C w().
(11) A vast reduction of the search space is realized by considering the following branch and bound principle: any node C can be pruned if there exists a node C , for which the following is true: f (C)up < f (C) low , under the condition: V (C) 	= (12) Thus, if we encountered a branch whose worst-case error is better than the best-case error of another branch, we can safely discard the latter.
After the complete search tree is traversed, the set V (Q) is determined by the union of all the nodes that resulted in a non-empty V (C).
In Equation (12), the parameter is included to ensure that set V (Q) includes n (Theorem 2).
An optimal leaf ordering is obtained when the samples are sorted based on their weight w().
This ensures that f (C)up decreases as quickly as possible, in effect pruning the tree early in the search.
Also, note that most V (C) will contain many duplicates when symmetries in the topology of B are considered.
By filtering these from V (C) before evaluating the succeeding node results in an additional search speedup.
2.2.4 Tolerance level A final, yet influential, search-space reduction is achieved by only considering solutions for which a certain minimum level of association is achieved.
This is realized by enforcing that f low can never be below a user defined tolerance level.
In other words, for this bounded f low, we can write: f low =max(f tol, f low).
As a result, branches for which f low f tol can be pruned even before the search is started.
The search procedure is explained by example in Supplementary Figure S2.
2.2.5 Estimating the false discovery rate Because our primary interest lies with the interpretation of the selected genotype markers and combinatorial logic, it is of critical importance to assess frequency of false positives among the networks called significant.
Due to the efficiency of the proposed method, it is possible to employ a permutation procedure to obtain a nulldistribution for each Bj.
From this distribution, it is possible to estimate the false discovery rate (FDR) and the associated q-values by using the method proposed in Storey and Tibshirani (2003).
Not surprisingly, in many cases, multiple network topologies yield significant associations with the same gene.
The q-values, available for each of the solutions, provide a convenient way of performing selection of the most parsimonious model by accepting only the topology for which the q-value is minimal.
3 RESULTS 3.1 Genetical genomics dataset The genetical genomics dataset used to demonstrate our method contains genome-wide RNA transcript measurements performed on four related hematopoietic cell populations (Gerrits et al., 2009).
These were isolated from the bone marrow of 25 BXD recombinant inbred mouse strains that were derived by crossing C57BL/6J (B6) and DBA/2J (D2) (Peirce et al., 2004).
A typical analysis of these data includes determining eQTLs, i.e.regions in the genome for which the genotype across strains associates well with RNA transcript levels.
We inferred associations only for the myeloid cell population, as for this cell type data for the largest number (T =24) of unique BXD strains were available.
The expression data were preprocessed as described in the Supplementary Methods.
Because the CAL networks inferred for highly correlated genes are equivalent, rather than starting the optimization for each gene separately, we constructed gene clusters and searched for CAL networks for the centroids of each gene cluster.
To ensure only tightly correlated probes were clustered, we employed a stringent cutoff (correlation distance cutoff 0.2).
This resulted in 6139 clusters that were used to determine eQTLs.
Genotype information for the strains was retrieved from The GeneNetwork (http://www.genenetwork.org/dbdoc/BXDGeno.html).
Genotype markers that were highly similar across strains and on the same chromosome were also grouped into clusters to prevent the algorithm from finding many combinations of genotype markers that are equivalent (such as the markers in linkage disequilibrium).
This resulted in 453 marker clusters (L=453).
The cluster centroids were defined as the majority vote of the individual markers in the cluster and were used as putative inputs to the network (see also the Supplementary Methods and Supplementary Figs.
S3 and S4).
For setting the tolerance level ftol no straightforward method exists.
Preferably, the tolerance level is set close to the final significance threshold to minimize the effort spent on finding optima for gene clusters that can never be significant.
We settled for a tolerance level equal to the 75th percentile of the f opt distribution (ftol =7.6), obtained by computing the f-values associated with each yopt.
Gene clusters for which the maximum f-score is below this tolerance level (i.e.in case f opt < ftol) were not included in the CAL network search, to result in a set of 1525 high-potential gene clusters.
3.2 Algorithm performance From the methods section it follows that, under the condition that an appropriate value for is found, our algorithm produces an optimal solution.
We empirically validate this claim by comparing solutions of the proposed algorithm with the global optimum obtained with an exhaustive search.
To ensure realistic conditions, we do this using the real data described above.
For each gene expression vector, we performed our CAL network search as described with seven network topologies containing and, or and xor logic as well as a more complex combination of these Boolean functions.
A rather low tolerance level (ftol =4) was used, which turned out to capture most of the solution-space (>80% for all topologies).
The solutions obtained were compared with the optimal solutions determined by means of an exhaustive search for the same seven Boolean logic functions using Grid computing i153 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i154 i149i157 J.de Ridder et al.A B C D Fig.4.
Algorithm performance in terms of accuracy and runtime under various conditions.
(A) Bargraph displaying accuracy for different network topologies and different values of the f-score.
For each of the network topologies the 75th percentile of the solution distribution is also given, showing that for solutions in the tail 100% accuracy is obtained.
For the two missing bars in the 4-6 and 5-6 bins no solutions were found.
(B) and C) Runtimes for different network topologies and dataset sizes.
The horizontal lines reflect runtimes for exhaustive search.
From bottom to top these represent the runtimes for: a single input network, two input network and three input network with one, two and four times the number of predictors, respectively.
facilities.
The accuracy is expressed as the percentage of times that the algorithm finds the same solution as the exhaustive search.
Figure 4A shows the resulting accuracy.
We observe that for solutions with f-scores between 5 and 6 already >95% accuracy is achieved, while for solutions with f-scores of 6, virtually 100% accuracy is achieved for each topology.
For comparison, Figure 4A also gives the 75th percentiles of the solution distributions for each topology.
Because solutions of interest (putatively significant solutions) are required to have f-scores substantially higher than the 75th percentile, we can conclude that our method achieves 100% accuracy for a reasonable operating range (solutions with f-scores between 4 and 5where the accuracy is below 95%are well the 75th percentile for all networks).
While comparing our method to the method presented in Mukherjee et al.(2009), using simulated gene expression vectors and a predetermined random network (ground truth), we found that our method reaches higher true positive rates (see Supplementary Material).
These results illustrate the benefit of searching for solutions for each of the network topologies separately, and employing a significance estimate to enforce parsimony.
Obtaining the same accuracy as an exhaustive search is only useful if this is achieved for runtimes that are substantially lower.
To asses this, we randomly selected 200 gene expression vectors from the 1525 gene clusters and measured runtimes for both our CAL network search as well as the exhaustive search.
Figure 4BD shows these runtimes for a range of conditions.
The boxplots represent the results obtained with the CAL network search and the horizontal lines the runtimes for the exhaustive search.
Figure 4B compares runtimes for different network topologies.
Clearly, the branch and bound algorithm significantly outperforms the exhaustive search under all experimental conditions with differences in runtime of up to four orders of magnitude.
For the three input networks in particular, the runtime required for exhaustive search (>5 h per gene per network) prohibits any further permutation procedures.
The CAL network search, on the other hand, is able to find the solution in a matter of seconds, thereby enabling the large number of permutations required to obtain reliable significance estimates.
Compared to the variance in runtime of the exhaustive search, which was negligible, the variance of the CAL network search is quite high.
This is expected as our CAL network search finishes rapidly when a good solution presents itself early in the search, while more time is needed to conclude that no acceptable solution is present.
For a similar reason, the more complex networks, those containing xor logic, have higher median runtimes.
On no occasion, however, does this increase runtimes >100 s for any of the networks.
To evaluate performance as a function for dataset size we artificially increased the number of predictors and the number of samples (Fig.4C).
In addition, runtimes for different tolerance levels were examined (Fig.4D).
The number of predictors was increased by horizontally concatenating the original matrix L with copies of L containing 10% random bit-flips.
The sample size was increased by vertically concatenating matrix L as well as all gene expression vectors g with copies of L and g, respectively.
In case of the latter, normally distributed noise was added to the copies with noise =0.1g.
We observe that for both the exhaustive search as well as the CAL network search runtimes increase substantially as the number of predictors increase.
In case of the CAL network search, this is explained by the fact that many very good solutions are present due to the increased imbalance between the number of predictors and the sample size.
It is expected, yet not quantitatively established, that better performance is observed when this balance is restored.
The increase in runtime as a result of an increased number of samples is moderate, with a median runtime considerably lower than an exhaustive search for only two input networks.
Likewise, increasing the tolerance level only moderately speeds up the CAL network search, demonstrating that runtime is robust for the setting of this parameter.
3.3 Combinatorial eQTLs We performed the CAL network search for the set of 1525 highpotential gene clusters.
The complete search (e.g.for all gene clusters and all topologies) was repeated 100 times using a permuted version of the gene-expression vectors.
For each topology, this resulted in a null-distribution containing 152 500 values, which was used to estimate q-values for each of the resulting solutions.
We considered network topologies with a maximum of three inputs listed in Supplementary Figure S5.
Notably, we included two single-input networks to account for direct positive and negative association, respectively, which is equivalent to positive association with the i154 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i155 i149i157 Combinatorial association logic networks A B C Fig.5.
(A) Bargraph with an overview of the number of gene clusters for which a significant (10% FDR) solution is found.
Network topologies are sorted according to the 10% FDR level (blue line).
(B) CAL networks significant at 10% FDR.
The color and shape of the symbols correspond to the symbols used in (C).
Small circles at the inputs of the networks denote negation, i.e.for these inputs the mapping from allele to binary representation is switched.
We also indicate whether the best single marker coincides, for that gene cluster, with one of the inputs of the CAL network.
(C) Marker/probe-plot for the top CAL networks showing both the eQTLs (blue crosses) and ceQTLs (sets of colored symbols of various shapes).
The colors and shapes of the markers refer to the network topologies listed in (B).
Horizontal gray lines connect the inputs and the output of the CAL network.
Because probes were clustered, it occurs that the ceQTLs map to multiple probes in case these probes were part of the same cluster.
The numeric labels near the the colored symbols correspond to the input of the network.
Notably, some probes seem to be predicted by more ceQTLs than there are inputs to the CAL network reported.
This occurs when there are multiple combinations of markers that show the same association with the gene expression level of the network output, and can be explained by similarity among markers.
The cis-band (diagonal) is clearly visible, and in one occasion contains a ceQTL.
Overlap among ceQTLs from different networks is marked by red dashed lines, overlap between ceQTLs and eQTLs by black dashed lines.
D2 and B6 allele, respectively.
This ensures that the algorithm has the option of choosing the least complex model in case an eQTL is capable of explaining a significant portion of the variance in the expression of the gene cluster.
Figure 5A gives an overview of the number of gene clusters for which the output of a CAL network significantly (at the 10% FDR level) associated with its expression (red bars).
To obtain additional confidence in the significance threshold, we calculated q-values for 10 additional permutations of the whole dataset.
For none of the network topologies did the mean number of significant gene clusters across the 10 permutations exceed 0.6, indicating that the expected number of false discoveries is conservatively kept under control.
The yellow bars indicate the number of significant gene clusters after model size selection based on the q-value as detailed in Section 2.
It appears that most of the gene clusters for which association is observed can be explained by one of the single input networks.
For nine gene clusters (corresponding to 17 genes), however, a CAL network was capable of explaining significantly more of the variance than one of the single input networks or any one of the other CAL networks.
The network topologies, q-values and association scores of the significant CAL networks are given in Figure 5B.
Not surprisingly, for all gene clusters at the output of these networks, the combination of loci is vastly superior in explaining the variance in expression over any of the markers in isolation.
Interestingly, many of these genomic regions would have been missed, as in seven of the networks the best markers do not coincide with one of the inputs of the CAL network.
The sets of markers that were found as the optimal inputs for the seven topologies were mapped onto the genome.
Combinatorial eQTLs (ceQTL) were then defined as stretches of consecutive markers.
A genome map of the (c)eQTLs is given in Figure 5C, showing the eQTLs (red and blue crosses for positive and negative association, respectively) and ceQTLs (colored symbols) on the x-axis versus the genomic positions of the probes measuring expression on the y-axis.
The numbers near the ceQTL symbols correspond to the inputs of the CAL networks depicted in Figure 5B.
Before we zoom in on one of the CAL networks in more detail, some general observations can be made.
In particular, we note that in some cases overlap exists among the markers selected at the i155 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i156 i149i157 J.de Ridder et al.Fig.6.
Input regions of the CAL network for Lilrb4 The line graphs give the f-score for association between the output gene and the individual markers (blue) and the network output (red).
The latter was computed by taking the maximum f-score of the network using the marker under evaluation for one input and any of the other markers for the second input of the network.
Where possible the IDs of the genetic markers are given, but some were omitted for readability.
The dot plots gives the expression values separated by network output (right) and the best markers in the inputs (left).
Finally, for one particular combination of markers the genotype for all strains is depicted as a Boolean heat map.
In these diagrams, the not gates were already incorporated.
inputs of the CAL networks and between other network inputs and eQTLs.
In seven instances, the identified ceQTLs coincide with eQTLs (connected by black dashed lines in the figure).
Some of these eQTLs are located in cis.
The finding of CAL networks that share one of their inputs (ceQTLs) with an eQTL suggests that the local genotype associated with the eQTL is involved in the regulation of a local gene (cis-regulation), but in addition collaborates with the other CAL input locus/loci to regulate the CAL network output gene(s).
Furthermore, two of the CAL networks (ranked sixth and ninth) share a ceQTL between the inputs (connected by red dashed lines).
It is not inconceivable that a gene present in this ceQTL is indeed involved in the regulation of the target genes of both networks, but that the interaction partners through which this regulation is established differs for both target genes.
Among the list of output genes of the nine most significant CAL networks is Lilrb4 (ranked third).
Lilrb4 encodes a leukocyte immunoglobulin-like receptor which is expressed on the surface of mast cells, neutrophils, and macrophages.
It plays a key role in counter-regulating the inflammatory response to prevent pathologic excessive inflammation (reviewed in Katz, 2007).
Figure 6 shows small regions around the ceQTLs that were selected as inputs for the CAL network of Lilrb4.
For each region, the association was measured between the expression of Lilrb4 and the individual markers (blue lines).
The red lines, on the other hand, give the association score for the network output.
Clearly, the association between the logical combination of inputs and the expression of Lilrb4 is markedly higher than considering any of the markers in isolation.
The regions for which the red curves reach their maximum correspond to the ceQTLs.
The Boolean heat map, displayed at the bottom of Figure 6, outlines the genotype of one particular combination of genetic markers in the ceQTLs across the BXD mouse strains.
The bottom two rows of this heat map give the optimal network output and predicted output, respectively.
For the Lilrb4 network the optimal network output is exactly recapitulated by the CAL network.
For Lilrb4 elevated expression is exclusively observed in case of B6 alleles in both the ceQTL regions of Chromosomes 7 and 19.
To focus our attention to the most interesting genes in the ceQTLs we performed a literature search using Ingenuity pathway analysis (IngenuitySystems, www.ingenuity.com).
Interestingly, we found a substantial number of interactions between genes localized in the ceQTLs and Lilrb4.
For example, the literature search revealed a link between Apba1 (located in the ceQTL region on Chromosome 19) and Lilrb4.
Both protein products have been described to bind ITGB3 (Calderwood et al., 2003; Castells et al., 2001).
In addition, the search revealed a link between Psenen (Chromosome 7 ceQTL) and Apba1 (Chromosome 19 ceQTL).
Both protein products have been described to bind PSEN1 and PSEN2 (Biederer et al., 2002; Steiner et al., 2002).
While literature is able to link the genes in the ceQTLs to Lilrb4 and thereby gives the first clues as to how the expression of Lilrb4 may be regulated, we do not exclude that other interactions (not yet represented in literature) exist.
In any case, the result of our method should provide a set of testable hypotheses that can be validated in the laboratory.
4 DISCUSSION Unravelling (transcriptional) regulatory networks by inferring complex associations, for instance, between genotype and gene expression, necessitates algorithms that take into account possible (allele-specific) interactions.
For this purpose, we have proposed a method to efficiently infer CAL networks, i.e.small logic networks in which allele-specific interactions are modeled by Boolean functions.
To find the best possible fit of the model given the data, a computationally challenging optimization problem had to be solved.
i156 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i157 i149i157 Combinatorial association logic networks This was achieved by rewriting the optimization such that it could be effectively solved by a customized branch and bound algorithm.
Proof and empirical evidence for optimality of the solution, under appropriate conditions, was given.
At the same time, differences in runtimes of up to four orders of magnitude were observed when compared to exhaustive search.
Because the CAL network search is able to find the optimal solution in a matter of seconds a permutation procedure becomes feasible, which can be employed to obtain estimates of the FDR.
This is a major advantage as the resulting q-values allow selection of the most parsimonious model and enable ranking the network topologies in terms of their complexity.
We demonstrated our algorithm on a genetical genomics dataset, and found that, from the 1525 gene clusters (2913 genes) that resulted after selection of high potential genes, 9 gene clusters (17 genes) were significantly associated (at 10% FDR level) through a logical combination of genomic loci rather than a single eQTL.
Notably, without incorporating the complex interactions, these associations would have gone unnoticed.
Many of the discovered input regions were found to overlap eQTLs or were shared inputs of CAL networks explaining the expression of other genes, suggesting that these regions, indeed, are involved in transcriptional regulation.
ACKNOWLEDGMENTS The authors thank Daoud Sie and Leonid V. Bystrykh for their valuable input.
Funding: This work was part of the BioRange programme of the Netherlands Bioinformatics Centre (NBIC), which is supported by a BSIK grant through the Netherlands Genomics Initiative (NGI); Dutch Life Science Grid initiative of NBIC and the Dutch eScience Grid BiG Grid, SARA-High Performance Computing and Visualisation; Netherlands Genomics Initiative (Horizon, 05071-055); Dutch Cancer Society (RUG2007-3729); Netherlands Organization for Scientific Research (NWO) (VICI to G.d.H., 918-76-601); European Community (EuroSystem, 200720).
Conflict of Interest: none declared.
ABSTRACT Motivation: Transcription factors (TFs) are proteins that regulate gene activity by binding to specific sites on the DNA.
Understanding the way these molecules locate their target site is of great importance in understanding gene regulation.
We developed a comprehensive computational model of this process and estimated the model parameters in (N.R.Zabet and B.Adryan, submitted for publication).
Results: GRiP (gene regulation in prokaryotes) is a highly versatile implementation of this model and simulates the search process in a computationally efficient way.
This program aims to provide researchers in the field with a flexible and highly customizable simulation framework.
Its features include representation of DNA sequence, TFs and the interaction between TFs and the DNA (facilitated diffusion mechanism), or between various TFs (cooperative behaviour).
The software will record both information on the dynamics associated with the search process (locations of molecules) and also steady-state results (affinity landscape, occupancy-bias and collision hotspots).
Availability: http://logic.sysbiol.cam.ac.uk/grip Contact: n.r.zabet@gen.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on November 14, 2011; revised on February 2, 2012; accepted on March 12, 2012 1 INTRODUCTION It is well established now that transcription factor (TF) find their target site through facilitated diffusion, a combination between 1D random walk on the DNA and 3D diffusion in the cytoplasm (Berg et al., 1981; Elf et al., 2007).
Once bound to the DNA, TFs perform three main types of movements: (i) sliding , (ii) hopping and (iii) jumping (Mirny et al., 2009).
The first two mechanisms, sliding and hopping, assume that the TF performs small movements on the DNA without releasing into the cytoplasm, whereas the third assumes a 3D diffusion in the cytoplasm before rebinding.
With few exceptions, most of the theoretical efforts have been invested into analytical solutions of the facilitated diffusion mechanism.
If one wants to consider real DNA sequences and dynamic crowding on the DNA (mobile roadblocks), then this rules out analytical solutions.
Computational methods and, in particular, stochastic simulations overcome these limitations and To whom correspondence should be addressed.
provide a more accurate mechanistic representation of the underling biological process.
In particular, these type of stochastic simulations can be used to answer question related to how TFs perform the search process.
For example, one could investigate whether molecules prefer to hop or to slide and what is the contribution of these two alternative movements on the DNA to the overall 1D random walk in a crowded environment.
Building on the comprehensive model constructed in (N.R.Zabet and B.Adryan, submitted for publication), we developed GRiP (gene regulation in prokaryotes), a program that allows stochastic simulation of the search process of TFs for their target sites on the DNA.
The analyzed systems can be large.
For example, Escherichia.coli K-12 has a 4.6 Mbp genome and there are 104 DNA binding proteins (agents).
To produce results within relative short time, previous software had to either rely on coarse grain models (Wunderlich and Mirny, 2008) or to consider small subsystems (Chu et al., 2009).
GRiP represents a new and efficient implementation of the TF search process, which considers a highly detailed model of 1D diffusion and, at the same time, it simulates at least 4 times faster than previous software (Barnes and Chu, 2010; Chu et al., 2009).
Consequently, by allowing genome-wide stochastic simulations of a highly detailed model of facilitated diffusion, GRiP can highlight possible biases in the results, where the level of details was insufficient (coarse grain models) or the size of the analyzed system was too small.
A few studies, such as Das and Kolomeisky (2010), addressed the problem of facilitated diffusion through simulations focusing on the 3D diffusion rather than the 1D case.
The 3D diffusion is time and resource consuming, especially for simulations at the genome level.
van Zon et al.(2006) showed that the model based on the zerodimensional Chemical Master Equation can reliably represent the rate at which TFs associate non-specifically with the DNA, as long as the model takes into account that once a molecule unbinds from the DNA, it has a high probability of fast rebinding in close proximity.
This suggests that there is no need to simulate the 3D diffusion explicitly, but rather have this replaced by a simple arrival rate and ensuring that the model incorporates the fast rebinding probability in the unbinding rate, a strategy which we also adopt.
2 DESCRIPTION We implemented the target finding process as a hybrid model mixing agent-based methods with event driven stochastic simulation algorithms (Gillespie, 1977).
The software is implemented in Java 1.6, which ensures high portability.
The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: APPLICATIONS NOTE [12:59 9/4/2012 Bioinformatics-bts132.tex] Page: 1288 12871289 N.R.Zabet and B.Adryan In the simulator, each TF molecule is represented as an agent able to perform certain actions, whereas the DNA molecule is modelled as a string of base pairs (A, T, C, G).
There is no measure of distance between the molecules, but the TF molecules can be either free in the cytoplasm or bound on the DNA at certain positions.
The free TF molecules have only one action available, namely to bind to the DNA.
The cytoplasm is assumed to be a perfectly mixed reservoir from where the free TF molecules can find the DNA at exponentially distributed times.
To simulate the 3D diffusion we use the Direct Method implementation of Gillespie Algorithm (Gillespie, 1977) which generates a statistically correct trajectory of the Master Equation.
The model considers volume exclusion, allowing only one TF to cover certain base pair at any specific time point.
A bound molecule will occupy a number of consecutive base pairs on the DNA.
The size on the DNA of each TF molecule is computed as the number of base pairs of the DNA binding motif added to the number of obstructed base pairs on the left side of the molecule and the number of obstructed base pairs on the right side.
A feature which was not considered by previous models (Barnes and Chu, 2010; Chu et al., 2009) is TF orientation on the DNA.
If TFs are not symmetric, the user can set TF molecules to have two orientations on the DNA, which can lead to different affinities depending on the molecule orientation.
Whenever a TF binds to the DNA, the system selects a random orientation.
This can be changed only after the TF molecule unbinds and rebinds to the DNA, including during hops.
The simulator supports the definition of multiple TF species, which are classified in two types: (i) non-cognate TFs and (ii) cognate TFs.
The cognate ones are the TFs that are of interest and that we can follow, whereas the non-cognate ones main purpose is to simulate the other proteins on the DNA, which might interfere with the search process of the cognate TFs.
For efficiency reasons, we pre-calculate the affinities of each TF species, both cognate and non-cognate, and store them in individual arrays.
The non-cognate binding energy is randomly generated using a Gaussian distribution with the mean and variance provided as inputs for each non-cognate species.
For cognate TFs, there are several ways in which the binding energy can be computed, but this work is restricted to three well known ones: (i) mismatch energy (Gerland et al., 2002); (ii) position frequency matrix (PFM) and information theory (Stormo, 2000); and (iii) PFM and binding energy (Berg and von Hippel, 1987).
In all scenarios, we assume that each position in the DNA binding motif is approximately independent and additive.
A bound TF molecule can perform, with user-defined probabilities, one of the following actions: (i) slide left; (ii) slide right; (iii) hop to a position that is Gaussian distributed around original position with a user-defined variance; or (iv) unbind from the DNA.
We assume reflecting boundaries.
In the case the molecule unbinds, there is a certain probability that it will rebind fast near the original place.
Finally, the model allows cooperative behaviour between TF molecules and this can be either mediated by DNA (binding of one molecule to a certain site on the DNA can alter the affinity between another molecule and a different site) or represented as direct TFTF interaction (two molecules bound to the DNAand in physical contact can have different affinities for their current positions compared with the case where they are not in contact); for more details see (N.R.Zabet and B.Adryan, submitted for publication).
The simulation speed is sensitive to the number of agents in the system.
This mainly comes from the fact that the events queue becomes larger with increasing number of molecules in the system and, consequently, higher queues require higher maintenance time.
For 106 TF molecules and the genome of E.coli K-12 (4.6 Mbp), we can simulate 4 105 events per second on a Mac Pro 2x2.26 GHz quad-core Intel Xeon with 32 GB memory running Mac OSX 10.6.8.
3 DISCUSSION GRiP is a highly versatile program which comes with both command-line interface and graphical user interface.
Furthermore, being written in Java, the software can be run on any machine where the Java Runtime Environment 1.6 (or higher) is installed.
The program takes as input a parameters file, which can specify, among many other parameters, three additional data files, namely: (i) the DNAsequence file (from a FASTAfile); (ii) TF file (a csv file with TF-specific characteristics) and, optionally; (iii) TF cooperativity file (a csv file).
Note that, if either the DNA sequence file or the TF file are not provided, then the simulator can randomly generate that data (DNA sequence or TF species).
Once started, the simulation runs until the time in the cell reaches a predefined stop time, or until all target sites are reached (if the stop time is set to 0).
As output, the simulator can print information on: (i) the position of TF molecules on the DNA (or proportion of bound molecules to the DNA); (ii) computed affinity landscapes for each TF species; (iii) measured occupancy bias for each TF species; (iv) statistical information related to TF species (such as residence time, sliding lengths, actual sliding lengths, binding events etc.
); (v) simulation speed; (vi) stored sliding lengths for each species; and (vii) statistics on collisions (total number, total number per species and hot spots on DNA).
GRiP can simulate 1 s of E.coli K-12 and lacI using biologically plausible parameters between 1 h and 4 h (depending on the simulation parameters, the machine on which the simulation is run and even on the interface of the application, GUI or command line), which means that one can simulate up to 10 min of a bacterial cell within a month; for details see Supplementary Material.
Funding: Medical Research Council [G1002110 to N.R.Z.]
and the Royal Society [B.A.].
Conflict of Interest: none declared.
ABSTRACT Motivation: High throughput sequencing technologies generate large amounts of short reads.
Mapping these to a reference sequence consumes large amounts of processing time and memory, and read mapping errors can lead to noisy or incorrect alignments.
SNP-o-matic is a fast, memory-efficient and stringent read mapping tool offering a variety of analytical output functions, with an emphasis on genotyping.
Availability: http://snpomatic.sourceforge.net Contact: mm6@sanger.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Analysis of genome variation has been revolutionized by the advent of next-generation sequencing technologies (Bentley et al., 2008; Li et al., 2008b; Shendure and Ji, 2008).
The short length of sequence reads, e.g.50 base pairs, can pose considerable challenges in achieving accurate genome alignment, particularly if the genome sequence is highly polymorphic.
Discovery of single nucleotide polymorphisms (SNPs) and other variants depends on the alignment algorithm allowing some mismatches to the reference sequence, but allowing too many mismatches may lead to incorrect alignments.
Thus the process of discovering novel variants amounts to a complex statistical problem, particularly if sequencing errors and other sources of noise are taken into account.
Various discovery algorithms have been developed and this is an area of much research interest (for example MAQ, Li et al., 2008a; and bowtie, Langmead et al., 2009).
Here we focus on the problem of describing the genotype of an individual using short-read sequencing data.
In principle, this can be incorporated into the same algorithms used for discovering novel variants, an approach that appears to work well for the human genome (Bentley et al., 2008).
However, there are circumstances in which it may be useful to separate SNP discovery from SNP genotyping.
For example, SNP discovery in Plasmodium falciparum is particularly complicated due to 80% AT content, many repeat sequences, regions of extreme polymorphism and the multiclonality of natural isolates.
Thus different SNP discovery algorithms return widely different results.
One way of addressing this problem is to begin by annotating the reference genome with all the putative SNPs generated by different discovery algorithms.
Then individual SNPs may be genotyped by performing a stringent alignment of the sequencing reads against the reference genome, allowing for all the putative variable positions.
To whom correspondence should be addressed.
To support this sort of genotyping analysis, we developed SNP-o-matic as a fast way of mapping short sequence reads to a reference genome with a list of putative variable positions that are specified at the outset.
The default settings are highly stringent, returning only those sequence reads that align perfectly with the reference genome after allowing for the putative variable positions.
An important feature of SNP-o-matic, which allows the rapid processing of large volumes of sequencing data, is that the reference genome sequence is first indexed (on the fly or by using a precomputed index from disk), and then each sequence read or read pair is examined one at a time.
This avoids having to build and store an index of the reads saving both compute cycles and memory.
Indexing of the reference genome is done in memory on the fly from a generic FASTA file.
A list of putative SNPs, if supplied, is integrated into the reference before indexing, and all permutations of this SNPcontaining sequence are indexed.
Indexing the 25 Mb P. falciparum genome (without SNPs) takes about 30 s on a single CPU core and occupies 1 GB of memory.
A memory-saving option can reduce both memory and indexing time significantly at the expense of a longer mapping phase.
The index can be stored in a file for future use, further reducing the time required for this step, or to facilitate the analysis for larger genomes (Supplementary Material).
Reads are supplied in either FASTA or FASTQ (http://maq.sourceforge.net/fastq.shtml) format; read pairs can be in either single or split files.
In performance tests, mapping 10-million 37 base paired reads against the P. falciparum genome takes 70 s on a single CPU core, not counting the indexing.
No additional memory is required for the mapping.
Additional time and memory may be required for some of the output functions.
For genotyping, a variable length indexed kmer (default 26 bases) is compared to the same length kmer for each read (or both reads in a read pair).
Matches in these bases thus have to be perfect, with respect to the putative SNPs.
The remaining bases of the read are then compared base-by-base to the reference.
By default, these matches have to be perfect as well, but a limited number of mismatches can be allowed.
This stringency will avoid false SNP calls in genotyping mode that would otherwise be caused by aligning reads containing sequencing errors.
Thus, SNP-o-matic will generally map less reads than other algorithms, but the mapping will have much higher accuracy.
When allowing mismatches, the kmer length can be varied to increase mapping tolerance (Supplementary Material).
Both parts of a read pair have to map on the same chromosome for valid mapping; a fragment range can be used to limit their mapping distance to conform to the expected size distribution for the library.
An optional mode can increase stringency by ensuring that at least one read of a read pair maps uniquely within reference the genome.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:46 14/8/2009 Bioinformatics-btp403.tex] Page: 2435 24342435 SNP-o-matic By default, SNP-o-matic will find and use all valid mappings for a read or read pair within the reference.
When using read pairs, the stringent mapping algorithm can sometimes map one of the reads in the pair, but not the other.
SNP-o-matic can output various data about such read pairs.
From the mapping position, orientation, and fragment size, a likely position can be estimated for the non-mapping read.
Based on this information, reads can be grouped by position and assembled to discover variation.
Additionally, the estimated area can be searched for mappings with some mismatches, resulting in potential new SNP calls.
This output is the primary method used by SNP-o-matic to discover new SNPs and small-scale variation, both of which require further downstream analysis (Supplementary Material).
Scripts for such analysis are under development and will eventually be incorporated into the SNP-o-matic package to augment its core function.
Similarly, both reads of the pair may map to the reference, but not on the same chromosome.
This information can be used to detect misassemblies.
When using (super)contigs as reference sequence, read pairs can thus be used to link contigs together, determine their order, and estimate the size of the gap between two contigs.
An output type of SNP-o-matic is a read bin, a file containing reads grouped by mapping behavior.
Bins are a quick and easy way to filter a read set, for example to remove DNA contamination and noise from non-uniquely mapping reads, or to gather non-mapping reads for further study or assembly.
Available bins are single mapping reads (uniquely mapped in the genome), multiple-mapping reads, non-mapping reads, and reads containing IUPAC bases (e.g.N); the later are ignored by SNP-o-matic for mapping.
Mapping/alignment output is supplied for pileup, coverage (base count per position), CIGAR format (http://biowiki.org/CigarFormat), gff format (http://biowiki.org/GffFormat), SAM format (http://samtools.sourceforge.net/) and sqlite database (http://www.sqlite.org/).
For accurate SNP genotyping, it is advantageous to take account of sequence quality scores, especially in regions with low coverage.
SNP-o-matic can generate an output file showing each instance where a mapped read covers a putative SNP.
Each output line contains the read name, allele position on the reference, reference and observed allele, quality score of the allele base, average and minimum quality of both the entire read as well as the five bases on either side of the allele-calling base, and auxiliary data.
This data can be further quality filtered, and used to generate a list of non-reference majority alleles.
Other outputs include observed fragment size distribution, insertion/deletion predictions and inversion detection.
These can also be determined by alternative algorithms from the aforementioned mapping/alignment outputs.
SNP-o-matic is written in C++/ C (for performance optimization).
Compilation with the Intel icpc compiler has shown significant runtime improvements over g++.
We carried out a number of performance tests which are described in the Supplementary Material and briefly summarized below.
The initial tests were based on an artificial dataset consisting of a 1mbp reference genome whose AT content (80%) is similar to the P. falciparum genome, and a duplicate genome with randomly introduced SNPs and indels.
Solexa read pairs (2 37 bases) with random errors (one in five reads) were generated from the altered genome.
SNP-o-matic correctly genotyped the SNPs when they were given as a putative SNP list.
As expected, coverage dropped substantially when a SNP list was not supplied, unless the mapping stringency was reduced.
We have not attempted to conduct a comprehensive comparison of the performance of SNP-o-matic with SNP discovery algorithms such as MAQ as it is designed primarily as a tool to be used after the stage of SNP discovery.
However, as an illustration of where SNP-o-matic may be useful, we found that, when analyzing clusters of six SNPs in the simulated dataset, MAQ only called two of the SNPs, whereas SNP-o-matic called all six correctly when they were supplied in a putative SNP list.
The current version of SNP-o-matic does not directly detect indels, but can be adapted to do so by using an optional wobble function to identify read pairs where one read maps perfectly but the other does not, and then using an algorithm such as velvet (Zerbino and Birney, 2008) to assemble the non-mapping reads into a contig which is then mapped to the region covering the deletion site using an algorithm such as blat (Kent, 2002).
Using this approach, we found that it was possible to detect a five-base deletion that was introduced into the simulated dataset described above.
Finally, in the Supplementary Material, we provide data on the performance of SNP-o-matic on human chromosomes 1, X, and Y.
Based on these findings we estimate that processing an entire human genome using a pre-computed index and the memory saving option, mapping the test reads should take 20 min and 29 GB of RAM.
A similar timeframe, with <3 GB RAM usage, would be expected for a chromosome-by-chromosome serial execution; this would require an additional, albeit simple, filtering step to ensure uniqueness across the entire genome.
ACKNOWLEDGEMENTS The authors thank Chris Newbold for several discussions and suggestions, and Gareth Maslen for help with preparing the manuscript.
Funding: Wellcome Trust, Bill and Melinda Gates Foundation and Medical Research Council.
Conflict of Interest: none declared.
ABSTRACT Motivation: Cells receive a wide variety of environmental signals, which are often processed combinatorially to generate specific genetic responses.
Changes in transcript levels, as observed across different environmental conditions, can, to a large extent, be attributed to changes in the activity of transcription factors (TFs).
However, in unraveling these transcription regulation networks, the actual environmental signals are often not incorporated into the model, simply because they have not been measured.
The unquantified heterogeneity of the environmental parameters across microarray experiments frustrates regulatory network inference.
Results: We propose an inference algorithm that models the influence of environmental parameters on gene expression.
The approach is based on a yeast microarray compendium of chemostat steady-state experiments.
Chemostat cultivation enables the accurate control and measurement of many of the key cultivation parameters, such as nutrient concentrations, growth rate and temperature.
The observed transcript levels are explained by inferring the activity of TFs in response to combinations of cultivation parameters.
The interplay between activated enhancers and repressors that bind a gene promoter determine the possible up-or downregulation of the gene.
The model is translated into a linear integer optimization problem.
The resulting regulatory network identifies the combinatorial effects of environmental parameters on TF activity and gene expression.
Availability: The Matlab code is available from the authors upon request.
Contact: t.a.knijnenburg@tudelft.nl Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Transcription factors (TFs) mediate the activation or repression of gene expression by binding specific regulatory sequences (motifs) in gene promoters.
The combinatorial interactions of multiple TFs play an essential role in transcriptional regulation.
A classical example is Escherichia colis lactose system, where the lac operon is expressed only if the concentration of TF CRP is high and that of TF LacI is low.
Presently, many studies have revealed an important role for combinatorial interactions between different TFs in establishing the To whom correspondence should be addressed.
complex patterns of gene expression (Balaji et al., 2006).
The advent of high-throughput genomic measurement techniques enabled the application of genome-wide computational approaches aimed at inferring these regulatory relations.
Sequence data, microarray gene expression data and ChIPchip TF binding data have been integrated in many different ways to derive regulatory networks.
Several approaches fit expression data using linear regression models, where the predictors are the TFs, i.e.their binding potential or number of motifs in a gene promoter (Bussemaker et al., 2001; Gao et al., 2004; Nguyen and Dhaeseleer, 2006).
The effect of multiple TFs on gene expression is modeled as the weighted sum of the contribution of individual TFs.
Combinatorial regulation by TFs, i.e.synergistic or antagonistic effects of multiple TFs on gene expression, are not incorporated into these models.
Most methods that do include combinatorial effects limit the scope to TF pairs, e.g.(Bonneau et al., 2006; Chang et al., 2006; Das et al., 2004; Yu et al., 2006).
Bonneau et al.employ continuous versions of logic functions (OR, AND and XOR) of the activities of TF pairs as additional predictors in the regression model.
Although, in principle, these methods can be extended to model the combinatorial effects of more than two TFs, the model will be too complex to reliably estimate its parameters given the currently available data.
Segal et al.(2003) and Yeang and Jaakkola (2006) present quite different approaches to the problem of combinatorial regulation in transcription networks.
Segal et al.constructed regulatory networks by building decision trees.
Genes are grouped into regulatory modules, which are defined by a hierarchical decision tree, where the decisions at the nodes of the tree are based on the expression levels of TFs.
In Yeang and Jaakkola, a TF is characterized as an enhancer or a repressor, being either necessary or sufficient to cause up-or downregulation of a gene.
The combinatorial function of all TFs that can bind a gene promoter is modeled as the consensus prediction of the individual TFs.
It should be noted that these two approaches, as well as many of the abovementioned ones, rely on the often incorrect assumption that the activity of a TF can be derived from the expression of the gene that codes for the TF.
So far, regulatory networks have been presented as graph structures showing the (combinatorial) regulatory effect of TFs on individual genes, modules of similarly expressed or otherwise related genes or on other TFs.
The extracellular signals that trigger the activation or deactivation of TFs are usually not part of the generated network.
Yet they could provide more direct and trustworthy evidence to infer TF activity than other signals, such as the gene expression of a TF.
Three main reasons for 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
file:t.a.knijnenburg@tudelft.nl [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i173 i172i181 Combinatorial influence of environmental parameters on TF activity their exclusion can be identified.
First, many studies on yeast are based on shake-flask cultures, where parameters like growth rate and nutrient availability are continuously changing and cannot be controlled or accurately measured.
Consequently, conditions can not be accurately defined.
Second, very often research questions are approached from a single perspective, i.e.a condition of interest is compared to a reference condition.
Differential gene expression is then attributed to the difference between the condition of interest and the reference condition.
These approaches ignore combinatorial effects of growth parameters, the presence of which have been established by various studies, e.g.Castrillo et al.(2007); Knijnenburg et al.(2007); Regenberg et al.(2006).
That is, if the measurements were repeated using a different medium composition or temperature, chances are that a different set of differentially expressed genes would be identified.
Thus, these approaches only model the differences between growth conditions, and not the growth conditions themselves.
Note that this strategy is implicitly incorporated into two-channel microarray measurements, which output the gene expression ratio between the condition of interest and the reference condition.
Third, when combining different microarray experiments, differences in mRNA extraction protocols, microarray platform and possibly normalization and summarization algorithms, add to the already large amount of unquantified heterogeneity amongst experimental conditions (Bammler et al., 2005; Tan et al., 2003).
The context dependency of regulatory networks has been identified and acknowledged in many studies.
For example, in Bar-Joseph et al.(2003) annotation data are employed to identify the biological context in which the inferred regulatory interactions are assumed to take place.
In Luscombe et al.(2004) conditionspecific regulatory networks were derived.
In this case, conditionspecific refers to one of five phenomena (cell cycle, sporulation, DNA damage, stress response or diauxic shift), which were investigated with five different microarray datasets.
Myers and Troyanskaya (2007) propose a Bayesian approach for contextsensitive integration of diverse genomic data.
Note however, that in these approaches, the precise environmental conditions under which the microarray measurements were taken are not included in the model.
In this work we do incorporate the actual cultivation parameters in the computational framework and use this information to infer combinatorial regulation by TFs.
The work is based on a yeast transcriptome compendium, comprised of 170 microarray measurements (Knijnenburg et al.manuscript in preparation).
These measurements encompass 55 unique growth conditions with a variable number of independent biological replicates per condition.
All cultivations were performed in chemostat fermentors under steady-state conditions.
In a chemostat, culture broth (including biomass) is continuously replaced by fresh medium at a fixed and accurately determined dilution rate.
When the dilution rate is lower than max, the maximal specific growth rate of the micro-organism, a steady-state situation will be established in which the specific growth rate equals the dilution rate.
In such a steady-state chemostat culture, is controlled by the (low) residual concentration of a single growth-limiting nutrient.
Across the 55 different conditions, there are nine varying cultivation parameter types, including limiting element, growth rate, carbon source, aeration and temperature.
Each type can assume a unique set of values.
For example, in a given experiment, the employed limiting element is either carbon, nitrogen, sulfur, phosphorus, zinc or iron.
Thus, each condition is characterized by a configuration of settings of these nine cultivation parameter types (Fig.1).
In order to model the effects of the cultivation parameters on gene expression while explicitly incorporating TFs, we follow a two-step procedure.
An overview of this procedure is presented in Figure 2.
First, we apply a forward stepwise regression strategy to quantify the (combinatorial) effect of these environmental parameters on gene expression.
The regression is performed for each gene individually.
Figure 1 depicts the results of the regression analysis for one particular gene.
The influence of a cultivation parameter on the expression of a gene is represented by its regression weight.
These weights are discretized by mapping non-zero elements to 1 or 1, depending on the sign of the weight.
Given that changes in gene expression levels as observed across different environmental conditions can be attributed to changes in the activity of TFs, we aim to infer the activity of TFs as a function of the cultivation parameters.
This forms the second step of our approach.
The goal is to estimate M, such that R is the optimal approximation of the discretized regression coefficients in R. The elements of M are 1, 0 or 1 and indicate whether a TF is activated as an enhancer (1) or a repressor (1) under a (combinatorial) cultivation parameter.
Additionally, each TF has a particular generic enhancer strength and a repressor strength.
In the procedure we employ auxiliary matrix T, which is derived from ChIPchip experiments and literature and indicates whether a TF can bind a gene promoter.
To decide whether a gene is upregulated, downregulated or not affected by a particular cultivation parameter, indicated by a 1, 1 and 0 in R, respectively, we use the following rules concerning transcriptional regulation: if there is at least one active enhancer in a gene promoter, then the gene can be upregulated.
If there are only active enhancers in a gene promoter, then the gene is upregulated.
Similar rules apply to the repressors.
If there are both active enhancers and repressors in a gene promoter, we compare total enhancer strength, which is the sum of the strengths of the activated enhancers, with its repressor counterpart.
When the enhancer strength is larger than the repressor strength, the gene is upregulated.
The gene is downregulated when the repressor strength exceeds the enhancer strength.
Figure 2c visualizes the active TFs that bind the gene promoters of genes g1, g2 and g3 under cultivation parameter A.
From M we deduce that three TFs are activated; and are enhancers, is a repressor.
From T we deduce that binds all three promoters, binds the g2 and g3 promoters and only binds the promoter of g3.
Gene g1 and g2 are upregulated, since only active enhancers bind the promoters.
For gene g3, the repressor strength of TF exceeds that of the sum of the two enhancers, thereby downregulating the gene.
The concept of TF strength enables the inference of hierarchical or combinatorial effects amongst TFs that bind a gene promoter.
The inference algorithm is translated into a linear mixed integer optimization problem and solved accordingly.
Both the elements of M as well as the TF strengths are estimated, such that the predicted gene regulation in R maximally corresponds with the discretized regression coefficients in R. The abovementioned rules become constraints in the optimization problem.
See the Methods section for details.
The resulting model identifies the combinatorial influence of cultivation parameters on TF activity and gene expression.
Furthermore, it infers the combinatorial regulatory code of multiple TFs in gene promoters.
i173 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i174 i172i181 T.A.Knijnenburg et al.Aerobic Anaerobic AceEth EthGal Glucose GlucoseMal Chlo ChlSulfate (Ammonium) Sulfate (A Sulf Sulfate (Ammonium) Sulfate (Amm Sul SulAspLeu LeuMet Met MethPhen Pro Met MetSulate Sulate Sulate Carbon CarbonIro Nitrogen NitrogenPhos PhoSulf SulZin Zin.03.03.05.1.1.1.1.1.2.2 12 1230 30 30 3.55 5 56.5 Ace BenzCO2 CO2 CO2EthEthFor ProSorTwe Twenone none none none none 7 7.5 8 8.5 9 9.5 10 10.5 11 11.5 12 E xp re ss io n le ve l Aeration type Csource(s) Nsource Ssource Limiting element Growth rate Temperature (C) pH Extra compound Fig.1.
Expression levels of a gene (COX5A) across the 55 cultivation conditions.
The colored matrix is a schematic representation of the settings of the nine cultivation parameter types across the 55 conditions.
The colored lanes indicate the cultivation parameter types that are employed to order the experiments, in this case, aeration type and limiting element.
The regression model which models the gene expression as a function of the cultivation parameters, selected one single effect, i.e.aeration type, and one combinatorial effect, i.e.aeration type anaerobic together with limiting element carbon.
The reconstructed expression pattern based on these two effects is indicated by the shaded area.
2 METHODS 2.1 Microarray data The Saccharomyces cerevisiae laboratory reference strain CEN.PK 113-7D (MATa) was grown in chemostat fermentors under 55 different conditions.
For each condition, a variable number of independent biological replicates was performed, although mostly three, summing up to 170 microarray measurements.
Across the 55 conditions, nine different cultivation parameter types can be identified.
A cultivation parameter type, e.g.the carbon source, is described as a categorical variable and contains two or more categories, e.g.the used carbon source can be either maltose, glucose or ethanol.
Each condition is characterized by a specific combination of these categories across the nine cultivation parameter types.
Figure 1 presents an overview of the relevant categories assumed by the parameter types per condition.
Sampling of the chemostat cultures, probe preparation and hybridization to single-channel Affymetrix GeneChip YG S98 microarrays was performed as previously described (Piper et al., 2002).
Chip quality control, condensing probe intensities to gene expression levels and normalization was performed using GeneData Refiner Array.
The RMA algorithm was used to derive the log2 scale measure of the expression levels (Irizarry et al., 2003).
Quantile normalization was applied to normalize between arrays (Bolstad et al., 2003).
2.2 Inferring the influence of cultivation parameters on gene expression A design matrix was created, containing both main (or single) effects and interaction (or combinatorial) effects: each category within each cultivation parameter type is represented by a binary indicator column with 170 entries.
These columns represent the main effects, which indicate, for each array, under which category of a particular cultivation parameter type, the yeast was grown.
Interaction effect columns were obtained by applying the logic AND function to all possible pair-wise combinations of main effect columns.
Redundant columns and columns containing only zeros were removed, resulting in 112 columns, of which 37 represent main effects and 75 represent interaction effects.
These data are stored in the binary [AC] design matrix D. Here, A equals 170 and is the number of arrays.
C equals 112 and is the number of (combinatorial) cultivation parameters.
A forward stepwise ordinary least squares regression strategy was applied to each gene individually: y=X + (1) Here, yi denotes the measured gene expression level of a particular gene for array i, with i=1,...,A; X is the predictor matrix, represents the regression coefficients and the error, which is assumed to be independent zero-mean normally distributed.
Initially, X contains only the intercept, i.e.a column of A ones.
In an iterative fashion, columns from D are added to X.
For this we apply a leave-one-out cross validation (LOOCV) scheme, where a single sample is used for testing, while the remaining (A1) samples are used for training the regression model.
This is repeated such that each sample is used once as test data.
The column from D, with the smallest root-meansquared (RMS) LOOCV error and absolute regression coefficient larger than one, is selected and added.
The iterative process of adding columns is discontinued when the P-value, as output by a t-test that determines whether the regression coefficient significantly differs from zero, exceeds 0.05/C.
To prevent the inclusion of spurious combinatorial effects, the following strategy is applied: when a combinatorial effect column is selected, we check whether the addition in explained variance is larger than the addition in explained variance when adding the two main effect columns that constitute the combinatorial effect.
Only in the cases where this is true, we add the combinatorial effect column.
Otherwise the two main effect columns are added, provided that they satisfy the P-value threshold and their absolute regression coefficients are larger than one.
Note that only coefficients larger than 1 or smaller than 1 are allowed.
In terms of the absolute expression measure, this means we only take into i174 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i175 i172i181 Combinatorial influence of environmental parameters on TF activity (a) (b) (c) Fig.2.
Schematic overview of the approach.
The goal is to build R, the optimal approximation of the discretized regression coefficients in R. (a) The coefficients in R are derived from a regression analysis, which assesses the influence of cultivation parameters on gene expression by employing these parameters as predictors in the regression model.
The discretization procedure maps non-zero regression weights to 1 or 1, depending on their sign.
(The schematic representation of R is given for five genes and three cultivation parameters.)
(b) The elements of R are determined by T and M. T is fixed and indicates binary TF binding potential to gene promoters.
The elements of M are estimated and indicate the activity of TFs as enhancers or repressors under the different (combinatorial) cultivation parameters.
A logic circuit derived from M is graphically depicted above the representation of M. (c) Visualization of the active TFs on the gene promoters of genes g1, g2 and g3 under cultivation parameter A. Enhancers are depicted as red boxes; repressors are depicted as green boxes.
(TF can bind the promoter of g1, but is not active under A.)
The height of a box indicates the enhancer or repressor strength.
The strength of a particular enhancer or repressor is the same for all genes.
A gene is upregulated when its activator strength, i.e.the sum of the heights of the red boxes, is larger than the repressor strength, which equals the sum of the heights of the green boxes.
Downregulation is inferred in the opposite situation.
See text for details.
account expression differences of 1-fold change or more.
(The expression data are on log2 scale.)
So, we focus on the cases where a cultivation parameter has a large influence on expression.
Finally, the regression coefficients for all yeast genes are discretized and put in R ([GC]Z[1,0,1]), where G is the number of yeast genes.
The discretization procedure maps positive coefficients to 1 and negative coefficients to 1.
R is quite sparse since for most of the genes only two or three columns from D were selected as significant predictors.
2.3 TF binding data For 111 TFs we extracted their known regulatory sites from TRANSFAC (Wingender et al., 2000) and ChIPchip data (Harbison et al., 2004; MacIsaac et al., 2006) (no conservation, binding P-value cutoff 0.001).
These geneTF pairs were put in the binary [GF] TF-binding matrix T, where 1 indicates that a TF can bind a gene promoter.
F equals 111 and is the number of TFs.
2.4 Inferring TF activity and TF strengths The goal of our optimization problem is to infer the activity of TFs as a function of cultivation parameters, such that we can optimally explain the regression coefficients, which were distilled from the observed gene expression data.
These TF activities form tertiary matrix M ([F C] Z[1,0,1]).
A non-zero element in M indicates that a TF is activated under a cultivation parameter and either acts as an enhancer (1) or a repressor (1).
Other data used in the optimization problem are: TF binding matrix T ([GF]Z[0,1]), discretized regression coefficient matrix R ([GC] Z[1,0,1]) and its reconstructed version R ([GC]Z[1,0,1]).
First, from the tertiary matrix R two binary matrices with the same dimensions, R+ and R, are derived.
R+ has non-zero entries, where R contains 1s, and thus indicates the elements, where genes are upregulated under influence of a particular cultivation parameter.
R has non-zero entries, where R contains 1s, and thus indicates the downregulated elements.
A similar procedure is undertaken for tertiary matrix M, thereby obtaining M+, which contains the active enhancers and M, which contains the active repressors.
Now, all i175 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i176 i172i181 T.A.Knijnenburg et al.variables consist of binary integers (and are restricted to remain binary integers).
The objective function for the optimization problem is as follows: minimize g,cI+ [Rgc R+gc]+ g,cI [Rgc Rgc]+ + F f =1 C c=1 [M+f ,c +Mf ,c] (2) where I+ is the set of index pairs referring to the elements where R is 1, and similarly, I refers to the negative elements of R. Thus, we only try to explain the non-zero elements of R, which represent the large expression changes due to the influence of the cultivation parameters.
The zero elements of R do not only contain cases where there is no change in expression, but they contain the whole spectrum of no change in expression up to moderately large changes in gene expression.
Therefore, we do not want to enforce TFs to be deactivated because of these zero elements.
The last term of Equation (2) restricts the model complexity by penalizing the number of activated TFs.
Parameter can be interpreted as the number of non-zero elements in R that a TF needs to help explain in order for it to be activated.
Below, the constraints of the optimization problem are stated.
These constraints are linear in M+, M, R+ and R, which are the variables in the system.
In the appendix a detailed explanation for constraints c5, c8 and c12 is given.
The first two constraints are straightforward.
Constraint c1 states that a TF cannot be an active repressor and an active enhancer at the same time.
Constraint c2 states that a gene cannot be upregulated and downregulated at the same time.
c1: M+fc +Mfc 1 f ,c c2: R+gc +Rgc 1 g,c Constraint c3 states that if there is at least one active enhancer in a gene promoter, i.e.the inner product is positive then the gene can be upregulated, i.e.the regression coefficient can be 1.
Constraint c4 is the analogue constraint for the case of active repressors.
Constraint c5 forces a gene to be either upregulated or downregulated, when there is at least one active enhancer or one active repressor in the gene promoter.
c3: Tg,M+c R+gc g,c c4: Tg,Mc Rgc g,c c5: Tg,M+c + Tg,Mc F (Rgc +R+gc) g,c To decide upon upregulation or downregulation when multiple active enhancers and repressors bind a promoter, four continuous variables are introduced: S+ and S; both ([F C]R[0,F]) and S+ and S; both ([F 1]R[1,F]).
S+fc , represents the strength of TF f as an enhancer under cultivation parameter c. S+fc is zero when M + fc is zero, i.e.when f is not activated as an enhancer under c. This rule is stated in constraint c6.
S+fc equals the generic TF strength for f , S+f , when M + fc is one.
Thus, the strength of a TF f is the same for all genes under the cultivation parameters, where the gene is activated (and zero otherwise).
This rule is stated in constraints c7 and c8.
Analogue rules apply for S and S. The corresponding constraints c9, c10 and c11 are omitted for brevity.
c6: S+fc F M+fc f ,c c7: S+fc S+f f ,c c8: S+fc S+f F (M+fc 1) f ,c Constraint c12 states that when the sum of the strengths of active enhancers that bind a gene promoter is larger than its repressing counterpart, the gene is upregulated.
Constraint c13 encloses the reverse scenario.
Note that if an identical set of enhancers and repressors is active on a promoter, this will lead to the same reconstructed regression coefficient for any gene and under any cultivation parameter.
c12: Tg,S+c Tg,Sc (F2 +F2) R+gc F2 g,c c13: Tg,Sc Tg,S+c (F2 +F2) Rgc F2 g,c The optimization problem is implemented within the MATLAB environment and executed using the MOSEK optimization toolbox with standard settings for mixed integer optimization.
Given constraints c1 to c13, MOSEK estimates variables M+, M, R+, R, S+, S, S+ and S such that the optimization function in Equation (2) is minimized.
3 RESULTS 3.1 TF activity in response to changes in oxygen and carbon presence The regulatory network inference algorithm is run on a subset of the data.
In particular, we focus on oxygen and carbon; two environmental factors, which have a large and well studied effect on the transcriptional program of S.cerevisiae.
Four cultivation parameters are selected, i.e.aeration type, carbon-limitation and the combinatorial cultivation parameters, carbon-limited aerobic growth and carbon-limited anaerobic growth.
Note that aeration type is actually a cultivation parameter type that assumes two values, i.e.aerobic growth and anaerarobic growth.
Since these are mutually redundant, only aerobic growth was included in the regression model and subsequent optimization algorithm.
(Downregulation under aerobic growth and upregulation under anaerobic growth are interchangeable.)
There are 40 genes, which are influenced by at least two of these four cultivation parameters, i.e.there are 40 rows in R with at least two non-zero elements in the four columns of interest.
These 40 genes are bound by 46 different TFs.
In this experiment is set to two.
The algorithm correctly inferred the regression coefficients of 58 of the 84 (70%) non-zero elements in R. A particularly large concentration of incorrectly predicted values appears toward the bottom of R, where zeros are predicted while the true expression coefficients are non-zero.
See Figure 3d.
This stems from the fact that the promoters of these genes have almost no motifs for the activated TFs, in which case the model cannot explain the up-or downregulation.
3.1.1 Inferred TF activity In total, nine different TFs were activated across the four cultivation parameters, some under more than one cultivation parameter.
Three of these TFs, HAP1, HAP2/3/4 and ROX1, have a significantly larger strength, when compared to the others.
See Figure 3a, b.
The large strength indicates their dominating effect on transcriptional regulation.
If one of these TFs is active and binds the promoter, it will determine the direction of transcriptional regulation.
For example under aerobic conditions (Aer) the promoter of gene PAU3 (the tenth gene from the bottom in Fig.3c) is bound by one active enhancer, i.e.YAP7, and one active repressor, i.e.ROX1.
Since the repressor strength of ROX1 is (much) larger than the enhancer strength of YAP7, the gene is (correctly) predicted to be downregulated.
Interestingly, in the resulting network for this data, the TF strength of ROX1 equals 45.9995, which is very close to the maximum value of 46, the number of TFs F. However, this number is slightly smaller than i176 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i177 i172i181 Combinatorial influence of environmental parameters on TF activity (a) (c) (b) Fig.3.
Overview of the results obtained for the oxygen and carbon limitation data.
(a) Inferred influence of cultivation parameters aerobic growth (Aer), anaerobic growth (Ana) and carbon limitation (Clim) on TF activity.
Only the three dominating TFs are reported.
(b) Representation of S, indicating the strength of the activated TFs under each of the four cultivation parameters.
Enhancers are depicted in red; repressors are depicted in green.
(c) Representation of T, indicating which gene promoters can be bound by the activated TFs.
The enhancer or repressor strengths for the four cultivation parameters are visualized by the colored blocks inside the rectangle that represents a binding site.
(d) Representation of R, indicating the inferred regression coefficients.
Upregulation is indicated by red; downregulation is indicated by green.
Incorrectly inferred elements are marked with a gray cross.
White boxes without a cross are the zero elements of R. These elements are not part of the optimization scheme.
the strength of HAP2/3/4 which has the maximal strength of 46.
This difference can be attributed to gene PET9 (the ninth gene from the top in Fig.3c).
Both HAP2/3/4 and ROX1 can bind the PET9 promoter.
To ensure that this gene is upregulated when grown aerobically, as was deduced from the regression analysis, the active enhancers should have a larger strength than the active repressors.
Therefore, the strength of ROX1 is set a bit smaller than the strength of HAP2/3/4, however, still large enough to dominate other active enhancers.
3.1.2 Regulation of gene expression by oxygen The role of the three dominant TFs in the regulation of gene expression by oxygen is widely reported in the literature.
Both HAP1 and the HAP2/3/4 complex activate genes in response to heme, which is synthesized only in the presence of oxygen (Zitomer and Lowry, 1992).
TF ROX1 is needed for the repression of hypoxic or heme-repressed genes under aerobic conditions (Lowry and Zitomer, 1988).
Also, the relation between carbon source and the HAP2/3/4 complex has been investigated.
The HAP2 and HAP3 proteins enable DNA binding of the complex, whereas HAP4 contains the transcriptional activation domain.
The synthesis of the activator subunit HAP4 is regulated by the carbon source.
More specifically, the expression of HAP4 is repressed by glucose, S.cerevisiaes preferred carbon source (Forsburg and Guarente, 1989).
Tai et al.(2005) reports that HAP4 mRNA is present in carbon-limited cultivations even under anaerobic conditions, where HAP4 has no obvious role.
We can corroborate and even further substantiate these findings with the observation that the HAP4 protein is an activator under carbon-limited anaerobic conditions.
Note that all genes, which are upregulated under carbon-limited anaerobic growth are also upregulated under aerobic growth.
See the top 13 genes in Figure 3c.
The expression profile of one of these genes, COX5A, across all conditions is depicted in Figure 1.
This expression profile is typical for all the 13 members of this group.
It shows that these genes are most highly expressed when grown aerobically.
Yet, in the anaerobic case, where the expression is in general lower, these genes show different expression behavior in carbon-limited growth compared to other nutrient limitations.
That is, these genes have a higher expression level in carbon-limited cultivations, where there is hardly any glucose, compared to the situation, where glucose is abundant.
Also, for the other TFs, which are activated according to the inference algorithm, evidence is found in literature.
For example REB1, which acts as an enhancer under three cultivation parameters, is a RNA polymerase I enhancer binding protein as well as an activator for many genes transcribed by RNA polymerase II (Ju et al., 1990).
STE12 is known to activate genes associated with pseudohyphal (low oxygen) growth (Norman et al., 1999).
SUT1 is reported to encode a glucose transporter (Weierstall et al., 1999), however SUT1 also has a putative role in the regulation of some hypoxic genes (Regnacq et al., 2001).
In general, the precise regulatory role of these TFs in (an)aerobiosis and response to the carbon source is not known.
The results of this analysis provide hints for elucidating the regulatory mechanisms of these factors.
3.1.3 Setting Parameter , which restricts the model complexity by penalizing the number of activated TFs, is chosen using a 5-fold CV scheme.
The genes are divided into five parts, where consecutively four parts are used for training and one part is i177 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i178 i172i181 T.A.Knijnenburg et al.Fig.4.
CV errors for different values of.
used for testing.
The M and S matrices, which are computed on the training set, are applied to the test set to obtain the reconstructed regression coefficients for the test set, Rtest.
The error on the test set is defined as: Err = 1 J g,cI Rtestgc Rtestgc (3) where I is the set of index pairs referring to the non-zero elements of Rtest and J the number of these non-zero elements.
The CV scheme is repeated 10 times.
Figure 4 depicts the average error over all CV runs.
For small values of , many TFs are activated in order to approximate the regression coefficients.
Clearly, this strategy is prone to overfitting, which is also illustrated by the large CV error.
For large values of , activating a TF is severely penalized, such that only a few TFs will be activated.
(For =20, no TF is activated and every element of Rtest is zero).
The high CV error in this case, indicates that a lot of true regulation is missed.
The optimal will be found between these extremes.
In this experiment, =2 led to the smallest CV error and was therefore selected.
3.2 Transcriptional regulation of nitrogen metabolism Across the conditions of the compendium, yeast was grown on six different nitrogen sources.
This inspired the second experiment, where we analyzed the transcriptional regulation of the genes that comprise the nitrogen compound metabolism category of GO biological processes (Ashburner et al., 2000).
A total of 119 of these genes are influenced by at least one cultivation parameter and bound by one of 78 different TFs.
In total, there are 68 cultivation parameters that cause up-or downregulation of at least one of these 119 genes.
The resulting transcription regulation network (with opt =2) revealed the activation of 14 different TFs under 28 different cultivation parameters, of which 11 are combinatorial.
Figure 5 depicts the network for the cultivation parameters, which are most straightforwardly related to nitrogen metabolism, i.e.the different nitrogen sources, nitrogen as growth limiting element and combinatorial effects involving these cultivation parameters.
The six different nitrogen sources can be dichotomized into preferred and non-preferred nitrogen sources.
The preferred nitrogen sources are asparagine (Asn) and ammonium [in ammonium sulfate (AS)].
Proline (Pro), phenylalanine (Phe), methionine (Met) and leucine (Leu) are non-preferred (or poor) nitrogen sources (Boer et al., 2007; Magasanik and Kaiser, 2002).
In S.cerevisiae, the use of nitrogen sources is controlled by a transcriptional regulation mechanism known as nitrogen catabolite repression (NCR).
When a good nitrogen source is present, NCR shuts down the pathways for the use of poor nitrogen sources.
NCR is mediated by a fourmember family of GATA-binding TFs: GLN3, GAT1, DAL80 and GZF3 (Hofman-Bang, 1999).
In the absence of a good nitrogen source, GLN3 is activated and in turn activates the transcription of NCR-sensitive genes.
Indeed, for three of the four non-preferred Fig.5.
Inferred TF activity derived from genes, which are involved in nitrogen metabolism.
Preferred nitrogen sources are printed in bold; nonpreferred nitrogen sources are printed in italic style.
Abbreviations for the nitrogen and sulfur sources are explained in the text.
nitrogen sources, GLN3 acts as an enhancer.
When methionine is the nitrogen source, the MET31/32 complex is activated.
This complex controls the biosynthesis of sulfur containing amino acids (Blaiseau et al., 1997).
(Methionine is also used as a sulfur source.)
In the case of leucine, two additional TFs are activated; LEU3 and GCN4, the two key regulators in the regulation of branched-chain amino acid metabolism (Boer et al., 2005).
The inferred role of GCN4 as an activator in the presence of a poor nitrogen source and as a repressor in the presence of good nitrogen sources corroborates the work of Sosa et al.(2003).
It further supports the fact that NCR is not solely achieved through the action of the abovementioned family of GATA factors, but conceivably also through GCN4.
3.2.1 Missing and dubious TF activity Remarkably, the other three GATA factors, GAT1, DAL80 and GZF3, are not part of generated network.
Inspection of the TF binding data in the promoters of the 119 nitrogen metabolism genes revealed that GAT1, DAL80 and GZF3 bind only 3, 4 and 0 genes, respectively.
This could indicate that their targets are not transcriptionally regulated under the influence of the cultivation parameters.
However, this observation should also be related to the ChIPchip data.
From TRANSFAC, we extracted many TFgene pairs, which are not present in the ChIPchip data.
This indicates that not all TF targets are detected by the ChIPchip experiments.
Furthermore, Gao et al.(2004) estimate that 40% of the ChIPchip TF targets are non-functional.
Obviously, this complicates regulatory network inference.
Another dubious result was identified when analyzing the cases in which two or more TFs were active on a promoter.
In this experiment, there are 72 such cases, of which 10 are unique.
Amongst the most frequent cases, we found the combinatorial regulation of TFs, which have already been reported in literature, e.g.the interplay between LEU3 and GCN4 (Boer et al., 2005) and that of CBF1 and GCN4 (OConnell et al., 1995).
Also, GLN3 and GCN4 were found activated together in a set of nine gene promoters.
These nine genes were upregulated under two cultivation parameters, i.e.sulfur limitation and zinc limitation, where both GLN3 and GCN4 are enhancers.
However, under another cultivation parameter, i.e.where leucine is used as a nitrogen source, the same genes were downregulated, where now GLN3 acts as a repressor (which is stronger than enhancer GCN4).
These results seem i178 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i179 i172i181 Combinatorial influence of environmental parameters on TF activity Fig.6.
Representation of S for the regulatory program inferred using the compendium.
Color coding is identical to Figure 3b.
implausible and imply that this regulation pattern should involve another TF, which might not be present in the employed TF binding data set.
Preliminary experiments with artificial datasets have demonstrated that especially missing TFs (simulated by removing colums from T) can have a large negative effect on the ability to reconstruct the correct regulatory network (results not shown).
3.3 Compendium analysis The algorithm was also run on the complete compendium for all genes that are up-or downregulated under at least two cultivation parameters and for all cultivation parameters that influence the expression of at least 10 genes (G=766,C =67,F =101,opt =5).
In the resulting regulatory network, 41 (61%) cultivation parameters activated at least one of the TFs, resulting in 29 (29%) different activated TFs in total.
See Figure 6.
Network inference on the complete dataset allows for a more rigorous and unbiased estimation of the regulatory program.
It reveals confounding factors, with respect to the previously discussed programs, which were based on a subset of the data.
For example, the regulatory program of GATA factor GLN3, as discussed before, is also depending on other (combinatorial) cultivation parameters, e.g.zinc limitation and nitrogen limitation at low temperature.
These results offer interesting leads, however the combinatorial regulation of TFs, as inferred by this analysis, becomes complicated.
There are up to four active TFs on gene promoters.
This calls for an automated procedure that uses these inferred TF activities and accompanying strengths to derive logic rules, in which the influence of multiple TFs on transcriptional regulation is formalized.
Note that the inference algorithm was run on a selection of genes and cultivation parameters.
The number of variables and constraints in optimization problem is 4FC +2GC and 7FC +6GC, respectively, which becomes quite large for the complete dataset.
It is yet unclear (due to computation time) if converge is reached for the dataset with all genes and cultivation parameters.
4 DISCUSSION The transcriptional program of a cell is largely determined by its extracellular environment.
The accurate measurement of environmental parameters, e.g.with chemostat cultures, have inspired several approaches that analyze the (combinatorial) effect of environmental parameters on gene expression.
In this study, we have, for the first time demonstrated how environmental parameters can be employed to derive transcriptional regulation networks.
In these networks, the cultivation parameters form the signals that trigger the activation or deactivation of TFs.
Since many TFs are regulated post-transcriptionally, this approach seems more natural than the often employed strategy of deducing the TF activity from the mRNA expression of TFs.
The inference algorithm was translated into a linear optimization problem, solvable without having to rely on greedy and/or heuristic search strategies.
The combinatorial regulatory code of multiple TFs that are able to bind a promoter, is modeled using the linearly weighted sum of inferred enhancers and repressor strengths.
Previous approaches have also modeled gene expression as a linearly weighted sum of TF contributions, e.g.Gao et al.(2004).
The main improvement of our method is the fact that the activity of TFs can be explicitly turned on or off, and that the inference algorithm optimizes this choice with respect to the direction of regulation, i.e whether a gene is up-or downregulated.
This strategy enables the inference of combinatorial effects between TFs.
For example, a repressor, which interacts directly with the TATA binding protein, thereby completely blocking transcription independent of the possible enhancers that bind the promoter, would acquire a strength that is larger than the sum of the strengths of all enhancers that can bind the promoter.
Thus, the repressor, when active, will cause downregulation of the gene, thereby nullifying the influence of the enhancers.
This is in contrast with the linear regression strategies, where these enhancers would still have influence on the gene expression level.
Additional validation experiments indicate that more pairs of TFs, which are simultaneously active according to our approach, are found to co-occur in PubMed abstracts when compared to TF pairs uncovered with Gao et al.(2004).
This difference can be attributed to the fact that we decompose the expression in terms of cultivation parameters, and analyze these cultivation parameters separately.
When using only the expression data itself, some cultivation parameters (such as aeration type) can have a much larger influence than others, thereby dominating the expression pattern and thus controlling which TFs are found to be the most significant, leading to less diversity in activated TFs (and thus fewer TF pairs).
An overview of this comparison is published as Supplementary Material.
A future challenge lies in the integral interpretation of the inferred regulatory networks, which must be accompanied by a computational approach that derives logic rules, which are able to describe the interplay of multiple TFs on gene promoters.
ACKNOWLEDGEMENTS Funding: T.A.K.
and M.J.T.R.
are part of the Kluyver Centre for Genomics of Industrial Fermentation, which is supported by the Netherlands Genomics Initiative (NGI).
L.F.A.W.
is part of the Cancer Genomics Centre, which is supported by the Netherlands Genomics Initiative (NGI).
i179 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i180 i172i181 T.A.Knijnenburg et al.Conflict of Interest: none declared.
ABSTRACT Motivation: Exome sequencing (exome-seq) data, which are typically used for calling exonic mutations, have also been utilized in detecting DNA copy number variations (CNVs).
Despite the existence of several CNV detection tools, there is still a great need for a sensitive and an accurate CNV-calling algorithm with built-in QC steps, and does not require a paired reference for each sample.
Results: We developed a novel method named PatternCNV, which (i) accounts for the read coverage variations between exons while leveraging the consistencies of this variability across different samples; (ii) reduces alignment BAM files to WIG format and therefore greatly accelerates computation; (iii) incorporates multiple QC measures designed to identify outlier samples and batch effects; and (iv) provides a variety of visualization options including chromosome, gene and exon-level views of CNVs, along with a tabular summarization of the exon-level CNVs.
Compared with other CNV-calling algorithms using data from a lymphoma exome-seq study, PatternCNV has higher sensitivity and specificity.
Availability and implementation: The software for PatternCNV is implemented using Perl and R, and can be used in Mac or Linux environments.
Software and user manual are available at http://bioinformaticstools.mayo.edu/research/patterncnv/, and R package at https://github.com/topsoil/patternCNV/.
Contact: Asmann.Yan@mayo.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 11, 2013; revised on April 22, 2014; accepted on May 22, 2014 1 INTRODUCTION DNA copy number variations (CNVs) are genomic structural changes that result in regional or chromosomal loss or gain of DNA copies (Hastings et al., 2009).
Owing to the significant roles in human diseases, various laboratory techniques have been developed to detect CNVs, including recently advanced massive parallel sequencing of whole genomes and coding exomes.
For exome-seq, it is commonly observed that coverage depths of short reads across regions vary, caused by different target capture efficiencies (Parla et al., 2011), as well as the differences in mappability of exons.
Such coverage variations impose substantial challenges for reliable CNV detection.
Most existing methods use a paired-sample approach, based on the intuitive assumption that somatic sample and its paired reference share similar coverage bias that can be cancelled out through pairing (Koboldt et al., 2012; Sathirapongsasuti et al., 2011).
Although this assumption approximately holds, it oversimplifies the problem with two limitations unaddressed: (i) The region-specific noise (coverage variability) of a local region is not accounted for, leading to amplified noise in log-ratio values of coverage between sample and the paired reference.
(ii) In the case of a missing or low-quality reference sample, CNV detection based on paired reference will be infeasible or have degraded accuracy/sensitivity.
A recent published method, FishingCNV, tried to address the second limitation by using the average of multiple reference samples as the denominators in log-ratio calculation, but did not address the regional noises in individual samples (the numerator), which led to false CNV calls (details in Supplementary Section S2.3).
Considering these issues, we proposed a novel method called PatternCNV, which summarizes overall consistent patterns of both depths and variability of exonic region coverage across samples, where patterns of coverage and variability are summarized using multiple normal or reference samples.
We observed that the same patterns only exist between samples prepared using the same version of exome capture kit.
During CNV detection, we compute the differences of observed coverage versus the common pattern, while penalizing regions associated with larger variability using a weighting scheme.
Further, wholegenome CNV can be interpolated from exon-level CNV using any third-party segmentation method, e.g.circular binary segmentation (Olshen et al., 2004).
The PatternCNV was implemented in two different versions: a Mac and Linux/Unix version, and an R package version.
We also developed a conversion tool to transform Binary version *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Due ; Koboldt, Zhang etal.
2012 1 2 2nd (ii) (CBS) of sequence Alignment/Map (BAM) format files to much smaller wiggle (WIG) format files (51% of BAM file size), which greatly speeds up pattern learning and CNV calculation.
When compared with other state-of-the-art CNV algorithms in a lymphoma case study, PatternCNV displayed higher resolution and greater sensitivity/specificity.
2 FEATURES 2.1 Input, output and major functions PatternCNV is divided into three major functional components: (i) BAM-to-WIG conversion for improved computational performance: a BAM2WIG converter using SAMtools (Li et al., 2009) and BEDtools (Quinlan and Hall 2010), which takes as input a BAM file, a file of Browser Extensible Data (BED) format defining exon regions and a second BED file for capture targets defined by the exome capture kit.
The outputs are WIG files with greatly reduced file sizes compared with BAM files; (ii) CNV detection: starting with WIG files, PatternCNV estimates the coverage and variability patterns from multiple reference samples and calculates CNVs relative to the pattern for all samples including the references; and (iii) CNV summary and visualization: this module outputs a detailed exon-level CNV summary file per sample, and provides several visualization options for viewing CNVs at the whole-genome level or chromosome level.
In addition, there are built-in QA/QC steps to detect sample outliers and batch effects.
Figure 1 displays the overall workflow of PatternCNV along with illustrative examples of program output.
2.2 Description of the PatternCNV algorithm Each exon is first divided into consecutive bins of user-defined size (e.g.10 base pairs).
To make the exon coverage of different samples comparable, log2-transformed RPKM (reads per kilobase per million total reads) is used to standardize the bin coverage.
Denoting xl as log2-transformed RPKM coverage of l-th bin in a given exon, the standard coverage of a bin without CNV is assumed to approximately follow a normal distribution Nl; l. The = l l=1;...;L and = ll=1;...;L are estimated from a pool of reference samples as the coverage and variability patterns.
For a bin with a copy number of C, the bin signal is calculated as r=log2C=2, xlNr+l; l. Hence, a bin-level CNV can be estimated as r l=xl l. Considering variability of bin coverage depending on its relative position in an exon or with respect to capture probe, we further smooth multiple bins within k-th exon (we denote related bin indices as l 2 Ek), leading to a maximum likelihood estimation: rk= P l2Ekwlxl l, where wl is designed to take variability of each bin into consideration (details of the statistical formulation are described in Supplementary Section S1).
2.3 Lymphoma case study We applied PatternCNV to a set of 15 germ linetumor pairs of diffuse large B-cell lymphoma exome-seq data (Lohr et al., 2012).
When comparing CNV results derived from exome-seq using PatternCNV with those calculated from SNP microarray data profiled on the same samples, the two sets of results largely correlate for large CNVs.
As expected, PatternCNV identified many small CNV regions at the single exon and/or multiple exon level (Supplementary Section S2.3) that the SNP array failed to detect owing to lack of probe coverage/density at the region.
In addtion, thanks to the digitalized dynamic range of read coverages, PatternCNV can differentiate high versus low amplifications, while microarrays are limited by the saturation of probe hybridization signal.
We compared PatternCNV with three other exome-seq-based CNV detection methods, ExomeCNV (Sathirapongsasuti et al., 2011), Varscan2 (Koboldt et al., 2012) and FishingCNV (Shi and Majewski 2013) using CNV detected by SNP microarrays as the ground truth.
PatternCNV displayed superior visual resolution and achieved better specifity and sensitivity when compared with the paired approaches used by ExomeCNV and Varscan2 (Supplementary Section S2.2), and had much less false positives compared with FishingCNV (Supplementary Section S2.3).
In several focused comparisons, we also saw an increased resolution of PatternCNV-based estimations compared with these two methods (Supplementary Section S2.1).
In situations where a reference sample had less reliable quality than its paired counterpart, we often observed dramatically reduced performance of both Varscan2 and ExomeCNV for CNV detection, but not PatternCNV (Supplementary Section S2.1).
This highlights the robustness of the pattern-based approach over conventional paried approaches.
FishingCNV uses a method of taking the average across normal samples, which is more similar to PatternCNV Fig.1.
PatternCNV workflow is demonstrated in the upper panel.
Examples of whole-genome and chromosome-level visulization are displayed in the bottom panel, along with Exon-level CNV summary table 2679 PatternCNV to 1 , to 2 3 In order w.r.t the (DLBCL)due vs. seq ,to PatternCNV to than the paired methods used by the other two tools.
However, a detailed comparison shows that FishingCNV has different data processing and CNV detection methods (Supplementary Section S2.3).
FishingCNVs principle component analysis (PCA) step over corrects batch effects and consequently removes CNV signals, resulting in false negative calls.
We recommend that the users do not perform the default PCA step of FishingCNV.
Moreover, it also oversimplifies average read-depth approach, producing an alarmingly high number of false-positive CNV calls (Supplementary Section S2.3).
In contrast, PatternCNVs novel use of both the weighted average read depth and coverage variability produces results that are superior and simpler to use by improving true positives and greatly reducing false-positive CNV calls.
3 DISCUSIONS AND CONCLUSIONS We introduce PatternCNV, a software package designed to focus on exon-level CNV detection from exome-seq data.
CNV estimate is based on coverage and variability patterns summarized from multiple reference samples.
The implemented algorithm uses WIG file format, which improves the runtime and space efficiency.
Several post-processing functions are included to facilitate interpretation, through visualization, segmentation and tabular summarization.
As demonstrated by the case study, we believe it is a useful utility for exome-seq studies where robust detection of germ line and/or somatic CNVs is of interest.
Funding: Support for this work was provided by Center for Individualized Medicine at Mayo Clinic and the NIH (P50 CA97274).
We thank Dr Todd R. Golub and colleagues at the Broad Institute, where the genomic data were generated.
Conflict of interest: none declared.
ABSTRACT Motivation: Comparative analyses of gene expression data from different species have become an important component of the study of molecular evolution.
Thus methods are needed to estimate evolutionary distances between expression profiles, as well as a neutral reference to estimate selective pressure.
Divergence between expression profiles of homologous genes is often calculated with Pearsons or Euclidean distance.
Neutral divergence is usually inferred from randomized data.
Despite being widely used, neither of these two steps has been well studied.
Here, we analyze these methods formally and on real data, highlight their limitations and propose improvements.
Results: It has been demonstrated that Pearsons distance, in contrast to Euclidean distance, leads to underestimation of the expression similarity between homologous genes with a conserved uniform pattern of expression.
Here, we first extend this study to genes with conserved, but specific pattern of expression.
Surprisingly, we find that both Pearsons and Euclidean distances used as a measure of expression similarity between genes depend on the expression specificity of those genes.
We also show that the Euclidean distance depends strongly on data normalization.
Next, we show that the randomization procedure that is widely used to estimate the rate of neutral evolution is biased when broadly expressed genes are abundant in the data.
To overcome this problem, we propose a novel randomization procedure that is unbiased with respect to expression profiles present in the datasets.
Applying our method to the mouse and human gene expression data suggests significant gene expression conservation between these species.
Contact: marc.robinson-rechavi@unil.ch; sven.bergmann@unil.ch Supplementary information: Supplementary data are available at Bioinformatics online.
Received on January 6, 2012; revised on April 11, 2012; accepted on May 1, 2012 1 INTRODUCTION Changes in gene expression have been suggested to underlie many differences in gene function or in phenotype.
More generally, To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the last two authors should be regarded as joint Last Authors.
expression is an important component of gene function, and studying the evolution of gene expression is a key step in evolutionary genomics.
While there has been a great deal of research concerning the primary treatment of expression data in general (see Garber et al.(2011), and Quackenbush (2002) for reviews), there has been little investigation into the methods used more specifically to quantify expression evolution (Pereira et al., 2009).
This can make it difficult to critically assess contradictory results, such as the reports that broadly expressed genes are more conserved (Khaitovich et al., 2005) or less conserved (Liao et al., 2010; Liao and Zhang, 2006b) than specifically expressed genes.
To assess whether and how much expression has been conserved between two orthologous genes by selection, we need an expectation for expression similarity under neutral evolution.
Thus, the estimation of gene expression conservation requires two components: (i) a measure of gene expression similarity; and (ii) the expected value of the divergence level under neutrality.
The two most common measures of similarity between expression profiles of orthologous genes are Pearsons correlation coefficient (Chan et al., 2009; Liao and Zhang, 2006a, b; Xing et al., 2007; Yanai et al., 2004; Yang et al., 2005; Zheng-Bradley et al., 2010) and Euclidean distance (Jordan et al., 2005; Liao and Zhang, 2006a; Yanai et al., 2004).
The results obtained with Pearsons and Euclidean distances have been reported to be poorly correlated (Liao and Zhang, 2006a; Pereira et al., 2009).
This poses the question which of these measures provides a better description of expression similarity.
It has been demonstrated that Pearsons correlation coefficient, in contrast to Euclidean distance, underestimates the expression similarity between orthologous genes with a conserved uniform pattern of expression.
In consequence, use of the Euclidean distance has been encouraged (Pereira et al., 2009).
For neutral evolution, one expects that similarity between expression profiles of orthologous genes gradually decreases with time.
For species that have diverged for sufficiently long time no detectable similarity in expression is expected to remain; this has been postulated to be the case between mouse and human (100 million years; Jordan et al., 2005).
It has been suggested that such large neutral divergence could be approximated by calculating the distance between expression profiles of randomly chosen pairs of genes from the species compared.
The standard approach used to generate random pairs of genes is to permute the orthology relationship between them (Chan et al., 2009; Liao and Zhang, 2006a, b; Xing et al., 2007; Zheng-Bradley et al., 2010).
The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
marc.robinson-rechavi@unil.ch sven.bergmann@unil.ch Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [11:05 18/6/2012 Bioinformatics-bts266.tex] Page: 1866 18651872 B.Piasecka et al.Here, we show formally and empirically that, in contrast to previous reports (Liao and Zhang, 2006a; Pereira et al., 2009), there exists a relationship between the Pearsons correlation coefficient and the Euclidean distance, which depends on the data normalization.
We also extend the previous study of Pereira et al.(2009) by considering more than just the uniform pattern of expression.
We demonstrate that in fact both distance measures depend on the expression specificity of analyzed genes.
Next, we discuss these observations in the context of the assessment of gene expression conservation.
We show that the comparison of expression profiles for randomly permuted gene pairs is biased when broadly expressed genes are abundant in the data, a distribution characteristic of many datasets.
To overcome this problem, we propose a novel procedure to generate random gene pairs.
This procedure is not biased by the over-or underrepresentation of any expression profile in the datasets.
Finally, we use our approach to provide clear evidence for constrained evolution of gene expression between mouse and human.
2 METHODS 2.1 Gene expression data We used the human and mouse gene expression data from the GNF Gene Expression Atlas of Su et al.(2004) as a case study.
This study was performed on the Affymetrix HG-U133A array as well as on the custom array GNF1H for human, and on the custom array GNF1M for mouse.
In total, expression profiles for 79 human and 61 mouse organs were measured, with 44 928 probe sets for human and 36 182 probe sets for mouse.
We only took into account organs belonging to the homologous organ groups (HOGs) defined in the Bgee database (Bastian et al., 2008).
Using the mapping available in the Bgee database we could connect 36 human organs and 30 mouse organs to 27 HOGs.
See Supplementary Table S1 for the list of HOGs and their corresponding organs.
Microarray data were normalized with the gcrma R package (Wu et al., 2004).
To assign the probe sets to their corresponding human or mouse genes we used the mapping available in Bgee.
We kept only probe sets which matched to a unique Ensembl gene.
A total of 15 121 probe sets corresponding to 13 853 mouse genes, and 23 920 probe sets corresponding to 15 338 human genes were found.
To estimate the expected values of distances for gene pairs with conserved expression patterns, we used data from replicated experiments, performed in each species.
Thus, for each probe set we had two vectors of values representing its expression over the organs.
The datasets contained 36 organs and 23 920 probe set pairs for human, and 30 organs and 15 121 probe set pairs for mouse.
The results of the study on mouse gene expression data are presented in the Supplementary Materials.
To study gene expression evolution between mouse and human we merged human and mouse organs into 27 HOGs.
For every probe set in each HOG the arithmetic mean of the gcRMA normalized expression values was calculated (each HOG was represented by at least two microarrays).
We used a subset of 8942 one-to-one orthologous gene pairs (see HumanMouse Orthologous Genes).
If the gene was matched by more than one probe set on the microarray, we randomly picked one probe set to represent that gene.
2.2 Humanmouse orthologous genes Homology information of human and mouse genes was retrieved from Ensembl release 55 (Hubbard et al., 2009), using BioMart (Smedley et al., 2009).
A total of 8942 pairs of humanmouse one-to-one orthologous genes had expression information in the datasets we used.
2.3 Normalization procedures For a given gene we consider a vector x of expression intensities xi across n different organs indexed by i=1,...n. The Manhattan normalization of x is calculated by dividing it by its L1 norm: ||x||1 = n i=1 |xi|.
In some studies (Liao and Zhang, 2006a; Pereira et al., 2009) this normalization is called relative abundance.
The Euclidean normalization of vector x is calculated by dividing the vector by its L2 norm: ||x||2 = n i=1 x2i.
Finally, we introduce a so-called z-like normalization of x which corresponds to the Euclidean normalization of x minus its mean value: zx = xx||xx||2.
2.4 Pearsons and Euclidean distances The Pearsons distance (dP) between two expression profiles is defined as 1r, where r = 1 n n i=1 (xi x)(yi y) sxsy = zTx zy = 1 n zxT zy (1) is the Pearsons correlation coefficient between vectors x and y.
Here the vector elements xi and yi are the expression signal intensities of two genes in the condition i, x and y are the sample means, sx and sy are the sample SDs.
zx and zy are the z-scores of vectors x and y.
The Euclidean distance (dE) between two expression profiles is defined as dE = n i=1 (xi yi)2 (2) with notations as for Equation (1).
2.5 Organ specificity of gene expression To measure the expression specificity of human genes we used the organ specificity index (Yanai et al., 2005).
The of a given gene with an expression vector x is defined as follows: = n i=1 (1 xi) n1 , (3) where xi = xi||x|| = xi max1in(xi).
The value of varies between 0 and 1, with higher values indicating higher organ specificity.
2.6-group composition To study the relation between dE and we used replicated expression data for human genes (36 organs, 23 920 probe sets).
We sorted the probe set pairs according to the organ specificity index [Equation (3)] of the first replicate, and we divided the probe set pairs into three-groups of equal size (e.g.the first group contained 1/3 of the probe set pairs with the first replicate having lowest ).
For each group we recorded the minimum and maximum value of the first replicate, and used these values to filter out probe sets with the two replicates having values from different groups.
The resulting-groups were of similar, but not equal, size (Table 1).
An alternative-group composition, with a more balanced distributions of values (first group containing genes with [0,0.2); second group with [0.2,0.6); and third group with  [0.6,1]) leads to unbalanced sizes of three groups.
Nevertheless, for both approaches the results are qualitatively the same (Supplementary Figs.
S6 and S7).
1866 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [11:05 18/6/2012 Bioinformatics-bts266.tex] Page: 1867 18651872 Correcting for the bias in evolutionary study of expression 2.7 Randomization procedures Changes in gene expression patterns between randomly chosen genes from two species have been suggested as an approximation for the result of neutral expression evolution (Jordan et al., 2005).
We used two different randomization procedures to create such sets of random gene pairs.
First, we permuted the gene order within replicates (or within species).
We refer to these as randomly permuted pairs.
Second, we performed what we refer to as-uniform sampling.
We first randomly chose an organ specificity index ( ), uniformly from the interval of (min, max), where min and max are the lowest and the highest values of the observed , respectively.
Next, we picked the gene with the value of closest to the randomly chosen within one dataset (i.e.within one replicate, or one species).
Then, independently, we repeated the procedure for the second dataset.
Thus, we obtained two randomly chosen genes which form a new random pair.
Repeating the procedure provides the-uniform random gene pairs.
3 RESULTS AND DISCUSSION 3.1 Correlation between Pearsons and Euclidean distances depends on data normalization To compare gene expression between species, over many different conditions, it is important to normalize the expression levels between the conditions to obtain a common scale between species.
This is distinct from the preprocessing normalization (within condition), which is typically done using methods such as LOESS (Yang et al., 2002) or gcRMA (Wu et al., 2004), and is not specific to interspecies evolutionary studies.
In the following, we only consider the impact of the between conditions normalization on the evolutionary comparisons.
We discuss three normalization procedures commonly used for evolutionary studies: Manhattan normalization [also referred to as relative abundance (Liao and Zhang, 2006a)], Euclidean normalization and z-like normalization (see Section 2.3 for mathematical definition of all three normalizations).
One can use any of these normalizations before calculating the Pearsons or Euclidean distance between two gene expression profiles.
However, the choice of normalization can affect the results.
Pearsons distance (dP) between two expression profiles remains the same, regardless of whether and how the data are normalized, and it ranges between 0 and 2.
The reason is that r is defined on the z-scores [see Equation (1) in Section 2.4], which are invariant with respect to linear transformation.
In contrast, the Euclidean distance between two expression profiles (dE) changes its value depending on the normalization used, even though the interval of possible dE values is always between 0 and 2.
The correlation between dP and dE is poor for Manhattan (Supplementary Fig.S1A; see also Liao and Zhang, 2006a; Pereira et al., 2009) and Euclidean normalizations (Supplementary Fig.S1B).
In contrast, z-like normalization leads to an interdependent relationship between dP and dE, defined by d2E =2dP (4) (see Theoretical Analysis in Supplementary Material, and Supplementary Fig.S1C).
As dP gives the same results for all three normalizations, and for z-like normalization it is equal to d2E/2, we focused on the Euclidean distance.
If not stated otherwise, the Euclidean distance was calculated for all three normalizations: Manhattan, Euclidean and z-like, referred to as dME , d E E and d Z E , respectively.
Table 1.
Composition of three-groups of human probe set (ps) pairs Organ specificity ( ) Number of ps pairs-group 1 0.003 0.117 6348-group 2 0.117< 0.295 5280-group 3 0.295< 0.879 6692 3.2 Commonly used measures of gene expression similarity depend on the organ specificity of the genes Intuitively, one might assume that the distance between two orthologous genes which have conserved the expression profile of their last common ancestor should be close to zero, and that this should hold regardless of the gene expression pattern.
To assess if this is indeed the case, we performed an empirical study.
We used human microarray data with the expression information from 36 different organs in two replicates (Su et al., 2004).
The replicates were used to simulate pairs of genes with conserved expression profiles.
We calculated the organ specificity index [Equation (3)] for each pair of replicates, and then divided them into three groups of similar size (see Section 2.6 for details).
The first two groups contained broadly expressed genes ( 0.295), and only the third group consisted of genes with more specific expression patterns ( >0.295; Table 1).
We measured the Euclidean distances (dME , d E E and d Z E ) for probe set pairs within each-group.
The resulting levels of expression similarity between replicates strongly depended on the organ specificity level.
Values of dME and d E E were significantly lower for broadly expressed genes than for organ-specific genes (p<1016, MannWhitney U test, Fig.1A and B; Supplementary Fig.S5A and B).
In contrast, values of dZE were significantly higher for broadly expressed genes than for organ-specific genes (p<1016, MannWhitney U test; Fig.1C; Supplementary Fig.S5C).
See Supplementary Figure S3 for the correlation analysis between the Euclidean distances and organ specificity index.
3.3 The rate of neutral expression evolution estimated with randomly permuted gene pairs depends on the organ specificity of the genes The rate of neutral expression evolution is typically approximated by calculating the distance between expression profiles of randomly paired genes.
The random choice of the genes is assumed to remove any similarity between them (Jordan et al., 2005).
The standard approach to generate random gene pairs is to permute the ortholog relationship between the genes in the datasets.
We created random probe set pairs by permuting the probe set order within each of the three-groups separately, and we then calculated the Euclidean distances (dME , d E E and d Z E ) between their expression profiles.
We found that dME and d E E were significantly lower for random pairs from the first-group, than for random pairs from the third-group (p<1016, Mann-Whitney U test; Fig.1A and B; Supplementary Fig.S5A and B).
This is because the first-group consisted of broadly expressed genes.
Consequently, even the randomly matched probe set pairs tended to have similar expression patterns and thus low distances.
In contrast, the third-group consisted of genes with 1867 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [11:05 18/6/2012 Bioinformatics-bts266.tex] Page: 1868 18651872 B.Piasecka et al.A B C Fig.1.
The distribution of expression similarity between human replicates depends on their organ specificity.
(A) dME and (B) d E E are significantly lower for broadly expressed genes (group 1) than for organ-specific genes (group 3).
For randomly permuted gene pairs dME and d E E also differ between the three-groups.
They are significantly lower for random pairs in group 1 than in group 3.
(C) dZE is significantly higher for broadly expressed genes (group 1) than for organ-specific genes (group 3).
dZE for randomly permuted pairs is high in all three groups, even in the first-group, where random pairs consist of two broadly expressed genes (this is a consequence of low r for uniformly expressed genes).
Note that the scale of the x-axis differs strongly between graphs.
1868 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [11:05 18/6/2012 Bioinformatics-bts266.tex] Page: 1869 18651872 Correcting for the bias in evolutionary study of expression more specific expression patterns, and so the random pairs were truly different.
dZE between random pairs was not affected by organ specificity, in the sense that in all three-groups the median dZE was around 1.4 (Fig.1C; Supplementary Fig.S5C).
Values of dZE were high even in the first-group, although it consisted of random pairs with similar, broad patterns of expression.
The reason is that dZE = 2(1r) is a decreasing function of r, which for broadly expressed gene pairs reflects mainly the noise of the measurement and is close to 0 (for details see Pereira et al., 2009 and Supplementary Fig.S2).
Thus, random gene pairs from the first-group tend to have high dZE values (around 2).
3.4 A large fraction of broadly expressed genes leads to an underestimation of expression conservation Our analysis shows that if the fraction of broadly expressed genes is large, the level of gene expression conservation is likely to be underestimated.
This is especially important if we consider the fact that housekeeping genes (broadly expressed) are more frequent than organ-specific genes (Ramskld et al., 2009).
We found such skewed distributions not only in the human data considered here (Fig.3A), but also in several other datasets, e.g.most mouse genes are broadly expressed over different organs, most Arabidopsis genes are broadly expressed over different light conditions, and most zebrafish genes are broadly expressed over different developmental stages (Supplementary Fig.S4).
To illustrate the extent to which the abundance of broadly expressed genes affects measures of gene expression conservation, we re-analyzed all the human probe set pairs, without dividing them into-groups.
We created random probe set pairs by permuting the probe set order within both replicates, and we calculated the Euclidean distances (dME , d E E and d Z E ) both for the pairs of replicates and for the random pairs.
Ideally, one would expect to detect very high similarity between replicates, and very low similarity between random pairs.
For Manhattan and Euclidean normalizations, distances for most human random pairs were very small, indistinguishable from the distances between replicates (Fig.2A and B; Supplementary Fig.S8A and B).
This contradicts the assumption that differences between randomly paired genes are to approximate well the rate of neutral divergence, with very low similarity (i.e.high distance) expected (Jordan et al., 2005).
For the z-like normalization, distances between random pairs were high, which is consistent with the assumption of pseudo-neutrality (Jordan et al., 2005).
However the dZE values for the replicates were similarly high (Fig.2C; Supplementary Fig.S8C), whereas they are expected to be low.
Thus, the presence of numerous broadly expressed genes causes systematically low values of dME and d E E between randomly paired genes, and systematically high values of dZE between conserved gene pairs.
The first is a consequence of the fact that it is easier to randomly choose two broadly expressed genes, and thus to get a low value of dME or d E E. The second is a consequence of low values of r for uniformly expressed genes, leading to the high values of dZE (as discussed in the Section 3.3).
In all cases, the level of gene expression conservation is underestimated.
Although we show this effect using a specific set of human microarray data, our conclusions are very general and hold for any study in which a significant fraction of the genes is uniformly expressed over conditions (see Fig.S2 and its caption for a mathematical explanation).
3.5 An alternative construction of random gene pairs improves the estimation of expression conservation To overcome the limitation of using randomly permuted gene pairs to estimate the expression divergence under neutrality, we propose a new procedure to create random gene pairs.
This procedure is unbiased regardless of over-or underrepresentation of any expression profiles in the datasets.
Consequently, it provides a better approximation of the expression divergence under neutral evolution between distant species.
To generate a single random pair of genes, one randomly chooses two expression specificity values, 1 and 2, uniformly from the interval of (min, max), where min and max are the lowest and the highest values of the observed , respectively.
Next, one picks the two genes from the two datasets that have the closest values to 1 and 2, respectively.
The resulting pairs of genes have the two values uniformly distributed, and not biased as for randomly permuted gene pairs (Fig.3B and C).
We applied our new procedure 23 920 times to create as many random probe set pairs for human datasets.
Then, we calculated the Euclidean distances (dME , d E E , and d Z E ) both for replicates and random probe set pairs.
We found that, relative to classical randomly permuted pairs, the distribution of dEE and d M E for-uniform random pairs differs strongly from that for replicates (Fig.2A and B), with a high frequency of large distance values, as expected for very divergent pairs.
Of note, dME and d E E give the same shape of distribution (Figs.
1A and B, and 2A and B).
While both of these measures could be combined with-uniform sampling to estimate gene expression conservation, for mathematical consistency we prefer the use of dEE.
The estimation of gene expression conservation with dZE cannot be corrected by creating the set of random gene pairs differently, because dZE varies significantly with organ specificity for replicates, i.e.for conserved genes, and not for random gene pairs.
Thus, we do not recommend using dZE , and consequently the Pearsons correlation coefficient, in any study which aims to detect similarity between genes expressed uniformly over all conditions.
Of note, neither the standard procedure used to generate random pairs, nor our new proposed approach takes into consideration the time passed since the divergence of two organisms.
Therefore, the estimated neutral divergence will be the same for closely related species (e.g.human and chimp) and more distant species (e.g.human and mouse).
3.6 Results of the comparative study of human and mouse gene expression differ strongly according to the choice of randomization method To demonstrate the importance of our novel approach, we investigated how much evidence of selectively constrained gene expression evolution we can detect between human and mouse.
We selected 8942 one-to-one orthologous gene pairs from the human and mouse datasets (Su et al., 2004).
We created two sets of random gene pairs, using both random permutation and the procedure of-uniform sampling, and we calculated the Euclidean distance (dEE ) for orthologous gene pairs and for both sets of random pairs (see Fig.S9 for analogous analysis with dME ).
If the d E E value for 1869 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [11:05 18/6/2012 Bioinformatics-bts266.tex] Page: 1870 18651872 B.Piasecka et al.A B C Fig.2.
Overrepresentation of broadly expressed human genes causes underestimation of the conservation of expression when randomly permuted pairs are used to approximate the neutral evolution rate.
(A, B) For most randomly permuted pairs (grey) the distance (dME and d E E ) is small, indistinguishable from the distances between replicates (green).
For-uniform random pairs (blue) dEE and d M E are higher, which is more consistent with the assumption about neutral evolution (Jordan et al., 2005).
(C) dZE is high both for randomly permuted gene pairs and for the group of replicates.
The distribution of d Z E does not change with the new random pairs set.
Fig.3.
Random gene pairs have their values differently distributed depending on the randomization procedure used.
(A) distribution for human replicates.
The pairs are distributed along the diagonal, which is expected for replicates.
(B) distribution for randomly permuted gene pairs.
The pairs are biased towards low values, which are the most frequent values in human datasets.
(C) distribution for-uniform random pairs.
The pairs are uniformly distributed, and not biased towards the low values.
a humanmouse orthologous gene pair is smaller than the fifth percentile of dEE for randomly paired genes, there is some evidence that the expression evolution of this pair has been constrained (Liao and Zhang, 2006a).
Using randomly permuted gene pairs did not provide clear evidence for constrained evolution (Fig.4).
Only 8% of orthologous pairs were identified to have a conserved expression pattern, which was close to the random expectation of 5%.
In contrast,with-uniform random pairs, 29% of orthologous genes were identified to have conserved expression (Fig.4).
The number of detected genes with conserved expression pattern may seem surprisingly low in comparison to Liao and Zhang (2006a), who reported that as much as 84% of genes showed conserved expression between human and mouse.
However, we note that Liao and Zhang (2006a) used two different metrics to calculate the distance between orthologous genes and between randomly paired genes the so called net distance and the Euclidean distance, respectively.
We show that this inconsistency caused an overestimation of the expression conservation between human and mouse (see Supplementary Materials and Supplementary Fig.S10).
Consequently, we believe that correcting for the randomization process yields more accurate results than a one-sided correction of the distance.
We are aware that the alternative way of creating random gene pairs proposed in this article has some weaknesses, such as visible artificial peaks in the dEE distribution (Fig.4), which are the consequence of the non-uniform distribution of between 0 and 1.
This is because with the-uniform sampling one chooses the genes with less frequent values more often than genes with more frequent 1870 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [11:05 18/6/2012 Bioinformatics-bts266.tex] Page: 1871 18651872 Correcting for the bias in evolutionary study of expression Fig.4.
The choice of the randomization method changes the conclusions about gene expression evolution between mouse and human.
There is no clear evidence for constrained evolution if we compare the distribution of dEE for orthologous (green) and randomly permuted gene pairs (grey).
Whereas, comparison of dEE distribution for orthologous (green) and-uniform random pairs (blue) suggest that expression evolution is far from neutral.
values.
For example here, the number of narrowly expressed genes was increased at the expense of decreasing the number of broadly expressed genes.
Consequently, when only a few genes have a value in some non-negligible range, these few genes might repeat many times in the randomized set, and discrete effects may manifest themselves causing artificial peaks.
Note that the peaks would disappear if values were uniformly distributed between 0 and 1, but then there would be no need for-uniform sampling of gene pairs at all.
Note also that the peaks do not affect the analysis, as they do not change the overall shape of the distribution of distance values between the randomized gene pairs (Fig.4).
Finally, one may argue that the-uniform sampling contradicts the very purpose of randomization because it makes a probability of choosing a gene higher, if its value is underrepresented in the dataset.
But the aim of the set of randomized gene pairs is not to be just random, but to display maximal divergence between gene pairs, i.e.to simulate the neutral evolution defined in Jordan et al.(2005).
In contrast to the standard approach, the-uniform sampling makes the distribution of distance values between gene pairs actually independent of the distribution observed in the analyzed dataset.
Thus, we believe that the distance between-uniform random gene pairs approximates better a large neutral divergence.
4 CONCLUSIONS The Euclidean distance should be used with caution as an estimator of gene expression conservation because it varies as a function of expression specificity.
Our results strongly suggest that to assess whether gene expression evolves neutrally, one should use dEE (Euclidean distance preceded by Euclidean normalization) and compare its distribution for orthologous and-uniform random pairs.
Importantly, we validated this approach on real data, and recovered clear evidence for gene expression conservation between mouse and human.
Previous small differences reported between real and random gene pairs were likely caused by the way the random pairs were constructed (Liao and Zhang, 2006a, b).
Although in this study we applied our approach to microarray data analysis, the issues highlighted here are also relevant to data acquired with RNA-seq technology (Mortazavi et al., 2008).
We would like to emphasize that while it is possible to verify whether the expression of a given set of genes was under selective pressure, there is no straightforward way to compare the strength of selection acting on two groups of genes with different expression patterns.
Indeed, if we compare a group of broadly expressed genes with a group of narrowly expressed genes, with similar high conservation of expression, the latter will always have higher dEE values (and lower dZE values).
This methodological problem suggests a need to re-interpret results from previous evolutionary studies comparing the evolution of broadly and narrowly expressed genes.
In particular, studies which have reported higher conservation of organ-specific genes (Liao et al., 2010; Liao and Zhang, 2006b; Movahedi et al., 2011) could have been biased by the fact of using the Pearsons correlation coefficient (equivalent to dZE ) as a measure of conservation.
In this article, we thoroughly analyzed, formally and experimentally, the common measures of expression conservation, and we showed the superiority of the Euclidean distance paired with the Euclidean normalization.
We also highlighted the limitation of using randomly permuted pairs to approximate neutrally evolving genes, and proposed a new methodology to better estimate the rate of neutral evolution.
With the increase of expression data for many species, our work is likely to become very useful for evolutionary studies of gene expression.
ACKNOWLEDGEMENTS The authors thank P. Lichocki for fruitful discussion.
The authors also thank F. Bastian, N. Galtier and O. Riba-Grognuz for critical comments on the manuscript.
Funding: Etat de Vaud; Swiss National Science Foundation [ProDoc grant 1206624/1]; Swiss Institute of Bioinformatics [to S.B.]
Conflict of Interest: none declared.
ABSTRACT Summary: MiSearch is an adaptive biomedical literature search tool that ranks citations based on a statistical model for the likelihood that a user will choose to view them.
Citation selections are automatically acquired during browsing and used to dynamically update a likelihood model that includes authorship, journal and PubMed indexing information.
The user can optionally elect to include or exclude specific features and vary the importance of timeliness in the ranking.
Availability: http://misearch.ncibi.org Contact: dstates@umich.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION With the rapidly increasing volume of publications in the biomedical literature, finding relevant work is an ever more difficult challenge.
General solutions to the literature search problem are difficult because biomedical science is very diverse; the articles most relevant to one reader may not be relevant to another.
Relevance feedback is a well-established technique to improve performance in information retrieval (Rocchio, 1971; Salton, 1971; Salton and Buckley, 1990).
Feedback may be acquired explicitly by asking users to rate retrieval results.
However, many users find this task burdensome.
Even for widely deployed search engines such as Excite, where relevance feedback is available and effective, it is rarely used (Spink et al., 2000).
An alternative is to acquire feedback implicitly by observing user behavior (Kelly and Teevan, 2003).
MiSearch is an adaptive literature search tool using implicit relevance feedback that helps users rapidly find PubMed citations relevant to their specific interests.
MiSearch automatically saves information on citations a reader has viewed during search and browsing, and uses this information to build a statistical profile describing the readers choices.
This profile is used to rank the results of future searches, placing those articles that this reader is most likely to view at the top of the list.
In effect, MiSearch is using query expansion with probabilistic weighting of terms derived from the implicitly defined relevant document set.
Using this implicit feedback approach is effective and improves the relevance ranking of bibliographic search results.
The NCBI Entrez search tool is widely used and alternative interfaces have been developed allowing users to manually vary the weight of different features in determining relevance (Muin et al., 2005) and to reformulate and refine Boolean queries (Bernstam, 2001; Ding et al., 2006), but unlike MiSearch, these tools do not adapt to user behavior.
2 METHODS 2.1 Ranking algorithm MiSearch records the users search history and the history of documents selected for viewing using an HTTP redirect mechanism.
Four domains are considered: authors (Au), journal (Jl), MeSH terms (Me) and substance names (Sn) indexed by NLM (Nelson et al., 2004).
Each domain is described using a statistical profile of term use.
The frequency fu(t) of term t occurring in citations that user, u, has selected for viewing is defined as fu t  Nu t  fP t  Nu 1 where Nu(t) is the count of citations indexed with term t that were viewed by the user,Nu is the total number of citations viewed by the user and fP(t) is the absolute frequency with which papers indexed with term t occur in the entire PubMed database.
The pseudo counts smooth behavior when the profile has few citations and avoid division by zero if a specific term does not occur in the citations selected by a user.
If no feedback is available for the user (Nu 0), then fu(t) is fP(t).
When the user has viewed many articles, fu(t) asymptotically approaches Nu(t/Nu).
MiSearch uses the PubMed eUtils interface to query the PubMed database and ranks citations based on a log likelihood score, S, S X DAu, Jl,Me, Sn SD  T T0 where SD are log likelihood scores for each domain and (TT0) is term weighting the timeliness of an article.
T is the date of publication for a citation, T0 is a reference date (January 1, 2000) and is an adjustable factor that allows the user to vary the weight given to timeliness in ranking citations.
The score SD for domain D is calculated for each citation as a log likelihood ratio that the term t associated with this citation occur in citations viewed by the user, fu, relative to their frequency in all of PubMed, fP SD X t2Term log fu t fP t  *To whom correspondence should be addressed.
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
A positive score indicates that a citation is more likely to be viewed by the user, and a negative score indicates that a citation is less likely to be selected for viewing.
2.2 Implementation MiSearch is implemented in two components, a PHP script running on an Apache web server that generates forms, dispatches search requests to NCBI Entrez and communicates with a local citation datbase, and a relational database server that stores both the PubMed corpus and user search histories.
Ranking is implemented as an SQL stored procedure.
Users can label profiles with a string and can define several different profiles for different search tasks.
3 RESULTS Figure 1 illustrates the effect of adaptive re-ranking of citation searches based on a profile created from the publication of one author (AWL).
Dr Lees research focuses on signal transduction downstream of the CSF-1 receptor (CSF1R).
Using a profile based on viewing Dr Lees publications (top panel), MiSearch ranks two of Dr Lees publications and a third recent and highly relevant publication at the highest relevance in a PubMed search for CSF1R.
In contrast, without adaptive ranking (lower panel), the publications are ranked in reverse chronological order with only one moderately relevant publication highly ranked and that citation is in a journal that Dr Lee does not frequently read.
Because MiSearch ranks citations using a statistical profile, the user does not need to explicitly specify the ranking criteria.
MiSearch thus complements Boolean search strategies.
In Boolean searches, a relevant article may be missed if the user specifies an overly restrictive Boolean filter and the citation uses a synonym for a term not specified in the search query.
Using MiSearch, a broader Boolean query can be performed.
The MiSearch relevance ranking places the citations most likely to be of interest to this user at the top of the list and avoids the need to view large numbers of citations.
Further, a reader may not be aware that all the citations they are viewing contain a common term such as reference to a chemical substance.
The MiSearch statistical profile will automatically capture this information and rank other citations, mentioning this term more highly.
Optionally, the user can request that MiSearch use the results of the query itself to construct the profile.
This results in a ranking where the citations sharing features with the largest number of other citations in the result set are ranked highly.
In this view, citations that are most central to the topic rank highly while citations peripheral to the topic rank lower on the list.
For example, in a search about a gene, citations where the gene is the major focus of the paper will be at the top of the query profile ranking while citations that only mention the gene in passing will rank lower on the list.
Query profile mode is invoked by using query or username query as the username.
3.1 Evaluation To assess the effectiveness of implicit relevance feedback, we use a cross validation approach.
A training profile is constructed by sampling from the citations selected as relevant for viewing by a user.
The test set consists of the remaining citations selected by this user.
Typical results are shown in Figure 2.
Increasing the number of citations in the training set Fig.1.
Compares the results of a relevance ranked citation search (top) with the same search ranked in reverse chronological order (bottom).
The top three articles in each ranking are shown.
1 2 3 60 50 40 30 20 10 ra nk training examples Fig.2.
Shown in the figure are the ranking for a representative search.
The query Xist Tsix that returns 66 articles in PubMed.
The user selected four articles from this list related to epigenetic regulation of X inactivation.
Leave one out cross validation of the relevance ranks were computed for training samples containing 1, 2 or 3 of the citations in the users profile.
The circles on the left show where each article appeared in the PubMed/Entrez ranking.
The on the right show the ranking of each article based on the MiSearch algorithm.
MiSearch adaptive pubMed search tool 975 progressively improves the ranking of the test citations.
In Supplementary Data, we compare the performance of MiSearch to relevance feedback using the Entrez related articles/feature.
The improved results in cross validation demonstrate that users are consistent in the articles that they select for viewing and that these selections are an effective implicit source of relevance feedback yielding improved biomedical literature search performance.
4 DISCUSSION Automated collection of implicit relevant feedback information gathered using a click-through mechanism improve bibliographic search performance.
Users find this interface intuitive and easy to use.
Relevance feedback is applied by simply rerunning a query periodically during normal browsing.
We find that response time is a critical factor in user acceptance of a relevance feedback system.
While more sophisticated algorithms for classification and ranking based on relevance feedback have been proposed, the likelihood ratios used in MiSearch are effective and easily implemented within an RDBMS.
This avoids the need to move large data sets in and out of the database server and improves user response time.
We encountered a number of issues in implementing MiSearch.
Optimizing the performance of the relevance feedback system to work with small numbers of events is important.
In a typical biomedical literature search task, users often view fewer than a dozen articles.
Many author names are not unique.
In the MiSearch formulation, such author names are not resolved, but are expected to occur with higher frequency in the reference corpus and thus provide less information in ranking articles.
Documents vary greatly in the number of authors, MeSH terms and substance names applied to them.
It is thus necessary to rank articles based on variable number of terms in these domains.
We attempt to avoid bias in the formulation of the scores and by using pseudo counts where zero term counts give zero scores.
The MiSearch ranking is necessarily dependent on the NLM indexing processing.
We are developing ways to base retrieval on automatically scored name, substance and MeSH headings, so that we can process documents such as web pages or journal articles that are not indexed by NLM.
Response time is an issue, particularly with very large result sets.
The major performance bottleneck is that the system needs to calculate usage frequencies for every term appearing in every document in the result set.
This is done on the fly so that rankings reflect the users most recent search and retrieval behavior, but the reference term frequencies are pre-computed for all of PubMed.
This is a compromise.
For the task of ranking documents a user is likely to select, the reference corpus would ideally be the collection of documents that the users decided not to view among the citations that their queries had retrieve from Entrez.
Implementing this would, however, be computationally intensive.
ACKNOWLEDGEMENTS We thank Dr Angel W. Lee for her advice and comments during the course of preparing this manuscript.
This work was supported in part by grants R01 LM008106 and U54 DA021519 from NIH.
Conflict of Interest: none declared.
ABSTRACT Motivation: Bayesian methods are widely used in many different areas of research.
Recently, it has become a very popular tool for biological network reconstruction, due to its ability to handle noisy data.
Even though there are many software packages allowing for Bayesian network reconstruction, only few of them are freely available to researchers.
Moreover, they usually require at least basic programming abilities, which restricts their potential user base.
Our goal was to provide software which would be freely available, efficient and usable to non-programmers.
Results: We present a BNFinder software, which allows for Bayesian network reconstruction from experimental data.
It supports dynamic Bayesian networks and, if the variables are partially ordered, also static Bayesian networks.
The main advantage of BNFinder is the use exact algorithm, which is at the same time very efficient (polynomial with respect to the number of observations).
Availability: The software, supplementary information and manual is available at http://bioputer.mimuw.edu.pl/software/bnf/.
Besides the availability of the standalone application and the source code, we have developed a web interface to BNFinder application running on our servers.
A web tutorial on different options of BNFinder is also available.
Contact: dojer@mimuw.edu.pl 1 INTRODUCTION Computational methods of Bayesian network inference are very popular in many different areas of bioinformatics and other fields of science.
Examples include: regulatory network reconstruction (Dojer et al., 2006; Husmeier, 2003) where nodes represent genes and edges represent statistical dependencies which may indicate regulatory interactions; predicting gene expression from promoter sequence (Beer and Tavazoie, 2004; Segal et al., 2003) where edges lead from promoter features (motif occurrences, their positions, etc.)
to expression patterns (affinity to overlapping expression clusters); neural signal transduction analysis (Smith et al., 2006) where network topology mimics the topology of connections between different parts of the brain and many others.
Despite differences in the interpretation of network structure, the methodology of these studies is remarkably similar [see, Needham et al.(2007) for overview and further examples].
We aim to provide new software To whom correspondence should be addressed.
which could be used for different applications of Bayesian network reconstruction.
Most programs learning Bayesian networks from data are based on heuristic search techniques of identifying good models.
This is due to a number of discouraging complexity results (Chickering, 1996; Chickering et al., 2004; Meek, 2001) showing that, without restrictive assumptions, learning Bayesian networks from data is NP-hard with respect to the number of network vertices.
On the other hand, the known exact algorithms learn the structure of optimal networks having up to 2040 vertices (Ott et al., 2004).
In an extensive comparison, Murphy (2007) lists over 50 software packages available for different applications of Bayesian networks.
However, if one is searching for a free software able to infer the structure of static and dynamic Bayesian networks from data there are only two such applications: Banjo package (Smith et al., 2006): Bayesian ANalysis with Java Objects, Bayes Net Toolbox (Murphy, 2002) for Matlab with an extension for dynamic Bayesian networks inference using MCMC (Husmeier, 2003).
Both of these software packages use heuristic search algorithms to find the best scoring network topology in a vast space of possible directed graphs, usually with some constraints on the maximal vertex in-degree.
2 METHODS For a thorough treatment of the contents of the present section, we refer the reader to Supplementary Materials.
The BNFinder program is based on a novel polynomial-time algorithm for learning an optimal Bayesian network structure (Dojer, 2006).
The algorithm was designed to save reasonable speed and perfect quality of learning in a wide class of problems occurring in the computational molecular biology.
It works under the assumption that there is no need to examine the acyclicity of the graph, which is satisfied in the following cases: When dealing with dynamic Bayesian networks, a dynamic Bayesian network describes stochastic evolution of a set of random variables over discretized time.
Therefore, conditional distributions refer to random variables in neighboring time points and the graph is always acyclic.
In case of static Bayesian networks, the set of possible network structures must be restricted.
BNFinder lets the user divide the set of variables into an ordered set of disjoint subsets of variables, where edges can only lead from upstream to downstream subsets.
If such 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
ordering is not known beforehand, one can try to run BNFinder with different orderings and choose a network with the best overall score.
BNFinder learns optimal networks with respect to two generally used scoring criteria: BayesianDirichlet equivalence (BDe) and minimal description length (MDL).
The (default) BDe score originates from Bayesian statistics and corresponds to the posterior probability of a network-given data.
The MDL score originates from information theory and corresponds to the length of the data compressed with the compression model derived from the network structure.
It also has a statistical interpretation as an approximation of the posterior probability.
The algorithm works in polynomial time for both scores, but computations with the MDL are faster, especially for large datasets.
However, we recommend using the BDe score due to its exactness in the statistical interpretation.
Both MDL and BDe scores were originally designed for discrete variables.
Continuous variables are handled with corresponding scores, derived under the assumption that conditional distributions belong to a family of Gaussian mixtures.
BNFinder may learn either dynamic Bayesian networks (from time series data) or static ones (from independent experiment data).
In the second case it is necessary to specify constraints on the networks structure forcing its acyclicity.
A special treatment is required for experiments, in which the values of some variables were perturbed (e.g.knockout experiments).
Since perturbations change the structure of interactions, learning procedures have to use data selectively.
BNFinder handles perturbations in the way following Dojer et al.(2006), i.e.for scoring sets of parents of a variable v, it takes into account only the experiments where v was not perturbed.
A prior distribution on the network structure may be specified through assigning weights to potential variable interactions in the way following Tamada et al.(2003).
Moreover, the size of regulator sets of each variable may be bounded to a given number and the spaces of possible conditional probability distributions of selected variables may be restricted to noisy-and or noisy-or distributions.
There are important biological applications of Bayesian networks, in which usually the amount of learning data is small relative to the network size (e.g.reconstruction of gene regulatory networks from microarray data).
Typically in such cases suboptimal models explain the data nearly as well as the optimal (highest scoring) one.
For this reason, Friedman and Koller (2003) propose to pay attention for network features frequently appearing in suboptimal networks.
Following this idea, BNFinder splits a potential network structure into independently learned features, each one composed of a vertex and its parent set.
For each vertex BNFinder returns as an output a user-specified number of suboptimal parents features with their relative posterior probabilities.
Setting this parameter to 1 causes BNFinder to learn the optimal network structure composed of the highest scoring features.
Otherwise returned features constitute a class of suboptimal networks.
Output may be written in a few formats, supported in various graph and Bayesian network applications.
3 IMPLEMENTATION The BNFinder software is implemented in the Python programming language so it can be installed and run on all popular operating systems.
The only requirement is the availability of a recent version (>2.4) of the Python interpreter.
Detailed installation instructions can be found on the Supplementary Web Page.
Besides of the stand-alone version of BNFinder we have made a publicly available web server which allows for using BNFinder running on our servers on users data.
The server uses a very simple web form for input and sends the results to the e-mail address provided.
To save the resources, we have limited the web version to handle at most 20 variables and 500 observations.
In order to judge the performance of our software, we have compared it to the Banjo library (Smith et al., 2006).
As a realistic dataset, we have chosen the dataset attached as an example to the Banjo package, consisting of 20 variables and 2000 observations, published by Smith et al.(2006).
The authors search for a dynamic Bayesian network with an in-degree of all vertices not larger than 5.
It should be noted, that the number of such networks is extremely large (((2019181716)/(12345))20 6.41084).
Even though Banjo is able to analyze approximately 1 million networks per minute on a single CPU it would take it more than 1070 years to search through all possible networks.
Thanks to the new algorithm (Dojer, 2006) our method is able to find the correct topology for the same dataset in a few hours on the same computer.
ACKNOWLEDGEMENTS The computational resources were provided by CoE BioExploratorium project: WKP 1/1.4.3/1/2004/44/44/115.
Funding: Polish Ministry of Science and Higher Education (No.
PBZ-MNiI-2/1/2005 and 3 T11F 021 28, partial); Foundation for Polish Science (to B.W.).
Conflict of Interest: none declared.
ABSTRACT Summary: The variant call format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations.
VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome.
The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project.
VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing and also provides a general Perl API.
Availability: http://vcftools.sourceforge.net Contact: rd@sanger.ac.uk Received on October 28, 2010; revised on May 4, 2011; accepted on May 28, 2011 1 INTRODUCTION One of the main uses of next-generation sequencing is to discover variation among large populations of related samples.
Recently, a format for storing next-generation read alignments has been standardized by the SAM/BAM file format specification (Li et al., 2009).
This has significantly improved the interoperability of nextgeneration tools for alignment, visualization and variant calling.
We propose the variant call format (VCF) as a standardized format for storing the most prevalent types of sequence variation, including SNPs, indels and larger structural variants, together with rich annotations.
The format was developed with the primary intention to represent human genetic variation, but its use is not restricted to diploid genomes and can be used in different contexts as well.
Its flexibility and user extensibility allows representation of a wide variety of genomic variation with respect to a single reference sequence.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.Although generic feature format (GFF) has recently been extended to standardize storage of variant information in genome variant format (GVF) (Reese et al., 2010), this is not tailored for storing information across many samples.
We have designed the VCF format to be scalable so as to encompass millions of sites with genotype data and annotations from thousands of samples.
We have adopted a textual encoding, with complementary indexing, to allow easy generation of the files while maintaining fast data access.
In this article, we present an overview of the VCF and briefly introduce the companion VCFtools software package.
A detailed format specification and the complete documentation of VCFtools are available at the VCFtools web site.
2 METHODS 2.1 The VCF 2.1.1 Overview of the VCF A VCF file (Fig.1a) consists of a header section and a data section.
The header contains an arbitrary number of metainformation lines, each starting with characters ##, and a TAB delimited field definition line, starting with a single #character.
The meta-information header lines provide a standardized description of tags and annotations used in the data section.
The use of meta-information allows the information stored within a VCF file to be tailored to the dataset in question.
It can be also used to provide information about the means of file creation, date of creation, version of the reference sequence, software used and any other information relevant to the history of the file.
The field definition line names eight mandatory columns, corresponding to data columns representing the chromosome (CHROM), a 1-based position of the start of the variant (POS), unique identifiers of the variant (ID), the reference allele (REF), a comma separated list of alternate non-reference alleles (ALT), a phred-scaled quality score (QUAL), site filtering information (FILTER) and a semicolon separated list of additional, user extensible annotation (INFO).
In addition, if samples are present in the file, the mandatory header columns are followed by a FORMAT column and an arbitrary number of sample IDs that define the samples included in the VCF file.
The FORMAT column is used to define the information contained within each subsequent genotype column, which consists of a colon separated list of fields.
For example, the FORMAT field GT:GQ:DP in the fourth data entry of Figure 1a indicates that the subsequent entries contain information regarding the genotype, genotype quality and read depth for each sample.
All data lines are TAB delimited and the number of fields in each data line must match the number of fields in the header line.
It is strongly recommended that all annotation tags used are declared in the VCF header section.
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:26 5/7/2011 Bioinformatics-btr330.tex] Page: 2157 21562158 Variant call format (a) (b) (c) (d) (e) (f) (g) Fig.1.
(a) Example of valid VCF.
The header lines ##fileformat and #CHROM are mandatory, the rest is optional but strongly recommended.
Each line of the body describes variants present in the sampled population at one genomic position or region.
All alternate alleles are listed in the ALT column and referenced from the genotype fields as 1-based indexes to this list; the reference haplotype is designated as 0.
For multiploid data, the separator indicates whether the data are phased (|) or unphased (/).
Thus, the two alleles C and G at the positions 2 and 5 in this figure occur on the same chromosome in SAMPLE1.
The first data line shows an example of a deletion (present in SAMPLE1) and a replacement of two bases by another base (SAMPLE2); the second line shows a SNP and an insertion; the third a SNP; the fourth a large structural variant described by the annotation in the INFO column, the coordinate is that of the base before the variant.
(bf ) Alignments and VCF representations of different sequence variants: SNP, insertion, deletion, replacement, and a large deletion.
The REF columns shows the reference bases replaced by the haplotype in the ALT column.
The coordinate refers to the first reference base.
(g) Users are advised to use simplest representation possible and lowest coordinate in cases where the position is ambiguous.
2.1.2 Conventions and reserved keywords The VCF specification includes several common keywords with standardized meaning.
The following list gives some examples of the reserved tags.
Genotype columns: GT, genotype, encodes alleles as numbers: 0 for the reference allele, 1 for the first allele listed in ALT column, 2 for the second allele listed in ALT and so on.
The number of alleles suggests ploidy of the sample and the separator indicates whether the alleles are phased (|) or unphased (/) with respect to other data lines (Fig.1).
PS, phase set, indicates that the alleles of genotypes with the same PS value are listed in the same order.
DP, read depth at this position.
GL, genotype likelihoods for all possible genotypes given the set of alleles defined in the REF and ALT fields.
GQ, genotype quality, probability that the genotype call is wrong under the condition that the site is being variant.
Note that the QUAL column 2157 [12:26 5/7/2011 Bioinformatics-btr330.tex] Page: 2158 21562158 P.Danecek et al.gives an overall quality score for the assertion made in ALT that the site is variant or no variant.
INFO column: DB, dbSNP membership; H3, membership in HapMap3; VALIDATED, validated by follow-up experiment; AN, total number of alleles in called genotypes; AC, allele count in genotypes, for each ALT allele, in the same order as listed; SVTYPE, type of structural variant (DEL for deletion, DUP for duplication, INV for inversion, etc.
as described in the specification); END, end position of the variant; IMPRECISE, indicates that the position of the variant is not known accurately; and CIPOS/CIEND, confidence interval around POS and END positions for imprecise variants.
Missing values are represented with a dot.
For practical reasons, the VCF specification requires that the data lines appear in their chromosomal order.
The full format specification is available at the VCFtools web site.
2.1.3 Variation types VCF is flexible and allows to express virtually any type of variation by listing both the reference haplotype (the REF column) and the alternate haplotypes (the ALT column).
This permits redundancy such that the same event can be expressed in multiple ways by including different numbers of reference bases or by combining two adjacent SNPs into one haplotype (Fig.1g).
Users are advised to follow recommended practice whenever possible: one reference base for SNPs and insertions, and one alternate base for deletions.
The lowest possible coordinate should be used in cases where the position is ambiguous.
When comparing or merging indel variants, the variant haplotypes should be reconstructed and reconciled, such as in the Figure 1g example, although the exact nature of the reconciliation can be arbitrary.
For larger, more complex, variants, quoting large sequences becomes impractical, and in these cases the annotations in the INFO column can be used to describe the variant (Fig.1f).
The full VCF specification also includes a set of recommended practices for describing complex variants.
2.1.4 Compression and indexing Given the large number of variant sites in the human genome and the number of individuals the 1000 Genomes Project aims to sequence (Durbin et al., 2010), VCF files are usually stored in a compact binary form, compressed by bgzip, a program which utilizes the zlib-compatible BGZF library (Li et al., 2009).
Files compressed by bgzip can be decompressed by the standard gunzip and zcat utilities.
Fast random access can be achieved by indexing genomic position using tabix, a generic indexer for TAB-delimited files.
Both programs, bgzip and tabix, are part of the samtools software package and can be downloaded from the SAMtools web site (http://samtools.sourceforge.net).
2.2 VCFtools software package VCFtools is an open-source software package for parsing, analyzing and manipulating VCF files.
The software suite is broadly split into two modules.
The first module provides a general PerlAPI, and allows various operations to be performed on VCF files, including format validation, merging, comparing, intersecting, making complements and basic overall statistics.
The second module consists of C++ executable primarily used to analyze SNP data in VCF format, allowing the user to estimate allele frequencies, levels of linkage disequilibrium and various Quality Control metrics.
Further details of VCFtools can be found on the web site (http://vcftools.sourceforge.net/), where the reader can also find links to alternative tools for VCF generation and manipulation, such as the GATK toolkit (McKenna et al., 2010).
3 CONCLUSIONS We describe a generic format for storing the most prevalent types of sequence variation.
The format is highly flexible, and can be adapted to store a wide variety of information.
It has already been adopted by a number of large-scale projects, and is supported by an increasing number of software tools.
Funding: Medical Research Council, UK; British Heart Foundation (grant RG/09/012/28096); Wellcome Trust (grants 090532/Z/09/Z and 075491/Z/04); National Human Genome Research Institute (grants 54 HG003067, R01 HG004719 and U01 HG005208); Intramural Research Program of the National Institutes of Health, the National Library of Medicine.
Conflict of Interest: none declared.
ABSTRACT Motivation: Functional genomics research has expanded enormously in the last decade thanks to the cost reduction in highthroughput technologies and the development of computational tools that generate, standardize and share information on gene and protein function such as the Gene Ontology (GO).
Nevertheless, many biologists, especially working with non-model organisms, still suffer from non-existing or low-coverage functional annotation, or simply struggle retrieving, summarizing and querying these data.
Results: The Blast2GO Functional Annotation Repository (B2GFAR) is a bioinformatics resource envisaged to provide functional information for otherwise uncharacterized sequence data and offers data mining tools to analyze a larger repertoire of species than currently available.
This new annotation resource has been created by applying the Blast2GO functional annotation engine in a strongly high-throughput manner to the entire space of public available sequences.
The resulting repository contains GO term predictions for over 13.2 million non-redundant protein sequences based on BLAST search alignments from the SIMAP database.
We generated GO annotation for approximately 150 000 different taxa making available 2000 species with the highest coverage through B2G-FAR.
A second section within B2G-FAR holds functional annotations for 17 non-model organism Affymetrix GeneChips.
Conclusions: B2G-FAR provides easy access to exhaustive functional annotation for 2000 species offering a good balance between quality and quantity, thereby supporting functional genomics research especially in the case of non-model organisms.
Availability: The annotation resource is available atContact: aconesa@cipf.es; sgoetz@cipf.es Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 25, 2010; revised on January 4, 2011; accepted on February 1, 2011 1 INTRODUCTION Functional genomics research has gained importance in the last decade thanks to the fast improvement and cost reduction in To whom correspondence should be addressed.
high-throughput technologies.
Beyond traditional model species, also non-model organisms are in the genomics race and today it is hard to find a biological domain without a functional genomics initiative.
This expansion would not have been that successful without the accompanying development of computational tools that generate, standardize and share information on gene and protein function.
The Gene Ontology (GO) project is one such standard.
GO is a collaborative effort aiming at the establishment of a controlled vocabulary that provides biologically meaningful annotations for gene (products) across species (Ashburner et al., 2000).
There are two main aspects of this project: (a) the definition of a comprehensive ontology of functional terms and (b) the generation of an annotation database containing the assignment of GO terms to genes and proteins (The Gene Ontology Consortium, 2008).
Annotation for each organism in the GO database is supplied and maintained by the respective consortium member.
Evidence codes (ECs) are added to each individual annotation to reflect the information source used in a GO term assignment.
ECs indicate if the annotation is supported by some (and which) experimental evidence, whether it was transferred (and how) from related genes or if it was generated by other prediction methods.
The large majority of GO annotations is centralized in the Gene Ontology Annotation (GOA) Database (Camon et al., 2004).
This resource contains highquality functional annotations for proteins mostly obtained from the UniProt Knowledgebase (The Uniprot Consortium, 2007).
However, the great majority (95%) of GO corresponds to automatically transferred annotations based on InterProScan (Quevillon et al., 2005) results.
Currently, only a small number of assignments have experimental evidence or are revised by expert curators.
While the GO project is improving the ontology definition and quality of the GOA database (Barrell et al., 2009), it is still far from providing extensive functional annotation for the wealth of sequence data that populate public databases.
However, thanks to the structured and universal nature of GO, large-scale annotation using an automated process is conceivable and could be feasible given that adequate computational resources are available.
Such an annotation effort complement current established annotation initiatives by generating preliminary functional labels for sequences not yet covered by any of the reference projects of the GO consortium.
Examples of functional data-intensive resources in the field of functional genomics such as the Integr8 (Kersey et al., 2005) and PEDANT (Riley, 1993) The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:17 17/3/2011 Bioinformatics-btr059.tex] Page: 920 919924 S.Gtz et al.projects, which offer genome-oriented views on different types of annotation data, can be found.
An example of a specialized resource in the area of functional plant genomics is the PlexDB (Wise et al., 2007), a project to support transcriptomics studies on plants and plant pathogens.
Although these resources provide comprehensive information on many genomic aspects, they do not represent a universal resource where up-to-date large-scale functional data on species can be readily accessed and analyzed.
The Blast2GO project started in 2005 to build an universal bioinformatics platform for the functional annotation and analysis of novel sequence data (Conesa et al., 2005).
The tool was designed to provide high-throughput and quality assignments with a user interface strongly based on visualization and intuitive use (Gtz et al., 2008) (Conesa and Gtz, 2008).
So far, Blast2GO has been used in nearly 300 functional profiling projects involving a wide range of model and non-model species, from bacteria and fungi, through arthropods and mollusks, to plants, avian, fish and mammals, which evidences the strong need for functional annotations beyond that provided by public sequence databases.
During the last 5 years of Blast2GO, we have witnessed many cases of redundant annotation of public data by different labs and limitations for keeping results updated.
These facts, in combination with the annotation gaps mentioned above, have encouraged us to consider the generation of an extensive repository of automatic functional annotation making use of the widely accepted functional annotation engine Blast2GO.
Lastly advances in supercomputing and distributed computing have paved the way for undertaking highly intensive genome analysis tasks as demonstrated by many examples in different research fields such as proteomics (Marti-Renom et al., 2007), phylogenomics (HuertaCepas et al., 2007; Sjlander, 2004) and sequence analysis (Rattei et al., 2008).
In this work, we have made use of these technologies to create the Functional Annotation Repository (B2G-FAR), a webbased large-scale resource of precomputed functional annotations.
Generated by pipelines of intensive computing, B2G-FAR is a centralized repository of automatically annotated sequence data, which can directly be downloaded and used, saving processing time and avoiding redundant processing.
The resource covers all biological kingdoms and is structured in such a way that easy download and processing of data for a species of interest is possible.
The B2G-FAR follows the philosophy of Blast2GO development: (i) high throughput: over 25 million sequences have been processed and 2000 species are now present in the B2G-FAR database; (ii) the application to functional genomics projects: the repository contains GO term predictions for both, gene products and probe set collections of Affymetrix GeneChips; and (iii) user friendly: annotations are provided in the form of plain text files for direct download.
The main benefit is obtained in the case of non-model species where functional information and bioinformatics resources are limited.
2 METHODS 2.1 Construction The data contained in the B2G-FAR repository consists of two parts: speciescentered functional annotations obtained through the Simap2GO project and a collection of microarray probe set annotations.
Although generated by different pipelines, both data sources implement the same basic annotation scheme as established by Blast2GO (Gtz et al., 2008): (i) sequence similarity search (BLAST) (Altschul et al., 1990) to obtain a list of potential Fig.1.
The B2G-FAR annotation pipeline.
The scheme shows how the different data-sources are related and contribute to the generation of the B2G-FAR annotations, all passing through the Blast2GO annotation algorithm.
homologues;, (ii) mapping of the corresponding GO terms and their evidence codes for all BLAST hits; and (iii) the annotation step applying the Blast2GO annotation rule taking into account GO evidence codes, the GO hierarchy and the degree of sequence similarity (an extensive explanation of the Blast2GO annotation rule can be found in the Supplementary Material).
In a final step, protein domains, families and motifs are identified through InterProScan and the corresponding GO terms are merged with the already transferred annotations taking care of redundancies and GO hierarchy.
Simap2GO data and processing: Simap2GO is the result of the collaboration between the SIMAP (Rattei et al., 2008) and Blast2GO projects.
SIMAP is a database first published in 2005 (Arnold et al., 2005), which provides an all-against-all sequence similarity matrix containing precalculated pairwise comparisons of almost all publicly available proteins.
Computations are accomplished by volunteer grid computing based on the Berkeley Open Infrastructure for Network Computing (BOINC).
By May 2010, SIMAP contained over 29 million non-redundant sequences covering the content of all major public sequence resources.
The Simap2GO project, with the ambitious goal of functionally annotating this comprehensive sequence space, adopted and applied successfully the Blast2GO annotation methodology to the SIMAP resource.
Blast2GO is primarily based on sequence similarity and therefore perfectly suited to be run on the SIMAP database.
The computation of the annotations is run in parallel for the entire sequence space.
For each uncharacterized sequence, GO annotated (only non-electronic annotations) homologues are retrieved from the SIMAP sequence similarity matrix.
The applied Blast2GO method selects the GO terms above a given confidence threshold according to the established annotation rule.
Here, the algorithm takes into account the actual sequence similarity, the original GO evidence code and the location of the potentially transferred GO term within the hierarchy (see the Supplementary Material for a more detailed description of the Blast2GO annotation strategy and annotation parameters).
Apart from the annotation transfer through potential homologues, Simap2GO makes use of the InterPro domain information also provided in a precalculated manner by SIMAP.
This information is merged with the already predicted assignments to improve and increase the GO term assignment.
The sequence annotations generated in this way are stored in the B2G-FAR database and the routine continues with the next unprocessed sequence until completion (see Fig.1).
One entire annotation process takes approximately 3 days with 10 CPUs running in parallel and with direct access to the SIMAP database.
The annotation of the same amount of sequence data 920 [13:17 17/3/2011 Bioinformatics-btr059.tex] Page: 921 919924 B2G-FAR without precalculated alignments would have taken over half a year on a 150 CPU cluster.
All annotations are further processed to summarize and present species-centered information online.
Available charts and data files are given in the Supplementary Table S1.
Micorarray probe set data: the probe-set collection of 17 Affymetrix GeneChip designs corresponding to non-model species was annotated with Blast2GO.
As GeneChip probe sets do not necessarily target protein sequences available in public databases, their functional annotation cannot be recovered by Simap2GO and therefore has been computed using local resources.
FASTA files containing the target sequences of the probe sets were downloaded from the official Affymetrix web site.
The annotation pipeline started by splitting source FASTA files into smaller chunks and launching them against a distributed BLAST setup.
A 150 CPU cluster at the CIPF Bioinformatics and Genomics Department was used to run BLAST searches against the NCBI non-redundant (NR) database.
Simultaneously, protein domain information was obtained through a local installation of InterProScan (Quevillon et al., 2005).
Once BLAST and InterProScan searches were completed, results for every species were gathered and processed within Blast2GO for automatic function prediction.
Charts were generated during the annotation process and are provided online in Supplementary Table S3.
Finally, to assess the coverage of annotation results, each GeneChip was compared with the GO information provided by Affymetrix.
All currently annotated and available datasets are listed in the Supplementary Table S2.
2.2 Contents B2G-FAR presents contents in a user-friendly data sheet concept based on Wiki technology.
All the given information (annotations, data files, images) is generated beforehand by the B2G-FAR annotation pipeline and is summarized on automatically generated web pages.
This facilitates fast access to data files, images and charts describing genome-wide information.
Annotation data can be further visualized and analyzed through its upload into the Blast2GO application (see below: Download and query options).
B2G-FAR is periodically updated every 6 months.
Species annotations: by applying the above-described steps, we could assign GO terms to 14 million sequences that represents 56.4% of the entire SIMAP database (excluding metagenomic data).
The remaining sequences are entries without significant alignments (35.7%) or that did not surpass the annotations quality threshold (7.7%).
Sequences from 150 000 taxa were functionally annotated and the 2000 most represented species are now available through B2G-FAR.
Table 1 contains the numbers of annotated sequences compared with the whole SIMAP dataset and the available source annotations by the GO.
Species can be accessed directly by their scientific name or NCBI taxa ID through a search function.
For every species, several precalculated files and statistical charts are available.
These include a GO annotation flat file and its corresponding GO-Slim version.
Statistical charts provide information about GO annotation distributions, GO level distributions or about the most abundant functional terms within one of the three GO categories.
Microarray annotations: This section is organized as annotation sheets for each probe-set collection corresponding to the 17 non-model Affymetrix GeneChips.
Model species Affymetrix chips were purposely not included in the repository as there already exist extensive functional annotation projects.
The annotation sheet contains detailed information on the Blast2GO annotation process from the BLAST step up to the augmentation by ANNEX [a data mining procedure to annotate from links between molecular function and biological process/cellular component GO terms (Myhre et al., 2006)] and InterProScan.
In contrast to the previous section, which provides only final annotation records, the microarray probe-set annotation sheets include a great variety of descriptive charts that offer a comprehensive view of the functional information contents gathered throughout the annotation pipeline.
Likewise, Blast2GO project files are provided.
The charts and files included in the annotation sheets are listed in the Supplementary Table S3.
Table 1.
Simap2GO annotation coverage: the table shows the number of Blast2GO-annotated sequences in relation to the whole SIMAP dataset (May 2010) and the number of GO sequences which has been used as annotation source/reference dataset Data source Unique sequences Whole Simap 29 906 548 Simap without metagenomes 25 099 929 Simap protein sequences annotated by Blast2GO 14 175 984 Sequences which do not surpass the annotation threshold 1 938 862 Sequences without sequence alignment 8 985 083 GO annotation source sequences (only sequences with non-electronic annotations) 465 677 Only sequences with at least one non-electronic annotations (non-IEA) were used (GOLite data-set).
Additionally, the number of sequences which could not be annotated is given, i.e.sequences without sequence alignments and sequences whose annotations did not surpass the annotation threshold.
Download and query options: in both Species and Microarray sections, final annotation files are provided in plaintext format as GO and GOSlim data.
The text file format allows direct upload into the Blast2GO application for further analysis of annotation results as well as integration in other applications accepting GO annotation data.
Additionally, all species annotations are available in the standard GO annotation format.
Some descriptive charts are included in B2G-FAR for a quick overview of the results.
Dynamic access to the data is provided by the Blast2GO Java application.
This guarantees optimal reutilization and synchrony within Blast2GO developments.
For example, new query options have been incorporated into Blast2GO to support diverse access to B2G-FAR data (see online tutorial available as Supplementary Material).
Annotated sequences can be queried and filtered by their name/id, description, GO code and GO name, either as exact or contains matches.
Existing Blast2GO functions such as the generation of summary charts, single or combined graphs, annotation pies and enrichment analysis can be performed for the sequences selected by the user.
Moreover, the.annot files from B2GFAR are fully compatible with the Babelomics suite (Al-Shahrour et al., 2008; http://www.babelomics.org) for functional profiling analysis, where additional statistical methods for pathway analysis [FatiGO (Al-Shahrour et al., 2004) and FatiScan (Al-Shahrour et al., 2007)] are available.
This is especially interesting in the case of microarray probe files or when a functional enrichment needs to be assessed with experimental data involving any of the non-model species included in the repository.
Comparison of B2G-FAR annotations with GO annotations: the quality of the Blast2GO annotation method has been extensively assessed and proved in previous works (Conesa and Gtz, 2008; Conesa et al., 2005; Gtz et al., 2008).
However, we performed an additional evaluation of the annotation process to provide B2G-FAR users with a general feeling of the performance and nature of the annotations contained in the repository.
We selected 10 000 random sequences from B2G-FAR which were also present in the GO database and compared their annotations.
We recorded the number of exact GO term matches, more specific or more general terms (different specificity levels of the annotation), other branch or other GO category (true novel annotations) as described previously (Gtz et al., 2008).
Results are given in Table 2.
The comparison study revealed that most of the original GO annotations (93.5%) were contained in the B2GFAR repository as exact matches and more specific/general terms and only a small fraction (6.5%) were lost (other GO branch and category annotations) during the annotation process, presumably due to GO version differences or the removal of root category terms in the B2G-FAR repository.
When comparing in the opposite direction, we observed that 49% of the B2F-FAR annotations were represented as exact matches in the GOA, and an additional 13% of terms are provided as more specific concepts.
The remaining 38% 921 [13:17 17/3/2011 Bioinformatics-btr059.tex] Page: 922 919924 S.Gtz et al.Table 2.
Functional annotation of 10 000 random sequences from the GO and B2G-FAR compared against each other (annotation score 70, evalue 1E10, GOw=5, 5 BLAST hits) Compared GO versus FAR FAR versus GO Compared terms 46 414 (GO) 61 176 (B2G-FAR) Exact GO term match 29 446 29 446 More specific GO terms 510 7960 More general GO terms 13 457 156 Other GO branch 1126 16 193 Other GO category 1875 7421 Comparisons are given as reference database versus comparing database, and numbers refer to the reference database.
are terms in other branches and in other main GO categories.
To have an impression on the nature of these novel B2G-FAR annotations, we checked manually 20 randomly selected sequences for which differences between the two databases were found (see manual_evaluation.xls in Supplementary Material).
Curation of the novel GO terms implied contrasting against scientific papers and established functional databases, such as UniProt, Tair, Saccharomyces Genome Database, Entrez, etc.
From these 20 sequences one (AT5G35370.1) resulted to have doubtful sequence identity and was not considered in further computations.
The remaining 19 sequences accounted for 109 novel GO terms, 9 of which could not be verified from the available literature.
One sequence (Cyclin CLB2 of S.cerevisiae) obtained 4 presumably false GO functions due to sequence similarity to a paralogue with different functional specification.
The remaining 96 GO terms (88%) were confirmed from literature data and assessed as valid annotations.
These results evidence the quality of the GO term assignments contained in B2G-FAR.
3 UTILITY We illustrate the utility of the B2G-FAR on two examples of functional genomics studies taken from the literature and show how B2G-FAR can speed up or facilitate new data analyses.
The first example is in the field of next-generation sequencing (NGS).
These methods are rapidly extending within the genomics community as they greatly outperform both in sensitivity and accuracy hybridization-based approaches.
B2G-FAR can support functional assessment in NGS research.
In a pioneering study, Holt and colleagues analyzed genome variation and evolution in Salmonella typhi using NGS (Holt et al., 2008).
The authors applied 454 and Solexa technologies to resequence 19 different S.typhi strains and isolates.
The authors carried out a phylogenetic analysis of SNPs variance and identified genome insertions, deletions and modified genes across strains.
However, although the impact of genomic changes on certain coding regions was discussed, no genome-wide functional analysis of strain variations was attempted.
By typing S.typhi on the B2G-FAR species search box, we can readily locate the annotation file for this species, which contains GO assignments for 3917 genes (Supplementary Fig.S1).
GeneBank IDs included in the annotation file provide the means for matching functional and genomic variation data.
This annotation file can be opened with the Blast2GO software and by uploading each list of strain-specific varying genes, Blast2GO functions can be used to interrogate data for significant functional differences between isolates at the genome level, and to obtain the functional profiling of the genomic alterations or to locate mutated genes in metabolic pathways.
This example illustrates how readily available functional data can complement the analysis of experimental results with little additional effort.
The second example relates to the use of Affymetrix probeset annotation data available at the repository.
B2G-FAR offers an annotation coverage which is substantially higher than the NetAffx GO annotations provided by the manufacturer and also has fast and reliable access to functional data for these GeneChips.
The study by Espinoza et al.(2007) can serve as an illustrative example for this section of the repository.
The article presents a transcriptomics analysis of viral infection in wine grape cultivars using the Affymetrix Grape GeneChip.
In this study, authors generated functional annotations for upand downregulated gene groups through similarity-based function transfer from Arabidopsis thaliana by WU-BLAST, GO terms being directly transferred for all retrieved alignments.
The obtained annotation was summarized to reflect the abundance of distinct functional classes within regulated genes.
Although valid, this basic functional description does not allow the identification of those functional categories which are specifically activated at viral infection.
For this, a functional comparison to the whole genome represented in the array would be required, which implies that functional data for all probes would be needed.
This information, absent in the article and presumably costly for the authors to obtain, is readily available from the B2G-FAR site.
The B2GFAR annotation file for the Grape GeneChip contains 54 841 GO terms and covers 11 971 probe sets.
The list of differentially expressed genes provided in the article as Supplementary Material was used in Blast2GO to perform a GO term enrichment analysis based on the B2G-FAR annotation file.
The analysis indicated a significant overrepresentation of chloroplast genes in the Camnre downregulated gene set (adjusted P-value: 1.2105) (see Supplementary Fig.S2a) and only a slight enrichment of membrane, L-arginine and L-glutamate import and other membrane transport activities (P-values: 6103) for the upregulated gene set (see Supplementary Fig.S2b).
4 DISCUSSION The major purpose of B2G-FAR is to offer biologists easy access to functional information.
B2G-FAR has been conceived as a repository of automatic annotations generated by Blast2GO using high-throughput computing technologies to save annotation time to the functional genomics community.
The B2G-FAR is species centered, which means that data can readily be obtained for any of the 2000 organisms present in the database.
The Blast2GO annotation strategy has shown to render good recall values for sequence similarity function transfer methods and to match functional assignments by curated computational analysis (Gtz et al., 2008).
The B2G-FAR retains these quality levels: we showed that the majority of B2G-FAR assignments are identical or functionally related to GO Database annotations for sequences present in this database and, additionally, novel predictions are generally supported by the available literature.
It should be stressed, however, that the quality of B2G-FAR is closely linked to the completeness and accuracy of the GO and InterPro databases.
B2G-FAR complements the GO effort by offering high-throughput automatic annotations on a species basis.
Compared with GOA, where automatically generated annotations are to a big extent based 922 [13:17 17/3/2011 Bioinformatics-btr059.tex] Page: 923 919924 B2G-FAR Fig.2.
Comparison between NetAffx and Blast2GO generated annotations for GeneChips contained in B2G-FAR.
on protein domains, B2G-FAR combines both sequence similaritybased annotations through Blast2GO together with domain-based information through InterProScan.
In this way, GO term assignments could be increased in number and the amount of available annotated sequences could be nearly doubled.
Comparing the generated Affymetrix GeneChip annotations to the current GO annotation available at the NetAffx site, the B2G-FAR resource increased the coverage of functional annotations from an average of 7.89% (NetAffx) to 40.89% (Blast2GO) (Fig.2).
Only the Bovine and Chicken NetAffx annotations were richer than the ones generated by Blast2GO due to intensive proteome annotation efforts of GOA in collaboration with the International Protein Index (Barrell et al., 2009).
Currently, most of the GeneChips of non-model species processed in B2G-FAR contain sufficient annotation coverage for a successful evaluation of microarray results in terms of pathways and biological functions.
Moreover, the compatibility of B2GFAR file formats with functional profiling tools make functional assessment methods readily accessible for a much larger diversity of organisms.
Finally, B2G-FAR should not be understood as a competitive annotation source to annotation projects as carried out within the GO consortium, nor as a replacement to high-quality manual annotation of single-gene products, but as a complementing resource.
Although automated annotation is by nature more error prone than manually curated one, B2G-FAR offers novel valuable information, making functional data accessible to a large users community working on different species.
5 CONCLUSIONS B2G-FAR provides easy access to exhaustive functional information for a broad range of species encompassing most organisms under genome investigation.
The repository is simple in architecture and still offers many analysis possibilities through the proximity to the Blast2GO software.
In its current form, the resource is species centric.
Future developments will consider multispecies scenarios such as metagenomics data or comparisons across taxa.
6 AVAILABILITY AND REQUIREMENTS The annotation resource is freely available at http://b2gfar.bioinfo.cipf.es, is based on the DokuWiki framework and works with any common web browser.
There are no other requirements or plugins needed to use the repository.
Data files can be downloaded and unzipped or directly uploaded into the Blast2GO application through Java WebStart technology.
Therefore, Java has to be installed.
For both, B2G-FAR and Blast2GO, tutorials and quick-start sections are provided online.
Funding: Spanish Ministry of Science and Innovation (MICINN) (grants BIO2008-04638-E, BIO2008-05266-E, BIO2008-04212, BIO2009-10799 and CEN-20081002) and the PlanE Program; GVAFEDER (PROMETEO/2010/001); Red Tematica de Investigacion Cooperativa en Cancer (RTICC), ISCIII, MICINN (grant RD06/0020/1019, in part).
Further financial support was granted by the European Science Foundation (ESF) with the activity entitled Frontiers of Functional Genomics.
Conflict of Interest: none declared.
ABSTRACT Summary: High-resolution, three-dimensional (3D) imaging of large biological specimens generates massive image datasets that are difficult to navigate, annotate and share effectively.
Inspired by online mapping applications like GoogleMaps, we developed a decentralized web interface that allows seamless navigation of arbitrarily large image stacks.
Our interface provides means for online, collaborative annotation of the biological image data and seamless sharing of regions of interest by bookmarking.
The CATMAID interface enables synchronized navigation through multiple registered datasets even at vastly different scales such as in comparisons between optical and electron microscopy.
Availability: http://fly.mpi-cbg.de/catmaid Contact: tomancak@mpi-cbg.de 1 INTRODUCTION High-throughput and high-resolution imaging technologies generate many more images than can be practically shown in printed journals and thus these massive image datasets are presented to the scientific community through web interfaces.
Recently, a new class of large-scale biological image data emerged that focuses on high-resolution description of large biological specimens using three-dimensional (3D) microscopy techniques.
Since most biological specimens are large in comparison to the scales employed by high-resolution microscopes, the entire specimens are captured by stitching many overlapping image tiles into a single canvas of virtually unlimited size.
Microscopy techniques used in the tiling mode present new challenges for the annotation, analysis and sharing of gigantic datasets.
An analogy can be drawn between high-resolution imaging of large biological specimens and Geographical Information Systems (GIS) showing satellite imagery of the earth.
In both cases, the raw image data must be viewed at a number of different scales to reveal notable features.
Similarly, both data types become meaningful only when significant landmarks in the images are labeled.
For geographical data, an impressive array of computational tools have been developed to represent the imagery overlaid with annotated features to form high-resolution maps of the planet available from everywhere via web-based interfaces.
In biology, To whom correspondence should be addressed.
features such as tissues, cells or organelles can be distinguished on different scale levels and serve as a map to orient in the complex anatomy of the biological entity.
It is clear that large anatomical scans of biological specimens must be accompanied with proper maps of relevant biological features to enable insights into the organization and function of biological systems.
Modern neurobiology is particularly active in mapping high-resolution anatomy (Mikula et al., 2007) and patterns of gene expression (Lein et al., 2007) in the brain.
We present here a decentralized web interface, modeled after GoogleMaps, to navigate large multidimensional biological image datasets and collaboratively annotate features in them.
We demonstrate the navigation, annotation and sharing functionality of the Collaborative Annotation Toolkit for Massive Amounts of Image Data (CATMAID) on a serial section Transmission Electron Microscopy (ssTEM) dataset covering the neuropile of one half of the first instar larval brain of Drosophila melanogaster.
2 IMPLEMENTATION CATMAID combines three main components: a centralized data server, decentralized image servers and the client-side user interface (Fig.1A).
The data server stores meta-information about datasets, users and annotations in a PostgreSQL database.
The entities of the database are projects, stacks, annotations, spatial entities and users.
Projects implicitly define global reference frames in 3D space and thus provide a spatial context for annotations and images.
Stacks are image datasets prepared for viewing through CATMAID (see below).
A stack stores its dimensions in pixels, the 3D resolution in nm/px and a base URL to the image server.
Stacks reference projects through a translation vector.
That is, each stack may appear in several contexts registered relative to different reference frames.
All stacks referencing the same project can be navigated synchronously.
Annotations are textlabels or references to higher level semantic frameworks, e.g.ontology terms.
An annotation may be referenced by an arbitrary number of spatial entities and vice versa.
Spatial entities are point locations or 3D regions in a project reference frame.
By this means, all image stacks in the same project context share a common set of annotations.
The current implementation supports point locations and textlabels being the simplest form of spatial entities and annotations.
Access to projects is modulated by user privileges whereas users are defined by a unique name and password.
Optionally, projects may be visible for the public.
The user interface is implemented in Javascript.
It requires no third party plugins and runs platform independent on the most popular web browsers.
All interactions between the user interface and the data server are realized as 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:20 26/6/2009 Bioinformatics-btp266.tex] Page: 1985 19841986 CATMAID Fig.1.
(A) Sketch of the information flow between the three components of CATMAID: the central data server, the client-side user interface and several image servers.
(B) Visualization of the multi-scale tiling of images.
In the upper row, an exemplary ssTEM section is overlaid with the tiling grid at 25%, 50% and 100% scale.
Below, exemplary tiles at the respective scale are shown (13).
The location of a tile in the section and in lower scale tiles is indicated with the corresponding number.
(C) The user interface in the synchronous navigation mode with an ssTEM dataset on the left and a relatively registered confocal stack on the right.
asynchronous HTTP data requests using Javascript Object Notation (JSON) for data transfer.
The server-side implementation is realized in PHP.
CATMAID is a web companion to the TrakEM2 software (Cardona, 2006) for management, registration and analysis of large-scale ssTEM datasets.
TrakEM2 is able to export image data in CATMAID compatible format.
While the classical GIS presents the surface of a planet that is a 2D dataset, CATMAID was designed to show 3D microscopy data, in particular from serially sectioned volumes.
The canvas provides the means to zoom and pan a section and navigate through the volume alongside the section index.
For rapid browsing at multiple scales, we consider each section as a tiled scale pyramid (Fig.1B).
Each scale level has half the resolution of the previous and is split into 256256px-tiles that are saved in JPEGcompressed format.
By this means, the browser requests only those tiles that are visible at once.
Instead of generating the tiles on the fly from the original image data, we initially generate the whole tiled scale pyramid and store it to the file system.
This results in 1 13 the number of pixels of the original image, which is typically reduced to about 10% storage space by JPEG-compression.
Tiles are ordered by file name convention.
Each section is stored in a directory whose name is the section index.
A tiles name contains the row and column in tile coordinates and the scale index i where f =1/2i is the scale factor relative to the original resolution.
For example, 4/14_20_2.jpg identifies the tile at row 14 and column 20 in Section 4 at scale level 2.
The scale pyramid can be exported directly from qualified software such as TrakEM2 or alternatively generated using a custom ImageMagick shell script that is executed locally on the image server.
Subsequently, the user registers the dataset as a project within the CATMAID viewer by providing a worldaccessible URL pointing to the scale pyramid.
In this way, the primary image data remain decentralized, while the project properties stored in a centralized database enable cross-referencing.
3 DESCRIPTION OF THE USER INTERFACE Initially, the interface shows the project toolbar that contains a pulldown list for stack selection and input fields for user login.
The main screen shows a comprehensive list containing all accessible projects and related stacks.
After successful user log-in, the pull-down list and the main screen are updated respectively.
As soon as a stack is opened in the context of a project by selection in the pull-down or on the main window, the navigation toolbar appears.
All stacks referencing the same project can be opened at the same time.
The focused stack binds and updates all control devices in the navigation toolbar.
The user can navigate all opened stacks with mouse, mouse wheel, keyboard or by using the sliders and input fields in the navigation toolbar.
Each navigation command is caught by the focused stack, transferred into the project coordinate frame and sent to the project instance.
The project instance propagates the instruction to all opened stacks.
In this way, all opened stacks are navigated in perfect synchrony regardless of their resolution or scale.
Annotations are placed directly on top of a stack.
The textlabel toolbar provides input elements to change color and font size.
Annotations reference the project as a whole, that is, each stack that is linked to the project will display all project annotations regardless of whether they were created on top of another stack.
We implemented an asynchronous message system for long-term server-side image processing tasks such as retrieval of a small 3D subset of the entire dataset (micro-stack) for offline processing.
Such a job is processed on the server side and notifies the user when it is finished.
The interface can export the current view including all opened stacks, location and scale as a parameterized URL (bookmark).
If invoked by such a URL, the interface immediately recovers the view.
In this way, researchers can easily share bookmarks pointing to particular regions of interest.
4 RESULTS AND DISCUSSION We demonstrate the application of the viewer on two tiled serial section TEM datasets of Drosophila first instar larval brains (http://fly.mpi-cbg.de/catmaid-suppl).
The registered dataset consists of 85 sections of 60nm thickness and shows lateral neuronal layers and part of the neuropile.
It was imaged using a moving stage operated by the Leginon software (Suloway et al., 2005).
The images are 20482048px at 4nm/px and were taken with 6% tile overlap, 99 images per section, resulting in 6885 images.
The tiles were registered both within and across sections using a fully automatic global registration approach implemented as part of TrakEM2 (Saalfeld et al., manuscript in preparation).
The registered Drosophila first instar larval brain dataset was converted into a CATMAID compatible scale pyramid yielding 21846 tiles per section and 1856910 tiles for the whole dataset.
For such a massive 3D image mosaic covering a substantial portion of the larval brain at 4nm/px resolution, the CATMAID interface offers unprecedented flexibility in navigation, and enables collaborative 1985 [15:20 26/6/2009 Bioinformatics-btp266.tex] Page: 1986 19841986 S.Saalfeld et al.annotation and sharing of the locations of regions of interest via bookmarks.
Moreover, the interface allows linked navigation of multiple registered datasets, even at vastly different resolutions such as that of electron and confocal microscopy (Fig.1C; http://fly.mpicbg.de/catmaid-suppl).
The CATMAID interface is applicable to any 2D or 3D multimodal biological image datasets, as shown in examples of stitched confocal 3D volumes (http://fly.mpi-cbg.de/catmaidsuppl).
The tool will become especially powerful when comparing registered 3D stacks of different biological specimens labeled to visualize tissue-specific gene expression.
Additionally, the CATMAID interface can be used to navigate, annotate and bookmark locations in any large image canvas.
Some of the possible applications in biology are viewing of scientific posters and browsing large-scale in situ image datasets (Tomanck et al., 2007; http://fly.mpi-cbg.de/catmaid-suppl).
Future versions of the interface will feature ontology-based project-specific semantic frameworks and interactive tools for drawing of 3D regions of interest.
We plan to expand the scope of the viewer to support an arbitrary number of dimensions allowing navigation of multimodal, time-lapse microscopy data.
Conflict of Interest: none declared.
ABSTRACT Summary: Copy number variants (CNVs) contribute substantially to human genomic diversity, and development of accurate and efficient methods for CNV genotyping is a central problem in exploring human genotypephenotype associations.
SCIMMkit provides a robust, integrated implementation of three previously validated algorithms [SCIMM (SNP-Conditional Mixture Modeling), SCIMMSearch and SCOUT (SNP-Conditional OUTlier detection)] for targeted interrogation of CNVs using Illumina Infinium II and GoldenGate SNP assays.
SCIMMkit is applicable to standardized genome-wide SNP arrays and customized multiplexed SNP panels, providing economy, efficiency and flexibility in experimental design.
Availability: Source code and documentation are available for noncommercial use at http://droog.gs.washington.edu/scimmkit.
Contact: troyz@u.washington.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Copy number variation (CNV) in the human genome contributes substantially to genomic diversity and disease etiology (Lupski, 2009; McCarroll, 2008).
Use of genome-wide SNP genotype data to perform ab initio discovery of individual CNVs has provided valuable insight into the spectrum of human genomic variation (Itsara et al., 2009; Redon et al., 2006).
However, with the development of larger catalogs of common variation (Kidd et al., 2008; McCarroll et al., 2008) and continuing discovery of rare variants with severe phenotypic effects (Sebat et al., 2008; Walsh et al., 2008), it is critical to efficiently genotype specific CNVs in large populations.
Targeted detection strategies generally outperform ab initio detection strategies for this task (McCarroll, 2008).
Therefore, we have developed SCIMMkit, a toolkit for targeted genotyping of CNVs using Illumina Infinium II and GoldenGate SNP assays.
SNP assays typically generate two measurements per site (A and B allele fluorescence) forming the canonical genotype clusters A/A, A/B and B/B when visualized by scatterplot.
Deletions of sequence result in decreased signal intensity (i.e.states A/, B/,/) (Fig.1), and duplications result in increased signal intensity (i.e.states AAA and BBB) and aberrant allelic ratio (i.e.states AAB and ABB) (Supplementary Fig.S1).
States corresponding to individual CNVs often fail to form distinct clusters To whom correspondence should be addressed.
0.0 0.2 0.4 0.6 0.
0 0.
2 0.
4 0.
6 0.
8 rs12098109 (chr1:150828032) A allele fluoresence (normalized units) B al le le fl uo re se nc e (n or m al iz ed u ni ts ) Fig.1.
Fluorescence data for 125 HapMap samples (Cooper et al., 2008) at a single SNP probe (rs12098109) within a common deletion polymorphism identified as a susceptibility factor for psoriasis (de Cid et al., 2009).
Copy number genotypes (blue diamonds, 0; red triangles, 1; black circles, 2) were computed by SCIMM using three SNP probes; superimposed curves describe components of the estimated mixture distribution.
due to dynamic range limitations; therefore, methods using multiple SNP probes per site are required for robust copy number inference (Cooper et al., 2008; Korn et al., 2008; Mefford et al., 2009).
2 DESCRIPTION OF FUNCTIONALITY SCIMMkit provides three tools for targeted interrogation of CNVs, each of which assumes prior knowledge of the approximate location of each interrogated variant: SCIMM (SNP-Conditional Mixture Modeling), for genotyping polymorphic deletions (frequency exceeding 1%); SCIMM-Search, for automatically generating informative probe sets to be used by SCIMM; and SCOUT (SNPConditional OUTlier detection), for detecting rare deletion and duplication variants (frequency <1%).
Each of these tools uses a statistical model of observed fluorescence data which contains, for each SNP probe, separate location parameters for each homozygous allelic state (i.e.A/A, A/, B/B, B/) and a single dispersion parameter shared by all homozygous allelic states.
The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:32 2/12/2009 Bioinformatics-btp606.tex] Page: 121 120122 Targeted interrogation of CNV by SCIMMkit SCIMM assigns diallelic insertion/deletion genotypes (i.e.copy number 0, 1 and 2), using SNP calls and normalized fluorescence measurements for a set of n SNP probes hybridizing specifically to sequence spanning the deleted region (Cooper et al., 2008).
Two rounds of mixture likelihood-based clustering are used: the first round uses intensity data to call samples near the origin as 0, and the second round uses intensity data and supplied SNP genotypes to call remaining samples as 1 or 2, using a twocomponent, 2n-variate lognormal mixture model.
Copy number for each sample is assumed to be constant for all probes in a set; accordingly, samples that are SNP heterozygous at any probe are assumed to have copy number 2 for the purposes of model fitting and copy number assignment.
These statistical assumptions do not hold for SNP probes that hybridize non-specifically (Supplementary Fig.S2); such probes are rejected during probe set generation, below.
SCIMM also generates a score for the probe set, defined as the difference of the Bayesian information criterion (BIC) value for the two-component model and the BIC value for the corresponding one-component model.
Genotypes are reported only for sites with positive scores.
SCIMM-Search can be used to automatically generate informative probe sets in circumstances where the specificity assumptions of SCIMM may not be satisfied for all probes in the putatively deleted region.
SCIMM-Search uses the BIC to select between alternate probe sets, and allows the investigator to specify constraints on consistency with reference genotypes, internal consistency of the probe set, probe spacing and dynamic range (Cooper et al., 2008).
SCOUT detects rare deletions and duplications at each targeted site by initially calculating per-probe scores for each sample, using a one-component SCIMM model extended to describe fluorescence data for SNP heterozygotes (Mefford et al., 2009).
For SNP homozygotes, per-probe score is determined solely by intensity; for SNP heterozygotes, per-probe score is determined by intensity and deviation from 1:1 allelic ratio (specifically, by distance of the observed datum from the line connecting the origin to the center of the heterozygote cluster).
Per-probe scores are approximately normally distributed, with samples at the center of each canonical SNP genotype cluster receiving a score of zero.
The per-probe scores are combined additively to obtain per-site scores, which are then compared with an empirically determined threshold to generate a list of putative deletion and duplication events.
Hemizygous and duplicated haplotypes for strongly scoring events are also reported, allowing inference of complex allelic states (e.g.AAB, Supplementary Fig.S1) and parental chromosome of origin (in cases where parental data are available).
SCIMMkit also implements an initial SCOUT quality-control pass which rejects samples with a genome-wide excess of extreme per-probe scores, improving the positive predictive value of the per-site SCOUT scores generated for the remaining samples (Supplementary Fig.S1).
SCIMMkit requires as input a target file and one or more data files.
Each line of the target file specifies a set of probes (with probe ID and coordinates) and an action associated with the probe set (i.e.SCIMM genotyping, SCIMM-Search probe set generation or SCOUT scoring).
Input is supplied in Illumina BeadStudio genotype report format (or similar tabular format).
SCIMMkit generates two primary output files: a comma-delimited matrix with scores and numeric genotype codes (one row per sample and one column per target site), and a comma-delimited table with per-site summary information including genotype counts, probe set scores and SCIMM-Search generated probe sets.
SCIMMkit can optionally generate scatterplots with superimposed SCIMM genotypes and mixture distribution curves in postscript format.
SCIMMkit is implemented in PERL (used for command-line interpretation, input parsing and data consolidation) and R (used for numerically intensive tasks), and has been tested on Apple Macintosh OS X, Linux and Microsoft Windows platforms.
3 DISCUSSION SCIMM and SCOUT use a common statistical model to facilitate distinct applications.
SCIMM genotypes polymorphic deletions by estimating the location of each genotype cluster (/, A/, B/, A/A, A/B, B/B).
SCOUT detects rare deletion and duplication variants by analyzing the location of each sample relative to the canonical SNP genotype clusters (A/A, A/B, B/B), avoiding estimation of location parameters for rare allelic states (e.g.deletion states A/, B/ and duplication states AAB, ABB).
SNP-based genome-wide association studies have generated a wealth of resources for retrospective analysis of CNV (Itsara et al.2009).
The first step in analyzing polymorphic variation in such data is identification of CNVs that can be accurately genotyped.
To generate a database of polymorphic deletion sites and validated copy number-informative probe sets, we used SCIMM-Search to analyze data generated by the Illumina 1M-DuoV3 array for 269 HapMap samples.
We compared the resulting SCIMM-generated diallelic deletion genotypes with previously published genotypes generated by BirdSuite software using the Affymetrix SNP 6.0 array (McCarroll et al., 2008).
SCIMM produced diallelic deletion genotypes for 113 common (sample allele frequency at least 5%) autosomal deletions (84% of which have per-site concordance to BirdSuite genotypes exceeding 99%), 392 autosomal deletions of lower frequency (88.5% of which have genotype concordance exceeding 99% and positive predictive value for deletion status exceeding 80%) and 6 X-linked diallelic deletions (all of which have concordance exceeding 98.5%).
These concordance rates are consistent with earlier analyses using independently generated reference genotypes (Cooper et al., 2008).
The resulting list of highly concordant sites and corresponding Illumina 1M-DuoV3 probe sets produced by SCIMM-Search are provided on the SCIMMkit web site for genotyping polymorphic deletions in other genome-wide datasets.
(See Supplementary Material for details).
Detection of highly pathogenic CNVs presents a distinct challenge: individually, such variants tend to be rare (frequency <1%) in affected individuals and very rare or completely absent in control populations; thus, definitively establishing a difference in allele frequency between cases and controls requires analysis of a large number (many thousands) of samples (International Schizophrenia Consortium, 2009).
To assess the feasibility of large-scale targeted detection studies, SCOUT was recently used in conjunction with a customized Illumina BeadXpress assay to genotype deletions and duplications at 69 non-allelic homologous recombination hotspots in 1005 individuals with unexplained intellectual disability (ID).
SCOUT correctly detected 48 rare deletion and duplication events, including 22 events known to be pathogenic, with only seven false positives (score threshold |6|, events validated by oligo-array CGH) (Mefford et al., 2009).
Although, SCOUT does not explicitly include batch effects in its statistical model, the robustness of its model-fitting procedure at 121 [12:32 2/12/2009 Bioinformatics-btp606.tex] Page: 122 120122 T.Zerr et al.small sample sizes allows known batch effects to be remedied by independent scoring of batches.
In the ID study above, each 96-well plate was analyzed independently to provide robustness against plate-to-plate variation in signal intensity and dynamic range.
We anticipate that future studies of association between CNV and phenotype will follow a model similar to SNP-based studies: an ab initio discovery stage (often in a population enriched for the phenotype of interest), an initial phenotypic association testing stage and a validation stage where the strongest associations are tested in a much larger population.
SCIMMkit allows efficient and accurate detection of CNVs in the latter two stages of this model, facilitating further exploration of the link between CNV and human phenotypic variation.
Funding: T.Z.
and D.A.N.
acknowledge support from NIH grants HL66682 and HL66642.
G.M.C.
is supported by a Merck, Jane Coffin Childs Memorial Fund Fellowship.
E.E.E.
acknowledges support from NIH grant HG004120 and HD065285.
E.E.E.
is an investigator of the Howard Hughes Medical Institute.
Conflict of Interest: EEE is a SAB member of Pacific Biosciences.
ABSTRACT Motivation: With the exponential growth of expression and protein protein interaction (PPI) data, the frontier of research in systems biology shifts more and more to the integrated analysis of these large datasets.
Of particular interest is the identification of functional modules in PPI networks, sharing common cellular function beyond the scope of classical pathways, by means of detecting differentially expressed regions in PPI networks.
This requires on the one hand an adequate scoring of the nodes in the network to be identified and on the other hand the availability of an effective algorithm to find the maximally scoring network regions.
Various heuristic approaches have been proposed in the literature.
Results: Here we present the first exact solution for this problem, which is based on integer-linear programming and its connection to the well-known prize-collecting Steiner tree problem from Operations Research.
Despite the NP-hardness of the underlying combinatorial problem, our method typically computes provably optimal subnetworks in large PPI networks in a few minutes.
An essential ingredient of our approach is a scoring function defined on network nodes.
We propose a new additive score with two desirable properties: (i) it is scalable by a statistically interpretable parameter and (ii) it allows a smooth integration of data from various sources.
We apply our method to a well-established lymphoma microarray dataset in combination with associated survival data and the large interaction network of HPRD to identify functional modules by computing optimal-scoring subnetworks.
In particular, we find a functional interaction module associated with proliferation overexpressed in the aggressive ABC subtype as well as modules derived from non-malignant by-stander cells.
Availability: Our software is available freely for non-commercial purposes at http://www.planet-lisa.net.
Contact: tobias.mueller@biozentrum.uni-wuerzburg.de 1 INTRODUCTION Construction and analysis of large biological networks have become major research topics in systems biology (Aittokallio and Schwikowski, 2006).
Various aspects have been analyzed including the inference of cellular networks from gene To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
expression (Friedman, 2004), network alignments (Flannick et al., 2006; Kelley et al., 2003; Sharan and Ideker, 2006) and other related strategies as reviewed by Srinivasan et al.(2007).
At the same time, well-established microarray technologies provide a wealth of information on gene expression in various tissues and under diverse experimental conditions.
Integrating protein protein interaction (PPI) and gene-expression data generates a meaningful biological context in terms of functional association for differentially expressed genes.
Frequently, large scale expression profiling studies investigate many experimental conditions simultaneously, thereby generating multiple P-values.
Especially in tumor biology expression profiling has become a well-established tool for the classification of different tumors and tumor subtypes.
Furthermore, in the clinical context, various patient-associated data are available thatin conjunction with expression dataprovide valuable information of the influence of specific genes on disease-specific pathophysiology.
In particular the analysis of survival data allows to establish gene expression signatures to make predictions about the prognosis and to assess the disease relevance of certain genes.
However, the cellular function of an individual gene cannot be understood on the level of isolated components alone, but needs to be studied in the context of its interplay with other gene products.
The combined analysis of expression profiles and PPI data thus allows the detection of previously unknown dysregulated modules in interaction networks not recognizable by the analysis of a priori defined pathways.
Ideker et al.(2002) have proposed to identify interaction modules in this setting by devising firstly an adequate scoring function on networks and secondly an algorithm to find the high-scoring subnetworks.
The underlying combinatorial problem has been proven to be NP-hard for additive score functions defined on the nodes of the network.
The authors proposed a heuristic strategy based on simulated annealing and developed a score to measure the significance of a subnetwork that includes the integration of multivariate P-values.
This score has been extended by Rajagopalan and Agarwal (2005) to incorporate an adjustment parameter in order to obtain smaller subgraphs in conjunction with a greedy search algorithm.
This approach however, excludes the possibility to combine multiple P-values.
Variants of greedy search strategies have also been used by Nacu et al.(2007) and Sohler et al.(2004).
Subsequently Cabusora et al.(2005) proposed an edge score by adapting the scoring concept of Ideker et al.(2002).
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:14 18/6/03 Bioinformatics-btn161.tex] Page: i224 i223i231 M.T.Dittrich et al.An alternative edge scoring based on correlation of gene expression has been proposed by Guo et al.(2007).
All the former methods are heuristic approaches that cannot guarantee to identify the maximally scoring subgraph.
Some of these often computationally demanding approaches tend to deliver large high-scoring networks, which may be difficult to interpret.
Here we present a novel approach (i) that is characterized by a modular scoring function, based on signal-noise decomposition implemented as a mixture model, (ii) permits the smooth integration of multivariate P-values derived from various sources, (iii) delivers provably optimal and suboptimal solutions to the maximalscoring subgraph problem by integer-linear programming (ILP) in reasonable running time and (iv) allows to control the resultant subnetwork size by an adjustment parameter, which is statistically interpretable as false-discovery rate (FDR).
The presented algorithm is, to our knowledge, the first approach that really tackles and solves the original problem raised by Ideker et al.(2002) to optimality.
We strongly believe that the optimal and suboptimal solutions produced by our method provide a considerable benefit over heuristic solutions in that they allow for a sound evaluation and adaptation of the underlying model.
Given only a heuristic solution it is impossible to decide whether poor results are due to inappropriate parameter settings or due to the optimality gap.
Based on extensive simulations we evaluate our exact approach in comparison to the heuristic method proposed by Ideker et al.(2002).
Finally, analyzing a comprehensive microarray and survival dataset of lymphoma patients we detect functional modules, extending the results of Rosenwald et al.(2002).
The remainder of this article is organized as follows: after describing the data and methods we use (Section 2), we introduce our approach and its application to lymphoma interactome data in Section 3.
Section 4 presents the subnetworks we found and a validation of our approach in comparison to the method by Ideker et al.We conclude with Section 5, where we discuss our findings.
2 METHODS 2.1 Microarray and survival data We used the published gene-expression data set from diffuse large Bcell lymphomas (DLBCL) (Rosenwald et al., 2002).
In particular, gene expression data from 112 tumors with the germinal center B-like phenotype (GCB DLBCL) and from 82 tumors with the activated B-like phenotype (ABC DLBCL) were included in this study.
Gene expression analysis was performed on the Lymphochip including 12 196 cDNA probe sets (Rosenwald et al., 2002).
In addition, survival information was available from 190 patients (Rosenwald et al., 2002).
Statistical analyses were performed using the software package R (R Development Core Team, 2006) and Bioconductor (Gentleman et al., 2004) and the routines implemented in limma (Smyth, 2004).
For normalization of gene expression within arrays we used the loess method, normalization between arrays was performed by using the scale method to adjust the log ratios to the same median absolute deviation (MAD) across arrays as detailed in Blenk et al.(2007).
For subsequent analyses the expression values for different spots of the same gene have been aggregated by taking the median.
Significance of differential expression between the two subtypes ABC and GCB was calculated using robust statistics based on linear models and a moderated t-test (Smyth, 2004).
Survival analysis was performed by Cox regression as implemented in the R-package survival (Andersen and Gill, 1982; Therneau et al., 1990).
2.2 Network The dataset of literature-curated human PPI has been obtained from HPRD (Mishra et al., 2006; Peri et al., 2003).
Using R and the network structures and algorithms in the Bioconductor packages graph and RBGL (Carey et al., 2005), we derived an interactome network consisting of 36 504 interactions between 9392 different proteins.
With the subset of genes shared between the interactome dataset and the chip a Lymphochip-specific interactome network was derived as the vertex-induced subgraph.
The resulting network comprises 2561 different gene products and 8538 interactions, with a large connected component of 2034 proteins (79.4%) and 8399 interactions (98.4%).
The remaining proteins were either non-interacting singletons (472) or tiny clusters of sizes between two and six (23).
Our analysis focuses exclusively on the giant connected component.
visualization and further network analysis was performed with Cytoscape (Cline et al., 2007; Shannon et al., 2003).
2.3 Optimization algorithm Our algorithm is based on the software dhea (district heating) from Ljubic et al.(2006).
We have extended the C++ code in order to generate suboptimal solutions and have created several Python scripts to control the transformation to a Steiner tree problem, the use of dhea and the retransformation to a PPI subnetwork.
The dhea code uses the commercial CPLEX callable library version 9.030 by ILOG, Inc. (Sunnyvale,CA) (Sunnyvale, CA).
All experiments were run on a 64 bit 2.2 GHz Opteron Intel with 8 GB of main memory.
Our software and the datasets used in this study are publicly available for academic and research purposes within the heinz (heaviest induced subgraph) package of the open source library LiSA (http://www.planet-lisa.net).
3 SCORING FUNCTION AND ALGORITHM This section introduces our new integrated exact approach to support the identification of functional modules in PPI networks.
Section 3.1 focuses on the order statistics-based method to determine score values for the network nodes.
We illustrate our approach by analyzing a network obtained by combining the data from a expression profiling study of lymphoma patients (Rosenwald et al., 2002) with the comprehensive interactome data from HPRD (Peri et al., 2003).
We derive P-values from the analysis of differential expression between two tumor subtypes (ABC and GCB) as well from the analysis of survival data by Cox regression for each node in the interaction network.
Section 3.2 describes how the score values will be used as input for the maximum-weight connected subgraph (MWCS) problem and a novel algorithmic approach based on mathematical optimization.
Our algorithm solves this problem to provable optimality and, furthermore, is able to enumerate sufficiently distinct suboptimal solutions.
3.1 Statistics of scoring function 3.1.1 Aggregation statistics of P-values Having annotated each node of the interaction network with experimentally derived P-values, we are faced with the problem to aggregate these P-values into one number.
A simple aggregation statistics proposed in the literature is the so-called Fishers method, which combines n P-values pi by 2 n i=1 log(pi)2(n) (Fisher, 1948).
This method however does not provide the necessary flexibility to control the number of significant P-values, instead it only provides a significance measurement over the entire set of P-values.
Due to the heterogeneous nature of the data a more flexible approach is i224 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i225 i223i231 Identification of optimal PPI subgraphs required.
Therefore we use an aggregation statistic based on the distribution of the order statistics of P-values.
Let X1,...,Xn be independently identically distributed (iid) then the density of the ith smallest observation X(i) is given by fX(i) (x)= n!
(ni)!(i1)!
f (x)F(x) i1(1F(x))ni, (1) where F(x) denotes the probability density function of Xi, for i=1,...,n (Lindgren, 1993).
Now we propose to aggregate the P-values at each node in the network by asking for its ith order statistic of the associated P-values, resulting in one P-value of P-values.
Because P-values are uniformly distributed under the null hypothesis (Wasserman, 2005), we apply Equation (1) with density fX (x)=1 and density function FX (x)=x and get X(i) n!(ni)!(i1)!
1x i1(1x)ni, 0x1, (2) or, in other words, X(i) B(i,ni+1) with the associated cumulative distribution function FX(i) = n!
(ni)!(i1)!
x 0 zi1(1z)ni dz, where B(,) denotes the beta distribution.
Applying Equation (2), each gene in the network can be assigned an overall P-value given by the ith order statistic.
This approach is also applicable in case of missing data: for missing P-values the ith order statistic can be used after correcting the parameter n in Equation (2) appropriately.
3.1.2 Signal-noise decomposition Based on these aggregated P-values we derive our new scoring function.
Following Pounds and Morris (2003) we consider the distribution of the P-values as a mixture of a noise and a signal component.
The signal is assumed to be B(a,1) distributed whereas the noise is B(1,1)= uniform (0,1) distributed (P-values under the null hypothesis).
The B(a,b) distribution is given by f (x)= (a+b) (a)(b) xa1(1x)b1, where () denotes the gamma function.
Thus the distribution of the derived P-values reduces to f (x |a,)=+(1)axa1 for 0<x1;0<a<1 with mixture parameter and shape parameter a of the beta distribution.
For given data x=x1,...,xn the log likelihood is defined as logL(,a;x)= n i=1 log(+(1)axa1i ), and consequently the maximum-likelihood estimations of the unknown parameters are given by [,a]=argmax,aL(,a;x).
We obtain both parameters by numerical optimization using the L-BFGS-B method (Byrd et al., 1995) as implemented in R. For the lymphoma dataset analyzed here we obtained a value of 0.536 for the mixture parameter and 0.276 for the shape parameter a of the beta distribution.
This relates to signal and noise proportions of 46.4% versus 53.6%, respectively.
Since P-values are uniformly distributed under the null hypothesis the noise component will be adequately modeled by a uniform distribution.
Modeling the signal component by a beta distribution is justified by Figure 1 and a QuantileQuantile (QQ) plot (data p-values (second order statistics) D en si ty 0.0 0.2 0.4 0.6 0.8 1.0 10 8 6 4 2 0 12 14 Fig.1.
The fitted mixture model fits nicely the empirical distribution.
The parameters of the mixture model are a=0.276 and =0.563.
The histogram of the observed P-values displays the strong consistency with the expected densities under the fitted model (red line).
The blue line indicates the fraction of P-values derived from the uniform noise model.
The excellent fit of the model has been confirmed in a QQ plot of the fitted distribution versus the empirical P-value distribution (data not shown).
not shown).
This is furthermore supported by the associated QQ plot of the fitted density function with the empirical distribution function, which is extremely close to a straight line (data not shown).
These analyses indicate that the signal is well-captured by the beta distribution.
Our aim is to develop an additive score, where positive values signify signal content and negative values denote background noise.
Inspired by the ideas of the likelihood ratio test our approach is as follows: for the fitted parameter a the signal component is equal to the B(a,1) density, whereas that of the noise component is given by B(1,1).
Since B(1,1) is equivalent to the uniform distribution the denominator is 1 for the score, which is given by S(x)= log ( B(a,1)(x) B(1,1)(x) ) = log(a)+(a1)log(x).
Obviously for a1 the density of the signal component converges to that of the background model and consequently the score converges to 0 for all x.
In particular even very low P-values will be scored zero.
Moreover, for a fitted parameter set a and : S(x) x1 log(a) and S(x)x0+.
This demonstrates that the score properly combines the parameters a and.
Similar to classical hypothesis tests where a certain significance level is proposed, we derive a threshold value that discriminates signal from noise.
As detailed in (Pounds and Morris, 2003) the mixture model allows the estimation of the FDR.
From this we calculate a threshold P-value (FDR), which controls the FDR for the positively scoring P-values.
Thus we derive an adjusted log likelihood ratio score given as SFDR(x)= log ( axa1 aa1 ) = (a1)(log(x)log( (FDR))) , (3) i225 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i226 i223i231 M.T.Dittrich et al.Fig.2.
Scoring of combined P-values.
All genes have been assigned to the ABC and GCB subtype-based fold changes.
The x-axis shows coefficients of the univariate Cox proportional hazards regression model fitted for each gene separately.
A coefficient greater than zero denotes an increased risk association.
Genes scoring positively by our combined scoring function (for a FDR of 0.01) are colored.
This evidences that our score selects genes specifically associated with the different malignancy of the two tumor subtypes.
thus adjusted and unadjusted scores differ by an additive offset dependent on the parameter.
Here, the value is the significance threshold.
P-values below this threshold are considered to be significant and will score positively whereas those above the threshold are assumed to have arisen from the null model and will be assigned negative scores.
It can easily be seen that for 0 the score S(x) and for 1 all scores will be positive and our FDR will be equal to , the mixture parameter of the noise model.
Under the null hypothesis, the xi are iid and the subnetwork score is consequently given by the sum over all protein scores of the subnetwork: SFDRnet = xinet SFDR(xi), where net denotes the set of all P-values in the network to be scored.
Obviously, under this assumption the expectation value of the network score SFDRnet scales linearly with network size.
Similar as in the case of local sequence alignments an appropriate choice of the FDR is essential to ensure a negative subnet score to guarantee locality of the solution.
Otherwise, however, the optimization would tend to collect as many genes as available to increase the score regardless of an underlying biological signal.
Analyzing our lymphoma network we search for genes that are differentially expressed between the GCB and ABC DLBCL subgroups and, in addition, show an association with overall survival (Fig.2).
To aggregate the derived P-values from gene expression analysis and Cox regression we use the second-order statistic as detailed above.
Our score thus combines information about the classification of these tumor subtypes with information about prognosis association.
As illustrated in Figure 2 our data contain a strong signal that can be captured by an adequate combination of these two different aspects.
Thus, it becomes evident that the ABC subtype characteristically over-expresses genes with an association for a higher risk, whereas in the GCB subtype mainly genes associated with a better prognosis are over-expressed.
Hence we search for interaction modules that specifically contribute to the malignant behavior of the ABC subtype as compared to the more benign GCB subtype.
3.2 Mathematical optimization algorithm Combinatorially, the problem from the previous section can be cast as finding an optimal-scoring subgraph in a vertex-weighted graph: Problem 1.
(Maximum-Weight Connected Subgraph Problem, MWCS) Given a connected undirected, vertex-weighted graph G = (V ,E,w) with weights w :V R, find a connected subgraph T = (VT ,ET ) of G, VT V, ET E, that maximizes the score w(T ) := vVT w(v).
It is easy to see that any solution for MWCS can always be trimmed to a tree of same weight, and, if all node weights are positive, an optimal solution is easily computed by determining any spanning tree.
In case of both positive and negative edge weights, finding the MWCS is not so easy.
In fact, in the supplement of Ideker et al.(2002), Karp has shown that MWCS is an NP-complete problem, and the authors use this as a justification for their heuristic search method.
We propose to solve this problem to provable optimality using techniques from mathematical programming.
More precisely, we transform the problem into the well-known prize-collecting Steiner tree problem (PCST) and use a mathematical programming-based algorithm for PCST to find subgraphs of maximum weight.
As our computational results in the next section show, this approach is very successful and reliable in that it finds provably optimal subnetworks in short computation time for all biologically relevant instance sizes.
The PCST problem occurs in classical applications from Operations Research such as planning district heating or telecommunications networks, where profit-generating customers and a connecting network have to be chosen in the most profitable way.
Formally, it can be stated as follows: Problem 2.
(Prize-Collecting Steiner Tree Problem, PCST) Given a connected undirected vertex-and edge-weighted graph G= (V ,E,c,p) with vertex profits p :V R0 and edge costs c : E R0, find a connected subgraph T = (VT ,ET ) of G, VT V, ET E, that maximizes the profit p(T ) := vVT p(v) eET c(e).
Similar to MWCS, every optimal solution T can be reduced to a tree.
Now, let (G,w) be an instance of MWCS with positive and negative vertex weights, and let w =minvV (G)wv be its smallest node weight.
We construct an instance (G,p,c) of PCST by setting the vertex profits to p(v)=w(v)w for all vV and the edge costs to c(e)=w for all eE.
Clearly, this is a valid PCST instance since all vertex profits and edge costs are positive.
Figure 3 illustrates the transformation.
The following theorem justifies that we can concentrate on the transformed instance in order to solve the MWCS problem.
Theorem 1.
A prize-collecting Steiner tree T in the transformed instance (G,p,c) is a connected subgraph in (G,w) with w(T )= p(T )w. Proof.
Obviously, T is a connected subgraph.
First, observe that its profit is given by p(T )= vVT p(v) eET w = vVT p(v)+|VT 1|w , (4) i226 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i227 i223i231 Identification of optimal PPI subgraphs Fig.3.
Example of an MWCS instance (a) and its transformed PCST counterpart (b).
The minimum weight in (a) is 15.
Optimal solutions are marked in red color.
The MWCS has weight 36, the optimal PCST has profit 12675=51(=36+15).
since T is a tree.
The score of T is w(T )= vVT w(v) = vVT (p(v)+w) = vVT p(v)+|VT |w ()=p(T )w. Corollary 1.
A maximum-weight connected subgraph in (G,w) corresponds to an optimal prize-collecting Steiner tree in the transformed instance (G,p,c).
In fact, the two problems are very related.
It is not difficult to give also a reduction from PCST to MWCS: we just have to split each edge eE and set the weight of the newly created vertex to c(e).
This simple reduction to the NP-complete PCST problem gives an alternative NP-completeness proof for MWCS.
Having reduced the MWCS problem to the PCST problem we briefly summarize the algorithm from Ljubic et al.(2006), which we use to find provably optimal solutions to the MWCS problem.
This mathematical programming-based algorithm is currently the fastest way to solve PCST problems to optimality and works very well on the transformed MWCS instances.
Mathematical programming is a powerful tool to address NPhard combinatorial optimization problems (Nemhauser and Wolsey, 1988).
Starting from an ILP formulation modeling the problem under consideration, i.e.a linear program with integer variables, sophisticated techniques like cutting plane methods or Lagrangian relaxation can be combined with branch-and-bound to generate provably optimal solutions.
Of course, these methods do not guarantee polynomial-running time in the general case.
For many practically relevant instances, however, techniques from mathematical programming work astonishingly well.
The advantages over ad hoc heuristic methods are threefold: (i) having provably optimal solutions at hand allows evaluating the quality of a model, e.g.the appropriateness of an objective function.
(ii) Methods from mathematical programming guarantee the quality of solutions, i.e.each new feasible solution comes with a maximal distance to an optimal solution.
This allows the implementation of a trade-off between running time and solution guarantee.
(iii) ILP formulations can be interpreted as polyhedra in high-dimensional space.
Mathematical analysis of these objects often leads to new insights into understanding the original problem.
The algorithm from Ljubic et al.(2006) starts by applying a number of preprocessing steps to simplify the input network.
Then, it transforms the remaining network into a directed graph by introducing an artificial root vertex r and by splitting each original edge into two directed edges, or arcs, of opposite directions.
Arc weights and additional arcs from the root to the nodes in the network are set such that a feasible Steiner arborescence, i.e.a directed tree rooted at r, in which only one arc is incident to r, corresponds to a PCST of equal weight.
The algorithm then works on an ILP built on the transformed directed graph.
Each vertex and arc has an associated binary variable modeling its presence or absence in the solution.
A number of linear inequalities constrain the solution vector to represent a feasible Steiner arborescence.
Besides a degree constraint for the artificial root, a class of constraints ensuring that for each chosen vertex exactly one incoming edge must be chosen as well, the model concentrates on the connectedness of the solution: An exponentially large class of inequalities, the cut constraints, ensure that for every selected vertex, which is separated from r by a cut, there must be an arc crossing this cut.
Due to their large number, cut constraints are not considered at once, but iteratively added to the formulation if violated by the current solution.
This technique, combined with a linear programming-based branch-and-bound algorithm, is called branchand-cut and works particularly well if violated inequalities can be found in polynomial time.
Here, this is the case since violated cut constraints can be detected by a maximum-flow algorithm in a support graph with arc-capacities given by the current linear programming solution.
The above algorithm outputs one optimal solution.
In practice, users often like to obtain a list of promising solutions for manual inspection.
Instead of applying straightforward deletion and re-iteration, we propose a different approach to generate suboptimal solutions: in our ILP approach, binary variables xv determine the presence of nodes in the optimal subgraph S, that is, xv =1 if vV (S) and xv =0 otherwise.
Now let S be an optimal subnetwork as identified by the branch-and-cut algorithm.
Adding the Hamming distance-like inequality vV (S) (1xv)|V (S)| with [0,1] and re-optimizing leads to a best solution differing in at least |V (S)| nodes from S. This procedure can be iterated k times.
The advantages of this strategy are 2-fold: first, the user can determine the number k of suboptimal solutions that should be reported and, second, he or she may adjust the variety of solutions via the parameter.
4 RESULTS 4.1 Functional modules in lymphoma network Using our novel approach we identify the optimal-scoring subnetwork (Fig.4) for the combined score using a restrictive FDR of 0.001.
This subnetwork consists of 46 nodes and has a cumulative score of 70.2.
The 37 positive-scoring nodes attain a weight of 102.9 and the 9 negatively scoring nodes have a i227 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i228 i223i231 M.T.Dittrich et al.Fig.4.
Optimal subnetwork detected using a score based on the P-values of a gene-wise two sided t-test and an univariate Cox regression hazard model.
A restrictive threshold equivalent to an FDR of 0.001 was used.
The derived subnetwork captures the characteristically differentially expressed-interaction modules associated with the increased malignancy of the ABC subtype.
Coloring is according to the fold change where red denotes an over-expression in ABC and green in GCB.
Diamond nodes represent negative-scoring genes additionally included in the optimal solution.
score of 32.8.
The theoretical upper bound of the solution in a completely connected graph, given by the cumulative score of all positive nodes, is in this case 145.4.
For the given network and under these restrictive conditions our algorithm collects 70.8% of all positive scores.
Figure 5 shows the next best solutions with =0.5.
Further we capture interactome modules that have been described to play major biological roles in the GCB and ABC DLBCL subtypes.
For example, the proliferation module which is more highly expressed in the ABC DLBCL subtype (Rosenwald et al., 2002) is also evident in our current analysis and includes the genes MYC, CCNE1, CDC2, APEX1, DNTTIP2 and PCNA.
Likewise, genes IRF4, TRAF2 and BCL2, which are associated with the potent and oncogenic NFB pathway, also clustered together as illustrated in Figure 4.
Whereas the two previously described interactome modules were derived from genes/proteins expressed in the malignant cells, our algorithm also identified interactome modules (Fig.6) derived from non-malignant by-stander cells in the lymphoma specimens.
In particular, Fibronectin, SPARC, MMP9, CTSK, ITGA5 and ITGB5 showed tight clustering and represent proteins that are expressed in non-malignant fibroblasts and histiocytes (Rosenwald et al., 2002).
4.2 Validation To validate the performance of our approach including the scoring function and search algorithm we simulated an artificial module according to Rajagopalan and Agarwal (2005).
Based on the topology of our lymphoma network we selected two subnetworks of biologically relevant sizes (46 and 143) as signal components.
Following the proposal of Rajagopalan and Agarwal (2005) we set signal P-values uniformly distributed between 0 and 103 and background noise P-values uniformly distributed between 0 and 1.
Fig.5.
Examples of suboptimal solutions corresponding to the optimal solution depicted in Figure 4.
A Hamming distance of 50% was requested for these solutions.
Both subgraphs share 23 nodes with the optimal solution (circles) but also include new ones (triangles).
The upper solution achieves a score of 61.5 (87.7%), the lower solution has a score of 52.5 (74.8%) as compared to the optimal solution (70.2).
The addition of FGFR1 (first and second suboptimal solution) and GRB2 (first suboptimal solution) within the by-stander cell module highlights the biologically relevant interaction between the malignant B-cells and the non-neoplastic network of by-stander cells.
Fig.6.
Optimal subnetwork detected using a score based on the P-values of a one sided t-test for over-expression in GCB and survival as in Figure 4 for an FDR of 0.05.
Since our approach allows for the finetuning of the signal noise decomposition by the FDR we scan a large range of FDRs and evaluate the obtained solutions in terms of recall (true-positive rate) and precision (ratio of true positives to all positively classified).
To assess the variability of the solutions we ran 10 repetitions for each single FDR step.
The silhouette of the recall/precision curve, (adapted from Sing et al., 2005) for the module of size 46 includes the optimal solutions with a maximum recall and precision of exactly one (Fig.7, upper plot).
In particular we find a large i228 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i229 i223i231 Identification of optimal PPI subgraphs Fig.7.
Plot of the recall versus precision of a batch of solutions calculated for wide range of FDRs with 10 replications each.
For the algorithm by Ideker et al.(2002) we display the convex hulls of solutions obtained by applying their algorithm recursively three times to five independent simulations.
We evaluated two different signal component sizes (46, upper plot and 143, lower plot) with the same procedure.
Clearly, the presented exact approach captures the signal with high precision and recall over a relatively large range of FDRs.
None of the solutions delivered by the heuristic approach falls within the upper right region of high precision and high recall (colored in yellow).
For better visualization data points have been jittered in y-direction.
number of solutions covering the FDR range of 0.7 to 0.3 in the upper right region with a recall and precision higher than 80%.
We contrast the performance of our approach to that of Ideker et al.(2002) as implemented in the Cytoscape (Cline et al., 2007; Shannon et al., 2003) plugin jActiveModules.
Since their algorithm provides no adjustable scoring function, we follow the proposal of Ideker et al.(2002) and recursively apply their algorithm to the obtained solution three times for five independent simulations.
Thus we obtain three discrete solution spaces visualized as shaded polygons representing their convex hulls in Figure 7.
Clearly none of these solutions falls within the region of high precision and recall in the upper right corner.
Instead one obtains a set of overly large subnetwork constisting of 865 nodes on average, corresponding to 42.5% of the entire network and 18.8 times the size of the hidden signal component.
This is reflected by a poor precision of 0.05.
After two recursive iterations the number of false positives was reduced substantially and the resultant subnetworks were considerably smaller ranging from 11 to 36 nodes.
However, this solution displayed a large variance especially of the recall ranging from 23 to 71%.
A similar behavior was observed for the larger module (size 143), see Figure 7, lower plot.
5 DISCUSSION In the recent years, the integrated analysis of gene expression data in the context of PPI has received considerable attention (Cabusora et al., 2005; Guo et al., 2007; Ideker et al., 2002; Nacu et al., 2007; Rajagopalan and Agarwal, 2005; Sohler et al., 2004).
The main objective of these analyses is the derivation of biologically interesting subnetworks of interpretable size from large scale PPI data.
This can be expressed as the problem of finding optimal-scoring subgraphs as stated, for the first time, by Ideker et al.(2002).
Here we transform the problem to the wellknown PCST problem from Operations Research.
Thus, we give an alternative NP-completeness proof and, more importantly, we are able to solve large instances of this problem in reasonable computation time to provable optimality by an ILP approach for the transformed problem.
Additionally, this allows us to calculate suboptimal solutions with given Hamming distances to previously found solutions.
We present an application of our approach using a large PPI network from HPRD (Mishra et al., 2006; Peri et al., 2003) in combination with the comprehensive and well-established microarray dataset from lymphoma patients (Rosenwald et al., 2002).
This dataset also provides valuable information of patients survival which can be used in a Cox regression hazard model to measure the contribution of each gene to malignancy of the tumor.
In contrast to previous studies we do not restrict our analysis to differences in expression between conditions (in our case two lymphoma subtypes) but also include the P-values of the Cox regression into our analysis to derive functional modules that are specifically associated with the different malignant behavior of the tumor subtypes.
In an effective-algorithmic approach, a well-defined objective function is as important as a good search procedure.
Therefore we first combine the set of P-values derived from various experiments by an order statistics-based approach to obtain a P-value of P-values as a scalable measure of overall significance.
Then we fit a beta-uniform mixture model on the entire set of raw P-values of all nodes in the interaction network.
Thus, we achieve a signalnoise decomposition on which we deduce a scoring function of P-values as a likelihood ratio of the signal and noise component.
Thus, we deduce a scalable scoring function with a meaningful interpretation of the adjustment parameter as FDR.
The additivity of this logarithmic score allows us to effectively formulate and exactly solve the problem of optimal subgraph identification by an ILP approach.
Inspired by the problem of finding local-sequence alignments we strive to identify local maximal-scoring network regions.
Given a negative-expectation value of the scoring functions as realized i229 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i230 i223i231 M.T.Dittrich et al.by an appropriate choice of the FDR, we achieve an efficient localization of the resulting region of interest.
Our approach makes it possible to fine-tune the size of the resultant subnetworks and thereby zoom into the maximal-scoring region of interest in the interaction network.
Our order statistics-based approach to aggregate the P-values from different experiments is equivalent to that adopted by Ideker et al.(2002).
However, we explicitly allow the user to require a predefined number of P-values to be significant (e.g.the first, second,... , nth order statistic) instead of only taking the maximum over all order statistics, which is naturally included in our approach.
However, asking for the maximum only can lead to serious problems in cases of highly variable signal content among the P-values of different experiments where the highest signal content will dominate the resulting score.
As a case in point, analyzing our lymphoma network, the algorithm of Ideker et al.(2002) only yields solution based on the gene expression P-value.
Obviously, the biologically important but statistically weaker signal cannot be detected in combination with a more dominant signal by this approach.
To our knowledge, the presented method allows for the first time the exact decomposition of PPI networks into optimal-scoring subnetworks and suboptimal networks of a given dissimilarity as defined by the Hamming distance of the graphs node-incidence vectors.
In contrast to previously published methods, our algorithm computes provably optimal solutions without computationally demanding parameter optimization usually necessary in heuristic approaches.
Furthermore, heuristic methods do not guarantee to find the optimal solution and are unable to assess the solution quality.
As a representative of these heuristic approaches we chose the algorithm of Ideker et al.(2002) as implemented in Cytoscape (Cline et al., 2007; Shannon et al., 2003) and compared the performance to that of our exact approach.
The results clearly demonstrate the shortcomings of the heuristic approaches.
Since recursive applications of the algorithm are required, only a limited number of isolated solution sizes can be obtained.
None of the solutions comes close to the high-accuracy region showing both, high precision and high recall.
The high number of true positives in the first run is paid for with an exorbitantly high number of false positives as reflected by the sizes of the results from the initial run.
On average 865 nodes were reported for networks with the small signal component of 46 in the first step, corresponding to a true positive (TP)/false positive (FP) ratio of 18.8.
A subsequent application of the algorithm yielded on average networks of 75 nodes, 46.4% of which where true positives.
Precision can still be improved by a third run but this comes at the expense of recall rates down to 24%.
This is an effect of the nestedness of the recursive solutions, where true positives neglected in one step will not be contained in any later solution.
Although Ideker et al.(2002) claim that many high-scoring subnetworks highlight biologically interesting regions although not being optimal in the sense of the objective function it must be kept in mind that the solutions provided by their algorithm are quite variable and heavily dependent on the choice of the parameter settings (seed, number of iterations and annealing temperature).
More importantly, the scoring system of Ideker et al.(2002) and those related to it lack an explicit signal/noise decomposition and thus provide no estimation about the size of the signal content.
This can pose a serious obstacle for these approaches in the case of low-signal content or the even worse scenario of random noise only.
Applying batches of P-values randomly drawn from a uniform(0,1) distribution to our network the implementation of Ideker et al.(2002) still reports solutions of 770 nodes (37% of the entire network) on average with scores within the same range as those of containing the signal modules.
Subsetting these solutions by reapplying the algorithm still yields networks of sizes between 130 and 210 nodes.
Obviously a major drawback of these scoring systems is that due to the lacking estimate of the signal content prior to the search phase no distinction between a true-signal component and a best noise aggregate can be made.
This problem is solved by the integrated signal-noise decomposition based on a beta-uniform mixture model of our approach.
In fact all tests with random P-values as input yielded a fitted model with a mixture parameter of 1 corresponding to a signal content of 0.
Consistent with that we obtain a parameter a=1 of the signal beta distribution and consequently a score of zero for all nodes (Equation 3).
Nevertheless, heuristic approaches may be able to deliver biologically relevant solutions as claimed by Ideker et al.(2002) if the proportion of signal is high enough.
Especially in case of lowsignal content the biological relevancy of the obtained solution may be questionable, and even after recursive application of the algorithm the quality of the obtained subnetwork is hardly assessable due to the high variability.
Inherently, all published heuristic methods based on the approach of Ideker et al.(2002) share one of the discussed drawbacks either in terms of search algorithm or scoring function.
Therefore it is highly desirable to attain truly optimal solutions with an explicit estimate of the signal content and control of the FDR as provided by the presented approach.
We emphasize that, despite the underlying computational complexity, our algorithm runs very fast on biologically relevant instance sizes: our software tool heinz computes provably optimal results usually in a few minutes; profiling our implementation we measured a median runtime of 182 seconds for test runs on 1000 score-permuted graphs.
Most importantly we demonstrate that our approach discovers biologically meaningful modules in a lymphoma interaction network which include and extend the results reported by Rosenwald et al.(2002) which have been described to be of importance in the pathogenesis of the GCB and ABC DLBCL subtypes.
In general, the integration of survival and expression data into the analysis of PPI networks exhibits perturbed interaction modules associated with the malignancy of the tumor and can yield new insights into tumor biology on a cellular level.
In the future, we plan to generalize our method to an even broader application setting.
As a first step, we propose to integrate edge weights, which could, for example, be derived from correlation of gene expression as used by Guo et al.(2007) or from P-values of interaction predictions with STRING (von Mering et al., 2007).
Furthermore, we intend to provide an interface to non-commercial optimization libraries and to integrate our algorithm into the Bioconductor environment (Gentleman et al., 2004).
ACKNOWLEDGEMENTS G.W.K.
thanks A. Bley for helpful discussions and I. Ljubic and A. Moser for support with the dhea code.
MTD thanks the IZKF (MD/PhD program) and the SFB688 (TPA2).
Conflict of Interest: none declared.
i230 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i231 i223i231 Identification of optimal PPI subgraphs
ABSTRACT Motivation: Gene regulatory networks underlying temporal processes, such as the cell cycle or the life cycle of an organism, can exhibit significant topological changes to facilitate the underlying dynamic regulatory functions.
Thus, it is essential to develop methods that capture the temporal evolution of the regulatory networks.
These methods will be an enabling first step for studying the driving forces underlying the dynamic gene regulation circuitry and predicting the future network structures in response to internal and external stimuli.
Results: We introduce a kernel-reweighted logistic regression method (KELLER) for reverse engineering the dynamic interactions between genes based on their time series of expression values.
We apply the proposed method to estimate the latent sequence of temporal rewiring networks of 588 genes involved in the developmental process during the life cycle of Drosophila melanogaster.
Our results offer the first glimpse into the temporal evolution of gene networks in a living organism during its full developmental course.
Our results also show that many genes exhibit distinctive functions at different stages along the developmental cycle.
Availability: Source codes and relevant data will be made available at http://www.sailing.cs.cmu.edu/keller Contact: epxing@cs.cmu.edu 1 INTRODUCTION Many biological networks bear remarkable similarities in terms of global topological characteristics, such as scale-free and smallworld properties, to various other networks in nature, such as social networks, albeit with different characteristic coefficients (Barabasi and Albert, 1999).
Furthermore, it was observed that the average clustering factor of real biological networks is significantly larger than that of random networks of equivalent size and degree distribution (Barabasi and Oltvai, 2004); and biological networks are characterized by their intrinsic modularities (Vszquez et al., 2004), which reflect presence of physically and/or functionally linked molecules that work synergistically to achieve a relatively autonomous functionality.
These studies have led to numerous advances towards uncovering the organizational principles and functional properties of biological networks, and even identification of new regulatory events (Basso et al., 2005).
However, most such results are based on analyses of static networks, i.e.networks with invariant topology over a given set of molecules.
One example is a proteinprotein interaction (PPI) network over all proteins of an organism, regardless of the conditions under which individual interactions may take place.
To whom correspondence should be addressed.
Another example is a single-gene network inferred from microarray data even though the samples may be collected over a time course or multiple conditions.
A major challenge in systems biology is to understand and model, quantitatively, the dynamic topological and functional properties of cellular networks, such as the rewiring of transcriptional regulatory circuitry and signal transduction pathways that control behaviors of a cell.
Over the course of a cellular process, such as a cell cycle or an immune response, there may exist multiple underlying themes that determine the functionalities of each molecule and their relationships to each other, and such themes are dynamical and stochastic.
As a result, the molecular networks at each time point are contextdependent and can undergo systematic rewiring rather than being invariant over time, as assumed in most current biological network studies.
Indeed, in a seminal study by Luscombe et al.(2004), it was shown that the active regulatory paths in a gene-regulatory network of Saccharomyces cerevisiae exhibit dramatic topological changes and hub transience during a temporal cellular process, or in response to diverse stimuli.
However, the exact mechanisms underlying this phenomena remain poorly understood.
We refer to this time-or condition-specific active parts of the biological circuitry as the active time-evolving network, or simply, time-varying network.
Our goal is to recover the latent time-evolving network of gene interactions from microarray time course.
What prevents us from an in-depth investigation of the mechanisms that drive the temporal rewiring of biological networks during various cellular and physiological processes?
A key technical hurdle we face is the unavailability of serial snapshots of the timeevolving rewiring network during a biological process.
Current technology does not allow for experimentally determining a series of time-specific networks, for a realistic dynamic biological system, based on techniques such as yeast two-hybrid or ChIP-chip systems; on the other hand, use of computational methods, such as structural learning algorithms for Bayesian networks, is also difficult because we can only obtain a few observations of gene expressions at each time point which leads to serious statistical issues in the recovered networks.
How can one derive a temporal sequence of time-varying networks for each time point based on only one or at most a few measurements of node-states at each time point?
If we follow the naive assumption that each temporal snapshot of gene expressions is from a completely different network, this task would be statistically impossible because our estimator (from only the observations at the time point in question) would suffer from extremely high variance due to sample scarcity.
Previous methods would instead pool observations from all time points together and infer a single average network (Basso et al., 2005; Friedman et al., 2000; Ong, 2002), which means they choose to ignore network rewiring and simply assume that the observations are independently and 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i129 i128i136 KELLER: estimating time-varying networks identically distributed.
To our knowledge, no method is currently available for genome-wide reverse engineering of time-varying networks underlying biological processes, with temporal resolution up to every single time point based on measurements of gene expressions.
In this article, we propose kernel-reweighted logistic regression (KELLER), a new machine learning algorithm for recovering timevarying networks on a fixed set of genes from time series of expression values.
KELLER stems from the acronym KERWLLOR, which stands for KErnel ReWeighed l1-regularized LOgistic Regression.
Our key assumption is that the time-evolving networks underlying biology processes vary smoothly across time, therefore temporally adjacent networks are likely to share more common edges than temporally distal networks.
This assumption allows us to aggregate observations from adjacent time points by reweighting them, and to decompose the problem of estimating time-evolving networks into one of estimating a sequence of separate and static networks.
Extending the highly scalable optimization algorithms of 1-regularized logistic regression, we are able to apply our method to reverse engineer a genome-wide interactions with a temporal resolution up to every single time point.
It is worth emphasizing that earlier algorithms, such as the structure learning algorithms for dynamic Bayesian network (DBN) (Ong, 2002), learns a time-homogeneous dynamic system with fixed node dependencies, which is entirely different from our approach, which aims at snapshots of rewiring network.
Our approach is also very different from earlier approaches which start from a priori static networks and then trace time-dependent activities.
For example, the trace-back algorithm (Luscombe et al., 2004) that enables the revelation of network changes over time in yeast is based on assigning time labels to the edges in a priori static summary network.
The Achilles heel of this approach is that edges that are transient over a short period of time may be missed by the summary static network in the first place.
The DREM program (Ernst et al., 2007) reconstructs dynamic regulatory maps by tracking bifurcation points of a regulatory cascade according to the ChIP-chip data over short time course.
This is also different from our method, because KELLER aims at recovering the entire time-varying networks, not only the interactions due to proteinDNA binding, from long time series with arbitrary temporal resolution.
One related approach is the Tesla algorithm by Ahmed et al.(2008).
However, Tesla aims at recovering bursty rather than smoothly varying networks.
We apply our method to reverse engineer the time-evolving network between 588 genes involved in the developmental process during the life cycle of Drosophila melanogaster.
These genes are a subset of the 4028 genes whose expression values are measured in a 66-step time series documented in Arbeitman et al.(2002).
We validate the biological plausibility of the estimated timeevolving network from various aspects, ranging from the activity of functionally coherent gene sets, to previous experimentally verified interactions between genes, to regulatory cascade involved in nervous system development, and to gene functional enrichment.
More importantly, the availability of time-evolving networks gives us the opportunity to further study the rich temporal phenomena underlying the biological processes that is not attainable using the traditional static network.
For instance, such a downstream analysis can be a latent functional analysis of the genes in the time-evolving network appeared in Fu et al.(2008).
The remainder of the article is structured as follows.
In Section 2, we will introduce our kernel reweighted method.
In Section 3, we will use synthetic data and a time series of gene expression data collected during the life cycle of D.melanogaster to show the advantage as well as biological plausibility of estimating a dynamic network.
We conclude the article with a discussion and outlook on future work in Section 4.
2 METHODS First, we introduce our time-evolving network model for gene expression data, then explain our algorithm for estimating the time-evolving network and finally discuss the statistical property and parameter tuning for our algorithm.
2.1 Modeling time series of gene expression Microarray profiling can simultaneously measure the abundance of transcripts from tens of thousands of genes.
This technology provides a snapshot into the cell at a particular time point in a genome-wide fashion.
However, microarray measurements are far from the exact values of the expression levels.
First, the samples prepared for microarray experiments are usually a mixture of cells from different tissues and, possibly, at different points of a cell cycle or developmental stage.
This means that microarray measurements are only rough estimates of the average expression levels of the mixture.
Other sources of noise can also be introduced into the microarray measurements, e.g.during the stage of hybridization, digitization and normalization.
Therefore, it is more robust if we only consider the qualitative level of gene expression rather than its actual value.
That is we model gene expression as either being upregulated or downregulated.
For this reason, we binarize the gene expression levels into X :={1,1} (1 for downregulated and 1 for upregulated).
For instance, for cDNA microarray, we can simply threshold at 0 the log ratio of the expression levels to those of the reference, above which a gene is declared to be upregulated and otherwise downregulated.
At a particular time point t, we denote the microarray measurements for p genes as a vector of random variables X (t) := (X (t)1 ,...,X (t)p )X p, where we have adopted the convention that the subscripts index the genes and the bracketed superscripts index the time point.
We model the distribution of the expression values for these p genes at any given time point t as a binary pair-wise Markov Random Field (MRF): P (t) (X (t)) := 1 Z( (t)) exp  (u,v)E(t) (t)uv X (t) u X (t) v , (1) where (t)uv = (t)vu R is the parameter indicating the strength of undirected interaction between genes u and v; and a (t)uv =0 means that the expression values for genes u and v are conditionally independent given the values of all other genes.
Therefore, a MRF is also associated to a network G(t) with a set of nodes V and a set of edges E (t): V corresponds to the invariant set of genes and hence without the superscript for time; each edge in E (t) corresponds to an undirected interaction between two genes (and a non-zero (t) uv ).
The difference between E (t) and (t)uv can be viewed as follows: E (t) only codes the structure of the model while (t)uv contains all information about the model.
Finally, the partition function Z( (t)) in a MRF normalizes the model to a distribution.
The dynamic interactions between genes underlying temporal biological processes are reflected in the change of the magnitude of parameter (t)uv across time.
In particular, increased values of (t)uv indicate strengthened or emerging interaction between gene u and v, and decreased values indicate weakened or disappearing interaction.
Furthermore, we assume that the dynamic interactions between genes vary smoothly across time.
Mathematically, this means that the change of (t)uv is small across time, i.e.the difference | (t)uv (t+1)uv | is upper bounded by a small constant C. In other i129 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i130 i128i136 L.Song et al.words, the networks at adjacent time points, G(t) and G(t+1) are very similar, i.e.|E (t)E (t+1)|/|E (t)| is lower bounded by a large constant CE (here, we used || to denote the cardinality of a set).
Given time series of gene expression data measured at n time points, D :={x(t1),...,x(tn)}, our goal is to estimate a temporal sequence of networks G :={G(t1),...,G(tn)} with each network for 1 time point.
Note that, we will focus on estimating the structures of the interactions between genes (G(t)) rather than the detailed strength of these interactions ( (t)).
We hope by restricting our attention to estimating the structure, we can obtain better guarantees in terms of the ability of our algorithm to recover the true underlying interactions between genes.
In Sections 2.2 and 2.4, we will provide further explanation on the advantage of focusing on G(t).
Another important point of clarification is that the interactions between genes we are modeling are the statistical dependencies between their expression levels.
This is a common choice for many existing methods, such as the methods by Friedman et al.(2000) and Ong (2002).
Note that statistical dependency is different from causality, which focuses on directed statistical relations between random variables.
In other words, it is more appropriate to view networks from our model as the co-regulation relations between genes.
That is, if there is an edge between two genes in the dynamic network at time point t, then the changes of the expression levels of these two genes are likely to be regulated by the same biological process.
2.2 Estimating time-varying network Two questions need to be addressed when we estimate a time-evolving network.
First, what is the objective to optimize and second, what is the algorithmic procedure for the estimation?
The first question is addressed in this section and it concerns both the consistency and efficiency of our method while the second question only concerns the efficiency of the algorithm, which we will discuss more in Section 2.3.
First, estimating the parameter vector (t) by maximizing log-likelihood is not practically feasible since the evaluation of the partition function Z( (t)) involves a summation of exponential number of terms.
Another approach to address this problem is to use a surrogate likelihood function, which can be tractably optimized.
However, there is no statistical guarantee on how close an estimate obtained through maximization of a surrogate likelihood is to the true parameter (Banerjee et al., 2008).
Therefore, we adapt the neighborhood selection procedure of Wainwright et al.(2006) to estimate the time-evolving network G(t) instead.
Overall, we have designed a method that decomposes the problem of estimating the time-evolving network along two orthogonal axes.
The first axis is along the time, where we estimate the network for each time point separately by reweighting the observations accordingly; and the second axis is along the set of genes, where we estimate the neighborhood for each gene separately and then joining these neighborhoods to form the overall network.
The additional benefit of such decomposition is that the estimation problem is reduced to a set of identical atomic optimization tasks in Equation (3).
In the next section, we will discuss our procedure to solve this atomic optimization task efficiently.
In this new approach, estimating the network G(t) is equivalent to recovering, for each gene uV , its neighborhood of genes that u is interacting with, i.e.N (t)(u) :={vV|(u,v)E (t)}.
It is intuitive that if we can correctly estimate the neighborhood for all genes u in V , we can recover the network G(ti) by joining these neighborhoods.
In this alternative view, we can decompose the joint distribution in Equation (1) into a product of conditional distributions, P (t) (X (t) u |X (t)\u ), each of which is the distribution of the expression value of gene u conditioned on the expression values of all other genes (we use \u to denote the set of genes except gene u, i.e.\u :=V{u}).
In particular, P (t) (X (t)u |X (t)\u ) takes the form of a logistic regression: P (t) \u (X (t)u |X (t)\u )= exp ( 2X (t)u  (t) \u ,X (t) \u ) exp ( 2X (t)u (t) \u ,X (t) \u ) +1 , (2) where a,b=ab denotes inner product and (t)\u :={ (t)uv | v\u} is the (p1)-dimensional sub-vector of parameters associated with gene u.
The neighborhood N (t)(u) can be estimated from the sparsity pattern of the sub-vector (t)\u.
Therefore, estimating the network G(t) at time point t can be decomposed into p tasks, each for the sub-vector (t)\u corresponding to a gene.
For later exposition, we denote the log-likelihood of an observation x under Equation (2) as ( (t)\u ;x)= logP (t)\u (xu|x\u).
Recall that we assume that the time-evolving network varies smoothly across time.
This assumption allows us to borrow information across time by reweighting the observations from different time points and then treating them as if they were i.i.d.
observations.
Intuitively, the weighting should place more emphasis on observations at or near time point t with weights becoming smaller as the observations move further away from time point t. Such reweighting technique has been employed in other tools for time series analyses, such as the short-time Fourier transformation where observations are reweighted before applying the Fourier transformation to capture transient frequency components (Nawab and Quatieri, 1987).
In our case, at a given time point t, the weighing is defined as w(t)(ti) := Khn (t ti)/ n i=1 Khn (t ti), where Khn () :=K(/hn) is a symmetric nonnegative kernel and hn is the kernel bandwidth.
We used the Gaussian RBF kernel, Khn (t)=exp(t2/hn), in our later experiments.
Note that multiple measurements at one time point can be trivially handled by assigning them the same weight.
We consider multiple measurements to be i.i.d.
observations.
Additionally, we will assume that the true network is sparse, or that the interactions between genes can be approximated with a sparse model.
This sparsity assumption holds well in most cases.
For example, a transcription factor only controls a small fraction of target genes under a specific condition (Davidson, 2001).
Then, given a time series of gene expression data measured at n time points, D={x(t1),...,x(tn)}, we can estimate (t)\u or the neighborhood of N (t)(u) of gene u at time point t using an 1 penalized log-likelihood maximization.
Equivalently the estimator (t)\u is the solution of the following minimization problem: (t) \u = argmin (t) \u Rp1 ( n i=1 w(t)(ti) ( (t) \u ;x(ti))+ (t)\u 1 ) , (3) where 0 is a regularization parameter specified by user that controls the size of the estimated neighborhood, and hence the sparsity of the network.
Then, the neighborhood for gene u can be estimated as N (t)(u)= {vV | (t)uv 	=0}, and the network can be estimated by joining these neighborhoods: E (t)= { (u,v)|vN (t)(u) or uN (t)(v) }.
(4) 2.3 Efficient optimization Estimating time-evolving networks using the decomposition scheme described in previous section requires solving a collection of optimization problems given in Equation (3).
In a genome-wide reverse engineering task, there are tens of thousands of genes and hundreds of time points, so one can easily have a million optimization problems.
Therefore, it is essential to develop an efficient algorithm for solving the atomic optimization problem in Equation (3), which can then be trivially parallelized across different genes and different time points.
The optimization problem in Equation (3) is an 1 penalized logistic regression with observation reweighting.
This optimization problem has been an active research area in the machine learning community and various methods have been developed, including interior point methods (Koh et al., 2007), trust region newton methods (Lin et al., 2008) and projected gradient methods (Duchi et al., 2008).
In this article, we employed a projected gradient method due to its simplicity and efficiency.
i130 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i131 i128i136 KELLER: estimating time-varying networks The optimization problem in (3) can be equivalently written in a constrained form: (t) \u = argmin (t)\u 1 C ( n i=1 w(t)(ti) ( (t) \u ;x(ti)) ) , (5) where C is an upper bound for the 1 norm of (t) \u and defines a region in which the parameter lies.
There is an one-to-one correspondence between C in Equation (5) and in Equation (3).
In this formulation, the objective L( (t)\u ) is a smooth and convex function, and its gradient with respect to (t)\u can be computed simply as (t) :=L( (t)\u )= n i=1 w(t)(ti) ( (t) \u ).
The key idea of a projected gradient method is to update the parameter along the negative gradient direction.
After the update, if the parameter lies outside the region , it is projected back into the region , otherwise, we move to the next iteration.
The essential step in the algorithm is the efficiency with which we can project the parameter into the region : (t) \u ( (t) \u (t) ) , (6) where (a) :=argminb{ab|b} is the Euclidean projection of a vector a onto a region.
We employed an approach by Duchi et al.(2008) which involves only simple operations such as sorting and thresholding for this projection step.
Algorithm 1 gives a summary of the projected gradient method for the optimization problem in Equation (3).
Note that the projected gradient algorithm has several internal parameters , and , which, in our experiments, we set to typical values given in the literature (Bertsekas, 1999).
Algorithm 1 Projected Gradient Method for Equation (5) Input: A time series D={x(t1),...,x(tn)}, an upper bound C Output: (t)\u 1: Initialize (t)\u , (t) \u , set =0.1, =106, =102 2: repeat 3: (t) \u (t) \u , 1.0 4: repeat 5: (t) \u ( (t) \u (t) ) , 6: until L( (t)\u )L( (t) \u )(t)( (t) \u  (t) \u ) 7: until (t)\u  (t) \u 8: (t) \u (t) \u 2.4 Statistical property The main topic we discuss here is whether the algorithm described in Section 2.2 can estimate the true underlying time-evolving network correctly.
In order to study the statistical guarantees of our algorithm, we need to take three aspects into account.
First, a genome-wide reverse engineering task can involve tens of thousands of genes while the number of observations in time series can be quite limited (hundreds at most).
Therefore, it is important to study the case in which the dimension p scales with respect to the sample size n, but still allows for recovery of networks.
Second, the time-evolving nature of networks adds extra complication to the estimation problem, so, we have to take the amount of change between adjacent networks, C :=maxuv (t)uv (t+1)uv , into account.
Third, the intrinsic properties of the interactions between genes will also affect the correct recovery of the networks.
Intuitively, the more complicated interactions the more difficult it is to recover networks, e.g.each gene interacts with a large fraction of other genes.
In other words, the maximum size of the neighborhood for a gene CN :=maxuV N (u) is also a deciding factor.
To our knowledge, none of the earlier methods (Basso et al., 2005; Friedman et al., 2000; Ong, 2002) provide a statistical guarantee for recovered networks or are amenable to such analysis.
In contrast, the method we presented in Section 2.2 is highly amenable to a rigorous statistical analysis.
Statistical guarantees have been provided for estimating static networks under the model in Equation (1) (Wainwright et al., 2006) and we can extend them to the time-varying case.
A detailed proof of a similar result for our approach is beyond the scope of this article and deserves a full treatment in a separate paper.
At a high level, we can show that under a set of suitable conditions over the model, C , CN , hn and , with high probability, we can recover the true underlying time-evolving network even when the number of genes p is exponential in size of the number of observations n [for details of the proof, see M.Kolar and E.Xing (submitted for publication)].
A different analysis have been provided for time-varying Gaussian graphical models (Zhou et al., 2008), in which the consistency of the interaction strengths is addressed, but not the consistency of the network topology.
2.5 Parameter tuning The regularization parameter controls the sparsity of the estimated networks.
Large values of result in sparse networks, while small values result in a dense networks that have higher log-likelihood, but more degrees of freedom.
We employ the Bayesian Information Criterion (BIC) for choosing that trades off between the fit to the data and the model complexity.
More specifically, we use an average of the BIC score defined below for each time point t and for each gene u: BIC(t,u) := n i=1 w(t)(ti) ( (t) \u ;x(ti)) log(n) 2 Nz( (t)\u ) (7) where Nz() counts the number of non-zero entries in (t)\u.
Then the final score is BIC :=1/n|V|uVnj=1 BIC(tj,u).
A larger BIC score implies a better model.
The bandwidth parameter hn controls the smoothness of the change in the time-evolving networks.
Using wide bandwidths effectively incorporate more observations for estimating each network snapshot, but it also risks missing sharp changes in the network; using narrow bandwidths makes the estimate more sensitive to sharp changes, but this also makes the estimation subject to larger variance due to the reduced effective sample size.
In this article, we use a heuristic for tuning the initial scale of the bandwidth parameter hn: we first form a matrix (dij) with its entries dij := (titj)2 (i={1,...,n}).
Then the scale of the bandwidth parameter is set to the median of the entries in (dij).
Intuitively, the bandwidth parameter reflects the characteristic interval between time points.
In our simulation experiments, we find that this heuristic provides a good initial guess for hn, and it is quite close to the value obtained via more exhaustive search.
3 EXPERIMENTS In this section, we use synthetic data to demonstrate the advantage of estimating a time-evolving network, and we used data collected from Drosophila to show that our method, KELLER, can estimate a biologically plausible time-evolving network and reveal some interesting properties of the dynamic interactions between genes.
3.1 Recovering synthetic networks In this section, we compare KELLER with structural learning of DBN (Friedman et al., 2000) and 1-regularization logistic regression for static network estimation using synthetic networks.
Note that 1-regularization logistic regression can be obtained from KELLER: we only need to apply a uniform weight w(ti)=1/n to all observations and estimate a single network using the same objective as Equation (5).
i131 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i132 i128i136 L.Song et al.Fig.1.
As we increase the number of i.i.d.
samples at each time point, KELLER estimating a time-evolving network clearly outperforms DBN and 1-regularized logistic regression for estimating a static network.
Subplot (a) displays the performances for the overall networks, while (b) and (c) display the performance for the static edges and dynamic edges, respectively.
Starting at time t1, we generate an ErdsRnyi random graph G(t1) of p=50 nodes with an average degree of 2.
The parameter (t1)uv for the non-zero edges is chosen uniformly random from the range [1,2].
Then, we randomly select 15 edges and gradually decrease their weights to zero in 200 time points.
Starting from t1, we also chose 15 new edges and gradually increase their weights to a random target value between [1,2] in 200 time points.
Therefore, in the first 200 discrete time steps, 15 existing edges are deleted, 15 new edges are added and the final network maintains an overall size of 50 edges.
We call these 30 time-evolving edges as dynamic edges and the remaining 35 edges as static edges.
Then from time point 200, we start the second cycle of deleting 15 edges and adding 15 edges.
This cycle is repeated five times, which results in a smooth timeevolving network with n=1000 time point.
Furthermore, we draw 10 i.i.d.
observations from the network at each time point and study how the performance of different methods scales with the number of i.i.d.
observations at each time point.
We evaluate the estimation procedures using an F1 score, which is the harmonic mean of precision (Pre) and recall (Rec), i.e.F1 :=2PreRec/Pre+Rec.
Precision is calculated as 1/n n i=1 |E (ti)E (ti)|/|E (ti)|, and recall as 1/n n i=1 |E (ti)E (ti)|/|E (ti)|.
The F1 score is a natural choice of the performance measure as it tries to balance between precision and recall; only when both precision and recall are high can F1 be high.
Furthermore, we use an initial bandwidth parameter hn as explained in Section 2.5, then searched over a grid of parameters (10[0.5:0.1:0.5] for and hn[0.5,1,2,5,10,50] for the bandwidth), and finally chose one that optimizes the BIC criterion defined in Section 2.5.
When estimating the static network, we use the same range to search for.
The recovery results for the overall time-evolving network, the dynamic and static edges, are presented, respectively, in Figure 1.
From the plots, we can see that estimating a static network does not benefit from increasing number of i.i.d.
observations at all.
In contrast, estimating a time-varying network always obtains a better performance and the performance also increases as more observations are available.
Note that these results are not surprising since our time-varying network model fits better the data generating process.
As time-evolving networks occur very often in biological systems, we expect our method will also have significant advantages in practice.
3.2 Recovering time-evolving interactions between genes in D.melanogaster Over the developmental course of D.melanogaster, there exist multiple underlying themes that determine the functionalities of each gene and their relationships to each other, and such themes are dynamical and stochastic.
As a result, the gene-regulatory networks at each time point are context-dependent and can undergo systematic rewiring, rather than being invariant over time.
In this section, we use KELLER to reverse engineer the dynamic interactions between genes from D.melanogaster based on a time series of expression data measured during its full life cycle.
We use microarray gene expression measurements collected by Arbeitman et al.(2002) as our input data.
In such an experiment, the expression levels of 4028 genes are simultaneously measured at various developmental stages.
Particularly, 66 time points are chosen during the full developmental cycle of D.melanogaster, spanning across four different stages, i.e.embryonic (130 time point), larval (3140 time point), pupal (4158 time points) and adult stages (5966 time points).
In this study, we focused on 588 genes that are known to be related to developmental process based on their gene ontologies.
We use a regularization parameter of 102, and a bandwidth parameter of 0.5hn in this experiment (hn is the median distance as explained in Section 2.5).
In Figure 3a, we plot two different statistics of the reversed engineered gene-regulatory networks as a function of the developmental time point (166).
The first statistic is the network size as measured by the number of edges; the second is the average local clustering coefficient as defined by Watts and Strogatz (1998).
The first statistic measures the overall connectedness of the networks, while the latter measures the average connectedness of the neighborhood local to each gene.
For comparison, we normalize both statistics to the range between [0,1].
It can be seen that the network size and its local clustering coefficient follow very different trajectories during the developmental cycle.
The network size exhibits a wave structure featuring two peaks at midembryonic stage and the beginning of pupal stage.
Similar pattern of gene activity has also been observed by Arbeitman et al.(2002).
In contrast, the clustering coefficients of the time-evolving networks drop sharply after the mid-embryonic stage, and they stay low until the start of the adult stage.
One explanation is that at the beginning of the developmental process, genes have a more fixed and localized function, and they mainly interact with other genes with similar functions; however, after mid-embryonic stage, genes become more versatile and involved in more diverse roles to serve the need of rapid development; as the organism turns into an adult, its growth slows down and each gene may be restored to its more specialized role.
To illustrate how the network properties change over time, we visualize two networks from mid-embryonic stage (time point 15) and mid-pupal stage (time point 45) in Figure 3b and 3c respectively.
Although the size of the two networks are comparable, we can see that there are much clearer local clusters of interacting genes during mid-embryonic stage.
To provide a better view of the evolving nature of these clusters, we cluster genes based on the network at time point 1 using spectral clustering, and visualize the gradual disappearance of these clusters in Figure 2.
Note that our visualization does not indicate that genes do not form clusters in later developmental stage.
Genes may cluster under different groupings, but these clusters cannot be revealed by the visualization since the positions of the genes have been fixed in the visualization.
To judge whether the learned networks make sense biologically, we zoom into three groups of genes functionally related to different stages of development process.
In particular, the first group (30 genes) is related to embryonic development; the second group i132 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i133 i128i136 KELLER: estimating time-varying networks (a) t = 1 (b) t = 8 (c) t = 16 (d) t = 24 (e) t = 35 (f) t = 47 (g) t = 62 Fig.2.
(ag) Circular plots of the networks (left or top part of each table cell) and dot plots of the adjacency matrices of the networks (right or bottom part of each table cell) at 7 time points.
Note that we have clustered the genes according to the network connections at time point 1 and used this clustering result to fix the order of the genes in all plots.
In the circular plots, genes are arranged along the outer rim and the colors indicates the boundaries between different clusters (20 clusters in total).
Furthermore, we have added curvature to the edges such that connections within and between clusters can be seen more clearly.
(a) Network statistics (b) Embryonic stage (c) Pupal stage Fig.3.
Characteristic of the time-evolving networks estimated for the genes related to developmental process.
(a) Plot of two network statistics as functions of development time line (NS, network size; CC, clustering coefficient).
(b and c) visualization of two example of networks from different time point.
We can see that network size can evolve in a very different way from the local clustering coefficient.
(a) (b) (c) Fig.4.
Interactivity of three groups of genes related to (a) embryonic development; (b) post-embryonic development; and (c) muscle development.
The higher the interactivity, the more active the group of genes.
We see that the interactivity of the three groups is very consistent with their functional annotation.
(27 genes) is related to post-embryonic development; and the third group (25 genes) is related to muscle development.
(The genes are assigned to their respective groups according to their ontology labels.)
We used interactivity, which is the total number of edges a group of genes is connected to, to describe the activity of each group genes.
In Figure 4, we plotted the time courses of interactivity for the three groups, respectively.
For comparison, we normalize all scores to the range of [0,1].
We see that the time courses have a nice correspondence with their supposed roles.
For instance, embryonic development genes have the highest interactivity during embryonic stage, and post-embryonic genes increase their interactivity during larval and pupal stage.
The muscle development genes are less Table 1.
Timeline of 45 known gene interactions CycE CycA CycE Rca1 Dp CycE Gi Go Hem blow Ice Ark Jra dnc Nf1 dnc Pak trio Sb Rho1 Snap Syx1A Src42A ksr W nej brk tkv brm N brm shg btl stumps cact dl caps Chi da Dl dally sgl dl Dif dom E(z) ea Tl emc bs esc E(z) gl peb hep p53 mam wg msn Nrt msn dock nej th numb Rca1 pbl CycE pbl Src64B pbl dl pbl tum pnr svr pros Abl pros pnt sdt baz sno Dl spen ksr tsh wg up Mhc Each cell in the plot corresponds to one gene pair of gene interaction at one specific time point.
The cells in each row are ordered according to their time point, ranging from embryonic stage (E) to larval stage (L), to pupal stage (P) and to adult stage (A).
Cells colored blue indicate the corresponding interaction listed in the right column is present in the estimated network; blank color indicates the interaction is absent.
specific to certain developmental stages, since they are needed across the developmental cycle.
However, we see its increased activity when the organism approaches its adult stage where muscle development becomes increasingly important.
The estimated networks also recover many known interactions between genes.
In recovering these known interactions, the timeevolving networks also provide additional information as to when interactions occur during development.
In Table 1, we listed these i133 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i134 i128i136 L.Song et al.(a) Summary network (b) Embryonic stage (c) Larval stage (d) Pupal stage (e) Adult stage Fig.5.
The largest TFs cascade involving 36 TFs.
(a) The summary network is obtained by summing the networks from all time points.
Each node in the network represents a TF, and each edge represents an interaction between them.
The width of an edge is proportional to the number of the times the edge is present during the development; the size of a node is proportional to the sum of its edge weights.
During different stages of the development, the networks are different, (be) shows representative networks for the embryonic (t=15), larval (t=35), pupal (t=49) and adult stage of the development, respectively (t=62).
recovered known interactions and the precise time when they occur.
This also provides a way to check whether the learned networks are biologically plausible given the prior knowledge of the actual occurrence of gene interactions.
For instance, the interaction between genes msn and dock is related to the regulation of embryonic cell shape and correct targeting of photoreceptor axons.
This is consistent with the timeline provided by the time-evolving networks.
A second example is the interaction between genes sno and Dl which is related to the development of compound eyes of Drosophila.
A third example is between genes caps and Chi which are related to wing development during pupal stage.
What is most interesting is that the time-evolving networks provide timelines for many other gene interactions that have not yet been verified experimentally.
This information will be a useful guide for future experiments.
We further study the relations between 130 transcriptional factors (TFs).
The network contains several clusters of transcriptional cascades, and we will present in detail the largest TF cascade involving 36 TFs (Fig.5).
This cascade of TFs is functionally very coherent, and many TFs in this network play important roles in the nervous system and eye development.
For example, Zn finger homeodomain 1 (zhf1), brinker (brk), charlatan (chn), decapentaplegic (dpp), invected (inv), forkhead box, subgroup 0 (foxo), Optix, eagle (eg), prospero (pros), pointed (pnt), thickveins (tkv), extra macrochaetae (emc), lilliputian (lilli), doublesex (dsx) are all involved in nervous and eye development.
Besides functional coherence, the networks also reveal the dynamic nature of gene regulation: some relations are persistent across the full developmental cycle while many others are transient and specific to certain stages of development.
For instance, five TFs, brkpnt zfh1prosdpp, form a long cascade of regulatory relations which are active across the full developmental cycle.
Another example is gene Optix which is active across the full developmental cycle and serves as a hub for many other regulatory relations.
As for transience of the regulatory relations, TFs to the right of Optix hub reduce their activity as development proceeds to later stages.
Furthermore, Optix connects two disjoint cascades of gene regulations to its left and right side after embryonic stage.
The time-evolving networks also provide an overview of the interactions between genes from different functional groups.
In Figure 6, we grouped genes according to 58 ontologies and visualized the connectivity between groups.
We can see that large topological changes and network rewiring occur between functional groups.
Besides expected interactions, the figure also reveals many seemingly unexpected interactions.
For instance, during the transition from pupa stage to adult stage, Drosophila is undergoing a huge metamorphosis.
One major feature of this metamorphosis is the development of the wing.
As can be seen from Figure 6r and s, genes related to metamorphosis, wing margin morphogenesis, wing vein morphogenesis and apposition of wing surfaces are among the most active groups of genes, and they carry their activity into adult stage.
Actually, many of these genes are also very active during early embryonic stage (for example, Fig.6b and c); though the difference is that they interact with different groups of genes.
On one hand, the abundance of transcripts from these genes at embryonic stage is likely due to maternal deposit (Arbeitman et al., 2002); on the other hand, this can also be due to the diverse functionalities of these genes.
For instance, two genes related to wing development, held out wings (how) and tolloid (td), also play roles in embryonic development.
4 CONCLUSION Numerous algorithms have been developed for inferring biological networks from high-throughput experimental data, such as microarray profiles (Ong, 2002; Segal et al., 2003), ChIP-chip genome localization data (Bar-Joseph et al., 2003; Harbison et al., 2004; Lee et al., 2002) and PPI data (Causier, 2004; Giot et al., 2003; Kelley et al., 2004; Uetz et al., 2000), based on formalisms such as graph mining (Tanay et al., 2004), Bayesian networks (Cowell et al., 1999) and DBN (Friedman et al., 2000; Kanazawa et al., 1995).
However, most of this vast literature focused on modeling static network or time-invariant networks, and much less has been done towards modeling the dynamic processes underlying networks that are topologically rewiring and semantically evolving over time.
The method presented in this article represents a successful and practical tool for genome-wide reverse engineering dynamic interactions between genes based on their expression data.
Given the rapid expansion of categorization and characterization of biological samples and improved data collection technologies, we expect collections of complex, high-dimensional and feature-rich data from complex dynamic biological processes, such as cancer progression, immune response and developmental processes, to continue to grow.
Thus, we believe our new method, KELLER, is a timely contribution that can narrow the gap between i134 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i135 i128i136 KELLER: estimating time-varying networks (a) Avgerage network.
Each color patch denotes an ontological group, and the position of these ontological groups remain the same from (a) to (u).
The annotation in the outer rim indicates the function of each group.
(b) t = 1 (c) t = 4 (d) t = 8 (e) t = 12 (f) t = 16 (g) t = 20 (h) t = 24 (i) t = 28 (j) t = 32 (k) t = 35 (l) t = 38 (m) t = 41 (n) t = 44 (o) t = 47 (p) t = 50 (q) t = 53 (r) t = 56 (s) t = 59 (t) t = 62 (u) t = 65 Fig.6.
Interactions between gene ontological groups related to developmental process undergo dynamic rewiring.
The weight of an edge between two ontological groups is the total number of connection between genes in the two groups.
In the visualization, the width of an edge is proportional to its edge weight.
We thresholded the edge weight at 30 in (b)(u) so that only those interactions exceeding this number are displayed.
The average network in (a) is produced by averaging the networks underlying (b)(u).
In this case, the threshold is set to 20 instead.
imminent methodological needs and the available data and offer deeper understanding of the mechanisms and processes underlying biological networks.
Acknowledgements We thank Thomas Gulish for proofreading the manuscript.
Funding: National Science Foundation CAREER Award (grant DBI-0546594 and NSF IIS-0713379 to E.P.X.
); Alfred P. Sloan Research Fellowship (to E.P.X.
); Ray and Stephenie Lane Research Fellowship (to L.S.).
Conflict of Interest: none declared.
ABSTRACT Summary: It has been argued that the missing heritability in common diseases may be in part due to rare variants and genegene effects.
Haplotype analyses provide more power for rare variants and joint analyses across genes can address multi-gene effects.
Currently, methods are lacking to perform joint multi-locus association analyses across more than one gene/region.
Here, we present a haplotypemining genegene analysis method, which considers multi-locus data for two genes/regions simultaneously.
This approach extends our single region haplotype-mining algorithm, hapConstructor, to two genes/regions.
It allows construction of multi-locus SNP sets at both genes and tests joint genegene effects and interactions between single variants or haplotype combinations.
A Monte Carlo framework is used to provide statistical significance assessment of the joint and interaction statistics, thus the method can also be used with related individuals.
This tool provides a flexible data-mining approach to identifying genegene effects that otherwise is currently unavailable.
Availability: http://bioinformatics.med.utah.edu/Genie/hapConstruc tor.html Contact: ryan.abo@hsc.utah.edu Received on March 8, 2010; revised on October 13, 2010; accepted on October 31, 2010 1 INTRODUCTION Haplotype and genegene analyses have been suggested as strategies to identify disease loci that single nucleotide polymorphism (SNP) approaches may have missed (Manolio et al., 2009).
Haplotypes have the potential for improved characterization of variation across the locus set (Clark, 2004; Schaid, 2004).
Yet, it is usually unclear which haplotypes to test and how to model them.
Numerous methods consider all haplotypes spanning the entire locus set, with attempts to reduce the degrees of freedom that this approach otherwise confers (Liu et al., 2007; Tzeng and Zhang, 2007).
Other techniques have been designed to analyze contiguous and non-contiguous locus subsets (Abo et al., 2008; Browning, 2006; Browning and Browning, 2007; Laramie et al., 2007; Lin, 2004).
It has been hypothesized (Moore, 2003), and in some cases shown (Combarros et al., 2009), that genetic factors at one gene can modify the effects of another gene on disease susceptibility.
If such biological interaction exists, the association may only be evident by considering both genes simultaneously.
Genegene studies are complicated by issues surrounding what constitutes a genegene To whom correspondence should be addressed.
interaction.
For example, some approaches for testing interactions focus on association between two unlinked loci (Wu et al., 2008; Zhao et al., 2006), which do not provide any measure of departure from additivity as a statistical interaction is classically defined.
Most often haplotype analyses are performed for a single region and genegene studies concentrate on single SNPs in each region.
Methods that consider multi-locus data at more than one gene would be desirable to maximize the ability to detect association evidence.
One such method exists to test specific haplotype interactions at unlinked regions (Becker et al., 2005).
However, both haplotype and genegene analyses can result in high-dimensionality, and how to combine them is therefore a challenging problem.
To address these challenges, we have extended our single region haplotype-mining approach (Abo et al., 2008) to consider multilocus data at two genes and test for association and interaction.
We concentrate on a broad set of tests that considers both joint effects and interaction effects.
In our genegene-mining process, data considered at each gene can be single or multi-locus.
We anticipate that this genegene-mining approach will be most useful for hypothesis generation.
However, if required, haplotype testing can also be performed using an empirical correction for multiple testing.
Casecontrol and case-only designs are available, in addition to statistics to test joint and interaction effects.
The method is implemented in a Monte Carlo (MC) testing framework and empirical construction-wide significance assessment is available for hypothesis testing.
2 METHODS For both genes/regions considered, maximum likelihood estimates (MLE) for all individuals haplotype pairs and population haplotype frequencies are determined.
All SNPs in each region and all individuals with sufficient data at both regions are considered (based on a user-defined genotype call rate threshold).
Full-length MLE haplotypes, or sub-haplotypes extracted from them, are the genetic variables considered in the construction and testing process.
Consider h and k loci in unlinked genes, G1 ={M1,...,Mh} and G2 ={Mh+1,...,Mh+k}.
The full locus set S =G1 G2.
First, all single locus association tests are conducted.
These single locus associations are assessed against the first significance threshold, T1, which is user-defined.
For any locus i with P-value T1, all locus pairs {Mi, Mj |Mj S; j = i} are considered at the second step.
The locus pair {Mi, Mj} is the locus set, L, being considered.
When the two loci in L span both genes, genegene tests between the loci are performed.
When loci in L are all within the same gene, the two loci are tested as a haplotype or composite genotype.
Tests at step n are assessed at significance threshold Tn ({T1,...,Th+k}), which are usually chosen to be increasing in stringency with n. A locus set can be The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:39 2/12/2010 Bioinformatics-btq616.tex] Page: 135 134136 Multi-locus genegene associations written as L={g1 g2 |g1 G1 and g2 G2} where g1 denotes loci that reside in G1 and g2 those that reside in G2.
In steps n>2, if there are multiple SNPs in both genes, genegene tests between haplotypes across g1 and haplotypes across g2 will be performed.
The steps continue until no further locus sets pass the defined threshold values or the full locus sets have been tested.
To avoid a strict uphill climb algorithm, which is susceptible to identifying local minimums, we have incorporated a backward step.
At each backward step, the algorithm considers subsets of size n1 from the current locus set that were not previously tested.
Any subsets which pass the significance threshold, Tn, will be retained and the process will continue forward again.
For locus sets where g1 and/or g2 are multi-locus, haplotypes or composite genotypes are considered.
The algorithm considers each haplotype across gi as a potential risk haplotype, and compares with all other haplotypes grouped together.
For any specific haplotype, this reduces the multi-locus data to a biallelic system which can be used for standard allelic, dominant, recessive and additive models for testing both within and across genes.
For composite genotype combinations, phase is unimportant, each locus in L is modeled separately as dominant or recessive and the combinations of these considered across loci.
Hence, composite genotypes tests can be performed within or across genes.
To reduce the tests performed, at step n+1 the algorithm only expands the specific risk haplotypes that passed the significance threshold (i.e.the alleles at loci from step n are fixed).
A similar rule is applied to the composite genotypes.
Single locus, haplotype and composite genotype models are tested using odds ratios, chi-square and chi-square trend association statistics.
For locus sets containing loci in two genes, L={g1 g2 |g1 G1 and g2 G2}, an interaction odds ratio test and a correlation-based statistic are offered to identify genegene effects between the two loci sets, g1 and g2.
As described above, multi-locus sets within genes are considered using biallelic recoding.
We refer to specific haplotypes across g1 and g2 as h1 and h2.
The interaction odds ratio between h1 and h2 is calculated using the method described by Thomas (2004), IORm,n, where m and n denote dominant or recessive models imposed on h1 and h2, respectively, and 0 indicates the wildtype.
IORm,n = ( ORm,nOR00 ) ( ORm,0OR0,n ) Under the null hypothesis, H0: IORm,n =1, the odds of disease given h1 and h2 is the product of the odds of disease for each hi.
We have also implemented interaction tests based on correlation (Wu et al., 2008; Zhao et al., 2006).
Correlation of specific haplotypes, h1 and h2, from locus sets g1 and g2 are performed.
Following Wang et al.(2007), the correlation is determined as follows, where each individuali is assigned a value xij for locus set gj based on its MLE haplotype pairs: xi,j = 1 for 0 copies of hj 0 for 1 copy of hj 1 for 2 copies of hj  The correlation between h1 and h2 is estimated by the correlation coefficient: r = Sx.1x.2 Sx.1 Sx.2 , where Sx1,x2 = N i=1 ( xi1 x.1 )( xi2 x.2 ) and Sx.j = N i=1 ( xij x.j ) , j= (1, 2), and N is the number of individuals.
This correlation coefficient is an estimate of the composite correlation statistic (Zaykin et al., 2008) which is robust to HardyWeinberg disequilibrium.
For a casecontrol study design, the method tests H0 : rcase rcontrol =0.
For a case-only H0 : rcase=0 and the first step in the automated process considers the correlation between pairs of single SNPs.
We also note the availability of meta-statistics for analyzing multiple datasets.
Statistical significances are determined with a MC procedure.
The validity of the MC procedure is based on properly matching the null simulations with the observed data with regard to pedigree structure, missing data structure and phasing procedure (Curtis and Sham, 2006).
Our MC procedure is based on a two-region multi-locus gene-drop.
In both regions, haplotype pairs are assigned to founders and independent individuals based on the estimated full-length haplotype frequencies.
Full-length haplotypes for both regions are then assigned to pedigree descendants using gene-dropping techniques based on Mendelian inheritance (MacCluer et al., 1986).
The missing data structure is then imposed on the simulated multi-locus genotype data and the known phase is ignored.
These simulated data are then statistically phased, to match the procedure performed with the observed data.
The procedure generates null genotype configurations from which null statistics are calculated and a null empirical distribution created.
It must be noted that this MC procedure assumes a null of no linkage and no association.
If strong linkage exists (but no association), there is the potential for inflated type 1 errors; although in simulations we find that for reasonable linkage models that the MC procedure remains a good approximation for the null and type 1 errors remain valid.
Correction for the data-mining process is also available and, if selected, will provide construction-wide significance and false discovery rates.
Correction for construction is implemented in the same way as for hapConstructor (Abo et al., 2008), where the null distribution for a complete construction run is generated by conducting the same search process starting from 1000 null configurations.
3 IMPLEMENTATION Our method is implemented as a Java-based program.
It is an extension of the hapConstructor module (Abo et al., 2008) in the Genie software (Allen-Brady et al., 2006).
The program can be run on Windows, Unix or Linux machines with Java 1.6 and at least 2 GB of RAM.
An example dataset consisting of 14 SNPs in one gene and 11 SNPs in the second gene required 7 h and 11 min with 4 GB of memory to complete building to step 3.
Parameter options for this example included default critical thresholds, 10 000 null simulations and no construction-wide assessment.
It is important to note that this example may not provide useful insight to other implementations of the method because there are many factors that will affect the running time of the program.
These include: number of SNPs, number of samples, number of null simulations selected for significance assessment, critical thresholds selected for the steps in the building process, use of the multiple-testing correction procedure and whether or not there is an association signal.
Program details, including the example described above, are available atFunding: R.A. is an NLM fellow (grant T15 LM0724); National Institutes of Health (CA 098364); the Susan G. Komen Foundation and the Avon Foundation Breast Cancer Fund (to N.J.C.).
Conflict of Interest: none declared.
ABSTRACT Summary: The advent of next-generation sequencing for functional genomics has given rise to quantities of sequence information that are often so large that they are difficult to handle.
Moreover, sequence reads from a specific individual can contain sufficient information to potentially identify and genetically characterize that person, raising privacy concerns.
In order to address these issues, we have developed the Mapped Read Format (MRF), a compact data summary format for both short and long read alignments that enables the anonymization of confidential sequence information, while allowing one to still carry out many functional genomics studies.
We have developed a suite of tools (RSEQtools) that use this format for the analysis of RNA-Seq experiments.
These tools consist of a set of modules that perform common tasks such as calculating gene expression values, generating signal tracks of mapped reads and segmenting that signal into actively transcribed regions.
Moreover, the tools can readily be used to build customizable RNA-Seq workflows.
In addition to the anonymization afforded by MRF, this format also facilitates the decoupling of the alignment of reads from downstream analyses.
Availability and implementation: RSEQtools is implemented in C and the source code is available at http://rseqtools.gersteinlab.org/.
Contact: lukas.habegger@yale.edu; mark.gerstein@yale.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on June 23, 2010; revised on November 5, 2010; accepted on November 8, 2010 1 INTRODUCTION The advent of next-generation sequencing technologies has revolutionized the study of genomes and transcriptomes.
In particular, the application of deep sequencing approaches to transcriptome profiling (RNA-Seq) is increasingly becoming the method of choice for studying the transcriptional landscape of cells (Hillier et al., 2009; Mortazavi et al., 2008; Wang et al., 2009).
Typically, the first step in this analysis is the alignment of the To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
sequence reads to a reference sequence set.
Recently, a number of different alignment tools have been developed to map short reads in an efficient manner (Trapnell and Salzberg, 2009).
While much progress has been made on this front, there is still a great need for a set of software tools that facilitate the downstream analysis of mapped RNA-Seq reads.
Further, two other issues remain to be addressed.
First, the immense file size of next-generation sequencing data poses many challenges in terms of data processing, storage and sharing.
Secondly, mechanisms to protect personal confidential genetic information need to be established.
With the birth of personal genomics, sequencing data stems fundamentally from individuals, and this type of data cannot be distributed as easily because significant privacy concerns arise with sharing all the sequence variations of a particular individual (Greenbaum et al., 2008; Lowrance and Collins, 2007).
One critical challenge for genomics, then, is to devise new data summaries that allow the sharing of large amounts of information from sequencing experiments without exposing the genotypic information of the underlying individual (Supplementary Material).
Although many data formats have been developed such as SAM (Li et al., 2009), there is no practical solution yet that addresses the privacy concerns when sharing large sequence alignment files.
Addressing this challenge is precisely what we have endeavored to do in putting together the Mapped Read Format (MRF), a format that allows data summaries to be exchanged, enabling many aspects of the RNA-Seq calculation to be performed such as expression measurements, but that also detaches the actual sequence variation in a person into separate files.
Further, it provides a very clear way of linking these two pieces of information so that the data summaries can be subsequently conjoined back to the original sequences for more in-depth analyses with potentially confidential data.
Here, we present an overview of a flexible suite of tools (RSEQtools) that are designed to facilitate easily customizable workflows and efficient pipeline building for the analysis of RNASeq experiments using this compact format (Fig.1).
Briefly, we first convert the aligned reads into MRF and thus decouple the alignment step from the downstream analyses.
RSEQtools implements several modules using this standardized format for performing common RNA-Seq analyses, such as expression quantification, discovery of transcribed regions, coverage computations annotation manipulation, etc.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:39 16/12/2010 Bioinformatics-btq643.tex] Page: 282 281283 L.Habegger et al.Fig.1.
Schematic overview of RSEQtools.
Mapped reads are first converted into MRF from common alignment tool output formats, including SAM.
The resulting MRF files can be divided in two files: one with the alignment only and another with the corresponding sequence reads.
The read identifiers provide a mapping between the two files.
Then, several modules perform the downstream analyses independently from the mapping step, such as expression quantification, visualization of the mapped read and the calculation of annotation statistics, etc.
Other tools have been developed based on this framework to perform more sophisticated analyses such as transcript assembly, isoform quantification (IQSeq, http://rnaseq.gersteinlab.org/IQSeq), fusion transcript identification (FusionSeq, http://rnaseq.gersteinlab.org/fusionseq), as well as aggregation and correlation of signal tracks (ACT, http://act.gersteinlab.org).
2 FEATURES AND METHODS 2.1 MRF and converters MRF only stores a minimal set of information, i.e.information that cannot be derived from the MRF data itself.
This has the advantage of keeping the format succinct, while still capturing the relevant information for most analyses.
MRF consists of three components: comment lines (optional) denoted by a leading # sign, a header line and the mapped reads.
The header line specifies the data type of each column: AlignmentBlocks, Sequences, QualityScores and QueryID.
The column type AlignmentBlocks is required and represents the mapped reads.
Each alignment block contains the coordinates with respect to the reference genome to which the read aligns as well as the read coordinates.
A read spanning multiple regions, e.g.multiple exons, is denoted by multiple alignment blocks that are separated by a comma.
Paired-end reads can be represented by using a set of alignment blocks for each end, which are separated by the | symbol.
By using this format, it is straightforward to specify both gapped and paired-end alignments.
The RSEQtools package includes various utilities to convert the output of several mapping tools into MRF.
A converter for the commonly used SAM format is included as well.
The first example below represents two paired-end reads where one end is spliced, whereas the second example shows two un-spliced single-end reads with their associated QueryIDs: # Example 1 AlignmentBlocks chr2:+:601:630:1:30,chr2:+:921:940:31:50|chr2:+:1401:1450:1:50 chr9:+:451:460:1:10,chr9:+:831:870:11:50|chr9:+:945:994:1:50 # Example 2 AlignmentBlocks QueryID chr4:-:1221:1270:1:50 1 chr16:+:511:560:1:50 2 The optional types Sequences, QualityScores and QueryID provide additional information.
In particular, the confidentiality issues can be addressed by generating two files: one including the alignments and a second one containing the sequences such as a FASTQ file.
The former is useful for most analyses and can be publicly shared because it does not contain confidential information, whereas the latter can be subjected to a higher level of security and control.
The two files can be conjoined, if necessary, by using the common QueryID as shown in Figure 1.
2.2 RNA-Seq analysis with RSEQtools The RSEQtools suite contains a set of modules to perform a large variety of tasks including the quantification of expression values, manipulation of gene annotation sets, visualization of the mapped reads, generation of signal tracks, the identification of transcriptional active regions and several auxiliary utilities (Supplementary Table S1).
Genome annotation tools: to generate a splice junction library from any annotation set, we extract the genomic sequences of all the exons and synthetically create all splice junctions specified in the annotation set.
This splice junction library can be used in combination with the reference sequences.
A second tool is particularly useful when estimating expression 282 [13:39 16/12/2010 Bioinformatics-btq643.tex] Page: 283 281283 RSEQtools levels.
In order to capture the information of the various transcript isoforms, a gene model is required.
The module mergeTranscripts collapses the transcript isoforms into a single gene model by either taking the union or intersection of the exonic nucleotides.
Quantification of gene expression: one of the key features of RNA-Seq is the quantification of expression at different levels.
Hence, a key module calculates the gene expression values for a given annotation set and a collection of mapped reads in MRF format.
The annotation set specifies which elements will be quantified.
The program mrfQuantifier calculates RPKM (reads per kilobase per million mapped reads) values at the nucleotide level (Mortazavi et al., 2008).
Briefly, for a given entry in the annotation set (typically an exon or gene model), the number of nucleotides from all the reads that overlap with this annotation entry are added up and then this count is normalized by sequence length of the annotation entry (per killobase) and by the total number of mapped nucleotides (per million).
This calculation is not performed at the transcript level, which requires a more sophisticated analysis (Guttman et al., 2010; Trapnell et al., 2010).
Visualization of mapped reads: the RSEQtools package also contains various tools for visualizing the results in genome browsers, by means of wiggle (WIG) and bedGraph files, which are commonly used to represent a signal track of mapped reads.
Also, a GFF file can be generated from MRF files to visualize splice junction reads (example in Fig.1).
Identification of transcriptionally active regions (TARs): transcribed regions can be identified de novo by performing a maxGap/minRun segmentation (Kampa et al., 2004; Royce et al., 2005) from the signal files using the wigSegmenter program.
Briefly, the signal is first thresholded to identify transcribed elements.
Contiguous elements whose distance is less than maxGap are joined together and then filtered if the final size is less than minRun.
This type of analysis is particularly useful in discovering novel TARs such as small RNAs, etc.
MRF selection and auxiliary utilities: lastly, RSEQtools includes a set of utilities to easily manipulate MRF files and a collection of format conversion tools allowing for rapid pipeline development.
Implementation and run time: the modules of the RSEQtools suite were implemented in C and the code was optimized in order to efficiently handle large datasets.
The importance of code scalability cannot be overemphasized in a time where datasets become increasingly large and easily exceed several gigabytes.
For example, the conversion of an ELAND export file (uncompressed file size: 4 GB; total number of reads: 20 million; number of mapped reads: 12 million) to MRF takes 2 min and the resulting MRF file is significantly smaller (400 MB uncompressed, 130 MB compressed with gzip).
Converting the same ELAND export file to SAM generates a file of 3.1 GB (uncompressed) and the corresponding BAM file has a size of 1.2 GB.
The subsequent quantification of gene expression using mrfQuantifier requires 45 s to calculate estimates for about 20 000 genes.
In addition, the modularity of RSEQtools also enables the development of additional programs in any programming language and their seamless integration into this framework.
Finally, most modules use STDIN and STDOUT to process the data, making them suitable to be integrated into an automated pipeline.
3 CONCLUSIONS In summary, RSEQtools contains a number of useful and highly specific modules that can rapidly analyze RNA-Seq data.
The MRF format has two major features: it allows the decoupling of downstream analysis from the mapping strategy and addresses the issue of confidentiality that is intrinsic in any sequencing experiments involving human subjects.
By separating the actual sequencing reads from the alignments, MRF provides a mechanism to protect the private genotypic information of the underlying individual.
Although this approach removes the most obvious genotypic features, other distinctive attributes do remain.
First of all, the information in a MRF file is at least equivalent to that in traditional expression array, which can potentially identify the underlying individual.
Secondly, some information about structural variants may be contained in the MRF file of an RNA-Seq experiment.
However, it is not obvious how to extract genotypic information from a subset of structural variations just affecting genes.
In addition, inferring structural variations from RNA-Seq data as opposed to DNA sequencing would be more complicated due to the presence of alternative splicing.
Another advantage of storing the alignments without the underlying sequences is that it saves space, especially as reads become longer.
Moreover, a possible future extension is the development of a specific compression schema that could further reduce the size of the files.
In addition, this data format could be easily applied to sequence alignments obtained from other high-throughput functional genomic assays such as ChIP-Seq or chromosome conformation capture (3C).
ACKNOWLEDGEMENT We thank Raymond Auerbach for critical reading and editing as well as Wasay Hussain for testing of the software.
Funding: National Institutes of Health.
Conflict of Interest: none declared.
ABSTRACT Motivation: Most of the previous data mining studies based on the NCI-60 dataset, due to its intrinsic cell-based nature, can hardly provide insights into the molecular targets for screened compounds.
On the other hand, the abundant information of the compound target associations in PubChem can offer extensive experimental evidence of molecular targets for tested compounds.
Therefore, by taking advantages of the data from both public repositories, one may investigate the correlations between the bioactivity profiles of small molecules from the NCI-60 dataset (cellular level) and their patterns of interactions with relevant protein targets from PubChem (molecular level) simultaneously.
Results: We investigated a set of 37 small molecules by providing links among their bioactivity profiles, protein targets and chemical structures.
Hierarchical clustering of compounds was carried out based on their bioactivity profiles.
We found that compounds were clustered into groups with similar mode of actions, which strongly correlated with chemical structures.
Furthermore, we observed that compounds similar in bioactivity profiles also shared similar patterns of interactions with relevant protein targets, especially when chemical structures were related.
The current work presents a new strategy for combining and data mining the NCI-60 dataset and PubChem.
This analysis shows that bioactivity profile comparison can provide insights into the mode of actions at the molecular level, thus will facilitate the knowledge-based discovery of novel compounds with desired pharmacological properties.
Availability: The bioactivity profiling data and the target annotation information are publicly available in the PubChem BioAssay database (ftp://ftp.ncbi.nlm.nih.gov/pubchem/Bioassay/).
Contact: ywang@ncbi.nlm.nih.gov; bryant@ncbi.nlm.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.
Received on June 17, 2010; revised on September 2, 2010; accepted on September 22, 2010 1 INTRODUCTION Understanding the mechanism of interaction of small molecules with their macromolecular targets is critical for drug and chemical probe development.
The innovation of drug has long been recognized as time-consuming and labor-intensive, costing on To whom correspondence should be addressed.
average about $800 million as well as 1012 years to bring a new drug to market (DiMasi et al., 2003).
Apart from the challenges in optimizing pharmacokinetic properties and minimizing toxicities of lead compounds, the lack of publicly available/accessible biomedical assay data may represent another barrier for the success of drug discovery.
Fortunately, this is changing since more public resources are emerging, offering new opportunities to chemical biology researchers for drug development.
Open-access, information-rich resources include the Protein Data Bank (PDB; Berman et al., 2000), DrugBank (Wishart et al., 2006, 2008) and KEGG (Kanehisa et al., 2004), to name only a few.
Without a doubt, existing public resources, as well as new ones, will evolve in future with both speed and capacity.
PubChem is a public repository for the chemical structures of small molecules and information of their biological properties (Wang et al., 2009, 2010).
It was launched as a component of the NIH Molecular Libraries Roadmap Initiative (Zerhouni, 2003), with the aim to discover chemical probes via high-throughput screening (HTS) of small molecules.
It also receives biological property contributions from many other organizations.
As of March 17, 2010, PubChem contains more than 26 million unique compounds, among which over 870 000 have biological assay data for more than 3000 molecular targets, including proteins and genes.
The public accessibility to such assay data is particularly valuable to the community, since this kind of critical information needed by drug research is typically held by pharmaceutical companies.
The public availability and information-rich features altogether make PubChem an extremely valuable resource for biomedical research, as well as data mining studies (Chen and Wild, 2010; Han et al., 2008; Li et al., 2009; Rohrer and Baumann, 2009; Weis et al., 2008; Xie and Chen, 2008).
Launched by the National Cancer Institute (NCI), the Developmental Therapeutics Program (DTP) provides in vitro screening for new anticancer drugs that tested in 60 human tumor cancer cell lines (often known as the NCI-60 dataset; Shoemaker, 2006).
This well-curated, publicly available dataset has been recognized as a rich resource for studying the mechanism of growth inhibition for tumor cells (Shoemaker, 2006; Weinstein et al., 1997).
It has also inspired interests for developing and validating data mining tools (Paull et al., 1989; Zaharevitz et al., 2002).
Bioactivity profiles derived from the NCI-60 cell lines can provide insights into the mode of actions for tested compounds (Rabow et al., 2002; Shi et al., 1998a, b, 1999; Wallqvist et al., 2003; Weinstein, et al., 1992).
Structure-activity relationships (SARs) studies have also been reported for predicting or characterizing the cytotoxicity of the The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
ftp://ftp.ncbi.nlm.nih.gov/pubchem/Bioassay/[13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2882 28812888 T.Cheng et al.screened compounds in the NCI-60 dataset (Guha, 2008; Lee et al., 2008; Wang et al., 2007).
However, due to the intrinsic cell-based nature of the NCI-60 dataset, most of the studies described above can hardly provide insights into the molecular targets for screened compounds.
On the other hand, PubChem has more than 1200 publicly available HTS bioassays with over 690 defined protein targets (as of March 17, 2010).
In addition, the screening laboratories under the NIH Molecular Libraries Program (MLP) share a common compound library, i.e.the Molecular Libraries Small Molecule Repository (MLSMR), which is required to be tested for each assay project if possible.
As a result, the compounds in the MLSMR library are often tested in hundreds of bioassays with many of them having associated protein targets.
It thus represents a rich resource for constructing the compoundtarget interaction network, deriving target profiles and evaluating polypharmacological properties for a large library of compounds (Chen et al., 2009).
Moreover, there is a significant overlap between the MLSMR compound library and those screened in the NCI-60 cell lines.
Therefore, the bioassay data in PubChem can provide experimental evidence for the interactions between the compounds in the NCI-60 dataset and their targets.
In this work, we proposed a new strategy for combining and data mining the NCI-60 dataset and PubChem HTS assays, and investigated the correlations among the bioactivity profiles, compoundtarget interaction network and chemical structures of small molecules.
Bioactivity profiles were derived from the screening results contained in the NCI-60 dataset.
Compounds were hierarchically clustered based on their bioactivity profiles.
Compoundtarget interaction networks were constructed using the annotated bioassay data in PubChem.
Strong correlations were suggested between bioactivity profiles and target networks, especially when chemical structures were related.
2 METHODS 2.1 NCI-60 dataset The NCI-60 dataset is also available in the PubChem BioAssay database as 73 bioassays with the name of NCI human tumor cell line growth inhibition assay under the NCI/DTP data source.
In this study, 13 bioassays were eliminated considering their relatively small number of tested compounds (less than 16 000).
The screening data for the remaining 60 bioassays (will be referred to hereafter as the NCI-60) was downloaded from the PubChem FTP site (accessed on March 17, 2010).
In total, 5083 unique compounds were compiled and further filtered by the following rules: (1) Compounds must have been tested in all of the 60 NCI cell lines with a complete spectrum of log (GI50) values, where GI50 is the compound concentration required for 50% inhibition of tumor cell growth.
That is, any compound with missing log (GI50) value in one or more of the NCI-60 cell lines was discarded.
4452 compounds met with this criterion.
(2) Compounds must demonstrate activity in at least one PubChem bioassay which has a defined protein target.
This resulted in an initial set of 257 compounds with both complete bioactivity profiles and known protein targets.
(3) Compounds must have log (GI50) values below 6 for at least 15 out of the 60 NCI cell lines.
A final set of 37 compounds matched all of the above three criteria, and were analyzed in this study.
2.2 Clustering analysis based on bioactivity profiles End-point activity data from a single cell line may give only limited information on a compounds biological response.
However, the tested activities in a broad panel of 60 cell lines (i.e.bioactivity profile) can be used to characterize the mechanism of drug action, resistance and modulation (van Osdol et al., 1994; Weinstein et al., 1992).
In this study, bioactivity profiles were subjected to hierarchical clustering by using the Hierarchical Clustering Explorer (HCE, version 3.5; Seo and Shneiderman, 2002), with the complete-linkage algorithm and Euclidean distance: dAB = 60 i=1 (Ai Bi)2 (1) where Ai and Bi are the log (GI50) values in the i-th NCI-60 cell line for the compound A and B, respectively.
2.3 Compoundtarget interaction network A compoundtarget interaction network can offer a direct view of the interactions between compounds and their protein targets.
The first step to construct such a network is to identify the protein targets for the compounds of interest.
The detailed target annotations in the PubChem BioAssay database (assay identifier: AID) made this step very straightforward.
For each of the 37 compounds in this study, the PubChem bioassays in which the compound was tested active (see each assay description for the definition of bioactivity outcome) were identified.
If the bioassay was specified with a protein target, then the target was assigned to the compound and included for network construction.
Note that a compound may be found active in several bioassays, so it is possible for a compound to have multiple targets associated with it.
In our network, the compound and target were denoted as two different nodes, respectively.
An edge was drawn to link a compound node (labeled by the PubChem compound identifier: CID) and a target node (labeled by the NCBI protein identifier: GI) if the compound is active against the target.
We applied the E-Utilities tool (http://eutils.ncbi.
nlm.nih.gov/entrez/query/static/eutils_help.html) to get the target GI for a respective bioassay.
For example, the following URL: http://eutils.ncbi.
nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pcassay&db=protein&cmd= neighbor&linkname=pcassay_protein_target&id=915 will return an XML file containing the GI of the protein target for the bioassay with AID 915.
An in-house script was used to extract GI from the resultant XML file.
To avoid the ambiguity in target specification, several PubChem bioassays that associate with multiple GIs were excluded from analysis.
The PubChem bioassays as well as the target information used for network construction are listed in Supplementary Table S1 based on the assay data in the PubChem BioAssay database as of March 17, 2010.
The compoundtarget interaction network was visualized by using the Cytoscape (version 2.3.6; Shannon et al., 2003).
3 RESULTS AND DISCUSSION 3.1 Hierarchical clustering analysis based on bioactivity profiles Hierarchical clustering was first carried out for the initial set of 257 compounds, which was obtained prior to the application of the third filter.
The dendrogram graph of the clustering result is given in Supplementary Figure S1.
The log (GI50) value of 6 was adopted as the bipartite cutoff to determine whether a compound is active (6) or inactive (>6) in a respective NCI-60 cell line.
This criterion for discriminating active compounds from inactive ones is consistent with that specified in the PubChem BioAssay database by the original NCI/DTP depositors, and as well as in other studies (Lee et al., 2008).
2882 [13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2883 28812888 Investigating the correlations among the chemical structures, bioactivity profiles and molecular targets of small molecules Fig.1.
Hierarchical clustering of the 37 compounds in the final set based on their bioactivity profiles in the NCI-60 cell lines.
The bioactivity profile of each compound is shown in spectrum (horizontal view).
A minimum similarity threshold of 0.88 (red solid line) is employed in HCE.
Six clusters that contain more than one compound are marked as A through F from top to bottom.
Relevant compounds (24 in total) are labeled with PubChem compound identifiers (CID).
As shown in Supplementary Figure S1, compounds that demonstrate similar bioactivity spectra were clustered.
However, a majority of compounds were clustered in proximity simply because they were inactive (shown in blue) in most of the NCI-60 cell lines.
Though inactive information is also important, it is less relevant to our study, which is to investigate the correlations among the bioactivity profiles, molecular targets and chemical structures of bioactive compounds.
Furthermore, previous studies have shown that the log (GI50) values in the NCI-60 dataset are skewed toward certain thresholds (Lee et al., 2008).
In our case, nearly 25% of the log(GI50) values for the initial set of 257 compounds were 4.
The reason is that the highest tested concentration in the NCI60 cell lines is generally 4 (in log units), and if a compound is not sufficiently active to show 50% cell growth inhibition at this highest concentration, a upper bound of log(GI50) value of 4 is typically reported (Shi et al., 1998b).
Therefore, the bioactivity profiles that contain primarily skewed inactivity data may provide biased information.
To avoid such bias to certain extent, a third filter is needed to require every compound in the initial set to be active, i.e.log(GI50)6, in at least 15 out of the 60 NCI cell lines.
This criterion can ensure, at least to a partial extent, that the derived similarity in bioactivity profiles result from biological activity rather than inactivity.
As a result, the initial compound set (257) was narrowed down to contain only 37 compounds.
With a much reduced dataset, we were able to investigate the SAR and compoundtarget interaction network for those compounds in greater details.
The hierarchical clustering for the 37 bioactive compounds was carried out by using exactly the same algorithm as applied to the initial compound set.
The results are shown in Figure 1.
The dendrogram graph indicates that compounds with biologically similar bioactivity were grouped together.
By setting a relatively tight cutoff of the minimum similarity of 0.88 in the HCE, clusters containing more than one compound were obtained and labeled as A through F from top to bottom.
The minimum similarity cutoff of 0.88 was chosen empirically based on visual exploration.
Interesting results obtained on these clusters are given below.
3.2 Highly similar structures with highly similar bioactivity profiles (Cluster B) Five compounds (CID: 24360, 97226, 72402, 354677 and 60699) were identified from cluster B.
Their 2D chemical structures are depicted in Figure 2A.
Compared to the compounds in other clusters, they gave the highest structural similarity as calculated by using the Tanimoto metric and PubChem fingerprint (ftp://ftp.
ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.
txt).
The average inter-compound structural similarity for these compounds was 0.887.
Therefore, it may not be surprising to see that they also exhibited similar biological responses in the NCI-60 cell lines (Fig.2B).
Indeed, similar bioactivity profiles were observed for these compounds, indicating strong and consistent inhibitory activity for a considerable number of cell lines.
The results from Figure 2A and B suggest that this group of compounds demonstrated strong SAR.
Further analysis on chemical structures shows that these compounds are the analogs of camptothecin, a selective inhibitor of the topoisomerases I (TOP1).
Among the five compounds, two (CID: 24360 and 60699) are well-known inhibitors of TOP1 (Pizzolato and Saltz, 2003; Wethington et al., 2008).
The former is camptothecin itself, while the latter has recently been approved by the FDA (trade name Hycamtin) in 2007 for oral use to treat ovarian cancer (http://en.wikipedia.org/wiki/Hycamtin, accessed on April 12, 2010), which is consistent with its activity in the ovarian cell lines (Fig.2B).
Considering the significant similarity in both chemical structures and bioactivity profiles, we proposed that the other three compounds (CID: 97226, 72402 and 354677) might be novel candidates of TOP1 inhibitors.
Nevertheless, one must always keep in mind that this may only be confirmed if the binding mechanism is understood.
In this case, the binding modes of the two known inhibitors (CID: 24360 and 60699) have already been previously clarified (Staker et al., 2002, 2005).
The X-ray crystal structures of the enzymeinhibitor complexes indicate that the oxygen atoms connected to the positions 10, 17, 20, 21 and 22 (Fig.2A) are critical for the binding process by forming several hydrogen bonding interactions directly or indirectly (through water salt bridges) with relevant residues on TOP1.
These key interacting sites are basically preserved in other three compounds (CID: 97 226, 72 402 and 354 677).
Therefore, it further suggests that these compounds might be true TOP1 inhibitors, as supported by previous studies (Ping et al., 2006; Rapisarda et al., 2002; Wethington et al., 2008).
As mentioned in the Section 1, PubChem can provide rich information of the compoundtarget associations for a number of tested compounds in the NCI-60 dataset.
By combining such data from both repositories, it is possible for us to characterize the bioactivity of tested compounds at cellular level and molecular level simultaneously.
The compoundtarget interaction network drawn from the available PubChem HTS bioassays for the five compounds in cluster B is shown in Figure 2C.
As one can see, these five compounds were closely packed by sharing some common or relevant protein targets.
Among the three compounds (CID: 24360, 354677 and 72402), the first compound shared four common protein targets (GI: 119579178, 134304838, 2883 ftp://ftp[13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2884 28812888 T.Cheng et al.Fig.2.
The five camptothecin analogs identified from cluster B.
(A) 2D chemical structures, (B) bioactivity profiles in the NCI-60 cell lines on nine different organs and (C) compoundtarget interaction network (see Fig.4 for general description).
124263658 and 5174617) with the second compound.
In addition, it also shared three common protein targets (GI: 11545912, 25952111 and 5174617) with the third compound.
Moreover, these three compounds shared a common protein target (GI: 5174617).
This observation again demonstrates the effectiveness of the similarity principle.
While it remains to be further evaluated when sufficient data is available, we propose that compounds sharing significant similarity in both chemical structures and bioactivity profiles may have a higher chance for sharing similar patterns of interactions in the compoundtarget interaction network, comparing to those with only structural similarity.
The remaing two compounds (CID: 97226 and 60699) appear to be apart from the above three compounds in the common (or shared) compoundtarget interaction network.
This is mainly because they had not been tested on the same targets, against which the previous three compounds were tested, and thus gave insufficient information on their interactions with those relevant protein targets.
However, as indicated in Figure 2C, they had demonstrated similar activity to the compound CID: 24360 against several common protein targets in a pairwise manner, which may provide links to the other two compounds (CID: 354677 and 72402).
It should be mentioned that the abundant information of compoundtarget association in PubChem bioassays may also contain experimental noises such as promiscuous results.
While we cannot rule out the possibilities of the promiscuous effects or other artifacts in our analysis, we found that some of the compoundtarget associations were supported by previous studies.
For example, for the two compounds (CID: 24360 and 60699) in cluster B, they both exhibited activity against two common targets with one of them, hypoxia-inducible factor 1 (HIF-1, GI: 32879895), having been reported as the biological target for these two compounds (Klausmeyer et al., 2007; Rapisarda et al., 2002).
These investigations support that our findings result from experimental signals rather than noises.
It is noticeable that the compound CID: 354677 is also a known HIF-1 inhibitor (Rapisarda et al., 2002), though it had not been included in the compound target network due to insufficient data in PubChem (Fig.2C).
This again demonstrates that similarity in bioactivity profiles and patterns of interactions with relevant targets can be used to identify novel compounds for a certain target.
Nevertheless, further experiments will be needed to validate some of the potentially novel compounds identified by the MLP project.
3.3 Moderately similar structures with highly similar bioactivity profiles (Cluster F) Three compounds (CID: 2723601, 3246652 and 5351879) bearing partially structural similarity were identified from cluster F. Their 2D chemical structures and bioactivity profiles in the NCI-60 cell lines are given in Figure 3A and B, respectively.
These three compounds stood out from the rest because they exhibited the maximal intracluster similarity in their bioactivity profiles (Fig.3B).
In fact, cluster F was the first merged sub-tree during the hierarchical clustering process (Fig.1).
Compared to cluster B, the observation in cluster F may be even more interesting as the inter-compound structural similarities were significantly lower than those of cluster B.
For example, the two compounds (CID: 2723601 and 3246652), which produced the highest structural similarity (0.462) among this cluster, indicates only a moderate level of similarity in their chemical structures.
Another interesting observation seen from Figure 3B is that these three compounds show effective but still selective activity in the six leukemia cell lines, suggesting their potential 2884 [13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2885 28812888 Investigating the correlations among the chemical structures, bioactivity profiles and molecular targets of small molecules Fig.3.
The three compounds identified from cluster F. (A) 2D chemical structures, (B) bioactivity profiles in the NCI-60 cell lines on nine different organs and (C) compoundtarget interaction network (see Fig.4 for general description).
treatment to leukemia.
Indeed, one compound (CID: 2723601) is an approved small-molecule drug used in the therapy of several forms of leukemia (http://www.drugbank.ca/drugs/DB00352, last accessed date October 7, 2010).
The compoundtarget interaction network for the three compounds in cluster F is given in Figure 3C.
A single, common protein target (-globin, GI: 4504349) was shared by all three compounds, demonstrating again a strong correlation between the similarity in bioactivity profiles and that in the patterns of interactions with relevant protein targets.
According to the PubChem BioAssay database, the compound CID: 2723601 was tested active in the bioassay (AID: 910), while the other two compounds (CID: 3246652 and 5351879) were tested active in another bioassay (AID: 925).
Despite two separate bioassays, they were actually part of a series of assays in an attempt to seek for the modulators of hemoglobin-splicing (Supplementary Table S1).
A further analysis shows that though the overall structural similarity was relatively low, these three compounds possessed a common fragment of thioguanine, which may play a key role for the compounds to exhibit activity in modulating the hemoglobin splicing and some other biological processes.
This example suggests that similarity in bioactivity profiles derived from a broad panel of assays, together with the common features in chemical structures, can also indicate similarity in the mode of action for respective compounds, and can be used as a basis to determine information such as molecular targets or biological pathways for uncharacterized compounds.
3.4 Results for clusters A, C, D and E Unlike the compounds in clusters B and F, where strong correlations among chemical structures, bioactivity profiles and patterns of interactions with relevant protein targets can be observed, the compounds in the other four clusters did not fall in the same category for various reasons.
The three compounds in cluster A did not show significant similarity in either chemical structures or bioactivity profiles based on the data available (Supplementary Fig.S2), yet the results are still interesting despite the lack of overall coherence in the compoundtarget interaction network.
For example, two compounds (CID: 107985 and 253602), regardless of the difference in chemical scaffolds, were identified as showing inhibitory activities for the heat shock factor 1 (HSF1, GI: 62740231), which is in agreement with previous findings that both compounds are involved in the heat shock response pathway (Park and Liu, 2001; Westerheide et al., 2006).
Likewise, the four compounds in cluster E generally show low similarity in their bioactivity profiles (Supplementary Figure S3).
Moreover, structural similarity is also missing among compounds, making the current bioactivity profiles less useful in discovering novel compounds for the given targets.
Nevertheless, it remains interested to investigate the relationships among the compounds and their target networks in future when more bioassay data become available in PubChem.
The chemical structures and bioactivity profiles for the three compounds identified from cluster C are listed in Supplementary Figure S4.
These three compounds exhibited certain structural similarity by sharing a common fragment of di-ketone (Supplementary Figure S4A), which may be responsible for the notable similarity in their bioactivity profiles (Supplementary Figure S4B).
This observation resembled the results of cluster F, where compounds moderately similar in chemical structures with common fragment demonstrated significant similarity in bioactivity profiles.
As for the compoundtarget interaction network (Supplementary Figure S4C), however, only one compound (CID: 4212) in cluster C had been extensively assayed on multiple protein targets, against which the other two compounds were not tested.
Therefore, target networks cannot be compared directly due to lack of experimental support.
Nevertheless, for the compound CID: 548171, considering its notably high similarities to the compound 2885 [13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2886 28812888 T.Cheng et al.Fig.4.
The complete diagram of the compoundtarget interaction network for the 24 compounds identified from the six clusters (i.e.A to F) obtained by hierarchical clustering.
Compounds are denoted as ellipses, which are labeled with PubChem compound identifier (CID) and colored according to the clusters they belong to.
Targets are denoted as rectangles, which are labeled with NCBI protein identifier (GI) and colored with dark or light red if the corresponding assay is a confirmatory or primary bioassay in PubChem, respectively.
The edge linking an ellipse and a rectangle indicates that there is an interaction if the current compound is found active against the target of interest.
No edge is allowed between either two ellipses or two rectangles.
For simplicity, target nodes that have only single connecting compound node are not shown.
CID: 4212 in both chemical structures and bioactivity profiles (Supplementary Figure S4A and B), it may also interact with certain protein targets shared by the compound CID: 4212.
This hypothesis for predicting partially characterized compound according to wellcharacterized ones remains highly interested to be verified by future experiments.
As for the six compounds identified from cluster D, though there was no obvious similarity either in chemical structures or bioactivity profiles, they seemed to considerably show several common patterns of interactions with relevant protein targets (Supplementary Figure S5).
For example, four (CID: 262093, 5614, 221363 and 252101) out of the six compounds in cluster D were found active against several protein targets belonging to various cytochrome p450 families and/or subfamilies, suggesting that they may be effective in the p450-regulated pathways.
This observation suggests that the compoundtarget interaction network derived from PubChem bioassays may be useful to identify a set of related compounds involving in the same/similar biological pathway.
3.5 Overview of compoundtarget interaction network The compoundtarget interaction networks analyzed in the above sections were drawn for the compounds within the same hierarchical cluster of bioactivity profiles.
It remained highly interested to investigate how compounds can be organized solely by their patterns of interactions with relevant protein targets, and how that can be compared to the hierarchical clusters derived from the bioactivity profiles analysis.
To this end, a complete diagram of compound target interaction network, as shown in Figure 4, was built for all the compounds identified from the six clusters (i.e.A to F) using the abundant information of compoundtarget association in PubChem bioassays.
In general, the network was rather complex and presented a great challenge for data analysis as a considerable number of compounds had demonstrated interactions against multiple protein targets.
Though these interactions remained to be further evaluated by identifying and excluding noises in the current assay data, the multitude of compoundtarget associations may reveal the promiscuous properties for certain compounds at the first glance and may facilitate the investigation of the polypharmacological properties of small molecules.
Despite the observed complexity, the compounds shown in Figure 4 can still be roughly grouped by their patterns of interactions with relevant protein targets.
For instance, the six compounds obtained from the bioactivity profile cluster D (colored in green) tended to pack into a group, which was well supported by the fact that there were so many common interacting targets shared by two or more compounds (Supplementary Figure S5).
Similarly, the three compounds from the bioactivity profile cluster F (colored in cyan) can also be identified as a group due to a commonly shared protein 2886 [13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2887 28812888 Investigating the correlations among the chemical structures, bioactivity profiles and molecular targets of small molecules target (GI: 4504349).
Therefore, it is interesting to observe that the groups of compounds identified from the target network were, to certain extent, consistent with those obtained by the clustering analysis based on bioactivity profiles.
This observation indicates that there could be strong correlations between a compounds bioactivity profile (cellular level) and its pattern of interactions with relevant protein targets (molecular level).
The compounds in the above two clusters exhibited much larger variances (i.e.higher specificity) in their bioactivity profiles, which may contribute to their relatively converged patterns of interactions with relevant protein targets.
In contrast, some compounds presented in bioactivity profile cluster A (colored in yellow) showed generalized toxicity with low selectivity and specificity (Supplementary Figure S2B), making them difficult to be identified as a group from Figure 4.
This analysis was done using a binary bioactivity outcome when considering the compound target association.
Further analysis may be performed in future work by incorporating the quantitative potency data (e.g.IC50) of each compound to provide more insights.
4 CONCLUSIONS By taking advantages of the publicly available data from both PubChem HTS bioassays and NCI-60 human tumor cancer cell line screens, we have investigated the correlations among the bioactivity profiles, molecular targets and chemical structures of small molecules.
Hierarchical clustering of tested compounds was carried out based on their bioactivity profiles derived from the NCI-60 cell line screens, and several interesting clusters were identified.
First, the correlation between bioactivity profiles and chemical structures was analyzed and strong SAR was suggested.
For example, the compounds in cluster B, which were highly similar in chemical structures, also demonstrated notable similarity in their bioactivity profiles.
Even more interesting observations were given by cluster F, where compounds were only moderately similar in chemical structures and produced extremely significant similarity in bioactivity profiles.
Second, analysis on the compoundtarget interaction network was performed and showed clear correlations between the bioactivity profiles of compounds and their patterns of interactions with relevant protein targets, especially when chemical structures were related.
Furthermore, a complete compoundtarget interaction network, which was drawn for all the compounds identified from the six clusters, produced roughly the same groups of compounds as that obtained by hierarchical clustering analysis based on bioactivity profiles.
This study shows that strong correlations can be observed between similarity in bioactivity profiles (cellular level) and that from the patterns of interactions with relevant protein targets (molecular level), and suggests that novel compound candidates with desired pharmacological properties can be identified by comparing their bioactivity profiles and/or compoundtarget interaction network to well-characterized compounds.
ACKNOWLEDGEMENTS We thank the National Institutes of Health Fellows Editorial Board (FEB) for article revision.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine.
Conflict of Interest: none declared.
ABSTRACT Motivation: The computational identification of transcription factor binding sites is a major challenge in bioinformatics and an important complement to experimental approaches.
Results: We describe a novel, exact discriminative seeding DNA motif discovery algorithm designed for fast and reliable prediction of cis-regulatory elements in eukaryotic promoters.
The algorithm is tested on biological benchmark data and shown to perform equally or better than other motif discovery tools.
The algorithm is applied to the analysis of plant tissue-specific promoter sequences and successfully identifies key regulatory elements.
Availability: The Seeder Perl distribution includes four modules.
It is available for download on the Comprehensive Perl Archive Network (CPAN) at http://www.cpan.org.
Contact: martina.stromvik@mcgill.ca Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION The binding of transcription factors to relatively short and variably degenerate regulatory DNA sequences (cis-regulatory elements) is central to the regulation of gene expression (Orphanides and Reinberg, 2002).
While several sequenced genomes are nearly deciphered in terms of the protein-coding gene repertoire, the inventory and comprehensive characterization of cis-regulatory elements remains elusive.
Motif discovery has motivated the development of numerous tools and algorithms, and the use of various motif models and statistical approaches (Guha Thakurta, 2006).
Motif discovery can be broadly divided into sequence-driven and pattern-driven methods.
The former methods typically involve building a position-weight matrix (PWM) from sequence data, and local search techniques such as expectationmaximization or Gibbs sampling are used to optimize the log likelihood ratio until convergence or a maximum number of iterations is reached.
Though routinely fast, those methods are not guaranteed to yield the best solution, or global optimum (Stormo, 2000).
Enumerative methods, on the other hand, are guaranteed to find a global optimum but have the drawback of being computationally expensive and limited to short motifs.
Searching a set of sequences for patterns that are overrepresented relative to a given background model may converge towards To whom correspondence should be addressed.
motifs that are prevalent in the genome thus not likely to represent regulatory elements.
Sinha (2003) introduced the notion of discriminative motif discovery in which a motif is treated as a feature that leads to good classification between positive sequences deemed to contain common cis-regulatory elements and a set of background sequences.
In this work, we present the Seeder algorithma novel, exact discriminative seeding DNA motif discovery algorithm inspired by Keich and Pevzner, 2002; Pizzi et al., 2005.
The major benefits of the Seeder algorithm are (i) the use of intuitive and reliable statistics for the choice of motif seeds and (ii) a data structure that significantly accelerate the computation of motifs and background models.
The algorithm is benchmarked against popular motif finding tools and demonstrates greater performance.
The algorithm is applied to the analysis of Arabidopsis thaliana seed-specific (the plant structure seed, not to be confused with motif seed) promoters and identifies motifs with high similarity to seed-specific cis-regulatory elements experimentally characterized in Brassica napus, a closely related species.
2 METHODS 2.1 The Seeder algorithm Our algorithm starts by enumerating all nucleotide combinations (words) of a given length, usually six.
For each word, it calculates the Hamming distance (HD) between the word and its best matching subsequence (we call this distance the substring minimal distanceSMD) in each sequence of a background set.
This data is used to produce a word-specific background probability distribution for the SMD.
For each word, it then calculates the sum of SMDs to sequences in a positive set.
The P-value for this sum is calculated using the word-specific background probability distribution.
The word for which the P-value is minimal is retained, and a seed PWM is built from the closest matches to this word found in every positive sequence.
The seed PWM is extended to full motif width and sites maximizing the score to the extended PWM are selected, one in each positive sequence.
A new PWM is built from those sites and the process is iterated until convergence, or a maximum number of iterations is reached.
2.1.1 Input data and parameters Our algorithm takes as input a set B = {B1, , Bm} of m background sequences of length L, a set P = {P1, , Pn} of n positive sequences of length L, the length k of the motif seed and the length l of the full motif to discover.
2.1.2 Substring minimal distance The HD between two strings of equal lengths is the number of positions at which symbols differ (Hamming, 1950).
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
F.Fauteux et al.We define the SMD d(w,w) between a short nucleotide sequence w and a longer sequence w as the minimal HD between w and a |w|-length substring of w. 2.1.3 Background model A discrete random variable Y (w) is associated with each word w of seed length k, corresponding to the SMD between w and a randomly selected background sequence from B.
This w-specific distribution function is obtained empirically from B; for each word w, we set gw(y) = Pr[Y (w) = y] = |{Bi: d(w,Bi) = y}|/m, for y = 0, , k. 2.1.4 Seed position weigth matrix For each word w, the sum of SMDs to the positive sequences S(w)=j d(w,Pj) is computed.
Under the background model, the distribution function of this sum of n independent and identically distributed (i.i.d.)
random variables is gn w (y), the n-fold selfconvolution of gw(y) (Grinstead and Snell, 1997).
The P-value (p) for word w with sum S(w), which is the probability of obtaining a sum lower or equal to S(w) under the assumption that Pjs are random in respect to w, is p(S(w))= S(w) y=0 gn w (y) (1) The word w for which the P-value p (S(w)) is minimal is retained.
For each positive sequence in P, the set of one or more subsequences of length k having the SMD to w are retained.
A PWM P0 is built from this set of selected subsequences using standard procedures and pseudocounts proportional to n (Wasserman and Sandelin, 2004), with the modification that when a sequence contains more than one match, each match (subsequence) weight is reduced proportionally.
The subsequence associated with the highest score to P0 is retained in each sequence, and the seed PWM Ps is built from this optimal set of n subsequences, as described above.
2.1.5 Full length motifs The seed PWM Ps is of width k, smaller than the full motif width.
It is extended to full motif width l by adding null weights at (lk)/2 positions upstream and downstream.
The full length PWM is then refined by iterating the following process.
(i) Sites (one per sequence in P) maximizing the score to the extended weight matrix are selected and (ii) a revised full length PWM is built from those sites.
This process is repeated until convergence (i.e.the sites maximizing the PWM score are fixed in all sequences) or for at most a default number of 10 iterations, which we observed to often be sufficient for the convergence of significant seeded motifs.
2.1.6 N-fold self-convolution Our implementation of the n-fold selfconvolution uses the binary expansion of n (Sundt and Dickson, 2000), and is an adaptation of the square and multiply algorithm (Gordon, 1998) while convolutions per se are computed using the input side algorithm (Smith, 1997).
2.1.7 Multiple hypothesis testing correction For each motif predicted, a list of 4kP-values is generated thus prompting for a multiple testing correction.
This is carried out by generating a list of q-values from the list of P-values associated with words of seed length k, using the general algorithm for estimating q-values described in (Storey and Tibshirani, 2003).
The statistical significance of a motif is evaluated with the q-value of the sum S(w), which is the expected proportion of false positives incurred when calling the sum significant (i.e.not likely to have occurred if the positive sequences were randomly selected).
2.1.8 Searching both strands Because transcription factor binding sites (TFBS) can be located either on the forward or the reverse strand, motifs are typically searched for on both strands.
This is easily achieved with Seeder: one simply redefines the SMD so as to consider matches one both strands (for both the background and positive sequences) and perform PWM matching similarly.
Fig.1.
SMD index generation.
The SMD index generation is illustrated for the word CAG.
N, top-level tree node nucleotide numerical value; d, level.
2.1.9 Multiple motifs When the user asks to retrieve more than one motif, the sites identified in the preceding run(s) are masked and the motif-finding process is repeated.
The positions of the sites are obtained by scanning each sequence (plus strand first) until the highest scoring subsequence is found.
2.2 Data structures The calculation of SMDs using direct string comparison approaches requires a considerable amount of operations and this probably explains in part why this quantity has not been more often exploited for DNA motif discovery.
We have designed a data structure based on the organization of the matrix of HDs between words of length 6 (see Fig.4, supplied as supporting information).
This structure, called the SMD index (Fig.1), allows very efficient lookup, in a given sequence, for a subsequence minimally distant to a given word, hence improving the efficiency of the SMD computation.
2.2.1 SMD index generation Each nucleotide is mapped to a numerical value (A,C,G,T0,1,2,3).
For a given word w = w1,w2, ,wk of length k, a list of indices is generated equivalent to a tree structure with levels d = 0, , k 1.
At each new level of the tree, each node is expanded into four nodes, one for each possible nucleotide N {0,1,2,3} at that position.
An index id = N + (4 id1) is assigned to each new node, where id1is the index of the parent node.
At the final level, the tree has nodes and indices corresponding to all possible nucleotide sequences of length k. For a given node at a given level d, the HD is one more than that of the parent, except for the node corresponding to nucleotide wd+1, where the HD is unchanged (Fig.1).
The SMD index is precomputed for every word w of seed length k and HDs between 0 and 3, which requires a marginal amount of memory and appreciably accelerates the process.
2.2.2 SMD calculation The number of occurrences of every word of length k in each sequence in P is stored using base 4 indexing (word count array).
The SMD between w and sequence Pj is obtained by looking up elements in word count array of Pj , in order of increasing HD to w, until a nonzero count is found.
2304 Discriminative seeding motif discovery 2.3 Benchmarking of motif discovery tools The performance of the Seeder algorithm was compared with that of popular motif discovery tools using benchmarks designed for robust assessment of motif discovery algorithms (Sandve et al., 2007).
In the benchmark suites, binding site sequences from the Transfac database (Wingender et al., 1996) are represented either in their original genomic context sequences (Model RealMR, Algorithm RealAR) or in sequences generated with a third-order Markov model (MM) (Algorithm MarkovAM).
The reverse complement of sequences is used in cases where the original binding site appears on the negative strand, so all sites within the benchmark suites appear in the forward sequence.
The MR suite contains motifs that, according to Sandve et al.(2007), are harder to distinguish from the local background using common motif models (consensus, PWM and mismatch).
The AM and AR suites each contain 50 datasets and a total of 810 sequences of mean length 1300 nucleotides, and the MR suite contains 25 datasets and a total of 410 sequences of mean length 1250 nucleotides.
2.3.1 Parameter settings In order to be representative of common usage where parameter adjustment is nominal while providing homogeneous instructions to different software, sequences were scanned in the forward orientation, searching for one motif of width 12 with one occurrence (site) per sequence.
Other parameters were left to default values.
We ran Seeder v. 0.01 (this article), Weeder v. 1.3.1 (Pavesi et al., 2004), BioProspector v. 1 (Liu et al., 2001), MEME v. 3.5.4 (Bailey and Elkan, 1994), the Gibbs Motif Sampler v. 3.03.003 (Lawrence et al., 1993) and Motif Sampler v. 3.2 (Thijs et al., 2001) on each dataset.
The DIPS algorithm (Sinha, 2006) was not included in the benchmark study because it was associated with prohibitive runtime requirements under our computational conditions.
Background models were generated separately for each suite using all sequences within the suite.
Background distributions for words of length 6 were generated using the Seeder::Background module.
Frequency files (expected values for 6-mers and 8-mers) used by Weeder were generated using a custom Perl script.
A sixth-order MM was generated for MEME using a custom Perl script, and for Motif Sampler using the INCLUSive CreateBackgroundModel program (Thijs et al., 2002).
The default (thirdorder) MM was generated for BioProspector using the genomebg program provided with the software.
2.3.2 Evaluation of motifs versus known binding sites The predictions were evaluated using the suite of tools described in (Sandve et al., 2007) (http://tare.medisin.ntnu.no).
The predictions were scored using the nucleotide-level Pearsons correlation coefficient (nCC) (Tompa et al., 2005).
Differences between scores were assessed using paired t-tests ( = 0.05).
2.4 Motif discovery in the promoters of Arabidopsis seed-specific genes A background set of 22 032 nuclear protein-coding gene promoters (500 bp upstream of the transcription start site) was generated using the TAIR (release 7) loci upstream sequences dataset (sequences preceding the 5 end of each transcription unit) and the protein-coding with transcript support listing (loci with supporting cDNA or ESTs deposited in Genbank), downloaded from the TAIR ftp server (ftp://ftp.arabidopsis.org).
Tissuespecific promoter sequence sets were assembled according to marker gene data from Schmid et al.(2005).
The Seeder algorithm was used to perform motif prediction in seed-specific promoters using a seed length of six and a motif length of 12, and the protein-coding with transcript support gene promoters as a background.
3 RESULTS 3.1 Performance of motif discovery tools Figure 2 shows the differences between scores of different motif discovery tools on the benchmark suites of Sandve et al.(2007).
Fig.2.
Average benchmarking scores and pairwise differences between motif discovery tools.
Average nucleotide-level Pearson correlation coefficient (nCC) and pairwise differences ( nCC) for six motif discovery tools tested on three benchmark suites.
Error bars correspond to 95% confidence intervals.
Stars indicate significant differences ( = 0.05) between scores.
On the AM suite, the performance of each tool was statistically equivalent.
Interestingly, the tool that performed the best (though by a nonsignificant margin), BioProspector, models background sequences using a third-order MM, the same type as that used by Sandve et al.(2007) to generate the AM background sequences.
Seeder, BioProspector, Weeder, MEME and the Gibbs Sampler scored equally on the AR suite, which contains binding sites in their original sequence.
The MR suite also contains binding sites in their original sequence, but in this case the binding sites have a composition that is more similar to that of the surrounding background sequence.
This suite was assembled for the purpose of testing novel motif models (Sandve et al., 2007).
Seeder scored significantly higher on the MR suite than any other algorithm tested.
At first glance, it may seem surprising that the performance of some tools is actually higher on the MR suite than on AR suite.
However, although the similarity of motifs to their local background does complicate the task of motif-finding approaches using local background models, this does not overly affect those based on global background models.
It nonetheless appears that our discriminative approach to seed selection yields a nonnegligible advantage to Seeder.
Having said that, it should be noted that for a number of individual datasets the scores obtained by other tools are higher than that of Seeder, which highlights the complementary of these programs.
2305 F.Fauteux et al.A B Fig.3.
Arabidopsis seed-specific motifs.
Sequence logos of motifs overrepresented in the promoters of A. thaliana seed-specific marker genes.
(A) Full-length forward motifs.
(B) Reverse complement of motifs.
3.2 Arabidopsis seed-specific motifs The Seeder algorithm was used to discover motifs (on both strands) in a set of 57 promoter sequences of A. thaliana seedspecific marker genes identified by expression data analysis (Schmid et al., 2005).
The computation of the background distributions (motif seed length of 6) took 35 min using a single Intel 86 processor, and motif computation took 3.5 min per motif reported.
This example shows that most of the computing time is used to compute the background model, particularly when using genomescale background datasets.
The Seeder::Background module was therefore designed to precompute background models which can be reused for any number of motif finding operations.
The top two predictions (q-value < 0.01) were compared to known plant motifs in the PLACE database (Higo et al., 1998) using the STAMP web server (Mahony and Benos, 2007).
The first motif (Fig.3, m1) (q-value = 4.4 109, information content = 7.4) and the second motif (Fig.3, m2) (q-value = 1.1 103, information content = 7.6) are similar to two experimentally characterized cisregulatory elements found in the napA promoter in B. napus, the RY repeat (CATGCA) (E = 6.32 108) and the G-box (CACGTG) (E = 2.92 105) (Ezcurra et al., 1999).
The function of these regulatory elements was shown by substitution mutation analysis using promoterreporter gene fusions, leading to a strong reduction of the napA promoter activity in seeds (Ezcurra et al., 1999).
The second motif is also highly similar to a sequence (ACGTGTC) (E = 4.70 1011) overrepresented in the promoters of A. thaliana genes downregulated during seed germination (Ogawa et al., 2003).
4 CONCLUSION We have described a novel algorithm for DNA motif discovery and demonstrated its capacity to discover motifs in real biological datasets.
Advantages of the algorithm over other approaches include (i) the enumerative-guaranteed optimality of seed selection; (ii) a background model based on empirical distribution of SMDs; and (iii) efficient data structures that make background and motif computations relatively fast at moderate seed lengths.
We have benchmarked the algorithm against popular motif finding tools and demonstrated its performance to be equal or better than that of other tools on biological datasets.
We note however that, although the Sandve et al.(2007) benchmarks proved extremely useful for our performance analysis, it would be ideal to have suites designed specifically for discriminative motif-finding algorithms.
Tompa et al.(2005) recommend biologists to use a few complementary tools, and to consider the top few predicted motifs of each tool.
Based on the benchmarks results presented in this study, we recommend the inclusion of Seeder in the biologists DNA motif discovery toolbox.
The present implementation of Seeder allows for motif searches in the mode one occurrence per sequence (oops).
This assumption is deeply engrained in the algorithm and statistics for the selection of the motif seed and the construction of the seed PWM.
Of course, once a good seed PWM has been selected, other search modes [e.g.zero-or-one occurrence per sequence (zoops) or anynumber of repetitions (anr)] could be implemented using the type of frameworks previously implemented in tools like MEME or BioProspector.
We have applied the algorithm to the analysis of A. thaliana seedspecific promoters and found that the top two motifs were similar to experimentally characterized cis-regulatory elements found in the promoters of B. napus seed-storage protein genes.
This was unanticipated, considering the array of gene families and functions found in the seed-specific gene set from (Schmid et al., 2005).
ACKNOWLEDGEMENTS We thank G.K. Sandve (Norwegian University of Science and Technology, Trondheim, Norway) for helpful comments, and the Perl Monks (http://perlmonks.org) for support in the development of the Perl modules.
We also thank the reviewers for their constructive comments.
We also acknowledge support from Fonds qubcois de recherche sur la nature et les technologies (FQRNT) and Centre SVE.
Funding: Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant No.
283303 (to M.V.S.
); NSERC Postgraduate Scholarship (PGS D) (to F.F.).
Conflict of Interest: none declared.
ABSTRACT Summary: ArrayExpress is one of the largest public repositories of microarray datasets.
R/Bioconductor provides a comprehensive suite of microarray analysis and integrative bioinformatics software.
However, easy ways for importing datasets from ArrayExpress into R/Bioconductor have been lacking.
Here, we present such a tool that is suitable for both interactive and automated use.
Availability: The ArrayExpress package is available from the Bioconductor project at http://www.bioconductor.org.
A users guide and examples are provided with the package.
Contact: audrey@ebi.ac.uk Supplementary information: Supplementary data are available Bioinformatics online.
1 INTRODUCTION ArrayExpress is a public database for high-throughput functional genomics data (Parkinson et al., 2009).
It consists of a repository, which is a MIAME (Brazma et al., 2001) supportive public archive of microarray data, and an added value gene expression Atlas created from the repository data.
Currently, nearly 8000 experiments comprising 230 000 arrays are available from ArrayExpress.
Retrieving publicly available data for analysis is a repetitive and error prone task for which automation is desirable.
As Bioconductor (Gentleman et al., 2004) contains many widely used tools for the data analysis, tools to make a connection with public databases are useful.
The GEOquery package (Davis and Meltzer, 2007) was developed to load GEO datasets into Bioconductor, and the RMAGEML package (Durinck et al., 2004) was designed to import the MAGE-ML files that in the past were used by ArrayExpress for data transfer.
The ArrayExpress database now supports the MAGE-TAB format (Rayner et al., 2006), a metadatarich, but much simpler and more resource-efficient format based on tab-delimited files and all data are made available in this format.
We have developed the ArrayExpress package for R/Bioconductor to query ArrayExpress and convert MAGE-TAB formatted datasets from the ArrayExpress repository into objects of the Bioconductor class for microarray datasets, eSet.
To whom correspondence should be addressed.
2 MIAME MIAME is a guideline that describes the Minimum Information About a Microarray Experiment needed to ensure interpretation of a microarray dataset.
It has five elements: (i) the raw data for each hybridization, (ii) the final processed data for the set of hybridizations in the experiment, (iii) the experiment design including sample data relationships and the essential sample annotation including experimental factors and their values, (iv) sufficient annotation of the array design and (v) essential laboratory and data processing protocols.
3 MAGE-TAB MAGE-TAB is a tabular MIAME supportive file format and MAGE-TAB documents consist of five different types of files.
(i) A raw zip archive contains the raw data files, i.e.the files produced by the microarray image analysis software, such as CEL files for Affymetrix GeneChips or GPR files from GenePix.
(ii) A data matrix file contains processed values, as provided by the data submitter, converted into a common tab-delimited text format representing a matrix of numbers.
(iii) The Sample and Data Relationship Format (SDRF) tab-delimited file contains the relationships between samples and arrays, as well as sample properties and experimental factors, as provided by the data submitter.
(iv) The Array Design Format (ADF) tab-delimited file describes the design of an array, i. e. the sequence located at each feature on the array and annotation of the sequences.
(v) The Investigation Description Format (IDF) tab-delimited file contains top-level information about the experiment including title, description, submitter contact details and protocols.
4 BIOCONDUCTOR CLASSES The Bioconductor class eSet is a different implementation of the MIAME standard.
The class has various specializations, or subclasses, that are adapted to specific array technologies, among these are ExpressionSet for generic one-colour datasets, NChannelSet for generic two-colour datasets and AffyBatch for data from Affymetrix GeneChips.
Objects of this class contain one or more identical-sized numeric matrices as assayData elements.
They also include a table describing the samplearray relationship asphenoData and a table describing the array features 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:57 14/7/2009 Bioinformatics-btp354.tex] Page: 2093 20922094 Importing ArrayExpress datasets into R/Bioconductor as featureData.
Details of experimental methods are in the component structure experimentData.
5 RETRIEVING AND CONVERTING MAGE-TAB DATA The ArrayExpress package uses the zip archive with either the raw or the processed data to build the assayData component.
The SDRF file is used to construct the phenoData table.
The ADF file is used to construct the featureData, and the IDF file to fill in the experimentData components.
5.1 Raw data To import a raw dataset from ArrayExpress, one can use the following R code after loading the package: AEset <-ArrayExpress("E-ATMX-18") As E-ATMX-18 is a two-colour experiment, the returned R object is of class NChannelSet.
If the identifier refers to an Affymetrix experiment, the output is an AffyBatch, if it refers to a one-colour experiment using a platform other than Affymetrix, the output is an ExpressionSet.
The ArrayExpress function extracts feature intensity summaries from columns of the raw data files based on the common conventions for the data file sources.
If the data source is not recognized, or the file does not have the expected column names, the user is asked to explicitly provide the name of the column(s) to extract, for instance, Cy3 Median.
In some cases, there is a mismatch between the sample or feature annotations and the intensity data files; in such cases, a warning is emitted, the phenoData and/or featureData components are left empty and an incomplete (but syntactically valid) object is returned.
Tested on the 5298 accessions with raw datasets that were available from the ArrayExpress repository in March 2009, the ArrayExpress function managed to create a complete object in 58% of the cases (Table 1).
The 42% of cases in which the function failed or an incomplete object was produced are due to a variety of reasons, including missing or contradictory data in the repository.
We are actively working on manually curating these cases and resolving problems as much as possible; however, due to the repositorys role as a public record of scientific activity, problems inherent to information submitted by the contributors may persist.
In addition to calling the one-stop function ArrayExpress, it is possible to download the data for local storage using the function getAE and to import a locally stored MAGE-TAB document with the function magetab2bioc.
5.2 Processed data The way processed data are handled in the database is less uniform than for raw data, because processing methods vary more than the microarray image analysis software outputs.
To import a processed dataset from ArrayExpress, three steps are required: download the dataset, identify which column is of interest, create the R object.
Example code looks as follows: dat <-getAE("E-TABM-1", type = "processed") cn <-getcolproc(dat) show(cn) AEset <-procset(dat, cn[2]) Table 1.
Application of the ArrayExpress package to the ArrayExpress database in March 2009 Number of accessions 6117 Number of datasets 6891 Objects created fully automatically 5550 81% Complete objects created 4017 58% Affymetrix 3407 Two-colour 89 One-colour 521 Incomplete objects 1533 22% Missing feature annotation 1121 Missing sample annotation 466 Objects created with manual selection of columns 619 9% Object creation failed 722 10% The number of datasets is higher than the number of accessions since some accessions store multiple datasets (we consider measurements made with different arrays and different datasets).
Manual setting of column names was necessary for 1082 (16%) of the 6891 datasets, and we were successful in 619 (9%) cases.
Here, cn is a character vector of all columns in the processed data, and after visual inspection, we decided to use the second one.
6 APPLICATION We used the queryAE function to list all datasets concerned with breast cancer in Homo sapiens.
Then, using the ArrayExpress function, we created R objects from all datasets for which raw data were available.
We counted, for each dataset, the number of arrays and features.
The Supplementary table summarizes the results of this analysis.
This could now be followed by an integrative analysis of the data, a complex and open-ended task for which essential tools are provided in the Bioconductor project: the quality of the datasets could be assessed with the help of the arrayQualityMetrics package (Kauffmann et al., 2009), they could be normalized and analysed for differential expression of genes and gene sets (Hahne et al., 2008), and the combination of different datasets is facilitated, for example, by the MergeMaid package (Cope et al., 2004).
7 CONCLUSIONS The ArrayExpress package is freely available, open source and easy to use.
As most of the Bioconductor tools for microarray analysis process eSet objects, the package facilitates large-scale analyses of public data.
A strength of the package is the richness, accuracy and standardized format of the metadata that it imports together with the array intensity data.
In fact, the diagnostics produced by the package during dataset import from the ArrayExpress repository are currently used by the curators to decrease the number of problematic experiments and improve the quality of the content delivered.
For the end user, the ArrayExpress package eliminates, or at least greatly reduces the amount of manual intervention needed and helps towards automated processing of large collections of datasets.
ACKNOWLEDGEMENTS We would like to thank A. Tikhonov, J. Cho, R. Santamaria and the ArrayExpress group.
Funding: EU FP6 (EMERALD, LSHG-CT-2006-037686); National Institutes of Health (5P41HG003619-05).
2093 [15:57 14/7/2009 Bioinformatics-btp354.tex] Page: 2094 20922094 A.Kauffmann et al.Conflict of Interest: none declared.
ABSTRACT Motivation: The anatomy of model species is described in ontologies, which are used to standardize the annotations of experimental data, such as gene expression patterns.
To compare such data between species, we need to establish relations between ontologies describing different species.
Results: We present a new algorithm, and its implementation in the software Homolonto, to create new relationships between anatomical ontologies, based on the homology concept.
Homolonto uses a supervised ontology alignment approach.
Several alignments can be merged, forming homology groups.
We also present an algorithm to generate relationships between these homology groups.
This has been used to build a multi-species ontology, for the database of gene expression evolution Bgee.
Availability: download section of the Bgee websiteContact: marc.robinson-rechavi@unil.ch Supplementary information: Supplementary data are available at Bioinformatics online.
Received on January 14, 2010; revised on May 6, 2010; accepted on May 26, 2010 1 INTRODUCTION Databases dedicated to model species rely on the usage of ontologies, for example the zebrafish anatomy for ZFIN (Sprague et al., 2006), or the Mouse gross anatomy and development (Baldock et al., 2003).
Such ontologies of anatomy and development facilitate the organization of functional data pertaining to a species.
For example, all gene expression patterns described in ZFIN are annotated using the zebrafish anatomical ontology.
A list of such ontologies is kept on the Open Biomedical Ontologies (OBO) website (Smith et al., 2007).
To pool the experimental data from different model species, we need to encode corresponding information between ontologies which describe different anatomies (e.g.zebrafish and human).
For example, we are interested in integrating and comparing gene expression patterns between several species (Bastian et al., 2008).
The most widely accepted criterion to make such comparisons in biology is homology (Hall, 1994; Hossfeld and Olsson, 2005).
When we compare two elements, whether or not they are derived from the same ancestral element defines our expectation of similarity To whom correspondence should be addressed.
between them, and the interpretation of differences.
For example, if a chicken wing is not homologous to a fly wing, we do not expect the same underlying structures, and similarities can be attributed to functional convergence.
Whereas the chicken wing is homologous (as a limb) to the human arm, thus we do expect the same underlying structures, and differences can be attributed to divergent evolution.
There are different definitions of homology (Roux and RobinsonRechavi, 2010), and our algorithm does not in itself impose one on the user.
We do recommend choosing an explicit definition and using it consistently throughout the analysis.
In practice, hundreds of terms must be compared between ontologies that may differ both in the actual biology modeled (i.e.a fish is not a mammal) and in the representation used.
Although a purely manual annotation of homologies is possible, it would be too time consuming to be done for all terms between several divergent species.
Kruger et al.(2007) have used a manual approach to find similarities between simplified anatomy ontologies for human and mouse.
As both are mammals, they share most structures and terminology.
There are also on-going efforts to integrate anatomical ontologies (Haendel et al., 2008; Washington et al., 2009), which are often geared towards the comparison of phenotypes (Lussier and Li, 2004).
As far as we know, the question of using homology to align anatomical ontologies has never been explicitly addressed.
Since the problem is to find correspondences between the concepts of two ontologies, we draw on methods from schema matching, or ontology alignment (Euzenat and Shvaiko, 2007; Lambrix and He, 2008).
As opposed to more generalist solutions, we present a algorithm which is specialized in the alignment of anatomical ontologies.
The specificities of these ontologies include high redundancy of terms, and few types of relations.
Finally, a specific issue is that structures which have the same name and are related to similar concepts may not be homologous.
This is the case of the insect eye and the mammalian eye.
While some underlying molecular mechanisms are similar, these structures evolved independently and are not considered homologous (discussed in Hall, 1994; Shubin et al., 2009).
Unsupervised alignment algorithms would misleadingly align such similarities; this is for instance the case for the LOOM software used on the NCBO portal (Ghazvinian et al., 2009).
In principle, an alignment algorithm should aim at finding the largest number of true positives, while avoiding false positives.
In practice, our experience is that the size and structure of anatomical ontologies leads to very large numbers of false positives if a naive approach is taken (i.e.common words).
Thus, the basic aim of Homolonto is to propose in priority to the user the best candidate The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:48 16/6/2010 Bioinformatics-btq283.tex] Page: 1767 17661771 Homolonto pairs of homologs, and avoid the need to consider many irrelevant pairs.
2 SYSTEMS AND METHODS Homolonto is implemented in Java.
Ontologies are read in the OBO format (Smith et al., 2007).
Homolonto is freely available in the download section of the Bgee website (http://bgee.unil.ch/).
3 ALGORITHM 3.1 Principle Ontology alignment is the process of determining correspondences between ontology concepts.
We present our approach based on the classification of ontology matching systems proposed by Euzenat and Shvaiko (2007; Shvaiko and Euzenat, 2005).
Biological ontologies simplify some aspects relative to the general case.
The types of concepts (e.g.anatomical structures) and the relationships (e.g.part_of ) are known in advance, and known to be common between the ontologies to align.
Moreover, in the present implementation we only seek to establish one type of relation, homology.
Our algorithm can be described as a composite system (Fig.1), using: (i) language-based comparison of names with tokenization (element level, syntactic technique); (ii) graph-based matching of children of elements (structure level, syntactic technique); (iii) data analysis, e.g.statistics on word occurrence (structure level, syntactic technique); (iv) external input from the user (element level, external technique; classification following Euzenat and Shvaiko, 2007).
We combine the results in parallel, as opposed to in sequence, by using a sum of scores from different techniques.
Thus, we make use both of schema and element level information.
The algorithm produced in a first step anchors at the element level, generated by language technique, and potentially by the user (external), then uses information from the schema, the elements, and user input, to improve the alignment based on these anchors.
Importantly, each proposition of homology between elements must be validated by the user (external input), to take into account Fig.1.
Homolonto pairwise alignment architecture.
O1 and O2 are ontologies to align.
P and P are lists of propositions.
H is a list of validated homologies (invalidation information).
A is the final alignment, generated when the user chooses to stop iterations.
User input appears twice: to propose original pairings, and to validate propositions.
such cases as the eye, discussed in the Introduction section.
Thus our process is a supervised one.
Finally, we note that the alignment we obtain is of the form many to many, not one to one.
3.2 Definitions A central concept in our algorithm is that of a proposition (similar to suggestion in Lambrix and He, 2008).
A proposition is a pair of terms (also called class in OWL) from the two ontologies for which a score has been computed.
This may have been done based on homonymy (common words) of the term names (also called class label in OWL), or propagation through the ontology.
It is important to note (i) that not all possible propositions (i.e.pairs of terms) are created during the alignment, and (ii) that the list of propositions evolves during iterations of the algorithm.
For performance, our algorithm is not symmetric.
Propositions are managed relative to one ontology, to align, which is being aligned to the reference ontology (the one loaded first by the user).
This allows us to store explicitly the information that term A of the ontology to align has two propositions, with term X and with term Y, of the reference ontology.
If X has propositions not only with A but also with B of the ontology to align, this will not be taken into account explicitly.
3.3 Algorithm (1) Computing word specific scores: score modifiers are computed for all words of the ontologies being aligned.
Each word present at least once in both ontologies being aligned (O1 and O2) is given a score modifier based on its number of occurrences f (word, O): Mod(word,Oi)=1/(1+log10( f (word,Oi))) (1) Mod(word)=Mod(word,O1)Mod(word,O2) (2) (2) Starting list of propositions (P in Fig.1): to initialize the algorithm we define first obvious similarities between the terms of the ontologies to align.
Based on the assumption that two structures that have the same name are likely homologous, the initial propositions are formed of terms with identical names.
For example, optic cup of ZFA (zebrafish, Table 1) and optic cup of EHDAA (human, Table 1) will form a proposition.
But ventricleand cardiac ventricle will not.
In this process, we also consider the synonym field of the terms.
For example the ZFA term melanocyte (synonym melanophore) will form a proposition with the term melanophore (synonym melanocyte) from XAO (Xenopus, Table 1).
Each pair of names n1, n2, is given a base score, dependent on the words shared: Base_score(n1,n2)= base_homonymy_scoremax(Mod(word))|n1 n2| max(|n1|,|n2|) (3) where |n| is the number of words in n, |n1 n2| is the number of words shared by n1 and n2, and max(Mod(word)) is computed over all shared words.
In the starting list, |n1 n2|=|n1|=|n2| by definition, but this is not the case at further iterations of the algorithm.
The comparison of terms names is intentionally quite basic, and does not take advantage of, e.g.etymology of words.
In our experience, terms names used in anatomical ontologies are similar 1767 [10:48 16/6/2010 Bioinformatics-btq283.tex] Page: 1768 17661771 G.Parmentier et al.Table 1.
Summary of the alignments discussed Zebrafish Xenopus Human Mouse Ontologya ZFA XAO EHDAA EMAPA Number of terms 1974 569 2327 3525 with synonyms 1080 122 0 0 with definitions 772 186 0 0 Number of validationsb 189 1959 Number of invalidations 543 1003 Number of unique terms aligned 183 182 1541 1754 a
ABSTRACT Motivation: Nonlinear small datasets, which are characterized by low numbers of samples and very high numbers of measures, occur frequently in computational biology, and pose problems in their investigation.
Unsupervised hybrid-two-phase (H2P) procedures specifically dimension reduction (DR), coupled with clustering provide valuable assistance, not only for unsupervised data classification, but also for visualization of the patterns hidden in high-dimensional feature space.
Methods: Minimum Curvilinearity (MC) is a principle thatfor small datasetssuggests the approximation of curvilinear sample distances in the feature space by pair-wise distances over their minimum spanning tree (MST), and thus avoids the introduction of any tuning parameter.
MC is used to design two novel forms of nonlinear machine learning (NML): Minimum Curvilinear embedding (MCE) for DR, and Minimum Curvilinear affinity propagation (MCAP) for clustering.
Results: Compared with several other unsupervised and supervised algorithms, MCE and MCAP, whether individually or combined in H2P, overcome the limits of classical approaches.
High performance was attained in the visualization and classification of: (i) pain patients (proteomic measurements) in peripheral neuropathy; (ii) human organ tissues (genomic transcription factor measurements) on the basis of their embryological origin.
Conclusion: MC provides a valuable framework to estimate nonlinear distances in small datasets.
Its extension to large datasets is prefigured for novel NMLs.
Classification of neuropathic pain by proteomic profiles offers new insights for future molecular and systems biology characterization of pain.
Improvements in tissue embryological classification refine results obtained in an earlier study, and suggest a possible reinterpretation of skin attribution as mesodermal.
Availability: https://sites.google.com/site/carlovittoriocannistraci/home Contact: kalokagathos.agon@gmail.com; massimo.alessio@hsr.it Supplementary information: Supplementary data are available at Bioinformatics online.
To whom correspondence should be addressed.
1 INTRODUCTION 1.1 The machine learning perspective Visualization and discrimination as well as supervised and unsupervised classifications are widely employed in computational biology for the investigation and analysis of patterns hidden in wetlab data.
In the literature, supervised classification is frequently simplified into classification, and unsupervised classification into clustering and this may give rise to misunderstanding.
To avoid terminological ambiguity, classification is adopted throughout this article to describe the general task of sample group attribution, while the issue of whether such attribution is supervised or unsupervised will be specified as and when necessary.
Supervised methods for feature selection and classification present several pitfalls (Smialowski et al., 2009), and small datasets make analysis problematic (Martella, 2006).
Complications particularly intensify when samples are nonlinearly related in the high-dimensional feature space obtained from high-throughput genomic and proteomic measures.
When the aim is to classify a low number of samples characterized by a very large number of genes, problems with parameter estimation may arise, and dimensional reduction followed by clustering (Martella, 2006) is a valuable response to this scenario.
Principal component analysis (PCA) has often been employed (Martella, 2006) in combination with a clustering algorithm that groups homogeneous classes on the basis of principal components, but this approach is insufficiently powerful to deal with nonlinear datasets.
In this article, we describe the use of nonlinear hybrid-two-phase (H2P) unsupervised machine learning (ML) methodologiesspecifically dimension reduction (DR) in conjunction with clusteringfor the concurrent visualization and classification of biological samples.
Our aim is to address the issue of nonlinearity and to improve the classification accuracy of recently proposed small nonlinear datasets.
The methodological innovation we introduce is a principle called Minimum Curvilinearity (MC), which is used as framework for two novel forms of nonlinear ML (NML): Minimum Curvilinear embedding (MCE) for DR and Minimum Curvilinear affinity propagation (MCAP) for clustering.
For small datasets, the MC principle suggests the estimation of curvilinear (geodesic) distances between sample data points as pairwise distances over their minimum spanning tree (MST) constructed in feature space.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i532 i531i539 C.V.Cannistraci et al.To test efficacy of the proposed algorithms, we considered locally linear embedding (LLE) (Roweis and Saul, 2000), the proposed MCE and four other unsupervised MLs for nonlinear DR and we compared their ability to solve dataset nonlinearity.
Furthermore, we compared support vector machine (SVM), classical affinity propagation (AP) and the proposed MCAP for their ability to classify samples projected in reduced feature space.
1.2 Computational biology motivations H2P ML procedures are extensively employed for image processing (Lattner et al., 2004) and for other applications, including bioinformatics (Baldi and Brunak, 1998).
A recent study by Cannistraci et al.(2009; Ravasi et al., 2010), which analyzed genomic transcription factor (TF) measurements, uncovered the presence of specific human tissue patterns.
Based on nonlinear DR coupled to clustering in bi-dimensional reduced space, the method offered efficient data visualization and discrimination and, more interestingly, achieved high accuracy in the unsupervised classification of 32 human tissues, on the basis of their embryonic origin.
Improvements obtained in the analysis of this dataset are shown in the last part of the article, in which several unsupervised H2P ML methods are compared.
However, the main aim of this article is to uncover insights and perspectives that in turn generate solutions for real classification problems in medicine.
Accordingly, the first topic selected consists in the development of methods for the classification of subjects with neuropathic pain, which is a major issue in translational and clinical medicine (Baron, 2006; Finnerup and Jensen, 2006).
Specifically, we deal with peripheral neuropathy that occurs either with or without pain.
Interestingly, some of the patients without pain (NP) can, as the disease progresses, develop a pathological variant with pain (P).
Since current knowledge of molecular disease mechanisms is poor, no single pain measure has sufficient reliability and validity.
New integrative strategies for early diagnosis could greatly enhance the timeliness of therapy planning, and interest in discovering reliable classification and prediction methods for pain patients is accordingly considerable (Baron, 2006; Finnerup and Jensen, 2006; Meyer-Rosberg et al., 2001).
Cerebrospinal fluid (CSF) is a valuable source of information for biologists and physicians.
A recent computational study analyzed a dataset of 2D electrophoresis (2DE) gel images derived from proteomic CSF profiles of peripheral neuropathic patients (Pattini et al., 2008).
Control (C) and Pain (P) groups were partially separated (leave-one-out cross-validation accuracy 68.75%) by a nonlinear surface in the space of the first three principal components extracted by PCA.
The discriminative characterization found for patients with pain, along with a further reasons, led us to reconsider this dataset (n=23) (Pattini et al., 2008).
The first additional reason was our interest in assessing the efficiency of differing NML as solutions for the nonlinearity revealed in the profile of pain subjects.
Particularly, we tested whether it is possible to solve this nonlinearity by projecting the data in a reduced, 2D space.
The result was a clear visualization of proximity and separation between controls and pain subjects, and a minimization of the problem faced by the classifier in finding a line of separation in two dimensions.
Our second incentive was that we had the opportunity to follow disease progression; neuropathic patients were still under clinical observation, and four NP patients had developed the clinical features of neuropathic pain (P group).
The third reason was the enlargement of the dataset sample to its current total of 42 individuals.
2 DATA AND ALGORITHMS 2.1 Dataset descriptions The proteomic dataset was obtained from 2DE images generated from CSF samples.
2D gel generation was described in the original proteomics study (Conti et al., 2005).
Each 2DE image was denoised by median modified wiener filter (MMWF) (Cannistraci et al., 2009) and spot detected by means of Progenesis PG240 v2006 software (Nonlinear dynamics, Newcastle, UK).
Spot calibration in accordance with protein chemo-physical coordinates (isoelectric point, pI; relative molecular mass, Mr)enabled the correction of spot location differences between differing gels.
Spot volume was estimated by means of its optical density (sum of the spot pixels) normalized as a percentage of total spot optical density in the gel image (Pattini et al., 2008).
From each image a vector of 2050 proteomic features was obtained by means of a strategy previously developed, described in depth and validated by Pattini et al.(2008).
This dataset (dataset 1) was reduced from the original 24 to 23 samples and divided into three groups: C =8, NP = 8, P=7.
As suggested by Pattini et al.(2008), we excluded the strongly noised 2DE gel image corresponding to sample P7, which had been used in the previous study exclusively as an internal check.
The validation phase introduced a new proteomic dataset (M =19) which, together with dataset 1, formed dataset 2.
The new M samples derived from a neurological study of amyotrophic lateral sclerosis (ALS) patients not affected by neuropathic pain (Conti et al., 2008).
The total number of subjects analyzed in dataset 2 of the current study is 42.
The demographic and clinical features of dataset 2 subjects/patients are shown in Supplementary Table S1.
The dataset is provided on the web site indicated in Section 5.3.
The dataset of human tissues (dataset 3) was provided as supplementary material in the original paper (Ravasi et al., 2010) and consists of 32 human tissues and two monocyte cell lines.
We exclusively considered human tissues, because cell lines had originally been introduced as an internal check.Atotal 1321 genomic TF measurements were considered.
2.2 Layout of the neuropathic pain study A flux graph is provided in Supplementary Figure S1.
It clarifies the steps of the H2P procedure, which was used for feature reduction and supervised classification of the proteomic samples, as well as for comparison with the unsupervised H2P variants explained at the end of this paragraph.
The layout consists of two stages.
The nonlinear mapping of the data in 2D reduced space requires the tuning of a free parameter k that occurs in some MLs for nonlinear dimensionality reduction.
This parameter can vary between 1 and n1 neighbors, where n is the dataset sample size, and is generally used to infer local and/or global manifold topology.
Here, the idea is also to tune this parameter in order to offer DR projections that are more informative for pain discrimination.
The best tunings for LLE (Roweis and Saul, 2000), Gaussian kernel-PCA (KPCA) (Shawe-Taylor and Cristianini, 2004), Local Tangent Space Analysis (LTSA) (Zhang and Zha, 2004) and Isomap (Tenenbaum et al., 2000) were learned in order to optimize assignment of subjects to the C and P samples.
This assignment, together with the comparison of ML performance i532 [10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i533 i531i539 Nonlinear dimension reduction and clustering by Minimun Curvilinearity in solving dataset nonlinearity, was accomplished in Stage 1.
In the comparison, a further NML was considered, namely Sammon multidimensional scaling (S-MDS), also known as Sammon Mapping (Sammon, 1969).
A nonlinear MDS that preserves small distances between data points in the reduced space better than classical MDS, S-MDS is a parameter-free NML that accordingly does not require tuning.
In addition, the proposed parameter-free NML called MCE was considered, and its algorithm is presented in Section 2.5.
MCE and LLE offered the best dimensionality reduction for linear discrimination of controls C and subjects with pain P, and they were therefore selected for comparison in the reduction of feature space in the second classification step.
In particular, on the basis that LLE was tuned to preserve similarities in relation to the presence or absence of pain, it was also tested in combination with SVM (supervised H2P approach) for the classification of pain neuropathic patients.
In Stage 2design for validation, we propose a procedure in which the SVM classifier is applied in the 2D feature space obtained by LLE; the free LLE parameter is fixed to the best value learned in Stage 1.
The SVM classifier also requires a training phase to learn the decision rule used for sample supervised classification in the reduced, 2D feature space.
The training of the manifold-NML helps to learn the similarities related to the presence or absence of pain between samples in the high-dimensional feature space, and to map the samples enhancing these similarities in a reduced feature space.
In contrast, SVM training helps to learn a rule for separation and discrimination of the samples by exploiting the advantage that the similarities between close samples are enhanced in the new reduced feature space.
To ensure robustness, the training both in the tuning procedure and in the classification procedure applied leave-oneout cross validation (LOOCV).
For the LOOCV procedure, five of the total eight C and four of the total seven P subjects were randomly selected and used as training exemplars.
The dataset was ordered in the following way: C1C5 and P1P4 were used as labels of the training data.
The same data were used both for training of the NMLs in Stage 1 and for training of the SVM in Stage 2.
The remaining samples were randomly labeled C6C8 and P5P7, and used only for validation.
The validation stage was divided into two tasks: (i) disease course in NP patients (n=8) was predicted as pain or no pain state; in addition, the subjects not used for training (C6, C7, C8 and P5, P6, P7) were also classified by SVM.
(ii) the dataset was extended and the new control patients M (n=19) were classified as belonging to the pain or no pain state.
As already mentioned, training a supervised classifier with a small number of samples (five controls versus four pain subjects) in order to infer a model is risky, and it could be further argued that the use of SVM for classification in the reduced linearized feature-extracted space is excessive for this purpose.
To address these points, we designed a second H2P approach, completely unsupervised, that substitutes the supervised classifier with an algorithm for unsupervised classification (clustering).
Statistical evaluation (accuracy, sensitivity, specificity, precision) of the SVM was performed only on the testing samples and excluding the samples used for training.
The unsupervised classification (which does not require training samples) was in turn performed on the entire set of samples in the datasets.
2.3 Minimum Curvilinearity MC principle has its starting point in the consideration that for datasets of reduced size the idea of estimation or inference of manifold topology in the feature space might be misleading due to the small number of samples.
We speculate that in this case it might be more congruous to simply speak of estimation of nonlinear sample distances.
MC is proposed as a way to estimate nonlinear sample distances by MST without any need for tuning parameters.
A different, interesting principle, summarized in the phrase: think globally and fit locally, was introduced with LLE.
Exploiting local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds (Roweis and Saul, 2000).
This procedure, however, costs the introduction of one free parameter for neighbourhood estimation, which can be a point of weakness in unsupervised tasks.
A recent study by Bogu et al.(2009) on the navigability of complex networks found that a general property is present in the hidden metric spaces of several artificial and biological networks.
This property is dictated by the shape of the hidden metric space, which forces the system to form local interactions between subsets of its elements mapped in the observable network topology as different sub-networks of interacting nodes.
On the other hand, the hidden space also guides the greedy-routing process that connects nodes located in differing sub-networks.
If this theory is adopted in the framework of our studyapplied to the sample representation in the hidden feature spaceit offers a valid theoretical support for approaches such as think globally and fit locally and or MC.
Indeed, in small datasets, MST provides a reasonably accurate map both of the local connection geometry of near and sub-network-related samples (nodes) and of the global connection geometry between samples (nodes) located in separated regions of the multi-dimensional space.
MC suggests the estimation of curvilinear distances between sample data points in small datasets as pair-wise distances over their MST constructed in the feature space.
The collection of all these nonlinear pair-wise distances forms a distance matrixthe MC-distance matrixto be used as an input in algorithms for DR or clustering.
2.4 Minimum Curvilinear affinity propagation Although classical AP is a powerful algorithm for clustering that works very well for regularly shaped clusters, with elongated or irregular multi-dimensional data it may force division of single clusters into separate ones or it may provide low-clustering results (Leone et al., 2007).
The innovation we propose to solve these issues in elongated datasets is a clustering algorithm that runs AP (Frey and Dueck, 2007) over the MST of the samples (here represented in 2D-reduced space).
We named this algorithm MCAP clustering.
More generally, MCAP is able to define clusters that pass messages between the samples that are nodes on the MST obtained in the multidimensional feature space.
Taken from another perspective, we can say that this algorithm is MST-guided: we estimate the curvilinear distances between the samples distributed in feature space by MST and then, in accordance with a message passing procedure we send messages between sample points following the preferential highway tracked by MST.
Details of the algorithm are reported in Section 5.3.
We compare MCAP results with those offered by the classical AP approach (Frey and Dueck, 2007), where the similarities (negative distances) between the samples are computed as negative squared i533 [10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i534 i531i539 C.V.Cannistraci et al.error distances (Frey and Dueck, 2007).
For MCAP, the similarities are the negative values extracted from the MC-distance matrix.
The preference parameter (Frey and Dueck, 2007) for both MCAP and AP is tuned to the value that offers two clusters as an algorithm result.
2.5 Minimum Curvilinear embedding In terms of comparative ML theory, dimensional reduction and clustering are the two cornerstones of unsupervised learning (Ghahramani, 2004).
We can therefore imagine an algorithm for DR that is a distance-matrix analog of the MCAP clustering algorithm.
This nonlinear dimensionality reduction algorithm was named MCE and uses the MC-distance matrix as an input for the classical MDS or the nonlinear S-MDS.
MCE algorithm details are provided in Section 5.4.
MCE can be interpreted as the minimum curvilinearextension of MDS.
The fact that MCE is a nonlinear (and curvilinear) extension of MDS represents a point of similarity with Isomap, which in turn is the manifold geodesic extension of MDS.
Isomap computes sample geodesic distances over the manifold as shortest paths on the neighborhood graph, as constructed on the bases of the first k Euclidian-distance neighbors, where k is the free parameter to tune.
The principal weakness of Isomap is the algorithm instability encountered in embedding of manifolds with local nonlinearity or discontinuity (Balasubramanian and Schwartz 2002).
With the low number of samples available for inferring manifold topology as our starting point, we argue that the strategy of global manifold reconstruction used by Isomap might be not congruous for small and irregular datasets.
3 RESULTS AND DISCUSSION 3.1 Data nonlinearity is successfully addressed Figure 1A shows algorithms performance in linear discrimination between controls and pain subjects.
Performance optimality was estimated by the maximization of a proposed index for tuning evaluation (TE).
This index was evaluated: (i) for increasing values of k, which is the free parameter present in the manifold NML; (ii) for increasing values of standard deviation, which is the free parameter present in the kernel of the Gaussian KPCA.
MCE and S-MDS were evaluated by the same index, but they do not present free parameter to tune.
The TE index is computed by means of LOOCV in order to prevent overfitting.
LOOCV is also used to estimate classification accuracy, as reported in Figure 1B.
TE is evaluated as an average measure of linear separation obtained by the removal of one sample per LOOCV round and by the subsequent SVM estimation of the margin of linear separation between the remaining samples.
Accuracy is estimated as an average of classification successes calculated by including the sample omitted in the LOOCV round, and by evaluating its label (control or pain) by means of the SVM separation line.
Details about TE index and accuracy evaluation are provided in Section 5.1.
Figure 1C summarizes the best performance for each tested algorithm.
Data nonlinearity between C and P is successfully addressed during tuning (Stage 1), where four out of six NMLs (LLE, MCE, Isomap, S-MDS) attained linear separation with an accuracy of 1 (Fig.1B, result not represented for S-MDS), and two out of six (KPCA and LTSA) attained linear separation with an accuracy Fig.1.
Tuning and comparison to address the data nonlinearity.
(A) TE for LLE (yellow line), LTSA (blue line) and Isomap (green line).
The x-axis reports different values of neighborhood parameter k; y-axis reports the index for TE.
(B) Classification accuracy for LLE (yellow line), LTSA (blue line) and Isomap (green line).
The x-axis reports different values of neighborhood parameter k; y-axis reports values of accuracy.
(C) Best performance in linear discrimination compared between tested algorithms.
of 0.89 (eight successes in nine LOOCV rounds; the result for KPCA is not displayed in the figure).
This demonstrates that linearization is obtained as a result of generalized NML capacity, and that it is not related to an ability of a single algorithm.
In particular, LLE and MCE achieved the highest TE value and they scored the best performance in linear discrimination (Fig.1A and C).
Surprisingly, MCE attained this result without tuning of any parameter and with a score of 1 on accuracy, while LLE was the only manifold NML that enabled high-linear separation for low numbers of neighborsin a range between 1 and 4where Isomap and LTSA failed (Fig.1A and B).
Isomap for small k values was not able to recover the manifold structure because the reconstruction of the neighborhood graph was not complete, and this failure did not permit the embedding of the overall number of samples.
This weakness of Isomap is due to its topological instability (Balasubramanian and Schwartz 2002).
Isomap may construct erroneous connections in the neighborhood graph, and such shortcircuits impair its performance (Balasubramanian and Schwartz 2002).
The fact that LTSA showed very low performance in the same range where Isomap showed topological instability is a further confirmation of local nonlinearity present in the dataset structure.
The locality of the problem is demonstrated by the fact that both algorithms showed inefficiency using a small number of neighbors for manifold reconstruction, and this confirms the result obtained in the previous computational study (Pattini et al., 2008).
The reasons why Isomap and LTSA show the same behavior, in contrast to LLE, which yields the best performances in this range, are to be found in the differing hypotheses underlying ML applicability.
Both Isomap and LTSA are sensitive to the assumption of local manifold linearity (Zhang and Zha, 2004)which seems to be not satisfied by the dataset considered in our studywhereas LLE provides a local reconstruction that is less sensitive to this assumption.
LLE preserves the local properties of the manifold by means of a reconstruction weights operation, which locally linearizesby solving a constrained least-squares problemthe manifold in the neighborhood of each sample (Roweis and Saul, 2000).
For datasets with high-intrinsic dimensionality and low number of samples, i534 [10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i535 i531i539 Nonlinear dimension reduction and clustering by Minimun Curvilinearity Fig.2.
Evaluation on dataset 1.
(A) Result of AP clustering in the space of first two dimensions extracted by LLE.
(B) Result of MCAP clustering in the space of first two dimensions extracted by LLE.
(C) Results of AP and MCAP clustering in the space of first two dimensions extracted by MCE.
Blue line for pain cluster, red line for control cluster.
White skeleton in panel (B) indicates the partition generated by MCAP over the MST; link deleted by clustering was between samples P6 and NP3.
SVM decision rule for classification (yellow line in panels A and B) is obtained considering the training samples (C1C5; P1P4).
(D) Evaluation of SVM, MCAP and AP for pain classification in LLE reduced space.
(E) Evaluation of MCAP and AP for pain classification in MCE reduced space.
the underlying estimation of the manifold might be difficult and highly variable.
Moreover, the local linearity assumption around certain data points may be violated, at least anisotropically (i.e.only in some manifold directions).
Thus, techniques such as Isomap and LTSA may be less successful.
In contrast, MCEdesigned to estimate nonlinear distances and to deal with local irregularity in small datasetsaddresses nonlinearity by considering even the total groups of control (C, red spots) and pain (P, blue spots) patients present in dataset 1 (Fig.2C).
LLE, as expected, attained comparable performance in this task as well (Fig.2A and B).
3.2 Prediction and classification of pain subjects Figure 2 displays the results of different H2P procedures on dataset 1 obtained by combining: (i) LLE with SVM, MCAP or AP; (ii) MCE with MCAP or AP.
Although LLE offered a clear linear separation over the first reduced dimension between pain and no-pain subjects, the elongated shape in the bi-dimensional space of the no-pain group caused the failure of AP to identify the right cluster attributions (Fig.2A).
This evidence was already reported in the literature (Leone et al., 2007).
In contrast, MCAP succeeded in this task (Fig.2B) because the message passing procedure was guided by the MST skeleton (white skeleton, Fig.2B).
MCE too provided linear separation over the first reduced dimension (Fig.2C), but its embedded groups were more regular than those of LLE (Fig.2A and B), this is why both AP and MCAP provided the same clustering in the MCE reduced space (Fig.2C).
The statistical evaluation displayed in Figure 2D and E suggests that MCEMCAP, which provided the same result as LLEMCAP but without any tuning, enjoys high efficiency: a completely unbiased achievement.
This superiority was particularly evident in the second evaluation, performed on dataset 2 (Fig.3), in which the introduction of the novel sample set M caused LLE to shift the linear separation between pain and no-pain state from the first to the second dimension, while the first dimension became discriminative for the various pathological states (Fig.3A and B).
Surprisingly, MCE was still able to discriminate the mixture of five different states over the first dimension (Fig.3C, data and code to reproduce the figure are provided at the link indicated in Section 5.3): on the left patients affected by ALS neuropathy (M); in the centre controls (C), while at the bottom-centered peripheral neuropathic patients without pain (NP); on the right, patients with peripheral neuropathy and pain (P, and NP with pain).
The fact that MCE only needs the first dimension to offer a gradual and shaded landscape of this intricate scenario is impressive, especially if we consider the simplicity of the principle behind this NML, and the absence of parameters to tune.
The result of the statistical evaluation displayed in Figure 3D and E suggests that MCEMCAP provides superior unsupervised discrimination of pain and no-pain subjects, which in turn shows that pain is the prevalent discrimination factor over the first MCE dimension.
On the other hand, the performance of the supervised H2P procedure consisting in LLESVM (Fig.3A, B and D) proved to be robust despite the introduction of new samples.
However appraisal of this last finding should be tempered by the fact that there were very few test samples.
From the biological point of view, our findings strongly support the efforts to discover reliable methods for the classification of subjects with pain, and encourages speculation about possible ways to distinguish the patients states in relation to the proteomic pain pattern hidden in their CSF.
In order to advance any serious biological claim, a further study with a larger dataset and a congruous investigation of the relation between the significant features is mandatory, yet this result is important because of i535 [10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i536 i531i539 C.V.Cannistraci et al.Fig.3.
Evaluation on dataset 2.
(A) Result of AP clustering in the space of first two dimensions extracted by LLE.
(B) Result of MCAP clustering in the space of first two dimensions extracted by LLE.
(C) Results of MCAP clustering in the space of first two dimensions extracted by MCE.
Blue line for pain cluster, red line for control cluster.
White skeleton in panel (B) indicates the partition generated by MCAP over the MST; link deleted by clustering was between samples C3 and NP3.
SVM decision rule for classification (grey line in panel A and B) is obtained considering the training samples (C1C5; P1P4).
(D) Evaluation of SVM, MCAP and AP for pain classification in LLE reduced space.
(E) Evaluation of MCAP and AP for pain classification in MCE reduced space.
the high-throughput proteomic screening approach employed to characterize every sample.
An interesting final note from the clinical standpoint is that the unsupervised analysis provided accurate identification (only one misclassification NP3, out of a total eight NP samples) of future pain for NP patients (Figs 2A C and 3AC).
This result is summarized in Supplementary Table S2 (see the computationally predicted state column), together with the clinical follow-up at 612 months and at >1 year (see follow up columns).
3.3 The tissue embryological classification is improved On dataset 3, MCE and LLE (Fig.4A and B) demonstrated the best dimensionality reduction (same clustering accuracy, Fig.4D) by solving the nonlinearity better than Gaussian KPCA (Fig.4D), while PCA performance was much lower (Fig.4C and D).
Isomap suffered from instability and its DR was not effective for evaluation.
The ability of MCE to provide a discriminative landscape where the sample classes are gradually unfolded along the first dimension is maintained in this dataset too (Fig.4A).
In contrast, and as in the previous evaluation, LLE, needs to combine the first and second dimensions for a complete discrimination of the classes (Fig.4B).
As previously mentioned, this result in DR by MCE is very important because it is completely unbiased (absence of free parameter to tune in the algorithm).
LLE allowed best clustering considering k =5 both for MCAP and AP, and this value was tuned on the basis of knowledge of the sample labels.
Surprisingly, the results for MCE and LLE are not only similar in accuracy, but also in cluster shape and in the co-localization of differing samples, especially in the endodermal cluster (red color, Fig.4A and B).
We do not have any biological explanation for the misclassification of lymphnode (22, Fig.4A and B), but it was suggested (Dorshkind, 2002) that bone marrow (21, Fig.4A and B) might also contain distinct endodermal progenitors capable of contributing to components of i536 [10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i537 i531i539 Nonlinear dimension reduction and clustering by Minimun Curvilinearity Fig.4.
Evaluation on dataset 3.
(A) Result of MCAP clustering in the space of first two dimensions extracted by MCE.
(B) Result of MCAP clustering in the space of first two dimensions extracted by LLE.
(C) Results of MCAP clustering in the space of first two dimensions extracted by PCA.
Green line for ectodermal cluster attribution, yellow line for mesodermal cluster attribution, red line for endodermal cluster attribution.
(D) Evaluation of MCAP and AP for unsupervised classification in reduced space obtained by different methods.
the gastrointestinal system such as liver (27 and 28, Fig.4A and B).
Conversely, for MCE the mesodermal attribution of thyroid and salivary-gland samples (23 and 24, Fig.4A) seems to be an error due to limitations of unsupervised classification rather than to a misleadingly low-dimensional localization.
MCAP invariably offered better accuracy (Fig.4D) than did AP, and thus confirmed the results previously obtained on datasets 1 and 2.
The most impressive result is that the MCEMCAP H2P procedure achieved 84% accuracy on the genomic TF expressions in a completely unsupervised manner.
This is an improvement that supports the result (82% accuracy) reported in the previous article (Ravasi et al., 2010) by the small (only six interactions) TF-homeobox subnetwork, and confirms both the procedures power in embryological discrimination and its potential importance in tissue differentiation processes.
Interestingly, skin (labeled as ectodermal) was always classified in the mesodermal cluster in each of the different ML analyses (label 8, Fig.4AC).
This is in accordance with the classification attained in the original article (Ravasi et al., 2010), but there interpreted as misclassification.
In the light of the latest results, a possible mesodermal re-attribution of the skin label could be considered.
The biological explanation resides in the multi-layer structure of the skin: although the first layer of the skin (epidermis) is ectodermal, the extracted skin sample might also contain the second layer (dermis), which is of mesodermal origin.
4 CONCLUSION AND FUTURE PERSPECTIVES MCE and LLE were very effective for DR because they allowed similarly high-clustering accuracy, and they occasionally uncovered analogous geometry in sample localization (Fig.4A and B).
We speculate that these similarities between MCE and LLE results are evidence of closenessas far as small datasets are concerned between the principles of MC and think globally and fit locally that respectively underlay MCE and LLE.
Moreover, the fact that MCE only required the first dimension to completely unfold as many as five different classes (Fig.3C) is striking, especially if we consider the simplicity of the principle this NML is based upon, and the absence of a parameter to tune.
In our evaluations, LLE needed the first and second dimensions to yield the same discriminative results, and this comparable performance was obtained at the cost of a free parameter to tune.
If no label hypothesis is provided (as was the case for tissue embryological attribution, the uncovering of which was unsupervised), it is hard to imagine an unsupervised strategy that indicates the right tuning for unfolding the classes hidden in a nonlinear dataset.
However, we showed that this defect can be transformed into a merit through combination with supervised classifiers like SVM, where the tuning parametersuch as a kernel parametercan be used to enhance sample discrimination in relation to the aim of the supervised task.
We expect PCA to exceed MCE on linear data, and for practical applications we accordingly suggest initial use of PCA in combination with differing normalizations; if the dataset shows subsequent resistance and nonlinearity (as in Fig.4C), we recommend structural exploration by means of MCE (as in Fig.4A) and other NML techniques.
Another solution could be the direct employment of the MCEMCAP H2P approach, which in our results proved to be very powerful for visualization and unsupervised classification.
In particular, MCAP overcame AP in the clustering of elongated data in the bi-dimensional reduced space, but we expect that, for regularly shaped clusters, AP might perform similarly or better.
i537 [10:53 28/8/2010 Bioinformatics-btq376.tex] Page: i538 i531i539 C.V.Cannistraci et al.Although the MC principle provided a valuable framework for the estimation of curvilinear distances in small, nonlinear, multidimensional datasets, its extension to large datasets needs careful consideration and adaptation.
In such extension, the MST-measure may not be sufficient to estimate the distances over the manifold with adequate approximation.
Large numbers of samples can cause the overestimation of sample distances over the MST, and these large distances could prevail in magnitude over the shorter ones in the low-dimensional representation.
An idea for future developments is the extension of the MC approach to other ML algorithms.
In this perspective, an option for future investigation might consider the minimum curvilinear LLE (MCLLE), in which neighbors are estimated by distances over the MST and not, as in classical LLE, by Euclidean distances (EDs).
On the other hand, there might be additional benefit in the use of re-sampling techniques to compute more refined estimations of manifold topology, where one possible solution would be to estimate pair-wise distances by bootstrapping samples and/or features.
To the best of our knowledge, the current study is the first to derive unsupervised classification of pain onset from CSF proteomic profiles, and this result could offer new insights for the future characterization of pain in molecular and systems biology.
A final observation regards tissue embryological classification.
Prompted by the improved accuracy here reported, we suggest that the skin label might be reinterpreted as a mesodermal attribution.
We also conjecture that data nonlinearity could be completely addressed by methods for DR that more finely exploit the intrinsic patterns hidden in biological TF-network topology.
5 METHODS 5.1 Tuning stage: for addressing data nonlinearity Index TE is estimated by means of LOOCV in order to prevent overfitting.
Having fixed the value of the free parameter k, the considered DR algorithm provides a two-DR for each leave-one-out step, excluding one sample from the training dataset in each round of LOOCV (nine samples in the dataset provide nine DRs during the leave-one-out procedure), and then estimating a proposed cluster validity measure (CVM) (Stein et al., 2003) in the 2D reduced space.
The CVM is here used as a measure of separation between the two classes C and P present in the training dataset.
Higher CVM values mean better separation between the two considered groups in the 2D-reduced space; in the absence of linear separation CVM provides value zero.
Thus, for every value of k an ensemble of CVMs is computed during LOOCV.
This ensemble is adopted to calculate the index TE in correspondence to any value of the free parameter k according to the following formula: TE ( k )= mean ( CVMs ) 1+SD(CVMs).
The mean is divided by the SD (+1) of the CVMs.
This estimation is used to measure the training optimality of the considered algorithm in correspondence to each value assumed by the free parameter k. For SD equal to zero, TE takes a value corresponding to the mean CVM.
For SD >0, TE is penalized with respect to the mean CVM.
In the absence of linear separation, TE has zero value if the CVM has zero value for each of the LOOCV steps.
The rationale is to select the parameter value that offers high-cluster separation (high-CVM mean value) and at the same time ensures high reliability and robustness (low-CVM SD) during the cross-validation procedure.
Details on CVM and accuracy computing, as well as details of the toolbox used for implementation of LLE, LTSA and Isomap algorithms, are provided in Supplementary Data (paragraph 1).
5.2 Validation Stage 2: prediction and classification of pain subjects During the validation Stage 2, DRin a 2D spaceof the entire dataset was obtained by exploiting the best parameter setting k =3, which was learned for the LLE algorithm during the tuning stage.
An ensemble of decision boundary (DB) was subsequently obtained, by means of SVM and the procedure based on the LOOCV (described above), using the same C (n=5, C1, , C5) and P (n=4, P1, , P4) samples as those previously employed in the training of the tuning stage.
The DB offering median distance between the support vectors was designated as the decision rule.
5.3 MCAP The first step is to calculate a distance matrix (MC-matrix) as pair-wise sample distances over the MST, as computed by the Kruskal method in the feature space (in our case, 2D-reduced space).
For MST computation, we suggest the use of a heuristic metric that we found fitted efficiently in combination with the message passing procedure run by AP over the MST.
The suggested heuristic is the square root of the EDs between the samples.
This device attenuates the estimation of large distances and amplifies the estimation of short distances; consequently the device helps to regularize the distances over the MST for the message passing procedure.
In the second step, AP is run assuming sample similarities equal to the negative values of the elements in the MC-matrixcomputed as previously describedand tuning the preference parameter (Frey and Dueck, 2007) to the value that offers two clusters as algorithm result.
Matlab code at: https://sites.google.com/site/carlovittoriocannistraci/home http://www.mathworks.com/matlabcentral/ (tag: MC).
5.4 MCE The first step is to calculate a distance matrix (MC-matrix) as pair-wise sample distances over the MST as computed by the Kruskal method in the feature space.
To compute the MST in the feature space, we tested the ED and the correlation distance (CD) obtained as: corr(x,y)=1corrperson(x,y) In general, the two different distances provided comparable results.
We used the CD in our computation, except for the analysis of dataset 2, in which the ED was preferred.
In the second step we performed the embedding transformation by performing the classical MDS of the MC-matrix.
We also tested Sammon nonlinear MDS (S-MDS), but the result on our data, although comparable, was less impressive.
Matlab code is provided on the web sites indicated in Section 5.3.
ACKNOWLEDGEMENTS We thank Ewa Aurelia Miendlarzewska for her generous assistance and for language revision and Sven Bergmann for precious suggestions.
Funding: Fondazione CARIPLO (NOBEL GuARD Project); MoH RF-FSR-2007-637144; Italian Interpolytechnic School of Doctorate SIPD (http://sipd.polito.it/) (to C.V.C.
); US National Institute of Mental Health and the King Abdullah University of Science and Technology (grant MH062261 to T.R.
and C.V.C.).
Conflict of Interest: none declared.
ABSTRACT Summary: Evolutionary biologists are often interested in finding correlations among biological traits across a number of species, as such correlations may lead to testable hypotheses about the underlying function.
Because some species are more closely related than others, computing and visualizing these correlations must be done in the context of the evolutionary tree that relates species.
In this note, we introduce PhyloDet (short for PhyloDetective), an evolutionary tree visualization tool that enables biologists to visualize multiple traits mapped to the tree.
Availability: http://research.microsoft.com/cue/phylodet/ Contact: bongshin@microsoft.com.
1 INTRODUCTION Biomedical research often begins with a large-scale search for correlated traits.
As the number of genetic sequences continues its exponential growth, there is an increasing interest in identifying underlying genetic causes of traits by comparing genetic sequences across a large number of species.
Felsenstein (1985) pointed out a key flaw in traditional comparative methods; namely, individual species cannot be considered statistically independent samples because of shared ancestry.
For example, it should not be surprising to see a large number of correlations between mice and rats when the other species in the study are reptiles.
Rather, it is when a correlation persists across a diverse range of species that we should take notice.
Conversely, some traits are expected to be independent of the phylogeny and any clustering with respect to the tree may imply unexpected biological mechanisms or errors in data collection.
The evolutionary tree is typically inferred from genetic data and is annotated by branch lengths that indicate the genetic similarity between two species, with the leaves of the tree representing the species and internal nodes representing (unobserved) speciation events.
Thus, the tree structure provides a natural visual representation of which species are generally similar (or different) to each other.
Although dozens of visualizations for correlated traits have been developed, the recent explosive growth in the number of traits and species has created a need for a visualization that can scale to dozens of traits mapped to thousands of species and To whom correspondence should be addressed.
Fig.1.
PhyloDet, here showing six traits mapped to 1134 HIV sequences and their evolutionary tree.
The arrows, added for illustration, highlight the clustering of attributes corresponding to IDU (blue), PI (red) and NRTI (green).
The dialogue box allows a user to drag the circular tabs to specify a range of values that should be displayed.
The current box is for the number of drugs to which an HIV sequence is resistant.
their evolutionary tree, allowing interactive exploration of complex interactions.
In this article, we present PhyloDet (Fig.1), a scalable visualization tool for mapping multiple traits to large evolutionary trees.
By allowing multiple traits to be visualized on a large tree that preserves branch lengths, biologists can visualize complex interactions and how they relate to the evolutionary history of the species.
2 EXAMPLE: HIV AND DRUG RESISTANCE To demonstrate PhyloDet, we reanalyzed the HIV anti-retroviral (ARV) drug resistance data from Harrigan et al.(2005).
In this study, the authors investigated the correlations between several risk factors and drug resistance to highly active anti-retroviral therapy (HAART), a cocktail of several ARV drugs, using a large cohort of 1191 patients who were initiating HAART for the first time.
For each patient, the authors sequenced the infecting virus at several time points throughout the course of therapy and measured several attributes, including resistance to several classes of drugs and if the 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:16 31/8/2009 Bioinformatics-btp454.tex] Page: 2612 26112612 B.Lee et al.patient was an injection drug user.
Correlations to HAART failure were measured using Cox proportional hazards; the underlying phylogeny that relates the HIV quasi-species was not considered.
Here, we selected one HIV sequence per patient (N = 1134) and constructed a phylogenetic tree relating HIV quasi-species.
We then mapped each of the traits provided by Harrigan et al.(personal communications) to visualize interactions among the traits and between the traits and the evolutionary history (Fig.1).
3 PHYLODET FUNCTIONALITIES 3.1 Color and shape coding for leaf nodes Large cohorts often consist of heterogeneous sources and it can be useful to identify how those sources are distributed across the tree.
For example, the extreme diversity of HIV-1 is often simplified by identifying individual sequences as belonging to one of several clades, which roughly map to continental regions.
In our drug resistance example, coloring the leaf nodes and their connecting branches based on whether or not the strain is clade B (the predominant clade in North America, in blue) makes it immediately clear which sub-tree corresponds to non-clade B patients (red).
The researchers may disregard results from this sub-tree, as they represent a substantially different population from the rest of the sequences.
Leaf coloring may also be useful when multiple centers combine data, as it becomes immediately apparent how similar sequences are among different cohorts.
In addition, the shape of leaf nodes can be customized to further highlight different leaves.
A simple dialogue box allows users to specify colors and shapes that map to leaf names that contain user-defined substrings.
3.2 Visualization of multiple traits To help biologists gain insight into the underlying mechanism, PhyloDet enables visualization of multiple traits (or attributes), which are mapped to the phylogenetic tree through use of a colorencoded attribute bar in which the columns correspond to the selected attributes and rows line up vertically with the leaves they represent.
Each row is semi-transparent so that the attributes of leaves that overlap in vertical space can be distinguished.
Preserving relative branch lengths means some leaves may be far from the attribute bar, making it difficult to identify which attributes correspond to a given leaf.
Thus, the first column of the attribute bar repeats the leaf link colors and users can drag the entire attribute bar to precisely line up colors with leaves.
Attributes are assumed to be numeric, with colors assigned using a heat map that ranges from blue (smallest value) through green to red (largest value).
The currently selected attributes are listed in the attributes list at the bottom of the left panel, which displays the range of observed values for each attribute.
In addition, users can hide leaves that are missing data, or can specify a range of values for each attribute that should be displayed (see callout in Fig.1).
In our example, Harrigan et al.tracked several attributes related to drug resistance or else relevant to the demographics of the cohort.
In Figure 1, we display attributes corresponding to whether the patient was an injection drug user (IDU, second attribute column) and whether the infecting HIV quasi-species had already selected for one of several resistance mutations (starting at third attribute column: M184, NRTI, NNRTI or PI).
The last attribute we display shows the total number of drug classes to which each patient is resistant (values range from 0 to 5).
By displaying these attributes simultaneously on the tree, we can visualize several relationships reported by Harrigan et al., as well as observe some new effects.
For example, we see that HIV sequences sampled from IDUs tend to cluster together on the tree (blue arrow), an observation that may indicate that infection in the IDU population is largely circulating separately from the rest of the cohort.
Interestingly, IDU status does not appear to be strongly correlated with resistance.
Other attributes that contain strong tree-based clusters include NRTI (green arrow) and PI (red arrow) resistance mutations, whereas the remaining attributes appear to be evenly dispersed throughout the tree.
It should be noted that the clustering of the NRTI and PI attributes may be caused by convergent evolution of the underlying mutations that define those attributes and not from shared ancestry.
Thus, a careful analysis would include reconstructing the tree with the resistance sites removed to determine if the clustering is supported by non-resistance-associated mutations (Matthews et al., 2009).
Finally, we note that PhyloDet is intended as a data exploration tool, and any derived hypotheses should be tested with statistical methods that can test for correlations in the context of evolutionary trees (e.g.Carlson et al., 2008).
For example, because the definition of which branch is drawn to the left or right for each node is arbitrary, apparent correlations that appear in small groups of closely related sequences may be an artifact of layout decisions.
3.3 Additional features and specifications PhyloDet allows users to customize the appearance of the tree, allowing users to pick a new root, hide arbitrary sub-trees and specify parameters that determine the display properties of the tree.
PhyloDet reads one tree file (in the Newick tree format) and multiple attribute files (in one of several tab-delimited text file formats).
PhyloDet requires.NET 3.5, which is available for all windows platforms.
Additional details are described at http://research.microsoft.com/cue/phylodet/manual.html and in Lee et al.(2008).
ACKNOWLEDGEMENTS The authors wish to thank Philippa Matthews, Thomas Kuntzen, Toshi Miura, Yaoyu Wang, Jenna Rychert, Zabrina Brumme and Chanson Brumme for their feedback.
We also would like to thank Richard Harrigan for providing his valuable data and comments for the HIV drug resistance example, and Meredith Skeels, who initiated the collaboration that made this project possible.
Conflict of Interest: none declared.
ABSTRACT Summary: Identifying mentions of named entities, such as genes or diseases, and normalizing them to database identifiers have become an important step in many text and data mining pipelines.
Despite this need, very few entity normalization systems are publicly available as source code or web services for biomedical text mining.
Here we present the GNAT Java library for text retrieval, named entity recognition, and normalization of gene and protein mentions in biomedical text.
The library can be used as a component to be integrated with other text-mining systems, as a framework to add user-specific extensions, and as an efficient stand-alone application for the identification of gene and protein names for data analysis.
On the BioCreative III test data, the current version of GNAT achieves a Tap-20 score of 0.1987.
Availability: The library and web services are implemented in Java and the sources are available from http://gnat.sourceforge.net.
Contact: jorg.hakenberg@roche.com Received on May 18, 2011; revised on July 26, 2011; accepted on July 30, 2011 1 INTRODUCTION The extremely rapid growth of published literature in the biological sciences necessitates the constant improvement of automated textmining tools to extract relevant information and convert it into structured formats.
Terms for the same entities used in biomedical articles can vary widely between authors and across time (Tamames and Valencia, 2006).
Thus, two key tasks in biomedical text analysis are named entity recognition (NER; finding names of genes, cell lines, drugs, etc.)
and entity mention normalization (EMN; mapping a recognized entity to a repository, such as Entrez Gene or PubChem).
Both tasks enable indexing, retrieval and integration of literature with other resources.
Gene and protein names in particular represent central entities that are discussed in biomedical texts.
While a growing number of tools for gene NER are freely available (e.g.Leaman and Gonzalez, 2008), only a limited number of tools provide gene normalization capabilities that can be used off-the-shelf by end users (e.g.Huang et al., 2011).
To whom correspondence should be addressed.
In this article, we present a new version of the Gnat system for gene mention recognition and normalization (Hakenberg et al., 2008) and make it available as an open-source Java library and as a remote web service.
Gnat now relies on a modular architecture, allowing integration of new components by implementing relatively simple HTTP interfaces and allows its components to be distributed on servers (local or remote; public or private).
The framework allows end users to send PubMed or PubMed Central document identifiers as well as free text to our server, returning lists of gene mentions with Entrez Gene IDs.
Text mining application developers can make use of the same service by using Gnat as a component in their own processing pipelines or by customizing Gnat toward their requirements.
Here we present the major components in Gnat, demonstrate how they interact and how they can be exchanged and extended by developers.
We present an overview of the web services provided, which can be used remotely from our server or set up by users at their local sites.
Finally, we discuss the performance of gene mention normalization provided by Gnat.
2 APPROACH Gnat consists of a set of modules to handle all steps required in a text processing pipeline, from document retrieval to named entity normalization.
A general Gnat processing pipeline (Fig.1) consists of modules that perform the following steps: (1) Retrieve documents, (2) Pre-process each text, (3) Perform named entity recognition for genes and species, (4) Remove likely false positive gene mentions, (5) Assign candidate identifiers to genes, (6) Validate identifiers, and (7) Rank candidate gene identifiers.
Steps (1) and (2) comprise essential text retrieval and pre-processing tasks.
Document retrieval uses NCBI e-utils to obtain records from PubMed and PubMed Central when such identifiers are given.
Preprocessing of texts includes, for instance, a name range expansion that replaces mentions such as freac1-3 with freac1, freac2, and freac3, to aid subsequent gene NER.
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:12 5/9/2011 Bioinformatics-btr455.tex] Page: 2770 27692771 J.Hakenberg et al.Fig.1.
Overview of the Gnat processing pipeline with typical components [(1) through (7); see text] and final output (8).
Gnat is designed in a modular manner, where data exchange is performed using the HTTP protocol.
It allows memory-and CPU-intensive components (A and B) to be run separately on appropriate hardware.
Memory-intensive components typically run as (remote or local) services, as they require longer startup times less suited for small queries.
The Gnat client (center) manages which components to invoke in which manner, and sends data to the components for annotation.
Some components rely on annotations provided by other components, such as the assignment of candidate identifiers during step (5), which requires species annotations from step (3a).
In step (3), Gnat recognizes names referring to both genes and species using a dictionary-based approach.
A set of candidate Entrez Gene identifiers is assigned to each gene mention in this step as well, comprising all potential matches based on the genes name alone.
The NER modules available in the current default version of Gnat include the species-dependent dictionary lookups present in previous versions (Hakenberg et al., 2008) for 20 common model organisms including human, mouse, rat and Escherichia coli.
In addition to the dictionary-based gene NER taggers, we now provide an interface to Banner (Leaman and Gonzalez, 2008), which uses conditional random fields to recognize candidate gene names.
Users can select either of these NER modules, the joint results of both methods or implement their own NER component (3b in Fig.1).
To identify species names, we incorporated Linnaeus (Gerner et al., 2010) (3a in Fig.1), whose output determines which dictionary-based gene taggers to run, and to narrow down identifiers for ambiguous gene names later in the pipeline [step (6)].
Steps (4) to (7) comprise the actual gene mention normalization task, for which we have implemented a range of filters to remove likely false positive gene mentions as well as candidate IDs.
Removal of false positives (FPs) uses information in the gene name itself, the surrounding text, as well as entire paragraphs or full text to ensure that a found name refers to a specific gene, and not another non-gene term.
Likely FPs are further removed if not also recognized by BANNER.
Note that in contrast to most gene name identification tools, mentions that refer to gene families are considered FPs in the current version of Gnat, since the aim is to find gene mentions that can be mapped to a specific entry in Entrez Gene.
Thus, one of the filters removes mentions such as G proteins, although this step can be tailored to specific needs.
Candidate identifiers can then be further filtered or validated, for example, by removing genes from species not mentioned in the text, or by other user-defined methods [step (6)].
In step (7), the remaining ambiguous cases (gene mentions with more than one potential Entrez Gene ID) are ranked by comparing contextual information found in the text surrounding the mention with knowledge about each gene.
For example, known Gene Ontology annotations for a gene increase its rank when that GO term is found in the nearby text, and similar methods are used for chromosomal locations, associated diseases, enzymatic activity, tissue specificity, etc.
More details on the individual components, especially for disambiguation and normalization, can be found in (Hakenberg et al., 2008) and (Solt et al., 2010), which discuss specific implementations for BioCreative II and III, respectively (also see Section 5).
3 USING THE GNAT JAVA LIBRARY For each of the aforementioned steps, we provide implementations as Gnat components that can be used as they are, extended or replaced by developers within their own pipelines.
Most components can run either locally within the client (for instance, during development) or as remote services (with public or restricted access).
For example, users might want to implement different NER strategies or supply custom dictionaries for species currently not provided in the default version.
Users might alternatively want to include non-specific gene mentions that could be mapped to structured vocabularies such as MeSH that include gene families, or to include information from DNA or protein sequences in the text to improve gene mention normalization (Haeussler et al., 2011).
Likewise, the final ranking methods can be adapted, or different input/output formats could be defined.
4 USING THE GNAT WEB SERVICE The Gnat system also implements web services using HTTP POST and GET requests that can be used by both end users and developers.
To submit a text for annotation, the following three URL parameters can be used: pmid, pmc and text (combinations are allowed).
The pmid parameter takes one or more comma-separated PubMed IDs as values, pmc takes PubMed Central ID(s) and text takes a text (sentence, paragraph, full document) in plain ASCII format.
Note that for large requests (especially when submitting full-text articles), an HTTP POST request should be used instead of GET.
Users can also modify the default behavior of the web service to specify the particular tasks to perform with the parameter task, which can take gner (gene NER), sner (species NER) or gnorm (normalization to Entrez Gene IDs) as arguments.
Specifying tasks can be useful when application developers want to take Gnats NER results as input for their own pipelines, for instance.
Finally, the user can specify the format of the returned results (parameter: returntype), either as a tab-separated list (value: tsv, which is the default) or XML (xml).
A help page listing all current parameters and valid values is available by calling the service without parameters.
In addition to making these web services available as source code, we also host a remote 2770 [15:12 5/9/2011 Bioinformatics-btr455.tex] Page: 2771 27692771 GNAT for gene mention normalization service for a set of 10 common model organisms (available at5 DISCUSSION Large-scale community challenges to assess the status and compare methods for gene mention normalization have been ongoing since 2003 (see the overview of BioCreative I, Task 1B, in Hirschman el al., 2005).
Gnat has been evaluated on three BioCreative datasets: BioCreative I is composed of abstracts from papers on mouse, fruit fly, and yeast genes, BioCreative II is composed of abstracts from papers on only human genes, and BioCreative III is composed of full-text articles with no restriction on species.
For human genes only, an earlier version of Gnat was ranked first among the participants in BioCreative II (Morgan et al., 2008), achieving a precision and recall of 82.1 and 81.6%, respectively, on a test set of 262 abstracts.
Subsequent modifications to Gnat improved precision to 90.1% and with recall at 81.1% (Hakenberg et al., 2008).
On a dataset derived from BioCreative I+II, covering genes from 13 species in 100 abstracts (Hakenberg et al., 2008), the provided default processing pipelines achieves 79% precision at 65% recall.
For BioCreative III, performance was evaluated using the TAP-k metric (Lu et al., 2010), which is based on a ranked list of predictions (Carroll et al., 2010).
The 50 manually annotated full-text articles chosen for maximal variability among submissions served as the gold standard for BioCreative III, on which Gnat achieves a TAP-20 score of 0.1987, with the highest ranking method yielding only 0.3466 (Lu et al., 2010).
Due to the difference in the scoring metrics, results are not easy to compare directly between BioCreative challenges; our own experiments show precision and recall values for the current system of 53.6 and 47.4%, respectively, on the manually curated training data (see Lu et al., 2010).
One drawback of the current default processing pipeline of Gnat relative with the BioCreative III test set comes from limiting our predictions to genes from 20 model organisms.
The manually curated gold standard for 50 full-text documents includes an unusual composition of species compared with the training set: for example, 23% of all genes in the gold standard belong to Enterobacter sp.
638.
This species and three more that contribute an additional 15% to the gold standard genes are not currently supported by the default dictionary-based NER in Gnat, but user-specific dictionaries could be added quickly when new species are anticipated, a procedure for which we provide detailed instructions in the documentation.
Future extensions of the Banner library within Gnat to map any recognized gene name to species and candidate IDs should also help to make up for the low recall introduced by the current species limitation in species supported.
The current version of Gnat implements a clientserver architecture, where individual modules can be set up to run within the client or as servers.
Typically, a module would run as a server if it performs a memory-intensive processing step, requires a certain time for startup or is a finalized component; modules run as client are those which are suited to individual usersneeds or those undergoing development.
Using the default pipeline, it takes an average of 5 s to annotate a PubMed abstract; however, this number clearly depends on the underlying hardware and usage of remote services and can thus serve only as a rough estimate.
Given the modular architecture, Gnats modules can be easily tailored or replaced.
For example, Gnat currently relies on Linnaeus for species NER and provides an interface to Banner for gene NER, demonstrating the ability to easily incorporate external tools, especially if they provide a Java API.
6 CONCLUSION Here we presented the Gnat library for gene name recognition and normalization in biomedical text, now freely available from SourceForge at http://gnat.sourceforge.net.
Gnat is written in a modular fashion to allow end users to annotate their textual data using the public web services, as well as text-mining developers to customize Gnat and host their own remote services, either public or private.
Gnat provides many individual components of a typical text processing and gene name normalization pipeline, which can be extended or swapped by developers where necessary.
As such, Gnat adds to the set of open-source tools now available for researchers to use for large-scale gene name normalization studies, providing a variety of access points to different users, from end users submitting text to a web service and treating Gnats processing pipeline as a black box, to developers who use only some of Gnats modules and replace others.
Precision and recall of Gnat can range from 54/47% on fulltext documents that do not match main model organisms, to 82/82% on abstracts that reflect the species composition of the majority of PubMed.
For the latter, consisting of an ensemble of ten common model organisms, we host web services that accept PubMed and PubMed Central IDs and free text as input, and return mentions and/or EntrezGene identifiers, which we hope will provide an opportunity to enhance research across many domains of bioinformatics.
Funding: Biotechnology and Biological Sciences Research Council (CASE studentship to M.G., grant BB/G000093/1 to C.M.B., G.N.
); the European Commission (grant HEALTH-F4-2008-223210 to C.M.B.
); German Academic Exchange Service (DAAD) to I.S.
Conflict of Interest: none declared.
ABSTRACT Motivation: The classification of biological entities in terms of species and taxa is an important endeavor in biology.
Although a large amount of statements encoded in current biomedical ontologies is taxon-dependent there is no obvious or standard way for introducing taxon information into an integrative ontology architecture, supposedly because of ongoing controversies about the ontological nature of species and taxa.
Results: In this article, we discuss different approaches on how to represent biological taxa using existing standards for biomedical ontologies such as the description logic OWL DL and the Open Biomedical Ontologies Relation Ontology.
We demonstrate how hidden ambiguities of the species concept can be dealt with and existing controversies can be overcome.
A novel approach is to envisage taxon information as qualities that inhere in biological organisms, organism parts and populations.
Availability: The presented methodology has been implemented in the domain top-level ontology BioTop, openly accessible at http://purl.org/biotop.
BioTop may help to improve the logical and ontological rigor of biomedical ontologies and further provides a clear architectural principle to deal with biological taxa information.
Contact: stschulz@uni-freiburg.de 1 INTRODUCTION The classification of biological entities according to their morphological, genetic, evolutionary and functional characteristics is a fundamental organizing principle since Carolus Linnaeus established conventions for naming living organisms (Ereshefsky, 2001).
One century later, the distinction of species received its theoretical underpinning with Charles Darwins theory of evolution (1859) and was finally demystified by the spectacular advances of molecular biology in the late 20th century.
Although these changes have drastically challenged the basic assumptions of Linnaeus biological theory and have given rise to an ongoing debate about the concept of biological species and taxa Hey (2006), his main organizing principle remains the same.
All biology is, in some way, related to the concept of biological taxa.
Taxa are hierarchically structured labels or categories used for biological classification, such as species, family, class, etc.
All organisms, populations, tissues, cells, cell components and biological macromolecules that are under scrutiny of experimental or descriptive biologists are related to some hierarchy of taxa and most biological discoveries have their scope related to one To whom correspondence should be addressed.
species or taxon.
Table 1 gives an exemplary overview of the hierarchical order of taxa.
The basic taxon is the species.
Several species are grouped together by a genus.
Several genera constitute a family, several families an order, several orders a class and then several classes a phylum or division.
Finally, the top-most level, the kingdom distinguishes between animals and plants.
Similar to the several criteria that are discussed to delineate the concept of species, no clear principles exist that govern the division of superordinate taxa.
For instance, orders can be further split into superorders and suborders.
Even more, the number of taxonomic divisions is variable, and there are also divisions without rank name.
The importance of species and biological taxa is evidenced by many sources.
Biological taxa constitute 3497 out 24 766 descriptors of MeSH1, the indexing vocabulary of Medline.
In the Open Biomedical Ontologies (OBO) collection2 (Smith et al., 2007), 30 out of 66 ontologies are taxon specific, with taxa ranging from species such as Homo Sapiens or Caenorhabditis elegans, genera such as Plasmodium over families such as Poaceae to classes such as Mammalia.
Due to the sheer number of taxa there is no universal authoritative source, but every important subfield within biology has been independently maintained by curators, so-called systematists, and for a long time the field of biological systematics has been considered an important research discipline.
A converging effort in unifying taxon information for whole biology is the Catalogue of Life3 targeted for complete coverage of all 1.75 million known species by 2011.
In the mentioned OBO collection, nearly half a million taxon entries of medical interest is available in computer-processable form via the rapidly growing NCBI Taxonomy (Wheeler et al., 2008).
To sum up, biological taxa constitute an overarching and systematic ordering principle that is relevant in practically all biological subject areas.
In this article, we will show how the realm of biological systematics can be embedded into an ontological framework.
It is structured as follows: We start with a summary introduction of domain ontologies in general, as well as in the context of the biology, addressing the OBO ontologies and the BioTop biomedical topdomain ontology.
Then we provide a formal account of different aspects of the conceptualization of biological taxa and demonstrate how this is implemented in BioTop.
Finally, we briefly describe our tentative implementation supporting our claim that an overarching ontological framework for biology must have a conclusive and practical account of biological taxa.
1Medical Subject Headings, http://www.nlm.nih.gov/mesh 2Open Biomedical Ontologies, http://www.obofoundry.org 3Catalogue of Life, http://www.catalogueoflife.org 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[19:58 18/6/03 Bioinformatics-btn158.tex] Page: i314 i313i321 S.Schulz et al.Table 1.
Biological taxa with examples Taxon (rank) Asian elephant Chimpanzee Drosophila Species Elephas maximus Simia troglodytes Drosophila melanogaster Genus Elephas Pan Drosophila Subfamily Drosophilinae Family Elephantidae Hominides Drosophilidae Superfamily Elephantoidea Order Proboscidea Primates Diptera Class Mammalia Mammalia Insecta Subphylum Vertebrata Vertebrata Phylum Chordata Chordata Arthropoda Kingdom Animalia Animalia Animalia 2 BIOMEDICAL ONTOLOGIES 2.1 The foundations of biomedical ontology It is mainly the information explosion in biology and the necessity to process huge amounts of research data that have stimulated the proliferation of biomedical ontologies.
Rubin et al.(2008) give an overview of the broad range of biomedical information services that can be supported by domain ontologies, with the Gene OntologyAshburner et al.(2000) and the OBO collection as the most prominent examples.
Whereas this tenet used to be addressed in the past mainly by what had been termed biomedical terminologies (with the UMLS4 as prototypical example), more recently we have seen a steady growth in the usage of the term ontology.
Due to the lack of a clear notion of what an ontology actually constitutes (Kusnierczyk, 2006) there is a tendency for either insupportable expectations or general rejection of this term.
In this article, we detach the concept of terminology from the one of ontology subscribing to the following definitions: According to ISO (2000), a terminology is defined as a set of terms representing the system of concepts of a particular subject field.
Terminologies relate the senses or meanings of linguistic entities.
In contrast, according to Quine (1948), Ontology (in singular and upper case) is the study of what there is.
In our understanding, ontologies (plural and lowercase) are formal theories that attempt to give precise formulations of the types of entities in reality, of their properties, and of the relations between them (Guarino, 1998).
In contradistinction to terminology, formal ontologies strive for describing (as much as possible) what the consensus in a given scientific domain is, independently of human language.
Their constituent nodes are referred to as types, kinds or universals.
As they are well suited to hierarchically order and classify particular entities (e.g.a given piece of tissue, a cell under a microscope, an amount of biological substance, an animal, a particular population of bacteria, etc.
), they are also referred to as classes, a parlance we will use in the following, in accordance with the more recent language use in current biomedical ontology engineering and research.5 Although the question whether certain entities really exist are subject to major philosophical disputes, we contend that at any 4Unified Medical Language System (UMLS): http://umlsinfo.nlm.nih.gov 5We follow a general trend and restrict the use of the word concept to the realm of terminologies, where it denotes artifacts that represent meanings of linguistic expressions.
We avoid it in relation to formal ontologies.
given stage in the development of science, there is a consensus core of scientific understanding of reality, and in our view, it is this which should serve as starting point for developing science-based ontologies.
Examples of statements belonging to this consensus core are that: primates are vertebrates, cells contain cytoplasm, aspirin tablets contain a derivative of salicylic acid, ADP is phosphorylated in mitochondria or that certain biochemical compounds have a clearly delineated composition.
2.2 Top-level ontologies It is widely recognized that the construction of formal ontologies should obey principled criteria.
To this end, several top-level ontologies have been devised, such as DOLCE (Gangemi et al., 2002), BFO (Smith et al., 2005), or GOL (Heller and Herre, 2004).
These ontologies mainly coincide in their fundamental division between continuants (also called endurants, e.g.material objects) and occurrents (also called perdurants, e.g.events, processes).
Orthogonal to this distinction, there is also a coincidence in clearly separating concrete entities or particulars (e.g.the chimpanzee named Washoe, the elephant named Clyde, or the 3rd author of this article) from the classes they instantiate (e.g.Chimpanzee, Asian Elephant, Human).
To this end, we introduce the irreflexive, anti-transitive and asymmetric instantiation relation instance_of which relates particulars to classes.
In addition, we need a formal relation for subsumption between classes.
Here we follow the OBO standard and introduce, for this purpose, the taxonomic subsumption relation Is_a by means of instance_of6 just as proposed by Smith et al.(2005): Is_a (A,B)=def x : (instance_of (x,A) instance_of (x,B)) In the following discussion, we are proposing several possible alternative solutions for an ontological account of species.
2.3 Domain top-level ontologies Whereas top-level ontologies contain only a restricted set of highly general classes, such as the aforementioned Continuant, Occurrent, Function or Object, which are not tied to any particular domain of interest, a domain top-level ontology contains all the classes that are essentially needed to describe a certain domain, like Organism, Tissue, Cell and also Species in the case of biology.
Those more specific classes are in turn a specialization of the top-level classes as expressed in the formula Is_a (Cell, Object).
2.4 BioTopa domain top-level ontology Recently, two separate implementations to encode the top-level of the biomedical domain into ontologies have been created, namely, BioTop7 (Stenzhorn et al., 2007) and the Simple Top Bio (Rector et al., 2007).
At the moment, efforts set forth by the authors are ongoing to converge these two implementations.
The goal of BioTop is to provide classes and classificatory criteria to categorize the foundational kinds of biology, without any restriction to granularity, species, developmental stages or states 6Throughout this article, we use capitalized initial letters for the names of relations between universals, as well as for the names of universals.
Particulars are highlighted by lower case or by quoted names, bold face is used for relations between particulars.
7Available at http://purl.org/biotop i314 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i315 i313i321 Ontology of biological taxa of structural well-or ill-formedness (Schulz and Hahn, 2007).
The initial impetus for creating the BioTop ontology was the idea of redesigning and expanding the GENIA ontology (Ohta et al., 2002) in a comprehensive and formally sound way, i.e.to adhere to the fundamental principles of formal rigor, explicitness and precision of ontological axioms.
In BioTops initial development, no definitive commitment existed towards any existing upper ontology, except for the distinction between continuants and occurrents (cf.
Section 2.2).
The primary focus at this stage was set on representing continuants from the area of interest.
In the continued development, however, the focus was broadened to include the representation of biological processes, functions and qualities.
Additionally, BioTop was aligned with the BFO upper level ontology.
BioTop is implemented in OWL DL,8 an official Semantic Web standard published by the World Wide Web Consortium (W3C).
By using this language, our ontology can benefit from a large amount of support tools for editing, automatic classification, etc.
OWL DL is also one of the languages accepted by the OBO consortium.
The significance of this lies in the fact that, in our view, the high-level BioTop classes can serve as a bridge to link and interface the domain-specific ontology classes in the OBO collection.
Using such interfacing facility can both potentially reveal overlaps or design errors in OBO ontologies and also create synergetic effects.
2.5 The difficult concept of species Before we embark on a more general ontological account of biological taxa, we first turn to the most basic taxon, namely, species.
Both biologists and philosophers disagree on the proper definition of the term species and its ontological status (Ereshefsky, 2001).
It had been principally the criterion of similarity between organisms and organism groups that guided Linnaeus classificatory efforts.
Although there are rarely any two individuals with exactly identical characteristics, we made the following observations in regard to the similarity of organisms.
From a diachronic point of view, there are generally significant but relatively minor differences between an organism and its offspring due to sexual or asexual reproduction and spontaneous mutations.
However, the distance increases with the number of generations and so todays organisms have little in common with their ancestors.
The genetic and phenotypic modifications can be assumed to lie on a mainly continuous scale, and the boundary of the emergence of a new species cannot be drawn by unambiguous criteria, a phenomenon that is ubiquitous in biology (Schulz and Johansson, 2007).
No obvious distinguishing feature exists that is apt to clearly divide the species Homo sapiens from Homo erectus and nothing indicates any sort of qualitative leap.
As a corollary of this, the parallel evolution of independent lines of organisms increases their genetic and phenotypic distance.
Under a synchronic viewpoint, this manifests itself as groups of organisms with clear criteria of species identity.
In contrast to the diachronic view, the distinguishing features do not lie on a continuous scale but they are clearly discrete.
For instance, the boundary between the species Homo sapiens and Simia troglodytes (chimpanzee) can be clearly drawn, as there are no organisms existing in the middle.
Even under the diachronic perspective, the distinction between groups of organisms with diverging characteristics may be blurred, 8Web Ontology Language (OWL): http://www.w3.org/TR/owl-features e.g.by the distinction of subgroups of the same species.And different species may even form hybrids and merge to a new species.
All these peculiarities claim for a non-arbitrary conceptualization of what constitutes exactly a species.
There are different types of species concepts, from which the concept of biological species as a group of organisms that can interbreed and produce fertile offspring (Mayr, 1969), has found the widest acceptance.
Nevertheless, this definition provides only necessary but not sufficient criteria.
A defined population of organisms (e.g.the Asian elephants living in Thailand) certainly fulfills this criterion although they do not form a species of their own since they can mate and produce fertile offspring with elephants from Cambodia, for instance.
Abbreviating the ability of producing fertile offspring by , according to the biological species concept, the pertinence of biological organisms to the same species is expressed by the predicate : ( o1,o2 )= def (t :(o1,o2,t ))(o,t1,t2 : ( ( o1,o,t1 )(o2,o,t2 ))) The shortcomings of Mayrs definition are well known (Grene and Depew, 2004, ch.
10): first, it only allows the comparison of organisms living at the same time.
Second, the definition depends on the dispositional criterion , the verification of which remains speculative in many cases.
Third, the definition fails with infertile individuals, as well as with species in extinction of which only female or male individuals remain.
Fourth, it fails in the numerous cases of asexual reproduction such as bacteria.
It is therefore neither easily applicable, nor generally valid, in spite of its theoretical soundness (Hull, 1997).
So it is not surprising that other species concepts compete with Mayrs one.
The 22 different conceptualizations of species identified and discussed by Mayden (1997) bear witness on the intensive discussions and disagreements among theoretical biologists and philosophers.
For our practical purpose of biomedical ontologies the formalization of species ormore generallyof biological taxa that we propose, is intended to be neutral to the different and conflicting species conceptualizations.
It departs from the principle that biological taxa are something that regardless of its existence in nature or its (fiat) attribution by biologists has a highly ranked importance in biology and therefore requires to be accounted for in biomedical ontologies.9 In the following, we will analyze the ontological status of biological taxa and propose and critically assess alternative solutions.
3 CONCURRENT ACCOUNTS OF BIOLOGICAL TAXA 3.1 Biological taxa as meta-properties The above restriction to a two-leveled ontological framework (i.e.dividing the world exhaustively into particulars and universals) has often been challenged.
(Gangemi et al., 2001) contend that there is a fundamental difference between instances in an ontology on the one hand and domain entities (particulars, cf.
Section 2.1) on 9The approach should be flexible enough to support even classification schemes that contradict classic taxonomic principles such as carnivore and herbivore.
The authors are aware of the fact that this may challenge some of the philosophical foundations underlying Basic Formal Ontology (BFO).
i315 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i316 i313i321 S.Schulz et al.the other hand.
They argue that we can extend a Theory A (which follows the two-level assumption) by a meta-Theory B.
Whereas Theory A describes domain entities (particulars) that instantiate universals (classes), B takes As universals as instances of so-called meta-properties.
Indexing the instantiation relation by theory level (using subscripts in the formulae) we may state in Theory A that instance_ofA(x,y) and then place this in the context of Theory B with instance_ofB (y,z) To give a concrete example: instance_ofA(Clyde, Elephas maximus) instance_ofB (Elephas maximus, Species) Due to the algebraic property of antitransitivity (as claimed by (Gangemi et al., 2001), we can then coherently reject the hypothesis that our elephant Clyde is an instance of Species.
There are several arguments against this solution.
Let us consider the second-level predications instance_ofB (Elephas maximus, Species) on the one hand and instance_ofB (Elephas maximus, Genus Elephas) on the other hand.
Whereas the first one asserts that the class Elephas maximus is an instance of a Species, the second one states that the species class Elephas maximus as a member of the genus Elephas.
In the same right as we have stated instance_ofB (Elephas maximus, Species) we could then assert in a third-level predication (instance_ofC) instance_ofC (Genus Elephas, Genus) Clyde would then be a second-level instance of Species and a second-level instance of Genus Elephas, as well as, in virtue of the latter, a third-level instance of Genus.
Given instance_ofC (Species, Taxon) and instance_ofC (Genus, Taxon), Clyde would finally act simultaneously both as third and fourth-level instance of Taxon.
Together with the argument that Clyde might also directly instantiate Genus Elephas and the fact that some taxonomic levels (such as subfamilies) are sometimes skipped, it is very obvious that this solution leads to an obscure and inconsistent picture.
Another shortcoming of this approach lies in the fact that it lacks a transitive hierarchical relation between taxa of different levels that would be able to express in simple terms (e.g.that all Indian elephants are vertebrates).
From a computational viewpoint, there is also an important performance argument.
For example, efficient reasoning algorithms which have been developed for description logics (Baader et al., 2003) and are coherent with the Semantic Web standard OWL DL do not provide support for reasoning capabilities about instances of instances.
3.2 Biological taxa as hierarchies of classes We could simplify the above approach (and render it well-suited for description logics-based reasoning) by conflating the level of classes with the one of the meta-level classes.
Given the definitions above and a division of all entities in either particulars or classes, it may appear straightforward to use the Is_a relation for expressing that Chimpanzees, Indian Elephants, Humans, etc.
are species, or that Genus Pan, Genus Elephas and Genus Homo are genera: Is_a (Elephas maximus, Species) Is_a (Simia troglodytes, Species) Is_a (Genus Elephas, Genus) Is_a (Genus Pan, Genus), just as Is_a (Elephas maximus, Genus Elephas) Is_a (Simia troglodytes, Genus Pan) The weakness of this solution, however, immediately derives from the above definition of the Is_a relation.
So given that instance_of (Clyde, Elephas maximus) instance_of (Washoe, Simia troglodytes) we can infer that instance_of (Clyde, Genus Elephas) instance_of (Washoe, Genus Pan) as well as that instance_of (Clyde, Species) instance_of (Washoe, Species) instance_of (Clyde, Genus) instance_of (Washoe, Genus) We finally end up with all taxa in a specialization hierarchy, having individual organisms as instances.
This neither captures the nature of a biological organism, nor the intended meaning of Species or Genus, since neither Clyde nor Washoe or any other individual animal is an instance of the class Species.
Nevertheless, we could consistently do this excluding the terms species, genus, etc.
This would reduce the instances of taxa (Elephant, Elephantidae, Vertebrates) to classes of organisms and we would no longer be able to account for the meaning of terms like Genus or Species in a description logic-based framework.
However, the resulting assertions such as Clyde is an instance of Mammalia (on par with Clyde is an instance of Elephant) would collide with the plural meaning of the taxon terms.
3.3 Biological taxa as populations Several authors have argued in favor of the inclusion of collectives into an ontological framework (Bittner et al., 2004; Rector et al., 2006; Schulz et al., 2006a).
BioTop has embraced these aspects by introducing the relation has_granular_part, an irreflexive and intransitive subrelation of the OBO Relation Ontology relation has_part (Schulz et al., 2006b).
This allows us to relate a collective entity to each of its constituent elements, without, however, resorting to set theory.
For instance, has_granular_part (PopulationofThaiElephants,Clyde) asserts that there is a collective entity Population of Thai Elephants that is constituted by granular parts like our elephant Clyde and a number of other individuals similar to Clyde.
It permits to define i316 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i317 i313i321 Ontology of biological taxa Is_A W ashoe Genus Pan Quality Family Elephantidae Quality Order Primates Quality Family Hominides Quality Class Mammalia Quality Subphylum Vertebrata Quality Genus Elephas Quality Simia troglodytes Quality Elephas maximus Quality Order Proboscidea Quality Phylum ChordataQuality Kingdom Animaliaa Quality Is_A Is_A Is_A Is_A Is_A Is_A Is_AIs_A Is_A Is_A Is_A Clydes taxon quality Washoes taxon quality instance_ofinstance_of inheres_in Clyde Washoe P ar tic ul ar s U ni ve rs al s (C la ss es ) inheres_in Fig.1.
Taxon qualities inhering in individual organisms.
collectives in terms of granular parts such as x : instance_of (x, ElephantPopulation) y1,y2,...yn : instance_of (y1,y2,...,yn, Elephant) has_granular_part (x,y1,y2,...,yn) z : (instance_of (z, Elephant) has_granular_part (x,z)) Note that Population of Thai Elephants is a particular collective and an instance of the universal collective ElephantPopulation.
The union of all possible instances of ElephantPopulation, namely, Total ElephantPopulation would then be the maximal population of elephants every individual elephant is a granular part of.
x : instance_of (x, Elephant) has_granular_part (Total ElephantPopulation,x) Yet, TotalElephantPopulation is a particular entity.
Our proposal here is to consider it as an instance of Species.
In the same way, we could introduce other populations in different degrees of abstraction such as TotalVertebratePopulationwhich would then be an instance of Phylum.
It may be practical for many purposes to equate biological taxa with biological populations although the meaning of Elephantidae or Vertebratae, in practice, goes further.
Especially in molecular biology, species information is not only attributed to whole organisms, but also to organism parts, their constituting cells and derived cell lines.
As an example, individual cells from the HELA cell line are considered human cells, but their existence is not dependent on any human population.
The interpretation of biological taxa as populations is therefore not adequate for such cases.
We can use the OBO relation derives_from in order to express that a HELA cell is a human cell: x : instance_of (x, HELA Cell) y : instance_of (y, Human) derives_from (x,y) has_granular_part (TotalHumanPopulation,y) 3.4 Biological taxa as qualities Most top-level ontologies coincide in granting qualities a prominent status.
For instance, BFO describes the class Quality as A dependent continuant that is exhibited if it inheres in an entity or categorical property.
Examples: the color of a tomato, the ambient temperature of air, the circumference shape of a nose, the mass of a piece of gold, the weight of a chimpanzee.10 DOLCE introduces qualities as the basic entities we can perceive or measure: shapes, colors, sizes, sounds, smells, as well as weights, lengths, electric charges (Masolo, 2003) and also makes reference to the relationship of inherence.
The position of the class Quality in BFO makes clear that qualities are dependent entities, i.e.they can only exist in dependence on the entities they inhere in.
Our proposal here is to interpret the relation of a biological object to a given taxon as the ascription of a quality.
For example, the quality of belonging to the species Homo sapiens is a quality that inheres in any human organism, tissue or cell.
The quality of belonging to the phylum Chordata is a quality that inheres in any biological object that is part of or derived from an organism the species of which belongs to the phylum Chordata.
Figure 1 depicts a segment of our proposed subclass hierarchy of taxon qualities.
The hierarchy exhibits two organizational principles: generalization versus specialization on one side, and the relevance to an organizational level on the other.
Every instance of a material biological object has one inherent taxon quality.
Since, e.g.every human is a hominid, every inhering instance of the class Homo sapiens Quality is also an instance of Family Hominides Quality, etc.
The introduction of qualities is helpful for 10SNAP Continuant Definitions: http://www.ifomis.org/bfo/manual/snap.pdf i317 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i318 i313i321 S.Schulz et al.Fig.2.
Taxon qualities inhering in individual organism and their location in Taxon Regions consistent with the DOLCE upper level ontology.
ontological definitions such as x : instance_of (x, Human) instance_of (x, Organism) y : instance_of (y, HomosapiensQuality) inheres_in (y, x) x : instance_of (x, Vertebrate) instance_of (x, Organism) y : instance_of (y, VertebrateQuality) inheres_in (y, x) Based on a hierarchy of qualities, such definitions permit inferences such as that every human is a vertebrate or that every human population is part of some vertebrate population.
In addition, it allows for linking organism parts with qualities such as x,: instance_of (x,VertebrateHeart) instance_of (x, Heart) y : instance_of (y,VertebrateQuality) inheres_in (y, x) If the import of the taxon concept should be extended from biological organisms to their parts, as argued in Section 3.3 (e.g.human leukocyte), the attribution of qualities to organism parts or derivatives can easily be axiomatized by the so-called right identity rules (with being the relation concatenation symbol): part_of (x, y) inheres_in (z, y) inheres_in (z, x) derives_from (x, y) inheres_in (z, y) inheres_in (z, x) 3.5 Biological taxa as Qualia An alternative approach to a subclass hierarchy based on the DOLCE upper ontology (Masolo, 2003) is represented in Figure 2.
Since DOLCE is inspired by trope theory (Goodman, 1951), which distinguishes between qualities and their values (i.e.Qualia) this proposal introduces another layer of abstraction.
Each quality type has an associated quality space (i.e.Region) in which it is located.
As in BFO, qualities are dependent entities which are inherent in their respective particulars.
Compared to the representation depicted in Figure 1 only few taxon qualitiesone for every taxonare organized in a flat hierarchy and are related to corresponding value regions.
The subsumption hierarchy of taxon qualities of the former approach is represented as a partonomic hierarchy of the Taxon Regions in the latter, e.g.the Species Region is part of the Class Region which is itself part of the Kingdom Region.
The variety of features is represented as subclasses of the basic Taxon Regions, e.g.Mammalia Class Region Is_a Class Region.
The main advantages of this approach are a clearer separation of hierarchies and the possibility to make explicit assertions on the specialized Taxon Regions without uncontrolled inheritance of restrictions.
Its disadvantage lies in a higher complexity.
3.6 Synthesizing different taxon accounts We have proposed four mutually dependent kind of ontologically relevant entities that describe different aspects of what is meant i318 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i319 i313i321 Ontology of biological taxa by biological taxa on the one hand, and that are expressible in a description logics-based framework on the other.
The totality of organisms belonging to one taxon (e.g.all Grampositive bacteria, all primates or all humans).
This entity is a particular one that instantiates the class Maximal Biological Population.
For each taxon there is one such instance.
Population classes, the instances of which are defined as parts of some instance of Maximal Biological Population.
For example, Elephant Population in Thailand is an instance of the class Elephas Maximus Population, the latter being a subclass of Elephas Population and so on.
For each taxon there is one such population class.
Taxon quality classes that are instantiated by each and every particular object to which a taxon can be ascribed.
There is one such taxon quality class for each taxon.
Because taxon classes are arranged in an Is_a hierarchy, the quality of a subordinate taxon is also the quality of a superordinate taxon.
For example, an instance tqClyde of Elephas Maximus Quality can be ascribed to the elephant Clyde.
tqClyde is equally an instance of Genus Elephas Quality, of Family Elephantidae Quality, and so on.
Taxon quality regions that are represented by a mereological inclusion hierarchy.
In contrast to the third approach, every taxon-relevant entity has an inherent quality instance from each taxonomic level.
4 IMPLEMENTATION We extended BioTop by the notion of biological taxa following the quality approach discussed in Section 3.4. bfo:Entity  bfo:Continuant bfo:DependentContinuant  bfo:SpecificallyDependentContinuant  bfo:Quality   biotop:ContinuantQuality   biotop:TaxonQuality The class biotop:TaxonQuality has the following restrictions11: biotop:TaxonQuality implies inheres_in.
(has_part.biotop:NucleicAcid)AND inheres_in.
(has_part.biotop:NucleicAcid) So we claim the existence of genetic information as a limiting and necessary condition for those entities biological taxa can be ascribed to.
In the inverse direction, we claim the inherence of taxon qualities to the classes biotop:Cell, biotop:Organism, biotop:Tissue,biotop:OrganismPart, biotop:NucleicAcid, e.g.biotop:Cell implies inv_inheres_in.biotop:TaxonQuality The class biotop:TaxonQuality is then the interface to a specialized ontology such as the NCBI taxon ontology.
For demonstration purposes we created taxdemo, a small example ontology.12 11For the Description Logics notation cf.
(Baader et al., 2003), or12Available at http://purl.org/biotop taxdemo:TaxonQuality  biotop:TaxonQuality  taxdemo:KingdomAnimaliaQuality  taxdemo:PhylumChordataQuality   taxdemo:ClassMammaliaQuality   taxdemo:OrderPrimatesQuality    taxdemo:FamilyHominidaeQuality    taxdemo:GenusHomoQuality     taxdemo:HomoSapiensQuality In parallel, the taxonomic ranks (TaxonQuality, KingdomQuality, etc.)
are indirectly represented as a second hierarchy.
taxdemo:TaxonQuality biotop:TaxonQuality  taxdemo:KingdomQuality  taxdemo:KingdomAnimaliaQuality taxdemo:KingdomBacteriaQuality taxdemo:KingdomVirusesQuality  taxdemo:PhylumQuality  taxdemo:PhylumChordataQuality  taxdemo:ClassQuality  taxdemo:ClassMammaliaQuality  taxdemo:OrderQuality  taxdemo:OrderPrimatesQuality  taxdemo:OrderProboscideaQuality  taxdemo:FamilyQuality  taxdemo:FamilyHominidesQuality  taxdemo:FamilyElephantidaeQuality  taxdemo:GenusQuality  taxdemo:GenusHomoQuality taxdemo:GenusPanQuality taxdemo:GenusElephasQuality  taxdemo:SpeciesQuality  taxdemo:HomoSapiensQuality  taxdemo:ElephasMaximusQuality This allows us to define population as a plurality of organism of the same species as follows: taxdemo:Population IMPLIES has_granular_part.biotop:OrganismAND =1 inv_inheres_in.taxdemo:SpeciesQuality These criteria are not met by mixed groups of individuals, e.g.a group of different primates which coincide only at the level of taxdemo:OrderQuality The flexibility of our approach becomes obvious when we use taxon information for parts of the organisms.
For instance, the class HumanLeukocyte can be defined as taxdemo:HumanLeukocyte EQUIVALENT TO taxdemo:Leukocyte AND inv_inheres_in.taxdemo:HomoSapiensQuality If we define taxdemo:AnimalCell EQUIVALENT TO taxdemo:Cell AND inv_inheres_in.taxdemo:KingdomAnimaliaQuality i319 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i320 i313i321 S.Schulz et al.then taxdemo:HumanLeukocyte can be classified as taxdemo:AnimalCell, provided that the ontology supports: taxdemo:HomoSapiensQuality Is_a taxdemo:KingdomAnimaliaQuality together with taxdemo:Leukocyte Is_abiotop:Cell It is obvious that this kind of reasoning can be of great advantage for biological fact retrieval from databases or for semantically enriched information extraction from texts.
From a computational perspective, however, we acknowledge that there still is a bottleneck with regard to the use of inverses (such as inheres_in versus inv_inheres_in) and qualified number restrictions (such as =1) in description logics reasoners.13 We admit that the meaning of the taxonomic rank classes SpeciesQuality, GenusQuality, KingdomQuality, etc.
is somewhat counterintuitive, since every instance of SpeciesQuality is also an instance of GenusQuality and so on.14 They are, therefore, not suited to comprehensively represent the meaning of Species as disjoint from Genus, Kingdom, etc.
Such a reading would require the metaclass representation as discussed in Section 3.1, discarded due to computational reasons.
In our framework, the only way to have an instantiable Species (Genus, Kingdom) class would be to collect all maximal populations (cf.
Section 3.3) with identical species(genus-, kingdom-) level qualities as instances of Species (Genus, Kingdom) which, again, would only partially match the meaning of Species (Genus, Kingdom).
We refrained from implementing the solution discussed in Section 3.5, because its more differentiated approach to the representation of qualities is not supported by the BFO upper ontology, currently in use for BioTop.
5 RELATED WORK Literature on the ontology of taxa roughly falls into two categories: the conceptualization of the nature of species on the one hand, and the ontological status of taxa on the other.
In both cases, the focus lies mainly on species whereas higher taxa are seldom addressed.
The first line of scientific discussion is characterized by numerous publications that started with the seminal book of Mayr (1942), who compared several approaches to delineate the nature of species15 and propagated the popular concept of species as a group of organisms that interbreed and produce fertile offspring.
Hull (1997) casts doubt on the monistic assumption that there is one single and ideal way to define species and hypothesizes a trade-off between theoretical significance and practical applicability of species concepts.
He classifies the existing species concepts into three categories, namely, (i) similarity-based (which, of course, hinges on some unambiguous notion of phenic or genetic resemblance), (ii) biological and evolutionary (which includes Mayrs and other proposals such as Hennig, 1966) centering around the behavior (i.e.mating, reproduction) of biological organisms and (iii) phylogenetic, focusing the historic development of species.
Mayden (1997) 13See frequently updated list at http://www.cs.man.ac.uk/sattler/ reasoners.html 14An instance of HomoSapiensQuality would be an instance of KingdomQuality, too.
15For an overview of earlier approaches see Hey (2006).
performed an extensive literature review and identified 22 distinct species concepts.
In contradistinction to Hull, he propagates the cladistics-based evolutionary significant unit (Evolutionary Species Concept, Simpson, 1961), rooted in the philosophical principle of identity: An evolutionary species is an entity composed of organisms that maintains its identity from other such entities through time and over space and that has its own independent evolutionary fate and historical tendencies.
According to (Goodman, 1951) this concept of species is the most acceptable and most compatible with other species concepts that are rather criterion-based detection protocols than theoretically underpinned concepts.
He argues that no criterion that presumes to delineate natural boundaries can overcome the generic vagueness (Hull, 1965) of species concepts.
Our approach advocates neutrality towards the conceptualization of species and is apt to coexist with both monistic and pluralistic approaches.
We are aware of the fact that in the latter case species qualities with multiple parents may be taken into account, due to different categorizations according to conflicting species concepts.
The second line of discussion is on more abstract grounds, and scrutinizes the ontological nature of species, regardless of the species concepts subtleties as exposed above.
A fundamental question in here is whether speciesseen as single evolving lineage that act as units of evolutionare classes or individuals, the latter being advocated by Ghiselin (1974) and Hull (1978), with the consequence that every single organism is a spatiotemporal part of its species.
This theory comes close to our view of species as the totality of organisms belonging to one specific species, which can be generalized from species to taxa.
We prefer this mereological approach over the set-theoretical one (also pointed out by (Ereshefsky, 2007), because the view of a group of organisms as mathematical sets (that are not localized in space and time) is rather counterintuitive.
The conceptualization of species as universals or natural kind conflicts with the fact that there are relatively few essential properties that are shared by all individuals of a species (including developmental stages and malformations).
Boyds (1999) Homeostatic Property Cluster Theory tries to overcome this, but is still too much committed to similarity-based criteria according to (Ereshefsky, 2007).
The approach pursued in this article, namely, introducing theory-neutral species qualities that are extensible to general taxon qualitiesseems to be rather novel.
6 CONCLUSION We have proposed an ontological approach to biological taxa in the context of the domain top-level ontology BioTop.16 It is essentially based upon the assumption that every biological organism, population or biological matter has some inherent taxon quality.
Since it does not raise further reaching ontological claims, our approach largely bypasses the ongoing dispute on species concepts.
This enables us to delineate biological populations in terms of shared taxon qualities and to formulate taxon-specific axioms in the framework of description logics.
Our proposal is fully embedded into the standards of Open Biological Ontology and is in line with a major top-level ontology, BFO.
Our account of taxon qualities (i.e.the preference of the 16BioTop, together with a tentative taxon-specific extension is available at[19:58 18/6/03 Bioinformatics-btn158.tex] Page: i321 i313i321 Ontology of biological taxa simpler approach described in Section 3.4 over the more complex solution found in Section 3.5) also demonstrates how fundamental ontology design decisions depend on the choice of the underlying top-level model.
As our approach represents taxon qualities as a simple is_a hierarchy, the import of subsets of existing taxonomy databases such as the NCBI taxonomy is straightforward and scalable.
These data can automatically be transformed into an OWL subtype hierarchy and linked to the BioTop node TaxonQuality.
ACKNOWLEDGEMENTS The authors would like to thank Alan Rector (Manchester), Elena Beiwanger (Jena), Udo Hahn (Jena), Eric van Mulligen (Rotterdam) and Lszl van den Hoek (Rotterdam), as well as Olivier Bodenreider (Bethesda), for fruitful discussions.
Funding: This work was supported by the EC STREP project BOOTStrep (FP6 028099).
Conflict of Interest: none declared.
ABSTRACT Motivation: In cancer genomes, chromosomal regions harboring cancer genes are often subjected to genomic aberrations like copy number alteration and loss of heterozygosity.
Given this, finding recurrent genomic aberrations is considered an apt approach for screening cancer genes.
Although several permutation-based tests have been proposed for this purpose, none of them are designed to find recurrent aberrations from the genomic dataset without paired normal sample controls.
Their application to unpaired genomic data may lead to false discoveries, because they retrieve pseudoaberrations that exist in normal genomes as polymorphisms.
Results: We develop a new parametric method named parametric aberration recurrence test (PART) to test for the recurrence of genomic aberrations.
The introduction of Poisson-binomial statistics allow us to compute small P-values more efficiently and precisely than the previously proposed permutation-based approach.
Moreover, we extended PART to cover unpaired data (PART-up) so that there is a statistical basis for analyzing unpaired genomic data.
PART-up uses information from unpaired normal sample controls to remove pseudo-aberrations in unpaired genomic data.
Using PART-up, we successfully predict recurrent genomic aberrations in cancer cell line samples whose paired normal sample controls are unavailable.
This article thus proposes a powerful statistical framework for the identification of driver aberrations, which would be applicable to ever-increasing amounts of cancer genomic data seen in the era of next generation sequencing.
Availability: Our implementations of PART and PART-up are available from http://www.hgc.jp/niiyan/PART/manual.html.
Contact: aniida@ims.u-tokyo.ac.jp Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Cancer genomes often exhibit chromosomal aberrations like copy number alteration and loss of heterozygosity (LOH).Achromosomal aberration potentially leads to the functional alteration of cancer genes and could be a driver for oncogenesis.
For example, if the copy number of some locus is amplified, residing oncogenes would be functionally activated.
Conversely the presence of tumor suppressor genes are associated with chromosomal deletion and LOH.
However, most aberrations are so-called passengers, which accompany driver aberrations by chance and do not have any causal relationship with oncogenesis.
Therefore, it is important problem to discriminate the driver aberrations from the passenger ones.
Given that driver aberrations recurrently occur around driver cancer genes whereas To whom correspondence should be addressed.
passenger aberrations randomly exist across chromosomes, finding recurrent chromosomal aberrations is deemed a powerful approach for discovering driver aberrations and associated driver genes.
In the past decade, microarray technology has enabled genomewide profiling of copy number and homozygosity (Michels et al., 2007).
The application of microarrays to cancer genomes has revealed prevalent aberrations in cancer cells, and produced a large amount of genomic data, which are rich resources for the identification of potential driver loci (Beroukhim et al., 2010).
By examining the presence of aberrations across all chromosomal positions in multiple samples, we have a binary aberration profile matrix whose rows and columns correspond to chromosomal positions and samples, respectively.
We wish to find chromosomal positions where a significantly large fraction of samples are subjected to aberration.
There are a number of computational methods to statistically screen for recurrent genomic aberrations, most of which are based on permutation tests (Morganella et al., 2011).
For example, GISTIC calculates the value of statistic scoring recurrence for each genomic position while the null distribution of the statistic is obtained using null aberration profile matrices, which are generated by permuting positions of the binary aberration profile matrix for each sample.
Finally, GISTIC reports the significance of recurrence at each position and predicts driver loci by detecting the peaks of the significance plot.
(Beroukhim et al., 2007).
Although the permutation approach is successful in finding driver aberrations, it is computationally intensive, especially when we need to calculate small P-values precisely.
Usually, aberration profiling of a cancer genome is performed by comparing a tumor sample with the paired normal sample.
For example, LOH is called for a position whose genotype changes from a heterozygous state in a normal genome to a homozygous state in the paired-tumor genome.
When the paired normal sample is not available, aberration profiling is also possible: LOH could be called for a chromosomal segment that has successive homozygous state in the tumor genome (Beroukhim et al., 2006).
However, it has been reported that we would confront false positive calls in such unpaired experimental designs: an obtained LOH might be only a polymorphic homozygous segment that exists in the paired normal genome (Heinrichs et al., 2010).
To find recurrent aberrations, we developed a novel parametric test, parametric aberration recurrence test (PART).
Using Poissonbinomial statistics (Wang, 1993), PART can be used for efficient and precise calculation of small P-values, as compared with the permutation approach.
Moreover, we extend PART to cover unpaired data (PART-up) to find recurrent aberrations in unpaired experimental designs.
PART-up computes the significance of aberration recurrence by taking into consideration the false positive rate, which is calculated from the unpaired normal sample data.
By applying PART-up to simulated and real data, we demonstrate that The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: [19:59 28/5/2012 Bioinformatics-bts203.tex] Page: i116 i115i120 A.Niida et al.our approach can identify recurrent genomic aberrations even in unpaired datasets.
2 METHODS 2.1 PART First, we introduce a simple parametric test to find the recurrence of genomic aberrations (e.g.LOH) given an aberration profile matrix.
Let R denote an nm input binary matrix, whose rows and columns index chromosomal n positions and m samples.
If the sample j has a genomic aberration at the position i, we set Rij =1; otherwise, Rij =0.
The problem to be addressed is to statistically test whether a genomic aberration recurrently appears at each position.
Namely, we need to calculate a P-value for a test statistic, which is defined as the count of aberrant samples in each position.
For the position i, it is given by si = m j=1 Rij.
To calculate the P-value, we assume a null model where the aberration at each position appears with an equal probability within each sample, Pr(Rij = 1)=pj , and the probability can be estimated by the aberration ratio of the sample j: pj = 1 n n i=1 Rij.
Note that this null model is equivalent to the one generated by position permutation in each sample, which is the approach taken by GISTIC (Beroukhim et al., 2007).
Under the null model, if the aberration rate is constant across samples, that is p1 =p2 =...=pm, testing for aberration in each sample can be done by using independent Bernoulli trials with the equal success probability; in such a case, the test statistic si , which is the sum of the independent Bernoulli trials, follows the binomial distribution.
However, in real data, the aberration varies across samples: some tumors have more genomic aberrations during oncogenesis than others.
Therefore, for unequal pjs, testing for aberration at a position in each sample must consider independent Bernoulli trials with unequal success probabilities.
In this case, the test statistic si follows a general case of the binomial distribution, the Poisson-binomial distribution (Wang, 1993) with the probabilistic function PB(k =si;p1,p2,...,pm)= GFk jG pj jGc (1pj), where Fk is the set of all subsets of k integers that can be selected from 1,2,...,m. For example, for m=3 and k =2, we have F2 ={{1,2}, {1,3}, {2,3}}.
Gc is the complement of G, that is Gc ={1,2,3,...,n}\G.
Fk will contain m!/(mk)!k!
elements, over which summation is infeasible in practice unless m is small.
However, efficient calculations using a discrete Fourier transformation or recursive formulae have been proposed (Hong, 2011).
Using the Poisson-binomial distribution, the P-value p(si) is calculated by p(si)=1 si k=1 PB(k;p1,p2,...,pm).
To calculate the cumulative distribution function, we use the DFT-CF (the Discrete Fourier Transform of the Characteristic Function) method implemented in the R poibin package (Hong, 2011).
2.2 PART-up In unpaired experimental designs, the aberration profile matrix of tumor samples, R, may contain false aberration calls.
Here, we address the problem of how to test for the recurrence of genomic aberration in the presence of false positive aberration calls.
When the aberration profile matrix of unpaired normal samples from the same cohort is available, the Poisson-binomial approach enables us to test for the recurrence of aberration by denoising the false positive aberration calls.
Let S and T denote nl and nm binary matrices, and be referred to as the false aberration profile matrix and the true aberration profile matrix, respectively.
S was prepared for l unpaired normal samples with the same procedure as that used for R, and T is unobserved data that we can obtain by removing false positive calls from R. We define the probability of aberration at the position i of the sample j as pij =Pr(Rij =1), and use {pi1,pi2,...,pim} as the Poisson-binomial parameters.
Note that we must compute the Poissonbinomial parameters for each position by considering the existence of the false positive aberration calls from normal samples.
Under the assumption that the false positive rate of aberration calls at each position is constant across samples, the false positive rate pFi can be estimated from S: pFi = l j=1 Sij/l.
We also define the probability that the aberration observed at the position i of the sample j is true: pTij =Pr(Tij =1).
As an observed aberration must be either a true aberration or a false positive aberration call, the following equation holds among these probabilities: pij =pTij +(1pTij )pFi.
(1) Under the null model, the probability of true aberrations should be constant within each sample, that is, pTij =pTj.
By noting this, we can take the average of Equation (1) over positions: pj =pTj +(1pTj )pF, (2) where pj = 1n n i=1 pij and pF = 1n n i=1 pFi.
As pj , pF and pFi are available from the data, we can calculate pTij from Equation (2): pTij = pTj = pj pF 1 pF.
(3) By substituting Equation (3) for Equation (1), we obtain: pij = pj (1 p F i ) pF + pFi 1 pF.
Now, we have {pi1,pi2,...,pim}, which are Poisson-binomial parameter estimates adjusted for each position.
Using these parameter estimates, the P-value p(sj) is calculated as described in the previous section.
2.3 Preparation of simulation data Simulation data for benchmark tests were generated partially based on a study by Guttman et al.(2007).
Assuming unpaired experiments, we simulate nm and nl binary matrices, R and S. In our simulation, we assumed three types of aberrations: concordant true, non-concordant true and concordant false positive aberrations.
All types of aberrations exist in R; therefore, we independently generated nm binary matrices, Rc, Rn and Rf.
To obtain R, we combined them using Rij =max{Rcij,Rnij,Rfij}.
These matrices are illustrated in Figure 1.
Only concordant false positive aberrations exist in S. Concordant true aberrations.
Rc contains concordant true aberrations of width wc at row intervals specified by an integer set C, which contains the row indices of aberration centers.
For each interval, aberrations recurrently appear in columns randomly sampled with rate rc.
Note that this type of aberrations should be a target of PART and the positions in the specified concordant intervals are used as actual positives in benchmark tests.
Non-concordant true aberrations.
Each column of Rn has k nj nonconcordant true aberrations of length wn.
For each aberration, we sampled the interval length wn Geometric (1/wn), and the interval position randomly.
The number of aberrations k nj was sampled for each column so that k nj Poisson(kn).
Concordant false positive aberrations.
Intervals of concordant false positive aberrations have width wf and are specified by the concordant row interval set of size kf.
The kf elements were randomly sampled from 1 to n. For each of the concordant intervals, Rf and S have aberrations in columns randomly sampled with rate rf.
i116 Copyedited by: TRJ MANUSCRIPT CATEGORY: [19:59 28/5/2012 Bioinformatics-bts203.tex] Page: i117 i115i120 Evaluate the recurrence of genomic aberrations In our simulation, we fix the parameters as n=1000, m=100, l =300, wc =wn =wf =5 and C ={200,400,600,800}.
For the other parameters, kn, kf , rc and rf , several combinations of parameter values were examined, as described later.
2.4 Preparation of real data We obtain paired and unpaired LOH profile matrices for 294 pairs of colorectal cancer and normal samples.
As a data source, TCGA (The Cancer Genome Atlas) Level 3 SNP data profiled by Affymetrix SNPArray 6.0 were downloaded from the TCGA data portal site (http://tcga-data.nci.nih.gov/ tcga/tcgaHome2.jsp).
To obtain a paired LOH profile matrix, we performed LOH detection for each pair of the samples-based allelic imbalance (Staaf et al., 2008).
For SNPs whose genotypes are called heterozygous in the normal genome, the ratio of allelic copy intensities was calculated as the B allele frequency (BAF) score: B=b/(a+b), where a and b are the copy number intensity for A and B alleles in the cancer genome.
The BAF score should be 0.5 if the position has no allelic imbalance.
We then computed the absolute deviation of the BAF score from 0.5 as the BAF: B =|B0.5|.
BAF was plotted along chromosomes and segmented using the circular binary segmentation algorithm with parameter =0.01 (Venkatraman and Olshen, 2007).
We assumed that chromosomal segments with B >0.1 are subjected to LOH.
To obtain the unpaired LOH profile matrices, we applied the basic Hidden Markov Model method proposed by Beroukhim et al.(2006) to the cancer genome data.
His method uses a hidden Markov model to detect successive LOH while taking into account SNP intermarker distances.
LOH in unpaired samples was also detected by the same procedure.
We also prepared the unpaired aberration profile matrices for the Sanger cell line data.
We obtained Affymetrix SNP Array 6.0 data containing 764 cell lines and 466 unpaired normal samples from the Cancer Genome Project site (http://www.sanger.ac.uk/genetics/CGP/Archive/) (Bignell et al., 2010).
The unpaired LOH profile matrices were obtained in the same way as the TCGA data.
The copy number amplification and deletion profile matrices were obtained by binarizing the copy number profiles predicted by PICNIC (Greenman et al., 2010).
3 RESULTS 3.1 Simulation data test First, we numerically show that our Poisson-binomial approach is statistically equivalent to the permutation approach adopted by other methods.
We prepared an LOH profile matrix from the Sanger cell line data, and compare PART P-values with those based on 10 000 permutations of chromosomal positions for each sample.
The minus log scale P-value plot in Figure 2 shows that they correspond with each other, although the permutation approach has a limitation for the calculation of small P-values.
We also permutated chromosomal positions for each sample, apply PART to the permutated data and plot a histogram of P-values.
Supplementary Materials Figure S1 shows that the histogram is close to uniform distributions, demonstrating that our method successfully controls the rate of false positives.
Next, we numerically compare the performance of PART and PART-up.
We simulated aberration profile matrices; in each simulation, we obtain a pair of true and false positive aberration matrices.
To generate matrices, we assumed three types of aberrations: concordant true non-concordant true and concordant false positive aberrations.
In the true aberration matrix, concordant true aberrations appear recurrently at specific positions whereas nonconcordant true aberrations appear randomly.
Namely, concordant true aberrations mimic drivers targeted by PART whereas nonconcordant true aberrations mimic passengers.
As concordant false positive aberrations mimic polymorphisms which exist in both cancer and normal genomes, they appear recurrently at the same position and frequency in both the true and false positive aberration matrices.
The simulation data were generated from a simulator with four free parameters: kn for the number of non-concordant true aberrations, kf for the number of concordant false positive aberrations, rc for the rate of samples subjected to concordant true aberration, and rf for the rate of samples subjected to concordant false positive aberrations.
A simulation instance is illustrated in Figure 1.
We prepared 16 types of simulators with different parameter settings and obtained 100 matrix pairs from 100 Monte Carlo trials for each simulator.
For each Monte Carlo matrix pair, we applied PART to the true aberration matrices and PART-up to both the matrices.
The result for a Monte Carlo matrix pair is presented in Figure 3.
From the results pooled across the 100 Monte Carlo trials, we calculated precision and recall for each method over the whole range of significance cutoffs to depict precision-recall (PR) curves.
Precision is defined as the proportion of actual in predicted positives, whereas recall is defined as the proportion of predicted in actual positives.
We assumed positions within concordant intervals Fig.1.
Simulation of aberration matrices.
In a Monte Carlo trial, three matrices, Rc, Rn and Rf , were simulated with kn =10, kf =10, rc =0.3 and rf =0.3.
Rc, Rn and Rf contain concordant true, non-concordant true, and concordant false positive aberrations, respectively.
The aberration matrix R is obtained by overlapping the three matrices.
Although the false positive aberration matrix S is not shown here, its simulation is similar to that of Rf i117 Copyedited by: TRJ MANUSCRIPT CATEGORY: [19:59 28/5/2012 Bioinformatics-bts203.tex] Page: i118 i115i120 A.Niida et al.Fig.2.
Comparison of P-values between the Poisson-binomial and permutation approaches.
The P-values for recurrent genomic aberration were obtained by the Poisson-binomial statistics and permutations, and plotted in minus log scale A B Fig.3.
An example of significance plots for simulation data.
PART and PART-up were applied to the simulated data presented in Figure 1.
The Pvalues from PART (A) and PART-up (B) were plotted in minus log scales across chromosomal positions as actual positives and positions determined by a significance cutoff as predicted positives.
The PR curve shows the discriminative ability of each method to find positions associated with true concordant aberrations.
For this type of benchmark tests, the receiver operating characteristic curve is also popular; however, we chose the PR curve because it is preferred for our case where the number of actual positives is relatively small (Davis and Goadrich, 2006).
The heatmap in Figure 4 shows the PR curves for the 16 different parameter settings.
PART-up performs better in the presence of more concordant false positive aberrations (i.e.when the parameters controlling the frequency of concordant false positive aberrations, kf and rf , are larger), while the performance of both methods are attenuated by the presence of non-concordant true aberrations (i.e.when the parameter controlling the frequency of non-concordant Fig.4.
PR curves of PART and PART-up.
PR curves were obtained by applying PART (blue lines) and PART-up (red lines) to simulation data from 16 different parameter settings.
The horizontal axis indicates recall whereas the vertical axis indicates precision true aberrations, kn, is larger).
These results demonstrate that PARTup can successfully discriminate concordant true aberrations from concordant false positive aberrations, suggesting its applicability to genomic data obtained in unpaired experimental designs.
3.2 Real data test In this section, we compare PART and PART-up using real experimental data.
First, we focus on TCGA colorectal cancer SNP data that were obtained in a paired experimental design.
To examine the performance of PART-up, we prepared two types of LOH profile matrices in two different ways: paired and unpaired LOH profile matrices.
The former was authentically obtained based on paired genomes: LOHs were called by comparing genotypes between tumor and paired normal genomes.
The latter was approximately obtained based only on tumor genomes: LOHs were calls for segments that harbor successive homozygous calls in tumor genomes.
An unpaired LOH profile matrix for normal samples was also prepared for PART-up input.
We applied PART to the paired and unpaired matrices, and plot minus log P-value across chromosomes.
The paired LOH profile matrix produces a clear significance plot, whereas the unpaired LOH profile matrix yields a noisy plot with many spikes, as shown in Figure 5A and B.
The spikes would reflect false positive LOH calls for polymorphic homozygous segments that exist in normal genomes.
We also applied PART-up to the same unpaired LOH profile matrix from tumor samples combined with that from normal samples.
Figure 5C shows that PART-up successfully removes most of the spikes and the result corresponds well to that from PART applied to the paired matrix.
This observation demonstrates that, if the aberration profiles for unpaired normal samples are available, PART-up performs as well for unpaired data as PART does for paired data.
i118 Copyedited by: TRJ MANUSCRIPT CATEGORY: [19:59 28/5/2012 Bioinformatics-bts203.tex] Page: i119 i115i120 Evaluate the recurrence of genomic aberrations A B C Fig.5.
Significance plots for recurrent LOH in the TCGA colorectal cancer data.
Minus log scaled P-values for the recurrence of LOH were plotted across chromosomes, each of which is indicated by vertical stripes.
(A) PART was applied to the paired LOH profile matrix.
(B) PART was applied to the unpaired LOH profile matrix from the tumor samples.
(C) PART-up was applied to the unpaired LOH profile matrices from the tumor and normal samples Next, we obtained unpaired LOH profile matrices from a cancer cell line dataset published by the Sanger institute (Bignell et al., 2010).
As paired normal controls are usually unavailable for cell lines, it has been difficult to find recurrent aberrations in cell line data.
However, as the Sanger dataset is accompanied by hundreds of unpaired normal sample data items, it can be subject to PARTup.
We apply PART and PART-up to the unpaired LOH profile matrices and the significance plots are shown in Figure 6A and B.
As in the TCGA case, although PART produces a noisy plot, PART-up produces a much clearer plot, revealing recurrent LOH region in the cancer cell lines.
We also applied PART and PARTup to the unpaired copy number amplification and deletion profile matrices, and these results are shown in Supplementary Materials Figures S2 and S3.
The comparison between the significance plots demonstrates that PART-up removes some spikes, which would be from copy number polymorphisms in normal genomes.
The differences are less dramatic than in the LOH case, reflecting the low rate of pseudoaberrations from normal genomes for copy number aberrations as compared with LOH (see Supplementary Materials Table S1).As such, we conclude that PART-up successfully identifies recurrently aberrant regions from unpaired genomic data.
4 DISCUSSION In this study, we presented a novel statistical method, PART, to test the recurrence of genomic aberration.
Although a A B Fig.6.
Significance plots for recurrent LOH in the Sanger cell line cancer data.
Minus log scaled P-values for the recurrence of LOH were plotted across chromosomes, each of which is indicated by vertical stripes.
(A) PART was applied to the unpaired LOH profile matrix from the tumor samples.
(B) PART-up was applied to the unpaired LOH profile matrices from the tumor and unpaired normal samples number of methods have been developed for similar purposes, most of them take permutation approaches to assess statistical significance.
Conversely, our method takes a novel parametric approach by employing Poisson-binomial statistics.
There are pros and cons between the two approaches.
Our parametric approach needs less computational time and can calculate small P-values more accurately than the permutation approach.
This property is important for genomic analysis, because we usually need to calculate small P-values to correct large-scale multiple hypothesis testing.
Conversely although our approach must take the count of aberrant samples as a test statistic, the permutation approach is flexible for the type of statistic and can enable more biologically plausible tests.
For example, the GISTIC statistic takes into consideration aberration strength in addition to recurrence (Beroukhim et al., 2007).
However, in spite of these differences, we found that PART and GISTIC produce consistent results on the copy number data (See Supplementary Note).
The most notable advantage of our parametric approach is highlighted by the extension of PART to PART-up.
Although it is preferable that genomic aberration profilings are performed in paired experimental designs, it is not always possible to obtain paired normal samples.
However, PART-up is applicable only if data are accompanied with unpaired normal samples from the same cohort, which are generally easier to obtain.
Although previously proposed methods are not able to deal with such unpaired data, the introduction of Poisson-binomial statistics enables us to test recurrent aberrations in unpaired data while considering the rate of pseudo-aberrations that originate from normal genomes.
Although a method has been proposed to call aberrations in individual tumor samples using the pooled unpaired normal sample as a reference (Yamamoto et al., 2007), testing for the recurrence of aberrations in a group of unpaired tumor samples is a novel approach.
It is expected that combining these complementary approaches will reduce false positives and lead to higher performance.
i119 Copyedited by: TRJ MANUSCRIPT CATEGORY: [19:59 28/5/2012 Bioinformatics-bts203.tex] Page: i120 i115i120 A.Niida et al.In this study, we applied our method to genomic aberration profiles obtained by the microarray technology.
The next-generation sequence technology has result in a torrent of cancer genome data (Meyerson et al., 2010).
The current dataset is not only large but also complex, in that the new technology can profile various types of genomic aberrations that cannot be captured by the old technology: point mutations, short indels, translocations, and so forth.
A population-scale sequencing project targeting thousands of normal genomes is also ongoing (1000 Genomes Project Consortium, 2010); data produced by the project would be used as unpaired normal sample controls for PART-up.
Based on this study, future studies would address the application of our Poissonbinomial approach to a wider spectrum of genomic aberrations revealed by the next-generation sequence technology.
ACKNOWLEDGEMENTS Computation time was provided by the Super Computer System, Human Genome Center, Institute of Medical Science, University of Tokyo.
Funding: This work was supported by Research Fellowship for Young Scientists from Japan Society for the Promotion of Science, and Systems Cancer (Project No 4201), a Grant-inAid for Scientific Research on Innovative Areas by the Ministry of Education, Culture, Sports, Science and Technology (MEXT), Japan.
Conflict of Interest: none declared.
ABSTRACT Summary: BigWig and BigBed files are compressed binary indexed files containing data at several resolutions that allow the highperformance display of next-generation sequencing experiment results in the UCSC Genome Browser.
The visualization is implemented using a multi-layered software approach that takes advantage of specific capabilities of web-based protocols and Linux and UNIX operating systems files, R trees and various indexing and compression tricks.
As a result, only the data needed to support the current browser view is transmitted rather than the entire file, enabling fast remote access to large distributed data sets.
Availability and implementation: Binaries for the BigWig and BigBed creation and parsing utilities may be downloaded at http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/.
Source code for the creation and visualization software is freely available for noncommercial use at http://hgdownload.cse.ucsc.edu/admin/jksrc.zip, implemented in C and supported on Linux.
The UCSC Genome Browser is available at http://genome.ucsc.edu Contact: ann@soe.ucsc.edu Supplementary information: Supplementary byte-level details of the BigWig and BigBed file formats are available at Bioinformatics online.
For an in-depth description of UCSC data file formats and custom tracks, see http://genome.ucsc.edu/FAQ/FAQformat.html and http://genome.ucsc.edu/goldenPath/help/hgTracksHelp.html Received on February 18, 2010; revised on June 10, 2010; accepted on June 28, 2010 1 INTRODUCTION Recent improvements in sequencing technologies have made it possible for labs to generate terabyte-sized genomic data sets.
Visualization of these data sets is a key to scientific interpretation.
Typically, loading the data into a visualization tool such as the Genome Browser provided by the University of California, Santa Cruz (UCSC) (Kent et al., 2002; Rhead et al., 2010) has been difficult.
The data can be loaded as a custom annotation track, but for very large data sets the upload form times out before the data transfer finishes.
To work around this limitation, some labs with access to Solexa and later-generation sequencing machines have installed a local copy of the Genome Browser, but this requires a significant initial time investment by systems administrators and other informatics professionals, as well as continuing efforts to keep the data in the local browser installation current.
To whom correspondence should be addressed.
Though visualization of results is just one of the many informatics challenges of next-generation sequencing, it is one that we are well positioned to address at UCSC.
We have developed two new data formats, BigWig and BigBed, that make it practical to view the results of next-generation sequencing experiments as tracks in the UCSC Genome Browser.
The BigWig and BigBed files are compressed binary indexed files that contain the data at several resolutions.
Rather than transmitting the entire file, only the data needed to support the current view in the Genome Browser are transmitted.
Collectively, BigWig and BigBed are referred to as Big Binary Indexed (BBI) files.
2 SYSTEM AND METHODS BigBed files are generated from Browser Extensible Data (BED) files.
Like the BED format, the BigBed format is used for data tables with a varying number of fields.
BED files consist of a simple text format: each line contains the fields for one record, separated by white space.
The first three fields are required, and must contain the chromosome name, start position and end position.
The standard BED format defines nine additional, optional fields, which (if present) must appear in the predefined order (Supplementary Table 1).
Alternatively, BED files may depart from the standard format after the third field, continuing with fields specific to the application and data set.
BigBed files that contain custom fields, unlike those of simple BED format, must also contain the field name and a sentence describing the custom field.
To help others understand custom BED fields, an autoSql (.as) (Kent and Brumbaugh, 2002) declaration of the table format can be included in the BigBed file (Supplementary Table 2).
BigWig files are derived from text-formatted wiggle plot (wig) or bedGraph files.
They associate a floating point number with each base in the genome, and can accommodate missing data points.
In the UCSC Genome Browser, these files are used to create graphs in which the horizontal axis is the position along a chromosome and the vertical axis is the floating point data (Fig.1).
Typically, these graphs are represented by a wiggly line, hence the name wiggle.
Three text formats can be used to describe wiggle data at varying levels of conciseness and flexibility.
Values may be specified for every base or for regularly spaced fixed-sized windows using the fixedStep format.
The variableStep format encodes fixed-sized windows that are variably spaced.
The bedGraph format encodes windows that are both variably sized and variably spaced.
Data files of fixedStep format are divided into sections, each of which starts with a line of the form: fixedStep chrom=chrN start=position step=N span=N where chrom is the chromosome name, start is the start position on the chromosome, step is the number of bases between items and span shows the number of bases covered by each item.
Step and span default to 1 if they are not defined.
This section line is followed by a line containing a single floating point number for each item in the section.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:37 30/7/2010 Bioinformatics-btq351.tex] Page: 2205 22042207 BigWig and BigBed Fig.1.
Genome Browser image of BigWig annotation tracks.
The top track is displayed as a bar graph, the bottom track as a point graph.
Shading is used to distinguish the mean (dark), one standard deviation above the mean (medium) and the maximum (light).
Peaks with clipped tops are colored magenta.
The variableStep format is similar, but the section starts with a line of the format: variableStep chrom=chrN span=N and each item line contains two fields: the chromosome start position, and the floating point value associated with each base.
The bedGraph format is a BED variant in which the fourth column is a floating point value that is associated with all the bases between the chromStart and chromEnd positions.
Unlike the zero-based BED and bedGraph, for compatibility reasons the chromosome start positions in variableStep and fixedStep are one-based.
To create a BigBed or BigWig file, one first creates a text file in BED, fixedStep, variableStep or bedGraph format and then uses the bedToBigBed, wigToBigWig or bedGraphToBigWig command-line utility to convert the file to indexed binary format.
In addition to the text file and (in the case of BigBed) the optional.as file, the conversion utilities require a chrom.sizes input file that describes the chromosome (or contig) sizes in a two-column format (chromosome name, chromosome size).
The fetchChromSizes program may be used to obtain the chrom.sizes file for any genome hosted at UCSC.
All of the command-line utilities can be run without options to display a usage summary.
The wigToBigWig program accepts fixedStep, variableStep or bedGraph input.
The bedGraphToBigWig program accepts only bedGraph files, but has the advantage of using much less memory.
The wigToBigWig program can take up to 1.5 times as much memory as the wig file it is encoding, while bedGraphToBigWig and bedToBigBed use only about one-quarter as much memory as the size of the input file.
Once a BigBed or BigWig file is created, it can be viewed in the UCSC Genome Browser by using the custom track mechanism (Supplementary Material).
In brief the indexed file is put on a website accessible via HTTP, HTTPS or FTP, and a line describing the file type and data location in the form: track type=bigBed bigDataUrl=http://srvr/myData.bb is entered in the custom track section of the browser.
Additional settings in var=value format can be used to control the name, color, and other attributes of the track.
When the custom track is loaded and displayed, the Genome Browser fetches only the data it needs to display at the resolution appropriate for the size of the region being viewed.
While it may take a few minutes to convert the input text file to the indexed format, once this is done there is no need to upload the entire file, and the response time on the browser is nearly as fast as if the file resided on the local UCSC server.
Because the BigWig and BigBed files are binary, we have created additional tools that parse the files and describe the contents.
The bigWigSummary and bigBedSummary programs can quickly compute summaries of large sections of the files corresponding to zoomed-out views in the Genome Browser.
The bigWigInfo and bigBedInfo can be used to quickly check the version numbers, compression status and data ranges stored in a file.
The bigBedToBed, bigWigToWig and bigWigToBedGraph programs can convert all or just a portion of files back to text format.
3 IMPLEMENTATION The BigBed and BigWig readers and writers are written in portable C; other programs that can interface with C libraries can make use of the code directly.
For those working in languages that do not interface well with C, the Supplemental Information describes the file format in sufficient detail to reimplement it in another language.
Several layers of software are involved in enabling the remote access of the BigBed and BigWig files.
This section describes the software architecture, algorithms and data structures at a high level, and should be useful to anyone trying to understand the code enough to usefully modify it or to implement similar file formats that work well in a distributed data environment.
3.1 Data transfer layer Though BigBed and BigWig can be used locally, the primary design goal for this format was to enable efficient remote access.
This is done using existing web-based protocols that are generally already available at most sites.
Unlike typical web use, bigBed and bigWig files require random access.
At the lowest layer, we take advantage of the byte-range protocols of HTTP and HTTPS, and the protocols associated with resuming interrupted FTP transfers, to achieve random access to binary files over the web.
Web servers supporting HTTP/1.1 accept byte-ranges when the data is nonvolatile.
OpenSSL provides SSL support for HTTPS via the BIO protocol.
FTP uses the resume command and simply closes the connection when sufficient data has been read.
2205 [15:37 30/7/2010 Bioinformatics-btq351.tex] Page: 2206 22042207 W.J.Kent et al.3.2 URL data cache layer Since remote access is still slow compared to local access, and data files typically are viewed many times without changing, we implemented a cache layer on top of the data transfer layer.
Data are fetched in blocks of 8 Kb, and each block is kept in a cache.
The cache is implemented using two files for each file that is cached: a bitmap file that has a bit set for each file block in cache and a data file that contains the actual blocks of data.
The data file is implemented very simply using the sparse file feature of Linux and most other UNIX-like operating systems.
The cache software simply seeks to the position in the file where the block belongs and writes it.
The operating system allocates disk space only for the parts of the file that are actually written.
The cache layer is critical to performance.
Parts of the file, including the file header and the root block of the index, are accessed no matter what part of the genome is being viewed.
These parts need be transmitted only once.
In addition if multiple users view the same region of the genome, later users will benefit from the cache, as will a single user looking at the same region multiple times.
Though a cache can help convert remote access to local access, a minimum of one remote accessto check whether the file has changed at the remote siteis required even on a completely cached file.
Minimizing the number of cache checks is one of the motivations for keeping the index and the zoomed data in the same file as the primary data.
Even though a change check involves little in the way of data transfer, it does require a round trip on the network, which can take from 10 to 1000 ms depending on the network connectivity.
For similar reasons, though data are always fetched at least one full block at a time, the system will combine multiple blocks into a single fetch operation whenever possible.
3.3 Indexing The next layer handles the indexing.
It is based on a single dimensional version of the R tree that is commonly used for indexing geographical data.
The index size is typically less than 1% of the size of the data itself.
A BigBed file can contain overlapping intervals.
Overlapping intervals are not as easy to index as strings, points or nonoverlapping intervals, but several effective techniques do exist, including binning schemes (Kent et al., 2002), nested containment lists (Alekseyenko and Lee, 2007) and R trees (Guttman, 1984).
R trees have several properties that make them attractive for this application.
They perform well for data at a variety of scales in contrast to binning schemes that typically have a sweet spot at a particular scale of data close to the smallest bin size.
R trees also minimize the number of seeks (and hence network roundtrips) compared to nested containment lists, another popular genomics indexing scheme.
The basic idea behind an R tree is fairly simple.
Each node of the tree can point to multiple child nodes.
The area spanned by a child node is stored alongside the child pointer.
The reader starts with the root node, and descends into all nodes that overlap the query window.
Since most child nodes do not overlap, only a few branches of the tree need to be explored for a typical query.
Though a separate R tree for each chromosome would have been simpler to implement, we elected to use a single tree in which the comparison operator includes both the chromosome and the position.
This allows better performance on roughly assembled genomes with hundreds or thousands of scaffolds, and also lets the files be applied to RNA as well as DNA databases.
To improve the efficiency of the single R tree, we store the chromosome ID as an integer rather than a name, and include a B+ tree to associate chromosome names and IDs in the file.
In the source code, the combined B+ tree and R tree index is referred to as a cirTree.
One additional indexing trick is used.
Because the stored data are sorted by chromosome and start position, not every item in the file must be indexed; in fact by default only every 512th item is indexed.
The software finds the closest indexed item preceding the query, and then scans through the data, discarding some of the initial items if necessary.
This may seem wasteful, since hundreds of thousands of bytes may be transferred in the same time that it takes to seek to a new position on disk, but in practice little time is lost and as a benefit the index is less than 1% of the size of the data.
3.4 Compression The data regions of the file (but not the index) are compressed using the same deflate techniques that are used in gzip as implemented in the zlib library, a very widespread, stable and fast library built into most Linux and UNIX installations.
The compression would not be very efficient if each item was compressed separately, and it would not support random access if the entire data area were compressed all at once.
Instead the regions between indexed items (containing 512 items by default) are individually compressed.
This maintains the same degree of random accessibility that was enabled by the sparse R tree index while still achieving nearly the same level of compression as compressing the entire file would.
The final layer of software is responsible for fetching and decoding blocks specified by the index.
It is only this final layer that differs between BigWig and BigBed.
4 RESULTS AND DISCUSSION The BigBed and BigWig files succeed in overcoming browser upload timeout limits.
By deferring the bulk of the data transfer to be on demand, the upload phase of BigWig and BigBed files now takes less than a second even on home and remote networks, well within the 300-s upload time limit at UCSC.
The on-demand connectivity requirements are modest, adding 0.51.0 s of data transfer time overhead depending on where the Big file is hosted (Supplementary Table 3).
BigBed and BigWig files are similar in many ways to BAM files (Li et al., 2009), which are commonly used to store mappings of short reads to the genome.
BAM files are also binary, compressed, indexed versions of an existing text format, SAM.
The samtools C library associated with SAM and BAM (http://samtools.sourceforge.net/) caches the BAM index, though not the data files.
Samtools also can fetch data from the internet via FTP and HTTP, but not HTTPS.
BAM files are not designed for wiggle graphs, and are more complex than BED files, but they do store alignment, sequence and sequence quality information very efficiently.
While this capability theoretically could be added as an extension to BigBed, we have adopted BAM for short read mapping to avoid a proliferation of formats.
BAM files are supported as custom tracks at UCSC, and we have added HTTPS support to BAM using the data transfer and data cache layers developed for BigBed and BigWig.
2206 [15:37 30/7/2010 Bioinformatics-btq351.tex] Page: 2207 22042207 BigWig and BigBed BigBed and BigWig files have been in use at genome.ucsc.edu since June 2009, and have proven to be popular.
As of February 2010, we have displayed data from nearly 1300 files using these formats.
The broader bioinformatics community has started to support these files as well, with Perl bindings available at http://search.cpan.org/lds/Bio-BigFile/ and a Java implementation in progress (Martin Deacutis, personal communication) for use in the Integrative Genome Viewer (http://www.broadinstitute.org/igv/).
Though the use of BigBed and BigWig requires access to the command line creation tools needed to create the files and a website or FTP site on which to place them, this is not an undue burden in the context of the informatics demands of a modern sequencing pipeline, and is clearly preferable to the long and uncertain uploads of large custom tracks in text formats.
ACKNOWLEDGEMENTS We would like to acknowledge James Taylor, Heng Li and Martin Deacutis for their testing and feedback on these formats, and Lincoln Stein for developing the Perl bindings.
Funding: This work was supported by the National Human Genome Research Institute (5P41HG002371-09, 5U41HG004568-02).
The open access charge was funded by the Howard Hughes Medical Institute.
Conflict of Interest: none declared.
ABSTRACT Motivation: Several computational methods have been developed to identify cancer drivers genesgenes responsible for cancer development upon specific alterations.
These alterations can cause the loss of function (LoF) of the gene product, for instance, in tumor suppressors, or increase or change its activity or function, if it is an oncogene.
Distinguishing between these two classes is important to understand tumorigenesis in patients and has implications for therapy decision making.
Here, we assess the capacity of multiple gene features related to the pattern of genomic alterations across tumors to distinguish between activating and LoF cancer genes, and we present an automated approach to aid the classification of novel cancer drivers according to their role.
Result: OncodriveROLE is a machine learning-based approach that classifies driver genes according to their role, using several properties related to the pattern of alterations across tumors.
The method shows an accuracy of 0.93 and Matthews correlation coefficient of 0.84 classifying genes in the Cancer Gene Census.
The OncodriveROLE classifier, its results when applied to two lists of predicted cancer drivers and TCGA-derived mutation and copy number features used by the classifier are available at http://bg.upf.edu/oncodrive-role.
Availability and implementation: The R implementation of the OncodriveROLE classifier is available at http://bg.upf.edu/oncodriverole.
Contact: abel.gonzalez@upf.edu or nuria.lopez@upf.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Research in cancer genomics has identified hundreds of genes involved in different stages of tumorigenesis due to specific somatic events.
Single nucleotide variants, and large-scale amplifications and deletions of chromosomal regions have been identified as two of the main driver alterations in human tumors.
The genes suffering these alterations are traditionally classified as oncogenes and tumor suppressors, depending on their role in cancer development.
When the product of tumor suppressors lose their function, tumor cells tend to proliferate faster.
Driver alterations in these genes frequently exhibit a recessive behavior.
The loss of function (LoF) can be achieved through truncating or missense mutations, DNA deletions or hypermethylation of their promoters.
Some known LoF genes, most notably BRCA1 and BRCA2, carry germline variants that increase the susceptibility to develop a tumor because only one hit is required to inactivate their function.
Oncogenes, on the other hand, increase or change their function upon somatic variants in tumorigenesis.
Therefore, theirmode of action follow a dominant pattern, as one faulty copy of the gene is frequently enough to provide the required phenotype.
A copy number gain may exponentiate the oncogenic function of the gene; a point mutation may achieve the same result by changing key amino acid residues, which results in constitutive activation of the protein, or produce a new biochemical function.
These special cases are also regarded as activating driver mutations, as the new function is gained much like in the case of classic oncogenes.
TheCancerGeneCensus (CGC;Futreal et al., 2004) is a regularly updated compilation of well-studied cancer genes, which classifies their mode of action as dominant or recessive, following the oncogene/tumor suppressor paradigm, LoF and Act (activated), hereafter.
The CGC contains some 500 genes implicated in cancer (November 2013).
This is a rather small fraction of the 20 000 genomes in the human genome (International HumanGenome Sequencing Consortium, 2004), but recent largescale re-sequencing projects of tumor genomes (Hudson et al., 2010) suggest many additional genes may be involved in tumorigenesis.
One important first step in the analysis of datasets of cancer genomics alterations is the identification of the genes that drive tumorigenesis.
This is a non-trivial problem because tumor samples contain up to thousands of somatic alterations.
The list of genes altered in tumors is heterogeneous, even within the same cancer type.
Therefore, the difficult task is to distinguish between driver and passenger alterations.
The most intuitive way to identify driver genes is to detect signals of positive selection across tumor samples because cancer cell populations undergo a selection process during the progression of the disease.
Different methods that aim to identify driver genes tackle different evidences to achieve their goal (Gonzalez-Perez et al., 2013a).
Two recent efforts to comprehensively identify driver genes across large cohorts carried out by Lawrence et al.(2014) and Tamborero et al.(2013b), combining several signals of positive selection (Dees et al., 2012; Gonzalez-Perez and LopezBigas, 2012; Lawrence et al., 2013; Reimand et al., 2013) detected, respectively, 291 and 260 likely driver genes.
Although years of experimental work have revealed the role of most well-known cancer genes, now our capability of detecting drivers has surpassed our capacity to probe their mode of action.
Thus, revealing the mode of action of driver genes in tumorigenesis is becoming crucial to fully understand the mechanisms of tumorigenesis.
This is essential for the development of new targeted cancer therapies because as a general rule only Act drivers are in principle susceptible to targeted drugs.
Although exceptionally, some mutated tumor suppressors may be targeted (e.g.*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Nucleotide Variants (SNVs) large , since ) (Loss of function).
, The , therefore , While , XPath error Undefined namespace prefix Lambert et al., 2009), other strategies, such as synthetic lethality, are needed to compensate for their LoF.
This is the reason why we need to develop bioinformatics approaches to make this classification as accurately as possible.
Vogelstein et al.recently described the so-called 20/20 rule to detect tumor suppressor genes and oncogenes based on their mutational pattern across tumor samples (Vogelstein et al., 2013).
It states that genes with 20% truncating mutations are tumor suppressors, whereas genes with420% of missense mutations in recurrent positions are oncogenes.
While it correctly detects and classifies most of the well-known cancer genes, the rule fails to identify drivers included in newer catalogs (Tamborero et al., 2013b), mostly the lowly recurrent ones.
Building upon the same idea, Davoli et al.developed a machine learning approach to directly identify tumor suppressor genes and oncogenes from the somatic alterations observed across cohorts of tumor samples through their mutational and copy number patterns.
Many cancer drivers are recognized correctly by carefully selected features (Davoli et al., 2013).
We recently proposed a strategy to obtain a comprehensive list of drivers minimizing the probability of detecting false-positive findings by combining complementary methods that detected different signals of positive selection (Tamborero et al., 2013b).
Once a list of high-confidence drivers (HCDs) is obtained, it is important to classify those in their mode of action.
To this aim, we first carefully assessed the capability of 30 features to differentiate between these two groups of cancer genes.
Then, we combined different sets of features with various classification algorithms to create several automated classifiers.
We trained these classifiers with CGC genes, and after careful check of their performance, we selected a random forest algorithm that achieves an accuracy (ACC) of 93%, which we call OncodriveROLE.
It is the first freely available automatic classifier that undertakes the task of assessing the mode of action of driver genes.
Used in this setting, it may shed light upon the mechanisms of tumorigenesis in major cancer types.
We have used it to classify the two previously mentioned lists of mutational drivers that have been recently published, namely, HCDs (Tamborero et al., 2013b) and Cancer5000 (Lawrence et al., 2014), and describe the results of this analysis.
2 METHODS 2.1 Mutation data, copy number alteration data and cancer driver lists We retrieved data for the 17 TCGA (The Cancer Gene Census) projects currently available without restriction: BLCA, BRCA, COAD/READ, GBM, HNSC, KIRC, LAML, LGG, LUAD, LUSC, OV, PRAD, SKCM, STAD, THCA and UCEC.
We designed and computed several features that we hypothesized might be useful to classify driver genes according to the role using mutation and copy number data.
These features are based on the patterns of mutations and copy number alterations (CNAs) across tumor samples.
Tumors with at least one mutation in the TCGA pan-cancer 17 dataset available at Synapse (syn1729383.2) were retrieved after excluding those considered as hypermutators (Kandoth, 2014; Kandoth et al., 2013).
Hypermutators of a tumor type contained more than (Q3+4.5 IQR) somatic mutations, where Q3 and IQR are the third quartile and the interquartile range of the distribution of mutations across all samples of the tumor type, respectively.
After filtering, the pan-cancer 17 dataset was composed of 4327 samples.
These mutations were mapped to protein positions, and their consequence types were assessed using the IntOGen-mutations pipeline (Gonzalez-Perez et al., 2013b), which makes use of the Ensembl Variant Effect Predictor (v70; Chen et al., 2010).
The CNA status for all probed genes was downloaded from the January run of the TCGA FIREHOSE pipeline at the Broad Institute (http://gdac.broadinstitute.org/).
To apply the OncodriveROLE classifier, we gathered two lists of likely cancer drivers from the Supplementary Material of two independent papers (Lawrence et al., 2014; Tamborero et al., 2013b).
From the Tamborero et al.(2013b), we selected the list of 291 genes annotated as HCDs, discarding one non-coding gene.
From Lawrence et al.(2013), we obtained a list of 260 genes from the spreadsheet Individual q-values.
For comparison purposes, we retrieved the classifications of genes carried out by the previous work by Davoli et al.from the Supplementary Material of their paper, applying the same cutoffs described in the manuscript (Davoli et al., 2013).
We also obtained the classification carried out by applying the 20/20 rule (Vogelstein et al., 2013) to the mutational dataset of 17 tumors types.
Whenever possible, data were obtained associated to Ensembl gene identifiers (Flicek et al., 2013).
Other identifiers have been mapped to Ensembl gene identifiers with a dataset obtained from Ensembl v70.
2.2 Classifiers We chose six different classifiers to test: cforest.party (cforest method in R), conditionalTree (ctree), logisticRegression (glm), naiveBayes (train), simpleTree (rpart) and randomForest (Breiman, 2001; Hothorn et al., 2006; Kuhn, 2008; Olshen et al., 1984; R Core Team, 2013).
Some classifiers either do not accept missing values or perform variable imputation for those.
Therefore, we opted to remove genes if they had missing values in one or more of the features and leave them unclassified.
From each classifier we obtained a score of the certainty that each gene belongs to the Act class.
2.3 Training set To use cancer genes with well-established roles as training set, we downloaded the material available at the CGC in November 2013 (Futreal et al., 2004).
See below details on the curation of this dataset for training the classifier.
The CGC contains extensive and manually annotated information on well-known cancer genes and classifies the cancer genes into dominant (Dom) and recessive (Rec) influence on tumorigenesis.
We have used the CGC classification into Rec and Dom classes as proxy for LoF and Act genes.
Genes with ambiguous annotation, such as Rec?
or Dom?
or not citing observed somatic mutations were discarded, leaving 381 entries (see Supplementary Table S7 for their classification).
To only include CGC driver genes, which are likely to act across the TCGA pan-cancer 17 cohort, we used a one-signal filter: we discarded genes not detected as significant by MutSigCV (recurrence signal), OncodriveFM (mutations impact signal) or OncodriveCLUST (mutations clustering signal).
We also rejected genes with512 protein affecting mutations (PAMs; Gonzalez-Perez and Lopez-Bigas, 2012; Lawrence et al., 2013; Tamborero et al., 2013a).
Only 115 CGC genes passed this filter.
Equally, all CGC genes that were solely associated to translocation eventsall labeled with Domwere not allowed in the training set, finally leaving 76 entries in the training set.
2.4 Computing features All features we computed are listed in Table 1 along with a brief explanation of their computation: some of them are similar to the ones used previously (Davoli et al., 2013; Vogelstein et al., 2013).
Truncating mutations include mutations causing a frameshift, a gained or lost stop codon as well as mutations in splice donor or acceptor sites.
PAMs include truncating mutations and missense mutations.
Benign missense refers to missense mutations that i550 M.P.Schroeder et al.so `` '' or more more than false a total of , &amp; , ; Kandoth, 2014 * employing VEP, )In order t supplementary High-confidence driver `` ''; Hothorn etal., 2006 , In order t Cancer Gene Census ( ) `` '' `` '' See In order t We either ; ; less than    , Protein affecting mutations ( ) are categorized as low or unknown functional impact by TransFIC (Gonzalez-Perez et al., 2012).
OncodriveFM P-values (Gonzalez-Perez and Lopez-Bigas, 2012) and the location of OncodriveCLUST clusters of mutations (Tamborero et al., 2013a) for all driver genes were obtained by running the IntOGen-mutations pipeline on the TCGA pan-cancer 17 dataset.
The R implementation of Wilcoxons signed rank (R Core Team, 2013) was used to compare the distribution of each feature between the CGC Rec and CGC Dom genes.
We also used the variable importance function from the party library (Hothorn et al., 2006; Strobl et al., 2008) to rank features for their selection to be taken into account by the classifiers.
2.5 Training and prediction The selected CGC genes were therefore used as training set of the classifiers.
With all different classification settings, we performed a leave-one-out crossvalidation: each item in the training set is classified with amodel built with the rest of the training set items.
We found three genes whose initial classification extremely contradicted their CGC category: NOTCH1, NPM1 and CEBPA genes,which have evidence in the literature for a dual role (Halmos et al., 2002; Sportoletti et al., 2008; Vogelstein et al., 2013).
Therefore, we decided to discard them from the training set.
Thus, the final, trimmed CGC training set included 28 Dom and 45 Rec genes.
For the classification of HCD and Cancer5000 genes, we considered that values between 0.7 and 1 as Act and those with values between 0 and 0.3 as LoF.
We computed the ACC and MCC (Matthews correlation coefficient) of each classifier at the leave-one-out cross-validation of the training set.
Furthermore, we calculated the coverage (COV) of the classifier, which reflects the percentage of the entire training set for which a prediction could be made.
3 RESULTS 3.1 Identifying features that differentiate Act from LoF driver genes We tested 30 features that we initially hypothesized could be used to characterize and discriminate between LoF and Act drivers Table 1.
List of mutational and CNA features for cancer driver genes Attribute name Description CNA_cbs_countGain # samples in cohort with CBS value41.1 CNA_cbs_countLoss # samples in cohort with CBS value51.1 CNA_cbs_logratio_GvL Log10-ratio of countGain VS countLoss CNA_gain_freq # samples in cohort with CBS value41.1/ cohort size CNA_loss_freq # samples in cohort with CBS value51.1/ cohort size MUTS_clusters_miss_VS_pam Log10-ratio of missense VS PAM within OncodriveCLUST peaks MUTS_freq_clustered # of mutations in OncodriveCLUST peaks/ # of samples with gene mutated MUTS_freq_disruptive # of samples with truncating mutations or high impact missense/ # of samples having gene mutations MUTS_freq_missH # of high impact missense mutations not in OncodriveCLUST peaks/ # samples with gene mutated MUTS_freq_missHM # of high and medium impact missense mutations not in OncodriveCLUST peaks/ # samples with gene mutated MUTS_freq_truncating # of samples with truncating mutations/ # of samples with at least one mutation MUTS_missense_clustercov # missense mutations in OncodriveCLUST peaks/ # missense mutations/ # amino acids covered by peaks MUTS_missense_mutrec # recurrent missense mutations/ # high and medium impact missense mutations MUTS_missense_rec_freq # recurrent missense mutations/ # mutations (as in Vogelstein et al.)
MUTS_missense_recHM # samples with high and medium impact recurrent missense mutations/ # samples with missense mutations MUTS_OncoFM_pvalue OncodriveFM P-value MUTS_pams_count # samples with PAM MUTS_pams_freq # samples with PAM/ # samples with gene mutations MUTS_pams_ratio # samples with PAM VS # samples with no PAM MUTS_pamsrec_freq # samples with PAM VS # of samples with gene mutation MUTS_trunc_count # samples with truncating mutations MUTS_trunc_freq_cohort # of truncating mutations/ # of samples with gene mutations MUTS_trunc_mutfreq # truncating mutations/ # mutations (as in Vogelstein et al.)
MUTS_trunc_vs_missbenign_ratio # samples with truncating mutations VS # samples with benign missense mutations MUTS_trunc_vs_missense_ratio # samples with truncating mutations VS # samples with missense mutations MUTS_trunc_vs_notrunc_ratio # samples with truncating mutations VS # samples without truncating mutations MUTS_tuson_missHM_missbenign_ratio # samples with high and medium impact mutations VS # samples with benign missense mutations (as described in Davoli et al.)
MUTS_tuson_splicing_missbenign_ratio # samples splicing variants mutations VS # samples with benign missense mutations (as described in Davoli et al.)
MUTS_tuson_trunc_missbenign_ratio # samples with truncating (excluding splicing variants) mutations VS # samples with benign missense mutations (as described in Davoli et al.)
Note: List of features initially created for characterizing LoF and Act genes.
The description reflects the formula applied for the calculation of the features.
All features elaborated describe either mutation or CNA characteristics.
Abbreviations used in the descriptions are: # (number sign): Count/number of,/ (slash): divided by, CBS : circular binary segmentation, truncating mutations: frameshift, stop gained and lost, splice donor and acceptor, missense: all missense mutations and insertions and deletions not altering the reading frame, high and medium impact mutations: all missense mutations with and TransFIC impact of 1 and 2 , benign missense: all missense with low or unknown TransFIC impact, PAM : protein affecting: frameshift, stop gained and lost, splice donor and acceptor, missense, (gene) mutations: all mutations-affecting coding sequence, VS : versusa ratio has been obtained.
i551 OncodriveROLE classifies cancer driver genes p cross 3 We t accuracy ( ) cross  (see Table 1 for detailed description of each).
All features elaborate on somatic mutation and CNA patterns across data from the pan-cancer 17 cohort.
We expected LoF genes to be affected more frequently by deleterious events such as CNA loss and truncating mutations.
Act genes should be more frequently amplified and receive protein-affecting non-truncating mutations, which may increase and/or alter the protein function.
To select the most informative features for the task of distinguishing between Act and LoF genes, we compared the distribution of the features in both categories of CGC genes (Fig.1).
The features we considered can be divided into four broad categories (Fig.1A): (i) features that measure the relative abundance of truncating mutations, (ii) features that reflect the CNA status of the gene across tumors, (iii) features that account for the relative abundance of PAMs and (iv) features that measure the degree of clustering of missense mutations along the protein sequence.
Features in Group iii show the poorest performance to discriminate between CGC Dom and CGC Rec genes (light blue in Fig.1A).
On the other hand, all the features in Group i (green in Fig.1A) rank at the top of performance of all features analyzed.
As expected, this reflects that Act genes (or proto-oncogenes) are intolerant to truncating mutations because an active protein product is required for tumorigenesis.
In LoF (or tumor suppressor) genes the truncation of the protein product gene is positively selected, which facilitates the identification of LoF candidates.
The best performing feature in this group was the ratio of truncating mutations to the total number of coding mutations in the protein (Fig.1B).
The distribution of mutations within the gene (Group iv, dark blue in Fig.1A) differs significantly between CGC Dom and CGC Rec genes.
The CGC Dom genes have fewer mutational hotspots, detected as clusters by OncodriveCLUST, than CGC Rec genes, whose mutations tend to be more evenly distributed (Supplementary Fig.S1) along the protein sequence.
This is probably because Act driver genes receive mutations that potentiate their function, e.g.by constitutively activating a regulatory site, or cause a switch of the protein function.
To achieve such behavior through mutations, these must occur at specific places in the sequence, which results in fewer numbers of recurrent sites (clusters) than in CGC Rec genes (Supplementary Fig.S1).
We elaborated a series of features based on impact, frequency and clustering of missense mutations.
Many did not show any power of discrimination of CGC Rec and Dom.
The features that perform reasonably well are based on the recurrence of missense mutations.
The best-performing feature in this group compares the ratio of missense mutations with total number of PAMs within OncodriveCLUST peaks (MUTS_ clusters_miss_VS_PAM; Fig.1).
Another feature in this group that performs relatively well is the ratio recurrent missense mutations (MUTS_missense_rec_freq).
All features in Group ii are designed to capture the known fact that LoF genes have a tendency to be deleted, whereas Act genes are more frequently affected by amplifications (Davoli et al., 2013).
In this case, we found that the ratio of amplifications to deletions across all tumors in the cohort achieved the best separation of the two groups of genes.
3.2 Developing a classifier to differentiate between LoF drivers and Act drivers Thereafter, we created a feature set that contained non-redundant best-performing features from Groups i, ii and iv, disregarding those of Group iii because of their poor performance resulting in three features: MUTS_trunc_mutfreq, MUTS_clusters_miss_VS_PAM and CNA_cbs_logratio_GvL.
We tested six machine learning approaches trained with the trimmed version of the CGC (see Section 2).
For each gene, the classifiers produced a score of the likelihood that it belonged to the CGC Dom class.
A score of value 0 means that the classifier regards the gene as an LoF beyond all doubt, whereas a score of value 1 means it exactly resembles the model of an Act gene.
We assessed the performance of each classifier through the ACC, the MCC and the COV of the driver set (all listed in Supplementary Table S1).
ACC and MCC validate the performance of the classifiers on the 76 CGC driver genes by means of a leave-one-out cross-validation approach.
We computed these values for different classification probabilities thresholds to select the cutoff that maximize the ACC and MCC, even at the cost of reducing the COV.
Then, we used these sets of values to choose the classifier with the best performance and a reasonable COV.
Overall, randomForest produced the best results Fig.1.
A) The list of features ordered by MannWhitneyWilcoxon rank sum test P-value significance.
Features dependant on truncating mutations are the best discriminators for LoF and Act genes.
Features described in (B) are marked with asterisk.
A detailed explanation of each feature can be found in Table 1.
(B) Box plots comparing the distribution of the three non-redundant top-ranking features that have been selected for the OncodriveROLE classifier in CGC genes annotated as Dom and Rec i552 M.P.Schroeder et al.See , In order t a ; b ; c ; d group c group a , group dIn order t fewer numbern't best to b g groups a b d group c due to Methods while accuracy ( ) Matthew's Correlation Coefficient ( ) coverage ( )cross in order employed coverage (Supplementary Table S1).
We also trained classifiers with different combinations of the three selected features and included MUTS_missense_rec_freq feature for testing purposes.
We found that multiple combinations of these features perform similarly (Supplementary Table S2 and Supplemental Text).
We decided to use the randomForest classifier trained with the three non-redundant features shown in Figure 1B to create OncodriveROLE, under the rationale that features representing the three independent groups could provide more information to classify novel drivers.
The method shows an ACC of 0.94, MCC of 0.84 and COV of 88% in the leave-one-out cross-validation.
We further tested OncodriveROLE in an independent set of tumor suppressor genes (Zhao et al., 2013) that are not present in the CGC.
OncodriveROLE accurately classified 91.7% of those genes as LoF drivers (Supplemental Text).
3.3 Applying OncodriveROLE to lists of cancer driver genes We identified two recent studies in which identified novel cancer driver genes could be classified with OncodriveROLE.
The first study detected cancer drivers by integrating four methods that assess different signals of positive selection across samples of the pan-cancer 12 dataset.
This analysis resulted in 291 high-confidence cancer drivers (Tamborero et al., 2013b).
In the second study, MutsigCV was applied in a cohort of about 5000 tumor samples to obtain a cancer driver list composed of 260 genes (Lawrence et al., 2013, 2014).
The two lists will be referred to as HCD and Cancer5000 further on.
Even though both lists have similar sizes, their overlap is only 50%, making the two gene sets different as can be seen in Figure 2.
As for the training set, we applied the one-signal filter to only predict the role of genes possibly acting as drivers in the dataset under evaluation resulting in 200 HCD and 144 Cancer5000 genes.
The overall distribution of probabilities of these two groups of genes is roughly bimodal in both driver lists, which allowed us to choose these symmetric cutoff values (Fig.2 and Supplementary Fig.S2) such as 0.3 and 0.7 for LoF and Act genes, respectively.
Other cutoffs may be used for the datasets under analysis depending on how strict a classification the user wants for their list of cancer drivers.
Interestingly, we classified three CGC Dom genes as LoF (Dom?
in Fig.2).
The genes in question are NOTCH1, NPM1 and CEBPA.
All three have been implicated in leukemia (Cancer Genome Atlas Research Network, 2013; Liu et al., 2013; Ohlsson et al., 2014) and both NOTCH1 and NPM1 are annotated in the CGC as partners of translocation events in leukemia.
NOTCH1 has been described as an oncogene as well as a tumor suppressor.
Its actual rolemay dependon the tumor type (Licciulli et al., 2013; Liu et al., 2013; Vogelstein et al., 2013).
Equally, CEBPAandNPM1have been characterized as tumor suppressors in the literature (Halmos et al., 2002; Sportoletti et al., 2008).
We cannot be certain of the functional impact of the translocation on the function of the product of the fused gene.
It may associate to a new promoter and change its expression accordingly, or it may be truncated as a result of the fusion and thus function as an LoF.
For this reason, we had previously excluded all CGC Dom genes that are solely associated to translocation events in the Census.
The plot inFigure 2 shows those genes labeled asDomT, and their classification shows no clear resemblance to LoF or Act, which supports our decision to remove them from the training set.
3.4 Comparison of OncodriveROLE with other bioinformatics approaches The 20-20 rule was created to identify mutational driver genes, both oncogenes and tumor suppressor genes (Vogelstein et al., 2013).
Therefore, it differs from OncodriveROLE, designed to classify previously identified driver genes into their most probable roles.
The simple 20-20 rule reaches a high ACC (Table 2) when applied to the trimmed CGC list.
However, it is unable to reach a decision on many drivers where none of its two estimators (see Section 2) surpasses the threshold of 20% (Tables 2 and 3).
We also compared the results obtained by the approach designed by Davoli et al.(2013), implemented in a classifier named Tuson.
As with the 20-20 rule, Tuson was created to distinguish oncogenes and tumor suppressor genes from genes with passenger mutations, instead of classifying previously identified cancer drivers as is the case of OncodriveROLE.
We found OncodriveROLE slightly outperforms Tuson in ACC and MCC on the trimmed CGC dataset.
Note that Tuson method was trained with CGC genes, and the performance reported in Table 2 does not remove genes in the training set, as it is done in the leave-one-out cross-validation of OncodriveROLE.
We can conclude that well-known cancer genes are classified with a high Fig.2.
Classification of 200 (HCD list) and 144 (Cancer5000 list) cancer driver genes into the classes Act and LoF.
The training set of OncodriveROLE constitutes of all Dom and Rec labeled data points.
Dom?
are CGC-annotated dominant genes excluded from the training set because of strong resemblance to the Rec genes and previous literature evidence of this role.
DomT genes are CGC-annotated dominant genes only citing translocation events as prove and therefore not included in the training set.
All-labeled data points are driver genes not annotated in CGC, and whose prediction was the main goal of the study.
The thresholds are drawn at 0.3 (as top limit of the LoF class) and 0.7 (as bottom limit of the Act class).
Working with classification score thresholds of 0.3 (as top limit of the LoF class) and 0.7 (as bottom limit of the Act class), we classified 109 genes as LoF, 76 as Activating and left 15 genes as unclassified in the HCD list; meanwhile, we classified 97 genes as LoF, 43 as Activating and left 4 genes as unclassified (Fig.2) in the Cancer5000 list.
Genes for which we have observed 512 mutations were directly classified as No class and assigned NA values in the classifications results (see Supplementary Tables S4 and S6) i553 OncodriveROLE classifies cancer driver genes cross high of ( high confidence drivers) quite in order Leukemias s Licciulli etal., 2013 n It t/ accuracy methods well ACC with all approaches.
The main difference between the three approaches lies in the COV that can be reached when predicting the role of novel cancer drivers in tumorigenesis.
4 DISCUSSION Two main rationales to detect LoF and Act driver genes acting across tumor samples exist.
The first approach consists in directly detecting genes that exhibit known alterations patterns corresponding to these two classes from mutations and CNA data.
This strategy was first conceptualized by Vogelstein et al.(2013) to be implemented later on as a machine learning algorithm by Davoli et al.(2013).
In the second approach, first driver genes acting in tumor samples are detected by combining the signals of positive selection they exhibit (Lawrence et al., 2014; Tamborero et al., 2013b).
Then, in a second step, these drivers are classified into the two aforementioned classes exploiting similar alteration patterns as in the first approach.
This second two-step approach has two main advantages.
First, genes that do not exhibit clear alterations pattern that define them as LoF or Act can still be detected as drivers if they show clear signals of positive selection.
Second, the combination of several signals controls the ratio of false-positive drivers identified (Tamborero et al., 2013b), which is unattainable to the direct classification of genes.
This is the reason why we have decided to develop OncodriveROLE, a machine learning classifier, which takes a list of pre-selected driver genes and sorts them according to their mode of action.
We first carefully compared and selected a set of features that best captures the differences of alterations patterns of these two groups of drivers.
We then used those features to train the classifier, on a carefully trimmed subset of the CGC genes.
When applied to two recent lists of drivers, we found that, even under strict classification conditions, OncodriveROLE was able to classify more drivers than the 20-20 rule and the Tuson machine learning algorithm.
The OncodriveROLE validation procedure identified several likely misclassified drivers in the CGC.
The most salient examples of these are probably some genes that drive hematopoietic malignancies upon translocation and fusion with other genomic regions, all classified as Dom in the GCG.
However, when analyzed using mutational and CNAs data from the pancancer 17 dataset, some of them appear as clear LoF drivers.
For instance, OncodriveROLE assigns MLL, RUNX1 and SUZ12 classification probabilities under 0.003 (see Supplementary Tables S3S6 for feature and classification values).
These genes could be Act drivers upon fusion to other genes, but LoF upon mutations.
Even though OncodriveROLE is able to classify most of the genes in the two drivers lists as LoF or Act, it still leaves few of them unclassified.
Some of these correspond to lowly recurrent drivers whose mutational features are not correctly computed because of the scarcity of their alterations.
Sequencing more tumors will certainly improve their classification.
Others may not have a clear enough pattern to be classified in one of the two classes, as they could be exhibiting different roles in different contexts.
In some rare cases, the method misclassifies known cancer genes.
For example, KEAP1 is classified as an Act driver, although it is reported to lose its function upon mutation (Hayes and McMahon, 2009; Shibata et al., 2008).
A close look at its mutational pattern reveals missense mutations dominate and accumulate in certain regions of the protein.
As member of a ubiquitin-mediated proteolysis complex, the function of KEAP1 is probably essential to the cell, and its impairment is likely lethal.
Therefore, few truncating mutations may appear in KEAP1, and it is ultimately misclassified by OncodriveROLE.
Future finer measurements of the impact of missense mutations may help correcting this problem.
Summing up, in this article, we have described the development and validation of OncodriveROLE, an approach to differentiate between LoF and Act driver genes.
The OncodriveROLE classifier is freely available at http://bg.upf.edu/oncodrive-role as an R object that researchers may use to classify the drivers they have detected across a cohort of tumor samples.
At the same URL, the pre-computed TCGA pan-cancer 17 mutational and copy number features used for the classification are available for download.
Funding: We acknowledge funding from the Spanish Ministry of Economy and Competitivity (grant number SAF2012-36199) and the Spanish National Institute of Bioinformatics (INB).
M.P.S.
and C.R.-P. are supported by FPI fellowships.
Table 3.
List of approaches and their performance on the 290 drivers from the HCD list and 260 drivers from the Cancer5000 list Method Act/ Oncogene LoF/ Tumour suppressor Unclassified Coverage (%) HCD Oncodrive ROLE 0.3/0.7 76 109 15 92 Oncodrive ROLE 0.2/0.8 58 96 46 77 20-20 rule 23 96 81 60 Tuson 44 92 64 68 Cancer5000 Oncodrive ROLE 0.3/0.7 43 97 4 97 Oncodrive ROLE 0.2/0.8 40 91 13 91 20-20 rule 18 90 36 75 Tuson 32 90 22 85 Table 2.
List of approaches and their performance on trimmed CGC dataset Method ACC MCC COV (%) OncodriveROLEa 0.925 0.848 83 20-20 rule 0.895 0.769 75 Tuson 0.914 0.817 92 aResults of leave-one-out cross-validation.
i554 M.P.Schroeder et al.accuracy coverage n't false very missclassified inantdue to E.g.
n Ubiquitin proteolisis paperConflict of Interest: none declared.
ABSTRACT Motivation: Early and accurate detection of human pathogen infection is critical for treatment and therapeutics.
Here we describe pathogen identification using short RNA subtraction and assembly (SRSA), a detection method that overcomes the requirement of prior knowledge and culturing of pathogens, by using degraded small RNA and deep sequencing technology.
We prove our approachs efficiency through identification of a combined viral and bacterial infection in human cells.
Contact: nshomron@post.tau.ac.il Received on February 18, 2011; revised on May 18, 2011; accepted on June 2, 2011 1 INTRODUCTION Early and accurate detection of microbial pathogens in both clinical and environmental samples is critical for effective public health care, treatment and therapeutics.
Most pathogen detection methods [polymerase chain reaction (PCR) amplification or microarrays] rely on prior knowledge of the exact sequence of the potential pathogen, or the ability to cultivate the pathogen (for microbial cultures), which is unreasonable in many cases (Douglas, 2005; Straub et al., 2005).
An alternative detection technique recently offered, which circumvents these limitations, is the sequencing of infected cells and the subsequent comparison of these sequences to a reference pathogen library for identification (MacConaill et al., 2008).
Given the massive increase in nucleic acid sequence databases of all organisms, and the advancement in massive parallel sequencing technologies, sequencing possibly infected samples evolves as an increasingly prominent and logical alternative for pathogen characterization.
The major advantages of this approach are the unbiased detection of all known pathogens, overcoming the requirement for cultivation of slow-growing and fastidious microbial agents; the ability to recognize pathogens, even at minute expression levels; simultaneous identification of several microbial agents in a co-infected sample; and the rapid turnaround and processing.
2 RESULTS AND DISCUSSION Here we introduce a novel approach for pathogen detection using short reads, generated by deep sequencing of short RNA extracts.
This three step approach includes: (i) alignment of the short reads against the human reference genome; (ii) subtraction and assembly of the remaining unmapped reads; and (iii) categorization To whom correspondence should be addressed.
and identification of the pathogen infection, based on nucleic acid databases.
We term our approach short RNA subtraction and assembly (SRSA).
We applied our method to Human Immunodeficiency Virus (HIV) infected cells, precisely identifying the infected cells and the infecting agents.
In order to identify minute quantities of non-host organisms, we chose to analyze RNA in sizes that maximize the pathogen-tohost nucleotide ratio.
Utilizing small RNA (2050 nt) for pathogen detection, rather than the currently used cDNA and ESTs (Weber et al., 2002), has several apparent advantages.
Specific short RNA extracts provide a larger pathogen-to-host ratio than DNA samples since host DNA is usually several orders of magnitude larger than pathogen DNA/RNA, and thus much more abundant in the sample.
An increased non-host RNA quantity is achieved due to a higher RNA degradation rate in bacteria (Rauhut et al., 1999) and the presence of fragmented viral sequences (due to RNA interference) (Obbard et al., 2009) leading to prevalent bacterial and viral RNA at lower molecular weights.
Unambiguous mapping of short RNAs to the human genome is preferred to circumvent splicing events of longer transcripts, since the proportion of reads spanning splice junctions is minute.
This also increases our confidence in the unmapped reads being derived from non-human origin.
Finally, short RNA extraction excludes the highly abundant host RNA species (e.g rRNA and tRNA) that can potentially over-cloud non-short RNA experiments.
All infected tissue contains nucleic acids from both the host and the infecting agent.
It has been shown that mapping long sequence reads from a transcript sample against the human genome and analyzing the non-human reads has the potential of identifying non-human pathogen genes (Xu et al., 2003).
These transcript subtraction methods utilize thousands of long sequence reads (>200 nt), produced by standard Sanger sequencing or Roche 454 sequencer.
They were not, however, applied on reads shorter than 200 nt, like the ones produced by currently more common sequencing platforms, such as the Illumina Genome Analyzer or HiSeq 2000, that produce millions of short reads (<100 nt) in a single run (Voelkerding et al., 2009).
Methods utilizing short RNA sequencing and assembly were previously applied mainly for viral detection and classification in plants and invertebrates.
Kreuze et al.(2009) sequenced and assembled viral small RNA in sweet potato without the need for subtraction of the host-derived sequences.
However, this method would not suit mammalian samples, as skipping the subtraction step could result in excess of, or over-clouding by, host-derived reads (>90%).
Briese et al.(2009) selected larger RNA products (>70 bp) from clinical samples taken from human serum and tissue and, following assembly, they were able to detect and classify a new The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:48 6/7/2011 Bioinformatics-btr349.tex] Page: 2028 20272030 O.Isakov et al.strain of infecting Arenavirus.
However, the majority of their viral sequences were obtained from the serum sample, where competing cellular abundant RNA species are absent.
This approach would likely reduce the pathogen-to-host-ratio in non-serum samples and as a consequence limit detection sensitivity.
Wu et al.(2010) applied an approach termed vdSAR to assemble previously sequenced small RNA libraries from invertebrates, in order to both demonstrate their sequence overlap, and to classify the infecting viral agents.
However, dealing with mammalian genomes and a large number of sequencing reads (>10 million) necessitates a host-based sequence subtraction step for increasing accuracy.
Thus, our approach is unique, as we utilize short RNA subtraction and assembly in mammalian-derived cells, to maximize pathogen-to-host-ratio; demonstrate recognition of both viral and bacterial infecting agents; and suggest a possible siRNA-related immune response.
Short sequence reads present a confounding problem of multiple organisms alignment (Trapnell et al., 2009a) and thus inconclusive identification of the organisms.
In order to identify the non-human sequences and target to which organism it aligns, we applied de novo sequencing (sequence assembly), using an assembly software [Velvet (Zerbino et al., 2008)] to produce longer consensus sequences from our given short read sample.
These longer assembled reads were then compared to known organisms references [using BLAST (Dumontier et al., 2002)] to produce high scoring unique alignments and thus a valid and conclusive identification, which could not have been reached otherwise.
We used cell line infected with Human Immunodeficiency Virus 1 (HIV-1; see Section 2).
Using an alignment software [BurrowsWheeler Aligner (BWA) (Li and Durbin, 2009)], we aligned the reads produced by our sequencing platform against the human reference genome.
Our sequencing produced >10 million reads for each sample, with an average read length of 34 nt.
Filtering the human genome-associated reads produced 6% and 17% unmapped reads in the HIV-1 negative and positive samples, respectively.
In order to reduce the number of multiple organism hits we expect when matching such short reads against a large database, we assembled each read groups into longer contigs.
The assembly process produced 16 and 878 contigs, with an average length of 75 and 91 nt for the HIV-1 negative and positive samples, respectively.
As we expected, the assembly process was far more productive in the HIV-1 positive sample than the negative one, due to the presence of more specific non-human organisms sequences.
We then used NCBIs megablast to match these contigs against any known organism.
Megablast was chosen since it is an optimal tool for identical sequence detection for both short and long sequences.
Since our goal was not to find homologous sequences but rather to detect the most probable and specific sequence in the nucleotide database, megablast was preferred over other optional tools.
Using an in-house specific software (available upon request), we incorporated only the highest scoring hits in the downstream analysis, and filtered out non-unique organism hits, meaning that any contig that matched more than one organism was discarded.
We also set the combined E-value of all unique hits to be 1*E200 at most (See Section 2 for more detail).
After applying these filters to the Blastn results, we could not identify any non-human organism in the HIV-1 negative sample.
In the HIV-1 positive sample, however, we identified two non-human organisms: HIV-1 and Mycoplasma Hyorhinis HUB-1 (Table 1).
We then further analyzed the distribution of hits matching any HIV strain, finding the strain accumulating the highest number Table 1.
Identified pathogens in our samples using SRSA method Organism Unique Multiple Total Total hits hits E-value score Mycoplasma Hyorhinis HUB-1 564 51 0 48 951 Mycoplasma Hyorhinis 12 42 0 1544 Human immunodeficiency virus 1 13 26 0 1138 Unique hits, describes the number of queries that matched only one specific organism.
Multiple hits, describes the number of queries in which the organism was one of the organisms matched.
Total E-value, the combined E-value of all unique organism hits.
Total score, describes the sum of all scores for each organism query hit, with reduced weight for multiple organisms hits score.
Table 2.
Distribution of HIV related BLAST hits HIV hit name Total hits HIVHXB2CG HIV type 1 (HXB2), complete genome; HIV1/HTLV-III/LAV reference genome 1.147 HIV-1, complete genome 0.809 Human T-cell leukaemia type III (HTLV-III) proviral genome 0.475 HIVBH102 HIV type 1, isolate BH10, genome 0.475 HIVPV22 HIV type 1, isolate PV22, complete genome (H9/HTLV-III proviral DNA) 0.454 HIVTH475A HIV type 1 (individual isolate: TH4-7-5) gene 0.333 HIV-1 isolate F233 from Argentina vpu gene, complete sequence 0.333 HIVH3BH5 HIV type 1, isolate BH5 0.316 HIV-1 proviral vif gene DNA 0.294 HIVMCK1 HIV 1 DNA 0.275 The distribution of blast HIV-related hits, after the multiple hit adjustments in which one of the hits for a query matching n number of organism is added 1/n to its total hits.
This adjustment was utilized in intraspecies strain differentiation and was able to single out HXB2 as the strain of HIV in our sample.
of hits was, in fact, the exact strain used to infect the sample (HIV-HXB2; Table 2).
To confirm the detection of Mycoplasma in our HIV-1-infected sample, we used a standard Mycoplasma test kit (Fig.1), followed by a PCR and a sequencing confirmation that verified the Mycoplasma strain was indeed of the Hyorhinis strain.
The detection of Mycoplasma and HIV-1 validates the accuracy and sensitivity of our method in identifying both intracellular viral agents and environmental bacterial contaminants, in the same given sample.
In addition to the efficient detection of HIV in our sample, sequencing data coverage analysis showed that when mapping our human-alignment unmapped reads against HIV reference genome, 87% of the bases were covered with an average coverage of 18 reads.
This comprehensive read depth could be utilized for further phylogenetic, strain and mutation analysis, relevant in microbial detection and research (Wang et al., 2007).
Due to the decrease in cost and increase in efficiency of deep sequencing platforms, we expect sequencing utilization in the field of pathogen detection and identification to increase.
Our method, based on using small RNA, to increase the pathogen-to-host ratio, proves to be a useful tool for microbial identification and detection.
Our computational approach of subtraction and assembly (SRSA) presents an easily implemented pipeline, appropriate for all types 2028 [17:48 6/7/2011 Bioinformatics-btr349.tex] Page: 2029 20272030 Pathogen detection Fig.1.
PCR products visualized in 1.5% agarose gel, a single band for both SupT1 + HIV-1 sample (lane 4) and the positive control (c) from the mycoplasma detection kit (lane 2) observed within the predicted marker (m) size range (270 bp).
Lane 3 is the uninfected cells.
of current sequencing platforms.
We envision early and accurate detection of pathogen infection, using short RNA reads to accelerate clinical biomedical investigation.
3 METHODS 3.1 Sample preparation SupT1 cells (human, Caucasian, pleural effusion, lymphoma, T cell) were infected with HIV-1 (HXB2 strain) on day 0.
Four days post-infection, 50% of naive cells were added to the cultures, and 4 days later they were harvested.
Total RNA was extracted using TRIzol reagent (Invitrogen), and 10 g of each sample were prepared for deep sequencing following Illuminas small RNA sample preparation protocol v1.
Briefly, samples were ligated with 3 and 5 adapters, reverse transcribed and then PCR amplified.
cDNA library was prepared from 93 to 100 bp PCR products, and sequenced in separate lanes on an Illumina Genome Analyzer IIx instrument at the Tel Aviv University Genome High-Throughput Sequencing Laboratory.
3.2 Alignment and assembly The sequenced reads were clipped for the standard short RNA adapter, using fastx clipper (http://hannonlab.cshl.edu/fastx_toolkit/), discarding all reads <16 nt.
Our sequencing method produced 21 048 677 and 12 003 830 reads after clipping for HIV-1 negative and positive samples, respectively.
We then aligned the reads using BWA alignment software (using the default parameters) against the human genome (hg19) reference, retrieved from NCBI.
We re-aligned our positive sample using TopHat (Trapnell et al., 2009b), which considers splice junctions in the alignment process, to test the confounding effect of splice junctions presence on our results.
Since BWA demonstrated higher mapping accuracy (83% as opposed to 78% using TopHat), all downstream analysis was conducted on its output.
Filtering the human genome-associated reads produced 1 251 267 (6%) and 1 992 557 (16.6%) unmapped reads in the HIV-1 negative and positive samples, respectively.
The unmapped reads were then assembled, using Velvets AssemblyAssembler (v1.3), which conducts assemblies with predefined parameter values across a user-specified range of k-mer values, followed by utilization of the contigs from all previous assemblies, as input for a final assembly.
We ran the script with a wide range of hash lengths of 931 nt in order to optimize the length of the contigs.
The most effective length was found to be within the range of 17 to 25 nt.
The assembly produced 16 and 878 contigs with an average length of 75 and 91 nt for the HIV-1 negative and positive samples, respectively.
The difference in the amount of produced reads can be accounted for by the lack of non-human-associated sequences in the negative sample.
3.3 Categorization and identification The assembled contigs were then aligned, using NCBIs nucleotide megablast with a word size of 28, against the All non-redundant GenBank CDS translations+PDB+SwissProt+PIR+PRF database, with an inclusion threshold E-value of 0.01.
Using an in-house developed software (available upon request), we included only the highest ranking hits (with lowest E-value and highest total score) per query for the downstream analysis.
Using the blast results, we produced an organism table for each sample.
Unique organism hits were added, when all the highest ranking included hits per query matched only a single organism [organism taxonomy retrieved using NCBIs E-utils (http://eutils.ncbi.nlm.nih.gov/) and taxonomy database].
Multiple hits were counted for each organism matched per query.
The organisms total E-value was calculated by multiplying the E-value of all the unique hits (E-value is nearly identical to P-values for E <0.01).
To account for the multiple hits in the blast results, we also calculated an organism total score, by summing the score for each unique organism hit with the score for each multiple hit, divided by the number of different organisms per query.
This summation approach did not increase our interspecies differentiation capabilities, since different species could have a different number of strains and annotations.
It was, however, utilized in our intraspecies analysis, prioritizing the different strains (Table 2).
Deciding on a standard of maximum total E-value of 1*E200, no organism was identified in the HIV-1 negative sample, the most prominent was Homo sapiens with nine unique hits.
In the HIV-1 positive sample, four organisms were identified, one of which was Homo sapiens, the other three were Mycoplasma Hyorhinis HUB-1, Human Immunodeficiency Virus 1 and Mycoplasma Hyorhinis.
While optimizing our method, we found that its sensitivity was very high, detecting both infecting agents and host-related sequences with very high certainty, and an extremely low E = 0.
We also observed that the number of different taxonomies passing the basic filter of having the highest query score and lowest E-value was very high, demonstrating a low specificity rate.
Out of 385 different taxonomies passing the basic filter, 14 were Mycoplasma out of which 3 were of the Hyorhinis strain.
There were also 19 different human immunodeficiency virus taxonomies and 1 Homo sapiens.
Counting only the Hyorhinis, HIV-1 and Homo sapiens as true positives (true positive rate; TPR = 0.06), our method demonstrated a false positive rate (FPR) of 0.94.
We then sought to implement more rigorous inclusion criteria to reduce the FPR, while maintaining high sensitivity.
Setting our inclusion criteria to only include taxonomies that have unique query hits, resulted in eight different taxonomies, one of which is Homo sapiens, one HIV-1 and four Mycoplasma out of which two are Hyorhinis reaching an FPR of 0.5.
We then added a maximal E-value threshold equal to 1*E200, which resulted in only four taxonomies remaining, all of which are true positives, with Mycoplasma Arginini being the closest taxonomy for inclusion with 4*E181.
We also noticed that the total score, calculated by dividing each query score against the number of taxonomies it hits and summing it for each taxonomy, could also serve as a reliable filtration standard, though further tests are required.
3.4 Mycoplasma confirmation To confirm the detection of a Mycoplasma contamination in our HIV-1infected sample, we used EZ-PCR Mycoplasma test kit (Biological Industries, Beit-HaEmek) on the samples (50 ng in 50 l reaction volume) following High-Capacity Reverse Transcription Kit with random primers (Applied Biosystems) (1 g RNA in 15 l total reaction volume).
The products were separated in Agarose gel (Fig.1), and bands were excised from the gel using Wizard SV Gel Clean-Up System (Promega).
Confirmation sequencing was done using the forward primer 5GGGAGCAAACAGGATTAGATACCCT-3.
This confirmation strengthens our methods fidelity, since Mycoplasma is indeed present in the sample.
2029 [17:48 6/7/2011 Bioinformatics-btr349.tex] Page: 2030 20272030 O.Isakov et al.ACKNOWLEDGEMENTS We thank Tel Aviv University Genome High-Throughput Sequencing Laboratory staff, Drs Varda Oron-Karni, Orly Yaron and Nitzan Kol, for their dedicated and professional work.
We thank Judit Kovarsky for assisting and advising in the assembly process and David Golan in the statistics.
We thank Prof. Zvi Bentwich and Drs Eran Bacharach and Eran Halperin for helpful discussions.
We thank Dana Braff for commenting on the manuscript.
This work was performed in partial fulfillment of the requirements for a PhD degree of O.I.
and S.M.
at the Sackler Faculty of Medicine, Tel Aviv University.
Funding: The Shomron laboratory is supported by the Chief Scientist Office, Ministry of Health, Israel; Kunz-Lion Foundation; Ori Levi Foundation for Mitochondrial Research; Israel Cancer Association; the Wolfson family Charitable Fund.
O.I.
is supported by a fellowship from the Edmond J. Safra Bioinformatics program at Tel-Aviv University.
Conflict of Interest: none declared.
ABSTRACT Summary: Illumina produces a number of microarray-based technologies for human genotyping.
An Infinium BeadChip is a twocolor platform that types between 105 and 106 single nucleotide polymorphisms (SNPs) per sample.
Despite being widely used, there is a shortage of open source software to process the raw intensities from this platform into genotype calls.
To this end, we have developed the R/Bioconductor package crlmm for analyzing BeadChip data.
After careful preprocessing, our software applies the CRLMM algorithm to produce genotype calls, confidence scores and other quality metrics at both the SNP and sample levels.
We provide access to the raw summary-level intensity data, allowing users to develop their own methods for genotype calling or copy number analysis if they wish.
Availability and Implementation: The crlmm Bioconductor package is available from http://www.bioconductor.org.
Data packages and documentation are available from http://rafalab.jhsph.edu/software.html.
Contact: mritchie@wehi.edu.au; rafa@jhu.edu 1 INTRODUCTION In recent years, large-scale genome-wide association studies have provided significant insight into the genetics underpinning many complex diseases (Grant and Hakonarson, 2008).
High-density microarrays, which allow many single nucleotide polymorphisms (SNPs) to be genotyped simultaneously in a sample at low cost, have been the technology driving this research.
Illumina Inc. (San Diego, CA, USA) is a major provider of such arrays.
Illumina BeadChips are composed of a number of rectangular strips, each containing many randomly arranged, replicated beads.
For Infinium genotyping, beads are coupled with specific 50mer probes designed to be complementary to the sequence adjacent to the SNP site, and the two alleles (A, B) are discriminated using either a red or green dye (Steemers et al., 2006).
Data are acquired by scanning each strip at different wave lengths using Illuminas To whom correspondence should be addressed.
scanning device followed by automatic image analysis (Galinsky, 2003).
A robust summary of the intensity in each channel for each SNP assayed is reported in proprietary idat files.
BeadChips of varying SNP density and sample format (single, duo, quad) are available for human genotyping.
Some contain non-polymorphic probes for assessing copy number variation.
Many algorithms that take summarized alleles A and B signals as inputs to produce genotypes (AA, AB, BB) have been developed for Affymetrix SNP arrays (Carvalho et al., 2007; Hua et al., 2007; Rabbee and Speed, 2006; Xiao et al., 2007).
A smaller number of Illumina-specific methods (Giannoulatou et al., 2008; Teo et al., 2007) including Illuminas GenCall algorithm in BeadStudio/GenomeStudio are also available.
Software for the analysis of Illumina data such as beadarray (Dunning et al., 2007), beadarraySNP and lumi (Du et al., 2008) is available in R/Bioconductor (Gentleman et al., 2004); however, current packages do not deal specifically with Infinium BeadChip data.
In this article, we present the crlmm package for Illumina genotyping.
Our software extracts summarized intensities, performs normalization and applies the CRLMM algorithm (Carvalho et al., 2007) to remove chip-and SNP-specific biases and call genotypes.
2 METHODS To begin, summarized data are read from idat files (two per array, one for each channel) using the function readIdatFiles.
Binary idat files are a convenient starting point, as they are routinely output by the scanning software, provide a compact representation of the data and have a consistent format (unlike output from Illuminas BeadStudio/GenomeStudio software, which is exported at the users discretion, meaning the raw signals needed for the analysis are not always available).
Access to the raw data allows for low-level plotting to help visualize trends and biases that may be present (Fig.1A and B).
It also allows alternative genotyping algorithms, which require data on the raw scale, to be applied.
Next, the allele A (X Raw) and allele B (Y Raw) signals are normalized between channels and samples simultaneously using strip-level quantile normalization.
The between-channel aspect of the normalization [also recommended in Oosting et al., (2007) and Staaf et al., (2008)] aims to remove any dye-bias effects, while the strip-level component corrects 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:24 31/8/2009 Bioinformatics-btp470.tex] Page: 2622 26212623 M.E.Ritchie et al.1.
1 2.
1 3.
1 4.
1 5.
1 6.
1 7.
1 8.
1 9.
1 10.1 1.
2 2.
2 3.
2 4.
2 5.
2 6.
2 7.
2 8.
2 9.
2 10.2 2 4 6 8 10 12 14 16 A strip lo g 2 (in te ns ity ) 8 9 10 11 12 5 0 5 B S M 0 10 20 30 40 50 60 14 15 16 17 18 19 20 C array S N R Fig.1.
(A) Plot of the log2 alleleB (green) intensity by strip (labelled by Row.Column position), which steadily increases from rows 1 to 10 down the BeadChip.
This effect is less prominent in the allele A (red) channel (data not shown).
The source of this trend is related to the way post-hybridization reagents are applied to the BeadChip and scan order, and its presence motivates strip-level normalization.
(B) A smoothed scatter plot of M versus S for a typical array, where darker regions indicate a higher density of points.
This plot shows intensity-dependent effects in M which vary for the AA and BB genotypes, and motivate the three-component mixture model in CRLMM.
The curves represent the smoothing splines that model this effect.
(C) SNR for 60 arrays, with the median (solid line) and medianmedian absolute deviation (dashed line) SNR values plotted.
Lower scores correspond to poorer separation between the genotype clouds depicted in (B).
This metric can be used to flag low-quality arrays to exclude from further analysis.
for intensity gradients which can occur within BeadChips (Fig.1A).
Normalization at the strip-level has also proven useful for data from Illuminas gene expression BeadChips (Wei Shi, personal communication).
By default, the strip-level quantiles are standardized against a reference distribution obtained from HapMap samples (International HapMap Consortium, 2007) run on the same platform to correct for lab and batch effects.
After normalization, the CRLMM genotyping algorithm (Carvalho et al., 2007; Lin et al., 2008) is applied.
For each array, SNP-specific log-ratios (M = log2 alleleA log2 alleleB) and average intensities [S = (log2 alleleA + log2 alleleB)/2] are calculated.
As noted for Affymetrix data (Carvalho et al., 2007), S appears to have an effect on M. The effect appears to be a smooth function of S, but only applies to the AA and BB intensities (Fig.1B).
To remove this effect, we fit a three-component mixture model with a spline used to model the smooth function.
This model is fitted per array via the expectation-maximization (EM) algorithm using a random sample of datapoints.
Due to the different chemistry used for Illumina genotyping, the fragment length covariate described in Carvalho et al.(2007) can be ignored.
Next, a two-level hierarchical model is applied.
SNP-specific means and standard deviations (SDs) are obtained for each genotype via supervised learning using HapMap data.
Independent genotype calls (available from http://www.hapmap.org/) provide the true states for samples that have been genotyped using the respective BeadChip platform.
Normalized signals from these arrays are then used to estimate robustly the genotype means and SDs.
The intensity-dependent splines from the EM (which explain the betweenSNP variation) and the SNP-specific genotype means and SDs (obtained from training data) are combined in the model.
New genotype calls are assigned by choosing the class that minimizes the negative log likelihood.
CRLMM produces a number of metrics for quality assessment (Lin et al., 2008).
Confidence scores for each call are provided using the log-likelihood ratio test from the hierarchical model.
The sample-specific SNR (signal-tonoise ratio) assesses the separation of the three genotypes within an array.
Lower SNR values indicate poorer quality, and this metric can be used to exclude samples from further analysis (Fig.1C).
SNP-specific quality is measured as the minimum distance between the heterozygote centre and either of the two homozygous centres.
The preprocessing and genotyping steps above are performed by the crlmmIllumina function.
All code is written in R (R Development Core Team, 2009) and existing Biobase classes are used to store the data.
The software requires chip-specific data packages (available at http://rafalab.jhsph.edu/software.html) that store basic SNP annotation information and various parameters used by CRLMM.
We also provide the hapmap370k data package, which contains idats from 40 HapMap samples hybridized to HumanHap 370 K Duo BeadChips, and a user guide that provides example R code to analyse these samples (see Supplementary Material).
A 64-bit Linux system takes 90 s and uses up to 1.2 GB of RAM to read these data, while normalization and genotyping takes a further 470 s and uses up to 3.3 GB of RAM.
This equates to processing around 600 SNPs per second.
3 DISCUSSION The crlmm package provides bioinformaticians with an additional tool outside of Illuminas proprietary software for analysing Infinium BeadChip data.
Our software also facilitates the analysis of Affymetrix SNP chips and the use of a consistent algorithm and framework to process both the platforms allows data from different studies to be combined more easily.
Implementation in R/Bioconductor gives users the opportunity to exploit other tools that have been adapted for Illumina data.
For example, if raw bead-level data were available, the BASH spatial artefact detection method (Cairns et al., 2008) in the beadarray package could be applied.
Once summarized, the data could be further processed using crlmm.
The CRLMM algorithm can be applied to new versions of Illumina BeadChips for humans and other species, provided that the necessary training data and prior information on genotype calls are available.
Future work will benchmark the performance of our method with other genotyping algorithms tailored to suit Illumina data, such as Illuminus (Teo et al., 2007) and Illuminas own 2622 [15:24 31/8/2009 Bioinformatics-btp470.tex] Page: 2623 26212623 R/Bioconductor software for Illuminas Infinium BeadChips algorithms in BeadStudio/GenomeStudio.
Furthermore, tools for copy number analysis are being developed in the crlmm package.
ACKNOWLEDGEMENTS We thank Illumina for providing access to HapMap datasets for each platform and for technical support on their products; Keith Baggerly (MD Anderson Cancer Center) for sharing R code to read idat files; Kimberly Doheny and Elizabeth Pugh (CIDR) for providing test HapMap data; and Stephen Wilcox and Melinda Ziino (AGRF) for providing example idat files for testing purposes.
Funding: Isaac Newton Trust and NHMRC Program (grant 406657); NHMRC IRIISS (grant 361646); Victorian State Government OIS grant (to M.E.R.
); National Institutes of Health grants R01GM083084, R01RR021967 and P41HG004059 (to B.S.C., R.A.I.
); Cancer Research UK (S.T.).
Funding for open access charge: National Institutes of Health (grant P41HG004059).
Conflict of Interest: none declared.
ABSTRACT Motivation: Proteins with solenoid repeats evolve more quickly than non-repetitive ones and their periodicity may be rapidly hidden at sequence level, while still evident in structure.
In order to identify these repeats, we propose here a novel method based on a metric characterizing amino-acid properties (polarity, secondary structure, molecular volume, codon diversity, electric charge) using five previously derived numerical functions.
Results: The five spectra of the candidate sequences coding for structural repeats, obtained by Discrete Fourier Transform (DFT), show common features allowing determination of repeat periodicity with excellent results.
Moreover it is possible to introduce a phase space parameterized by two quantities related to the Fourier spectra which allow for a clear distinction between a non-homologous set of globular proteins and proteins with solenoid repeats.
The DFT method is shown to be competitive with other state of the art methods in the detection of solenoid structures, while improving its performance especially in the identification of periodicities, since it is able to recognize the actual repeat length in most cases.
Moreover it highlights the relevance of local structural propensities in determining solenoid repeats.
Availability: A web tool implementing the algorithm presented in the article (REPETITA) is available with additional details on the data sets at the URL: http://protein.bio.unipd.it/repetita/.
Contact: silvio.tosatto@unipd.it 1 INTRODUCTION Proteins can adopt a wide range of structures uniquely determined by sequence, with the vast majority being globular and stabilized by a unique cooperative hydrophobic core formed upon folding.
It is long known that not all structures follow this general schema, e.g.fibrous proteins in silk (Kajava et al., 2006).
There has been an increasing interest over the last years for such cases that apparently do not fold in the same way as globular proteins (Main et al., 2005).
Several proteins fold into conformations with repeated structural regions (Andrade et al., 2000).
Such repeat proteins are present in 14% of known protein sequences with specific functions generally associated to higher organisms (Marcotte et al., 1999).
Protein repeats can be broadly divided in three different classes, depending on their length (Kajava, 2001).
Short repeats of up to four residues have crystalline structures or form fibrous structures, while domainforming repeats are longer than about 45 residues and behave like short globular proteins.
In between are the so-called solenoid To whom correspondence should be addressed.
Fig.1.
Cartoon representation of sample solenoid structures.
Rainbow coloring from blue to red shows the topology from the N-to the C-terminus.
(A) Antifreeze protein (PDB 1EZG), (B) Pectate Lyase(PDB 1AIR), (C) Leucine Rich Repeat (LRR) variant (PDB 1JL5), (D) LRR (PDB 1YRG) and (E) Armadillo (PDB 2BCT).
All pictures were drawn using PyMol (URL:proteins forming peculiar protein folds (Kobe and Kajava, 2000).
Solenoids are modular assemblies of identical units, containing secondary structure elements, which are coiled along a common axis or direction in space with a fixed curvature.
Secondary structure varies from-strand to-helix with increasing repeat length (Fig.1).
Recently it has been experimentally established that folding of solenoid repeats is sequential (Kajander et al., 2005).
A distinctive subclass is formed by the-propeller proteins with 4460-residue repeats arranged like the blades of a propeller: unlike solenoid repeats, these apparently have to form a closed circular structure.
Short repeats can be easily identified due to their low sequence complexity and regularity (Wootton, 1994).
Domainforming repeats are generally of sufficient length to allow recognition with sensitive database search tools, e.g.PSI-BLAST 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:14 15/5/2009 Bioinformatics-btp232.tex] Page: i290 i289i295 L.Marsella et al.(Altschul et al., 1997).
Solenoid proteins are more difficult to predict, as the single repeat can be quite degenerate in sequence and vary widely in number of repeat units (Kobe and Kajava, 2000).
In fact, conservation appears more related to certain characteristics of the protein sequence, e.g.hydrophobicity, than to any given amino-acid type.
This makes application of tools like PSI-BLAST difficult, since it relies on clear conservation patterns.
A number of methods for solenoid repeat detection have been published over the years to overcome this limitation.
Most methods are based on the self-alignment of the sequence (Biegert and Soding, 2008; George and Heringa, 2000; Gruber et al., 2005; Heger and Holm, 2000; Soding et al., 2006; Szklarczyk and Heringa, 2004).
Ideally, repeated parts of the sequence should appear as off-diagonal regions of similarity, allowing identification of the basic repeat unit and the number and location of units along the sequence.
Sequence conservation remains an issue, as highly degenerate repeat units may escape detection.
Alternative approaches based on spectral analysis have been recently proposed (Murray et al., 2002; Murray et al., 2004; Gruber et al., 2005).
The method of Murray and coworkers (Murray et al., 2002; Murray et al., 2004) is mainly aimed at the automated detection of repeats in known protein structures, in itself a highly non-trivial problem.
REPPER (Gruber et al., 2005) implements a Fourier transform of the sequence using a hydrophobicity scale, but was not extensively benchmarked and has been tested to detect mainly periodicities of fibrous proteins.
One of the most sensitive methods to detect degenerated repeats of solenoid proteins is based on sequence profiles (Lupas et al., 1997).
The profile can identify tandem repeats of solenoids as well as protein domains, i.e.autonomously folding parts of the protein with distinct functions, if it spans more than one repeat (Kajava et al., 2004).
The present study aims to detect solenoid repeats and discriminate them from globular proteins, using the information coming from sequence profiles together with the discrete Fourier transform (DFT), based on the assumption that few characteristics of sequence repeats uniquely identify structural repeats.
The Fourier transform is a mathematical tool capable of highlighting latent periodicities in a protein sequence given one or more adequate metrics used to characterize the amino-acid sequence efficiently.
For this purpose, we employ the five numeric scales proposed by Atchley and coworkers after a rigorous statistical analysis of almost 500 different attributes associated with each amino acid (Atchley et al., 2005).
In the following, we describe the development of a new DFT-based method and relevant statistical parameters for the identification of solenoid repeats and their periodicities in protein sequences.
The REPETITA algorithm is compared to published methods and the implications are discussed.
2 METHODS 2.1 Datasets used An initial set of 32 proteins with solenoid repeats was taken from the website (URL: http://www.crbm.cnrs.fr/kajava/solenoidtable.html) of a previous review (Kobe and Kajava, 2000).
The TESE server (Sirocco and Tosatto, 2008) was used to find more protein domains belonging to the same solenoid folds as the initial set.
TESE allows the user to generate ad hoc non-redundant sets of proteins with known structure, by limiting the maximal residual structural similarity according to the CATH classification (Pearl et al., 2003).
Choosing representatives with at most 35% pairwise sequence identity (i.e.CATH S level) yielded the final set of 105 solenoid domains.
The set of non-solenoid protein domains was generated with TESE by randomly choosing X-ray structures with different topologies and no detectable sequence similarity (i.e.CATH T level), for a total of 247 domains.
The rationale for having a larger number of non-solenoid proteins is that the method should work well over all known protein folds.
Both sets are shown on the web site.
The RADAR (Heger and Holm, 2000) and TRUST (Szklarczyk and Heringa, 2004) methods were downloaded and run locally on the two datasets.
Solenoid predictions for both were considered when at least two consecutive repeat units were detected.
The overall set of 105 solenoid repeat proteins and 247 non-solenoid protein domains was randomly split into a training set of 50 solenoid proteins and 119 non-solenoid domains and a test set of 55 solenoid proteins and 128 non-solenoid domains, with the constraint that solenoid structures of low similarity fall in the same partition.
Sequence profiles for use with the DFT method are generated with PSI-BLAST (Altschul et al., 1997).
The non-redundant database is searched for four interactions with standard parameters and an e-value threshold for inclusion in the profile of 0.001.
The final alignments are used to derive the frequency profiles by counting each amino-acid type while ignoring gaps.
2.2 DFT formulation We first translate the sequence profile of the candidate solenoid repeat into the numerical functions derived by Atchley and collaborators (Atchley et al., 2005).
These five functions summarize 494 attributes of amino acids obtained from an online database (Kawashima et al., 1999), and characterize polarity, secondary structure, molecular volume, codon diversity and electrostatics charge, etc.
In the following step, the five functions measuring amino-acid properties are normalized, to allow a straightforward comparison between the numerical values of the direct functions and the corresponding Fourier transforms.
Normalization is performed on the squares of the functions, so that they sum up to 1: X [ ga ( X )]2 =1 (a=1,2,...5) (1) where X = [A, C, D, E, , W , Y ] is the one-letter code corresponding to each of the 20 amino acids, and ga are the five normalized sequence metrics.
We are then ready to measure the sequence profile of the candidate solenoid repeat with the newly normalized functions.
A sequence profile pk(X) of length N, giving the probability of finding amino acid X in the profile at position k along the sequence, will be described by means of the set of five discrete functions, whose values are given according to the previously defined metrics: f ka = X ga(X)pk(X) with X pk(X)=1 (k =1,...,N) (2) The problem related to the detection of the periodicities hidden along the sequence is then mapped on the frequency space, using the DFT.
It is applied to each function separately to obtain the corresponding transform Fa = F [fa], whose values Fna are needed to expand the original functions as a sum of trigonometric functions with angular frequencies and corresponding periods given by Equation (3): n = 2n N Tn = 2 n = N n (3) The DFT is computed for each n = 0, , N 1 according to Equation (4): Fna = 1 2N N1 k=0 f ka e 2 ik(n/N) (4) The five-sequence functions are real, so that Fna = (F Nn a ) and the resulting spectrum has only Nsp <N spectral amplitudes.
The latter are normalized in such a way that their height is not increased with sequence length, thus making comparison easier among different sequences: Ana = Fna F Nn a N (n=0,...,Nsp 1) (5) i290 [10:14 15/5/2009 Bioinformatics-btp232.tex] Page: i291 i289i295 Detection of protein solenoid repeats by discrete Fourier transform The amplitude with n = 0 is related to the average of the function fa.
The number of independent spectral lines is: Nsp = { N+1 2 N odd N 2 +1 N even (6) In this way we are not considering the information coming from the N Nsp independent phases associated to the Fna.
2.3 Confidence estimates In order to identify the existence of a periodicity in the DFT signal of a candidate solenoid repeat sequence it is necessary to locate peaks in the spectra that should correspond to the exact period or to higher harmonics.
In practice, the DFT spectra of protein sequence profiles display a quite noisy background, so that in order to correctly discriminate significative peaks we found it useful to employ a two-dimensional analysis based on the two parameters and zmax defined below.
We first introduce a threshold = a to select spectral amplitudes above the threshold a + a where the real number is the only fitted parameter of REPETITA, a is the average and a is the standard deviation of each spectrum (a = 1, , 5).
The spectrum of each function of the metric is separately checked to count the number of spectral amplitudes above the threshold.
The use of z-scores zna of the amplitudes makes the check straightforward, since z-scores are defined as: zna = Ana a a (7) so that the threshold condition becomes zna >.
Averages a and standard deviations a for the five metric functions are computed discarding A0a (a = 1, , 5), which are the averages of the function fa.
These numbers depend on the overall sequence composition and are not meaningful to detect periodicities.
We then proceed by counting the number of amplitudes in the spectrum of all the five functions, which have z-scores larger than , which we call N : N = N(zna > ), where a = 1, , 5 and n = 1, , Nsp 1.
Note that z0a is discarded from the procedure, since it has no meaning, after having discarded A0a from the computation of averages and standard deviations used to obtain the z-scores.
This number is then normalized by dividing by the number of spectral amplitudes considered for all five metric functions.
The quantity obtained will give the percent ratio of spectral amplitudes above the selected threshold, which we will shortly call-ratio and write as : =100 N 5(Nsp 1) (8) The-ratios of the test sequences are computed for different values of ranging from 1 to 5.
More information might be obtained from further analysis of the spectra, and a second parameter which strengthens the significance of the detected periodicity within the sequences under analysis is the maximum z-score found among all spectral amplitudes (zmax), for all the five metric functions.
In summary, a signal in sequence periodicity should be reflected in large enough spectral amplitudes (after proper normalization z-scores are used in place of raw amplitudes).
We use two different parameters to extract as much information as possible from our data: zmax, the largest spectral amplitude; , the percent ratio of spectral amplitudes with z-score larger than.
2.4 Evaluation criteria In order to derive a simple confidence estimate, we begin by testing different values of ranging from 1 to 5.
For any given value of we proceed systematically deriving separating lines in the zmax plane.
For each separating line with slope m and intercept q, the sets of solenoid (actual positives, ap) and globular sequences (actual negatives, an) are scanned to check the number of correct and wrong predictions.
This procedure is similar to linear discriminant analysis.
A prediction is correct if a solenoid (globular) sequence has a positive (negative) sign of mzmax q 1+m2 (9) which tells whether the sequence under consideration lies above (predicted solenoid) or below (predicted globular) the separating line.
The number of sequences predicted to be solenoid (predicted positives, pp) or to be globular (predicted negatives, pn) are then checked against the actual positives, ap, and actual negatives, an.
The outcome of the comparison are the number of true and false positives (tp,fp) and the number of true and false negatives, which may be obtained from the previous ones (tn = an fp,fn = ap tp).
For each separating line we compute the Matthews correlation coefficient CM = tp tn fp fn ap an pp pn (10) CM values lie in the range [1, 1], with 1 representing perfect agreement between predictions and actual values.
For a given , we select the optimal separating line (m,q) as the one maximizing CM.
Sensitivity (true positive rate, tp/ap) and specificity (true negative rate, tn/an) are also computed.
A further optimization of CM is carried out on the training set upon varying.
The final values obtained will be then left fixed in the implementation of the REPETITA algorithm.
3 RESULTS 3.1 DFT calculation From the homology profile of a given protein sequence, DFT spectra are calculated with standard techniques, as described in Section 2.
The DFT provides the representation of the original function in the frequency domain, highlighting its periodicities with peaks at the corresponding frequencies computed according to Equation (3).
Rather than using a single function to transform the amino-acid profile at each sequence position into a numerical value used in the DFT, we use the five scales introduced by Atchley and co-workers (Atchley et al., 2005).
An example of the frequency amplitudes obtained from the five spectra is shown in Figure 2 for the 82 residue antifreeze protein (PDB code 1EZG).
This protein forms a regular solenoid repeat (see Fig.1A) with a period of 12 residues.
The peak above threshold with the lowest frequency rank (i.e.longest periodicity) reads 7 on scale 2.
This transforms to a correctly predicted period of about 12 residues, as a result of the substitution of N = 82 and n = 7 in Equation (3).
Notice that looking at other scales peaks above threshold correspond roughly to higher harmonics (ranks 27, 41).
However, it is clear from these spectra that the determination of the correct period is not a trivial task, since the corresponding peaks are not always very pronounced for all scales, higher harmonics do not always appear, and there is a strong background noise which makes the analysis quite complicated.
In order to overtake this problem we perform an analysis as described in the Section 2 by using the two parameters zmax and.
In order to assess the validity of the DFT predictions on a representative set, 352 protein sequences were selected among solenoid repeats (105) and globular proteins (247) without structural repeats from CATH (Pearl et al., 2003).
The proteins belonging to the two sets are listed on the web site.
The solenoid repeats were chosen to cover the main repeat classes (all-,/ and all-) with available structural information.
Other known repeats are structurally and evolutionarily related to these major folds, e.g.HEAT and ARM repeats (Andrade et al., 2001).
The large class of-propeller proteins was excluded as these have to i291 [10:14 15/5/2009 Bioinformatics-btp232.tex] Page: i292 i289i295 L.Marsella et al.Fig.2.
Fourier spectral amplitudes of Atchleys functions of the 3-solenoid domain of the antifreeze protein with sequence length N = 82 (PDB identifier: 1EZG).
The peaks around frequencies n = 14, 21, 28, 35 belong to the harmonic series of the fundamental frequency rank n = 7, which appears as global maximum in the spectrum of Atchleys function 2 (top right) and as local maximum in the others.
It corresponds to a periodic repeat T = 12 [computed using Equation (3)], in agreement with the actual structural repeat.
form closed structures and are not true solenoids.
The representative set was divided into a training set and a test set (see Section 2).
3.2 Detection of solenoid repeats For all the proteins in the training and test sets we computed the values of zmax and , as was varied from 1 to 5.
The scatter plot in Figure 3 shows the-ratios for the final value = 2.1, obtained from our optimization procedure on the training set, versus the maximum z-scores for the sequences in the joint training and test sets, where solenoid proteins are represented by red crosses and non-solenoid protein domains by green crosses.
The large majority of solenoid proteins are characterized by high values of zmax and and are therefore found in the upper-right part of the diagram.
As explained in Methods, in order to make this observation more quantitative we estimate an optimal line which discriminates between solenoid and non-solenoid sequences that can be used to make our algorithm predictive.
We compute the signed distance of each representative point in the plot of Figure 3 from a set of lines, identified by the values of their slope and intercept (see Section 2).
A positive distance will be interpreted as a solenoidrepeat sequence; while a negative distance will mark a non-solenoid one.
The Matthews correlation coefficient CM is then computed for each line in the set and the whole procedure is repeated for different values of.
The procedure returns an optimal line (drawn in Fig.3) corresponding to an overall Matthews correlation coefficient CM = 0.52, which allows for a very accurate determination of the existence of repeated motifs.
In particular for the training set we obtain the optimal values = 2.1, m = 0.787, q = 6.591.
In order to assess the predictive power of the algorithm introduced so far, we have considered its outcome on the test set (Table 1).
It is remarkable to notice that the optimal line is again separating the region of solenoid proteins from the region of globular proteins with a very good accuracy.
In order to investigate the effect of the sequence profiles, we have repeated the experiments without the PSI-BLAST search.
As expected, the results are far worse with overall 19% sensitivity Fig.3.
Maximum z-score of the amplitudes (zmax, x-axis) and optimal-ratio ( , y-axis) are shown in the scatter plot for the joint training and test set of sequences.
The separation of the regions with mainly non-solenoids (green crosses, bottom left) and solenoid repeat sequences (red crosses, top right) is remarkable, even if few proteins lay on the opposite side, in the vicinity of the optimal line separating the two sets.
The result corresponding to the 3-solenoid domain of the antifreeze protein (PDB identifier: 1EZG) is shown as a blue square.
Table 1.
REPETITA benchmark results for the training and test sets Training set Test set Overall Sensitivity 70% 69% 70% Specificity 85% 83% 84% CM 0.54 0.51 0.52 and 81.4% specificity using the previously established optimal values ( = 2.1).
Re-optimizing the parameters without profiles (new = 3.4) does not improve the results sufficiently, with 57.5% sensitivity and 63% specificity overall.
While our approach combining sequence profiles and DFT gives remarkable results, DFT methods are able to identify well mainly tandem repeats with approximately the same size and sufficient number of copies.
Many solenoid structures have repeats of variable lengths that reduce the height of the corresponding DFT peaks.
Therefore, the method is not able to identify correctly few sequences which appear in fact as false positives in the plots of Figure 3.
However, the stability of the results obtained with the two sets of proteins shows the robustness and the validity of the method which can then be used as predictor.
For any new protein the position on the zmax plane can be used to estimate the existence of repeat units.
The distance from the optimal line will be used as a measure of the confidence of the estimation (see next section).
As an example of the application of the algorithm, Figure 3 also shows the output of the REPETITA method for the 3-solenoid domain of the antifreeze protein (PDB code: 1EZG).
The data for the 82 residue sequence of 1EZG has been computed using the web server.
i292 [10:14 15/5/2009 Bioinformatics-btp232.tex] Page: i293 i289i295 Detection of protein solenoid repeats by discrete Fourier transform Fig.4.
Comparison of REPETITA, RADAR and TRUST on the total set of sequences: the number of false positives (x-axis) is plotted against the number of true positives (y-axis).
Predictions are ranked according to the values of the parameter measuring the reliability of the methods (for REPETITA, it is the signed distance from the optimal line of Fig.3).
Two black circles are drawn to highlight REPETITA predictions with signed distance thresholds at +1 and 0, respectively.
Note that the first 25 predictions of REPETITA are all true positives.
3.3 Comparison to available methods We compared the predictions of REPETITAfor the total set, obtained by joining the training and the test set together, of sequences against two computational tools for the detection of protein repeats: RADAR (Heger and Holm, 2000) and TRUST (Szklarczyk and Heringa, 2004).
Given the limited number of solenoid sequences, the full set was used to benchmark on a larger sequence database, as performance on both subsets is similar for REPETITA.
It is also unknown whether RADAR and TRUST have been trained on some of the sequences.
Both make use of self-alignment and while the first one validates repeats by iterative profile alignment, the second one improves the predictions applying the concept of transitivity in order to detect missed sub-optimal self-alignments.
We chose to compare our method to these two algorithms in order to have a reference against a classical algorithm (RADAR) and a state of the art one (TRUST), both capable of addressing solenoid repeats without introducing gaps within or between repeats and not using a priori knowledge of repeat families.
Results of this comparison are summarized in Figure 4: for every method predictions are ranked according to the parameter assessing how good the latter are.
In the case of REPETITA, this parameter is the signed distance from the optimal line [given by Equation (9)].
In the case of TRUST and RADAR, we consider predictions where at least two repeat units have been detected.
Figure 4 shows the number of false positives vs. true positives.
Among all methods, REPETITA has the unique remarkable feature of yielding a virtual certainty in identifying solenoid repeats when the distance from the optimal line is larger than 1.0.
On the other hand, if one wishes to identify a larger number of solenoid repeats, false positives are more and more present.
Under these conditions, TRUST is performing better than REPETITA.
3.4 Estimating repeat periodicities Once the presence of a solenoid repeat has been established for a given protein sequence, the next issue to address is periodicity.
Fig.5.
Detection of periodicity of repeats: comparison of REPETITA, RADAR and TRUST.
Predictions were counted as correct if they were respectively within one residue of the full, half or double of the structural repeat length.
REPETITA outperforms both RADAR and TRUST.
Within the DFT method, the estimation of the period is straightforward and can be derived from the frequency number of the peak with the largest amplitude zmax.
The periods which would be estimated from nearby peaks naturally yield the confidence window for period prediction, which is defined as the interval [N/(n + 1),N/(n 1)] if the frequency with the largest amplitude is n. The results for the set of 105 solenoid repeat sequences are summarized in Figure 5, together with their comparison with the predictions yielded by REPPER and TRUST on the same sequences.
A given period is said to have been recognized correctly if it falls within the confidence window defined above.
Results are shown for predictions based on recognition of full period and adding the possibility of recognizing just half or double periodicity.
REPETITA performs much better than RADAR and at least as good as TRUST in recognizing full periods.
REPETITA, however, scores better than TRUST if we allow for recognition also of half or double periodicity, which might prove meaningful for the periodicity of physicochemical properties of the sequence.
We observed the fact that the second function (scale 2) of (Atchley et al., 2005), which represents local conformational propensity, is most frequently seen corresponding to the predicted repeat period (shown in Fig.5).
An example of the peak list with corresponding estimated periods for the 3-solenoid antifreeze protein (PDB code: 1EZG) is shown on the web page.
It is interesting to note that the method identifies the correct period for short repeats of up to 28 residues and after that breaks down the period in two halves.
From a structural point of view, this corresponds to-helix proteins where each helix, rather than pair, is seen as the structural repeat.
Arguments can be brought in favor of this view being correct, as LRR repeats for example are known to sometimes contain half repeats at the N-or C-terminus.
Taken together, periodicity results for the DFT method correspond to a substantially correct prediction.
i293 [10:14 15/5/2009 Bioinformatics-btp232.tex] Page: i294 i289i295 L.Marsella et al.4 DISCUSSION 4.1 Implications The DFT method combined with sequence profiles has been shown to be competitive with other state of the art methods in recognizing difficult structural repeats.
Furthermore, it definitely outperforms the existing methods tested in this work in the identification of the repeat length, once the solenoid sequence has been detected.
This is of relevance as DFT methods alone have, so far, not been widely used mainly due to the difficulty in discriminating globular from solenoid structures and to the difficulty of identifying repeats with variable length.
Given a proteins power spectrum, it is often difficult to judge whether there is any signal coming from tandem repeats or rather just the presence of spurious internal similarity.
The three novel strategies that we have adopted here proved instead to be promising.
The first is to use the five different similarity metrics proposed by Atchley, which cover a wider range of amino-acid characteristics, while most methods using the power spectrum are limited to hydrophobicity (Gruber et al., 2005; Murray et al., 2002).
Depending on the protein sequence we find interesting signals in other features, which yield a wider range of spectra from which to choose.
The second innovation is the proposal of two mathematical parameters, the maximum z-score zmax and the-ratio , that allow an immediate and systematic comparison across different proteins and make possible to identify with a good reliability the periodicity of the repeats, employing the properties of DFT.
Finally, the third relevant improvement to the method comes from the inclusion of sequence profiles in place of the single protein sequence as input to the DFT of Atchleys functions.
The use of a database search method such as PSI-BLAST (Altschul et al., 1997) to derive a sequence profile helps considerably to remove spurious hits.
Solenoid proteins, even if degenerate in sequence, appear to be significantly more conserved in terms of amino-acid characteristics at any given position.
Previous DFT methods used only the single query sequence to derive the features on which the power spectrum is calculated.
4.2 Possible improvements In the present implementation, the DFT method considers the entire protein sequence at once.
While this is justified for single domain proteins, it is an obvious disadvantage for long multi-domain proteins.
In such cases the repeat signal is averaged through the different regions and likely to fall below the detection threshold.
This issue can be addressed in future developments of the current implementation of REPETITA by running the DFT method with different sliding window sizes over the sequence.
The choice of sliding window size will require some optimization, as will the threshold values required to discriminate globular from solenoid sequences.
Intuitively, the longer the sliding window, the clearer the signal may become, at the expense of detecting shorter solenoid domains.
4.3 Reassessing local conformations One of the most striking conclusions that emerge from our work is the evidence that the propensity to form repeated proteins is mainly encoded in the function shown by Atchley to be related to local conformational properties (scale 2).
The importance of local information is a well established concept in molecular biology and it is underscored by the fact that many schemes for secondary structure prediction do well using just local sequence information.
However in the last years there is a growing body of evidence that also global folding is strongly dictated by short range information: it was observed that in folding models [such as Go-models (Go and Scheraga, 1976)] which rely on the knowledge of the native state, a complete and successful folding can be achieved only by biasing the sampling of dihedral angles towards their native values (Hoang and Cieplak, 2000).
More recently, the importance of local interactions in determining protein structure was the basis of the ROSETTA structure prediction algorithm (Simons et al., 1999) which is possibly the best performing method in de novo structure prediction.
This is also consistent with recent work by Fang and Shortle (Fang and Shortle, 2005) on knowledge-based local potentials.
Chikenji et al.have shown that local structure preferences strongly shape up the protein folding funnel (Chikenji et al., 2006), whereas Tosatto has elucidated that when applied to model selection in protein structure recognition, torsion angle potentials present the strongest correlation with model quality (Tosatto, 2005; Tosatto and Battistutta, 2007).
Our conclusion that even the presence of repetitive motifs in protein structures can be inferred by local properties is another step to understand their relevance for determining protein structure.
5 CONCLUSION We have presented a novel method for the detection of solenoid repeats from their amino-acid sequence.
The method is based on a DFT of the sequence using five different metrics and sequence profiles.
Parameters and thresholds were derived to allow the reliable discrimination of solenoid repeats from globular structures and the identification of their periodicities.
The comparison with two established methods demonstrates the performance of the method and highlights the relevance of local conformational preferences in solenoid repeats.
ACKNOWLEDGEMENTS The authors are grateful to Amos Maritan for insightful discussions.
L.M.
is profoundly indebted to Thomas Blicher for the critical reading of the article and acknowledges support of the EC through the Marie Curie project BiMaMoSi (MEXT-CT-2005-023311).
Funding: Rientro Dei Cervelli grant from the Italian Ministry for education, University and Research (MIUR) (to S.T.
); PRIN No.
2005027330 in 2005 and PRAT No.
CPDA083702 (to F.S).
Conflict of Interest: none declared.
Abstract A major focus of systems biology is to characterize interactions between cellular components, in order to develop an accurate picture of the intricate networks within biological systems.
Over the past decade, protein microarrays have greatly contributed to advances in proteomics and are becoming an important platform for systems biology.
Protein microarrays are highly flexible, ranging from large-scale proteome microarrays to smaller customizable microarrays, making the technology amenable for detection of a broad spectrum of biochemical properties of proteins.
In this article, we will focus on the numerous studies that have utilized protein microarrays to reconstruct biological networks including proteinDNA interactions, posttranslational protein modifications (PTMs), lectinglycan recognition, pathogenhost interactions and hierarchical signaling cascades.
The diversity in applications allows for integration of interaction data from numerous molecular classes and cellular states, providing insight into the structure of complex biological systems.
We will also discuss emerging applications and future directions of protein microarray technology in the global frontier.
Introduction Since the completion of major whole genome sequencing efforts, the scientific community has been faced with the challenge of identifying and characterizing the expressed gene products of given organisms [1].
The post-genomics era gave ).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese Ac birth to the field of proteomics that aimed to systematically chart the biochemical properties and functions of all expressed proteins [2].
With a global view in mind, we now strive to integrate complex omics-data from all molecular ranks.
The scope of proteomics is not limited to identifying proteinprotein interactions, but also includes identification of protein posttranslational modifications (PTMs) and of interactions with DNA and RNA sequences, lipids and glycans.
Weaving these layers together will allow us to construct the carefully tuned network that exists within live cells.
Improvements in high throughput proteomic technologies coupled with advances in genomics and bioinformatics have laid a framework to enable this level of research.
Two of the most powerful platforms for proteomic studies are mass spectrometry and protein microarray technologies.
ademy of Sciences and Genetics Society of China.
Published by Elsevier Ltd mailto:Heng.Zhu@jhmi.eduUzoma I and Zhu H/ Using Protein Microarrays to Construct Protein Networks 19 Although mass spectrometry is well suited for high throughput protein identification, quantification and PTM site mapping [3], it still has its disadvantages such as bias against low abundance proteins and modifications, as well as undersampling of complex proteomes [4].
On the contrary, the protein microarray platform avoids these limitations and is particularly suited for unbiased global profiling [5].
A protein microarray, also termed a protein chip, is created by immobilization of thousands of different proteins (e.g., antigens, antibodies, enzymes and substrates, etc.)
in discrete spatial locations at high density on a solid surface [6].
Depending on their applications, protein microarrays can be categorized into two varieties: analytical and functional protein microarrays.
Analytical protein microarrays are usually composed of well-characterized biomolecules with specific binding activities, such as antibodies, to analyze the components of complex biological samples (e.g., serum and cell lysates) or to determine whether a sample contains a specific protein of interest [7].
They have been used for protein activity profiling, biomarker identification, cell surface marker/glycosylation profiling, clinical diagnosis and environmental/food safety analysis [810].
Alternatively, functional protein microarrays are constructed by printing a large number of individually purified proteins and are mainly used to comprehensively query biochemical properties and activities of those immobilized proteins.
In principle, it is feasible to print arrays composed of virtually all annotated proteins of a given organism, effectively comprising a whole-proteome microarray [11].
In 2001 the Snyder group reported the fabrication of the first proteome microarray in the budding yeast, representing a major advance for the field [12].
In order to construct this array, approximately 5800 full-length yeast ORFs were individually expressed in yeast and their protein products purified as N-terminal GST-fusion proteins.
Each purified protein was then robotically spotted on a single glass slide in duplicate at high density to form the first proteome microarray, covering more than 75% of the yeast proteome.
More recently, proteome microarrays have been fabricated from the proteomes of viruses, bacteria, plants and humans [8,1316].
Functional protein microarrays have been successfully applied to identify proteinprotein, proteinlipid, proteinantibody, proteinsmall molecules, proteinDNA, proteinRNA, proteinlectin and lectincell interactions [8,9,12,14,1619], to identify substrates or enzymes for phosphorylation, ubiquitylation, acetylation and nitrosylation [11,2024], as well as to profile immune response [25].
In this review, we will focus on inventive applications for protein microarrays and the significant findings that contribute to understanding the complex interactomes within cells (Table 1).
Network construction A solid understanding of the molecular mechanisms of biological functions requires systematic profiling of dynamic interactions between biomolecules.
Processes such as transcriptional regulation, viral infection, numerous PTMs and proteinprotein interactions account for a small fraction of the potential molecular interactions within a cell but highlight how fundamental these networks are for essential functions.
High throughput technologies strive to provide an unbiased platform for charting these relationships at the proteome and genome scale.
In this section we will review several studies that demonstrate the utility of protein microarrays in reconstructing interaction networks.
ProteinDNA interactions With the completion of the human genome sequencing, decoding the functional elements is a major challenge.
Computational approaches have the power to identify conserved DNA regulatory elements; however, computational strategies cannot confidently predict the proteins that bind to these elements.
Identification of the interaction networks between the DNA functional elements and the human proteome requires extensive predictions and powerful high throughput techniques.
Hu and colleagues undertook a large-scale analysis of proteinDNA interactions (PDIs) using a protein microarray composed of 4191 unique full length human proteins, encompassing 90% of the annotated transcriptions factors (TFs) and members of many other protein categories, such as RNA-binding proteins, chromatin-associated proteins, nucleotide-binding proteins, transcription co-regulators, mitochondrial proteins and protein kinases [18].
The protein microarrays were probed with 400 predicted and 60 known DNA motifs.
As a result, a total of 17,718 PDIs were identified.
Many known PDIs and a large number of new PDIs for both well characterized and predicted TFs were recovered, as well as new consensus sites for human TFs.
Surprisingly, over 300 proteins that do not encode any known DNA-binding domains showed sequence-specific PDIs, suggesting that many human proteins may bind specific DNA sequences as a secondary function.
To further investigate whether the DNA-binding activities of these unconventional DNA binding proteins (uDBPs) were physiologically relevant, Hu et al.carried out in-depth analysis on a well-studied protein kinase, Erk2, to determine the potential mechanism behind its DNA-binding activity [18].
Using a combination of in vitro and in vivo approaches, such as electrophoretic mobility shift assays (EMSA), luciferase assays, mutagenesis, and chromatin immunoprecipitation (chIP), they demonstrated that the DNA-binding activity of Erk2 is independent of its protein kinase activity and it acts as a transcription repressor of transcripts induced by interferon gamma signaling [18].
This approach allows for sophisticated network mapping of proteinDNA interactions and enables the discovery of the uncharacterized DNA-binding proteins.
The emergence of uDBPs strengthens the ability to piece together the machinery involved in transcriptional regulation.
MAP kinase substrate phosphorylation network The mitogen-activated protein kinase (MAPK) signaling cascade involves a hierarchy of kinases that activate one another through consecutive phosphorylation events in response to extracellular or intracellular signals [15].
Standard methods have only been able to establish a few combinatorial connections from upstream MKK-activating kinases (MKKKs) to downstream MPK-activating kinases (MKKs), MAPKs and their cytoplasmic and nuclear substrates [26,27].
Constructing this complicated interconnected network necessitates a systematic unbiased high-throughput approach to avoid confounding issues of redundancy and functional pleiotropy [15].
Akin to Table 1 Applications of protein microarrays in diverse biological network construction Assay type Array content Type of probe Application Ref Network construction ProteinDNA interaction 4191 Human proteins DNA motif ProteinDNA interaction network [16] Kinase assay 2158 Arabidopsis proteins Protein kinase Signaling network [23] Ubiquitylation assay Yeast proteome Ubiquitylation enzymes PTM network [20] 9000 Human proteins Human protoarray, invitrogen Concentrated cell extract PTM network [19] Acetylation assay Yeast proteome Acetyltransferase PTM network [21] Pathogenhost interactions Viral kinase assay 4191 Human proteins Conserved viral kinases Viral PTM target network [31] Proteinprotein interaction 60 EBV viral proteins Human protein Proteinprotein interaction network [13] 4191 Human proteins Viral protein Proteinprotein interaction network [44] ProteinRNA interaction Yeast proteome BMV SLD RNA loop ProteinRNA interaction network [17] Biomarker identification Antigenantibody interaction 5011 Human proteins AIH patient sera Biomarker identification [47] 82 Corona virus proteins SARS patient sera Antibody profiling [48] E. coli K12 proteome IBD patient sera Biomarker identification [8] Lectinglycan interaction Yeast proteome Lectins Protein glycosylation profiling [53] 94 Lectins Live mammalian cells Cell surface biomarker identification [9] 20 Genomics Proteomics Bioinformatics 11 (2013) 1828 the protein microarray based kinase assays developed by Ptacek et al.[20], Popescu et al.employed high-density protein microarrays to identify novel MPK substrates.
The authors first determined which Arabidopsis thaliana MKKs preferentially activate 10 different MPKs in vivo and used the activated MPKs to probe Arabidopsis protein microarrays containing 2158 unique proteins to reveal their phosphorylation substrates [15].
The initial screen identified 570 nonredundant MPK phosphorylation substrates with an average of 128 targets per activated MPK.
With this data the authors were able to reconstruct a complex signaling cascade involving nine MKKs, 10 MPKs and 570 substrates [15].
Moreover, the resulting nodes and edges highlighted the specificity conserved within these interactions: 290 (51%) of MPK phosphorylation targets were hit by only one MPK and only 94 (16%) were phosphorylated by two or more MPKs [15].
Gene ontology (GO) analysis of effector substrates showed enrichment in TFs involved in the regulation of development, defense and stress responses [15].
The network that emerged from this study suggests the MAPK signaling cascade regulates transcription through combinatorial enzyme specificity and discrete phosphorylation events.
Ubiquitin E3 ligase substrate discovery Ubiquitylation is one of the most widespread PTMs and mediates a huge range of cellular events and processes in eukaryotes [28].
Understanding ubiquitin substrate specificity is a complex combinatorial question, as it is conferred by unique permutations of E1, E2 and E3 enzymes.
Lu et al.developed an assay to determine substrates of a HECT domain E3 ligase, Rsp5, using yeast proteome microarrays [22].
Over 90 novel proteins were found to be readily ubiquitylated by Rsp5, eight of which were validated as in vivo targets.
Deeper in vivo characterization of two substrates, Sla1 and Rnr2, revealed that Rsp5dependent ubiquitylation affects either the posttranslational process of the substrate or subcellular localization [22].
This design offers the ability to dissect the molecular mechanisms of a complex enzymatic cascade and gives the field a tool to understand how the system is organized globally.
Identification of non-histone substrates of protein acetyltransferases in yeast Acetylation is a major epigenetic PTM widely known for its role in regulating chromatin state.
However, it is suspected to regulate nonnuclear functions as well [29].
In yeast, no non-histone proteins were reported as substrates of histone acetyltransferases (HATs) and histone deacetylases (HDACs).
The catalytic enzyme, Esa1, of the essential nucleosome acetyltransferase of the complex, NuA4, is the only essential HAT in yeast [30], strongly suggesting that it may mediate acetylation of non-histone proteins critical for cell survival.
Another intriguing question was whether HATs could regulate activity of cytosolic proteins or even enzymes like protein kinases.
To comprehensively discover the non-chromatin substrates of the NuA4 HAT complex in the yeast proteome, Lin et al.developed in vitro acetylation reactions on the yeast proteome microarrays, containing 5800 yeast proteins, using NuA4 and [14C]-acetyl-CoA [23].
Over 90 non-histone proteins were readily acetylated by the NuA4 complex.
Although it was expected that the majority of the substrates would be involved in nucleosome assembly and histone binding categories, a significant number of the identified substrates were cytoplasmic proteins and metabolic enzymes [23].
Twenty proteins involved in a variety of cellular functions such as metabolism, transcription, cell cycle progression, RNA processing and stress response were selected for further validation.
Standard double-immunoprecipitation techniques were used to validate 13 of the 20 substrates, including phosphoenolpyruvate carboxykinase (Pck1p).
To understand the physiological relevance of nonchromatin acetylation, the authors focused on the cytosolic enzyme Pck1p to explore a connection between acetylation and metabolism.
Tandem mass spectrometry (MS/MS) identified lysine 19 (K19) and K514 as the acetylation sites of Pck1p and site-directed mutagenesis revealed that acetylation of K514 is critical for its enzymatic activity and promotes exten Uzoma I and Zhu H/ Using Protein Microarrays to Construct Protein Networks 21 sion of life span in yeast growing under starvation conditions.
These findings demonstrate a functional role for non-chromatin acetylation in yeast metabolism and longevity.
Based on GO analysis, acetylation may regulate several other cellular processes as well.
In a follow up study, Lu et al.investigated the impact of acetylation on another NuA4 substrate, Sip2, a regulatory subunit of the SNF1 kinase complex (yeast AMPK).
Based on the MS/MS analysis and site-directed mutagenesis studies, the authors found that Sip2 acetylation enhances its interaction with the catalytic subunit Snf1 and inhibits Snf1s kinase activity [31].
As a result, phosphorylation of one of Snf1s downstream targets, Sch9 (homolog of Akt/ S6K), is decreased, ultimately leading to slower growth but extended replicative life span.
Finally, the authors demonstrated that the anti-aging effect of Sip2 acetylation is independent of extrinsic nutrient availability and TORC1 activity.
These studies are now echoed by recent discoveries of many mitochondrial and cytosolic enzymes as substrates of acetyltransferases in higher eukaryotes via MS-based PTM profiling [3234].
Global ubiquitylation substrate discovery from cell extracts Readily generating a snapshot of global protein PTM profiles under various cellular conditions could be considered the Holy Grail for those researching PTMs.
General PTM substrate identification strategies require enrichment from a cell extract sample followed by MS or in vitro assays using purified components.
While both approaches have their strengths and weaknesses, a hybrid of the two is possible.
The use of concentrated mammalian cell extracts in combination with protein microarrays can serve to identify PTM targets in a semiin vivo setting while alleviating the challenge of analyzing a complex mixture.
Merbl and Kirschner generated cell extracts that replicate the mitotic checkpoint and anaphase release to identify differentially regulated polyubiquitylation substrates [21].
The synchronized cell extracts were incubated with Invitrogens Human ProtoArray composed of 8000 proteins and the resulting polyubiquitylated proteins were detected with antibodies directed to ubiquitin chains [21].
The authors expected to recover substrates of the anaphase promoting complex (APC), the major ubiquitin ligase in mitosis and G1.
To differentiate polyubiquitylation substrates of the APC from other ligases, Merbl and Kirschner designed three experimental set ups.
All cell extracts were arrested with nocodozole as the control which inhibits the APC, in the second condition the sample was released from checkpoint arrest with the addition of UbcH10, an E2 ligase, and the final condition was supplemented with both UbcH10 and a specific inhibitor of APC.
Approximately 132 proteins were differentially polyubiquitylated, 11 of which were known APC substrates, confirming the validity on the experimental design.
Validation studies performed in rabbit reticulocyte lysate confirmed the degradation/ ubiquitylation of 7 novel APC substrates [21].
This study demonstrates the efficacy of using protein microarrays in combination with cell extracts to recapitulate the global PTM signature in a specific cellular state.
Pathogenhost interactions Protein microarrays allow for exploration of hypotheses that cannot be addressed by standard methods.
Investigating the interactions between viral encoded proteins and the proteins within the infected host has been an important yet cumbersome task.
Protein microarrays composed of either the host or the viral proteome can be fabricated and subsequently used to examine the relationships between the viral machinery and the host.
This in vitro approach recapitulates viral infection in that the viral genome/proteome are allowed to physically interact with the host.
The Hayward and Zhu groups have recently developed this new paradigm to examine direct interactions between viral and host proteins [14,35,36], leading to a deeper understanding of the mechanisms by which the viral proteins hijack the host as well as uncovering the direct targets of major viral enzymes.
Herpesvirus kinase-phosphorylome The human a, b, and c herpesviruses cause diseases distinct from one another, ranging from mild cold sores to pneumonitis, birth defects and cancers [35].
Although the viruses are different, once they enter the host cells they all must reprogram cellular gene expression, sense cell-cycle phase, modify cell-cycle progression and reactivate the lytic life cycle to produce new virions to spread infection [37].
Many lytic cycle genes involved in replication of the viral genomes are highly conserved across the herpesvirus family.
For example, each herpesvirus encodes for an orthologous serine/threonine kinase [38] that shares structural similarity with human cyclin-dependent kinases (CDKs) [39] and phosphorylates the substrates of CDKs [38].
The ability of viral kinase to mimic host CDKs results in hijacking of key pathways to potentiate their own replication.
Particular cellular phosphorylation events are observed during herpes infection and specific phosphorylation of antiviral drugs in infected cells are mediated by the conserved viral kinases [40].
Identifying the collective host targets of the viral kinases would reveal the commonly shared mechanisms and signaling pathways among different herpesviruses to promote their lytic replication.
This knowledge will increase the therapeutic target options necessary for developing pan-antivirals.
To test this idea, Li et al.utilized the human transcription factor (TF) proteome array containing 4191 human proteins to identify commonly shared substrates of herpesvirus-encoded kinases [35].
Parallel kinases assays were performed using the four viral kinases, UL31, UL97, BGLF4 and ORF36, which is encoded by herpes simplex type 1 (HSV1), human cytomegalovirus (HCMV), Epstein-Barr virus (EBV) and Kaposi Sarcoma associated-virus (KSHV), respectively [38].
In total, 643 nonredundant substrates were identified across the four kinases and 110 substrates were targets of at least three kinases.
GO analysis of the 110 shared substrates indicates that DNA damage functional class was significantly enriched.
Among the DNA damage proteins, TIP60 was selected as a lead candidate for regulation of viral replication, due to its roles in DNA damage as well as transcriptional regulation through its HAT activity.
Phosphorylation of TIP60 by BGLF4 in EBV-infected B cells was validated during further analysis.
BGLF4 is known to phosphorylate multiple EBV proteins and only a small number of host proteins [38,41].
The functions of its previously-characterized targets are varied, implying that the kinase plays multiple roles to promote viral replication [41].
It is expressed in the early phase of the lytic infection cycle and is localized mainly in the nuclei of 22 Genomics Proteomics Bioinformatics 11 (2013) 1828 EBV-infected cells [42].
BGLF4 knockdown revealed that it is critical for release of infectious virus during viral lytic reactivation [41].
Subsequent experiments demonstrated that BGLF4mediated phosphorylation enhanced TIP60 HAT activity by 10-fold, linking the phosphorylation event to viral replication.
They also demonstrated the importance of phosphorylation of host DNA damage proteins for viral replication.
More specifically, phosphorylation and activation of TIP60 by BGLF4 triggers EBV-induced DNA damage response (DDR) and promotes positive transcriptional regulation of critical lytic genes involved in viral replication.
Lastly, the study confirmed that TIP60 was also required for efficient lytic replication in HCMV, KSHV and HSV-1.
Taken together, this unbiased approach provides a novel paradigm for discovery of conserved targets of viral enzymes.
While herpes kinases have been credible therapeutic candidates, knowing their targets and the signaling pathways they exploit will better enable the development of widely effective antiviral drugs.
BGLF4SUMO2 In a follow up study, Li et al.took the inverse approach that employed a herpesvirus EBV protein microarray to assess human-host protein binding events [14].
Small ubiquitin-related modifier (SUMO) is covalently attached to proteins via an enzymatic cascade analogous to the ubiquitin pathway.
SUMO is involved in a broad range of cellular processes including signal transduction, regulation of transcription, DNA damage response and mediation of proteinprotein interactions [43,44].
Both latent and lytic EBV proteins interact with components of the SUMO machinery [14,44].
While covalent modification by SUMO is more commonly understood, noncovalent interactions with SUMO also contribute to SUMO effector signaling [43,44].
Noncovalent binding to SUMO is often mediated through SUMO-interaction motif (SIM) domains on target proteins [43,44].
To comprehensively identify the EBV proteins that bind to the SUMO moiety, the authors fabricated a protein microarray of full length proteins from EBV and KSHV individually purified from yeast.
The array was used to perform a proteinprotein binding assay using the SUMO2 paralog.
They identified 11 EBV proteins as potential SUMO partners, including BGLF4, a conserved kinase [14].
As BGLF4 is known to play a multitude of roles in EBV, the authors pursued the importance of the cellular PTM in BGLF4 function.
The BGLF4 SIM domains were mapped and when mutated at both the N-and C-terminal SIMs, the intracellular localization of the kinase shifted from nuclear to cytoplasmic.
A mutation in the N-terminal SIM showed largely nuclear localization, whereas the C-terminal SIM mutation generated an intermediate phenotype with nuclear and cytoplasmic expression.
The authors found that BGLF4 inhibits SUMOylation of lytic cycle transactivator ZTA and demonstrated that the SIM domains as well as kinase activity are required for inhibition [14].
SIM domains of BLGF4 were also shown to be necessary for suppressing global SUMOylation, inducing cellular DDR and promoting EBV lytic replication.
The virus takes advantage of the SUMOylation system by encoding proteins that are SUMO modified and those that bind to SUMO [14].
As previously mentioned SUMO is involved in DDR, which is further supported by the finding that BGLF4 appears to interact with sites of DNA damage via SUMO binding, revealing an additional mechanism promoting EBV-mediated DDR and lytic replication.
SUMO interaction is as important as the kinase activity for the function of BGLF4.
LANA-interacting cellular protein Another variation of protein microarray used for investigating pathogenhost interaction involves the human TF array to profile the interactions between KSHV latency proteins and host proteins.
In KSHV-associated malignancies, majority of the tumor cells are latently-infected and express viral latency proteins including LANA [45].
LANA functions to maintain KSHV latency by driving viral replication [46,47], promoting dysregulated cell growth [48] and dynamically regulating both viral and cell gene transcription [4951].
Identification of LANAs interacting partners would provide new insights into the mechanisms LANA uses to maintain latent infection.
LANA has been an attractive target and previous efforts to identify LANA binding proteins have attempted yeast two-hybrid screens [52], glutathione S-transferase (GST) affinity immunoprecipitation [53] and MS, resulting in apparent approachdependent binding partners [54].
In a recent study, Shamay et al.purified FLAG-tagged LANA and probed it against the human TF array, which recovered 61 candidate binding partners [36].
Eight candidates validated by co-immunoprecipitation assays included TIP60, protein phosphatase 2A (PP2A), replication protein A (RPA) and XPA.
LANA-associated TIP60 retained its acetyltransferase activity and showed enhanced stability, which is consistent with Li et al.s finding that TIP60 in critical for KSHV lytic replication (see above).
The binding interactions between LANA, RPA and XPA seem to echo LANAs role in DNA damage, but further characterization of the LANAs ability to bind to additional RPA complex members, RPA1 and RPA2, spawned a new hypothesis that LANA may also regulate host telomere length.
To test this hypothesis, the authors performed ChIP assays with anti-RPA1 and-RPA2 antibodies using primers specific to the telomere regions and found that the presence of LANA drastically reduced the recruitment of both RPA1 and RPA2 to the host telomeres, while it had no impact on the protein level of the RPA complex.
This observation raised the possibility that LANA might affect telomere length.
Using Southern blot analysis of terminal restriction fragments, the standard method for quantifying telomere length, the authors demonstrated that the average length of telomeres was shortened by at least 50% in both LANA-expressing endothelial cells and KSHV-infected primary effusion lymphoma cells [55].
Biomarker identification Biomarker identification represents a major effort in modern biomedical and clinical research, as it allows for better screening methods, diagnosis criteria, prognosis predictions and ultimately superior treatment for a broad range of diseases.
Traditionally, biomarker discovery has utilized popular methods such as MS, ELISA, gene expression and antibody arrays to profile serum samples [56].
In recent years, protein microarray technology has extended into clinical proteomics and is becoming a powerful tool for biomarker discovery.
Proteins on functional protein microarrays were originally viewed as Figure 1 Scheme of the two-phase strategy for biomarker identification in human autoimmune diseases taking AIH as example In Phase I, a small cohort is used to rapidly identify a group of candidate biomarkers via serum profiling assays on a human protein microarray of high cost.
Because a small number of microarrays are needed, cost of the experiments is relatively low.
In Phase II, a focused protein microarray of low cost is fabricated by spotting down purified candidate proteins.
A much larger cohort is then assayed on these arrays in a double blind fashion to validate the candidates identified in Phase I. AIH, autoimmune hepatitis; ASGR2, asialoglycoprotein receptor 2; PBC, primary biliary cirrhosis; HBV, hepatitis virus B; HCV, hepatitis C virus; SLE, system lupus erythematosus; pSS, primary Sjogrens syndrome.
Uzoma I and Zhu H/ Using Protein Microarrays to Construct Protein Networks 23 substrates and binding partners, but when applied to immunology, the proteins on the array could be potential antigens associated with certain diseases.
By comparison, protein microarray based-serum profiling is much more sensitive and can be performed at higher throughput while requiring less amount of sample.
Here we will review a variety of clinically-relevant applications for protein microarrays in biomarker identification.
Autoantigen discovery for autoimmune hepatitis In many autoimmune diseases, there is an unmet clinical need for cost-effective and accurate diagnostic methods.
Improving upon the current standard requires discovery and characterization of reliable autoantigens coupled with sensitive and reproducible assays.
Take autoimmune hepatitis (AIH) as an example: AIH is a chronic necroinflammatory disease of human liver with little known etiology.
Detection of non-organ-specific and liver-related autoantibodies using immunoserological approaches has been widely used for diagnosis and prognosis [57].
However, these traditional autoantigens, such as anti-smooth muscle autoantibodies (SMA) and anti-antinuclear autoantibodies (ANA) are often mixtures of complex biological materials.
Unambiguous and accurate detection of the disease demands identification and characterization of these autoantigens.
Therefore, Song et al.fabricated a human protein microarray of 5011 non-redundant proteins that were expressed and purified as GST fusions in yeast [25].
There are several advantages associated with producing human proteins in yeast rather than bacteria: (1) higher solubility, (2) higher yields of large proteins (e.g., >50 kD), (3) better preserved conformation of proteins and (4) less immunogenicity of proteins when produced in yeast than in Escherichia coli [7,12,17].
However, unlike a viral or bacterial protein microarray, a significant obstacle to the use of a human protein microarray of high content is the high cost.
For example, cost for a human protein array of 9000 proteins can exceed $1000 per array.
In order to reduce the cost, Song et al.developed a two-phase strategy to identify new biomarkers in AIH.
Phase I is designed for rapid selection of candidate biomarkers, which are then validated in Phase II (Figure 1).
In Phase I, serum samples from 22 AIH patients and 30 healthy controls were selected and individually used to probe the human protein microarrays at a 1000-fold dilution, followed by detection of bound human autoantibodies using a Cy-5-conjugated anti-human IgG antibody.
Statistical analysis revealed 11 candidate autoantigens.
To validate these candidates and to avoid a potential overfitting problem (see below), which is especially likely when dealing with a small sample size, the 11 proteins and 3 positive controls were re-purified to build a large number of low-cost small arrays for Phase II validation.
These arrays were then sequentially probed with serum samples used in Phase I and serum samples obtained from an additional 52 AIH, 50 primary biliary cirrhosis (PBC), 43 hepatitis B virus (HBV), 41 hepatitis C virus (HCV), 11 system lupus erythematosus (SLE) and 11 primary Sjogrens syndrome (pSS) patients.
As negative controls, they also included 26 serum samples from patients suffering from other types of severe diseases and 50 samples from healthy subjects.
Three new antigens, RPS20, Alb2-like and dUTPase, were identified as highly AIH-specific biomarkers with sensitivity of 47.5%, 45.5% and 22.7%, respectively, which were further validated with additional AIH samples in a double-blind design.
Finally, they demonstrated that these new biomarkers could be readily applied to ELISA-based assays for clinical diagnosis and prognosis [25].
24 Genomics Proteomics Bioinformatics 11 (2013) 1828 This study represents a new paradigm in biomarker identification using protein microarrays for three reasons.
First, a manageable number of candidate biomarkers can be rapidly identified at low cost because fewer expensive protein microarrays of high-content are needed in the first phase of this two-phase strategy.
Second, by using small arrays comprised of selected candidate proteins, the validation step can be rapidly carried out with a much larger cohort at low cost.
This validation step is extremely important for avoiding the overfitting problem associated with statistical analysis in biomarker or classifier identification, especially when dealing with a small cohort (e.g., <40).
Overfitting is a problem in which a statistical model describes random error or noise instead of the underlying relationship.
It generally occurs in biomarker identification when the system is excessively complex, such as having too many individual-to-individual variations relative to the number of samples used.
As a result, biomarkers that have been overfit generally have poor predictive performance.
Therefore, testing an additional, larger cohort in a doubleblind design is an effective way to rule out overfit biomarkers.
Third, the authors developed ELISA-based assays to examine the performance of the validated biomarkers with additional samples.
These newly identified biomarkers could serve as a translational step toward clinical practice.
SARS-CoV diagnosis Protein microarrays can also be used as a diagnostic tool for infectious diseases.
Severe acute respiratory syndrome (SARS) is an infectious disease, caused by a novel coronavirus (CoV), which appeared in Guangdong, China in November 2002.
As of March 2003, the virus had spread globally and by July over 8000 SARS cases and approximately 800 deaths were reported worldwide [58].
At the time of the outbreak, no effective treatment of SARS was available, thus isolation and infection control were the best way to limit the spread of the virus.
Therefore, rapid and reliable, early diagnosis is critical to control such an epidemic.
Zhu et al.developed the first virus protein microarray, which included all the SARS-CoV proteins as well as proteins from five additional coronaviruses that can infect human (HCoV-299E and HCoV-OC43), cow (BCV), cat (FIPV) and mouse (MHVA59) [13].
The SARS microarray was used to screen sera from infected and noninfected individuals in a double-blind format.
The samples were quickly distinguished as SARS positive or SARS negative based on the presence of human IgG and IgM antibodies against SARSCoV proteins, with a 94% accuracy rate compared to a standard ELISA diagnostic test.
The SARS microarray improved the sensitivity of the assay 50-fold over the ELISA and dramatically reduced the amount of sample required.
This method may be suitable for diagnosis for many viral infections.
Novel serological biomarkers for inflammatory bowel disease The two most common subtypes of inflammatory bowel disease (IBD) are Crohns disease (CD) and ulcerative colitis (UC).
They are idiopathic in nature and are both characterized by an abnormal immunological response in the gut [59].
IBD is clinically thought to have autoimmune etiology, although, anti-microbial antibodies to normal bacteria are present in the sera of patients, leading to the pathogenesis of the disease [8].
The known serological antibodies are currently used as partial diagnostic criteria as they are not robust enough to stand alone [60].
Chen et al.elected to use an E. coli proteome microarray to characterize the differential immune response (serum anti-E. coli antibodies) in patients with CD and UC compared to healthy controls (HC).
The microarray included 4256 E. coli proteins, encompassing the vast majority of the proteome of E. coli K12 strain.
The sera from HC (n = 29), CD (n = 66) and UC (n = 39) were profiled using this array and the reactive anti-E. coli antibodies were detected with anti-human IgG antibodies.
Data analysis revealed differential immunogenic response to 417 proteins between these three groups: 169, 186 and 19 were highly immunogenic in HC, CD and UC, respectively.
Two robust sets of novel serological biomarkers were identified that can discriminate CD from HC or UC with >80% overall accuracy and sensitivity [8].
This is the first study to identify serological biomarkers in human immunological diseases with respect to the entire proteome of a microbial species.
The underlying molecular pathology of other immune system related diseases can also be examined with this proteome microarray approach.
Lectin study: proteinglycan interaction Cell surface glycosylation is a complex and highly-varied PTM that in turn is not amenable to standard high-throughput techniques.
Glycosylation is present on the surface of all vertebrate cells, and it serves to distinguish cell types through very delicate differences [9].
It is also shown to be associated with cell differentiation, malignant transformation and subcellular localization [6165].
Glycan binding proteins, known as lectins, are used to characterize glycosylation marks due to their ability to discriminate sugar isoforms [66].
Lectin microarrays have already been employed to characterize glycoproteins and lysates [67,68], however, they have not been used to systematically profile cell surface glycosylation signatures of mammalian cell types.
Such studies have the potential to provide a tool for distinguishing normal versus abnormal cell surface profiles based on glycanlectin interactions.
Tao et al.fabricated a lectin microarray composed of 94 non-redundant lectins selected for defining cell surface glycan signatures [5].
Using 23 well-studied mammalian cell lines, the authors developed a systematic binary analysis of binding interactions of the selected lectins and cell types.
They observed a broad range of binding potential and specificity across cell types, implying a high level of variation in cell surface glycans within mammalian cell types.
For example, less than 20 lectins could capture the hESC, Caco-2, D407 and U937 cells, while more than 50 lectins captured the HEK293, K1106 and MCF7 cells [9].
Interestingly, similar cell types such as various breast cancer cell lines did not reveal overlapping lectin binding profiles, indicating lectins can discern subtle differences between physiologically-related cells.
To further test the utility of the lectin microarray for biomarker discovery, Tao et al.analyzed lectin binding in a model cancer stem-like system by comparing cell surface glycan signatures of all 24 cell types [9].
Focusing on MCF7, a breast cancer cell line that adopts cancer stem-like phenotypes when grown under specific conditions, the authors demonstrated that different growth conditions give rise to distinct lectin binding profiles that can distinguish these cancer cell subpop Uzoma I and Zhu H/ Using Protein Microarrays to Construct Protein Networks 25 ulations [9].
The lectin LEL was identified as a biomarker that can discriminate between MCF7 subpopulations.
The authors propose that combined with other stem cell enrichment methods, lectin microarray technology is a potential tool for identifying cell surface markers in tumors, enabling the discovery of cancer stem cell-like targeted therapies.
Perspectives Over the past decade, protein microarrays have evolved into a powerful and versatile tool for systems biology.
They capitalize on femtomolar sensitivity, profiling full proteomes and highthroughput yet straightforward assays.
We have described their utility for a myriad of applications that have resulted in impactful scientific findings including pathogenhost interactions, biomarker identification, unconventional transcription factors and PTM substrates (Figure 2).
While protein microarrays leverage the advantage of uniform protein expression, for proteomics, their impact is limited by the extent of coverage.
A remarkable advance was put forth by the Zhu laboratory with the construction of the first human proteome microarray containing over 17,000 full length proFigure 2 Reconstituted interaction networks in cellular systems genera Interaction mapping with protein microarrays has been applied to nu networks.
A. Li et al.probed a human transcription factor (TF) microar the host targets of the viral kinases [35].
Verified interactions between t microarray, Lu et al.identified the substrates of the HECT E3 ligase subgroups of substrates based on function.
C. The A. thaliana MAP protein microarray [15] (adapted with permission from Dr. Savithram depicts the MKKs (upper nodes), MPKs (middle nodes) and substrat DNA-binding proteins (uDBPs) was characterized using the TF mic similarity and proteins of different functional classes are color-coded.
The E. coli proteome microarray was used to identify differentially imm the heat map [8].
The yellow and blue colors indicate high and low Crohns disease.
teins [16], the largest available to date (Figure 3).
The discovery potential for this technology is dramatically increased by expanded proteome coverage.
Multiple large-scale studies intended to link PTM substrates with their upstream enzymes, such as kinases, SUMO E3 ligases and ubiquitin ligases, are ongoing with the human proteome microarrays.
As the number of bona fide PTMs increase and more substrates are found to acquire numerous modifications, we cannot ignore coregulation of PTMs.
Directed studies to recapitulate crosstalk between enzymes, PTMs and their common substrates are possible with protein microarrays and may uncover key nodes of regulation and critical points where pathways converge.
While MS is an ideal technology for the discovery of novel PTMs, such as the crotonylation PTM [69], it is not well suited to identify the enzymes responsible for novel modification.
The richness of 17,000 natively-purified proteins on a single surface provides an ideal platform for discovery of novel enzyme function.
The human proteome array can also be harnessed as a tool for high-throughput characterization of monoclonal antibody (mAb) specificity from hybridomas [16].
The capabilities of microarray technology are further expanding with the development of label-free optical techted through protein microarray studies merous organisms to achieve diverse representations of molecular ray with four conserved kinases encoded by herpesviruses to reveal he viral target host proteins are shown.
B.
Using a yeast proteome Rsp5 [22].
Through gene ontology analysis Rsp5 was linked to kinase signaling network was reconstructed using an Arabidopsis ma P. Dinesh-Kumar).
The hierarchical phosphorylation network es (bottom nodes).
D. DNA binding specificity of unconventional roarray [18].
The uDBPs are clustered based on target sequence C denotes consensus sequences for each sub-branch are shown.
E. unogenic proteins between HC and CD patient samples depicted in immunogenic responses, respectively.
HC, healthy controls; CD, Figure 3 The human proteome microarray A.
The human proteome microarray composed of 16,368 unique full-length recombinant proteins printed in duplicate on Full Moon glass slides.
To monitor the quality, the microarray was probed with anti-GST monoclonal antibody, followed by Alexa-555 secondary antibody to visualize the signals.
The proteins positively detected by the anti-GST antibody are represented in green.
B.
Cellular distribution of the proteins included in the human proteome microarray.
ER, endoplasmic reticulum; Mito, mitochondria.
26 Genomics Proteomics Bioinformatics 11 (2013) 1828 niques that monitor the real-time dynamics of biomolecular interactions.
Oblique-incidence reflectivity difference (OIRD) is an emerging technique that measures the changes in reflectivity of polarized light [70,71].
OIRD has recently been applied to DNA and protein microarrays and has successfully determine association and dissociation rates of biomolecular interactions in a high-throughput format [72,73].
Constructing complex interaction networks involving the full range of cellular components is critical for deciphering how organisms are organized and is essential for understanding the aberrant changes that result in diseases.
We have discussed the vast applications of protein microarrays for global characterization of interactomes and the significance of their findings for creating a comprehensive view of biological systems.
In conclusion, protein microarray technology is no longer in its infancy and will undoubtedly serve as an invaluable tool for proteomics and systems biology.
Competing interests The authors have declared no conflicts of interest.
Acknowledgements We thank the NIH for supporting this work through the Grants awarded to HZ (Grant No.
RR020839, DK082840, RO1GM076102, CA125807, CA160036 and HG006434) and an F31 NRSA Predoctoral Fellowship to IU (Grant No.
5F31GM096716).
Abstract Understanding the mechanism of complex human diseases is a major scientific challenge.
Towards this end, we developed a web-based network tool named iBIG (stands for integrative BIoloGy), which incorporates a variety of information on gene interaction and regulation.
The generated network can be annotated with various types of information and visualized directly online.
In addition to the gene networks based on physical and pathway interactions, networks at a functional level can also be constructed.
Furthermore, a supplementary R package is provided to process microarray data and generate a list of important genes to be used as input for iBIG.
To demonstrate its usefulness, we collected 54 microarrays on common human diseases including cancer, neurological disorders, infectious diseases and other common diseases.
We processed the microarray data with our R package and constructed a network of functional modules perturbed in common human diseases.
Networks at the functional level in combination with gene networks may provide new insight into the mechanism of human diseases.
iBIG is freely available at http://lei.big.ac.cn/ibig.
Introduction Among many great challenges in the field of biological sciences, disease mechanism is of imminent relevance to every single person in the whole world.
From the perspective of cellular , leihx@big.ac.cn (Lei H).
eijing Institute of Genomics, tics Society of China.
g by Elsevier jing Institute of Genomics, Chinese A networks, complex diseases are progressive transformations of the cellular network.
For heritable diseases, the network is flawed at the very beginning.
For chronic complex diseases, lifelong gene and environment interaction results in dynamic adjustment and, at certain points, breakdown of the network.
Understanding the specific destruction of the network in specific diseases and at specific stages is the key starting point for subsequent design of rescue or remedial strategies.
Network analysis has been increasingly utilized in interpreting high throughput data.
Networks can be constructed purely based on gene expression information, including transcriptional regulatory networks [1] and co-expression networks [2].
Networks can also be built upon prior knowledge of proteinprotein interactions [3].
Several network tools have been cademy of Sciences and Genetics Society of China.
Production and hosting Sun J et al/ iBIG, A Network Tool for Integrative Biology 167 implemented as Cytoscape plugins, including BisoGenet [3], MIMI [4] and APID2NET [5], which mainly focus on protein interactomes.
Network building is also provided by web services such as STRING (http://string-db.org/).
Since cellular networks consist of various types of interaction and regulation, networks reflecting this complex scenario will provide better insight into the problem in hand.
In this work, we developed a network tool iBIG (stands for integrative BIoloGy), which incorporates information on both interaction and regulation.
The main architecture consists of a client interface by HTML and JavaScript, a server-side script written in CakePHP and a MySQL database.
The network visualization is implemented based on the Cytoscape Web application programming interface (API).
An important R package ArrayPro (http://lei.big.ac.cn/download/open_download_page) is also provided for processing of microarray data and construction of networks based on functional gene sets.
To illustrate this unique feature, an example of network perturbation in common human diseases is provided.
Design and implementation iBIG architecture iBIG is a client-server based application following the CakePHP framework, a popular MVC model.
In MVC models, model (M) is used to access database and pass the result to control (C), which responds to client request and view (V) is set according to the result from control.
The three main units in MVC models include client, server and database (Figure 1).
When constructing networks of functional gene sets, ArrayPro can remotely access our database.
Visualization can be used for networks generated by iBIG or other network tools.
iBIG has been extensively tested on IE8 and Firefox.
Database design A unique internal gene ID is used to represent every gene and its product.
The IDs from public databases are converted to the internal gene IDs.
Our integrated database mainly consists of two parts: gene interaction and gene annotation.
The Figure 1 iBIG architecture The client, server and database correspond to view, control and model in the MVC model, respectively.
ArrayPro can access the database by RMySQL.
Visualization is developed based on Cytoscape Web API.
interaction data are further classified into primary interaction, secondary interaction and network regulation.
Primary interactions include pathway interaction, protein complex interaction and general proteinprotein interactions.
Secondary interactions include gene-gene interaction, chromosome position interaction, transcription factor-target gene interaction and kinase-target interaction.
To facilitate the understanding of regulatory relationships, the latter two together with microRNA-target gene interactions form the network regulation category.
Sources of the integrated database Pathway interactions were collected from Kyoto Encyclopedia of Genes and Genomes (KEGG) (http://www.genome.jp/ kegg/) by R package KEGGSOAP (available for downloading pathway gene information), WikiPathway (http://www.wikipathways.org/), NCI-Nature (http://pid.nci.nih.gov/), PathwayCommons [6] (this composite pathway database can be selected independently or further merged with other databases), Reactome (http://www.reactome.org/) and EHMN [7].
Protein complex interactions were collected from MIPS (http://mips.helmholtz-muenchen.de/genre/proj/corum).
Proteinprotein interactions were collected from HPRD [8], BioGrid (http://thebiogrid.org/), DIP [9], MINT [10], IntAct [11] and BIND [12].
Gene-gene interactions were downloaded from BioGrid.
Chromosome position interactions were collected from the Molecular Signatures Database (MSigDB) of GSEA (http://www.broadinstitute.org/gsea/msigdb/index.jsp).
Transcription factor-target gene interactions were collected from a recent paper [13].
Kinase-target interactions were collected from PhosphoSitePlus (http://www.phosphosite.org/).
MicroRNA-target gene interactions were collected from TarBase [14], miRecords [15] and MicroCosm [15].
The network annotation includes the following information: pathway, protein complex, chromosome position, transcription factor, microRNA, kinase, epigenetics-related gene, housekeeping gene, tissue-specific gene, gene ontology (GO) biological process, GO molecular function and GO cellular component.
We used the same data as in the interactions for pathway, protein complex, chromosome position, transcription factor, microRNA and kinase.
Epigenetics-related genes were collected from GO and NCBI Entrez Gene.
Housekeeping genes were collected from three papers [1618].
Tissue-specific genes were collected from a recent paper [17].
Data for GO biological process, molecular function and cellular component were collected from MSigDB of GSEA.
Construction of gene networks Construction of a gene network consists of several steps: (1) submit a gene list with one gene per line (gene symbol and Entrez gene ID are supported); (2) select databases for primary or secondary interaction (selection of primary interactions is mandatory while second interactions are optional); (3) set the network filtering strategy to reduce network complexity; (4) select databases for network regulation (optional) if the user wants to know about the upstream regulators and downstream targets; and (5) choose preferred network annotation to illustrate the functions of genes in the network.
The generated network can be visualized online directly or downloaded in 168 Genomics Proteomics Bioinformatics 11 (2013) 166171 XGMML format.
The online visualization facilitates the interactive refinement of the network by modifying the selections.
Construction of networks with functional gene sets ArrayPro is a supplementary R package mainly for microarray data processing, including data preprocessing, identification of differentially expressed genes (DEGs), functional enrichment analysis and construction of networks with functional gene sets.
Networks with functional gene sets can be built by calculating the correlation among selected functional gene sets.
In our recent work, ArrayPro has been applied to the investigation of network perturbation in Alzheimers disease [19].
Calculation of relationship among functional gene sets ArrayPro is an independent R package which can be downloaded from http://lei.big.ac.cn/download/open_down load_page.
One of the functions of ArrayPro is to build networks with functional gene sets instead of individual genes.
The detailed procedures are described as follows.
(1) Genes belonging to the relevant gene sets are selected.
(2) A gene interaction network is constructed based on selected types of interactions.
(3) For any pair of gene sets, such as gene sets A and B, the significance of node overlap Pnode_overlap is calculated.
(4) The significance of direct interaction between the two gene sets Pdirect is also calculated (for two gene sets, genes from one gene set may interact with genes from the other gene set.
This type of interaction is called direct interaction).
(5) The combined probability of Pnode_overlap and Pdirect is calculated Figure 2 Visualization interface A.
The numbers of nodes and edges of the network.
B.
The visual style style.
C. The window used to show the global visual style.
D. The visua filter nodes and edges according to specified attributes.
E. The menus network.
F. The main window to display the network.
G. The four but new attribute, delete attribute and export selected attributes.
H using Fishers method [20].
In formula (1), Pnode_overlap is the P-value of node overlap between gene sets A and B, Pdirect is the P-value of direct interaction between gene sets A and B, and S is the score transformed from the combined probability.
The P-value of S, which follows chi-square distribution with 2k degrees of freedom (k is the total number of variables to be combined, 2 in this case), is calculated by formula (2).
If the P-value of S (PS) between gene sets A and B is less than a given threshold such as 0.05, we consider the two gene sets functionally related and the Score, calculated by formula (3), is taken as the final score for the relationship between the two gene sets.
S 2logPnode overlap logPdirect 1 S v22k 2 Score logPs 3 Network visualization Standalone network tools such as Cytoscape [21] and VisANT [22] have been developed for the visualization of biological networks.
Recently, a web-based visualization tool Cytoscape Web [23] has been developed, which uses flash technologies and provides a javascript API for developers.
Implementation of our visualization tool is based on Cytoscape Web 0.7.4 release with the goal of mimicking the standalone Cytoscape.
This convenient visualization tool (Figure 2) can be independently accessed at http://lei.big.ac.cn/visualization/start_ container where users can create, rename and delete selected visual l mapper panel and filter panel, where users can set visual style or to import network, export network, import attribute and lay out tons from left to right, including show selected attribute, create.
The window used to show attributes of nodes and edges.
Sun J et al/ iBIG, A Network Tool for Integrative Biology 169 visualization, with which construction and operation of networks based on web browser can be easily achieved with the tactics from standalone Cytoscape.
Case study Construction of a network for common human diseases Microarray datasets were downloaded from NCBI gene expression omnibus (GEO) and EBI ArrayExpress.
Fifty-four microarray datasets were used in this study, including 12 for cancer, 7 for neurological disorders, 29 for infectious and inflammatory diseases and 6 for metabolic diseases.
Microarray data preprocessing, differential expression identification, enrichment analysis and construction of functional networks were all performed with ArrayPro.
The microarray raw data (CEL files) was preprocessed with the GCRMA algorithm to get the expression values for every probe.
Any probe sets with a call value of less than 10% returned by mas5calls function in affy package were removed.
Then, probe sets were mapped to Entrez Gene ID.
Any probe sets not mapped to known genes were also removed from further analysis.
If there are multiple probe sets mapped to the same gene, we averaged their expression values as the expression of the gene.
Differential expressional genes were identified by the FC-based RankProd algorithm.
Enrichment analysis was based on gene sets including EHMN, KEGG, NCI and GO from GSEA.
For every disease group, 60 functional terms (gene sets) were selected according to the enrichment score.
A total of 240 functional terms from the four disease groups were merged together, Tcell_R Cell_ Toll_lr Proliferation Neurotrophin Comp_coag Endocytosis Integrin_all Protein_dig Phagosome CC_sig ECR ECM_RI Cytokine GTPase Receptor B1_Integr Receptor_B TCR_cd4 Natural_kc N3PhosphtaseActin_cyto TCR_cd8 CDC42 PI3K B_cellR CXCR4 Chemokine MAPKPDGFRb IL6 ErbB1 LTP LTD NFAT Axon_guid IFNg Insulin Nervous_sd Pancreatic Nerve_Imp Hydrolase Pyrophos Calcium Sys_dev Synaptic GapJunction Integrin_ang Fcg_Phago Membrane Proteoglycan Focal_adh Leukocyte Sig_trans Protein_ER Immune CAM De Antigen Cytoskeleton Cytoskel_O&B Metabolic Infectious Neurologic Cancer BP_MF Pathway CC Figure 3 Network perturbation in common human diseases The most significantly perturbed functional modules in four disease perturbed in common human diseases, including cancer, infectious dise for biological process and molecular function in gene ontology (GO), w for curated pathways in KEGG and NCI pathway databases.
which resulted in 117 nodes (functional terms) for the functional network.
The functional network was constructed by ArrayPro based on the HPRD database.
Interactions with P < 0.01 were considered significant.
Network perturbation in common human diseases One of the unique features of iBIG is the construction of networks with functional modules.
This feature can facilitate the understanding of the investigated biological problem at a higher level compared to gene networks.
In our recent work, we have used this functionality in the investigation of pathogenesis of Alzheimers disease [19].
Here we demonstrate this functionality by constructing a functional network perturbed in common human diseases.
The most significantly perturbed functional modules in each of the four classes of diseases were selected and merged together.
The connectivity among this set of 117 functional modules was calculated again by ArrayPro and the network was thus constructed (Figure 3).
Here we briefly describe the relevance of this network to the mechanism of human diseases.
Many of the uniquely-perturbed functions in a specific disease class are consistent with the current knowledge.
For example, cell cycle, DNA replication and p53 pathway (KEGG) are perturbed only in cancer, while transmission of nerve impulse, synaptic transmission, nervous system development, long term potentiation, long term depression, axon guidance and gap junction are perturbed only in neurologic diseases.
Therefore, other uniquely-perturbed functions may also play important roles in the specific class of diseases.
For Lipid_MP Metabolic Purine_met Spliceosome Pyrimi_met Mitochondria Ubiquitin Biosynthetic Androgen Anti_Apopt Protein_mod ApoptosisK Protein_MP EOL Identical_PB Resp_Stress Polo p53_KEGG DNA_bind Transcription Cell_cycleG FOXM1 PLK1 AP1 IL12 ApoptosisG Glucocort_R Aurora_kin HIF1a RNA_proc Ribonucleo RNA_MP DNA_repli DNA_MP CMYC Cell_cycleK Nucleus RNA_bind p53_NCI Ribosome OxidoRed Arachidonic LeukotrieneSC_ribosome RNA_trans OxidaPhos Structural Translation Di_ufattyacid Glycerophos Prostaglandin Urea Glycolysis Fatty_acid TCA dev fense Cytoplasm classes are merged together to form a comprehensive network ases, neurologic diseases and metabolic diseases.
BP_MF stands hile CC stands for cellular component in GO.
Pathway stands 170 Genomics Proteomics Bioinformatics 11 (2013) 166171 cancer, lipid metabolism and protein processing in endoplasmic reticulum (ER) are two of the less-known factors.
Lipid metabolism is involved in membrane formation and energy production which are both critical for cell proliferation [24].
Cell proliferation will also have a different demand on the protein folding and recycling in ER.
For neurologic diseases, more attention may be paid to calcium signaling, insulin signaling and HIF1alpha transcriptional regulation.
Calcium signaling is of great importance to the maintenance of normal neurologic activities.
Insulin signaling is involved in nutrient sensing and adjustment of cellular activity [25,26].
HIF1alpha transcriptional regulation is involved in oxygen sensing and cell fate decisions.
In our recent work, we have proposed that the cause of Alzheimers disease is the prolonged low supply of oxygen and nutrients in the brain [19].
For infectious diseases, the unique perturbation of ribonucleoprotein complexes is of particular interest.
The significant perturbation of this functional module likely reflects the enhanced translational activity in the ribosome.
On the other hand, some functional modules are perturbed in three classes of diseases but not in the fourth class.
For example, the extracellular region and cell adhesion molecules are not significantly perturbed in infectious diseases, consistent with the transcriptome measurement on blood for this class of disease which lacks tight cellular connection as in other tissues [27].
The non-significant perturbation of the complement and coagulation cascade is a little surprising.
The complement and coagulation cascade is involved in immune response and blood clotting [28].
Up-regulation of this functional module is only observed in Tuberculosis (data not shown), leading to overall non-significant perturbation in this disease class.
The non-significant perturbation of the AP1 transcriptional network in infectious disease may also deserve further investigation.
AP1 functions in many cellular activities including cell cycle proliferation and apoptosis [29].
The dysregulation of the AP1 transcriptional network has been reported in cancer and neurological diseases, while its connection with infectious diseases has rarely been reported.
For cancer, the ribosome is not significantly perturbed.
This may indicate non-significant overall perturbation of translational activity in cancer despite the significant dysregulation of cell cycle.
For neurologic diseases, CMYC pathway and antigen processing and presenting are not significantly perturbed.
CMYC is involved in apoptosis under certain conditions, but this may not be relevant to neurologic disorders.
Antigen processing and presenting is involved in the immune response process.
The lack of significant perturbation of this functional module may indicate a non-significant immune response in neurologic diseases.
An interesting observation is the lack of functional modules significantly perturbed in three disease classes but not in metabolic diseases, likely due to the less consistent and specific perturbation among metabolic diseases.
In addition, hidden links between a pair of disease classes can be revealed on this network.
Cancer and neurological diseases shared significant perturbation of the integrin family cell surface interaction and focal adhesion.
This integrin-focal adhesion axis is involved in cell proliferation and apoptosis, which are prominent features of the two disease classes [30].
Cancer and infectious diseases shared significant perturbation of endocytosis.
Endocytosis is an important defense mechanism against pathogens.
It has also been found that endocytosis is involved in other functions including a variety of signaling events [31].
Neurological diseases and infectious diseases shared significant perturbation on energy metabolism related functional modules including mitochondria and TCA cycle.
This may reflect the special energy requirement in these two disease classes.
In addition, cancer and metabolic diseases shared significant perturbation of receptor activity and receptor binding.
Neurologic diseases and metabolic diseases shared significant perturbation of hormone mediated signaling pathways including glucocorticoid receptor signaling pathway and androgen mediated signaling.
Infectious diseases and metabolic diseases shared significant perturbation of translation related functional modules.
Due to the heterogeneous nature of metabolic diseases, it is not immediately clear how those intersections are related to the disease mechanism.
Network analysis has been widely applied to the investigation of human disease mechanisms.
In most of the studies, the major focus is on the gene network.
Here we provide the functional network as a complementary view of the studied biological problem.
Gene networks can provide detailed information on genegene interaction and regulation, while functional networks can provide a global view of the cellular transformation.
The combination of these two types of networks will provide more comprehensive understanding of the studied problem including the disease mechanism.
Currently we are applying this strategy to the in-depth investigation of cancer and Alzheimers disease.
Conclusion In summary, iBIG is a new tool for network construction and visualization.
Distinct features include classification of interactions, web-based visualization and networks of functional gene sets.
The web-based visualization provides a convenient way to refine networks interactively.
Future development of iBIG will include integrating more functional data and further improvement the network visualization.
Because our remote database is based on several external databases, we plan to update it manually and periodically (twice a year).
Authors contributions JS conceived the design, wrote major part of the iBIG code and wrote the first draft.
YP conducted the analysis on disease network.
XF wrote the ArrayPro code.
HZ wrote part of the iBIG code.
YD conceived the design and revised the manuscript.
HL conceived the design and wrote the manuscript.
All authors read and approved the final manuscript.
Competing interests None declared.
Sun J et al/ iBIG, A Network Tool for Integrative Biology 171 Acknowledgements This work is supported by research grants from National Institute of Health to YD (Grant No.
GM79383 and GM67168), Natural Science Foundation of China to HL (Grant No.
30870474) and Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry to HL.
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.2012.
08.007.
ABSTRACT Motivation: Quantitative real-time PCR (qPCR) is one of the most widely used methods to measure gene expression.
Despite extensive research in qPCR laboratory protocols, normalization and statistical analysis, little attention has been given to qPCR non-detectsthose reactions failing to produce a minimum amount of signal.
Results: We show that the common methods of handling qPCR nondetects lead to biased inference.
Furthermore, we show that nondetects do not represent data missing completely at random and likely represent missing data occurring not at random.
We propose a model of the missing data mechanism and develop a method to directly model non-detects as missing data.
Finally, we show that our approach results in a sizeable reduction in bias when estimating both absolute and differential gene expression.
Availability and implementation: The proposed algorithm is implemented in the R package, nondetects.
This package also contains the raw data for the three example datasets used in this manuscript.
The package is freely available at http://mnmccall.com/software and as part of the Bioconductor project.
Contact: mccallm@gmail.com Received on October 16, 2013; revised on March 21, 2014; accepted on April 20, 2014 1 INTRODUCTION Quantitative real-time PCR (qPCR) (Bustin, 2000; Gibson et al., 1996; Higuchi et al., 1992; Wittwer et al., 1997) remains the gold standard for measuring gene expression due to a combination of greater sensitivity and lower cost than gene expression microarrays or RNA-sequencing.
It is commonly used to validate results from high-throughput studies and to develop clinical biomarkers.
Recently, qPCR-based technologies have been developed to simultaneously measure thousands of transcripts, e.g.the TaqMan OpenArray Real-Time PCR Plates contain 3072 wells.
These plates have been used, for example, to simultaneously measure the expression of all microRNAs in a sample.
The increased use of qPCR (Ginzinger, 2002) has prompted research examining qPCR laboratory protocols (Bustin, 2002; Bustin and Nolan, 2004; Nolan et al., 2006) and more recently normalization (Mar et al., 2009; Mestdagh et al., 2009; Qureshi and Sacan, 2013) and statistical analysis strategies (Karlen et al., 2007; Schmittgen and Livak, 2008; Yuan et al., 2006).
In 2009, the Minimum Information for Publication of Quantitative RealTime PCR Experiments (MIQE) guidelines were published.
These guidelines are designed to encourage better experimental practice, allowing more reliable and unequivocal interpretation of qPCR results (Bustin et al., 2009).
Briefly, qPCR is used to measure the expression of a set of target genes in a given sample through repeated cycles of sequence-specific DNA amplification followed by expression measurements.
Between subsequent cycles, the amount of each target transcript approximately doubles during the exponential phase of amplification.
The cycle at which the observed expression first exceeds a fixed threshold is commonly called the threshold cycle (Ct) or quantification cycle (Cq).
The latter is the MIQEpreferred nomenclature but is not currently widely used.
These Ct values represent a quantitative assessment of gene expression and are often treated as the raw data for subsequent analyses.
However, relatively little attention has been given to handling non-detectsthose reactions failing to attain the prespecified minimum signal intensity.
Currently, there is no consensus manner in which to handle these non-detects in subsequent analyses.
The default in the Applied Biosystems DataAssist v3.0 software is to set non-detects equal to the number of PCR cycles performed (typically 40).
One has the option of setting a lowerMaximum Allowable Ct Value to which any greater value is set or excluding these values from subsequent calculations (Life Technologies, 2011).
Integromics RealTime StatMiner distinguishes between two types of non-detectsundetermined values are those that do not exceed the Ct threshold and absent values are those for which no reaction occurred.
RealTime StatMinder handles non-detects by setting undetermined values to a maximum Ct (e.g.40) and absent values to the median of the detected replicates (Goni et al., 2009).
Researchers have also developed their own methods to handle non-detects that combine filtering and thresholding, for example, summarizing replicates with a value of 40 when the majority are non-detects and with an average of the detected Ct values otherwise (Mar et al., 2009).
2 APPROACH We begin by showing that the common practice of setting nondetect values equal to 40 introduces substantial bias in normalized gene expression, "Ct, and differential expression, ""Ct, estimates (Pfaffl, 2001).
Next, we provide evidence that nondetects are not missing completely at random and are likely missing not at random; therefore, filtering these data will also introduce bias in subsequent analyses (for an introduction to missing data terminology, see Gelman and Hill, 2007, Chap.
25).
To address non-detects, we propose a method to model the missing data mechanism that can be used to impute Ct values for the non-detects or to directly estimate the quantities*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com    of interest.
Finally, we show that the proposed approach greatly reduces the bias introduced by non-detects in qPCR data analysis.
Three published qPCR datasets (described in the Methods Section) are used throughout the manuscript to motivate and illustrate the results.
3 METHODS 3.1 Three example datasets The first dataset consists of nine gene perturbations with matched control samples (Almudevar et al., 2011); the second dataset is composed of two cell types and three treatments (Sampson et al., 2013); the third dataset is a study of the effect of p53 and/or Ras mutations on gene expression (McMurray et al., 2008).
In the first dataset, cells transformed to malignancy by mutant p53 and activated Ras are perturbed with the aim of restoring gene expression to levels found in non-transformed parental cells via retrovirus-mediated reexpression of corresponding cDNAs or shRNA-dependent stable knockdown.
The data contain four to six replicates for each perturbation, and each perturbation has a corresponding control sample in which only the vector has been added (Almudevar et al., 2011).
The second dataset consists of two cell typesyoung adult mouse colon (YAMC) cells and mutant-p53/activated-Ras transformed YAMC cellsin combination with three treatmentsuntreated, sodium butyrate or valproic acid.
Four replicates were performed for each cell-type/treatment combination (Sampson et al., 2013).
The third dataset is a comparison between four cell typesYAMC cells, mutant-p53 YAMC cells, activated-Ras YAMC cells and p53/Ras double mutant YAMC cells.
Three replicates were performed for the untransformed YAMC cells, and four replicates were performed for each of the other cell types (McMurray et al., 2008).
As in the original publications, all three datasets were normalized to a reference gene, Becn1, with the resulting values denoted as "Ct.
In the first dataset, ""Ct values were computed by comparing each perturbed sample to its corresponding control sample.
Additional details regarding each of these datasets can be found in the original publications.
4 RESULTS 4.1 Setting non-detects equal to 40 introduces bias We begin by examining the common practice of replacing nondetects with a Ct value of 40.
Replicates were summarized by calculating the average "Ct (datasets 2 and 3) or ""Ct (dataset 1) values for each unique gene/sample-type combination.
The residuals from this summarization for gene i, sample-type j and sample k were calculated as follows: Dataset 1 : rijk=""Ctijk 1 K XK k=1 ""Ctijk Datasets 2 and 3 : rijk="Ctijk 1 K XK k=1 "Ctijk The distribution of these residuals differs substantially between those in which the "Ct or ""Ct value contains a non-detect and those in which these values were observed (Fig.1).
Note that when calculating "Ct values, the reference gene, Becn1, is always detected, so non-detects can only occur in the target gene; therefore, datasets 2 and 3 are each split into two groups based on whether both Ct values were observed or a non-detect was present in the target gene.
A non-detect typically results in lower absolute expression estimates (Fig.1B and C).
When calculating ""Ct values, a non-detect can occur in the perturbed and/or control sample.
In general, a non-detect in the perturbed sample results in lower relative expression, and a non-detect ina the control sample results in higher relative expression (Fig.1A).
Non-detects in both samples yield ""Ct values close to zerothese values simply represent differences in Becn1 expression between the perturbed and control samples.
While one might expect some difference in the distribution of residuals between the observed values and those containing a non-detect, the large differences seen in Figure 1 likely A B C Fig.1.
Within replicate residuals stratified by the presence of non-detects.
The average ""Ct (A) or "Ct (B and C) values were calculated within each set of replicates (same gene and sample type).
The residuals, for each gene and sample from this summarization are plotted here, stratified by the presence of non-detects.
In dataset 1, a non-detect could occur in the perturbation sample, the control sample or both samples.
The left-most box in Panel A shows the distribution of residuals in dataset 1 when there are no non-detects.
The other boxes in Panel A (from left to right) show the distribution of residuals when there are non-detects in the perturbation sample, the control sample and both samples.
Similarly, the left box in Panels B and C shows the distribution of residuals when there are no non-detects.
The right box in Panels B and C shows the distribution of residuals when there is a non-detect.
Although one would expect some difference in the distribution of residuals between the detects and non-detects, the differences seen here are much larger than one would expect and likely represent bias introduced by setting non-detects equal to 40 2311 qPCR non-detects comprised 4-6     ,  , &amp; , &amp;  -represent bias introduced by the common method of handling non-detects.
To further illustrate the bias introduced by replacing nondetects with a value of 40, the "Ct and ""Ct values for one example gene from each dataset are shown in Figure 2.
These examples were chosen to demonstrate situations in which replacing non-detects with a value of 40 may lead to spurious differential expression.
In Figure 2A, the response of Sema7a to perturbation of each of nine genes is shown.
Looking at only the ""Ct values for which there were no non-detects, the expression of Sema7a does not appear to be greatly affected by any of the perturbations, except perhaps Hoxc13.
However, there do appear to be a relatively large number of outliers.
Focusing on Sema7as response to perturbation of Hoxc13, half of the ""Ct values contained a non-detect in the perturbed sample.
If one replaced these non-detects with a value of 40, the resulting ""Ct values would be approximately 3.23 and 5.05, while the ""Ct values without non-detects were approximately 0.16 and 1.54.
This would produce an average ""Ct value of 2.5.
This is probably a substantial overestimate of the down-regulation of Sema7a induced by perturbation of Hoxc13, resulting from the common method of handling non-detects.
Figure 2B shows the expression of Gpr149 in six conditions.
Among the normal samples, there does not appear to be a difference in expression between the untreated (UT), sodium butyrate (NB) and valproic acid (VA) samples when looking at only the "Ct values without non-detects.
However, there are three non-detects in the NB samples and one in the VA sample.
Replacing these non-detects with a value of 40 would lead to a large (and likely spurious) difference in expression between these treatments.
Finally, Figure 2C shows the response of Pdlim2 to mutation of p53 and/or Ras.
While there are non-detects in each group, the number of non-detects varies from 3/3 in the normal samples to 1/4 in the Ras and p53/Ras samples.
Replacing these non-detects with a value of 40 will produce a sizeable difference in average expression between the normal and p53 samples and the Ras and p53/Ras samples.
4.2 Filtering non-detect Ct values also introduces bias Whether one can filter missing values from ones data without biasing ones results depends on the type of missing data.
Data are said to be missing completely at random if the probability of a missing value is the same for all data points.
For qPCR data this implies that the probability of a non-detect is the same for every data point regardless of gene, sample-type, sample-replicate, etc.
A broader class of missing data is missing at random in which the probability of a missing value depends only on the available information.
For qPCR data this would imply that the probability of a non-detect is the same for each replicate within a given gene/ sample-type combination.
Finally, the data are called missing not at random when the probability of a missing value depends on either unobserved predictors or the missing value itself.
A wellstudied example of the latter is censoring.
For data missing not at random, filtering the missing values produces bias in ones inferences.
If the non-detects are missing completely at random, then the proportion of non-detects should be roughly constant across genes.
For each gene, we compute the proportion of non-detects and the average Ct value across replicate samples (Fig.3).
There appears to be a strong relationship between the average expression of the genes across replicate samples and the proportion of non-detects.
In other words, it seems that genes with lower average expression are far more likely to be non-detects.
From this we can conclude that the non-detects do not occur completely at random.
While it is relatively easy to distinguish between missing completely at random and missing at random, it is generally not possible to distinguish between missing at random and missing not at random from the observed data.
However, in the case of qPCR non-detects, we are able to use two pieces of additional information to suggest that non-detects are likely missing not at random.
A B C Fig.2.
Examples of the potential for spurious differential expression produced by replacing non-detects with values of 40.
Panel (A) shows the response of Sema7a to the perturbation of nine genes from dataset 1.
Panel (B) shows the expression of Gpr149 in each combination of normal/tumor samples and one of three treatments from dataset 2.
Panel (C) shows the response of Pdlim2 to p53 and/or Ras mutation from dataset 3.
"Ct and ""Ct values produced by replacing a non-detect with a value of 40 are shown as asterisks.
Note that in panel A, a non-detect could have also occurred in one of the control samples; however, in these data this did not occur for Sema7aall of the non-detects happened to occur in the perturbed samples 2312 M.N.McCall et al.i-, First, the PCR reactions are run for a fixed number of cycles (typically 40), implying that the observed data are censored at the maximum cycle number.
This is a type of non-random missingness in which the missing data mechanism depends on the unobserved value.
Knowledge of the technology allows us to conclude that the data are at least subject to fixed censoring; however, as we will later show, the qPCR censoring mechanism may actually be a probabilistic function of the unobserved data.
Second, the experimental design of the first dataset, in which there are a large number of control samples, allows one to estimate an additional piece of information that is not typically availablethe proportion of non-detects as a function of the average sample expression across a large number of replicates (Fig.4).
Here, we see a similar relationship between average expression and proportion of non-detects.
It appears that samples with overall lower signal, as a result of technical not biological variability, also result in a greater number of non-detects.
Because most qPCR experiments are not designed to allow one to estimate the relationship between overall sample signal and the proportion of non-detects, qPCR data typically exhibit a type of non-randommissingness in which the missing data mechanism depends on an unobserved variable.
This suggests that qPCR non-detects are probably not missing at random; therefore, filtering non-detects will introduce bias in ones inference.
The only principled approach is to attempt to model the missing data mechanism and incorporate this into ones analysis.
4.3 The missing data mechanism Before proposing a missing data mechanism for qPCR nondetects, it is important to first determine what a non-detect represents.
There are several possibilities: (1) Truncation of a continuous expression distributiona non-detect represents a true Ct value 440.
This implies that if the PCR were run for more cycles, one would eventually see an amplification above the Ct threshold.
This would mean that the Ct values are a type of censored data.
(2) A completely unexpressed transcriptno matter how long the PCR was run, one would never see amplification above the Ct threshold.
(3) A failure to detect a true Ct value 540the Ct value should be540, but in the given experiment the transcript failed to amplify or its amplification efficiency was poor.
We begin by evaluating the first potential explanation for nondetects by examining the distribution of Ct values including nondetects coded as 40 (Fig.5).
The number of non-detects in these datasets far exceeds what one would expect based on fitting a normal distribution to the detected Ct values.
Approximately 1.2, 1.8 and 2.8% of the measurements are non-detects in datasets 1, 2 and 3, respectively, where one would expect 0.02, 0.03 and 50.01%.
This argues against non-detects being explained completely by a truncation of the Ct value distribution, unless the distribution has an extremely long upper tail.
A B C Fig.3.
The proportion of non-detects versus median observed gene expression within control samples (A) or within each sample condition (B and C).
Logistic regression fits (dashed lines) all show a strong relationship between the proportion of non-detects and the median observed gene expression P-values of (A) 2:57 106, (B) 1:58 1012, (C) 52 1016 Fig.4.
The proportion of non-detects versus median sample expression within controls in dataset 1.
Logistic regression fit (dashed line) shows a strong relationship between the proportion of non-detects and the median gene expressionP-value of 0.0003 2313 qPCR non-detects  t  greater than   less than   less than &percnt; &percnt; , , &percnt; &percnt; , Furthermore, if the non-detects represented censoring of values440, one would expect a reduction in bias by replacing the non-detects with a value 40.
However, in general, bias is reduced by replacing non-detects with a value of 35 rather than 40 (Fig.6).
This suggests that many non-detects are due to a failure to amplify rather than a true Ct value440.
Next, we evaluate the second potential explanation for nondetectsthat a non-detect represents a completely unexpressed transcript.
As previously mentioned, Figure 4 shows a strong relationship between low overall signal in a sample and a greater proportion of non-detects.
Although some non-detects may represent a completely unexpressed gene, this cannot be the only explanation, given that samples with low signal (due to technical not biological differences) typically have a greater proportion of non-detects.
Finally, examination of Figure 5 shows a relatively low number of Ct values between 35 and 40.
Together with Figure 6, in which replacing non-detects with a value of 35 rather than 40 reduced the bias in "Ct and ""Ct values, this suggests that some nondetects represent a failure to detect a true Ct value540.
4.4 A potential generative model One model to explain the observed behavior of non-detects in the Ct data is the following: Yij= fij+"ij ifZij=1 non-detect ifZij=0 ( where PrZij=1= gYij ifYij5Sij 0 otherwise ( Here, Yij is the observed Ct value of gene i for sample j, ij is the true expression of gene i for sample j, fij represents the nonbiological effects present in the observed data and "ij captures the technical and biological variability in the data.
Zij is a binary variable representing whether a Ct value was obtained for gene i and sample j that takes on a value of 1 with probability gYij for values of Yij less than some threshold Sij.
Here, Sij represents the upper Ct value detection limit for gene i and sample j.
In this framework, one can represent the standard assumptions regarding non-detects as: (i) Sij=40 8i; j, where 40 is the total number of PCR cycles performed and (ii) gYij=1 meaning that Ct values 540 are never reported as non-detects.
However, the results shown above suggest that these assumptions are probably not valid.
Specifically, Sij may be 540 for some genes and/or gYij may be51.
Furthermore, this model captures several important aspects of qPCR non-detects.
The relationship between technical variability in expression and the proportion of non-detects is formalized in the dependence of Zij on Yij rather than ij.
The gap in observed Ct values between 35 and 40, i.e.the potential for Ct values 540 to be non-detects, is captured by gYij51 and/or Sij540.
4.5 An EM algorithm to handle non-detects Having established that non-detects in qPCR data represent data missing not at random, we now propose a method that incorporates the missing data mechanism into subsequent statistical analyses.
The expectationmaximization (EM) algorithm provides a method to obtain maximum likelihood estimates in the presence of missing data by iteratively calculating the conditional expectation: Qjn=E ln fXj jY; n  and maximizing Qjn with respect to.
Here, X is the complete unobserved data andY is the incomplete observed data, is the set of all parameters, ln fXj is the complete data loglikelihood, and n is the estimate of at iteration n. This process is repeated until convergence (Dempster et al., 1977).
The challenging aspect of applying the EM algorithm to qPCR non-detects is calculating the conditional expectation.
This requires one to estimate the distribution of gene expression given a non-detect.
Here, we proceed via Bayes rule: fYijjZij=0= PrZij=0jYij fYij PrZij=0 We can estimate PrZij=0jYij by examining the relationship between the proportion of non-detects and average observed expression within replicates.
This approach permits flexible modeling of the data to either directly estimate the parameters of A B C Fig.5.
The distribution of Ct values in each of the three datasets.
Here, non-detects are coded as 40 2314 M.N.McCall et al.greater than greater than or equal to greater than   less than , 1 2 less than less than less than formalised interest or to obtain estimates of the missing data that can be used to impute the non-detect values.
To demonstrate the reduction in bias that one can achieve by treating non-detects as missing data, we propose the following model of the observed expression for gene i, sample-type j and replicate k, Yijk: Yijk= ij+k+"ijk ifZijk=1 non-detect ifZijk=0 ( where k represents a global shift in expression across samples and, PrZijk=1= gYijk ifYijk540 0 otherwise ( Here, gYijk can be estimated via the following logistic regression: logitPrZijk=1=0+1 ij where ij is an estimate of the average expression for gene i and sample-type j.
For the data presented here, k can be estimated using the reference gene, Becn1.
4.6 Treating non-detects as missing data reduces bias We begin by examining the effect of replacing non-detects with an imputed Ct value based on the conditional expectation calculated in the EM algorithm.
Looking at the residuals within replicates in each dataset, it is clear that replacing non-detects with these imputed values results in far less bias in the "Ct and ""Ct values than if we replaced the non-detects with a value of 40 (Fig.7).
The improvement in bias after imputing the non-detects can also be seen in the example genes shown in Figure 2.
After replacing the non-detects with values imputed using the EM algorithm, the non-detect "Ct and ""Ct values are far more similar to their replicate values, while retaining small differences due to the informative missingness (Fig.8).
Figure 8C shows one important limitation of the current implementation.
Because the "Ct values from the three normal samples all contained non-detects, their imputed values are fairly similar to the initial values based on replacing the non-detects with a value of 40.
One could address this by implementing a slightly more complex EM algorithm that shrinks the imputed values toward a global mean; however, such an approach assumes A B C Fig.6.
Same as Figure 1, with additional boxplots showing the residuals when non-detects are replaced with 35 rather than 40.
Here, Ct values435 are also replaced by a value of 35.
By replacing non-detects with a value of 35 rather than 40, the distribution of the residuals is far more similar between those in which the Ct values were observed and those containing a non-detect.
However, this does not imply that one should replace non-detects with a value of 35.
Such an approach makes very strong assumptions about the missing data mechanism and would require one to discard observed Ct values435 A B C Fig.7.
Same as Figure 1, but after imputing the non-detects using the proposed EM algorithm 2315 qPCR non-detects , that Pdlim2 is actually expressed in the normal samples in dataset 3.
Given that all three replicates resulted in a nondetect, it may be that Pdlim2 is truly unexpressed in these samples.
Any modeling for such situations will depend on the specific dataset being analyzed and the biological plausibility of the potential assumptions.
One can also use the EM algorithm to directly estimate the parameters of interest.
In the example datasets reported here, these might be the average expression of each gene within each sample-type, ij.
Alternatively, one could use this framework to directly estimate the "Ct or ""Ct values.
Furthermore, the EM algorithm allows one to easily combine the treatment of nondetects with more complex statistical analyses.
5 DISCUSSION In this manuscript, we have shown that the default procedure of replacing qPCR non-detects with the maximum PCR cycle number (typically 40) introduces a large bias in subsequent inference.
We have carefully examined the nature of non-detects and shown that they likely represent data missing not at random.
Furthermore, we have shown that many non-detects represent an amplification failure rather than a true Ct value440.
Finally, we propose a relatively simple EM algorithm and show that it is able to greatly reduce the bias caused by non-detects.
The flexibility of our approach allows one to easily tailor the method described here to ones own analyses.
Specifically, one could easily use a different normalization procedure or perform a more complex statistical analysis.
Additionally, any analysis based on imputed values (rather than direct estimation of a parameter of interest) would benefit from a multiple imputation procedure.
Funding: National Institutes of Health (grant numbers CA009363, CA138249, HG006853); and an Edelman-Gardner Foundation Award.
Conflict of Interest: none declared.
ABSTRACT Summary: SEAL is a scalable tool for short read pair mapping and duplicate removal.
It computes mappings that are consistent with those produced by BWA and removes duplicates according to the same criteria employed by Picard MarkDuplicates.
On a 16-node Hadoop cluster, it is capable of processing about 13 GB per hour in map+rmdup mode, while reaching a throughput of 19 GB per hour in mapping-only mode.
Availability: SEAL is available online at http://biodoopseal.sourceforge.net/.
Contact: luca.pireddu@crs4.it Received on March 7, 2011; revised on May 9, 2011; accepted on May 26, 2011 1 INTRODUCTION Deep sequencing experiments read billions of short fragments of DNA, and their throughput is steadily increasing (Metzker, 2010).
These reads need to be post-processed after sequencing to prepare the data for further analysis, which implies that the computational steps need to scale their throughput to follow the trend in sequencing technology.
Such high data rates imply the need for a distributed architecture that can scale with the number of computational nodes.
Typical post-processing steps include sequence alignment, which is a fundamental step in nearly all applications of deep sequencing technologies, and duplicate read removal, which is a major concern for Illumina sequencing (Kozarewa et al., 2009).
The pressure for better and faster tools has recently given rise to the development of new alignment algorithms that outperform traditional ones in terms of both speed and accuracy (Li and Homer, 2010).
Distributed alignment tools have also been created, with Crossbow (Langmead et al., 2009a) as one of the most prominent examples.
However, Crossbow is based on Bowtie (Langmead et al., 2009b), and thus does not currently support gapped alignment, an important feature for many applications (Li and Homer, 2010).
In this work we describe SEAL, a new distributed alignment tool that combines BWA (Li and Durbin, 2009) with duplicate read detection and removal.
SEAL harnesses the Hadoop MapReduce framework (http://hadoop.apache.org) to efficiently distribute I/O and computation across cluster nodes and to guarantee reliability by resisting node failures and transient events such as peaks in cluster load.
In its current form, SEAL specializes in the pair-end alignment of sequences read by Illumina sequencing machines.
SEAL uses a version of the original BWA code base (version 0.5.8c) that has To whom correspondence should be addressed.
been refactored to be modular and extended to use shared memory to significantly improve performance on multicore systems.
2 METHODS SEAL is currently structured in two applications that work in sequence: PairReadsQseq and Seqal.
PairReadsQseq is a utility that converts the qseq files (Illumina, Inc., 2009) produced by Illumina sequencing machines into our prq file format that places entire read pairs on a single line.
Seqal is the core that implements read alignment and optionally also performs duplicate read removal following the same duplicate criteria used by Picard MarkDuplicates (http://picard.sourceforge.net).
Both applications implement MapReduce algorithms (Dean and Ghemawat, 2004) which run on the Hadoop framework.
MapReduce and Hadoop: MapReduce is a programming model prescribing that an algorithm be formed by two distinct functions: map and reduce.
The map function receives one input record and outputs one or more key-value pairs; the reduce function receives a single key and a list of all the values that are associated to that key.
Hadoop is the most widespread implementation of MapReduce.
Pairing reads in PairReadsQseq: PairReadsQseq groups mate pairs from qseq data files into the same record, producing prq files where each line consists of five tab-separated fields: id; sequence and ASCII-encoded base qualities for read 1 and 2.
Read alignment and duplicates removal in Seqal: SEALs second MapReduce application, Seqal, takes input pairs in the prq format and produces mapped reads in SAM format (Li et al., 2009).
The read alignment is implemented in the map function.
Rather than implementing a read aligner from scratch, we integrated BWA (Li and Durbin, 2009) into our tool.
We refactored its functionality into a new library, libbwa, which allows us to use much of the functionality of BWA programmatically.
Although it is written in C, it provides a high-level Python interface.
To take advantage of this feature, the Seqal mapper is written in Python, and integrates into the Hadoop framework using Pydoop (Leo and Zanetti, 2010).
For each pair of reads, the aligner produces a pair of alignment records.
The user can choose to filter these by whether or not the read is mapped and by mapping quality.
Then, the reads may be directly output to SAM files, or put through a reduce phase where duplicates are removed; the choice is made through a command line option.
Like Picard MarkDuplicates, Seqal identifies duplicate reads by noting that they are likely to map to the same reference coordinates.
The specific criteria we use defines two pairs as duplicates if their alignment coordinates are identical, both for their first and second reads.
Likewise, lone reads are considered duplicates if they are aligned to the same position.
When a set of duplicate pairs is found, only the one with the highest average base quality is kept; the rest are discarded as duplicates.
Moreover, when a lone read is aligned to the same position as a paired read, the lone one is discarded.
If, on the other hand, only lone reads are found at a specific position then, as for pairs, only the one with the highest average base quality is kept.
2.1 Evaluation Correctness: we verified the correctness of SEAL by performing the alignment of the 5M dataset (Table 1) to the UCSC HG18 reference genome The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:53 6/7/2011 Bioinformatics-btr325.tex] Page: 2160 21592160 L.Pireddu et al.Table 1.
SEAL evaluation: input datasets Dataset No.
of lanes No.
of pairs Size (GB) Read length 5M 0 5.0106 2.3 91 DS1 1 1.2108 51 100 DS3 3 3.3108 147 100 DS8 8 9.2108 406 100 The 5M dataset consists of the first 5M pairs from run id ERR020229 of the 1000 Genomes Project (Durbin et al., 2010).
The three DS datasets are from a production sequencing run on an Illumina HiSeq 2000.
Table 2.
Comparison of running time in hours between BWA on a single node with 8 cores and SEAL running on 32 nodes without duplicates removal Dataset BWA time (h, 1 node) SEAL time (h, 32 nodes) 5M 0.49 0.04 DS1 11.26a 0.63 DS3 32.39a 1.72 DS8 89.35a 4.78 Note that the SEAL running time includes qseq to prq format conversion.
aTime is predicted as a linear extrapolation of the throughput observed on the 5M dataset.
(Fujita et al., 2010) with both SEAL and BWA ver.
0.5.8c and then comparing their output.
With BWA, we ran bwa aln and bwa sampe, while with SEAL we ran the PairReadsQseq and Seqal applications.
The result was identical for 99.5% of the reads.
The remaining 0.5% had slightly different map quality scores (mapq), while the mapping coordinates were identical for all but two reads.
Both of the latter two cases had multiple best hits but resulted in different alignment choices probably due to insert size statistics, in turn due to the particular input read batch.
Slight differences in mapq scores are expected because their calculation takes into account the insert size statistics, which are calculated on sample windows on the input stream of sequences.
Since the sample windows seen by the command line version of BWA and SEAL are different for each read, a slight change in the mapq value is expected.
To verify this hypothesis, we ran BWA with varying input datasets while keeping 3000 of those reads that produced mapq variations in the original experiment.
We observed that the mapq values for those reads varied between runs.
Speed and scalability: we tested SEAL with varying input size (DS datasets from Table 1) and cluster size (16, 32, 64 and 96 nodes).
Each node is equipped with dual quad-core Intel Xeon CPUs @ 2.83 GHz, 16 GB of RAM, two 250 GB SATA disks, one of which is used for Hadoop storage.
Nodes are connected via Gigabit Ethernet.
For each cluster size, we allocated a Hadoop cluster (ver.
0.20.2) and copied the input data and a tarball of the indexed reference sequence onto the Hadoop file system.
The SEAL application was run on all the DS datasets in both alignment-only and alignment plus remove duplicate modes.
The runs were repeated three times, with the exception of DS8 which was run only once.
The runtimes for the different datasets are reported in Table 2, while the throughput is shown in Figure 1.
Looking at Figure 1, we see that SEAL is generally capable of throughput levels comparable to single-node operation, meaning that the application and Hadoop keep the distribution overhead to a minimum.
As the cluster size increases, we would ideally see a constant throughput per node, giving a linear increase in overall throughput.
In practice, when the input is too small with respect to the computational capacity, nodes are often underutilized.
Therefore, the throughput per node with DS1 at 96 nodes is much lower than the other configurations.
On the other hand, we see that SEAL is capable of utilizing available resources efficiently when more data are available, although while scaling up from 64 to 96 nodes, the system achieved better throughput on the small DS3 dataset as opposed to the larger DS8.
We suspect this is due to network congestion, which can be alleviated by informing Hadoop about the cluster network topology.
Fig.1.
Throughput per node of the entire SEAL workflow: finding paired reads in different files; computing the alignment; and removing duplicate reads.
An ideal system would produce a flat line, scaling perfectly as the cluster size grows.
The three datasets used are described in Table 1.
By comparison, a single-node workflow we wrote for testingperforming the same work as SEAL but using the standard multithreaded BWA and Picard reaches a throughput of 1100 pairs/s on the 5M dataset.
SEAL is able to achieve such scalability rates principally thanks to libbwas efficient use of memory.
In fact, libbwa stores the reference in shared memory, allowing all libbwa instances running on the same system to share the same memory space.
In practical terms, this feature makes it possible to run in parallel 8 alignments on a system with 8 cores and 16 GB of memory, fully operating in parallel.
While BWA does have a multithreaded mode of operation, it only applies to the bwa aln step.
On the contrary, SEAL is able to parallelize all steps in the alignment.
ACKNOWLEDGEMENTS We would like to thank our colleagues R. Berutti, M. Muggiri, C. Podda and F. Reinier for their feedback and technical support.
Conflict of Interest: none declared.
ABSTRACT Summary: High-throughput screening (HTS) is a common technique for both drug discovery and basic research, but researchers often struggle with how best to derive hits from HTS data.
While a wide range of hit identification techniques exist, little information is available about their sensitivity and specificity, especially in comparison to each other.
To address this, we have developed the open-source NoiseMaker software tool for generation of realistically noisy virtual screens.
By applying potential hit identification methods to NoiseMaker-simulated data and determining how many of the predefined true hits are recovered (as well as how many known non-hits are misidentified as hits), one can draw conclusions about the likely performance of these techniques on real data containing unknown true hits.
Such simulations apply to a range of screens, such as those using small molecules, siRNAs, shRNAs, miRNA mimics or inhibitors, or gene over-expression; we demonstrate this utility by using it to explain apparently conflicting reports about the performance of the B score hit identification method.
Availability and implementation: NoiseMaker is written in C#, an ECMA and ISO standard language with compilers for multiple operating systems.
Source code, a Windows installer and complete unit tests are available at http://sourceforge.net/projects/noisemaker.
Full documentation and support are provided via an extensive help file and tool-tips, and the developers welcome user suggestions.
Contact: amanda.birmingham@thermofisher.com Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 14, 2010; revised on July 23, 2010; accepted on August 5, 2010 1 INTRODUCTION Data analysis and hit identification are points of confusion for many screeners (Birmingham et al., 2009).
Those asking questions such as Which method identifies the most true hits for my particular screen circumstances?
or What will the false positive rate of my chosen method be?
are frequently stymied, since answering these requires them to know the identity of the real hits.
However, developing a list of the anticipated real biological hits for any given assay is extremely challenging and is likely to be both noisy and incomplete, especially for medium-to weak-strength effects.
The difficulty in assessing the performance of hit identification methods can be avoided by moving to in silico-based strategies.
In the computational environment, one can generate a virtual screen containing defined true hits at known locations, and then perturb these true values with varying degrees and types of noise (both To whom correspondence should be addressed.
systematically biased and random) to simulate the variation inherent in biological screens; statistical techniques can then be evaluated based on their ability to identify known true positives and true negatives.
These evaluations will be valid to the extent that the in silico hit distributions and types of noise are congruent with those of the real system.
This approach offers both speed and flexibility, providing the opportunity to profile a methods performance in many different realistic screening scenarios as well as the ability to simulate whole screens within minutes.
To enable such in silico testing, we have developed the NoiseMaker tool for generating simulated high-throughput screening datasets.
A NoiseMaker user selects a realistic scenario for his or her simulated screen, including a range of hit properties as well as noise characteristics, derived from previous screens or assay development work (Supplementary Appendix 1); the software then randomly assigns true hits conforming to this scenario and generates noisy replicates of the screen.
The user applies potential analysis approaches to this noisy data, using the known true hits to calculate metrics of interest (such as sensitivity, specificity or positive predictive value), and selects the most effective method.
2 MAIN FEATURES This simulation software offers two main features: (i) the ability to generate a random set of true hits that conform to expected characteristics and (ii) the ability to apply user-specified noise to a list of true hits to model realistically messy screening results.
On the tab for generation of true hits (Fig.1A), the user inputs a tab-delimited plate map file containing reagent identifiers represented by one row per well and a default true value to be assigned to all reagents that are not treated as hits or controls.
Controls are specified by reagent identifier and assigned a name (such as up-regulating positive control) and a true value.
The user may specify as many types of controls as desired as long as each has a unique name; e.g.an siRNA-based screen might have lipid controls, negative controls for transfection and positive controls for both up-and down-regulation, all with different identifiers and different expected values.
All instances of a controls reagent identifier in the plate map will be assigned the value specified for that control type.
Hits may represent either an increase or a decrease from the default value.
They are specified by their unique name, strength and frequency; the latter number can be either an absolute value (e.g.eight wells) or a percentage of the non-control wells (e.g.8% of the wells).
For each hit type, the NoiseMaker software will randomly select, without replacement, the appropriate number of non-control wells and assign them the value specified for that hit type.
The Hit Type input can also be used to model random equipment or assay failures that could be mistaken for hits.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[14:01 28/8/2010 Bioinformatics-btq457.tex] Page: 2485 24842485 NoiseMaker Fig.1.
(A) Hit and (B) Noise tabs of the NoiseMaker interface.
The output of the true hit generation is a plate map file that is annotated with the true values for every well and the type of value for that well (which is either Default or the wells assigned control or hit name).
For convenience, this files name is automatically copied to the input field of the Noise tab (Fig.1B).
Noise can also be added to true values files not created with NoiseMaker, as long as they conform to the expected format; this can be useful for modeling the effect of noise on clumped hit distributions such as those from non-randomly plated screening libraries.
On the Noise tab, the user specifies the plate dimensions and number of replicates and then describes the systematic noise to be applied.
Noise can be applied at the level of several different elements of the screen, including the entire screen, the edges of every plate, an individual plate in the screen, a particular row on every plate, a particular column on every plate and/or a particular well on every plate.
This allows the user to simulate a wide variety of realistic outcomes, from evaporation of reagents in edge wells to a blocked dispensing tip at a single well position.
Noise definitions are additive; e.g.if one is specified for the entire screen, one for Plate 2 and one for Row 5, then all values in Row 5 of Plate 2 will be permuted with the screen noise, the plate noise, and the row noise combined.
This simulates the convergence of disparate systematic effects in real screens.
All Noise definitions model noise as a Gaussian perturbance of the true values.
They adjust the well values away from their initial values by approximately the amount assigned to the Noise definitions mean change value, with the exact amount of adjustment being randomly chosen from a Gaussian distribution centered on the mean change value and with the specified standard deviation (SD) value.
This ensures that each Noise definition produces realistically noisy adjustments even as it introduces the intended systematic effects.
Noise can also be limited to a specific range (such as that simulating an instruments detection range) using optional floor and/or ceiling values.
The output file contains the input true values and one column of noisy values for each simulated replicate.
Currently NoiseMaker is limited to Gaussian noise distributions, additive noise and linear positional effects.
Future development will address non-Gaussian and multiplicative noise, as well as bowlshaped (non-linear) positional biases.
3 SAMPLE APPLICATION The B score (Brideau et al., 2003) is a normalization and hit identification method employing Tukeys median polish, and has been proposed for use in screens displaying within-plate positional effects such as row and/or column biases.
However, Makarenkov et al.(2007) have reported that it failed to recover correct hits in a scenario with noisy standard normal data with systematic error stemming from row column interactions which are constant across plates.
To address this apparent inconsistency in the literature and demonstrate how NoiseMaker can be applied in evaluating the performance of statistical techniques, we evaluated the B score in scenarios with different types of row and column positional effects: varying size of SD (Group A), varying size of mean change (Group B) and varying size of both mean change and SD (Group C).
After data sets with appropriate noise were created by NoiseMaker, we calculated B scores for all wells and identified the wells whose scores were in the top 1% as positives.
We found that the true positive rates of datasets in Group A decrease and the false positive rates slightly increase as SD of the row and column noise increases (Supplementary Appendix II).
However, the true positive rates and false positive rates of datasets in Group B remain steady regardless of the amount of mean change of the row and column noise, while the true positive rates and false positive rates of data sets in Group C behave similarly to those in Group A.
These results suggest that B score is an appropriate choice for correction of systemic influences that primarily affect mean rather than variance.
Notably, Makarenkovs work examined simulated data with varying SDs, which is consistent with this finding.
4 CONCLUSION NoiseMaker is simulation software for creating realistic, virtual high-throughput screens that can be used to evaluate hit identification methods and quality criteria.
We establish its power by using it to clarify the utility of the B score under various screening conditions.
This tool will be useful for broader comparisons of available hit identification methods, and is freely available for download and use by others interested in modeling screens in silico.
ACKNOWLEDGEMENTS The authors thank Gabor Bakos of Trinity College, Dublin, and members of the RNAi Global Initiative for suggestions and helpful discussions, and Kevin Sullivan and John Quinn for code-review.
Conflict of Interest: none declared.
ABSTRACT Motivation: The Basic Local Alignment Search Tool (BLAST) is one of the most widely used bioinformatics tools.
The widespread impact of BLAST is reflected in over 53 000 citations that this software has received in the past two decades, and the use of the word blast as a verb referring to biological sequence comparison.
Any improvement in the execution speed of BLAST would be of great importance in the practice of bioinformatics, and facilitate coping with ever increasing sizes of biomolecular databases.
Results: Using a general-purpose graphics processing unit (GPU), we have developed GPU-BLAST, an accelerated version of the popular NCBI-BLAST.
The implementation is based on the source code of NCBI-BLAST, thus maintaining the same input and output interface while producing identical results.
In comparison to the sequential NCBI-BLAST, the speedups achieved by GPU-BLAST range mostly between 3 and 4.
Availability: The source code of GPU-BLAST is freely available atContact: sahinidis@cmu.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on June 3, 2010; revised on October 19, 2010; accepted on November 12, 2010 1 INTRODUCTION BLAST was introduced as a sequence alignment heuristic that was an order of magnitude faster than earlier approaches for analyzing biological information.
Very quickly, this software became a landmark enabling technique for bioinformatics.
According to the Web of Science, the paper that describes the first version of ungapped BLAST (Altschul et al., 1990) has been cited more than 28 000 times.
In addition, the paper that describes the gapped version of the algorithm and a technique to speed up the earlier version by a factor of three (Altschul et al., 1997) has been cited more than 25 000 times.
The level of usage of BLAST suggests that any improvement in its execution speed will result in significant impact in bioinformatics.
Research efforts in this direction have been substantial and have relied mainly on custom-designed hardware (Sotiriades and Dollas, 2007) and parallel supercomputing (Lin et al., 2008).
Even though these efforts have resulted in impressive speedups of up to three orders of magnitude, neither custom hardware nor supercomputers are easily accessible by the majority of BLAST users.
To whom correspondence should be addressed.
With the advent of multicore processors, there have been several efforts to parallelize BLAST and speedup its execution on commodity hardware.
The National Center for Biotechnology Information (NCBI) has developed a version of BLAST that exploits multicore processors for the first phase of the algorithm (Camacho et al., 2009).
Another parallel version of BLAST (Nguyen and Lavenier, 2009) exploits two features of modern microprocessors SSE instructions and multithreadingand achieves speedups of up to 5.6 times compared with NCBI-BLAST.
However, the resulting protein alignments are up to 5.9% different than those produced by NCBI-BLAST (Table 4 in Nguyen and Lavenier, 2009).
Recently, Graphics Processing Units (GPUs) became available as a general purpose processing platforms.
We were drawn to GPUs because of their exceptionally high performance-to-cost ratio.
For around $1500, it is possible to combine a personal computer with a GPU and achieve trillions of peak floating point operations per second (FLOPS) performance.
GPU technology brings supercomputing power to the desktop, thus facilitating the widespread use of parallel algorithms by bioinformaticians.
However, algorithms that perform well on a CPU may not perform as well on a GPU (c.f.
Elble et al., 2010).
Algorithm developers must develop new algorithms in order to harvest the GPUs massive parallel nature.
GPUs were designed to accelerate graphics processing and quickly outperformed CPUs by over an order of a magnitude in terms of FLOPS and memory bandwidth performance.
This potential was initially difficult to harness in applications beyond graphics.
The situation changed in 2007 with the introduction of NVIDIAs Compute Unified Device Architecture (CUDA), a software and hardware environment that facilitates the adoption of GPUs in general purpose computing (Nickolls, 2007).
Since then, the use of GPUs has proved advantageous in a number of computationally intensive bioinformatics problems, including the SmithWaterman alignment algorithm (Manavski and Valle, 2008), molecular docking (Sukhwani and Herbordt, 2009), the protein-folding problem (Beberg et al., 2009; Shirts and Pande, 2000), DNA sequencing (Schatz et al., 2007), computational proteomics (Hussong et al., 2009), statistical phylogenetics (Suchard and Rambaut, 2009), biological systems simulation (Dematte and Prandi, 2010) and cellular-level simulation (Richmond et al., 2010).
Several GPU-based bioinformatics software can be found at http://www.nvidia.com/object/tesla_bio_workbench.html.
A GPU-based BLAST was recently developed by Ling and Benkrid (2010), leading to speedups between 1.7 and 2.7 in comparison to NCBI-BLAST.
Its authors report that this The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:58 27/12/2010 Bioinformatics-btq644.tex] Page: 183 182188 GPU-BLAST implementation is not guaranteed to give results identical to those from NCBI-BLAST.
Lius GPU-based BLAST (www.nvidia.com/ object/blastp_on_tesla.html) achieves speedups of six.
Using default options for Lius code and NCBI-BLAST 2.2.24, we obtained different alignments for all 51 sequences provided in the queries directory of the installation of GPU-BLAST.
However, most users of bioinformatics software are reluctant to use implementations of BLAST that may produce alignments that are not identical to those obtained from NCBI-BLAST.
We built GPU-BLAST directly on top of the NCBI-BLAST code.
As a result, GPU-BLAST has a familiar interface to the user and, most importantly, produces identical search results with NCBIBLAST.
We took advantage of the algorithms parallel aspects by mapping it on the GPU multithreaded processing environment, while also allowing the concurrent utilization of multiple CPU threads in parallel.
Although GPU-BLAST shares many data structures with NCBI-BLAST, we made necessary modifications to exploit the GPU without compromising the accuracy of the produced alignments.
The current version of GPU-BLAST can perform protein alignments up to 4 times faster than the single-threaded NCBI-BLAST.
Even compared to a six-threaded NCBI-BLAST, the GPU-BLAST is nearly twice as fast.
These attributes are likely to facilitate the adoption of GPU-BLAST by the bioinformatics community.
The remainder of the article is organized as follows.
In Section 2, we present an overview of the BLAST algorithm.
In Section 3, we describe the most important architectural features related to GPUBLAST, and in Section 4 we present the implementation of the algorithm on the GPU.
In Section 5, we present the quantitative results of the implementation, followed by conclusions in Section 6.
2 ALGORITHM The sequence alignment problem calls for searching a sequence database for matches with a query sequence.
The first proposed method to solve this problem was the SmithWaterman algorithm (Smith and Waterman, 1981).
Although this algorithm produces an optimal alignment between two sequences and runs in time polynomial in the length of the two sequences, it is computationally expensive for long sequences, and, in many cases, overlooks alignments that are suboptimal but may provide useful biological information.
These shortcomings became increasingly pronounced with the increasing size of biological databases.
BLAST addresses these problems with a heuristic that is fast and biologically relevant.
The approach consists of three main steps: seeding, extension, and evaluation.
The seeding step identifies short words common between the query and a database sequence and uses them as seeds in the extension step.
The word length is user defined and affects the accuracy and speed of the algorithm; longer words result in fewer seeds, and, consequently, shorter execution times.
The second step investigates whether the seeds belong to longer, common subsequences.
This step discards the false positive seeds that occur by chance and keeps the seeds that occur because they are part of a longer common subsequence.
This is achieved by extending the alignment to the left and right of the seed.
The unconditional left and right extension in the initial version of BLAST (Altschul et al., 1990) is called the one-hit method and typically consumes over 90% of BLASTs total execution time (Peters and Sikorski, 1997).
In order to improve this computationally intensive part of the algorithm, the two-hit extension was introduced in 1997 (Altschul et al., 1997).
In this method, an extension is invoked only for seeds that are within a user-defined distance from nonoverlapping seeds, thus reducing the computational cost by half.
Initially, the seeds are extended from both the left and right without inserting any gaps.
During this process, the quality of the ungapped alignment is gauged by the score of each pair of aligned amino acids using a scoring matrix, such as the popular BLOSUM62 (Henikoff and Henikoff, 1992).
If the ungapped score is above a user-defined threshold, the seed can be used to produce a gapped alignment based on a SmithWaterman type algorithm (Smith and Waterman, 1981).
The evaluation step relies on the score produced by the ungapped or the gapped extension step, the query and database sequence lengths, the substitution matrix and the sequence statistics.
With this information, the alignment is accepted as statistically significant if the probability of finding such an alignment by chance is lower than a user-defined value (Karlin and Altschul, 1990).
A profiling study of NCBI-BLAST for protein alignment is depicted in Figure 1.
The time spent in each step of the algorithm can vary substantially with different queries.
However, as Figure 1 reveals, the seeding and the ungapped extension are the most computationally intensive parts.
For ungapped alignments, these two steps consume over 95% of the total execution time.
For gapped alignments, the seeding and the ungapped extension steps consume 75% of the time, while 20% of the time is spent on the gapped extension.
Based on these observations, we decided to focus our parallelization efforts on the seeding and ungapped extension steps.
3 SYSTEM AND METHODS A GPU is a massively parallel computer designed to accelerate computationally intensive applications by operating in a single-instruction multiple-thread (SIMT) mode.
The same instructions are executed in parallel by multiple threads that run on identical cores and can operate on different data.
Figure 2 presents a block diagram of an NVIDIA GPU as it is executing GPU-BLAST.
The schematic shows that there are N GPU multiprocessors, each containing M processors.
The executing threads are organized into so-called blocks, and the blocks are organized into a so-called grid of blocks.
The number of threads and blocks is user defined, and the GPU scheduling mechanism assigns the execution of each thread block to a specific GPU multiprocessor.
Since each GPU multiprocessor has N processors, there is a maximum number of threads that can physically execute in parallel.
This thread group is called a warp.
A thread block may contain more than one warp, and the GPU scheduler decides in which order and when to execute each warp.
This gives the capability to the scheduler to increase the overall utilization of the GPU multiprocessor by putting a warp on hold, e.g.when waiting for data, and allow another one to execute.
Since each GPU multiprocessor has a single-instruction unit, there is one instruction dispatched at any given time.
Hence, parallelization is maximized when all threads of a warp agree on their execution path.
If threads of a warp diverge via a data-dependent conditional branch, the warp serially executes each branch path taken, disabling threads that are not on that path.
When all paths complete, the threads converge back to the same execution path.
Extensive thread divergence can have a detrimental effect on performance.
The schematic in Figure 2 illustrates that there are available different types of memory, each with different functionality, size and speed.
Depending on the amount of data and anticipated data access pattern, the programmer must organize and store the data in the most appropriate memory, in order to achieve the best possible utilization of the available memory bandwidth.
The global memory is the largest in size and the slowest.
It can be read and written by the CPU and the GPU threads, thus allowing the CPU to send data to the GPU and vise versa.
The global memory access pattern by the threads 183 [15:58 27/12/2010 Bioinformatics-btq644.tex] Page: 184 182188 P.D.Vouzis and N.V.Sahinidis A B Fig.1.
Profiling of the NCBI-BLAST code for queries of length 2 to 4998 for two-hit extensions.
(A) For ungapped alignments, on average, the seed identification and extension steps, respectively, consume 75% of the total time (blue) and 20% of the total time (green).
(B) For gapped alignment, on average, the seed identification, two-hit ungapped extension and gapped extension steps, respectively, consume 55% of the total time (blue), 20% of the total time (green) and 20% of the total time (orange).
Fig.2.
The architecture and the memory organization of an NVIDIA GPU as it is executing GPU-BLAST.
The data structures used by GPU BLAST are stored in the appropriate memory type, according to their size and access pattern.
can affect substantially the data transfer bandwidth; the more coalesced the memory accessing within a half warp, the higher the achieved bandwidth.
The constant memory is part of the global memory and is read-write for the CPU and read-only for the GPU threads.
Constant memory can offer higher bandwidth than the global memory when all threads of a half warp access the same input data.
The shared memory is the smallest and the fastest and is shared by all processors of a GPU multiprocessor.
It is a read-write memory by the GPU threads only, and it can be used to communicate data between threads that belong to the same block.
Shared memory bandwidth can be affected by the thread-access pattern, but to a lesser extent than the global memory accessing.
The registers of a GPU multiprocessor are shared between its processors, and each thread uses an exclusive set of registers.
The programmer does not have explicit control on the registers, as the latter are used for the execution of a program in the same way as on a general purpose CPU.
GPUs also contain local and texture memory, which were not found useful in the context of GPU-BLAST and are not depicted in Figure 2.
Local memory, in particular, is used by the compiler automatically to store variables 184 [15:58 27/12/2010 Bioinformatics-btq644.tex] Page: 185 182188 GPU-BLAST if needed, but was not used for GPU-BLAST.
Texture memory, on the other hand, is controlled by the programmer and can benefit applications with spatial locality where global memory access is the bottleneck.
As discussed later, however, thread divergence instead of global memory access is GPU-BLASTs bottleneck.
4 IMPLEMENTATION The most important component of the implementation is the design of the data structures, which affect the efficiency of the parallelization and overall implementation.
Since GPU-BLAST is embedded in the NCBI-BLAST code, the two implementations share data structures.
As shown in Figure 2, the most important data structures used by GPU-BLAST consist of a table holding the substitution matrix, a presence bit vector holding information on whether a specific amino acid word is present in the query, a queryindex table and an overflow table holding the positions of the words of the query, a table holding the database subjects and an index table holding the resulting ungapped alignments between the query and each of the database subjects.
The location of each data structure in memory was carefully selected, depending on data size and how often each structure is accessed during execution.
In particular, we have stored frequently accessed structures in the fastest possible memories that could accommodate their size.
The query-index table is created in a preprocessing step of the algorithm.
For each word, this table stores how many times the word appears in the query, and the location of each appearance.
Theoretically, if the word length is w and the query length is l, a word can appear up to lw+1 times (corresponding to the case when all the query amino acids are identical).
In practice, however, each word appears only a few times in a query.
For this reason, the locations of words that appear up to three times are stored in the query index table.
For all other cases, instead of locations, the index table contains a pointer to an overflow vector that holds the locations of the words in the query.
The index table and the overflow vector cannot fit in the shared memory and are stored in the global memory, as shown in Figure 2.
Each bit of the presence vector corresponds to a word and is set only if that word appears in the query.
Since its size is only a fraction of the query index table, this vector can be stored in the smaller but faster shared memory.
The query is uniformly accessed by all GPU threads.
For this reason, it is stored in the constant memory, which is the most suitable for this access pattern.
The protein database that the query is compared against is also stored in the global memory due to its size.
Parallelization in the execution step of GPU-BLAST involves assigning the database subjects to different GPU threads.
In order to balance the load between the threads and avoid having threads of the same warp work on database sequences with substantial length differences, the sequences are first sorted according to the number of amino acids they contain.
Sorting is embedded in the formatting of a FASTA database, which is required by NCBI-BLAST.
This operation is done once per database before this database is used and does not affect the alignment obtained.
Thus, this operation does not add any overhead to the execution time of the algorithm for NCBI-or GPU-BLAST.
Not having the database sorted would result in cases where threads of the same warp have to compare the query with sequences that differ significantly in length, thus causing excessive thread divergence.
By sorting, this thread divergence overhead is reduced considerably.
Each thread scans consecutive words of a different subject and checks, via the presence vector, whether these words exist in the query or not.
The presence vector is not necessary for the implementation of the algorithm since the query index table stores the word locations.
Yet, the presence vector is small enough to fit in the shared memory of a GPU multiprocessor.
Thanks to the information provided by this vector, a processor can identify word matches through information readily available in the fast shared memory, without having to access the slow global memory.
Only when matches exist, the processor accesses the query index table in the global memory to retrieve information on the number and locations of the seeds.
The substitution matrix is stored in the shared memory because it is used very frequently during the alignment score calculations.
In the next step, each seed is extended left and right according to the two-hit method.
Each extension that achieves a score above the user-defined threshold is characterized as a high scoring pair and its coordinates are stored in the output table.
Since it is not known in advance how many high scoring pairs per database subject will be discovered, for their storage we follow a similar technique used for the query index table.
In practice, for each subject, there are only a few high scoring pairs discovered.
Thus, the coordinates of only up to two pairs are stored in the output table.
Database subjects that have more pairs are processed by the CPU after completion of the GPU execution.
The CPU has a copy of the database and the data structures, and, instead of waiting idle for the GPU to parse the entire database, carries out part of the alignment task, thus reducing the total execution time.
The database is split in two parts that are processed separately, and when both processors finish execution, the CPU merges the results, and, if desired, carries out a gapped alignment.
In BLAST, comparison of a query with any sequence in the database can be carried out independently from comparisons with other database sequences.
While this observation makes parallelization of this algorithm appear an obvious task, the challenge here is to develop a mechanism capable of distributing comparisons to different processors so that processors are fully utilized and complete their assigned tasks at the same time.
The processing of short and long database sequences must be done in a way that minimizes idle times for processors.
The data structures used in GPU-BLAST result in a carefully orchestrated parallel execution of comparisons of short and long sequences, thus utilizing the GPU as much as possible.
Since GPU-BLAST is built on top of NCBI-BLAST, both share a common user interface.
GPU-BLAST has the following additional options: *** GPU options-gpu <Boolean> Use GPU for blastp Default = F-gpu_threads <Integer, 1...1024> Number of GPU threads per block Default = 64-gpu_blocks <Integer, 1...65536> Number of GPU block per grid Default = 512-method <Integer, 1...2> 185 [15:58 27/12/2010 Bioinformatics-btq644.tex] Page: 186 182188 P.D.Vouzis and N.V.Sahinidis Method to be used 1 = for GPU-based sequence alignment 2 = for GPU database creation Default = 1 * Incompatible with: num_threads-gpu <Boolean> determines whether the GPU is used or not.
With-gpu_blocks and-gpu_threads, the user can define the number of blocks and threads per block to be used by the GPU.
When-gpu T GPU-BLAST carries out the sequence alignment using the aforementioned options.
When-method 2, and provided that-gpu T, GPU-BLAST converts the input database into the format required by GPU-BLAST, stores the produced database into a separate file and produces a second file which includes information about the GPU database.
The conversion has to be done only once for each database, and all subsequent executions of GPU-BLAST read this database from the disk.
Hence, this time is amortized over thousands or millions of future queries.
For-gpu F, all the previous options are ignored and GPUBLAST executes according to NCBI-BLAST.
The current version of GPU-BLAST works only for protein alignments and can utilize more than one CPU threads in parallel with the GPU by using the NCBIBLAST option-num_threads <Integer>=1>.
The option-method is incompatible with the option-num_threads because the creation of the GPU database does not support multiple threads.
The execution of GPU-BLAST consists of three basic components: (i) initialization of the GPU and data transfer from the CPU to the GPU, (ii) concurrent GPU-CPU algorithm execution and (iii) transfer of results from the GPU to the CPU.
These basic steps are depicted in the flow chart of Figure 3.
We can see BLASTs basic steps and their execution sequence depending on whether the user chooses to use the GPU or not.
If the GPU is used (-gpu T), the CPU reads the GPU-BLAST database and sends all the necessary data to the GPU.
For the seeding and extensions steps, the CPU and GPU work concurrently; the GPU by deploying multiple parallel threads on the sequences, as defined by-gpu_blocks and-gpu_threads, and the CPU on the remaining sequences by using one or more threads as defined by-num_threads.
After both platforms finish, the high scoring pairs are transferred from the GPU to the CPU, and the CPU merges them with its own high scoring pairs.
From that point, the algorithm follows the NCBIBLAST execution path without any modifications.
GPU-BLAST follows the NCBI-BLAST execution path when-gpu F. GPU-BLAST was implemented on an NVIDIA Fermi C2050 GPU with 448 processors at 1.15 GHz, 64 KB of shared memory per GPU multiprocessor, 64 KB of constant memory and 3 GB of global memory.
The implementation was built using CUDA, which offers better performance than OpenCL on NVIDIA GPUs (Weber et al., 2010).
While this software choice limits GPU-BLAST to NVIDIA cards, future versions will provide support for OpenCL in order to extend applicability to other GPU hardware.
The GPU was combined with a six-core Intel Xeon host CPU at 2.67 GHz with 12 GB of memory.
Since GPU-BLAST uses the GPU and the CPU concurrently, the workload has to be properly distributed between the two in order to maximize the utilization of the CPUGPU combination.
The ideal load balancing is achieved when the execution times on the CPU and the GPU are equal.
GPU-BLAST assigns predetermined fractions of the Fig.3.
Execution flow of GPU-BLAST.
The blue blocks are executed on the CPU and orange ones on the GPU (HSP: high scoring pairs, DB: database).
database between the CPU and the GPU, based on the number of available CPU threads.
We determined these ratios after extensive experimentation with different databases and number of available CPU threads.
5 RESULTS The database used for computations was the latest releases of the env_nr (ftp://ftp.ncbi.nlm.nih.gov/blast/db/) protein database, which contains 6 031 291 sequences and its size is 1.3 GB (October 2010).
The queries were 51 mouse sequences with lengths from 2 to 4998.
These sequences were obtained from the UniProt database (http://www.uniprot.org/) and are provided in the queries directory of the GPU-BLAST distribution.
Figure 4A depicts the speedups achieved by the ungapped and gapped versions of GPU-BLAST, in comparison to one-threaded and six-threaded NCBI-BLAST for the env_nr database.
These speedups depend on the query length.
The speedups increase for query lengths 186 ftp://ftp.ncbi.nlm.nih.gov/blast/db/[15:58 27/12/2010 Bioinformatics-btq644.tex] Page: 187 182188 GPU-BLAST A B Fig.4.
GPU-BLAST speedups relative to the CPU as a function of sequence length (A), and average speedups as a function of CPU threads working in parallel with GPU-BLAST (B).
Speedups were calculated based on start-to-finish wall-clock times.
of approximately up to 1000 amino acids for the one-threaded and 2000 for the six-threaded implementations, after which the speedup remains essentially constant.
For shorter queries, the speedup is slightly lower because the seed identification and the extension steps consume a smaller percentage of the total execution time, as seen in Figure 1.
The scattering of the speedups in Figure 4A can be attributed to several factors, including the number of seeds identified, the extension length around each seed and the number of ungapped and/or gapped extensions, which affect the thread divergence on the GPU and consequently its performance.
The GPUs theoretical peak performance is 1030 GFLOPS in single precision and 515 GFLOPS in double precision.
The corresponding numbers for the CPU are 128 GFLOPS and 64 GFLOPS.
Although the GPUs peak performance in GFLOPS is about eight times higher than the CPUs, the speedups achieved by GPU-BLAST are currently around four.
The reason for this difference is that the SIMT architecture of the GPU executes concurrently multiple threads that operate on different data and follow the same execution path in each warp.
Whenever the execution paths within a warp diverge, the threads are serialized and overall performance is reduced.
The one-threaded GPU-BLAST is faster for ungapped than gapped alignments because it is possible to transfer 95% of the computations to the GPU in the ungapped case, compared with only 75% in the gapped case as shown in Figure 1.
For the six-threaded GPU-BLAST, the total speedup is smaller and the difference between the ungapped and gapped version diminished because the CPU can handle a bigger workload leaving a smaller margin to the GPU to speedup the total running time.
For the one-threaded GPU-BLAST the speedup is always bigger than one, except for the first sequence which has length two.
The six-threaded GPU-BLAST offers speedup for sequences longer than 500 amino acids.
In Figure 4B, we present average GPU-BLAST speedups when using up to six CPU threads in parallel with the GPU.
The times used to calculate each speedup are elapsed times to carry out a sequence alignment, which start from the beginning of GPUBLASTs execution and finish with the writing of the output alignments to a file.
We can see that GPU-BLAST achieves the largest speedups compared with single-threaded NCBI-BLAST, and the speedups decrease as the number of CPU threads increase.
Finally, in Figure 5, we present speedups relative to a single-threaded CPU.
Both multi-threaded CPU and CPU/GPU combinations are considered as a function of the number of available CPU threads.
In all cases, speedups were calculated based on the total time to align the entire set of queries.
As this figure shows, the multithreaded NCBI-BLAST itself does not scale linearly.
For instance, with six CPU threads, the NCBI-BLAST speedup is less than four.
GPU-BLAST inherits some of these limitations as it is built on top of NCBI-BLAST in order to guarantee the same output results.
Nonetheless, in all cases, the addition of the GPU considerably increases the observed speedups.
For instance, the sixthreaded GPU-BLAST achieves a speedup of nearly six for both gapped and ungapped alignments.
6 CONCLUSIONS Using carefully orchestrated parallel execution of comparisons of short and long sequences on a GPU, this article has demonstrated that GPU-BLAST can speed up the popular NCBI-BLAST code by nearly four times while producing identical results.
Moreover, our implementation is capable of using the GPU along with multiple CPU cores concurrently.
Hence, the performance of GPU-BLAST will benefit from future hardware advances of both CPU and GPU technologies.
The present version of GPU-BLAST only works for BLASTP.
Future work will extend the implementation to other BLAST methods, including PSI-BLAST which is more sensitive in detecting weak relationships between protein sequences (Altschul et al., 1997).
PSI-BLAST uses multiple iterations to scan the database, with each iteration constructing a position-specific score matrix that replaces the simple query.
Although there are differences in the implementations of BLAST and PSI-BLAST, both algorithms share several subroutines.
A profiling study of PSI-BLAST reveals that PSI-BLAST and gapped BLAST share the same subroutines that take most of their execution time.
In particular, the profiling graph 187 [15:58 27/12/2010 Bioinformatics-btq644.tex] Page: 188 182188 P.D.Vouzis and N.V.Sahinidis Fig.5.
Speedups relative to a single-threaded CPU as a function of CPU threads.
Speedups were calculated based on start-to-finish wall-clock times to align the entire set of queries.
of PSI-BLAST is almost identical to Figure 1B.
This suggests that PSI-BLAST can be implemented on the GPU in a similar fashion with GPU-BLAST and that similar speedups are likely.
Conflicts of Interest: none declared.
ABSTRACT Motivation: The study of metabolites (metabolomics) is increasingly being applied to investigate microbial, plant, environmental and mammalian systems.
One of the limiting factors is that of chemically identifying metabolites from mass spectrometric signals present in complex datasets.
Results: Three workflows have been developed to allow for the rapid, automated and high-throughput annotation and putative metabolite identification of electrospray LC-MS-derived metabolomic datasets.
The collection of workflows are defined as PUTMEDID_LCMS and perform feature annotation, matching of accurate m/z to the accurate mass of neutral molecules and associated molecular formula and matching of the molecular formulae to a reference file of metabolites.
The software is independent of the instrument and data pre-processing applied.
The number of false positives is reduced by eliminating the inaccurate matching of many artifact, isotope, multiply charged and complex adduct peaks through complex interrogation of experimental data.
Availability: The workflows, standard operating procedure and further information are publicly available at http://www.mcisb.org/ resources/putmedid.html.
Contact: warwick.dunn@manchester.ac.uk Received on November 5, 2010; revised on February 4, 2011; accepted on February 7, 2011 1 INTRODUCTION Systems biology is applied to study the components of, and more importantly their complex interactions in, biological systems.
One set of components which are studied in systems biology investigations are metabolites, either by targeted or holistic profiling experimental strategies (Dunn et al., 2011).
Low molecular weight inorganic and organic metabolites play important roles in the To whom correspondence should be addressed.
operation and maintenance of biological systems.
The study of metabolites (metabolomics) is increasingly being applied to investigate microbial (Bradley et al., 2009; MacKenzie et al., 2008; Mashego et al., 2007), plant (Allwood et al., 2008; Fernie and Schauer, 2009; Hall et al., 2008), environmental (Bundy et al., 2009; Viant et al., 2006) and mammalian (Griffin, 2008; Kenny et al., 2010; Lewis et al., 2008) systems.
Many studies follow a hypothesis generating or inductive strategy (Kell and Oliver, 2004) and start from a small and known subset of biological knowledge.
Valid experiments are designed to acquire robust and reproducible data on a wide range of different metabolites and metabolite classes from carefully selected samples.
Subsequent data analysis procedures define the metabolic differences associated with a biological change related to genotype, biological perturbation or environmental intervention (for example, drug therapy).
These studies employ a metabolic profiling strategy to detect a wide range of (but not all) metabolites from numerous biochemical classes to obtain maximum metabolic information rapidly.
This strategy provides the detection of hundreds or thousands of metabolites.
However, due to the diverse range of chemical and physical properties and the wide concentration range of metabolites within the metabolome, no single analytical technology can provide the non-biased quantitative detection of all metabolites in a biological system (Dunn, 2008).
Metabolic profiling provides semi-quantitative data, typically as chromatographic peak areas, rather than absolute quantitation where metabolite concentrations would be reported.
Mass spectrometry (MS) and nuclear magnetic resonance spectroscopy (NMR) have been widely employed and provide complementary roles (Dunn et al., 2005, 2011).
Chromatography-MS techniques provide advantages for these highly complex biological samples and include gas chromatography (GC-MS), liquid chromatography (LC-MS) and LC derivatives including ultra performance liquid chromatography (UPLC-MS).
Capillary Electrophoresis-MS (Soga et al., 2003) and LC-MS apply electrospray ionization.
Direct infusion mass spectrometry (DIMS) can also be applied, though lacks the chromatographic separation of 1108 The Author 2011.
Published by Oxford University Press.
All rights reserved.
For Permissions, please email: journals.permissions@oup.com at O U P site access on July 12, 2013nloaded from  [11:35 21/3/2011 Bioinformatics-btr079.tex] Page: 1109 11081112 Automated workflows for accurate mass-based putative metabolite identification metabolites (Dunn et al., 2005; Southam et al., 2007).
Hyphenated platforms can provide (with suitable operation and mass calibration) high separation resolution, high mass resolution and mass accuracy, typical limits of detection of micromol per litre and the ability to identify metabolites through a combination of Retention Time (RT)/index, accurate mass and gas-phase fragmentation-derived mass spectra.
Each of these platforms, whether hyphenated or nonhyphenated, provide different advantages and disadvantages for metabolite identification as has been reviewed previously (Dunn et al., 2011).
The increasing use of high mass resolution LC-MS and UPLC-MS platforms provides the detection of many thousands of features [see Brown et al.(2009) for a comparison of the features detected related to sample type] with high mass accuracy and has led to a need to develop data handling methods for the conversion of this raw analytical data into biological knowledge.
One of the data processing procedures essential in metabolic profiling is metabolite identification which has been reviewed previously (Dunn et al., 2011; Wishart, 2009).
Guidelines have been provided by the Metabolomics Standards Initiative to define how the different levels of metabolite identification can be reported (Sumner et al., 2007).
A range of approaches can be applied for metabolite identification.
Two generalized types of identification are achievable: putative identification and definitive identification.
Putative identification usually employs one or more molecular properties for identification, but does not compare these to the same properties of an authentic chemical standard as is performed for definitive identification.
The accurate mass (or m/z) of an analyte and its associated isotopologues can be used to define molecular formulae (MFs) from which suitable metabolites can be derived by searching a range of electronic resources [e.g.PubChem (http://pubchem.ncbi.nlm.nih.gov/), HMDB (http://www.hmdb.ca/), KEGG (http://www.genome.jp/kegg/) and MMD (Brown et al., 2009)] and has been previously shown (Brown et al., 2005; Junot et al., 2010; Lane et al., 2008; Rogers et al., 2009).
Direct matching of accurate mass (or m/z) to data in electronic resources without intermediate matching to MF can also be performed.
However, structural isomers and stereoisomers have the same accurate mass and therefore require a separate, orthogonal property for identification of all potential isomers.
Typically, this is chromatographic separation though separation of isomers is not always achievable.
Separation of enantiomers requires a chiral chromatography column.
Definitive identification employs at least two properties (typically RT or index and fragmentation mass spectrum) and compares these properties to an authentic chemical standard analysed under identical analytical conditions.
In LC-MS and UPLC-MS applications, the accurate masses of the detected ions is employed in combination with other rules (e.g.isotope ratio of 12C and 13C isotopic peaks to define the number of carbon atoms present in the MF; calculated as peak area 13C isotopologue/peak area 12C isotopologue) to generate MF and thus provide putative metabolite identification(s).
Specific rules are not always applicable.
For example, 13C/12C isotopic peaks can only be applied on instruments where accurate isotopic ratios are detected and where 13C-artificially labelled metabolites have not been applied in the biological experiment.
Fragmentation mass spectra (MS/MS or MSn) are then used to provide increased confidence through comparison to authentic chemical standards or to in silico-derived fragmentation mass spectra to give an unequivocal metabolite identification (Wolf et al., 2010).
It should be noted that not all authentic chemical standards are commercially available and that MS/MS and MSn fragmentation mass spectra are not always accurate in unequivocal identification of two isomeric metabolites.
Studies by the authors have assessed the level of complexity of electrospray UPLC-MS data derived from biological extracts in a metabolic profiling strategy (Brown et al., 2009).
This work has shown that a multitude of different ion types are observed including commonly described ions (e.g.protonated, deprotonated, sodium or potassium adducts and 13C isotope).
However, many other unexpected types of ions including adducts (e.g.complex combinations of sodium chloride and formate dependant on the matrix type and mobile phase), fragments, dimers, multiply charged and instrument specific ions (e.g.Fourier Transform (FT) artifact peaks) are also detected.
Each different ion type is defined as a feature, whose accurate mass (or m/z) is unique but whose RT and chromatographic peak profiles are identical.
Information on the type of ion should be applied in metabolite identification (Brown et al., 2009; Draper et al., 2009).
Automated software or workflows for high-throughput identification of large metabolomic datasets are not freely available.
Currently, metabolite identification is a manual or semi-automated process assessing those features of biological interest and not the complete set of detected features (Dunn, 2008).
For metabolomics to be successful it is essential to derive biological knowledge from analytical data, a view emphasized by a recent Metabolomics ASMS Workshop Survey 2009 which found that the biggest bottlenecks in metabolomics were thought to be identification of metabolites and assigning of biological interest (http://fiehnlab.ucdavis.edu/staff/ kind/Metabolomics-Survey-2009).
To fill the gap in requirements, three workflows have been written to perform for the first time integrated, automated and high-throughput annotation and putative metabolite identification of electrospray LC-MS and UPLC-MS metabolomic datasets in a freely available package.
The workflows were developed in the Taverna Workflow Management System to provide flexibility in their operation and the ability to rapidly and simply integrate with web services and other Taverna workflows in the future [for example, see Li et al.(2008)], so as to provide integrated data analysis and bioinformatics or cheminformatics packages.
Examples are available on myExperiment, a repository of workflows freely available to the scientific community (http://www.myexperiment.org/), including a workflow to perform data pre-processing with XCMS and a workflow to perform in silico fragmentation applying MetFrag.
The achievement of this level of integration would be more technically demanding and would require significantly greater expertise and time if coded in many other programming languages.
Taverna is also easy to operate for relative novices with minimal training as the process involves defining parameter values and files only.
2 METHODS AND IMPLEMENTATION The current lack of freely available workflows or software to process deconvoluted data acquired from electrospray LC-MS experiments led the authors to develop three workflows.
The workflows have been developed in Taverna (Hull et al., 2006) using Beanshell, a Java scripting language, which is enabled in Taverna and can perform data manipulation, parsing and formatting.
Taverna can be downloaded from http://www.taverna.org.uk.
The workflows were developed under Windows using Taverna v1.7.0 1109 at O U P site access on July 12, 2013nloaded from  [11:35 21/3/2011 Bioinformatics-btr079.tex] Page: 1110 11081112 M.Brown et al.and subsequently tested using Taverna Workbench 2.2.0.
In combination, the workflows perform the automated, high-throughput annotation and putative metabolite identification of electrospray LC-MS and UPLC-MS metabolomic datasets.
The software has been coded as a series of separate workflows to allow more flexibility in the analysis of data by obviating the need to re-run the whole pipeline when altering one of the workflow parameters, such as the mass tolerance or database.
This approach also reduces the likelihood of memory problems when handling large datasets on computers with small RAM.
The workflows, related files and standard operating procedure (SOP) are available to the user community on http://www.mcisb.org/resources/ putmedid.html and will also be placed on MyExperiment (http://www.myexperiment.org/).
In general, the input and output files are tab-delimited (*.txt) files and are sorted by ascending accurate mass or MF as appropriate (ordered as C, H, N, O, P, S, Br, Cl, F, Si in ascending alphanumeric form as is standard for PubChem).
Internal checks are made within the workflows to ensure that the number of features in both peak and data files match (workflow for correlation analysis) and that the study and reference input files are sorted either by accurate m/z (workflow for metabolic feature annotation) or MF (workflow for metabolite annotation).
Termination of the process and reporting of an informative error message occurs if this is not the case.
The three workflows are described in detail in the available SOP and briefly below.
2.1 Workflow for correlation analysis The workflow for correlation analysis (List_CorrData) allows the user to calculate either Pearson or Spearman rank correlations or read in previously calculated correlation data.
The correlation calculations allow for NaN, Inf and 0 in the input data and are equivalent to using the Matlab (http://www.mathworks.co.uk/) corr function with the following parameters: corr(Xdata, rows, pairwise, type, Pearson) or corr(Xdata, rows, pairwise, type, Spearman) 2.2 Workflow for metabolic feature annotation The workflow for metabolic feature annotation (annotate_MassMatch) uses correlation coefficient information calculated in the workflow for correlation analysis, accurate m/z difference, RT and median peak area data to group together and annotate features with the type of ion (isotope, adduct, dimer, others) originating from the same metabolite.
The same metabolite can be detected as different ion types each with different m/z.
Following annotation, the experimentally determined accurate m/z are matched to the accurate m/z of unique MF in a reference file within a specified m/z tolerance.
2.3 Workflow for metabolite annotation In the workflow for metabolite annotation (matchMMF_MF), the MF from the output file calculated in the workflow for metabolic feature annotation is matched to the MF from the Reference file of metabolites (trimMMD_sortMF.txt or other appropriate reference file).
The metabolite information for all matched MFs is added to the input data and output data are generated in three formats, each of which can be saved as tab-delimited files by the user.
3 RESULTS An assessment has been made of the workflows ability to perform putative metabolite identification using two reference files: (i) a listing of unique accurate mass/MF data from PubChem using specific elements (C, H, N, O, P, S, Br, Cl, F, Si only; file downloaded from http://fiehnlab.ucdavis.edu/projects/Seven_Golden_Rules/) selected to give a wider selection of MFs and (ii) The Manchester Metabolomics Database (Brown et al., 2009) constructed with data Table 1.
Distribution of annotated peaks in negative and positive ion mode Features summary Negative ion mode Positive ion mode No.
of features 2173 4348 No.
of correlations (>0.7, RT 5 s) 11 867 50 867 Invalid RT (40 s < RT > 1200 s) 224 487 FT artifact peaks 61 66 Isotopes (13C, 34S, 37Cl) 455 1170 Multiply charged ions 22 444 Salt ions (not adducted to metabolites) 40 31 Total no.
of excluded features 802 (36.9%) 2198 (50.6%) No.
of features remaining for identification 1371 (63.1% of all detected features) 2150 (49.4% of all detected features) from genome-scale metabolic reconstructions, HMDB, KEGG, LIPIDMAPS, BioCyc and DrugBank.
Data from all these sources are included to provide a comprehensive set of metabolites.
For example, HMDB does not contain all lipids that are theoretically present in human biofluids and tissues and therefore inclusion of data from LIPIDMAPS provides greater complementary metabolite coverage.
A clinical dataset of fasting blood serum samples were taken from participants according to ethical guidelines and stored before being analysed with quality control samples in a random order and within 48 h of reconstitution using an UPLC (Waters UPLC Acquity, Elstree, UK) coupled on-line to an electrospray LTQ-Orbitrap hybrid mass spectrometer (ThermoFisher Scientific, Bremen, Germany).
The collection and storage of serum samples and the UPLC and mass spectrometer methods applied have been previously described (Dunn et al., 2008; Zelena et al., 2009).
Independent samples (118 in total) were analysed in both positive and negative ion mode.
Raw data files (.RAW) were converted to the NetCDF format using the File converter program in XCalibur (ThermoFisher Scientific, Bremen, Germany).
Deconvolution of data was performed using XCMS, running on R version 2.6.0, an open-source deconvolution program available for LC-MS data (Smith et al., 2006) using identical settings to those reported previously (Dunn et al., 2008).
This produced a list of features with associated RT, accurate m/z and chromatographic peak area.
For these data, the mass accuracy was assessed using a set of 35 and 50 metabolites commonly detected in serum and plasma in positive and negative ion modes, respectively.
Shown in Table 1 is the distribution of annotated features found in the dataset and excluded from further mass matching.
In positive ion mode >50% of features were marked for exclusion from further metabolite identification, which was considerably higher than in negative ion mode (36.9%) due in part to the much greater occurrence of multiply charged ions (10% of all features).
It should be noted that multiply charged ions can be peptides, proteins or high molecular weight metabolites capable of carrying multiple charges.
Approximately 25% of these excluded features are isotopic peaks.
These are annotated and linked to the related molecular ion.
In the workflow for metabolite annotation, all isotope and FT artifact peaks are labelled with the accurate identification observed for the molecular or adduct ions.
1110 at O U P site access on July 12, 2013nloaded from  [11:35 21/3/2011 Bioinformatics-btr079.tex] Page: 1111 11081112 Automated workflows for accurate mass-based putative metabolite identification Using a mass tolerance of 3 p.p.m., 6% of the remaining features were not matched to any unique MFs in the PubChem reference file (301507 entries).
Seventy-six percent in negative and 60% in positive ion mode of the remaining features (791 and 1292 features) were fully annotated and given putative metabolite identification using a revised version of the MMD database (31648 entries).
The reference data file is based on molecules and parent compounds carrying no charge and is derived from the MMD which contains an array of information from a wide variety of electronic resources.
The MMD data file was revised by removal (or modification) of charged species of salts e.g.sodium ascorbate, calcium citrate, metamphetamine hydrochloride.
Obvious duplicates of data were removed and for many common metabolites e.g.amino acids and sugars only a single stereochemical form of the compound was retained.
The included form usually related to the one most well described in HMDB, and if not present in HMDB then as described in KEGG, and if not present in HMDB and KEGG then as described in LIPIDMAPS and resulted in a much cleaner dataset for putative metabolite identification.
A fairly stringent mass tolerance of 3 p.p.m.
was used in this analysis and in a number of cases the annotation of adducts is based on strong evidence, but the exact mass matching may be outside the allowed tolerance.
This is certainly seen for metabolites such as tryptophan where many ions/adducts are detected, some of which are within the 3 p.p.m.
mass tolerance and others, particularly K and NaCl/HCOONa adducts of low response, are in the mass error range of 310 p.p.m.
Increasing the mass tolerance would result in more of these adducts being correctly matched but would greatly increase the overall number of putative metabolite identifications through matching to a greater number of MFs.
This is possible if further data are acquired to reduce the number of potential hits (e.g.13C/12C isotope ratios or MS/MS fragmentation).
Additionally, neither reference file had complete information relevant to the human metabolomethe MMD is from a wide variety of sources and includes drugs (but not drug metabolites), and the PubChem reference file contains a limited number of elements and limited numbers of atoms per element.
The workflow for correlation analysis processed correlation data in 320 min depending on the option selected and the number of features.
The workflow for metabolic feature annotation processed the negative ion data in 3 min for the PubChem reference file.
In positive ion mode with 100% more peaks and nearly 5 times as many correlations, the processing time was <20 min.
The number of correlations is the rate determining factor and in most cases processing is <30 min and frequently <5 min.
The workflow for metabolite annotation matched MF derived from experimental data to MF in MMD rapidly when fewer than 5000 matches were present.
This process took just over 1 min to perform in each ion mode.
However, when using large reference files such as PubChem, typically up to 30 000 matches, processing time may be of the order of 1 h. The workflows developed are automated, rapid, open-source and freely available with all features fully annotated or given putative metabolite identifications.
The approach is flexible, it is independent of chromatographic deconvolution method and analytical instrument applied.
Additional adducts can be added to the adducts file for user-specific instruments, and data and organism-specific metabolite reference files can be used.
Two additional differently formatted outputs are available for all matched features and the workflows have the potential to be integrated with other Taverna workflows.
Large numbers of features are recognized as artifact, isotope, salt and multiply charged ions and removed from the metabolite identification process.
This in combination with data with good mass accuracy (in raw data or following post-acquisition mass alignment) results in a greatly reduced number of putative metabolite identifications within a specified mass tolerance (typically 3 p.p.m.
for the ThermoFisher Scientific LTQ-Orbitrap acquired data).
Using this approach, putative metabolite identification does not depend on a high level of experience in dealing with MS data and can be used as a starting point for subsequent definitive identification.
A number of false positives are always found although they are significantly reduced using this approach by annotation of ion type prior to assignment of accurate m/z to MF or metabolite.
Wide variation in m/z or RT range or missing values following deconvolution can result in any or all of the following: (i) mass difference can be outside mass tolerance limits (e.g.missed adduct); (ii) RT difference between two features may be outside given value (missed grouping); and (iii) correlation between two features may be below specified limit (missed adduct, missed grouping).
Within the software, allowance is made for this, for example, a feature (sodium adduct) that has a correlation with the parent metabolite below the correlation limit will not be grouped with this feature.
However, if the m/z is within the mass tolerance it will still be putatively identified as the appropriate sodiated ion.
Additional grouping information based on correlation is present in the output file and can assist the user when two or more MFs matches are reported for a feature.
4 CONCLUSIONS The workflows presented are rapid and high-throughput and greatly reduce the number of false positives by eliminating the inaccurate matching of many artifact, isotope and complex adduct peaks.
Subsequent definitive identification employing at least two properties of sample-derived metabolite and an authentic chemical standard (typically RT and fragmentation mass spectrum) can then be performed.
Additional information based on similarity measures (e.g.metabolite class or metabolite pathway) are being incorporated into the Manchester Metabolomics Database and will allow in time for further interrogation of the biological changes of interest within microbial, plant and mammalian metabolomic studies.
Further developments are planned to amalgamate the separate workflows together and to integrate with separate workflows to increase the applicability and ease of data analysis and interpretation.
Funding: UK Biotechnology and Biological Sciences Research Council (BBSRC) (BBC0082191); The Wellcome Trust (088075/A/08/Z); Johnson and Johnson, Cancer Research UK; The Manchester National Institute for Health Research (NIHR) Biomedical Research Centre.
Conflict of interest: none declared.
Abstract As a class of cis-regulatory elements, enhancers were first identified as the genomic regions that are able to markedly increase the transcription of genes nearly 30 years ago.
Enhancers can regulate gene expression in a cell-type specific and developmental stage specific manner.
Although experimental technologies have been developed to identify enhancers genome-wide, the design principle of the regulatory elements and the way they rewire the transcriptional regulatory network tempo-spatially are far from clear.
At present, developing predictive methods for enhancers, particularly for the cell-type specific activity of enhancers, is central to computational biology.
In this review, we survey the current computational approaches for active enhancer prediction and discuss future directions.
Introduction Gene transcription is regulated by a series of accurately orchestrated interactions between transcription factors (TFs) and cisregulatory DNA elements, e.g., promoters and enhancers [1].
Enhancers are often found in non-coding regions of a genome and generally distal to their target promoters.
The first characterized enhancer was a DNA segment that markedly increased the transcription of the b-globin gene in a transgenic assay in ang Z).
eijing Institute of Genomics, tics Society of China.
g by Elsevier jing Institute of Genomics, Chinese A the SV40 tumor virus genome about 30 years ago [2].
Nonetheless, global identification of enhancers and their activities remains challenging, since enhancers can activate transcription regardless of their location or orientation [3].
The development of computational enhancer recognition approaches has been greatly facilitated by the massive amount of genomic data available owing to the rapid advances in sequencing technologies in recent years.
Early algorithms were developed largely based on evolutionary constraints with the assumption that highly conserved non-coding regions should have functional potential [4].
However, conservation by itself is not sufficient to confer cell-type specific enhancer activities, suggesting that additional (e.g., epigenetic) information is required for accurate prediction.
Genome-wide maps of chromatin marks have been used to show that active enhancers are likely to be associated with certain characteristic chromatin signatures, e.g., monomethylation of histone H3 at lysine residue 4 (H3K4me1) [5].
But, Bonn et al.reported that H3K4me1 is cademy of Sciences and Genetics Society of China.
Production and hosting mailto:zhangzhihua@big.ac.cnWang C et al/ Computational Methods for Enhancer Prediction 143 distributed similarly between mesodermally active and inactive enhancers, indicating that the placement of H3K4me1 is not cell type specific during embryonic development [6].
Hitherto, to the best of our knowledge, there is no evidence that active enhancers should necessarily exhibit the same single or a combination of epigenetic marks across all the cell types [7].
Therefore, it is necessary to select optimal combinations of epigenetic marks to predict when and where an enhancer is active [811].
In this review, we first survey the most commonly adopted strategies in enhancer recognition and then discuss potential future directions.
The principle of enhancer recognition Enhancers may be characterized by quantitative measures, termed features, associated with the underlying DNA sequences.
In principle, an enhancer recognition algorithm utilizes informative and discriminative features as input to discriminate enhancers from non-enhancers, ideally from other non-enhancer cis-regulatory elements.
Algorithms and features are both important.
We therefore will discuss them separately.
Features can be briefly classified into three categories, namely comparative genomic features, TF binding related genetic features and epigenetic features (Figure 1).
Comparative genomic features mainly refer to the conservation scores calculated by comparing the genome sequences of different species.
The predictive power of comparative genomic features stems from the fact that functional genome regions (e.g., enhancers) are subjected to negative selection [12,13].
TF binding related genetic features use quantitative scores presumably representing the TF binding affinity at the DNA sequence of interest.
Figure 1 Features used in enhancer prediction algorithms The comparative genomic features are usually generated from compari features result from two sources, one from known TF binding motifs measured by various technologies.
See the main text for more details.
The DNA binding sites of a given TF are usually determined by the DNA nucleotide sequence and the binding affinity between the TF and the DNA sequence [1416].
It is believed that TFs are the actual operators for enhancer regulatory activities [17], which may explain why TF binding related genetic features are predictive.
Direct measurement of the binding affinity between a TF and DNA sequence is not easy.
However, the binding affinity can be inferred indirectly, either by experimentally measuring frequency of TF binding events, such as chromatin immunoprecipitation (ChIP) [18], or by calculating the similarity of the DNA sequences with a known TF binding motif [19,20].
The epigenetic feature mainly includes the level of histone modifications and of DNA methylation.
Recent experimental evidence supports the association of several histone modifications with enhancer activity.
The histone modification levels thus have served as features to predict active enhancers in humans [21,22].
Researchers also attempt to seek optimum combinations of these features for whole-genome prediction of active enhancers [5,911] (Table 1).
Obviously, not all the aforementioned features are equally important for active enhancer prediction.
The level of some dominant features showed strong correlation with enhancer activity [5,23], although the nature of the relationship between the features and enhancer states is poorly understood.
Further development of superior predictive methods can not only help us to reveal such structure, but also help to improve sensitivity and specificity of the predictions.
Algorithms for enhancer recognition can be roughly divided into two groups.
One group comprises probabilistic graphical models which describe the generative process of specific signals, such as Bayesian networks (BNs) [24] and hidden Markov models (HMMs) [25].
The other group employs son between DNA sequences in closely-related species.
TF binding and the other from ChIP experiments.
Epigenetic features can be Table 1 Features of computational methods for enhancer prediction Feature Method Ref Comparative genomic features Aparicios method [4] Visels method (2008) [30] Chens method [8] Yips method [50] Sequence-based TF binding related features Narlikars method [65] Chens method [8] Lees method [44] Yips method [50] Experiment-based TF binding related features Visels method (2009) [46] Zinzens method [67] Mays method [48] Chens method [8] Epigenetic features Heintzmans method [5] Wons method [11] Firpis method [10] SEGWAY [69] Kharchenkos method [60] Hes method [23] Ernsts method [61] ChromaGenSVM [9] Yips method [50] Chens method [8] Bonns method [6] Note:More than one type of features were employed to build enhancer recognition model in some studies.
For example, Chen et al.used all four types of features to develop active enhancer recognition model [8].
144 Genomics Proteomics Bioinformatics 11 (2013) 142150 discriminative filters and includes thresholds or classification boundaries in the features.
This group mainly includes support vector machines (SVMs) [26] and artificial neutral networks (ANNs) [27].
The features used in enhancer recognition Comparative genomic features Comparative genomic features comprise conservation scores calculated from multi-species genome sequence alignment.
With the completion of more vertebrate genome sequencing projects, many methods have been developed to discern slowly evolving genome regions.
For example, by comparing point substitutions, insertions and deletions between humans, mice and rats, Cooper et al.comprehensively annotated slowly evolving regions in the human genome [28].
Phastcon score, another example representing the evolutionary conservation of genomic regions [29], has been employed to predict putative enhancer location [8].
In early systematic recognition of potential enhancers in fugu [4], a pair-wise identity score of Hoxb-4 between mouse and fugu was used to detect conserved sequence blocks, followed by transgenic mouse assays to measure their enhancer activities.
Likewise, ultraconserved non-coding elements between humans, mice and rats were also found to be highly enriched in enhancer regions [30].
However, conservation per se is not sufficient to deduce enhancer activity in any given cell type.
Moreover, several enhancers with little conservation were found carrying identical regulatory patterns in different species [3133].
Therefore, additional information is required to predict enhancer activity in a given cell type.
Transcriptional factor binding related genetic features Transcriptional factor binding related genetic features can be roughly classified into two groups.
One group includes quantitative scores of similarity to a known TF binding motif, representing the TF binding affinity to the DNA segments (sequence-based TF binding related genetic features).
The other group includes experimental measurements of TF binding frequency, which also presumably represents TF binding affinity (experiment-based TF binding related genetic features).
The sequence-based TF binding related genetic features comprise individual TF binding and the enrichment of modular combinations of TF binding.
Measuring TF binding affinities is not an easy task experimentally; however, it can be approached from the nucleotide preferences at each sequence position [20], e.g., position weight matrix (PWM).
PWM describes the probability of observing the respective nucleotides A, C, G, and T in each position of a sequence motif.
It has been found that there is a strong correlation between PWM scores and the TF binding affinity [15,16,20].
PWMs for known TFs have been cataloged in databases [34,35].
These matrices enable people to assign a quantitative score to any sequence to evaluate the binding affinity of the specific TF at that sequence (Figure 1).
In vertebrates, functional TF binding sites are usually clustered into a modular structure, which motivates researchers to seek cis-regulatory modules (CRMs) as the advanced predictive features for cis-regulatory element recognition [36,37].
The CRM features are often calculated as the likelihood of the CRM in a given genome context [38].
For example, MSCAN value measures the statistical significance of the appearance of potential combinatorial TF binding sites [39].
All the TF binding sites are represented by PWM scores and MSCAN returns the significance of the CRM.
A similar strategy is adopted in MCAST [40].
To further improve the performance, additional phylogenetic footprinting is employed to align interested orthologous DNA sequences to define a conserved region and then the significance of the CRM is calculated in the regions.
For example, EEL approach was used to scan a given pair of orthologous sequences to identify conserved TF binding sites, and, then EEL scores were calculated by considering both distances and differences in the angles between adjacent binding sites [41].
Another example, MorphMS, implemented a pair-HMM statistical alignment between two species [42].
A first order Markov networkwith three states (match, deletion and insertion)was implemented and emits two strings, one for each species.
The string emitted in the match state was chosen by another probabilistic process, which models the arrangement of binding sites and non-binding (background) sites by PWM.
Then, two log likelihood ratio (LLR) scores were reported.
The two scores (LLR1 and LLR2) compare the likelihood of a sequence under the MORPH model to the likelihood of the sequences under null models.
The null model used in LLR1 only considers background PWM, while the null model for LLR2 assumes that the two orthologous sequences were generated independently.
Besides the similar strategy used in MorphMS, another algorithm EMMA incorporates gains and losses at binding Wang C et al/ Computational Methods for Enhancer Prediction 145 site, a process that is believed to be an important part of CRM evolution [43].
However, the computational cost increases exponentially with the number of TFs considered.
One alternate choice for this type of sequence features is k-mer profile, which is the frequency of all possible k-mer (putative motifs with length of k) in a given sequence region [44].
The profile measures how likely the k-mers in one enhancer would be found in another independently-generated sequence.
Using such k-mer features, Leung and Eisen developed a profile similarity between pairs of sequences to detect novel enhancers [45].
However, the search space is growing exponentially with k. The sequence-based TF binding related genetic features alone are not sufficient for active enhancer recognition.
First, most of the features are conserved TF binding sites, while many enhancer elements are not conserved.
For example, in Drosophila, the cone-specific Pax2 enhancer carries barelyconserved TF binding sites, which have been shown to possess similar enhancer functions in transgenic assays [31].
Similarly, a large proportion of a 40 kb region in the Phox2b locus showed regulatory activity by transgenic assay in zebrafish, while only 2961% identified regulatory sequences were conserved [32].
Second, in any given tissue, only a subset of enhancers is active.
This tissue-specific activity may result from a tissue-specific combination of binding TFs or from regulation at the epigenetic level.
TF binding in given tissues or cell types can be experimentally measured, which gives experiment-based TF binding related genetic features.
For example, data from chromatin immunoprecipitation followed by massively parallel DNA sequencing (ChIP-seq) technology precisely provide binding loci for the TFs under the given conditions [18].
Visel et al.mapped the genome-wide occupancy of p300 in three cell lines by ChIP-seq.
Using transgenic mouse assay, they show that p300 binding sites are predictive for enhancer activity in the cell types examined [46].
Similarly, CREBBP-bound enhancers also show environment-dependent activity in neurons [47], or in transgenic mouse enhancer assays [48].
Recently, ENCODE project has generated high-throughput sequencing (ChIP-seq or ChIP-chip) data sets for 119 distinct transcription factors over five main cell lines [49].
These experimental results have been used for enhancer recognition [50].
Epigenetic features Epigenetic features consist of chromatin structure, histone modifications, DNA-methylation levels and non-coding RNAs.
In this review, we mainly focus on the first two types of epigenetic features, since other features have been reviewed elsewhere (such as [51]).
Chromatin structure controls DNA accessibility of TFs to enhancer or other regulatory elements.
DNA accessibility can be inferred as DNase I hypersensitivity [52,53] or by Formaldehyde-Assisted Isolation of Regulatory Elements (FAIRE) technology [54].
The regions detected by DNase I or FAIRE are associated with all known classes of active DNA regulatory elements, including enhancers [55].
For example, Wiench et al.found that CpG methylation at glucocorticoid receptor (GR)-associated DNase I hypersensitive sites was a cell type-specific event and suggested that these sites could be a unique class of active enhancers [56].
Comparing DNase I-seq and FAIRE-seq data in seven human cell types indicated that data from these two assays were not fully overlapping [57].
DNase I tended to find the regions around transcriptional start sites, while FAIRE was more sensitive in detecting distal regulatory elements.
Notably, neither DNase I nor FAIRE hypersensitive sites detected in one cell type are sufficient to demonstrate their enhancer state, as many other regulatory element sites, such as repressors or insulators, are also DNase I or FAIRE hypersensitive [57].
Therefore, DNase I or FAIRE hypersensitivity data should be regarded as a necessary but not sufficient input for active enhancer prediction.
In addition to DNA accessibility, the presence of characteristic histone modifications is another sign for the activity of enhancers, e.g., elevated H3K4 monomethylation (H3K4me1) levels and depleted H3K4 trimethylation (H3K4me3) levels have been correlated with enhancer activity [5].
Further experiments showed that active enhancers marked by H3K4me1 in ES cells are also flanked by H3K27 acetylation (H3K27ac), while regions marked by H3K27 trimethylation (H3K27me3) are associated with early developmental genes which are poised in ES cells [58,59].
In another study, however, Bonn et al.found that H3K4me1 was distributed similarly between mesodermally active and inactive enhancers, indicating that the placement of H3K4me1 is not completely cell type specific during embryonic development [6].
Instead, they found a conditional link between the presence of H3K79me3, H3K27ac marks and enhancer activity.
Although the histone modification patterns mentioned above showed promising potential for enhancer activity prediction in certain cell types, the general pattern of histone modifications for prediction still remains elusive.
In human CD4+ T cells, 39 histone modification types have been mapped and several histone mark combinations showed correlation with enhancers, yet no single mark is associated with more than 40% of enhancers [7].
Integrating more epigenetic marks may render a more reliable, robust and precise model to capture active enhancers.
Several attempts have been made [5,9 11].
One such attempt employed 10-fold cross-validation for all possible combinations of six histone modification marks to predict p300 binding sites, and found that enrichment of H3K4me1 and depletion of H3K4me3 is the most predictive combination for p300 binding [5].
Many more sophisticated computational technologies have also been applied to search for optimal combinations for active enhancers.
For example, Won et al.coupled HMM with simulated annealing to search for the most informative combination of histone modification marks [11].
In Drosophila, Kharchenko and coworkers found that active enhancers lack H3K4me3 and are enriched for H3K4me1, H3K27ac and H3K18ac [60].
Similarly, ChromHMM labeled active enhancers with the H3K4me1, H3K4me2 and H3K27ac signature [61].
In a vast collection of epigenetic marks (20 histone methylations and 18 histone acetylations), genetic algorithms indicated that the most predictive histone modification signals within enhancers are H3K4me1 and H3K4me3 [9].
A similar pattern was also extracted from nearly 40 ENCODE histone modifications by using fisher discriminate analysis [10].
The features we discussed above can also be roughly classified into two classes, based on the prediction power for enhancer activity.
One class of features represents the potential of a locus to be an enhancer, e.g., comparative genomic features or sequence-based TF binding related genetic features, because Figure 2 Flow scheme of model building To improve model interpretability and reduce overfitting, sophisticated computational strategies implement feature selection algorithm to select a subset of relevant features for model building.
Then, appropriate classification model is employed to differentiate active enhancers from non-enhancers.
Generally, there are two major classification models.
The first is the discriminative models which find the optimal classification border in the feature space (lower left panel).
The other one is the probabilistic graphical models that try to model the joint distribution of states and associated features with graph (lower right panel).
ANN, artificial neutral network; BN, Bayesian network; HMM, hidden Markov model; SVM, support vector machine.
146 Genomics Proteomics Bioinformatics 11 (2013) 142150 the features describe the static DNA sequence characteristics which are shared by almost all cells in an organism.
The other class of features, e.g., experiment-based TF binding related genetic features or epigenetic features, further indicates enhancer activity of the loci in a given tissue or cell type.
These features are the actual measurement of cellular or molecular activities that had already been associated with enhancer activity in living cells.
For example, when Visel and colleagues compared the evolutionary conservation score and p300 binding sites, they found that only 47% (246 out of 528) of conserved enhancer candidates were active in a transgenic mouse assay, whereas 87.7% of p300 binding sites were reproducibly active in the same transgenic assay [46].
Another study employed chromatin signatures of H3K4me1, H3K4me3 and H3K27ac to recognize active enhancers in 19 mouse cell lines.
By comparing predicted enhancers with 726 experimentally validated enhancers, they found that 82% of predicted enhancers were correctly identified [62].
Androgen receptor binds primarily to active enhancers in human prostate cancer cells [63].
Interestingly, He et al.found that the H3K4me2 signal was detected in the known androgen receptor binding sites [23].
At present, although some features showed strong preference in the putative enhancer regions, and some other features showed association with enhancer activity, the relationship between features and enhancer activity is complicated, and sophistic models are still essential to achieve sensitive and specific active enhancer prediction.
Model building The general process of enhancer prediction is summarized in Figure 2, and the commonly used methods are listed in Table 2.
The simplest method to differentiate active enhancers from background is to look for the presence of characteristic features.
For example, p300 ChIP-seq data were used to determine p300-enriched regions, which were considered as putative active enhancers.
Of the 122 tested p300 binding elements in mouse, 107 (87.7%) showed reproducible enhancer activity [46].
Heintzman et al.exhaustively searched all combinations of six different histone modification marks, and identified the optimal combinations of H3K4me1 and H3K4me3 [5].
Despite the fine performance of this simple model, the best predictive power in one dataset does not guarantee its performance in another.
Moreover, an ever increasing number of features would challenge these simple methods.
This is not only because of the inter-correlations between the features, but also because of the difficulties in interpreting the relative importance of each feature.
A class of computational technology, named feature selection, has been applied to solve such problems [64].
For example, Narlikar et al.built a linear regression model to identify active enhancers in heart based on 727 sequence features including 721 TF binding related genetic features [65].
The LASSO linear regression method was then applied to find features relevant to enhancer activity and 45 Table 2 Model building strategies and performance of enhancer prediction methods Category Method Operational model Positive predictive value (%) N te Ref Discriminative model Heintzmans method Thresholds of histone modification profiles 39.5 M pped to distal p300 binding sites in HeLa cells [5] Visels method (2009) Thresholds of p300 binding profiles 87.7 W th reproducible enhancer activity in transgenic m use [46] Narlikars method Linear regression 62 W th reproducible enhancer activity in vivo in m use and zebrafish [65] Zinzens method Support vector machine 71.4 W th reproducible enhancer activity in transgenic D osophila [67] Firpis method Time-delay neural network 66.3 O erlapped with p300 binding sites, Dnase I h persensitivity sites or TRAP220 binding sites in H La cells [10] Lees method Support vector machine 74.5 O erlapped with Dnase I hypersensitive enhancers i embryonic mouse whole brain cells [44] ChromaGenSVM Support vector machine 57 O erlapped with p300 binding sites, Dnase I h persensitivity sites or TRAP220 binding sites in H La cells [9] Probabilistic graphical model Wons method Hidden Markov model 54.8 O erlapped with p300 binding sites, Dnase I h persensitivity sites or TRAP220 binding sites in H La cells [11] Bonns method Bayesian network 78 O erlapped with previously identified TF binding s s in Drosophila [6] Other Chens method Multinomial logistic 83 O erlapped with at least one TF peak from 7 m use embryonic stem cell ChIP-seq datasets [8] Yips method Random forest 67 W th enhancer activity in vivo in mouse and m daka fish (28/42) [50] Note: The performance shown here is the reported performance compared to experimental results.
The positive predictive value (percentage as calculated as follows: positive predictive value = true positive/(true positive + false positive).
W a n g C et a l/ C o m p u ta tio n a l M eth o d s fo r E n h a n cer P red ictio n 1 4 7 o a i o i o i r v y e v n v y e v y e v ite v o i e ) w 148 Genomics Proteomics Bioinformatics 11 (2013) 142150 features were assigned nonzero weights.
The accuracy of 92% was achieved in distinguishing heart enhancers from a large pool of random noncoding sequences.
Recently, more sophisticated methods have been implemented to find the optimal classification border in the feature space.
Typical methods include ANNs and SVMs.
A neural network is a parallel system, capable of resolving paradigms that linear computing cannot [27].
A case concerning enhancer recognition is a time-delay neural network (TDNN) which combines 39 histone modifications [10].
In an independent test, 66.3% of the putative regions identified by this model overlapped with experimentally supported enhancers [10].
A SVM performs classification by seeking a hyperplane in high dimensional labeled feature space that optimally separates the data into two categories regarding the classification labels [66].
A SVM model has been applied to ChIP-seq data of five different TFs and 77% of all known muscle-specific enhancers in Drosophila have been correctly predicted [67].
In addition, using ChromaGenSVM, which was based on five histone modification marks, 57.0% of identified potential enhancers overlapped with experimentally supported enhancers in the pilot ENCODE region in HeLa cells [9].
In fact, SVM models are closely related to ANNs.
SVMs are alternative training methods for multi-layer perception classifiers, in which the weight of the network is found by solving a quadratic programming problem with linear constrains, rather than by solving an unconstrained minimization problem in ANNs [26].
A comparison in HeLa cells between ChromaGenSVM and TDNN showed that ChromaGenSVM recovered 70.2% of the p300bound putative active enhancers, while TDNN achieved a precision of 84.0% [9].
However, due to different feature sets used by these two models, these data do not necessarily indicate that SVM is more effective than ANN for active enhancer recognition.
Another type of approaches try to model the joint distribution of states and associated features with graph, generally termed as probabilistic graphical models.
The nave Bayes classifier (NBc) is the simplest one of this type [68].
For enhancer recognition, NBc learns the conditional probability of each feature related to enhancer activity from a training data.
For example, a NBc on 6-mer features has been trained to detect active enhancers in the mouse genome [44].
However, compared with a SVM model with the same feature set [area under receiver operating characteristic curve (AUC) > 0.9], the NBc preformed significantly less accurately in discriminating active enhancers from random sequences (AUC < 0.79).
HMM is another example in probabilistic graphical models.
The current model of the genome is a linear combination of stated DNA sequences, e.g., promoter, enhancer or coding region.
By assuming that the state of any locus is only dependent on its nearest neighbor, HMM provides a natural solution for the task of segmenting the stated DNA sequences [25].
For example, Kharchenko and coworkers used a HMM to identify the prevalent combinatorial pattern of 18 histone modifications and captured the overall complexity of chromatin profiles observed in Drosophila S2 and BG3 cells with 9 states [60].
They found that enhancer regions are always enriched with H3K4me1, H3K27ac and H3K18ac.
A similar strategy was implemented in ChromHMM, which mapped 15 chromatin states in nine human cell lines [61].
BN represents another probabilistic graphical model that allows effective representation of the joint probability distribution over feature set [24].
BN provides a powerful framework for modeling the complicated hidden relationships that explain the observed chromatin patterns in a genome.
For instance, SEGWAY used BN techniques to simultaneously segment and cluster 1% of the human genome with 31 ENCODE signal tracks including histone modifications, TF binding and open chromatin, and revealed active enhancer associated patterns at nucleosomal resolution [69].
BN has also been applied to predict active enhancers in Drosophila [6], and the trained BN identified a conditional link between the H3K79me3 and H3K27ac marks and enhancer activity.
This BN model achieved better performance (AUC = 0.82), compared to the aforementioned NBc model.
Conclusion and outlook Enhancers are regulatory DNA elements that can activate transcription largely independent of their location or orientation.
Often, enhancers regulate gene expression in a tissue-specific manner and play important roles in cell differentiation [17].
In this review, we have described the general computational strategies for enhancer prediction.
It has been suggested that H3K4me1 and p300 binding signatures are the most predictive features for active enhancer recognition [5,46], however, this notion may be disputed by new data.
For example, a recent study found that H3K79me3 and H3K27ac, instead of H3K4me1, are predictive for cell type specific enhancer activity during embryonic development [6].
Recently, a more complicated picture, which involves nuclear organization, chromatin structure and non-coding RNAs, is emerging for enhancer activation.
Accumulating data suggested that the insulators are critical in the regulation of enhancerpromoter interaction which is believed to be accomplished by long-range inter-or intra-chromosomal chromatin interactions [70].
From the perspective of computational biology, the field of enhancer research is now moving toward the modeling of 3D chromatin structure in nuclei, to reveal the principle of enhancer-promoter interactions.
Polymer models are valuable tools in 3D chromatin structure study, e.g., the dynamic random loop model [71] and the fractal globular model [72].
To understand enhancers in the context of gene regulatory networks, it is necessary to integrate data from ultra-heterogeneous data sources in this big data era.
For example, enhancer transcribed RNAs (eRNAs) were recently found prevalent at enhancer loci [47].
Some of such non-coding RNAs even act like enhancers [73].
Therefore, the integration of RNA-seq data is essential for a model which aims to understand eRNA associated enhancer activity.
Competing interests The authors have declared that no competing interests exist.
Acknowledgements We apologize to many authors whose important works could not be cited owing to space limitations or our ignorance.
This work was supported by grants from the National Natural Science Foundation of China (NSFC, Grant No.
31271398 and Wang C et al/ Computational Methods for Enhancer Prediction 149 91131012) and 100 Talents Project to ZZ, NSFC (Grant No.
91019016) and National Basic Research Program of China (NBRPC, Grant No.
2012CB316503) to MQZ.
Abstract The vast majority of multi-exon genes in higher eukaryotes are alternatively spliced and changes in alternative splicing (AS) can impact gene function or cause disease.
High-throughput RNA sequencing (RNA-seq) has become a powerful technology for transcriptome-wide analysis of AS, but RT-PCR still remains the gold-standard approach for quantifying and validating exon splicing levels.
We have developed PrimerSeq, a user-friendly software for systematic design and visualization of RT-PCR primers using RNA-seq data.
PrimerSeq incorporates user-provided transcriptome profiles (i.e., RNA-seq data) in the design process, and is particularly useful for largescale quantitative analysis of AS events discovered from RNA-seq experiments.
PrimerSeq features a graphical user interface (GUI) that displays the RNA-seq data juxtaposed with the expected RT-PCR results.
To enable primer design and visualization on user-provided RNA-seq data and transcript annotations, we have developed PrimerSeq as a stand-alone software that runs on local computers.
PrimerSeq is freely available for Windows and Mac OS X along with source code at http://primerseq.sourceforge.net/.
With the growing popularity of RNA-seq for transcriptome studies, we expect PrimerSeq to help bridge the gap between high-throughput RNA-seq discovery of AS events and molecular analysis of candidate events by RT-PCR.
Introduction Alternative splicing (AS) is a prevalent mechanism of gene regulation in higher eukaryotes [1].
AS plays an important role in both normal biological processes [2] and disease [3].
In recent years, high-throughput RNA sequencing (RNA-seq) has become a powerful and popular technology for global analysis of AS [4].
From the massive amount of RNA-seq reads, one can discover novel splicing events, quantify the usage level of hosting Figure 1 The flow diagram of PrimerSeq PrimerSeq flow diagram designates inputs as blue, computations as green and decisions as orange.
If flanking exons are specified by the user, PrimerSeq will immediately design primers.
If not specified, PrimerSeq will first identify the alternative splicing module (ASM) containing the target exon and then iteratively search for the closest flanking exons above a user-defined PSI (w) cutoff.
Results are subsequently visualized through plotting the RNA-seq data juxtaposed with the expected RT-PCR results, which include estimated w values for the target exon.
Visualizing read density, an optional feature, requires a BigWig file.
PSI stands for percent-spliced-in.
106 Genomics Proteomics Bioinformatics 12 (2014) 105109 alternatively spliced exons in any RNA sample of interest, and identify genome-wide changes in AS between different biological states.
However, RT-PCR is still regarded as the most reliable and standard approach to quantify and validate exon splicing levels [5].
In fact, researchers customarily perform RT-PCR validation of AS events discovered from RNA-seq data prior to downstream functional studies.
A widely used measure of AS is the percent-spliced-in (PSI, or w) metric, which measures the percent inclusion level of an alternatively spliced exon (or splice site) in the final mRNA products [4].
In an RNA-seq project, the PSI value of an AS event can be first estimated from RNA-seq data using software like mixture-of-isoforms (MISO) [4] or multivariate analysis of transcript splicing (MATS) [6] and then validated independently by RT-PCR.
However, the design of RT-PCR primers for AS analysis is typically a time-consuming step that requires tedious manual operations.
Software that allow input of a template sequence for primer design like Primer3 [7] and associated interfaces Primer3Web, Primer3Plus and BatchPrimer3 [8] can theoretically design primers for any AS event of interest.
However, it is left to the user to manually extract sequences for primer design, which is time-consuming and error-prone.
This is particularly challenging for RNA-seq projects, where researchers may need to validate tens or even hundreds of AS events identified from the transcriptome-wide AS analysis (for example, see [9,10]).
Consequently validation of AS from big RNA-seq data has become a major bottleneck between high-throughput discovery of AS events and in-depth analysis of molecular function and regulation.
It should also be noted that the repertoire of expressed genes and mRNA isoforms is dynamically regulated, while current tools and databases for RT-PCR primer design use static (pre-defined) gene and transcript annotations and do not incorporate transcriptome profiles for the specific sample(s) of interest.
Primer databases like GETPrime [11], RTPrimerDB [12], PrimerBank [13] and qPrimerDepot [14] only contain primers for a restricted set of species and are built on pre-defined gene annotations.
Most primer design tools (e.g., PerlPrimer [15] and QuantPrime [16]) or primer databases (mentioned above) focus on gene expression and occasionally transcript expression rather than AS events.
In general, there is a lack of primer design tools or databases for AS analysis, with a few exceptions being GETPrime (gene and transcript specific) [11] and RASE (alternative splicing) [17].
RASE is the method most specifically designed for AS analysis, but its web interface only works with human genes and requires time-consuming manual input of sequences.
It should also be noted that RNA-seq is a flexible technology, which can be applied to any organism of interest.
In fact, researchers have used RNA-seq to study AS in a wide variety of organisms such as honey bee [18], silkworm [19], Plasmodium falciparum [20] and frog [21].
Additionally, computational tools such as Cufflinks [22] and Scripture [23] can be used to construct transcript annotations de novo from RNAseq data aligned to the genome.
Therefore, an ideal primer design tool for AS analysis should have the flexibility to work with user-provided RNA-seq data on a diverse range of organisms, instead of being restricted to a small set of species and predefined transcript annotations.
Here we present PrimerSeq, a user-friendly stand-alone software for systematic design and visualization of RT-PCR primers for AS analysis.
PrimerSeq has a graphical user interface (GUI) and one-click type installation for convenient access by a wide range of users.
It utilizes user-provided RNA-seq data to define splicing patterns, estimate exon inclusion levels (PSI, or w), select suitable regions for placement of RT-PCR primers and visualize RNA-seq data along with highlighting expected RT-PCR results.
Users can conveniently compare the graphical output of PrimerSeq to their RT-PCR experimental result.
Methods PrimerSeq workflow and algorithm PrimerSeq designs RT-PCR primers for AS analysis.
The design process can incorporate the transcriptome profiles of the samples of interest through user-provided RNA-seq data files, or only utilize pre-defined gene and transcript annotations.
As shown in the flow diagram (Figure 1), the input to PrimerSeq includes a genome sequence file (FASTA), a gene and transcript annotation file (GTF), mapped RNA-seq reads (BAM, recommended but optional) and a list of exon coordinates representing the events of interest.
Visualizing read density also requires a BigWig file, although this visualization step is optional.
For each AS event, PrimerSeq attempts to place a pair of forward and reverse PCR primers on suitable flanking exons.
Such flanking exons can be specified by users in the input.
Alternatively, PrimerSeq can automatically choose Tokheim C et al/ PrimerSeq: RT-PCR Primer Design Using RNA-seq Data 107 appropriate flanking exons by finding the nearest suitable flanking exons whose inclusion levels (PSI) are above a userdefined threshold (95% by default), a procedure that typically finds constitutive exons.
PrimerSeq then runs Primer3 [7] to perform primer design on the selected flanking exons.
Through configuration options, users can fully customize the parameters for primer design, such as the size range of the PCR products.
As part of the primer design procedure, PrimerSeq utilizes the biconnected components algorithm [24] as a generalized definition of AS events called alternative splicing modules (ASMs).
Conceptually, if we consider the transcript structure of an alternatively spliced gene as a directed acyclic graph (i.e., splice graph [25,26]), each ASM represents a subset of the splice graph, where the transcripts diverge from a single point and then converge back to a single point.
In the case of a simple exon skipping (ES) event, the ASM includes two splice forms corresponding to the exon inclusion and skipping isoforms.
To understand this, it is clear for an ES event that an upstream constitutive exon is used by both isoforms before the skipped exon of interest.
Similarly, a downstream constitutive exon is also utilized for both the skipping and inclusion isoforms.
The two isoforms differ by whether the middle exon is utilized (included) or skipped.
Therefore a simple ES event fits our definition of an ASM, because an ES event has two paths that start from a common origin of the upstream constitutive exon and reconvene at the downstream constitutive exon.
However, an ASM could contain more than two splice forms if multiple alternative splicing events are coupled.
Using this generalized definition of AS, PrimerSeq can design primers for all types of AS events, such as exon skipping, alternative splice sites and mutually exclusive exons.
For a more technically detailed description, please see the technical manual at http://primerseq.sourceforge.net/technical_manual.pdf.
Ideally, primers should be placed on constitutive exons flanking an ASM, so the RT-PCR analysis can obtain the PSI measurements for all splice forms within the ASM.
If users do not specify flanking exons, PrimerSeq uses RNA-seq read counts to estimate relative isoform abundance using an expectation maximization (EM) algorithm [27] (also see the technical manual of PrimerSeq at http://primerseq.sourceforge.net/technical_manual.pdf), then places primers on flanking exons with constitutive splicing, or at minimum exons with high inclusion levels (PSI > 95% by default).
Regardless of whether flanking exons are specified by the user or selected by PrimerSeq, the abundance estimates from the EM algorithm are used to calculate PSI estimates for the target AS exon.
Such estimates will be used for predicting the relative ratios of the RT-PCR products in the subsequent visualization step.
Novel isoforms supported by RNA-seq reads can optionally be considered in the design process.
If this option is enabled (disabled by default), our current algorithm adds novel exon-exon junctions detected from the RNA-seq data to the supplied transcript annotations.
For a more refined control of novel isoforms, or for organisms with poor transcript annotations, users are suggested to perform novel transcript assembly using tools like Cufflinks [22] and then load the resulting gene and transcript annotations (GTF) into PrimerSeq.
Following primer design, PrimerSeq visualizes RNA-seq data along the expected RT-PCR results.
Specifically, PrimerSeq can automatically generate figures that display the expected sizes and relative ratios of the RT-PCR products, together with the RNA-seq read density profile along the transcripts.
Displaying the read density plot requires a BigWig file.
The entire AS module will be displayed, if flanking exons are selected by PrimerSeq, i.e., not specified by the user.
PrimerSeq can display figures within the GUI and save the results as a static web page (HTML).
Additionally, PrimerSeq provides links to UCSC In-Silico PCR [28] for users to inspect potential offtarget amplifications.
Implementation and availability PrimerSeq is mainly written in Python using the wxPython library (http://wxpython.org/) to create a GUI.
The identification of AS events using the biconnected components algorithm was performed using the NetworkX library [29] in Python.
The Java libraries SAM-JDK v1.77 (http://picard.sourceforge.net/) and BigWig API r39 (revision 39, https://code.google.com/p/ bigwig/) were used to enhance the performance of handling RNA-seq data and read density files, respectively.
PrimerSeq uses standard file formats for gene and transcript annotations (GTF), RNA-seq data (SAM/BAM), genome sequence (FASTA) and read density (BigWig).
BAM, FASTA and BigWig files are indexed, which provides significant speed improvements for handling large datasets.
Primer3 v2.3.4 [7] is used to perform primer design after the appropriate exonic sequences are retrieved from the FASTA file.
The stand-alone PrimerSeq software is free and open to all users and there is no login requirement to download the software.
PrimerSeq is available as a Windows installer and a Mac OS X binary on SourceForge at http://primerseq.
sourceforge.net/.
Source code for PrimerSeq is hosted on GitHub at https://github.com/ctokheim/PrimerSeq.
The technical manual of PrimerSeq which includes a detailed description of nomenclature and algorithms can be found at http://primerseq.sourceforge.net/technical_manual.pdf.
User tutorials can also be found on the PrimerSeq website at http://primerseq.sf.net/getting_started.html and http://primerseq.sf.net/user_tutorial.html.
RT-PCR validation of PrimerSeq design Total RNA samples from human heart and testes were purchased from Applied Biosystems (Foster City, CA, USA) and Clontech (Mountain View, CA, USA), respectively.
RTPCR was carried out and 5% TBE-PAGE gel was used for resolving PCR products as described before [30].
Results As an example, we compared the splicing profiles of human heart and testes using RNA-seq data from the Illumina Human Body Map 2.0 Project (NCBI GEO Accession No.
GSE30611).
From the top 100 differential AS events detected by MATS (version 3.0.6.beta) [6], five were chosen at random for primer design by PrimerSeq and the efficacy of the primers was evaluated by RT-PCR experiments (see Table S1 for details regarding the RT-PCR primers).
All five AS events had successful primer design as evidenced by target amplification during RT-PCR.
Figure 2 illustrates the AS event in the gene TJP1 encoding tight junction protein 1.
The PSI estimates matched TJP1-heart R N Ase q re ad c ou nt s B A TJP1 Chr15:30,011,98030,012,220 He art Te ste s 78 52 (%)   532 399 266 133 Chr15: 30,010,469                        30,012,912 Primers 54.5% 126 bp 45.5% 366 bp Chr15: 30,010,469                        30,012,912 442 331 221 110 82.5% 366 bp 17.5% 126 bp Primers TJP1-testes R N Ase q re ad c ou nt s Figure 2 Example result from PrimerSeq Example automatically generated figures (A) for a selected AS event in the TJP1 gene and corresponding RT-PCR results (B) with the w values displayed.
The RNA-seq data are from heart and testes in the Illumina Human Body Map 2.0 project [Gene Expression Omnibus (GEO) Accession No.
GSE30611].
TJP1, tight junction protein 1; Chr, chromosome.
108 Genomics Proteomics Bioinformatics 12 (2014) 105109 well between RNA-seq (Figure 2A) and RT-PCR (Figure 2B).
In another gene, HNRPLL encoding heterogeneous nuclear ribonucleoprotein L-like, the RNA-seq data suggested a novel isoform and this was confirmed by RT-PCR (Figure S1).
In all five events, the RT-PCR primers successfully amplified the target regions and the sizes of PCR products were consistent with the design results from PrimerSeq (Figure 1 and Figures S14).
Discussion To the best of our knowledge, PrimerSeq is the only software that incorporates RNA-seq data in the design and visualization of RT-PCR primers for AS analysis.
This has several advantages.
By incorporating RNA-seq data, we ensure that the primers will be placed on flanking exons with constitutive splicing (or at minimum, high inclusion levels of close to 100%) in the sample(s) of interest.
Second, we produce figures based on the RNA-seq data to illustrate the expected results of the RT-PCR experiments (Figure 2), so that researchers can easily compare the RT-PCR results to RNA-seq predictions.
In addition, novel isoforms not present in the transcript annotations can be identified and visualized before performing RT-PCR (Figure S1).
We have chosen to develop PrimerSeq as a stand-alone software that runs on local computers, as opposed to a web-based tool.
This is important given the typical size of RNA-seq data files and the goal of working with a diverse range of organisms.
For example, the RNA-seq BAM files of human heart and testes in the Illumina Body Map 2.0 dataset [Gene Expression Omnibus (GEO) Accession No.
GSE30611] are 3.5 GB and 2.7 GB, respectively.
Due to computational costs and network speeds, it is impractical to process and manipulate such big RNA-seq data files through typical web-based tools.
By implementing PrimerSeq as a stand-alone software, we have the flexibility to utilize any user-provided RNA-seq data and transcript annotations.
For organisms with poor transcript annotations, we suggest researchers to use de novo RNA-seq transcript assembly tools, such as Cufflinks [22] and Scripture [23], to generate transcript annotations from their RNA-seq data, which can then be loaded into PrimerSeq for primer design.
With the growing popularity of RNA-seq for transcriptome studies, we expect PrimerSeq to help bridge the gap between high-throughput RNA-seq discovery of AS events and molecular validation of candidate events by RT-PCR.
Authors contributions YX and CT conceived the idea of PrimerSeq.
CT implemented PrimerSeq as a software package.
JWP helped with implementation of RNA-seq data handling and alternative splicing analysis.
CT and YX wrote the manuscript.
All authors had final approval of the manuscript.
Competing interests The authors declare no competing financial interests.
Acknowledgements This work was supported by the National Institutes of Health of USA (Grant No.
R01GM088342) awarded to YX.
We thank Zhixiang Lu for testing PrimerSeq and performing RT-PCR experiments.
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.2014.
04.001.
ABSTRACT Motivation: We noted that the sumoylation site in C/EBP homologues is conserved beyond the canonical consensus sequence for sumoylation.
Therefore, we investigated whether this pattern might define a more general protein motif.
Results: We undertook a survey of the human proteome using a regular expression based on the C/EBP motif.
This revealed significant enrichment of the motif using different Gene Ontology terms (e.g.transcription) that pertain to the nucleus.
When considering requirements for the motif to be functional (evolutionary conservation, structural accessibility of the motif and proper cell localization of the protein), more than 130 human proteins were retrieved from the UniProt/Swiss-Prot database.
These candidates were particularly enriched in transcription factors, including FOS, JUN, Hif-1, MLL2 and members of the KLF, MAF and NFATC families; chromatin modifiers like CHD-8, HDAC4 and DNA Top1; and the transcriptional regulatory kinases HIPK1 and HIPK2.
The KEPE motif appears to be restricted to the metazoan lineage and has three length variantsshort, medium and longwhich do not appear to interchange.
Contact: toby.gibson@embl.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Members of the ubiquitin multiprotein family function as covalent modifiers of other proteins.
These post-translational modifications (PTMs) then cause the target protein to be relocated to another subcellular location (Dye and Schulman, 2007).
In the case of SUMO (the small ubiquitin-like modifier), attachment can affect processes including gene transcription and cell-cycle progression, although the mechanisms by which relocation within the nucleus achieves this are far from clear (Geiss-Friedlander and Melchior, 2007).
While sumoylation seems largely to be restricted to the nucleus, a few non-nuclear proteins have been proposed to be sumoylated (Watts, 2004).
SUMO substrates are often difficult to To whom correspondence should be addressed.
validate due to the low stoichiometry of the SUMO modification: however, proteomic approaches have lead to the identification of many putative substrates (reviewed in Rosas-Acosta et al., 2005).
Like other PTMs, sumoylation occurs at accessible linear motifs (LMs), usually in regions of natively disordered polypeptide (reviewed in Diella et al., 2008).
Sumoylation occurs on a lysine in a motif that can be described by the pattern K.E where = hydrophobic (Girdwood et al., 2004; Rodriguez et al., 2001) or the regular expression [VILMAFP]K.E as used in the ELM linear motif resource (Puntervoll et al., 2003).
Sumoylated proteins that do not have the classical consensus motif have also been reported (Zhou et al., 2006).
The K.E pattern matches nearly half of the proteins in Swiss-Prot (Yang et al., 2006), indicating that most of the matches are false positive.
As a consequence, there have been several attempts to try to extend this motif to get more specificity, resulting in the identification of different extended SUMO consensus motifs.
Thus, the phosphorylation-dependent sumoylation motif (PDSM) K.E..SP has been described in a subset of substrates, mainly transcriptional regulators: the phosphorylation of the SP motif regulates the interaction between the substrates and the SUMO-conjugating machinery, promoting sumoylation of the substrates (Hietakangas et al., 2006).
In a second analysis, a cluster of acidic residues downstream from the core of many SUMO sites has been shown to be important for substrate binding and subsequent sumoylation (Yang et al., 2006).
The importance of negative charges was also identified in substrates like Elk-1 and LRH-1; this extended SUMO consensus motif was named NDSM, negatively charged amino acid-dependent sumoylation motif.
The C/EBP transcription factors regulate cellular proliferation and differentiation of a range of cell types.
They have been described as both tumour promoters and tumour suppressors, indicating that their regulatory system is complex (Nerlov, 2008).
In C/EBP, a regulatory domain motif (RDM) has been shown to inhibit the activity of an activation domain in a position-independent, but dosedependent manner.
The RDM was characterized by the consensus [VIL]K.EP and it was shown that sumoylation of lysine at position 2 decreases its inhibitory function in vitro (Kim et al., 2002; Nerlov, 2008).
A major hindrance to bioinformatic investigation of LM occurrences is that simple database searches do not yield significant 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
F.Diella et al.results, while the false instances of motifs vastly outnumber the true ones.
However, with improved sequence database annotation, LMs can sometimes be significantly enriched with certain keywords.
Thus, Copley used transcriptional keywords to detect and justify new examples of the EH1 transcriptional repressor motif (Copley, 2005).
A similar approach in combination with disorder prediction and conservation scoring, has shown that KEN-box destruction motifs are significantly enriched in the set of UniProt/Swiss-Prot entries annotated with cell-cycle keywords and Gene Ontology (GO) terms (Michael et al., 2008).
In this article, we report a computational investigation based on the RDM of C/EBPs.
We refined the motif in the aligned C/EBP RDM sequence segments and then deployed a protocol involving keyword enrichment, native disorder prediction and conservation scoring in a survey of protein sequence databases.
Highly significant results for motif matches were obtained with sets of entries annotated with keywords, such as nucleus, transcription and chromatin.
The conservation pattern of the C/EBP motif was found to be the archetype of a linear motif, which we term KEPE, which is present in many nuclear proteins of the metazoa.
2 METHODS 2.1 Motif search The interactive motif search tool SIRW combines regular expression searches with keyword searches of text annotation (Ramu, 2003).
SIRW (sirw.embl.de/) was used to explore the human UniProt/Swiss-Prot database (release 54.7; 18054 entries for Homo sapiens) with the C/EBP-derived KEPE regular expression, [MILVFT]K.EP.{1,4}[DE].
To limit the search space, relevant annotation terms, such as nucleus, transcription and chromatin were used to search the fields for the GO terms [the Gene Ontology annotation (Harris et al., 2004; The_Gene_Ontology_Consortium, 2008)] as well as the separate keyword (KW) fields for keywords.
(Note that the GO and KW searches are not formally equivalent because the GO terms include longer phrases in the definitions, while the annotators are also likely to have used different guidelines.)
In order to assess the significance of the relative enrichment, we calculated P-values using Fishers exact test (available in Excel and the R package).
2.2 Motif permutation controls It is important to exclude artefactual or trivial reasons for motif enrichment, such as a bias in favour of the amino acids K, E and P. Therefore, in order to control for the background frequency, we permuted the three fully conserved residues in the KEPE motifs (KPEE; EKPE; EPKE; PEKE; PKEE) as well as a specific permutation (KEEP) for the two residues P and [DE] that extend the SUMO motif.
Then we examined them for keyword association and conservation score (CS) as for the KEPE.
Results for the controls are presented in detail in the Supplementary Material.
2.3 Modular protein architecture context Motif matches were evaluated for presence in known globular domains using the Pfam and SMART domain databases (Letunic et al., 2006; Sammut et al., 2008).
IUPred (http://iupred.enzim.hu/) (Dosztanyi et al., 2005) was used to test whether the motifs were found in predicted globular or natively disordered regions (also known as IUP, intrinsically unstructured polypeptide).
Using the IUPred long parameter setting to predict longer stretches of disorder, flanking regions of 15 amino acids upstream and downstream of the motif were scored, applying a value of 0.4 as the cut-off threshold.
2.4 Evolutionary conservation Each match of the KEPE and the permuted motifs in Swiss-Prot proteins was also scored for conservation in homologous proteins using the CS method described in Chica et al.(2008).
This approach has already been applied for the KEN box motif (Michael et al., 2008).
The dataset used for calculating the CS included proteins (i) that are annotated to be in the nuclear or cytoplasmic compartment and (ii) whose motif match is found in a disordered/unstructured region according to the IUPred prediction.
To compare the CS distribution between KEPE and the permutation controls, we used the KolmogorovSmirnov (KS) goodness of fit test.
2.5 Proteome analysis We wrote a script to analyse the frequency of the motif and its permutations in human and yeast proteomes.
We downloaded all proteins having associated GO terms from the EnsEMBL resource for H.sapiens (Hubbard et al., 2007) and from the SGD resource (Hong et al., 2008) for Saccharomyces cerevisiae and we obtained two datasets of 16 504 and 5327 proteins, respectively.
Subsequently, we ran IUPred using the longparameter as before (Dosztanyi et al., 2005).
The ELM conservation filter (Chica et al., 2008) was then applied to assess the conservation of the matches.
3 RESULTS 3.1 Survey of Swiss-Prot with the KEPE regular expression Using an alignment of Drosophila C/EBP and the four vertebrate paralogues C/EBP,-,-and-(Fig.1, Supplementary Fig.1) we noted that downstream of the RDM motif [VIL]K.EP there are some additional conserved acidic residues, especially in positions 3 and 4 after the proline.
Earlier studies have partially described the motif that matches the observed sequence conservation (Kim et al., 2002).
Fig.1.
The KEPE motif in C/EBP transcription factors.
(A) The IUPred plot predicts human C/EBP to be almost entirely natively disordered (the higher the peak, the more disordered).
Like the KEPE motif, the leucine zipper (BRLZ) is also predicted as natively disordered (correctly so, since it must dimerise as coiled coil to acquire a stable folded structure).
(B) C/EBP KEPE motif: an alignment of the RDM motif from Drosophila C/EBP and four vertebrate paralogues C/EBP,-,-and-(CEBPA, CEBPB, CEBPD and CEBPE) show the conservation of the motif (K is the sumoylated residue).
2 We term the motif KEPE after the conserved residues.
The motif is not in a known globular domain but rather in a region predicted to be natively disordered (Fig.1A).
To investigate if the observed motif could be present in other proteins, we undertook a survey for the KEPE-bearing proteins in the human entries of the UniProt/SwissProt database (The_UniProt_Consortium, 2008) using the motif [MLIVFT]K.EP.{1,4}[DE].
We evaluated whether the matching proteins were found in the nuclear compartment or more widely.
Since most LMs are known to be in natively disordered polypeptide segments (Fuxreiter et al., 2007), the KEPE matches were also evaluated for a clash with known globular domains using the SMART server (Letunic et al., 2006) or in predicted globular structure reported by IUPred (Dosztanyi et al., 2005).
Of 331 human proteins matching the KEPE regular expression, 168 were annotated as localized in the nuclear compartment.
Of those, more then 130 had KEPE matches localized in non-globular regions according to the IUPred prediction.
These sequences were enriched in the functional classes transcription factor or chromatin modifierand in the GO class protein function related to transcription.
In only three cases was the KEPE motif found within a known globular domain according to SMART prediction.
In these three paralogous bromodomain and PHD-finger containing proteins BRD1, BRF1 and BRF3 (Swiss-Prot:O95696, P55201, Q9ULD4), the KEPE motifs fell within PHD-finger domains.
Since these KEPE motifs were found in the most variable loop of the PHD-finger (where an insertion of 15 residues or more is often found, SMART: SM00249), they could be potentially accessible for interaction.
Results of combined motifkeyword searches with SIRW are summarized in Table 1: Several terms show enrichments that Table 1.
Enrichment of KEPE motif matches with various term combinations from the KW and/or GO term fields in Swiss-Prot entries Keywordsa,b No.
Total Total P-value P-value KEPEc Hd Sd He Se Homo sapiens 331 18 054 7707  GO = cytoplasm*!nucleus 35 2303 1146 2.45E01 2.67E02 or KW = cytoplasm*!nucleus* GO = nuclear*|nucleus| 168 3608 1789 2.10E36 9.41E29 nucleolus!cytoplasm* or KW = nuclear*!cytoplasm* |nucleus!cytoplasm* Link = znf* 79 1577 799 6.82E17 4.01E13 Link = bzip* 17 9 35 7.58E18 8.62E15 GO = transcription* 90 1239 627 7.43E31 2.36E26 GO = chromatin* or 21 213 139 4.12E10 4.08E07 KW = chromatin* GO:0003700 36 506 264 3.55E12 4.65E10 (transcription factor activity) GO:0008270 9 164 74 3.32E03 4.20E03 (zinc ion binding) GO:0006355 17 253 131 4.60E06 4.11E05 (Regulation of transcription, DNA-dependent) aSearch terms which have been used to retrieve the KEPE sequences.
bIn Swiss-Prot the annotation cytoplasm is used (incorrectly) as a synonym for cytosol.
cNumber of sequences matching the KEPE pattern [MLIVTF]K.EP.
{1,4}[DE] used in combination with the various search terms.
dTotal number of sequences (as obtained with the SIRW search tool, in Swiss-Prot release 54.7) matching the search terms shown in the left column.
(H): all human sequences; (S): human sequences with the SUMO motif [VILMTF]K.E.
eThe P-value for the relative enrichment was calculated by the Fishers exact test from the R package (for total H and total S).
are highly significant according to the Fishers exact test.
The enrichment was particularly significant with the transcription, bzip and znf keywords as well as for nuclear compartment.
There is a possibility that this enrichment could be driven by the high background frequency of the embedded SUMO motif.
In order to test this possibility, we calculated the enrichment using human sequences matching the SUMO motif as the background distribution.
The P-values S in the right column show that the enrichment is still significant.
Significant enrichment is not, per se, proof of function and could be for a trivial reason, such as strong amino acid bias or 4-fold increased mean protein length in transcriptional proteins.
This can be controlled for by using test motifs that contain the same amino acids and information complexity, which can be obtained by permuting residues in the motif.
When we performed the same analysis using permuted motifs, we found moderate enrichment for some keywords (see Supplementary Tables 1 and 2) but KEPE enrichment is always greater.
Genuine LMs that function in cell regulation are found to be conserved in homologous proteins (Neduva and Russell, 2005).
Therefore, we applied the ELM CS pipeline (Chica et al., 2008) to assess KEPE motif conservation.
Figure 2 compares the distributions of CS values for matches to KEPE and its permuted motifs; the comparison was repeated for nuclear and cytoplasmic proteins.
In the nuclear set, the KEPE motif shows much stronger conservation than the permuted motifs.
Furthermore the CS distributions of all permuted instances are significantly different to the KEPE distribution with P-values ranging from 0.00 to 0.01 (Fig.2A).
This result strongly supports a predicted function for KEPE in a nuclear role.
We were worried that matches in multiprotein families might have skewed the results in favour of the KEPE motif.
Therefore, the set of sequences matching the KEPE and the permuted motifs were checked for the number of paralogous assignments retrieved using EnsEMBL mappings (Hubbard et al., 2007).
Most of the matches are in proteins with one or no paralogues (Supplementary Fig.2).
This result shows that the higher frequency observed for the KEPE matches in the maximum CS range is not artificially caused by a higher number of paralogues in the corresponding protein families.
This implies that the number of KEPE matches appearing in paralogues of the same protein reflects their functional value and not a tendency of those protein families to have more paralogues.
The non-significant differences obtained for KEPE versus the motif permutations for proteins annotated as cytoplasmic serve as a negative control (Fig.2C).
Indeed, here the KEPE instances are as non-conserved as the permuted ones, consistent with a lack of functionality in the cytosol.
3.2 Surveys of human and yeast proteomes with the KEPE regular expression Although Swiss-Prot GO terms are also mapped to the EnsEMBL human proteome via the GOA database (Camon et al., 2004), EnsEMBL provides additional electronically generated GO annotation.
The EnsEMBL human proteome is also more complete than in Swiss-Prot.
Since the Swiss-Prot searches were interactive, we wanted to evaluate whether a fully automated proteome pipeline could produce qualitatively similar results.
As shown in Supplementary Fig.3, an equivalent GO termIUPredCS 3 F.Diella et al.assessment protocol yields an even stronger nuclear conservation plot than for the interactive Swiss-Prot survey (Fig.2A).
An automated pipeline allows the component stages to be evaluated separately.
The keyword and the IUPred assignment steps each Fig.2.
Conservation score distributions for the KEPE motif and the six permutations comparing the nuclear and cytoplasmic compartments.
KEPE bearing proteins were retrieved from UniProt/Swiss-Prot with the compartment keyword expressions in Table 1, processed for IUPred disorder prediction and evaluated for the ELM CS score.
To enable comparison, the sets have been normalized into percentages and sorted into five CS score bins.
The table in (A) shows that the KEPE matches have a significantly different conservation distribution in the nucleus compared with the controls.
n = number of instances, D = maximum difference between the cumulative distributions, P-value = significance of the difference D, according to the KS test.
KEPE matches also show a peak of strong conservation (unmatched by the controls) in the nucleus (B) but not in the cytoplasm (C).
These results are consistent with a lack of functionality of the KEPE motif in the cytosol (since this is the implied meaning of cytoplasm in Swiss-Prot).
individually contributed clear enrichment of conserved motifs, affirming their individual and combinatorial relevance to motif prediction (Supplementary Fig.3).
The annotation of the yeast S.cerevisiae proteome in the SGD project is also extensive.
The CS distributions were obtained for the yeast proteome using the same pipeline.
In this case neither the keywords, nor the IUPred assignments provide any support for the KEPE motif, relative to the permutation controls.
Moreover Supplementary Figure 4 shows that most of the matches of KEPE and the permuted motifs are non-conserved in yeast.
In addition, the number of matches to the KEPE motif in the yeast proteome is lower than expected (55) compared with the human proteome (331).
This result is independent from the distributions of the K, P and E amino acids since their distributions are very similar in the human and yeast proteomes (Echols et al., 2002).
Therefore, the difference in the number should depend only in the total sequence length of both proteomes.
While the human to yeast proteome length ratio is 3.66, the ratio of the number of retrieved matches is nearly the double, 6.01.
Thus, our protocol was unable to provide any evidence in favour of the existence of KEPE motifs in yeast.
Manual screening of KEPE matches for plant, fungal and other non-metazoan protein entries in UniProt/Swiss-Prot likewise failed to provide evidence for plausible KEPE motifs.
We surmise that KEPE motifs arose and proliferated in the metazoan lineage.
3.3 Three KEPE length variations Inspection of the KEPE motif conservation in individual protein families showed that there are three length variants in the flexible gap after the P and preceding the last conserved negatively charged position.
Typical KEPEs as in C/EBPs allow a 23 residue gap.
Juns have longer variants and some Mafs have shorter variants (Supplementary Fig.5).
In all the alignments examined, the three variant motifs were never observed to interconvert during evolutionary change (although they are often found superimposed, e.g.in C/EBP, NFATC1-3, TOP1, HDAC4, FOS, see Supplementary Table 3).
This curious behaviour suggests that the length variants are functionally distinct, perhaps in a subtle way: for example, they could be recognized by different paralogous proteins; or they might all be recognized by the same protein but be modulated by interactions with distinct additional factors.
3.4 KEPE-bearing proteins KEPE motifs are mostly found in transcription factors and proteins that are broadly involved in modifying chromatin conformation.
Therefore, a role in modulating gene expression seems to be inevitable.
The highest KEPE enrichment is in the leucine zipper class of transcription factor, where 30% possess the motif.
As many KEPE sites are known to be sumoylated (Supplementary Table 3), a clear inference is that all KEPE sites are modified by sumoylation.
The motif is sometimes found to be conserved in orthologous proteins for more then 500 million years, as in C/EBPs from Drosophila and vertebrates (Fig.1B).
However, in many paralogous gene families that originated with the genome expansion associated with the origin of the vertebrates (Gibson and Spring, 1998; Kasahara, 2007; Meyer and Van de Peer, 2005), KEPE motif evolution is much more dynamic.
In the Fos transcription factor family, KEPE is conserved in cFos and Fra2 but absent from FosB and Fra1.
It is found in HDAC4 and 9 but not in other 4 KEPE histone deacetylases.
There can be from 0 to 3 KEPE motifs in various NFATC transcription factor paralogues.
Several Klf zincfinger proteins have KEPEs in separate non-superposable locations: these motifs are likely to have independent origins by point mutation within large natively disordered polypeptide segments.
The KEPE motif is larger than the sequence conservation associated with sumoylation sites.
It is possible that the additional conserved residues might be important to (i) be recognized by other binding proteins and/or (ii) in regulating the modification of the motif in other ways, e.g.by lysine acetylation, methylation or ubiquitinylation.
[Thus, the tumour suppressor HIC1 can be sumoylated on a lysine which is also a target for acetylation, suggesting that this motif might represent a sumoylation/acetylation switch (Stankovic-Valentin et al., 2007).]
The simplest model for KEPE function would be for a KEPE-binding protein to block access to the sumoylation site.
Since sumoylation is reported to relieve transcriptional inhibition by the RDM element of C/EBP (Kim et al., 2002), unsumoylated KEPE should be bound by a protein that acts as a repressor (at least in this context).
Since many of the KEPE proteins are assigned as chromatin modifiers, rather than as transcription factors per se, such a shared system of repression would be expected to be interlinked to chromatin conformational state.
Experimental identification of the ligand proteins binding to the short, medium and long KEPEs may provide a new perspective on gene regulation.
4 CONCLUSIONS LMs constitute nodes in cell regulatory networks that are acted upon by regulatory and signalling proteins and their domains.
Here, we describe a new linear motifKEPEthat is widespread in metazoan nuclear proteins classified as transcription factors or chromatin modulators.
KEPE function is expected to regulate sumoylation, a proposal, which may be tested experimentally by biochemical and genetic means.
Since KEPE is a common motif, elucidation of its function will have broad significance for understanding gene regulation in animals.
ACKNOWLEDGEMENTS We thank the contributors to the ELM resource for making in silico linear motif discovery feasible, Pl Puntervoll, Rein Aasland and Manfred Koegl for checking interaction networks for any hints to the ligand, Evangelos Pafilis for help with the Ontology Lookup Service and Niall Haslam for critically reading the article.
Funding: EU EMBRACE (LHSG-CT-2004-512092).
Conflict of Interest: none declared.
ABSTRACT Motivation: A large number of experimental studies on ageing focus on the effects of genetic perturbations of the insulin/insulin-like growth factor signalling pathway (IIS) on lifespan.
Short-lived invertebrate laboratory model organisms are extensively used to quickly identify ageing-related genes and pathways.
It is important to extrapolate this knowledge to longer lived mammalian organisms, such as mouse and eventually human, where such analyses are difficult or impossible to perform.
Computational tools are needed to integrate and manipulate pathway knowledge in different species.
Results: We performed a literature review and curation of the IIS and target of rapamycin signalling pathways in Mus Musculus.
We compare this pathway model to the equivalent models in Drosophila melanogaster and Caenorhabtitis elegans.
Although generally wellconserved, they exhibit important differences.
In general, the worm and mouse pathways include a larger number of feedback loops and interactions than the fly.
We identify functional orthologues that share similar molecular interactions, but have moderate sequence similarity.
Finally, we incorporate the mouse model into the web-service NetEffects and perform in silico gene perturbations of IIS components and analyses of experimental results.
We identify sub-paths that, given a mutation in an IIS component, could potentially antagonize the primary effects on ageing via FOXO in mouse and via SKN-1 in worm.
Finally, we explore the effects of FOXO knockouts in three different mouse tissues.
Availability and implementation: http://www.ebi.ac.uk/thornton-srv/ software/NetEffects Contact: ip8@sanger.ac.uk or thornton@ebi.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 21, 2014; revised on June 24, 2014; accepted on July 15, 2014 1 INTRODUCTION The insulin/insulin-like growth factor signalling pathway (IIS) and the target of rapamycin (TOR) signalling pathway have been shown to be important regulators of ageing across species via the transcription factor FOXO (Kenyon, 2011).
The mechanisms by which FOXO increased activity leads to lifespan extension are still unclear.
However, it is thought that lifespan extension is achieved through cell-cycle arrest by FOXO in the absence of insulin signalling (van der Horst and Burgering, 2007).
In addition, identification of FOXO transcriptional targets has revealed a second tier of transcription factors regulating a variety of downstream responses (Alic et al., 2011).
Ageing via the IIS pathway has been intensively studied at the level of invertebrate model laboratory organisms.
With their short lifespans, well-described genomes and a variety of mutants already available, Drosophila melanogaster (Clancy et al., 2001) and Caenorhabtitis elegans (Kenyon et al., 1993) provide excellent frameworks for fast identification of genetic determinants of ageing.
Relating results from fly and worm to a longer lived mammalian model, such as Mus musculus (Bl uher et al., 2003), is critical for the understanding of the ageing processes in human, but it is often difficult owing to the large evolutionary distance between invertebrates and mammals.
The general flow of the pathway is as follows.
The insulin and insulin growth factor receptors can be activated by two different insulin molecules or two insulin-like growth factor molecules.
On activation, the two receptors can activate the insulin receptor substrates (IRS1-4) by tyrosine phosphorylation.
The role of IRS1 especially has been well examined and found to propagate the signal further downstream, via the PI3K complex.
The phospholipid products of PI3K [phosphatidylinositol-3,4,5-triphosphate (PIP3)], once produced, can activate phosphoinositide-dependent kinase-1 (PDK1) that leads to the activation of AKT/protein kinase B-like proteins (AKT1-3, with AKT1 being well studied) and serum and glucocorticoid-inducible kinases (SGK1-3).
AKT1 and the SGK1-3 kinases inhibit the activity of the Forkhead transcription factors FOXO by retaining them in the cytoplasm.
The IIS pathway is generally well conserved, with the main building blocks (INSR, PI3K, PDK1, AKT, FOXO) present in both mammals and invertebrates.
The TOR pathway is also well conserved with its main building blocks present (TOR complexes 1 and 2, RHEB, S6 kinase).
Important differences also exist.
There are seven known insulin molecules in the fly, as opposed to 40 in the worm.
In the mouse, there are two insulin and two insulin-like growth factors.
Flies and worms possess a single insulin receptor each, whereas mice possess two insulin-like growth factor receptors in addition to the insulin receptor.
In mice, we *To whom correspondence should be addressed.
yPresent address: Mouse Informatics Group, Welcome Trust Sanger Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.
due Up ( ( ) Transcription Factors very forty XPath error Undefined namespace prefix observe more copies of certain proteins, such as four copies of AKT (Akt1-4) instead of one in the fly and two in the worm and four genes encoding FOXO (Foxo1, Foxo2, Foxo4 and Foxo6) in contrast to a single FOXO in flies and worms (foxo and daf-16).
The TOR pathway is remarkably similar between flies and mice, but exhibits differences with the worm (Ivanov et al., 2013; Riesen et al., 2014).
Previously, we developed the web-service NetEffects to solve the problem of relating gene expression results to their effects at the protein signalling level of the IIS pathway and the ageing phenotype (Papatheodorou et al., 2012).
This is a common problem of many studies on ageing that use whole organism mutants to study changes in lifespan and uncover protein interactions within the signalling pathways by use of transcriptomic datasets.
NetEffects uses Answer Set Programming, a logic-based method for inference that uses manually curated maps of the IIS pathway and its relationship to lifespan as prior knowledge.
Given a gene mutation and the resulting genome-wide differential gene expression, NetEffects will deduce signalling effects and how they can influence lifespan according to the prior knowledge.
Our applications on fly and worm datasets (Ivanov et al., 2013; Papatheodorou et al., 2012) have revealed consistent homeostatic mechanisms across both long-and short-lived mutants.
2 METHODS The signalling network model of the IIS and TOR pathways was built using GraphML, using the editor yEd (http://www.yworks.com).
This enabled a computationally readable representation of the pathways, as well as a graphical visualization that relates molecular topology to cellular components.
During the curation process, we used the insulin pathway available in KEGG (Kanehisa et al., 2012) as a starting point and the rest of the connections were built by literature review.
Literature searches were performed using PubMed (http://www.ncbi.
nlm.nih.gov/pubmed/) by querying mouse gene names and identifiers.
Only connections with experimental support, rather than just suggestive or hypothetical, were used.
Proteinprotein interactions from yeast twohybrid system or other screens were omitted.
Supporting literature for each relationship is available by clicking on the pathway connections within NetEffects.
All graphs from the comparative analyses across the fly, worm and mouse pathways were produced by custom-made R (http://www.r-project.org/) scripts.
These cross-species analyses were based on genomic sequence-based orthologous relationships, downloaded from Ensembl Compara (http://www.ensembl.org).
Orthology relationships that were not predicted by Compara but were suggested by the topology and connectivity of the genes in the pathway models, were sought in TreeFam (http://www.treefam.org/), OrthoDB (http://cegg.unige.ch/orthodb7) and Phylome (http://phylomedb.org/).
The mouse pathway was incorporated to the web-service NetEffects, built using PHP (PHP: Hypertext Preprocessor), JavaScript and Perl.
Proteins on the pathway have been annotated with Ensembl identifiers to enable the import and analysis of experimental datasets.
The theoretical perturbations option of NetEffects was used to query the pathway and produce inferences on the possible paths to longevity from different mutations.
The Experimental Results Analysis option was used to analyse the gene expression datasets.
Raw files of the expression datasets in Paik et al.(2007) were analysed using the Limma package in R (Smyth, 2005).
3 RESULTS AND DISCUSSION 3.1 A model of the insulin and TOR signalling pathways for M.musculus We curated a model of the IIS, TOR and neighbouring pathways in the mouse, providing access to the underlying literature as clickable connections on the pathway within the web-service.
We found the IIS pathway to be well connected and to include several points of cross-talks with neighbouring pathways.
This enables it to respond to signals from a variety of sources rather than just extracellular insulin molecules.
IRS and SHC1 bind to GRB2, which then activates the MAPK/ERK pathway.
In addition to propagating insulin signals, IRS also receives feedback from other pathways, such as TOR by inhibition from S6 kinase (gene name Rps6kb1), JNK by inhibition from JNK1 and Wnt signalling through an inhibition by GSK3-beta.
SHC1 is phosphorylated by the activated Insulin receptor, propagating the signal to MAPK/ERK via GRB2, SOS1 and SOS2.
AKT1 provides another point of cross-talk with the TOR pathway, by directly inhibiting PRAS40, which then inhibits RPTOR, inhibiting TSC2.
AKT1 can also be activated by TOR complex 2.
Previous work on mouse mutants of the IIS and TOR signalling pathways have clearly shown a role for the insulin pathway in the regulation of lifespan, with null S6K (Selman et al., 2009) mice and null IRS-1 (Selman et al., 2008) mice showing significant lifespan extension when compared with wild-type.
Although there is so far no experimental confirmation of lifespan regulation by mammalian FOXOs, results from mice lacking one or more of FoxO1, FoxO3 and FoxO4 have revealed ageing-related phenotypes, such as reduced bone mass (Ambrogini et al., 2010) and the development of ageing-related diseases like thymic lymphomas, hemangiomas (Paik et al., 2007).
These results suggest that mammalian FoxOs play a protective role against agerelated diseases.
In addition, Willcox et al.(2008) provide evidence for FoxO3A genetic variation being associated with lifespan in a large, well-phenotyped cohort of humans through a casecontrol study of five candidate genes.
3.2 Comparison of insulin and TOR signalling in fly, worm and mouse With the availability of thoroughly curated pathway models for each of the three species, we are now able to make comparisons of their components and connections.
Supplementary Table S2 summarizes the similarities and differences between the IIS and TOR pathway molecules across the three different species.
Figure 1 presents on the mouse IIS and TOR pathway model the occurrence of fly and/or worm orthologues, also present in the species-specific models.
Functional orthologues were also identified, where sequence similarity across species is moderate but molecular interactions and experimental evidence suggest that these pairs are indeed orthologues.
Such cases include ist-1, a worm orthologue to the IRS (OrthoDB); drr-2, a worm orthologue to eukaryotic translation initiation factors 4H (Ching et al., 2010) and 4B (Phylome Orthology); unc-51, worm orthologue to Ulk1/Atg1; let-363, worm orthologue to Mtor (TreeFam) and age-1, worm orthologue to Pik3ca (TreeFam).
Gene Deptor encodes a protein associated with the mammalian TORC1 that is absent from the 3000 I.Papatheodorou et al., in order se ; Ivanov etal., 2013visualisation in order `` T P '' `` '' &amp; S P M very , , to , 5 insulin receptor substrate fly and worm genomes.
Glatter et al.(2011) hypothesized that the gene appeared later in vertebrate evolution.
In general, the IIS and TOR pathways in the mouse appear considerably larger and with more cross-talk (see Supplementary Section S3).
This effect is partly because of the fact that the IIS and TOR pathways in the mouse have been more thoroughly studied, as well as to the different extents of the curation of the neighbouring pathways within each organism.
Being used as a model organism for studies on human diseases such as cancer and diabetes and with the availability of a large number of murine cell lines, more interactions within and between the IIS, TOR and their neighbouring pathways have been discovered.
Cross-talk points between the IIS and TOR pathway include interactions between AKT1 and TORC2 in all three organisms, as well as RPS6KBA (S6 kinase) and IRS1 in mouse and fly.
In the mouse, both AKT and IRS involve several paralogous copies.
The IRS genes in the mouse are also involved in the cross-talk between JNK and IIS pathways, an interaction that is conserved across species.
In some cases, the interactions between neighbouring pathways are not conserved due to the lack of orthologues in worms or flies or both.
For example, there is no orthologue for TSC2 in the worm which is inhibited by AKT in flies and mice.
There are also cases where orthologues in the invertebrates exist, but the interactions have not been observed experimentally as exemplified by the interaction of IIS and MAPK/ERK pathways via GRB2 and SOS1.
Finally in the case of MAPK/ERK to TOR cross-talk, facilitated by the activation of RPS6KA1 by MAPK1 in the mouse, we found orthologues in both other organisms but no interaction in the fly.
In the worm, there is evidence for a proteinprotein interaction between them (see Supplementary Section S3D for the complete table of cross-talk points).
3.3 Paths to FOXO-mediated longevity with NetEffects We incorporated the mouse pathway model into the web service NetEffects (Papatheodorou et al., 2012) to enable computational analyses.
We can now compare the effects on FOXO-mediated ageing across the three species.
Using the theoretical perturbations functionality, we tested the paths to FOXO-mediated longevity in the mouse and worm from already known mutants in flies (see Supplementary Section S4 for full results).
Knocking out Ins1 or Ins2 in mouse results in a path consistent with those obtained when doing the equivalent test in the fly and worm models (Table 1).
This leads to increased lifespan through inhibition of AKT, which then allows translocation of FOXO into the nucleus.
However, in mouse and worm, we also encounter paths that reduce longevity.
In the mouse, this path involves the IGF1receptor and RACK1 (gene Gnb2l) that leads to enhanced AKT phosphorylation and activation.
This effect, however, appears to be cell-type-specific and probably also context-specific, as Fig.1.
The model of the IIS and TOR pathways in M.musculus.
Colours indicate the existence of orthologues in flies, worms, both or none.
A larger version of this model is available in Supplementary Figure S1 3001 Mouse model of IIS and TOR hypothesised s s due to s s. in order `` '' s sdescribed in Kiely et al.(2005).
In the worm, there is also an effect that might be antagonizing the FOXO-mediated lifespan increase, but is mediated by LET-60 (Kras orthologue) and the transcription factor SKN-1.
Similar effects were produced when Igf1r was mutated.
Mutation of Insr had similar results to the equivalent manipulations in the fly and worm.
We also tested known mouse mutants in the IIS pathway that affect lifespan.
The Irs1/ mutant (Selman et al., 2008) results in a long-lived phenotype, as expected from previous knockout experiments on the fly orthologue chico (Clancy et al., 2001).
In contrast, the Irs2/ mutant is short-lived (Selman et al., 2008).
NetEffects infers similar paths for both Irs1/ and Irs2/ mutants, as they exhibit similar connections to other components of the IIS and TOR pathways (Table 1).
The shortest path that leads to increase in lifespan involves inhibition of AKT.
The shortest paths leading to a reduction of lifespan require increased activity of GSK3B leading to inhibition of FOXO via SIRT1 and E2F1.
Functional experiments, as well as genome-wide gene expression in the two mutants could show whether Irs2/ mutant mice reduce their lifespan via a different route, and whether the activity of GSK3B plays a role.
We also tested ist-1, the functional worm orthologue for Irs1 and Irs2, where a knockout experiment with lifespan analysis has not been performed.
In addition to the FOXO-mediated lifespan extension, we obtained a sub-path of the same antagonistic effect via SKN-1 as in the INS1/2 tests shown above.
Partial support for the opposing effect of LET-60 (RAS) via SKN-1 to FOXO-mediated lifespan extension comes from a study on long-lived age-1 (PI3K) worms, where downregulation of let-60 and skn-1 genes was observed (Tazearslan et al., 2009).
Finally, we analysed the expression datasets in cells derived from three different tissues of null and conditional alleles in the three main Foxo genes (FoxO1/+; FoxO3/; FoxO4/).
The datasets were generated by Paik et al.(2007).
We analysed the Foxomutants in liver and lung endothelial cells and thymus cells.
According to the authors, liver cells presented cancer-related phenotypes, whereas lung cells did not present a detectable phenotype.
With NetEffects we can show that 16 genes within the pathway model are differentially expressed in liver, one in lung and three in thymus (excluding the three Foxo genes that have been knocked out).
Almost all of the shortest paths starting from these differentially expressed genes correspond to negative feedback to the mutation of Foxo genes, as shown by the predicted impact on lifespan (Supplementary Section S5).
This suggests that the function of the three Foxo genes plays a greater part in the liver and thymus rather than in lung endothelial cells.
Similar negative feedback was identified in our previous analyses of Foxo null mutants in the fly Papatheodorou et al.(2012).
4 CONCLUSION The molecular basis of nutrient signalling is largely comparable across a large evolutionary space, despite striking differences in the presence or number of copies of certain components between species.
The pathways, in all organisms except worm focus only on FOXO-mediated lifespan, thereby ignoring any effects through different transcription factors.
However, using the mouse pathway, we can identify neighbouring signalling pathways with the potential to influence the signal transduction of the IIS through the identified cross-talk points.
Comparison of the pathways in a systematic and qualitative way has the potential to explain differences in effects on lifespan across species and help design experiments that will evaluate the pathway flux.
The richness and detail of the mammalian model can inform the interpretation of results in invertebrates, where molecular interactions have not been so extensively studied.
Being able to compare the pathways side by side, we recorded all differences and identified functional orthologues.
By use of NetEffects, we were able to suggest possible paths affecting FOXO-mediated Table 1.
Shortest paths to FOXO-mediated longevity, derived from NetEffects, where DAF-16 is the FOXO orthologue in C.elegans 3002 I.Papatheodorou et al.s-&Unicode_x2215;-, ,-&Unicode_x2215; &Unicode_x2215; &Unicode_x2215; &Unicode_x2215;-knock-&Unicode_x2215; &Unicode_x2215;+-&Unicode_x2215;/-1 3lifespan given a single mutation and how these differ across species, thus generating hypotheses for further investigation.
ACKNOWLEDGEMENTS The authors thank Dobril Ivanov and Matthias Ziehm for useful discussions.
Funding: This work was funded by the Wellcome Trust Strategic Award WT081394MA (I.P., J.M.T.)
and by the European Molecular Biology Laboratory (EMBL) (R.P., J.M.T.).
Conflict of interest: none declared.
