ABSTRACT Motivation: Advanced technologies are producing large-scale proteinprotein interaction data at an ever increasing pace.
A fundamental challenge in analyzing these data is the inference of protein machineries.
Previous methods for detecting protein complexes have been mainly based on analyzing binary protein protein interaction data, ignoring the more involved co-complex relations obtained from co-immunoprecipitation experiments.
Results: Here, we devise a novel framework for protein complex detection from co-immunoprecipitation data.
The framework aims at identifying sets of preys that significantly co-associate with the same set of baits.
In application to an array of datasets from yeast, our method identifies thousands of protein complexes.
Comparing these complexes to manually curated ones, we show that our method attains very high specificity and sensitivity levels (80%), outperforming current approaches for protein complex inference.
Availability: Supplementary information and the program are available at http://www.cs.tau.ac.il/roded/CODEC/main.html.
Contact: roded@post.tau.ac.il Supplementary information: Supplementary data are available at Bioinformatics online.
Received on December 17, 2008; revised on February 25, 2009; accepted on March 16, 2009 1 INTRODUCTION Procedures such as yeast two hybrid and co-immunoprecipitation (CoIP) (Mann et al., 2001) are routinely employed nowadays to detect new proteinprotein interactions, producing large-scale protein interaction networks for various species.
The networks provide a step stone for finding protein complexesthe fundamental units of macromolecular organization (Alberts, 1998).
The discovery of protein complexes based on yeast two hybrid data is a challenging task, since a protein complex may share common members with other complexes, and not all members of a certain protein complex directly interact with one another.
CoIP data, however, can be used for finding complexes by itself since co-immunoprecipitation experiments directly test complex co-membership: a bait protein is tagged and a purification of its complex co-members (prey proteins) is made followed by mass spectrometry.
To whom correspondence should be addressed.
Surprisingly, most methods for detecting protein complexes are based on treating protein interaction data as binary, i.e.
interactions are between pairs of proteins only.
This is commonly done by translating non-binary CoIP associations, of a bait to the set of preys obtained by tagging it, into binary interactions using the spoke model (Bader and Hogue, 2002), where a purification is translated into a set of pairwise interactions between the bait and each of the precipitated preys.
One of the most commonly used algorithms for this task is the Molecular Complex Detection (MCODE) algorithm by Bader and Hogue (2003).
MCODE detects densely connected components of the protein network.
It is based on weighing vertices by the density of their local neighborhoods.
Starting from a high weight vertex, a local expansion is done in a greedy fashion.
Another common clustering algorithm is the Markov clustering algorithm (MCL) (Enright et al., 2002).
MCL simulates random walks on the protein interactions network.
Random walks are performed iteratively; after sufficiently many iterations, the probability that a walk that starts in a dense area of the graph will end in the same dense area is high.
In order to magnify this effect, MCL applies, after each walk, an inflation step that separates high probability connections from low probability ones.
Eventually, the process converges and a cluster structure of the graph is formed.
MCL was shown to outperform other clustering algorithms for protein complex detection (Brohee and van Helden, 2006).
Recently, Rungsarityotin et al.
(2007) presented a new clustering method based on Markov random fields (MRF).
MRF applies a statistical model that assumes that the membership of each protein in a given cluster is only dependent on the membership status of its neighbors.
Finally, Friedel et al.
(2009) presented an unsupervised approach to find protein complexes that uses a bootstrapping mechanism to derive reliability scores for interactions between proteins.
The resulting weighted network is then clustered using MCL.
The only unsupervised approach we are aware of that uses CoIP data directly is that of Scholtens et al.
(2005).
This approach is called Local Modeling and is probabilistic in nature.
It relies on building a directed network of baitprey relationships and searching for subnetworks in which all protein pairs that were tested for a bait prey relation are connected.
Such fully connected subnetworks are shown to correspond well to protein complexes.
Supervised methods for identifying protein complexes have also been developed.
Gavin et al.
(2006) defined a socio-affinityscoring system that measures the log ratio of the number of times two proteins are seen together in CoIP purifications, relative to what would be expected from their frequency in the dataset.
These scores are used for clustering the proteins employing various The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 112 111117 G.Geva and R.Sharan clustering algorithms and parameters.
Result sets that exhibit poor correspondence to manually curated complexes are discarded.
Complex cores are identified as those stable parts of complexes that are not affected by the clustering algorithms/parameters.
Collins et al.
(2007) devised another scoring system for protein pairs, which combines the evidence in each purification for baitprey and preyprey relationships.
They applied hierarchical clustering to these scores to produce putative complexes.
Pu et al.
(2007) used the (Collins et al., 2007) scoring system together with the MCL algorithm to produce complex predictions.
Another scoring system was used by Hart et al.
(2007) in combination with the MCL algorithm to derive protein complexes.
Another scoring scheme was developed by Zhang et al.
(2008), who used a maximum clique finding algorithm to derive complex predictions.
In this article, we propose a novel method for inferring protein complexes from CoIP data, which we call CODEC (COmplex DEtection from Coimmunoprecipitation data).
We represent the data using a bipartite graph, where one set of vertices corresponds to the prey proteins, and the other one corresponds to the bait proteins.
Edges connect a bait to its associated preys.
Ideally, protein complexes should be manifested as fully connected bipartite subgraphs of this graph, as also argued in Scholtens et al.
(2005).
In practice, experimental noise results in false positive and false negative associations in the CoIP data.
In addition, for proteins that occur both as baits and preys in the data, we expect that if the bait (prey) instance is included in a complex, also its corresponding prey (bait) instance will be part of the complex.
Thus, a complex is expected to appear as a dense bipartite subgraph such that every participating protein has both its bait and prey instances present.
To identify those dense balanced bipartite subgraphs of the bait prey graph, we adapted the SAMBA biclustering algorithm (Tanay et al., 2002).
We applied CODEC to three datasets from three large-scale experiments in yeast (Gavin, 2002; Gavin et al., 2006; Krogan et al., 2006), identifying thousands of protein complexes.
We evaluated CODEC and compared it to extant approaches by using a collection of manually curated complexes from the MIPS (Mewes, 2002) and GO (Cherry et al., 1998) databases.
First, we compared CODEC to the three clustering approaches: MCODE, MCL and MRF.
We show that CODEC outperforms these approaches on two large-scale datasets, attaining higher values of specificity and sensitivity.
We did not include a comparison to the bootstrap method of Friedel et al.
(2009) as the software was not readily available.
Second, we show that CODEC can be useful even when supervised approaches are applicable, comparing it to two representative supervised approaches: those of Gavin et al.
(2006) and Collins et al.
(2007).
Remarkably, CODEC outperforms these approaches as well, even though they use curated information in the protein complex identification process.
Finally, we show that CODEC compares favorably to the Local Modeling approach (Scholtens et al., 2005), and at the same time it is much more scalable, allowing the analysis of much larger datasets.
2 METHODS 2.1 Data acquisition We downloaded CoIP data for three datasets: (i) Gavin et al.
(2006), which contains 1993 bait proteins, 2670 prey proteins and 19 277 baitprey relationships; (ii) Krogan et al.
(2006), which contains 2233 bait proteins, 5219 prey proteins [94 prey proteins were omitted from the raw data, since they were suspected as non-specific contaminants (Krogan et al., 2006)] and 40 623 baitprey relations; and (iii) Gavin (2002), which contains 455 bait proteins, 1364 prey proteins and 3413 baitprey relations.
MIPS complexes were obtained from the MIPS database (Mewes, 2002) (February 2007 download).
Only manually annotated complexes were used (category 550 was excluded).
From the 243 manually annotated MIPS complexes, we considered only complexes at level 3 or lower.
Higher level complexes were collapsed to level 3.
Overall, the data contained 229 complexes.
Gene ontology (GO) complexes were obtained from the Saccharomyces Genome Database (Cherry et al., 1998) (March 2007 download).
The GO dataset contained 193 complexes.
2.2 Graph construction and statistical data modeling We represent the CoIP data using a bipartite graph G= (U,V ,E), where vertices on one side (U) represent purifications with specific baits, and vertices on the other side (V ) represent the union of the set of preys detected in all the purifications and the set of baits.
For convenience, we name the vertices according to the proteins they represent.
Edges connect baits to their associated preys.
In addition, every purification with a bait u is connected to u on the prey side.
A candidate protein complex corresponds to a connected subgraph H = (U ,V ,E) of this graph, where V V is the set of member proteins in the complex, and U U is a set of purifications.
We use a likelihood ratio score to evaluate a candidate protein complex.
The score measures the fit of a subgraph to a protein complex model versus the chance that the subgraph arises at random.
The protein complex model assumes that each edge in the subgraph occurs with high probability pc, independently of all other vertex pairs.
This assumption ignores possible dependencies between baitprey associations, but allows computing candidate complex scores in an efficient manner.
The null model assumes that each edge (u,v) occurs with probability pu,v, independently of all other vertex pairs, where pu,v is the probability of observing an edge between u and v in a random bipartite graph with the same vertex degrees as G. In practice, we use pc =0.9 as recommended in Tanay et al.
(2002).
pu,v is approximated by d(u)d(v)|E| (Itzkovitz et al., 2003), where d(v) denotes the degree of a vertex v. Thus, the score of H is: L(H )= (u,v)E log pc pu,v + (u,v)E log 1pc 1pu,v By setting the weight of each edge (u,v) to be log pcpu,v >0 and the weight of each non-edge (u,v) to be log 1pc1pu,v <0, we have that the score of a subgraph is the sum of weights of its vertex pairs.
There are two exceptions to setting the edge weights: (i) an edge of the form (v,v) is assigned zero weight.
(ii) We call a vertex whose corresponding protein serves as a bait in some purification, but never detected as a prey, artificial.
For such a vertex, we consider two weighting schemes.
The first, which we call w0, sets all weights involving artificial vertices to 0, based on the assumption that these cases represent proteins that cannot be detected as preys due to experimental limitations.
The second scheme, which we call w1, treats such vertices the same as all other vertices, resulting in all the weights involving artificial vertices being non-positive.
2.3 The algorithm Our algorithm for protein complex identification employs a greedy search heuristic, which starts from high weight seeds and expands them using local search.
We describe these phases below.
2.3.1 Seed definition.
Recall that we seek heavy subgraphs of the bait prey graph with the additional requirement that these subgraphs are consistent, namely that a bait instance of a protein is included if and only if the prey instance of the same protein is included.
As seeds, we use complete bipartite subgraphs (bicliques) of the baitprey graph, augmented by additional vertices so that the consistency requirement is satisfied.
For a 112 [16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 113 111117 Identification of protein complexes prey vV , denote its corresponding bait (if such exists) by m(v).
Similarly, for a bait uU, denote its corresponding prey (which might be artificial) by m(u).
Then a prey subset S V with a set of common (bait) neighbors N(S) induces the following consistent seed: C(S)=SN(S){m(v) :vSN(S)} 2.3.2 Seed identification.
We start by identifying a high weight seed around each protein.
To find consistent seeds, we adapt the algorithm in Tanay et al.
(2002).
Basically, as shown in Tanay et al.
(2002), the heaviest biclique in a bipartite graph can be identified by an iterative algorithm.
At each iteration, the neighborhood of a vertex uU is scanned, and each subset of its neighbors is credited by the weight from u to the vertices of this subset.
After scanning all vertices in U, the subset that attained the highest weight induces the heaviest biclique.
In our case, we have a further consistency requirement.
Hence, we have to augment each of the possible seeds by appropriate vertices.
To this end, we add a post-processing step to the algorithm above which updates the weight of every subset according to the consistent seed it induces.
For computational efficiency, we limit the size of the scanned subsets to 24.
We only scan subsets that contain the prey vertex that corresponds to u.
Each candidate seed is scored by its log-likelihood ratio.
We retain the 500 000 highest scoring candidates and store them in a heap to prevent duplicates.
2.3.3 Greedy expansion.
This phase iteratively applies modifications to the seed so as to expand it and increase its weight.
Seeds are sorted by their log likelihood in a descending order.
The greedy expansion is applied to the seeds by that order.
At each iteration, all possible vertex additions to the seed and vertex deletions from the seed are considered, where baits are coupled to their corresponding preys to maintain consistency under these modifications.
The modification that improves the score the most is accepted.
This process continues until the score of the subgraph cannot be further improved.
For efficiency reasons, this phase is applied only to seeds that were not contained in previous expanded subgraphs.
2.3.4 Filtering the results.
We focus on clusters with at least three preys.
We evaluate the significance of a cluster by comparing the score of its corresponding subgraph to those obtained on randomized instances.
Specifically, we create random graphs with the same vertex degrees as G by using the MaslovSneppen procedure (Maslov and Sneppen, 2002).
The procedure switches a pair of edges (u,v) and (u,v) with (u,v) and (u,v), provided that the latter did not exist in the first place.
The switches are done 100m times, where m is the number of edges in the original graph (Milo et al., 2003).
Our algorithm is applied to these randomized instances to compute a null distribution of subgraph scores.
We use this distribution to compute a P-value for each of the clusters and retain only clusters whose P-value is smaller than a threshold.
To avoid redundant solutions, we filter putative protein complexes with high similarity to one another.
The similarity is measured based on the intersection of the prey sets of the compared clusters.
Specifically, for two putative complexes V1 and V2 we measure their similarity as |V1 V2|/min{|V1|,|V2|}.
If the similarity exceeds a predefined threshold, then the subgraph with the higher P-value is discarded.
We used 80% as the similarity filtering threshold [as in (Sharan et al., 2005)]; a lower value of 50% yielded a similar performance (see Supplementary Table S2).
2.3.5 Implementation and running time We implemented CODEC using the microsoft .net framework 2.0 and the C# programming language.
CODEC was applied to three datasets, as detailed above, on a Intel core 2 duo 1.86 GHz processor with 1 GB memory.
The running time ranged from minutes to hours, depending on the size of the dataset.
The running time of CODEC on the smallest (Gavin, 2002) dataset was 5 min; the application to the medium (Gavin et al., 2006) dataset took 3 h; finally, the run on the largest (Krogan et al., 2006) dataset lasted 30 h. 2.4 Quality assessment We assess the quality of the produced complexes by measuring their specificity and sensitivity with respect to a set of gold standard (known) complexes.
To this end, for each output cluster we find a known complex with which its intersection is the most significant according to a hypergeometric score.
The hypergeometric score is compared with those obtained for 10 000 random sets of proteins of the same size, and an empirical P-value is derived.
These P-values are further corrected for multiple hypothesis testing using the false discovery rate procedure (Benjamini and Hochberg, 1995).
We say that a cluster is a significant match to a complex if it has a corrected P-value lower than 0.05.
Let C be the group of clusters from the examined result set, excluding clusters that do not overlap any of the true complexes.
Let C C be the subset of clusters that significantly overlap a known complex.
The specificity of the result set is defined as |C|/|C |.
Let T be the set of true complexes, excluding complexes whose overlap with the examined dataset is less than 3 proteins and ensuring a maximum inter-complex overlap of 80%.
Let T T be the subset of true complexes with a significant match by a cluster.
The sensitivity of the result set is defined as |T|/|T |.
The F-measure is a measure combining the specificity and sensitivity measures.
It is defined as the harmonic average of these two measures: 2 specificitysensitivity specificity+sensitivity In addition, we also used the Accuracy measure suggested by Brohee and van Helden (2006).
This measure also evaluates the quality of complex predictions against a gold standard set.
The accuracy measure is the geometric mean of two other measures: positive predictive value (PPV) and sensitivity.
PPV measures how well a given cluster predicts its best matching complex.
Let Ti,j be the size of the intersection between the i-th annotated complex and the j-th complex prediction.
Denote PPVi,j = Ti,jn i=1 Ti,j = Ti,j Tj where n is the number of annotated complexes, and Tj is the sum of the sizes of all of cluster j intersection sizes.
The PPV of a single cluster j is defined as PPVj = nmax i=1 PPVi,j The general PPV of the complex prediction set is defined by PPV= m j=1 TjPPVjm j=1 Tj where m is the number of complex predictions.
The sensitivity measure used by Brohee et al.
(which is different from the one defined above) represents the coverage of a complex by its best-matching cluster.
Denote Sni,j = Ti,j Ni where Ni is the number of proteins in the annotated complex i. Complex-wise sensitivity is defined as Sni = mmax j=1 Sni,j The sensitivity of a complex set is defined as Sn= n i=1 NiSnin i=1 Ni The Accuracy measure can be influenced by small and insignificant intersections of a predicted complex and an annotated one.
For example, if a predicted complex intersects only one annotated complex, and the size of the intersection is 1, the PPV of that predicted complex will be 1.0.
Thus, we used a threshold to limit the effect of such small intersections, and evaluated the different solutions under varying thresholds ranging from 0 to 10.
For each such threshold t, all intersections of size at most t were not included in the Accuracy computation.
113 [16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 114 111117 G.Geva and R.Sharan 2.5 Parameter tuning The input for the MCL and MCODE clustering algorithms was the set of interactions resulting from connecting a bait protein to its preys [the spoke model (Bader and Hogue, 2002)] for each of the datasets.
For setting the parameters of the algorithms, we used the values recommended by Brohee and van Helden (2006).
Specifically, we used the inflation parameter 1.8 for MCL.
For MCODE, we used the parameters depth = 100, node score percentage = 0, Haircut = TRUE, Fluff = FALSE and percentage for complex fluffing = 0.2.
MRF was applied using the spoke model, using the parameters suggested by Rungsarityotin et al.
(2007), i.e.
K =698 and =3.5.
We obtained the Local Modeling implementation from the bioconductor http://www.bioconductor.org.
The parameters used to run Local Modeling are the default parameters mentioned in Scholtens et al.
(2005).
When creating complex estimates from Collins et al.
(2007), we used MCL with the same parameters as described above, and used the PE values as the input to the MCL algorithm.
3 RESULTS AND DISCUSSION 3.1 CODEC overview CODEC is based on reformulating the protein complex identification problem as that of finding significantly dense subgraphs in a bipartite graph.
We construct a bipartite graph whose vertices on one side represent prey proteins, and vertices on the other side represent bait proteins.
Edges connect a bait protein to its associated preys.
Ideally, a complex should appear as a fully connected bipartite subgraph (biclique) in this graph.
In practice, due to experimental noise, a complex will appear as a dense bipartite subgraph.
We note that further experimentation using methods such as cross-linking and sequential CoIP can improve the detection process, but is far more costly.
In addition, we impose a consistency requirement: some proteins occur in the data both as baits and as preys.
For such proteins, we require that if a certain prey (bait) vertex is included in the subgraph, so must be the corresponding bait (prey).
These definitions are exemplified in Figure 1.
The example dataset contains 10 proteins marked as P1-P10 (Fig.1a).
Four purifications are made.
The proteins used as baits are P3, P4, P5 and P7.
There are two sets of preys that are supported by more than one bait: {P2,P3,P4,P5} and {P5, P6, P7, P8}.
It can be hypothesized that these sets correspond to two protein complexes, shown in Figure 1b.
In both cases, the consistency requirement is satisfied.
The missing edge between P5 and P2 is a likely false negative, since both P3 and P4 interact with P2.
There may be additional complexes in this toy example, but there is only weak evidence for their existence since they are detected as preys by a single bait protein.
We adapted the SAMBA algorithm (Tanay et al., 2002) to find putative complexes, henceforth called clusters.
As further detailed in the Methods, the algorithm relies on a scoring component and a search heuristic to identify high scoring subgraphs.
The scoring of a subgraph is based on a likelihood ratio score, which measures the density of the subgraph versus the chance that its connections arise at random.
We experimented with two scoring variants: a permissive one, w0, and a stricter one, w1 (see Section 2).
In all the applications below, we report on the results of both variants.
The search heuristic starts from small bicliques and expands them using greedy search.
Unlike SAMBA, the search procedure also ensures that the consistency requirement is met by coupling together the prey and bait instances of a protein.
Fig.1.
An example data set.
(a) An input baitprey graph.
Baits are colored in blue and preys are colored in red.
(b) Two possible protein complexes and their corresponding subgraphs.
The significance of the identified clusters is evaluated by comparing their scores to those obtained on randomized instances, where the edges of the bipartite graph are shuffled while maintaining node degrees.
We retain only significant clusters and further eliminate redundant clusters with high overlap among them.
3.2 Application and evaluation As a first test of CODEC, we applied it to two recently published large-scale CoIP datasets in yeast.
The first dataset from Gavin et al.
(2006) contains 1993 bait proteins and 2670 prey proteins, and its edge density in the bipartite graph model is 0.006.
The second dataset from Krogan et al.
(2006) contains 2233 bait proteins and 5219 prey proteins, and its edge density in the bipartite graph model is 0.003.
This dataset has a much lower bait to prey ratio than the former one and, thus, serves as a different test case for our method.
CODEC was applied to the two original datasets; no proteins were filtered.
The application of CODEC to the first dataset using the w0 weighting scheme yielded clusters with 12 baits and 22 preys on average The average edge density within an output cluster was very high (0.65).
When using the stricter w1 scheme, a similar number of clusters was obtained, but the clusters were much smaller (4.5 baits and 13.5 preys on average).
The application of CODEC to the second dataset using the w0 weighing scheme produced clusters with 4 baits and 16 preys on average.
The average interaction density within the output clusters was high (0.54).
When using the w1 scheme, the number of clusters dropped by 3-fold although their sizes remained similar to the w0 case.
The size distributions of the obtained protein clusters in each of two applications are provided in Supplementary Table S1.
To assess the quality of our results, we measured their specificity and sensitivity with respect to a collection of manually curated complexes taken from the MIPS (Mewes, 2002) database (see Section 2).
Specificity is defined as the fraction of clusters that significantly overlap a known complex; sensitivity is defined as the fraction of known complexes that significantly overlap an identified cluster.
We computed receiver operating characteristic (ROC) curves for the two datasets, which plot the sensitivity and (1specificity) values over a range of P-value cutoffs for the output clusters (Figures 2 and 3).
In each plot, we chose the point that maximizes 114 [16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 115 111117 Identification of protein complexes Fig.2.
A comparison of protein complex identification approaches on the data of Gavin et al.
(2006).
For each method shown is the sensitivity of the output solution as a function of one minus its specificity.
For CODEC shown are two receiver operating characteristic (ROC) curves, corresponding to different weighting strategies (w0 and w1).
The evaluation is based on a comparison to known protein complexes from the MIPS database (Mewes, 2002).
The CODEC plots were smoothed using a cubic spline.
Fig.3.
A comparison of protein complex identification approaches on the data of Krogan et al.
(2006).
See legend of Figure 2 for details.
the sum of sensitivity and specificity (Coffin and Sukhatme, 1997) as the P-value cutoff for the output clusters.
The results attained are summarized in Table 1.
We compared CODEC to three clustering algorithms: MCODE, MCL and MRF (Table 1 and Figures 2 and 3).
On both datasets, CODEC outperformed MCODE and MCL, yielding significantly higher sensitivity values.
The cluster set provided by Rungsarityotin et al.
(2007) was computed by applying MRF using the spoke model to the Gavin et al.
(2006) dataset (the MRF results with the matrix model were inferior and, hence, were not used in the comparison).
CODEC and MRF achieved similar sensitivity scores, but at the same time CODEC attained significantly higher specificity.
Qualitatively similar results were obtained when evaluating the collections of protein complexes based on known complexes from the GO (Cherry et al., 1998) database (see Supplementary Figures S1 and S2).
When using an alternative evaluation measure the Accuracy measure suggested by Brohee and van Helden (2006)CODEC was again shown to outperform MCL and MCODE, while providing results that were only slightly better than those of MRF (see Supplementary Figures S3 and S4).
Notably, all the tested methods perform worse on the data of Krogan et al.
because of its low bait to prey ratio.
3.3 Comparison to extant CoIP-based approaches The results above demonstrate the utility of using CoIP data for protein complex identification.
Next, we compared CODEC to extant protein complex inference methods that use such data.
As a first test, we compared CODEC to two other methods that use CoIP data for scoring pairs of putatively interacting proteins.
The first (Gavin et al., 2006) computes cores of complexes based on socio-affinity scoring system that measures the log ratio of the number of times two proteins are seen together in CoIP purifications, relative to what would be expected from their frequency in the dataset.
The second (Collins et al., 2007) scores pairs of proteins using a purification enrichment (PE) score, which combines the evidence in each purification for baitprey and preyprey relationships.
We used these PE scores as input to the MCL algorithm [as suggested in Brohee and van Helden (2006)].
Importantly, both methods use manually curated information (known protein complexes from MIPS) to tune their parameters.
We conducted the comparison on the the Gavin et al.
(2006) dataset, for which we had the complex cores from Gavin et al.
(2006).
The results are summarized in Table 2 and depicted in Figure 2.
Notably, even though the methods of Gavin et al.
(2006) and Collins et al.
(2007) use prior biological information in the inference process, CODEC outperforms both, attaining higher sensitivity and specificity values.
The most pronounced difference is with respect to the specificity of Gavins cores (78% versus 51%).
Our final comparison was to the Local Modeling method (Scholtens et al., 2005).
The available implementation of the method could not run on the datasets of Gavin et al.
(2006) and Krogan et al.
(2006) due to their relatively large size.
Hence, we used a smaller data set as a test case (Gavin, 2002), containing 455 bait proteins and 1364 prey proteins.
The protein complexes inferred by Local Modeling are partitioned into three categories: complexes that are supported by multiple baits (marked as MBME), complexes that are supported by a single bait (marked as SMBH) and complexes that contain two baits where only one of the baits identifies the other bait as its prey.
We focused on the 272 MBME complexes, which represent the highest confidence predictions.
As can be seen in Table 3 and Figure 4, CODEC outperforms local modeling, attaining higher specificity and sensitivity.
When including in the Local Modeling solution also the SMBH complexes (336 in total) the sensitivity increased to 93%, at the price of a decrease in specificity 115 [16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 116 111117 G.Geva and R.Sharan Table 1.
Comparison to MCODE, MCL and MRF Gavin et al.
(2006) Krogan et al.
(2006) Number of Specificity Sensitivity F-measure Number of Specificity Sensitivity F-measure Complexes (%) (%) (%) Complexes (%) (%) (%) CODEC using w0 1082 77.5 77 77 8348 30 76.2 43 CODEC using w1 1005 78.5 79 78.5 2973 46.5 72 56.5 MCODE 73 73.5 32 44.5 130 25 14 18 MCL 411 49.5 44.5 47 818 19.5 46 27.5 MRF 698 79.7 46.7 59   A comparison of CODEC, MCODE, MCL and MRF on the datasets Gavin et al.
(2006) and Krogan et al.
(2006).
The best result in each column appears in bold.
Table 2.
Comparison to Collins et al.
and Gavin et al.
2006 Number of Specificity Sensitivity F-measure Complexes (%) (%) (%) CODEC using w0 1082 77.5 77 77 CODEC using w1 1005 78.5 79 78.5 Gavin et al.
2006 480 51.5 70.5 59.5 Collins et al.
258 70 69.5 69.5 A comparison of CODEC and the methods of Collins et al.
and Gavin et al.
on the dataset of Gavin et al.
(2006).
The best result in each column appears in bold.
Table 3.
Comparison to Local Modeling Number of Specificity Sensitivity F-measure Complexes (%) (%) (%) CODEC using w0 185 80 85 82.5 CODEC using w1 180 79.5 81 80 Local Modeling 272 73 67 70 A comparison of CODEC to the Local Modeling approach on the dataset of Gavin (2002).
The best result in each column appears in bold.
(to 69%).
Overall, these results are comparable to those of CODEC, although providing a slightly worse F-measure (79% compared with CODECs 82.5%).
4 CONCLUSION We have provided a novel algorithm for identifying protein complexes from co-immunoprecipitation data, which is based on reformulating the problem as that of finding heavy subgraphs in a bipartite graph.
We have shown that our approach, which uses non-binary co-complex information, is superior to clustering methods that dissect binary proteinprotein interaction data.
Our algorithm was also shown to outperform existing approaches for inferring protein complexes from CoIP data.
All complex predictions made by CODEC can be found at http://www.cs.tau.ac.il/roded/CODEC/main.html.
An interesting open challenge is to combine yeast two-hybrid data into the inference process.
Such a combined approach is expected to become increasingly important as proteinprotein interaction databases continue to grow in size and species coverage.
Fig.4.
A comparison of protein complex identification approaches on the data of Gavin (2002).
See legend of Figure 2 for details.
Funding: Israel Science Foundation (grant no.
385/06) to R.S.
Conflict of Interest: none declared.
ABSTRACT Motivations: The design of RNA sequences folding into predefined secondary structures is a milestone for many synthetic biology and gene therapy studies.
Most of the current software uses similar local search strategies (i.e.
a random seed is progressively adapted to ac-quire the desired folding properties) and more importantly do not allow the user to control explicitly the nucleotide distribution such as the GC-content in their sequences.
However, the latter is an important criterion for large-scale applications as it could presumably be used to design sequences with better transcription rates and/or structural plasticity.
Results: In this article, we introduce IncaRNAtion, a novel algo-rithm to design RNA sequences folding into target secondary struc-tures with a predefined nucleotide distribution.
IncaRNAtion uses a global sampling approach and weighted sampling techniques.
We show that our approach is fast (i.e.
running time comparable or better than local search methods), seedless (we remove the bias of the seed in local search heuristics) and successfully generates high-quality sequences (i.e.
thermodynamically stable) for any GC-content.
To complete this study, we develop a hybrid method combining our global sampling approach with local search strategies.
Remarkably, our glocal methodology overcomes both local and global approaches for sampling sequences with a specific GC-content and target structure.
Availability: IncaRNAtion is available at csb.cs.mcgill.
ca/incarnation/ Contact: jeromew@cs.mcgill.ca or yann.ponty@lix.polytechnique.fr Supplementary Information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION At the core of the emerging field of synthetic biology resides, our capacity to design and reengineer molecules with target func-tions.
RNA molecules are well tailored for such applications.
The ease to synthesize them (they are directly transcribed from DNA) and the broad diversity of catalytic and regulation func-tions they can perform enable to integrate de novo logic circuits within living cells (Rodrigo et al., 2012) or reprogram existing regulation mechanisms (Chang et al., 2012).
Future advances and applications of these techniques in gene-therapy studies will strongly rely on efficient computational methods to design and reengineer RNA molecules.
Most of RNA functions are, at least partially, encoded by the 3D molecular structures, which are themselves primarily determined by the secondary structures.
The development of ef-ficient algorithms for designing RNA sequences with predefined secondary structures is thus a milestone to enter the synthetic biology era.
RNAinverse pioneered RNA secondary structure design algorithms.
It has been developed and distributed with the Vienna RNA package (Hofacker et al., 1994).
However, only posterior experimental studies revealed the potential and prac-tical impact of these techniques.
Thereby, during the past 6 years, many improvements and variants of RNAinverse have been proposed.
Conceptually, almost all of existing algorithms follow the same approach.
First a seed sequence is selected, then a local search strategy is used to mutate the seed and find, in its vicinity, a sequence with desired folding properties.
Using this strategy, INFO-RNA (Busch and Backofen, 2006), RNA-SSD (Aguirre-Hernandez et al., 2007) and NUPACK:Design (Zadeh et al., 2011) significantly improved the performance of RNA secondary structure design algorithms.
More recent research studies aimed to include more constraints in the selection criteria.
RNAexinv focused on the design of sequences with enhanced thermodynamical and mutational ro-bustness (Avihoo et al., 2011), while Frnakenstein enables to design RNA with multiple target structures (Lyngs et al., 2012).
We recently introduced with RNA-ensign a novel paradigm for the search strategy of RNA secondary structure design algo-rithm (Levin et al., 2012).
Instead of a local search approach, we proposed a global sampling strategy of the mutational landscape based on the RNAmutants algorithm (Waldispuhl et al., 2008).
This methodology offered promising performances, but suffered from prohibitive runtime and memory consumption.
Following our work, Garcia-Martin et al.
(2013) proposed RNAiFOLD, an alternate methodology that uses constraint programming tech-niques to prune the mutational landscape.
While also suffering from prohibitive running times, it is worth noting that this latter algorithm also proposes a seedless approach to the RNA second-ary structure design problem.
In this article, we introduce IncaRNAtion, an RNA second-ary structure design algorithm that benefits from our recent al-gorithmic advances (Reinharz et al., 2013) to expand our original RNA-ensign algorithm (Levin et al., 2012).
IncaRNAtion addresses previous limitations of RNA-ensign and offers new functionalities.
First, while our previous program had a running time complexity of On5, IncaRNAtion now runs in linear-time and space complexity, allowing it to demonstrate similar speeds as any local search algorithm.
Next, IncaRNAtion is seedless.
Unlike RNA-ensign, it does not require a seed se-quence to initiate its search.
Finally, IncaRNAtion implements a novel algorithm based on weighted sampling techniques*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com https://github.com/McGill-CSB/IncaRNAtion https://github.com/McGill-CSB/IncaRNAtion mailto:jeromew@cs.mcgill.ca mailto:yann.ponty@lix.polytechnique.fr(Bodini and Ponty, 2010) that enables us to control, for the first time, explicitly the GC-content of the solution.
This functionality is essential because wild-type sequences within living organisms often present medium or low GC-content, presumably to offer better transcription rates and/or structural plasticity.
Previous programs do not allow to control this parameter and tend to output sequences having high GC-contents (Lyngs et al., 2012).
We demonstrate the performance of our algorithms on a set of real RNA structures extracted from the RNA STRAND data-base (Andronescu et al., 2008).
To complete this study, we de-velop a hybrid method combining our global sampling approach with local search strategies such as the one implemented in RNAinverse.
Remarkably, our glocal methodology overcomes both local and global approaches for sampling sequences with a specific GC-content and target structure.
2 METHODS We introduce a probabilistic model for the design of RNA sequences with a specific GC-content and folding into a predefined secondary structure.
For the sake of simplicity, we choose to base this proof-of-concept im-plementation on a simplified free-energy function E, which only con-siders the contributions of stacked canonical base pairs.
We show how a modification of the dynamic programming scheme used in RNAmutants allows for the sampling of good and diverse design candidates, in linear time and space complexities.
2.1 Definitions A targeted secondary structure S of length n is given as a non-crossing arc-annotated sequence, where Si stands for the base-pairing position of position i in S if any (and, reciprocally, SS i i), or 1 otherwise.
In addition, let us denote by #gcs the number of occurrences of G and C in an RNA sequence s. 2.1.1 Simplified energy model We use a simplified free-energy model, which only includes additive contributions from stacking base pairs.
Using individual values from the Turner 2004 model [retrieved from the NNDB (Turner and Mathews, 2010)].
Given a candidate se-quence s for a secondary structure S, the free energy of any sequence s of length jSj is given by Es,S X i, j!i0 , j0 2S stacking pairs Esisj!si0 sj0 where Eab!a0b0 is set to 0 if ab ; (no base pair to stack onto), the tabulated free energy of stacking pairs ab=a0b0 in the Turner model if available, or 2 0,1 for nonWatson-Crick/Wobble pairs (i.e.
not in fGU,UG,CG,GC,AU or UAg).
This latter parameter allows one to choose whether to simply penalize invalid base pairs (40), or forbid them altogether ( 1).
Position-specific sequence constraints can also be enforced at this level (details omitted for the sake of clarity) by assigning to E a 1 penalty (leading to a null probability) in the presence of a base incompatible with a user-specified constraint mask.
2.1.2 GC-weighted Boltzmann ensemble and distribution To counterbalance the documented tendency of sampling methods to gener-ate GC-rich sequences (Levin et al., 2012), we introduce a parameter x 2 R, whose value will influence the GC-content of generated se-quences.
For any secondary structure S, the GC-weighted Boltzmann factor of a sequence s is BxS s such that BxS s e Es,S RT x#gcs 1 where R is the Boltzmann constant and T the temperature in Kelvin.
Summing the GC-weighted Boltzmann factor over all possible se-quences of a given length jSj, one obtains the GC-weighted partition function ZxS , from which one defines the GC-weighted Boltzmann prob-ability PxS s of each sequence s, respectively, such that ZxS X jsjn BxS s and P x S s BxS s ZxS : 2 2.2 Linear-time stochastic sampling algorithm for the GC-weighted Boltzmann ensemble Let us now describe a linear-time algorithm to sample sequences at random in the GC-weighted Boltzmann distribution.
This algorithm fol-lows the general principles of the recursive approach to random gener-ation (Wilf, 1977), pioneered in the context of RNA by the SFold algorithm (Ding and Lawrence, 2003).
The algorithm starts by precom-puting the partition function restricted to each substructure occurring in the target structure, and then performs a series of recursive stochastic backtracks, using precomputed values to decide on the probability of each alternative.
2.2.1 Precomputing the GC-weighted partition function Firstly, a dynamic programming algorithm computes Za, bN,S the GC-weighted par-tition function (the dependency in x is omitted here for the sake of clarity) for a structure S, assuming its (previously chosen) flanking nucleotides are a and b, respectively, either forming a closing base pair (N T) or not (N F).
Remark that the empty structure only supports the empty se-quence, having energy 0, so one has Za, bT, " Z a, b F, " e0=RT 1: 3 The general recursion scheme consists in three different terms, depend-ing on the first position in S: Case 1.
First position is unpaired (S S0): Za, bT, S0 Z a, b F, S0 : X a02B x#gca 0  Za 0 , b F,S0 4 Case 2.
First position is paired with last position [S S0], stacking onto a preexisting exterior pair (N T): Za, bT, S0 : X a0 , b02B2 x#gca 0 :b0  e E ab!a0b0 RT Za 0 , b0 T,S0 5 Case 3.
First position is involved in a base pair [S S0S00], which is not stacking onto an exterior base pair (N F or S00 6 "): Za, bN, S0 S00 : X a0 , b02B2 x#gca 0 :b0  e E ;!a0b0 RT Za 0 , b0 T,S0 Z b0 , b F,S00 6 Remark that the number of combinations of a, b and N re-mains bounded by a constant, thus the complexity of computing Za, bN,S mainly depends on the values taken by S on subsequent recursive calls.
Such values are entirely determined by S at any given step of the recursion, and their dependency can be summarized in a tree having jSj.
Therefore, the computation of Za, bN,S requires n time and space using dynamic programming.
i309 Constrained RNA design using a weighted sampling algorithm 2.2.2 Stochastic backtrack Once the GC-weighted partition func-tions have been computed and memorized, a stochastic backtrack starts from the target structure S with any exterior bases a, b and no nesting base pair, corresponding to a call SBx ;,;,F,S to Algorithm 1.
At each step, a suitable assignment for one or several positions is chosen, using probabilities derived from the precomputation, as illustrated by Figure 1.
One or several recursive calls over the appropriate substructures are then performed.
On each recursive call, the algorithm assigns at least 1 nt to a previously unassigned position.
Moreover, the number of exe-cutions of each loop is bounded by a constant.
Consequently, the com-plexity of Algorithm 1 is in n time and space.
2.2.3 Self-adaptive sampling strategy Let us remind that our goal is to produce a set of sequences whose GC-content matches a prescribed value gc.
An absolute tolerance may be allowed, so that the GC-content of any valid sequence must fall in gc , gc .
Because sequences of arbitrary GC-content may be generated by Algorithm 1, we use a rejec-tion-based approach (Bodini and Ponty, 2010), previously adapted by the authors in a similar context (Waldispuhl and Ponty, 2011).
This gives an algorithm that generates k valid sequences in expected time k n ffiffiffi n p when  0 [or k n when is a positive constant] and memory in k n. A complete analysis of the rejection process can be found in an earlier contribution (Waldispuhl and Ponty, 2011), but let us briefly out-line the approach, and the main arguments used to establish its complexity.
As summarized by Figure 2, our adaptive sampling approach simply generates sets of sequences by repeatedly running the stochastic backtrack algorithm.
The average GC-content induced by the current value of the x parameter can then be adequately estimated from the sample, or com-puted exactly using recent algorithmic advances (Ponty and Saule, 2011).
The set of sequences is filtered to only retain valid sequences.
The value of the parameter x is then adapted to match the average GC-content (induced by the value of x) with the targeted one.
It can be shown that the expected GC-content is a continuous and strictly increasing mono-tonic function of x, whose limits are 0 when x 0 and n when x!
1.
Consequently, for any targeted GC-content gc 2 0%, 100%, there exists a unique value xgc such that generated sequences feature, on the average, the right GC-content.
In practice, a simple binary search (Waldispuhl and Ponty, 2011) is used in our implementation, and typically converges after few iterations.
An optimal value for x can also be derived analytically using interpolation after n evaluations of Za, bi, j for different candidate values of x, as previously noted (Waldispuhl and Ponty, 2011), and could be implemented using the Fast-Fourier Transform (Senter et al., 2012).
2.2.4 Overall complexity It was previously established (Waldispuhl and Ponty, 2011) that, for each value of x, there exists constants x and x such that the distribution of GC-content asymptotically converges toward a normal law having expectation in x n 1 o1 and stand-ard deviation in x ffiffiffi n p 1 o1.
Furthermore, the distribution of GC-content is highly concentrated, as asserted by its limited standard deviation; therefore, the expected number of attempts required to gener-ate a valid sequence when  0 [respectively 2 1= ffiffiffi n p ] grows like ffiffiffi n p [respectively 1, i.e.
a constant], leading to the announced com-plexities.
Formally, because a suitable weight x must be recomputed for each targeted structure and GC-content, then the number M of iterations Fig.1.
Stochastic backtrack procedure for a given substructure S. Either the first position is left unpaired (top), a base pair is formed between the two extremities, stacking onto an exterior base pair (middle) or paired without creating a stacking, defining two regions on which subsequent recursive calls are needed (bottom).
For the empty structure (omitted here), the empty sequence is returned.
Positions indicated in red are assigned at the current stage of the backtrack i310 V.Reinharz et al.
required for the converge can be accounted for explicitly, leading to time complexities in M ffiffiffi n p  k n (if  0, i.e.
without any tolerance) and M k n (if 40).
2.3 Postprocessing unpaired regions: a local/global (glocal) hybrid approach Owing to our simplified energy model, unpaired regions are not subject to design constraints other than the GC-content, leading to modest prob-abilities for refolded design candidates to match the targeted structure.
To improve these performances and test the complementarity of our global sampling approach with previous contributions based on local search, we used the RNAinverse software to redesign unpaired regions.
We speci-fied a constraint mask to prevent stacking base pairs from being modified and, whenever necessary, reestablished their content a posteriori, as RNAinverse has been witnessed to take some liberties with constraint masks.
As shown in the Supplementary Material, this postprocessing does not drastically alter the GC-content, so the glocal approach reason-ably addresses the constrained GC-content design problem.
3 RESULTS 3.1 Implementation Our software, IncaRNAtion, was implemented in Python 2.7.
We used RNAinverse from the Vienna Package 2.0 (Hofacker et al., 1994).
All-time benchmarks were run on a single AMD Opteron(tm) 6278 Processor at 2.4GHz with cache of 512 kb.
The penalty , associated with invalid base pairs, was set to 15.
Figure 3 presents the average times spent running IncaRNAtionRNAinverse to generate one sequence with the required GC-content.
As expected, the time grows linearly in function of the length of the structures for IncaRNAtion.
3.2 Dataset To evaluate the quality of our method, we used secondary struc-tures from the RNA STRAND database (Andronescu et al., 2008).
Those are known secondary structures from a variety of organisms.
We considered a subset of 50 structures selected by Levin et al.
(2012), whose length ranges between 20 and 100nt.
To ease the visualization of results, we clustered together struc-tures having similar length, stacks density and proportion of free nucleotides in loops, leading to distributions of structures shown in Figure 4.
3.3 Design We ran our method as follows.
First, we sampled approximately 100 sequences per structure.
Then, we use these sequences as seed in RNAinverse.
Finally, we computed the Minimal Free-Energy (MFE) with the RNAfold program from the Vienna Package 2.0 (Hofacker et al., 1994).
Before starting our benchmark, we asses the need for our methods and performed an analysis of the GC-content drift achieved with state-of-the-art software.
Using our dataset of 50 structures, we generated 100 samples per structure with classical softwares that do not control the GC-content.
Namely, RNAinverse, INFO-RNA, NUPACK:Design and Frnaken-stein.
We show the distribution of the GC-content of the se-quences produced with these softwares in Figure 5.
As anticipated, we observe a clear bias toward high GC-con-tents and a complete absence of sequence with530% of GC.
This striking result motivates a need for methods that enable to explicitly control the GC-content and more precisely that enable to design sequences with low GC-content (i.e.
30%).
To pro-vide a complete overview of the performance of IncaRNAtion, we provide additional statistics for these softwares in the Supplementary Material.
3.4 Success rate We started by estimating the success rate of our methodology and computed the percentage of sequences with a MFE structure identical to the target secondary structure.
Figure 6 shows our results.
We clearly see that before the postprocessing step (i.e.
RNAinverse) the sequences sampled by IncaRNAtion have a Fig.2.
General workflow of our adaptive sampling algorithm (Waldispuhl and Ponty, 2011) Fig.3.
Average time in seconds to generate one sequence for IncaRNAtion and RNAinverse i311 Constrained RNA design using a weighted sampling algorithm low success rate (first row).
As mentioned earlier, this could be explained by the fact that no selection criterion has been at this stage applied to unpaired nucleotides.
Remarkably, after the local search optimization (with RNAinverse) of nucleotides in unpaired regions (second row), we observe a dramatic improve-ment of our success rate.
As expected, we observed that length is, in general, not a good predictor for the hardness of designing a structure.
Instead, a high number of free nucleotides in the struc-ture seems to be a good measure of the hardness of its design.
Similarly, these data also show that designing sequences with low GC-content is challenging for all types of targets.
We investigated further the quality of the sequences generated by IncaRNAtion.
In particular, we estimated the capacity of our methods to generate good sequences with desired folding capabilities regardless of the property to fold exactly into the target structure.
In Figure 7, we show the ratio of well-predicted base pairs in the MFE structure of our sampled sequences.
As above, we can observe that, in all cases, the sequences that are the hardest to design are those with an extremely low GC-con-tent.
Indeed, the energetic contribution of the base pairs to the stability of the structure is weaker.
Interestingly, we also notice that the most accurate sequences yield a GC-content of 70 10%.
Overall, we observe that all our samples have good folding properties, and that there is a correlation between the precision of the samples and the hardness of the design.
We noticed a highly decreased structural sensitivity for the sequences with 15% free nucleotides in the loops.
However, one must remain careful interpreting this observation, as the structures within this class all originate from the PDB, and are relatively small (for the complete STRAND DB, the average length is 	 526 nt, compared with 	 38 nt around 15% unpaired bases).
3.5 Properties of designed sequences In this section, we further analyze the generated sequences with a MFE structure that folds into the target structure.
A desirable feature in sequence design is to produce samples with a high sequence diversity and stable secondary structure.
Therefore, in the following, we will use two useful measures, which are the sequence identity of the samples, and the Boltzmann probability of the target structure in the low-energy ensemble.
The sequence identity is defined over a set S of aligned se-quences (in our case, all sequences have the same length and can be trivially aligned) as follows: X s1, s22S S 1 js1j X i s1 i s2 i 1 0 BB@ 1 CCASeq:identity 7 where si is the nucleotide at position i in sequence s. Intuitively, this measure captures the diversity of sequences generated by a given method.
Next, the Boltzmann frequency is defined for a structure S and a sequence s as follows: e Es,S RT =ZsFrequency 8 where Zs is the partition function of sequence s. This measure tells us how dominant is a structure S in the Boltzmann ensemble of structures over a sequence s. A high value implies a stable structure.
We compute this frequency with RNAfold from the Vienna Package 2.0 (Hofacker et al., 1994).
Figure 8 shows the number of solutions generated (i.e.
se-quences with a MFE structure identical to the target structure).
Here, we note that low GC-contents have a strong (negative) influence on the number of sequences generated, and in parallel also affect negatively the sequence diversity.
This observation emphasizes the difficulty to design sequences with low GC-con-tent.
Once again, large percentages of free nucleotides increase the difficulty of the task.
The thermodynamical stability of the target structure on the designed sequence is another important property when estimat-ing the performance of RNA design algorithms.
We estimate the quality of our solutions in Figure 9.
First, we observe a slow Fig.5.
Overall GC-content distribution for sequences designed using RNAinverse, INFO-RNA, NUPACK:Design and Frnakenstein folding in the desired structure Fig.4.
Number of secondary structures per bin, according to our three clustering criteria i312 V.Reinharz et al.
Fig.8.
Number of solutions generated with IncaRNAtionRNAinverse on the first row and their average sequence identity on the second Fig.6.
Success rate IncaRNAtion before and after RNAinverse postprocessing.
The first row shows the percentage of sampled sequences folding into the target when using only IncaRNAtion.
The second shows after processing previous results with RNAinverse Fig.7.
Structural sensitivity (i.e.
number of well predicted base pairs/number of base pairs in target) of the sampled sequences MFE Fig.9.
Thermodynamical stability of the target structure.
The curves report the average Boltzmann probability of the target structure (which is also the MFE structure) at various GC-contents with respect to the length of the target (left), density of stacked base pairs (centre) and number of unpaired nucleotides in loops (right) i313 Constrained RNA design using a weighted sampling algorithm decline of the structure stability (i.e.
the frequency) when the target structure increases in size.
Yet, for an average GC-content, the frequency stays410% even at size of 100 nt.
Next, we note that for the most difficult target structures (i.e.
the longer ones or those with high percentages of unpaired nucleotides in loops) the GC-content has a limited (almost null) influence on the stability of the target structure on the designed sequence.
By contrast, this is less true for easiest and small structures with only few free nucleotides in internal loops.
3.6 Global sampling versus Local search versus Glocal approach To conclude this study, we estimate the impact of the design methodology on the performances.
More precisely, we aim to de-termine the merits of a global sampling approach (IncaRNA tion), compared with a glocal procedure (IncaRNAtion RNAinverse) and a local search methodology (RNA-SSD).
To the best of our knowledge, RNA-SSD, beside IncaRNAtion, is the only software that implements an explicit control of the GC-content.
Here, we compare the running time and the sequence diversity of the solutions produced by each software.
In addition, we focus on the design of sequences with low GC-contents (30%) as they are almost impossible to design with classical software (Figure 5).
Figure 3 shows the running time of each software.
These data demonstrate the efficiency and scalability of our techniques.
In particular, this figure suggests that our strategy has the potential to be applied efficiently for designing sequences on long (and difficult) target secondary structures at low GC-contenta task that could have not been achieved before due time requirements.
Next, we show in Figure 10 the average sequence identity achieved by the various methods.
Our results show that at ex-tremely low GC-contents (i.e.
10%), IncaRNAtion slightly out-performs RNA-SSD, while this advantage becomes less evident when the GC-content increases.
Our experiments on higher GC-contents (i.e.
50%) showed that our glocal strategy and the local search approach perform similarly.
Similarly, we did not find any clear evidence that a global, local or glocal approach outperforms others when we compare at the thermodynamical stability of the target structure (data not shown).
4 CONCLUSION In this article, we described a novel algorithm, IncaRNAtion, for the RNA secondary structure design problem, i.e.
the design of an RNA sequence adopting a predefined secondary structure as its minimal free-energy fold.
Implementing a global sampling approach, it optimizes affinity toward the target secondary struc-ture, while granting the user full control over the GC-content of the resulting sequences.
This extended control does not necessar-ily induce additional computational demands, and we showed the linear complexity of both the preprocessing stage and the generation of candidate sequences for the design, allowing for the design of larger and more complex secondary structures in a matter of minutes on a single processor (e.g.
28min for 100 candidate sequences for a 	1500nt 16S rRNA).
We evaluated the method on a benchmark composed of target secondary struc-tures extracted from the RNA STRAND database.
We observed good overall success rate, with the notable exception of very low targeted GC-content (10%), and a good to excellent entropy within designed candidates.
Finally, we implemented a hybrid approach, using the RNAinverse software as a postprocessing step for unpaired regions.
This approach greatly increased the success rate of the method, allowing for the design of highly diverse candidates for almost all of the structures in our bench-mark, while largely preserving the targeted GC-content.
In the future, we would like to complement this study by fur-ther investigating the potential of hybrid local/globalor glocalapproaches.
A global sampling approach would capture the positive aspects of design, optimizing affinity toward a given structure while allowing the specification of expressive systems of constraints.
Designed sequences would serve as a seed for a Fig.10.
Sequence identity of IncaRNAtion and RNAinverse for 10 and 30% of GC i314 V.Reinharz et al.
restricted local approach which, by breaking unwanted symme-tries, would perform the negative part of the design, while ideally maintaining obedience to the constraints.
Another perspective of this work is the incorporation of the full Turner energy model, which should in principle yield better designs for unpaired regions.
ACKNOWLEDGMENTS The authors would like to thank Rob Knight for his suggestions and comments.
Funding: This work was funded by the French Agence Nationale de la Recherche (ANR) through the [check] Magnum ANR 2010 BLAN 0204 project (to Y.P.
), the FQRNT team grant 232983 (to V.R.
and J.W.)
and NSERC Discovery grant 219671 (to J.W.).
This article has been developed as a result of a mobility stay funded by the Erasmus Mundus Programme of the European Commission under the Transatlantic Partnership for Excellence in Engineering TEE Project (V.R.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Structural variants, including duplications, insertions, deletions and inversions of large blocks of DNA sequence, are an important contributor to human genome variation.
Measuring structural variants in a genome sequence is typically more challenging than measuring single nucleotide changes.
Current approaches for structural variant identification, including paired-end DNA sequencing/mapping and array comparative genomic hybridization (aCGH), do not identify the boundaries of variants precisely.
Consequently, most reported human structural variants are poorly defined and not readily compared across different studies and measurement techniques.
Results: We introduce Geometric Analysis of Structural Variants (GASV), a geometric approach for identification, classification and comparison of structural variants.
This approach represents the uncertainty in measurement of a structural variant as a polygon in the plane, and identifies measurements supporting the same variant by computing intersections of polygons.
We derive a computational geometry algorithm to efficiently identify all such intersections.
We apply GASV to sequencing data from nine individual human genomes and several cancer genomes.
We obtain better localization of the boundaries of structural variants, distinguish genetic from putative somatic structural variants in cancer genomes, and integrate aCGH and paired-end sequencing measurements of structural variants.
This work presents the first general framework for comparing structural variants across multiple samples and measurement techniques, and will be useful for studies of both genetic structural variants and somatic rearrangements in cancer.
Availability: http://cs.brown.edu/people/braphael/software.html Contact: braphael@brown.edu 1 INTRODUCTION Characterizing the DNA sequence differences that distinguish individuals is a major challenge in human genetics.
Until recently, these differences were thought to be mostly single nucleotide changes.
There is increasing appreciation for the prevalence of structural variation, including duplications, deletions and inversions of large blocks of DNA sequence, in the human genome (Sharp et al., 2006).
Structural variants have recently been linked to diseases such as autism (Marshall et al., 2008), and cataloging these variants is an important step in determining the genetic basis of disease.
To whom correspondence should be addressed.
Structural variants typically span thousands of nucleotides and are more difficult to define than single nucleotide polymorphisms (SNPs).
Two techniques have been used to identify structural variants in the human genome: array comparative genomic hybridization (aCGH) and end sequence profiling (ESP), also called paired-end mapping.
Both of these techniques were first developed for the analysis of somatic structural rearrangements in cancer genomes (Pinkel and Albertson, 2005; Pinkel et al., 1998; Volik et al., 2003) and later applied to discover structural variation in normal genomes (Iafrate et al., 2004; Kidd et al., 2008; Korbel et al., 2007; Sebat et al., 2004; Tuzun et al., 2005).
In aCGH, differentially fluorescently labeled DNA from test and reference genomes are hybridized to an array of genomic probes derived from the reference genome.
Measurements of test:reference fluorescence ratio at each probe identifies locations of the test genome that are present in higher or lower copy in the reference genome.
This technique detects copy number variants but is blind to copy neutral variants such as inversions.
In paired-end mapping approaches, DNA fragments, or clones, from a test genome are sequenced from both ends, and these sequences are mapped to a reference genome sequence.
Pairs of end sequences, called end sequence pairs or mate pairs, with discordant mappings identify inversions, translocations, transpositions, duplications and deletions that distinguish the test genome from the reference genome.
Next-generation DNA sequencing technologies such as those from Illumina, Applied Biosystems and 454 Life Sciences now make it possible to apply this approach to a large number of individuals.
We distinguish paired-end mapping from whole-genome assembly, which would provide the ultimate dataset for studies of structural variation (Levy et al., 2007; Wheeler et al., 2008), but remains cost prohibitive for a large number of human genomes.
Both aCGH and paired-end mapping do not precisely identify the boundaries, or breakpoints, of the measured variant.
In aCGH, a breakpoint is localized only to the distance between the genomic probes straddling the copy number change, while in paired-end mapping the localization depends on the number and size of fragments that span the variant (Fig.1A).
There are no standard methods for identifying the boundaries of structural variants in paired-end mapping studies and different heuristics have been used (Kidd et al., 2008; Korbel et al., 2007).
Remarkably, despite ambiguity resulting from both measurement and analysis, published studies of structural variants report the breakpoints to single nucleotide resolution without revealing the measurement uncertainty, or error bars, in the localization of the variant.
Consequently, the existing databases of human structural variants 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i223 i222i230 Geometric structural variation z Fig.1.
Derivation of regions of uncertainty (breakpoint regions) from ESP and aCGH data.
(A) (Top panel) In ESP, or paired-end mapping, both ends of a fragment of a test genome are sequenced and aligned to the reference genome.
Here, alignment of ends of fragments C and D yields ES pairs (xC ,yC ) and (xD,yD) on the reference genome that suggest an inversion.
(Bottom panel) The intersection of breakpoint regions defined by Equation (1) indicates the possible locations of inversion breakpoints a and b that are consistent with the ES pairs.
(B) (Top panel) In an aCGH experiment, the reference genome is segmented into regions of equal copy number according to measurements at genomic probes (boxes).
A deletion with breakpoints a and b is identified as a change in copy number between probes pi and pi+1 and between probes pj and pj+1.
(Bottom panel) The intervals [pi,pi+1] and [ pj,pj+1 ] define a rectangular breakpoint region.
This region is intersected with the breakpoint region defined by an ES pair (xC ,yC ) to refine the locations a and b of the deletion.
(Iafrate et al., 2004) also do not contain information on the uncertainty of the breakpoints.
Each new study of structural variation compares the newly discovered variants to those previously reported.
In addition to the ambiguities described above, there is also the problem of deciding when two variants (perhaps measured via different approaches) are the same.
The usual approach is to define two variants to be the same if they are near each other, where near is defined using an arbitrary and study-dependent threshold.
With such an approach, there is no assurance that the two variants are indeed the same, or are merely closely located on the genome.
The situation is further exacerbated by reports that different human structural variants may overlap or have multiple states (Perry et al., 2008; Scherer et al., 2007), and that recurrent (but not identical) variants may exist at the same locus.
Standard methods for defining structural variants and publicly available tools for comparing structural variants across different studies are urgently needed.
Two recent works introduced more refined approaches for analysis of structural variants and are promising steps in this direction.
Lee et al.
(2008) describe a probabilistic method for resolving ambiguities in mapping end-sequenced fragments using the distribution of fragment lengths in a single sample.
Bashir et al.
(2008) estimate the probability that paired-end sequenced clones from cancer genomes contain fusion genes and explicitly incorporate the uncertainty in measurement of rearrangement breakpoint into their calculation.
Neither of these approaches address the comparison of variants across multiple samples, and are further limited in their handling of measurement uncertainty and consideration of all classes of structural variants, respectively.
Here, we introduce a general geometric framework for classification and comparison of structural variants.
Our approach provides a principled way to cluster multiple measurements of a variant in a single sample and to compare variants across samples.
We explicitly model the underlying measurement uncertainty of both paired-end mapping (from both older and next-generation sequencing technologies) and aCGH.
We represent the uncertainty in the measurement of a structural variant, which we refer to as the breakpoint region, as a polygon in the plane.
We formulate the problems of comparing variants as computing all intersections and maximal intersections of breakpoint regions.
These formulations allow the user to examine conserved variants at varying levels of granularity, instead of only producing a single best cluster of overlapping variants.
We derive an efficient plane sweep algorithm from computational geometry to compute these intersections.
We demonstrate our Geometric Analysis of Structural Variants (GASV) program with three applications.
First, we apply our method to recent paired-end sequencing studies of nine human individuals.
We show that GASV identifies rearrangement breakpoints with high precision.
In dozens of cases, we localize rearrangement breakpoints to <2.5 kb by combining the measurements from 40 kb clones across multiple individuals.
In the most extreme example, eight end-sequenced clones from four different individuals localize the inversion breakpoints to within 286 bp.
Such precise localization was not reported in the original published analysis of these nine individuals.
Moreover, we show that the published locations of many variants are different from the breakpoints supported by the data.
Second, we perform a comparative analysis of variants from the nine normal individuals with variants identified in paired-end sequencing of several cancer samples.
We find that a significant fraction (553%) of rearrangements identified in the cancer genomes are consistent with inversion and deletion variants found in the normal genomes.
Finally, we show how GASV integrates both aCGH and paired-end sequencing measurements of variants in three cancer genomes.
Our geometric method for multi-sample and multi-platform identification and comparison of structural variants should prove useful for studies of human structural variation such as the 1000 Genomes Project and for cancer genome sequencing studies such as The Cancer Genome Atlas.
2 METHODS Consider a reference genome represented as a single interval G (i.e.
we concatenate multiple chromosomes) and a closely related test genome.
We define a structural variant to be a difference between a test genome i223 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i224 i222i230 S.Sindi et al.
and reference genome that is due to a rearrangement resulting from DNA breakage followed by a aberrant repair or insertion of a new DNA.
Structural variants include inversions, translocations, transpositions, and insertions/deletions.
Each of these variants is thus associated with a set of breakpoints where DNA breaks and/or repair occurs.
For example, an inversion is a result of the reference genome being cut at two genomic coordinates, a and b, and the DNA segment between a and b flipped in the test genome so that the nucleotide at position a1 is adjacent to the nucleotide at position b and a is adjacent to b+1 (Fig.1A).
Similarly, a deletion is defined by coordinates a and b in the reference such that a1 is joined to b+1 in the test genome (Fig.1B).
Note that this is a simplification of the underlying biology, as there are sometimes small insertions or deletions at breakpoints, but these small changes have limited effect on the analysis of larger structural variants.
2.1 Breakpoint regions and variant uncertainty Neither paired-end mapping nor aCGH measure the breakpoints of a structural variant exactly.
Rather, each technique localizes breakpoints to a region of the reference genome, which we refer to as the breakpoint region.
We describe the derivation of this region for each of these experimental techniques.
2.1.1 Paired-end mapping In the paired-end mapping, or ESP, fragments of genomic DNA from a test genome are sequenced from both ends, and the resulting pair of end sequences are aligned to the reference genome.
We assume that each fragment1 C has ends that map uniquely to the reference genome.
Thus, each fragment C corresponds to a pair of locations in the reference genome where the end sequences map.
An end sequence may align to either DNA strand, and so each mapped end has a sign (+ or ) indicating the mapped strand.
We call such a signed pair (xC ,yC ) an end sequence pair (ES pair), where by convention |xC |< |yC |.
Typically, the length of the fragment, LC , is known to lie within a range [Lmin,Lmax].
Fragment sizes range from 150 kb for BAC clones to a few hundred base pairs for next-generation sequencing methods.
We say that a ES pair is a valid pair (Raphael et al., 2003) if the ends have opposite, convergent orientations and the distance between the mapped ends is within the range of fragment lengths: i.e.
(+xC ,yC ) is valid if Lmin |y||x|Lmax.
Otherwise, if the ends have abnormal distance or orientation, we say that the pair is an invalid pair.
Invalid pairs indicate putative genome rearrangements or possibly mapping/assembly errors.
For concreteness, consider the case of a test genome that differs from the reference genome by a single inversion with breakpoints a and b (Fig.1A) that fuse at a single coordinate in the test genome.
A fragment C from the test genome with length between Lmin and Lmax and containing is end-sequenced.
The resulting ES pair (xC ,yC ) will be an invalid pair indicating that C is not a contiguous piece of the reference genome (Fig.1A).
The invalid pair (xC ,yC ) does not uniquely identify the breakpoint (a,b).
However, if we assume that: (i) only a single breakpoint is contained in the fragment C; and (ii) a>xC and b>yC (without loss of generality); then the length LC of C is equal to (axC )+(byC ).
Thus, a breakpoint (a,b) that is consistent with (xC ,yC ) must satisfy Lmin (axC )+(byC )Lmax.
(1) We define the breakpoint region B(C) of an invalid fragment C to be the breakpoints (a,b) satisfying the above equation.
The constraint (1) has a straightforward geometric interpretation: if we plot an invalid pair (xC ,yC ) as a point in the 2D space GG then the breakpoint region defines a trapezoid (Fig.1A).
We emphasize that a and b cannot be chosen independently; doing so corresponds to defining the breakpoint region to be a rectangle, and allows breakpoints that give insert sizes outside the allowed range [Lmin,Lmax].
1We use the term fragment to describe both genomic clones (BACs, fosmids, plasmids) used by older sequencing technologies and DNA fragments in the mate-pair libraries employed in next-generation sequencing technologies.
If another fragment D contains the same fusion point , then the corresponding breakpoint (a,b) lies within the intersection B(C)B(D) of the trapezoids B(C) and B(D) (Fig.1A).
Conversely, we will assume that if the trapezoids defined by several invalid pairs intersect, then they share a common breakpoint.
As the number of fragments that are end-sequenced increases, more fragments will contain the same fusion point and the area of the intersection of breakpoint regions will decrease.
Thus, the uncertainty in the location of the breakpoint (a,b) decreases.
We define a cluster to be a set of fragments whose breakpoint regions have non-empty intersection.
The description above generalizes to other types of structural variants including translocations, insertions, deletions and transpositions.
For example, invalid pairs with sign(xC )=sign(yC )= also indicate inversions (corresponding to the other fusion point), while invalid pairs with sign(xC )= + and sign(yC )= indicate insertions or deletions.
Fragments with ends mapped to different chromosomes indicate translocations.
As above, we assume that the fragment C contains only a single breakpoint.
The breakpoints (a,b) that are consistent with the invalid pair (xC ,yC ) satisfy the inequalities Lmin (sign(xC )axC )+(sign(yC )byC )Lmax (2) This equation generalizes (1) and is summarized by the rule: end sequences point toward the breakpoint.
2.1.2 Array comparative genomic hybridization In aCGH, a breakpoint region is defined as the genomic interval between the two adjacent probes pi and pi+1 that define the endpoints of segments with unequal copy number (Fig.1B).
A pair of such breakpoint regions (e.g.
those resulting from a deletion) give two intervals U = [pi,pi+1] and V = [ pj,pj+1 ] that define a rectangle U V in 2D space GG.
This rectangle determines the locations of breakpoints (a,b)U V consistent with the segmentation.
Note that in addition to fragments that span deletions (Fig.1B), the boundaries of aCGH segments often indicate the locations of other types of rearrangements including translocations (Aerni et al., 2009; Campbell et al., 2008).
2.2 Efficient computation of overlapping breakpoint regions Given a set B1,...,Bn of breakpoint regions, our goal is to identify subsets of intersecting breakpoint regions.
Such a subset suggests these breakpoint regions are multiple measurements of the same structural variant.
In addition, we want to identify all such regions of intersection, and to label these by the breakpoint regions that are part of the intersection.
We formalize these problems as follows: All Intersections of Breakpoint Regions.
Given a set B={B1,...,Bn} of breakpoint regions, identify and label all non-empty intersections of subsets of B.
Since each breakpoint region Bi is a convex polygon (trapezoid or rectangle), the solution to the above problem relies on computing intersections of convex polygons, a well-known problem in computational geometry (Preparata and Shamos, 1985).
A naive brute-force approach that checks all 2n subsets of B for intersection is very inefficient.
Moreover, a single breakpoint region can have distinct intersections with different subsets of other breakpoint regions (Fig.2).
Thus, it is not sufficient to consider only pairwise intersections or iteratively merge breakpoint regions to existing intersections.
Below, we describe an efficient plane sweep algorithm that solves the All Intersections Problem.
While the All Intersections Problem provides the most comprehensive description of the overlaps between breakpoint regions, the output can be quite large since the number of regions of intersections grows rapidly as n increases.
However, many of these regions of intersection are not interesting because they are dominated by intersections of a larger number of breakpoint regions.
For example, if three breakpoint regions Bi, Bj and Bk have a non-empty intersection, then reporting this intersection is perhaps more desirable i224 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i225 i222i230 Geometric structural variation than reporting the (geometrically larger) intersections Bi Bj and Bi Bk , particularly as the number of such intersecting regions becomes large.
Thus, it is desired to identify regions of intersection of a maximal number of Bi.
We formalize this problem by defining a partial order on intersections of subsets of B.
For S {1,...,n}, let BS =sSBs denote the intersection of the breakpoint regions indexed by S. Let In be the set of subsets of {1,...,n} whose corresponding breakpoint regions have non-empty intersection.
Formally, In ={S {1,...n}|BS =}.
(3) In has the natural partial order of subset inclusion , where for two elements I and J of In, I J provided I J .
We denote this partially ordered set (poset) as (In,).
We formalize the problem as follows.
Maximal Intersections of Breakpoint Regions.
Given a set B= {B1,...,Bn} of breakpoint regions, identify all maximal elements of (In,).
Below, we describe how to solve the Maximal Intersections Problem by extending the plane sweep algorithm for the All Intersections Problem.
2.3 Plane sweep algorithm The plane sweep algorithm was introduced by Shamos and Hoey (1976) for the problem of determining whether n line segments in the plane have any intersections.
Clearly this question can be answered in O(n2) time by checking all pairs of segments for intersection.
A plane sweep algorithm performs the same task in O(nlogn) time by first sorting the segments by the x-coordinate of their left endpoint, and then moving the line x=c, called the sweep line through the plane from left to right.
The efficiency of the plane Fig.2.
Breakpoint regions determined by fragments from Kidd et al.
(2008) whose orientations suggest an inversion variant(s).
Breakpoint region 2 has distinct intersections with regions 1 and 3, and thus iterative merging of breakpoint regions will not identify all intersections.
sweep algorithm is derived from two observations.
First, not all coordinates c need to be considered.
A data structure called the event-point schedule E records the necessary values of c, and is updated dynamically as the sweep line moves from left to right.
Second, for a given position c of the sweep line, the segments intersecting the sweep line can be ordered by the y-coordinate of the intersection.
These ordered segments are stored in a data structure called sweep-line status L. Only adjacent segments in L need to be examined for intersections.
By employing appropriate data structures for E and L one obtains an efficient algorithm for segment intersection.
Further details of this algorithm can be found in Preparata and Shamos (1985).
The basic framework of the plane sweep algorithm has been extended to numerous related problems in computational geometry such as the counting of the k intersections of n segments in provably optimal O(nlogn+k) time (Chazelle and Edelsbrunner, 1992), and reporting the regions of intersection of polygons in the plane (Nievergelt and Preparata, 1982).
Here, we modify the algorithm of Nievergelt and Preparata (1982) to solve the All Intersections and Maximal Intersection problems described above.
Our extension exploits the particular geometry of the trapezoids and rectangles that define breakpoint regions in order to: (i) efficiently compute their intersection; (ii) label the intersecting regions by the breakpoint regions that are inside; and (iii) iteratively determine the maximal elements of (In,).
2.3.1 Overview of the algorithm We provide an overview of the plane sweep algorithm for the case of breakpoint regions defined by inversion variants; i.e.
fragments with parallel orientations (+,+) or (,).
These breakpoint regions are trapezoids with the two parallel sides having slope 1 (Fig.1A).
Thus, we define the sweep line to be a line y=x+c of slope 1.
The sweep line will encounter all inversion breakpoint regions as c increases from cmin to cmax.
The algorithm is identical for insertion/deletion variants except the sweep line is chosen to have slope +1, y=x+c, to match the parallel sides of the trapezoids in this case (Fig.1B).
As the sweep line advances through the plane, one of three possible events (Fig.3) can occur: (i) addition of a breakpoint region; (ii) intersection between two line segments defining the boundaries of breakpoint regions; (iii) removal of a breakpoint region (Fig.3).
Since the sweep line is parallel to the two sides of each trapezoid, it is only necessary to consider intersections between the horizontal/vertical sides of the trapezoid.
For each breakpoint region B in B, we define Btop and Bbottom as the horizontal/vertical sides of the trapezoid.
We designate the side with the largest y value as top.
2.3.2 Data structures As in the plane sweep algorithm for line segment intersection, we maintain two data structures E and L. We define the event point schedule E as the list of positions for the sweep line.
The event point schedule E is initialized with the starts and ends of Btop and Bbottom; these correspond to the addition/removal events.
E is updated with new Fig.3.
Examples of the three events of the plane sweep: (A) addition, (B) intersection and (C) removal.
In each case black dots label the points recorded in the cyclic lists a and b (indicated as dashed paths) that form R. In addition, we show in {}s the labels assigned to the intersecting breakpoint regions.
i225 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i226 i222i230 S.Sindi et al.
intersection points as they are discovered.
For a given event point (position of the sweep line), the sweep line status structure L stores an ordered list of the segments intersecting the sweep line and two terminal segments y=.
L is analogous to the same structure in the plane sweep algorithm for line segment intersection.
In this case all line segments intersecting the sweep line are either horizontal or vertical edges of breakpoint regions.
We first check for intersection between line segments that are adjacent in L (Fig.3B).
If a non-empty intersection is computed, the intersection point is added to the event schedule E .
The regions of intersection are recorded using a third data structure R introduced by Nievergelt and Preparata (1982).
R is attached to the sweep-line status L and records the vertices of the regions of intersection encountered thus far on the sweep.
For each segment s in L, R maintains two cyclic lists, a(s) and b(s), that contain the points on the boundaries of the regions above and below s, respectively.
Equivalently, if s and t are adjacent line segments in L, let [s,t] denotes the region to the left of the sweep line and between s and t. Then R contains the vertices defining the boundary of [s,t].
We augment the R structure of Nievergelt and Preparata (1982) with a label for each region of intersection.
This label is the set of breakpoint regions that contain the region of intersection.
For example, the region consisting of the non-empty intersection of breakpoint regions B1 and B2 is labeled {1,2}.
Finally, we maintain a interval tree (Preparata and Shamos, 1985) H from which we derive the maximal elements of the poset (In,) encountered thus far on the sweep line.
The algorithm (Algorithm 1) consists of iterating through the event point schedule and updating the regions of intersection found at each step according to whether the event point is an addition, removal, or intersection.
This update is briefly described in the next section.
2.3.3 Computing regions of intersection The procedureProcessEvent in Algorithm 1 updates the data structures, E , L, R and H according to the type of event.
For a removal event, ProcessEvent ends the regions [s,t] for each pair of adjacent segments in L by joining b(s) and a(t) with the points p(s) and p(t) where s and t intersect the sweep line (Fig.3C).
For an intersection event, ProcessEvent also swaps the order of s and t in L (Fig.3B).
Further details of these operations are described in (Nievergelt and Preparata, 1982) and in the Supplementary Text (available at http://www.cs.brown.edu/people/braphael/supplements/structvar).
Finally, each identified region of intersection is labeled by the constituent breakpoint regions.
Region labels are represented as sets of breakpoint region names and are updated using the following procedure.
If s and t are consecutive line segments along a sweep line, let I([s,t]) denote the label set of the region [s,t].
When processing an addition event of breakpoint region i, new regions are introduced with labels I([s,t])i.
When a processing a removal event of breakpoint region i, new regions are introduced with label I([s,t])\i.
Region labels also change during intersection events.
When regions are completed, their labels and list of boundary vertices are inserted into the interval tree H. Finally, all regions, or alternatively only maximal regions, are output as they are identified.
2.4 Extensions We briefly describe two natural extensions of our method.
First, we include aCGH data.
Second, we compute the probability that a paired-end sequenced fragment matches an existing structural variant.
2.4.1 Incorporating aCGH data As described above, the uncertainty in the breakpoints of a copy number change measured by aCGH is represented as a rectangle (Fig.3B).
The plane sweep algorithm is readily extended to include intersections with the rectangular breakpoint regions.
2.4.2 Incorporating fragment length distribution In most paired-end sequencing approaches, various procedures are used to select fragments of a specified size L, with the resulting fragments having lengths distributed around this selected size.
Thus far, we considered each fragment length between Lmin and Lmax to be equally likely.
We can instead derive the empirical distribution f (L) of the values |y||x| over all valid pairs (x,y) and use this distribution to better ascertain whether fragments provide evidence for a specific rearrangement.
The breakpoint (a,b) defines a length lC (a,b)= (sign(xC )axC )+(sign(yC )byC ) for fragment C. Given a polygon P defining the breakpoint region of a structural variant, we compute the probability that the invalid pair (xC ,yC ) is consistent with this variant as P f (lC (a,b))dadb GG f (lC (x,y))dxdy .
Note that this length is constant for all points (a,b) on the same line of slope 1 or 1, according to the orientation of the invalid pair (Bashir et al., 2008).
3 RESULTS We implemented our geometric approach in a program called GASV.
We applied GASV to: (i) analyze recent paired-end sequencing data of nine human individuals; (ii) perform a comparative analysis of genetic structural variants and those identified in paired-end sequencing of several cancer samples; and (iii) integrate data across measurement techniques by comparing variants identified by both aCGH and paired-end sequencing in cancer samples.
3.1 Paired-end sequencing of human structural variants We used GASV to analyze fosmid paired-end sequencing data from eight individuals from the HapMap populations (Kidd et al., 2008) and another individual from an earlier study (Tuzun et al., 2005).
The Kidd et al.
(2008) study reported a total of 224 inversion, 724 insertion and 747 deletion variants, which were validated by fingerprint analysis, clone sequencing or FISH.
These studies are presently the most comprehensive, high-resolution survey of structural variants in the human genome.
The mean insert sizes for the fosmid clones ranged from 36 kb to 41 kb with SD from 1.4 kb to 3.9 kb.
In our analysis, we used Lmin =20 kb and Lmax =60 kb to provide a generous buffer for intersecting breakpoint regions.
3.1.1 Analysis of reported inversion variants We first analyzed the 180 validated inversions reported on the 22 autosomes in Kidd et al.
(2008).
We obtained the list of the boundaries of each inversion, the names of the clones that support each variant and the mapped coordinates of the end sequences.
We used our geometric approach to compute the intersections of the breakpoint regions for each set of supporting clones, and we compared the reported boundaries of the inversions with the intersections we obtained.
Surprisingly, 41/180 i226 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i227 i222i230 Geometric structural variation A 8,9,13,G All 13 B Fig.4.
Geometric analysis of inversion polymorphisms from Kidd et al.
(2008) reveals disparities between the reported boundary of variants (black dots) and the intersections of breakpoint regions.
(A) An inversion on chr1 with 79 reported supporting clones from all nine individuals has no point in common to all breakpoint regions.
The number x next to each of the three regions indicates a clone from individual labeled ABCx in Kidd et al.
(2008) is present in the cluster; a G indicates the G95 individual from Tuzun et al.
(2005).
The bottom right region contains clones from all nine individuals, while individual ABC13 has clones from all three regions suggesting multiple distinct structural variants or mapping difficulties at this locus.
(B) An inversion from chr3 with 22 supporting clones from all eight HapMap individuals.
We examined one fully sequenced clone (dashed trapezoid) from individual ABC7 and found two possible inversion breakpoints (black squares).
Both of these lie in the intersection of all breakpoint regions but are 37 kb from the reported boundary.
of the validated inversions had an empty intersection of breakpoint regions.
That is, there were no candidate inversion breakpoints common to all of the reported supporting clones suggesting that the mapped clones are inconsistent with only a single inversion at the locus.
Figure 4A shows an example of one such set, where multiple, distinct non-overlapping intersections are visible.
One hypothesis is that the three distinct regions of intersection might represent slightly different breakpoints in different individuals.
However, one individual contains clones from all three regions, suggesting that this genomic locus harbors a more complex rearrangement.
In the remaining 139 cases the reported boundaries were not in the region of intersection.
Figure 4B shows one example where the reported coordinates for an inversion are clearly outside the region of intersection.
In this case, Kidd et al.
(2008) sequenced one of the clones in this cluster.
We aligned this sequence to the reference genome and obtained two possible inversion breakpoints, both of which lie in the region of intersection computed by GASV.
These two breakpoints could not be further resolved due to repetitive sequence near the inversion breakpoints.
Analysis of additional sequenced clones from Kidd et al.
(2008) showed a number of additional inversion breakpoints that occur within segmental duplications.
Thus even with complete sequence data available, resolving the breakpoint with greater precision is challenging.
The method used to derive the reported boundaries of the variants in Kidd et al.
(2008) is mysterious.
It is possible that the reported boundaries were intended to represent a consensus of a single structural variant locus.
For complicated loci with multiple, overlapping rearrangements (Fig.4A), consensus coordinates might provide a reasonable summary of the data.
However, we find that in many cases, the data allow us to refine the breakpoint region, and that significant information is lost when only a single pair of coordinates is reported for an inversion.
3.1.2 Analysis of intersecting breakpoint regions Using the complete set of mapped locations provided by Kidd et al.
(2008), we computed the intersections of breakpoint regions for all nine individuals using GASV.
In total, 30 853 clones on the 22 autosomes were consistent with an inversion.
There were 1361 groups of intersecting breakpoint regions.
Of these, 1200 had non-empty intersections, indicating that these clusters have a putative breakpoint in common for all of the clones.
The remaining 161 groups had no breakpoints common to all the clones.
Thus, over 10% the intersecting breakpoint regions suggest either complicated loci that are not easily explained as single inversion variants, or mapping/alignment artifacts.
For the 1200 clusters with non-empty intersection, we computed the area of the intersection and defined the localization of a breakpoint as the square root of this area.
Thus, if the region of intersection was a square, the localization would give the genomic range allowed for each breakpoint.
There are 19 clusters with breakpoint localization <2500 bp.
In the best example, eight clones from four individuals localize the breakpoints to within 286 bp on each end (Supplementary Text).
This is a remarkably small region of uncertainty, considering that a single fosmid clone localizes a breakpoint to 40 kb.
We also found that breakpoint localization is not directly correlated with the number of clones in the breakpoint region.
In 21/1200 cases the breakpoint region was supported by more than 50 clones.
In only two of these cases was the breakpoint localization <5000 bp.
A possible reason for this discrepancy is the presence of repeats/duplications near the inversion breakpoints.
These would lead to a relatively small genomic region where end sequences can be mapped uniquely.
3.1.3 Overlap of inversion and deletion variants We used GASV to compare the locations of inversions and deletions in the data of Kidd et al.
(2008).
We identified 5054 instances of intersection between inversion and deletion breakpoint regions.
Figure 5 shows a sample cluster containing 33 clones indicating an inversion and i227 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i228 i222i230 S.Sindi et al.
Fig.5.
Intersection of 33 inversion breakpoint regions (blue) and 4 deletion breakpoint regions (red), indicates common genomic location of two structural variants.
4 clones indicating a deletion at the same locus.
There are several examples where inversion heterozygotes lead to deletions in progeny (Stankiewicz and Lupski, 2002), a possible explanation for these overlapping variants.
Alternatively, this overlap might suggest that these regions are unstable and subject to repeated rearrangement (Stankiewicz and Lupski, 2002).
3.2 Cross-study comparison of structural variants Our geometric approach allows for the comparison of structural variants identified in different individuals with different measurement techniques.
We tested this feature by comparing the genetic structural variants identified by Kidd et al.
(2008) with variants identified in ESP studies of cancer genomes (Raphael et al., 2008; Volik et al., 2006).
The later studies aimed to identify somatic, and possibly cancer-related, rearrangements in three breast cancer cell lines and five primary tumors from various tissues.
The cancer ESP studies used large insert clones (BACs) with average sizes of 150 kb.
We first identified clusters of invalid pairs in each cancer dataset that were suggestive of either inversions or deletions, and then computed the intersection of these clusters with the inversion and deletion clusters computed from the nine normal individuals.
Approximately 553% of invalid clusters from the cancer clusters are consistent with inversion or deletion variants identified in normal individuals (Table 1).
The larger percentages are found in the primary tumor samples; this is consistent with the lower sequence coverage in the primary tumor samples, and the fact that tumor samples frequently contain significant admixture of normal cells resulting from difficulty of separating normal from tumor cells.
We then clustered all the cancer data together with the nine normal individuals, and identified overlapping breakpoint regions containing at least two invalid pairs from different cancer samples.
Of the 22 such clusters, 10 are consistent with inversion variants identified in at least one normal individual (9/10 cases were observed in at least four normal individuals), demonstrating that a large fraction of structural variants found in more than one cancer dataset are inherited genetic variants and not somatic rearrangements.
Table 1.
A comparison of the inversion and deletion variants identified in nine normal individuals (Kidd et al., 2008; Tuzun et al., 2005) and several cancer genomes (Raphael et al., 2008; Volik et al., 2006) Cancer No.
of concordant No.
of concordant sample inversions (%) deletions (%) MCF7 8 (5) 40 (28) BT474 12 (19) 8 (11) SKBR3 8 (13) 7 (11) Breast 11 (19) 21 (27) Breast 12 (32) 19 (38) Prostate 3 (9) 12 (27) Ovary 8 (53) 12 (29) Brain 2 (11) 10 (26) Fig.6.
Intersection between six breakpoint regions from ESP data (blue trapezoids) and two breakpoint regions determined by aCGH (red rectangle) on chr17 in the BT474 breast cancer cell line.
In this case, the spacing between aCGH probes provides a more precise localization of the breakpoint region that the paired-end sequencing data.
3.3 Comparing variants identified by paired-end sequencing and aCGH We used GASV to compare the breakpoints identified by ESP and aCGH for three cancer cell lines, MCF7, BT474 and SKBR3, using data from Volik et al.
(2006), Raphael et al.
(2008), and Aerni et al.
(2009).
We formed rectangles corresponding to pairs of copy number changes identified by segmentation (Fig.1B) of aCGH data using CBS (Olshen et al., 2004).
We found that 35/152, 20/380 and 35/149 of the clusters defined from paired-end sequenced data intersected aCGH breakpoint regions in BT474, MCF7 and SKBR3, respectively.
Figure 6 shows an example of a cluster containing 19 breakpoint regions identified by ESP in the BT474 cell line, intersecting a breakpoint region determined by aCGH.
4 DISCUSSION We introduced GASV, a geometric approach for classification and comparison of structural variants.
To our knowledge, this is the first comprehensive method for structural variant analysis across multiple samples that supports both paired-end sequencing data with arbitrary fragment sizes and aCGH with varying array resolutions.
i228 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i229 i222i230 Geometric structural variation We illustrated the generality of our approach through several applications, including the clustering of variants from a paired-end sequencing study of nine individuals, the comparison of variants in normal and cancer genomes derived through different sequencing approaches, and the comparison of variants identified by aCGH and paired-end sequencing of the same cancer samples.
In many cases we are able to localize the breakpoints of single variants, but in other cases the end-sequence pairs suggest more complicated variants.
The precise localization of the boundaries of structural variants provided by the GASV is helpful for distinguishing simple variants shared across multiple individuals from more complex variants resulting from repeated rearrangements at the same locus.
These results also demonstrate the importance of identifying and reporting the uncertainty in structural variant boundaries.
The current convention of publishing approximate coordinates that were derived from study-specific heuristics can lead to unnecessary errors and misannotations of complicated variants.
We expect that GASV will be useful for analyzing data from the 1000 Genomes Project and for cancer genome sequencing efforts that are part of The Cancer Genome Atlas.
In the latter application, GASV will help distinguish genetic from somatic rearrangements.
There are several directions for future work.
First, it would be useful to perform a more comprehensive comparison of the variants that are identified by different measurement techniques.
aCGH has limited power to detect variants whose breakpoints lie in repeat-rich regions of the genome due to the inability to identify probes in these regions.
Paired-end sequencing approaches can be similarly limited, particularly if small fragment sizes are used, since the end sequences will not align uniquely to repeat-rich regions of the genome.
Current studies of structural variants with next-generation sequencing technologies have used small fragment sizes from 200 bp (Campbell et al., 2008) to 3 kb (Korbel et al., 2007).
An unresolved question is the optimal fragment size to use for studies of human structural variation.
We have shown that clustering of breakpoint regions from relatively large clones (40 kb) with GASV can yield very precise localization of variant breakpoints (a few hundred base pairs).
Kidd et al.
(2008) reported that most of the clones that they sequenced had highly repetitive sequence at the breakpoints, complicating the precise breakpoint identification and assembly of the clone sequence.
Thus, even in cases where complete sequence is available GASV can be used to record uncertainty in breakpoint location.
An additional area of future work is to incorporate breakpoint uncertainty into databases of known structural variants.
Our geometric approach could then be used to query this database and thus provide a more robust procedure for comparing newly discovered and existing variants.
In addition, knowledge of existing structural variants can be used to guide mapping of end sequences that do not map uniquely to the reference genome.
This is a common problem in human genome resequencing, where up to 60% percent of ES fragments are not used because of their ambiguous mappings (Korbel et al., 2007).
Lee et al.
(2008) recently described a probabilistic model for resolving ambiguities that arise when mapping ES pairs in a single sample.
Developing a model for multi-sample comparison that incorporates variant ambiguity across samples is a promising future endeavor.
Finally, we focused exclusively on structural variation in the human genome, but such variation is also found in the mouse genome (Egan et al., 2007) and other model organisms (Dopman and Hartl, 2007).
Thus, there will continue to be an increasing demand for better analysis tools for structural variation.
ACKNOWLEDGEMENTS We thank Franco Preparata and Crystal Kahn for helpful technical discussions, and Anna Ritz for assistance in preparing the manuscript.
Funding: Career Award at the Scientific Interface from the Burroughs Wellcome Fund (to B.J.R.
); the Department of Defense Breast Cancer Research Program (to B.J.R.
); ADVANCE Program at Brown University, which is funded by the National Science Foundation under grant number 0548311 (to B.J.R.).
Conflict of Interest: none declared.
Abstract Next-generation sequencing (NGS) technology, with its high-throughput capacity and low cost, has developed rapidly in recent years and become an important analytical tool for many genomics researchers.
New opportunities in the research domain of the forensic studies emerge by harnessing the power of NGS technology, which can be applied to simultaneously analyzing multi-ple loci of forensic interest in different genetic contexts, such as autosomes, mitochondrial and sex chromosomes.
Furthermore, NGS technology can also have potential applications in many other aspects of research.
These include DNA database construction, ancestry and phenotypic inference, monozygotic twin studies, body fluid and species identification, and forensic animal, plant and microbiological analyses.
Here we review the application of NGS technology in the field of forensic science with the aim of providing a reference for future forensics studies and practice.
Introduction Since the introduction of the Sanger sequencing method in the 1970s [1], DNA sequencing technology has enabled enormous advances in molecular biology and genetics.
Several large pro-jects have been successfully completed using this technology, such as the Human Genome Project, Rice Genome Project and Swine Genome Project, as well as genome projects of many other species.
However, disadvantages of the conventional Sanger sequencing technology, including its low throughput, high cost and operation difficulties, have limited its use in dee-per and more complex genome analyses [2].
The recent intro-duction of next-generation sequencing (NGS) technology, with its high-throughput capacity and low cost, has largely overcome these problems, and these technologies have been applied in various fields of life sciences, including forensics [3], disease diagnosis [4], agrigenomics [5] and ancient DNA analysis [6].
In this article, the use of NGS technology in foren-sic science is reviewed with the aim of providing a reference for future frontier research and application in forensic science.
Overview of NGS technology NGS technology refers to non-Sanger-based high-throughput DNA sequencing technology.
Millions or billions of DNA nces and Yang Y et al/ Next-generation Sequencing in Forensics 191 molecules can be sequenced in parallel, thereby increasing the throughput substantially and minimizing the need for the frag-ment-cloning method often used in Sanger sequencing.
It includes second-generation sequencing technology based on loop array sequencing, which can analyze a large number of samples simultaneously, as well as third-generation sequencing technology, which can determine the base composition of single DNA molecules.
In 2005, Roche introduced the 454 Genome Sequencing System [7], the worlds first pyrosequencing-based high-throughput sequencing system.
The first 454 Genome Sequen-cer was capable of generating approximately 200,000 reads of 110 base pairs (bp) in length (the current maximum read length is 1000 bp).
In 2007, Applied Biosystems (ABI) introduced the SOLiD second-generation sequencing system based on the oli-gonucleotide ligation technique and two-base encoding system, whereas Illumina released Solexa sequencing technology.
The Illumina and SOLiD sequencers generated much larger num-bers of reads than the 454 system (30 and 100 million reads, respectively); nonetheless, the reads produced were only 35 bp long.
In 2010, Ion Torrent, a faster and low-cost sequen-cer based on semiconductor technology, was introduced.
This sequencer does not rely on fluorescence, chemiluminescence, or enzyme cascades for sequencing signal detection.
Currently, a maximum read length of up to 400 bp can be obtained using this system.
All these new sequencing methods have led to three major improvements from the conventional technologies.
First, these technologies do not require bacterial cloning of DNA frag-ments; instead, they rely on the preparation of NGS libraries in a cell-free system.
Second, instead of hundreds of sequenc-ing reactions, they can parallelize the thousands-to-many-millions of sequencing reaction.
Third, the sequencing output is directly detected with no need for electrophoresis.
The enor-mous number of reads generated by NGS enabled the sequenc-ing of entire genomes at an unprecedented speed and thus it came to be widely used in various fields of life sciences.
How-ever, one drawback of second-generation sequencing technol-ogy is its relatively short read lengths, which has resulted in difficulties in subsequent sequence splicing, assembly, annota-tion and bioinformatics analysis [8].
Furthermore, standard PCR was used to randomly amplify genomic fragments during library preparation.
Due to the complex structure of genomes, factors such as secondary structure and thermal stability will affect the efficiency of PCR amplification.
Therefore, the com-plete genomic sequence may not be represented in the library produced by such amplification.
This can be problematic due to the relative deviation between amplified and non-amplified DNA molecules, resulting in potential inaccuracies in gene expression analysis.
This concern is particularly relevant for highly-expressed genes [9].
Moreover, these shortcomings have restricted the application and development of second-genera-tion sequencing technology to some extent and have necessi-tated the development of third-generation single-molecule sequencing technology [1012].
The third-generation sequencing technology not only allows the detection of single molecules but also enables real-time sequencing.
The current leader in this field is the PacBio RS system (Pacific Biosciences), which utilizes the single-mol-ecule, real-time (SMRT) DNA sequencing technology.
SMRT sequencing is based on the sequencing-by-synthesis approach; an SMRT chip contains thousands of zero-mode waveguides, in which the DNA polymerase molecules used to synthesize the DNA fragments of interest are attached.
Compared to sec-ond-generation sequencing, the latest SMRT technology can achieve an average read lengths of 55008500 bp.
Moreover, it can also directly detect epigenetic modifications such as 4-methylctosine (mC), 5-mC and 6-methyladenine (mA) [13].
Forensic application prospects of NGS technology The application of DNA technologies in forensic investiga-tions has rendered DNA analysis an important tool in forensic science.
Compared to other fields of life sciences, forensic DNA analysis is confronted with template of low copy num-ber, highly-degraded and contaminated samples, the need for high accuracy and reproducibility, as well as time and cost considerations.
Today, the majority of forensic DNA tests employ PCR and capillary electrophoresis (CE)-based fragment analysis methods to detect length variation in short tandem repeat (STR) markers.
The CE-based Sanger sequencing has been used to analyze specific regions of mitochondrial DNA (mtDNA) [14].
The development of miniaturized gel electrophoresis and the auto-mation of reaction gel loading and signal detection allowed the Sanger methodology to become the gold standard for DNA sequencing.
However, CE-based analysis has its limitations, for example, the inability to analyze multiple genetic polymorphisms in a single reaction using a single workflow, low-resolution genotyping of current markers, loss of useful genomic information from degraded DNA samples, and low-resolution mtDNA and mixture analysis.
These limitations of first-generation sequencing prompt the forensic scientists worldwide to explore the usefulness of NGS technology for forensic studies.
STR analysis STR analysis is likely to remain the most important and commonly-used genetic technique in forensic science for the foreseeable future.
It displays multiple advantages, such as rapid and precise allele determination, low DNA template requirement, multiplex amplification and fluorescence-based detection, digitized results and utilization of the abundant genomic element, At present, more than 60 countries world-wide have established forensic DNA databases based on STRs, and these databases continue to grow rapidly.
For example, China now has more than 27 million entries in its forensic database [15].
The probability of a random match between unrelated individuals will increase if statistical analyses were based only on the 13 routinely-used Combined DNA Index System (CODIS) STR markers (i.e., CSF1PO, FGA, THO1, TPOX, VWA, D3S1358, D5S818, D7S820, D8S1179, D13S317, D16S539, D18S51 and D21S11) or 15 markers (13 CODIS loci plus D2S1338 and D19S433).
To avoid this, incor-poration of more STR markers into the common forensic typ-ing assay currently used has been recommended.
However, simultaneous detection of more STR markers would be very difficult, due to the technical limitations of fluorescent-based CE sequencers currently in use.
Traditional CE-based STR typing using CE is based on the detection of DNA fragment size.
Therefore, alleles of identical or similar length but of dif-ferent sequences cannot be distinguished.
Consequently, STR 192 Genomics Proteomics Bioinformatics 12 (2014) 190197 mutations in complex paternity cases often cannot be resolved with traditional CE-based STR analysis.
An additional challenge for forensic DNA tests is the analysis of complex DNA mixtures comprising DNA from more than one individ-ual.
Contemporary analyses of mixed DNA samples often yield low detection rates, thus are not useful in crime investigations.
When NGS technology was firstly introduced to genomics, it was not suitable for STR testing because the read length was generally too short.
With technological advances, the average read length has been continually increasing.
Since alleles with similar length can be easily distinguished using NGS technol-ogy and digital read count could significantly facilitate the identification of mixed DNA samples and analysis of complex paternity cases, some researchers have recently started using NGS technology for STR testing.
For example, a pioneer study was performed by Zajac and colleagues, who analyzed three CODIS STR loci, TPOX, CSF1PO and D18S51, using the trinucleotide threading (TnT) approach by 454 Genome Sequencing System [7,16].
Subsequently, Irwin et al.
[17] ana-lyzed 13 CODIS STR loci using the 454 GS Junior system in combination with multiplex identifier technology for single source samples.
Bornman et al.
[18] went further to show that high-throughput sequencing technology can accurately iden-tify the 13 CODIS STR loci as well as the AMEL gene for not only single source but also mixed samples.
To process the forensic NGS data, Warshauer et al.
[19] developed STRait Razor, a software that can analyze the NGS data for 44 STRs, including 23 autosomal and 21 Y chromosome STRs.
In addi-tion, Van Neste et al.
[20] used Illuminas MiSeq system to establish a reference allele database to detect single source and mixed DNA samples; they observed that most locus geno-typing results were stable and reliable.
NGS technology has many potential advantages for STR analysis.
These include high throughput, low cost, simulta-neous detection of large numbers of STR loci on both auto-somes and sex chromosomes, and the ability to distinguish alleles with similar length or digital read count.
NGS technol-ogy would therefore significantly facilitate the identification of mixed DNA samples and analysis of complex paternity cases, and ultimately greatly increase the efficiency and cost-effective-ness of legal cases.
Mitochondrial genome analysis mtDNA has proved to be a useful forensic tool in cases involving low amounts of DNA or wherein the maternal lineage needs to be investigated, due to its characteristics of small size, multiple copies, maternal inheritance, high mutation rate and lack of recombination.
Currently, forensic mtDNA analyses usually detect only polymorphisms within a hypervariable region.
However, for mtDNA to be used as a genetic haplotype marker, additional polymorphic loci are required to increase the discrimination power of identifi-cation.
Therefore, NGS technology has the potential to greatly assist in the analysis of whole mitochondrial sequences.
With the increased application of NGS technology in vari-ous fields, the cost of equipment and reagents has decreased markedly.
Parallel sequencing technology, which allows for simultaneous analysis of multiple samples, has also led to cost-effectiveness.
For instance, the number of picotiter plates used in the GS-FLX instrument has increased from 2 to 16, and each channel can simultaneously analyze 192 samples using multiplex identifier (MID) technology.
Furthermore, Binladen et al.
[21] used a primer coding technique and produced 256 tagged primers for use in multiple parallel sequencing, allowing 256 samples to be sequenced in a single run.
Moreover, Gunnarsdottir et al.
[22] used NGS technology to sequence whole mitochondrial genomes of 109 Filipino indi-viduals at the same time and obtained on average 55 cover-age per sequence, with <1% missing data per sequence.
Human mtDNA heteroplasmy is common and hetero-plasmy of cells from different tissues within a single individual has also been observed [23].
mtDNA heteroplasmy is one of the factors affecting the performance of forensic mitochondrial analysis.
The detection of heteroplasmy at the whole mito-chondrial genome level has been reported [24], supporting the advantages of using NGS to detect mitochondrial hetero-plasmy, including high accuracy and sensitivity, high through-put, low cost, and simple operation [25].
In a separate study, multiple mitochondrial hypervariable regions, an autosomal STR locus (D18S51) and a Y chromosome STR locus (DYS389I/II) were simultaneously examined using the 454 GS Junior system.
The results demonstrated that a mixing ratio of two DNA sources as low as 1:250 can be detected, and the authors concluded that by increasing the sequencing coverage, a mixing ratio of 1:1000 might be detectable as well [26].
To compare the haplotypes defined by using NGS tech-nology at the whole mitochondrial genome level with conven-tional Sanger sequencing, 64 whole mitochondrial genome sequences were analyzed.
The results showed differences in <0.02 % of nucleotides using these two methods and that approximately two-thirds of the differences were observed in or around homopolymeric stretches, since these areas were prone to sequencing errors [27].
To evaluate the reproducibility between samples that were sequenced twice with NGS, Mik-kelsen et al.
[28] reported that using the 454 NGS method, 95% of the reads was sequenced correctly in homopolymers of up to 6 bases if the results were carefully and visually inspected.
Previously-unreported heteroplasmy in the GM9947A component of the National Institute of Standards and Technology (NIST) human mtDNA SRM-2392 standard reference was detected in this study.
Y chromosome analysis Genetic markers on the Y chromosome have assumed a valu-able role within forensic molecular biology.
Most commonly, Y-STRs are used to unambiguously resolve the male compo-nent of DNA mixtures when a high female background is pres-ent, or to reconstruct paternal relationships between male individuals.
Using NGS technology, more than 10 million nucleotides of the Y chromosome were compared between two male individuals who shared the same ancestor 13 gener-ations ago [29].
Four genetic differences were detected, sug-gesting that Y chromosome sequencing could solve the problem of distinguishing between mixed male samples from the same parent.
In addition, Van Geystelen et al.
[30,31] developed AMY-tree using Y chromosome single nucleotide polymorphisms (SNPs) and successfully verified the differences between 118 unrelated male individuals from 109 different geo-Yang Y et al/ Next-generation Sequencing in Forensics 193 graphical regions.
This study demonstrated that AMY-tree can determine Y chromosome pedigrees and identify unknown Y-SNPs from different geographical regions.
Forensic microbiological analysis Microbial forensics is a new discipline developed by the Fed-eral Bureau of Investigation (FBI) after the Anthrax attack on 18 September 2001 in the USA.
It is based on the fast and accurate detection and identification of microorganisms founded at biological crime, with the aim of tracing the source of the microbe [32].
Because microbiological terrorist attack could lead to serious consequences, forensic microbio-logical analysis has been attracting considerable attention [33].
Using whole-genome sequencing by the SOLiD system in a real investigative case, Cummings et al.
[34] identified suspects by sequencing four strains each of Bacillus anthracis and Yersinia pestis at a cost of only $1000 and reported that this would be reduced to less than $50 if the HiSeq 2000 sys-tem were used.
Brenig et al.
[35] used the 454 sequencing system to identify biological traces using deep sequencing and metagenomic anal-ysis and indicated that the method can be used for the forensic identification of biological traces.
Fierer et al.
[36] examined the bacteria left by human skin on the surface of contact objects by using an NGS-based metagenomic method and showed that the bacteria left by human skin possess sufficient DNA information for forensic analysis.
A study by Lilje et al.
[37] investigated the criteria for soil metagenome data management and database searching.
Eleven samples collected from different environments (forests, fields, grasslands and an urban park) with different microbial flora were sequenced using the Roche/454 platform.
The results demonstrated that 18S rRNA gene marker analysis could be used to create and run a filtered database, which was very com-putationally efficient and flexible.
Similarly, Giampaoli et al.
[38] successfully applied a metabarcoding approach to forensic and environmental soil samples, allowing accurate and sensi-tive analysis of the DNA of microflora, plants, metazoa and protozoa.
All the studies described above demonstrate that NGS has the advantages of high throughput, multiplexing capability and accuracy, which makes it suitable for rapid whole-genome typing of microbial pathogens during forensic or epidemiolog-ical investigation.
Rare polymorphisms can be reliably detected by analyzing every base of the genome, thus giving forensic data higher resolution and greater accuracy.
It is expected that a high-quality forensic microbial database will soon become a reality and aid in the fast and accurate identi-fication of criminals and biological terrorists.
Animal and plant DNA analysis Species identification is one of most important components of forensic practice.
For example, in some cases of poaching [39] and trading of endangered species [40], it has been used to pro-vide important information and assist in police investigations.
In the food industry, identification of the species present in meat products can be achieved [41], and in archeology, human remains can be distinguished from non-human remains [42].
At present, most DNA typing methods for species determination are based on PCR amplification using species-specific primers for single species.
However, forensic scientists are often faced with situations in which no a priori species information is available.
The development of NGS technology has allowed DNA typing to be used in more projects involving species iden-tification [43,44].
For example, Cheng et al.
recently identified plants and animals in traditional Chinese medicines using a cost-effective and efficient next-generation deep sequencing method [45].
Ancestry studies and phenotypic inferences Information embedded within the human genome may pro-vide insights into personal characteristics such as ethnicity [46], physical and physiological characteristics and age [47,48].
In forensic studies, characteristics inferred from DNA analysis make it possible for criminal investigations to evolve from the passive comparison into the active search stages.
In criminal cases where a possible suspect and database information are unavailable, it is possible to rapidly narrow down the potential suspects by using ances-try studies and phenotypic inference derived from a DNA sample.
For example, in the 2004 Madrid train bombings, source population of the suspects was inferred by using 34 autosomal SNPs related to the ancestry of population [49].
Other studies reported SNPs closely related to colors of the iris [50] and hair [51] with an accuracy of 90%.
Klimen-tidis et al.
[52] investigated facial features using DNA test and association analysis and validated their results using facial reconstruction (molecular photo fitting).
The results demonstrated that there is a relationship between social and biological measures of race/ethnicity but that it is far from perfect.
In all these studies, only commercial SNP chip scanning were used.
If NGS technology for whole-genome sequencing were applied in these cases, more information and accuracy would be obtained.
Epigenetic analysis DNA sequencing analysis is a powerful tool in forensic identi-fication [53].
Recently, a number of studies have suggested that epigenetic markers can also have various applications in foren-sic science.
For example, evidence supports that epigenetic markers can be used to distinguish monozygotic (MZ) twins [54], predict tissue type [55] and accurately determine the age of a DNA donor [56].
Epigenetic approaches based on NGS technology include whole-genome bisulfite sequencing [57], methylation beadchips, reduced representation bisulfite sequencing [58] and methylated DNA immunoprecipitation sequencing [59].
These sequencing methods require large amounts of DNA; their ability to use trace DNA samples will therefore be crucial to the success of forensic epigenetic analy-sis.
Interestingly, extremely low amounts of starting DNA (100 pg) were successfully analyzed through genome-wide amplification of a bisulfite-modified DNA template, followed by quantitative methylation detection using pyrosequencing [60].
Additionally, another encouraging study performed bisul-fite genomic DNA sequencing with micro-volume blood spot samples [61].
194 Genomics Proteomics Bioinformatics 12 (2014) 190197 MZ twin studies continue to be a hot topic in the field of forensic science.
As both individuals have exactly the same DNA sequence, conventional genotyping approaches such as STR, SNP, sex chromosome STR, and mtDNA analyses can-not tell them apart.
In 2014, Weber-Lehmann et al.
[62] described how identification of extremely-rare mutations by ultra-deep NGS can differentiate between MZ twins, suggest-ing a solution to paternity and forensic cases involving MZ twins.
Li et al.
[54] used Illumina Human Methylation Bead-Chip technology to examine the DNA methylation status of 27,578 CpG sites from 22 MZ twins.
As a result, they filtered 92 significantly-methylated CpG sites, representing potential targets for epigenetic studies aimed at distinguishing between MZ twins.
In 2010, the BGI and twin research group Twin-sUK at Kings College, London, co-sponsored epigenetic research projects using NGS technology to conduct an in-depth study aimed at capturing the subtle differences in epi-genetic signals from 5000 pairs of twins [63].
The research out-comes are likely to be highly applicable in the forensic identification of MZ twins.
TTTCAGGGCGCAGGTCG          CCACCGTCTGTCTGCAGAAA TACAAAACACGGAGTTTGCCTT ATAA ACTTT GGGACTGAGCACATCGCGTGGTGCGCTAA AAGACACTTTTTCTTCTTTGTGGCACAAAAAA GGCCGTTTTCAGGGCGCAGGTCGGTGG TCGGCGCCACCGTCTGTCTGCAGAAAA TATAA ACAAAACACGGAGTTTGCCTT ATAA ACTGTT GGACTGAGCACATCGCGTGGTGCGCTCAA GACACTTTTTCTTCTTTGTGGCACAAAA TAAA TAATT AAGACACTTTTTCTTCTTTGTGGCACAAAAAA TGGCCGTTTTCAGGGCGCAGGTCGGTG CTCGGCGCCACCGTCTGTCTGCAGAAA ATATAA ACAAAACACGGAGTTTGCCTT ATAA ACTGTT GGGACTGAGCACATCGCGTGGTGCGCTAA AAGACACTTTTTCTTCTTTGTGGCACAAAAAA GGCCGTTTTCAGGGCGCAGGTCGGTGG TCGGCGCCACCGTCTGTCTGCAGAAAA TATAA ACAAAACACGGAGTTTGCCTT ATAA ACTGTT GGACTGAGCACATCGCGTGGTGCGCTCAA GACACTTTTTCTTCTTTGTGGCACAAAA TAAA TAATT CCGTTTTCAGGGCGCAGGTCGGTGGGA GCGCCACCGTCTGTCTGCAGAAAAGAC AAGACACTTTTTCTTCTTTGTGGCACAAAAAA TGGCCGTTTTCAGGGCGCAGGTCGGTG CTCGGCGCCACCGTCTGTCTGCAGAAA ATATAA ACAAAACACGGAGTTTGCCTT ATAA ACTGTT GGGACTGAGCACATCGCGTGGTGCGCTAA AAGACACTTTTTCTTCTTTGTGGCACAAAAAA GGCCGTTTTCAGGGCGCAGGTCGGTGG TCGGCGCCACCGTCTGTCTGCAGAAAA TATAA ACAAAACACGGAGTTTGCCTT ATAA ACTGTT GGACTGAGCACATCGCGTGGTGCGCTCAA GACACTTTTTCTTCTTTGTGGCACAAAA TAAA TAATT CCGTTTTCAGGGCGCAGGTCGGTGGGA GCGCCACCGTCTGTCTGCAGAAAAGAC c Microbiology Animals and plants Ancestry and phenotypic infererence Monozygotic twins MicroRNAs Datab Figure 1 Forensic analysis by next-generation sequencing The introduction of next-generation sequencing (NGS) technology sequencing method has revolutionized our thinking about scientific st aspects of forensic science, including short tandem repeats (STRs recognition, Y chromosome and mitochondrial whole-genome studies and ancestry and phenotype inference.
More importantly, high-throug facilitating a systematic understanding of relationships between molecu in combination with the techniques of genomics, proteomics, transcri applied forensics.
MicroRNA analysis Although mRNA analysis has become a well-established tech-nique in many forensic laboratories, microRNAs (miRNAs) have only recently been introduced to forensic science.
miR-NAs are a class of endogenous small RNA molecules 1824 nucleotides in length.
Owing to their small size, resistance to degradation and tissue-specific or highly tissue-divergent expression, they are suitable for forensic body fluid identifica-tion, species identification and post-mortem interval (PMI) inference analysis [64].
Currently, analysis of miRNA is mostly achieved by real-time PCR and biochip technology, whereby only known miRNA sequences can be analyzed.
In 2009, Han-son et al.
[65] introduced miRNA profiling to forensic science and showed that 452 miRNAs were genotyped via the quanti-tative PCR method from forensic samples.
In another study, the expression levels of 718 miRNAs in semen, saliva, venous blood, menstrual blood and vaginal secretions were profiled on a microarray [66].
Among them, 14 differentially-expressed GTGGGACTGAGC AGACACTTTTTCTTCT GGCCGTTTTCAGGGCGCAG CGGCGCCACCGTCTGTCTGCAGA TATAA ACAAAACACGGAGTTTGCCTT ATAA ACTTT GACTGAGCACATCGCGTGGTGCGCAA GACACTTTTTCTTCTTTGTGGCACAA GCCGTTTTCAGGGCGCAGGTCGGTG GGCGCCACCGTCTGTCTGCAGAAAA ACAAAACACGGAGTTTGCCATAA ACTGGTT TATAA ACAAAACACGGAGTTTGCCTT ATAA ACTT GGACTGAGCACATCGCGTGGTGCGAA AGACACTTTTTCTTCTTTGTGGCACA GCCGTTTTCAGGGCGCAGGTCGGT CGGCGCCACCGTCTGTCTGCAGAA TATAA ACAAAACACGGAGTTTGCCTT ATAA ACTTT GACTGAGCACATCGCGTGGTGCGCAA GACACTTTTTCTTCTTTGTGGCACAA GCCGTTTTCAGGGCGCAGGTCGGTG GGCGCCACCGTCTGTCTGCAGAAAA ACAAAACACGGAGTTTGCCATAA ACTGGTT CTGAGCACATCGCGTGGTGCGCTCGAA ACTTTTTCTTCTTTGTGGCACAACGTG TATAA ACAAAACACGGAGTTTGCCTT ATAA ACTT GGACTGAGCACATCGCGTGGTGCGAA AGACACTTTTTCTTCTTTGTGGCACA GCCGTTTTCAGGGCGCAGGTCGGT CGGCGCCACCGTCTGTCTGCAGAA TATAA ACAAAACACGGAGTTTGCCTT ATAA ACTTT GACTGAGCACATCGCGTGGTGCGCAA GACACTTTTTCTTCTTTGTGGCACAA GCCGTTTTCAGGGCGCAGGTCGGTG GGCGCCACCGTCTGTCTGCAGAAAA ACAAAACACGGAGTTTGCCATAA ACTGGTT CTGAGCACATCGCGTGGTGCGCTCGAA ACTTTTTCTTCTTTGTGGCACAACGTG STR-SNPs Mixed stains Multiple STRs Mitochondrial genome Y hromosome ase that is much cheaper and more rapid than the classic Sanger rategies in forensic research.
NGS will potentially influence many ) and microRNA analysis, monozygotic twin and mixed stain , forensic microbiological analysis, multiple species identification, hput screening techniques have generated large amounts of data, lar components.
Therefore, comprehensive genome-wide analysis, ptomics and epigenomics, will provide new insights in the field of Yang Y et al/ Next-generation Sequencing in Forensics 195 miRNAs were identified, which could serve as potential candi-dates for body fluid identification.
Using NGS technology, millions of miRNA sequences can be rapidly analyzed to iden-tify organ-and developmental stage-specific expression, as well as miRNA expression in different disease states, thus provid-ing a powerful tool for forensic analysis.
Conclusion In practical forensic science, DNA samples are usually limited and often cannot fulfill the requirements of simultaneously analyzing multiple loci on different chromosomes in mitochon-drial genome [6769].
This may result in difficulties in provid-ing sufficient information and can limit their use as legal evidence.
In addition, mixed stain identification and complex paternity cases cannot be solved with traditional STR genotyp-ing strategies.
NGS technology not only meets these require-ments but can also potentially be applied in many areas of research, including DNA database construction, ancestry and phenotypic inference, MZ twin studies, body fluid and species identification, and forensic microbiological analysis (Figure 1).
In forensic science, standard STR typing provides sufficient discrimination power for most applications, and most countries have already established large-scale forensic DNA databases for resolving crimes based on STR technology.
Although the use of whole-exome or whole-genome sequencing could provide more information for forensic analyses, considering the compatibility and cost, NGS A Mit ch E Biological evidence from criminal scene Figure 2 Diverse range of information can be obtained by NGS of bio Through applying NGS technology, multiple results can be obtained crime scenes, such as STRs, single nucleotide polymorphisms (SNPs) of as epigenetic information.
By integrating all the information, the eviden for inferring the criminal suspects physical, psychological and geogra technology would not likely soon replace conventional STR typing.
Forensic scientists performed custom-designed target-enrichment panels to analyze STR loci.
However, these meth-ods only covered some of the currently-common used loci in forensic studies [1618].
With the development of NGS tech-nology, it is likely that its cost will rapidly decrease and NGS kits for forensic application will soon be commercially available.
This will allow the simultaneous detection of multi-ple STR loci on both autosomes and sex chromosomes, anal-ysis of mitochondrial genome polymorphisms and analysis of SNPs related to ancestry and physical and psychological char-acteristics, providing important information for forensic inves-tigations (Figure 2).
Using these methods, NGS is capable of providing data on loci across the genome.
As law enforcement agencies have started to share information, multiple interna-tional databases have recognized the need to analyze addi-tional loci in a single run and have expanded their locus database.
These expanded locus sets have improved the effi-ciency of law enforcement investigations, where a suspect was identified or the database yielded a match.
When neither a suspect nor a hit exists, additional information from evidence samples might provide valuable clues regarding the phenotype of the offender, who may be out of custody and at risk of re-offending.
Although NGS technology appears to have an important role in future forensic studies, more work is required to fully achieve this goal, which includes overcoming problems with low-template library preparation, error rate, type estimations and issues with NGS data processing and mining.
Guidelines NGS utosome ochondrial Sex romosome pigenetic Likelihood ratio = logical evidence samples collected from crime scenes simultaneously from biological evidence samples collected from autosomes, sex chromosomes and mitochondrial genomes, as well ce samples can be used not only for suspect identification but also phical characteristics, as well as the source population.
196 Genomics Proteomics Bioinformatics 12 (2014) 190197 for the application of NGS in forensic science also need to be generated.
With the technical advances of NGS technology and continuous translational efforts of forensic scientists, we believe that NGS technology is likely to become an easily accessible routine method in forensic practice.
Competing interests The authors have declared that no competing interests exist.
Acknowledgements The authors thank Dr. Hongzhu Qu and Dr. Xiangdong Fang for critical reading and Miss Nan Ding for figure preparation.
This study was supported by the National Natural Science Foundation of China (Grant Nos.
81172909 and 81330073).
ABSTRACT Motivation: RNA-seq techniques provide an unparalleled means for exploring a transcriptome with deep coverage and base pair level resolution.
Various analysis tools have been developed to align and assemble RNA-seq data, such as the widely used TopHat/Cufflinks pipeline.
A common observation is that a sizable fraction of the frag-ments/reads align to multiple locations of the genome.
These multiple alignments pose substantial challenges to existing RNA-seq analysis tools.
Inappropriate treatment may result in reporting spurious ex-pressed genes (false positives) and missing the real expressed genes (false negatives).
Such errors impact the subsequent analysis, such as differential expression analysis.
In our study, we observe that3.5% of transcripts reported by TopHat/Cufflinks pipeline correspond to anno-tated nonfunctional pseudogenes.
Moreover,10.0% of reported tran-scripts are not annotated in the Ensembl database.
These genes could be either novel expressed genes or false discoveries.
Results: We examine the underlying genomic features that lead to multiple alignments and investigate how they generate systematic errors in RNA-seq analysis.
We develop a general tool, GeneScissors, which exploits machine learning techniques guided by biological knowledge to detect and correct spurious transcriptome inference by existing RNA-seq analysis methods.
In our simulated study, Gen-eScissors can predict spurious transcriptome calls owing to misalign-ment with an accuracy close to 90%.
It provides substantial improvement over the widely used TopHat/Cufflinks or MapSplice/ Cufflinks pipelines in both precision and F-measurement.
On real data, GeneScissors reports 53.6% less pseudogenes and 0.97% more expressed and annotated transcripts, when compared with the TopHat/Cufflinks pipeline.
In addition, among the 10.0% unannotated transcripts reported by TopHat/Cufflinks, GeneScissors finds that 416.3% of them are false positives.
Availability: The software can be downloaded at http://csbio.unc.edu/ genescissors/ Contact: weiwang@cs.ucla.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION RNA-seq techniques provide an efficient means for measuring transcriptome data with high resolution and deep coverage (Ozsolak and Milos, 2011).
Millions of short reads sequenced from cDNA provide unique insights into a transcriptome at the nucleotide-level and mitigate many of the limitations of microarray data.
Although there are still many remaining unsolved problems, new discoveries based on RNA-seq analysis ranging from genomic imprinting (Gregg et al., 2010) to differ-ential expression (Anders and Huber, 2010; Trapnell et al., 2012) promise an exciting future.
Current RNA-seq analysis pipelines typically contain two major components: an aligner and an assembler.
An RNA-seq aligner [e.g.
TopHat (Trapnell et al., 2009), SpliceMap (Au et al., 2010) and MapSplice (Wang et al., 2010)] attempts to determine where in the genome a given sequence comes from.
An assembler [e.g.
Cufflinks (Trapnell et al., 2010) and Scripture (Guttman et al., 2010)] addresses the problems of which transcripts are present and estimating their abundances.
Existing RNA-seq pipelines can be divided into two major categories: align-first pipelines and assembly-first pipelines (Ozsolak and Milos, 2011).
Assembly-first pipelines attempt to assemble and quantify the complete transcriptome without a ref-erence.
Several algorithms, such as Trinity (Grabherr et al., 2011) and TransABySS (Robertson et al., 2010), have been developed.
However, alignment to a reference genome is still necessary to interpret the results from an assembly-first pipeline and to relate them to existing knowledge.
The assembly-first pipeline is com-pute-intensive and may require several days to complete.
In align-first pipelines, a high-quality reference genome serves as a scaffold for inferring the source of RNA-seq fragments.
Current alignment approaches are both computationally efficient and easily parallelized.
Thus, the align-first RNA-seq analysis can be finished within hours even on a normal desktop machine.
Therefore, align-first pipelines such as TopHat/Cufflinks (Trapnell et al., 2010, 2012) or MapSplice/Cufflinks (Wang et al., 2010) are generally preferred when a suitable reference genome is available.
1.1 Multiple-alignment problem In this article, we assume that the RNA-seq data are paired-end reads, which are widely used for transcriptome inference.
Our approach can be used for single-end reads as well.
In paired-end RNA-seq data, a fragment is a sub-sequence from an expressed transcript.
High-throughput sequencing provides two reads corresponding to the two ends of the fragment.
If a*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com fragment can be mapped to more than one location in the genome, we say that this fragment has multiple alignments, as showed in Figure 1.
As each fragment originates from one loca-tion in the genome, multiple alignments must be pro-cessed/corrected before subsequent analysis can proceed.
Inappropriate handling of the multiple alignment fragments im-pacts the subsequent analysis and may lead to questionable con-clusions.
For example, the widespread RNA and DNA sequence differences (Li et al., 2011) are suspected to be (at least partially) due to systematic technical errors, including misalignments (Kleinman and Majewski, 2012).
Current RNA-seq analysis pipelines handle the multiple-align-ment problem in both the alignment and assembly steps.
Most existing aligners [e.g.
TopHat (Trapnell et al., 2009)] use a scor-ing system where only the alignments with the best score are kept.
However, a fragment may still have multiple alignments with equally good scores.
In our experiments on real mouse RNA-seq data, we observe that at least 5% fragments have mul-tiple alignments.
The assembler [e.g.
Cufflinks (Trapnell et al., 2010)] either assumes that they contribute equally to each loca-tion or uses a probabilistic model to estimate their contributions based on the abundance of the corresponding transcripts (Li et al., 2010).
1.2 Genomic factors causing multiple alignments In general, multiple alignments are caused by the existence of paralogous sequences within a genome.
Duplicated and repetitive sequences need not be strictly identical.
In this subsection, we discuss genomic factors that may lead to multiple alignments and their impact on RNA-seq analysis.
Retrotransposition and gene duplication are two biological phenomena that generate se-quences with high levels of nucleotide similarity.
Interspersed highly repetitive sequences, such LINEs and SINEs, can be ex-pressed in an autonomous or nonautonomous manner, but they are not our focus.
That leaves us with three major types of gen-omic factors: processed pseudogenes (Balakirev and Ayala, 2003; Vanin, 1985; Zhang et al., 2003), nonprocessed pseudogenes (Hurles, 2004) and repetitive sequences shared by gene families (Hasler et al., 2007; Jurka and Smith, 1988).
Pseudogenes (Harrison et al., 2003; Khelifi et al., 2005) are typically nonfunctional, though some of them may be expressed (Hirotsune et al., 2003).
They can be further categorized in two groups: processed pseudogenes and nonprocessed pseudogenes based on their causes.
Both of them lead to the repetitive gen-omic sequences.
In general, the pseudogenes are nonfunctional and under reduced selection pressure; thus, they typically exhibit a higher mutation rate than the expressed genes from which they originated.
1.2.1 Processed pseudogene A processed pseudogene (Vanin, 1985) is generated when an mRNA is reverse transcribed and reintegrated back to the genome.
The resulting DNA sequence of the processed pseudogene is the concatenated exon sequences from its original transcript.
Because there are no splice junctions in the sequence of the processed pseudogene, it is easier for the current RNA-seq aligners to map the fragments to processed pseudogene than the actual gene from which they are expressed, especially those fragments that cross a splice junction.
Both the unexpressed pseudogene and its corresponding expressed gene may be reported by the assembler if the implementation of the assembler does not consider such cases.
For example, Guttman et al.
(2010) observed that a few highly expressed transcripts may not be able to be fully reconstructed owing to alignment artifacts caused by the processed pseudogenes.
1.2.2 Nonprocessed pseudogene Nonprocessed pseudogenes (Hurles, 2004) are typically caused by a historical gene duplica-tion event, followed by an accumulation of mutations, and an eventual loss of function.
Nonprocessed pseudogenes often share similar exon/intron structures with their originating gene.
From the aligners perspective, fragments can be mapped to either the expressed original gene, or its nonprocessed pseudogene, or both.
Similar to processed pseudogenes, the assembler may report a nonprocessed pseudogene when its corresponding functional genes are expressed.
1.2.3 Repetitive shared sequences Besides pseudogenes, many functional gene families share subsequences that are almost iden-tical to each other.
One repetitive sequence shared by different genes in human genome is Alu (Hasler et al., 2007; Jurka and Smith, 1988).
Consider the case when, among all genes that share the Alu sequence, but only a subset is expressed.
Hence, the aligner will map the fragments originating from the expressed subset to all similar sequences on the genome.
The assembler may report all genes sharing the repetitive sequence as being expressed.
Any of these three biological factors may lead to multiple alignments.
Without proper post-processing, an assembler may report many unexpressed pseudogenes or even random regions as expressed genes, and it may also miss a few highly expressed genes.
Existing RNA-seq analysis pipelines provide heuristics for ad-dressing the multiple alignment problem, however, they do not explicitly consider their genomic causes.
In our study, using mouse RNA-seq data, the transcripts reported by Cufflinks in-clude 3.5% from known pseudogenes and 10% from unan-notated regions.
A quarter of these 13.5% transcripts are likely to be false positives caused by multiple alignments.
Figure 2 shows the pile-up plots of two regions from a mouse genome reported by a current RNA-seq pipeline.
The top one is a gene named Caml3, whereas the bottom one is unknown.
The unknown genes sequence is similar to the sequence of concate-nated exons from Caml3.
Fragments that are uniquely aligned to the unknown gene by the aligner can also be aligned to Caml3.
However, the aligner fails to find the proper alignment because it does not consider all possible alignments crossing splice junctions Read Read Fragment Genome Fragmmentm Alignment Alignment Fig.1.
A fragment with paired end reads that can be aligned to two locations in the genome i292 Z.Zhang et al.
owing to the search complexity.
This collection of evidence indi-cates that the unknown gene is actually an unannotated pro-cessed pseudogene of Caml3.
Therefore, the identification of expressed genes and unex-pressed pseudogenes is a significant confounding factor in RNA-seq analysis.
No existing analysis methods explicitly at-tempt to identify and reassign fragments that are mapped to pseudogenes.
A similar observation was made by ContextMap (Bonfert et al., 2012) that multiple alignments from a RNA-seq aligner could be handled by removing the incorrect alignments based on the context of the alignments.
However, ContextMap simply defines the context as a fixed window around the align-ment on the genome.
It also does not try to rescue any missed alignments.
In contrast, we introduce the concept of fragment attractor, which leverages the results from both an aligner and an assembler to determine the appropriate context for each indi-vidual alignment.
Sharing maps between fragment attractors are built to help discover and restore missed alignments.
In this article, we introduce the GeneScissors pipeline, a com-prehensive approach to address the problem of detecting and correcting those fragments errantly aligned to unexpressed gen-omic regions.
When compared with the standard TopHat/ Cufflinks pipeline, GeneScissors is able to remove 57% pseudo-genes without using any annotation database.
GeneScissors can reduce inference errors in existing analysis pipelines and aid in distinguishing truly unannotated genes from errors.
2 METHODS In this section, we present GeneScissors, a general component that can be applied to any align-first RNA-seq pipeline to detect and correct errors in transcriptome inference owing to fragment misalignments.
In a standard RNA-seq pipeline, the best alignment for a fragment with multiple alignments is determined without considering the surrounding alignments of other fragments.
Such decisions may be premature without considering the other fragments aligned to these regions.
In the GeneScissors pipeline, we first collect all possible alignments for all fragments, and then examine those regions of the genome where multiple alignments map and then consider the other fragments aligned to these regions.
In this way, GeneScissors is able to leverage statistics of fragment distribution and other features of the alignments.
Figure 3 describes the proposed workflow for RNA-seq analysis.
It uses existing aligner and assembler (with minor modifications to keep all possible alignments discovered, details in Section 3.1) to identify regions to which fragments align.
To distinguish from expressed genes, we refer to each such region as a fragment attractor.
Fragments with multiple align-ments link corresponding fragment attractors.
We refer to these frag-ments and their alignments as shared fragments and shared alignments, respectively.
The relationships among linked fragment attractors are defined by their shared fragments.
GeneScissors uses sharing graphs to represent the linked fragment attractors and to discover new fragment alignments.
We create training instances using simulated RNA-seq frag-ments from annotated genes in Ensembl to build a classification model.
Then, on real data, the classification model predicts and removes the fragment attractors that are likely due to misalignments.
Existing assem-bly methods can be applied on the remaining fragment alignments to re-estimate the abundance level of expressed fragment attractors.
We introduce the sharing graph in Section 2.1, a classification model to iden-tify the unexpressed fragment attractors in Section 2.2 and the features extraction method from the sharing graphs in Section 2.3.
2.1 Sharing graph We construct sharing graphs as follows.
Each fragment attractor is rep-resented by a node, and each pair of linked fragment attractors are con-nected by an edge.
Each connected component is called a sharing graph.
For each edge in a sharing graph, we build a position-by-position sharing map between the pair of linked fragment attractors through their shared fragments.
For any fragment f aligned to a fragment attractor g, we first define function f)g, which returns the aligned position in fragment at-tractor g, given a position in fragment f and its inverse function 1g)f, which returns the corresponding position in f (if it exists), given a position in g. For a pair of linked fragment attractors ga and gb and one of their shared fragments f1, position k in f1 may be aligned to position i in fragment attractor ga and position j in gb.
This provides a correspondence between position i in ga and position j in gb by j f1)gb 1ga)f1 i and i f1)ga 1gb)f1 j.
A sharing map can be built between ga and gb through this approach by using all their shared fragments.
It is possible that two shared fragments f1 and f2 map the same position in ga to two different positions in gb, i.e.
f1)gb 1ga)f1 i 6 f2)gb 1 ga)f2 i. Empirically, such cases are rare, and when it happens, we use the majority rule to resolve the conflict.
The region of a fragment attractor that is covered by the sharing map is called the shared region.
In addition to the shared fragments, some other fragments uniquely aligned to the fragment attractor may align to the shared region.
These fragments should have been aligned to the linked fragment attractor too, but the aligner might have failed to recog-nize the alignments owing to the reasons we discussed previously.
Therefore, with the help of the sharing map, we can restore these missed alignments from existing aligners result.
For example, in Figure 4a, we show a sharing graph among three fragment attractors.
The red regions in the bottom row of each fragment attractor are the shared regions.
The red dashed boxes contain the fragments uniquely aligned to one fragment attractor by the aligner but should have been aligned to the linked fragment attractors too.
In Figure 4b, we show more details on how the new alignments of the fragments are established through the sharing map.
This alignment discovery operation needs to be done in both directions for each pair of linked fragment attractors.
In our previ-ous example in Figure 2, the uniquely aligned fragments (between the black curve and the red curve) in the shared regions should have been Fig.2.
Two transcripts reported by Cufflinks.
The top one maps to a known gene named Caml3, and the bottom one does not map to any known gene.
Two transcripts are aligned by their shared fragments in the plot.
Owing to the space limitation, the top figure is truncated, and only shows the region containing shared fragments.
The dashed line indicates the truncated boundary.
The three vertical lines in purple represent three splice junctions in the top transcript i293 GeneScissors aligned to both fragment attractors.
Restoring fragment alignments to multiple positions does not cause inflation in abundance level estimation because transcriptome inference methods such as Cufflinks already con-sider the shared alignments.
This approach enables us to safely rescue fragment alignments missed by an aligner.
2.2 Classification model GeneScissors processes RNA-seq data at the granularity of linked frag-ment attractors.
Because there is no easy way to determine whether a fragment attractor are expressed in real datasets, we build our training model from simulated data and apply it to real data.
We first generate our training set from a simulated population, and each sample is a set of fragments simulated based on a set of selected transcripts from the an-notation database.
(More details are in Section 3.1).
Then, we apply the aligner and the assembler on each sample of the simulated data, build the sharing graphs based on their results and generate training instances from the sharing graphs.
The fragment attractors that cannot be mapped back to the selected transcripts are unexpressed ones.
We use a classification model to infer whether a fragment attractor (hereby referred to as the target fragment attractor gt) is expressed using features of gt and another fragment attractor (hereby referred to as the assistant fragment attractor ga) linked to gt by an edge in the sharing graph.
For every pair of linked fragment attractors, we build two instances.
The instance is labeled ac-cording to whether the target fragment attractor is expressed.
Therefore, one fragment attractor may be the target fragment attractor in multiple instances.
The intuition is that, for an unexpressed target fragment at-tractor, there should always be some instances in which the assistant fragment attractors are expressed.
In such instances, the assistant frag-ment attractor should have less consistent mismatches, longer sequence and lower proportion of shared fragments than the target fragment at-tractor (More details are in Section 2.3, which describes all features we use).
Thus, we can train a binary classification model using these features to identify unexpressed target fragment attractors.
When we apply the model to test data and real data, all target fragment attractors, which are predicted as unexpressed at least once will be removed from the result of the assembler, and the reads that are uniquely aligned to these fragment attractors will be redistributed to the corresponding expressed fragment attractors.
We experimented with support vector machines (SVMs), DecisionTrees and RandomForests as the learning method and found that RandomForests had the best overall performance.
Once the classifier is built, we apply it on test data to evaluate the prediction accuracy and then apply it to real data to predict unexpressed fragment attractors and remove their fragment alignments.
Recall that, for all uniquely aligned fragments in the shared regions of these fragment attractors, we also discover new alignments to their linked fragment attractors using the sharing map.
2.3 Fragment attractor features We extract features from both target fragment attractor gt and assistant fragment attractor ga in each instance.
Each instance contains 14 features, listed in Table 1.
All features except the number of consistent mismatch locations are straightforwardly calculated: features NE and NI are dir-ectly collected from the assemblers output, and NR, MF, MR and CM are calculated by our sharing graph generator.
The use of consistent mismatch count CM, as a feature is motivated by the observation that the pseudogenes usually have higher mutation rate.
The concept of con-sistent mismatch and the method to find consistent mismatch locations across the genome are described in Appendix 1.
The number of exons is helpful in distinguishing processed pseudogenes, which are singletons.
(b) (a) Fig.4.
(a) A sharing graph of three fragment attractors A, B and C. Each solid box represents a pile-up of fragments of a fragment attractor.
Each pair of connected hollow rectangles represents a fragment of paired end reads.
The red fragments are the shared fragments that can be mapped by the aligner to all three fragment attractors.
The bottom row in each box represents the transcript sequence.
The red regions (except the splice junctions in the transcript sequences) are the region to which the shared fragments align.
(b) A sharing map between fragment attractors A and C and the discovered new alignments (shown in dashed rectangles).
These new alignments are rescued from the uniquely aligned fragments in the shared region of one of the two fragment attractors Fig.3.
The workflow of GeneScissors Pipeline.
The traditional RNA-seq analysis pipeline is the path on the left side.
Its alignment and assembly results are used by GeneScissors to infer fragment attractors, build shar-ing graphs and identify all fragment alignments in the genome.
GeneScissors then builds a classification model to detect and remove unexpressed genes i294 Z.Zhang et al.
All the other features are motivated by our observation that the unex-pressed fragment attractors tend to have smaller number of alignment fragment and shorter region than their corresponding expressed ones.
3 RESULTS We first describe a series of modifications made to open-source RNA-seq analysis tools to support GeneScissors.
Then, we de-scribe the various datasets used for evaluation.
We evaluated two standard pipelines that do not use GeneScissors: one using TopHat and the second using MapSplice as an aligner.
We then added GeneScissors to each pipeline, to improve the align-ment results, and we refer to these as GeneScissors(TopHat) and GeneScissors(MapSplice) pipelines.
All four pipelines use Cufflinks as the transcriptome assembler.
3.1 Software GeneScissors uses modified versions of TopHat and Cufflinks and uses components written in C, Python and the BamTools (Barnett et al., 2011) library.
Cuffcompare is used to map the reported genes back to Ensembl annotations and categorize them into three types: annotated normal genes/tran-scripts, annotated pseudogenes and unannotated regions.
3.1.1 Modifications to TopHat and Cufflinks We first present the algorithms used by TopHat and Cufflinks in ranking and reporting alignments and genes and then discuss our modifica-tions to retain all fragment and partial fragment (unpaired reads) alignments.
In TopHat, if the fragment f has multiple alignments x and y, TopHat retains only alignment y and does not report alignment x, when one of the following conditions is satisfied (tests are applied in order): Mismatch rule: x has more mismatches than y. Splice junction rule: x crosses more splice junctions than y.
Other rules: Owing to the space limitations, we omit the conditions that are not relevant to the article.
Only alignments with the best score are reported by TopHat.
We observed that the splice-junction rule tends to favor pro-cessed pseudogenes; the correct alignment of a fragment with a splice junction is frequently discarded by TopHat if the fragment can be aligned to a processed pseudogene with the same number of mismatches.
In Cufflinks, a gene that meets the following criteria is suppressed: 75% rule: More than 75% of the fragment alignments sup-porting the gene are mappable to multiple genomic loci.
Consider the example shown in Figure 2.
Cufflinks fails to remove the unannotated pseudogene, which is composed mostly of uniquely aligned fragments.
This suggests that the 75% rule is insufficient.
Therefore, in the GeneScissors pipeline, we disabled the splice junction rule in TopHat and the 75% rule in Cufflinks.
3.1.2 Simulator To generate training data for our classification model and evaluate the effectiveness of GeneScissors for detect-ing and removing unexpressed fragment attractors, we built a RNA-seq simulator to provide a ground truth model for fragment attractors.
The simulator randomly chooses a (user-specified) number of genes, and for each gene, it samples a subset of its transcripts.
Then, it uniformly samples paired-end fragments up to a certain abundance level for each selected transcript.
For each fragment, it assigns a quality score to each base pair, drawing from an empirical distribution derived from real data, and generates base pair errors based on their quality scores.
3.2 Data Our study used inbred and F1 crosses of three mouse strains: CAST/EiJ, PWK/PhJ and WSB/EiJ.
To minimize the impact of unknown SNPs to the alignments, we generated strain-specific genomes by incorporating high-confidence SNPs detected in a recent DNA sequencing project of laboratory mouse strains con-ducted by the Welcome Trust (Keane et al., 2011) into the mm9 reference genome.
We used the Ensembl database (build 63) (Flicek et al., 2011) to annotate and evaluate the results from real and simulated data.
3.2.1 Simulated Data A RNA-seq simulator was used to gen-erate synthetic data from 60 RNA-seq samples also derived from three inbred mouse strains: CAST/EiJ, PWK/PhJ and WSB/EiJ.
In each sample, we selected 13, 000 annotated functional genes in Ensembl as the expressed genes and randomly set them to different levels of abundance.
Many genes included multiple transcripts.
We generated 10 million fragments with 100 base pair paired-end reads for each sample.
We used TopHat and Table 1.
The features used for detecting fragment attractors resulting from misalignments Features Description NEga 1, NEgt 1 NEga andNEgt are the observed numbers of exons.
These two Boolean features tell whether the genes are singleton of exons.
NRga, NRgt, NRga=NRgt NRga, NRgt are the proportions of the fragments that can be aligned to ga and gt to the total fragments, respectively.
MFga, MFgt, MFga=MFgt MFga, MFgt are the proportions of the shared fragments to the fragments aligned ga and gt, respectively.
MRga, MRgt, MRga=MRgt MRga, MRgt are the proportions of the entire regions of ga and gt that are covered by shared fragments.
CMga, CMgt, CMga CMgt CMga, CMgt are the numbers of base pairs that have consistent mismatches in the shared regions of ga and gt, respectively.
i295 GeneScissors MapSplice as aligners and Cufflinks as the assembler to analyze the simulated data.
More than 7.5% of the genes reported in the results were not from the selected genes in our simulation setting.
From the results, we built shared graphs and used cross-validation to train and test our model.
A feature selection study using the simulated data can be found in the supplementary material.
3.2.2 Real data We applied GeneScissors to RNA-seq data from nine inbred samples and 53 F1 samples derived from three inbred mouse strains CAST/EiJ, PWK/PhJ and WSB/ EiJ.
We sequenced cDNA from mRNA extracted from brain tissues of three to six replicates of both sexes and the six possible crosses (including the reciprocal).
To mitigate misalignment errors owing to heterozygosity, for each F1 sample, we aligned each fragment to the genome of each parent separately (i.e.
the mm9 reference sequence with annotated SNPs) and then merged the two alignments while retaining all distinct multiple alignments (a union of the set of all mapped frag-ments each identified by their mapping coordinate and read identifier).
For comparison purposes, we also applied this alignment strategy in the TopHat and MapSplice pipelines.
3.3 Results from simulated data In Table 2, we first present the average precision, recall, F scores and Area under the Curve when LinearSVM, DecisionTree and RandomForests were used to build the classification models.
All scores were measured by 10-fold cross-validation.
The results demonstrate that our feature set is adequate and can help detect unexpressed genes efficiently.
The RandomForests is the best and most consistent among all three methods.
The classifi-cation model trained by RandomForests can detect near 90% spurious calls owing to misalignments.
Though SVM has a slightly higher precision score, the recall is much lower than RandomForests.
This is because RandomForests is more suit-able than SVM for data with discrete features and is more powerful in handling correlations between features.
Therefore, we chose RandomForests as the default classification method for our GeneScissors pipeline.
Next, we investigated how much improvement GeneScissors could bring to the overall transcriptome calling by correcting fragment misalignment.
We compared the results of our im-proved GeneScissors pipelines with those from the TopHat and MapSplices pipelines.
Both GeneScissors pipelines used the modified version of Cufflinks.
The GeneScissors (TopHat) pipe-line used the modified version of TopHat.
The MapSplice and TopHat pipelines used the regular version of Cufflinks.
We used the following three measurements to compare the performance at the gene level: GenePrecision Number of Correct Genes Number of Reported Genes , 1 GeneRecall Number of Correct Genes Number of Simulated Genes , 2 GeneFmeasurement 2 GenePrecision GeneRecall GenePrecision GeneRecall : 3 The results of different pipelines are summarized in Table 3.
All statistics are averaged over a 10-fold cross-validation.
We observe that Cufflinks tends to report a much higher number of genes in all four pipelines.
There are only 13 000 expressed genes, but Cufflinks reports 430000 genes in the TopHat or MapSplice pipelines and 426 000 genes in the GeneScissors pipelines.
A significant percentage of these reported genes can be mapped back to the expressed genes from which we generated synthetic reads.
Several reported genes are often mapped back to the same expressed gene by Cuffcompare.
Cufflinks failed to recognize them as (possibly different transcripts of) the same gene, perhaps owing to both the length and variable number of splice junctions and/or the low fragment coverage seen for some transcripts.
In this case, when we computed GenePrecision and GeneRecall, only one of them was counted as the correct gene, the remaining ones were counted as incorrect genes.
As all four pipelines used Cufflinks to infer transcriptome, all of them had relatively low GenePrecision.
The GeneScissors (MapSplice) pipeline had a 12.6% improvement in GenePrecision over the original MapSplice pipeline, at the cost of a slight drop in GeneRecall.
The GeneScissors (TopHat) pipeline had a 6.5% improvement in GenePrecision over the TopHat pipeline while retaining the same level of GeneRecall.
GeneScissors was able to detect and remove44000 spurious (gene) calls by correcting frag-ment misalignments.
We also observed that the MapSplice pipeline has the highest score on GeneRecall, but a much lower GenePrecision score comparing with TopHat pipeline and GeneScissors pipeline.
This is because MapSplice can find more possible alignments than TopHat but is not able to identify the correct alignment when a fragment has multiple alignments.
Hence, the MapSplice pipeline reported more false positives than the TopHat pipeline.
Overall, the GeneScissors (TopHat) pipeline performed best among the four pipelines on this challenging test case.
It is ob-vious that (i) detecting and correcting fragment misalignments can improve the accuracy in transcriptome inference under all circumstances and (ii) given the correct fragment alignments, better transcriptome inference algorithms are still needed.
In addition, GeneScissors does not assume all pseudogenes are un-expressed.
GeneScissors is able to distinguish expressed pseudo-genes from the rest with a comparable accuracy, demonstrated by a simulation study in the supplementary material.
3.4 Results from real RNA-seq data We also applied both TopHat pipeline and our GeneScissors (TopHat) pipeline on the real RNA-seq data.
The running Table 2.
Summary of the results from different classification methods Statistics LinearSVM DecisionTree RandomForests Precision 81.90% 83.70% 89.60% Recall 83.00% 84.80% 87.80% F-measurement 85.70% 84.20% 88.60% Area under the curve 0.843 0.837 0.91 i296 Z.Zhang et al.
time for TopHat pipeline was 24h per sample, and the extra running time for GeneScissors (TopHat) pipeline were 10h per sample.
Overall, the GeneScissors (TopHat) pipeline reported 4.25% fewer transcripts in real data than the TopHat pipeline (Fig.5a).
Considering that GeneScissors removed most of false positives in our simulation study, it suggests that the transcripts reported by the TopHat pipeline include a significant number of false positives.
Despite the fewer number of transcripts reported by GeneScissors, Figure 5b shows that GeneScissors actually re-ported 0.97% more transcripts that exactly match or partially match the splice junction annotations in the Ensembl database than the TopHat pipeline (The improvement is statistically sig-nificant with a P-value lower than 1014 under the paired stu-dents t-test).
These transcripts are likely the false negatives missed by the TopHat pipeline owing to misalignments.
(a) (b) (c) (d) Fig.5.
Comparisons between multiple samples run through both the GeneScissors pipeline and the TopHat pipeline.
Results from the same sample are connected by an arrow.
The three strains used were CAST/EiJ, PWK/PhJ and WSB/EiJ, and they are indicated by the initials C, P and W, respectively.
The two letter designations indicate the direction of the cross with the initial of the maternal strain followed by the initial of the paternal strain.
The samples are clustered according to replicates from the same sex and F1 cross, followed by the reciprocal cross.
The sex is indicated by F(female) and M(male) Table 3.
Comparison of MapSplice, TopHat, GeneScissors (MapSplice) and GeneScissors (TopHat) pipelines Statistics MapSplice pipeline TopHat pipeline GeneScissors (MapSplice) GeneScissors (TopHat) Number of reported genes 36 516 30 622 26556 26473 GenePrecision 35.6% 41.8% 48.2% 48.3% GeneRecall 95.1% 93.2% 93.0% 93.2% GeneF-measurement 51.5% 58.2% 63.5% 63.6% Note: The bold value of each row represents the best pipeline measured by the corresponding metric.
i297 GeneScissors Figure 5c shows that the TopHat pipeline reported4800 tran-scripts that are annotated as pseudogenes in Ensembl.
GeneScissors managed to remove453.6% of them, and the frac-tion of transcripts that overlap any pseudogenes decreased from 3.2 to 1.57%.
Figure 5d shows that GeneScissors reported 16% fewer unannotated transcripts than the TopHat pipeline.
All these results indicate that GeneScissors is effective in detecting and correcting false positive and false negative transcript reports caused by fragment misalignments.
Furthermore, the number of pseudogenes reported by the ori-ginal TopHat/Cufflinks pipeline in inbred samples is fewer than the number in F1 hybrids.
Similarly, the fraction of pseudogenes (57%) removed by GeneScissors in the inbred samples is smal-ler than the fraction (36%) removed in the F1 hybrids.
This indicates that the additional complications of F1 samples pose additional challenges to RNA-seq analysis pipelines and makes them more prone to errors than the inbred samples.
4 DISCUSSION AND CONCLUSION In this article, we presented GeneScissors, a general approach to detect and correct transcriptome inference errors caused by mis-alignments, which can be applied to any RNA-seq analysis pipe-line.
GeneScissors considers three underlying biological factors that lead to fragment misalignments and spurious transcript re-porting.
We proposed a classification model to detect false dis-coveries owing to misalignment, and the results show that it can provide significant improvement in overall accuracy.
Other heuristic approaches have been used to avoid reporting unexpressed genes in the RNA-seq assembly result, such as dis-carding all known pseudogenes reported by the TopHat pipeline, masking repeated elements in genome or aligning fragments to known transcriptome instead of genome.
The key difference is that our RNA-seq analysis does not require any additional an-notations beyond adding SNPs, and it still supports a novel transcript discovery.
Transcript discovery is important because current annotations are incomplete with regard to genes, isoforms and allele-specific variants.
For example, in the real data, we observed4000 unan-notated transcripts clustered 2300 unannotated genes on aver-age.
These transcripts persist after applying GeneScissors, which attempts to identify and correct misaligned fragments.
This implies that current annotations are neither complete nor entirely accurate.
For example, recent studies (Hirotsune et al., 2003; Khelifi et al., 2005) found that some regions previously thought to be pseudogenes can actually be transcribed to mRNA.
Hence, removing all annotated pseudogenes or highly repeated regions may lead to the removal of actual expressed transcripts.
In con-trast, GeneScissors might choose a pseudogene over the anno-tated paralog based on which better matches known genetic variants.
Furthermore, current pipelines using Cufflinks tend to over-report genes, especially when the genes share a high degree se-quence similarity with other expressed genes in the data.
The problem is alleviated to some extent by GeneScissors by recover-ing missed multiple fragment alignments and discarding frag-ment alignments to unexpressed genes/regions.
However, there is still room for improvement.
In the future work, though our precision and recall scores are near 90%, we plan to exploit additional features and constraints to improve the classification accuracy.
Example constraints in-clude that each sharing graph must contain at least one expressed gene and each shared fragment must belong to an expressed gene.
In addition, we plan to investigate how to rescue the dis-carded fragment alignments to an unexpressed fragment attrac-tor, but not in the shared regions with any linked fragment attractors because these fragments should belong to some ex-pressed genes.
ACKNOWLEDGEMENTS The authors thank those center members who prepared and pro-cessed samples as well as those who commented on and encour-aged the development of GeneScissors; in particular, Weibo Wang, Isa-Kemal Pakatci, Zhishan Guo, John Calloway, James J. Crowley and Patrick F. Sullivan.
They also thank three anonymous reviewers for their thoughtful comments.
Funding: [NIMH/NHGRI P50 MH090338], [NIH GM P50 GM076468], [NSF IIS-1313606], [NSF IIS-0812464].
Conflict of Interest: none declared.
ABSTRACT Motivation: Recent improvement in homology-based structure modeling emphasizes the importance of sensitive evaluation measures that help identify and correct modest distortions in models compared with the target structures.
Global Distance Test Total Score (GDT_TS), otherwise a very powerful and effective measure for model evaluation, is still insensitive to and can even reward such distortions, as observed for remote homology modeling in the latest CASP8 (Comparative Assessment of Structure Prediction).
Results: We develop a new measure that balances GDT_TS reward for the closeness of equivalent model and target residues (attraction term) with the penalty for the closeness of non-equivalent residues (repulsion term).
Compared with GDT_TS, the resulting score, TR (total score with repulsion), is much more sensitive to structure compression both in real remote homologs and in CASP models.
TR is correlated yet different from other measures of structure similarity.
The largest difference from GDT_TS is observed in models of mid-range quality based on remote homology modeling.
Availability: The script for TR calculation is included in Supplementary Material.
TR scores for all server models in CASP8 are available at http://prodata.swmed.edu/CASP8.
Contact: grishin@chop.swmed.edu Supplementary information: All scripts and numerical data are available for download at ftp://iole.swmed.edu/pub/tr_score/ 1 INTRODUCTION The current improvement of methods for sequence-based structure prediction (Kopp et al., 2007; Shi et al., 2009) leads to the increasing importance of accurate and biologically relevant measures of structural model quality.
These measures are essential not only as criteria for benchmarking of method performance, but also as standards for the development of new, more powerful approaches to protein structure modeling.
Any measure rewards certain aspects of model quality.
Using the measure to guide methods design inevitably emphasizes these aspects in the produced models.
Among various measures of model quality, Global Distance Test Total Score (GDT_TS) is widely considered one of the most informative and robust.
Proposed in a seminal paper (Zemla et al., 2001) as a benchmarking criterion at early stages of Comparative To whom correspondence should be addressed.
Assessment of Structure Prediction (CASP), this measure has been successfully used by many independent assessor groups (Kinch et al., 2003; Kopp et al., 2007; Wang et al., 2005; Zemla et al., 2001).
In brief, GDT_TS is based on several targetmodel superpositions, each maximizing the number of equivalent target and model residues separated by a distance shorter than a given distance cutoff.
These maximized numbers are normalized by the chain length and then averaged over the superpositions, producing the final score.
Thus, GDT_TS is a reward for placing as many model residues as possible in a vicinity of their target counterparts.
This approach, although extremely effective in most cases, may still be insensitive to more modest deviations of models from native-like structure.
As an example, in the models based on remote sequence homologs GDT_TS does not penalize and may even reward unrealistic placement of several model residues in a close vicinity of a single target residue (Aloy et al., 2003).
When target and its closest homolog of known structure share only remote similarity, it is hard to predict the correct position of model residues based on the homolog as a template.
In this situation, placing model residues closer to each other may improve the chances of some of them being rewarded for closeness to the target.
Noted in previous CASPs (Aloy et al., 2003), this effect has become more pronounced among recent models (Shi et al., 2009; Fig.1).
The last years have brought a significant improvement in the accuracy of remote homology modeling.
As demonstrated by recent CASPs (Kopp et al., 2007; Shi et al., 2009), the current methods produce much more accurate models whose quality often differs only by a small GDT_TS margin; thus optimizing a method for higher GDT_TS might improve the apparent methods standing, even at the cost of less realistic models.
As a consequence, more fine-grained evaluation is often needed to distinguish between alternative models of reasonable quality.
Most of the existing approaches to structure comparison reward similar regions but do not explicitly penalize dissimilar regions.
Taking an analogy with physical forces, these approaches concentrate on attraction but not on repulsion.
It might have been reasonable a few years ago, when the quality of structure predictions was much poorer, and rewarding for the spatial closeness of structure regions to the target would suffice to discriminate between models.
It was important to detect any positive feature of a model, since there were more negatives about a model than positives.
Today, many models reflect structures well.
When the positives start to 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[16:49 8/4/2009 Bioinformatics-btp148.tex] Page: 1260 12591263 R.I.Sadreyev et al.
Fig.1.
Examples of non-native structure geometries in CASP8 models.
In (A) and (B), left is a helix from a target structure and right is the same region modeled by one of the servers.
(A) Unrealistic compression of a helix along the axis.
(Model helix is rotated for clarity.)
In the model, the helix is positioned at a different angle; its compression improves GDT_TS.B.
Smart compression: C distance restrictions are not violated, but helix distortion concentrates residues around the same point.
outweigh the negatives, it becomes important to pay attention to the negatives.
Here, we introduce an explicit repulsion component into the GDT_TS score, which penalizes for incorrect pairing of non-equivalent residues in the compared structures.
We show that unlike GDT_TS, the new measure is sensitive to mild structure compression and thus may be a valuable tool in discriminating biologically unrealistic structure predictions.
2 METHODS 2.1 Compression of real homologous domains We choose pairs of structural classification of proteins (SCOP) domain representatives that are confident homologs but do not have a strong structure similarity.
From the total of 100 000 ASTRAL40 domain pairs that share the same superfamily, we select two subsets based on different definitions of marginal structure similarity.
First set includes 27 000 pairs with lower range but still significant DALI Z-scores, DALI Z between 2.0 and 5.0.
Second set includes the lower third of all domain pairs ranked by DALI Z-score (30 000 pairs, DALI Z between 2.0 and 5.8).
In each pair, the set of equivalent residues is chosen according to the blocks (capital letters) of confidently aligned positions in DALI alignment.
In separate experiments, each of the domains was compressed (compression ratio of 0.9 1.0), followed by the calculation of GDT_TS and total score with repulsion (TR) on the set of equivalent residues.
2.2 Compression of the CASP-fold recognition models CASP targets and server models are downloaded from http://predictioncenter.
org/casp8.
We further processed target and model structures, split them into domains and assigned them to standard categories by prediction difficulty as described at http://prodata.swmed.edu/CASP8.
We use targetmodel pairs for 25 target domains in the fold recognition category, subject each model to varying degrees of compression and calculate corresponding GDT_TS and TR scores.
As an example in Figure 3B, we show the plot for models by Robetta server, which does not introduce significant distortions of local geometry.
2.3 Perturbation of the torsion angles We perturb torsion angles , in fold recognition models with no chain breaks.
We chose Robetta models as they have very few chain breaks: among 125 FR models by Robetta, only five models for the same target have a break.
To every original/ value, we add a randomly distributed Gaussian term with a mean of 0 and a certain SD.
The original structure is modified by consequent rotation of backbone segments according to the new angle values, keeping all other geometric parameters intact.
2.4 Correlation between TR and the existing scores For each CASP8 target, we perform GDT_TS-based ranking of first models submitted by servers and select top 10 models.
We average targetmodel similarity scores for those models and plot TR against other measures, with each target represented by a single point.
To make the scale of DALI Z-scores compatible to TR, we normalize them by DALI Z-scores for the comparison of target domain to itself.
3 RESULTS Analyzing structure predictions produced by automated servers in the recent CASP, CASP8 (Shi et al., 2009), we observe a number of models that are assigned high GDT_TS scores but include unrealistically distorted regions (Fig.1).
These regions often show violations of secondary structure geometry, shorter distances between C atoms, sidechain clashes and distortion of hydrogen bond patterns.
In fold recognition modeling, when the closest homolog with 3D structure is relatively distant from the target, it is often difficult to predict the correct location of target residues, even if the general positioning of secondary structure elements can be inferred from the sequence-based alignment.
Such situations typically involve ambiguity of angles between secondary structure elements, register shifts in alpha-helices and beta-strands and unknown loop conformations.
By definition, GDT_TS rewards relatively close positioning of equivalent C atoms in model and target, but does not penalize situations where two or more model residues are close to the same target residue.
Therefore, a GDT_TS-trained automatic predictor may in some cases choose to concentrate Catoms in a smaller volume, which increases the probability for the target residue to have the correct model residue in the vicinity, even if the correct residue is unknown.
A schematic example of such a conformation is shown in Figure 2A, where both correct and incorrect model C atoms are located closely to the target C .
Such deviations from native protein geometry may be loosely viewed as an implementation of the minimax principle from game theory: select the conformation that minimizes the maximal possible loss in the case of failure.
For example, for an ambiguous helix register, putting residues closer to each other may produce a positive contribution to GDT_TS even if the register is wrong (Fig.1).
In the case of unknown loop conformation, predicting an unrealistic collapsed loop bears less risk of large GDT_TS loss than predicting an extended (and locally correct) conformation that may deviate further from the target if overall orientation is wrong.
More surprisingly, even a simplistic uniform contraction of the model can in many cases produce higher GDT_TS (Fig.2; Section 3.3).
3.1 Score for model quality with explicit repulsion term To develop a measure sensitive to the observed structure compression, we complement GDT_TS with an explicit repulsion 1260 [16:49 8/4/2009 Bioinformatics-btp148.tex] Page: 1261 12591263 Structure similarity measure with penalty Fig.2.
Calculation of penalty for close positioning of non-equivalent residues in target and model.
(A) Schema of superposition for a pair of equivalent target and model residues (ai and bi, black circles).
To calculate the penalty, we count model residues located close to the target residue ai, excluding its counterpart bi and adjacent residues bi1, bi+1.
In this example, there is one residue located within a given distance d = 4 from the target residue (marked with an arrow).
(B) We average the numbers of such incorrect residues for three different distance cutoffs d = 1, 2 and 4 , and combine these averages for both target and model residues.
The resulting penalty is subtracted from the pairs GDT_TS score.
If the penalty is higher than the GDT_TS, the pairs contribution is set to zero.
term that penalizes for close spatial placement of incorrect residue pairs (Fig.2A).
We call this score TR.
The score is calculated as follows (Fig.2B).
(1) Superimpose model with target using local-global alignment method (LGA) (Zemla, 2003) in the sequence-dependent mode, maximizing the number of aligned residue pairs within distance cutoffs of d = 1, 2, 4 and 8 .
(2) For each pair of target and model residues (ai, bi), calculate a GDT_TS-like score: so = 1/4 (1 + 2 + 4 + 8), where d = 1 if C C distance< d and d = 0 otherwise.
(3) In LGA superposition for d = 4 , consider individual aligned residues in both structures.
For each residue R, choose residues in the other structure that are spatially close to R, excluding the residue aligned with R and its immediate neighbors in the chain.
Count numbers of such residues with CC distance to R within cutoffs of 1, 2 and 4 .
(As opposed to GDT_TS, we do not use the cutoff of 8 as too inclusive: in native proteins, this distance is not prohibited for different residues of the same chain.)
(4) The average of these counts defines the penalty assigned to a given residue R: p(R) = 1/3 (n1 + n2 + n4).
(5) Finally, for each aligned residue pair (ai, bi), the average of penalties for each residue = 1/2 [p(ai) + p(bi)] is weighted and subtracted from the GDT_TS score for this pair.
The final score is prohibited from being negative, in order to avoid rewarding shorter models limited to only the confident structure core: s=max(s0 w,0 ) .
Among tested values of weight w, we find that w = 1.0 produces the scores that are most consistent with the evaluation of model abnormalities by human experts.
The score for the two compared structures is calculated as the sum of scores for individual residue pairs, normalized by target chain length L : TR=(1/L)si.
The cutoff of 4 for the used LGA superposition was set after testing multiple values around typical GDT_TS distance cutoffs.
This value produced the scoring of CASP8 models that was most consistent with the manual expert assessment of the model quality.
3.2 The new score has improved sensitivity to model distortions As an example of effects of mild structure distortion on GDT_TS and TR, we observe behavior of these scores in two simplistic experiments.
In each experiment, we calculate the scores on a pair of structures and then perturb one of the structures.
We vary the degree of the perturbation and observe corresponding changes in GDT_TS and TR scores.
In the first experiment, we uniformly contract one of the structures towards its center of mass, so that its radius of gyration decreases and its residues become slightly closer to each other (Fig.3A, B).
In the second experiment, we perturb torsion angles , by adding a random Gaussian term to each angle in the structure (Fig.3C).
By design, TR score is expected to bring improvement in sensitivity at medium levels of modeling difficulty, where sequence alignment to a homolog template is possible yet non-trivial.
TR should not be better than GDT_TS either for models based on close homology where modeltarget misalignments are rare, or for template-free models where even a general fold prediction is a challenge.
Therefore in our experiments, we concentrate on cases of clear yet distant templatetarget homology.
1261 [16:49 8/4/2009 Bioinformatics-btp148.tex] Page: 1262 12591263 R.I.Sadreyev et al.
Fig.3.
Effects of structure distortion on GDT_TS and TR scores.
(A and B) One of the two compared proteins is uniformly compressed, and dependency of the scores on the degree of compression is shown relative to the score for intact structures.
(A) Pairs of remotely homologous protein domains.
Average relative scores are based on the set of domain pairs that share the same SCOP superfamily and have DALI Z-score between 2 and 5.
Inset: the same dependency for a different definition of remote homologs: lower third (by DALI Z-score) of all domain pairs that share the same SCOP superfamily and have DALI Z > 2.0.
(B) Targets and models in the fold recognition category of the recent CASP, CASP8.
As an example, average relative scores are based on models by Robetta server.
The dependency is shown for the whole set of FR domain models by Robetta (125 models).
Inset: dependency for the subset of FR domain models whose GDT_TS grows with compression (48 models).
(C) Random perturbation of torsion angles (,) in Robetta models of FR domains.
Dependency of relative score values on the SD of added random Gaussian perturbations.
Fig.4.
Correlation of the new score with existing scores for structure similarity.
For each CASP8 target, average scores for the top 10 first server models are plotted.
(A) TR versus GDT_TS.
(B) TR versus DALI Z-score (scaled by self Z-score).
(C) TR versus TM score.
In the first experiment (Fig.3A), we use real pairs of remotely homologous protein domains.
Among all pairs of ASTRAL40 (Chandonia et al., 2004) representatives that share the same superfamily in SCOP classification (Andreeva et al., 2008), we choose 27 000 pairs with marginal structure similarity according to DALI Z-scores (Holm and Sander, 1993) and compress one of the domains in each pair.
On average, GDT_TS does not decrease until compression reaches 5% (Fig.3A).
Moreover, in 40% cases GDT_TS actually increases at mild compression levels.
In contrast, TR is consistently reduced on compressed structures, with the rate of reduction much higher than for GDT_TS.
We perform the same experiment (Fig.3B) on pairs of target and model domains from automatic CASP8 server predictions in the fold recognition category, as defined by our analysis (Shi et al., 2009).
Similar to the first dataset, mild compression leads to GDT_TS increase in 40% cases.
Figure 3B shows the average degree of GDT_TS gain for these models, as well as for the whole model set.
At the same time, TR penalizes the compression much more effectively.
Full sets of GDT_TS and TR scores for all server models of CASP8 are available at http://prodata.swmed.edu/CASP8.
When we perturb torsion angles in the same set of models, TR decreases with the variance of the perturbation significantly faster than GDT_TS, although the difference is somewhat less pronounced (Fig.3C).
3.3 Correlation with existing measures for structure similarity We compared the new score with other similarity scores based on the set of first server models produced for all CASP8 targets.
Figure 4 shows correlation plots for TR versus GDT_TS, DALI Z-scores, and TM scores (Zhang and Skolnick, 2005).
The table of correlation coefficients of TR with these and other measures is included in the supplement.
It is clear that GDT_TS and TR scores are well correlated (Fig.4A), with Pearson correlation coefficient of 0.99.
By design, TR is always equal or lower than GDT_TS.
Notably, the trend curve of the correlation is concave, so that the difference is more pronounced around the mid-range GDT_TS.
This range roughly corresponds to the targets from the categories of harder comparative modeling and fold recognition, where models become less similar to targets and modeled residues are frequently placed nearby non-equivalent residues, which results in higher penalty by TR.
For very low model 1262 [16:49 8/4/2009 Bioinformatics-btp148.tex] Page: 1263 12591263 Structure similarity measure with penalty quality (GDT_TS below 30%) the reward is much lower, and penalty drops as well (Fig.4A).
TR also shows general correlation with other similarity measures, although to a lesser degree.
Correlation coefficient with conceptually different contact-based DALI Z-scores (normalized to have a compatible scale) is relatively high (0.95).
Interestingly, TM score (Zhang and Skolnick, 2005) based on a concept similar to GDT_TS is less correlated with TR than DALI Z-scores (Fig.4C, correlation coefficient of 0.88), but the trend curves concave shape is similar to the plot of TR against GDT_TS.
In conclusion, we develop a new measure for protein structure similarity with explicit repulsion term that penalizes for spatially close positioning of non-equivalent residues.
This measure improves sensitivity of GDT_TS to moderate structure distortion and has a potential value for the assessment of structure similarity and homology-based structure modeling.
ACKNOWLEDGEMENTS We would like to thank Yang Zhang for stimulating discussions and critical reading of the manuscript and Bong-Hyun Kim for sharing DALI Z-scores for ASTRAL domain pairs.
The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing high-performance computing resources.
Funding: National Institutes of Health (GM67165 to N.V.G.).
Conflict of Interest: none declared.
Abstract Although three-dimensional protein structure determination using nuclear magnetic res-onance (NMR) spectroscopy is a computationally costly and tedious process that would benefit from advanced computational techniques, it has not garnered much research attention from special-ists in bioinformatics and computational biology.
In this paper, we review recent advances in com-putational methods for NMR protein structure determination.
We summarize the advantages of and bottlenecks in the existing methods and outline some open problems in the field.
We also dis-cuss current trends in NMR technology development and suggest directions for research on future computational methods for NMR.
Introduction Nuclear magnetic resonance (NMR) spectroscopy is one of the main methods for determining three-dimensional (3D) struc-tures of proteins [1].
The underlying idea for NMR protein structure determination is that if a large number of distance constraints are known between atom pairs of a target protein, the conformational space of possible protein structures will be restricted to a few structures [2].
The physical principle of NMR structure determination is that when a certain isotope (e.g., 1H, 13C or 15N) is placed in a strong magnetic field, the nucleus will absorb electromagnetic radiation at a frequency that is characteristic of the isotope.
Depending on different X).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ng Institute of Genomics, Chinese Ac local chemical and geometric environments, different nuclei resonate at different frequencies.
Since frequency is a magnetic field-dependent measure, it is often converted into a relative frequency with respect to a reference frequency.
Such relative frequencies are referred to as chemical shifts.
The resonances of nuclei that are close in Euclidean space couple, either through covalent bonds or through space.
NMR experiments capture such coupling.
The outputs from NMR experiments are NMR spectra, which are, mathematically speaking, multi-dimensional matri-ces.
The indices for each dimension are the discrete chemical shift values of a certain nucleus, and the entries of the matrices are the intensity values of the coupling.
For instance, 15N-HSQC is one of the most commonly-used NMR spectra.
It captures the coupling between the backbone nitrogen (N) and the hydrogen (H) that is attached to this nitrogen.
For a protein with n amino acids, there are (np) expected peaks in the 15N-HSQC spectrum, where p is the number of proline (Pro) in the protein.
However, the amine groups in the side chains of some amino acids are also visible in the 15N-HSQC spectrum, such as arginine (Arg), asparagine (Asn) and gluta-mine (Gln).
To eliminate the peaks of these side chains, ademy of Sciences and Genetics Society of China.
Published by Elsevier Ltd mailto:xin.gao@kaust.edu.sa30 Genomics Proteomics Bioinformatics 11 (2013) 2933 information from different spectra needs to be combined.
There are additional sources of error in NMR spectra, includ-ing missing signals, chemical shift degeneracy, sample impu-rity, water bands, artifacts and experimental errors [2].
All of these sources of error need to be taken into account.
Another important NMR spectrum is the nuclear Overha-user enhancement (NOE) spectrum, which is a through-space experiment that captures certain atoms that are close to each other in the Euclidean space.
Here, close often refers to a dis-tance smaller than 6 A.
Thus, the NOE spectrum is a through-space spectrum and each peak in the NOE spectrum provides a distance constraint that can reduce the conformational space of possible protein structures.
In contrast to NOE that provides short-range interactions (<6 A), there are experiments that can provide long-range information.
One example is residual dipolar couplings (RDCs), which provides long-range orientational information relative to an external alignment tensor [35].
Another example is paramagnetic relaxation enhancement (PRE) [6,7].
PRE ef-fect can be detected in large magnetic moment of protons and unpaired electron up to 35 A.
Traditionally, determination of NMR protein structure mainly follows the four-step process described by Wuthrich [1].
After the spectra are collected, the four steps involve peak picking, resonance assignment, NOE assignment and structure calculation.
The peak picking step takes the through-bond and through-space NMR spectra as inputs and identifies peaks in these spectra.
The peaks of certain through-bond spectra are then used to assign the chemical shift values to the correspond-ing atoms of the protein, which is the so-called resonance assign-ment step.
After resonance assignment, mapping between the chemical shift values and the indices of the atoms is built.
Such mapping is applied to interpret the NOE peaks and extract dis-tance constraints.
Since the chemical shift values of all the atoms of the protein are distributed within a small range, overlaps in chemical shift values are expected.
Thus, the interpretation of the NOE peaks can be ambiguous.
The structure calculation step takes the distance constraints (both ambiguous and unam-biguous) to determine the final structure(s) of the protein.
Most NMR labs process NMR data either manually or semi-automatically with the help of visualization tools.
The en-tire process is computationally costly and time-consuming.
Re-cently, attention has been paid to developing computational methods that can significantly accelerate the NMR data pro-cessing and reduce the errors introduced by manual process-ing.
However, NMR is still a new field to the computational community.
Even in the field of bioinformatics and computa-tional biology, computational problems in NMR structure determination have not been well studied.
Here, we review some recent advances in computational methods for NMR protein structure determination.
Peak picking The goal of the peak picking step is to identify peaks, i.e., the chemical shift coordinates of the coupling nuclei, in any given spectrum.
This is the key step in the entire NMR protein struc-ture determination process because the following steps are all built upon this step [8,9].
The automated peak picking problem was first studied two decades ago [10].
Expected properties of peak shapes, such as the symmetry property, were used to identify peaks.
Since then, a variety of computational methods have been utilized, including peak-property-based methods [11,12], machine learning methods [1316], and spectra-decom-position-based methods [1719].
Recently, image processing techniques have been applied to the peak picking problem and they have demonstrated promis-ing performance [20,21].
Alipanahi et al.
proposed a multi-stage method, PICKY, to automatically identify peaks from a given set of NH-rooted NMR spectra [20].
PICKY considers an NMR spectrum as an image and estimates the noise level by estimating the variance in local neighborhoods, which is based on the assumption that the noise is white Gaussian noise.
All the pixels of the image, i.e., data points of the spectrum, that have intensity values lower than the estimated noise level are believed to contain no signal and are thus removed.
The disconnected components of the remaining spectrum are identified, some of which may contain a number of peaks due to peak overlapping or inaccuracy in the estimation of the noise level.
The compo-nents are further decomposed to smaller ones by checking the levels of overlapping of adjacent local maxima.
Rank-one sin-gular value decomposition (SVD) is applied to each small com-ponent to identify peaks, which can eliminate false local maxima in the component.
Finally, cross-referenced informa-tion between spectra that share common nuclei, such as 15N and 1H, is used to refine the peak lists.
Another contribution of [20] is to propose a benchmark set that contains 32 2D and 3D spectra extracted from eight proteins.
This is the most com-prehensive data set to date for the peak picking problem.
Although PICKY demonstrated significantly better perfor-mance than previous peak picking methods, it has two bottle-necks.
PICKY is not sensitive enough to replace manual peak picking in the sense that weak peaks may be eliminated in the denoising step of PICKY if they have intensity values lower than the estimated noise level.
On the other hand, the number of false positives is high in PICKYpeak lists due to the fact that PICKY ranks peaks by intensity values, which can be badly biased.
WaVPeak was developed to overcome these two bottlenecks [21].
Like PICKY, WaVPeak is also based on image processing techniques.
Specifically, WaVPeak uses wavelets.
Wavelets are mathematical functions that cut data into different frequency components.
Each component is then studied with a resolution matched to its scale.WaVPeak applies multi-dimensional wave-lets to the NMR spectra to smooth the spectra.
In contrast to PICKY,WaVPeak aims to eliminate noise from the data points instead of eliminating noisy data points.
This can preserve the shapes of the peaks, including the weak ones.
Furthermore, WaVPeak ranks the peaks by their estimated volumes.
On PICKYs benchmark set,WaVPeak showed significantly higher sensitivity and included a smaller number of false positives than did PICKY.
To bemore specific,WaVPeak achieved an average of 88% recall value and 74% precision value.
One remaining problem in automatic peak picking is how to select true peaks from a large number of predicted peaks [9].
If a set of spectra is available for a target protein, the peak lists for these spectra can be used as cross-checks for each other [20,22].
For instance, the chemical shifts of 15N and 1H in a true peak in a CBCA(CO)NH spectrum are expected to be visible in the 15N-HSQC spectrum of the same protein and they can be cross-checked.
It is also possible to select the true peaks of a single spectrum.
To do so, Abbas et al.
cast the peak selection problem as a multiple testing problem in statistics [22].
They first converted the peak ranking criterion, Gao X/ Recent Advances in Computational NMR Data Processing 31 such as intensity or volume, into a P-value.
They then applied a BenjaminiHochberg algorithm to control the false discovery rate (FDR) and select the true peaks.
Their method can be potentially applied to different bioinformatic problems in which true predictions must be differentiated from a large number of false ones, such as protein function annotation [23].
However, the BenjaminiHochberg algorithm only selects a cutting point in the ranked peak list.
Its performance there-fore depends on the quality of the ranking criteria.
Designing a ranking measure that is better than volume or symmetry still remains an open problem in peak picking.
Resonance assignment After the peaks are identified, the peak lists from the through-bond spectra are first combined to assign the chemical shift values to the corresponding atoms of the protein.
For reso-nance assignment, the peaks that share common nuclei, 15N and 1H, are first grouped into spin systems.
The spin systems are then assigned to the residues of the protein using both in-ter-residue and intra-residue information contained in the spin systems.
Ideally, there are n spin systems to be assigned to n residues.
However, due to incomplete peak picking, there are often missing spin systems, missing chemical shifts in spin sys-tems and false spin systems, which make the resonance assign-ment problem practically difficult.
A variety of computational methods have been explored to solve the resonance assignment problem, including search algorithms [2427], maximum inde-pendent set algorithms [28], sequential algorithms [29,30], logic algorithms [31], fragment-based algorithms [32,33] and optimi-zation algorithms [3437].
Many target proteins of NMR experiments have closely homologous structures that are stored in the protein data bank (PDB) [38].
Depending on whether the homologous structures are utilized to assist the assignment process, resonance assign-ment methods can be classified as either ab initio or structure-based assignments.
To make an assignment method practically useful, themethodhas to be error-tolerant because the input peak lists or spin systems could contain missing or false information.
Another major difficulty is caused by chemical shift degeneracy, that is, the same nucleusmay have slightly different chemical shift values in different spectra.
This introduces ambiguities in the assignment process, especially for large proteins and proteins containing residues with similar chemical shift values, such as all-a proteins, which is a class of structural domains in which the secondary structure is composed entirely of a-helices.
IPASS was developed as an error-tolerant assignment method that automatically takes picked peaks as inputs [34].
IPASS is built based on the optimization techniques.
The peaks from different spectra are first grouped into spin systems by a two-round algorithm that can eliminate the effects of chemical shift degeneracy.
The spin systems are then evaluated by a probabilistic model to calculate the probability of being assigned to different residues.
After that, the problem becomes one of finding the mapping between the spin system set and the residue set.
Finding the optimal mapping, however, is NP-hard in the worst case.
IPASS formulates the problem as an integer linear programming (ILP) formulation.
For most of the cases, the probabilistic model can reduce the search space to a reasonable size in which state-of-the-art ILP solvers can find the optimal solutions.
Tycko and Hu, on the other hand, solved the resonance assignment problem in a completely probabilistic manner [30].
They formulated the assignment problem as a local search problem and developed a Monte Carlo simulated annealing algorithm to explore the assignment search space.
In this way, they could handle chemical shift degeneracy and missing/false chemical shifts in spin systems.
When close homology to the target protein can be found in PDB, the problem becomes more tractable.
Jang et al.
pro-posed the structure-based assignment problem and developed a general integer linear programming framework to solve the problem [35,36].
Their method simultaneously assigns back-bone chemical shifts and interprets NOE peaks.
The underly-ing idea is that given the homologous structure, a contact graph can be built in which each node is a residue and each edge denotes a pair of residues that are closer than 6 A in Euclidean space.
A similar graph can also be built based on spin systems and the NOE peaks that are associated with such spin systems.
In this graph, each node is a spin system and each edge represents two spin systems that are associated by an NOE peak.
The goal is to find the common edge matching be-tween the two graphs that maximizes the matching scores.
Their method was highly accurate, even when automatically picked peaks were used as the inputs.
The performance of all the aforementioned methods, how-ever, largely depends on the accuracy of amino acid typing and secondary structure prediction of spin systems.
Probabilistic models have been built based on statistics from the Biological Magnetic Resonance Bank (BMRB) [39], to predict amino acid and secondary structure types of spin systems to reduce the search space [34,35,40].
However, the accuracy of such models remains modest, which leaves room for improvement.
NOE assignment and structure calculation NOE assignment and structure calculation are often combined together to calculate final structures [34,4144].
A widely used method is the CYANA package [43].
CYANA is based on lo-cal search techniques, i.e., simulated annealing by molecular dynamics simulations in the torsion angle space.
However, CYANA requires manually processed assignments and NOE peaks to accurately determine the final structures.
To make the structure calculation more error-tolerant, Gao et al.
devel-oped AMR (automated NMR protocol) [2,34].
AMR is an end-to-end computational pipeline that consists of the peak picking module, PICKY, the resonance assignment module, IPASS, and the NOE assignment and structure calculation module, FALCON-NMR [45].
Given a target protein and its resonance assignment, FALCON-NMR first searches for homologs of the protein in PDB.
If homologs are found, it re-fines the structure by encoding chemical shift information.
Otherwise, it makes an ab initio prediction of the structure of the protein.
The chemical shifts are used to search for frag-ments of the target protein, from which the backbone angle distributions are extracted.
An order-nine hidden Markov model (HMM) is built to sample the conformational space.
It has been shown recently that little information is worthwhile beyond the residues that are more than nine residues apart [46].
The sampled structures are thus ranked by the ambiguous NOE constraints and the top ones are selected to generate fragments for the next iteration.
FALCON-NMR works in an iterative manner until convergence.
32 Genomics Proteomics Bioinformatics 11 (2013) 2933 The main bottleneck to ab initio protein structure calcula-tion methods is that the size of the search space is intractable.
Although the aforementioned methods use chemical shift information to significantly reduce the search space, they do not work well on large proteins.
Besides, NMR information has mainly been used in the scoring function and the fragment selection parts of such methods.
A method that can encode the chemical shift information to direct the search procedure may give better scalability.
Automated structure determination from spectra The ultimate goal for all the aforementioned efforts is to greatly accelerate, and even fully automate, the currently time-consuming NMR protein structure determination pro-cess, i.e., from the set of NMR spectra to the final 3D structure of the protein.
Despite the large number of computational methods developed for different steps of the NMR data pro-cessing procedure, a crucial question is that whether the iso-lated methods can be combined into a pipeline to work together.
In fact, this is one of the most important questions for the general bioinformatics field.
In bioinformatics, a com-plex problem is often decomposed into smaller ones or consec-utive steps.
Computational efforts can usually solve the smaller problems relatively well.
However, such methods are devel-oped independently of each other and often have different assumptions, inputs and outputs, and error tolerant levels.
From a user point of view, it is very difficult to make a correct combination of the methods to solve the big problem.
As mentioned in the previous section, Gao et al.
developed a fully automated pipeline, AMR, as a proof-of-concept [2].
PICKY was applied to identify peaks from a set of six spectra, including 15N-HSQC, HNCO or HNCA, CBCA(CO)NH, HNCACB, HCCONH-TOCSY and N-NOESY [20].
The six peak lists were then used to cross check to remove false positives.
The refined peak lists were fed into IPASS for resonance assign-ment [34].
IPASS was specifically developed to deal with highly noisy and incomplete peak lists generated by automatic peak picking methods.
The resonance assignment was then applied to assign NOE peaks.
FALCON-NMR was used to calculate the final 3D structure by using both chemical shift information and distance constraints [34].
AMRwas applied on the spectrum sets of four proteins and generated final structures within 1.5 A to the experimentally determined ones.
Another successful at-tempt is FLYA [47,44], which uses AUTOPSY as the peak pick-ing tool [17], GARANT as the chemical shift assignment tool [48], ARIA as the NOE assignment tool [49] and CYANA as the structure calculation tool [43].
Outlook Despite of some progress in developing computational meth-ods for NMR data processing, the main bottlenecks to analysis of NMR spectroscopy data remain, i.e., solving structures of large proteins and solving loop structures.
If the target protein is a large protein, the number of atoms will be higher and the spectra will become more crowded.
On the other hand, if the target protein contains flexible loops, their peaks tend to have weak intensities and sometimes overlap with each other.
To overcome these bottlenecks, efforts have been extended in three directions.
First, NMR spectrometers with stronger mag-netic fields, such as 950 MHz, have been developed and uti-lized in labs.
Such machines can generate spectra with much higher resolutions and their peaks are more concentrated.
Sec-ond, higher-dimensional NMR experiments have been devel-oped and used.
Up to now, 6D spectra have been used in practice [50].
Far fewer overlapping peaks are expected in higher-dimensional spectra.
Third, 13C-labeled spectra can be used to replace traditional 1H-labeled proteins to reduce the number of peaks significantly and thus reduce ambiguities.
Any of these directions will require computational efforts to extend the current methods or develop novel methods to deal with new types of data, especially for the peak picking step and the structure calculation step.
Conclusion Here, we have briefly reviewed recent advances in computa-tional methods for NMR protein structure determination, which is a relatively new field of inquiry for bioinformaticians and computational biologists.
We have provided a summary of the advantages to and bottlenecks in existing methods and out-lined some open questions.
We have also discussed current trends in the development of NMR technologies and have pointed out directions for the development of future computa-tional methods.
Competing interests None declared.
Acknowledgements We are grateful to Ahmed Abbas, Babak Alipanahi, Cheryl Arrowsmith, Vladimir B. Bajic, Frank Balbach, Dongbo Bu, Meghana Chitale, Logan Donaldson, Jianhua Huang, Richard Jang, Bing-Yi Jing, Emre Karakoc, Daisuke Kihara, Xinbing Kong, Ming Li, Shuai Cheng Li, Zhi Liu, Mehdi Maadooliat, Mario Messih and Jinbo Xu for their contributions to the pro-jects discussed in this review.
We thank Virginia Unkefer for editorial work on the manuscript.
This work was supported by the GRP-CF award (Grant No.
GRP-CF-2011-19-P-Gao-Huang) and a GMSV-OCRF award from King Abdullah Uni-versity of Science and Technology (KAUST).
ABSTRACT Motivation: Gene regulatory networks underlying temporal processes, such as the cell cycle or the life cycle of an organism, can exhibit significant topological changes to facilitate the underlying dynamic regulatory functions.
Thus, it is essential to develop methods that capture the temporal evolution of the regulatory networks.
These methods will be an enabling first step for studying the driving forces underlying the dynamic gene regulation circuitry and predicting the future network structures in response to internal and external stimuli.
Results: We introduce a kernel-reweighted logistic regression method (KELLER) for reverse engineering the dynamic interactions between genes based on their time series of expression values.
We apply the proposed method to estimate the latent sequence of temporal rewiring networks of 588 genes involved in the developmental process during the life cycle of Drosophila melanogaster.
Our results offer the first glimpse into the temporal evolution of gene networks in a living organism during its full developmental course.
Our results also show that many genes exhibit distinctive functions at different stages along the developmental cycle.
Availability: Source codes and relevant data will be made available at http://www.sailing.cs.cmu.edu/keller Contact: epxing@cs.cmu.edu 1 INTRODUCTION Many biological networks bear remarkable similarities in terms of global topological characteristics, such as scale-free and small-world properties, to various other networks in nature, such as social networks, albeit with different characteristic coefficients (Barabasi and Albert, 1999).
Furthermore, it was observed that the average clustering factor of real biological networks is significantly larger than that of random networks of equivalent size and degree distribution (Barabasi and Oltvai, 2004); and biological networks are characterized by their intrinsic modularities (Vszquez et al., 2004), which reflect presence of physically and/or functionally linked molecules that work synergistically to achieve a relatively autonomous functionality.
These studies have led to numerous advances towards uncovering the organizational principles and functional properties of biological networks, and even identification of new regulatory events (Basso et al., 2005).
However, most such results are based on analyses of static networks, i.e.
networks with invariant topology over a given set of molecules.
One example is a proteinprotein interaction (PPI) network over all proteins of an organism, regardless of the conditions under which individual interactions may take place.
To whom correspondence should be addressed.
Another example is a single-gene network inferred from microarray data even though the samples may be collected over a time course or multiple conditions.
A major challenge in systems biology is to understand and model, quantitatively, the dynamic topological and functional properties of cellular networks, such as the rewiring of transcriptional regulatory circuitry and signal transduction pathways that control behaviors of a cell.
Over the course of a cellular process, such as a cell cycle or an immune response, there may exist multiple underlying themes that determine the functionalities of each molecule and their relationships to each other, and such themes are dynamical and stochastic.
As a result, the molecular networks at each time point are context-dependent and can undergo systematic rewiring rather than being invariant over time, as assumed in most current biological network studies.
Indeed, in a seminal study by Luscombe et al.
(2004), it was shown that the active regulatory paths in a gene-regulatory network of Saccharomyces cerevisiae exhibit dramatic topological changes and hub transience during a temporal cellular process, or in response to diverse stimuli.
However, the exact mechanisms underlying this phenomena remain poorly understood.
We refer to this time-or condition-specific active parts of the biological circuitry as the active time-evolving network, or simply, time-varying network.
Our goal is to recover the latent time-evolving network of gene interactions from microarray time course.
What prevents us from an in-depth investigation of the mechanisms that drive the temporal rewiring of biological networks during various cellular and physiological processes?
A key technical hurdle we face is the unavailability of serial snapshots of the time-evolving rewiring network during a biological process.
Current technology does not allow for experimentally determining a series of time-specific networks, for a realistic dynamic biological system, based on techniques such as yeast two-hybrid or ChIP-chip systems; on the other hand, use of computational methods, such as structural learning algorithms for Bayesian networks, is also difficult because we can only obtain a few observations of gene expressions at each time point which leads to serious statistical issues in the recovered networks.
How can one derive a temporal sequence of time-varying networks for each time point based on only one or at most a few measurements of node-states at each time point?
If we follow the naive assumption that each temporal snapshot of gene expressions is from a completely different network, this task would be statistically impossible because our estimator (from only the observations at the time point in question) would suffer from extremely high variance due to sample scarcity.
Previous methods would instead pool observations from all time points together and infer a single average network (Basso et al., 2005; Friedman et al., 2000; Ong, 2002), which means they choose to ignore network rewiring and simply assume that the observations are independently and 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i129 i128i136 KELLER: estimating time-varying networks identically distributed.
To our knowledge, no method is currently available for genome-wide reverse engineering of time-varying networks underlying biological processes, with temporal resolution up to every single time point based on measurements of gene expressions.
In this article, we propose kernel-reweighted logistic regression (KELLER), a new machine learning algorithm for recovering time-varying networks on a fixed set of genes from time series of expression values.
KELLER stems from the acronym KERWLLOR, which stands for KErnel ReWeighed l1-regularized LOgistic Regression.
Our key assumption is that the time-evolving networks underlying biology processes vary smoothly across time, therefore temporally adjacent networks are likely to share more common edges than temporally distal networks.
This assumption allows us to aggregate observations from adjacent time points by reweighting them, and to decompose the problem of estimating time-evolving networks into one of estimating a sequence of separate and static networks.
Extending the highly scalable optimization algorithms of 1-regularized logistic regression, we are able to apply our method to reverse engineer a genome-wide interactions with a temporal resolution up to every single time point.
It is worth emphasizing that earlier algorithms, such as the structure learning algorithms for dynamic Bayesian network (DBN) (Ong, 2002), learns a time-homogeneous dynamic system with fixed node dependencies, which is entirely different from our approach, which aims at snapshots of rewiring network.
Our approach is also very different from earlier approaches which start from a priori static networks and then trace time-dependent activities.
For example, the trace-back algorithm (Luscombe et al., 2004) that enables the revelation of network changes over time in yeast is based on assigning time labels to the edges in a priori static summary network.
The Achilles heel of this approach is that edges that are transient over a short period of time may be missed by the summary static network in the first place.
The DREM program (Ernst et al., 2007) reconstructs dynamic regulatory maps by tracking bifurcation points of a regulatory cascade according to the ChIP-chip data over short time course.
This is also different from our method, because KELLER aims at recovering the entire time-varying networks, not only the interactions due to proteinDNA binding, from long time series with arbitrary temporal resolution.
One related approach is the Tesla algorithm by Ahmed et al.
(2008).
However, Tesla aims at recovering bursty rather than smoothly varying networks.
We apply our method to reverse engineer the time-evolving network between 588 genes involved in the developmental process during the life cycle of Drosophila melanogaster.
These genes are a subset of the 4028 genes whose expression values are measured in a 66-step time series documented in Arbeitman et al.
(2002).
We validate the biological plausibility of the estimated time-evolving network from various aspects, ranging from the activity of functionally coherent gene sets, to previous experimentally verified interactions between genes, to regulatory cascade involved in nervous system development, and to gene functional enrichment.
More importantly, the availability of time-evolving networks gives us the opportunity to further study the rich temporal phenomena underlying the biological processes that is not attainable using the traditional static network.
For instance, such a downstream analysis can be a latent functional analysis of the genes in the time-evolving network appeared in Fu et al.
(2008).
The remainder of the article is structured as follows.
In Section 2, we will introduce our kernel reweighted method.
In Section 3, we will use synthetic data and a time series of gene expression data collected during the life cycle of D.melanogaster to show the advantage as well as biological plausibility of estimating a dynamic network.
We conclude the article with a discussion and outlook on future work in Section 4.
2 METHODS First, we introduce our time-evolving network model for gene expression data, then explain our algorithm for estimating the time-evolving network and finally discuss the statistical property and parameter tuning for our algorithm.
2.1 Modeling time series of gene expression Microarray profiling can simultaneously measure the abundance of transcripts from tens of thousands of genes.
This technology provides a snapshot into the cell at a particular time point in a genome-wide fashion.
However, microarray measurements are far from the exact values of the expression levels.
First, the samples prepared for microarray experiments are usually a mixture of cells from different tissues and, possibly, at different points of a cell cycle or developmental stage.
This means that microarray measurements are only rough estimates of the average expression levels of the mixture.
Other sources of noise can also be introduced into the microarray measurements, e.g.
during the stage of hybridization, digitization and normalization.
Therefore, it is more robust if we only consider the qualitative level of gene expression rather than its actual value.
That is we model gene expression as either being upregulated or downregulated.
For this reason, we binarize the gene expression levels into X :={1,1} (1 for downregulated and 1 for upregulated).
For instance, for cDNA microarray, we can simply threshold at 0 the log ratio of the expression levels to those of the reference, above which a gene is declared to be upregulated and otherwise downregulated.
At a particular time point t, we denote the microarray measurements for p genes as a vector of random variables X (t) := (X (t)1 ,...,X (t)p )X p, where we have adopted the convention that the subscripts index the genes and the bracketed superscripts index the time point.
We model the distribution of the expression values for these p genes at any given time point t as a binary pair-wise Markov Random Field (MRF): P (t) (X (t)) := 1 Z( (t)) exp  (u,v)E(t) (t)uv X (t) u X (t) v , (1) where (t)uv = (t)vu R is the parameter indicating the strength of undirected interaction between genes u and v; and a (t)uv =0 means that the expression values for genes u and v are conditionally independent given the values of all other genes.
Therefore, a MRF is also associated to a network G(t) with a set of nodes V and a set of edges E (t): V corresponds to the invariant set of genes and hence without the superscript for time; each edge in E (t) corresponds to an undirected interaction between two genes (and a non-zero (t) uv ).
The difference between E (t) and (t)uv can be viewed as follows: E (t) only codes the structure of the model while (t)uv contains all information about the model.
Finally, the partition function Z( (t)) in a MRF normalizes the model to a distribution.
The dynamic interactions between genes underlying temporal biological processes are reflected in the change of the magnitude of parameter (t)uv across time.
In particular, increased values of (t)uv indicate strengthened or emerging interaction between gene u and v, and decreased values indicate weakened or disappearing interaction.
Furthermore, we assume that the dynamic interactions between genes vary smoothly across time.
Mathematically, this means that the change of (t)uv is small across time, i.e.
the difference | (t)uv (t+1)uv | is upper bounded by a small constant C .
In other i129 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i130 i128i136 L.Song et al.
words, the networks at adjacent time points, G(t) and G(t+1) are very similar, i.e.
|E (t)E (t+1)|/|E (t)| is lower bounded by a large constant CE (here, we used || to denote the cardinality of a set).
Given time series of gene expression data measured at n time points, D :={x(t1),...,x(tn)}, our goal is to estimate a temporal sequence of networks G :={G(t1),...,G(tn)} with each network for 1 time point.
Note that, we will focus on estimating the structures of the interactions between genes (G(t)) rather than the detailed strength of these interactions ( (t)).
We hope by restricting our attention to estimating the structure, we can obtain better guarantees in terms of the ability of our algorithm to recover the true underlying interactions between genes.
In Sections 2.2 and 2.4, we will provide further explanation on the advantage of focusing on G(t).
Another important point of clarification is that the interactions between genes we are modeling are the statistical dependencies between their expression levels.
This is a common choice for many existing methods, such as the methods by Friedman et al.
(2000) and Ong (2002).
Note that statistical dependency is different from causality, which focuses on directed statistical relations between random variables.
In other words, it is more appropriate to view networks from our model as the co-regulation relations between genes.
That is, if there is an edge between two genes in the dynamic network at time point t, then the changes of the expression levels of these two genes are likely to be regulated by the same biological process.
2.2 Estimating time-varying network Two questions need to be addressed when we estimate a time-evolving network.
First, what is the objective to optimize and second, what is the algorithmic procedure for the estimation?
The first question is addressed in this section and it concerns both the consistency and efficiency of our method while the second question only concerns the efficiency of the algorithm, which we will discuss more in Section 2.3.
First, estimating the parameter vector (t) by maximizing log-likelihood is not practically feasible since the evaluation of the partition function Z( (t)) involves a summation of exponential number of terms.
Another approach to address this problem is to use a surrogate likelihood function, which can be tractably optimized.
However, there is no statistical guarantee on how close an estimate obtained through maximization of a surrogate likelihood is to the true parameter (Banerjee et al., 2008).
Therefore, we adapt the neighborhood selection procedure of Wainwright et al.
(2006) to estimate the time-evolving network G(t) instead.
Overall, we have designed a method that decomposes the problem of estimating the time-evolving network along two orthogonal axes.
The first axis is along the time, where we estimate the network for each time point separately by reweighting the observations accordingly; and the second axis is along the set of genes, where we estimate the neighborhood for each gene separately and then joining these neighborhoods to form the overall network.
The additional benefit of such decomposition is that the estimation problem is reduced to a set of identical atomic optimization tasks in Equation (3).
In the next section, we will discuss our procedure to solve this atomic optimization task efficiently.
In this new approach, estimating the network G(t) is equivalent to recovering, for each gene uV , its neighborhood of genes that u is interacting with, i.e.
N (t)(u) :={vV|(u,v)E (t)}.
It is intuitive that if we can correctly estimate the neighborhood for all genes u in V , we can recover the network G(ti) by joining these neighborhoods.
In this alternative view, we can decompose the joint distribution in Equation (1) into a product of conditional distributions, P (t) (X (t) u |X (t)\u ), each of which is the distribution of the expression value of gene u conditioned on the expression values of all other genes (we use \u to denote the set of genes except gene u, i.e.
\u :=V{u}).
In particular, P (t) (X (t)u |X (t)\u ) takes the form of a logistic regression: P (t) \u (X (t)u |X (t)\u )= exp ( 2X (t)u  (t) \u ,X (t) \u ) exp ( 2X (t)u (t) \u ,X (t) \u ) +1 , (2) where a,b=ab denotes inner product and (t)\u :={ (t)uv | v\u} is the (p1)-dimensional sub-vector of parameters associated with gene u.
The neighborhood N (t)(u) can be estimated from the sparsity pattern of the sub-vector (t)\u .
Therefore, estimating the network G(t) at time point t can be decomposed into p tasks, each for the sub-vector (t)\u corresponding to a gene.
For later exposition, we denote the log-likelihood of an observation x under Equation (2) as ( (t)\u ;x)= logP (t)\u (xu|x\u).
Recall that we assume that the time-evolving network varies smoothly across time.
This assumption allows us to borrow information across time by reweighting the observations from different time points and then treating them as if they were i.i.d.
observations.
Intuitively, the weighting should place more emphasis on observations at or near time point t with weights becoming smaller as the observations move further away from time point t. Such reweighting technique has been employed in other tools for time series analyses, such as the short-time Fourier transformation where observations are reweighted before applying the Fourier transformation to capture transient frequency components (Nawab and Quatieri, 1987).
In our case, at a given time point t, the weighing is defined as w(t)(ti) := Khn (t ti)/ n i=1 Khn (t ti), where Khn () :=K(/hn) is a symmetric non-negative kernel and hn is the kernel bandwidth.
We used the Gaussian RBF kernel, Khn (t)=exp(t2/hn), in our later experiments.
Note that multiple measurements at one time point can be trivially handled by assigning them the same weight.
We consider multiple measurements to be i.i.d.
observations.
Additionally, we will assume that the true network is sparse, or that the interactions between genes can be approximated with a sparse model.
This sparsity assumption holds well in most cases.
For example, a transcription factor only controls a small fraction of target genes under a specific condition (Davidson, 2001).
Then, given a time series of gene expression data measured at n time points, D={x(t1),...,x(tn)}, we can estimate (t)\u or the neighborhood of N (t)(u) of gene u at time point t using an 1 penalized log-likelihood maximization.
Equivalently the estimator (t)\u is the solution of the following minimization problem: (t) \u = argmin (t) \u Rp1 ( n i=1 w(t)(ti) ( (t) \u ;x(ti))+ (t)\u 1 ) , (3) where 0 is a regularization parameter specified by user that controls the size of the estimated neighborhood, and hence the sparsity of the network.
Then, the neighborhood for gene u can be estimated as N (t)(u)= {vV | (t)uv 	=0}, and the network can be estimated by joining these neighborhoods: E (t)= { (u,v)|vN (t)(u) or uN (t)(v) } .
(4) 2.3 Efficient optimization Estimating time-evolving networks using the decomposition scheme described in previous section requires solving a collection of optimization problems given in Equation (3).
In a genome-wide reverse engineering task, there are tens of thousands of genes and hundreds of time points, so one can easily have a million optimization problems.
Therefore, it is essential to develop an efficient algorithm for solving the atomic optimization problem in Equation (3), which can then be trivially parallelized across different genes and different time points.
The optimization problem in Equation (3) is an 1 penalized logistic regression with observation reweighting.
This optimization problem has been an active research area in the machine learning community and various methods have been developed, including interior point methods (Koh et al., 2007), trust region newton methods (Lin et al., 2008) and projected gradient methods (Duchi et al., 2008).
In this article, we employed a projected gradient method due to its simplicity and efficiency.
i130 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i131 i128i136 KELLER: estimating time-varying networks The optimization problem in (3) can be equivalently written in a constrained form: (t) \u = argmin (t)\u 1 C ( n i=1 w(t)(ti) ( (t) \u ;x(ti)) ) , (5) where C is an upper bound for the 1 norm of (t) \u and defines a region in which the parameter lies.
There is an one-to-one correspondence between C in Equation (5) and in Equation (3).
In this formulation, the objective L( (t)\u ) is a smooth and convex function, and its gradient with respect to (t)\u can be computed simply as (t) :=L( (t)\u )= n i=1 w(t)(ti) ( (t) \u ).
The key idea of a projected gradient method is to update the parameter along the negative gradient direction.
After the update, if the parameter lies outside the region , it is projected back into the region , otherwise, we move to the next iteration.
The essential step in the algorithm is the efficiency with which we can project the parameter into the region : (t) \u ( (t) \u (t) ) , (6) where (a) :=argminb{ab|b} is the Euclidean projection of a vector a onto a region .
We employed an approach by Duchi et al.
(2008) which involves only simple operations such as sorting and thresholding for this projection step.
Algorithm 1 gives a summary of the projected gradient method for the optimization problem in Equation (3).
Note that the projected gradient algorithm has several internal parameters , and , which, in our experiments, we set to typical values given in the literature (Bertsekas, 1999).
Algorithm 1 Projected Gradient Method for Equation (5) Input: A time series D={x(t1),...,x(tn)}, an upper bound C Output: (t)\u 1: Initialize (t)\u , (t) \u , set =0.1, =106, =102 2: repeat 3: (t) \u (t) \u , 1.0 4: repeat 5: (t) \u ( (t) \u (t) ) , 6: until L( (t)\u )L( (t) \u )(t)( (t) \u  (t) \u ) 7: until (t)\u  (t) \u 8: (t) \u (t) \u 2.4 Statistical property The main topic we discuss here is whether the algorithm described in Section 2.2 can estimate the true underlying time-evolving network correctly.
In order to study the statistical guarantees of our algorithm, we need to take three aspects into account.
First, a genome-wide reverse engineering task can involve tens of thousands of genes while the number of observations in time series can be quite limited (hundreds at most).
Therefore, it is important to study the case in which the dimension p scales with respect to the sample size n, but still allows for recovery of networks.
Second, the time-evolving nature of networks adds extra complication to the estimation problem, so, we have to take the amount of change between adjacent networks, C :=maxuv (t)uv (t+1)uv , into account.
Third, the intrinsic properties of the interactions between genes will also affect the correct recovery of the networks.
Intuitively, the more complicated interactions the more difficult it is to recover networks, e.g.
each gene interacts with a large fraction of other genes.
In other words, the maximum size of the neighborhood for a gene CN :=maxuV N (u) is also a deciding factor.
To our knowledge, none of the earlier methods (Basso et al., 2005; Friedman et al., 2000; Ong, 2002) provide a statistical guarantee for recovered networks or are amenable to such analysis.
In contrast, the method we presented in Section 2.2 is highly amenable to a rigorous statistical analysis.
Statistical guarantees have been provided for estimating static networks under the model in Equation (1) (Wainwright et al., 2006) and we can extend them to the time-varying case.
A detailed proof of a similar result for our approach is beyond the scope of this article and deserves a full treatment in a separate paper.
At a high level, we can show that under a set of suitable conditions over the model, C , CN , hn and , with high probability, we can recover the true underlying time-evolving network even when the number of genes p is exponential in size of the number of observations n [for details of the proof, see M.Kolar and E.Xing (submitted for publication)].
A different analysis have been provided for time-varying Gaussian graphical models (Zhou et al., 2008), in which the consistency of the interaction strengths is addressed, but not the consistency of the network topology.
2.5 Parameter tuning The regularization parameter controls the sparsity of the estimated networks.
Large values of result in sparse networks, while small values result in a dense networks that have higher log-likelihood, but more degrees of freedom.
We employ the Bayesian Information Criterion (BIC) for choosing that trades off between the fit to the data and the model complexity.
More specifically, we use an average of the BIC score defined below for each time point t and for each gene u: BIC(t,u) := n i=1 w(t)(ti) ( (t) \u ;x(ti)) log(n) 2 Nz( (t)\u ) (7) where Nz() counts the number of non-zero entries in (t)\u .
Then the final score is BIC :=1/n|V|uVnj=1 BIC(tj,u).
A larger BIC score implies a better model.
The bandwidth parameter hn controls the smoothness of the change in the time-evolving networks.
Using wide bandwidths effectively incorporate more observations for estimating each network snapshot, but it also risks missing sharp changes in the network; using narrow bandwidths makes the estimate more sensitive to sharp changes, but this also makes the estimation subject to larger variance due to the reduced effective sample size.
In this article, we use a heuristic for tuning the initial scale of the bandwidth parameter hn: we first form a matrix (dij) with its entries dij := (titj)2 (i={1,...,n}).
Then the scale of the bandwidth parameter is set to the median of the entries in (dij).
Intuitively, the bandwidth parameter reflects the characteristic interval between time points.
In our simulation experiments, we find that this heuristic provides a good initial guess for hn, and it is quite close to the value obtained via more exhaustive search.
3 EXPERIMENTS In this section, we use synthetic data to demonstrate the advantage of estimating a time-evolving network, and we used data collected from Drosophila to show that our method, KELLER, can estimate a biologically plausible time-evolving network and reveal some interesting properties of the dynamic interactions between genes.
3.1 Recovering synthetic networks In this section, we compare KELLER with structural learning of DBN (Friedman et al., 2000) and 1-regularization logistic regression for static network estimation using synthetic networks.
Note that 1-regularization logistic regression can be obtained from KELLER: we only need to apply a uniform weight w(ti)=1/n to all observations and estimate a single network using the same objective as Equation (5).
i131 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i132 i128i136 L.Song et al.
Fig.1.
As we increase the number of i.i.d.
samples at each time point, KELLER estimating a time-evolving network clearly outperforms DBN and 1-regularized logistic regression for estimating a static network.
Subplot (a) displays the performances for the overall networks, while (b) and (c) display the performance for the static edges and dynamic edges, respectively.
Starting at time t1, we generate an ErdsRnyi random graph G(t1) of p=50 nodes with an average degree of 2.
The parameter (t1)uv for the non-zero edges is chosen uniformly random from the range [1,2].
Then, we randomly select 15 edges and gradually decrease their weights to zero in 200 time points.
Starting from t1, we also chose 15 new edges and gradually increase their weights to a random target value between [1,2] in 200 time points.
Therefore, in the first 200 discrete time steps, 15 existing edges are deleted, 15 new edges are added and the final network maintains an overall size of 50 edges.
We call these 30 time-evolving edges as dynamic edges and the remaining 35 edges as static edges.
Then from time point 200, we start the second cycle of deleting 15 edges and adding 15 edges.
This cycle is repeated five times, which results in a smooth time-evolving network with n=1000 time point.
Furthermore, we draw 10 i.i.d.
observations from the network at each time point and study how the performance of different methods scales with the number of i.i.d.
observations at each time point.
We evaluate the estimation procedures using an F1 score, which is the harmonic mean of precision (Pre) and recall (Rec), i.e.
F1 :=2PreRec/Pre+Rec.
Precision is calculated as 1/n n i=1 |E (ti)E (ti)|/|E (ti)|, and recall as 1/n n i=1 |E (ti)E (ti)|/|E (ti)|.
The F1 score is a natural choice of the performance measure as it tries to balance between precision and recall; only when both precision and recall are high can F1 be high.
Furthermore, we use an initial bandwidth parameter hn as explained in Section 2.5, then searched over a grid of parameters (10[0.5:0.1:0.5] for and hn[0.5,1,2,5,10,50] for the bandwidth), and finally chose one that optimizes the BIC criterion defined in Section 2.5.
When estimating the static network, we use the same range to search for .
The recovery results for the overall time-evolving network, the dynamic and static edges, are presented, respectively, in Figure 1.
From the plots, we can see that estimating a static network does not benefit from increasing number of i.i.d.
observations at all.
In contrast, estimating a time-varying network always obtains a better performance and the performance also increases as more observations are available.
Note that these results are not surprising since our time-varying network model fits better the data generating process.
As time-evolving networks occur very often in biological systems, we expect our method will also have significant advantages in practice.
3.2 Recovering time-evolving interactions between genes in D.melanogaster Over the developmental course of D.melanogaster, there exist multiple underlying themes that determine the functionalities of each gene and their relationships to each other, and such themes are dynamical and stochastic.
As a result, the gene-regulatory networks at each time point are context-dependent and can undergo systematic rewiring, rather than being invariant over time.
In this section, we use KELLER to reverse engineer the dynamic interactions between genes from D.melanogaster based on a time series of expression data measured during its full life cycle.
We use microarray gene expression measurements collected by Arbeitman et al.
(2002) as our input data.
In such an experiment, the expression levels of 4028 genes are simultaneously measured at various developmental stages.
Particularly, 66 time points are chosen during the full developmental cycle of D.melanogaster, spanning across four different stages, i.e.
embryonic (130 time point), larval (3140 time point), pupal (4158 time points) and adult stages (5966 time points).
In this study, we focused on 588 genes that are known to be related to developmental process based on their gene ontologies.
We use a regularization parameter of 102, and a bandwidth parameter of 0.5hn in this experiment (hn is the median distance as explained in Section 2.5).
In Figure 3a, we plot two different statistics of the reversed engineered gene-regulatory networks as a function of the developmental time point (166).
The first statistic is the network size as measured by the number of edges; the second is the average local clustering coefficient as defined by Watts and Strogatz (1998).
The first statistic measures the overall connectedness of the networks, while the latter measures the average connectedness of the neighborhood local to each gene.
For comparison, we normalize both statistics to the range between [0,1].
It can be seen that the network size and its local clustering coefficient follow very different trajectories during the developmental cycle.
The network size exhibits a wave structure featuring two peaks at mid-embryonic stage and the beginning of pupal stage.
Similar pattern of gene activity has also been observed by Arbeitman et al.
(2002).
In contrast, the clustering coefficients of the time-evolving networks drop sharply after the mid-embryonic stage, and they stay low until the start of the adult stage.
One explanation is that at the beginning of the developmental process, genes have a more fixed and localized function, and they mainly interact with other genes with similar functions; however, after mid-embryonic stage, genes become more versatile and involved in more diverse roles to serve the need of rapid development; as the organism turns into an adult, its growth slows down and each gene may be restored to its more specialized role.
To illustrate how the network properties change over time, we visualize two networks from mid-embryonic stage (time point 15) and mid-pupal stage (time point 45) in Figure 3b and 3c respectively.
Although the size of the two networks are comparable, we can see that there are much clearer local clusters of interacting genes during mid-embryonic stage.
To provide a better view of the evolving nature of these clusters, we cluster genes based on the network at time point 1 using spectral clustering, and visualize the gradual disappearance of these clusters in Figure 2.
Note that our visualization does not indicate that genes do not form clusters in later developmental stage.
Genes may cluster under different groupings, but these clusters cannot be revealed by the visualization since the positions of the genes have been fixed in the visualization.
To judge whether the learned networks make sense biologically, we zoom into three groups of genes functionally related to different stages of development process.
In particular, the first group (30 genes) is related to embryonic development; the second group i132 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i133 i128i136 KELLER: estimating time-varying networks (a) t = 1 (b) t = 8 (c) t = 16 (d) t = 24 (e) t = 35 (f) t = 47 (g) t = 62 Fig.2.
(ag) Circular plots of the networks (left or top part of each table cell) and dot plots of the adjacency matrices of the networks (right or bottom part of each table cell) at 7 time points.
Note that we have clustered the genes according to the network connections at time point 1 and used this clustering result to fix the order of the genes in all plots.
In the circular plots, genes are arranged along the outer rim and the colors indicates the boundaries between different clusters (20 clusters in total).
Furthermore, we have added curvature to the edges such that connections within and between clusters can be seen more clearly.
(a) Network statistics (b) Embryonic stage (c) Pupal stage Fig.3.
Characteristic of the time-evolving networks estimated for the genes related to developmental process.
(a) Plot of two network statistics as functions of development time line (NS, network size; CC, clustering coefficient).
(b and c) visualization of two example of networks from different time point.
We can see that network size can evolve in a very different way from the local clustering coefficient.
(a) (b) (c) Fig.4.
Interactivity of three groups of genes related to (a) embryonic development; (b) post-embryonic development; and (c) muscle development.
The higher the interactivity, the more active the group of genes.
We see that the interactivity of the three groups is very consistent with their functional annotation.
(27 genes) is related to post-embryonic development; and the third group (25 genes) is related to muscle development.
(The genes are assigned to their respective groups according to their ontology labels.)
We used interactivity, which is the total number of edges a group of genes is connected to, to describe the activity of each group genes.
In Figure 4, we plotted the time courses of interactivity for the three groups, respectively.
For comparison, we normalize all scores to the range of [0,1].
We see that the time courses have a nice correspondence with their supposed roles.
For instance, embryonic development genes have the highest interactivity during embryonic stage, and post-embryonic genes increase their interactivity during larval and pupal stage.
The muscle development genes are less Table 1.
Timeline of 45 known gene interactions CycE CycA CycE Rca1 Dp CycE Gi Go Hem blow Ice Ark Jra dnc Nf1 dnc Pak trio Sb Rho1 Snap Syx1A Src42A ksr W nej brk tkv brm N brm shg btl stumps cact dl caps Chi da Dl dally sgl dl Dif dom E(z) ea Tl emc bs esc E(z) gl peb hep p53 mam wg msn Nrt msn dock nej th numb Rca1 pbl CycE pbl Src64B pbl dl pbl tum pnr svr pros Abl pros pnt sdt baz sno Dl spen ksr tsh wg up Mhc Each cell in the plot corresponds to one gene pair of gene interaction at one specific time point.
The cells in each row are ordered according to their time point, ranging from embryonic stage (E) to larval stage (L), to pupal stage (P) and to adult stage (A).
Cells colored blue indicate the corresponding interaction listed in the right column is present in the estimated network; blank color indicates the interaction is absent.
specific to certain developmental stages, since they are needed across the developmental cycle.
However, we see its increased activity when the organism approaches its adult stage where muscle development becomes increasingly important.
The estimated networks also recover many known interactions between genes.
In recovering these known interactions, the time-evolving networks also provide additional information as to when interactions occur during development.
In Table 1, we listed these i133 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i134 i128i136 L.Song et al.
(a) Summary network (b) Embryonic stage (c) Larval stage (d) Pupal stage (e) Adult stage Fig.5.
The largest TFs cascade involving 36 TFs.
(a) The summary network is obtained by summing the networks from all time points.
Each node in the network represents a TF, and each edge represents an interaction between them.
The width of an edge is proportional to the number of the times the edge is present during the development; the size of a node is proportional to the sum of its edge weights.
During different stages of the development, the networks are different, (be) shows representative networks for the embryonic (t=15), larval (t=35), pupal (t=49) and adult stage of the development, respectively (t=62).
recovered known interactions and the precise time when they occur.
This also provides a way to check whether the learned networks are biologically plausible given the prior knowledge of the actual occurrence of gene interactions.
For instance, the interaction between genes msn and dock is related to the regulation of embryonic cell shape and correct targeting of photoreceptor axons.
This is consistent with the timeline provided by the time-evolving networks.
A second example is the interaction between genes sno and Dl which is related to the development of compound eyes of Drosophila.
A third example is between genes caps and Chi which are related to wing development during pupal stage.
What is most interesting is that the time-evolving networks provide timelines for many other gene interactions that have not yet been verified experimentally.
This information will be a useful guide for future experiments.
We further study the relations between 130 transcriptional factors (TFs).
The network contains several clusters of transcriptional cascades, and we will present in detail the largest TF cascade involving 36 TFs (Fig.5).
This cascade of TFs is functionally very coherent, and many TFs in this network play important roles in the nervous system and eye development.
For example, Zn finger homeodomain 1 (zhf1), brinker (brk), charlatan (chn), decapentaplegic (dpp), invected (inv), forkhead box, subgroup 0 (foxo), Optix, eagle (eg), prospero (pros), pointed (pnt), thickveins (tkv), extra macrochaetae (emc), lilliputian (lilli), doublesex (dsx) are all involved in nervous and eye development.
Besides functional coherence, the networks also reveal the dynamic nature of gene regulation: some relations are persistent across the full developmental cycle while many others are transient and specific to certain stages of development.
For instance, five TFs, brkpnt zfh1prosdpp, form a long cascade of regulatory relations which are active across the full developmental cycle.
Another example is gene Optix which is active across the full developmental cycle and serves as a hub for many other regulatory relations.
As for transience of the regulatory relations, TFs to the right of Optix hub reduce their activity as development proceeds to later stages.
Furthermore, Optix connects two disjoint cascades of gene regulations to its left and right side after embryonic stage.
The time-evolving networks also provide an overview of the interactions between genes from different functional groups.
In Figure 6, we grouped genes according to 58 ontologies and visualized the connectivity between groups.
We can see that large topological changes and network rewiring occur between functional groups.
Besides expected interactions, the figure also reveals many seemingly unexpected interactions.
For instance, during the transition from pupa stage to adult stage, Drosophila is undergoing a huge metamorphosis.
One major feature of this metamorphosis is the development of the wing.
As can be seen from Figure 6r and s, genes related to metamorphosis, wing margin morphogenesis, wing vein morphogenesis and apposition of wing surfaces are among the most active groups of genes, and they carry their activity into adult stage.
Actually, many of these genes are also very active during early embryonic stage (for example, Fig.6b and c); though the difference is that they interact with different groups of genes.
On one hand, the abundance of transcripts from these genes at embryonic stage is likely due to maternal deposit (Arbeitman et al., 2002); on the other hand, this can also be due to the diverse functionalities of these genes.
For instance, two genes related to wing development, held out wings (how) and tolloid (td), also play roles in embryonic development.
4 CONCLUSION Numerous algorithms have been developed for inferring biological networks from high-throughput experimental data, such as microarray profiles (Ong, 2002; Segal et al., 2003), ChIP-chip genome localization data (Bar-Joseph et al., 2003; Harbison et al., 2004; Lee et al., 2002) and PPI data (Causier, 2004; Giot et al., 2003; Kelley et al., 2004; Uetz et al., 2000), based on formalisms such as graph mining (Tanay et al., 2004), Bayesian networks (Cowell et al., 1999) and DBN (Friedman et al., 2000; Kanazawa et al., 1995).
However, most of this vast literature focused on modeling static network or time-invariant networks, and much less has been done towards modeling the dynamic processes underlying networks that are topologically rewiring and semantically evolving over time.
The method presented in this article represents a successful and practical tool for genome-wide reverse engineering dynamic interactions between genes based on their expression data.
Given the rapid expansion of categorization and characterization of biological samples and improved data collection technologies, we expect collections of complex, high-dimensional and feature-rich data from complex dynamic biological processes, such as cancer progression, immune response and developmental processes, to continue to grow.
Thus, we believe our new method, KELLER, is a timely contribution that can narrow the gap between i134 [09:52 15/5/2009 Bioinformatics-btp192.tex] Page: i135 i128i136 KELLER: estimating time-varying networks (a) Avgerage network.
Each color patch denotes an ontological group, and the position of these ontological groups remain the same from (a) to (u).
The annotation in the outer rim indicates the function of each group.
(b) t = 1 (c) t = 4 (d) t = 8 (e) t = 12 (f) t = 16 (g) t = 20 (h) t = 24 (i) t = 28 (j) t = 32 (k) t = 35 (l) t = 38 (m) t = 41 (n) t = 44 (o) t = 47 (p) t = 50 (q) t = 53 (r) t = 56 (s) t = 59 (t) t = 62 (u) t = 65 Fig.6.
Interactions between gene ontological groups related to developmental process undergo dynamic rewiring.
The weight of an edge between two ontological groups is the total number of connection between genes in the two groups.
In the visualization, the width of an edge is proportional to its edge weight.
We thresholded the edge weight at 30 in (b)(u) so that only those interactions exceeding this number are displayed.
The average network in (a) is produced by averaging the networks underlying (b)(u).
In this case, the threshold is set to 20 instead.
imminent methodological needs and the available data and offer deeper understanding of the mechanisms and processes underlying biological networks.
Acknowledgements We thank Thomas Gulish for proofreading the manuscript.
Funding: National Science Foundation CAREER Award (grant DBI-0546594 and NSF IIS-0713379 to E.P.X.
); Alfred P. Sloan Research Fellowship (to E.P.X.
); Ray and Stephenie Lane Research Fellowship (to L.S.).
Conflict of Interest: none declared.
ABSTRACT Summary: Copy number variants (CNVs) contribute substantially to human genomic diversity, and development of accurate and efficient methods for CNV genotyping is a central problem in exploring human genotypephenotype associations.
SCIMMkit provides a robust, integrated implementation of three previously validated algorithms [SCIMM (SNP-Conditional Mixture Modeling), SCIMM-Search and SCOUT (SNP-Conditional OUTlier detection)] for targeted interrogation of CNVs using Illumina Infinium II and GoldenGate SNP assays.
SCIMMkit is applicable to standardized genome-wide SNP arrays and customized multiplexed SNP panels, providing economy, efficiency and flexibility in experimental design.
Availability: Source code and documentation are available for noncommercial use at http://droog.gs.washington.edu/scimmkit.
Contact: troyz@u.washington.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Copy number variation (CNV) in the human genome contributes substantially to genomic diversity and disease etiology (Lupski, 2009; McCarroll, 2008).
Use of genome-wide SNP genotype data to perform ab initio discovery of individual CNVs has provided valuable insight into the spectrum of human genomic variation (Itsara et al., 2009; Redon et al., 2006).
However, with the development of larger catalogs of common variation (Kidd et al., 2008; McCarroll et al., 2008) and continuing discovery of rare variants with severe phenotypic effects (Sebat et al., 2008; Walsh et al., 2008), it is critical to efficiently genotype specific CNVs in large populations.
Targeted detection strategies generally outperform ab initio detection strategies for this task (McCarroll, 2008).
Therefore, we have developed SCIMMkit, a toolkit for targeted genotyping of CNVs using Illumina Infinium II and GoldenGate SNP assays.
SNP assays typically generate two measurements per site (A and B allele fluorescence) forming the canonical genotype clusters A/A, A/B and B/B when visualized by scatterplot.
Deletions of sequence result in decreased signal intensity (i.e.
states A/, B/,/) (Fig.1), and duplications result in increased signal intensity (i.e.
states AAA and BBB) and aberrant allelic ratio (i.e.
states AAB and ABB) (Supplementary Fig.S1).
States corresponding to individual CNVs often fail to form distinct clusters To whom correspondence should be addressed.
0.0 0.2 0.4 0.6 0.
0 0.
2 0.
4 0.
6 0.
8 rs12098109 (chr1:150828032) A allele fluoresence (normalized units) B al le le fl uo re se nc e (n or m al iz ed u ni ts ) Fig.1.
Fluorescence data for 125 HapMap samples (Cooper et al., 2008) at a single SNP probe (rs12098109) within a common deletion polymorphism identified as a susceptibility factor for psoriasis (de Cid et al., 2009).
Copy number genotypes (blue diamonds, 0; red triangles, 1; black circles, 2) were computed by SCIMM using three SNP probes; superimposed curves describe components of the estimated mixture distribution.
due to dynamic range limitations; therefore, methods using multiple SNP probes per site are required for robust copy number inference (Cooper et al., 2008; Korn et al., 2008; Mefford et al., 2009).
2 DESCRIPTION OF FUNCTIONALITY SCIMMkit provides three tools for targeted interrogation of CNVs, each of which assumes prior knowledge of the approximate location of each interrogated variant: SCIMM (SNP-Conditional Mixture Modeling), for genotyping polymorphic deletions (frequency exceeding 1%); SCIMM-Search, for automatically generating informative probe sets to be used by SCIMM; and SCOUT (SNP-Conditional OUTlier detection), for detecting rare deletion and duplication variants (frequency <1%).
Each of these tools uses a statistical model of observed fluorescence data which contains, for each SNP probe, separate location parameters for each homozygous allelic state (i.e.
A/A, A/, B/B, B/) and a single dispersion parameter shared by all homozygous allelic states.
The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:32 2/12/2009 Bioinformatics-btp606.tex] Page: 121 120122 Targeted interrogation of CNV by SCIMMkit SCIMM assigns diallelic insertion/deletion genotypes (i.e.
copy number 0, 1 and 2), using SNP calls and normalized fluorescence measurements for a set of n SNP probes hybridizing specifically to sequence spanning the deleted region (Cooper et al., 2008).
Two rounds of mixture likelihood-based clustering are used: the first round uses intensity data to call samples near the origin as 0, and the second round uses intensity data and supplied SNP genotypes to call remaining samples as 1 or 2, using a two-component, 2n-variate lognormal mixture model.
Copy number for each sample is assumed to be constant for all probes in a set; accordingly, samples that are SNP heterozygous at any probe are assumed to have copy number 2 for the purposes of model fitting and copy number assignment.
These statistical assumptions do not hold for SNP probes that hybridize non-specifically (Supplementary Fig.S2); such probes are rejected during probe set generation, below.
SCIMM also generates a score for the probe set, defined as the difference of the Bayesian information criterion (BIC) value for the two-component model and the BIC value for the corresponding one-component model.
Genotypes are reported only for sites with positive scores.
SCIMM-Search can be used to automatically generate informative probe sets in circumstances where the specificity assumptions of SCIMM may not be satisfied for all probes in the putatively deleted region.
SCIMM-Search uses the BIC to select between alternate probe sets, and allows the investigator to specify constraints on consistency with reference genotypes, internal consistency of the probe set, probe spacing and dynamic range (Cooper et al., 2008).
SCOUT detects rare deletions and duplications at each targeted site by initially calculating per-probe scores for each sample, using a one-component SCIMM model extended to describe fluorescence data for SNP heterozygotes (Mefford et al., 2009).
For SNP homozygotes, per-probe score is determined solely by intensity; for SNP heterozygotes, per-probe score is determined by intensity and deviation from 1:1 allelic ratio (specifically, by distance of the observed datum from the line connecting the origin to the center of the heterozygote cluster).
Per-probe scores are approximately normally distributed, with samples at the center of each canonical SNP genotype cluster receiving a score of zero.
The per-probe scores are combined additively to obtain per-site scores, which are then compared with an empirically determined threshold to generate a list of putative deletion and duplication events.
Hemizygous and duplicated haplotypes for strongly scoring events are also reported, allowing inference of complex allelic states (e.g.
AAB, Supplementary Fig.S1) and parental chromosome of origin (in cases where parental data are available).
SCIMMkit also implements an initial SCOUT quality-control pass which rejects samples with a genome-wide excess of extreme per-probe scores, improving the positive predictive value of the per-site SCOUT scores generated for the remaining samples (Supplementary Fig.S1).
SCIMMkit requires as input a target file and one or more data files.
Each line of the target file specifies a set of probes (with probe ID and coordinates) and an action associated with the probe set (i.e.
SCIMM genotyping, SCIMM-Search probe set generation or SCOUT scoring).
Input is supplied in Illumina BeadStudio genotype report format (or similar tabular format).
SCIMMkit generates two primary output files: a comma-delimited matrix with scores and numeric genotype codes (one row per sample and one column per target site), and a comma-delimited table with per-site summary information including genotype counts, probe set scores and SCIMM-Search generated probe sets.
SCIMMkit can optionally generate scatterplots with superimposed SCIMM genotypes and mixture distribution curves in postscript format.
SCIMMkit is implemented in PERL (used for command-line interpretation, input parsing and data consolidation) and R (used for numerically intensive tasks), and has been tested on Apple Macintosh OS X, Linux and Microsoft Windows platforms.
3 DISCUSSION SCIMM and SCOUT use a common statistical model to facilitate distinct applications.
SCIMM genotypes polymorphic deletions by estimating the location of each genotype cluster (/, A/, B/, A/A, A/B, B/B).
SCOUT detects rare deletion and duplication variants by analyzing the location of each sample relative to the canonical SNP genotype clusters (A/A, A/B, B/B), avoiding estimation of location parameters for rare allelic states (e.g.
deletion states A/, B/ and duplication states AAB, ABB).
SNP-based genome-wide association studies have generated a wealth of resources for retrospective analysis of CNV (Itsara et al.
2009).
The first step in analyzing polymorphic variation in such data is identification of CNVs that can be accurately genotyped.
To generate a database of polymorphic deletion sites and validated copy number-informative probe sets, we used SCIMM-Search to analyze data generated by the Illumina 1M-DuoV3 array for 269 HapMap samples.
We compared the resulting SCIMM-generated diallelic deletion genotypes with previously published genotypes generated by BirdSuite software using the Affymetrix SNP 6.0 array (McCarroll et al., 2008).
SCIMM produced diallelic deletion genotypes for 113 common (sample allele frequency at least 5%) autosomal deletions (84% of which have per-site concordance to BirdSuite genotypes exceeding 99%), 392 autosomal deletions of lower frequency (88.5% of which have genotype concordance exceeding 99% and positive predictive value for deletion status exceeding 80%) and 6 X-linked diallelic deletions (all of which have concordance exceeding 98.5%).
These concordance rates are consistent with earlier analyses using independently generated reference genotypes (Cooper et al., 2008).
The resulting list of highly concordant sites and corresponding Illumina 1M-DuoV3 probe sets produced by SCIMM-Search are provided on the SCIMMkit web site for genotyping polymorphic deletions in other genome-wide datasets.
(See Supplementary Material for details).
Detection of highly pathogenic CNVs presents a distinct challenge: individually, such variants tend to be rare (frequency <1%) in affected individuals and very rare or completely absent in control populations; thus, definitively establishing a difference in allele frequency between cases and controls requires analysis of a large number (many thousands) of samples (International Schizophrenia Consortium, 2009).
To assess the feasibility of large-scale targeted detection studies, SCOUT was recently used in conjunction with a customized Illumina BeadXpress assay to genotype deletions and duplications at 69 non-allelic homologous recombination hotspots in 1005 individuals with unexplained intellectual disability (ID).
SCOUT correctly detected 48 rare deletion and duplication events, including 22 events known to be pathogenic, with only seven false positives (score threshold |6|, events validated by oligo-array CGH) (Mefford et al., 2009).
Although, SCOUT does not explicitly include batch effects in its statistical model, the robustness of its model-fitting procedure at 121 [12:32 2/12/2009 Bioinformatics-btp606.tex] Page: 122 120122 T.Zerr et al.
small sample sizes allows known batch effects to be remedied by independent scoring of batches.
In the ID study above, each 96-well plate was analyzed independently to provide robustness against plate-to-plate variation in signal intensity and dynamic range.
We anticipate that future studies of association between CNV and phenotype will follow a model similar to SNP-based studies: an ab initio discovery stage (often in a population enriched for the phenotype of interest), an initial phenotypic association testing stage and a validation stage where the strongest associations are tested in a much larger population.
SCIMMkit allows efficient and accurate detection of CNVs in the latter two stages of this model, facilitating further exploration of the link between CNV and human phenotypic variation.
Funding: T.Z.
and D.A.N.
acknowledge support from NIH grants HL66682 and HL66642.
G.M.C.
is supported by a Merck, Jane Coffin Childs Memorial Fund Fellowship.
E.E.E.
acknowledges support from NIH grant HG004120 and HD065285.
E.E.E.
is an investigator of the Howard Hughes Medical Institute.
Conflict of Interest: EEE is a SAB member of Pacific Biosciences.
ABSTRACT Summary: Recently developed methods that couple next-generation sequencing with chromosome conformation capture-based tech-niques, such as Hi-C and ChIA-PET, allow for characterization of genome-wide chromatin 3D structure.
Understanding the organization of chromatin in three dimensions is a crucial next step in the unraveling of global gene regulation, and methods for analyzing such data are needed.
We have developed HiBrowse, a user-friendly web-tool con-sisting of a range of hypothesis-based and descriptive statistics, using realistic assumptions in null-models.
Availability and implementation: HiBrowse is supported by all major browsers, and is freely available at http://hyperbrowser.uio.no/3d.
Software is implemented in Python, and source code is available for download by following instructions on the main site.
Contact: jonaspau@ifi.uio.no Supplementary Information: Supplementary data are available at Bioinformatics online.
Received on October 18, 2013; revised on January 17, 2014; accepted on February 3, 2014 1 INTRODUCTION Methods for detection of genome-wide chromatin 3D conform-ation, such as Hi-C (Lieberman-Aiden et al., 2009) and ChIA-PET (Fullwood et al., 2009), are drastically expanding our understanding of genome biology.
However, statistical and com-putational methods to analyze chromatin conformation capture-based data are needed.
Many of the available methods focus on data visualization, or are not suited for genome-wide statistical investigations (Bau et al., 2010; Servant et al., 2012; Thongjuea et al., 2013; Zhou et al., 2013).
The structure of chromatin makes statistical analysis complicated, due to correlations between the interaction frequencies caused by both sequence-dependent and topological constraints (Paulsen et al., 2013).
A few statistical tests have been proposed, with varying possibilities to account for structural dependencies (Botta et al., 2010; Kruse et al., 2013; Paulsen et al., 2013; Wang et al., 2013; Witten and Noble, 2012).
Two useful command-line tools are the hiclib-package (Imakaev et al., 2012), and the HOMER software suit (Heinz et al., 2010), which both allow for noise-removal, outlier detection and com-partment identification.
The HOMER software additionally allows for identification of significant interactions in a given dataset, assuming a binomial distribution and a background model taking into account sequence-based and compartmental biases.
The global nature of these data allow for other types of stat-istical investigations beyond detecting significance of individual interactions.
A common type of analysis is to analyze a set of genomic elements (genes, regulatory elements, transcription fac-tors, etc.
), and ask how this subset, or query track, is spatially arranged in 3D space as represented by a Hi-C dataset, for ex-ample.
Here we present HiBrowse, a web-based analysis server for performing statistical analysis of 3D genomes in a range of different settings.
The available statistics provide a flexible and expandable catalog of tools based on state-of-the-art statistical methods utilizing Monte Carlo (MC) and analytic methods as suited, in addition to a range of tools for visualization and hy-pothesis-generating investigations.
2 FEATURES AND METHODS 2.1 Data representation and analysis framework We build on general software components of the Genomic HyperBrowser (Sandve et al., 2010, 2013), a web-based analysis server for genome-scale data.
The graphical user interface (GUI) is based on Galaxy (Goecks et al., 2010), a user-friendly point-and-click environment familiar to many researchers.
All tracks are based on a representation of elements as mathematical objects, consisting of points, segments, functions and variants of these [see Gundersen et al.
(2011) for an in-depth discussion].
Any given analysis can be performed on all chromosomes, specific chromosomes or selected sub-parts of chromosomes, depending on the needs.
In practice, an analysis is initiated by selecting one or more tracks either from the HyperBrowser repository, or from the user history.
At least one of the selected tracks must be a Hi-C (3D) track, and the accompanying selected tracks (called query tracks) determine the types of statistical analyses that are possible, and therefore selectable in the system.
A range of publicly available 3D-datasets have been installed in the repository.
Since it has been shown that Hi-C and similar data can con-tain systematic biases, all the available Hi-C datasets have been corrected*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Zhou etal.,2013; Witten and Noble,2012; ).
, `` '' , ( ).
, `` '' for such biases using the method of Imakaev et al.
(2012).
Furthermore, a specialized tool has been developed to allow users to upload their own Hi-C data (or similar) into the history, even if the dataset itself does not conform to well-known formats.
See Supplementary Table S1 for a list of already installed and pre-processed Hi-C datasets.
2.2 Overview of statistical methods Statistical tools are divided into two broad categories: hypothesis tests and descriptive statistics.
Hypothesis tests are both MC based and ana-lytical.
Due to the complex structure of chromatin conformation capture data, finding suited explicit null distributions is generally not possible (Paulsen et al., 2013; Witten and Noble, 2012), and even randomization of the data through MC is difficult.
Therefore, we consistently perform permutations on the query track only.
The hypothesis tests can be divided into three types, defined by the query track type, as illustrated in Figure 1A.
For example, Points (P) are used to analyze general (all-versus-all) 3D co-localization by specifying a set of genomic elements using the BED format, while Linked Points (LP) are used to analyze 3D co-localization between selected pairs of elements by providing add-itional information about which genomic elements that should be linked together.
In the most basic case, if the user selects a set of points (genomic elements) in BED-format in addition to a Hi-C data track, one may ask whether all the genomic elements in the BED-file are more/less co-localized in 3D, in an all-versus-all fashion, than what would be expected by chance.
In this case, the mean of the observed standardized interaction frequencies is compared to the expected value estimated from the per-muted positions in representative regions of the rest of the Hi-C (3D) track.
This analysis was introduced in Paulsen et al.
(2013), and in this article we expand the methodologies by allowing a much wider variety of query tracks.
For example, by specifying two point-tracks (two BED files), in addition to a Hi-C (or similar) track, the user can ask whether the points in track 1 are more/less co-localized with track 2, than expected by chance.
In this type of statistical question, the permutations can be performed on both of the point-tracks, or by preserving one of the point-tracks completely.
It is also possible to specify particular interactions between a set of genomic elements, and compare these interactions with randomly per-muted interactions within the same set of elements.
In HiBrowse, inter-actions between genomic elements are defined using LP, a format described in detail elsewhere (Gundersen et al., 2011).
Such linked track types can easily be created by using a dedicated tool that converts from a simple BED file format containing information about which elements that should be linked together (see Supplementary Fig.S1, for an ex-ample).
Since this type of analysis only permutes interactions intrinsically with regards to the query track, the positions of all elements will be completely preserved.
This type of analysis should be used whenever specific interactions between genomic elements are considered, and it would be natural to compare with random links between the same elem-ents.
Since regions of the genome can have varying properties (active/ inactive genes, open/closed chromatin, etc.
), global shuffling of links be-tween all selected elements is not always preferable.
To take such proper-ties into account during the permutation, each of the points can be marked by a value, such that the link-permutations will be performed by preserving the value-combinations on both sides of the links.
If the user wants full control over exactly what pairs of interactions that are allowed to take part in the link-permutations, it is possible to specify a case/control value on each of the links via a dedicated tool which accepts two BED files (case and control) of the same format as described above (see Supplementary Fig.S2, for an example).
The case/control-linked elements can then be selected together with a Hi-C (3D) track, allowing the user to compare the interaction frequency of all the links marked as case with the expected interaction frequency given by permuting the case/control labels.
This type of statistic is optimal for data that is only sampled from a pre-defined set of elements of the genome, and where the user wants to find out whether a subset of these elements are co-localized in 3D.
Finally, it is possible to find statistically significant differences be-tween two Hi-C datasets, for example comparing treatments [as e.g.
in A B C Fig.1.
(A) Overview of statistical hypothesis tests implemented in HiBrowse.
See Gundersen et al.
(2011) for an in-depth explanation of track types, and the Supplementary Material for details about each statistic.
(B) Example of a HiBrowse analysis using the Linked elements more/less co-localized in 3D?
statistic, investigating whether fusion transcripts are co-localized in 3D.
(C) Result page from the analysis, presenting the question asked by the user together with both a simplistic and a more detailed answer giving the P-value and model assumption details.
Links are provided to full details of the results at individual chromosome regions 1621 Analysis of genome-wide chromatin 3D organization data setdata sets.
Monte Carlo ( ) ;Paulsen etal.,2013 paper Linked Points ( ), Sdata sets (Rickman et al.
(2012)].
The statistical test implemented for this type of analysis is based on the edgeR-tool (Robinson et al., 2010).
Details about the mathematical formulation of the different types of statistics and their corresponding null-hypotheses are found in the Supplementary Material.
In addition to hypothesis tests, a range of descriptive statistics have been implemented.
For example, each hypothesis test is accompanied by an enrichment score, giving the degree of over/under-representation of 3D co-localization, compared to the expected 3D co-localization (see Supplementary Material for details).
Other types of available descriptive statistics are visualization of clustered Hi-C matrices as heatmaps or graphs, principal component analysis on Hi-C matrices and other sum-mary statistics (see Supplementary Table S2 for a comprehensive list).
All available analyses are described thoroughly on the help pages linked from the main site, where example histories are provided such that users can explore each statistic in detail.
Demo-buttons are provided for all tools, giving small example runs.
See Figure 1B and C for an analysis example.
Funding: This work was supported by the Norwegian Cancer Society [PR-2006-0433].
Conflict of Interest: none declared.
REFERENCES Bau,D.
et al.
(2010) The three-dimensional folding of the-globin gene domain reveals formation of chromatin globules.
Nat.
Struct.
Mol.
Biol., 18, 107114.
Botta,M.
et al.
(2010) Intra-and inter-chromosomal interactions correlate with CTCF binding genome wide.
Mol.
Syst.
Biol, 6.
Fullwood,M.J.
et al.
(2009) An oestrogen-receptor--bound human chromatin interactome.
Nature, 462, 5864.
Goecks,J.
et al.
(2010) Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences.
Genome Biol., 11, R86.
Gundersen,S.
et al.
(2011) Identifying elemental genomic track types and represent-ing them uniformly.
BMC Bioinformatics, 12, 494.
Heinz,S.
et al.
(2010) Simple combinations of lineage-determining transcription factors prime cis-regulatory elements required for macrophage and B cell iden-tities.
Mol.
Cell, 38, 576589.
Imakaev,M.
et al.
(2012) Iterative correction of Hi-C data reveals hallmarks of chromosome organization.
Nat.
Methods, 9, 9991003.
Kruse,K.
et al.
(2013) A complex network framework for unbiased statistical ana-lyses of DNADNA contact maps.
Nucleic Acids Res., 41, 701710.
Lieberman-Aiden,E.
et al.
(2009) Comprehensive mapping of long-range inter-actions reveals folding principles of the human genome.
Science, 326, 289293.
Paulsen,J.
et al.
(2013) Handling realistic assumptions in hypothesis testing of 3D co-localization of genomic elements.
Nucleic Acids Res., 41, 51645174.
Rickman,D.S.
et al.
(2012) Oncogene-mediated alterations in chromatin conform-ation.
Proc.
Natl Acad.
Sci.
USA, 109, 90839088.
Robinson,M.D.
et al.
(2010) edgeR: a bioconductor package for differential expres-sion analysis of digital gene expression data.
Bioinformatics, 26, 139140.
Sandve,G.K.
et al.
(2010) The Genomic HyperBrowser: inferential genomics at the sequence level.
Genome Biol., 11, R121.
Sandve,G.K.
et al.
(2013) The Genomic HyperBrowser: an analysis web server for genome-scale data.
Nucleic Acids Res., 41, W133W141.
Servant,N.
et al.
(2012) HiTC: exploration of high-throughput C experiments.
Bioinformatics, 28, 28432844.
Thongjuea,S.
et al.
(2013) r3Cseq: an R/Bioconductor package for the discovery of long-range genomic interactions from chromosome conformation capture and next-generation sequencing data.
Nucleic Acids Res., 41, e132.
Wang,H.
et al.
(2013) Topological properties of chromosome conformation graphs reflect spatial proximities within chromatin.
In: Proceedings of the International Conference on Bioinformatics, Computational Biology and Biomedical Informatics.
ACM, Washington, DC, USA, p. 306.
Witten,D.M.
and Noble,W.S.
(2012) On the assessment of statistical significance of three-dimensional colocalization of sets of genomic elements.Nucleic Acids Res., 40, 38493855.
Zhou,X.
et al.
(2013) Exploring long-range genome interactions using the WashU Epigenome Browser.
Nat.
Methods, 10, 375376.
1622 J.Paulsen et al.
)).
(PCA) ,
ABSTRACT Motivation: Multivariate experiments applied to mammalian cells often produce lists of proteins/genes altered under treatment versus control conditions.
Such lists can be projected onto prior knowledge of kinasesubstrate interactions to infer the list of kinases associated with a specific protein list.
By computing how the proportion of kinases, associated with a specific list of proteins/genes, deviates from an expected distribution, we can rank kinases and kinase families based on the likelihood that these kinases are functionally associated with regulating the cell under specific experimental conditions.
Such analysis can assist in producing hypotheses that can explain how the kinome is involved in the maintenance of different cellular states and can be manipulated to modulate cells towards a desired phenotype.
Summary: Kinase enrichment analysis (KEA) is a web-based tool with an underlying database providing users with the ability to link lists of mammalian proteins/genes with the kinases that phosphorylate them.
The system draws from several available kinasesubstrate databases to compute kinase enrichment probability based on the distribution of kinasesubstrate proportions in the background kinasesubstrate database compared with kinases found to be associated with an input list of genes/proteins.
Availability: The KEA system is freely available at http://amp.pharm.
mssm.edu/lib/kea.jsp Contact: avi.maayan@mssm.edu 1 INTRODUCTION Protein phosphorylation causes the addition of a phosphate group onto serine, threonine or tyrosine amino-acid residues of proteins.
Phosphorylations are precise reversible changes that are used to regulate intracellular events such as protein complex formation, cell signaling, cytoskeleton remodeling and cell cycle control.
Consequently, protein kinases, which are responsible for the phosphorylations, play an important role in controlling protein function, cellular machine regulation and information transfer through cell signaling pathways.
Kinase activities therefore have definitive regulatory effects on a broad variety of biological processes, in which activated kinases typically target a large number of different substrate proteins.
There are over 500 protein kinases encoded in the human genome, and it is approximated that 40% of all proteins are phosphorylated at some stage in different cell types and at different cell states (Manning et al., 2002).
Furthermore, kinases To whom correspondence should be addressed.
regulate each other through phosphorylation, resulting in a complex web of regulatory relations (Maayan et al., 2005).
High-throughput techniques such as stable isotope labeling coupled with affinity purification and mass-spectrometry proteomics are now able to identify phosphorylation sites on multiple proteins under different experimental conditions.
Databases that integrate the results from such studies are emerging, e.g.
phosphosite (Hornbeck et al., 2004).
However, such data does not provide the kinases responsible for the phosphorylation.
Several resources are available to link identified phosphorylation sites to the kinases that are most likely responsible for protein phosphorylations (Huang et al., 2005; Linding et al., 2008).
For example, NetworKIN (Linding et al., 2007; Linding et al., 2008) uses an algorithm to predict the most probable kinase that is responsible for phosphorylating an identified phosphosite.
The NetworKIN algorithm is accompanied with a database containing 1450 predicted mammalian substrates that are mapped to 73 upstream protein kinases belonging to 21 kinase families.
Although useful, the coverage of this dataset is not comprehensive enough for kinase statistical enrichment analysis.
To achieve more comprehensive prior knowledge kinasesubstrate dataset, large enough for statistical enrichment analysis, we merged interactions from several other online sources reporting mammalian kinasesubstrate relations.
Additionally, we included binary protein-protein interactions involving kinases from proteinprotein interaction databases as these were recently proposed to be highly enriched in kinasesubstrate relations: in a recent study that identified 14 000 phosphosites at different stages of the cell cycle in Hela cells (Dephoure et al., 2008) it was shown that many phosphosites experimentally identified using phosphoproteomics can be associated with four known kinases (CDC2, PLK1, Aurora-B and Aurora-A) using the literature-based proteinprotein interactions from the HPRD database (Mishra et al., 2008).
Hence, having a large background knowledge dataset of kinasesubstrate interactions and proteinprotein interactions that involve kinases, we can associate large lists of proteins/genes with many kinases that phosphorylate them.
This allows the computation of statistical enrichment which can be used to suggest the kinases that are most likely to be involved in regulating the proteins/genes from a list generated under specific experimental conditions.
2 IMPLEMENTATION We first constructed a database that consolidates kinase substrate interactions from multiple online sources.
We integrated data describing kinasesubstrate interactions from NetworKIN 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Kinase enrichment analysis (Linding et al., 2008), Phospho.ELM (Diella et al., 2004), MINT (Chatr-aryamontri et al., 2007), HPRD (Mishra et al., 2008), PhosphoPoint (Yang et al., 2008) and Swiss-Prot (Quintaje and Orchard, 2008) as well as phosphorylation interactions we manually previously extracted from literature (Maayan et al., 2005).
The NetworKIN database contains 3847 kinasesubstrate unique pairs made of 73 kinases (21 families) linked to 1452 substrates.
HPRD contains 1794 kinasesubstrate pairs made of 229 kinases linked to 864 substrates.
Phospho.Elm has 1451 interactions between 225 kinases and 784 substrates.
MINT has 269 interactions between 145 kinases and 184 substrates.
In phosphoPoint there are 436 kinases, 3076 substrates, 9251 kinasesubstrate relations from which only 1587 are unique in this dataset, while the rest overlaps with the other databases.
In Maayan et al., there are 66 interactions between 19 kinases and 43 substrates.
There is some overlap among these sources such that the number of unique kinasesubstrate relations totals 6414 links between 352 kinases and 2014 substrates in the combined dataset.
We consolidated interactions from mouse and rat into human by converting all protein/gene IDs to human Entrez gene symbols.
Each kinasesubstrate data record is associated with a specific kinase, kinase family and kinase subfamily.
To group kinases into families, we used the kinome tree from Manning et al.
(2002) where kinases are classified into 10 major classes and 119 families.
To further increase the size of our background dataset, we included all direct proteinprotein interactions involving kinases from HPRD (Mishra et al., 2008) and MINT (Chatr-aryamontri et al., 2007).
By this expansion the current dataset contains a total of 11 923 interactions between 445 kinases having 3995 substrates.
The analysis begins with an input list of gene symbols entered by the user for kinase enrichment analysis (KEA).
Before performing the KEA, we remove all input entries that do not match a substrate in the consolidated background kinasesubstrate dataset.
This step is necessary for achieving proportional comparison.
The expected value for a randomly generated list of kinasesubstrates can be found by determining the cardinality of the set of substrates that are targeted by specific kinases (or family of kinases) dividing such number by the total number of substrates in the background dataset.
In order to detect statistical significant deviations from this expected value, we use the Fisher Exact Test (Fisher, 1922).
The P-value can be used to distinguish specific kinases among the large number of kinases appearing in the output table.
To implement the web-based system we use Java Server Pages (JSP) and MySQL database running on a Tomcat server.
All reported results can be exported to Excel via CSV files.
Additionally, users can mouse over on the number of targets for each kinase, kinase family or class to see the list of substrates and view a connectivity diagram that visualizes known proteinprotein interactions within the substrates using a database of proteinprotein interactions we previously published (Berger et al., 2007).
The map is dynamic where users can move nodes around and click on nodes for more detail (Fig.1).
The visualization of these connectivity diagrams was achieved using Adobe Flash CS4 with ActionScript.
Such subgraphs can be used to link kinase specific substrates to pathways and complexes.
As prior knowledge is increasingly used to interpret high-throughput results, e.g.
Balazsi et al.
(2008), we anticipate that KEA is going to be especially useful for the analysis of proteomics and phosphoproteomics data.
KEA can be used for analyzing multivariate datasets collected on a time-course to observe trends Fig.1.
Screenshot of the KEA user interface.
Users can paste lists of Entrez gene symbols, representing human proteins; select the level of analysis: kinase-class, kinase-family or kinase and then the program outputs a list of ranked kinase-classes, kinase-families or kinases based on specificity of phosphorylating substrates from the input list.
Substrates can be then connected based on their known proteinprotein interaction using an original network viewer developed using Adobe Flash CS4.
in kinase activity overtime.
Results that show changes in kinase enrichment under different conditions can be due to one of the following reasons: change in kinase enzymatic activity, change in kinase subcellular localization or changes in kinase concentration.
Furthermore, KEA can help researchers understand how they can perturb cellular systems toward a desired phenotype by targeting a kinase or group of kinases with pharmacological or gene silencing means.
Kinase signaling is well-established to be disturbed in many disease states, especially in cancer (Blume-Jensen and Hunter, 2001), while it is apparent that phenotypic integrity is controlled by the activity of the regulated behavior of multiple kinases.
Hence, mapping kinase activation patterns based on different experimental conditions and time points when measuring many genes/proteins at once in diseased/perturbed versus normal/control may directly suggest combinations of kinase inhibitors that would shift the cellular state towards a desired phenotype.
ACKNOWLEDGEMENTS We would like to thank Ben MacArthur, Amin Mazloom, Ihor Lemischka, Kevin Xiao and Robert Lefkowitz for useful discussions.
Funding: National Institutes of Health (Grant No.
P50GM071558); Seed fund, Mount Sinai School of Medicine (to A.M.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Modern systems biology aims at understanding how the different molecular components of a biological cell interact.
Often, cellular functions are performed by complexes consisting of many different proteins.
The composition of these complexes may change according to the cellular environment, and one protein may be involved in several different processes.
The automatic discovery of functional complexes from protein interaction data is challenging.
While previous approaches use approximations to extract dense modules, our approach exactly solves the problem of dense module enumeration.
Furthermore, constraints from additional information sources such as gene expression and phenotype data can be integrated, so we can systematically mine for dense modules with interesting profiles.
Results: Given a weighted protein interaction network, our method discovers all protein sets that satisfy a user-defined minimum density threshold.
We employ a reverse search strategy, which allows us to exploit the density criterion in an efficient way.
Our experiments show that the novel approach is feasible and produces biologically meaningful results.
In comparative validation studies using yeast data, the method achieved the best overall prediction performance with respect to confirmed complexes.
Moreover, by enhancing the yeast network with phenotypic and phylogenetic profiles and the human network with tissue-specific expression data, we identified condition-dependent complex variants.
Availability: A C++ implementation of the algorithm is available atContact: koji.tsuda@tuebingen.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Today, a large number of databases provide access to experimentally observed proteinprotein interactions (PPIs).
The analysis of the corresponding protein interaction networks can be useful for functional annotation of previously uncharacterized genes as well as for revealing additional functionality of known genes.
Often, function prediction involves an intermediate step where clusters of densely interacting proteins, called modules, are extracted from To whom correspondence should be addressed.
the network; the dense subgraphs are likely to represent functional protein complexes (Sharan et al., 2007).
However, the experimental methods are not always reliable, which means that the interaction network may contain false positive edges.
Therefore, confidence weights of interactions should be taken into account.
A natural criterion that combines these two aspects is the average pairwise interaction weight within a module [assuming a weight of zero for unobserved interactions (Ulitsky and Shamir, 2007)].
We call this the module density, in analogy to unweighted networks (Bader and Hogue, 2003).
We present a method to enumerate all modules that exceed a given density threshold.
It solves the problem efficiently via a simple and elegant reverse search algorithm, extending the unweighted network approach by Uno (2007).
Remarkably, the required computation time between two consecutive solutions is polynomial in the input size.
The contribution of our article consists in (i) the development of a dense module enumeration (DME) algorithm for weighted networks, including a ranking scheme and an efficient strategy to identify locally maximal modules, (ii) its application to the protein interaction networks of yeast and human and (iii) the effective integration of constraints from additional data sources.
There is a large variety of related work on module discovery in networks.
The most common group are graph partitioning methods (Chen and Yuan, 2006; Newman, 2006; van Dongen, 2000).
They divide the network into a set of modules, so their approach is substantially different from DME, which provides an explicit density criterion for modules (Fig.1A).
Another group of methods define explicit module criteria, but employ heuristic search techniques to find the modules (Bader and Hogue, 2003; Everett et al., 2006).
This contrasts with complete enumeration algorithms, which form the third line of research: they give explicit criteria and return all modules that satisfy them.
For example, clique search has been frequently applied (Palla et al., 2005; Spirin and Mirny, 2003).
The enumeration of cliques can be considered as a special case of our approach, restricting it to unweighted graphs and a density threshold of one.
Further enumerative approaches use different module criteria assuming unweighted graphs (Haraguchi and Okubo, 2006; Zeng et al., 2006).
Biological complexes are dynamic objects of changing composition.
In particular, many proteins are not steadily present in the cell, but specifically expressed depending on organism, cell type, environmental conditions and developmental stage 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 934 933940 E.Georgii et al.
Fig.1.
DME approach.
(A).
DME versus partitioning.
While partitioning methods return one clustering of the network, DME discovers all modules that satisfy a minimum density threshold.
(B).
Combination with profile data.
Integration of PPI and external profile data allows to focus on modules with consistent behavior of all member proteins in a subset of conditions.
(Gavin et al., 2002).
Module enumeration offers a meaningful way to detect such variations of complexes.
Our DME algorithm can easily incorporate constraints from additional information like gene expression, evolutionary conservation, subcellular localization or phenotypic profiles.
Thus, the search can be guided directly towards the modules of interest, for example, modules that show coherent behavior in a subset of conditions.
The external data sources can provide further evidence for functional relationships of proteins and yield insights about possible functional roles of complexes and subcomplexes in different cellular contexts.
In recent years, many module finding approaches which integrate PPI networks with other gene-related data have been published.
One strategy, often used in the context of partitioning methods, is to build a new network whose edge weights are determined by multiple data sources (Hanisch et al., 2002).
Tanay et al., 2004 also create one single network to analyze multiple genomic data at once; however, they use a bipartite network where each edge corresponds to one data type only.
In both cases, the different datasets have to be normalized appropriately before they can be integrated.
In contrast to that, other approaches keep the data sources separate and define individual constraints for each of them.
Consequently, arbitrarily many datasets can be jointly analyzed without the need to take care of appropriate scaling or normalization.
Within this class of approaches, there exist two main strategies to deal with profile data like gene expression measurements.
In the first case, the profile information is transformed into a gene similarity network, where the strength of a link between two genes represents the global similarity of their profiles (Pei et al., 2005; Segal et al., 2003; Ulitsky and Shamir, 2007).
In the second case, the condition-specific information is kept to perform a context-dependent module analysis (Huang et al., 2007; Ideker et al., 2002; Yan et al., 2007).
Our approach follows along this line, searching for modules in the PPI network that have consistent profiles with respect to a subset of conditions.
In contrast to the previous methods, our algorithm systematically identifies all modules satisfying a density criterion and optional consistency constraints.
In this study, we evaluate our approach on the yeast interaction network in comparison with four other methods.
Also, we report yeast modules restricted by evolutionary conservation and phenotypic profiles.
Furthermore, we discuss our results obtained from human protein interactions in the context of gene expression data.
2 MODULE MINING APPROACH We address the problem of extracting functional modules from PPI data using an enumerative density-based mining approach.
Today, there exist various experimental techniques to determine PPIs.
To analyze these data, it is common practice to integrate all interactions into one network where each node represents a protein, and an edge between two nodes indicates an interaction (Sharan et al., 2007).
Then node sets with higher density in the interaction network are more likely to represent functional protein complexes.
We propose a method to exhaustively enumerate all modules which satisfy a minimum density threshold.
To avoid spurious modules, confidence weights of interactions are taken into account.
In this section, we first describe the basic algorithm and then show how to integrate additional constraints in this framework.
Finally, we explain our module ranking criterion.
2.1 Dense module enumeration Formally, let us consider the interaction network as undirected weighted graph with node set V .
Let W = (wij)i,jV be the corresponding matrix representation, containing positive weight entries for the given interactions and zero entries otherwise (for missing edges).
In the following, we assume wij 1.
Although we use weight matrices with non-negative entries in this work, the approach is suitable for mixed-sign data as well.
A module is defined as a set of nodes U V and its induced subgraph.
The density of U refers to the average pairwise weight, given by W (U)= i,jU,i<j wij |U|(|U|1)/2 .
(1) The largest possible density value is 1 [we define W (U) :=1 for |U|=1].
Now we define the problem of DME as follows.
Definition 1.
Given a graph with node set V and weight matrix W, and a density threshold >0, find all modules U V with W (U) .
The key point of any enumeration algorithm is the definition of an appropriate structure of the search space which allows for efficient traversal and pruning.
To enumerate sets of entities, a canonical approach is to start with the empty set and then iteratively form larger sets by adding one element at a time; if it is evident that no further solutions can be derived from a certain set, the process of extension is stopped, i.e.
unnecessary parts of the search space are pruned.
It turns out that conventional pruning strategies as used in itemset mining (Han and Kamber, 2006), for example, are not suitable for DME.
The reason is that supersets of a module can in general have arbitrarily higher or lower density than the module itself (see Supplementary Material).
However, it is possible to traverse the search space in a way that allows for straightforward pruning.
In fact, we define a tree-based parent child relationship between modules such that along each path from the root to a leaf, the module size is increasing, whereas the module density is monotonically decreasing.
Technically, our algorithm adopts the reverse search paradigm (Avis and Fukuda, 1996): in each iteration, we generate all direct supersets of the current module and select those which are indeed its children.
Due to the monotonicity guarantee in our search tree, only children 934 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 935 933940 Condition-dependent dense module enumeration that fulfill the density criterion have to be further processed.
To describe our approach in more detail, we need the definition of weighted degree.
Definition 2.
Let W be the given weight matrix.
For uU V, the weighted degree of u with respect to U is defined as degW (u,U)= jU,j =u wuj .
The following lemma yields the key for defining the search tree.
Lemma 1.
Let vU be a node with minimum weighted degree in U, i.e.
uU : degW (u,U)degW (v,U).
Then, W (U \{v})W (U).
The proof is given in the Supplementary Material.
Further, we introduce a function ord, which defines a strict total ordering on the nodes, i.e.
for each node pair u, v with u =v either ord(u)<ord(v) or ord(u)>ord(v) holds.
With this, we define the parentchild relationship for modules.
Definition 3.
Let U be a module and vV \U.
U :=U {v} is a child of U if and only if uU one of the following conditions holds: 1. degW (v,U )<degW (u,U) 2. degW (v,U )=degW (u,U)ord(v)<ord(u) In other words, we obtain the unique parent of a module by removing the smallest among the nodes with minimum weighted degree.
From the lemma we know that each module has a smaller or equal density than its parent.
Based on this, the DME algorithm starts with the empty set and recursively generates children as long as the density threshold is not violated (Algorithm 1), yielding thereby the complete set of dense modules.
By the definition of the parentchild relationship, we cannot directly derive the children of a module U.
Instead, we have to check for all possible extended modules with one additional node whether U is their parent or not (reverse search principle).
In terms of complexity, DME belongs to the class of polynomial-delay algorithms, which means that, independently of the size of the results, the computation time between two consecutive solutions is polynomial in the input size (see Supplementary Material).
By changing the density threshold, the user can regulate the size of the output.
Also note that the computation can easily be parallelized.
Finally, dense modules that are subsets of other solutions are not so informative; we call them non-maximal.
While these redundant results could be eliminated by checking for each new module all previous solutions, it is possible to identify locally maximal modules without requiring additional computation or storage, as shown in Algorithm 1.
A module U is locally maximal if and only if for all vV \U, U {v} does not satisfy the minimum density threshold.
Although a module with this property could still be non-maximal, it happens rarely in practice.
Algorithm 1 DME for node set V , weight matrix W , and minimum density .
U represents the current module.
DME is called with U =.
1: DME (V ,W ,,U) : 2: locallyMaximal = true 3: for each vV \U do 4: if W (U {v}) then 5: locallyMaximal = false 6: if U {v} is child of U then 7: DME (V ,W ,,U {v}) 8: end if 9: end if 10: end for 11: if locallyMaximal then 12: output U 13: end if 2.2 Integration of additional constraints The DME framework makes it easy to incorporate and systematically exploit constraints from additional data sources.
For illustration, consider the case where we have an additional dataset which provides profiles of proteins or genes across different conditions (Fig.1B).
For simplicity, let us assume binary profiles, being 1 if the protein is positively associated with the corresponding condition, and 0 otherwise.
Then, dense modules where all member proteins share the same profile across a certain number of conditions are of particular interest; we call these modules consistent.
The problem of DME with consistency constraints is formalized as follows.
Definition 4.
Given a graph with node set V and weight matrix W, a density threshold >0, a profile matrix (mij)iV ,jC and non-negative integers n0 and n1, find all modules U V with W (U) s.t.
there exist at least n0 conditions cC with muc =0 uU and there exist at least n1 cC with muc =1 uU.
Given such a consistency constraint, we can stop the module extension during the dense module mining as soon as the constraint is violated.
This is due to the fact that the number of consistent profile conditions cannot increase while extending the module; more generally, this property is called anti-monotonicity (see Supplementary Material).
So we simply add to line 4 of the algorithm a further condition which checks for the consistency requirements.
These are then automatically taken into account in the check for local maximality.
The use of additional constraints can restrict the search space considerably, so it accelerates the computation and helps to focus on biologically interesting solutions.
The described framework can incorporate any kind of anti-monotonic constraints.
Furthermore, one can use arbitrarily many of those constraints at the same time.
Sometimes, one might be interested in incorporating non-anti-monotonic constraints.
While they cannot be directly exploited for pruning, they can be used to filter the obtained modules.
As an example, our software allows to specify a minimum weighted degree threshold t such that degW (u,U)> t for all nodes u of all modules U.
We set t =0 throughout the article.
2.3 Module ranking The exhaustiveness of our DME approach enables us to exactly determine the uncommonness of the discovered substructures with respect to the network at hand.
Let W = (wij)i,jV be the matrix representation of the given network; the total number of nodes is denoted by |V |.
Let U be a module with |U| nodes and density W (U).
Then, the probability that a random selection of |U| nodes in the network produces a module with at least the same density as U is given by {U V : |U |=|U|W (U )W (U)}( |V | |U| ) .
(2) The exact value of the numerator can be obtained as a side product of the DME algorithm.
In the case of additional constraints, it includes only modules that satisfy them.
The modules in the DME output are sorted by their probability values (in ascending order).
This ranking scheme captures the intuition that the rank of a module should increase with its size and density, but from a theoretical point of view it is more principled than the ranking criterion used by Bader and Hogue (2003), which is the product of size and density.
Furthermore, our probability calculation refers specifically to the network at hand, in contrast to measures derived from network models (Koyuturk et al., 2007).
3 EXPERIMENTAL RESULTS 3.1 PPI data For our experiments with yeast (Saccharomyces cerevisiae), we combined protein interactions in PSI-MI format from DIP 935 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 936 933940 E.Georgii et al.
(Xenarios et al., 2000) and MPact (Guldener et al., 2006), which includes data from IntAct (Hermjakob et al., 2004), MINT (Chatr-aryamontri et al., 2007) and BIND (Bader et al., 2003), and interactions from the core datasets of the TAP mass spectrometry experiments by Gavin et al.
(2006) and Krogan et al.
(2006).
For the human analysis, interactions were extracted from the IntAct, MINT, BIND, DIP and HPRD (Peri et al., 2004) databases.1 One main challenge in the analysis of protein interaction networks are false positive edges.
To deal with this, we determined edge weights that indicate the reliability of the corresponding experimental techniques, following the method by Jansen et al.
(2003) (see Supplementary Material for details).
The resulting interaction network for yeast consisted of 3559 nodes with 14 212 non-zero interactions having an average weight of 0.67.
The human network contained 9371 nodes and 32 048 non-zero interactions having an average weight of 0.47.
3.2 Comparative analysis First, we validated the performance of DME on the yeast interaction network in comparison with four other methods: clique detection (Clique, implementation from http://www.cfinder.org.
), the clique percolation method (CPM, implementation from http://www.cfinder.org.)
(Palla et al., 2005), a procedure for joining cliques of a certain size to larger clusters, CPMw (implementation from http://www.cfinder.org.)
(Farkas et al., 2007), an extension of CPM which includes an additional clique filtering step, and Markov clustering (MCL, implementation from http://micans.org/mcl.)
(van Dongen, 2000), a popular graph clustering method simulating random walks.
As a reference set of confirmed complexes, we used the manually curated protein complexes provided by MIPS (Guldener et al., 2005).
To properly assess methods which can produce overlapping modules, we chose performance measures that are based on protein pairs rather than modules; in that way, we avoid taking the same subset of nodes several times into account even if it occurs in more than one module.
Defining the intersection of pairs from predicted modules and pairs from known complexes as correctly predicted pairs, we calculated precision and recall as follows.
Precision= No of correctly predicted protein pairs No of protein pairs in predicted modules (3) Recall= No of correctly predicted protein pairs No of protein pairs in known complexes (4) To obtain precisionrecall curves, we iteratively calculated the precision and recall values, each time extending the set of considered modules by the next highest-ranking module.
As the other methods do not provide a module ranking and our criterion is only applicable to enumerative approaches, we used the scoring scheme by Bader and Hogue (2003) mentioned in Section 2.3.
In fact, it produced for our DME results almost the same ranking as our criterion; the corresponding precisionrecall curves are virtually equivalent.
For each method, we tested a wide range of parameters (see Supplementary Material) and selected the configuration with the largest area under the precisionrecall curve for Figure 2.
Clique and CPM cannot handle edge weights directly, but they preselect edges according to a minimum weight threshold.
1For all datasets we used the database versions available in May 2007.
Fig.2.
Comparative precisionrecall analysis.
To account for module overlap, the measures are based on protein pairs, see text.
Table 1.
Module statistics of the comparative analysis (see text for details).
DME Clique CPM CPMw MCL No.
of distinct modules 1083 916 19 32 648 Average size of distinct modules 3 4 16 14 3 No.
of raw modules 24 803 1971 19 33 648 Average size of raw modules 10 6 16 14 3 No.
of matched complexes 84 54 9 20 59 Average complex size 5 7 19 14 7 No.
of partially recovered complexes 133 107 20 33 117 No.
of predicted interactions 5970 7066 2756 3935 6108 Area under prec.-rec.
curve (AUC) 0.183 0.166 0.107 0.153 0.148 No.
of enriched distinct modules 112 131 18 32 69 No.
of enriched among top-50 47 44  45 No.
of overlapping proteins 1010 1113 12 38 1 No.
of overlapping interactions 3664 4340 24 114 0 AUC for overlapping interactions 0.152 0.082 0.000 0.001 No.
of recovered complex overlaps 18 16 0 4 0 Running time (s) 2667 6 5 457 4 The average size of the raw modules can be larger than that of the distinct modules because larger modules allow for more variants.
The time measurements were performed on a 2.2 GHz processor.
Overall, DME shows the best prediction performance.
It has high precision with respect to the highest-ranking modules and then shows a sudden drop, which is due to a big module not annotated as a known complex.
Clique detects the same module, but there are some other higher ranked modules, so the drop happens later.
MCL and CPM stay always below DME.
Clique works quite well, however the precision drops quickly for higher recall values because edge weights are not taken into account.
It seems that DME has a clear advantage compared to CPM: by explicitly using the edge weights and tuning the density parameter, it allows for more flexibility than the two-stage procedure of CPM, first selecting edges and subsequently joining together cliques that satisfy an overlap criterion.
While CPMw allows to refine the module search, it still differs significantly from our approach.
As it joins preselected cliques, it does not control directly the density of the produced modules and might also miss some dense modules.
In our analysis, CPMw improved the result obtained by CPM, but is mostly inferior to Clique or DME.
Table 1 summarizes further statistics for the predicted modules.
As DME and Clique produced a large number of very similar modules, we computed for better comparability the number of 936 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 937 933940 Condition-dependent dense module enumeration distinct modules.
For that purpose, we grouped matching modules into clusters; each cluster was represented by its top-ranking module.
To decide whether two modules match each other, we here computed the overlap score proposed by Bader and Hogue (2003), using a stringent cutoff of 0.5.
It is defined as the fraction of overlapping proteins with respect to the size of the first module multiplied by the fraction of overlapping proteins with respect to the size of the second module.
The same criterion was used to determine matches between predicted modules and known complexes.
While DME and Clique discovered a comparable number of distinct modules, the DME modules match many more known complexes.
Among these, we also find small-sized complexes, so the overall average size of retrieved complexes is lower than that for Clique.
In addition, we report the number of complexes from which at least one protein pair was recovered as well as the area under the precisionrecall curve from the pairwise analysis (Fig.2).
In both cases, DME is leading.
Furthermore, we investigated the enrichment of the distinct modules with respect to Gene Ontology (GO) terms.
For that purpose, we applied the TANGO tool (Shamir et al., 2005) using the default setting with P-value threshold 0.05 after correction for multiple testing.
Beside the total number of enriched modules, we also counted the number of enriched modules among the top 50 distinct modules, showing that for each method that produced more than 50 modules, most of the high-ranking modules satisfy the enrichment threshold.
For small modules the enrichment test fails even if they are totally pure.
Finally, we assessed the impact of detecting overlapping modules.
Concerning the number of proteins or protein pairs that appear in more than one module, there is large variation among the different methods.
DME and Clique produced the largest numbers of overlapping proteins and overlapping pairs.
Remarkably, the accuracy of overlapping DME interactions clearly increases with the number of modules in which they occur, whereas this is not true for Clique, as reflected by the difference of their AUC values (see also Fig.4 in the Supplementary Material).
The overall precision of overlapping pairs is 45% for DME and 35% for Clique.
We also analyzed how many overlaps between known complexes were rediscovered by predicted modules.
Formally, we counted the cases of overlapping known complexes C1 and C2 where there existed overlapping modules M1 and M2 such that the following conditions were satisfied: (i) M1 M2 contains at least one element of C1 C2, (ii) M1 \M2 contains at least one element of C1 \C2 and (iii) M2 \M1 contains at least one element of C2 \C1.
Here, the number of recovered overlaps was only slightly higher for DME.
3.3 Phenotype-associated yeast modules An additional feature of DME is the possibility to directly integrate constraints from external data sources.
In this section, we investigated our yeast interaction network in the context of knockout phenotypes in order to identify essential parts of protein complexes.
We took the growth phenotype profiles for knockout mutants in yeast under 21 experimental conditions (Dudley et al., 2005), considering three different phenotypic states: enhanced growth, normal growth, and growth defect.
We applied DME requiring for each module at least one condition consistently associated with growth defect for all members.
In order to get a set of modules covering a large number of proteins, but being at the same time as reliable as possible, we tested density thresholds between 0.95 and 0.80 using decrements of 0.01 Table 2.
Results of DME experiments with constraints Phenotype Conservation Expression (yeast) (yeast) (human) No.
of distinct modules 137 1067 460 Average size of distinct modules 3 3 2 No.
of raw modules 160 1816 736 Average size of raw modules 4 5 3 No.
of matched complexes 14 49 52 Average complex size 4 4 4 No.
of partially recovered complexes 30 103 217 Running time (s) 19 5 8 and selected the one with the largest area under the precisionrecall curve.
The results are summarized in Table 2.
Each of the 13 highest-ranking modules covers a considerable part of the mitochondrial ribosomal large subunit as annotated by MIPS.
In addition, our output list contained one further module that overlaps with the complex.
Figure 3A shows the superposition of these 14 modules.
Mrpl16 and Img2 appear in all, many other proteins in almost all of those modules, so they can be considered as the core of the complex.
Knockout of any of the shown proteins caused growth defects with glycerol as carbon source.
Some module members belong to other MIPS complexes, as depicted by the ellipses.
In particular, there is a strong connection to the small subunit of the mitochondrial ribosome and to the mitochondrial translation complex.
Furthermore, our results suggest that the mitochondrial ribosome is associated with Mhr1, a protein involved in homologous recombination of the mitochondrial genome (Ling et al., 2000).
Some modules that are not related to MIPS complexes nevertheless represent known complexes.
For instance, we exactly recovered the nucleoplasmic THO complex (Hpr1, Mft1, Rlr1, Thp2), which is known to affect transcription elongation and hyper-recombination (Chavez et al., 2000).
Interestingly, all mutants exhibit growth defects under the stress condition of adding ethanol to the medium.
Finally, in Figure 3B we show the highest-ranking module which covers at least 50% of two different MIPS complexes.
The corresponding proteins are associated with growth defects under addition of the aminoglycoside hygromycin B.
The module links the vacuolar assembly complex with the class C Vps complex.
The latter is a specific subgroup of proteins involved in vacuolar protein sorting.
Indeed, it has been shown that this complex associates with Vam6 and Vps41 to trigger nucleotide exchange of a rab GTPase regulating the fusion of vesicles to the vacuole (Wurmser et al., 2000).
3.4 Evolutionary conserved yeast modules Next, we used the evolutionary conservation of proteins as side constraint for DME.
For that purpose, we extracted for all yeast genes orthologs from the InParanoid database (OBrien et al., 2005) with respect to 10 other representative eukaryotic species from Schizosaccharomyces pombe to Arabidopsis thaliana.
More precisely, we created a profile indicating for each S.cerevisiae gene and each other model species whether there exist orthologs with a full inParanoid score in the other model species.
We searched for modules in the yeast interaction network such that all member proteins have orthologs in at least three other species; the density 937 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 938 933940 E.Georgii et al.
Fig.3.
Phenotype-associated yeast modules.
(A).
Superposition of all 14 modules overlapping with the large subunit of the mitochondrial ribosome (node size depends on the number of modules in which the protein occurs).
(B).
Module linking two complexes.
The ellipses mark protein sets belonging to known complexes.
For module visualization we used the Osprey tool (Breitkreutz et al., 2003).
Fig.4.
Yeast complexes matched by DME modules and their overlap with conserved DME modules.
Only complexes with size 5 are shown.
The node size corresponds to the density of the confirmed complex, and the pie chart indicates to which degree the complex is covered by a conserved module.
Nodes are connected if there exist interactions between the corresponding sets of matching modules.
threshold was determined using the same procedure as before (for a summary of the results, see Table 2).
Figure 4 shows an overview of the larger MIPS complexes which were retrieved in our DME results, with or without the conservation constraint.
To define matches between complexes and predicted modules, we used the same criterion as in Section 3.2.
Apparently, we could identify some low-density complexes by discovering their dense core parts, for example the translation elongation factor complex eEF1 and the pre-mRNA 3-end processing factor CFI.
In black, we indicate the percentage of the known complex that is covered by a conserved dense module.
From the total set of 33 recovered complexes shown in the figure, 19 overlap by at least 50% with such a module.
Among them, we find the 20S proteasome and its cap and the translation initiation factor eIF2B complex.
The remaining complexes have rather small overlaps with conserved modules, even though they are quite accurately matched by their unconstrained counterparts.
Our conserved module predictions reveal putative core parts of complexes that are conserved across several species.
As an example, we analyze the SNF1 complex, an essential element of the glucose response pathway consisting of six proteins.
Indeed, while the components Snf1, Snf4 and Sip2 are strongly conserved in all eukaryotes and are covered by a conserved module, Sip1 and the transcription factor Sip4 have no orthologs in other species, and the Gal83 component has orthologs in two species only (Vincent and Carlson, 1999).
Our approach predicted one additional conserved component of the complex, Sak1.
This is biologically meaningful, as it functions as an activating kinase of the SNF1 complex (Elbing et al., 2006).
The unconstrained module contained Sak1 and all SNF1 components except Sip4.
3.5 Tissue-specific modules in the human interaction network Finally, we were interested in tissue-specific modules of the human interaction network.
As side information, we downloaded the gene expression profiles by Su et al.
(2004), containing measurements in 938 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 939 933940 Condition-dependent dense module enumeration Fig.5.
Tissue-specific modules in human.
(A).
The two top-ranking modules, covering the MCM complex.
Known complexes are indicated as solid ellipses, modules as dashed ellipses.
(B).
Top-five modules around the SCF ubiquitin ligase complex, revealing its tissue-specific organization.
Boxes show the tissues of consistent positive expression for the respective module.
Tissues associated with all modules are marked in bold, uniquely appearing tissues in italics.
79 different human tissues and present/absent/marginal calls.
For our purposes, we considered a gene to be expressed in a given tissue only if it was classified as present in both of the duplicated measurements.
In order to find complexes which are present in several, but not all tissues, we applied DME to enumerate all modules that are consistently expressed in at least three tissues and consistently not expressed in at least 10 tissues.
We used again the same procedure for selecting the density parameter and ended up with 460 distinct modules (Table 2).
The two top-ranking modules cover the MCM complex (Fig.5A).
As a reference, we used a manually curated set of human complexes collected by MIPS (Ruepp et al., 2008).
MCM is a hexameric protein complex required for the initiation and regulation of eukaryotic DNA replication.
The DME modules contain two additional proteins, Ssrp1 and Orc6l.
Orc6l is a member of the origin recognition complex (ORC), which plays a central role in replication initiation; in fact, the MCM and ORC complexes form the key components of the pre-replication complex (Lei and Tye, 2001).
This is nicely reflected by the high interaction density as well as the common expression profiles of the proteins: the module is fully expressed in three different types of bone marrow cells and fully non-expressed in 42 tissues like brain, liver and kidney, where cells are differentiated and divide less.
Ssrp1 is a member of the FACT complex, which is involved in chromatin reorganization (Orphanides et al., 1999).
Moreover, our analysis yields some insights about the tissue-specific reorganization of the SCF E3 ubiquitin ligase complex, which marks proteins for degradation.
Figure 5B depicts the five top-ranking modules that cover the complex (beyond those, there were three other modules covering only a single protein of the complex).
One of them contains as an additional component Cand1, a regulatory protein that inhibits the interaction of Cul1 with Skp1 (Zheng et al., 2002).
The four other peripheral proteins are F-box proteins, which serve as substrate recognition particles for the SCF complex.
Interestingly, the corresponding modules show different tissue specificities, indicating that the target proteins of SCF are selected in a tissue-dependent manner.
This finding is in accordance with experimental studies (Cenciarelli et al., 1999; Kipreos and Pagano, 2000; Koepp et al., 2001).
On the one hand, it has been shown that in human cells multiple variants of the SCF complex exist, each one containing a different F-box protein for substrate recognition.
On the other hand, brain and blood cells have been identified as tissues of major expression for some F-box components, and expression variation of F-box components has been observed in several tissues like testis, prostate and placenta.
In our results, all detected module variants are present in natural killer (nk) cells, which play an important role in immune response (Janeway et al., 2005), whereas only a few are present in B-cells and testis; in certain brain regions, for instance medulla oblongata, only the module variant with Fbxw7 is predicted to be active.
As illustrated by this example, DME integrated with gene expression data can be a powerful tool to reveal functional and condition-specific variants of protein complexes.
4 CONCLUSION Our algorithm, DME, extracts all densely connected modules from a given weighted interaction network.
In addition to its completeness guarantee, a strength of the method lies in the possibility of transparent data integration, which is of crucial importance in biological applications.
Due to its generality, we believe that DME is a useful tool in many different systems biology approaches.
Our framework can also solve more general problems arising in the analysis of structured data, like dense subgraph detection in multi-partite graphs (cf.
Everett et al., 2006; Tanay et al., 2004) or in hypergraphs (cf.
Zhao and Zaki, 2005).
Moreover, module finding can assist in network comparison and classification tasks (Chuang et al., 2007).
ACKNOWLEDGEMENTS We are very grateful to G. Rtsch and B. Schlkopf for their support; we thank C.S.
Ong for proofreading the article.
939 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 940 933940 E.Georgii et al.
Funding: Federal Ministry of Education, Science, Research and Technology (NGFN: 01GR0451 to S.D.).
Conflict of Interest: none declared.
Abstract Motivation: Post-sequencing DNA analysis typically consists of read mapping followed by variant calling.
Especially for whole genome sequencing, this computational step is very time-consuming, even when using multithreading on a multi-core machine.
Results: We present Halvade, a framework that enables sequencing pipelines to be executed in par-allel on a multi-node and/or multi-core compute infrastructure in a highly efficient manner.
As an example, a DNA sequencing analysis pipeline for variant calling has been implemented according to the GATK Best Practices recommendations, supporting both whole genome and whole exome sequencing.
Using a 15-node computer cluster with 360 CPU cores in total, Halvade processes the NA12878 dataset (human, 100 bp paired-end reads, 50 coverage) in <3 h with very high parallel efficiency.
Even on a single, multi-core machine, Halvade attains a significant speedup compared with running the individual tools with multithreading.
Availability and implementation: Halvade is written in Java and uses the Hadoop MapReduce 2.0 API.
It supports a wide range of distributions of Hadoop, including Cloudera and Amazon EMR.
Its source is available at http://bioinformatics.intec.ugent.be/halvade under GPL license.
Contact: jan.fostier@intec.ugent.be Supplementary information: Supplementary data are available at Bioinformatics online.
1 Introduction The speed of DNA sequencing has increased considerably with the introduction of next-generation sequencing platforms.
For example, modern Illumina systems can generate several hundreds of gigabases per run (Zhang et al., 2011) with a high accuracy.
This, in turn, gives rise to several hundreds of gigabytes of raw sequence data to be processed.
Post-sequencing DNA analysis typically consists of two major phases: (i) alignment of reads to a reference genome and (ii) variant calling, i.e.
the identification of differences between the reference genome and the genome from which the reads were sequenced.
For both tasks, numerous tools have been described in literature, see e.g.
Fonseca et al.
(2012) and Nielsen et al.
(2011) for an over-view.
Especially for whole genome sequencing, applying such tools is a computational bottleneck.
To illustrate this, we consider the re-cently proposed Best Practices pipeline for DNA sequencing analysis (Van der Auwera et al., 2013) that consists of the Burrow-Wheeler Aligner (BWA) (Li and Durbin, 2009) for the alignment step, Picard (http://picard.sourceforge.net) for data preparation and the Genome Analysis Toolkit (GATK) (Depristo et al., 2011; McKenna et al., 2010) for variant calling.
On a single node, the execution of this pipeline consumes more time than the sequencing step itself: a data-set consisting of 1.5 billion paired-end reads (Illumina Platinum genomes, NA12878, 100 bp, 50-fold coverage, human genome) VC The Author 2015.
Published by Oxford University Press.
2482 This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Bioinformatics, 31(15), 2015, 24822488 doi: 10.1093/bioinformatics/btv179 Advance Access Publication Date: 26 March 2015 Original Paper ( 1 ( 2 , ( ( ( ( ( ( ; Depristo etal., 2011 (requires over 12 days using a single CPU core of a 24-core machine (dual socket Intel Xeon E5-2695 v2 @ 2.40 GHz): 172 h for the alignment phase, 35 h for data preparation (Picard steps) and 80 h for GATK, including local read realignment, base quality score reca-libration and variant calling.
When allowing the involved tools to run multithreaded on the same machine, the runtime decreases only by a factor of roughly 2.5 to 5 days, indicative of a poor scaling behavior in some of the steps of the pipeline.
To overcome this bottleneck, we developed Halvade, a modular framework that enables sequencing pipelines to be executed in par-allel on a multi-node and/or multi-core compute infrastructure.
It is based on the simple observation that read mapping is parallel by read, i.e.
the alignment of a certain read is independent of the align-ment of another read.
Similarly, variant calling is conceptually par-allel by chromosomal region, e.g.
variant calling in a certain chromosomal region is independent of variant calling in a different region.
Therefore, multiple instances of a tool can be run in parallel on a subset of the data.
Halvade relies on the MapReduce program-ming model (Dean and Ghemawat, 2008) to execute tasks concur-rently, both within and across compute nodes.
The map phase corresponds to the read mapping step while variant calling is per-formed during the reduce phase.
In between both phases, aligned reads are sorted in parallel according to genomic position.
By mak-ing use of the aggregated compute power of multiple machines, Halvade is able to strongly reduce the runtime for post-sequencing analysis.
A key feature of Halvade is that it achieves very high paral-lel efficiency which means that computational resources are effi-ciently used to reduce runtime.
Even on a single, multi-core machine, the runtime can be reduced significantly as it is often more efficient to run multiple instances of a tool, each instance with a lim-ited number of threads, compared with running only a single in-stance of that tool with many threads.
As an example, both whole genome and whole exome variant calling pipelines were imple-mented in Halvade according to the GATK Best Practices recom-mendations (i.e.
using BWA, Picard and GATK).
The MapReduce programming model has been used before in CloudBurst (Schatz, 2009) and DistMap (Pandey and Schlotterer, 2013) to accelerate the read mapping process and in Crossbow (Langmead et al., 2009a) to accelerate a variant calling pipeline based on modified versions of Bowtie (Langmead et al., 2009b) and SOAPsnp (Li et al., 2008).
The Halvade framework extends these ideas, enabling the implementation of complex pipelines while sup-porting different tools and versions.
The software is designed to achieve a good load balance, maximize data locality and minimize disk I/O by avoiding file format conversions.
As a result, Halvade achieves much higher parallel efficiencies compared with similar tools.
More recently, MapReduce-like scripts were used in MegaSeq (Puckelwartz et al., 2014), a workflow for concurrent multiple gen-ome analysis on Beagle, a Cray XE6 supercomputer at Argonne National Laboratories.
Like Halvade, MegaSeq implements a whole genome analysis pipeline based on the GATK Best Practices recom-mendations.
However, whereas MegaSeq focuses on a high through-put of many genomes using a specific, extreme-scale compute platform, Halvade aims to maximally reduce the analysis runtime for the processing of a single genome, while supporting a wide var-iety of computer clusters.
This approach is particularly of use in a clinical setting, where the analysis step will typically be performed on a local cluster within a hospital environment, and where the time between obtaining a DNA sample from a patient and diagnosing should be kept as small as possible.
The source code of Halvade is publicly available.
2 Methods 2.1 Halvade framework Halvade relies on the MapReduce programming model (Dean and Ghemawat, 2008) to enable parallel, distributed-memory computa-tions.
This model consists of two major phases: the map and reduce phase.
During the map phase, different map tasks are executed in parallel, each task independently processing a chunk of the input data and producing as output a number of intermediate <key, val-ue> pairs.
Next, the intermediate <key, value> pairs emitted by all map tasks are sorted, in parallel, according to key by the MapReduce framework.
During the reduce phase, different reduce tasks are executed in parallel, each reduce task independently pro-cessing a single key and its corresponding values.
Conceptually, a read alignment and variant calling pipeline can be cast into the MapReduce framework: read alignment is then per-formed in the map phase where the different map tasks are processing part of the input FASTQ files in parallel, while the variant calling and, if required, additional data preparation steps, are handled in the re-duce phase where the different reduce tasks are processing chromo-somal regions in parallel.
Using the MapReduce framework, the reads are sorted according to their aligned position and grouped by chromo-somal region in between the two phases.
The Halvade framework provides access to data streams for individual tools that run in parallel during read mapping and variant calling.
An overview of the Halvade framework is depicted in Figure 1.
The different computational steps are described in more detail below.
2.1.1 Input data preparation The input data typically consist of paired-end reads stored in two distinct, compressed FASTQ files.
We provide a separate tool called Halvade Uploader which interleaves the paired-end reads, storing paired reads next to each other, and splits the data in chunks of 60 MB of compressed data.
These chunks are transferred on-the-fly to the file system from which they can be accessed by the worker nodes.
In case a generic Hadoop system is used, this is the Hadoop Distributed File System (HDFS); in case Amazon EMR is used, chunks are uploaded to the Amazon Simple Storage Service (Amazon S3) using the Amazon S3 API.
The number of input chunks corresponds to the number of map tasks that will be executed during the map phase.
The rationale behind the Halvade Uploader is that data have to be copied or uploaded onto the compute infrastructure anyhow, and that decompressing, interleaving, splitting and again compressing can easily overlap with this transfer, thus reducing file I/O to its minimum.
The interleaving of paired-end reads ensures that both pairs are accessible in a single task, which is required for the read alignment.
The Halvade Uploader is multithreaded and op-erates on data streams, which means that its execution can overlap with the data generation (i.e.
sequencing) step itself.
Prior to the actual execution of the MapReduce job, additional preparatory steps are required.
First, the reference genome is parti-tioned into a pre-determined number of non-overlapping chromo-somal regions of roughly equal size.
The number of chromosomal regions corresponds to the total number of reduce tasks that will be executed during the reduce phase and can be configured by the user based on the size of the reference genome in question.
Next, Halvade ensures that all required binaries and configuration files are available on each worker node.
It does so by adding all required files, in a compressed archive, to the distributed cache which is then copied to each worker node and again decompressed.
Note that when these files are persistently stored onto the worker nodes, this preparatory step can be omitted.
Scalable sequence analysis with MapReduce 2483 ( ours ours ( ours &sim; , , ( to  ( , ( ( ( ( to ( ( ( ( , 2.1.2 Map phase read alignment For the map phase, one map task is created per input FASTQ chunk.
These tasks are in turn executed in parallel on the worker nodes by a number of mappers.
Typically, the number of map tasks the num-ber of mappers which means that each mapper will process many tasks.
Key to the MapReduce model is that a map task will prefer-ably be executed by a worker node that contains the input chunk lo-cally on disk (as a part of the HDFS) in order to minimize remote file access and thus network communication.
Each mapper first checks if the indexed reference genome is locally available and re-trieves it from HDFS or Amazon S3 when this is not the case.
At this point, the reference genome itself is not partitioned, i.e.
the complete index is used by each individual mapper.
The input FASTQ chunks are read from HDFS or S3 and parsed into input <key, value> pairs using the Hadoop-BAM (Niemenmaa et al., 2012) API.
The values of these pairs contain the FASTQ entries and are streamed to an in-stance of the alignment tool.
Halvade requires that the alignments are accessible in SAM for-mat and provides a SAM stream parser that will create the required intermediate <key, value> pairs.
This intermediate key represents a composite object that contains the chromosome number and align-ment position of a read, along with the identifier of the chromo-somal region to which it aligns.
The value contains the corresponding SAM record, i.e.
the read itself and all metadata.
Reads that cannot be aligned are optionally discarded.
For individ-ual or paired-end reads that span the boundary of adjacent chromo-somal regions, two intermediate <key, value> pairs are created, one for each chromosomal region.
This redundancy ensures that all required data for the data preparation and variant calling is avail-able for each reduce task.
After all map tasks are completed, the MapReduce framework sorts, in parallel, the intermediate pairs according to chromosomal region (as part of the key).
This way, all reads that align to the same chromosomal region are grouped together thus forming the input of a single reduce task.
Halvade uses secondary sorting to further sort the SAM records for each chromosomal region by genomic position.
Both grouping and sorting effectively replace the sorting of SAM records typically performed by tools such as Picard or SAMtools (Li et al.
2009) and are performed in a highly efficient manner by the MapReduce framework.
2.1.3 Reduce phasevariant calling When all data have been grouped and sorted, the different reduce tasks are executed in parallel by different reducers.
Again, the num-ber of reduce tasks the number of reducers.
Before the reads are processed, Halvade can copy to each worker node additional files or databases that are required for variant calling.
A specific task takes as input all (sorted) intermediate <key, value> pairs for a single chromosomal region and converts it to an input stream in SAM for-mat.
Halvade iterates over the SAM records and creates a BED file (Quinlan and Hall, 2010) containing position intervals that cover all SAM records in the chromosomal region.
This file can optionally be used to specify relevant intervals on which tools need to operate.
Finally, instances of these tools are created in order to perform the actual variant calling.
Typically, at the end of each reduce task, a Variant Call Format (VCF) file has been produced which contains all variants identified in the corresponding chromosomal region.
Halvade provides the op-tion to merge these VCF files using an additional MapReduce job.
In this second job, the map phase uses the individual VCF files as input and the variants are parsed as <key, value> pairs using Hadoop-BAM.
The key contains the chromosome identifier and the position of the variant, while the value contains all other meta-information.
These values are collected in a single reduce task, which writes the aggregated output to either HDFS or Amazon S3.
At the interface of adjacent chromosomal regions, it is possible that overlapping vari-ants are called twice by GATK (once in each chromosomal region).
This is due to SAM records that span this interface and that were thus sent to both regions.
During the VCF merging step, Halvade as-signs a unique name to each of the overlapping variants, thus keep-ing all of them, or, optionally, retains only the variant with the highest Phred-scaled quality score.
Such overlapping variants are rarely observed.
Note that this second MapReduce job is very light-weight.
Fig.1.
Overview of the Halvade framework.
The entries of pairs of input FASTQ files (containing paired-end reads) are interleaved and stored as smaller chunks.
Map tasks are executed in parallel, each task taking a single chunk as input and aligning the reads to a reference genome using an existing tool.
The map tasks emit <key, value> pairs where the key contains positional information of an aligned read and the value corresponds to a SAM record.
The aligned reads are grouped and sorted per chromosomal region.
Chromosomal regions are processed in parallel in the reduce phase, this includes data preparation and variant de-tection again using tools of choice.
Each reduce task outputs the variants of the region it processed.
These variants can optionally be merged into a single VCF file.
Note that the names of the tools shown correspond to those of the GATK Best Practices DNA-seq implementation in Halvade 2484 D.Decap et al.
( , ( ( ( ( 2.2 Best practices DNA-seq implementation In Halvade, a DNA-seq variant calling pipeline has been imple-mented according to the Best Practices recommendations by Van der Auwera et al.
(2013).
Table 1 lists the different steps involved.
During the map phase, read alignment is performed by BWA; both BWA-mem and BWA-aln with BWA-sampe are supported in our implementation.
In case BWA-aln is used, paired-end reads are again separated and aligned individually by two instances of BWA-aln after which BWA-sampe is used to join these partial results.
The standard output stream of either BWA-sampe or BWA-mem is cap-tured, and its SAM records are parsed into intermediate <key, val-ue> pairs.
In the reduce phase, the SAM stream is first prepared according to GATKs requirements, i.e.
the readgroup information is added, read duplicates (i.e.
reads that are sequenced from the same DNA molecule) are marked and the data is converted to the binary, com-pressed BAM format.
Note that in the Best Practices recommenda-tions, readgroup information is added during read alignment.
In Halvade, this is postponed to the reduce phase in order to avoid sending this extra meta-information (as part of the SAM record) over the network during sorting.
For data preprocessing, Halvade can use either Picard or elPrep (http://github.com/exascience/elprep) with SAMtools (Li et al., 2009).
ElPrep is a tool that combines all data preparation steps and outputs a SAM file that conforms to the GATK requirements.
When using elPrep, the input SAM records are streamed directly to elPrep for marking duplicate reads and adding of readgroup information.
Its resulting SAM file is then converted to BAM format using SAMtools.
When using Picard, Halvade first writes the input SAM stream to local disk in a compressed BAM file and then invokes the Picard MarkDuplicates and AddReadGroups modules.
Note that both options (elPrep/SAMtools or Picard) pro-duce identical output.
However, the combination of elPrep and SAMtools is considerably faster than Picard.
Next, the actual GATK modules are executed.
To correct poten-tially misaligned bases in reads due to the presence of insertions or deletions, the RealignerTargetCreator module is used to identify intervals that require realignment followed by the IndelRealigner module to perform the actual realignment.
Next, using the dbSNP database of known variants (Sherry et al., 2001), the BaseRecalibrator module is used to generate co-variation data tables that are then used by the PrintReads module to recalibrate the base quality scores of the aligned reads.
Finally, the actual variant calling is done using either the HaplotypeCaller or UnifiedGenotyper module.
The DNA-seq analysis pipeline implementation in Halvade sup-ports both whole genome and exome sequencing analysis.
One important difference to note is that an additional BED file is required, containing the coordinates of the exome regions to be pro-cessed by GATK.
Additionally, the dbSNP database file used for the base quality score recalibration must be compatible with the exome being targeted.
2.3 Optimizations In order to get the best performance out of the available resources, two crucial factors come into play.
First, one needs to determine the optimal number of mappers and reducers per node.
This determines the number of map and reduces tasks that will be executed concur-rently on a worker node.
Second, the user needs to select appropriate map and reduce task sizes.
This determines the total number of map and reduce tasks that will be executed.
Both factors are described below.
To exploit parallelism in a workstation with one or more multi-core CPUs, one can either run a single instance of a tool with multi-threading on all available CPU cores, or run multiple instances of that tool, each instance using only a fraction of the CPU cores.
To il-lustrate the difference in performance, the parallel speedup for dif-ferent GATK modules as a function of number of threads was benchmarked on a 16-core machine (dual socket Intel Xeon CPU E5-2670 @ 2.60 GHz) with 94 GB of RAM (see Fig.2).
The bench-marks show that the maximum speedup gained by any of the GATK modules, using 16 threads, is <10, with one module exhibiting no speedup at all.
Most modules show good scaling behavior up to 4 or 8 threads, but show only moderate reduction in runtime when the number of threads is increased further.
It is thus more beneficial to start multiple instances of GATK, each instance using only a limited number of threads.
Note that this is also true for Picard (which is single-threaded) and to a lesser extent, also for BWA.
This concept is used in Halvade, which leads to better use of available re-sources and a higher overall parallel efficiency (see Supplementary Data S1.1).
On the other hand, the maximum number of parallel in-stances of a tool than can be run on a machine might be limited due to memory constraints.
A second important factor is the optimal task size, which in turn determines the total number of tasks.
For the map phase, the task size is determined by the size of a FASTQ chunk.
Very few, large chunks will lead to a high per-task runtime but an unevenly Table 1.
Overview of the steps and tools involved in the DNA-sequencing pipeline according to the GATK Best Practices recom-mendations described by Van der Auwera et al.
(2013) step program input output align reads BWA FASTQ SAM convert SAM to BAM Picard SAM BAM sort reads Picard BAM BAM mark duplicates Picard BAM BAM identify realignment intervals GATK BAM Intervals realign intervals GATK BAM and intervals BAM build BQSR table GATK BAM table recalibrate base quality scores GATK BAM and table BAM call variants GATK BAM VCF 0 4 8 12 16 0 4 8 12 16 P ar al le l s pe ed up Number of threads RealignerTargerCreator (-nt) UnifiedGenotyper (-nt) BaseRecalibrator (-nct) PrintReads (-nct) HaplotypeCaller (-nct) Ideal speedup Fig.2.
The parallel speedup (multithreading) of five GATK modules used in the Best Practices pipeline on a 16-core node with 94 GB of RAM.
The limited speedup prevents the efficient use of this node with more than a handful of CPU cores.
Option-nt denotes data threads while option-nct denotes CPU threads (cfr.
GATK manual) Scalable sequence analysis with MapReduce 2485 ( yte ( less than  ( (balanced workload, whereas many little files will result in a large task scheduling and tool initialization overhead.
After extensive test-ing, we determined that a file size of 60 MB leads to the lowest runtime (see Supplementary Data S1.2).
Such chunk size is suffi-ciently big to define a meaningful task size, and small enough for a chunk to fit into a single HDFS block (default64 MB) which is en-tirely stored on a single worker node.
If that worker node processes the corresponding map task, network traffic is avoided.
Similarly, for the reduce phase, the task size is determined by the size of the chromosomal regions.
For data preparation, Picard can be replaced by elPrep.
This tool combines all data preparation steps needed in the Best Practices pipeline.
Whereas Picard requires file I/O for every preparation step, elPrep avoids file I/O by running entirely in memory and merges the computation of all steps in a single pass over the data.
Using elPrep for data preparation again gives a significant speedup for this phase in Halvade.
3 Results Halvade was benchmarked on a whole genome human dataset (NA12878 from Illumina Platinum Genomes).
The dataset consists of 1.5 billion 100 bp paired-end reads (50-fold coverage) stored in two 43 GB compressed (gzip) FASTQ files.
The software was bench-marked on two distinct computer clusters, an Intel-provided big-data cluster located in Swindon, UK and a commercial Amazon EMR cluster.
Table 2 provides an overview of the runtime on these clusters.
For these benchmarks, GATK version 3.1.1, BWA (-aln and-sampe) version 0.7.5a, BEDTools version 2.17.0, SAMtools version 0.1.19 and Picard version 1.112 were used.
The dbSNP database and human genome reference found in the GATK hg19 resource bundle (ftp://ftp.broadinstitute.org/bundle/2.8/hg19/) were used.
The reference genome was stripped of all alternate allele informa-tion and contains chromosomes 1 through 22, M, X and Y.
3.1 Intel big data cluster benchmark This cluster consists of 15 worker nodes, each containing 24 CPU cores (dual-socket Intel Xeon CPU E5-2695 v2 @ 2.40 GHz) and 62 GB of RAM.
The nodes each dispose of four hard drives with a total capacity of 4 TB, intended as HDFS storage and a single 800 GB solid-state drive (SSD) that is intended for local storage dur-ing MapReduce jobs.
The nodes are interconnected by a 10 Gbit/s Ethernet network.
Cloudera 5.0.1 b which supports MapReduce 2.3 was used.
Initially, the input FASTQ files were present on a local disk of a single node.
Using the Halvade Uploader, both files were decompressed, interleaved, compressed into separate files of 60 MB each (1552 chunks in total) and copied onto the local HDFS storage.
This pre-processing step required 1.5 h using eight threads and can, in principle, overlap with the generation of the se-quence data itself.
For the chromosomal regions, a size of 2.5 Mbp was used, corresponding to 1261 reduce tasks in total.
On this cluster, Halvade runs four mappers/reducers per node in parallel to achieve optimal performance (see Supplementary Data S1.3), each mapper/reducer having 6 CPU cores and 15.5 GB of memory at its disposal.
The scalability of Halvade was assessed by running the analysis pipeline with an increasing number of 115 nodes.
As Cloudera reserves one slot for scheduling and job manage-ment, this corresponds to running 3 parallel tasks (1 node) to 59 parallel tasks (15 nodes) in total.
Figure 3 depicts the parallel speedup as a function of the number of parallel tasks and provides an accurate view of the scalability and efficiency.
When using 59 tasks (15 nodes), we observe a speedup of a factor 18.11 compared with using 3 tasks (1 node).
This corresponds to a parallel efficiency of 92.1%.
In absolute terms, the runtime reduces from 48 h (single node) to 2 h 39 min.
It is important to note that Halvade already attains a significant speedup when applied to a single node (3 tasks and 18 CPU cores), compared with the scenario of running the multithreaded versions of the individual tools using all 24 CPU cores.
Indeed, whereas Halvade requires 48 h on a single node, 120 h are required when Halvade is not applied (speedup of a factor 2.5).
This is due to the limited multithreaded scaling behavior of certain tools or modules (see Methods section).
It is hence far more efficient to run multiple instances of e.g.
GATK with a limited number of threads per in-stance than letting GATK make use of all available cores.
Ultimately, Halvade achieves a 45-fold speedup when applied to 15 nodes (2 h 39 min) compared with running the pipeline on a sin-gle node using only multithreading (120 h).
3.2 Amazon EMR benchmark Amazon Elastic Compute Cloud (Amazon EC2) provides, as a web service, a resizeable compute cluster in the cloud.
MapReduce can be used on this compute cluster with a service called Amazon EMR.
This provides access to a Hadoop MapReduce cluster which can be chosen according to the requirements of the task at hand, e.g.
the number of nodes and the node type.
The EMR cluster was initialized Table 2.
Runtime as a function of the number of parallel tasks (mappers/reducers) on the Intel Big Data cluster and Amazon EMR Cluster No.
worker nodes No.
parallel tasks No.
CPU cores Runtime Intel Big Data cluster 1 3 18 47 h 59 min 4 15 90 9 h 54 min 8 31 186 4 h 50 min 15 59 354 2 h 39 min Amazon EMR 1 4 32 38 h 38 min 2 8 64 20 h 19 min 4 16 128 10 h 20 min 8 32 256 5 h 13 min 16 64 512 2 h 44 min The time for uploading data to S3 over the internet is not included in the runtimes for Amazon EMR.
0 8 16 24 32 40 48 56 64 0 8 16 24 32 40 48 56 64 0 20 40 60 80 100 S pe ed up P ar al le l e ffi ci en cy # Tasks Halvade Ideal speedup Exome speedup; Amazon Exome efficiency; Amazon Whole genome speedup; Amazon Whole genome efficiency; Amazon Whole genome speedup; Intel cluster Whole genome efficiency; Intel cluster Fig.3.
The speedup (primary y-axis) and parallel efficiency (secondary y-axis) of Halvade as a function of number of parallel tasks (cluster size) on both an Intel Big Data cluster and Amazon EMR 2486 D.Decap et al.
about yte ( ( yte ( ( yte  ( .
.
( ( ftp://ftp.broadinstitute.org/bundle/2.8/hg19/ ( yte yte yte  ( about yte ( &sim; ours 8  (yte  to  ( ( ( to  ( , &sim; ours ( ours ute ( to &sim; ours &sim; ours  ( ( &sim; ( ours utes to  ( ours ( , with the Amazon Machine Images v3.1.0, providing access to Hadoop MapReduce v2.4, which takes 5 min.
One node is reserved for job scheduling and management while the remainder is used as worker nodes.
Using the Halvade Uploader, the input data were preprocessed and uploaded to S3, the Amazon cloud storage system, again as 1552 chunks.
The uploading speed is highly dependent on internet network conditions, which varies greatly.
Again, this step can over-lap with the generation of the sequence data itself.
As data stored on S3 is directly accessible by worker nodes, one can chose whether or not to copy these data to HDFS prior to starting the MapReduce job.
According to Deyhim (2013), data can be copied between S3 and HDFS at a rate of 50 GB per 19 min.
However, for these bench-marks, data was read directly from S3, as copying would increase the overall runtime considerably.
For this benchmark, worker nodes of the type c3.8large were used.
Each node provides 32 CPU cores (Intel Xeon E5-2680 v2 @ 2.80 GHz), 60 GB of RAM and two 320 GB SSDs which are avail-able for both HDFS and intermediate data.
To obtain optimal per-formance, Halvade again assigned four parallel tasks per node, with each task disposing of 8 CPU cores and 15 GB of memory.
The scal-ability was assessed by running Halvade with an increasing number of 116 worker nodes.
When using 64 tasks (16 nodes), a speedup of a factor 14.16 is achieved compared with using 4 tasks (1 node) (see Fig.3).
This corresponds to a parallel efficiency of 88.5%.
In absolute terms, the total runtime is reduced from 38 h 38 min (4 tasks) to 2 h 44 min (64 tasks) (see Table 2).
The runtime on Amazon EMR is slightly higher than that ob-tained using the Intel Big Data cluster, even though a higher number of CPU cores was used.
This is because the Intel Big Data cluster is configured with a persistent HDFS, whereas for the Amazon cluster, the HDFS is created on-the-fly when the MapReduce job starts.
On the Intel Big Data cluster, we can therefore instruct the Halvade Uploader to copy data directly to the HDFS after which only limited network communication is required for map tasks to access the data.
On Amazon, Halvade accesses the data straight from S3, which requires network communication and explains the increased runtime.
With a total runtime of <3 h, the financial cost of a whole genome analysis on Amazon EMR with 16 worker nodes amounts to 111.28 US dollar (based on the pricing of May 2014), which is in a similar price range as running the pipeline on a single, smaller Amazon instance without the use of Halvade (based on an expected runtime of 5 days on a c3.4large instance).
3.3 Exome sequencing analysis benchmark To assess the performance of Halvade on an exome sequencing data-set (Illumina HiSeq NA12878), the same Amazon EMR cluster was used.
The dataset consists of 168 million 100 bp paired-end reads stored in eight 1.6 GB compressed (gzip) FASTQ files.
Using an Amazon EMR cluster with eight worker nodes (32 par-allel tasks), Halvade can call the variants in under 1 h for a total cost of 19.64 US dollar (based on the pricing of May 2014).
As the input for exome analysis is considerably smaller, the load balancing is more challenging as there are only 225 map tasks and 469 reduce tasks in total.
A high parallel efficiency of 90% is obtained when using 8 worker nodes; the efficiency drops to 70% when using 16 worker nodes (see Fig.3).
4 Discussion and conclusion Especially for whole genome sequencing, the post-sequencing ana-lysis (runtime of 12 days, single-threaded) is more time-consuming than the actual sequencing (several hours to a few days).
Individual tools for mapping and variant calling are maturing and pipeline guidelines such as the GATK Best Practices recommendations are provided by their authors.
However, existing tools currently have no support for multi-node execution.
Even though some of them sup-port multithreading, the parallel speedup that can be attained might be limited, especially when the number of CPU cores is high.
As whole genome analysis is increasingly gaining attention, the need for a solution is apparent.
Halvade provides a parallel, multi-node framework for read alignment and variant calling that relies on the MapReduce pro-gramming model.
Read alignment is then performed during the map phase, while variant calling is handled in the reduce phase.
A variant calling pipeline based on the GATK Best Practices recommendations (BWA, Picard and GATK) has been implemented in Halvade and shown to significantly reduce the runtime.
On both a Cloudera clus-ter (15 worker nodes, 360 CPU cores) and a commercial Amazon EMR cluster (16 worker nodes, 512 CPU cores), Halvade is able to process a 50-fold coverage whole genome dataset in under 3 h, with a parallel efficiency of 92.1 and 88.5%, respectively.
To the best of our knowledge, these are the highest efficiencies reported to date (see Supplementary Data S1.4 for a comparison with Crossbow).
Like Halvade, MegaSeq supports a pipeline which is based on the GATK Best Practices recommendations.
As the source code of MegaSeq is not publicly available, a direct comparison to Halvade is difficult.
Puckelwartz et al.
(2014) estimate, based on benchmarks on 61 whole genomes, that 240 whole genomes could be processed in 50.3 h, using a supercomputer with 17 424 CPU cores (AMD Magny-Cours @ 2.1 GHz).
When rescaling this to 360 CPU cores @ 2.4 GHz, an average runtime of 8.9 h is obtained for a single gen-ome.
Halvade processes such dataset in 2 h and 39 min and thus pro-vides for a more cost-effective way to minimize runtime.
It should be noted that (i) MegaSeq used the GATK HaplotypeCaller whereas Halvade relied on the (much) faster UnifiedGenotyper during bench-marking, (ii) additional variant annotation was performed in MegaSeq, (iii) a comparison based on the number of CPU cores and clock frequency alone has its limitations as also disk speed, available RAM, network speed and other hardware aspects may play a role.
To enable multi-node parallelization of sequencing pipelines, Halvade provides and manages parallel data streams to multiple in-stances of existing tools that run on the different nodes.
No modifi-cations to these tools are required; they can thus easily be replaced by newer versions.
The same holds for replacing the current tools by alternatives: if input/output formats are not affected, the interchange of tools is straightforward.
For the map phase (read alignment), the use of tools other than the natively supported BWA(-aln and-mem) should be possible with minimal effort, provided that they output SAM records either to disk or standard output.
For the reduce phase (variant calling), it is more invasive to make modifications as the steps involved and the (intermediate) input and output formats are not standardized across tools.
Making major changes to this analysis step will require modification to the source code (Java) of Halvade.
However, Halvade provides all functionality to partition the refer-ence genome in chunks and provides functionality to copy external dependencies (files or databases) to the worker nodes.
This should greatly facilitate the implementation of variant calling pipelines using other tools.
To achieve optimal performance on computer clusters with multi-core nodes, Halvade can be configured to run multiple parallel instances of a tool per node, each instance using a limited number of threads.
This approach significantly increases the per-node perform-ance as the multithreaded scaling behavior of certain tools is limited.
Scalable sequence analysis with MapReduce 2487 ( AMI) &sim; utes yte utes x ( yte yte yte to ( to  ( ( ours utes ( ours utes ( ( less than ours ( ( about x ( yte ( 8  ( one our ( about about  ( C ( &sim; ( ( ( ( ours &percnt; ( ( ours ours utes ( a ( ( b ( c ( ( ( ( ( Indeed, on a single 24-core node with three parallel tasks, Halvade already attains a speedup of 2.5 compared with a multithreaded exe-cution of the same tools in a single task.
A second key factor in at-taining a high efficiency is the choice of appropriate task sizes.
Few, large tasks might result in an unevenly balanced load whereas lots of small tasks result in scheduling and tool initialization overhead.
In Halvade, it is assumed that read alignment is parallel by read and that variant calling is parallel by chromosomal region.
Certain tools, however, produce slightly different results when they operate on only part of the data.
We have analyzed these sources of vari-ation in detail (see Supplementary Data S1.5).
As for the accuracy of whole genome analysis, the variants found by Halvade match >99% with variants in the validation set created by a sequential run of the GATK Best Practices pipeline.
Additionally, almost all the 1% different variants have a very low variant score.
In the implementation of the GATK Best Practices pipeline, the lat-est versions of BWA, Picard and GATK are supported.
Both whole genome and exome analysis are supported.
An RNA-seq variant call-ing pipeline is currently in development.
Halvade is built with the Hadoop MapReduce 2.0 API and thus supports all distributions of Hadoop, including Amazon EMR and Cloudera, the latter which can be installed on a local cluster.
The Halvade source code is available at http://bioinformatics.intec.ugent.be/halvade under GPL license.
Funding This work is funded by Intel, Janssen Pharmaceutica and by the Institute of the Promotion of Innovation through Science and Technology in Flanders (IWT).
Some benchmarks were run at the Intel Big Data Lab, Swindon, UK.
We acknowledge the support of Ghent University (Multidisciplinary Research Partnership Bioinformatics: From Nucleotides to Networks).
Conflict of Interest: none declared.
ABSTRACT Motivation: Programs that evaluate the quality of a protein structural model are important both for validating the structure determination procedure and for guiding the model-building process.
Such programs are based on properties of native structures that are generally not expected for faulty models.
One such property, which is rarely used for automatic structure quality assessment, is the tendency for conserved residues to be located at the structural core and for variable residues to be located at the surface.
Results: We present ConQuass, a novel quality assessment program based on the consistency between the model structure and the proteins conservation pattern.
We show that it can identify problematic structural models, and that the scores it assigns to the server models in CASP8 correlate with the similarity of the models to the native structure.
We also show that when the conservation information is reliable, the methods performance is comparable and complementary to that of the other single-structure quality assessment methods that participated in CASP8 and that do not use additional structural information from homologs.
Availability: A perl implementation of the method, as well as the various perl and R scripts used for the analysis are available atContact: nirb@tauex.tau.ac.il Supplementary information: Supplementary data are available at Bioinformatics online.
Received on December 1, 2009; revised on March 9, 2010; accepted on March 13, 2010 1 INTRODUCTION The function of a protein is largely determined by its 3D structure.
Therefore, the determination of a proteins structure is an important step in understanding how the protein achieves its function, and it can also aid in predicting protein function or designing experiments.
However, experimental structure determination can be a long and difficult procedure, and naturally errors may occur (Kleywegt, 2009).
This was recently demonstrated when several protein structures published in the Protein Data Bank (PDB) were discovered to be erroneous (Chang et al., 2006).
Even when the structure determination process is correct, the determined structure may adopt a non-physiological fold, for example, due to non-physiological constraints imposed by the crystal in the case of X-ray crystallography.
Such errors can cause confusion and To whom correspondence should be addressed.
mislead further research, so it is important to be able to spot them before the structures are published.
Errors are even more frequent in computationally derived structures, which are built either by extrapolating from a homologous protein whose structure is already solved (Fiser and Sali, 2003; Ginalski, 2006) or by computer simulation (Das and Baker, 2008; Zhang and Skolnick, 2004).
In the latter case, many alternative conformations might be generated during the simulation, and differentiating between erroneous conformations and structures that are more likely to be correct could help guide the simulation and limit the search space.
Programs that try to numerically assess the correctness of a given structural model for a protein are called Model Quality Assessment Programs (MQAPs).
The need for such programs is widely recognized by the structural biology community, as evidenced by the inclusion of a category for assessing MQAP performance in the biennial Critical Assessment of Techniques for Protein Structure Prediction (CASP) experiment, starting from its seventh round (CASP7; Cozzetto et al., 2007).
The two pioneering MQAPs, still widely used today, are Verify3D (Eisenberg et al., 1997) and ProSa (Wiederstein and Sippl, 2007).
Both methods check the compatibility between the proteins structure and its sequence.
Verify3D, for example, classifies each residue in the protein into one of the 18 classes according to the residues structural environment in the input model.
The propensity of each amino acid to exist in each such structural environment class is calculated according to statistics collected from structures in the PDB, and the final score given to the protein structure is the sum of propensities of the individual residues.
Newer MQAPs were recently assessed in the blind experiments of CASP7 (Cozzetto et al., 2007) and CASP8 (Cozzetto et al., 2009).
The models given as input to the MQAPs were the server models, which are generated by the various servers participating in CASP shortly after the round starts, and long before the native structures are published.
Many of the participating MQAPs, including QMEAN (Benkert et al., 2009) and MULTICOM-REFINE (Cheng et al., 2009), functioned similarly to Verify3D and ProSa, receiving only one structure as input and assigning it a quality score based on the compatibility of various features computed from the sequence with the predicted 3D structure.
However, the most successful MQAPs in CASP8 were the consensus-based methods, such as Pcons (Larsson et al., 2009) and ModFOLDclust (McGuffin, 2009), which used as input the entire decoy set instead of just coordinates of a given model and took a consensus approach to rate each model according to how similar it was to the other structures in the set.
This approach, while clearly advantageous in the setting of the CASP experiment, is not applicable in many scenarios in which The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1300 12991307 M.Kalman and N.Ben-Tal few structures (possibly only one) are available, or when the decoy set is not likely to contain many correct models (Cozzetto et al., 2007; Wallner and Elofsson, 2008).
The single-structure MQAPs that performed best in CASP8 were LEE and LEE-server.
The group produced their own structural model for each target and ranked the decoys according to how similar they were to their model (Cozzetto et al., 2007).
The models produced by the LEE group were homology based, so at least part of the success of the method could be attributed to the fact that it used additional constraints from structural models of homologous proteins.
Two other methods from CASP8, SAM-T08-MQAU and SAM-T08-MQAO (Archie et al., 2009), also used such constraints taken from structural homologs, and they also performed significantly better than the rest of the single-structure methods.
While homology-based approaches have proven very promising, they are only usable when reliable structural homologs exist.
Therefore, there is still a need for devising methods that do not use additional structural information, neither from structural homologs nor from the other decoys.
We term such methods pure single-structure MQAPs.
An alternative strategy to that of most single-structure MQAPs is to check the compatibility of the suggested 3D structure with the evolutionary conservation pattern of the sequence.
There are various ways to calculate the conservation level (or evolutionary rate) of an amino acid position (Glaser et al., 2003; Mihalek et al., 2004).
A residue that is conserved throughout evolution has undergone strong purifying selection; this suggests that the conserved residue is important for the proteins normal function (Brndn and Tooze, 1999).
This observation has been used in many applications, such as identifying the active site of a protein, which is usually composed of a patch of clustered residues on the proteins surface (Nimrod et al., 2008).
An interesting observation is that for most proteins, the structural core is composed mainly of such conserved residues (see for example, Fig.1A).
These residues are usually not involved directly in the mechanism of the proteins function.
Rather, they are conserved because a mutation in such a buried residue would tend to perturb the architecture.
The protein surface, in contrast, is mostly variable.
If the association between residue accessibility and conservation level is strong enough, it might be used to differentiate between correct and incorrect model structures, as incorrect structures are unlikely to feature this pattern by chance.
This conservation pattern has been used for computational modeling of proteins, both manually for checking the validity of a built model (Landau et al., 2007) and automatically for generating a C model of transmembrane proteins starting from a low-resolution cryo-EM map (Fleishman et al., 2004a, b, 2006).
Conservation information has also been used for quality assessment in several studies.
The first conservation-based approach is to use the observation that conserved residues tend to be clustered in the native structure (Mihalek et al., 2003; Muppirala and Li, 2006; Schueler-Furman and Baker, 2003).
This clustering is expected both for structurally conserved residues, as they form the structural core, and for functionally conserved residues, which are usually localized on the surface, at the functional site of the protein.
Mihalek et al.
(2003) used the evolutionary trace method to collect a set of conserved residues and quantified the sets tendency to cluster using a measure they termed the selection clustering weight (SCW).
They applied this method to the Decoys RUs decoy set (Samudrala and Levitt, 2000) Fig.1.
Tendency of conserved residues to be buried in correct structures.
(A and C) The native structure for the CASP7 target T0289 (Aspartoacylase, PDB 2gu2A) in (A), and a poor model for the same target (model FPSOLVER-SERVER_TS1, GDT-TS = 7.9) in (C), colored by the ConSurf color map using the ConSurf-DB database.
ConSurf colors 17 are semitransparent to show the cluster of conserved residues buried at the structural core in the native structure.
(B and D) Distribution of relative residue accessibilities as calculated by Naccess for variable residues (cyan; ConSurf classes 1, 2, 3) and for conserved residues (purple; ConSurf classes 8, 9).
The distributions are shown for the native structure in (B) and for the poor model in (D).
(E) Distribution of relative residue accessibilities for all residues of all structures in the dataset, classified by their ConSurf conservation grades.
The different grades are colored according to the ConSurf color scheme.
There is a consistent shift to the right, with the most variable residues (ConSurf class 1) being most accessible.
Molecular graphic images were generated using UCSF Chimera (Pettersen et al., 2004).
and showed that indeed 78.1% of the decoys in the set were assigned a lower (less favorable) SCW score compared with the native structure.
However, the assessment of a method by its ability to rank a native structure higher than decoys has been shown to be problematic (Handl et al., 2009).
Schuler-Furman and Baker (Schueler-Furman and Baker, 2003) took a similar approach, adopting a simpler strategy for selecting the set of conserved residues based on entropy, as well as a different measure for quantifying the clustering.
However, they validated their method in a more relevant scenario, showing that when the method is used to select decoys generated by ROSETTA (Das and Baker, 2008), there is a statistically significant enrichment in correct models.
The second approach to exploit conservation data is to use it initially to make contact predictions and subsequently use the predictions for quality assessment.
Olmea et al.
(1999) provided a set of contact predictions, using the observation that in pairs of 1300 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1301 12991307 Assessment of protein model structures conserved residues, as well in pairs of residues whose mutations are correlated, the members of the pair tend to be spatially close to each other in the 3D structure.
They have shown that such predictions are usually more precise for native structures than for deliberately misfolded ones.
They also used this information as a post-processing step in a threading method and showed that it improved the methods results.
More recently, Miller and Eisenberg (Miller and Eisenberg, 2008) built an MQAP based on the agreement between such contact prediction information and the set of contacts in the proposed model.
They checked their method on several of the CASP7 targets and proved that it performed significantly better than random.
While previous studies have suggested that evolutionary information can be used for quality assessment, the performance of these methods was never compared with that of other MQAPs.
Furthermore, these methods only used the tendency of conserved residues to be spatially close to one other, which captures only partially the information that is present in the conservation accessibility relation.
In this study, we present a new very simple MQAP called ConQuass (conservation-based quality assessment), which is based on the correlation between each residues degree of evolutionary conservation and its accessibility in the structure.
We check the performance of ConQuass on the CASP8 dataset, and show our method to be comparable to the other pure single-structure MQAPs that participated in CASP8.
We also show that ConQuass is complementary to existing methods and could potentially be integrated with them to improve their performance.
2 METHODS 2.1 Collecting a training set of known structures The PISCES server (Wang and Dunbrack, 2003) was used to collect a non-redundant set of X-ray, full-atom protein structures from the PDB that have resolution better than 3.0 , R-factor better than 0.3 and sequence identity <25%.
This resulted in a set of 6132 protein chains.
Of those, we used only the 5648 proteins for which evolutionary conservation information was available in the ConSurf-DB database (Goldenberg et al., 2009).
We generated three structures for each protein chain.
The first contained only the given chain in isolation, without the other chains in the PDB structure.
In addition, two versions of the chain in the context of its biological unit were generated using either the protein quaternary structure (PQS) server (Henrick and Thornton, 1998) or the progressive iterative signature algorithm (PISA) server (Krissinel and Henrick, 2007).
Non-protein chains were removed.
Finally, we removed each protein whose complex contained >26 protein chains and eliminated protein structures that were too big to run in Naccess (Hubbard and Thornton, 1993), leaving a total of 5543 proteins in the final set.
2.2 Features collected for each structure 2.2.1 Conservation We collected the conservation level of each residue from the ConSurf-DB database (Goldenberg et al., 2009), which provides precalculated conservation profiles for every structure in the PDB.
These profiles assign each residue to one of nine conservation levels, with 9 being the most conserved and 1 being the most variable.
For some residues, the information in the multiple sequence alignment is not enough to compute the conservation level (for example, if that position consists mostly of gaps).
In these cases, ConSurf-DB assigns the residue value of insufficient data.
2.2.2 Accessibility We used the program Naccess to calculate the total relative accessibility for each residue (Hubbard and Thornton, 1993).
We further normalized these accessibility values by transforming them into quantiles, so that the most buried residue in a given protein would get the value 0 and the most exposed would get the value 1.
This normalization was done in light of the observation that some protein structures might overall be more accessible than others owing to their geometric properties, but within a single structure conserved residues still tend to be more buried compared with other residues in the same structure.
Each residue was then classified into one of ten evenly distributed accessibility classes.
2.2.3 Structure quality features For each structure, we extracted the resolution and the R-and free R-factors from the PDB as measures of the general structure quality.
This was done in order to validate our assumption that the correlation between the level of burial and evolutionary conservation of the amino acids would increase with the structure quality (see Section 3.1.1).
2.2.4 Alignment quality features We collected the following measures for each structure: (i) Nseq, the number of homologs in ConSurf-DBs alignment; (ii) Nseq20, the number of homologs in the alignment whose identity is >20% [the level of identity for each homolog is extracted from the PSI-BLAST output (Altschul et al., 1997), taken from ConSurf-DB]; (iii) Resnum, the number of residues with significant conservation information; (iv) %insig, the fraction of the protein residues whose conservation level is assigned the value insufficient data.
These features were chosen to reflect the general quality of the alignment and evolutionary rates generated by ConSurf-DB for each protein.
2.2.5 Finding the optimal filtering cutoffs The four measures of alignment quality that we collected could each help predict in advance whether a given protein would be well-suited for use with our method.
To find the optimal way to integrate these features, we solved the following optimization problem: given a ratio X (called the filtering degree), find the optimal quadruple of cutoffs such that when filtering the dataset according to these cutoffs, X of the proteins in the dataset pass the filter, and their average ConQuass score (as defined in Section 2.3) is maximal.
This problem was solved for each X in 0.01, 0.02, ,1 using an exhaustive enumeration, enumerating for each cutoff over 50 discrete values distributed evenly across the dataset.
In what follows we refer to proteins that passed the filter corresponding to a given filtering degree X as having a high-quality alignment, according to the X-filter, where a higher filtering degree corresponds to a more stringent requirement.
2.3 The ConQuass score Similarly to Verify3D (Bowie et al., 1991), we built a 109 propensity matrix, where each cell gives the compatibility score for assigning a residue with conservation class c an accessibility class a, as given by the information value (Fano, 1961): score(c, a)= ln ( P(c|a) P(c) ) where P(c|a) is the probability of finding a residue of conservation class c in the accessibility class a, and P(c) is the overall probability of finding a residue in conservation class c. These probabilities are estimated using the conservation and accessibility levels of the residues in the dataset of known protein structures.
The accessibilities were calculated using the biological unit given by PQS.
We also tried using the PISA biological unit or the isolated chain, but the propensity matrices generated were very similar (data not shown).
The final propensity matrix is shown in Supplementary Table S1.
ConQuass assigns each structure the average score of its residues: score(C, A)= 1 L score(Ci,Ai) where C and A are vectors of the same length L (number of amino acids in the protein), giving, respectively, the conservation and accessibility classes of the residues.
1301 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1302 12991307 M.Kalman and N.Ben-Tal 2.4 Assessment on the CASP dataset In the model quality assessment category of CASP, the participating groups were asked to rank the models built by the participating automatic servers.
We downloaded these server models, as well as the predictions of the participating MQAPs, from the CASP web site (http://predictioncenter.org).
We also downloaded for each server model its global distance test total score (GDT-TS) (Zemla, 2003), which is in the range (0, 100] and is the standard quality evaluation score given by CASP.
For each CASP target, we downloaded conservation information, if available, from the ConSurf-DB database entry for the native structure.
The same conservation information was aligned to all full-atom models of the target, as there is sometimes a shift between the residue sequence numbers in the native structures and in the CASP models.
To this end, we ranked each such alignment by giving each column a score of +1 if the residue identity in the native matched that of the model, 2 if the residues did not match, 1 for an insertion/deletion and 0 if the residue was missing in one of the structures.
The optimal alignment was then found using the SmithWaterman algorithm (Smith and Waterman, 1981).
For each model, we computed the accessibility class of each residue, as was done for the structures in the training set (see Section 2.1).
The conservation levels and accessibilities were used to calculate the MQAP score for each model (see Section 2.3).
We did not score targets whose native structure had no ConSurf-DB information.
When comparing ConQuass to the MQAPs that participated in CASP7, we considered only the 16 MQAPs that had ranked at least 15 000 models.
For MQAPs that participated in CASP8, we considered only the 22 pure single-structure methods that had ranked at least 20 000 models.
We restricted each analysis to models that had been ranked by all considered methods (including ConQuass), and from this set we eliminated targets for which fewer than 100 ranked models were available.
For each MQAP and each target, we calculated the Pearson correlation between the quality scores given by the MQAP and the GDT-TS scores downloaded from the CASP web site.
2.5 Integration of ConQuass with other methods To demonstrate that the conservation information used in ConQuass is complementary to that used by other methods, we built three new MQAPs, integrating the score given by ConQuass (Section 2.3) with the scores given by Circle-QA (Terashi et al., 2007), QMEANfamily (Benkert et al., 2009) and MULTICOM-REFINE (Cheng et al., 2009), respectively.
We chose Circle-QAbecause it was the leading pure single-structure method in CASP7, and we chose QMEANfamily and MULTICOM-REFINE because they were the leading pure single-structure methods in CASP8.
For each integrated MQAP, the score we assigned to each model was a simple linear combination of the ConQuass score and the score produced by the other method (the two scores were each assigned a weight of 0.5).
The analysis described in Section 2.4 was repeated for these three MQAPs.
We compared the first MQAP (integration with Circle-QA) to MQAPs that had participated in CASP7 and compared the other two (integration with QMEANfamily or MULTICOM-REFINE) to MQAPs that had participated in CASP8.
3 RESULTS AND DISCUSSION 3.1 Experimentally determined structures match their conservation pattern 3.1.1 Examining a dataset of high-quality structures It is widely recognized that residues buried in the protein core tend to be evolutionarily conserved, whereas residues on the surface are usually variable (Brndn and Tooze, 1999; Lichtarge et al., 1996).
This implies that the accessibilities of variable residues should be shifted toward higher values in comparison with those of conserved residues, as indeed seems to be the case for many experimentally solved protein structures we examined (e.g.
Fig.1A and B).
This characteristic is expected for true protein structures, and we would Fig.2.
ConQuass scores assigned to experimental structures from the PDB and to a few erroneous models.
The scatter plot shows the propensity score of the protein versus the number of residues with ConSurf information for all the structures in the dataset (in gray).
Only structures that have ConSurf information for at least 40 residues were included.
Also shown are pairs of incorrect (triangle) and correct (circle) structures for EmrE (black, 2f2m and 3b5d), Connexin (gray, 1txh and 2zw3) and MsbA (white, 1jsq and 3b5w).
For each of these structures, the ConQuass score was calculated for the residues of all the chains in the biological unit.
For models containing only the C-trace, the full-atom structures were rebuilt using MaxSprout and SCWRL4.
The correct structure of MsbA (3b5w) was truncated to contain the same set of residues as the erroneous structure (1jsq).
generally not expect to see it in incorrect models.
Figure 1C and D show the evolutionary profile of an extremely poor model structure (analysis of an intermediate quality model of the same protein is provided in Supplementary Fig.S1).
We first set out to measure the magnitude of this trend in real protein structures.
For that purpose, we collected a comprehensive dataset of high-quality experimentally determined structures, which we can reasonably assume to contain mostly correct structures (Section 2.1).
For this dataset, it is obvious that the more variable residues are consistently more accessible than the conserved residues (Fig.1E).
The information in this dataset was used to calculate a propensity matrix, giving the compatibility of each conservation class with each accessibility class (Section 2.3, and Supplementary Table S1).
The matrix confirmed our intuitive expectations, giving high propensity scores to accessible-variable residues and to buried conserved residues.
Consequently, the matrix was used to calculate each protein structures ConQuass score, which was the average of the propensity scores of the proteins residues.
A score was calculated for each structure in the dataset (Fig.2), using the biological unit complexes as given by PQS (Henrick and Thornton, 1998).
Only 7.9% of the structures received a negative score, meaning that for most structures the residues conservation levels tended to be compatible with their accessibility levels.
However, when we determined scores for the individual chains in the dataset without the context of the biological unit, more structures were assigned a negative score (12.5%).
This was due to monomers exposing conserved interface residues that are 1302 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1303 12991307 Assessment of protein model structures Fig.3.
Compatibility of the structure with the evolutionary profile of the protein is higher for higher-quality structures or higher-quality multiple sequence alignments, as described by different quality measures.
(A) The mean ConQuass score of the proteins in the dataset when filtering only for the top X proteins (x-axis), as measured by several crystallographic structure quality measures: the R-factor, free R-factor and the resolution.
(B) As in (A), but when filtering by non-structural measures: the number of residues (red), the number of homologous sequences in the alignment (black), the ratio of residues with insignificant conservation information as measured by ConSurf (green) and the number of homologous sequences in the alignment with at least 20% identity with the query (blue).
Also shown is the optimal ratio achieved by integrating these four measures (gray).
actually buried in the physiological complex.
We also tried to determine scores for the biological unit complexes given by PISA (Krissinel and Henrick, 2007) and the results were very similar to those obtained for the PQS complexes (data not shown).
The ConQuass scores also seemed to become progressively higher for structures of higher quality, as measured by various structure quality measures such as resolution, R-factor and free R-factor (Fig.3A).
The conservation data was calculated according to the multiple sequence alignment generated automatically by ConSurf-DB, and it is possible that a high-quality structure would be assigned a low ConQuass score if an inadequate alignment was used.
To discern these cases, we collected four measures that are indicative of the alignment quality or that could otherwise predict an incorrect ConQuass score for a protein model (see Section 2.2.4).
As can be seen in Figure 3B, the ConQuass score becomes progressively higher as the dataset is filtered to leave only structures whose alignment is of higher quality according to any one of the four measures.
Obviously, a better indicator for how suitable a protein is for ranking with ConQuass can be achieved by integrating the different alignment quality measures.
We used an exhaustive enumeration to find the optimal way to integrate these measures, each time filtering the database to leave only X% of the proteins such that the mean ConQuass score of the remaining proteins is maximal (Section 2.2.5).
This procedure assumes that after filtering, a higher mean ConQuass score is achieved because the remaining proteins have a higher quality alignment.
The integration achieves a much higher mean ConQuass score than does filtering by each measure separately (Fig.3B; gray).
The optimal cutoffs found for some selected filtering degrees are shown in Supplementary Table S2.
3.1.2 The conservation profile may reveal incorrect structures To test whether the ConQuass score is capable of discriminating incorrect structures, we collected three examples of structural models that had been deposited in the PDB but were later found to be incorrect.
All these structures also have corrected versions available, which we also scored using ConQuass (Fig.2).
The first two examples are EmrE (Fig.2; black) and MsbA (Fig.2; white).
Both structures were determined by Chang and coworkers using a faulty piece of in-house software, which caused the group to misinterpret the crystallization data and eventually yielded false models.
Following the detection of the error in the software, the structures were retracted (Chang et al., 2006), and corrected versions have since been published (Chen et al., 2007; Ward et al., 2007).
Calculating the ConQuass score for these structures is not straightforward, as they are all C-only models, with the exception of the erroneous EmrE structure.
However, we were able to apply ConQuass after reconstructing the full-atom models using MaxSprout (Holm and Sander, 1991) and SCWRL4 (Krivov et al., 2009).
Clearly, the correct structures are much more compatible with their conservation pattern than are the incorrect ones (Fig.2).
The third example is the gap junction connexin channel (Fig.2; gray), which was previously modeled by our group using low-resolution electron cryo-microscopy data (Fleishman et al., 2004b).
The helix assignment of the model recently turned out to be wrong when an experimentally determined high-resolution structure of a homologous protein was reported (Maeda et al., 2009).
For the purpose of comparing the two structures, we truncated the non-membrane residues from the experimental structure and also removed all non-C atoms.
This procedure left us with two C-only models composed of the same set of residues.
We then rebuilt the two full-atom models as above and scored them using ConQuass.
While both the truncation of non-membrane residues and the full-atom reconstruction lowered the score for the crystallographic model (data not shown), it was still assigned a much higher score than the erroneous model (Fig.2).
There are some cases in which a correct model seems not to match its conservation pattern, as denoted by a negative ConQuass score.
However, a closer examination can usually provide an explanation for the low score.
Some representative examples are discussed in Supplementary Section S1.1.
3.2 Ranking decoys in CASP ConQuass may also assess how distant a given model is from the native structure.
To show this, we checked how ConQuass scores models of varying quality for the same protein.
A good source for such models is the biennial CASP experiment (Moult et al., 2009), where each round consists of several targets, corresponding to proteins whose structure have recently been solved (but not yet published), and each participant submits computational models in an attempt to predict the structure of each target.
At the end of the round, the experimental structures are revealed, and the quality of each submitted model is measured by the similarity measure GDT-TS (Zemla, 2003), which is based on the superposition between the model and the native structure.
The seventh and eighth CASP rounds included a quality assessment category (Cozzetto et al., 2007, 2009), in which different MQAPs participated and were consequently evaluated according to their performance.
The models scored by the MQAPs were server models that were generated by the structure prediction servers participating in CASP and published shortly after the round began.
The MQAPs were evaluated according to the correlation between the scores they gave the different models and the quality of those models as measured by GDT-TS.
The scores given by the participating MQAPs are available for download from 1303 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1304 12991307 M.Kalman and N.Ben-Tal Fig.4.
The ability of the ConQuass score to rank decoys in the CASP8 dataset.
(A) Demonstration for target T0449.
For each decoy, the GDT-TS (similar to the native structure) is plotted versus the ConQuass score.
The vertical line is the ConQuass score assigned to the native structure.
The Pearson correlation for this target was 0.827.
(B) Box plots of the correlation values for the 22 MQAPs that ranked at least 20 000 models.
The number signifying each MQAP (x-axis) is the number assigned in the original CASP8 experiment (see http://predictioncenter.org/casp8).
Also shown is the box plot for the correlation values of ConQuass (999, gray).
The correlations were calculated only for models ranked by all 23 MQAPs.
The box plots were sorted by the mean correlation, indicated by the black dots.
The figure is cut to show only the correlation range [0.5, 1] in order to make the differences between the methods more apparent (the uncut version is shown in Supplementary Fig.S3).
(C) Same as (B), when looking only at targets with the highest quality alignment, using the 20% filter.
Although ConQuass is ranked first here, the specific ordering of the top ranking methods is irrelevant, as the correlation values achieved by ConQuass are not significantly higher than those achieved by MULTICOM-REFINE (013).
the CASP web site (http://predictioncenter.org), which allowed us to compare them with ConQuass.
The results for the CASP8 set are presented below.
The analysis of the CASP7 set showed a similar performance, and it is presented in Supplementary Section S1.2.
To be able to best compare ConQuass with the other pure single-structure methods, we have excluded from our analysis methods that use structural data from the other decoys or from homologs (a comparison of ConQuass with the latter methods is shown in Supplementary Fig.S2).
For brevity, we will use the term MQAP in this section to refer only to pure single-structure methods.
3.2.1 Example of the performance on one CASP8 target As an illustrative example, Figure 4A shows the GDT-TS values of the server models of CASP8 target T0449, plotted as a function of the assigned ConQuass scores.
There is a striking correlation between the score and the structure quality, and the set of highly scored models was enriched with high-quality structures.
The Pearson correlation in this case was 0.827, and the score of the native structure (Fig.4A; vertical line) was higher than the scores of all the decoys except three.
3.2.2 Overall performance on all CASP8 targets In our calculation on the CASP datasets (see Section 2.4), we used the conservation data recorded in the ConSurf-DB dataset.
There are cases in which the alignment could have been manually improved in order to achieve a better performance, but we deliberately refrained from doing this to avoid biasing our results.
Four CASP8 targets could not be ranked, because their native structures did not have any ConSurf-DB information.
This usually happens when ConSurf-DB cannot find enough homologs to construct a meaningful alignment.
Ten additional targets were cancelled by CASP8 or had no corresponding native structure listed in the CASP8 web site.
A ConQuass score was given to each of the full-atom models of the remaining 114 targets.
CASP allowed each participating MQAP to choose to rank any subset of models, for any subset of targets.
Indeed, many MQAPs are not applicable for all models.
This makes performance comparison problematic.
For example, it might be easier to assess the quality of full-atom models, and if so an MQAP (such as ConQuass) that ranks only such models would have an advantage over methods that also rank C models.
To avoid this problem, we carried out all calculations on the set of 11 686 models and 75 targets that were ranked by all participating MQAPs.
To avoid excluding too many models, only the 22 participating MQAPs that scored at least 20 000 models were used.
To evaluate the performance of each MQAP, we calculated for each CASP8 target the Pearson correlation between the scores determined by the MQAP and the GDT-TS values of all the models for that target.
The sets of correlation values for each MQAP are plotted in Figure 4B.
Our ranking of the methods is slightly different from the ranking published in the CASP8 proceedings (Cozzetto et al., 2009) due to differences in the ranking protocol (see a detailed explanation of the differences in Supplementary Section S1.3).
However, as in the CASP8 results, the MQAPs that performed best according to our assessment were the different variants of QMEAN (Benkert et al., 2009) and MULTICOM (Cheng et al., 2009).
The variants with the highest mean correlation were QMEANfamily (082) with a mean correlation of 0.778 and MULTICOM-REFINE (013) with a mean correlation of 0.768.
Following the different variants of QMEAN and MULTICOM, the method with the next 1304 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1305 12991307 Assessment of protein model structures Fig.5.
The relation between the ConQuass score assigned to the model and the models quality.
(A) Plot of the GDT-TS (similar to the native structure) versus the ConQuass score for the following models: all models in the CASP8 dataset (black, Pearson correlation 0.678); the models with the highest quality alignment by the 50% filter (blue, Pearson correlation 0.780); models that passed the less permissive 20% filter (red, Pearson correlation 0.843).
(B) Box plots of the GDT-TS values for models in each ConQuass score quartile.
For example, the median GDT-TS for the models scoring very low (below 0.017) is 22.8, and 50% of these models have GDT-TS values between 14.8 and 40.6.
For the models scoring very high (>0.12), the median GDT-TS is 73.1, and 50% of these models have GDT-TS values between 65.0 and 81.7. highest ranking, with a mean correlation of 0.722, was CIRCLE (396), which was the best performing single-structure MQAP in CASP7 (Cozzetto et al., 2007).
ConQuass (999, Fig.4B; gray) ranked next, with a mean correlation of 0.715.
As shown in Section 3.1.1, some structures were assigned a low ConQuass score because of a low-quality alignment rather than a low-quality model.
Indeed, for some CASP8 targets, the native structure itself scored very low by ConQuass.
Such targets were clearly not suitable for use with our method.
Many of these cases could be discerned in advance by using the alignment quality measures presented in Section 2.2.4.
To check how our method performs on more appropriate targets, we used the 20% filtering to select only the 17 targets with the highest quality alignment.
The performance of the different methods for this subset of targets is shown in Figure 4C.
With these targets, ConQuass performs significantly better, and it is ranked first with a mean correlation of 0.844.
It is important to stress that the set of targets that are more suitable for use with ConQuass can be selected a priori, as all the features used for the filtering are based on the multiple sequence alignment alone.
The Pearson correlations we evaluated were calculated for each target independently, so scores produced by an MQAP that achieves a high correlation value can be used to select among alternative structural models for the same protein.
However, in many scenarios one wants to evaluate the absolute quality of a single uncertain structural model without comparing it to other decoys.
For these cases, it is informative to know the relation between the MQAP score and the structure quality, as measured by the GDT-TS.
This relation, for all models of all CASP8 targets, is shown in Figure 5A.
The overall correlation between the ConQuass score and GDT-TS is good (0.678), especially when filtering for targets with a high-quality alignment (0.843 if using the 20% filtering, Fig.5A; red).
The overall correlation was also compared with that of the other participating MQAPs (Supplementary Table S3).
Figure 5B presents these results in a way that is more intuitive for interpreting the score given to a model by ConQuass.
If a model is assigned a very low ConQuass score (below-0.017), it is expected to be of rather low-quality (median GDT-TS 22.8, most GDT-TS values in the range [14.8, 40.6]).
However, if a model is assigned a very high ConQuass score (>0.12), it will very rarely be a low-quality structure (median GDT-TS 73.1, most GDT-TS values in the range [65.0, 81.7]).
3.2.3 Complementarity to other MQAPs ConQuass uses the evolutionary conservation properties of the protein structure, a feature that is not directly used by any other contemporary MQAP.
It therefore seems reasonable to suggest that ConQuass is complementary to the other prevalent methods.
To support this claim, we scored the CASP8 models using two new MQAPs that were trivial integrations of ConQuass with, respectively, MULTICOM-REFINE and QMEANfamily, the two best performing single-structure MQAPs in CASP8 (see Section 2.5).
The performance of these two integration methods was analyzed using the same procedure described above.
The integration with ConQuass significantly improved the correlations achieved by both MULTICOM-REFINE (P-value 4.2e-14) and QMEANfamily (P-value 1.1e-08); see Supplementary Section S1.4.
4 CONCLUSION Here we have presented ConQuass, a very simple MQAP based directly on the compatibility between the conservation and accessibility patterns of a given structural model.
We studied the scores that ConQuass assigns to experimental structures, demonstrated its ability to discern erroneous models and checked the relation between the ConQuass scores given to different models and the models resemblance to the native structure.
We have also shown that ConQuasss performance is comparable to that of other pure single-structure MQAPs, despite being much simpler than most.
Our approach is different from previous MQAPs that used evolutionary conservation, which were based on the spatial clustering of the conserved residues.
We feel our approach is more direct, since this clustering is mostly an effect of the conserved residues tendency to be buried in the structural core (for a direct comparison with the method developed by Mihalek and coworkers; see Supplementary Section S1.5).
ConQuass is also the first conservation-based approach to be rigorously compared with contemporary MQAPs.
In addition, our score is based on summation of information that is local in the structure (the propensity of the conservation class of each residue for its accessibility class), so it should be adaptable to provide a local quality score for each residue of the structure, as is done by local quality assessment tools (Fasnacht et al., 2007).
Preliminary tests for a local MQAP based on summing the propensities over a fixed-width window on the sequence have yielded promising results (data not shown).
In this study, we have clearly shown that evolutionary conservation is a powerful property for use in model quality assessment, so it would be advantageous for new MQAPs to integrate this property with other more commonly used properties.
Evolutionary conservation is currently not used directly by any MQAP, although some methods, like QMEAN and MULTICOM, use it indirectly by comparing model surface accessibilities with the predicted accessibilities, which are associated, in part, with the evolutionary conservation.
However, we have demonstrated that these methods do not use the conservation information to its full extent, as their results improve when their scores are integrated with 1305 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1306 12991307 M.Kalman and N.Ben-Tal those of ConQuass.
In this work, we have followed a very nave approach for such an integration, using a simple linear combination.
Much better results would doubtless be obtained by a more intricate approach, for example by using the residue conservation as one of the features in a machine learning-based tool.
In any case, such integration would have to take into account the quality of the alignment, as the evolutionary conservation property is more indicative for high-quality alignments.
The same approach could also be used to integrate conservation in many other practices, such as finding the physiological complex of a crystal structure and scoring docking results.
In addition, as the ConQuass score reflects the consistency between the alignment and the structure, its functionality could be reversed to check the quality of an alignment based on a high-quality structure.
While ConQuass is not the best performing of the examined MQAPs, many of which use a mixture of complex features including geometric and energetic properties of the structure, it has the advantage of being straightforward and easy to interpret.
The conservation pattern of the protein is not used by most modeling and structural determination programs, so ConQuass gives independent support for a structural model, whether experimental or computational.
If the model is assigned a low score, it is easy to visualize the discrepancy of the model with the conservation pattern by projecting it on the structure using ConSurf (Glaser et al., 2003), as we have done in the examples in Figure 1 and Supplementary Figure S4.
This can either yield relevant insights regarding the mechanism associated with the structure (for example, hint that it may bind to another molecule; see Supplementary Section S1.1), lead to a rejection of the model (see Fig.1C) or perhaps in some cases guide further refinements of the model.
ACKNOWLEDGEMENTS The authors thank Gilad Wainreb and Maya Schushan for helpful discussions.
Funding: Israel Science Foundation (grant 611/07); Edmond J. Safra Bioinformatics program at Tel-Aviv University (to M.K.).
Conflicts of Interest: none declared.
ABSTRACT Motivation: Numerous competing algorithms for prediction in high-dimensional settings have been developed in the statistical and ma-chine-learning literature.
Learning algorithms and the prediction models they generate are typically evaluated on the basis of cross-validation error estimates in a few exemplary datasets.
However, in most applications, the ultimate goal of prediction modeling is to pro-vide accurate predictions for independent samples obtained in differ-ent settings.
Cross-validation within exemplary datasets may not adequately reflect performance in the broader application context.
Methods: We develop and implement a systematic approach to cross-study validation, to replace or supplement conventional cross-validation when evaluating high-dimensional prediction models in independent datasets.
We illustrate it via simulations and in a col-lection of eight estrogen-receptor positive breast cancer microarray gene-expression datasets, where the objective is predicting distant metastasis-free survival (DMFS).
We computed the C-index for all pair-wise combinations of training and validation datasets.
We evaluate several alternatives for summarizing the pairwise validation statistics, and compare these to conventional cross-validation.
Results: Our data-driven simulations and our application to survival prediction with eight breast cancer microarray datasets, suggest that standard cross-validation produces inflated discrimination accuracy for all algorithms considered, when compared to cross-study valid-ation.
Furthermore, the ranking of learning algorithms differs, suggest-ing that algorithms performing best in cross-validation may be suboptimal when evaluated through independent validation.
Availability: The survHD: Survival in High Dimensions package (http://www.bitbucket.org/lwaldron/survhd) will be made available through Bioconductor.
Contact: levi.waldron@hunter.cuny.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Cross-validation and related resampling methods are de facto standard for ranking supervised learning algorithms.
They allow estimation of prediction accuracy using subsets of data that have not been used to train the algorithms.
This avoids over-optimistic accuracy estimates caused by re-substitution.
This characteristic has been carefully discussed in Molinaro et al.
(2005), Baek et al.
(2009) and Simon et al.
(2011).
It is common to evaluate algorithms by estimating prediction accur-acy via cross-validation for several datasets, with results sum-marized across datasets to rank algorithms (Boulesteix, 2013; Demsar, 2006).
This approach recognizes possible variations in the relative performances of learning algorithms across studies or fields of application.
However, it is not fully consistent with the ultimate goal, in the development of models with biomedical applications, of providing accurate predictions for fully inde-pendent samples, originating from institutions and processed by laboratories that did not generate the training datasets.
It has been observed that accuracy estimates of genomic pre-diction models based on independent validation data are often substantially inferior to cross-validation estimates (Castaldi et al., 2011).
In some cases this has been attributed to incorrect appli-cation of cross-validation; however even strictly performed cross-validation may not avoid over-optimism resulting from poten-tially unknown sources of heterogeneity across datasets.
These include differences in design, acquisition and ascertainment stra-tegies (Simon et al., 2009), hidden biases, technologies used for measurements, and populations studied.
In addition, many gen-omics studies are affected by experimental batch effects (Baggerly et al., 2008; Leek et al., 2010).
Quantifying these heterogeneities and describing their impact on the performance of prediction algorithms is critical in the practical implementation of persona-lized medicine procedures that use genomic information.
There are potentially conflicting, but valid, perspectives on what constitutes a good learning algorithm.
The first perspective is that a good learning algorithm should perform well when trained and applied to a single population and experimental set-ting, but it is not expected to perform well when the resulting model is applied to different populations and settings.
We call such an algorithm specialist, in the sense that it can adapt and specialize to the population at hand.
This is the mainstream per-spective for assessing prediction algorithms and is consistent with validation procedures performed within studies (Baek et al., 2009; Molinaro et al., 2005; Simon et al., 2011).
However, we argue that it does not reflect the reality that samples of conveni-ence and uncontrolled specimen collection are the norm in gen-omic biomarker studies (Simon et al., 2009).
We promote another perspective: a good learning algorithm should be generalist, in the sense that it yields models that may be suboptimal for the training population, or not fully representa-tive of the dataset at hand, but that perform reasonably well *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the last two authors should be regarded as Joint Last Authors.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com XPath error Undefined namespace prefix across different populations and laboratories employing compar-able but not identical methods.
Generalist algorithms may be preferable in important settings, for instance when a researcher develops a model using samples from a highly controlled envir-onment, but hopes the model to be applicable to other hospitals, labs, or more heterogeneous populations.
In this article we systematically use independent validations for the comparison of learning algorithms, in the context of microarray data for disease-free survival of estrogen receptor-positive breast cancer patients.
Although concern has been often expressed about the lack of independent validation of gen-omic prediction models (Micheel et al., 2012; Subramanian and Simon, 2010), independent validation has not been systematically adopted in the comparison of learning algorithms.
This defi-ciency cannot be addressed for prediction contexts where related, independent datasets are unavailable.
For many cancer types, however, several micro-array studies have been performed to develop prognostic models.
These datasets pave the way for a systematic approach based on independent validations.
For in-stance, a recent meta-analysis of prognostic models for late-stage ovarian cancer provides a comparison of publicly available microarray datasets (Waldron et al., 2014).
Furthermore, Riester et al.
(2014) showed that combining training datasets can increase the accuracy of late-stage ovarian cancer risk models.
Thus situations exist in genomic data analysis where comparable, independent datasets are available, and these pre-sent an opportunity to use independent validation as an explicit basis for assessing learning algorithms.
We propose what we term leave-one-dataset-in cross-study validation (CSV) to formalize the use of independent validations in the evaluation of learning algorithms.
Through data-driven simulations, and an example involving eight publicly available estrogen receptor-positive breast cancer microarray datasets, we assess established survival prediction algorithms using our leave-one-dataset-in scheme and compare it to conventional cross-validation.
2 METHODS 2.1 Notation and setting >We consider multiple datasets i=1, .
.
.
, I with sample sizes N1, .
.
.
, NI.
Each observation s appears only in one dataset i, i.e.
datasets do not over-lap, and the corresponding record includes a primary outcome Ysi and a vector of predictor variables Xsi ; throughout this article X s i will be gene-expression measurements.
Our goal is to compare the performance of dif-ferent learning algorithms k=1, .
.
.
, K that generates prediction models for the primary outcome using the vector of predictors.
Throughout this article, the primary outcome is a possibly censored survival time.
We are interested in evaluating and ranking competing prediction methods k=1, .
.
.
, K. Since the ranking may depend on the application, the first step is to define the prediction task of interest.
We focus on the prediction of metastasis-free survival time in breast cancer patients based on high-throughput gene-expression measurements.
Our approach and the concept of CSV, however, can be applied to different types of re-sponse variables and any other prediction task.
2.2 Algorithms We assess six learning algorithms (k=1, .
.
.
, 6) appropriate for high-dimensional continuous predictors and possibly censored time-to-event outcomes: Lasso and Ridge regression (Goeman, 2010), CoxBoost (Binder and Schumacher, 2008), SuperPC (Blair and Tibshirani, 2004), Unicox (Tibshirani, 2009) and Plusminus (Zhao et al., 2013).
All parameters were tuned either by default methods included in their implementation (Ridge and Lasso regression: R-package glmnet) or by testing a range of param-eters in internal cross-validation.
Our focus is not to provide a compre-hensive array of algorithms, but simply to use a few popular, representative algorithms to investigate CSV.
2.3 CSV matrices We refer in this article to m-fold cross-validation and related resampling methods collectively as cross-validation (CV).
Our ranking procedure for learning algorithms is based on a square matrix Zk of scores (k=1, .
.
.
,K).
The (i, j) element in the matrix measures how well the model produced by algorithm k trained on dataset i performs when validated on dataset j.
Since we consider K methods we end up with K method-specific square matrices Z1; .
.
.
;ZK: We set the diagonal entries of the matrices equal to performance estimates obtained with 4-fold CV in each dataset.
We will call Zk the CSV matrix.
Possible definitions for the Zki;j scores include the concordance index in survival analysis (Harrell et al., 1996), the area under the operating char-acteristic curve in binary classification problems, or the mean squared distance between predicted and observed values in regression problems.
We use survival models and focus on a concordance index, the C-index, which is a correlation measure (Gnen and Heller, 2005) between survival times and the risk scores, such as linear combinations of the predictors, provided by a prediction model.
The heatmap in Figure 1A displays the CSV matrix of C-statistics obtained through validation of eight models trained on the studies in Table 1 with Ridge regression.
2.4 Summarization of a CSV matrix In order to rank learning algorithms k=1, .
.
.
, K, we summarize each matrix Zk by a single score.
We consider following two candidate approaches.
(1) The Simple Average of all non-diagonal elements of the Zk matrix: CSV= X i X i6j Zki;j II 1 : (2) The Median or more generally a quantile of the non-diagonal entries of Zk.
Quantiles offer robustness to outlier values, and the possibility to reduce the influence of those studies that are consistently associated with poor validation scores, both when used for training and validation, and independently of the learning algorithm.
2.5 True global ranking Throughout our analyses the score Zki;j is a random variable.
First, studies i and j can be seen as randomly drawn from a population of studies.
Second, observations within each study can be considered as randomly drawn from the unknown and possibly different distributions Fi and Fj underlying studies i and j.
With this view of Zki;j as random variable, we consider the theoretical counterparts of the empirical aggregating scores (simple average and quantiles) described in Section 2.4 to summarize Zk.
The theoretical counterparts are the expected value or quantiles of each Zki;j score, i 6 j; obtained by integrating the two levels of randomness that we described.
The true global ranking of the learning algorithms k=1, .
.
.
,K is then defined by these expected values (or quantiles), one for each algorithm.
We will call the ranking global because it depends on the super-population (Hartley and Sielken, 1975) and not which popula-tions were sampled by the available datasets.
i106 C.Bernau et al.
;Micheel etal.,2012 `` '' ; paper gene generate paper gene cross-study validation , cross-study validation Cross-study validation paper .
cross-study validation matrix, or : .
, The true global ranking can be considered as the estimation target of evaluation procedures such as CV or CSV.
In Section 2.7 we present the design of a data-driven simulation study in which we can compute the true ranking through Monte Carlo integration.
This allows us to evaluate and compare the ability of CV and CSV to recover the true global ranking.
2.6 Datasets We used a compendium of breast cancer microarray studies curated for the meta-analysis of Haibe-Kains et al.
(2012) and available as supple-ment to their article.
We selected all eight datasets (Table 1) for which distant metastasis-free survival (DMFS), the most commonly available time to event endpoint, as well as Estrogen Receptor (ER) status, were available.
These studies were generated with Affymetrix HGU GeneChips HG-U133A, HG-U133B and HG-U133PLUS2.
We con-sidered exclusively ER-positive tumors.
Of these datasets, only one origi-nated from a population-based cohort (Schmidt et al., 2008).
Four studies considered only patients who did not receive hormone therapy or adju-vant chemotherapy.
Only four provided date ranges of patient recruit-ment (Chin et al.
2006; Desmedt et al., 2007; Foekens et al., 2006; Schmidt et al., 2008).
Table 1 points also to important differences in survival (for instance 3Q survival) that are not easily explicable based on known characteristics of these studies.
This variability in design stra-tegies, reporting, as well as outcomes, highlights the prevalence of sam-ples of convenience in biomarker studies discussed by Simon et al.
(2009).
Samples from dataset ST1 duplicated in dataset VDX were removed.
Expression of each gene was summarized using the probeset with A B C Fig.1.
CSV matrices Zk in simulated and experimental data for Ridge regression.
(A) C-indices for training and validation on each pair of actual datasets in Table 1.
The diagonal of this matrix shows estimates obtained through 4-fold CV.
(B) The heatmap for each pair of studies (i, j), the average C-index when we fit Ridge regression on a simulated dataset generated by resampling gene expression data and censored time to event outcomes from the i-th study in Table 1, and validate the resulting model on a simulated dataset generated by resampling study j. Computation of each diagonal element averages over pairs of independent datasets obtained by resampling from the same study.
The heatmaps strongly resemble each other.
CAL and MSK are outlier studies: cross-study C-index is 0.5 when they are used either for training or validation.
The values of the arrays in (A) and (B) that involve these two studies constitute the blue bad performance cluster in (C) which contrast the C-indices obtained for study pairs i; j; i 6 j, on simulated data (y-axis) and experimental data (x-axis).
Pearson correlation is 0.9.
The three plots illustrate similarity between our simulation model and the actual datasets in Table 1 Table 1.
Breast cancer microarray datasets curated by Haibe-Kains et al.
(2012) Number Name Adjuvant therapy Number of patientsa Number of ER+ 3Q survival [mo.]
Median follow-up [mo.]
Original identifiersb Reference 1 CAL Chemo, hormonal 118 75 42 82 CAL Chin et al.
(2006) 2 MNZ none 200 162 120 94 MAINZ Schmidt et al.
(2008) 3 MSK combination 99 57 76 82 MSK Minn et al.
(2005) 4 ST1 hormonal 512a 507b 114 106 MDA5, TAM, VDX3 Foekens et al.
(2006) 5 ST2 hormonal 517 325 126 121 EXPO, TAM Symmans et al.
(2010) 6 TRB none 198 134 143 171 TRANSBIG Desmedt et al.
(2007) 7 UNT none 133 86 151 105 UNT Sotiriou et al.
(2006) 8 VDX none 344 209 44 107 VDX Minn et al.
(2007) Datasets acronyms: CAL, University of California, San Francisco and the California Pacific Medical Center (USA); MNZ, Mainz hospital (Germany); MSK, Memorial Sloan-Kettering (United States).
ST1 and ST2 are meta-datasets provided by Haibe-Kains et al.
(2012), TRB denotes the TransBIG consortium dataset (Europe), UNT denotes the cohort of untreated patients from the Oxford Radcliffe Hospital (UK), VDX=Veridex (the Netherlands).
Number of ER+ is the number of patients classified as Estrogen Receptor positive.
3Q survival indicates the empirical estimate of the 75-th percentile of the distribution of the survival times (in months).
Median follow-up (in months) is computed using the reverse KaplanMeier estimate to avoid under-estimation due to early deaths (Schemper and Smith, 1996).
aNumbers shown are after removal of samples duplicated in the dataset VDX.
bDataset identifiers specified in Haibe-Kains et al.
(2012).
i107 CSV for the assessment of prediction algorithms 8 ); ); Chin et al .
( 2006 ) highlight `` '' maximum mean (Miller et al., 2011).
The 50% of genes with lowest variance were removed.
Subsequently, gene-expression values were scaled by linear scaling of the 2.5 and 97.5% quantiles as described by Haibe-Kains et al.
(2012).
2.7 Simulation design We simulate heterogeneous datasets with gene-expression profiles and time to event outcomes from a joint probability model.
We define the model through a resampling procedure that we apply to the eight breast cancer datasets in Table 1.
The resampling scheme is a combination of parametric and nonparametric bootstrap (Efron and Tibshirani, 1993).
The goal of our simulation study is to compare CV and CSV when used for ranking and evaluation of competing learning algorithms.
Here we use resampling methods to iteratively simulate realistic ensembles of breast cancer datasets from a hierarchical probability model that we define using the actual datasets in Table 1.
CV and CSV are then assessed with respect to their ability to recover the true global ranking, which we compute through Monte-Carlo integration.
We will quantify the ability to recover the ranking by using the Kendall correlation between the true global ranking and the estimates obtained with CV or CSV.
For b=1, .
.
.
,B=1000 iterations, we generate a collection of I=8 datasets as follows.
First, we sample eight study labels with replacement from the list of breast cancer studies in Table 1.
This step only involves simulations from a multinomial Mult(8,[1/8, .
.
.
, 1/8]) distribution.
We resample the collection of study labels to capture variability in study availability, and heterogeneity of study characteristics.
Second, for each of the eight randomly drawn labels, we sample N=150 patients from the corresponding original dataset, with replacement.
If a study is randomly assigned to the j-th label, then each of the N=150 vectors of predictive variables is directly sampped from the empirical distribution of the j-th study in Table 1.
Finally, we simulate the corresponding times to event using a proportional hazards model (parametric bootstrap) fitted to the j-th dataset: Mjtrue : j tjx =j0 t  exp xTj  ; 1 where j(tjx) is the individual hazard function when the vector of pre-dictors is equal to x and j denotes a vector of regression coefficients.
We combine the truncated inversion method in Bender et al.
(2005) and the NelsonAalen estimator for cumulative hazard functions to simulate sur-vival times that reflect survival distributions and follow-up of the real studies.
We set the vector j to be the coefficients fitted in study j=1, .
.
.
, I using the CoxBoostmethod (Binder and Schumacher, 2008).
A different regression method could have been used at this stage.
The collections of simulated datasets are then used both (i) to compute by Monte-Carlo method the true global ranking defined in Section 2.5, and (ii) to compute ranking estimates through CV and CSV.
Figure 1A displays, for each pair of studies (i, j) in Table 1, the C-index obtained when training a model by Ridge regression on dataset i (rows), and validating that model on dataset j (columns).
We computed the diagonal elements (i= j) by 4-fold CV.
Figure 1B displays mean C-indices for each (i, j) combination across simulations, when the training and validation studies are generated resampling the i-th and j-th study.
Here diagonal elements are computed by averaging C-indices with the training and validation datasets inde-pendently generated by resampling from the same study.
The strong similarity between the two panels is reassuring, in particular with regard to the clear separation of the eight studies into two groups.
The first group includes studies MNZ, ST1, ST2, TRP, UNT and VDX, and produces more accurate prediction models than the remaining stu-dies.
The datasets in this group are also associated with higher values of the concordance index when used for validation.
This difference between the two groups is also illustrated in Figure 1C.
It displays the non-diag-onal entries of the matrices represented in the left and middle panels, that is the average C-indices from simulated datasets, against the C-indices from real data.
This scatterplot shows a clear two-cluster structure: the yellow dots display the 30 training and validation combinations within studies MNZ, ST1, ST2, TRP, UNT and VDX.
We will return to this cluster structure in the discussion.
2.8 Evaluation criteria for simulations In simulation studies we can assess and rank algorithms based on their ability to recover the true underlying models Mitrue; i=1; .
.
.
; I: In this subsection, we introduce a criterion that reflects the degree of similar-ity between the true regression coefficients i that were used to simulate the i-th in silico dataset and the coefficients bkj fitted through algorithm k on the j-th simulated dataset.
We consider the i= j and i 6 j cases separately.
Similarity between vectors is usually quantified by com-puting the Euclidean distance between them.
However, since our focus is on prediction, we useccorXii;Xibkj ; the correlation between true and estimated patient-specific prognostic scores, to measure the similarity between the true i and estimated regression coefficients bkj : Here Xi is the matrix of pre-dictors of dataset i and ccor denotes Pearsons correlation.
The average Skself= 1=I  X i ccor Xii;Xib k i  ; 2 over the I studies, provides a measure of the ability of learning algorithm k to recover the model that has generated the training dataset, hence the index self.
Another criterion of interest is the ability of a learning algorithm k to recover the vector of regression coefficients i when it is trained on a separate dateset j 6 i and the unknown models underlying datasets i and j might differ from each other.
This can be quantified with Skacross= 1= I I 1   X i X j6i ccor Xii;Xib k j  ; 3 where the index across emphasizes the focus on cross-study similarity, i.e.
on the ability of algorithm k to recover the coefficients i when fitted on dataset j, with j 6 i: In alternative to averaging across studies, or pairs of datasets, as in Equations (23) one can also use different summaries, e.g.
quantiles, as we do in Section 2.4.
Both Skself and S k across are summary statistics to assess and compare learning algorithms.
We denote the ranking obtained by ordering the algorithms according to Sself(Sacross) by Rself(Racross).
Both Skself and S k across vary across simulations of the datasets ensembles, al-though the hierarchical simulation model remains fixed and their com-putations involve the vectors i, i=1, .
.
.
, I.
We will therefore call the rankings Rself and Racross local because they are specific to the collection of datasets at hand.
3 RESULTS 3.1 Simulated data Our focus in the simulation study is on differences between the rankings and performance estimates obtained by using CV and CSV.
We will use CV and CSV to denote the means of the diagonal and non-diagonal elements of a CSV matrix, respect-ively.
Recall that we compute the diagonal elements through CV.
Figure 2A shows, for K=6 algorithms, the distributions of CSV and CV; and Figure 2B shows the distribution of the rank-ings estimates, across 1000 simulated collections of eight data-sets.
Table 2 compares the medians of the distributions in Figure 2B with the true global rankings that we obtained using the criteria in Section 2.4.
The rank of method k is 1 if it i108 C.Bernau et al.
gene &percnt; gene &hellip;, 8 8-Monte .
, .
.
Section , 8 sub outperforms the remaining K 1 training algorithms.
We ob-serve large differences in the distributions of CSV and CV across simulations (Fig.1A): the average of the CV scores, under all the algorithms we considered, is close to 0.65, while the CSV scores are centered at 0.55.
The variability of CV and CSV across simulations, however, is comparable.
Performance differences across algorithms, whether estimated by CV or CSV, are relatively small compared to the overall dif-ference between CV and CSV performance estimates.
We also observe differences between the rank distributions produced by CV and CSV.
Accordingly, to both CV and CSV, in most of the simulations, Lasso regression is ranked as one of the worst per-forming algorithms, while Ridge regression and Plusminus are ranked first or second.
However, the CV summaries suggest an advantage of Ridge regression over Plusminus across most of the simulations while CSV rank Plusminus as the best performing algorithm in 50% of the simulations.
The median rank of CoxBoost across simulations has an improvement of two pos-itions when it is estimated through CV and compared to the CSV summaries; in this case CSV results are more consistent with the true global rankings (Table 2).
When we consider the criteria described in Section 2.4, Ridge regression and Plusminus ex-change the top-two positions of the true global rankings (see Table 2), although for these two algorithms the Zi,j distributions under our simulation scenario are nearly identical.
The local rankings Racross and Rself of the K=6 algorithms defined by Skacross and S k self in Section 2.8 vary across the 1000 simulated collections of studies.
The median Kendalls correl-ation between Racross and Rself across simulations is 0.5, i.e.
the performance measures Skacross and S k self tend to define distinct rankings of the competing algorithms, see also the Supplementary Figure S1.
We illustrate the extent to which CV and CSV recover the unknown rankings Racross and Rself.
The boxplots in Figure 3 display the Kendalls correlation between local rankings (i) Racross or(ii) Rself , and the rankings estimated through CV (gray boxes) and CSV (white boxes) across simula-tions.
Figure 3C shows the Kendalls correlation between the true global ranking and the ranking estimates.
The median Kendalls correlation between Rself and the corresponding CSV estimates across simulations is 0.5.
The CV ranking estimates tend to be less correlated with the local rankings Racross than the CSV estimates.
In contrast, the CV estimates tend to be more correlated with Rself than the CSV estimates.
We recall that both CV and Rself are defined summarizing performance measures, Zki;i and ccorXii;Xibki ; that refer to a single study, while CSV and Racross summarizes performance measures computed using two distinct studies that are used for training and validation.
Finally, CSV tends to be more correlated with the true global ranking than CV.
This suggests that CSV is more suitable for recovering the true global ranking.
When we removed the two outlier studies (CAL and MSK) and repeated the simulation study, the advantage of CSV over CV in recovering the true global ranking was confirmed (median Kendalls correlation 0.8 versus 0.6, see also Supplementary Figs S2S4), moreover after their removal Kendalls correlations between Rself and the CSV estimates tend to be larger than those between Rself and the CV estimates.
Overall, as displayed by the Supplementary Figure S3, it appears that, after outlier studies are removed, CSV out-performs substantially CV when used for ranking algorithms.
3.2 Application to breast cancer prognostic modeling We apply CV and CSV to the I=8 breast cancer studies described in Section 2.
Generally, the results resemble those ob-tained on simulated data.
The top panel in Figure 4 illustrates the distributions of the diagonal and off-diagonal validation statis-tics in Zk for each of the K=6 algorithms.
Except for the dis-tinctly larger interquartile ranges of the box-plots we observe several similarities with Figure 2.
Note that each box-plot A B Fig.2.
Comparison of CSV and CV on simulated data.
Each panel rep-resents evaluations of K=6 algorithms across 1000 simulations of a compendium of I=8 datasets.
For each simulation the diagonal or off-diagonal elements of the Zk matrix of validation C-statistics is sum-marized by (A) mean and (B) rank of the mean across algorithms.
CV estimates tend to be much higher than the CSV estimates.
In most of the simulations Lasso is ranked as one of the worst algorithms, both by CV and CSV, while Ridge and Plusminus are ranked among the best predic-tion methods Table 2.
True global rankings and estimates with CV and CSV on simulated data Algorithm Global true ranking CSV (median ranks) CV (median ranks) Criterion Average Medium Average Medium Average Medium Ridge 1 2 2 2 1 2 Plusminus 2 1 2 2 2 2 Superpc 3 3 4 3 4 4 Unicox 4 4 4 4 5 4 CoxBoost 5 5 5 5 3 4 Lasso 6 6 6 6 5 6 Median estimates across 1000 simulations are displayed for CV and CSV; individual columns refer to summarization of the Zki;j statistics by using the mean or the median as discussed in Section 2.4.
We also computed the true global ranking as well as CV and CSV estimates by using the third quartile of the Zki;j summaries, and obtained results identical to those displayed for the rankings obtained by summar-izing validation results through their median.
Both CV and CSV tend to rank Ridge regression and Plusminus as best performing algorithms.
Variability of CV and CSV rank estimates across simulations is shown in Figure 2B.
i109 CSV for the assessment of prediction algorithms around approximately subsection top Section approximately (a) (b) the approximately equal to , s.represents validation scores within a single Zk-matrix, whereas in Figure 2 each box-plot displays a summary of 1000 Zk matrices, one for each simulation.
This explains the higher variance observed in Figure 4.
We also observe the following.
CV estimates are 0.06 higher than CSV estimates on the C-index scale.
To interpret the magnitude of this shift on the C-index scale consider a population with two groups of pa-tients, high and low risk patients, covering identical propor-tions 0.5 of the population.
A perfect discrimination model that correctly recognizes the subpopulation of each individ-ual, when the hazard ratio between high versus low risk patients is 2.7, achieves on average a C-index of 0.62.
It is necessary to double the hazard ratio to 5.4 to increase the average C-index of the perfect discrimination model to 0.68.
Thus, it is fair to say that the CV results are considerably more optimistic than the CSV estimates.
The ranking defined by CSV, using median summaries of the Zki;j scores, is nearly identical to the global ranking in our simulation example (see Supplementary Table ST1 and Table 2).
With both, median and third quartile aggregation of the Zki;j statistics, the rankings defined by CV and CSV differ substantially (Kendalls correlations 0.6 and 0.07).
This is consistent with the results of the simulation study, where median correlation of the rankings estimated through CSV and CV was 0.4 (see Supplementary Fig.S1).
The presence of outlier studies (CAL and MSK) has a strong effect on the ranking estimates when we use the mean to summarize the Zk matrices.
After aggregating the validation statistics by averaging, both CSV and CV rank Superpc first.
This result might be due to the high variability, 0.5, of the Zki;j validation scores corresponding to models trained by outlier studies.
In particular, Superpc and Unicox are the only algorithms that produce models with substantial prediction performances when trained on the MSK study.
With median summarization the ranking estimates are less influenced by the presence or absence of outlier studies.
We therefore recommend the use of the median to summarize Zk matrices.
Figure 4B illustrates lack of agreement between CSV and CV performance estimates.
The black digits contrast, for each dataset i, the CSV summary X j 6i I 1 1Zki;j versus the CV summary Zki;i: Performance measures refer to Ridge regression.
Similarly, the gray digits in this panel contrastX j 6i I 1 1Zkj;i with Z k i;i: The CV performance statistics Zki;i are only moderately correlated with the CSV statistics X j 6i I 1 1Zki;j (correlation=0.2), and negatively correlated with the CSV summariesX j 6i I 1 1Zkj;i (correlation=0.33).
3.3 CV and CSV summaries Correlation between CSV and CV summary statistics, as dis-played in Figure 4B, suggests that cross-and within-study per-formances are less redundant than one might expect.
In Figure 4B study specific CSV summaries are plotted against CV for Ridge regression.
For each study we have a single CV statistic and two CSV statistics obtained by averaging the A B Fig.4.
Panel (A) describes the CSV and CV statistics in Zk, separately for each of the six algorithms that we considered.
Each box-plot represents the variability of CV or CSV performance statistics from a single Zk matrix.
The CV statistics tend to be higher than the CSV statistics.
Panel (B) contrasts with black digits, for each study i, the CSV summaryX j6i I 1 1Zki;j with the CV summary Z k i;i: Similarly, with gray digits it contrasts the CSV summary X j 6i I 1 1Zkj;i with the CV summary Zki;i: This panel shows results for the learning algorithm Ridge regression and the displayed numbers refer to Table 1 (outliers CAL and MSK were removed).
Cross-validation statistics on the y-axis are moderately corre-lated to the CSV summaries on the x-axis; identical considerations hold for all K=6 algorithms that we used A B C Fig.3.
Kendalls correlation between true global or local rankings and estimates obtained with CSV (white box-plots) or CV (cross-validation, gray box-plots) across simulations.
Panels (A) and (B) compare CV and CSV in terms of their correlation to the local rankings (Racross and Rself), while panel (C) considers the true global ranking.
Each box-plot repre-sents a correlation coefficient that was computed in each of the 1000 iterations of our simulation study.
CSV tend to achieve a higher correl-ation with the global ranking and Racross than CV.
The results displayed have been computed using the mean criterion discussed in Section 2.4 i110 C.Bernau et al.
at: approximately approximately around .
.-study Z-matrix column-and row-wise.
In the column-wise case correl-ations, between CSV and CV summaries, vary across algorithms 0.5, while in the row-wise case all the correlations are negative.
Overall, we can consider cross-and within-study prediction as two related but distinct problems.
We also noted that CV is less suitable for detection of outlier studies than CSV; in particular CV can estimate encouraging prediction performances even on studies associated, under each training algorithm, with poor CSV summaries Zki;i: For instance, with the SuperPC algorithm all but one C-index estimates ob-tained with CV are above 0.6.
3.4 Specialist and generalist algorithms Our analyses lead to the question of whether some algorithms can be considered as generalist or specialist procedures according to our definitions.
Our examples are not exhaustive and add-itional comparisons, within the development of new prognostic models, are necessary in order to determine specialist or gen-eralist tendencies of these algorithms.
However, the fact that Ridge regression, Lasso regression and CoxBoost are ranked dis-tinctly better accordingly to CV than CSV, in most iterations of our simulation study, suggests that these algorithms might be specialist procedures and adapt to the specific properties of the individual dataset.
The status of generalist versus specialist, for each algorithm, can be discussed using the local performance criteria Sself and Sacross, which are conceived to measure within-single-studies and generalizable prediction performances.
We note that CoxBoost and Ridge regression tend to achieve better ranks in Rself than in Racross.
In particular CoxBoost im-proves its position by 1 or 2 ranks in most simulations, which is similar to what we observed comparing CoxBoosts CSV and CV rankings.
In summary, in our study, these two algorithms seem to haveaccordingly to all the criteria that we considereda tendency to specialize to the dataset at hand.
We mention that, as one can expect, for all the algorithms Sself is consistently higher than Sacross.
We also compared CV to independent within-study validation using our simulation model.
For the inde-pendent within-study validation, we iteratively pair two datasets generated using identical regression coefficients and gene expres-sion distributions.
Subsequently, we train a model on the first dataset and evaluate it on the second one.
As can be seen in Supplementary Figure S5, CV values, as expected, are slightly smaller than for the independent within-study validations.
4 DISCUSSION AND CONCLUSION In applying genomics to clinical problems, it is rarely safe to assume that the studies in a research environment faithfully rep-resent what will be encountered in clinical application, across a variety of populations and medical environments.
From this standpoint, study heterogeneity can be a strength, as it allows to quantify the degree of generalizability of results, and to inves-tigate the sources of the heterogeneity.
This aspect has long been recognized in meta-analysis of clinical trials (Moher and Olkin, 1995).
Therefore, we expect that an increased focus on quantify-ing cross-study performance of prediction algorithms will con-tribute to the successful implementation of the personalized medicine paradigm.
In this article we provide a conceptual framework, statistical approaches and software tools for this quantification.
The con-ceptual framework is based on the long-standing idea that finite populations of interest can be viewed as samples from an infinite super-population (Hartley and Sielken, 1975).
This concept is especially relevant for heterogeneous clinical studies originating from hospitals that sample local populations, but where re-searchers hope to make generalizations to other populations.
As an illustrating example, we demonstrate CSV on eight in-dependent microarray studies of ER-positive breast cancer, with metastasis-free survival as the endpoint of interest.
We also de-velop a simulation procedure involving two levels of non-parametric bootstrap (sampling of studies and sampling of ob-servations within studies) in combination with parametric boot-strap, to simulate a compendium of independent datasets with characteristics of predictor variables, censoring, baseline hazards, prediction accuracy and between-dataset heterogeneity realistic-ally based on available experimental datasets.
Cross-validation is the dominant paradigm for assessment of prediction performance and comparison of prediction algorithms.
The perils of inflated prediction-accuracy estimations by incor-rectly or incompletely performed cross-validation are well known (Molinaro et al., 2005; Subramanian and Simon, 2010; Simon et al., 2011; Varma and Simon, 2006).
However, we show that even strictly performed cross-validation can provide optimistic estimates relative to CSV performance.
All algorithms, in simulation and example, showed distinctly decreased perform-ance in CSV compared to cross-validation.
Although it would be possible to further reduce between-study heterogeneity, for ex-ample by stricter filtering on clinical prognostic factors, we believe this degree of heterogeneity reflects the reality of clinical genomic studies and likely other applications.
Some sources of biological heterogeneity are unknown, and it is impossible to ensure consist-ent application of new technologies in laboratory settings.
Prediction models are used in presence of unknown sources of variation.
Formal CSV provides a means to assess the impact of unknown or unobserved confounders that vary across studies.
In simulations, the ranking of algorithms by CSV was closer to the true rankings defined by cross-study prediction, both when we considered Racross and the global true ranking.
Surprisingly, CSV was also competitive with CV for recovering true rankings based on within-study prediction, such as Rself.
Although the performance differences we observed between algorithms were smaller than the difference between CV and CSV, Lasso consist-ently compared poorly with most of the competing algorithms, both under CV and CSV evaluations.
Lasso, and other algo-rithms that ensure sparsity have been shown to guarantee poor prediction performances in previous comparative studies (Bvelstad et al., 2007; Waldron et al., 2011).
Systematic CSV provides a means to identify relevant sources of heterogeneity within the context of the prediction problem of interest.
By simple inspection of the CSV matrix we identified two outlier studies that yielded prediction models no better than random guessing in new studies.
This may be related to known differences in these studies: smaller numbers of observations, higher proportions of node positive patients, different treatments and larger tumors (Supplementary Figs S6S9).
Conversely, other known between-study differences do not seem to have created outlier studies or clusters of studies as seen in the Z i111 CSV for the assessment of prediction algorithms wise around study cross-study validation . '
'----for &amp; paper , `` '' cross-study validation , ; Varma and Simon,2006; ; cross-study validation cross-study validation cross-study validation ; cross-study validation , Supplemental matrix, such as between studies where all or no patients received hormonal treatment.
We note that incorporation of clinical prog-nostic factors into genomic prognostic models could likely pro-duce gains in CSV accuracy, and that such multi-factor prognostic models could also be assessed by the proposed matrix of CSV statistics.
In practice it is neither possible nor desirable to eliminate all sources of heterogeneity between studies and between patient populations.
The adoption of leave-one-in CSV, in settings where at least two comparable independent datasets are available, can provide more realistic expectations of future prediction model performance, identify outlying studies or clusters of studies, and help to develop generalist prediction algorithms whichwill hope-fully be less prone to fit to dataset-specific characteristics.
Further work is needed to formalize the identification of clusters of com-parable studies, to develop databases for large-scale cross-study assessment of prediction algorithms, and to develop better gen-eralist prediction algorithms.
Appropriate curated genomic data resources are available in Bioconductor (Gentleman et al., 2004) through the curatedCRCData, curatedBladderData and curatedOvarianData (Ganzfried et al., 2013) packages, and in other common cancer types through InSilicoDB (Taminau et al., 2011).
In realms where such curated resources are available, CSV is in practice no more difficult or CPU-consuming than cross-validation, and should become an equally standard tool for assessment of prediction models and algorithms.
ACKNOWLEDGEMENT We wish to thank Benjamin Haibe-Kains for making the curated breast cancer datasets used in this study publicly available.
Funding: German Science Foundation [BO3139/2-2 to A.L.B.].
National Science Foundation [grant number CAREER DBI-1053486 to C.H.
and DMS-1042785 to G.P.
]; National Cancer Institute [grant 5P30 CA006516-46 to G.P.
and 1RC4 CA156551-01 to L.W.
G.P.
and L.T.].
Conflict of interest: none declared.
ABSTRACT Motivation: Software applications for structural similarity searching and clustering of small molecules play an important role in drug discovery and chemical genomics.
Here, we present the first open-source compound mining framework for the popular statistical programming environment R. The integration with a powerful statistical environment maximizes the flexibility, expandability and programmability of the provided analysis functions.
Results: We discuss the algorithms and compound mining utilities provided by the R package ChemmineR.
It contains functions for structural similarity searching, clustering of compound libraries with a wide spectrum of classification algorithms and various utilities for managing complex compound data.
It also offers a wide range of visualization functions for compound clusters and chemical structures.
The package is well integrated with the online ChemMine environment and allows bidirectional communications between the two services.
Availability: ChemmineR is freely available as an R package from the ChemMine project site: http://bioweb.ucr.edu/ChemMineV2/ chemminer Contact: thomas.girke@ucr.edu 1 INTRODUCTION High-throughput screens (HTS) of small molecules for drug discovery and chemical genomics have evolved into routine technologies for analyzing protein functions and cellular networks on a systems biology level.
At the same time, millions of drug-like molecule structures have become freely available to the public, mainly through online compound database projects, such as PubChem, ChemBank, Zinc, ChemMine, ChemDB and others (Chen et al., 2005; Girke et al., 2005; Irwin and Shoichet, 2005; Seiler et al., 2008).
To search and analyze the vast amounts of available compound and screening information, and to assemble diverse screening libraries, efficient compound analysis tools are a critical enabling resource.
Unfortunately, most of the available software in this area is only commercially available, and open-source approaches are still the exception (e.g.
OpenBabel, JOELib, Guha et al., 2006).
The long-term goal of the ChemmineR project is to narrow this resource gap by providing free access to a flexible and expandable open-source framework for the analysis of small molecule data from chemical genomics, agrochemical and drug discovery screens.
To whom correspondence should be addressed.
2 APPROACH 2.1 Overview The development of compound analysis software for the statistical environment and programming language R has many obvious advantages (R Development Core Team, 2008).
To name just a few: (1) R is one of the most widely used data mining environments used in bioinformatics.
(2) The software and its associated packages are available for all common operating systems.
(3) CPU and memory intensive calculations can be computed in high-performance languages, like C. (4) The data objects and base functions available in R are extremely efficient for typical compound and screening data mining routines.
(5) Finally, an unmatched spectrum of data mining resources is available in R, such as extensive graphics utilities, powerful statistical functions, and a wide variety of algorithms for clustering and machine learning tasks.
The current release of the ChemmineR package contains functions for 2D structural similarity comparisons between compounds and similarity searching against compound databases.
Both methods use the highly accurate atom pair approach for scoring structural similarities (Carhart et al., 1985; Chen and Reynolds, 2002).
In addition, the package provides various functions for clustering entire compound libraries and visualizing clustering results and compound structures (Fig.1).
All functions and data objects are well integrated into the existing infrastructure of the R environment.
An overview of the ChemmineR-specific functions is provided in the online instructions and the PDF manual of this project.
2.2 Compound import and descriptor calculation Compound structures are imported into ChemmineR in the generic structure definition file (SDF) format.
Single compounds are imported by providing an SDF with one compound structure, whereas entire compound databases are imported by providing all SDF formatted structures concatenated in one batch file.
The atom pair descriptors are calculated during the SDF import and stored in a searchable descriptor database as a list object (Chen and Reynolds, 2002).
Because the calculation of descriptors for thousands of compounds can be time consuming, functions are provided to efficiently store and reload existing descriptor databases as binary files.
Custom compound databases can be generated with a SDF subsetting function.
2.3 Compound searching, clustering and viewing A search function is available to perform structural similarity searches against the generated atom pair descriptor databases.
The default setting uses the Tanimoto coefficient as similarity measure (Holliday et al., 2003).
The search function can return all entries in a compound database sorted by similarity score.
Alternatively, the search results can be limited by a similarity threshold or a desired number of similar compounds.
To view the compounds structures of search results, the function can automatically upload the returned compounds to the online ChemMine service where they are rendered into chemical structure images (see below).
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:32 17/7/03 Bioinformatics-btn307.tex] Page: 1734 17331734 Y.Cao et al.
0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 0.
4 0 .3 0 .2 0 .1 0 .0 0 .1 0 .2 0 .3 0 .4 0.4 0.2 0.0 0.2 0.4 Fig.1.
Sample 3D scatter plot of a clustering result and online visualization of the corresponding compound structures using ChemmineR.
The compound search function calls internally a generic function for calculating atom pair-based similarities between compound structures (Chen and Reynolds, 2002).
This similarity function can be used to calculate pairwise compound similarities or to design custom subroutines for similarity scoring and searching.
Structure-based clustering is required for many analysis steps of compound libraries and HTS datasets.
ChemmineR provides a novel binning clustering method that is optimized for these compound analysis tasks.
The algorithm uses single-linkage clustering to join compounds into similarity groups, where every member in a cluster shares with at least one another member a similarity value above a user-specified threshold.
The algorithm is optimized for speed and memory efficiency by avoiding the calculation of an all-against-all distance matrix.
This is achieved by calculating on-the-fly only the distance values that are required in each clustering step.
Because an optimum similarity threshold is often unknown, a series of binning clustering result can be calculated simultaneously for several user-specified thresholds.
Cluster results for several thresholds can be calculated almost with the same speed as for a single threshold by issuing multiple clustering processes simultaneously, but calculating the required distances only once.
If desired by the user, then the binning clustering function can generate an all-against-all distance matrix for clustering compound sets with many other classification algorithms available in R, such as hierarchical clustering or K-means.
In addition, ChemmineR provides an interactive wrapper function for multidimensional scaling (MDS) clustering.
The online instructions provide several examples on how to cluster compound sets in ChemmineR with external clustering utilities.
These include examples for using the fully interactive visual data mining tool RGGobi (Lang et al., 2007).
The ChemmineR package provides bidirectional communications with online tools and databases available on the ChemMine portal (Girke et al., 2005).
The service allows users to view and compare any combination of compound structure images in large batches via a standard internet browser along with extensive compound annotation information and custom data tables for basic QSAR analyses (Gedeck et al., 2006).
This includes structure viewing of extensive similarity search results generated by ChemmineR.
All online viewing utilities can be accessed directly from R simply by selecting the online viewing argument in various ChemmineR functions or issuing a dedicated data exchange function.
Uploading compound data to the ChemMine interface gives the user access to many additional tools available on ChemMines online compound analysis WorkBench.
This includes the calculation of physicochemical property descriptors (Guha et al., 2006), inter-conversions between different structure formats (e.g.
SMILES and SDF), searching of the millions of drug-like compounds available in ChemMine, and easy access to published bioactivity and target protein information.
The ChemmineR framework will be expanded in the future by adding many more useful compound and screening data analysis functions.
These include functions for (1) calculating physicochemical properties of compounds directly in R, (2) local similarity searching based on most common substructures (MCS, Raymond et al., 2002), (3) various utilities for QSAR modeling (Gedeck et al., 2006) and (4) wrapper functions for interfacing directly with other open-source small molecule analysis projects, such as OpenBabel and JOELib (Guha et al., 2006; OBoyle et al., 2008).
Extensive user tutorials and download options of different package versions will be available from the ChemmineR project site and from the BioConductor site (Gentleman et al., 2005).
3 DISCUSSION ChemmineR is the first open-access compound mining toolkit for the popular statistical environment R. The package provides flexible functions for powerful structural similarity searches, compound clustering, screening library management and online batch viewing of chemical structures.
Users with a basic understanding of the R environment can easily customize the provided functions and design sophisticated compound library analysis pipelines that utilize the extensive statistical and machine learning resources available in R. ACKNOWLEDGEMENTS Funding: This work was supported by NSF grants: IOB-0420033, IOB-0420152, DGE-0504249, and IIS-0711129.
Conflict of Interest: none declared.
ABSTRACT Motivation: Linking gene mentions in an article to entries of biological databases can facilitate indexing and querying biological literature greatly.
Due to the high ambiguity of gene names, this task is particularly challenging.
Manual annotation for this task is cost expensive, time consuming and labor intensive.
Therefore, providing assistive tools to facilitate the task is of high value.
Results: We developed GeneTUKit, a document-level gene normalization software for full-text articles.
This software employs both local context surrounding gene mentions and global context from the whole full-text document.
It can normalize genes of different species simultaneously.
When participating in BioCreAtIvE III, the system obtained good results among 37 runs: the system was ranked first, fourth and seventh in terms of TAP-20, TAP-10 and TAP-5, respectively on the 507 full-text test articles.
Availability and implementation: The software is available atContact: aihuang@tsinghua.edu.cn Received on October 27, 2010; revised on December 21, 2010; accepted on January 10, 2011 1 INTRODUCTION Gene normalization is one of the most challenging tasks in bio-literature mining due to the high ambiguity of gene names as they may refer to orthologous or entirely different genes, may be named after phenotypes and other biomedical terms, or may resemble common names with non-gene entities (Hakenberg et al., 2008).
It is time consuming and labor intensive to annotate full-text articles manually.
Therefore, a good assistive tool for this task may facilitate the process greatly.
There has been a large body of work addressing the problem of gene mention normalization.
ProMiner (Hanisch et al., 2005), a strict dictionary-based approach, relies on the quality of its gene dictionaries heavily.
Xu et al.
(2007) proposed a method using gene profiles generated from PubMed abstracts for gene disambiguation.
GNAT (Hakenberg et al., 2008) is a rule-based and machine learning (ML) based gene normalization system which used extensive background knowledge.
Built from open-source libraries and publicly available resources, GENO (Wermter et al., 2009) employed a carefully crafted suite of symbolic and statistical methods.
Moara (Neves et al., 2010) is a Java library for extracting and normalizing gene and protein mentions, and currently designed for four model organisms.
Our software departs from previous systems in two aspects: first, it combines local and global contexts to normalize genes at To whom correspondence should be addressed.
document-level.
The goal of this software is not to normalize every mention correctly, but to suggest a list of normalized genes given a target document, to assist human annotators.
Most previous systems are normalizing genes at mention-level and only local context surrounding a mention (e.g.
the sentence where the mention was recognized) were employed.
However, due to the high ambiguity of gene names, it may be insufficient to use only local context: inter-sentential or document-level context can be helpful in this task.
Second, the software is designed for simultaneously normalizing genes of many different species for full-text articles.
It is not limited to any specific organism, but rather deals with all species present in a gene database (Entrez Gene in this article).
2 METHODS AND SYSTEM The workflow of our software is shown in Figure 1.
The software has four main modules.
The first module is for gene mention recognition, the second one for gene ID candidate generation and the third one for gene ID disambiguation.
In the fourth module, the software generates confidence scores for each gene ID, where the confidence score indicates the strength of the association between a gene ID and the document.
We have used three methods for recognizing gene mentions in the first module.
The first method is a conditional random field-based approach, which was trained on the training dataset of BioCreAtIvE II Gene Mention Recognition Task (Smith et al., 2008).
The second method is a dictionary-based recognition approach where the dictionary was compiled from Entrez Gene.
The third method is ABNER (Settles, 2005), an open source named entity recognition system for biomedical literature.
The input text is processed by these methods separately, and the resulting mentions are maintained if a mention is recognized by at least two methods.
If two mentions are similar but have different boundaries, the overlapping part is taken as the final mention.
The second module generates gene ID candidates for a recognized mention.
In this module, an open-source indexing package, Lucene (http://lucene.apache.org/), was used to index all the genes in Entrez Gene.
Each mention was then queried and top 50 gene IDs were returned as candidates.
The text of mentions and Entrez Gene entries were, respectively, Fig.1.
The workflow of GeneTUKit.
Numbers in shaded boxes are gene IDs.
The real-number values in the last box are confidence scores.
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:09 17/3/2011 Bioinformatics-btr042.tex] Page: 1033 10321033 GeneTUKit processed by the following rules sequentially: (i) removing special characters such as dashes and underscores; (ii) removing stop words; (iii) changing words such as hBCL into h BCL; (iv) separating digits, Greek and Roman letters from alphabetic letters; and (v) converting the text to lowercase letters.
The third module is for disambiguating gene IDs, which is accomplished by a ranking algorithm.
The algorithm was trained on the 32 full-text articles provided by BioCreAtIvE III.
Each article has a list of tuples (gene mention, gene id and species); however, the annotations did not give the positions where a gene mention was recognized.
The training samples were generated as follows: for each gene ID candidate, if the ID appears in the manual annotation list, the candidate is taken as positive, otherwise negative.
For each gene ID candidate and its corresponding mention, we extract features from local and global contexts.
Some local context features are as follows: The ranking score of the gene ID given by the Lucene index.
Whether the species of the ID is implied by the gene mention, such as hBCL.
The edit distance between the mention and the official symbol of the ID.
The minimal edit distance between the mention and all synonyms of the ID.
Whether at least one word indicating gene functions of a gene ID appears in the sentences from which the mention was recognized.
The words indicating gene functions are obtained from the corresponding gene symbols after removing common words (such as protein, gene etc.)
and words containing capital letters or digits (e.g.
VDR, p65).
The document-level, global context features are listed partly as follows: Whether the species of the gene ID appears in the document.
Whether the species of the ID appears in the title.
Whether the species of the ID is the nearest species in the same paragraph where the mention is recognized.
If the mention has a full (or abbreviated) name through the document, compute the minimal edit distance between synonyms of the ID and the full (or abbreviated) name of the mention.
In constructing these features, we used dictionary-based matching to recognize species as such a simple method can produce fairly good performance.
For finding full/abbreviated name mappings, we adopted a method from (Schwartz and Hearst, 2003).
Once features were obtained, we used a ranking algorithm ListNet (Cao et al., 2007) to rank gene IDs for each mention and the top one ID was maintained for further processing.
The fourth module generates a confidence score for each predicted gene ID to measure the association of the given gene ID and the document using a support vector machine (SVM) classifier.
The training examples were constructed similarly as in the third module.
The features were constructed as follows: The best value of features used in the third module as each ID may correspond to many mentions.
For the edit distance features, best means minimal; for the ranking score feature, bestmeans maximal.
The total number of gene mentions associated with the ID.
The highest rank of the ID among all the mentions associating with the ID.
3 RESULTS We evaluated the system on the BioCreAtIvE III GN corpus (Lu and Wilbur, 2010) in terms of Threshold Average Precision (TAP-k, k =5,10,20, respectively) (Carroll et al., 2010).
For training, we used the 32 articles with gold-standard human annotation.
For testing, the first dataset has 50 articles, each of which has gold-standard annotation, and the second one has 507 articles whose ground truth was inferred from 37 team submissions (referred Table 1.
The evaluation results on the BioCreAtIvE III GN corpus Measures 50 articles (gold standard) 507 articles (silver standard) TAP-5 0.2973 (4/37) 0.4086 (7/37) TAP-10 0.3125 (4/37) 0.4511 (4/37) TAP-20 0.3248 (4/37) 0.4648 (1/37) Average precision of TOP k recommendations k =5 0.4880 0.5764 k =10 0.4340 0.4993 k =20 0.3231 0.3984 The number in the bracket is the rank of our score among the 37 submissions.
as silver standard).
The 507 articles also include the 50 articles from the first dataset.
The results presented in Table 1 show the official evaluation results from BioCreAtIvE III.
We have also tested the performance in terms of average precision.
The manual error analysis has revealed that two major error types are (i) wrongly recognized gene mentions, and (ii) wrong species mapping.
The Supplementary Material provide a more detailed analysis at http://www.qanswers.net/GeneTUKit/evaluation.html.
4 CONCLUSION GeneTUKit is a software designed for document-level gene normalization, which employs features from the local context and the global context within the whole full-text article.
It can normalize genes of many different species.
Given a target article, the software outputs a list of normalized genes, and each predicted gene is associated with a confidence score.
Funding: Natural Science Foundation of China (No.
60803075); Chinese 973 project (No.
2007CB311003).
Conflicts of Interest: none declared.
ABSTRACT Motivation: The mutation of amino acids often impacts protein function and structure.
Mutations without negative effect sustain evolutionary pressure.
We study a particular aspect of structural robustness with respect to mutations: regular protein secondary structure and natively unstructured (intrinsically disordered) regions.
Is the formation of regular secondary structure an intrinsic feature of amino acid sequences, or is it a feature that is lost upon mutation and is maintained by evolution against the odds?
Similarly, is disorder an intrinsic sequence feature or is it difficult to maintain?
To tackle these questions, we in silico mutated native protein sequences into random sequence-like ensembles and monitored the change in predicted secondary structure and disorder.
Results: We established that by our coarse-grained measures for change, predictions and observations were similar, suggesting that our results were not biased by prediction mistakes.
Changes in secondary structure and disorder predictions were linearly proportional to the change in sequence.
Surprisingly, neither the content nor the length distribution for the predicted secondary structure changed substantially.
Regions with long disorder behaved differently in that significantly fewer such regions were predicted after a few mutation steps.
Our findings suggest that the formation of regular secondary structure is an intrinsic feature of random amino acid sequences, while the formation of long-disordered regions is not an intrinsic feature of proteins with disordered regions.
Put differently, helices and strands appear to be maintained easily by evolution, whereas maintaining disordered regions appears difficult.
Neutral mutations with respect to disorder are therefore very unlikely.
Contact: schaefer@rostlab.org Supplementary Information: Supplementary data are available at Bioinformatics online.
Received on August 4, 2009; revised on December 18, 2009; accepted on January 9, 2010 To whom correspondence should be addressed.
1 INTRODUCTION Random, undirected mutation is a major driving force for change in nature.
In the protein universe, selection is realized through function: mutations leading to loss of function are rarely observed.
As protein structure determines protein function, it is also subjected to evolutionary selection.
Most problematic single nucleotide polymorphisms (SNP) that alter the amino acid sequence (non-synonymous SNPs) appear to impact the stability of protein structure (Yue et al., 2005; Yue et al., 2006).
Helices and strands constitute the major macromolecular building blocks of all well-ordered proteins (Benner et al., 1997; Kabsch and Sander, 1983; Levitt and Chothia, 1976; Morea et al., 1998; Pauling and Corey, 1951a; Pauling and Corey, 1951b).
The particular 3D structure of a protein is assumed to correspond to the global minimum free energy and hence defines the unique fold of an amino acid polymer (Anfinsen and Scheraga, 1975; Dill, 1993; Karplus and Petsko, 1990; Levitt and Warshel, 1975; Liwo et al., 1999; Reva et al., 1995; Sippl, 1993).
Another essential feature of protein structure is the unique interplay between well-ordered and flexible regions (Alexov and Gunner, 1997; Cavasotto and Abagyan, 2004; Claussen et al., 2001; Daniel et al., 2003; Gu et al., 2006; Morea et al., 2000; Radivojac et al., 2004; Schlessinger et al., 2006).
One particular aspect of this interplay is that between what we may loosely refer to as order and disorder (Dunker and Obradovic, 2001; Dunker et al., 2008; Radivojac et al., 2004; Uversky, 2003).
Many proteins have regions that remain unstructured unless bound to a substrate: they do not adopt a unique stable conformation in isolation.
Such regions are also referred to as intrinsically disordered or simply as disordered.
Our operational definition for this vague term is: we consider as disorder whatever is predicted as such.
Proteins with long-disorder regions have unique biophysical traits that enable the binding to different substrates, often at different cellular conditions (Wright and Dyson, 2009).
Very long regions without regular secondary structure (loosely referred to as loops) may resemble disorder (Liu et al., 2002); nevertheless, we can clearly distinguish between disorder-like and well-structured loops (Schlessinger et al., 2007a; Schlessinger et al., 2009).
Disorder is an important building block for the increase in complexity in the evolution from unicellular prokaryotes to multi-cellular eukaryotes.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:00 5/2/2010 Bioinformatics-btq012.tex] Page: 626 625631 C.Schaefer et al.
Our two hypotheses were: (i) we assumed that regular secondary structure is difficult to maintain evolutionarily, i.e.
single residue mutations are likely to impact helices and strands and that we would lose regular secondary structure and transit into loopy polypeptide chains with increasing random mutations away from the native state.
(ii) We assumed, furthermore, that disordered regions provide a means to become robust against mutations because most mutations would rather increase than decrease disorder by increasing the non-regular secondary structure.
Here, we present results that falsify both hypotheses as clearly as possible without investing tens of millions of dollars.
2 METHODS 2.1 Datasets We used protein sequences from two databases for the in silico mutation.
First, we assessed the robustness of secondary structure through globular proteins from the Protein Data Bank (PDB) (Berman et al., 2000).
Secondly, we assessed the robustness of disordered regions through proteins from DisProt (Vucetic et al., 2005) (version 4.9).
We applied UniqueProt (Mika and Rost, 2003) to reduce the redundancy in both sets filtering at a sequence similarity threshold of HVAL>10 (Rost, 1999; Sander and Schneider, 1991) (this corresponds to 30% pairwise sequence identityPIDEfor alignments over 250 residues).
The redundancy-reduced sets comprised 1369 (PDB) and 374 (DisProt) proteins.
For each of the two datasets (PDB and DisProt), we also created random sequences that had the same amino acid composition, same length distribution and same number of sequences as the natives.
The random sets served as convergence control: if we mutate enough to lose all memory (convergence), the random sets will not differ from the mutated sets.
To shed light on potential biases from the chosen databases, we additionally predicted the secondary structure in 33 812 proteins, representing the entire human proteome as taken from RefSeq 2006.
Finally, we sub-sampled a set of sequences from the PDB set with the same size, amino acid and length distribution as that of the DisProt set to examine the ability of ordered proteins to retain or lose their ordered state.
2.2 Mutation protocol We gradually mutated native protein sequences into quasi-random strings of amino acids by the following iterative procedure.
2.2.1 One mutation step It consisted of two moves: (i) select a particular residue position, i.e.
site in the sequence to mutate, and (ii) mutate the amino acid X at that position with amino acid Y with the probability pXY (X=Y).
For technical reasons (lack of CPU because after each step we have to apply several prediction methods), we repeat these two moves N/10 times (N number of residues in the protein).
Effectively, we thereby touch 10% of all residues in one mutation step.
2.2.2 Sixty-nine mutation steps We carried out 69 mutation steps (with 69N/10 mutations) for each protein.
Any other, sufficiently large, number would have worked.
We chose 69 because we had reached convergence in all the cases that we looked at in detail after 65 steps.
Effectively, we applied a Markovian-like model for evolution, i.e.
assuming that each residue mutates independently of all others and that the mutation depends only on the amino acid type.
We applied three alternative substitution schemes: (i) we mutated according to the PAM120 probability (Dayhoff, 1978).
(ii) PAM120 is valid for great evolutionary distance.
In order to also cover closer relations, we also implemented BLOSUM62 (Henikoff and Henikoff, 1992).
(iii) Finally, we took the underlying amino acid distribution in the database (PDB, DisProtordered/disordered regions in DisProt not distinguished) as substitution probabilities.
Note that for the most PAM120 and BLOSUM62 mutations, the most likely mutation step was the maintenance of the current amino acid as the diagonals are typically highest in these matrices.
We did not consider mutations that led to insertions or deletions.
BLOSUM62 and PAM120 behaved identically with respect to our results.
For readability, we confined the BLOSUM62 results to the Supplementary Material.
2.2.3 Single trajectory versus ensemble The mutation path for each native sequence constitutes a single unique trajectory in the space of all possible mutations.
We created five different such single paths (five different mutants) in order to investigate the divergence from the native of an ensemble of evolutionary paths.
From these five, we compiled a consensus by per-residue averaging over each of the five predictions (secondary structure/disorder).
Note that by default, we reported the results for single trajectories and added the ensemble comparison only where explicitly stated.
2.3 Secondary structure We predicted secondary structure through PROFsec (Rost, 2005).
Secondary structure prediction methods improve when using evolutionary information (Liu and Rost, 2001; Rost, 1996; Rost and Sander, 1993).
Without this information, PROFsec reaches a sustained single-sequence level of 68% three-state per-residue accuracy (Q3 is the percentage of residues predicted correctly in one of the three states helix, strand and other).
We had to use this single-sequence mode to monitor the effect of point mutations.
Prediction mistakes might invalidate the generality of our findings.
One way in which we addressed this concern was by monitoring the parameters that we plotted for our mutants also for the experimental observations from the native proteins as taken from DSSP (Kabsch and Sander, 1983) with the usual conversion of eight into three states (Andersen et al., 2002; Rost, 1996; Rost and Sander, 1993).
For each mutation step (i.e.
after each step of 10% change), we monitored the sequence similarity compared with the native sequence, the relative content of residues predicted in helix and strand and the average length of predicted helices and strands.
2.4 Disordered regions We predicted disordered regions by three methods: IUPred (Dosztnyi et al., 2005), MD (Schlessinger et al., 2009) and VSL2 (Obradovic et al., 2005; Peng et al., 2006) and compared the predictions to the experimental annotations in DisProt.
IUPred has three options (long, short and glob); we chose short for short and long for long disorder.
MD (Meta Disorder predictor) combines independent methods through machine learning.
We used it without alignments.
VSL2 is a collection of eight methods.
We used the VSL2B variant that uses only single sequences as input.
The three methods focus on different aspects of disorder and have different strengths and weaknesses.
We did not combine methods and, for simplicity, focused only on IUPred.
The results from the other methods that were crucial to rule out method-specific findings are given in the Supplementary Material.
We chose IUPred because it is accurate, fast and set up to work only with single sequences.
For each mutation step (i.e.
after each step of 10% change), we monitored sequence similarity to native, the relative content of residues predicted in short/long-disordered regions and the length of the regions (SOM).
2.5 Box plots to present results Box plots (McGill et al., 1978; Tukey, 1977) present our results concisely.
The lower and upper box edges depict the first and third quartile, respectively.
The length of a box is the interquartile range of the distribution.
The bold bar inside the box represents the median, while dashed lines reach to the most extreme data point that is no more than 1.5 times the interquartile range away from the upper or lower box edge.
Average (mean) values are connected through solid lines and intersect with box plots.
Median and mean are related to the protein level, i.e.
summarize the specific feature of all sequences that fall within the same interval of PIDE.
626 [15:00 5/2/2010 Bioinformatics-btq012.tex] Page: 627 625631 Robust protein secondary structure under in silico evolution 3 RESULTS AND DISCUSSION 3.1 Secondary structure surprisingly robust Comparisons of pairs of evolutionarily related protein structures reveal two major results (Abagyan and Batalov, 1997; Chothia and Lesk, 1986; Chung and Subbiah, 1996; Sander and Schneider, 1991): first, the less similar their sequences, the less similar their 3D structures [as well as their secondary structures (Rost et al., 1994; Rost et al., 1997)]; and second, the transition from the regime of similar structure to non-similar structure is highly non-linear and characterized by sigmoids indicative of phase transitions in physics.
Our mutation protocol yielded a very different outcome.
Secondary structure diverged to almost random levels over the course of our mutation protocol.
We compared this divergence to what is observed between naturally occurring homologues.
Towards this end, we used the HSSP database (Sander and Schneider, 1991) and compared homologues at the corresponding levels of PIDE (Supplementary Fig.SOM_5).
The change of secondary structure on random mutation was much more dramatic than that for homologous proteins (Fig.1A), e.g.
at 30%, PIDE natural homologues still had levels of Q3 63%, while the random mutants reached Q3 45% (Supplementary Fig.SOM_5).
This result is not surprising: evolution feels the pressure to enrich neutral mutations, i.e.
those that do not alter structure, while no such incentive was built into our in silico mutation protocol.
Nevertheless, secondary structure was surprisingly robust under mutation.
The consensus over ensembles of five different mutation trajectories (Fig.1C and D) diverged much more dramatically from wild type than any single mutant (Fig.1A and B).
Another important difference between our in silico mutation and natural evolution pertained to the shape of the transition: instead of a sigmoidal phase transition, we observed an almost linear transition from native wild-type to almost random mutant.
This was true for both the single trajectory (Fig.1A) and the ensemble (Fig.1C), although the signal was clearer for the ensemble.
We observed that some regions did not alter secondary structure even at the end of our protocol at which the mutant was as similar to the wild type as to any other sequence in our dataset (Fig.1B).
For the ensemble, in contrast, the consensus secondary structure had changed almost completely from the native (Fig.1D).
Nevertheless, the Q3 levels converged to the same level in both cases.
3.2 Helix and strand intrinsic to random sequences Our most surprising finding was that neither the overall content (Fig.2A and B) nor the length (Fig.2C and D) of predicted helices and strands was altered during the course of our mutation protocol.
The average helix content remained 30%, whereas the average strand content around 20%; the average helix was about 10 residues long (23 helix turns), and the average strand extended over about five residues.
In other words, regular secondary structure was predicted to be robust under extreme mutation.
In this respect, we observed no significant difference between choosing mutations according to the background distribution and PAM120, although the latter tends to follow the evolutionarily more accepted mutations (mutations according to BLOSUM62 gave similar results Supplementary Fig.SOM_6).
After the 69 mutation steps (Section 2), we reached a point at which the mutant was as similar to the native as to Fig.1.
Secondary structure changes proportional to sequence.
(A and C) For decreasing pairwise percentage sequence identity (x-axis, PIDE), we monitored the similarity between secondary structure predictions (Q3, i.e.
percentage of residues identical in one of the three states helix, strand and other) for native and for mutant (yellow: mutations according to PAM120, green: according to background distribution, Section 2).
(A and B) show results for a single trajectory, (C and D) the consensus over an ensemble of five trajectories (Section 2).
Box plots reflect the range of the distribution (Section 2); median values are marked by horizontal bars and mean values are connected by dotted lines.
For instance, at 90% pairwise sequence identity, 88% of the residues are predicted in the same secondary structure as the native; for the ensemble, this value is slightly higher (leftmost bars in A and C).
The curves converge nearly linearly towards values 35% corresponding to random.
(B and D) For one particular example (PDB identifier 1a2s chain A), we display the actual secondary structure predictions for each mutant: native on top; each row marks one of the 69 mutation steps (Section 2); mutation by PAM120.
The top (B) is for one single mutation trajectory, the bottom (D) for an ensemble of five trajectories.
One observation stands out and is representative for all such plots that we looked at: blocks of regular secondary appear to be more robust under mutation than the actual type of secondary structure, i.e.
helices flip to strands and vice versa and this happens more often than the transitions helixother and strandother.
Borders are much more fluid for the ensemble (D) than for a single mutation trajectory (B).
any other sequence.
This was reflected by the similarity in the prediction of helix/strand content/length between the final mutant and randomly created sequences (Fig.2: two rightmost bars almost identical).
Our results were based on predictions rather than on observations.
Prediction methods make mistakes.
One might hypothesize that rather than shedding light on protein features, our results are caused by those prediction mistakes.
As no large-scale experiments establish structure for random sequences, we cannot refute this view.
However, we could provide evidence that prediction mistakes might 627 [15:00 5/2/2010 Bioinformatics-btq012.tex] Page: 628 625631 C.Schaefer et al.
Fig.2.
Content and length of regular secondary structure unchanged.
Box plots and coloring as in Figure 1.
Change of regular secondary structure on mutation given by the composition of predicted helix (A) and strand (B), as well as the average lengths of predicted helices (C) and strands (D).
The second and third bar on the left in (A) and (B) compare predictions (light gray) with observations (taken from DSSP, dark gray) for the PDB dataset; the first bar on the left in (A) and (B) indicates the degree to which the predictions differ for the PDB dataset (dark gray) and for a set of all human proteins (light blue).
The right-most green bars mark the predictions for randomly assembled sequences (Section 2, labeled as Comp).
Overall, neither the length nor the content of regular secondary structure appears to differ between native and random.
not matter for the aspects of structure that we monitored.
In fact, by the measures that we used to report our results, predictions and observations were almost identical (Fig.2: left gray bars in each panel).
The precise levels of helix/strand content and length differed indeed more between different datasets (PDB subset versus entire set of human proteins) than between observation and prediction for any set for which we have experimental information.
In other words, prediction mistakes appeared not to matter for all the proteins for which we could verify this statement.
Our findings that random and wild-type sequences were predicted to have similar content of regular secondary structure along with the observation that mistakes in predicting this were negligible suggest that the formation of helices and strands is an intrinsic feature of amino acid sequences.
Neither helices nor strands were predicted to be significantly shortened during our drastic in silico mutation protocol.
Note that this is not a consequence of the fact that PROFsec is trained to predict a particular length distribution, because predicted length distributions deviate substantially between all-helical and coiled-coil proteins.
The maintenance of such regular secondary structure elements would then appear to come at seemingly low costs, i.e.
mutations that are neutral with respect to structure might be more likely than might have been anticipated.
Finally, we verified that the reliability of the predictions did not change during mutation (Supplementary Fig.SOM_10).
3.3 Long regions of disorder sensitive, short not Arguably, there are two different regimes of disorder (Dosztnyi et al., 2005; Liu et al., 2002; Obradovic et al., 2005; Peng et al., 2006; Schlessinger et al., 2007b; Schlessinger et al., 2009): very short and very long regions.
No threshold distinguishes between these two regimes in a biophysically meaningful way.
In particular, there likely exists an intermediate range that might belong to both regimes.
Here, we followed the typical convention in the field and defined as short disorder regions with eight or less consecutive residues and as long disorder regions with 30 or more consecutive residues.
Thereby, we ignored the uncertain regime in between these two extremes.
In order to establish that our results did not crucially depend on the particular threshold, we also tested other thresholds for long disorder, namely 20, 40 and 50.
We found that the trend of loss during in silico mutation is independent of the chosen cut-off and is even clearer for larger thresholds (40 and 50) (Supplementary Fig.SOM_09).
First, we observed that regions of short disorder behaved like regular secondary structure in that their content (Fig.3B, D and F; Supplementary Fig.SOM_2D and E) and length (Supplementary Fig.SOM_2AC) did not alter on mutation.
In stark contrast was the result for long regions with predicted disorder gradually diminished over the course of our mutation protocol (Fig.3A, C and E; by definition a prediction of 29 disordered residues for some mutant implies that for that mutant the long disordered region seemingly disappeared, e.g.
Fig.3E middle; Supplementary Fig.SOM_1).
The loss on mutation was much more dramatic for mutations according to PAM120 (yellow in Fig.3C) than for those according to the background distribution (green in Fig.3C).
This is understandable because disordered regions are abundant in polar residues, and these are more likely to be chosen if mutation probability is skewed toward this abundance.
Put differently, PAM120-driven mutations drifted toward sequences that resembled regular well-structured proteins and as such had no disorder, while background-driven mutations yielded sequences that were as abundant in disorder as the native wild types and therefore had many long regions with predicted disorder.
The actual numbers in terms of content of predicted long disorder decreased from 18% for the native to 9% for the final mutant by using the background mutation protocol (Fig.3C, green).
This reflected the fact that a considerable fraction of the residues in our DisProt dataset was polar: for mutations according to PAM120 (Fig.3C, yellow) or BLOSUM62 (Supplementary Fig.SOM_7), the content dropped to 0.
However, at this level of mutations, almost no single residue predicted as long disorder in the native was predicted as disorder in the mutant (Fig.3A).
For some, this might appear to PAM120.
Studies of particular mutation paths revealed that long disorder might just appear to vanish suddenly (Fig.3E).
This was partially a threshold issue: assume a region with 35 consecutive disordered residues and assume the mutant loses three on each side (six in total); we will no longer consider this as long disorder (356 <30).
This also explains how additional mutations may recover the long disorder (Fig.3E: after solid block of red bars, suddenly one mutant has disorder again as seen by a single bar below this block).
628 [15:00 5/2/2010 Bioinformatics-btq012.tex] Page: 629 625631 Robust protein secondary structure under in silico evolution Fig.3.
Predicted long disorder changes rapidly.
Panels on the left show results for long regions of disorder (30 or more consecutive residues), those on the right for short regions (less than eight).
The top panels (A and B) demonstrate how much the predictions of disorder changed over the course of mutations (y-axis: residues predicted identical as disorder between native and mutant as percentage of disorder predicted in native).
Disorder predictions differ much more rapidly from native than do secondary structure predictions, and much more for long (A) than for short (B) disorder.
The relative content of residues in predicted long (C) and short (D) disordered regions diverge differentially.
The first two box plots for (C) depict the observed (dark gray) and predicted (light gray) disordered content in native sequences.
Right box plots in both (C) and (D) show the disordered situation in the artificially created dataset sequences (Section 2, labeled as Comp).
For a representative example (DisProt identifier: DP 00006), the IUPred predictions for long (E) and short (F) disorder are shown for each mutant: native on top; each row marks 1 of the 69 PAM120 mutation steps (Section 2).
Red lines mark predictions that fall into the threshold category ((30 or more/less than eight).
Long disordered regions disappear (E) while especially short disorder remains at both termini, while re-and disappearing in the middle region during mutation (F).
Another observation reflects one of the important aspects when studying short disorder: a considerable fraction of the short disorder is predicted (and observed) near the protein termini (Fig.3F).
Short disorder comes and goes during mutation (middle region in Fig.3F).
Although this effect is biologically relevant and dominates the study of disorder in otherwise well-ordered proteins (Bordoli et al., 2007; Jin and Dunbrack, 2005), it again underlines the problem of not differentiating between long and short disorder.
Our analyses of regular secondary structure and disorder are based on very different datasets.
PDB is biased in many ways (Liu and Rost, 2001), one of those pertains to disorder (Liu and Deber, 1999; Peng et al., 2004).
One reason simply is that proteins with disordered regions pose extreme challenges to structure determination (Burley et al., 2008; Dunker et al., 2008; Graslund et al., 2008; Liu et al., 2004; Nair et al., 2009; Romier et al., 2006).
To address this difference, we predicted disorder also for the dataset of well-ordered proteins from the PDB.
As expected, the level of both long and short disorder for both of those was very low (Supplementary Figs SOM_3 and 4); given the lack of disorder in these proteins, we could therefore not observe any significant difference between close-to-zero in the wild type and close-to-zero in the mutants.
IUPred is arguably one of the best disorder prediction methods (Bordoli et al., 2007; Le Gall et al., 2007; Schlessinger et al., 2007b; Schlessinger et al., 2009; Shimizu et al., 2007); however, it is still only one of many and it has specific strengths and weaknesses.
Therefore, we also predicted disorder with two other state-of-the-art prediction methods, namely VSL2 (Obradovic et al., 2005; Peng et al., 2006) and MD (Schlessinger et al., 2009).
Although the predictions for those two differed slightly from those for IUPred, by the measures we reported here, they revealed exactly the same trend: while predicted long disorder disappeared on mutation, the content and length distribution of predicted short disorder remained largely unaffected by the mutation.
We addressed the impact of incorrect predictions by randomly introducing errors.
At any significant error rate, long disorder disappeared in the native.
This highlights the high prediction accuracy of todays methods.
For short disorder, the added error did not alter the content over the course of our mutation protocol (Supplementary Fig.SOM_8).
As short and long disorders have different physical traits, we need length thresholds.
However, we can drop these thresholds while monitoring the disappearance of disorder.
Toward this end, we began with all native regions longer than N (chosen in steps of between 20 and 50), and monitored the percentage of disorder predicted after mutation irrespective of the length of the predicted regions.
We found that long disordered regions indeed get decomposed into shorter ones and that disorder disappears throughout (Supplementary Figs SOM_11 and 12).
4 CONCLUSIONS We addressed the general question whether or not well-ordered regular secondary structure and disordered regions sustain random mutations.
Is it likely or unlikely that any mutation affects this particular coarse-grained feature of protein structure (and through its function)?
Do random sequences have different content in secondary structure and disorder than native proteins that have evolved to satisfy many constraints?
Our analysis clearly suggests two different answers for regular secondary structure and long disorder.
On the one hand, the maintenance of regular secondary structure might not be too challenging because its formation appears to be an intrinsic feature of random sequences.
It, therefore, appears surprisingly likely to transit from helix to strand and back.
In fact, this is exactly what we dynamically observed during the course of our mutations (Fig.4).
On the other hand, regions of long disorder do not appear to be robust under mutation.
Random changes likely disrupt this feature that thereby appears volatile and unique.
629 [15:00 5/2/2010 Bioinformatics-btq012.tex] Page: 630 625631 C.Schaefer et al.
Fig.4.
Examples of proteins with mutation trajectories.
For each of the four main SCOP classes (Murzin et al., 1995), we randomly picked one representative short enough to fit into the space here.
Ribbon plots were generated by Chimera (Pettersen et al., 2004) [red: helix, green: strand, according to DSSP (Kabsch and Sander, 1983)].
(AD) In each of the four panels, the ribbon diagram for the native is on the left, and on the right are the 69 mutation trajectories (top: native, degree of mutation decreases downwards; mutations according to PAM120, Section 2).
The sequence runs from the most N-terminal residues (labeled 1) to the most C-terminal ones.
Note that although we show only single trajectories, rather than ensemble averages here, almost no helix or strand withstands the mutation protocol to the end.
This has important impact on how we picture the role of long disorder in proteins: it is not easy to acquire.
Prokaryotes have only 1025% of the disorder observed in multi-cellular eukaryotes (Dunker et al., 2008; Ekman et al., 2005; Liu et al., 2002; Oldfield et al., 2005; Romero et al., 2004; Schlessinger et al., 2009; Ward et al., 2004).
Our observation of how volatile long disorder is provides another evidence for the importance of this feature for the transition from prokaryotes to eukaryotes.
Many SNPs that alter the protein sequence (nsSNPs) appear to be deleterious.
Is this a bias in the experimental technique (more likely to be observed/reported if deleterious), or is it a genuine feature of proteins imposed by the sensitivity of protein structure to mutations?
Although our work neither addresses nor answers this question, the surprising robustness of regular secondary structure might support the view that protein structure is more flexible and adaptable than the intricate details of the concert of interacting residues in protein 3D structures might suggest.
ACKNOWLEDGEMENTS The authors would like to thank the following for valuable discussions: Zsuzsanna Dosztanyi (Etvs Lornd University Budapest and Columbia University in the City of New York), Dietlind Gerloff (UCSC Santa Cruz), Marco Punta (Columbia University in the City of New York and TUM Munich), Reinhard Schneider (EMBL Heidelberg), Anna Tramontano (La Sapienza Rome and KAUST); the anonymous reviewers for very constructive and helpful suggestions that helped shaping this work; and also to all those who deposit their experimental data in public databases and to those who maintain these databases, in particular to those who contribute to PDB and DisProt.
Funding: National Institute of General Medical Sciences (NIGMS) at the National Institutes of Health (NIH) (grant number R01-GM079767).
Conflict of Interest: none declared.
ABSTRACT Motivation: The constraints under which sequence, structure and function coevolve are not fully understood.
Bringing this mutual rela-tionship to light can reveal the molecular basis of binding, catalysis and allostery, thereby identifying function and rationally guiding protein redesign.
Underlying these relationships are the epistatic interactions that occur when the consequences of a mutation to a protein are determined by the genetic background in which it occurs.
Based on prior data, we hypothesize that epistatic forces operate most strongly between residues nearby in the structure, resulting in smooth evolu-tionary importance across the structure.
Methods and Results: We find that when residue scores of evolu-tionary importance are distributed smoothly between nearby residues, functional site prediction accuracy improves.
Accordingly, we designed a novel measure of evolutionary importance that focuses on the interaction between pairs of structurally neighboring residues.
This measure that we term pair-interaction Evolutionary Trace yields greater functional site overlap and better structure-based proteome-wide functional predictions.
Conclusions: Our data show that the structural smoothness of evolutionary importance is a fundamental feature of the coevolution of sequence, structure and function.
Mutations operate on individual residues, but selective pressure depends in part on the extent to which a mutation perturbs interactions with neighboring residues.
In practice, this principle led us to redefine the importance of a residue in terms of the importance of its epistatic interactions with neighbors, yielding better annotation of functional residues, motivating experimental validation of a novel functional site in LexA and refining protein func-tion prediction.
Contact: lichtarge@bcm.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on April 30, 2013; revised on June 28, 2013; accepted on August 15, 2013 1 INTRODUCTION Protein functional sites and their key residue determinants are important to elucidate the molecular details underlying protein function (Laskowski and Thornton, 2008), design drugs (Hardy and Wells, 2004), engineer proteins (Thyme et al., 2009) and predict protein function (Erdin et al., 2010).
The experimental gold standard to map these sites is alanine scanning (Clackson and Wells, 1995; Onrust et al., 1997), but this approach is rarely exhaustive and limited by the availability of biologically relevant assays.
Therefore, complementary, inexpensive and scalable approaches search for functional sites and residues by analyzing the vast evolutionary record of protein sequences computation-ally (Aloy et al., 2001; Buslje et al., 2010; Casari et al., 1995; Engelen et al., 2009; Glaser et al., 2003; Halabi et al., 2009; Innis, 2007; Pupko et al., 2002; Pazos et al., 2006; Valdar, 2002).
The Evolutionary Trace (ET) (Lichtarge et al., 1996; Mihalek et al., 2004) identifies functionally important residue positions by finding sequence substitution patterns correlated with divergences among homologs, thereby explicitly taking phylogenetic relationships into account.
ET predictions have been extensively validated experimentally (Onrust et al., 1997; Rajagopalan et al., 2006; Ribes-Zamora et al., 2007; Rodriguez et al., 2010; Shenoy et al., 2006; Sowa et al., 2000, 2001) and through large-scale retrospective predictions of functional sites (Yao et al., 2003) and protein functions (Venner et al., 2010).
These studies point to a number of general and consistent obser-vations in well-structured protein domains: (i) sequence positions may be ranked by evolutionary importance; (ii) most important sequence residues cluster structurally (Madabushi et al., 2002); (iii) these structural clusters predict functional sites (Yao et al., 2003), such that (iv) small structurefunction motifs called 3D templates based on these clusters can predict protein function on a genomic scale (Erdin et al., 2010; Kristensen et al., 2008; Venner et al., 2010; Ward et al., 2008).
The evolutionary prin-ciples that give rise to these useful patterns remain unclear.
This work suggests that epistasis drives these patterns.
Traditionally, epistasis means interactions between genes; how-ever, it is also recognized as a major force in molecular evolution of individual proteins (Breen et al., 2012).
Strong epistatic inter-actions occur between contact residues (Ortlund et al., 2007), presumably because function and adaptation are intimately related to mutual interaction and variation of physically neigh-boring residues.
Indeed, improving the clustering quality of evolutionarily important residues improves predictions of*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com ) ) ) , ) ) ) ) ) ) ) ) ) ) ) ) )) ) ) 1 2 ) 3 ) 4-Ward etal.
(2008) ) ) but ) ) functional sites (Mihalek et al., 2006a, b; Wilkins et al., 2010).
In that light, the clustering of these residues simply reflects the fun-damental epistatic coupling of neighbors.
These observations motivate a series of hypotheses.
We hypothesize that if epistasis and function constrain residue neigh-bors during selective pressure, then evolutionary importance should distribute smoothly over a protein structure.
If so, opti-mizing ET rank smoothness, for example, by selecting sequences appropriately, should improve predictions of functional sites, molecular determinants and functions.
Thereby, a modified ET algorithm could directly enforce smoothness and improve pre-dictions by focusing primarily on epistatic interactions.
Our results show that, in practice, we can assess ET rank smoothness by treating the structure as a network, or graph, of amino acid nodes, linking these nodes by edges indicating structural contact and applying the discrete Laplacian operator from a graph theory to quantify ET smoothness.
Selections of input sequences that minimize the smoothing function con-structed from the Laplacian operator then led to better func-tional site analyses by ET.
Moreover, a new inherently smoother pair-interaction Evolutionary Trace (piET) algorithm built to measure the importance of neighbor-to-neighbor residue pairs, instead of single residues, improves functional site predic-tions in retrospective study and in an experimental application on Escherichia coli LexAa protein that triggers the SOS re-sponse through which bacteria evolve drug resistance.
Finally, piET improves large-scale functional annotations.
Together, these data show that the smoothest structural distribution of evolutionary importance reflects functional information best, and that epistatic interactions are strongly reflective of the effect-ive distance between residues.
2 METHODS 2.1 Measuring the smoothness of a rank distribution To measure the smoothness of ET ranks over a protein, we treat the structure as a graph.
The nodes of this graph are the residues, and its edges indicate adjacent sequence residues or close contacts in the known structure.
This focus on neighbors is because they will likely experience most strongly the impact of a substitution.
The Laplacian operator (Chung, 1997) is the discrete graph counterpart of the standard Laplacian operator used to measure smoothness in a continuous func-tion, and it is computed with two matrices: the adjacency matrix, denoted A, which specifies which residues contact each other in the protein struc-ture (within a minimum atomatom distance of four Angstroms); and the degree matrix, denoted D, which describes the number of residues adjacent to residue i.
Specifically, A is defined as Ai, j 1 residues i, j in contact 0 otherwise 1 This simple form could eventually be made to account for the number of atomatom contacts, their apparent distances, electrochemical propensities and other attributes of residue neighbor interactions.
The degree matrix, D, is a function of Ai, j Di, j P k Ai, k if i j 0 otherwise 2 The Laplacian operator L is then defined as L D A.
Following standard practice, we may measure the smoothness of any vector field x distributed across the nodes of a graph defined by A through the quadratic form of its Laplacian (Chung, 1997), which is also referred to as the smoothing function, and defined by xTLx X i, j Ai, jxj xi2 3 In this work, the vector field x is the relative evolutionary importance (ET rank) of each residue given by the real-value Evolutionary Trace (rvET) algorithm (Mihalek et al., 2004), which measures the size of a phylogenetic divergence associated with a substitution at each sequence position.
A short review of this algorithm can be found in Supplementary Materials.
By convention, lower values of xTLx indicate smoother distri-butions of the xi over the protein structure, meaning that the difference in ET ranks is smaller between residues that are in contact.
2.2 Functional determinant test set The dataset of functional determinants was taken from a previous work (Wilkins et al., 2010).
The gold standard functional sites for protein ligand interactions are defined by the database PDBsum (Laskowski et al., 2005).
The proteinprotein functional sites are the residues within five Angstroms of the residues in the complexed proteins.
To obtain a multiple sequence alignment (MSA) for each query protein, a set of sequences was retrieved with BLAST (Altschul et al., 1997) (using NCBIs non-redundant protein sequence database, the BLOSUM62 sub-stitution matrix and default parameters).
The top 500 homologs with an e-value better than 0.05 were retrieved from NCBI0s Protein database.
After we generated alignments, the set was curated to remove sequences with sequence identity 526% and length 570% when compared with query.
The homologues were then realigned after curation.
2.3 Measures of overlap and clustering To assess the recovery of known functional sites in proteins, we calculate an overlap z-score zo between top-ET ranked positions and the gold standard functional site, based on the hypergeometric distribution.
We first calculate the mean m and the variance 2 of the hypergeometric distribution m nM N and 2 nMNMN n N2N n 4 where N is defined as the length of the query protein,M is the number of residues that make up the functional site and n is the number of residues that fall under a certain ET rank-coverage.
We then calculate the hyper-geometric z-score zo am , where a is the actual number of functional site residues at a particular ET rank.
Each ET rank can be associated with a distinct z-score.
To access performance at multiple ranks, we developed the overlap measure hzoi, which is the average z-score over ET ranks that fall within a particular coverage range, hzoi 1 K XK i zio 5 Typically, we find that the most useful ET predictions are in the top 20%.
zio is the overlap z-score corresponding to the residues within a certain ET percentile rank i.
The sum is over K unique evolutionary ranks for residues that fall within the top 20% cutoff.
The measure of clustering is calculated in a similar fashion, hzci 1K PK i z i c where z i c are found analytically and have already been discussed at length in Mihalek et al., (2003).
2.4 Sequence selection simulation To test smoothing, 30000 ET analyses ran on randomly constructed MSAs.
Each alignment starts from a default alignment (described in previous section) from which randomly sequences are removed, such that the number of sequences removed was randomly chosen between 2715 Accounting for epistatic interactions improves functional analysis of protein structures ) ) , , the ET .--)--, ) ) )-)-In order ) , to `` '' , In order-, multiple sequence alignment 25 and the total number of sequences in the starting alignment.
The new set of ET ranks leads to unique values of smoothing function xTLx and average overlap z-score hzoi.
The multiple ET analyses are binned based on the value of the smoothing function xTLx.
The average hzoi was then found for the individual bins to evaluate the correlation.
2.5 Residueresidue evolutionary importance To motivate our approach, we reasoned that although mutations operate on individual residues, natural selection filters these mutations based on how they perturb molecular interactions.
Hence, neighboring residues, i, j, should share evolutionary constraints and their importance ranks should be closely related as observed by the clustering of top-ranked residues within proteins (Madabushi et al., 2002) and their mirroring across molecular interfaces (Raviscioni et al., 2005).
If so, we should focus measures of importance directly on molecular interactions rather than on individual residues.
By measuring the evolutionary importance of the link between residues, i, j, we could then infer the importance of i from the average of i, j over all its neighbors j.
In essence, a residues importance throughout evolution would be borne of its epistatic inter-actions with neighboring residues.
To implement this strategy and compute the evolutionary importance of the link between two neighbor-ing residues i, j, we followed an ET strategy.
Residues ranked highly by ET have been shown to knock out (Ribes-Zamora et al., 2007) or swap functions (Rodriguez et al., 2010), while control mutations to poorly ranked residues were neutral.
We can extend this same approach to a pair of residues, where the residue pair i : j is more informative if its sequence variations (among 20 20 400 possible unique states) corres-pond to greater evolutionary tree divergences, i.e.
those that are closer to the tree root.
The piET algorithm therefore applies the standard rvET procedure to pairs of residues within the MSA, to measure these residueresidue patterns in the context of the evolutionary tree.
The evo-lutionary importance of a structural neighbor pair i : j is denoted by i, j where, i, j XN1 n1 1 n Xn g1 n X400 ab1 fgabi, j ln f g abi, j o 6 where fgabi, j is the frequency of the pair of an amino acid ab of a type within group g of the sub-alignment in the n-th set of sub-alignments.
The number of possible nodes in the evolutionary tree is N 1 where N is the number of sequences in the alignment.
The factor 1n was adapted from a previous study (Mihalek et al., 2004) to give weight to the individual sub-alignments based on their location in the phylogenetic tree.
The rvET algorithm couples the phylogenetic tree to the pattern of variation of a pair of residues, viewed as a single evolving unit (Supplementary Fig.S1).
Once the importance of every pair is available, the piET rank i of an individual residue i is calculated by averaging i, j over all its neighbors, i 1 Di, i X j Ai, ji, j 7 As previously defined in Equation (2), Di, i is the number of residues in contact with residue i ( P k Ai, k).
This equation for i factors shared evolution of the contact residues into the ET phylogenetic framework.
2.6 Evolutionary trace annotation To test the piET algorithm in a large-scale application, we substituted it in place of rvET into the Evolutionary Trace Annotation (ETA) algo-rithm and asked whether it improved ETA predictions.
ETA is a suite of programs for automated discovery of protein function based on their structure.
It identifies protein structures that may have identical biochemical functions based on whether they share small structural motifs composed of top-ranked ET residues (Erdin et al., 2010; Kristensen et al., 2008; Ward et al., 2008).
In brief, ETA defines structural motifs by (i) mapping ET ranks onto the surface of a protein structure, (ii) detecting clusters of important amino acids and (iii) selecting six top-ranked amino acids from the cluster.
The geometry of the alpha carbon atoms of these six residues define a 3D template that is then searched for, by geometric similarity, in the protein data bank (PDB) (Berman et al., 2000).
Specificity is enhanced by filtering matches based on evolutionary and structural similarity and ensuring that protein structures match each other reciprocally.
These matches are used to construct a network, as previously described (Venner et al., 2010), in which nodes are protein structures and edges indicate functional similarity, as detected by the ETA algorithm.
We label this network with known functional information and use a diffusion model to control the propagation of those labels through the network, leading to predic-tions of function for protein structures currently lacking function anno-tations.
If using piET instead of rvET causes ETA predictions to improve, it suggests that piET is a more useful metric of evolutionary importance.
2.7 Functional annotation test set The function annotation tests included past query and target sets (Ward et al., 2008; Wilkins et al., 2010).
The query set included 1217 structural genomics enzymes annotated to the third or fourth level of the Enzyme Commission (EC) classification.
The target set is the subset of the 2008PDB90 (Hobohm et al., 1992), which contains 17 234 proteins, which contains 4387 enzymes with four-digit EC annotations.
The com-bination of the query and target sets resulted in a network of 17 952 proteins among which 5105 are annotated as enzymes.
Each protein in the test set was assigned a single enzymatic function.
2.8 Network construction and diffusion Networks were built and predictions followed as previously described (Venner et al., 2010).
Briefly, an ETA template match was converted into a real-valued (edge) weight by averaging the mean evolutionary distance and the rmsd: w 1 rmsd rmsd= rmsd ETScore ETScore=ETScore.
ETA outputs an rmsd and ETScore for each template match.
ETScore summarizes the average difference in evolutionary importance (ET Rank) between matched residues and rmsd is the average distance between the atoms in the structures of the matched templates.
Additionally, rmsd is the average rmsd over all template matches, rmsd is the standard deviation of all rmsds.
Likewise, ETScore is the average ETScore over all template matches and ETScore is the standard deviation of all ETScores.
Graph diffusion passes functional information between proteins that share similar ETA templates (Venner et al., 2010).
We can represent our knowledge of protein enzymatic function as y, a vector of labels repre-senting whether a protein i is associated with a particular EC number (yi).
Diffusion of the available information (in this case EC number) leads to a new label, f. We can solve for f by minimizing the following: f yTf y fTLf 8 In this expression, the first term is the loss function and represents the difference between initial and final labels.
The second term is the smoothness of the new label f in the context of the Laplacian matrix L. The diffusion coefficient balances the loss of the initial labels against the smoothness.
The previous equation has a closed form solution f I L1y 9 where I is the Identity matrix.
The diffusion coefficient is calculated as previously shown (Venner et al., 2010).
2716 A.D.Wilkins et al.-) ) In order ) ) x pair-interaction (piET) multiple sequence alignment in order-) Ward etal.
( 2008 ) ) Erdin etal.
( 2010 ) 1 2 3 ) ) ) ) ) , ) ) s s ) 2.9 Network integration To test for complementary functional information in rvET and piET, the networks were merged into a single network (Tsuda et al., 2005).
We perform diffusion with multiple networks by solving for f I X k kLk1y 10 where Lk represents the Laplacian form of network k. k is weighting factor that represents the importance of each network in the combination.
We can find k by minimizing y TI c P k kLk 1y.
To simplify the minimization problem, we set the additional restriction 1 2 1 (because in this case we have only two networks), and solved using the brute force optimization procedure in the scientific python package (SciPy: Jones, 2001).
We are then able to solve for f for a particular y vector that represents a specific enzymatic function (EC number).
We solve with a different y for each enzymatic function represented in the network, thus associating every protein in the test set with every function.
To compare these values, we normalize to a z-score yi ymean=ystd.
For each protein, the function with the highest z-score is our predicted enzymatic function for that protein, and we use the z-score as a confidence measure in the prediction.
3 RESULTS 3.1 Smoothing the evolutionary importance rank distribution improves functional site predictions To test whether ET rank smoothness correlates with the quality of functional site predictions, we applied the Laplacian operator to the ET rank distributions on 74 diverse proteins bound to various substrates, cofactors, DNA or proteins (see Methods section).
For each protein, a large number of alternative MSAs was randomly generated from a default sequence alignment (Fig.1).
This gave rise to multiple ET rank distributions, each one with its unique smoothness, xTLx and overlap z-score between top-ranked residues and the functional sites annotated in the pdb files (details found in Methods section).
In most cases (81%), the correlation was strong (Fig.1c).
Exceptions included five proteins with inverse correlations (40.4) when the ET clusters identified a functional site other than the one referenced in the pdb file gold standard.
For instance, in the rhodopsin structure [PDBID 1f88], ET found the G-protein interaction determinants instead of the retinal binding site noted in the crystal structure (Berman et al., 2000).
A specialized difference ET analysis would be needed to identify that site, which is specific to visual receptors (Madabushi et al., 2004).
A few pro-teins had small correlation because the functional site prediction was robust and insensitive to the randomization procedure.
Nevertheless, averaging over all 74 proteins, including these anomalies, the smoothest sequence selection improved the smoothing function xTLx by 12.6%; it increased the traditional clustering z-score hzci by 12.9%, and it raised the overlap z-scores hzoi by 8.6%.
In a second sequence simulation experiment (Supplementary Material), we found that the number of sequences had little influence on the correlations and improvement in functional site prediction.
These data show a strong association between improved functional site an-notations and smoother distributions of evolutionary importance rankings.
Fig.1.
To establish that smoother ET ranks are a desirable feature, we showed that smoothness correlated with the quality of functional site prediction.
MSAs of proteins with known functional sites were rando-mized by selecting a random number of sequences and then analyzed with the rvET algorithm.
Every variation in the alignment leads to a new distribution of ET ranks and, in turn, a unique value of the smoothness within the structure (xTLx) and functional site overlap measure (hzoi).
The individual analyses were then binned and counted (black lines) based on the value of xTLx where the average overlap measure (hzoi) for the analyses in each bin was found (green triangle).
Higher hzoi implies better site prediction and lower xTLx implies a smoother distribution of ET ranks over structure.
In both cases there is a steady and strong improve-ment in functional site overlap as smoothness increases, showed by the average overlap z-score hzoi for the corresponding bins in the histogram (green).
(a) In the GTPase Rac structure [PDBID 1e96A, Human] the default MSA (Blue) did not significantly recover the known binding site, whereas the smoother ET ranks from sequence selection did.
(b) By con-trast, in the example the structure for FeS cluster assembly protein sufD [PDBID 1vh4A, E.coli], the default MSA (blue) is already smoother than most of the randomly generated alternatives.
(c) The value of the smooth-ing function xTLx for the random input sequences correlates with func-tional site overlap.
The average correlation over the 74 proteins was 0.65 2717 Accounting for epistatic interactions improves functional analysis of protein structures Integration ) since [ ].
In order In order , multiple sequence alignments ( ) , ) ) , 3.2 New Algorithm identifies functional determinants These results justified a search for an Evolutionary Trace algo-rithm that is inherently smoother, dubbed piET, which was benchmarked and compared with rvET on the same test set used above Section 3.1. piET produced striking gains: 41% better smoothing, evaluated with the quadratic form of the Laplacian rose; 58% better clustering z-scores hzci among top-ranked residues; and 23% better overlap z-score hzoi against known sites.
These functional site prediction improvements were generally consistent across proteins, Figure 2.
Hence, the recovery of functional sites improves significantly with an algo-rithm that measures the importance of residue interactions first, and only deduces the importance of each residue second.
This strategy embodies the notion that smoothness is the byproduct of shared evolutionary constraints among interacting residue neigh-bors.
Its success demonstrates that the phylogenomics of piET brings correlated evolution to light, and that one of its hallmarks is the structural smoothness of evolutionary importance.
To illustrate these gains in a specific example we next turned to Hsp90, a eukaryotic chaperone critical for protein folding and involved in cell cycle regulation, steroid hormone responsiveness and signal transduction among many other processes.
Its func-tions depend on ATP hydrolysis and the crystallized structure [PDBID 1am1] identifies the ATPADP binding sites.
Although rvET identified some residues proximal to this site involved in ATP hydrolysis, piET identifies a much larger evolutionarily important site in that region (Fig.3a), and the overlap z-score hzoi increased more than 2-fold (hzoi 1:91 to 4.39).
In fact, piET also outperforms the rvET optimized by choosing the smoothest outcome after randomization of the sequence input.
In a second example, piET predicted the proteinprotein inter-face for the growth hormone and hormone receptor complex [PDBID 1a22] better.
Although the rvET had picked important residues in this functional region of the growth hormone, the evolutionary important site with piET is better resolved (Fig.3b) and statistically more significant (hzoi 0:625 to 2.51).
In a third example, rvET found the dimer site of the sufD structure [PDBID 1vh4] well, and no randomization of the input sequences could improve this result.
Yet, piET sharply raises the statistical significance of the site (hzoi 6:97 to 8.95), Supplementary Figure S3.
These representative examples show that piET is inherently smoother than rvET, and that this trans-lates into better clustering among top-ranked ET residues and better functional site identification.
3.3 Highlighting functional regions in LexA To demonstrate functional site prediction, piET was next focused on LexA, a well-studied protein that regulates the SOS response to DNA damage in E.coli (Butala et al., 2009).
On direct inter-action with recombinase A (RecA), LexA dimers self-cleave their DNA binding domain and thus lift transcriptional repression of more than 40 genes, including some that mediate error-prone DNA repair and subsequent escape from genotoxic stress (Butala et al., 2009).
The DNA binding and catalytic sites of LexA have been identified but not its RecA interaction site.
Although recently rvET suggested a novel composite LexA bind-ing site on RecA (Adikesavan et al., 2011), no such candidate site is apparent on LexA.
First, piET improved the identification of the known DNA binding site and active site of LexA.
While rvET for the most part does not find a cluster of top-ranked residues at the DNA binding site, except for a few nearby residues (Fig.4a, left panel), piET fully recovers that site (Fig.4a, right panel).
The statistical significance of these predictions (Supplementary Fig.S4) were similar regardless of whether the reference LexA structures was bound to DNA (as in PDBID 3jsp) or not (as in PDBID 1jhh).
Moreover, this improvement is not at the expense of loss of ET signal elsewhere in the protein: piET identifies the catalytic active site even better than rvET (Supplementary Fig.S4).
Thus, pre-viously characterized sites of LexA are better resolved by piET.
Next, we considered a small novel cluster of residues on the LexA structure identified by piET, shown in Figure 4b.
piET ranked these residues as 14% more evolutionarily important (rmsd is 3%) on average than rvET.
They are in immediate con-tact with each other and form a tight cluster, therefore fulfilling a hallmark of a functional site not previously recognized.
Previously, a single E170V mutation at this site proved import-ant for LexA self cleavage (Lin and Little, 1989).
To extend this observation, we performed additional mutations within the piET-identified site neighboring E170.
These mutations dis-turbed LexA function in response to ultraviolet-induced DNA damage, confirming that these residues form a previously unrec-ognized LexA functional site (Fig.4c).
Together these data show that piET pinpoints functional residues and active sites signifi-cantly better than rvET, even in a complex multifunctional pro-tein.
In LexA, this leads to the discovery of a novel functional site, possibly pointing to a binding site for RecA.
3.4 piET improves annotation of enzymatic function To test whether piET also captures functional information on a large scale, we constructed separate function prediction networks with rvETETA and piETETA.
These contained 17 952 pro-teins (nodes), and 115784 and 114 542 ETA matches (edges) in the piET and rvET networks, respectively.
The diffusion model (Venner et al., 2010) predicted enzymatic function and confi-dence scores on a test set of 1070 structural genomics enzymes-2 0 2 4 6 8 10 12 0 10 20 30 40 50 60 70 z o , Fu nc tio na l S ite O ve rl ap  74 Proteins in order of increasing functional overlap New algorithm improve functional site overlap piET Fig.2.
Smoothing the distribution of ET ranks in the protein structure improves the detection of functional residues.
A set of 74 proteins was tested for improvement in functional site detection with the piET algorithm.
The figure shows the consistent improvement in overlap z-score hzoi for the individual proteins in the test set 2718 A.D.Wilkins et al.
to ,-two-very scherichia ) Up-over )-, )--, ) In order UV In order--, , , ) with existing annotations, based on 5105 annotated proteins in the network.
Whenever possible, these predictions were up to the fourth level EC number, which describes not only the chemical reaction but also its substrate.
In this test, the piET algorithm performed slightly better, with a small improvement (Supplemen-tary Fig.S5) in area under the curve (AUCpiET 0:921 compared with AUCrvET 0:914).
To test whether these piET and rvET networks were redun-dant or complementary, we merged them into a single network (Tsuda et al., 2005).
This method creates a weighted combination based on the connectivity of the individual Laplacian matrices and without need for training.
The network mixture coefficients were rvET 0:37 and piET 0:63.
The first incorrect prediction of this combined network occurs at 8.1% coverage, and it is preceded by 86 correct ones (Supplementary Fig.S5).
By con-trast, the first incorrect prediction the rvET or piET networks alone occurred at 30 and 31, respectively.
This is of practical importance, as the high confidence predictions are generally the ones we would act on experimentally.
The individual algo-rithms mix in mistakes sooner, and by merging networks we can reduce mistakes.
At 100% coverage, the merged network method was 4.3% more accurate and the area under the curve improved to AUCrvETpiET 0:945.
Both the rvET and the piET algorithms for detecting evolutionary importance focus the ETA on different but complementary functional sites.
ETA per-formed best when the algorithms were integrated, showing that each algorithm is providing relevant but unique functional information.
4 DISCUSSION This study adds in three significant ways to a long-term effort to identify functional sites.
First, we show that the spatial distribution of evolutionary information (measured here by ET rank) in a folded structure is smooth.
This complements the Fig.3.
Functional site prediction improves with piET algorithm.
The piET algorithm (red) produces a smoother distribution and captures the known functional site better than both the rvET algorithm (blue) and the simulation (green).
(a) The top 10% residues for Hsp90 chaperone [PDBID 1am1] are marked on the protein surface for algorithms, rvET and piET.
The piET algorithm scored more top-ranked residues close to the known proteinligand site with ADP as shown.
(b) The proteinprotein interface of hormone and receptor complex [PDBID 1a22] is better identified with the new algorithm.
The residues ranked in the top 20% for the respective algorithms, piET and rvET, are shown in prismatic color where the residues marked red are the most evolutionarily important residuess Fig.4.
The piET algorithm provides better biological understanding of LexA.
(a) The piET algorithm identifies the DNA binding site of LexA better when compared with the rvET analysis (PDBID 3jsp).
The residues deemed to be in the top 30% are colored based on evolutionary import-ance where red is considered the most important.
(b) piET identifies a novel cluster of residues.
The rvETpiET difference scale is calculated by taking the normalized difference of the rank percentiles.
Residues are marked red (piET) or blue (rvET) when the residue is significantly more important to respective method.
(c) Mutations at this new LexA site disrupt DNA damage survival.
*P50.05, **P50.01 and ***P50.001 2719 Accounting for epistatic interactions improves functional analysis of protein structures to ) since  original notion of ET clusters (Lichtarge et al., 1996) with a mathematically simple interpretation that lends itself to compu-tation via the Laplacian operator of a graph.
This discrete Laplacian operator is fundamental to networks (Chung, 1997), and here it enables optimization in sequence selection better than the diverse measures of clustering used before (Wilkins et al., 2010).
These were entirely empirical and useful to suggest the simplifying notion of smoothness.
We show when we consider the functional linkage between residues, we can better interpret sequence information.
The second improvement builds on this notion of smoothness to develop an algorithm that focuses on a residues interactions with neighbors.
The method first scores the importance of these interactions and then averages over the neighbor interactions to give the total importance of each residue.
This is consistent with prior suggestions (Gutteridge et al., 2003; Raviscioni et al., 2005) that natural selection operates based less on the intrinsic charac-ter of an amino acid, than on the nature of its couplings to other residues, here primarily those in its immediate surrounding.
Previous studies have noted improvement in predictions when they average evolutionary information for residues over sequence (Capra and Singh, 2007; Pei and Grishin, 2001) and structure (Panchenko et al., 2004; Teppa et al., 2012).
We add to this work by quantifying the shared evolutionary pattern be-tween residues near in structure.
These interactions are the essence of the residues function.
Though the method is currently limited to structural information, we can use the constantly im-proving homology-modeling algorithms (Roy et al., 2010) or databases of pre-computed homology models (Bordoli and Schwede, 2012).
Third, we show how these results follow logically from epistatic interaction among residues.
Other methods focused on pairwise interactions via covariation (Pazos and Valencia, 2008), thermo-dynamic (Maksay, 2011) or energetic coupling (de la Lande et al., 2010).
Networks of such correlations often lead to clusters of pathways although their interpretation is not straightforward (Chi et al., 2008).
By contrast, clusters of ET residues lead to functional sites shown independently to be highly significant com-pared with other methods [see Supplementary Materials in Rausell et al.
2010, and extensively tested experimentally in a large variety of proteins (Lichtarge and Wilkins, 2010)].
These validations included mapping and then recoding of allosteric determinants of both interprotein and intraprotein signaling path-ways (Rodriguez et al., 2010).
In summary, this work finds and exploits the fact that epistatic forces mold evolution and, as a result, leads to the smooth dis-tribution of evolutionary importance throughout protein struc-tures.
This smoothness stems from the functional linkage of residues typically nearby in conformation, a hallmark of epista-sis.
This basic property leads to new algorithms for computing the evolutionary importance of (a) residueresidue interaction among neighbors, and (b) individual residues.
In turn, this sub-stantially improves functional site analysis and function predic-tion in test sets while also verified experimentally by predicting a novel site in LexA.
This should prove useful in guiding protein engineering and mutations to the most relevant parts of a protein.
A server performing piET calculations is available at our site: http://mammoth.bcm.tmc.edu/uet.
ACKNOWLEDGEMENT The authors thank Panagiotis Katsonis, Andreas M. Lisewski and Ilya Novikov for helpful discussion.
Funding: National Institutes of Health (NIH-GM079656, NIH-GM066099, NLM 5T15LM07093); National Science Foundation (NSF CCF-0905536, NSF DBI-1062455).
Conflict of Interest: none declared.
ABSTRACT Summary: The increasing availability of high-throughput sequencing technologies has led to thousands of human genomes having been sequenced in the past years.
Efforts such as the 1000 Genomes Project further add to the availability of human genome variation data.
However, to date, there is no method that can map reads of a newly sequenced human genome to a large collection of genomes.
Instead, methods rely on aligning reads to a single reference genome.
This leads to inherent biases and lower accuracy.
To tackle this prob-lem, a new alignment tool BWBBLE is introduced in this article.
We (i) introduce a new compressed representation of a collection of gen-omes, which explicitly tackles the genomic variation observed at every position, and (ii) design a new alignment algorithm based on the BurrowsWheeler transform that maps short reads from a newly sequenced genome to an arbitrary collection of two or more (up to millions of) genomes with high accuracy and no inherent bias to one specific genome.
Availability: http://viq854.github.com/bwbble.
Contact: serafim@cs.stanford.edu 1 INTRODUCTION Advancements in next-generation low-cost, high-throughput DNA sequencing technologies have made it possible to sequence a large number of human and other species genomes (Cherf et al., 2012; Stein, 2010).
Several large-scale sequencing efforts are under way, including the 1000 Genomes Project (The 1000 Genomes Project Consortium, 2010) and the International Cancer Genome Project (International Cancer Genome Consortium, 2013).
More than 2000 individuals have already been sequenced by the 1000 Genomes Project.
Although the next-generation sequencing technologies provide a vast amount of data samples for advancing genomic research, the ever-increasing volume of genomic data has become a tremendous challenge on multiple fronts (Durbin, 2009; Fritz et al., 2011; Kozanitis et al., 2010).
One challenge is the alignment of the short DNA sequences (short reads) produced by these technol-ogies to reference genomes, to discover the variation of a newly sequenced genome with respect to the previously sequenced human genomes.
Short read alignment is a common first step during genomic data analysis and plays a critical role in medical and population genetics.
Several efficient short-read alignment programs, such as BWA, SOAP2 and Bowtie (Langmead et al., 2009b; Li and Durbin, 2009; Li et al., 2009), have been developed in the past years (Li and Homer, 2010).
These methods use the Burrows Wheeler transform (BWT) (Burrows and Wheeler, 1994), which facilitates linear-time alignment to a large reference string (Ferragina and Manzini, 2000), such as a genome, and requires only a limited amount of memory (Hon et al., 2007; Nong et al., 2011).
These aligners typically precalculate the BWT (and some associated auxiliary structures) of a single reference genome and then map the newly sequenced reads to it using a variant of the BWT backwards search procedure.
Any observed differences from the chosen reference genome are treated as novel genomic variants or sequencing errors (e.g.
Li and Durbin, 2009).
In light of the increasing availability of thousands of sequenced human genomes and databases of human genome variation, the require-ment of these methods to use a single human reference genome leads to inherent biases towards the arbitrarily chosen reference.
Biased alignment output can be a hindrance to the in-depth and comprehensive understanding of the cancer genome (Lee et al., 2010; Roach et al., 2010), evolutionary history (Kumar et al., 2004), Mendelian diseases (Ng et al., 2010) and numerous other domain applications.
Beyond human, to date, millions of genomic variants have been observed in a wide variety of species and can serve as a valuable resource for alignment.
For example, many inbred mouse strains have been sequenced to characterize their genomic variation (Keane et al., 2011).
Two such mouse strains can have up to 20 million differences between them.
As a result, when they are crossed, their F1 offspring are heterozygous at all loci that differ between the parents.
Aligning reads from the F1 offspring to either parents genome using current alignment techniques will ignore this known heterozygosity and result in alignment bias towards the chosen reference strain.
This can make it difficult to perform an experiment like RNA-seq, which relies on high quality mappings.
The limitations of using conventional references and aligners are even more pronounced when sequencing organisms with a large amount of genomic variation among individuals.
For ex-ample, the sea squirt, Ciona savignyi, has a polymorphism rate of 4.5% (Rumble et al., 2009).
With such a high polymorphism rate, the genome of any newly sequenced individual of this spe-cies will be very different from a single conventional reference sequence.
Current short-read aligners operating on a single ref-erence genome are simply not designed to handle this scenario and will result in poor alignment accuracy.
Although humans have a lower polymorphism rate, the refer-ence bias remains an important problem.
Even though individual alignment results may be only slightly biased when aligning with conventional techniques, in a large study involving thousands of individuals, this effect can add up to distort the studys conclusion.
A vast number of genomic variants in human populations have already been discovered and are maintained in publicly ac-cessible databases, such as dbSNP and SNPedia.
The majority of variations between a newly sequenced genome and the reference are likely to already exist in these databases.
Moreover, struc-tural variants, such as transpositions, large insertions and *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com deletions and inversions, are difficult to analyse and require split-read alignment techniques (Pang et al., 2010; Snyder et al., 2010).
Once a structural variant is detected, it would be desirable to avoid repeating the process of identifying the same variant in other genomes.
With this issue in mind, we are facing the chal-lenging problem of how to efficiently incorporate the available genomic variant information during alignment.
The naive solution to this problem is to use the existing fast aligners to map new read samples to all available genomes indi-vidually.
The state-of-the-art aligners, albeit much faster than their predecessors, still take 35 h to map 12.2 million reads to a human genome on a single core processor (Li and Durbin, 2009), so spending 2000 more time (roughly a year) to map those reads to all the genomes currently in the 1000 Genomes Project is not a realistic solution, especially as this time will scale linearly with the number of genomes in the collection.
Furthermore, even though the space required to store the BWT-related data structures of each genome can be significantly reduced using various compression techniques (Hon et al., 2007), the total amount of space required for storing all the available genome data is considerable.
Finally, this solution does not take into account the scenario where the newly sequenced genome has a novel haplotype of several nearby SNPs (i.e.
when the newly sequenced genome contains a combination of SNPs that do not occur together in any known genome individually).
An alternative solution is to align the newly sequenced reads to a single reference genome and then query the genomic variation databases to analyze the mismatches.
This approach is used in programs such as Crossbow (Langmead et al., 2009a), VarScan (Koboldt et al., 2009), and others (Handsaker et al., 2011; Mokry et al., 2010).
However, if the reads are misaligned during the first step (e.g.
reads spanning a mutation), incorrect mismatch loca-tions will be propagated to the second step, which can bias the study results and lead to questionable conclusions, especially for a large study involving thousands of genomes (DePristo et al., 2011).
Taking advantage of the high redundancy among sequenced genomes, several techniques have been proposed for compressing genomic data and searching this compressed data directly (Lam et al., 2010; Loh et al., 2012; Makinen et al., 2009; Schneeberger et al., 2009; Siren et al., 2011).
Loh et al.
(2012) provides the first efficient scheme for compressing genomic libraries and presents compression-accelerated BLAST and BLAT algorithms that search over the compressed data.
In particular, their data com-pression scheme consists of finding fragments (by default, 300bp long) in the database that are highly similar, keeping only one version of the sequence fragment, and replacing each additional fragment with a link to this sequence and a list of differences.
The first approach for mapping short reads against a collec-tion of genomes simultaneously is presented by Schneeberger et al.
(2009).
Their algorithm, GenomeMapper, combines the genomes into a joint hash-based graph data structure.
More spe-cifically, it builds an index of all available reference genomes that maps sequence k-mers (513bp long) to their positions in the genomes.
Identical regions are stored only once, and poly-morphic regions are stored separately for each genome (these are represented as branches in the index).
During alignment, the hash-based index is first scanned to identify the exact matches, then nearly identical maximal substrings are detected, and finally, bounded dynamic programming is used to extend the partial alignments.
Although this technique proved to be success-ful for aligning with the small Arabidopsis Col-0 genome, its high memory requirement makes its applicability to a human genome still questionable (Li and Homer, 2010).
Siren et al.
(2011) proposed a novel index structure for a col-lection of genomes built by converting the genome multiple alignment into a prefix-sorted finite automaton that can recog-nize the strings corresponding to all the paths through the mul-tiple alignment.
This work generalizes the BWT-based index structure for labelled trees to labelled graphs and uses a modifi-cation of the backwards search algorithm to perform read map-ping.
The technique was demonstrated on the multiple alignment of four assemblies of the human chromosome 18 and is expected to support genomes of up to 100 Mbp using a single workstation.
Owing to its high memory consumption during prefix-sorted au-tomaton construction, an external memory implementation is needed to index a collection of human genomes, which is not yet available.
Therefore, the applicability of this method to a large collections of human genomes has not yet been demon-strated in practice.
In this article, we present BWBBLE, an aligner that handles genetic variants and thus avoids the inherent bias induced by mapping to a specific genome, while providing reasonable com-putation time and limited memory consumption for large collec-tions of genomes.
The main new contributions of our work are the following: (i) we introduce the concept of a linear reference multi-genome that incorporates the catalogue of all known gen-omic variants with a reference genome (e.g.
SNPs, insertions, deletions and inversions), and (ii) we develop a BWT-based read alignment algorithm, BWBBLE, that accurately maps reads to this multi-genome.
We evaluate the effectiveness and efficiency of BWBBLE in Section 4 with a set of experiments using simulated and real read data.
2 BACKGROUND: BURROWSWHEELER TRANSFORM The BWT (Burrows and Wheeler, 1994) of a given string is a reversible permutation of the string symbols that enables the search for a pattern P in the original string to take OjPj time (i.e.
linear time with respect to the length of the pattern).
Let S s0s1::: sn1 be a string of length n defined over some alphabet (e.g.
the A/C/G/T nucleotide alphabet if S is a genome) and let $ be a symbol not in that is lexicographically smaller than all the characters of .
When constructing the BWT of S, $ is first appended at the end of S, such that S s0s1:::sn, where sn $ (now jSj n 1).
Let SA be the suffix array of S (we have jSAj jSj), such that SAi stores the position of the i-th lexicographically smallest suffix of S (e.g.
SA0 n since $ is the smallest suffix).
A simple technique for constructing the BWT and SA is demonstrated in Figure 1.
It can be shown that BWTi SSAi 1 when SAi 6 0 (and $ otherwise).
If a pattern P does occur in S, then each of its occurrences will appear at the start of some suffix of S, and these suffixes will be grouped together into a single suffix array interval SALP,UP, where LP and UP represent the indices of the lexicographically smallest and largest suffix starting with P, i362 L.Huang et al.
respectively.
Given the SA interval LP,UP, the positions of P in S can be obtained from the corresponding SA values, namely, P will occur in S at all SAi, for LP i UP.
Ferragina and Manzini (2000) showed that if P occurs in S, then: LP C O,LP 1 1 1 UP C O,UP 2 where C is the number of symbols in S (not counting $) that are lexicographically smaller than , and O, i is the number of occurrences of in BWT0, i.
Therefore, to find the SA interval of P, we can start with the SA interval of an empty string (L 0,U n) and add a character of P at a time in reverse order (i.e.
starting with the last character of P).
Assuming that the C and O arrays have been pre-computed, this technique, known as backwards search, enables us to find the interval of all the occurrences of P in S in OjPj time.
3 MULTI-GENOME ALIGNMENT 3.1 Reference multi-genome In this section, we describe how a single reference genome can be augmented with genomic variant information gathered from an arbitrary collection of genomes.
We refer to the augmented ref-erence as the reference multi-genome.
We start by handling SNPs and then refine the proposed reference representation with all other possible variations (e.g.
insertions, deletions, inversions).
3.1.1 SNPs To handle SNPs during alignment, we extend the reference genome alphabet from the 4-letter A/C/T/G nucleotide code to the 16-letter IUPAC nucleotide code (Cornish-Bowden, 1985).
The IUPAC encoding makes it possible to capture which nucleotides have been observed at a given position across all the available sequenced genomes.
For example, if both A and G were discovered at some given position, then the IUPAC char-acter R can be used to represent this variation.
For convenience, in our IUPAC alphabet, we have replaced the IUPAC character U by the special character # that represents the absence of the four nucleotides.
As a result, when aligning a read to the reference multi-genome, a given read nucleotide can match more than one character of the reference.
For example, read base A can match the IUPAC characters A, D, H, M, N, R, V and W. Therefore, the substrings that a read can map to in the reference might be grouped into multiple separate SA intervals.
In particu-lar, if the suffixes are sorted in IUPAC lexicographic order (Fig.2), then the substrings matching A can fall into up to five separate SA intervals: suffixes starting with A, D, H, [M N R], and [V W] can be separated by suffixes starting with [B C], G, K, and [S T], respectively.
To minimize the number of separate SA intervals associated with each of the four nucleotides, we propose to use the four-bit Gray code (also known as the reflected binary code) (Gray, 1953) to order the IUPAC characters (instead of ordering them lexico-graphically).
The four-bit Gray code orders four-bit binary values such that two successive values differ by only one bit.
For this purpose, we encode each IUPAC character using four bits, such that each bit corresponds to a given nucleotide and is set to 1 if the IUPAC character matches this nucleotide and 0 otherwise.
For example, given the nucleotide-to-bit assignment bAbCbGbT, the Gray code value of 1001 corresponds to the IUPAC character W that represents both A and T. Figure 3 shows the IUPAC character order resulting from using this nu-cleotide-to-bit assignment.
We can see that given this new order-ing, the IUPAC characters matching A will all fall into a single SA interval (as they are now ordered consecutively).
The number of SA intervals per nucleotide will be the following: A !
1 interval, C !
1 interval, G !
2 intervals, T !
4 intervals.
Because A and T occur more frequently in the human genome, we expect the nucleotide-to-bit assignment bAbTbCbG to result in better performance.
The nucleotide-to-bit assignment can be easily adapted to a specific genome during indexing to prioritize the more frequent nucleotides (e.g.
bGbCbAbT should be used for prokaryotes with a high GC content).
It can be theoretically shown that the Gray code order is the optimal order for this problem (see Appendix).
Figure 4 shows an example of con-structing the BWT and SA of a toy multi-genome using the Gray code order.
3.1.2 Extension to other genomic variations In addition to SNPs, other types of genomic variations are common within and across populations.
Such variations include insertions, dele-tions, inversions and translocations.
If we superimpose the genomes of a given collection, we can collapse the matching nucleotides and encode SNPs with IUPAC characters (as described in Section 3.1.1).
The remaining varying-length segments (caused by other types of genomic variations) will visually form a set of bubbles composed of multiple branches, where each branch represents a variant of the genome sequence that was observed at that position in at least one of the genomes in the collection.
Figure 5 shows the result of superimposing three sample genomes where the SNPs have been encoded with M and K and the indels form a bubble with three branches.
We incorporate the indels into the reference multi-genome as follows.
One of the bubble branches is designated as the primary branch and included into the reference at the position at which the bubble occurs.
All other bubble branches are appended at the end of the reference genome.
Each appended branch is padded at both ends with the bases surrounding the bubble.
The length of the padding is a parameter that depends on the expected read Fig.1.
BWT and SA construction for Smamaliga$.
All the rotations of S are first listed (1) and then sorted in lexicographic order (2).
The BWT string is assembled from the last character of each sorted rotation (i.e.
the right-most column of the sorted rotations matrix) and the suffix array SA is given by the original position in S of the suffix at the start of each sorted rotation (the substring preceding $ in each rotation) i363 Short read alignment with populations of genomes length, jRj, and is set to jRj 1.
The reference multi-genome augmented with indel information for the example in Figure 5 is shown in Figure 6 for reads of length jRj 4 (padding of length three).
The special character # separates the reference se-quence and the bubble branches to prevent reads from being aligned across the sequence and bubble branch boundaries.
In the case of inversions, translocations and duplications, we can optionally avoid having the bubble branch length increase linearly with the size of the event, jEj, as follows.
We create two branches for the two ends of the structural event of length 2jRj 1 centred around the two event boundaries.
Reads that span across the original and the variant sequence boundaries will now map to these two branches.
We do not include the event sequence interval jRj, jEj jRj, which is already present in either the forward or the reverse complement strand of the ref-erence genome (depending on the type of the structural event).
While saving space, the downside of this approach is the fact that reads from the original and the duplicate sequences will be mapped to the original sequence only, which complicates the assessment of the quality of the read mappings (e.g.
a read should not be considered confidently mapped if it maps equally well to multiple positions), and other techniques have to be used to detect duplicated regions.
These structural events were not included in our experiments, as they are not present in the Integrated Variant Set release (The 1000 Genomes Project Consortium, 2010) we used to construct the reference multi-genome.
Note that although augmenting the reference with indels and other structural variants allows us to easily handle these events during alignment, it does lead to an increase in the size of the reference string due to padding and, therefore, a higher memory overhead.
However, by filtering out some rare branches, it is possible to trade-off some accuracy for a lower memory consumption.
3.2 Exact matching In this section, we present the algorithm for exactly matching a read to the reference multi-genome.
This algorithm is an exten-sion of the BWT-based backwards search algorithm presented in Section 2.
Let represent one of the four A/C/T/G read bases and let be the subset of the IUPAC characters that can match with .
We have: A fM,H,N,V,R,D,W,Ag, C fS, B,Y,C, M,H,N,Vg, G fK,G, S, B,N,V,R,Dg and T fT,K, B,Y,H,N,D,Wg.
Also, let represent an element of the set .
Because each read base can match more than one IUPAC character, a given read R can match multiple different substrings in the reference multi-genome (e.g.
RAT will match the sub-strings AT, RW, AY and others) and, therefore, can align to more than one SA interval.
Let hLR,URi represent the set of SA intervals that start with a substring that matches the read R. If R occurs in the reference, then it can be easily shown that: hLR,URi [ 82 LR,UR 3 where as before, LR C O,LR 1 1 4 UR C O,UR 5 This result enables the iterative backward search for a read in the reference multi-genome.
That is, we can start (as before) with the SA interval of an empty string, hL,Ui 0, n, and then iteratively fetch a base from the end of the read re-calculating the SA interval set hLR,URi using Equations (3)(5) (here R refers to the partially assembled read).
This procedure can be repeated until R equals the entire read.
Note that if LR4UR, the SA interval is not valid and is discarded.
For clarity, we demonstrate the alignment of the read AT with the reference multi-genome RWYAYA (the SA and BWT for Fig.4.
BWT and SA construction of a toy reference multi-genome SRWYAYA$ (i.e.
(AjG)(AjT)(CjT)A(CjT)A).
All the rotations of S are first listed (1) and then sorted in Gray code order (2) Fig.3.
Gray code order of the IUPAC code using the bAbCbGbT nucleo-tide-to-bit assignment Fig.2.
Lexicographic order of the IUPAC code Fig.6.
Reference multi-genome with various variations.
Branch padding corresponds to reads of length jRj 4 Fig.5.
Bubble formed by superimposing three sample genomes i364 L.Huang et al.
this reference were computed in Fig.4).
We start with the SA interval of the empty string, which is 0, 6.
In the first iteration, we consider the last read base T. We calculate LT and UT for each of the eight characters in T. Using Equations (4) and (5), we get LY,UY 1, 2, LW,UW 4, 4 and no matches for the remaining characters.
Therefore, hLT,UTi 1, 2 [ 4, 4.
In the next iteration, we add A (the first base of the read) and calculate LAT and UAT for all A 2 A.
The final result hLAT,UATi 3, 4 [ 6, 6 can be easily verified.
To achieve reasonable performance, the number of SA inter-vals has to remain relatively small during alignment.
An expo-nential increase in the number of SA intervals with respect to the length of the read is unacceptable.
We have conducted a small experiment to track the number of SA intervals created during exact alignment with a human multi-genome.
More specifically, we have created a reference multi-genome by combining the SNPs of 1092 individuals from the 1000 Genomes Project (The 1000 Genomes Project Consortium, 2010) with a popular refer-ence genome (build GRCh37) and then aligned to it 10-K simu-lated reads of length 100 uniformly sampled from the reference multi-genome.
Figure 7 illustrates the average and standard de-viation of the number of SA intervals created in each iteration.
As we can see, this value reaches its peak within the first 10 iterations and then dramatically drops to a small number.
In other words, the majority of SA intervals are created in the first 1214 iterations of the backwards search.
Precalculating the SA intervals for all 1214-base pair-long substrings can boost the performance.
This speedup technique is discussed later in the article.
3.3 Inexact matching To tolerate sequencing errors and other variations of the reads from the reference multi-genome, we have extended the exact-matching backwards search algorithm to allow mismatches and gaps.
Figure 8 shows the high-level pseudo-code of the inexact matching algorithm used for aligning a given read R with the reference multi-genome G with up to n differences (mismatches or gaps).
This algorithm is an extension of the inexact search algorithm used by BWA (Li and Durbin, 2009) and is guaranteed to find all the alignments with up to n differences.
To handle mismatches and deletions in the read, we consider all the IUPAC characters of the reference multi-genome alphabet (except #) in-stead of just the set of characters that a given read base matches to exactly.
After computing the new SA intervals for each of these characters, we advance the read position for matches/mis-matches but not for deletions.
To handle insertions, we just skip a given read position without recomputing the SA intervals.
We can easily check whether a read base matches a given character by computing the binary AND(&) of the read base and the char-acter Gray code values.
The ambiguous base N is considered a mismatch (not shown for simplicity).
By skipping #, we are able to avoid mapping across reference sub-sequence boundaries (e.g.
chromosome and bubble branch boundaries) since, when the reference multi-genome is assembled into one string, all the sub-sequences are separated by # (current aligners operate on four-letter A/C/T/G alphabets and need to discard such alignments during post-processing).
We have adapted several heuristics from the existing BWA aligner (Li and Durbin, 2009) for reducing the search space and improving the performance of the alignment algorithm.
In particular, given a read R, we compute a lower bound array, D, where Di is the lower bound on the number of differences of matching the substring R0, i with the reference.
This lower bound is computed in OjRj time as described in (Li and Durbin, 2009).
By replacing the first condition of the InexactMatch procedure with n5Di, we can terminate the search earlier, if Di40.
To reduce the search space further, this algorithm has also been modified to prune out alignments that are considered sub-optimal even though they might contain less than the maximum number of allowed differences.
Similar to Fig.8.
Inexact matching algorithm.
Returns the set of SA intervals of substrings of G matching R with up to n differences.
All the auxiliary BWT structures are assumed to have been already pre-computed for G Fig.7.
The average and standard deviation of the number of SA intervals generated per iteration during simulated read alignment on the reference multi-genome (averaged over 10K reads) i365 Short read alignment with populations of genomes BWA, we use a minimum priority heap-like structure of partial alignments based on alignment score (instead of using recursion) that allows us to process the entries with the best score first.
Depending on how many best hits are found and what is the best found difference, the search parameters are dynamically ad-justed and the search can be terminated early.
In particular, no sub-optimal alignments are explored if the number of best hits exceeds a given threshold.
Furthermore, if the number of differ-ences in the best hit, nbest, is less than n, then n is reset to nbest 1.
It is also possible to limit how many differences are allowed in the seed sequence (the first k base pairs at the beginning of the read, where k is the seed length and can be adjusted) and disallow insertions and deletions at the ends of the read.
Other search parameters available to the users of BWA (e.g.
mismatch, gap open and gap extension penalties) have also been incorporated to provide a similar user interface and improve the efficiency of the BWBBLE implementation.
Because the first 1214 iterations account for a significant share of the computation time (see Fig.7), we provide an option to precalculate the SA intervals resulting from exactly matching all possible k-length strings (k 12 by default) with the given reference multi-genome.
The inexact alignment process can then start at position jRj k of the read by looking up the result of mapping the last k read base pairs.
Similar to the con-struction of the BWT structures, this computation is a one-time effort done prior to the alignment stage.
We found the alignment to be six times faster with this setting when given 10-K 125-bp simulated reads and n 6.
In terms of alignment results, this setting is equivalent to setting the seed length to k and allowing no differences in the seed, which does lead to a considerable decrease in the confidence and accuracy of the results depending on k. 3.4 Memory consumption To reduce the memory requirement of the BWBBLE program, we compress the BWT string (using four bits to represent each of the 16 IUPAC characters) and only store a sampled subset of the auxiliary O and SA arrays, calculating the intermediate values as needed from the BWT string.
More specifically, we only store the values O, i and SAi where i is a multiple of the predefined interval sizes OCC_INTERVAL (default 128) and SA_INTERVAL (default 32), respectively.
To obtain the Oc, j value for a j that was not stored, we re-compute the number of times c occurs in the compressed BWT string after the closest available position.
To calculate SAj that was not stored, we use the following relation between the suffix array SA and the in-verse compressed suffix array 1 (Grossi and Vitter, 2000): SAj SA1kj k, where 1i CBWTi OBWTi, i and 1k means applying 1 k times.
Therefore, we can repeatedly apply 1 until we obtain a position for which the SA value has been stored.
A similar memory reduction technique is also used by other aligners, such as BWA, SOAP2 and Bowtie.
However, owing to the larger alphabet size of the reference multi-genome (16 IUPAC characters as opposed to four nucleotides), the memory requirement of the BWBBLE program is higher than that of the existing aligners.
Because these aligners only operate on four nucleotides A/C/G/T, they only need two bits to store each character of the BWT string and only need to record the occurrence of four different characters at every position of the O, array.
Therefore, given a genome of size n, they only need 2 n bits to store the BWT string and 4n log2 n bits to store the entire O array (for simplicity, we do not consider the reverse complement reference and the sampling of the O array here).
On the other hand, BWBBLE needs 4 n bits to store the BWT string and 16n log2 n bits to store the entire O array.
By increas-ing the interval at which the O values are stored, it is possible to reduce the BWBBLE memory consumption; however, this would increase the time needed to recompute the intermediate O values during the alignment stage.
The amount of n log2 n bits needed to store the SA array (used in the post-alignment stage) is the same for all the aligners.
It is also important to note that the length of the multi-genome reference is expected to be larger than the length of the single reference, as it includes bubbles as described in Section 3.1.2.
However, the cost of storing the reference multi-genome index is much smaller than the cost of storing the index for each genome in the population separately.
4 RESULTS 4.1 Implementation The BWBBLE aligner was implemented in C. The program per-forms the indexing of and short-read alignment with a reference multi-genome; for convenience, it also provides a separate (faster) mode for aligning to a single genome.
Currently the pro-gram only supports alignment of single-end reads (paired-end alignment will be supported in the near future).
The program accepts the reference genome in the standard FASTA file format and the reads in the FASTQ file format.
The alignment results are reported using the SAM file format.
We provide a script to generate the FASTA file for the refer-ence multi-genome.
The script accepts a single-genome build (e.g.
GRCh37) FASTA file and a set of VCF files with SNPs and indels to be incorporated into the reference multi-genome; it allows users to specify how many genomes to integrate, the min-imum number of genomes a variation has to be present in to be integrated, the expected read size and others.
The BWBBLE program supports parallel execution for the alignment process.
The code was parallelized using OpenMP by splitting the reads among the parallel threads.
Furthermore, the program also uses the 128-bit registers of the Streaming SIMD Extensions to parallelize the character count in the com-pressed BWT string when retrieving occurrence values for pos-itions at which they were not stored (see Section 3.4).
BWBBLE is freely available at http://viq854.github.com/bwbble.
4.2 Experiments To evaluate the performance of BWBBLE, we have created the reference multi-genome by combining the human genome build GRCh37 with the variants of 1090 individuals obtained from the October 2011 Integrated Variant Set release (The 1000 Genomes Project Consortium, 2010).
Only variants occurring in three or more individuals were included.
The resulting multi-genome ref-erence incorporates 1 442 639 indels and 25289 973 SNPs.
The size of its FASTA file is 3.2 GB (as opposed to 2.8 GB for the single GRCh37 file).
i366 L.Huang et al.
We compared the performance of BWBBLE with the state-of-the-art BWT-based single-genome aligner, BWA (Li and Durbin, 2009).
We evaluated BWBBLE against the multi-genome refer-ence, while BWA was run against the single-genome build GRCh37.
We ran BWA with its default parameters and used the same mapping quality threshold (namely, 10) to evaluate the confidence of the alignment results.
Reads with a mapping quality score higher than the given threshold were considered confidently aligned.
We compiled experimental results on simu-lated and real reads.
The accuracy of the results on simulated read data sets was computed by finding the percentage of the confidently aligned reads that were mapped to their correct pos-ition in the reference (note: owing to the presence of bubble branches, more than one multi-genome position can be con-sidered correct).
Furthermore, we also compared BWBBLE with the GCSA program (Siren et al., 2011) that performs indexing and short read alignment with a collection of multiple sequences.
All the experiments were performed on a 2.67GHz Intel Xeon X5550 processor.
4.2.1 BWT index construction To compute the suffix array SA, BWBBLE uses the sais library (Mori, 2008), which provides an implementation of the linear-time induced sorting SA construc-tion algorithm (Nong et al., 2011), adapted to support a larger input text size.
The BWT string can be easily computed from SA as described in Section 2.
The total time taken by the BWBBLE program to construct the BWT string and the auxiliary C, O and SA arrays of the reference multi-genome was 4025 s (1.1 h).
The BWA aligner took 4335.1 s (1.2 h) to index the GRCh37 genome.
4.2.2 Simulated read mapping We have prepared two types of simulated reads using the wgsim program (https://github.com/ lh3/wgsim).
The first type of simulated reads, simR, was gener-ated using the following standard wgsim parameters: sequencing base error rate 2%, SNP mutation rate 0.09% and indel mu-tation rate 0.01%.
To simulate the fact that in practice, we expect a set of reads to largely contain real known variants (rather than randomly generated SNPs), we have created two additional simulated read sets that incorporate known variants from two human genomes and have the wgsim mutation rate set to 0.
In particular, the second type of simulated reads was cre-ated by combining the SNPs and indels from two human genome builds, NA18626 (a Han Chinese from Beijing, China) and HG00122 (a British from England and Scotland), with the GRCh37 build and the following wgsim parameters: sequencing base error rate 2%, SNP mutation rate 0% and indel muta-tion rate 0%.
These reads are referred to as simNA and simHG, respectively.
The variations from these two human genome builds were not part of the 1090 individuals variants included into the reference multi-genome.
Table 1 presents the simulated read results.
4.2.3 Real read mapping We used the two real read sets NA18626 and HG00122 sequenced by Illumina Genome Analyzer II and mapped to build GRCh37.
Table 2 presents the real read evaluation results.
Because the true mappings are not available for these datasets, only the alignment confidence results are shown.
4.2.4 Comparison with GCSA We compared BWBBLE with the GCSA program (Siren et al., 2011) that performs short read alignment with a collection of multiple sequences by build-ing a prefix-sorted finite automaton from the multiple alignment of the sequences in the collection.
GCSA constructs a generalized BWT-based index from the prefix-sorted automaton.
This method has the theoretical advantage of not requiring additional branching due to SNPs during the backwards search, at the ex-pense of the large amount of time and space required to build the index (the construction algorithm has exponential time and space complexity).
Owing to the high memory requirement of the prefix-sorted automaton construction, an external memory implementation is needed for the tool to run on a collection of human genomes, which is currently not ready.
For consistency with the GCSA article, we conducted the experiments on the smaller sequence of a single human chromosome 18 and used a set of 1M reads of length 56 (corresponding to n 3 differences) simulated using the wgsim program with the same parameters as simR.
The current GCSA implementation accepts as input the mul-tiple alignment of the sequences in the collection; however, for a large number of sequences, the time and space required for creat-ing the multiple sequence alignment using available software can be extremely large.
To alleviate this problem, we have created a multiple alignment of the 1092 individuals manually by integrat-ing only known SNPs from each individual, respectively, into chromosome 18 of GRCh37; the optimal multiple alignment is trivial for this scenario.
We have also created a reference multi-genome with SNPs only from the 1092 individuals for BWBBLE.
Table 1.
Evaluation on 100-K simulated reads of length 125bp with at most n 6 differences Program Conf (%) Err (%) Time (s) bwa-simR 93.1 0.06 100 bwbble-simR 92.9 0.05 11295 bwa-simNA 92.7 0.09 110 bwbble-simNA 93.4 0.04 10896 bwa-simHG 92.6 0.11 107 bwbble-simHG 93.3 0.06 10892 Note: The percentage of confidently aligned reads (Conf), misaligned confident reads (Err) and sequential execution time of each program are reported.
Table 2.
Evaluation on 100-K real reads of length 108bp with at most n 5 differences Program Conf (%) Time (s) bwa-NA-108 78.3 109 bwbble-NA-108 79.5 9560 bwa-HG-108 90.5 90 bwbble-HG-108 90.4 6672 Note: The percentage of confidently aligned reads (Conf) and sequential execution time of each program are reported.
i367 Short read alignment with populations of genomes https://github.com/lh3/wgsim https://github.com/lh3/wgsim The GCSA index construction took 1.87 h, whereas the BWBBLE index construction took 48 s. Similar to the experiments in the GCSA article, we compared the running time and number of mapped reads for the two pro-grams: GCSA accuracy and confidence cannot be evaluated be-cause the position of mapped reads in the reference sequence is not reported.
The following results were obtained for the two tools: GCSA: (find operation only) 8932.93 s/98 307 matches; BWBBLE: 8439.21 s/94 364 matches.
GCSA finds more hits, al-though this does not imply that these matches are biologically plausible.
For example, the authors mention that they have no limits on gaps, which could result in some implausible read mappings.
Without being able to evaluate the quality of the re-ported matches, it is difficult to assess their significance.
As ex-pected, the number of exact matches was similar for both tools: GCSA: 15 226 matches; BWBBLE: 15268 matches.
5 DISCUSSION Because, in practice, the number of differences (mutations and sequencing errors) between a 125-bp read and a single-genome reference is usually smaller than the BWA default of n 6 dif-ferences, the BWA aligner is able to map most of the reads in the dataset, achieving high confidence and accuracy results.
However, on most datasets, the BWBBLE aligner does achieve slightly better confidence and accuracy values.
Because BWBBLE does not treat as mismatches the deviations from GRCh37 that are known SNPs, it can align reads that have more than n differences from GRCh37 when some of these dif-ferences are the SNPs included into the multi-genome reference.
Furthermore, because indels are incorporated by appending the bubble branches (with no restrictions on their length), BWBBLE can align reads that span known indels of any length (while BWA with default parameters can only handle indels up to length 6).
Therefore, BWBBLE can be expected to perform better on read sets that span a greater number of large indels or regions with a high SNP count (e.g.
reads from species that have a higher SNP density than humans; Tenaillon et al., 2001).
Zooming in on the evaluation results of the 100-K simNA dataset, we get the following performance counts: BWA (4332 unaligned, 92 735 confident, 92 645 correct); BWBBLE (3330 un-aligned, 93 412 confident, 93 375 correct).
Out of the 4332 reads unaligned by BWA, 973 are confidently mapped by BWBBLE out of which 933 are correctly aligned (with 134 reads aligned to appended bubble branches) and 40 are misaligned.
Figure 9 shows an example of a read mapped to a long indel and a read mapped to a region with a high concentration of SNPs (both of these reads were unaligned by BWA and correctly aligned by BWBBLE).
Note that because appending indels as bubble branches does not (by itself) require a change in the BWT back-wards search algorithm, existing aligners (e.g.
BWA) could use this technique to handle longer indels without a substantial modification to their code.
Furthermore, out of the 406 reads confidently aligned by BWA but not considered confident matches by BWBBLE, 376 were correctly aligned.
For 380 of these 406 reads BWBBLE found more than one best match and for the remaining reads it found too many sub-optimal matches.
Duplicated regions in the single-genome reference that have a few additional mutations will be likely aligned to with a different score by BWA and not treated as repeats.
However, if the mu-tations occurring in the repeats are only present in the subset of the population, they will be captured as SNPs in the multi-genome reference and can be aligned to with the same score by BWBBLE.
The alignment algorithm is entirely memory bound, and its running time is dominated by the random memory access pat-terns of the sampled occurrence array O, and the compressed BWT string during the SA interval computation.
Owing to the 4-fold increase in the size of the reference multi-genome alphabet, the BWBBLE program performs many more SA interval com-putations, which causes its running time to be significantly slower.
Namely, for each read base, the suffix array interval is computed for 16 (rather than four) different characters while differences (mismatches/gaps) are still allowed and for eight dif-ferent characters when only exact matching is allowed (an opti-mization used when the n differences have already been used up).
In particular, 188 826 valid SA intervals per read are found on average when aligning to the reference multi-genome during the inexact matching stage only (i.e.
while differences are allowed), as opposed to 8825 valid SA intervals for the single-genome refer-ence alignment.
For the 100-K read set, this is 18 billion more SA intervals (note: this figure does not include the additional SA interval computations that did not result in a valid interval and the SA intervals computed during the exact matching stage only).
However, although the runtime cost of aligning with a refer-ence multi-genome is very high, it is still significantly lower than the cost of aligning to each genome in the population separately.
For example, it would take about 109 200 s to align 100-K reads to 1092 genomes using BWA, and this time will grow linearly with the size of the genome collection.
On the other hand, while the BWBBLE running time does depend on the number of SNPs in the reference multi-genome, we expect that the SNP count (and the program performance) will not increase linearly with the number of genomes compiled into the multi-genome refer-ence.
This can be seen in Figure 10 that shows how the BWBBLE runtime changes as the number of genomes incorporated into the Fig.9.
(1) Read R from the simNA dataset unaligned to the single-genome reference S-G by BWA (with default parameters BWA can handle indels up to length 6) and correctly mapped to a bubble branch representing an insertion of 16bp in the multi-genome reference M-G using BWBBLE.
(2) Read R from the simNA dataset unaligned to the single-genome reference S-G by BWA and correctly aligned using BWBBLE.
The mismatches between the read and the references are shown in red (for BWA this number exceeds the allowed six differences); the SNPs in the M-G that match the read bases are shown in blue (these SNPs are treated as matches).
(Note: only the relevant portions of the read and references are shown.)
i368 L.Huang et al.
multi-genome reference grows.
Therefore, as the number of available genomes gets significantly larger, the speed-up of align-ing with BWBBLE as opposed to each genome individually will also be significantly greater.
Because BWBBLE runs on a multi-genome almost 100 times slower than BWA on a single genome, it is interesting to compute how many additional reads BWA can align in the same time frame to the genomes in the database (although when aligning to each reference separately, additional effort will have to be spent combining and interpreting the results).
To perform this experiment, we have created 99 genomes in addition to GRCh37 (100 genomes total) and aligned the simR, simNA, simHG, NA18626 and HG00122 read sets using BWA with each of these genomes individually.
More specifically, each genome was created by adding its known variants on top of build GRCh37.
The number of reads aligned has increased for each read set as follows: simR: 95 954 !
95 961; simNA: 95668 !
96 655; simHG: 95 619 !
96 656; NA18626: 81264 !
81 594; HG00122: 93 931 !
94 065.
BWBBLE aligned the following number of reads to the original 1090 individuals multi-genome and the 99 individuals multi-genome (note: no rare variant fil-tering was performed for this reference), respectively: simR: (96252; 96 283); simNA: (96 670; 96 633); simHG: (96 647; 96 641); NA18626: (81 627; 81 598); HG00122: (93 720; 93 713).
As expected, BWA (and BWBBLE on the 99 individuals multi-genome) aligned most of the same reads that BWBBLE aligned to the 1090 multi-genome, as many of the variants can be ex-pected to be present in these 100 genomes.
6 CONCLUSION In this article, we proposed a compact representation for a col-lection of genomes (the reference multi-genome) that captures the genomic variations in the collection and presented BWBBLE, a BWT-based short-read aligner that operates on this compiled reference.
We demonstrated the performance of BWBBLE on a human reference multi-genome incorporating variants from 1090 individuals.
A limitation of our reference multi-genome representation is its use of read length dependent padding to capture structural variants, which to achieve best results necessitates multiple reference indices to be built for each particular read length.
In addition, padding can also cause a significant increase in the length of the reference for highly variable species.
Although aligning to the reference multi-genome is much slower than to a single genome, it is considerably more efficient than aligning to each genome in a large collection separately.
As the number of sequenced genomes grows, aligning with BWBBLE should become much more time and space efficient.
Moreover, because BWBBLE can align reads that span long (known) indels, its utility will increase once large databases of structural variants are available.
BWBBLE can also be useful in functional genomics assays of parent/offspring trios, where un-biased peak calling is desired (Goncalves et al., 2012).
Finally, BWBBLE can output important information about the SNPs and structural variations that a given read was mapped to (e.g.
the number of genomes in the collection that contain this par-ticular variation), which should simplify and improve the current post-alignment processing pipeline.
ACKNOWLEDGEMENT We would like to thank our labmates, especially Jesse Rodriguez and Raheleh Salari, for discussions about this project.
Funding: This work is supported by a grant from the Stanford-KAUST alliance for academic excellence.
Conflict of Interest: none declared.
ABSTRACT This review focuses on recent trends in multiple sequence alignment tools.
It describes the latest algorithmic improvements including the extension of consistency-based methods to the problem of template-based multiple sequence alignments.
Some results are presented suggesting that template-based methods are significantly more accurate than simpler alternative methods.
The validation of existing methods is also discussed at length with the detailed description of recent results and some suggestions for future validation strategies.
The last part of the review addresses future challenges for multiple sequence alignment methods in the genomic era, most notably the need to cope with very large sequences, the need to integrate large amounts of experimental data, the need to accurately align non-coding and non-transcribed sequences and finally, the need to integrate many alternative methods and approaches.
Contact: cedric.notredame@crg.es 1 INTRODUCTION An ever increasing number of biological modeling methods depend on the assembly of accurate multiple sequence alignments (MSAs).
Traditionally, the main applications of sequence alignments have included phylogenetic tree reconstruction, Hidden Markov Modeling (profiles), secondary or tertiary structure prediction, function prediction and many minor but useful applications such as PCR primer design and data validation.
With the notable exception of ribosomal RNA, a large majority of these applications are based on the analysis of protein sequences, possibly back-translated into nucleic acid sequences in the context of phylogenetic analysis.
While this type of approaches still constitutes the vast majority of published applications for MSAs, recent biological discoveries coupled with the massive delivery of functional, structural and genomic data are rapidly expanding the potential scope of alignment methods.
In order to make the best of the available data, sequence aligners will have to evolve and become able to deal with a very large number of sequences or integrate highly heterogeneous information types such as evolutionary, structural and functional data.
Merely aligning all the known orthologs of a given gene will soon require aligning several thousand sequences, and the massive re-sequencing effort currently underway (Siva, 2008) could even mean that within a few To whom correspondence should be addressed.
decades, multiple comparison methods may be required to align billions of closely related sequences.
An MSA is merely a way to organize data so that similar sequence features are aligned together.
A feature can be any relevant biological information: structure, function or homology to the common ancestor.
The goal is either to reveal patterns that may be shared by many sequences, or identify modifications that may explain functional and phenotypic variability.
The features one is interested in and the way in which these features are described ultimately define the correct alignment, and in theory, given a set of sequences, each feature type may define a distinct optimal alignment.
For instance, a structurally correct alignment is an alignment where aligned residues play similar role in the 3D structure.
Given a set of distantly related sequences, there may be more than one alignment equally optimal from a structural point of view.
An alternative to structural conservation is homology (meant in a phylogenetic sense).
In that case, the alignment of two residues is a statement that these two residues share a similar relation to their closest common ancestor.
Aside from evolutionary inertia, there is no well defined reason why a structure and a homology-based alignment of the same sequences should be identical.
Likewise, in a functionally correct alignment, residues having the same function need to be aligned, even if their similarity results from convergent evolution.
Overall, a multiple sequence alignment is merely a way of confronting and organizing the specific data one is interested in.
Until recently, this notion was mostly theoretical, sequence information being almost the only available data.
Alignments were optimized for their sequence similarity, in the hope that this one-size-fits all approach would fare well for most applications.
The situation has now changed dramatically and the amount of data that could be integrated when building an MSA is rising by the day.
It includes new sequences coming from large scale genome sequencing, with a density of information that will make it more and more possible to reconstruct evolutionary correct alignments (Frazer et al., 2007).
Other high-throughput-based projects are delivering functional data in the form of transcript structure (Birney et al., 2007) and structural data is following a similar trend thanks to coordinated efforts like targetDB (Chandonia et al., 2006).
Another ongoing trend is the delivery of large-scale functional data, resulting from to the use of robotic techniques.
These make it possible to gather large amounts of functional information associated with homologous sequences (Fabian et al., 2005).
This data is usually applied to Quantitative Structure andActivity Relationships analysis, but it could just as well 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2456 24552465 C.Kemena and C.Notredame be used when comparing protein sequences.
Finally, the massive use of ChIp-Chip data makes it possible to reveal important protein/DNA interaction, thus allowing the enrichment of genomic data with functional data, an extra layer of information that could certainly be incorporated in sequence alignment strategies such as the ones we report here.
These trends have not gone un-noticed and over the last years, regular efforts have been made at developing and improving multiple sequence alignments methods so that they could take advantage of newly available data.
Three areas have been actively explored: (i) accuracy improvement, achieved through the use of consistency-based methods (Do et al., 2005; Notredame et al., 2000); (ii) an expansion of MSA methods scope, thanks to the development of template-based approaches (Armougom et al., 2006b; Pei and Grishin, 2007; Pei et al., 2008; Wallace et al., 2006; Wilm et al., 2008), a natural development of consistency-based methods that makes it possible to efficiently integrate alternative methods and alternative types of data; and (iii) large-scale alignments (Edgar, 2004a; Katoh and Toh, 2008; Lassmann and Sonnhammer, 2005b).
Most of the MSA methods currently available have been described and compared at length in several very complete reviews (Edgar and Batzoglou, 2006; Notredame, 2007; Pei 2008; Wallace et al., 2005a).
In this specific review, we will mostly focus on the latest developments in an attempt to identify the main trends of this very active research field.
We will also discuss an important challenge: the development of adequate benchmarking techniques, informative enough with respect to all potential applications of MSA methods, especially the reconstruction of accurate phylogenies.
The urgency of this issue recently received a striking illustration with two high-impact papers dealing with the complex relationship that exist between MSA reconstruction and accurate phylogenetic estimation (Loytynoja and Goldman, 2008; Wong et al., 2008).
2 TRADITIONAL ISSUES OF ACCURATE MULTIPLE SEQUENCE ALIGNMENT COMPUTATION Multiple sequence alignment computation stands at a cross-road between computation and biology.
The computational issue is as complex to solve as it is straightforward to describe: given any sensible biological criterion, the computation of an exact MSA is NP-Complete and therefore impossible for all but unrealistically small datasets (Wang and Jiang, 1994).
MSA computation therefore depends on approximate algorithms or heuristics and it is worth mentioning that almost every conceivable optimization technique has been adapted into a heuristic multiple sequence aligner.
Over the last 30 years, >100 multiple sequence alignment methods have been published, based on all kind of heuristics, including simulated annealing (Abhiman et al., 2006), genetic algorithms (Gondro and Kinghorn, 2007; Notredame and Higgins, 1996), Tabu search (Riaz et al., 2005), branch and bound algorithms (Reinert et al., 1997), Hidden Markov Modeling (Eddy, 1995) and countless agglomerative approaches including the progressive alignment algorithm (Hogeweg and Hesper, 1984), by far the most widely used nowadays.
The biological issue surrounding MSAs is even more complex: given a set of sequences, we do not know how to estimate similarity in a way that will guaranty the biological correctness of an alignment, whether this correctness is defined in evolutionary, structural or functional terms.
In fact, one could argue that being able to compare the biological features coded by a DNA sequence implies having solved most of the ab initio problems associated with genetic information interpretation, including protein structure prediction.
But, these problems are not solved and in practice multiple alignments are estimated by maximizing identity, in the hope that this simplistic criterion will be sufficiently informative to yield models usable for most type of biological inference.
The objective function thus maximized is usually defined with a substitution matrix and a gap penalty scheme.
The substitution matrix is relatively sophisticated when it comes to proteins, but barely more than an identity matrix for DNA and RNA.
For a long time, the maximization was carried out using a combination of dynamic programming (DP) and log odds scoring scheme, but over the last year, Bayesian techniques have been implemented that rely on pair-Hidden Markov Models (HMMs) and take advantage of a better defined statistical framework (Durbin et al., 1998).
While DP-and HMM-based approaches are mostly interchangeable, the last ones make it easier to explore the parameter space using off-the-shelves statistical tools such as BaumWelch and Viterbi training.
HMM modeling also offers easy access to a wider range of scoring possibilities, thanks to posterior decoding, thus making it possible to assess complex alignment scoring schemes.
For instance, a significant share of the improvements measured in the ProbCons (Do et al., 2005) algorithm over other consistency-based packages seems to result from the use of a bi-phasic penalty scheme (Table 1), pre-defined as a finite state automata (FSA) and parameterized by applying the BaumWelsch algorithm on BaliBase.
Sequence identity is only a crude substitute to biological homology, and in practice, it has often been argued that structurally correct alignments are those more likely to be useful for further biological modeling.
Similarity-based MSA methods have therefore been carefully tuned in order to produce structurally correct MSAs.
This tuning (or validation) has relied on the systematic usage of structure-based reference multiple sequence alignments.
This procedure has now been in use for more than a decade and has been a major shaping force on this entire field of research.
We will now review the most common validation procedures with their associated databases.
3 ACCURACY ESTIMATION USING STRUCTURE-BASED REFERENCE ALIGNMENTS The first systematic validation of a multiple sequence alignment using reference alignments was carried out by McClure (1994).
McClure was evaluating her alignments by assessing the correct alignment of pre-defined functional motifs.
Shortly after, Notredame and Higgins (1996) made the first attempt to systematically use structure-based alignments while evaluating the biological accuracy of the SAGA package.
The validation was carried out on a relatively small dataset named 3D-ali (Pascarella et al., 1996).
A few years later, Thompson developed a purpose built dataset named BaliBase I (Thompson et al., 1999).
The main specificity of BaliBase was to address a wide range of different issues related to multiple sequence alignments.
This included the alignment of distantly related homologues, the ability of alternative methods to deal with long insertions/deletions and their ability to properly integrate outliers.
Its main weakness was the questionable accuracy of some alignments and the relatively small size (82) of the dataset.
Most of 2456 [14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2457 24552465 Multiple sequence alignment methods in the high-throughput era Table 1.
Benchmarking of a selection of methods on the RV11 Balibase dataset.
BaliBase/RV11 is made of 38 datasests consisting of seven or more highly divergent protein sequences (<20% pair-wise identity on the reference alignment) Method Version Score Mode Templates RV11 Sever 3DPSI-Coffee 7.05 Consistency Accurate Profile + Structure 61.00 www.tcoffee.org PROMAL-3D Server Consistency Default Profile + Structure 58.66 prodata.swemd.edu/promals3d PROMALS Server Consistency Default Profile 55.80 prodata.swemd.edu/promals3d PSI-Coffee 7.05 Consistency Psicoffee Profile 53.71 www.tcoffee.org M-Coffee4 7.05 Consistency Muscl+Kal.
+ ProbC + TC 41.63 www.tcoffee.org T-Coffee 7.05 Consistency Default 42.30 www.tcoffee.org ProbCons 1.1 Consistency Default 40.80 probcons.stanford.edu ProbCons 1.1 Consistency Monophsic Penalty 37.53 probcons.stanford.edu Kalign 2.03 It + Matrix Default 33.82 msa.cgb.ki.se MUSCLE 3.7 It + Matrix Default 31.37 www.drive5.com/muscle Mafft 6.603b It + Matrix Default 26.21 align.genome.jp/mafft Prank 0.080715 Matrix Default 26.18 www.ebi.ac.uk Prank 0.080715 Matrix +F 24.82 www.ebi.ac.uk ClustalW 2.0.9 Matrix Default 22.74 www.ebi.ac.uk/clustalw All packages were ran using the default parameters.
Servers were ran in August 2008. these issues have been addressed in the latest version of BaliBase (BaliBase 3) (Thompson et al., 2005) and this database is now one of the most widely used reference standard.
Nonetheless, BaliBase remains a handmade dataset, with potential arbitrary and uneven biases resulting from human intervention.
The main alternative to BaliBase is Prefab (Edgar, 2004b), a very extensive collection of over a 1000 pairs of homologous structures, each embedded in a collection of 50 homologs (25 for each structure) gathered by PSI-BLAST.
In Prefab, the reference alignment is defined as the portions of alignments consistently aligned by two structural aligners: CE (Shindyalov and Bourne, 1998) and DALI (Holm and Sander, 1995).
Prefab, however, is not a multiple sequence alignment collection since each dataset only contains a pair of structures thus making it a less stringent than BaliBase where accuracy can be tested on entire multiple alignment columns rather than pairs of residues.
Other commonly used databases for protein multiple sequence alignments include OXBench (Raghava et al., 2003), HOMSTRAD (Stebbings and Mizuguchi, 2004) and SABmark (Van Walle et al., 2005).
One may ask why so many resources for addressing an apparently simple question.
The answer probably lies in the complexity of structural alignments.
While reasonably accurate structure-based alignments are easy enough to generate, owing to the strength of the structural signal, it is nonetheless very hard to objectively assess the relative merits of alternative structure-based alignments (Kolodny et al., 2005).
Several alternative references are therefore available and no simple way exists to objectively evaluate their relative merits.
In practice, the authors have taken the habit of running their methods on two or three datasets, verifying trend agreement.
Recently, Blackshield and Higgins (Blackshields et al., 2006) produced an extensive benchmarking, comparing the 10 main MSA methods using six available datasets.
The main trend uncovered by this analysis is that all the empirical reference datasets tend to yield similar results, quite significantly distinct from those measured on artificial datasets such as IRMbase (Subramanian et al., 2005, 2008), a collection of artificially generated alignments with local similarity.
We checked by re-analyzing some of the Blackshield and Higgins benchmark data (Table 2) in the context of this review.
The methodology is very straightforward: each reference dataset Table 2.
Comparison of alternative reference datasets (adapted from Blackshield and Higgins) Dataset #Categories Agreement (%) Self-agreement BaliBase 11 71.4 82.9 RV11 1 77.4 83.3 RV50 1 76.8 80.6 SabMark 4 69.8 81.3 Oxbench 10 65.0 70.8 Prefab 5 64.6 72.3 Homstrad 4 66.8 76.9 IRMdb 9 58.1 88.1 Empirical datasets 34 72.4 All datasets 43 66.1 Blackshield and Higgins published the average accuracy of 10 MSA packages (Mafft, Muscle, POA, Dialign-T, Dialign2, PCMA, align_m, T-Coffee, Clustalw, ProbCons) on six reference databases.
This table shows a new analysis of the original data.
Dataset indicates the considered dataset.
In this column, RV11 and RV50 are two BaliBase categories, Empirical Dataset refers to the five empirical datasets (BaliBase3, SabMark, Oxbench and Prefab).
All datasets includes IRMdb as well.
#Categories indicates the number of sub-categories contained in the considered datasets.
Agreement: average agreement between all the considered categories of a given dataset and all the categories of the other databases.
The agreement is defined as the number of times two given databases sub-categories agree on the relative accuracy of two methods.
The Empirical dataset average is obtained by considering all possible pairs of methods across all possible pairs of categories within the empirical datasets (i.e.
all datasets except IRMdb).
Self-agreement: same measure but restricted to a single database (i.e.
each category in turn against all the other categories of the considered database).
The last two rows show the average agreement between all respectively all empirical datasets.
is divided in sub-categories, and altogether the six datasets make a total of 43 sub-categories (34 for the empirical datasets, 9 for the artificial).
Given two MSA methods A and B, we counted how many times the ranking suggested by one sub-category is in agreement with the ranking suggested by another sub-categories (agreement in Table 2).
We then compared all the sub-categories of a dataset against all the sub-categories of the other datasets and reported the average figure in Table 2.
We also computed the average agreement within every dataset by measuring the agreement 2457 [14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2458 24552465 C.Kemena and C.Notredame across different categories within a dataset.
The results on Table 2 suggest that the five main empirical datasets are on average 72.4 % consistent with one another.
It means that any prediction of accuracy made on the basis of a single reference dataset is likely to be supported by 72.4% of similar measurements made on the five other empirical reference datasets.
A striking observation is the lower agreement between the artificial dataset (IRMdb) and the empirical ones.
Observations made on IRMdb are on average only supported by 58.1% of the observations made on the empirical datasets.
Two factors could explain this discrepancy: the local nature of IRMdb, mostly designed for assessing local alignment capacities, or its artificial nature.
The fact that empirical datasets biased toward local similarity (BaliBase RV50, long indels, 76.8% agreement) do not show a similar trend suggest that the discrepancy between IRMdb and the empirical datasets owes much to its simulated component.
Furthermore, at least three other studies reported similar findings, with results established on artificial datasets conflicting with empirical ones (Lassmann and Sonnhammer, 2002, 2005b; Loytynoja and Goldman, 2008) While there is no clear consensus on this matter, we would argue here that the discrepancy between artificial and empirical datasets pleads in favor on not using the artificial ones.
The use of artificial dataset should probably be restricted to situations where the process responsible for the sequence generation is well known and properly modeled, as happens in sequence assembly for instance.
It is interesting to note that some sub-categories of BaliBase are extremely informative albeit relatively small.
RV11 for instance is 77.4% consistent with the entire collection of empirical dataset which makes it one of the most compact and informative dataset.
This is not so surprising if one considers the nature of RV11, a dataset made of highly divergent sequences with < 25% sequence identity in the reference alignment.
So far, this dataset has proven fairly resistant to heavy tuning and over-fitting and it is a striking observation that ProbCons, the only package explicitly trained on BaliBase is not the most accurate (as shown on Table 1).
Table 1 shows a systematic benchmarking of most methods discussed here on the RV11 dataset.
Results are in broad agreement with those reported in most benchmarking studies published over these last 10 years, but the challenging nature of the dataset makes it easier to reveal significant difference in accuracy that are otherwise blurred by other less challenging datasets.
BaliBase has had a strong influence on the field, prompting the design of novel reference datasets for sequences other than proteins.
Similar to BaliBase, a reference dataset exists to validate ncRNA alignment methods, called BraliBase (Wilm et al., 2006).
BraliBase works along the same lines as BaliBase and relies on a comparison between an RNA alignment and its structure-based counterpart.
There is, nonetheless, a clear difference between these two reference datasets: in BraliBase, the reference structures are only predicted, and the final evaluation combines a comparison with the reference and an estimation of the predictive capacity of the new alignment.
As such, BraliBase is at the same time more sophisticated than BaliBase (because it evaluates the prediction capacity of the alignment) and less powerful because it is not based on a sequence-independent method (unlike BaliBase that uses structural comparison).
This limitation results from the relative lack of RNA 3D structures in databases.
We will see in the last section of this review that the current benchmarking strategies have many short comings and cannot address all the situations relevant to MSA evaluation.
These methods have nonetheless been used to validate all the currently available multiple sequence alignment packages and can certainly be credited (or blamed ) for having re-focused the entire methodological development toward the production of structurally correct alignments.
Well standardized reference datasets have also gradually pushed the MSA field toward becoming a fairly codified discipline, where all contenders try to improve over each others methods by developing increasingly sophisticated algorithms, all tested in the same arena.
Given the increased accuracies reported these last years, one may either consider the case closed or suspect that time has come to change arena.
4 THE MOST COMMON ALGORITHMIC FRAMEWORKS FOR MSA COMPUTATION An interesting consequence of the systematic use of benchmarking methods has been the gradual phase-off of most packages not based on the progressive algorithm (Hogeweg and Hesper, 1984).
With the exception of POA (Lee et al., 2002), most of the methods commonly used nowadays are built around the progressive alignment.
This popular MSA assembly algorithm is a straightforward agglomerative procedure.
Sequences are first compared two by two in order to fill up a distance matrix, containing the percent identity.
A clustering algorithm (UPGMA or NJ) is then applied onto this distance matrix to generate a rooted binary tree (guide tree).
The agglomerative algorithm follows the tree topology thus defined and works its way from the leaf to the root, aligning two by two each sequence pair (or profile) associated with each encountered node.
The procedure can be applied using any algorithm able to align two sequences or two alignments.
In most packages, this algorithm is the Needleman and Wunsch (1970) or more recently the Viterbi algorithm (Durbin et al., 1998).
As simple as it may seem, the progressive alignment strategy affords many possible adjustments, the most notable ones being the tree computing algorithm, the sequence weighting method and the gap weighting scheme.
In recent work (Wheeler and Kececioglu, 2007), the authors have shown that a proper tuning of these various components can take a standard method up to the level of the most accurate ones.
ClustalW (Thompson et al., 1994) is often considered to be the archetype of progressive alignments.
It is a bit paradoxical since its implementation of the progressive alignment significantly differs from the canonical one, in that it delays the incorporation of the most distantly related sequences until the second and unique iteration.
This delaying procedure was incorporated in ClustalW in order to address the main drawback of the progressive alignment strategy: the greediness.
When progressing from the leaves toward the root, a progressive aligner ignores most of the information contained in the dataset, especially at the early stage.
Whenever mistakes are made on these initial alignments, they cannot be corrected and tend to propagate in the entire alignment, thus affecting the entire process.
With a large number of sequences, the propagation and the resulting degradation can have extreme effects.
This is a well known problem, usually addressed via an iterative strategy.
In an iterative scheme, groups of sequences are realigned a certain number of time, using either random splits or splits suggested by the guide tree.
The most sophisticated iterative strategies [incorporated in Muscle and PRRP (Gotoh, 1996)], involve two nested iterative loops, an inner one in which the alignment is optimized with the respect to a guide tree, and an outer one in which the current 2458 [14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2459 24552465 Multiple sequence alignment methods in the high-throughput era MSA is used to re-estimate the guide tree.
The procedure keeps going until both the alignment and the guide tree converge.
It was recently shown that these iterations almost always improve the MSA accuracy (Wallace et al., 2005b), especially when they are deeply embedded within the assembly algorithm.
5 CONSISTENCY-BASED MSA METHODS The greediness of progressive aligners limits their accuracy, and even when using sophisticated iteration schemes, it can be very hard to correct mistakes committed early in the alignment process.
In theory, these mistakes could easily be avoided if all the information contained in the sequences was simultaneously used.
Unfortunately, this goal is computationally unrealistic, a limitation that has prompted the development of consistency-based methods.
In their vast majority, algorithms based on consistency are also greedy heuristics (with the exception of the maximum weight trace (MWT) problem formulation of Kececioglu (1993), but even so, they have been designed to incorporate a larger fraction of the available information at a reasonable computational cost.
The use of consistency for improved alignment accuracy was originally described Gotoh (1990) and later refined by Vingron and Argos (1991).
Kececioglu provided an exact solution to this problem, reformulated as a MWT problem.
This exact approach is limited to small datasets but was further expanded by Morgenstern who proposed the first heuristic to solve this problem for large instances, thanks to the concept of overlapping weights (Morgenstern et al., 1996).
While the notions developed in these four approaches are not totally identical, they have in common the idea of evaluating pair-wise alignments through the comparison of a third sequences (i.e.
considering an intermediate sequence).
In practice, Gotoh did not use consistency to construct alignments, but rather to evaluate them, and only considering three sequences.
The consistency described by Vingron is very strict because it results from dot-matrices multiplications, therefore requiring strict triplet consistency in order to deliver an alignment.
The overlapping weights described by Morgenstern also involve considering the support given by an intermediate sequence to a pair-wise alignment, but in this context, the goal is to help guiding the incorporation of pair-wise segments into the final MSA.
While the overlapping weights bear a strong resemblance to the most commonly used definition of consistency, it is important to point out that Morgenstern also uses the term consistency but gives it a different meaning to describe the compatibility of a pair of matched segments within the rest of a partly defined multiple sequence alignments.
The first combination of a consistency-based scoring scheme with the progressive alignment algorithm was later developed in the T-Coffee package (Notredame et al., 2000).
The main feature of a consistency-based algorithm is its scoring scheme, largely inspired by the Dialign overlapping weights.
Regular scoring schemes are based on a substitution matrix, used to reward identities and penalize mismatches.
In a consistency-based algorithm, the reward for aligning two residues is estimated from a collection of pair-wise residue alignments named the library.
Given the library, any pair of residues receives an alignment score equal to the number of time these two residues have been found aligned, either directly or indirectly through a third residue (Fig.1).
The indirect alignments are estimated by combining every possible pair of pair-wise alignments (i.e.
XY + YZ = XYZ).
Each observation can be weighted with a score reflecting the expected accuracy of COMPILATION (Template Based) Pair-Wise Comparison XY 2 ML 1 XZ 6 NL 1 ZY 5 EXTENSION XY= XY + Min (XZ,ZY) ML= ML XZ= XZ + Min (XY,YZ) XY 7 ML 1 XZ 8 NL 1 ZY 7 Fig.1.
Generic overview for the derivation of a consistency-based scoring scheme.
The sequences are originally compared two by two using any suitable methods.
The second box shows the projection of pair-wise comparisons.
These projections may equally come from multiple sequence alignments, pair-wise comparison or any method able to generate such projections, including posterior decoding of an HMM.
They may also come from a template-based comparison such as the one described in Figure 2.
Pairs thus identified are incorporated in the primary library.
These pairs are then associated with weights used during the extension.
The figure shows the T-Coffee extension protocol.
When using probabilistic consistency, the probabilities are treated as weights and triplet extension is made by multiplying the weights rather than taking the minimum.
See Supplementary Material for color version of the figure.
the alignment on which the observation was made.
In the original T-Coffee, the residue pairs contained in the library were generated using a global (ClustalW) and a local (Lalign) method applied on each pair of sequences.
At the time, the T-Coffee protocol resulted in a significant improvement over all alternative methods.
This protocol was later brought into a probabilistic framework with the package ProbCons.
In ProbCons, the sequences are compared using a pair HMM with a bi-phasic gap penalty (i.e.
a gap extension penalty higher for short gaps than long gaps).
A posterior HMM decoding of this HMM is then used to identify the high-scoring pairs that are incorporated in the library, using their posterior probability as a weight.
The library is then used to score the alignment with the T-Coffee triplet extension.
Because it uses a library generated with a probabilistic method, this protocols is often referred to as probabilistic consistency and has been incorporated in several packages, including SPEM (Zhou and Zhou, 2005), MUMMALS and PROMMAL (Pei and Grishin, 2006, 2007) as well as the latest version of T-Coffee (version 6.00 and higher).
Interestingly, the improvement is usually considered to be a consequence of the probabilistic framework when in fact it seems to result mostly from 2459 [14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2460 24552465 C.Kemena and C.Notredame the use of a more appropriate gap penalty scheme at the pair-wise level.
For instance, Table 1 shows the effect of applying a regular gap penalty scheme (monophasic) when compared with the bi-phasic gap penalty scheme that ProbCons uses by default.
This improvement has also been observed when incorporating the bi-phasic scheme in T-Coffee.
Consistency-based methods are typically 40 % accurate when considering the column score measured on the RV11 dataset.
This makes consistency-based aligners 10 points more accurate than regular iterative progressive aligners like ClustalW, Kalign, Muscle or Mafft.
This increased accuracy comes at a cost and consistency-based methods require on average N times more CPU time (N being the number of sequences) than a regular progressive aligner.
Aside from improved accuracy, an important aspect of consistency-based scheme is the conceptual separation it defines between the computation of the original alignments, merged into a library and the final transformation of this library into a multiple sequence alignment.
This procedure made it straightforward to combine seemingly heterogeneous algorithms, such as ClustalW and Lalign in the original T-Coffee package, but it also opened the way towards a more generic combination of aligners.
For instance, the latest version of T-Coffee (Version 6.00) is able to combine up to 15 different alignment methods, including pair-wise structural aligners, regular multiple sequence alignment methods and even RNA alignment methods such as Consan (Dowell and Eddy, 2006).
From the start, the T-Coffee framework made it possible to turn any pair-wise method into a multiple alignment method, thus opening the way to two major developments undergone by multiple aligners these last years: meta alignment methods and template-based alignments.
6 META-METHODS AS AN ALTERNATIVE TO REGULAR MSA METHODS The wealth of available methods and the lack of a globally accepted solution make it harder than ever for biologists to choose a specific method.
This dilemma is real and has recently received some renewed attention with a high-impact report establishing the tight dependency of phylogenetic modeling on the chosen aligner.
According to Wong and collaborators, phylogenetic trees may significantly vary depending on the methods used to compute the underlying alignment (Wong et al., 2008).
In a similar way, several editions of the CASP (Battey et al., 2007) contest have revealed that a proper multiple alignment is an essential component of any successful structural modeling approach.
A commonly advocated strategy is to use the method performing best on average, as estimated by benchmarking against structure-based reference datasets.
It is a reasonable martingale, like betting on the horse with the best odds.
One wins on average, but not always.
Unsurprisingly, benchmarks also make it clear that no method outperforms all the others, and that it is almost impossible to predict with enough certainty which method will outperform all the others on a specific dataset.
It is quite clear that the chosen method is irrelevant on datasets made of sufficiently similar sequences (>50% pair-wise identity).
Yet, whenever remote homologs need to be considered, the accuracy drops and one would like to run all the available methods before selecting the best resulting alignment.
This can be achieved when enough structural data is available (by selecting the alignment supporting the best structural superposition), or when functional information is at hand (by evaluating the alignment of similar features, such as catalytic residues).
Unfortunately, experimental data is rarely available in sufficient amount, and when using several packages, one is usually left with a collection of alignments whose respective value is hard to assess in absolute terms.
Meta-methods constitute an attempt to address this issue.
So far, M-Coffee (Wallace et al., 2006) has been the only package explicitly engineered to be used as a meta-method, although in theory all consistency-based packages could follow suit.
Given a multiple sequence dataset, M-Coffee computes alternative MSAs using any selected method.
Each of the alignments thus produced is then turned into a primary library and merged to the main T-Coffee library.
The resulting library is used to compute an MSA consistent with the original alignments.
This final MSA may be considered as some sort of average of all the considered alignments.
When combining eight of the most accurate and distinct MSA packages, M-Coffee produces alignments that are on average better than any of the individual methods.
The improvement is not very high (12-point percent) but relatively consistent since the meta-method outperforms the best individual method (ProbCons) on 2/3 of the 2000 considered datasets (HOMSTRAD, Prefab and BaliBase) (Wallace et al., 2006).
On a dataset like RV11, the improvement is much less marked (M-Coffee8 delivered alignments having an average accuracy of 37.5%) and one needs to restrict the combination to the four best non-template-based methods in order to obtain alignments with accuracy comparable to the best methods (Table 1).
Yet, as desirable as it may be, the improved accuracy is not the main goal of M-Coffee and one may argue that rather than its accuracy, M-Coffees main advantage is its ability to provide an estimate of local consistency between the final alignment and the combined MSAs.
This measure (the CORE index; Notredame and Abergel, 2003) not only estimates the agreement among the various methods (Fig.2) in a graphical way but it also gives precious indication on the local structural correctness (Lassmann and Sonnhammer, 2005a; Notredame and Abergel, 2003) and can therefore be considered as a good predictor of alignment accuracy.
Previous benchmarking made on the original CORE measure suggest that a position with a consistency score of 50% or higher (i.e.
50% of the methods agreeing on a position) is 90% likely to be correct from a structural point of view.
These results are consistent with those reported by Lassmann and Sonnhammer (2005a) who recently re-implemented this measure while basing it on libraries made of alternative multiple sequence alignments.
Even though these predictions are only restricted to a subset of the alignment, they can be an invaluable asset whenever a modeling process is very sensitive to alignment accuracy.
For instance, the CORE index is used by the CASPER server to guide molecular replacement (Claude et al., 2004).
From a computational point of view, meta-methods are relatively efficient.
Provided fast methods are used to generate the original alignment, the meta-alignment procedure of M-Coffee can use a sparse DP procedure that takes advantage of the strong agreement between the considered alignments.
A recent re-implementation of M-Coffee in the SeqAn (Doering et al., 2008) alignment library shows that the multiple alignment step of M-Coffee is about twice faster than standard consistency-based aligners based on pair-wise alignments like ProbCons or Promals (Rausch et al., 2008).
Yet, all things considered, meta-methods only offer a marginal improvement over single methods, and they even suggest that the current state of the art aligners are reaching a limit that may hard 2460 [14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2461 24552465 Multiple sequence alignment methods in the high-throughput era Fig.2.
Typical colored output of M-Coffee.
This output was obtained on the RV11033 BaliBase dataset, made of 11 distantly related bacterial NADH dehydrogenases.
The alignment was obtained by combining Muscle, T-Coffee, Kalign and Mafft with M-Coffee.
Correctly aligned residues (correctly aligned with 50% of their column, as judged from the reference) are in upper case, non-correct ones are in lower case.
In this colored output, each residue has a color that indicates the agreement of the four initial MSAs with respect to the alignment of that specific residue.
Dark red indicates residues aligned in a similar fashion among all the individual MSAs, blue indicates a very low agreement.
Dark yellow, orange and red residues can be considered to be reliably aligned.
See Supplementary Material for color version of the figure.
to break without some novel development in the field of sequence alignment.
While waiting for a method able to accurately align two remote homologs in an ab initio fashion (i.e.
without using any other information than the sequences themselves), the best alternative is to use extra information, evolutionary, structural or functional.
Template-based MSAmethods have been design to precisely address this aspect of data integration.
7 TEMPLATE-BASED MSA METHODS The word template-based alignment was originally coined by Taylor (Taylor, 1986) with reference to sequence/structure alignments.
The Experimental Data TARGET Experimental Data TARGET Template Aligner Template-Sequence Alignment Primary Library Template Alignment Template based Alignment of the Sequences Fig.3.
Overview of template-based protocols.
Templates are identified and mapped onto the target sequences.
The figure shows three possible types of templates: homology extension, structure and functional annotation.
The templates are then compared with a suitable method (profile aligner, structural aligner, etc.)
and the resulting alignment (or comparison) is mapped onto the final alignment of the original target sequences.
The residue pairs thus identified are then incorporated in the primary library.
See Supplementary Material for color version of the figure.
notion was later extended within the T-Coffee package in a series of publications dedicated to protein and RNA alignments (Armougom et al., 2006b; Notredame and Higgins, 1996; OSullivan et al., 2004; Wilm et al., 2008).
Template-based alignment refers to the notion of enriching a sequence with the information contained in a template (Fig.3).
The template can either be a 3D structure, a profile or a prediction of any kind.
Once the template is precisely mapped onto the sequence, its information content can be used to guide the sequence alignment in a sequence independent fashion.
Depending on the nature of the template one refers to its usage as structural extension or homology extension (sequence profile).
Structural extension is the most straightforward protocol.
It takes advantage of the increasing number of sequences with an experimentally characterized homolog in the PDB database.
Given two sequences with a homolog in PDB, one can accurately superpose the PDB structures (templates) and map the resulting alignment onto the original sequences.
Provided the sequence/template alignment is unambiguous, this protocol yields an alignment of the original sequences having all the properties of a structure-based sequence alignment.
This approach only defines pair-wise alignments, but the alignment thus compiled can be integrated into a T-Coffee library and turned into a consistency-based multiple sequence alignment (Figs 1 and 3).
Structural extension was initially implemented 2461 [14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2462 24552465 C.Kemena and C.Notredame in 3D Coffee (OSullivan et al., 2004).
EXPRESSO, a special mode of 3D Coffee was then designed so that templates could be automatically selected via a BLAST against the PDB database.
This protocol has recently been re-implemented in the PROMAL-3D (Pei et al., 2008) package.
Structural extension is not limited to proteins, and recently several approaches have been described using RNA secondary structures as templates, these include T-Lara (Bauer et al., 2005), MARNA (Siebert and Backofen, 2005) and R-Coffee (Wilm et al., 2008).
In all these packages, sequences are associated with a predicted structural template (RNA secondary structure).
The templates are then used by ad hoc algorithms to accurately align the sequences while taking into account the predicted structures (templates).
The resulting pair-wise alignments are combined into a regular T-Coffee library and fed to T-Coffee.
Homology extension works along the same principle as structural extension but uses profiles rather than structures.
In practice, each sequence is replaced with a profile containing homologs.
The profiles could be built using any available techniques although fast methods like PSI-BLAST have been favored.
The first homology extension protocol was described by Heringa and implemented in the PRALINE package (Simossis and Heringa, 2005).
PROMALS was described shortly afterwards (Pei and Grishin, 2007).
PROMALS is a consistency-based aligner, using libraries generated with the ProbCons pair-HMM posterior decoding strategy.
PROMALS also uses secondary structure predictions in order to increase the alignment accuracy, although this extra information seems to only have a limited effect on the alignment accuracy.
In Praline and PROMALS, sequences are associated with a PSI-BLAST profile.
A similar mode is also available in T-Coffee (Version 6.00+, mode = psicoffee) based on BLAST profiles (Table 1).
The use of structural and homology-extended templates results in increased accuracy in all cases.
For instance, the combination of RNAplfold (Bernhart et al., 2006) predicted secondary structures made R-Coffee more accurate at aligning RNA sequences than any of the alternative regular aligners, with a 4-point net improvement as estimated on BraliBase (Wilm et al., 2008).
The improvements resulting from homology extension on proteins are even more significant.
On Prefab, the authors of PROMALS reported nine points of improvement over the next best method (ProbCons).
A similar usage of PROMALS or PSI-Coffee on category RV11 (distant homologs) of BaliBase resulted in >10 points of improvement over the next best regular non-template-based aligner (Table 1).
Of course, the most accurate alignments are obtained when using structural extension.
In a recent work, Grishin and collaborators reported an extensive validation using a combination of structure and homology extension (Pei et al., 2008).
Their results suggest that template-based alignments achieve the best results when using structural extension.
They also indicate that the choice of the structural aligner can make a difference, with DALI-Lite possibly more accurate than SAP.
Given the same structural extension protocol, the authors report similar results between 3D Coffee and PROMALS-3D, suggesting that the structural aligner is the most important component of the protocol.
The improvement is very significant, and on Prefab for instance, the combined use of DaliLite with homology extension resulted in nearly 30 points improvement over alternative non-template-based protocols.
Results in Table 1 confirm these claims and suggest that the use of structural extension is the best way to obtain highly accurate alignments.
This very high accuracy, obtained when using structural information is, however, to be interpreted with some caution.
On the one hand, these high figures suggest a broad agreement between PROMALS-3D or 3D Coffee alignments with the references.
On the other hand, one should not forget that these methods use 3D information.
As such, they are not any different from the methods used to derive the reference benchmarks themselves.
It therefore means that PROAMLS-3D or 3D Coffee/Expresso alignments may be seen as new reference datasets, generated with a different structural alignment protocol.
Whether these are more or less accurate than the benchmarks themselves is open to interpretations, as it amounts to comparing alternative multiple structure-based sequence alignments.
7.1 New issues with the validation of template-based methods As reported by Kolodny et al.
(2005), the task of comparing alternative structure-based alignments is complex.
In order to address it, authors have recently started using alignment free evaluation methods.
These methods consider the target alignment as a list of structurally equivalent residues and estimate how good would be the resulting structural superposition.
These measures are either based on the RMSD (root mean squared deviation: average squared distance between homologous alpha carbons) or the dRMSD (distance RMSD: average square difference of distances between equivalent pairs of amino acids) like the DALI score (Holm and Sander, 1995), APDB (OSullivan et al., 2003) or the iRMSD (Armougom et al., 2006a).
So far, three extensive studies (Armougom et al., 2006a; OSullivan et al., 2003; Pei et al., 2008) have suggested that the results obtained with these alignment-free benchmarking methods are in broad agreement with those reported when using regular benchmarks.
The main drawback of these alignment-free evaluation methods is their reliance on distance measures strongly correlated with the methodology used by some structural aligners (Dali in particular) thus raising the question whether they might be biased toward this particular structural aligner.
A simpler and not yet widely used alternative would be to evaluate the modeling potential of the alignments, by measuring the accuracy of structural predictions based upon it.
This could probably achieved by recycling some components of the CASP evaluation pipelines.
8 ALIGNMENT OF VERY LARGE DATASETS Accuracy has been a traditional limitation of multiple sequence alignments for the last 20 years, and it is no surprise that this issue has been the most actively addressed, if only because inaccurate alignments are simply useless.
The other interesting development has been the increase of the number of sequences.
Traditionally, the length of the sequences (L) was greater than the number of sequences (N), and most methods were tuned so that they could deal with any value of L, assuming N would not be a problem.
This is especially true of consistency-based methods that are cubic in complexity with N , but only quadratic with L. With N <<L, the extra-cost incurred by consistency remains manageable, but things degrade rapidly when N becomes big.
Yet, it is now clear that L is bounded, at most by the average length of a genome.
N , on the other hand, has no foreseeable limit and could reflect the total number 2462 [14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2463 24552465 Multiple sequence alignment methods in the high-throughput era of species or the total number of individuals (past and present) in a population or even the total number of haplotypes in a system.
Dealing with large values of L should therefore be considered a prime goal.
In the context of a progressive algorithm, the first easy step is to speed up the guide tree estimation, for instance using a ktup-based method, as most packages currently do (-quicktree option in ClustalW).
The second step is to use an efficient tree reconstruction algorithm.
The default UPGMA and NJ algorithms are cubic with the number of sequences, but these algorithms can be adapted in order to become quadratic, as is the case with the current ClustalW implementation.
Even so, quadratic algorithms will not be efficient enough when dealing with very large datasets and more efficient data compression methods (such as those used to decrease redundancy in databases) will probably need to be used in the close future (Blackshields et al., 2008).
The next step for decreasing CPU requirements is to use an efficient DP strategy.
This is the strategy used by MAFFT that relies on a very efficient DP.
Consistency-based methods have a disadvantage because of the N-cubic requirement of consistency.
Yet, the protocol is relatively flexible and heuristics can probably be designed to estimate the original library more efficiently.
For instance, PCMA (Pei et al., 2003) starts by identifying sub-group of sequences closely related enough to be pre-aligned.
SeqAn (Rausch et al., 2008) takes advantage of the sparse matrix defined by the extended library and only does the minimum required computation to guarantee optimality.
SeqAn also makes an attempt to treat the sequences as a chain of segments rather than a chain of residues thus considerably reducing the CPU requirements for closely related sequences.
The SeqAn library has been designed to be linked with any of the consistency-based aligners.
Even so, the complexity of most consistency-based aligners remains too high to deal with the very large datasets that are expected to come.
Currently, the most promising approaches are those implemented in Muscle and Mafft.
Yet, it should be stressed that so far no dataset has been designed to evaluate the accuracy of very large number of sequences and it remains unclear how these methods scale and whether accuracy figures established on relatively small datasets can be safely extended to larger ones.
It is therefore urgent to establish reference datasets suitable for the validation of large scale aligners (1000 sequences and more).
Phylogeny being one of the main applications of large scale alignments, it will also be worth evaluating the phylogenetic potential of these large scale methods.
Doing so is far from trivial as it connects with the delicate issue of establishing reference tree collections.
More generally, it addresses the problem of predicting accurate trees from multiple sequence alignments.
9 PHYLOGENETIC RELEVANCE OF MULTIPLE SEQUENCE ALIGNMENTS The pace of accumulation of new entire bacterial genomes (and to a lesser extend eukaryotic genomes) can only be compared with the discovery of new species along the nineteenth century.
Never have we had so much molecular data at hand to reconstruct the natural history of life, a real challenge for intelligent design supporters.
Multiple sequence alignments constitute the ideal compost on which to grow these trees, and although there have been a few reports of alignment free tree reconstruction methods (Ferragina et al., 2007), the difficulty of aligning distantly related sequences probably means that unless a breakthrough happens in the field of sequence alignments and guarantees error free pair-wise alignments, MSAs will remain the starting point for most phylogenetic analysis.
An interesting paradox of the whole MSA field is that although most methods are defined within some sort of phylogenetic framework (progressive alignment), they are only evaluated for their capacity of producing structurally correct MSAs.
As a consequence, we do not really know how MSA methods fare with respect to phylogenetic reconstruction and, assuming the current structural benchmarks reflect well enough the evolutionary relation among proteins, we do not really know if this analysis can be safely extrapolated to ncRNAs.
Recent work suggest (Katoh and Toh, 2008) that the accuracy ranking of the best packages is roughly the same when benchmarking on RNA sequences (BraliBase) or protein sequences, but little is known about the accurate reconstruction of RNA-based phylogenetic tree.
This is a paradoxical situation when considering that most trees of life are derived from a multiple sequence alignment of ribosomal RNA sequences.
This passed year, two high-impact publications have made an attempt to raise the attention of the community on the issue of phylogenetic reconstruction (Loytynoja and Goldman, 2008; Wong et al., 2008).
The work by Wong shows that phylogenetic reconstruction can be very sensitive to the MSA method used to deliver the alignment.
The authors stopped short of proposing a way for selecting the best phylogenetic trees, but they make it clear that various methods can lead to different models, a new concept in a field where MSAs had always been considered to be data rather than models.
It is a context where meta-methods could certainly provide an element of answer, mostly by helping selecting the sites on the basis of their expected accuracy, using the CORE index or any related method.
In this context, the main advantage of the CORE index is to provide a filter independent from sequence conservation, as opposed to other accuracy predictors.
An MSA region can have a low level of conservation but a high CORE index, provided all the pair-wise alignments are in agreement with respect to the considered position.
Regions where conservation is low and consistency high may be considered prime targets for phylogenetic reconstruction.
The PRANK+F (Loytynoja and Goldman, 2008) algorithm was described shortly afterward and also addresses the issue of accurate phylogenetic reconstruction seen from an MSAperspective.
PRANK+F is a novel attempt to model the gap insertion required by the alignment process in a phylogenetically meaningful way.
This new approach opens up the possibility of incorporating the indel signal in the reconstruction of evolutionary scenarios, but it also raises an equally important question: given that alternative aligners lead to different trees, and given that the signal contained in the alignments can be used in many different ways, how are we going to evaluate the phylogenetic potential of multiple sequence alignment methods?
Building reference datasets is a very difficult task in phylogeny where an objective, independent source of information for establishing the correct history of a set of sequences is usually lacking.
Fossil records provide little help when it comes to selecting true orthologous sequences.
Given a sequence dataset, it is therefore very hard, and may be impossible to establish a correct reference tree.
So far, the validation of tree reconstruction methods has therefore focused on the methods ability to optimize a mathematical model (Guindon and Gascuel, 2003).
Even when this optimization is highly successful, the only guarantee is the mathematical correctness of the final tree with no clear guarantee on its biological relevance, except that provided by expert diagnostic 2463 [14:29 31/8/2009 Bioinformatics-btp452.tex] Page: 2464 24552465 C.Kemena and C.Notredame of the tree (i.e.
the observation that related species are grouped by the tree in a biologically meaningful way).
This situation is very similar to that encountered with MSA computation where one has on the one hand the mathematical correctness of a method, estimated by its capacity to optimize a given objective function (sums-of-pairs, viterbi, etc.)
and on the other hand, the biological accuracy, estimated by comparison with a reference alignment.
In the context of MSA analysis, the use of structure made it clear that there could be a significant discrepancy between mathematical correctness and biological accuracy.
Unfortunately, the equivalent of structural information is not available in phylogeny, and most current strategies, including Prank+F are validated on simulated data.
The simplest approaches simulate both the data and the trees using generators like ROSE (Stoye et al., 1997).
As pointed out earlier, the results obtained on simulated data differ significantly from those measured on empirical data, and for instance, PRANK outperforms all alternative packages on phylogenetic simulated data, but performs poorly when it comes to reconstructing structural alignments.
Assuming the relevance of results established on the simulated data, this suggests there could major differences between phylogenetically accurate alignments and structurally accurate ones, an hypothesis that remains to be further tested and confirmed.
10 CONCLUSIONS In this review, we have tried to give an overview of some of the newest aspects in the multiple sequence alignment field.
We have also tried to describe some of the challenges lying ahead now that we have entered what will probably be known as the genomic era of biology.
This review is not meant to be exhaustive and should be seen as a partly biased point of view on the direction in which we feel multiple sequence comparisons may develop over the next years.
Multiple comparison is the essence of biology and provides us with a very powerful observation tool, especially when one lacks a precise idea on the nature of forces that shaped the observed diversity.
One may argue that a multiple comparison of species formed the basis of Darwins work.
Likewise, it is certainly not a coincidence if the publication describing ClustalW has become one of the most widely cited paper in Biology (35 000 citations to this day).
As long as new data will accumulate, there will be an increasing need for informative multiple comparison methods.
In this review, we have outlined four major development directions: (i) the use of template-based methods that make it possible to combine heterogenous experimental data; (ii) meta-methods and the systematic use of consistency-based methods that make it possible to combine heterogeneous data but also to combine very different methods within a unified framework; (iii) the development of large-scale methods, necessary in a context where information is growing by the day; (iv) phylogentic reconstruction.
Accurate phylogenetic reconstruction is probably one of the most pressing issues, since such modeling is bound to play an increasing role in our data analysis routines.
Developing MSA methods that lend themselves to accurate phylogeny reconstruction should therefore be considered a prime goal.
The difference of behavior between methods like Prank+F and Promals3D or 3D Coffee/Expresso underlines dramatically the possible difference that may exist between structural and evolutionary analysis.
In our opinion, the recent progress on these two aspects have only started touching the problem, and one may expect that a sizeable amount of forthcoming work will be dealing with understanding whether there really is a discrepancy between structurally and phylogenetically accurate alignments and whether this discrepancy, if verified, can be turned into a usable signal for making sense of biological information.
The problem of multiple genome alignments and in general, the problem of multiply aligning non-transcribed sequences, has voluntarily been excluded from the scope of this review.
There is currently a very clear gap between the multiple alignment of genomic sequences, and the multiple alignment of transcribed sequences.
A good illustration of this separation is the relatively low overlap of authorship across these two neighbor fields of research.
It is probably a safe bet that over the coming years, this gap will gradually close, thanks to the development of a continuous algorithmic framework [like SeqAn or Pecan (Paten et al.
2009)] bridging the gap.
It is also an easy guess that the accurate alignment of non-coding DNA will become increasingly prominent field of research.
Conflict of Interest: none declared.
ABSTRACT Motivation: The research area metabolomics achieved tremendous popularity and development in the last couple of years.
Owing to its unique interdisciplinarity, it requires to combine knowledge from vari-ous scientific disciplines.
Advances in the high-throughput technology and the consequently growing quality and quantity of data put new demands on applied analytical and computational methods.
Exploration of finally generated and analyzed datasets furthermore relies on powerful tools for data mining and visualization.
Results: To cover and keep up with these requirements, we have created MeltDB 2.0, a next-generation web application addressing storage, sharing, standardization, integration and analysis of metabo-lomics experiments.
New features improve both efficiency and effect-ivity of the entire processing pipeline of chromatographic raw data from pre-processing to the derivation of new biological knowledge.
First, the generation of high-quality metabolic datasets has been vastly simplified.
Second, the new statistics tool box allows to inves-tigate these datasets according to a wide spectrum of scientific and explorative questions.
Availability: The system is publicly available at https://meltdb.cebitec.
uni-bielefeld.de.
A login is required but freely available.
Contact: nkessler@cebitec.uni-bielefeld.de Received on May 6, 2012; revised on July 11, 2013; accepted on July 14, 2013 1 INTRODUCTION Metabolomics research covers all aspects of the investigation of small molecule metabolite compositions resulting from cellular processes and constitutes an integrated part of systems biology (Bino et al., 2004).
Like transcriptomics and proteomics, meta-bolomics is capable of measuring extrinsically initiated changes in organisms.
The metabolome, the entity of all small molecules in a cell, organism or tissue, is considered to be the closest to the phenotype of all-omes (Fiehn, 2002).
Compared with other molecular levels or-omics methods, metabolomics is challenging in its high degree of interdiscipli-narity, interlinking experts from research fields as diverse as engineering, physics, chemistry and biology and from cheminfor-matics over bioinformatics to statistics, data mining and finally visualization.
Both sample acquisition and subsequent analysis are auto-mated in high-throughput instruments, which has continuously posed challenges on the systematic storage and computational processing of the gathered experimental datasets, starting in the early 2000s.
The increasing number and quality of measurements not only raised the generated data volume but also allowed to address more complex biological questions within conducted ex-periments.
To comprehensively address these demands, bioinfor-matics internet applications were developed.
MeltDB, a software platform for the analysis and integration of data from metabolo-mics experiments, has been published by Neuweger et al.
(2008).
Xia et al.
(2009) released MetaboAnalyst, a comprehensive tool suite for metabolomic data analysis.
Carroll et al.
(2010) pub-lished the MetabolomeExpress web server as a public place to process, interpret and share GC/MS metabolomics datasets.
Since around 2008, we have observed that the requirements to comprehensive metabolomics software platforms have changed: The general growth of the field of metabolomics and the increas-ing number of collaborations diversified the user community of researchers and their individual scientific goals.
It is obvious that the success of a metabolomics study depends on an efficient and effective collaboration of this interdisciplinary research commu-nity.
Thus, not only the availability and sharing of the data is important but also special functions have to be significantly ex-tended with specific features to consider all researchers demands and perspectives.
In addition, the ever-increasing throughput and the constant lack of time makes it immensely important that automated pre-processing methods are reliable and that analyses and manual intervention are fast and easy.
Since Metabolomics approaches are applied to more and more scientific objectives, a powerful set of statistical methods is mandatory, ranging from hypothesis-driven statistical tests to less specified and untargeted data-mining methods, such as clustering and dimension reduc-tion.
Finally, the wealth of generated data poses a necessity for exploratory data analysis tools and information visualization.
To tackle these new challenges systematically, a next gener-ation of bioinformatics tools needed to be developed, covering all of the aforementioned aspects of metabolome data analysis, ranging from processing raw data (RD) to finishing and finally the derivation of biological knowledge.
During the stages of that process, one can identify four successive data categories that represent different levels of data classification and annotation as well as different levels of abstraction.
First, RD, stored and organized in meaningful groups, build the basis.
Then, pre-processed data (PD) is computed, where peaks and their*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com https://meltdb.cebitec.uni-bielefeld.de https://meltdb.cebitec.uni-bielefeld.de mailto:nkessler@cebitec.uni-bielefeld.de `` '' to `` '' `` '' `` '' , , And (EDA) above raw data ( ) s quantities have been detected.
It follows integrated data (ID), where peaks that putatively originate from the same compound are consistently annotated over chromatograms of an experiment and thus become comparable.
Last, derivative data (DD) is achieved by statistical analyses of metabolite quantities in an experiment and then visualized to allow effective exploration and to draw conclusions.
In this manuscript, we present MeltDB 2.0, which offers novel tools to challenge the rising wealth of data quality and quantity and support the analysis of all four categories RD, PD, ID and DD and includes a multitude of updates.
New and improved preprocessing methods underpin the reliability of automatically created annotations.
At the same time, straightforward tools for manual peak annotation simplify the curation even of large ex-periments.
To help answering questions of different scientific objectives, the set of statistical analyses and data-mining tools has been strongly enriched.
To finally nail down the quintessence of an experiments outcome, data exploration is supported by new interactive and telling information visualizations.
2 IMPLEMENTATION AND METHODS The first version of the MeltDB software platform, a three-tiered web application and database server published in 2008 (Neuweger et al., 2008), provides means for the standardization, systematic storage and analysis of gas chromatographymass spectrometry (GC-MS) metabolomics experiments.
Within a powerful project and user management, raw chromatograms of various file formats can be uploaded and organized into chro-matogram groups (e.g.
replicates, factor levels) and experiments.
A flexible processing pipeline allows to find, quantify and iden-tify peaks in the raw chromatograms.
Subsequently, a set of statistical tools and visualizations can be applied to analyze the gathered data tables.
This fast growing, free online platform today hosts425 distinct projects conducted by4150 registered users from around the world.
More than 17 000 chromatograms have been uploaded and analyzed yet.
In the following, all major improvements to the entire process from RD to DD will be described in more details.
Figure 1 summarizes the four stages of data processing and associates visualizations and data mining methods that can be performed in MeltDB 2.0 to each stage.
2.1 From RD to PD: improved pre-processing In metabolomics data analysis, pre-processing is a critical step, as ID and DD build on PD.
To ensure a reliable data basis for statistical data exploration, MeltDB 2.0 is equipped with several new and updated algorithms for the early steps of experiment data analysis.
The growing list of pre-processing methods now includes sup-port for the centWave algorithm by Tautenhahn et al.
(2008) for chromatographic peak detection, which features a high sensitiv-ity, and updates of the XCMS package (Smith et al., 2006) for chromatogram alignment and profiling analyses.
In addition, the ChromA (Hoffmann and Stoye, 2009) software is added to the list of supported chromatogram alignment tools.
ChromA com-putes pairwise alignments of chromatograms without a priori knowledge, but it is capable of optionally using previously matched or identified peaks as anchor points, which speeds up the process.
The calculation of retention time indices in GC-MS measure-ments is improved and can now also be performed manually using the web interface.
Peaks of added substances can be as-signed with retention indices and will be used as anchors for interpolating other peaks retention indices (Ettre, 1994), which Fig.1.
The overview shows the information processing in MeltDB 2.0 as well as visualizations and tools that are applicable to each level of data: RD, PD, ID and DD.
Although different chromatogram viewers are available immediately after RD upload, heatmaps and data matrices can only be computed as soon as data have been integrated, i.e.
there are peaks that are consistently named across chromatograms.
To finally derive knowledge from the data, MeltDB 2.0 offers a versatile set of statistics and data-mining tools 2453 MeltDB 2.0 , 3 more than more than , raw data derivative data raw data pre-processed data I integrated data derivative data up pre-processed data In order  support subsequent peak identification (Kopka et al., 2005).
The detection of alkanes as retention markers can be automated.
Furthermore, peak identification itself is facilitated with a powerful feature: MeltDB 2.0 offers a new Reference list tool to save peaks of measured reference substances as Reference in the MeltDB database.
The stored data comprises retention indi-ces, quantification masses and mass spectra of reference com-pounds.
This helps to generate project specific databases that complement the Golm Metabolite Database (http://gmd.
mpimp-golm.mpg.de/) (Kopka et al., 2005) or the National Institute of Standards and Technology standard reference data-base 1A (http://www.nist.gov/srd/nist1a.cfm).
The tool allows to aggregate
ABSTRACT Motivation: Many types of omics data are compiled as lists of connections between elements and visualized as networks or graphs where the nodes and edges correspond to the elements and the connections, respectively.
However, these networks often appear as hair-ballswith a large number of extremely tangled edgesand cannot be visually interpreted.
Results: We present an interactive, multiscale navigation method for biological networks.
Our approach can automatically and rapidly abstract any portion of a large network of interest to an immediately interpretable extent.
The method is based on an ultrafast graph clustering technique that abstracts networks of about 100 000 nodes in a second by iteratively grouping densely connected portions and a biological-property-based clustering technique that takes advantage of biological information often provided for biological entities (e.g.
Gene Ontology terms).
It was confirmed to be effective by applying it to real yeast protein network data, and would greatly help modern biologists faced with large, complicated networks in a similar manner to how Web mapping services enable interactive multiscale navigation of geographical maps (e.g.
Google Maps).
Availability: Java implementation of our method, named NaviCluster, is available at http://navicluster.cb.k.u-tokyo.ac.jp/.
Contact: thanet@cb.k.u-tokyo.ac.jp Supplementary information: Supplementary data are available at Bioinformatics online.
Received on October 22, 2010; revised on January 31, 2011; accepted on February 9, 2011 1 INTRODUCTION In the post-genomic era, a great number of biological data that are available via the Internet or obtained through high-throughput experiments are significantly inhibiting researchers from making sense of the data and communicating them to others in a concise and meaningful way (Evanko, 2010).
Among these, one data type that has become increasingly common is binary relationship data, which are defined as sets of elements and 1-to-1 associations To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first and the last authors should be regarded as joint First Authors.
between them.
Proteinprotein interactions (PPI), correlatively expressed gene pairs, and genetic regulatory relationships exemplify this data type.
They are conventionally presented using network (graph) visualization where nodes (vertices) and edges correspond to the elements and associations, respectively (Merico et al., 2009; Suderman and Hallett, 2007).
The network visualization is widely used because it is typically assumed to be more interpretable by humans than a long list of associations.
High quality visualization should allow for effective investigation of the information, hypothesis generation and biological discovery (Merico et al., 2009).
Unfortunately, network representations often fail to effectively convey information to readers in cases where the networks are large and complicated (e.g.
>100 edges).
The drawings of such networks, referred to as hair balls (Suderman and Hallett, 2007), occur frequently when analyzing high-throughput biological data.
To avoid being overwhelmed by such complicated networks, effective navigation approaches that can abstract data properly and present them insightfully at a right level of detail are hence required (Gehlenborg et al., 2010; Hu et al., 2007; ODonoghue et al., 2010).
Hierarchical clustering is a technique used with many types of data, including networks or graphs, that meaningfully groups data elements in a recursive manner, thereby producing a hierarchy, or tree, of clusters (Andreopoulos et al., 2009) (Supplementary Figure S1).
Higher levels in the hierarchy contain fewer, larger clusters, each of which encompasses more data elements (or nodes, in the case of networks) than lower levels.
In the case of hair-balls, some methods (Abello et al., 2006; Freeman et al., 2007; Pavlopoulos et al., 2009; Royer et al., 2008; Vlasblom et al., 2006) use hierarchical clustering to create an interpretable visualization by displaying only the high-level clusters, thereby reducing the number of elements in the figure and abstracting the networks (e.g.
the top panel in Supplementary Figure S1).
By descending the hierarchy and showing the actual members of each cluster, detailed information can still be intuitively shown at a particular scale (e.g.
the dotted arrows and regions in Supplementary Figure S1).
A recent study reported that natural networks display hierarchical properties (Clauset et al., 2008), suggesting that hierarchical clustering of biological networks is both reasonable and promising.
Despite the advantages of hierarchical clustering, existing visualization methods using this technique have some drawbacks that hinder effective investigation of large biological datasets.
First, some methods (Freeman et al., 2007; Shannon et al., 2003; Vlasblom et al., 2006) require researchers to provide information on The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:38 21/3/2011 Bioinformatics-btr083.tex] Page: 1122 11211127 T.Praneenararat et al.
hierarchies or clusters, data which is usually not known in advance.
Secondly, existing methods do not allow for flexible navigation beyond fixed cluster boundaries (Abello et al., 2006; Freeman et al., 2007; Pavlopoulos et al., 2009; Royer et al., 2008; Vlasblom et al., 2006).
In other words, they can visualize the members of one cluster at a time but do not support visualization and navigation of members of different clusters, despite the fact that nodes/clusters of interest to biologists may belong to various high-level nodes in the hierarchy (e.g.
nodes in the light blue area in Supplementary Figure S1).
Thirdly, existing methods are inappropriate for interactive, real-time navigation.
Researchers frequently change their focus in the course of biological investigation to generate hypotheses and need to visualize different sets of nodes/clusters.
Thus the long running times (minutes to hours) needed to produce the abstractions are unacceptable (Abello et al., 2006; Enright et al., 2002; Pavlopoulos et al., 2009; Royer et al., 2008).
Methods that can provide appropriate abstractions of any given portion of the network rapidly and automatically, such as those that process about 100 000 nodes in seconds, are therefore necessary for efficient, interactive biological investigation.
In addition to the previously mentioned problems, the clustering techniques employed by existing methods are often insufficient for abstracting large networks to a level that is simple enough for interpretation (Abello et al., 2006; Pavlopoulos et al., 2009; Royer et al., 2008).
Recent investigations have revealed that in some common biological datasets, hub-like nodes tend to connect with low-degree nodes and the majority of nodes interact with only few partners (e.g.
yeast PPI networks) (Yamada and Bork, 2009).
Large, densely connected regions of such networks are therefore quite few; instead, small, densely connected modules are more frequently found.
Consequently, even the highest level of the created hierarchies can contain over 100 clusters, resulting in cluttered and difficult to manage visualizations.
Therefore, means for further abstraction are required to allow for effective navigation of large biological networks.
We developed an interactive, multiscale network navigation method with three advantages: (i) our method can work without a user-provided hierarchy; (ii) the method can rapidly, automatically and interactively produce abstractions of any region of the network, including nodes/clusters belonging to different ancestors in the hierarchy; and (iii) an intuitive visualization with a manageable amount of information is reliably produced at every step of navigation.
The effectiveness of our method was confirmed using real yeast protein network data.
Our approach will aid modern biologists faced with large and complicated network data.
2 METHODS Our method consists of three components: an ultrafast graph clustering component, a property-based clustering component, and an interface that presents an abstracted view and permits researchers to flexibly choose nodes/clusters (Fig.1).
First, the method abstracts the whole network using the ultrafast graph clustering component.
It detects topologically dense, connected regions, which may correspond to biologically meaningful clusters, such as protein complexes.
It rapidly identifies clusters in huge networks of about 100 000 nodes within a few seconds, thus particularly suitable to be applied to the problem of interactive navigation of large and complicated networks.
Additionally, if the property-based clustering needs to be executed afterwards, this graph clustering method displays another advantage in that it significantly reduces the number of clusters to be input to the next slower clustering.
Clusters Found (Louvain Clusters) Extract densely connected portions Further Grouped LCs Subnetwork Corresponding to Various Operations (e.g., Zooming and Re-centering) Yes No Input Ultrafast Graph Clustering Abstracted View with User Interface # LCs > threshold?
Group LCs with similar properties Property-Based Clustering Show clusters and meta/property edges between them Fig.1.
A diagram of the presented method.
For details, see the main text.
Secondly, in case the abstraction is insufficient because of the characteristics of the biological network, the property-based clustering component further abstracts the network to an extent sufficient for visual interpretation.
This component automatically groups clusters with similar biological properties by utilizing the fact that biological entities are often assigned property information, such as Gene Ontology (GO) terms.
The new clusters resulting from the property-based clustering are used in the next component instead of those generated by the ultrafast graph clustering component, thereby reducing the number of clusters on the screen.
There are two main advantages of the property-based clustering: (i) the property-based clustering allows researchers to directly control the number of clusters shown on the screen through a parameter K of its underlying algorithm.
To solve the problem of the cluttered visualization produced by applying only the graph clustering, the number of clusters shown on the screen must be decreased to an extent that biologists can manage to interpret.
In addition, because the preferred numbers of clusters on the screen might differ according to the circumstances, it is important that biologists be allowed to adjust the number of clusters displayed; and (ii) because the clusters generated by the property-based clustering are based on the property information that carries biological meaning, the clusters are expected to be highly intuitive.
Thirdly, the resultant clusters/nodes are immediately displayed with meta-edges and property edges, which represent the numbers of edges that exist between any members of two clusters and the similarities between their properties, respectively.
In the case that the number of clusters is less than the parameter K , the biggest cluster is recursively split until either the number of the clusters is equal to K or breaking only one more cluster makes the number of the clusters larger than K .
While showing the abstracted view, the interface allows researchers to interactively zoom, move laterally beyond cluster boundaries, focus on an arbitrary set of clusters/nodes, etc.
Any subset of the entire network of particular interest to the researcher can be fed into the clustering components and the abstracted view of that cluster is displayed.
This cycle can be completed in a few seconds on a typical PC with a CPU of about 2 GHz and a memory of about 1 GB for datasets with 100 000 nodes, permitting truly interactive navigation of large biological networks.
2.1 Graph clustering Graph clustering detects clusters in networks by finding densely connected sets of nodes where weighted connections of nodes within the sets are stronger/denser than weighted connections between nodes inside and outside 1122 [11:38 21/3/2011 Bioinformatics-btr083.tex] Page: 1123 11211127 Interactive, multiscale navigation of large networks of the set.
This metric is called the modularity, or Q function (Newman and Girvan, 2004), and numerous graph clustering algorithms have been developed to identify clusters that optimize modularity (Blondel et al., 2008; Clauset et al., 2004; Newman and Girvan, 2004; Wakita and Tsurumi, 2007).
The NewmanGirvan is a well-known, pioneering algorithm that iteratively removes edges most likely to lie between clusters, splitting the clusters into two, until no edges remain (Newman and Girvan, 2004).
This process results in a dendrogram (a tree showing the order of the splits) and the best clustering can be identified from this tree by choosing the split with the highest modularity.
This algorithm has a high computational cost, as it requires a traversal of all remaining edges at every step.
Until recently, the best known algorithm developed to overcome this shortage with near-linear time complexity was devised by Wakita and Tsurumi (Wakita and Tsurumi, 2007).
However, the speed of this algorithm was still insufficient and the quality of the clusters produced had room for improvement when incorporated into an interactive navigation of large networks (e.g.
human gene networks of >20 000 nodes) (Blondel et al., 2008).
Recently, Blondel et al.
(2008) developed a breakthrough algorithm for quickly identifying high modularity clusters in huge networks of about 100 000 nodes (the Louvain algorithm).
We found that this algorithm for finding meaningful communities in large and complicated networks could be applied to the problem of interactive navigation.
2.2 The Louvain algorithm The Louvain algorithm works in two phases, as follows: (1) Starting from the state that each node belongs to a cluster different from every other node, for each node the algorithm considers its neighbors clusters and moves the node to a neighboring cluster.
The cluster to be joined is determined by choosing the movement that results in the highest positive modularity gain among all possible movements to the nodes neighboring clusters.
If no movements result in a positive gain in modularity, the node is not moved.
This process is repeated until no members are added to/removed from any clusters and yields clusters with the maximum local modularity.
(2) Every cluster from Phase 1 is then treated as a new node.
For each pair of new nodes, an edge connecting them exists if there is at least one edge between any member of one of the new nodes and any member of the other.
Edge weights are determined based on the number of previous edges.
Self-loops are drawn on nodes to represent corresponding inter-cluster edges.
The output of Phase 2 is then fed back to Phase 1 and the algorithm iteratively runs these two phases until no additional changes are made.
More details of this algorithm can be found in (Blondel et al., 2008).
This algorithm can finish clustering networks of 70 000 nodes in one second (Blondel et al., 2008).
Thus, it works swiftly on many biological networks that generally contain less than 100 000 nodes (e.g.
yeast or human PPI networks).
The ultrafast speed of the algorithm is essential for accomplishing the goal of truly interactive navigation of large networks.
The clusters produced by this algorithm, which we call Louvain clusters or LCs, are characterized by high modularity.
It has been shown that clusters with high modularity in biological networks correspond to biologically functional units [e.g.
protein complexes in PPI networks and transcriptional modules in gene regulatory networks (Dunn et al., 2005)].
Thus, the LCs are expected to be intuitive and meaningful groups in navigation of biological networks.
Note that it is also possible to continue Louvain clustering to further reduce the number of clusters, even if the gain in modularity becomes negative.
However, in such cases, the biological intuitiveness of the clusters produced would be lowered due to the decreased modularity (Dunn et al., 2005).
Therefore, at this stage, it would be better to adopt another reliable source of information, in addition to the topology of the networks.
2.3 Property-based clustering The property-based clustering component aims to decrease the complexity remaining after the application of the Louvain algorithm by further grouping LCs based on property information typically associated with the nodes (Supplementary Figure S2).
The visualization step displays the clusters resulting from the property-based clustering instead of those generated by the graph clustering approach, thereby reducing the number of clusters on the screen.
The VisANT tool works similarly to our property-based clustering and offers integrated visualization of the GO hierarchy and user-specified networks, but it requires the user to manually create clusters containing the same GO terms (Hu et al., 2009).
In contrast, our property-based clustering automatically generates clusters having similar properties to achieve interactive navigation.
Let N be the number of nodes in the original input graph and L be the number of LCs.
For each n, where 1nN , node vn has a set of terms, T (vn ), that denotes the properties of the node (e.g.
a set of GO terms).
A weight, w (t ), is given to each term t to quantify its importance (e.g.
properties that are rare and/or of particular interest to researchers may be given higher weights).
Let Tall 1nN T (vn ) and Tall = { tj 1 j|Tall| }.
For each LC, LCl , where 1 lL, let Prop (t,LCl )=| { vLCl|t T (v ) }|/|LCl|.
Then the property vector for LCl or PV (LCl ) is a |Tall|-dimensional vector whose j-th element is the score of term tj , which is calculated as w (tj )Prop (tj,LCl ).
(Note that, in our implementation, a property term to be used for labeling LCl is the term th which is w (th )Prop (th,LCl )w (tj )Prop (tj,LCl ),j,1 j |Tall|.)
Next, the similarity between two LCs, LCa and LCb, is given as a normalized dot product of the two property vectors; that is, Sim (LCa,LCb )= PV (LCa )PV (LCb )/|PV (LCa )||PV (LCb )|.
Then LCs having similar property vectors are grouped by the Farthest First Traversal K-center (FFT) algorithm (Andreopoulos et al., 2009).
The FFT algorithm is a complexity-reducing variant of the K-means algorithm, where initial K cluster centers are chosen as follows.
The first center (vector) is chosen randomly and each remaining center is determined by greedily choosing a vector farthest from the set of already chosen centers.
The rest of the vectors are assigned to the cluster to which they are most similar.
3 IMPLEMENTATION The proposed method was implemented as a Java 6 Swing application with a graphical interface for flexible navigation.
The JUNG (Java Universal Network/Graph Framework) library (http://jung.sourceforge.net) was employed to create the visualization.
Three input files are required to run the application: a node list file, an edge list file and a property information file.
The node list file describes node names, property terms annotated with the nodes, and database names and IDs used in those databases (e.g.
SGD for yeast proteins).
The database information is used to provide URL links.
The edge list file contains connected pairs of node names and the weights of the connections (weights describe how strongly the nodes are connected).
The property information file describes the property terms in the node list file: terms IDs, names, display names (used in labeling clusters in abstracted views), namespaces, default weights and their parent terms.
In the case of the GO property information file bundled with the software, the default weights are terms depths in the GO hierarchy.
This treats more specific terms as more important properties.
In addition, each term belongs to one of three namespaces (biological process, molecular function or cellular component).
By using the namespace information, researchers can put heavier weights on all biological process terms at once if they want to group nodes having similar biological process terms, rather than other namespace terms.
If parent terms are provided for each term, they are automatically 1123 [11:38 21/3/2011 Bioinformatics-btr083.tex] Page: 1124 11211127 T.Praneenararat et al.
assigned to the nodes that the term annotates as well.
The is_a and part_of relationships in GO are handled by this entry.
The implemented software, NaviCluster, works on any platform that can run Java 6.
The program has a minimum memory requirement of 1 GB for networks of about 100 000 edges.
In addition to the highlighted features, NaviCluster has many additional functions designed for interactive network navigation.
Views containing only nodes/clusters that the user wants to explore can be created, as well.
Undo/Redo functions allow users to easily move backwards and forwards between previously created network views.
As previously mentioned, users can adjust the namespace factors to change their importance, which changes the weights of all terms in the same namespaces, as long as at least one namespace weight is not zero.
Furthermore, the number of clusters resulting from the property-based clustering, 12 by default, can be freely changed to match the individual preferences of the user.
The user can trace a node of interest easily with the search function; the software can highlight the cluster in the current graph view that contains the node of interest.
In addition, users can customize the view according to their preferences in many ways; for example, the property edges can be filtered based on the similarity value.
Besides, visual appearances are designed to intuitively describe the different characteristics of the nodes/clusters and edges (e.g.
node/cluster labels, cluster sizes and edge thicknesses).
Context-specific popup menus, which contain links to external databases, are also available to accelerate knowledge discovery and hypothesis generation as much as possible.
4 RESULTS 4.1 Interactive and multiscale navigation Our method provides zooming and re-centering functions, which imitate the functions of common Web mapping services such as Google Maps.
The zooming function executes the two-stage clustering instantly, using all node members of the selected clusters as input, and displays the abstracted network afterwards.
Given researcher-selected nodes/clusters, the re-centering function runs the clustering on all nodes in the entire network whose geodesic distances to the selected clusters/nodes are less than/equal to a provided value.
The neighbor nodes do not need to be included in the current view; this function corresponds to the panning function Web mapping services provide to see surrounding regions.
The resulting view shows the clusters/nodes of interest in the center of the display, as well as the nearby neighbors.
By changing the geodesic distance value, both fine and rough visualization centered on the clusters/nodes of interest can be obtained.
Note that the zooming operation can be run on more than one cluster at a time, useful for cases where the nodes of interest belong to different clusters.
This function, as well as the re-centering function, correspond to seeing a map of several cities in different countries in Web mapping services and were not possible with previously existing methods.
4.2 Abstraction of large and complicated biological networks Figure 2 illustrates the abstracted visualization of the entire Saccharomyces cerevisiae protein network YeastNet v.2 (Lee et al., 2007).
The dataset contains 5483 yeast proteins (nodes) and 102 803 linkages (edges), and our implemented tool can generate this Fig.2.
An abstracted view for all of YeastNet v.2, a probabilistic functional network containing 5483 yeast proteins and 102 803 linkages (Lee et al., 2007).
The associated log-likelihood scores were adopted as edge weights and used in the ultrafast graph clustering.
The implemented software, NaviCluster, directly produced the visualization with the number of clusters to be displayed set at 12.
For detail, see the main text.
figure in just a few seconds on a typical PC.
The log-likelihood scores indicating the probabilities of true functional linkages in YeastNet were adopted as the edge weights for the ultrafast graph clustering component.
Edges with high weights connote that the protein pairs of the edges have strong relationships and thus highly likely to be grouped in the same clusters in the ultrafast graph clustering.
GO terms assigned to the proteins in the SGD database [http://downloads.yeastgenome.org/ (access date: March 29, 2010)] were used as property information for the property-based clustering.
The property-based clustering was configured to focus only on the biological process namespace because we wanted to group proteins involved in the same biological process.
This can be done by excluding all terms of the other two namespaces in the property-based clusteringwhich can be easily performed using the namespace sliders in the implemented tool.
The numbers displayed above clusters represent the numbers of nodes within the clusters and the labels following those numbers are the abbreviated property terms that can best describe the properties of the clusters, providing insight into the biological meaning of the clusters.
Such property terms are the ones that are shared by most proteins in the clusters and meanwhile most specific (deepest in the GO hierarchy) among all terms annotated to the proteins in the clusters (see Section 2 for details).
In case there are more than one cluster labeled with the same property term, the next highest score term of each cluster is additionally displayed in brackets to discriminate the cluster from the others.
The display sizes of the clusters are proportional to linear normalizations of the numbers of the clusters over the interval from the minimum and the maximum numbers among all clusters.
The color saturations of the clusters also reflect the number of proteins inside.
Meta-edges are drawn between any two clusters that have at least one edge between at least one member in each of the two clusters (solid lines in Fig.2).
The gray number next to each meta-edge is the total numbers of all edges existing between the members of the two clusters, which are also reflected in the thickness of the 1124 [11:38 21/3/2011 Bioinformatics-btr083.tex] Page: 1125 11211127 Interactive, multiscale navigation of large networks Fig.3.
Multiscale navigation for a specific protein in a large network.
This figure shows the hierarchical organization of the clusters encompassing Cla4, a protein of interest, which are highlighted in pink.
The numbers by the arrows indicate the numbers of proteins contained in the processed sub-networks.
Hexagons represent proteins that are not contained in any cluster.
(A) The most abstract view (same as Fig.2).
Cla4 belonged to the highlighted protein amino acid phosphorylation cluster.
After zooming in on this cluster, the clusters containing Cla4 of two deeper views were also the clusters labeled protein amino acid phosphorylation and were excluded for the sake of conciseness.
(B) At this level, Cla4 was contained in the establishment of cell polarity cluster.
Clusters labeled with the same property terms as those of others are discriminated by additionally displaying the next highest score terms in brackets.
(C) Cla4 was contained in the regulation of exit from mitosis cluster.
(D) The most specific view.
Cla4 was clustered together with Ste20, Gic1, Gic2, Cdc24, Bem1, Skm1, Msb1, Msb2 and Tos2.
A large version of this figure is available at Supplementary Figure 3. meta-edge.
In addition, a property edge is drawn between every pair of clusters if the similarity between their property vectors, represented by the associated gray number, is larger than a specified threshold (dashed lines in Fig.3; the threshold is 0.1).
4.3 Untangling hierarchical organization of large and complicated protein networks Large biological networks can be interactively navigated in a multiscale manner as illustrated in Figure 3.
Using the YeastNet v.2 dataset as an input network, Figure 3 demonstrates how the zooming and searching functions of our implemented tool can be employed together to lead researchers to the protein of interest, in this case Cla4.
Cla4 is a p21-activated protein kinase that acts as an effector of Cdc42.
It has been implicated in many important biological processes such as cell polarization (Bi et al., 2000; Bose et al., 2001; Gulli et al., 2000), cytokinesis (Benton et al., 1997; Cvrckov et al., 1995) and exit from mitosis (Bosl and Li, 2005; Hfken and Schiebel, 2004; Jensen et al., 2002; Seshan et al., 2002; Tiedje et al., 2008).
In Figure 3, the clusters containing Cla4 are highlighted in all views; the granularities of detail in the views vary from coarsest to finest.
Each zooming operation (a solid arrow) performs clustering on the member proteins of the highlighted cluster and immediately shows 12 more detailed clusters.
In the first view, which abstracts the whole network, the clusters are labeled with broad biological processes such as DNA repair, rRNA processing and translation (Fig.3A).
Cla4 is grouped under the protein amino acid phosphorylation cluster, which is highlighted.
The members of this cluster are mostly involved in phosphorylation processes, that is also true for Cla4, which functions as a kinase to phosphorylate proteins.
After zooming in on the protein amino acid phosphorylation cluster, one can find clusters of more specific processes, such as mitotic cell cycle spindle assembly checkpoint, pseudohyphal growth and establishment of cell polarity (Fig.3B).
In this view, Cla4 is found in the cluster involved in establishment of cell polarity, which is consistent with previous studies suggesting this role for the protein (Bi et al., 2000; Bose et al., 2001; Gulli et al., 2000).
Some clusters found deeper in this cluster are characterized by actin filaments or budding processes (Fig.3C); as expected, they are related to cell polarity.
Cla4 is a member of the regulation of exit from mitosis cluster at this navigation level.
After zooming in on this cluster, the final view illustrates the relationships between Cla4 and other proteins such as Gic1, Gic2 and Ste20 (Fig.3D).
In fact, these proteins were found to be involved in mitotic exit by three different mechanisms (Tiedje et al., 2008).
Cla4 was discovered to promote mitotic exit and cytokinesis by activating a guanine nucleotide exchange factor, Lte1, which in turn causes the activation of Tem1, thereby terminating the M phase of the cell cycle (Jensen et al., 2002; Seshan et al., 2002; Tiedje et al., 2008).
In addition, Gic1 and Gic2 were also proposed to be involved in stimulating mitotic exit in parallel with Cla4 by inhibiting GTPase-activating proteins of Tem1, both of which result in Tem1 activation (Hfken and Schiebel, 2004; Tiedje et al., 2008).
Moreover, Cdc24 and Bem1, which play important roles upstream in the regulation of mitotic exit (Bi et al., 2000; Bose et al., 2001; Gulli et al., 2000; Tiedje et al., 2008), are also clustered together with Cla4 in this view.
This example illustrates that in all views proteins of related biological processes are clustered together sensibly and their roles are indicated informatively, correctly and appropriately based on the granularity of each view.
The amount of information displayed is kept tractable by showing coarse and fine information on the abstracted and detailed views, respectively.
Every zooming step is rendered in a matter of seconds, which allows researchers to gather interesting information at the desired degree of detail easily and effectively.
4.4 Discovering knowledge about proteins of interest from a large network intuitively and effectively As zooming is always performed on all members of selected clusters, it alone is insufficient to provide flexible enough navigation for 1125 [11:38 21/3/2011 Bioinformatics-btr083.tex] Page: 1126 11211127 T.Praneenararat et al.
researchers to explore relationships between some nodes that may belong to different clusters.
To fulfill this requirement, we designed the re-centering function.
By performing re-centering, researchers can focus on nodes/clusters of interest and their relationships with other nodes within a specified geodesic distance from the entire network (i.e.
not restricted to the nodes contained in the current view).
Figure 4A shows the view resulting from selecting the proteins Nas6, Rpn14, and Hsm3 and invoking the re-centering function to gather all direct interactors.
Recent studies have reported that these three proteins, previously known to bind to regulatory particles of proteasomes, actually function as chaperones, assisting in the regulatory particle assembly in yeast (Funakoshi et al., 2009; Le Tallec et al., 2009; Roelofs et al., 2009; Saeki et al., 2009).
In Figure 4A, the fundamental roles of the three proteins as proteasome-related proteins are delineated explicitly; specifically, they interact with many proteins involved in the ubiquitin-dependent protein catabolic process.
It is also possible to investigate the relationships between Nas6, Rpn14, and Hsm3 and other proteins that are farther from them, to understand their roles in a broader context (Fig.4B; the abstracted view collecting all proteins within two hops from the three selected proteins).
In addition to the relationship with the ubiquitin-dependent protein catabolic process cluster, this view reveals relationships with other clusters of more general processes.
Among them, the relationships with the protein folding and cellular response to heat clusters actually suggest the recently reported roles of the three proteins as chaperones (proteins denatured due to heat are rescued by chaperones, which help fold and assemble proteins).
It should be noted that these clusters were formed without using any property information from the three selected proteins.
These examples indicate that our method can be used to intuitively and effectively retrieve knowledge on nodes of interest from large and complicated networks.
Abstraction of the three proteins neighbors is necessary because, even if it is possible for biologists to manually investigate all 49 direct neighbors shown in Figure 4A, it is not practical to do the same for the 1443 indirect neighbors shown in Figure 4B.
This is a fundamental issue because this indirect information suggested the chaperone role; however, it was not possible to interactively create these views using methods that depend on fixed cluster hierarchies.
5 DISCUSSION To confirm the novelty and capabilities of the present method, we compared it to other existing visualization methods such as GenePro (Vlasblom et al., 2006), Power Graphs (Royer et al., 2008), VisANT (Hu et al., 2009), BioLayout Express3D (Freeman et al., 2007) and jClust (Pavlopoulos et al., 2009) (Supplementary Table 1).
Our method was the only one capable of representing the hard-to-manage and complicated visualization of the overwhelming numbers of nodes and edges and providing the capability to navigate networks beyond cluster boundaries.
GenePro, BioLayout Express3D and jClust can visualize clusters of nodes, but only at a single level.
They do not support visualization of recursive clustering.
VisANT provides multiscale visualization; however, the user must manually create metanodes (equivalent to clusters) themselves.
CyOog (Power Graphs) hierarchically visualizes power nodes (equivalent to clusters) created by the Power Graph algorithm, Fig.4.
Re-centered views for retrieving knowledge from both direct and indirect neighbors of proteins of interest in a large network.
Two views focusing on three proteins related to proteasome regulatory particle assembly (Nas6, Rpn14 and Hsm3) were created using the re-centering function.
Edges not connected to these center proteins are omitted for clarity.
(A) Re-centered with the geodesic distance from the central nodes set to one.
Twenty seven of the 49 direct neighbors of the three proteins form a ubiquitin-dependent protein catabolic process cluster, nine form a DNA repair cluster and eight form a chromatin silencing at telomere cluster.
(B) Re-centered with the geodesic distance set to two.
This view delineates the more general functions of the proteins (see the main text).
A large version of this figure is available at Supplementary Figure 4. but the speed is not fast enough to be used for interactive navigation of large biological networks.
It should be noted that our method is highly extendable in several ways.
First, any type of property information, not just GO categories, can be used in the property-based clustering.
For example, if a researcher is interested in diseases, she/he can use disease names associated with proteins derived from disease databases to investigate PPI networks by clustering proteins related to similar diseases.
Secondly, because the clustering components of the present method can abstract any sub-network very rapidly, any interactive function for producing network views of interest can be achieved if modules for selecting appropriate clusters/nodes are implemented.
For example, it is easy to devise a module that interactively produces 1126 [11:38 21/3/2011 Bioinformatics-btr083.tex] Page: 1127 11211127 Interactive, multiscale navigation of large networks networks of genes regulated by a selected regulatory factor, given information on gene regulatory relationships.
Thirdly, the presented method is not limited to biological applications.
In fact, it is general enough to be tailored to network data from other sources as well, as long as information adequately describing the properties of the nodes is provided.
For example, citation networks of biomedical research articles can be explored with the MeSH (Medical Subject Headings) vocabularies that are stored in the MEDLINE database, and friendship networks of university students can be explored with information of class names they attend.
In addition, implementing the method as a plug-in of an existing platform, e.g.
Cytoscape (Shannon et al., 2003) would be a promising way to effectively integrate the benefits from the method with the abundant features of the platform, such as connection with important databases.
To summarize, we present the first method for interactive and multiscale navigation of large, complicated biological networks that displays appropriately abstracted views at all levels of detail.
The specially designed interface, particularly the re-centering function, enables flexible navigation across cluster boundaries.
Application to real biological network data demonstrates how it achieves the goal of providing effective, intuitive and interactive navigation, and why it is eminently suitable for large biological networks.
As shown by analysis of Cla4, every time the researcher zooms in, clusters were constructed on the fly and visualized immediately along with meaningful labels.
All views were informative as to the hierarchical structure of the network.
As for Nas6, Rpn14 and Hsm3, their roles as chaperones were suggested in the view created by the re-centering function, which encompassed the indirect neighbors of these nodes.
This outcome indicates that, in addition to the usual network exploration accomplished by displaying direct interactors, our methods can also uncover interesting hidden facts in large, complicated networks.
This feature will help researchers formulate new hypotheses more easily and systematically, even if they possess little experience in bioinformatics.
We believe that the presented method will aid modern biologists in discovering knowledge from massive binary-relationship datasets, which are accumulating at an accelerating pace.
Funding: Grant-in-Aid for Scientific Research on Priority Areas Systems Genomics from the Ministry of Education, Culture, Sports, Science and Technology of Japan; Grant-in-Aid for Young Scientists (Start-up) from the Japan Society for the Promotion of Science.
The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.
Conflict of Interest: none declared.
ABSTRACT An analysis of the distribution of the Na+-translocating ATPases/ATP synthases among microbial genomes identified an atypical form of the F1Fo-type ATPase that is present in the archaea Methanosarcina barkeri and M.acetivorans, in a number of phylogenetically diverse marine and halotolerant bacteria and in pathogens Burkholderia spp.
In complete genomes, representatives of this form (referred to here as N-ATPase) are always present as second copies, in addition to the typical proton-translocating ATP synthases.
The N-ATPase is encoded by a highly conserved atpDCQRBEFAG operon and its subunits cluster separately from the equivalent subunits of the typical F-type ATPases.
N-ATPase c subunits carry a full set of sodium-binding residues, indicating that most of these enzymes are Na+-translocating ATPases that likely confer on their hosts the ability to extrude Na+ ions.
Other distinctive properties of the N-ATPase operons include the absence of the delta subunit from its cytoplasmic sector and the presence of two additional membrane subunits, AtpQ (formerly gene 1) and AtpR (formerly gene X).
We argue that N-ATPases are an early-diverging branch of membrane ATPases that, similarly to the eukaryotic V-type ATPases, do not synthesize ATP.
Contact: galperin@ncbi.nlm.nih.gov; amulkid@uos.de Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 9, 2010; revised on April 16; 2010; accepted on April 21, 2010 1 INTRODUCTION F1Fo-type (F-type) and V/A-type ATPases are membrane-anchored rotary enzymes that couple translocation of H+ or Na+ ions across the membrane to the synthesis or hydrolysis of ATP.
Although most of their subunits are evolutionarily related, the two classes of ATPases have clear differences in structure and phylogenetic distribution: F-type ATPases are found in bacteria, mitochondria and chloroplasts, whereas V/A-type ATPases are found in eukaryotic cell membranes (in particular, vacuoles), as well as in all archaea and in some bacteria (Hilario and Gogarten, 1998; Forgac, 2007; von Ballmoos et al., 2008; Mulkidjanian et al., 2009).
V/A-type ATPases are often subdivided into two classes based (i) on the ability of the prokaryotic (A-type) enzyme to function in the direction of ATP synthesis and (ii) presence of additional subunits in the To whom correspondence should be addressed.
eukaryotic (V-type) enzyme that normally functions only as an ion-translocating ATPase, using the energy of ATP hydrolysis to acidify cellular compartments (Hilario and Gogarten, 1998; Forgac, 2007).
Structural characterization of the Na+-binding sites in the c-subunits of F-and V-type ATPases (Meier et al., 2005; Murata et al., 2005) allowed unequivocal assignment of the cation specificity for the membrane ATPases encoded in numerous sequenced genomes: only those c-subunits that contain full sets of Na+ ligands appeared to transport Na+, the loss of at least one of those ligands correlated with the loss of Na+ specificity (Mulkidjanian et al., 2008a; Meier et al., 2009).
In the view of several reports on the sodium dependence of energy-coupled reactions in cyanobacteria (Willey et al., 1987; Skulachev, 1988; Pogoryelov et al., 2003), we have searched cyanobacterial genomes for the Na+-translocating ATPases.
We found several apparently Na+-dependent cyanobacterial ATPases, but always as second copies, in addition to the H+-translocating ATP synthases.
Here, we report the common properties of these ATPases, which are encoded in apparently highly mobile operons and have a set of specific traits that qualify them as a separate subfamily of the F-type ATPases.
Since these ATPases, besides forming a novel subfamily, are always encoded next to the typical rotary ATPases and are predominantly Na+-dependent, we refer to them hereafter as N-ATPases.
2 METHODS Phylogenetic distribution of the N-ATPase operons was deduced from protein BLAST (Altschul et al., 1997) searches against the NCBIs RefSeq database (Pruitt et al., 2009) (last searched February 1, 2010) and verified by examining gene neighborhoods of the retrieved ORFs and by checking for the presence of the N-ATPase-specific subunit AtpR.
Phylogenetic trees were constructed using the neighbor-joining algorithm with MEGA (Kumar et al., 2008).
Multiple alignments were constructed from BLAST outputs with manual editing.
Transmembrane segments were predicted using TMHMM (Krogh et al., 2001).
Sequence logos were drawn with WebLogo (Crooks et al., 2004).
3 RESULTS Search of the NCBIs RefSeq database (Pruitt et al., 2009) for cyanobacterial c (proteolipid) subunits that would have a full set of Na+-binding ligands (Mulkidjanian et al., 2008a; Meier et al., 2009) returned five hits, all coming from marine cyanobacteria (Supplementary Fig.S1a).
In each case, the operon encoding the Published by Oxford University Press on behalf of the US Government 2010.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[16:25 12/5/2010 Bioinformatics-btq234.tex] Page: 1474 14731476 D.V.Dibrova et al.
Na+-binding c subunit was present in the genome along with another ATPase operon, encoding an H+-translocating F-type ATPase.
As depicted below for Synechococcus sp.
PCC 7002, an alignment of the Na+-and H+-binding c-subunits (top and bottom, respectively) from the same cyanobacteria revealed a Glu substitution of the Gln residue (shown in blue) that serves as a Na+ ligand (uncharged residues of the transmembrane segments are highlighted in yellow, Na+ and H+ ligands are labeled with asterisks) Among the 4084 c-subunit sequences currently listed in the Pfam family ATP-synt_C (PF00137; Finn et al., 2010), only 227 (5.5%) contain a Glu residue in that position and only 1% combine it with a typical ESTxxY Na+-binding motif (Supplementary Fig.S1).
The Na+-dependent ATPase operons in all cyanobacteria had a similar gene order, with a single gene insertion in Acaryochloris marina and a two-gene insertion in the two strains of Cyanothece sp.
(Supplementary Fig.S2).
3.1 Always the second: conservation and distinctive properties of the N-ATPase A search of the complete genome database identified homologous N-ATPase operons in some representatives of the bacterial phyla Aquificae, Chlorobi and Planctomycetes, in certain members of-,-,-and-subdivisions of Proteobacteria, and in two archaea, Methanosarcina barkeri and M.acetivorans (see Supplementary Fig.S2).
All these operons had the same atpDCI-urf2-atpBEFAG gene order, encoding, respectively,-,-, 1-, Urf2-, a-, c-, b-,-and-subunits of this particular F-type ATPase, first described in M.barkeri by Sumi et al.
(1997).
The only exception outside of cyanobacteria was Persephonella marina, a member of the Aquificae, which has an N-ATPase operon with the urf2-atpBEFAGDCI gene order and also harbors operons for F-and A-type ATPases (Supplementary Fig.S2).
Identification of the N-ATPase operons in diverse microorganisms was simplified by the fact that these operons are always present in the genomes as second copies alongside operons that encode typical H+-transporting F-type ATPases (in M.barkeri and M.acetivorans, A-type ATPases).
In contrast, most N-ATPase c-subunits contain full sets of Na+ ligands, indicating that these enzymes are specific for Na+ ions.
Just like the cyanobacterial Na+-binding c-subunits, c-subunits of most N-ATPases had Glu residues in the middle of both transmembrane helices (Supplementary Fig.S1).
The same subunits of the N-ATPases from diverse bacteria are closely related and form distinct branches on the phylogenetic trees, well separated from the equivalent subunits of the F-and V-type ATPases (Fig.1 and Supplementary Fig.S3, see also Supplementary Fig.5 in Swingley et al., 2008).
Phylogenetic trees built for individual N-ATPase subunits showed similar topologies, so that the whole N-ATPase operons appeared to co-evolve (Supplementary Fig.S4).
Another distinct trait of the N-ATPase operons was the presence of an extra gene, urf2 (Sumi et al., 1997).
Its product is currently annotated as F1/F0 ATPase, Methanosarcina type, subunit 2 (F1F0_chp_2, TIGR03165, DH Haft, December 4, 2006) in the JCVIs Comprehensive Microbial Resource (Davidsen et al., 2010) Fig.1.
A neighbor-joining tree comparing-subunits of N-ATPases with-subunits of F-type ATPases and B-subunits of A-and V-type ATPases.
See Supplementary Fig.S3 for the full tree.
and as ATPase_F1/F0-cplx_su2_Meth-typ (IPR017581) in the InterPro database (Hunter et al., 2009), whereas UniProt uniformly annotates these proteins as Putative uncharacterized protein.
The urf2 (hereafter atpR) gene was found in every N-ATPase operon and could be used as a tell-tale sign of these operons.
Its product is a hydrophobic protein with three predicted transmembrane segments, two of which carry conserved Arg residues (Supplementary Fig.S5).
Presence of charged residues in the hydrophobic core of the membrane is rare and usually indicative of a functional role.
The N-ATPase operons also include so-called gene 1 (hereafter atpQ) that encodes another distinct membrane protein.
Although this gene was originally marked as atpI (Sumi et al., 1997) and is occasionally still labeled this way (Saum et al., 2009), its product shows no statistically significant sequence similarity to the genuine AtpI (UncI) proteins, previously described in the genomes of Escherichia coli and other model organisms.
In accordance with their distinct sequences, AtpI and AtpQ proteins are assigned to two different Pfam families, PF03899 and PF09527, respectively, and to two different COGs, COG3312 and COG5536 in the CDD database (Marchler-Bauer et al., 2009).
In contrast to atpR, the atpQ gene is not limited to the N-ATPase operons.
While the N-ATPase operon lacks the atpH gene that encodes the-subunit of the F-type ATPase, its atpF gene (atpFN ) is unusually long (Sumi et al., 1997).
We were able to align C-terminal regions of the AtpFN and AtpH gene products (Supplementary Fig.S6).
This showed that N-ATPase contains at least part of the-subunit, in agreement with the earlier analyses (Saum et al., 2009).
3.2 Lateral transfer of N-ATPase genes Presence of the N-ATPase operon in the genomes of M.barkeri and M.acetivorans, but not in the closely related M.mazei, suggested that this operon had been acquired via lateral gene transfer.
This suggestion is consistent with the gene neighborhoods of the atpDCQRBEFG operons in M.barkeri and M.acetivorans (Fig.2) and the absence of these operons in any other archaeal genomes sequenced so far.
Gene neighborhoods of the N-ATPase operons in various bacteria are also consistent with the insertion of this operon (Supplementary Fig.S7).
The widespread presence of the N-ATPase genes among diverse bacteria deprecates the historical designation of these enzymes as archaebacterial F1Fo-ATPases.
The strict conservation of the gene order and co-linearity of the phylogenetic trees for distinct N-ATPase subunits suggests that the whole operon is being transferred as a single unit.
However, the GC content of the N-ATPase operons shows a good correlation with the average 1474 [16:25 12/5/2010 Bioinformatics-btq234.tex] Page: 1475 14731476 Characterization of the N-ATPase Fig.2.
Conserved gene neighborhoods in M.acetivorans, M.mazei and M.barkeri, indicating the points of insertion of the N-ATPase operons in the former (top line, showing the subunit names) and the latter (bottom line, showing the gene names).
Orthologous genes are indicated with the same colors.
The arrows do not reflect the lengths of the genes.
GC content of the host genomes (Supplementary Fig.S8), indicating either a relatively ancient gene transfer or a rapid adaptation of the N-ATPase genes to their host environment.
An additional indication of the lateral mobility of the N-ATPase operon is its presence on plasmids, pREB4 in A.marina and pAQ7 in Synechococcus sp.
PCC 7002.
These data might be related to the earlier functional evidence of the presence of two ion-translocating ATPases in M.mazei G1.
While the A-type ATP synthase was apparently H+-dependent, the second, F-type ATPase appeared to be Na+-translocating (Becher and Mller, 1994; Pisa et al., 2007).
Although this second ATPase has not been found in the sequenced genome of M.mazei (Fig.2), the respective genes could be plasmid-borne, as is the case of N-ATPases in at least two cyanobacteria.
4 DISCUSSION Following the description of an archaebacterial F1Fo-ATPase in M.barkeri (Sumi et al., 1997), the presence of a Methanosarcina-like F-ATPase operon was repeatedly noted in bacterial genomes (Glckner et al., 2003; McInerney et al., 2007; Swingley et al., 2008), although the exact function(s) of these enzymes and their cation specificity remained obscure.
McInerney et al.
(2007) noted two F-type ATPases encoded in the genome of Syntrophus aciditrophicus and suggested that both of them were Na+-translocating (incidentally, the F-type ATPase of this organism is definitely H+-specific, whereas the cation specificity of its N-ATPase is unknown; it might be specific for Na+, see below).
In their analysis of the A.marina genome, Swingley et al.
(2008) noted the presence of a plasmid-encoded set of ATP synthase genes that were arranged into a unique operon conserved with full synteny in a remarkable array of organisms, including cyanobacteria, archaea, planctomycetes, chlorobi and proteobacteria.
The authors noted that the-subunits encoded in these operons clustered together on a phylogenetic tree and suggested that these enzymes formed a separate new family of ATP synthases.
However, they slightly overstated their case by claiming that its individual proteins do not clearly fit into any of the described families (Swingley et al., 2008).
They also disputed the idea that these enzymes were Na+-translocating.
In addition, the key observation by Daniel Haft that these enzymes always represent a second F1/Fo ATPase system has only been published online in the JCVIs Comprehensive Microbial Resource (Davidsen et al., 2010).
As a result, there still exists a significant confusion as to the phylogenetic distribution, organization and the functional role(s) of these enzymes.
The alignment in Supplementary Fig.S1 shows that, despite the doubts of Swingley et al.
(2008), the c subunit of the A.marina N-ATPase has a full set of Na+-binding ligands, including the recently recognized additional Thr residue of the ESTxxY motif (Mulkidjanian et al., 2008a; Meier et al., 2009).
While this residue is missing in c-subunits of several N-ATPases, including the one from S.aciditrophicus, a Glu residue is present instead of the Na+-coordinating Gln residue in the first transmembrane helix of the c subunit of nearly all N-ATPases (Supplementary Fig.S1).
As has been noted previously (Meier et al., 2009; Saum et al., 2009), this Glu residue could potentially provide two ligands for the Na+ ion and thereby complete the Na+ coordination shell.
If so, all these N-ATPases would end up being capable of binding Na+ ions.
A hallmark of the N-ATPase operons is the presence of the atpR gene.
Because of the low dielectric permittivity of the membrane, the strategic positioning of two Arg residues of AtpR in the hydrophobic core of the membrane (Supplementary Fig.S5) implies the presence of negatively charged residues in their vicinity.
Given the absence in the N-ATPase operons of the atpI gene, whose product was recently shown to interact with the c-ring (Suzuki et al., 2007), we suggest that the product of the AtpR gene serves essentially the same function, regulating N-ATPase assembly and/or its activity.
Just like AtpI assists c-ring assembly by directly interacting with the c-subunits (Suzuki et al., 2007), AtpR could do that through the interaction of its two Arg residues with N-ATPase-specific c-subunits, most of which carry two Glu residues in the middle of their transmembrane helices (Supplementary Fig.S1).
The observation that N-ATPases are always found alongside typical F-or A-type ATPases suggests that the N-ATPases cannot functionally replace those enzymes in their role as ATP synthases.
Indeed, in M.acetivorans, deletion of the N-ATPase operon had no visible effect on cell growth or ATP synthesis, whereas a mutant lacking theA-ATP synthase genes could not be obtained (Saum et al., 2009).
We conclude that, similarly to the eukaryotic V-ATPases, the N-ATPases do not catalyze ATP synthesis, which leaves ATP-driven ion pumping as the most plausible function for these enzymes.
By analogy with V-ATPases, the N-ATPase c-oligomer ring can be expected to consist of a smaller number of c-subunits than the c-oligomer ring of F-type ATP synthases.
Acquisition of an operon capable of extrusion of Na+ ions would be beneficial to the marine bacteria and other organisms living in high-salt environments.
Accordingly, many N-ATPase-encoding bacteria are either marine organisms or grow in the presence of salt.
Since Na+-translocating ATPases can also translocate protons (von Ballmoos et al., 2008), the N-ATPases could in principle function as outward proton pumps in low-sodium and/or acidic environments.
A typical N-ATPase operon (with untranslated AtpR) has been found in an industrial strain of Pseudomonas veronii growing on 2-butanone (Onaca et al., 2007).
The presence of N-ATPase genes in such pathogens as Burkholderia mallei appears to be inherited from their free-living relatives and might be related to their survival in blood.
Using a larger set of sequences than the one used by Swingley et al.
(2008), we have confirmed that N-ATPases branch separately from other F-type ATPases (Fig.1, Supplementary Fig.S3).
1475 [16:25 12/5/2010 Bioinformatics-btq234.tex] Page: 1476 14731476 D.V.Dibrova et al.
This separate branching suggests a possible early divergence of the N-ATPases, which is compatible with the following, supposedly ancestral traits of these enzymes: (i) Both AtpFN subunit of the N-ATPase and the E subunit of the peripheral stalk of the V/A-type ATPases correspond to a fusion of b-and-subunits of the typical F-ATPases (Pallen et al., 2006).
(ii) The presence of the second membrane-embedded Glu residue is consistent with the evolution of the c-subunit from a duplication of an amphiphilic helix that contained a Glu residue in the middle (Davis, 2002).
Similar two-Glu c-subunits are found in the A-type ATPases of methanogens and F-type ATPases of Thermotogae.
(iii) Outward pumping of Na+ ions, the predicted function of N-ATPases, appears to be an ancient trait and has been previously suggested as a function of the common ancestor of the F-and V-type ATPases (Mulkidjanian et al., 2007; 2008a, b; 2009).
All these features, which N-ATPases share either with V/A-ATPasesto the exclusion of most F-ATPasesor with the putative common ancestor of all rotary ATPases, suggest that N-ATPases represent a distinct early-diverging family of rotary ATPases.
Thus, the Na-translocating common ancestor of all F-type ATPases apparently gave rise to two different families of ATPases: (i) the reversible ATPases/ATP synthases (genuine F-ATPases) and (ii) ATP-driven ion pumps (N-ATPases).
In conclusion, the N-ATPases (until now usually referred to as archaebacterial F1Fo-ATPases) are encoded in an apparently highly mobile operon that, most likely, confers on its hosts the ability of ATP-driven outward pumping of Na+ ions, which complements the H+ specificity of the native chromosome-encoded F-ATPase (or A-ATPase).
We predict that, similarly to the eukaryotic V-ATPases, the N-ATPases do not catalyze ATP synthesis, which is why an N-ATPase is never found alone in a genome, only as the second enzyme in cells that already encode an F-type or a A-type ATP synthase.
Accordingly, caution should be exercised when referring to their components as subunits of the ATP synthase.
We believe that experimental verification of the predicted functions of the N-ATPases would be useful for the proper annotation of these interesting enzymes and could also help in understanding the evolution of energy conservation mechanisms.
ACKNOWLEDGEMENTS We thank Eugene Koonin and Thomas Meier for discussion.
Funding: Deutsche Forschungsgemeinschaft (to D.V.D., A.Y.M.
); German Academic Exchange Service (to D.V.D.
); Intramural Research Program of the NLM at the National Institutes of Health (to M.Y.G.).
Conflict of Interest: none declared.
ABSTRACT Summary: RefProtDom provides a set of divergent query domains, originally selected from Pfam, and full-length proteins containing their homologous domains, with diverse architectures, for evaluating pair-wise and iterative sequence similarity searches.
Pfam homology and domain boundary annotations in the target library were supplemented using local and semi-global searches, PSI-BLAST searches, and SCOP and CATH classifications.
Availability: RefProtDom is available from http://faculty.virginia.edu/ wrpearson/fasta/PUBS/gonzalez09a Contact: miledywgonzalez@gmail.com; pearson@virginia.edu Received on January 13, 2010; revised on June 8, 2010; accepted on July 18, 2010 1 INTRODUCTION Evaluation and improvement of protein sequence similarity searches, using algorithms such as BLAST or Smith-Waterman (SSEARCH) and more sophisticated searches such as PSI-BLAST or HMMER (Altschul et al., 1997; Durbin, 1998; Smith and Waterman, 1981), require query sequences and reference sets curated to accurately reflect homology relationships.
Because structural similarity is preserved well beyond sequence similarity (Gibrat et al., 1996), protein structures are often the gold standard for annotating homology relationships.
Although both structure-based homology annotations and manually annotated protein sequence relationships can very accurately record homology relationships, they do not reflect common practice in protein similarity searching, which is to characterize unknown proteins by searching large, comprehensive protein sets such as RefSeq (Pruitt et al., 2007) and UniProt (Consortium, 2009).
To better characterize similarity searching strategies, in particular, PSI-BLAST performance, against comprehensive protein databases, we identified a set of diverse protein domains from Pfam (Finn et al., 2010) v. 21 to use as queries against a set of real proteins containing those domains.
Our query domain families are taxonomically broad (to provide harder homology detection cases), and have long models (to better simulate full-length protein searches).Although we cannot be certain that all homologs have been found, we believe that statistically significant pair-wise alignments are annotated correctly.
To whom correspondence should be addressed.
2 DATABASE ASSEMBLY Evaluation datasets: from 681 initial Pfam (v. 21) families that met criteria for: (i) domain length (>200 residues); (ii) taxonomic diversity (present in two of bacteria, archaea and eukarya); (iii) family size (>100 instances); and (iv) available structure, we selected 344 query Pfam families after merging families that belonged to the same clan (Gonzalez and Pearson, 2010).
In this initial set, 81 families belonged to distinct clans, while 263 families did not have an associated clan.
This set was reduced to 320 non-homologous domains using information from Pfam (v. 23) (by Pfam v. 24, these domains belonged to 112 distinct clans with 168 families not in clans, for 280 non-homologous domains).
The target library was built from 234 505 full-length UniProt proteins (excluding viral sequences) containing Pfam v. 21 homologs to the original 320 Pfam families together with 1627 other domain families.
Two query sets were constructed and the members of these sets evaluated further: (i) a challenging query subset (50 hard) with the lowest family coverage with BLAST; and (ii) a randomly sampled representative query set (50 sampled with replacement).
Annotation extensions: when the original Pfam v. 21 annotations were used to characterize searches with our hard and sampled queries against the target library, thousands of alignments to very similar UniProt sequences (e.g.
E()<1080, with >95% identity) were annotated as partial homologs or non-homologs.
To correct these conservative annotations, we compared the bare domain query sequences to the target library using SSEARCH and GLSEARCH (a program that produces an alignment that is global in the query sequence but possibly local in the target or library sequence).
We identified all the significantly similar sequence regions (E()<0.001) with SSEARCH that were either shorter or unannotated in Pfam v. 21 and calculated the boundaries using GLSEARCH.
We extended annotations on 2106 partial domains and added 24 604 domain homology annotations based on SSEARCH alignments, 13 574 of which were included in Pfam v. 24.
RefProtDom describes relationships and alignment boundaries between query domains and the target library homologs according to Pfam v. 21, Pfam v. 24 and the SSEARCH/GLSEARCH alignment boundaries.
Although SSEARCH/GLSEARCH searches against the target library dramatically reduced the number of apparent false positives with very low E()-values, additional searches with PSI-BLAST using the queries sometimes found unrelated UniProt sequences with significant (E()<1040) scores.
We analyzed all significant (E()<104) non-homologous alignments found in the first three iterations of PSI-BLAST for the 100 queries (94 distinct families).
Non-homologous alignments to regions with no annotated Pfam domains were used as queries in reciprocal PSI-BLAST searches for three iterations.
Reciprocal searches that recovered at least 25% of a domain family were annotated as homologous, yielding 375 additional homology annotations across 33/94 families.
Structures of significant non-homologs that mapped to unrelated Pfam families were examined in SCOP and CATH; if they shared the same SCOP fold or CATH topology they were annotated as homologs.
For example, Pfam annotates a The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:55 11/8/2010 Bioinformatics-btq426.tex] Page: 2362 23612362 M.W.Gonzalez and W.R.Pearson PF00346 domain for residues 295537 on the Q8ZMJ0_SALTY sequence.
RefProtDom also annotates PF00374 in the same region (390535) because both domains share the same SCOP fold and superfamily classifications (e.18.1).
This structural annotation is also reported by SCOOP (Bateman and Finn, 2007).
We found structural evidence to add 37 additional clans; Pfam v. 24 matches 14 of those 37 structural clans.
These additional clans would reduce the number of non-homologous domains in our query set from 94 to 90, but the families were not combined, the cryptic homology was simply annotated.
Structure classifications yielded 2124 additional homology annotations across 16/94 queries.
3 SUMMARY Iterative similarity searches are usually performed against full-length proteins with complex domain architectures.
Evaluating similarity-searching methods against benchmarking sets with incomplete or missing annotations can introduce dramatic statistical inaccuracies.
RefProtDoms greatest strength is its use of a taxonomically diverse set of full-length, multi-domain, proteins in the target library.
Searches against RefProtDom resemble searches against SwissProt, UniProt or RefSeq (though those databases are much larger).
Moreover, the query sequences are evolutionarily independent; based on structural comparisons, 90 query domains are non-homologous.
Thus, RefProtDom can simulate searches against comprehensive sequence databases while evaluating success on challenging homologies.
Pfam is a powerful resource for identifying homology relationships and domain boundaries, but strategies that use a single hidden Markov model (HMM) to identify every homolog will be challenged by distant sequences at the detection horizon for the model.
For many families, Pfam has addressed this problem by grouping families into clans.
But, sometimes homologs are missed; sequences that share strong similarity across the length of a domain to an annotated homolog are surely homologous, even if they do not produce a significant score against the HMM model.
The RefProtDom query and target libraries seek to reduce the number of un-annotated homologies with statistically significant similarities, and to more accurately estimate homologous domain boundaries.
Although our curation may have missed some homologs, we are confident in the homologies we annotate.
Homology is annotated for domains that share significant pair-wise similarity, show significant family coverage after three PSI-BLAST iterations, or when they share structures.
Domain boundaries were revised based on significant local or global similarity.
By combining single domain queries with full-length, multi-domain proteins, RefProtDom can highlight alignment errors and evaluate improvements in alignment accuracy.
Accurate boundary annotation has been largely overlooked in pair-wise sequence comparison, because incorrect alignment boundaries rarely detract from the identification of homologous proteins.
Pfams annotations are now generated with HMMER3, which only performs local alignments (Finn et al., 2010).
Therefore, one might expect that future Pfam annotations may have an even harder time at identifying complete domains, and thus, should continue to benefit from the extension curation provided by RefProtDom.
Nonetheless, HMMER3 compensates with increased sensitivity as a result of better statistics.
In fact, 59% of the domain extensions and 55% of the missed homologs added to Pfam v. 21 using our protocol were incorporated in Pfam v. 24.
Thus, Pfam v. 24, using HMMER3, has independently addressed many of the missing annotations, validating our approach.
However, we believe that the problems inherent in using a single model for diverse protein family searches will always miss homologs and domain boundaries that can be found with individual domains across the familys phylogenetic tree.
We plan to continue to update the homology relationships and boundary assignments in RefProtDom.
For iterative sequence comparison methods, alignment accuracy is crucial; inaccurate alignments can cause non-homologous domains to be included in the profiles and decrease their specificity in subsequent iterations.
Using RefProtDoms annotations, we identified a previously unrecognized alignment overextension error in PSI-BLAST responsible for the corruption of its PSSMs and its poor specificity (Gonzalez and Pearson, 2010).
Additional evaluations with RefProtDom revealed that while JACKHMMER (HMMER3s iterative implementation) is susceptible to the same error, it overextends more slowly and, thus, shows better performance than unmodified PSI-BLAST (M.W.G.
and W.R.P., manuscript in preparation).
Domains are the basic units of protein function and evolution; thus, improved homology detection requires improved domain alignment accuracy.
Large-scale automatic annotation of gene function is limited by local alignments incomplete motif matches and fuzzy domain boundaries (Kann et al., 2007).
Establishing homology is central to a wide array of bioinformatics methodologies; improved domain alignments can improve 3D protein structural predictions that use homology modeling, and also clarify how protein domain networks interact to generate disease phenotypes.
RefProtDom provides a comprehensive set of full-length UniProt proteins that can be used to evaluate domain alignment accuracy.
Funding: National Library of Medicine, grant LM04969.
Conflict of Interest: none declared.
ABSTRACT Motivation: Most anatomical ontologies are species-specific, whereas a framework for comparative studies is needed.
We describe the vertebrate Homologous Organs Groups ontology, vHOG, used to compare expression patterns between species.
Results: vHOG is a multispecies anatomical ontology for the vertebrate lineage.
It is based on the HOGs used in the Bgee database of gene expression evolution.
vHOG version 1.4 includes 1184 terms, follows OBO principles and is based on the Common Anatomy Reference Ontology (CARO).
vHOG only describes structures with historical homology relations between model vertebrate species.
The mapping to species-specific anatomical ontologies is provided as a separate file, so that no homology hypothesis is stated within the ontology itself.
Each mapping has been manually reviewed, and we provide support codes and references when available.
Availability and implementation: vHOG is available from the Bgee download site (http://bgee.unil.ch/), as well as from the OBO Foundry and the NCBO Bioportal websites.
Contact: bgee@isb-sib.ch; frederic.bastian@unil.ch Received on September 9, 2011; revised on January 13, 2012; accepted on January 23, 2012 1 INTRODUCTION One of the main approaches to understand biological objects has long been comparative studies, from comparative anatomy in the 18th century to comparative genomics in the last decade.
Comparative analysis can notably help identify adaptation, as well as functional or structural constraints (Harvey and Pagel, 1991).
Many of the data which we would like to compare, such as gene expression or phenotypes, need to be mapped to anatomy and development of organisms to be of use.
To facilitate the automatic manipulation of this data, there has been an important effort to build ontologies, which describe the anatomy of human and of animal model organisms (Bard, 2008).
These ontologies have tended to be species-specific, resulting in an increasing number of ontologies corresponding to different projects (see the OBO Foundry and the NCBO Bioportal, Noy et al., 2009; Smith et al., 2007).
This makes To whom correspondence should be addressed.
Present address: Department of Human Genetics, The University of Chicago, Cummings Life Science Center, 920 East 58th Street-CLSC 325C Chicago, IL 60637, USA.
the comparison between species difficult, since differences in representational schemes and in vocabulary are added to the differences in biology.
Yet automatic comparisons are increasingly necessary, with large amounts of functional data generated in diverse model organisms.
An integrated view is advantageous both for a fundamental understanding of animal biology and evolution, and for the efficient transfer of information from model organisms to human or veterinary medicine.
Multispecies integration within anatomical ontologies poses a number of challenges.
One is the criterion of comparison.
While comparative studies can be performed in diverse frameworks, homology is the most widely recognized criterion (Hall, 1994).
This raises further problems.
First, homology is always a hypothesis (Haendel et al., 2008), which according to the principle of reality followed by the OBO Foundry should not be included within an ontology (Smith and Ceusters, 2010) (although see Merrill, 2010).
Second, there exist structures which will not be included in a homology comparison between a given pair of species, because they are specific to one or the other, and have no homolog.
Third, the exclusion of analogous structures might be limiting for some studies (e.g.
comparing insect and vertebrate eye development).
Fourth, homologous structures can diverge in function or structure, to an extent that representing them together in an ontology might be difficult.
Finally, there can be differences between species in the relationships among structures (Haendel et al., 2008).
There are several ongoing efforts to create multispecies ontologies for animal groups, which have chosen different answers to the challenges outlined above.
The Teleost Anatomy Ontology (TAO) (Dahdul et al., 2010) is a multispecies ontology for teleost fishes.
The TAO is based on the Zebrafish Anatomy Ontology (ZFA) (from the ZFIN database, Bradford et al., 2011), and uses general (higher level) terms from the Common Anatomy Reference Ontology (CARO) (Haendel et al., 2008).
The CARO was created to provide a common basis for all future anatomy ontologies and facilitate their interoperability.
Several other efforts follow the same model as the TAO, and include the Amphibian Anatomy Ontology and the Hymenoptera Anatomy Ontology.
In each of these cases, there is an effort to describe the morphological diversity of the clade.
Consequently, each ontology will include terms that are found in several species of the clade.
The use of a term for several species in the TAO does not imply homology (Dahdul et al., 2010).
A species-specific ontology (e.g.
ZFA) is considered a subset of the multispecies ontology (e.g.
TAO).
The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: ES MANUSCRIPT CATEGORY: ORIGINAL PAPER [13:05 12/3/2012 Bioinformatics-bts048.tex] Page: 1018 10171020 A.Niknejad et al.
A different approach is taken by the Uberon project (Mungall et al.
2012), which maps terms from several animal anatomy ontologies, but also anatomy-related terms from the Gene Ontology or medical ontologies, and other multispecies ontologies such as the TAO or our organs groups (Bastian et al., 2008).
Uberon aims to provide anatomical information in relation to the Gene Ontology and the Cell Ontology, and is neutral relative to the criterion of homology (C.Mungall, personal communication).
Uberon thus groups structures based on a criterion of similarity (Dahdul et al., 2010; C.Mungall, personal communication) that is the parent concept of homology, and also of homoplasy or of functional similarity (Roux and Robinson-Rechavi, 2010).
Despite the issues raised by the use of the criterion of homology, we feel that it also presents important advantages.
First, restricting to one criterion allows a clear interpretation of the ontology when used in a database; it especially allows a clear use of automatic reasoning.
Second, homology is transitive, which allows us to form an ontology of organs groups, rather than encode all pairwise relations between terms.
Finally, it is the one criterion that permits correct formulation of hypotheses of adaptation or constraints in evolution (Harvey and Pagel, 1991).
Our software Homolonto to align ontologies (Parmentier et al., 2010) generates a multispecies ontology of Homologous Organs Groups (HOGs).
These HOGs are used in our database of gene expression evolution, Bgee (Bastian et al., 2008), as well as in Uberon.
Mappings in the HOGs are restricted to manually curated relations of homology.
We use a strict definition of historical homology: Homology that is defined by common descent (HOM:0000007, Roux and Robinson-Rechavi, 2010).
The homology in Bgee has allowed the comparison of expression patterns between species in several applications, such as the characterization of gene interactions in development (Comte et al., 2010), the study of orthologs and paralogs (Huerta-Cepas et al., 2011) (Bastian,F.B.
et al., unpublished data) or the study of microRNA evolution (Roux et al., unpublished data).
The HOGs used in Bgee are part of the database schema, are constrained according to the database optimization, and are not formatted for easy external use.
Yet, it is also desirable to provide an ontology which is optimized for inter-operability and reuse by the community.
Thus, we present here a CARO-compliant version of the HOG ontology, with all terms and relations carefully curated.
The present version of this ontology is limited to vertebrates.
Based on the alignment of two mammals, one frog and one fish, it covers many of the morphological terms needed to describe most vertebrates.
Thus, we present both the first large and high-quality ontology of vertebrate anatomy, and the first ontology strictly limited to homologous structures: vHOG.
2 RESULTS All terms of the vHOG ontology were manually curated, to verify that they correspond to groups of homologous organs between vertebrate species, linked by relations of strict historical homology, as defined in the HOM ontology (Roux and Robinson-Rechavi, 2010).
When the structures are homologous but divergent, we use a combination of terms, such as limb-fin bud (VHOG:000125).
Relations between the terms were also manually curated (see algorithm in Parmentier et al., 2010), and set either to is_a or to part_of.
We aim to provide exactly one is_a relation for each term, according to OBO Foundry guidelines.
As noted in the TAO publication (Dahdul et al., 2010), it is at present difficult to implement this guideline in practice for anatomical ontologies.
In future versions of the vHOG, we will continue working toward this aim.
The terms and relations thus generated were incorporated into the framework of the CARO (Haendel et al., 2008).
We were careful to implement this in a manner consistent with other anatomical ontologies using CARO: TAO, XAO and ZFA.
This especially concerns the relations between vHOG terms derived from species-specific ontology alignments and higher level CARO terms, e.g.
digit is_a anatomical cluster.
The vHOG ontology version 1.4 (December 2011) has 1184 terms, 506 is_a relations and 1181 part_of relations.
There are 664 synonyms; 547 terms have definitions; 67% of terms have only one parent, 25% have two parents, and the others have three or more.
There are 493 cross-references (xref) to other multispecies OBO ontologies.
These do not include the mappings to species-specific ontologies, as we consider that mappings from multispecies ontologies to species-specific ontologies should not be treated as xrefs, but should be encoded separately in an association file, similar to the Gene Ontology Annotation mapping (Barrell et al., 2009).
This differs from the approach of Uberon, which includes cross-references to the source ontologies in the multispecies ontology itself (Dahdul et al., 2010).
All mappings of terms from the species-specific ontologies to the vHOG terms were manually curated.
There are 5129 terms from species-specific ontologies mapped to 1169 vHOG terms, which represent 2259 hypotheses of homology between vertebrates (i.e.
2259 pair-wise relations between structures in different species).
There are 15 vHOG terms with no mappings, which correspond to higher level CARO terms (e.g.
material anatomical entity).
The semantics of the mapping is treat-xrefs-as-equivalent for CARO, and equivalent to treat-xref-as-genus-differentia for the mapping to species-specific ontologies.
The mappings were annotated with support codes to provide confidence information (Table 1).
Means to provide confidence metadata information in support of annotations are currently being discussed in the framework of the Evidence Code Ontology (ECO) (Gene Ontology Consortium, 2010) and of the International Society for Biocuration.
Our objective is to rely on the use of the ECO, once a standard to address the issue of confidence has emerged in the biocuration and ontology communities.
In practice, the support codes obvious and inferred are essentially used for homology between the two mammals considered in our ontology: 97% of all mappings with the code inferred are for human and mouse.
All mappings with the code obvious are for human and mouse, except whole organism which has the code obvious for all ontologies.
Among the vHOG groups noted debated, several concern bones and the skeletal system, due to citations such as the following: Whether this biomineralization toolkit of genes reflects a parallel co-option of a common suite of genes or the inheritance of a skeletogenic gene regulatory network from a biomineralizing common ancestor remains an open debate (Murdock and Donoghue, 2011) (more references in the association file).
Another case of debate is the vomeronasal organ (VHOG:0000665).
None of the three bibliographical references cited in the association file is conclusive concerning the existence and homology of this 1018 Copyedited by: ES MANUSCRIPT CATEGORY: ORIGINAL PAPER [13:05 12/3/2012 Bioinformatics-bts048.tex] Page: 1019 10171020 vHOG Table 1.
Support for mapping of species-specific anatomical ontologies to vHOG Support code Meaning
ABSTRACT Mitochondria must uptake some phospholipids from the endoplasmic reticulum (ER) for the biogenesis of their membranes.
They convert one of these lipids, phosphatidylserine, to phosphatidylethanolamine, which can be re-exported via the ER to all other cellular membranes.
The mechanisms underlying these exchanges between ER and mitochondria are poorly understood.
Recently, a complex termed ERmitochondria encounter structure (ERMES) was shown to be necessary for phospholipid exchange in budding yeast.
However, it is unclear whether this complex is merely an inter-organelle tether or also the transporter.
ERMES consists of four proteins: Mdm10, Mdm34 (Mmm2), Mdm12 and Mmm1, three of which contain the uncharacterized SMP domain common to a number of eukaryotic membrane-associated proteins.
Here, we show that the SMP domain belongs to the TULIP superfamily of lipid/hydrophobic ligand-binding domains comprising members of known structure.
This relationship suggests that the SMP domains of the ERMES complex mediate lipid exchange between ER and mitochondria.
Contact: andrei.lupas@tuebingen.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
Received on February 15, 2010; revised on June 5, 2010; accepted on June 7, 2010 1 INTRODUCTION Mitochondria are organelles of endosymbiotic origin, found in virtually all eukaryotic organisms.
They are the main generators of adenosine triphosphate (ATP)the energy currency of the cell.
Additionally, they participate in a series of other important processes, including apoptosis, amino acid and lipid metabolism, ironsulphur cluster assembly and the regulation of calcium levels within the cell (Lill and Kispal, 2000; McBride et al., 2006).
However, only a small fraction of the biopolymers required to carry out these functions is synthesized in the mitochondria, the rest must be imported from the outside.
For example, they only produce some of the phospholipids that make up their membranes, whereas the remainder originates from the endoplasmic reticulum (ER).
Interestingly, mitochondria not only import phospholipids but also export a particular one, phosphatidylethanolamine (PtdEtn), to the ER, where To whom correspondence should be addressed.
Fig.1.
Domain organization of the four ERMES proteins.
All ERMES proteins, except the mitochondrial outer membrane protein (OMP) Mdm10, contain an SMP domain.
The SMP domain in Mdm34 was discovered in this study.
it is methylated to form the phospholipid phosphatidylcholine (Voelker, 2003).
Mitochondria synthesize PtdEtn by decarboxylating phosphatidylserine, a phospholipid imported from the ER.
The mechanisms responsible for the influx and efflux of phospholipids are unclear.
Unlike most organelles, mitochondria do not exchange phospholipids via vesicular transport.
Previous studies have suggested that this exchange takes place via ERmitochondria associations (Achleitner et al., 1999; Voelker, 2003).
More recently, Kornmann et al.
(2009) identified a complex, the ERmitochondria encounter structure (ERMES), that acts as a molecular tether between ER and mitochondria in Saccharomyces cerevisiae and is required for efficient inter-organelle phospholipid exchange.
However, it remained unclear whether ERMES merely tethers these organelles together, thereby aligning proteins that carry out the actual transport, or also recruits and transfers phospholipids itself.
ERMES comprises four proteins (Fig.1): the mitochondrial outer membrane protein Mdm10, the putative outer membrane protein Mdm34 (Mmm2), the ER-resident Mmm1 and the cytosolic Mdm12 (Kornmann et al., 2009).
These proteins have also been implicated in other mitochondrial functions, including morphology maintenance (Okamoto and Shaw, 2005) and protein import (Meisinger et al., 2004).
Two of the ERMES components, Mmm1 and Mdm12, were reported to contain the uncharacterized SMP domain (synaptotagmin-like, mitochondrial and lipid-binding proteins), which is also present in a number of other eukaryotic membrane-associated proteins (Lee and Hong, 2006).
SMP domain-containing proteins have been classified into four broad groups: C2 domain synaptotagmin-like, PH domain-containing HT-008, PDZK8 and mitochondrial protein families (Lee and Hong, 2006).
The functions of these proteins are poorly understood.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:24 23/7/2010 Bioinformatics-btq326.tex] Page: 1928 19271931 K.O.Kopec et al.
Fig.2.
Pairwise HMM comparison of SMP and TULIP domains.
Representatives of the four SMP domain-containing groups and of TULIP domains were chosen from Arabidopsis thaliana (At), Caenorhabditis elegans (Ce), Drosophila melanogaster (Dm), Dermatophagoides pteronyssinus (Dp), Epiphyas postvittana (Ep), Galleria mellonella (Gm), Homo sapiens (Hs) and Saccharomyces cerevisiae (Sc).
Group and protein names of these representatives, along with domain boundaries and names of source species are indicated (from left to right).
HHpred was used to perform pairwise HMM comparisons between them.
Cell color indicates the HHpred probability of the match as depicted in the scale on the right; probabilities <20% are shown as white cells.
Proteins with known structures are marked with an asterisk.
The abbreviations of the hydrophobic ligand binders are explained in the text.
In this study, we show that the SMP domain belongs to a superfamily of lipid/hydrophobic ligand-binding domains of known structure, which we call TULIP for tubular lipid-binding proteins, and propose a role for it in cellular phospholipid traffic.
2 METHODS All sequence similarity searches were carried out in the MPI bioinformatics toolkit (http://toolkit.tuebingen.mpg.de; Biegert et al., 2006) using HHpred (Sding et al., 2005) and HHsenser (Sding et al., 2006) with default settings.
HHpred searches were performed against a database comprising PDB70 (protein databank structures, as available on the 15th of April 2010, clustered at 70% sequence identity) and genomes of phylogenetically diverse organisms (Arabidopsis thaliana, Caenorhabditis elegans, Drosophila melanogaster, Homo sapiens and S.cerevisiae).
Representatives of the four SMP domain-containing groups from the aforementioned organisms were chosen as seeds for the searches in Figures 2 and 3, based on their presence in the core of their respective clusters in the sequence cluster map (Fig.5).
To identify sequences for cluster analysis, we searched the non-redundant protein sequence database (nr) at NCBI for homologs of the SMP domain from the yeast protein Mmm1 (residues 196409), the N-terminal domain of human cholesteryl ester transfer protein (2OBD, residues 16206), the Takeout 1 protein from Epiphyas postvittana (3E8T), and the dust mite allergen Der p 7 (3H4Z) using HHsenser.
We pooled the permissive sets returned by HHsenser to obtain 2033 sequences, which we clustered by their pairwise BLAST P-values (Altschul et al., 1990) in CLANS (Frickey and Lupas, 2004).
Clustering was done to equilibrium in 2D at a P-value cutoff of e-4 using default settings.
3 RESULTS The number of structural solutions available to a polypeptide chain is limited, making protein structures multiply convergent (Cheng Fig.3.
Fold predictions for SMP domains.
The highest scoring PDB matches for the 16 representative SMP sequences in Figure 2 were collected with three of the top-scoring prediction servers in CASP8.
Top matches to TULIP domains are shown in blue and to any other structure in red.
The color saturation is scaled linearly between the maximum and minimum scores returned by the respective method.
The value ranges corresponding to pale (low confidence), medium and dark (high confidence) saturation are: Phyre-estimated precision 033, 3466 and 67100, MULTICOM e-value 7.4 5, 5.12.6, 2.50, MUSTER Z-score 01.8, 1.93.5, 3.65.3.
The number of matches to TULIP domains against the total is shown in the right-hand column.
et al., 2008; Krishna and Grishin, 2004; Salem et al., 1999), while the combinatorial possibilities in sequence space are nearly endless.
For this reason, sequence similarity is considered the primary marker of homology.
We thus used sensitive sequence comparisons, as implemented in HHpred, to find homologs of the SMP domain in a database concatenating several complete genomes with PDB70 (see Methods section).
The search was seeded with the SMP domain from Mmm1.
The best hits were to other proteins that have previously been described to contain this domain (Fig.2).
In addition, we detected a hitherto unknown SMP domain in the ERMES protein Mdm34.
This protein has been reported as an 1928 [17:24 23/7/2010 Bioinformatics-btq326.tex] Page: 1929 19271931 Homology of SMP domains to the TULIP superfamily integral outer membrane protein (Youngman et al., 2004), but we were unable to identify any sequence motifs in it that would indicate membrane insertion.
HHpred searches with other representatives and reciprocal searches with Mdm34 confirmed the presence of an SMP domain, raising the number of SMP domains in ERMES to three (Fig.1).
We also found statistically significant matches to many eukaryotic proteins from the bactericidal/permeability-increasing protein-like (BPI-like) family (Fig.2), including two with known structures: BPI (1EWF) and cholesteryl ester transfer protein (CETP; 2OBD).
Other members of this family are lipopolysaccharide-binding protein (LPSBP), lipid-binding serum glycoprotein (LBSGP), phospholipid transfer protein (PLTP) and long and short paralogs of palate, lung and nasal epithelium carcinoma-associated protein (PLUNC).
Some of these proteins have been shown to bind lipids, e.g.
CETP facilitates lipid transport between different lipoproteins (Qiu et al., 2007).
BPI and CETP have similar structures, each containing two tandem domains that adopt the same fold, comprising a long-helix wrapped in a highly curved anti-parallel-sheet.
All BPI-like proteins contain these two domains, the only exception being short PLUNC, which has only one.
The domains show little sequence identity (<15%) and sequence comparisons do not yield significant matches between them.
Instead, the C-terminal domain only shows matches to the Aha1 protein, a co-chaperone of Hsp90 in eukaryotes which shares the same fold (1USU; d.83.2).
Nevertheless, the N-and C-terminal domains of BPI-like proteins are thought to have a common ancestry based on their structural similarity (Kleiger et al., 2000).
The Structural Classification of Proteins database (SCOP; Murzin et al., 1995) also considers them to be homologous and classifies them into the same family (d.83.1.1).
HHpred searches with SMP domains yielded many statistically significant matches to the N-terminal domain of BPI-like proteins (Fig.2), but not to the C-terminal domain.
We confirmed these findings with reciprocal searches using both domains of BPI-like proteins.
From the statistical significance of these matches, we conclude that SMP domains are homologous to BPI-like proteins and therefore predict that they share the same tubular fold and lipid-binding properties.
Further searches with the N-terminal domain of BPI-like proteins retrieved three more proteins of known structure: dust mite allergen Der p 7 (3H4Z), a juvenile hormone-binding protein from Galleria mellonella (JHBP, 2RCK), and a Takeout 1 protein from Epiphyas postvittana (3E8T).
These proteins are exclusively found in arthropods and are involved in binding hydrophobic ligands.
They are composed of a single domain homologous to the N-terminal domain of BPI-like proteins (Supplementary Fig.S1), a relationship that has been described previously (Hamiaux et al., 2009; Kolodziejczyk et al., 2008; Mueller et al., 2010).
In view of their similarities in sequence and structure, we propose to group the arthropod proteins together with the BPI-like family into the TULIP superfamily.
To confirm the membership of SMP domains in the TULIP superfamily, we generated fold predictions for 16 representative sequences using the servers Phyre (Kelley and Sternberg, 2009), MULTICOM (Wang et al., 2010) and MUSTER (Wu and Zhang, 2008), all of which performed very well in the most recent Critical Assessment of Structure Prediction Experiment, CASP8 (Kryshtafovych et al., 2009).
All three methods yielded many highest-scoring matches to TULIP domains (Fig.3 and Supplementary Fig.S2).
Additionally, we queried the fold prediction metaserver I-TASSER (Roy et al., 2010), which was the top performing server in CASP 7 & 8 (Zhang, 2007, 2009).
This server returned a TULIP domain as the top match for 10 of 16 queries and as one of the top three matches for all but one query (Supplementary Fig.S3).
These matches included both BPI-like and Takeout-like proteins.
A structure-assisted multiple sequence alignment of SMP domains to TULIP domains of known structure highlights the basis for these matches (Fig.4).
All sequences have similar length, distribution of (predicted) secondary structure, and pattern of hydrophobic residues.
However, there are no conserved sequence motifs, unsurprisingly as such motifs are not even detectable within individual families (Beamer et al., 1997; Kolodziejczyk et al., 2008).
To explore the relative positions in sequence space of proteins of the TULIP superfamily, we searched for homologs of SMP domains, N-terminal domains of BPI-like proteins, as well as allergens and Takeout proteins in the nr database using HHsenser, and clustered the obtained sequences in CLANS (see Methods section).
The resulting cluster map (Fig.5) shows three distinct but connected regions corresponding to SMP, BPI and Takeout-like domain families, confirming the proposed homology between them.
In addition to the SMP groups described by Lee and Hong (2006) and the group of Mdm34 proteins described in this article, the clustering revealed a further SMP group, the uncharacterized transmembrane 24 proteins.
It also yielded a number of additional groups of BPI-like proteins, including the expression site-associated gene 5 proteins (ESAG5) from trypanosomes, whose homology to BPI-like proteins has been reported previously (Barker et al., 2008).
BPI and Takeout-like domains are connected by the arthropod allergens, one form of which is unique in containing tandem domains with clear sequence similarity, indicating a domain duplication that occurred in insects (yellow and orange clusters in Fig.5).
4 CONCLUSIONS In this study, we have shown that the SMP domain belongs to the TULIP domain superfamily, a large group of proteins that bind lipids and other hydrophobic ligands within a central, tubular cavity (Fig.6).
In several cases (CETP, PLTP), members of this superfamily are known to exploit this binding activity in order to mediate lipid trafficking.
Given the extensive lipid exchange between the ER and the mitochondrial outer membrane and the location of the ERMES complex as a connector between them, it is attractive to consider that this exchange is mediated by the SMP domains of the ERMES subunits.
As the ERMES complex does not include a nucleotidase that could energize this process, we propose that it proceeds along an affinity gradient, amounting to facilitated diffusion.
Although this could be envisaged as resulting from many short, structurally unspecific contacts between the SMP domains (kiss-and-run mechanism), we prefer to consider that the domains assemble into structurally well-defined complexes, which establish a lipophilic, tubular path between the two membranes.
Since the stoichiometry of subunits within the ERMES complex is currently unknown, it is however not possible at this time to judge on whether 1 : 1 : 1 or some other ratio would most appropriately describe the composition of such complexes.
1929 [17:24 23/7/2010 Bioinformatics-btq326.tex] Page: 1930 19271931 K.O.Kopec et al.
Fig.4.
Multiple sequence alignment of TULIP domains.
An alignment comprising representatives of the SMP domain family and TULIP domains of known structure is shown.
Sequences are labeled as in Figure 2.
The alignment was generated by a three-step approach.
First, a multiple alignment of SMP sequences was obtained using HHpred in local maximum accuracy (MAC) alignment mode.
Second, a structure-based sequence alignment of TULIP domain structures was derived from a multiple structure superimposition calculated using MAMMOTH-mult (Lupyan et al., 2005).
In the final step, these two alignments were merged manually using as guide an alignment between 1EWF and Mmm1 obtained with HHpred.
The-helices are shown in red and-strands in blue.
Secondary structure predictions for the SMP domains were performed with Ali2D (Biegert et al., 2006).
Numbers in parentheses represent length of omitted segments.
Positions in the alignments that are highly conserved or strongly hydrophobic are shown in boldface.
Residues that could not be aligned in structure or sequence are shown in lower case.
Fig.5.
Cluster map of the TULIP domain superfamily.
We searched for relatives of SMP domains, Takeout proteins, dust mite allergens and N-terminal domains of BPI-like proteins in the non-redundant database using HHsenser and clustered them in CLANS based on their all-against-all pairwise similarities as measured by BLAST P-values.
Dots represent sequences.
Sequences within one group are indicated by the same color; sequences that could not be assigned to a group are not colored.
Line coloring reflects BLAST P-values; the darker a line, the lower the P-value.
Protein families containing members with known structure are indicated with an asterisk.
The lipid-binding proteins cluster (LBP) also comprises LPSBP and LPSBP.
Abbreviations are as in the text.
Accession details for representatives of all clusters are provided in Supplementary Table 1.
1930 [17:24 23/7/2010 Bioinformatics-btq326.tex] Page: 1931 19271931 Homology of SMP domains to the TULIP superfamily Fig.6.
View along the ligand-binding tunnel of a Takeout protein (3E8T, residues 5211).
The ligand is shown as red sticks.
Funding: This work was supported by institutional funds from the Max-Planck-Society.
Conflict of Interest: none declared.
ABSTRACT Summary: The Sample avAILability systemSAILis a web based application for searching, browsing and annotating biological sample collections or biobank entries.
By providing individual-level information on the availability of specific data types (phenotypes, genetic or genomic data) and samples within a collection, rather than the actual measurement data, resource integration can be facilitated.
A flexible data structure enables the collection owners to provide descriptive information on their samples using existing or custom vocabularies.
Users can query for the available samples by various parameters combining them via logical expressions.
The system can be scaled to hold data from millions of samples with thousands of variables.
Availability: SAIL is available under Aferro-GPL open source license: https://github.com/sail.
Contact: gostev@ebi.ac.uk, support@simbioms.org Supplementary information : Supplementary data are available at Bioinformatics online and from http://www.simbioms.org.
Received on September 29, 2010; revised on December 3, 2010; accepted on December 12, 2010 1 INTRODUCTION For many years biobanks have been collecting biological samples enriching them with annotations of phenotype, familial and environmental data stored in diverse computational systems.
With some notable exceptions, such as the UK DNA Banking Network (Yuille et al., 2010), biobank data are usually restricted to in-house use (Hirtzlin et al., 2003; Kauffman and Cambon-Thomsen, 2008; Yuille et al., 2007).
With an increasing need for improved statistical power in genome-wide epidemiological studies, accessibility to samples and their annotation from many collections is essential (McCarthy et al., 2008).
However, with considerable differences in how sample collections describe their content, and access to individual data restricted by ethical and legal regulations this may be a daunting task.
Even when access to summary data for individual phenotypes is available, it is difficult to estimate how many samples To whom correspondence should be addressed.
have measurements for a combination of phenotypes.
For instance, how many samples in a given cohort have the combination of genome-wide association results, metabolomic data and information on a given clinical phenotype?
There is a need for an efficient means of finding which biobanks contain samples relevant to a particular study annotated with the necessary metadata (Founti et al., 2009; Helgesson et al.
2007).
By sample availability data we understand the information describing which meta-data and measurements exist for each sample in a collection without necessarily revealing the actual content.
The Sample avAILability system (SAIL) is a platform that uses this paradigm to help researchers to integrate their resources and search sample availability across any number of sources.
The system can either be used by individual biobanks or by consortia of sample collections to facilitate their research.
2 BASIC CONCEPTS AND DATA STRUCTURES A sample in SAIL is a general concept which can for example represent a human individual, a biopsy or a derived sample preparation.
Biobank samples are typically annotated with relevant phenotypic information, such as the type of sample (e.g.
blood), the age and disease state of the person the sample was taken from and possibly with various measurement data such as glucose level and blood pressure at the time the sample was taken.
When the same biosample has been used to collect measurements at different time points, these are given separate sample identifiers to simplify searching and maximize the visibility of each individual measurement.
The situation where more than one sample has been taken from a person can be represented in various ways (a record per sample or a record per person) which is down to the user; however in the data sets currently loaded in SAIL this situation is not typical.
Samples are grouped in collections (e.g.
cohorts); collections may be annotated with information common to the collection (e.g.
descriptions of the origin of the samples and the collection protocols).
While a sample can only belong to one collection, it can participate in many studies.
Moreover studies may contain samples from multiple collections (see Fig.1).
The basic functional descriptor unit in SAIL is a parameter.
Parameters are described by variables, which are mandatory fields, The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:01 22/1/2011 Bioinformatics-btq693.tex] Page: 590 589591 M.Gostev et al.
Fig.1.
A high level data structure in SAIL.
(See Supplementary Materials for a complete database schema).
and qualifiers, which are optional additional descriptors.
For instance, we can define Glucose as a parameter with a variable concentration.
We can refine the Glucose parameter by adding a qualifier timing to specify whether the measurement was taken while fasting or non fasting.
Parameters can be grouped using tags.
Tags are assigned values to identify the parameter group.
They are a general concept in SAIL that provides the flexibility to group parameters without being bound by the semantics of the specific type of group.
For example, tags can be used to group parameters that come from a specific vocabulary, parameters that are used in the definition of a specific disease or parameters that are synonymous with each other.
Tags can themselves be grouped into classifiers that can be used for more general parameter classification.
For example, tags for different vocabularies can be grouped into the classifier Vocabulary.
Similarly, tags for specific types of parameter relationships are grouped into the classifier Relation.
When a tag is added to a parameter, the user indicates the value of the tag (such as the specific vocabulary) and the classifier to which the tag belongs (Vocabulary).
Tags can be used to map external taxonomic structures to data structures in SAIL, for example data schemas produced by DataSHaPER (Fortier et al., 2010), the supplier of standardized data schemas for biobanks, can be easily represented in SAIL.
Using such a flexible semantic structure SAIL can accommodate parameters from several vocabularies and store relations between them enabling searches that span across more than one data collection as well as for samples only partially matching the search term.
Relations can be defined by the users or data providers and can vary from generic forms (i.e.
synonym or partial match) to more detailed forms to express associations between specific vocabularies (or even specific for a group of terms).
3 QUERIES Users can form queries by selecting parameters listed in the interface and combining them via logical expressions.
Parameters can be filtered by a selection of classification tags, such as parameters associated with a particular disease.
Query results can be limited by defining value ranges for specific parameters.
When data from more than one collection is available, users can select which collections to query.
Users can also select what type of relation between parameters they allow; for example to retrieve samples annotated using particular vocabularies where only synonymous terms are considered (see Fig.2).
The result of a query is presented as a report showing how many samples with the specified parameters are available from each of the collections.
SAIL allows for the creation of predefined queries by the systems administrator as a part of the systems customization process.
Predefined queries can be used for work with rather complex combinations of variables.
Fig.2.
Sample availability matrix for three collections, where collection 1 and 2 are annotated with one vocabulary and collection 3 with a different vocabulary.
As Glucose and Glu_con are tagged as synonymous, the result of a query for Glucose will show samples available from the three collections.
4 IMPLEMENTATION SAIL is designed as a client-server system where the sample availability data and vocabularies are stored at a server instance, with tools for browsing, searching and editing content accessible through a web application interface using common web browsers.
Parameters can be introduced in SAIL by batch import of vocabularies (see Supplementary Material) or manually using the parameter creation tool.
Availability data for a group of samples, annotated using one of the preloaded vocabularies, is uploaded using a tab-separated spreadsheet file with one row per sample and one column per parameter.
Sample data may contain the actual values or the symbol @ representing the availability of data for this parameter without disclosing the actual value.
It is up to the data provider to decide for which parameters to provide the actual values or only the availability.
The actual values will not be presented at any time (except for values specified for presentation), but they allow for finer data filtering during querying.
Thus SAIL has been designed to minimize (even eliminate) the storage of identifiable human data, allowing the browsing and sharing of data without the need for access restrictions that need to be imposed for ethical reasons.
Nevertheless, depending on how the system is used, access control may still be useful, and SAIL allows for three types of users: (i) system administrator with full access; (ii) data manager with vocabulary and data upload rights; and (iii) basic user with search engine access.
The web interface of SAIL was developed in Java using the Google Web Toolkit and Ext-JS widget libraries.
The Java servlet specifications were followed.
The system is run as a Tomcat web application.
In our implementation, all data are stored in a relational database on the server and the part selected for queries are loaded into memory for highest performance.
This allows for fast execution and flexible query formulation.
The execution of queries is not dependant on the relational database, and can be formulated using linked data solutions such as RDF, OWL and SPARQL.
However, we opted for a simple customized semantic data structure instead, in order to maximize performance and to be able to flexibly contain annotation structures from different sources.
5 DISCUSSION SAIL has been developed as a tool that can be installed and used independently by any biobank or research group in need for a system 590 [13:01 22/1/2011 Bioinformatics-btq693.tex] Page: 591 589591 SAIL for keeping track of their samples.
However, we believe that the main use of SAIL is in a centralized instance holding data from many providers and used to identify samples across multiple data sources.
For SAIL to function effectively across multiple data sources, it is important that their vocabularies are compatible and that the correct relations are made between data elements in different vocabularies.
There are two possible strategies to achieve this: (i) harmonizing the vocabularies in advance of loading them into the system or (ii) by using SAIL to create mappings between terms of vocabularies already loaded into the system.
In each case this is not a trivial task and, although it is central to the system, describing this is outside the scope of this application note.
A particular use case is the design of meta-analysis studies based on genome-wide genotype data availability across many collections.
We give two examples on how SAIL can be used in this context.
Example 1.
Meta-analysis of genome-wide association studies for glucose levels in plasma.
A consortium wants to conduct a meta-analysis of genome-wide association studies for fasting plasma glucose levels.
It is of importance to know diabetes status for the study, and age, gender and BMI are to be used as study covariates.
For the study design, an estimate is sought for how many samples can be included in the study, and from which cohorts.
In the SAIL report constructor a query is constructed by selecting the parameters of interest, and adding them one by one to the query: glucose concentration (GLU), diabetes status (DB), age (AGE), gender (SEX) and BMI (BMI).
We add the requirement for genome-wide genotyping data (GW_GT) and retrieve a report with detailed information about the availability in each collection for samples supporting the query (Supplementary Figure S1).
The report tells us that 12 487 samples in three different cohorts may be eligible for the study.
A further query restricted to these cohorts show the exact genome-wide genotyping arrays these samples have been measured on, and whether or not genome-wide imputed genotypes are available (Supplementary Figure S2).
Based on the results from SAIL, it is now easy to contact the administrators of the different cohorts to ask for specific information and coordinate the sharing of data for the meta-analysis.
Example 2.
Metabolic Syndrome.
Metabolic Syndrome is a term for a combination of phenotypes that affect the risk for diseases involving the metabolic system and diseases that may follow as the conditions progress.
The definition of Metabolic Syndrome is complex and can be done in different ways.
Three commonly used definitions are those of the International Diabetes Federation (IDF), the US National Cholesterol Education Program (NCEP) and the World Health Organization (WHO).
For example, WHO defines Metabolic Syndrome as a combination of impaired glucose regulation and two out of four additional risk factors.
Impaired glucose regulation in itself is determined by the presence of either type 2 diabetes, impaired fasting glucose, impaired glucose tolerance or insulin resistance.
The four additional risk factors are: (i) central obesity (threshold is gender dependent and determined by waist-to-hip ratio or BMI); (ii) raised plasma triglyceride levels and raised HDL cholesterol level (where the threshold depends on gender); (iii) raised blood pressure; and (iv) raised urinary albumin secretion ratio or raised albumin : creatinine ratio in serum.
SAIL supports queries for such a complex combination of phenotypes, with operations such as two out of four, but encodes them as pre-defined queries.
To query SAIL for Metabolic Syndrome by the WHO definition, we select the eligible collections and simply add the predefined MetS_WHO query and submit the request.
In the current installation of SAIL at the EBI, 16 903 out of 85 979 samples across 13 collections have sufficient measurements available to determine Metabolic Syndrome status according to WHO (Supplementary Figure S3).
Although SAIL has been developed with a focus on biobanks and biological sample collections, its design allows for the integration of data from other sources where information can be arranged into annotated records.
The largest of the four SAIL instances which are currently run for various projects is accessible at http://www.ebi.ac.uk/Tools/sail/ and http://sail.simbioms.org with data from approximately 189 000 samples from 14 collections.
Technically SAIL software can scale to any number of cohorts though the system may slow down as more cohorts are added.
According to our current assessment, on the existing hardware (Intel Xeon 2.66 GHz, 4 GB RAM) the system can scale up to millions of samples.
ACKNOWLEDGEMENTS We would like to thank members of the ENGAGE consortium for their support and feedback, as well as the data providers, especially HUNT Biobank for the largest dataset deposited in SAIL and for the valuable discussions and continuous support.
Funding: Functional Genomics budget of the European Molecular Biology Laboratory and the ENGAGE grant HEALTH-F4-2007-201413 of the FP7 programme by the European Commission.
Conflict of Interest: none declared.
ABSTRACT Motivation: High-throughput technologies provide fundamental informations concerning thousands of genes.
Many of the current research laboratories daily use one or more of these technologies and end-up with lists of genes.
Assessing the originality of the results obtained includes being aware of the number of publications available concerning individual or multiple genes and accessing information about these publications.
Faced with the exponential growth of publications avaliable and number of genes involved in a study, this task is becoming particularly difficult to achieve.
Results: We introduce GeneValorization, a web-based tool that gives a clear and handful overview of the bibliography available corresponding to the user input formed by (i) a gene list (expressed by gene names or ids from EntrezGene) and (ii) a context of study (expressed by keywords).
From this input, GeneValorization provides a matrix containing the number of publications with co-occurrences of gene names and keywords.
Graphics are automatically generated to assess the relative importance of genes within various contexts.
Links to publications and other databases offering information on genes and keywords are also available.
To illustrate how helpful GeneValorization is, we will consider the gene list of the OncotypeDX prognostic marker test.
Availability: http://bioguide-project.net/gv Contact: cohen@lri.fr Supplementary information: Supplementary data are available at Bioinformatics online.
Received on April 15, 2010; revised on October 12, 2010; accepted on February 4, 2011 1 INTRODUCTION High throughput technologies (e.g.
comparative pan-genomic hybridization, gene expression, protein and methylation arrays, high-throughput DNA sequencing) are major, promising and very exciting tools to study biology.
Each of them provide fundamental informations concerning thousands of genes such as their normal functions or specific alterations (e.g.
DNA copy number alteration, loss of heterozygosity, change in expression, promoter methylation, mutation or post-translational modification).
Many of the current biological research laboratories daily use one or more of these technologies.
Selecting genes of interest to design further experiments is of paramount importance.
In this To whom correspondence should be addressed.
process, scientists need to access the latest publications concerning individual or multiple genes.
Among the genes they may consider, researchers need to distinguish three categories of genes: (i) genes already clearly shown to be associated with the process they are studying; (ii) new and promising genes for a particular research field whose interest is well known in other research fields; (iii) genes that have not been studied.
This task is becoming particularly difficult to achieve faced with the many gene lists which are retrieved and the exponential growth of publications available.
In this article, we introduce a web-based tool named GeneValorization.
Given a list of gene names and a set of keywords describing the context of the study, GeneValorization provides a matrix with the number of publications cociting each gene name and keyword.
GeneValorization thus gives very quickly a clear and handful overview of the bibliography corresponding to a gene list of interest within a given context of study.
To illustrate how helpful GeneValorization is, we consider here the gene list of the OncotypeDX prognostic marker test (Paik et al., 2004), which is composed of 16 genes and used to determine the individual relapse risk of a breast cancer patient.
2 MAIN FUNCTIONALITIES The main interface of GeneValorization is provided on Figure 1.
Basic queries: GeneValorization takes as input from the user a list of gene names and a list of keywords that we call filters.
Filters are used to describe the context of the study: while the main filter represents the main context (e.g.
Breast cancer), secondary filters are alternative ways of refining this context (e.g.
Proliferation, Migration).
Secondary filters can be as numerous as necessary.
Given this input, GeneValorization provides a matrix of data where each line is dedicated to a gene and each column is dedicated to a filter.
The first column considers the main filter only while the next columns consider both the main filter and secondary filters.
More precisely, cells (x,y) of the first column of the matrix contain the numbers of publications cociting the main filter (on column y) and the gene name (on line x).
Cells from the second column and the next ones contain the number of publications cociting the main filter and the secondary filter (on column y) and the gene name (on line x).
Columns where a secondary filter is provided thus allow to subdivide the set of papers considered.
In Figure 1, GeneValorization reported 5138 papers involving PGR in Breast Cancer (main filter).
Among them, 658 papers are also related to Proliferation while 234 mention Apoptosis.
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:22 26/3/2011 Bioinformatics-btr073.tex] Page: 1188 11871189 B.Brancotte et al.
Fig.1.
Main interface of GeneValorization.
Specifying species and using aliases: GeneValorization is able to perform its search not only using the gene name g provided by the user but also using all aliases of g within a given species, by exploiting information from EntrezGene.
The default species used is Human, which can be changed by the user.
It is also possible not to consider aliases.
All the menus available to make such choices are described in the manual.
Gene name disambiguation: GeneValorization provides assistance in the process of gene name disambiguation.
Ids from EntrezGene can be directly entered (prefixed by #) instead of gene names.
When users provide a gene name g, GeneValorization uses EntrezGene to check which of the three following cases occur: (i) g is an official gene name; (ii) g is not an official gene name but appears in the list of synonyms of one or several official gene names (within the same species); and (iii) none of the above.
In case (i), GeneValorization runs normally, the gene name is green to indicate that it is official.
In case (ii), the gene name appears in orange and by right clicking on the name, the user can access the list of official gene names having g in their aliases.
Users are provided with links to EntrezGene web pages describing each alternative so that they can then choose one of them to remove any ambiguity (the orange gene name can thus be renamed using one of the alternative official names).
In case (iii), the gene name appears in red to indicate that it is not an existing gene name.
By default, GeneValorization will still perform all searches using the value entered by the user.
In case (ii), and if the user allows synonyms to be considered, GeneValorization will choose to consider the first official gene name provided by EntrezGene.
Visualizing results: clicking on a cell of the main grid allows users to measure the relative importance of genes and keywords.
As an example, clicking on the cell corresponding to PGR and Proliferation generates the graph of Figure 1 (right-hand side) and states that PGR is the third most important gene (according to the number of publications) for the Breast Cancer and Proliferation topics, out of the 16 genes considered here.
All the papers associated with the keywords are provided and can be accessed.
Interestingly, it is also possible to compare the role of several secondary filters (several graphs can be displayed at the same time).
Accessing information from other sources: links to several databases are also provided.
As for genes, information from PubMed, EntrezGene, GeneCards and DrugBank can be obtained (e.g.
Fig.1 shows the DrugBank web page for PGR).
GeneValorization also makes calls to the NLM MeSH browser (http://www.nlm.nih.gov/ mesh/meshhome.html) to match the keywords provided by the user with MeSH terms and places them within the MeSH ontology.
Data import/export: GeneValorization allows users to load and save gene lists and filters using various formats (e.g.
text files, csv, xml).
During the export, EntrezGene ids used to search for the aliases are added to the saved matrix.
Advanced queries: Users can express advanced queries involving wilcards * and AND in gene names or filters.
Considering gene names with or without aliases (from EntrezGene) is possible.
Users may also indicate which part of the PubMed file should be queried (e.g.
abstract, title).
3 TECHNICAL INFORMATION GeneValorization is a Java webstart application.
It is thus multiplatform and can be used without any specific installation.
GeneValorization follows an on-the-fly querying process: queries are directly sent to portals (Entrez or SRS), and no warehousing is needed.
Loading a cell information may take 14 seconds.
It took 10 minutes (but each result is displayed as soon as it is available) to load the entire information associated to the gene list, considering all the aliases available in EntrezGene, of the OncotypeDX prognostic marker test, with the 19 secondary filters.
To deal with long list of genes or filters, a caching system has been implemented to optimize the response time.
It makes it possible to save previous loaded data 1188 [09:22 26/3/2011 Bioinformatics-btr073.tex] Page: 1189 11871189 GeneValorization which can be updated on demand later on.
Last, GeneValorization is currently able to run on the Entrez NCBI portal (http://www.ncbi .nlm.nih.gov/Entrez/) or the EBI SRS server (http://srs.ebi.ac.uk/); queries will be sent and interpreted by the respective portals.
Among the differences, MeSH terms are automatically considered during this search when Entrez is used while it is a pure cooccurrence-based search in SRS (more information is available in the Supplementary Material).
4 PROOF OF CONCEPT The OncotypeDX test quantifies the probability of distant recurrence in patients with node negative, estrogen receptor positive breast carcinoma.
It is composed of 16 cancer-related genes, selected after a validation step on independent studies.
This test has been included in the guidelines of the American Society of Clinical Oncology and the National Comprehensive Cancer Network.
We have used GeneValorization to analyze the literature corresponding to these 16 genes.
Two queries have been uploaded with Cancer and Breast Cancer as main filter.
The 19 secondary filters have been considered corresponding to examples of the most general items to depict the composition of a cancer gene list.
As a first result, GeneValorization allowed us to know that each of the genes has been refered in association with the term Cancer and Breast Cancer in 5 to 17 517 and 3 to 10 048 publications.
Second, GeneValorization made it possible to distinguish three sets of genes in the OncotypeDX test.
Five genes (ERBB2, ESR1, PGR, BCL2, MKI67) were highly studied (>1000 publications associated) while four genes (BAG1, CTSL2, MYBL2, SCUBE2) were not very actively studied (less than 50 publications) and the seven remaining genes (GSTM1, AURKA, BIRC5, CCNB1, CD68, MMP11, GRB7) had an intermediate level.
All the genes of the list are thus not equally known to be involved in breast cancer.
Genes associated to only a few or no publications are then of particular interest since they have been selected to be in the signature while not being studied extensively.
Our study suggests to conduct new experiments on some of these genes to better demonstrate how connected to breast cancer they may be.
Third, GeneValorization underlined that the secondary filters Proliferation, Apoptosis, Invasion, and Angiogenesis were strongly associated with the gene list while Immunity, Cell-cycle arrest, Epigenetic or microRNA were much less often associated.
This underlines the fact that the OncotypeDX signature is a publication-based molecular signature which contains genes involved in processes commonly known to be linked to breast cancer prognosis but does not contain genes known to be related to new trails in breast cancer studies (as the role of immune response in cancerogenesis).
5 DISCUSSION Mining PubMed abstracts and ranking publications have been of particular interest in the last years.
Approaches are mostly based on Text-Mining techniques [see for instance, Vellay et al.
(2009) or Krallinger et al.
(2008) and the references therein].
When available, softwares able to analyze genes such as PDQWizard (Grimes et al., 2006), GoGene (Plake et al., 2009) and CoPubMapper (Alako et al., 2005) differ from GeneValorization in several aspects.
From a technical perspective, (i) they make use of data warehouses to store publications, which poses the major problem of updating the local databases and makes it impossible to benefit from all of new publications daily available and (ii) they may not consider simultaneaous requests which makes the response time too long.
From a functionality perspective, (i) they may not be flexible, e.g.
providing only predefined lists of keywords and (ii) they may not consider gene aliases.
From a user perspective, they may not provide results in a concise and/or graphical manner and may not allow users to easily store and load their results at any time.
A more complete related work (eight tools compared with GeneValorization) is available in the Supplementary Material.
ACKNOWLEDGEMENT The authors would like to thank the reviewers for their helpful comments to improve the manuscript.
Funding: The CNRS (Brasero project) (to B.B.)
the CNRS, the Institut Curie, the Ligue Nationale Contre le Cancer (associated laboratory), and the Drop-Top FP6 European project (LSHB-CT-2006-037739) (to A.B., I.B.P., F.Ra., and F.Re.
); the Institut National du Cancer (to A.B.).
Conflict of Interest: none declared.
ABSTRACT Motivation: High-throughput protein identification experiments based on tandem mass spectrometry (MS/MS) often suffer from low sensitivity and low-confidence protein identifications.
In a typical shotgun proteomics experiment, it is assumed that all proteins are equally likely to be present.
However, there is often other evidence to suggest that a protein is present and confidence in individual protein identification can be updated accordingly.
Results: We develop a method that analyzes MS/MS experiments in the larger context of the biological processes active in a cell.
Our method, MSNet, improves protein identification in shotgun proteomics experiments by considering information on functional associations from a gene functional network.
MSNet substantially increases the number of proteins identified in the sample at a given error rate.
We identify 829% more proteins than the original MS experiment when applied to yeast grown in different experimental conditions analyzed on different MS/MS instruments, and 37% more proteins in a human sample.
We validate up to 94% of our identifications in yeast by presence in ground-truth reference sets.
Availability and Implementation: Software and datasets are available at http://aug.csres.utexas.edu/msnet Contact: miranker@cs.utexas.edu, marcotte@icmb.utexas.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION High-throughput protein identification in biological samples aids our understanding of complex cellular systems and their behavior.
Mass spectrometry (MS)-based shotgun proteomics offers fast, high-throughput characterization of complex protein mixtures.
Several thousand proteins may be identified in a sample using high-resolution MS/MS instruments and/or extensive biochemical fractionation (Brunner et al., 2007; Graumann et al., 2007), but standard approaches only identify a fraction of the expected proteins.
To whom correspondence should be addressed.
A shotgun proteomics experiment typically proceeds by MS/MS analysis of peptides from proteolytically digested proteins, followed by in silico matching of the MS/MS spectra against a database of theoretical peptide spectra derived from protein sequences (Fig.1).
Proteins are identified using combined evidence from constituent peptides, resulting in a list in which each protein is associated with a score signifying the confidence of correct identification.
We refer to this score as the MS/MS protein score, e.g.
ProteinProphets protein probability (Nesvizhskii et al., 2003).
Proteins with scores that satisfy an error threshold are labeled present by the MS analysis software.
Effective MS/MS protein identification is hindered by factors such as noisy spectra, low-concentration proteins, post-translational modifications and chemical properties that interfere with peptide ionization.
For complex samples such as cell lysates, current MS search algorithms typically only match a small percentage (<20%) of all MS/MS spectra to real peptides, resulting in higher error rates and low recall at the protein level.
As a result, only a percentage of the expected proteins are identified with confidence despite presence in the biological sample, and the MS/MS identification scores of many other proteins fall below acceptable confidence thresholds.
MS/MS protein identification scoring schemes, such as BioWorks (ThermoFinnegan) and ProteinProphet (Nesvizhskii et al., 2003), assume that all proteins are equally likely to be present.
In reality, other information may be available and can be used to influence the inferred probability of protein presence thereby rescuing proteins that fall below confidence thresholds.
We use gene functional networks (Marcotte et al., 1999) as an external information source to analyze proteins in a sample in the context of the biological processes that are active in the cell.
Given a list of proteins identified in an MS experiment (M), we determine a more complete list (M ) by considering the proteins that are expected to be present (or absent) based on their functional linkages to proteins in M. Each protein receives a revised identification score with contributions both from direct MS-based evidence, and MS evidence of neighbors in the gene functional network.
Since current gene networks can be incomplete, we intend for M to serve as a complement to M, rather than replace it as the authoritative list of expressed proteins.
The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:38 26/10/2009 Bioinformatics-btp461.tex] Page: 2956 29552961 S.R.Ramakrishnan et al.
Fig.1.
Integrative analysis of MS-based shotgun proteomics and gene functional networks.
A complex protein sample, e.g.
cellular extract, is enzymatically digested into peptides and subjected to tandem mass spectrometry.
Experimental spectra are searched against a database of theoretical spectra generated from protein sequences, or identified via de novo sequencing, using a peptide and protein identification software pipeline that produces a confidence score per protein [e.g.
PeptideProphet (Keller et al., 2002) and Protein-Prophet (Nesvizhskii et al., 2003)] and a list of high-confidence proteins with scores that satisfy an error threshold (e.g.
5% FDR).
We introduce a next stage of computational analysis which places proteins in a broader systems biological framework.
MSNet uses protein-protein links from a functional network to identify proteins that may not be identified with high confidence by MS evidence alone, but are nevertheless highly likely to be present as demonstrated by the combination of MS evidence with functional links to other MS identified proteins.
We find that the integrated analysis of mass spectrometry experiments and gene functional networks can improve the precision and sensitivity of protein identification at acceptable error rates.
Our data integration approach has the potential to enable pathway-based interpretation of high-throughput MS/MS experiments that are otherwise run in isolation.
For instance, by integrating mass spectrometry data from yeast grown in rich medium with a published yeast functional network (Lee et al., 2007), we were able to confidently identify many proteins from ribosomal complexes and proteins involved in RNA binding, processing and degradation, thereby increasing the protein coverage in several active pathways (Section 4).
When our method was applied to yeast grown in minimal medium, we increased the number of proteins identified in the reductive carboxylate cycle pathway (Ogata et al., 1999).
In both cases, we expect the newly identified proteins to be present in the sample, but they were not identified with confidence by the MS analysis software, despite having at least one peptide identified per protein.
We demonstrate the applicability of MSNet to data from different organisms, mass spectrometers, MS analysis pipelines, and experimental conditions.
We identify 829% more proteins on different yeast datasets at the same error rate, and evaluate the quality of protein identifications via ROC and precisionrecall plots.
In yeast grown in rich medium, analyzed on a high-resolution mass spectrometer, we identify 29% more proteins than the original MS analysis, 97% of which are present in a reference set derived from independent identification experiments.
We also demonstrate direct applicability to the human proteome using a human functional gene network, reporting 37% more proteins than the original MS analysis.
2 METHODS 2.1 MSNet algorithm MSNet introduces an additional stage of computational analysis to MS/MS shotgun protein identification (Fig.1).
In this section, we introduce the MSNet protein identification score.
Specifically, if two proteins are known to be functionally linked i.e.
proteins p1 and p2 are known to physically interact, be co-expressed or co-regulated across several biological conditions, and p1 has been observed in a MS experiment, we propose that p1 should be assigned a revised identification score that depends not only on its own MS-based identification score c1, but also on the MS identification of its functional neighbor p2 and the strength of belief in the functional link between p1 and p2.
The concept can be extended from two genes to pathways of co-functioning genes, generating revised identification scores for every protein encoded in the genome.
Note that the confidence score c1 represents protein presence, and not protein abundance.
We use the yeast gene functional network developed by Lee et al.
(Lee et al., 2004, 2007) which spans >95% of the yeast genes.
The network forms a graph G = (V ,E) with |V | = N genes and |E | weighted edges (wij) between nodes.
The weight wij of an edge between two genes i and j is defined as the log of the likelihood odds ratio that there exists a link, and is determined by Bayesian integration of thousands of diverse experiments that estimate functional association e.g.
mRNA co-expression, phylogenetic profiles, protein interaction experiments and co-citation in published literature (Lee et al., 2007).
Intuitively, wij denotes the strength of a functional link between two genes.
For human samples, we use a similarly constructed human gene network (Lee and Marcotte, manuscript in preparation).
MSNet computes a score yi for each protein i, which represents how likely it is for i to be present in the sample given MS evidence for i and its functionally related proteins j.
The MSNet score for protein i (Equation 2) is the convex combination of two terms: (i) the probability that the protein is present in the sample given evidence from a MS experiment (oi) and (ii) the weighted average of MSNet scores of is immediate network neighbors j (Equation 4).
We set oi to the MS protein probability generated by ProteinProphet (Nesvizhskii et al., 2003), but any posterior probability of protein presence given sample-specific experimental data may be used instead (see discussion in Section 4).
Since yi is defined in terms of yj , we update scores iteratively.
At each iteration t, the algorithm includes evidence from neighbors at path length = t. y(t+1)i =oi +(1)uijy(t)i (1) Y (t+1) =O+(1)U Y (t) (2) (t+1) = Y (t+1) Y (t)  1 (3) uij = wij js.t.
(i,j)E wij (4) The MSNet score can be rewritten in vector notation using the weighted adjacency matrix UNN and MS protein probability vector ON1 to generate score vector YN1 (Equation 2).
The MSNet algorithm is closely related to diffusion algorithms like Googles PageRank (Langville and Meyer, 2006; Page et al., 1999).
PageRank has been successfully used to determine a relevancy ranking of webpages based on the hyperlink structure of the web (Langville and Meyer, 2006).
MSNet generates a ranking of proteins that is based not only on the link structure of a gene functional network, but also on per-protein relevance to a given sample.
In Supplementary Appendix I, we show that MSNet is equivalent to a personalized (Page et al., 1999) or topic-sensitive variant of PageRank (Haveliwala, 2003) with two differences.
First, PageRank is defined on a directed graph.
Gene functional networks are undirected, so each edge must be interpreted as being bi-directional.
A second related difference 2956 [09:38 26/10/2009 Bioinformatics-btp461.tex] Page: 2957 29552961 Network mining for shotgun proteomics is that PageRank uses a column-normalized weight matrix H = UT .
We justify the use of U in Supplementary Appendix I, and show that it performs better in our domain in Supplementary Figure S6.
MSNet can be shown to converge to a unique solution irrespective of starting vector Y(0) (proof of convergence is in Supplementary Appendix I).
In practice, MSNet converges within 106 tolerance in tens of iterations (Equation 3).
In our experiments, we initialize Y (0) = O. Parameter (1)/ weights the networks contribution to the MSNet score.
We optimize in yeast by maximizing the area under the ROC curve (AUC) while maintaining similar error rates as the MS analysis across multiple datasets.
AUC is not very sensitive to (1)/ in the range [5,50] (see Supplementary Fig.S3).
We set (1)/ = 6 for yeast.
2.2 Evaluation methodology In this section, we describe the MSNet evaluation framework, introduce the error measures used and describe how they are computed.
For a given mass spectrometry experiment and gene functional network, we calculate the MSNet protein identification score for every protein on a genome-wide scale.
To test robustness to missing network links, the reported MSNet score is averaged across 10 runs of 10-fold cross-validation.
We restrict our evaluation to proteins with at least one peptide identified in the MS experiment.
We use a 5% false discovery rate (FDR) (Storey and Tibshirani, 2003) to determine a high-confidence list of proteins.
The FDR at a score t is the fraction of false instances among all identifications with score t. We employ two approaches to estimate the FDR: (i) using a protein reference set as ground-truth to categorize proteins as true or false instances; (ii) generating true and false (null) score distributions independent of ground truth as described in detail below.
We conducted functional analysis of yeast proteins using SGD (Nash et al., 2007), FunSpec (Robinson et al., 2002) and FuncAssociate (Berriz et al., 2003), applying Bonferroni corrections.
2.2.1 Evaluation against a protein reference set When a protein reference dataset is available, we use it to label a protein as a true instance (T ) if it is present in the reference set, and as a false instance (F) otherwise.
We estimate the FDR at score threshold s as FDRref = F/(T +F), the percentage of false instances that have score s. We also plot receiver operator characteristic (ROC) and precision-recall curves using the reference set to determine true and false instances.
A ROC curve plots true positive rate (TPR) versus the false positive rate (FPR).
A precision-recall curve plots (1-FDR) (precision) versus TPR (recall).
TPR at a score threshold t is the fraction of true instances with score t. FPR at score threshold t is the fraction of false instances with score t. FDR is defined above.
We also report the ROC AUC, the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (Fawcett, 2006).
2.2.2 Evaluation independent of a protein reference set When protein reference sets are unavailable, it is standard to compute error estimates by generating a null distribution of scores, and using the ratio of the areas of null and true distributions at scores s as an estimate of the FDR at score threshold s. Though there has been extensive recent work on the estimation of FDRs at the peptide-level (Choi and Nesvizhskii, 2008; Kall et al., 2008), there is no consensus at the protein identification level (Tabb, 2008).
Our purpose however is to develop an error model for MSNet, and we do not address the reliability of MS error models in this article.
We generate an error model using a method we refer to as network-shuffling, similar to randomization or permutation tests used in statistical hypothesis testing.
For a given dataset, we generate a null distribution of MSNet scores by running MSNet on a network where the labels on the nodes (protein names) are shuffled, such that proteins maintain features such as the MS protein identification score, but have a different set of network neighbors.
This label-shuffling destroys any biological genegene association signal, while maintaining the total node degree (topology).
We repeat the shuffling process multiple times and pool all generated scores to estimate the null score distribution.
The true score distribution is generated by running MSNet on the original network.
We plot density distributions for null and true scores (Supplementary Fig.S2) and estimate FDR as FDRshuff = Ns/Ts, where Ns is the area under the null distribution for scores s and Ts is the area under the true distribution for scores s. In this article, FDR refers to FDRshuff unless stated otherwise.
2.3 Datasets We evaluated MSNet on different organisms, experimental conditions and mass spectrometers (Table 1).
MS/MS data was collected on low and high-resolution mass spectrometers: ThermoFinnigans Surveyor/DecaXP+ (LCQ) and LTQ-OrbiTrap (ORBI).
MS/MS protein identification was conducted using Bioworks 3.3 (ThermoFinnigan), PeptideProphet (Keller et al., 2002) and ProteinProphet (Nesvizhskii et al., 2003).
We considered the entire yeast genome except for proteins annotated as dubious, since these proteins were not considered in the yeast network (Lee et al., 2007).
All MS yeast experiments were the result of combined MS analysis of multiple injections of the sample.
An identified protein was labeled as a true instance if it was present in the corresponding protein reference set (Table 1).
2.3.1 Yeast (rich medium) Cell lysate from wild-type yeast grown in rich medium was analyzed on both LCQ and ORBI mass spectrometers.
The LCQ data has been published previously (Lu et al., 2007).
2.3.2 Yeast (rich medium, polysomal fraction) Cellular lysate was separated in 747% sucrose gradient and fractions were monitored by UV absorbance for RNA content (Li et al., 2009).
We chose the fraction containing 80S ribosomes for LCMS/MS analysis on the LCQ.
2.3.3 Yeast (minimal medium) We used MS/MS data on wild-type yeast grown in minimal medium (MOPS9), previously published in (Lu et al., 2007), with cell lysate analyzed on an LCQ mass spectrometer.
2.3.4 Human Protein extracts from human HEK293T cell lines were prepared for MS/MS analysis as described in the Supplement.
We evaluated results using the shuffled network approach, since no comprehensive protein reference set was available for this dataset.
2.3.5 Availability Yeast LCQ data has been previously published (Lu et al., 2007).
Software and datasets are available at http://aug.csres.utexas.
edu/msnet.
Further details about sample preparation and protein reference sets are in the Supplement.
3 IMPLEMENTATION AND RESULTS We demonstrate that incorporating functional association information can substantially boost correct identification of proteins in a shotgun proteomics experiment, across a range of sample conditions and mass spectrometers.
For each dataset in Table 1, we measured the number of proteins identified by MSNet at 5% FDR as compared to the original MS experiment at its 5% FDR.
ProteinProphet (Nesvizhskii et al., 2003) computes FDR directly from protein probabilities, which the authors empirically show to be good estimates of the true posterior probability of protein presence.
MSNet consistently increased the number of identified proteins by 829% across yeast experiments (Table 2) and at least 94% of MSNet proteins were validatedeither by presence in the reference set, or previous identification in the MS experiment (Fig.2A).
When protein reference sets were available, MSNet increased the number of identifications at 5% FDRref by 12100% across datasets (Supplement Table S3) and increased 2957 [09:38 26/10/2009 Bioinformatics-btp461.tex] Page: 2958 29552961 S.R.Ramakrishnan et al.
Table 1.
Datasets and experimental setup Dataset MS/MS experiment Protein reference set Number of proteins YPD-ORBI Cell lysate from yeast BY4742 wild-type grown in rich medium (YPD) analyzed on LTQ-ORBItrap (8inj) YPD*: Proteins identified in 1 of three non-mass spectrometry experiments (Futcher et al., 1999; Ghaemmaghami et al., 2003; Newman et al., 2006) or 2 of four MS experiments (Chi et al., 2007; de Godoy et al., 2006; Peng et al., 2003; Washburn et al., 2001).
Total 4264 proteins (67% of yeast genes) 3816 YPD-LCQ Cell lysate from yeast BY4742 wild-type grown in rich medium (YPD) analyzed on LCQ (5inj) YPD* defined above 4385 YPD-LCQ-Fraction Cell lysate, fractionated in polysomal gradient from yeast grown in rich medium (YPD) analyzed on LCQ (3inj) Known ribosomal, translation and ribosome biogenesis proteins (Nash et al., 2007; Planta and Mager, 1998) 1393 YMD-LCQ Cell lysate from yeast BY4742 wild-type grown in minimal medium (YMD) analyzed on LCQ (6inj) YMD*: Proteins identified in at least one of three experiments (de Godoy et al., 2006; Newman et al., 2006; Zybailov et al., 2005).
4651 Human-293T, ORBI HEK293T kidney embryonic cells transfected with GFP lenti-virus vector No comprehensive reference set available 1860 The protein sample undergoes MS/MS analysis to generate a list of proteins identified by MS/MS identification software.
We generate MSNet protein identification scores, on a genome-wide scale, for each protein that has at least one peptide identified in the MS experiment (Number of proteins).
When available, we use a protein reference set as ground-truth to determine true and false identifications for evaluation.
Injinjection, i.e.
technical replicate during MS/MS experiment; LCQLCQ DecaXP+ MS/MS instrument; ORBILTQ-OrbiTrap MS/MS instrument).
Table 2.
MSNet performance evaluated with and without a protein reference set Number of proteins at 5% FDR AUC (using reference set) (using network shuffling) Experiment MS MSN % Increase MS MSN % Increase YPD-ORBI 0.69 0.76 10 1420 1835 29 YPD-LCQ 0.55 0.68 24 548 591 8 YPD-LCQ-Fraction 0.78 0.91 17 246 285 16 YMD-LCQ 0.59 0.69 17 644 699 9 Human-293T  877 [8701233] [040] First, we evaluated the performance of MSNet and the MS experiment using protein reference sets (Table 1), marking an identified protein as a true instance if it was present in the reference set and false otherwise.
MSNet increased the AUC by 1024% across datasets.
Next, we evaluated MSNet independent of protein reference sets using a network-shuffling procedure (Section 2.2.2).
We computed FDRshuff as the ratio between the cumulative null and true score densities at each score x. MSNet reported 829% more protein identifications at 5% FDRshuff in yeast and up to 40% more in human than ProteinProphet (Nesvizhskii et al., 2003) at its 5% FDR.
MSNMSNet, MSProteinProphet.
ROC-AUC by up to 24% (Table 2).
We also demonstrate MSNets applicability to data generated from different MS pipelines.
We describe these results in detail below.
3.1 Yeast grown in rich medium We tested the applicability of our method to whole-cell lysate samples using yeast grown in rich medium analyzed on high and low-resolution mass spectrometers.
In Table 2, we report the number of proteins identified by MSNet for the yeast rich medium sample Fig.2.
Performance of MSNet on yeast grown in rich medium analyzed on a high-resolution mass spectrometer.
(A) At least 94% of proteins identified by MSNet at 5% FDR can be validated either by presence in the protein reference set or by identification in the MS analysis; (B) ROC curves using a protein reference set to determine true and false identifications: MSNet identifies more true instances over a range of FPRs than original MS experiment and results in 10% higher AUC; (C) precisionrecall curves: MSNet identifies more proteins at high precision (i.e.
low FDR) than the MS analysis.
2958 [09:38 26/10/2009 Bioinformatics-btp461.tex] Page: 2959 29552961 Network mining for shotgun proteomics analyzed on the high resolution LTQ-Orbitrap (Table 1, YPD-ORBI).
MSNet reported 1835 identifications at 5% FDR, a 29% increase over the original MS experiment.
We validated 96% of MSNets 5% FDR proteins92% were present in the reference set and a further 4% were previously identified in the original MS experiment (Fig.2B).
There were 460 new MSNet proteins not previously identified in the MS experiment.
They were enriched for ribosome or translation-associated functions when compared against a background of the whole genome, and for proteins of unknown function compared to a background of MSNet 5% FDR proteins (P < 0.001).
Eighty-five percent of the 460 new identifications were present in the reference set and the remaining 15% were not enriched for any functional categorythus there were no obvious false-positive identifications based on protein function analysis.
We generated ROC and precisionrecall plots for both MSNet and the original MS experiment, marking protein as a true instance if it was present in the YPD* reference set (Table 1), and false otherwise.
In a ROC plot (Fig.2B), MSNet identified more true instances (proteins present in the reference set) than the original MS experiment over a range of FPRs.
Similarly, in a precision recall plot (Fig.2C) MSNet identified more true instances over a range of FDRs (1precision), e.g.
identifying 12% more proteins at 5% FDRref (Supplement Table S3).
MSNet also resulted in a 10% increase in ROC-AUC (Table 2), i.e.
MSnet is 10% more likely than MS analysis to rank a randomly chosen true instance higher than a randomly chosen negative instance (Fawcett, 2006).
MSNet improved performance even when the original MS experiment was limited by instrument resolution, as we observed on the same sample re-analyzed on a low-resolution mass spectrometer (Table 1, YPD-LCQ).
MSNet reported 8% more proteins than the original MS experiment (Table 2) and increased AUC by 24% (Table 2, Supplementary Fig.S1).
The new MSNet identifications were enriched for ribosomal proteins (P <0.001).
3.2 Yeast grown in minimal medium We expect our method to be applicable to yeast in different sample conditions, since the gene network was constructed by integrating diverse biological experiments.
Indeed, when applied to yeast grown in minimal medium (Table 1, YMD-LCQ), MSNet identified 9% more proteins at 5% FDR (Table 2).
The new MSNet identifications were enriched for ribosomal proteins (P < 0.001) as in the rich-medium yeast experiment, but also for proteins of small molecule biosynthesis (P < 0.001) e.g.
carboxylic acid, amine or folate metabolism, which is expected for growth in minimal medium.
MSNet increased AUC by 17% when evaluated against the YMD* reference set (Table 2, Supplementary Fig.S1).
3.3 Yeast polysomal fraction We expect MSNet to be especially effective on smaller, focused protein preparations.
Accordingly, we tested MSNet on a polysomal fraction of yeast grown in rich medium, fractionated on a sucrose density gradient (Table 1, YPD-LCQ-Fraction).
Proteins in this sample were restricted to those co-fractionating with 80S ribosomes and were expected to be associated with ribosomal and translation functions.
MSNet identified 16% more proteins at 5% FDR than the original MS experiment (Table 2).
Ninety-four percent of MSNet identifications were validated, either by presence in the fractionation reference set or by previous identification in the MS experiment (Fig.2A).
In a function analysis, all but three new MSNet proteins were found to be associated with the ribosome, ribosomal functions or translation.
The three proteins might represent false positives: inosine monophosphate dehydrogenase IMD2 which catalyzes the first step of GMP biosynthesis; ADK2, a mitochondrial adenylate kinase which catalyzes the reversible synthesis of GTP and AMP from GDP and ADP; and FLC1, a putative FAD transporter (Nash et al., 2007).
MSNet increased AUC by 17% when evaluated against the fractionation protein reference set (Table 1).
The corresponding ROC and precisionrecall curves are plotted in Supplementary Figure S1.
3.4 Applicability to higher organisms Finally, we tested MSNet in higher organisms by evaluating proteins expressed in human HEK293T cells analyzed on a high-resolution mass spectrometer (Table 1, Human-293T).
We used a human gene functional network (Lee and Marcotte, manuscript in preparation).
We considered 18 514 protein-coding genes present in the network, and reported up to 40% increase in the number of identified proteins at 5% FDR.
We present a range of results in Table 2 with parameter (1)/ varying in [6,10].
As in yeast (Section 2.1), this parameter may be optimized as reference sets for human data become available.
The new 5% FDR MSNet proteins were not enriched for any functional category.
3.5 Performance on different MS/MS pipelines We tested the applicability of MSNet to MS/MS data analyzed using different software pipelines.
There are several issues with systematic testing and comparison of different MS pipelines.
First, there is currently only one published, freely available analysis pipeline that generates protein-level probabilities and FDRs i.e.
the TransProteomicPipeline [TPP, (Keller et al., 2002; Nesvizhskii et al., 2003)], which we used for our main results.
Second, a systematic comparison is non-trivial since each pipeline makes different statistical assumptions and the hypotheses are not independent.
Third, any such effort also entails significant development to accommodate different data formats (Prince and Marcotte, 2008).
Nevertheless, we tested four pipelines: (i) TPP with SEQUEST (Bioworks) for spectral matching (used for main results); (ii) TPP with X!Tandem (Craig and Beavis, 2004) for spectral matching; (iii) CRUX for spectral matching (Park et al., 2008), Percolator (Kall et al., 2007) for peptide-matching and DTASelect (Tabb et al., 2002) for protein reports; and finally (iv) a simple average of protein probabilities from the above pipelines.
Since DTASelect does not generate protein scores or FDR, we implemented a simple protein probability as the probability that at least one constituent peptides identification was correct as described in (Nesvizhskii et al., 2003).
MSNet showed comparable performance across pipelines, with 1012% higher AUC, and 712% more proteins at 5% FDR than the original analysis.
The percentage increase in reported proteins depended on the coverage of the MS analysis software.
As expected, the more the proteins confidently identified at the MS stage, the fewer the new MSNet identifications (details are in Supplementary Tables S4S5 and Supplementary Fig.S5).
2959 [09:38 26/10/2009 Bioinformatics-btp461.tex] Page: 2960 29552961 S.R.Ramakrishnan et al.
Fig.3.
Protein YBR234C (ARC40) and its immediate neighbors from the yeast gene functional network (Lee et al., 2007).
The protein was identified with high confidence by MSNet, but not by the original MS analysis.
YBR234C is an essential subunit of the ARP2/3 complex required for the motility and integrity of cortical actin patches, and involved in cell growth and polarity.
Deletion of the gene causes notable growth defects (Giaever et al., 2002), a fact that strongly supports its expression.
It is also present in the yeast reference set (Table 1, YPD*).
MSNet gave YBR234C a high score because it had multiple neighbors that were either confidently identified in the MS experiment (circle) or had some MS evidence (hexagon, 1 peptide identified).
The other neighbors (square) had no peptides identified.
Figures were created using Cytoscape (Shannon et al., 2003).
4 DISCUSSION AND CONCLUSIONS We have presented a method that improves the sensitivity and precision of protein identification by integrating functional linkage information into the computational analysis of MS shotgun proteomics experiments.
Our methodology places MS experiments in a larger biological framework, where proteins expressed in a given cellular state may be readily analyzed in the context of their functionally related neighbors.
We have shown that integrating data sources from outside an MS experiment can improve the protein identification rate of current MS technology and software.
We increased the number of proteins identified at 5% FDR by 840%.
We also improved performance against the original MS analysis in ROC and precisionrecall plots, using our compilation of protein reference sets, showing 1024% increase in ROC-AUC.
We also presented an evaluation methodology to generate null distributions and FDRs for MSNet using network-shuffling, independent of gold-standard reference sets.
These null distributions may be used to compute any other desired error estimate (e.g.
p-and q-value).
In two specific examples, we examine the immediate neighbors of two proteins identified by MSNet at 5% FDR in the proteome for yeast grown in rich medium.
ARC40 is an essential subunit of the ARP2/3 complex (Fig.3), and RPS29B is a member of the 40S ribosomal complex (Supplementary Fig.S4).
Both proteins had multiple peptides identified in the MS experiment, but their MS protein scores fell below the error threshold of the MS software, and they were not identified with confidence.
Both proteins have functions appropriate for yeast growing in rich medium, and have previously been identified with high confidence in the YPD* reference set.
Moreover, deletion of either gene causes notable growth defects (Giaever et al., 2002); strongly supporting their expression in the sample.
MSNet effectively rescues both proteins and gives them higher scores, based on the their MS evidence and their functional associations to other proteins that were confidently identified in the MS analysis.
MSNet improved protein recall in several active pathways in rich-medium yeast e.g.
glycolysis/gluconeogenesis, fatty acid metabolism, RNA biosynthesis, amino-acid biosynthesis and degradation (Dennis et al., 2003) (EASE-value = 0.05).
MSNet may be viewed as a quantitative complement to graphical tools that map omics experiment results onto known functional pathways (Dennis et al., 2003; Paley and Karp, 2006).
MSNet improves protein identification by both increasing the number of true identifications and reducing false identifications.
Since MSNet produces a revised ranking of MS-identified proteins, some proteins can receive lower ranks than in the MS analysis and fall below MSNets 5% FDR threshold, despite satisfying the MS 5% FDR threshold.
There is some evidence that these demoted proteins might be false positive MS identifications: in yeast, the percentage of demoted proteins that can be validated by presence in the reference set is much smaller than the percentage of new MSNet proteins that can be validated similarly (Supplementary Table S6).
In human, all demoted proteins were network singletons i.e.
they had no network neighbors.
We list the demoted proteins for all experiments, as well as the union of MS and MSNet identifications in Supplementary Table S6.
Using the high-confidence list of MSNet identifications as a starting point, one may narrow the range of additional experiments that are run to validate the existence of computationally predicted proteins.
To the best of our knowledge our method is the first to use gene networks to improve protein identification in shotgun proteomics.
Gene functional networks have been widely used for predicting gene function.
For example, Deng et al.
(2003) modeled functional linkages as a Markov network, predicting a genes function based on the functions of its neighbors.
More recently, Wei and pan (2008) used functional associations to learn per-gene mixing proportions in a spatially correlated mixture model to improve large-scale studies such as differential gene expression.
We have shown that MSNet is able to exploit a single organism-wide gene functional network to improve protein identification across different sample conditions, including different growth media and ranging from proteome-wide analysis to subcellular fractions.
In contrast to previous approaches using MS and mRNA expression data (Ramakrishnan et al., 2009), MSNet is easily applicable across datasets and experimental conditions, and does not depend on the availability of matching sample-specific data.
MSNet is also directly applicable to smaller, focused protein preparations (Section 3.3) and to higher organisms, as we show for the proteome of cultured human cells.
It is also possible to incorporate other sample-specific data when available by replacing the mass-spectrometry specific term oi (Equation 1) by a probability conditioned on other data sources e.g.
LC separation profiles.
Omics integration approaches like MSNet will become increasingly powerful as functional association networks become broadly available, as for C.elegans (Lee et al., 2008), mouse (Guan et al., 2008; Kim et al., 2008; Pena-Castillo et al., 2008) and other organisms (Bowers et al., 2004; von Mering et al., 2003).
ACKNOWLEDGEMENTS The authors thank Dan Boutz for mass-spectrometry assistance, Insuk Lee for pre-publication access to the human gene network, and Prof. William Noble and Lukas Kall for assistance with Percolator.
2960 [09:38 26/10/2009 Bioinformatics-btp461.tex] Page: 2961 29552961 Network mining for shotgun proteomics They also thank Prof. Inderjit Dhillon, Prateek Jain and Raghu Meka for feedback on algorithm convergence, Martin Blom for discussions on network-shuffling, Lee Thompson for proofreading the convergence proofs and Lilyana Mihalkova, Prof. Raymond Mooney and Prof. William Press for useful discussions on relational learning.
Funding: National Science Foundation grant (DBI-0640923); the National Institutes of Health grants (GM067779, GM076536); the Welch (F-1515) and Packard Foundations grant; International Human Frontier Science Program support (to C.V.).
Conflict of Interest: none declared.
ABSTRACT Summary: Assemble is an intuitive graphical interface to analyze, manipulate and build complex 3D RNA architectures.
It provides several advanced and unique features within the framework of a semi-automated modeling process that can be performed by homology and ab initio with or without electron density maps.
Those include the interactive editing of a secondary structure and a searchable, embedded library of annotated tertiary structures.
Assemble helps users with performing recurrent and otherwise tedious tasks in structural RNA research.
Availability and Implementation: Assemble is released under an open-source license (MIT license) and is freely available at http://bioinformatics.org/assemble.
It is implemented in the Java language and runs on MacOSX, Linux and Windows operating systems.
Contact: f.jossinet@ibmc-cnrs.unistra.fr Received on April 22, 2010; revised on June 8, 2010; accepted on June 10, 2010 1 INTRODUCTION RNA molecules are able to adopt intricate 3D folds.
The number of RNA tertiary structures has increased dramatically in recent years.
Together with genomic sequence data, these structural data have sharpened our understanding of RNA structure and folding.
By exploiting this body of knowledge, 3D architectures of RNA molecules can be produced using various molecular modeling strategies.
Such theoretical approaches have proven to be valuable in the past for understanding folding and function.
We describe here a new graphical tool, Assemble, which combines automated and manual protocols within an iterative modeling process.
The modeling can be performed, either ab initio or by homology, on the basis of sequence alignment, chemical probing data and electron density maps derived by crystallography or cryo-electron microscopy.
2 RESULTS 2.1 General description of Assemble An RNA architecture is achieved through two levels of organization: (i) the RNA secondary structure constraining and (ii) a tertiary structure stabilized by recurrent tertiary modules and long-range interactions.
The manipulation and the construction of the RNA To whom correspondence should be addressed.
model can be done in a coherent fashion both at the 2D and 3D levels through two synchronized windows.
2.2 Construction of the secondary structure component Since an RNA secondary structure produces the skeleton constraining the tertiary structure, its definition constitutes a first and essential step.
Assemble is able to load any secondary structure described with the CT and BPSEQ file formats or stored in a FASTA file using the bracket notation.
It can also compute a secondary structure for an RNA sequence stored in a FASTA file (see Section 2.8 for details).
The secondary structure displayed by Assemble is made of two important elements that can be changed interactively.
First, the helices can be selected and moved to modify the 2D plot.
They can also be deleted and created to fit the secondary structure according to the users assumptions.
If a complete or partial 3D structure exists, the tertiary basebase interactions are described with the geometric symbols of the LeontisWesthof classification (Leontis and Westhof, 2001).
They can also be edited, deleted and created interactively.
They play key roles during the construction process, as described in the next sections.
2.3 Building the first draft of the tertiary structure component The helical regions and the non-helical linking elements constitute the building blocks that can be selected in the 2D panel and translated into the 3D scene with a regular A-form helical fold.
These building blocks are exported side-by-side in the 3D scene, but can be reorganized manually or automatically.
If the user alters the underlying secondary structure during the modeling process, the corresponding residues can be removed from the 3D scene and new ones can be re-created.
The default helical fold can be altered using a manual and an automated approach.
For manual intervention, a sliding button panel allows to modify the torsion angles of any single residue present in the 3D scene.
The rotation will be applied to all the residues linked through the sugarphosphate backbone (in the 3 or the 5 direction) and to the residues paired to them.
Consequently, the user can define the scope of this rotation by cutting/linking the molecular chains in the 3D scene and/or by editing the basebase interactions in the 2D panel.
The automated approach is described in the next section.
2.4 Extraction and application of local RNA folds RNA architectures are constituted of recurrent folds observed in various RNA molecules playing different biological functions.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[19:42 20/7/2010 Bioinformatics-btq321.tex] Page: 2058 20572059 F.Jossinet et al.
Consequently, Assemble provides the ability to extract and apply these 3D modules to selected regions made in the 2D/3D model.
Assemble provides an embedded and extensible library of high-resolution structures derived from the Protein Data Bank (PDB) (Berman et al., 2000).
This library is available through the MyPDB sliding panel and is provided with each 3D structure pre-annotated with the secondary structure.
A second sliding panel allows the user to query this library for specific RNA modules.
Each hit can be displayed in the 2D and 3D scenes.
The module can then be extracted and saved in a local RNA motifs repository with the Create RNA Motif panel.
The application of an RNA module will thread a selection of the same number of residues in the 3D model into the original 3D fold.
The basebase interactions stabilizing the original module will be added automatically to the secondary structure.
2.5 Fitting of RNA 3D model into electron density maps The progress in cryo-electron microscopy techniques has led to density maps of large RNA architectures at resolution around or below 7 (Becker et al., 2009; Schuler et al., 2006).
Consequently, we have added within Assemble the ability to display such density maps along with the current 3D model.
Assemble can load density maps described with the XPLOR or MRC file formats.
Small-angle X-ray scattering (SAXS) data can also be used by converting them to the XPLOR format with tools like the Situs program package (Wriggers, 2010).
2.6 Geometric refinement of the RNA 3D model Once a first 3D model is established, several geometric and structural deficiencies can subsist.
Consequently, Assemble provides a geometric refinement function to optimize structural parameters like nucleotide stereochemistry, all the basebase interactions, the sugar pucker and atoms distances.
The structural constraints used during this refinement step are deduced from the set of basebase interactions defined in the secondary structure displayed in the 2D panel.
By increasing the number of iterations during the refinement, Assemble converges to a state close to the structure described in the 2D panel.
The refinement is achieved by geometrical least squares using the KonnertHendrickson algorithm (Hendrickson and Konnert, 1980) as implemented in the program Nuclin/Nuclsq (Westhof et al., 1985).
2.7 The complementarity between Assemble and the automated methods A couple of automated methods have been published recently, generally limited in the sizes and resolutions of the produced models (Das et al., 2010; Jonikas et al., 2009; Parisien and Major, 2008).
With its ability to load tertiary structures described in PDB files and to annotate them automatically with a secondary structure, Assemble can consequently be used to improve 3D models produced automatically.
2.8 The distributed architecture of Assemble Several tasks ofAssemble are delegated to RNAalgorithms available as web services: Contrafold (Do et al., 2006) and RNAfold (Hofacker, 2003) for the 2D predictions; RNAplot (Hofacker, 2003) for the 2D plots; and RNAVIEW (Yang et al., 2003) for the 3D annotations.
These web services are hosted by our own laboratory server and are attached to the following website: http://paradise-ibmc.u-strasbg.fr/.
They have been implemented as independent modules that can be used without Assemble.
The web site provides several examples of usage with command-line tools like wget, curl or our own dedicated java client.
The RNAfold and RNAplot algorithms are also provided by the European Bioinformatics Institute (McWilliam et al., 2009).
This loose coupling between the graphical interface of Assemble and its algorithms will allow us to easily include new automated tasks in the framework.
2.9 The coupling of S2S and Assemble to construct a 3D model by homology Among the different kind of usable pieces of information to construct a 3D model, the availability of a solved tertiary structure for at least one RNA molecule within a family is the richest.
Since a molecular 3D architecture evolves much more slowly than sequences, structural data can be inferred for all the other members of an RNA family by homology.
More importantly, because RNA modules are recurrent and occur across the phylogenetic kingdoms, once a motif has been recognized, its sequence can be easily threaded onto the known 3D fragment.
In 2005, we have released the S2S application with the initial goal to find the conserved structural core within a multiple alignment (Jossinet and Westhof, 2005).
During the development of Assemble, we have updated S2S to be able to infer a 3D model for any sequence within this structural alignment.
Once a 3D model is inferred from S2S, it is saved in the directory of the structural alignment, where it can be loaded by Assemble to pursue the modeling process.
Consequently, S2S and Assemble can be used independently or as two complementary steps of a modeling workflow needing a solved tertiary structure and an orthologous sequence to model.
ACKNOWLEDGEMENT We would like to thank the S2S-Assemble community for help and support.
Funding: Human Frontier Science Program (RGP0032/2005-C to E.W., in part); French National Research Agency (AMIS-ARN, NT09_519218 to E.W.
and F.J.).
Conflict of Interest: none declared.
ABSTRACT Summary: The advent of next-generation sequencing for functional genomics has given rise to quantities of sequence information that are often so large that they are difficult to handle.
Moreover, sequence reads from a specific individual can contain sufficient information to potentially identify and genetically characterize that person, raising privacy concerns.
In order to address these issues, we have developed the Mapped Read Format (MRF), a compact data summary format for both short and long read alignments that enables the anonymization of confidential sequence information, while allowing one to still carry out many functional genomics studies.
We have developed a suite of tools (RSEQtools) that use this format for the analysis of RNA-Seq experiments.
These tools consist of a set of modules that perform common tasks such as calculating gene expression values, generating signal tracks of mapped reads and segmenting that signal into actively transcribed regions.
Moreover, the tools can readily be used to build customizable RNA-Seq workflows.
In addition to the anonymization afforded by MRF, this format also facilitates the decoupling of the alignment of reads from downstream analyses.
Availability and implementation: RSEQtools is implemented in C and the source code is available at http://rseqtools.gersteinlab.org/.
Contact: lukas.habegger@yale.edu; mark.gerstein@yale.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on June 23, 2010; revised on November 5, 2010; accepted on November 8, 2010 1 INTRODUCTION The advent of next-generation sequencing technologies has revolutionized the study of genomes and transcriptomes.
In particular, the application of deep sequencing approaches to transcriptome profiling (RNA-Seq) is increasingly becoming the method of choice for studying the transcriptional landscape of cells (Hillier et al., 2009; Mortazavi et al., 2008; Wang et al., 2009).
Typically, the first step in this analysis is the alignment of the To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
sequence reads to a reference sequence set.
Recently, a number of different alignment tools have been developed to map short reads in an efficient manner (Trapnell and Salzberg, 2009).
While much progress has been made on this front, there is still a great need for a set of software tools that facilitate the downstream analysis of mapped RNA-Seq reads.
Further, two other issues remain to be addressed.
First, the immense file size of next-generation sequencing data poses many challenges in terms of data processing, storage and sharing.
Secondly, mechanisms to protect personal confidential genetic information need to be established.
With the birth of personal genomics, sequencing data stems fundamentally from individuals, and this type of data cannot be distributed as easily because significant privacy concerns arise with sharing all the sequence variations of a particular individual (Greenbaum et al., 2008; Lowrance and Collins, 2007).
One critical challenge for genomics, then, is to devise new data summaries that allow the sharing of large amounts of information from sequencing experiments without exposing the genotypic information of the underlying individual (Supplementary Material).
Although many data formats have been developed such as SAM (Li et al., 2009), there is no practical solution yet that addresses the privacy concerns when sharing large sequence alignment files.
Addressing this challenge is precisely what we have endeavored to do in putting together the Mapped Read Format (MRF), a format that allows data summaries to be exchanged, enabling many aspects of the RNA-Seq calculation to be performed such as expression measurements, but that also detaches the actual sequence variation in a person into separate files.
Further, it provides a very clear way of linking these two pieces of information so that the data summaries can be subsequently conjoined back to the original sequences for more in-depth analyses with potentially confidential data.
Here, we present an overview of a flexible suite of tools (RSEQtools) that are designed to facilitate easily customizable workflows and efficient pipeline building for the analysis of RNA-Seq experiments using this compact format (Fig.1).
Briefly, we first convert the aligned reads into MRF and thus decouple the alignment step from the downstream analyses.
RSEQtools implements several modules using this standardized format for performing common RNA-Seq analyses, such as expression quantification, discovery of transcribed regions, coverage computations annotation manipulation, etc.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:39 16/12/2010 Bioinformatics-btq643.tex] Page: 282 281283 L.Habegger et al.
Fig.1.
Schematic overview of RSEQtools.
Mapped reads are first converted into MRF from common alignment tool output formats, including SAM.
The resulting MRF files can be divided in two files: one with the alignment only and another with the corresponding sequence reads.
The read identifiers provide a mapping between the two files.
Then, several modules perform the downstream analyses independently from the mapping step, such as expression quantification, visualization of the mapped read and the calculation of annotation statistics, etc.
Other tools have been developed based on this framework to perform more sophisticated analyses such as transcript assembly, isoform quantification (IQSeq, http://rnaseq.gersteinlab.org/IQSeq), fusion transcript identification (FusionSeq, http://rnaseq.gersteinlab.org/fusionseq), as well as aggregation and correlation of signal tracks (ACT, http://act.gersteinlab.org).
2 FEATURES AND METHODS 2.1 MRF and converters MRF only stores a minimal set of information, i.e.
information that cannot be derived from the MRF data itself.
This has the advantage of keeping the format succinct, while still capturing the relevant information for most analyses.
MRF consists of three components: comment lines (optional) denoted by a leading # sign, a header line and the mapped reads.
The header line specifies the data type of each column: AlignmentBlocks, Sequences, QualityScores and QueryID.
The column type AlignmentBlocks is required and represents the mapped reads.
Each alignment block contains the coordinates with respect to the reference genome to which the read aligns as well as the read coordinates.
A read spanning multiple regions, e.g.
multiple exons, is denoted by multiple alignment blocks that are separated by a comma.
Paired-end reads can be represented by using a set of alignment blocks for each end, which are separated by the | symbol.
By using this format, it is straightforward to specify both gapped and paired-end alignments.
The RSEQtools package includes various utilities to convert the output of several mapping tools into MRF.
A converter for the commonly used SAM format is included as well.
The first example below represents two paired-end reads where one end is spliced, whereas the second example shows two un-spliced single-end reads with their associated QueryIDs: # Example 1 AlignmentBlocks chr2:+:601:630:1:30,chr2:+:921:940:31:50|chr2:+:1401:1450:1:50 chr9:+:451:460:1:10,chr9:+:831:870:11:50|chr9:+:945:994:1:50 # Example 2 AlignmentBlocks QueryID chr4:-:1221:1270:1:50 1 chr16:+:511:560:1:50 2 The optional types Sequences, QualityScores and QueryID provide additional information.
In particular, the confidentiality issues can be addressed by generating two files: one including the alignments and a second one containing the sequences such as a FASTQ file.
The former is useful for most analyses and can be publicly shared because it does not contain confidential information, whereas the latter can be subjected to a higher level of security and control.
The two files can be conjoined, if necessary, by using the common QueryID as shown in Figure 1.
2.2 RNA-Seq analysis with RSEQtools The RSEQtools suite contains a set of modules to perform a large variety of tasks including the quantification of expression values, manipulation of gene annotation sets, visualization of the mapped reads, generation of signal tracks, the identification of transcriptional active regions and several auxiliary utilities (Supplementary Table S1).
Genome annotation tools: to generate a splice junction library from any annotation set, we extract the genomic sequences of all the exons and synthetically create all splice junctions specified in the annotation set.
This splice junction library can be used in combination with the reference sequences.
A second tool is particularly useful when estimating expression 282 [13:39 16/12/2010 Bioinformatics-btq643.tex] Page: 283 281283 RSEQtools levels.
In order to capture the information of the various transcript isoforms, a gene model is required.
The module mergeTranscripts collapses the transcript isoforms into a single gene model by either taking the union or intersection of the exonic nucleotides.
Quantification of gene expression: one of the key features of RNA-Seq is the quantification of expression at different levels.
Hence, a key module calculates the gene expression values for a given annotation set and a collection of mapped reads in MRF format.
The annotation set specifies which elements will be quantified.
The program mrfQuantifier calculates RPKM (reads per kilobase per million mapped reads) values at the nucleotide level (Mortazavi et al., 2008).
Briefly, for a given entry in the annotation set (typically an exon or gene model), the number of nucleotides from all the reads that overlap with this annotation entry are added up and then this count is normalized by sequence length of the annotation entry (per killobase) and by the total number of mapped nucleotides (per million).
This calculation is not performed at the transcript level, which requires a more sophisticated analysis (Guttman et al., 2010; Trapnell et al., 2010).
Visualization of mapped reads: the RSEQtools package also contains various tools for visualizing the results in genome browsers, by means of wiggle (WIG) and bedGraph files, which are commonly used to represent a signal track of mapped reads.
Also, a GFF file can be generated from MRF files to visualize splice junction reads (example in Fig.1).
Identification of transcriptionally active regions (TARs): transcribed regions can be identified de novo by performing a maxGap/minRun segmentation (Kampa et al., 2004; Royce et al., 2005) from the signal files using the wigSegmenter program.
Briefly, the signal is first thresholded to identify transcribed elements.
Contiguous elements whose distance is less than maxGap are joined together and then filtered if the final size is less than minRun.
This type of analysis is particularly useful in discovering novel TARs such as small RNAs, etc.
MRF selection and auxiliary utilities: lastly, RSEQtools includes a set of utilities to easily manipulate MRF files and a collection of format conversion tools allowing for rapid pipeline development.
Implementation and run time: the modules of the RSEQtools suite were implemented in C and the code was optimized in order to efficiently handle large datasets.
The importance of code scalability cannot be overemphasized in a time where datasets become increasingly large and easily exceed several gigabytes.
For example, the conversion of an ELAND export file (uncompressed file size: 4 GB; total number of reads: 20 million; number of mapped reads: 12 million) to MRF takes 2 min and the resulting MRF file is significantly smaller (400 MB uncompressed, 130 MB compressed with gzip).
Converting the same ELAND export file to SAM generates a file of 3.1 GB (uncompressed) and the corresponding BAM file has a size of 1.2 GB.
The subsequent quantification of gene expression using mrfQuantifier requires 45 s to calculate estimates for about 20 000 genes.
In addition, the modularity of RSEQtools also enables the development of additional programs in any programming language and their seamless integration into this framework.
Finally, most modules use STDIN and STDOUT to process the data, making them suitable to be integrated into an automated pipeline.
3 CONCLUSIONS In summary, RSEQtools contains a number of useful and highly specific modules that can rapidly analyze RNA-Seq data.
The MRF format has two major features: it allows the decoupling of downstream analysis from the mapping strategy and addresses the issue of confidentiality that is intrinsic in any sequencing experiments involving human subjects.
By separating the actual sequencing reads from the alignments, MRF provides a mechanism to protect the private genotypic information of the underlying individual.
Although this approach removes the most obvious genotypic features, other distinctive attributes do remain.
First of all, the information in a MRF file is at least equivalent to that in traditional expression array, which can potentially identify the underlying individual.
Secondly, some information about structural variants may be contained in the MRF file of an RNA-Seq experiment.
However, it is not obvious how to extract genotypic information from a subset of structural variations just affecting genes.
In addition, inferring structural variations from RNA-Seq data as opposed to DNA sequencing would be more complicated due to the presence of alternative splicing.
Another advantage of storing the alignments without the underlying sequences is that it saves space, especially as reads become longer.
Moreover, a possible future extension is the development of a specific compression schema that could further reduce the size of the files.
In addition, this data format could be easily applied to sequence alignments obtained from other high-throughput functional genomic assays such as ChIP-Seq or chromosome conformation capture (3C).
ACKNOWLEDGEMENT We thank Raymond Auerbach for critical reading and editing as well as Wasay Hussain for testing of the software.
Funding: National Institutes of Health.
Conflict of Interest: none declared.
ABSTRACT Motivation: Recent years have seen the development of a wide range of biomedical ontologies.
Notable among these is Sequence Ontology (SO) which offers a rich hierarchy of terms and relationships that can be used to annotate genomic data.
Well-designed formal ontologies allow data to be reasoned upon in a consistent and logically sound way and can lead to the discovery of new relationships.
The Semantic Web Rules Language (SWRL) augments the capabilities of a reasoner by allowing the creation of conditional rules.
To date, however, formal reasoning, especially the use of SWRL rules, has not been widely used in biomedicine.
Results: We have built a knowledge base of human pseudogenes, extending the existing SO framework to incorporate additional attributes.
In particular, we have defined the relationships between pseudogenes and segmental duplications.
We then created a series of logical rules using SWRL to answer research questions and to annotate our pseudogenes appropriately.
Finally, we were left with a knowledge base which could be queried to discover information about human pseudogene evolution.
Availability: The fully populated knowledge base described in this document is available for download from http://ontology.pseudogene.org.
A SPARQL endpoint from which to query the dataset is also available at this location.
Contact: matthew.holford@yale.edu; mark.gerstein@yale.edu 1 INTRODUCTION In recent years, formal ontologies have been suggested as a solution to the problem of describing complicated realms of biomedical knowledge (Rubin et al., 1997).
Well-designed ontologies possess a number of positive aspects including, (i) the ability to define controlled vocabularies of terms, (ii) the ability to inherit and extend existing terms, (iii) the ability to declare relationships between existing terms and (iv) the ability to infer new relationships by reasoning over existing terms.
Through the technologies known collectively as the Semantic Web, most especially the Web Ontology Language (OWL) (OWL2, 2009), researchers are able to share and extend ontologies throughout the scientific community.
Although biomedical ontologies have existed for a number of years, scientists are far from realizing the full benefits of their use.
There is still room for considerable advancement in this area, especially in the application of formal reasoning.
A unique strength of formal ontologies in the area of knowledge representation is their ability to be reasoned upon in a logically provable way.
This reasoning is performed using Description Logic To whom correspondence should be addressed.
(DL) , a form of logic developed to reason on objects, both individual objects and classes of objects.
Software called reasoners [examples include Pellet (Sirin et al., 2007), Fact++ (Tsarkov and Horrocks, 2006) and KAON (KAON, 2010)] use the rules of DL to perform particular operations on knowledge bases.
The most important of these are: (i) consistency checking: the adherence of the ontological model to the rules of DL; (ii) satisfiability checking: the ability for classes described to be realized by actual individuals; and (iii) classification: the expansion of relationships between objects inferred from explicitly stated relationships.
The DL version of the OWL language assures that DL reasoners can perform these services in a computationally tractable manner.
While the first two services are imperative for ensuring the integrity of data, the third, the ability to infer new relationships, is especially appealing to scientists as it hints at the possibility of new discovery.
Moreover, the relationships discovered are instantly provable, given the reasoners adherence to the rules of formal logic.
The OWL language offers a rich set of properties for inference, including class subsumption, property subsumption, transitivity of properties and inverse properties.
We consider a simple example.
We define a class Father as a subclass of a class Ancestor and a class Son as a subclass of a class Descendent.
We also define a property has_father as a sub-property of has_ancestor with an inverse property has_son that is a sub-property of has_descendent.
Now, if say that Matt has_father Ted, the reasoner can automatically infer that Ted has_descendant Matt.
Taken in isolation, such examples seem trivial and obvious but they can be very helpful when sifting through huge amounts of data.
Despite the richness of OWLs set of relational properties, it does not cover the full range of expressive possibilities for object relationships that we might like.
For example, it is often useful to declare data relationships in terms of conditional statements or production rules.
For this purpose, a specialized rule language is useful.
The Semantic Web Rule Language (SWRL) incorporates an existing rule language (RuleML) with OWL (SWRL, 2005).
Rules are defined in two parts: antecedents and consequents.
If all statements in the antecedent clause are determined to be true, then all statements in the consequent clause are applied.
In this way, new properties can be assigned to individuals in an ontology based upon the current state of the knowledge base.
A popular example is the Uncle Rule, which states that if a persons father has a brother, that brother is the persons uncle.
So, if Matt has_father Ted and Ted has_brother Doug, the reasoner can infer that Matt has_uncle Doug.
SWRL also specifies a library of built-in functions which can be applied to individuals.
These include numerical comparison, simple arithmetic and string manipulation.
At present, SWRL is the most widely used rule language in the Semantic Web community.
The popular ontology development environment The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i72 i71i78 M.E.Holford et al.
Protege includes a SWRLTab plugin for creating and processing SWRL rules (SWRLTab, 2010).
SWRL is supported by the Pellet reasoner up to the point where rules can be determined to be DL-safe, i.e.
they may be realized in a computationally tractable fashion.
Firing of SWRL rules is performed by Pellet as part of the classification process and new entailments thus generated can be added to an existing ontology.
We wish to exploit these reasoning capabilities in our research on pseudogenes.
For this purpose, we can build upon Sequence Ontology (SO)(Eilbeck et al., 2005), among the most notable of ontologies created in the biomedical community.
SO aims to provide the full set of terms and relationships necessary to perform sequence annotation.
Despite its central importance to molecular biology, sequence annotation has historically been more difficult than required due to a divergence in naming standards.
This makes the sharing of data a challenge.
SO provides a controlled hierarchy of terms that describe elements that may be found upon a sequence, referred to as sequence_features.
These may represent anything from genes and pseudogenes to smaller units such as individual bases.
A class sequence_variant describes variable elements on the sequence such as alleles and copy_number_variations.
Annotative tags which may be attached to elements on the sequence are subclasses of the sequence_attribute class.
Examples include conserved, retrotransposed and transgenic.
In addition to this structured set of terms, SO defines relationships for how sequence elements are inter-related.
In particular, the authors rigorously define part-whole relationships employing formalisms from the philosophical discipline of mereology (Winston et al., 1987).
The most common usage of SO is to label sequence annotations with appropriate SO terms.
Typically this is done by attaching terms to an annotation in a separate format, such as a flat file or a database which describes instances of sequence data (Eilbeck and Lewis, 2004).
The developers have created a modular relational database schema called CHADO for this purpose (Mungall et al., 2007).
Here, notably, sequence elements are not hard-coded with their location on a particular sequence but are linked to featureloc elements which contain individual location information.
CHADO and its companion mark-up format CHADO-XML offer a robust approach to annotating sequences in a formal and logically coherent manner.
Its strengths are particularly evident in the handling of large volumes of data.
We wished to explore an alternative approach.
We decided to create a full knowledge base by populating the SO ontology with individual instances of sequence features.
We would then perform reasoning using relationships defined as part of SO, extensions to these relationships and SWRL rules based upon these relationships.
From this.
we hoped to discover new relationships between sequence features and thereby strengthen our pseudogene annotation.
2 APPROACH Pseudogenes form an almost ideal subdomain for ontology development in that they are connected to normal genomic features while maintaining a large enough number of unique aspects to form an area for independent description.
Pseudogenes represent bits of genomic sequence that were once functional but have become inactive.
They are often the result of various genomic copying processes, principally duplication and retro-transposition.
There are two basic types of pseudogenesprocessed and duplicated.
The former arise through the process of retro-transposition, the latter through duplication events.
As artifacts of a history of copying, pseudogenes offer us a glimpse of evolutionary history, both of individual genes and of the genome as a whole.
An understanding of how and when particular pseudogenes were derived in relation to other genomic features is significant to our comprehension of both genomics and evolutionary biology (Zhang and Gerstein, 2003).
To this end, we have been involved in annotating pseudogenes in collaboration with researchers at the Sanger Institute and at UCSC, including researchers who were involved in the development of SO.
As SO already defines a number of classes related to pseudogenes, we were able use these terms to fill our knowledge base with individuals.
For those terms not present, we were able to extend existing SO classes.
SO currently defines a class pseudogene with several subclasses including processed_pseudogene.
We added additional subclasses duplicated_pseudogene and unitary_pseudogene for instances of these types of pseudogene.
Pseudogenes whose specific type was ambiguous were left as instances of the base class pseudogene.
All pseudogenes are defined as non-functional copies of parent genes.
These genes are generally identified using the transcribed protein.
For this reason, even though pseudogenes may derive from any type of gene including RNA genes, in our ontology all parent genes were instantiated as instances of the SOs protein_coding_gene class.
We created a sub-property of SOs property non_functional_homolog_of to describe the link between a pseudogene and the parent from which it derived.
This sub-property, has_parent_gene restricts the range of values to instances of protein_coding_gene and restricts the maximum cardinality to a single instance.
We used identifiers from the existing pseudogene ontology (PGO), which was created as part of the Pseudofam project (Lam et al., 2009).
We also incorporated information, where available, about the location of particular exons and introns within the pseudogenes, noting of course that these no longer have the same meaning in a non-functional context.
Here, we were able to use existing SO classes, pseudogenic_exon and intron.
To express containment of these features within a pseudogene, we adopted SOs recommended usage of the part_of property, which is defined as a core relationship in the OBO Relationship Ontology (RO) (Smith et al., 2007).
To simplify querying later, we created sub-properties contains_pseudogenic_exon and contains_pseudogenic_intron.
These constrain the domain of the property to members of the pseudogene class and the range of the property to pseudogenic_exons and introns, respectively.
We also defined inverse properties pseudogenic_exon_in_pseudogene and pseudogenic_intron_in_pseudogene.
These constrain the ROs has_part property which is the inverse of part_of.
These inverse properties can, of course, be automatically inferred by the reasoner.
One focus of our research is the relation of pseudogenes to segmental duplications (SDs).
SDs are defined as continuous stretches of DNA that map to multiple locations on the genome.
A common ground rule is that they are 1000 or more base pairs in length and have sequence similarities of 90% or more.
Like pseudogenes, they are residual artifacts of a history of copying.
Apprehending their origins is similarly important to understanding the evolutionary history of the genome (Bailey and Eichler, 2006).
To include SDs in our annotation, we defined a class sd_segment as a subclass of the relatively low-level SO term biological_region.
To keep track of the one or more duplicate segments each sd_segment has, we defined a property is_sd_pair_of.
The sd_segment class acts i72 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i73 i71i78 SWRL reasoning on pseudogenes as both domain and range of this property which is a sub-property of the SO term similar_to.
We also wanted to determine what if any pseudogenes or genes were located within a segment in an SD pair.
We used the same technique of constraining the has_part property that we did for contains_pseudogenic_exons.
Properties called contains_pseudogene and contains_gene were created, as were the inverse properties pseudogene_in_segment and gene_in_segment.
Finally, we created a property to indicate the number of pseudogenes (has_pseudogene_count) and the number of genes (has_gene_count) within a segment.
These were necessary for certain SWRL rules we created later.
On the surface it appears a deficiency that we must specify the size of the lists of genes and pseudogenes as a separate property.
Indeed, the SWRL built-in library provides an operator to determine the length of a list.
However, this is not considered DL-safe as it would violate the open-world assumption which is a central tenet of DL and the Semantic Web.
Essentially, though we list certain genes within a segment, it is not automatically guaranteed that these are the only genes unless we explicitly say so.
Knowledge that is unstated is not presumed to be false; it is merely presumed to be missing.
Attaching elements of the genome to a particular location is problematic in sequence annotation, as exact coordinate locations will always vary between individual members of a species.
For this reason, model sequences have been developed for a number of organisms.
Even where such sequences do exist, however, there will always be incompatibility between builds or versions of the model.
Previously, sequence annotations using SO terms have handled the description of individual sequence features outside of the ontology.
CHADO, for example, mitigates the issue of exact coordinates by abstracting location away from features through the use of a featureloc object.
For our purposes, however, because we perform reasoning on the locations of features, we must incorporate coordinates from an individual build into our knowledge base.
Thus, we needed to create a few relationships to specify exact feature location.
We stored genome loci using the SO class nuclear_sequence.
This class served as the domain for five new properties which help to spell out an exact location: in_build, on_chromosome, has_start_point, has_end_point and on_strand.
The RO property located_in is then used to link a sequence feature to its location.
The in_build property specifies which version of the model sequence we are using.
We declared custom datatypes to limit the legal values for chromosomes (122, X, Y) and for strands (positive, negative).
Our instances of the classes pseudogene (and its subclasses), protein_coding_gene and sd_segment all make use of these properties to specify their precise locations on the genome.
An outline view of the basic classes and relationships in our ontology is provided in Figure 1.
Using SO, one annotates a feature by assigning it an instance of one of the subclasses of sequence_attribute.
This is accomplished using the has_quality property.
Sequence attributes currently defined in SO are for labeling conditions that are either present or absent.
We found that many of the attributes we wanted to use took numerical values, be they counts, ratios or scores.
We decided to create a subclass of sequence_attribute for these types of attributes called sequence_numerical_attribute.
This new class serves as the domain for a has_numerical_value property which is used to represent the data value.
To help organize our attributes, we created a pseudogene_attribute class akin to the SO class gene_attribute.
We further subclassed this with a pseudogene_numerical_attribute Fig.1.
Diagram showing the relationships between some of the base classes of the ontology.
Dashed lines are used to indicate subclass relationships.
Regular lines indicate property relationships.
Classes in SO are highlighted in gray, while those which were added to our ontology have a white background.
Fig.2.
Diagram showing the hierarchy of annotation attributes for our pseudogene ontology.
The dashed lines denote subclass relationships.
Classes from SO are highlighted in gray while classes add by our ontology have a white background.
class.
Specific classes of attribute were created using this hierarchy for the counts (number_of_insertions, number_of_deletions, number_of_stops, number_of_shifts, disablements and polyA) and scores (log_kimura_score, fraction, evalue and identity) we wanted to track.
Figure 2 illustrates the hierarchy of pseudogene annotation attributes.
With the base relationships of our ontology in place, we created a set of SWRL rules to infer new relationships based upon them.
Their design was guided by the goals of our research and they will be discussed in detail in Section 5.
Having defined the terms and relationships of our ontology, we now populated it with instances of pseudogenes, parent genes and SDs.
We then performed consistency and satisfiability checking using the reasoner and classified the data.
At this point, we i73 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i74 i71i78 M.E.Holford et al.
were able to query the resulting knowledge base to answer biological questions.
Here, we were presented with two options.
We could leave the data in the reasoner and query against it programmatically.
Or we could export an XML document containing our fully entailed data and import it into a triple store, a piece of software that functions like a database for relational data.
In both cases, we would be querying the data using a variant of the SPARQL language (SPARQL, 2008), and RDF query language comparable with SQL for relational data.
Generally, triple stores do not support the degree of DL reasoning that actual reasoners do.
In fact, the plain SPARQL language does not expect data to be reasoned upon.
Reasoners typically use a reasoning-enhanced version of SPARQL such as Pellets SPARQL-DL.
The tradeoff however is speed; because they do not need to perform reasoning, triple stores can generally retrieve results much more quickly.
3 METHODS Data about pseudogenes were obtained from the pseudogene.org (Karro et al., 2007) website.
These data reference proteins by Ensembl identifiers in defining parent proteins for the pseudogenes.
We obtained the links between these proteins and the genes that code for them using the Ensembl (Hubbard et al., 2002) website.
To assign an exact location to a parent gene, we used the lowest start value and the highest stop value for all the transcripts of the gene.
Information on SDs was obtained from the web resources of the Eichler lab (Duplication, 2010).
The data were parsed from flat files using a custom program written in Java.
This program employed the OWLAPI library (Bechhofer and Philip Lord, 2003) to build an OWL 2 compatible ontology which included the SWRL rules that we used.
The program used the Pellet reasoner to check the ontology for consistency and satisfiability and then to classify it.
The full set of entailments generated by the reasoner were serialized into an OWL document.
This document was then loaded into the open-source version of Virtuoso, a universal database which includes a triple store (Virtuoso, 2010).
Because the full set of inferences was pre-created, we were able to take advantage of Virtuosos fast performance without losing the advantages of reasoning.
Virtuoso provides an HTTP interface to a SPARQL endpoint, which we were able to query through either a web interface or programmatically.
A key focus of our research is to explore the relationship between pseudogenes and SDs and determine what this can teach us about the evolution of the genome.
We were particularly interested in finding examples of two scenarios.
In the first (Case 1), we sought to find situations that would allow us to directly compare the evolution rate of a pseudogene and its parent gene.
To do this, we would need to find cases where a pseudogene and its parent were located on the separate segments of an SD pair.
Additionally, we needed to verify that no other pseudogenes or genes were on the same segment as that containing the parent gene.
We could then examine the relative substitution rates between the pseudogene and its surrounding area and the parent gene and its surrounding area.
We did this using the Kimura score metric.
If the log of this value fell within a specific range of values, we could argue that the pseudogene was evolving more rapidly, less rapidly or at an equal rate as its parent.
In the second scenario (Case 2), we tried to find pseudogenes that arose not from duplication of a parent gene but from duplication of another pseudogene.
To find these, we again needed to locate cases where a pseudogene and its parent were on the separate segments of an SD pair.
In this case, however, we wanted other pseudogenes to be present on the segment containing the parent gene.
We then looked at whether the original pseudogene was aligned with its parent or with another pseudogene.
In the latter scenario, we were able to conclude that the pseudogene arose from duplication of another pseudogene and suggested that it formed a new category, the duplicated-processed pseudogene.
It is worth noting the possibility of other scenarios, for example, the orginating pseudogene may be located on a third duplicate segment distinct from that of the duplicated pseudogene and the parent gene.
More complex possibilities such as these are discussed in detail in Khurana et al.
(manuscript in preparation).
These cases suggest a sort of flowchart which can be traversed by a series of rules which build upon each other.
We list these rules in Figure 3.
The flowchart can be seen in Figure 4.
Further discussion of this decisions tree, including the strategies employed and their biological rational can be found in the forth-coming paper by Khurana et al.
We created a total of seven rules to reach our goal.
Rule 1 is a foundational rule, necessary for all that follow it.
Its goal is to mark all pseudogenes that are in the segment of an SD pair whose parent gene is located in the other segment.
These pseudogenes are assigned the property has_parent_in_duplicate_segment whose value is the segment of the parent.
Rule 2 uses Rule 1 to find parent segments and then checks the has_gene_count and has_pseudogene_count values to determine if other genes or pseudogenes are present in the segment.
If these other features are present, the pseudogene is given the property has_not_only_parent_in_duplicate_segment.
Rule 3 functions similarly except that it expects the has_pseudogene_count value to be 0 and the has_gene count to be 1 (the parent gene).
Matching pseudogenes are given the property has_only_parent_in_duplicate_segment.
At this point, we can move to directly answer the questions raised by Case 1.
Rule 4 uses Rule 3 to find segments containing only the parent gene.
It then retrieves the log of the Kimura score of the pseudogene.
If its value is above a high cutoff, it determines the pseudogene is under positive selection and assigns it the sequence attribute maybe_positively_selected.
Rule 5 behaves as Rule 4, except that for log Kimura scores below a low cutoff, it assigns the pseudogene the maybe_negatively_selected attribute.
The path taken by Rule 5 is illustrated in Figure 5.
Rule 6 closes out Case 1 by assigning maybe_neutrally_selected to eligible pseudogenes with log Kimura scores between the high and low cutoff values.
We chose 0.4 and 0.4 as high and low cutoffs, as these correspond with the distribution of scores for all pseudogenes (Khurana et al., manuscript in preparation).
Rule 7 handles Case 2 by comparing the alignment of the pseudogene and its parent gene with the alignment of the pseudogene and the other pseudogenes on the duplicate segment.
It uses Rule 2 to find pseudogenes on one segment of an SD pair whose parent gene is on the other segment along with other pseudogenes.
It then uses the SWRL built-in arithmetic capabilities to measure the distance from the start of pseudogene (p1) to the start of its segment (p1dist).
It then looks at the distance from start of the features on the duplicate segment.
If the distance from start for one of the other pseudogenes (p2dist) is closer to p1dist than the distance from start of the parent gene is and if p2dist is within close enough range of p1dist (within the length of p1), it is determined that p1 is aligned with the pseudogene on the duplicate pair.
This allows us to spot potential duplicated-processed pseudogenes which can be given the property aligned_to_pseudogene.
Figure 6 indicates the path traversed by Rule 7.
4 RESULTS With a fully entailed ontology loaded into a triple store, we were able to issue SPARQL queries to find pseudogenes matching the criteria specified by Cases 1 and 2.
In both cases, the SPARQL queries are quite simple and direct.
Recall that in Case 1, we are trying to compare the evolution rate of a pseudogene with that of its parent.
Using Rules 16, we isolated pseudogenes and parent genes which occur on the segments of an SD pair, making sure that no other genes or pseudogenes were present on the segment containing the parent gene.
By analyzing the substitution rate we attached a quality to the pseudogene indicating whether it might be positively, negatively or neutrally selected.
We can now find pseudogenes of i74 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i75 i71i78 SWRL reasoning on pseudogenes Rule Antecedents Consequents R1-gene p has parent gene g p has_parent_in_duplicate_segment d p in segment s s has SD pair d d contains gene g R2-gene p has parent in duplicate segment d p has_not_only_parent_in_duplicate_segment d gene-count(d) > 0 pseudogene-count(d) > 0 R3-gene p has parent in duplicate segment d p has_only_parent_in_duplicate_segment d gene-count(d) = 1 pseudogene-count(d) = 0 R4-gene p has only parent in duplicate segment d p has_quality MaybeUnderPositiveSelection Kimura-score(p) >= 0.4 R5-gene p has only parent in duplicate segment d p has_quality MaybeUnderNegativeSelection Kimura-score(p) <=-0.4 R6-gene p has only parent in duplicate segment d p has_quality UnderNeturalSelection Kimura-score(p) >-0.4 and < 0.4 R7-gene p has not only parent in duplicate segment d p aligns_with p2 p in segment s p is pdist from start of s p has parent gene g g is gdist from start of d-gene p2 in segment d p2 is p2dist from start of d abs(p2dist-pdist) < abs(gdist-pdist) abs(p2dist-pdist) < length(p) Fig.3.
Informal pseudocode description of the rules implemented in SWRL to traverse the flowchart.
Fig.4.
The decision tree to be traversed by SWRL rules.
Dashed lines indicate a No answer; solid lines indicate a Yes answer.
The same convention is used in Figures 5 and 6. interest by naming those that possess this quality.
For example, to find quickly evolving pseudogenes one might issue the following: SELECT ?p WHERE ?p #has_quality #maybe_positively_selected Fig.5.
The path traversed by Rule 5 on the decision tree.
This path follows Case 1 in looking to examples of pseudogenes which evolve at a less rapid pace than their parent genes.
For the sake of brevity, we are skipping the necessary import statements.
In Case 2, we used Rule 7 to locate pseudogenes which were derived from the duplication of another pseudogene rather than a parent gene.
These were found by testing the alignment of a pseudogene to other pseudogenes present on the same segment of the SD pair as the parent gene.
A property relationship was created between these aligned pseudogenes.
To find examples of i75 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i76 i71i78 M.E.Holford et al.
Fig.6.
The path traversed by Rule 7 on the decision tree.
This path follows Case 2 in looking for pseudogenes which have arisen from the duplication of another pseudogene rather than their parent gene.
these duplicated-processed pseudogenes, we ask for pseudogenes fulfilling this relationship: SELECT ?p WHERE ?p #aligned_to_pseudogene ?p2 As a result of this query, we discovered that PGOHUM00000154773 is potentially a duplicated-processed pseudogene, as it is more closely aligned with the pseudogene PGOHUM00000154773 than to its parent gene, ENSG00000205946.
This relationship is illustrated in Figure 7.
5 DISCUSSION Because of their status as genomic fossils, pseudogenes are of interest not only for how they currently appear but how they arose and developed.
Much like examining and dating bones to a paleontologist, the issue of ascertainment is central to the student of pseudogenes.
In this, a certain amount of uncertainty is inherent.
For a number of pseudogenes, we can precisely describe their origin and place in time; for others we are less certain.
We can see an example of this in Case 1 above, where a higher substitution rate suggests that a pseudogene may be positively selected it also raises the possibility that the surrounding region is negatively selected.
We cannot say with full certainty which possibility is the case.
The handling of uncertainty is a problematic issue when formally describing pseudogenes.
OWL and most other mainstream ontology languages do not deal with the concept of probability with respect to knowledge.
This is largely because DL itself only deals with data that is certain.
Other branches of logic exist to handle situations of uncertainty, such as fuzzy or probabilistic logic and extensions to OWL have been proposed to build knowledge bases using these logics (Ding and Peng, 2004).
At present, however, these are confined to the more experimental reaches of knowledge representation studies.
The alternative would be to define terms using a conventional ontology to represent different levels of certainty with Fig.7.
A potential duplicated-processed pseudogene found by aligning one pseudogene with another on the same segment as the parent gene.
The pseudogene, PGOHUM00000154773, is located on chromosome 8 of the reference sequence between bases 7199348 and 7200542.
Its parent gene, ENSG00000205946 (USP17L6P), is found on chromosome 4 between bases 8978698 and 8979894.
PGOHUM00000154773 is found on an SD segment located between 7199348 and 7200542 on chromosome 8.
The parent gene is on the duplicate segment located between 8966987 and 9017856 on chromosome 4.
The duplicate segment also contains another pseudogene, PGOHUM00000149316 between bases 8992177 and 8992537.
Because this other pseudogene is a similar distance from the start of the segment as PGOHUM00000154773 is to the start of its segment (25 190 bp versus 24 249 bp) and the parent gene is in a different portion of the segment (11 711 bp from the start), the deduction that PGOHUM00000154773 is aligned to PGOHUM00000149316 rather than ENSG00000205946 makes sense.
This was found by applying SWRL Rule 7. regards to ascertainment.
As our knowledge base grows, we hope to explore this area more fully.
Performance presents another challenge to builders of biomedical ontologies.
Although OWL-DL guarantees computational tractability, it does not promise that classification can be completed using an amount of time and memory that we may find acceptable.
It is also an unfortunate truth that computational expense increases as an ontology becomes more expressive.
These problems are particularly acute for genomics researchers, where vast amounts of data can quickly bog down a DL reasoner even on a well apportioned machine.
For example, we initially ran out of memory while trying to classify our ontology using the full set of pseudogenes.
This occurred even when running the reasoner on a 32 GB server.
After some experimentation, we were able to get the ontology to classify by performing two steps.
First, we removed all properties that were not used for the creation of entailments for production rules, generating the full set of inferences and then re-inserting the non-essential properties.
Second, we changed individual instances of protein_coding_gene to a custom class SimpleGene which extends SOs biological_region class.
This freed the reasoner from applying the restrictions defined by SO for protein coding genes and saved considerable amounts of memory.
We felt this workaround was acceptable because our production rules do not make use of these restrictions.
After the reasoner had finished generating entailments, we added an assertion declaring SimpleGene a subclass of protein_coding_gene, thus allowing future inferences to i76 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i77 i71i78 SWRL reasoning on pseudogenes Fig.8.
Informal depiction of the coverage provided by the current ontology, including portions derived from SO, as well as areas to be covered in future work.
In the diagram, plain lines indicate class hierarchy (is-a) relationships, while dashed lines indicate property (has-a) relationships.
be drawn upon the genes using SO.
After applying these techniques, classification of the full set of pseudogenes took around 19 min using a 28 GB heap size.
A diminished set containing one of every 10 pseudogenes took around 11 min to classify and the set containing one of every 4 took around 13 min.
It is abundantly clear that at present our approach could not be used for large-scale annotations, such as that of an entire genome.
For large amounts of data, the integration of SO terms with relational database technology through the CHADO schema and CHADO-XML offer a far quicker solution.
It is promising that RDF data can be queried quickly using triple stores, but the process of creating the full set of entailments by classifying the data through the reasoner is still a significant performance bottleneck.
We can only hope that the future will continue to bring performance improvements in this area, both through more efficient algorithms and faster technology.
The ontology we have presented here is an extension of SO that joins additions related to pseudogenes with additions related to SDs.
It forms a useful prototype for describing pseudogenes and provides a useful framework for reasoning and drawing biological inferences.
It stops short, however, of providing a canonical ontology of the domain of pseudogenes.
As part of our future research, we intend to build upon the structure presented here to form a more complete ontology.
For example, it would be useful to add classes and relationships to describe pseudogene characteristics such as regulatory and transcribed.
These terms could be incorporated from previous work by Lam et al.
(2009).
We also wish to incorporate the notion of derivation of a pseudogene, whether from the nucleus or mitochondria.
We hope to enlist the support of other pseudogene researchers in this endeavor.
Finally, we see the potential for further development leading to an ontology of SDs.
Figure 8 illustrates the present coverage of our ontology and areas we hope to include in the future.
6 CONCLUSION We used the SO to build a knowledge base of pseudogenes, extending SO terms where necessary to describe our data and borrowing identifiers from the PGO ontology.
We created a series of custom SWRL rules to find situations of interest involving our research on the relation between pseudogenes and SDs.
Using these rules and the inherent capabilities of DL reasoners, we were able to infer new relationships about our existing data.
We moved this fully entailed knowledge base into a triple store with a SPARQL endpoint to allow us to query it for biologically relevant information.
Funding: The National Institutes of Health and AL Williams Professorship funds; National Institute of Health grants P01 DC04732 and R01 DA021253 (to K.C.).
Conflict of Interest: none declared.
ABSTRACT Summary: BigWig and BigBed files are compressed binary indexed files containing data at several resolutions that allow the high-performance display of next-generation sequencing experiment results in the UCSC Genome Browser.
The visualization is implemented using a multi-layered software approach that takes advantage of specific capabilities of web-based protocols and Linux and UNIX operating systems files, R trees and various indexing and compression tricks.
As a result, only the data needed to support the current browser view is transmitted rather than the entire file, enabling fast remote access to large distributed data sets.
Availability and implementation: Binaries for the BigWig and BigBed creation and parsing utilities may be downloaded at http://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/.
Source code for the creation and visualization software is freely available for non-commercial use at http://hgdownload.cse.ucsc.edu/admin/jksrc.zip, implemented in C and supported on Linux.
The UCSC Genome Browser is available at http://genome.ucsc.edu Contact: ann@soe.ucsc.edu Supplementary information: Supplementary byte-level details of the BigWig and BigBed file formats are available at Bioinformatics online.
For an in-depth description of UCSC data file formats and custom tracks, see http://genome.ucsc.edu/FAQ/FAQformat.html and http://genome.ucsc.edu/goldenPath/help/hgTracksHelp.html Received on February 18, 2010; revised on June 10, 2010; accepted on June 28, 2010 1 INTRODUCTION Recent improvements in sequencing technologies have made it possible for labs to generate terabyte-sized genomic data sets.
Visualization of these data sets is a key to scientific interpretation.
Typically, loading the data into a visualization tool such as the Genome Browser provided by the University of California, Santa Cruz (UCSC) (Kent et al., 2002; Rhead et al., 2010) has been difficult.
The data can be loaded as a custom annotation track, but for very large data sets the upload form times out before the data transfer finishes.
To work around this limitation, some labs with access to Solexa and later-generation sequencing machines have installed a local copy of the Genome Browser, but this requires a significant initial time investment by systems administrators and other informatics professionals, as well as continuing efforts to keep the data in the local browser installation current.
To whom correspondence should be addressed.
Though visualization of results is just one of the many informatics challenges of next-generation sequencing, it is one that we are well positioned to address at UCSC.
We have developed two new data formats, BigWig and BigBed, that make it practical to view the results of next-generation sequencing experiments as tracks in the UCSC Genome Browser.
The BigWig and BigBed files are compressed binary indexed files that contain the data at several resolutions.
Rather than transmitting the entire file, only the data needed to support the current view in the Genome Browser are transmitted.
Collectively, BigWig and BigBed are referred to as Big Binary Indexed (BBI) files.
2 SYSTEM AND METHODS BigBed files are generated from Browser Extensible Data (BED) files.
Like the BED format, the BigBed format is used for data tables with a varying number of fields.
BED files consist of a simple text format: each line contains the fields for one record, separated by white space.
The first three fields are required, and must contain the chromosome name, start position and end position.
The standard BED format defines nine additional, optional fields, which (if present) must appear in the predefined order (Supplementary Table 1).
Alternatively, BED files may depart from the standard format after the third field, continuing with fields specific to the application and data set.
BigBed files that contain custom fields, unlike those of simple BED format, must also contain the field name and a sentence describing the custom field.
To help others understand custom BED fields, an autoSql (.as) (Kent and Brumbaugh, 2002) declaration of the table format can be included in the BigBed file (Supplementary Table 2).
BigWig files are derived from text-formatted wiggle plot (wig) or bedGraph files.
They associate a floating point number with each base in the genome, and can accommodate missing data points.
In the UCSC Genome Browser, these files are used to create graphs in which the horizontal axis is the position along a chromosome and the vertical axis is the floating point data (Fig.1).
Typically, these graphs are represented by a wiggly line, hence the name wiggle.
Three text formats can be used to describe wiggle data at varying levels of conciseness and flexibility.
Values may be specified for every base or for regularly spaced fixed-sized windows using the fixedStep format.
The variableStep format encodes fixed-sized windows that are variably spaced.
The bedGraph format encodes windows that are both variably sized and variably spaced.
Data files of fixedStep format are divided into sections, each of which starts with a line of the form: fixedStep chrom=chrN start=position step=N span=N where chrom is the chromosome name, start is the start position on the chromosome, step is the number of bases between items and span shows the number of bases covered by each item.
Step and span default to 1 if they are not defined.
This section line is followed by a line containing a single floating point number for each item in the section.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:37 30/7/2010 Bioinformatics-btq351.tex] Page: 2205 22042207 BigWig and BigBed Fig.1.
Genome Browser image of BigWig annotation tracks.
The top track is displayed as a bar graph, the bottom track as a point graph.
Shading is used to distinguish the mean (dark), one standard deviation above the mean (medium) and the maximum (light).
Peaks with clipped tops are colored magenta.
The variableStep format is similar, but the section starts with a line of the format: variableStep chrom=chrN span=N and each item line contains two fields: the chromosome start position, and the floating point value associated with each base.
The bedGraph format is a BED variant in which the fourth column is a floating point value that is associated with all the bases between the chromStart and chromEnd positions.
Unlike the zero-based BED and bedGraph, for compatibility reasons the chromosome start positions in variableStep and fixedStep are one-based.
To create a BigBed or BigWig file, one first creates a text file in BED, fixedStep, variableStep or bedGraph format and then uses the bedToBigBed, wigToBigWig or bedGraphToBigWig command-line utility to convert the file to indexed binary format.
In addition to the text file and (in the case of BigBed) the optional .as file, the conversion utilities require a chrom.sizes input file that describes the chromosome (or contig) sizes in a two-column format (chromosome name, chromosome size).
The fetchChromSizes program may be used to obtain the chrom.sizes file for any genome hosted at UCSC.
All of the command-line utilities can be run without options to display a usage summary.
The wigToBigWig program accepts fixedStep, variableStep or bedGraph input.
The bedGraphToBigWig program accepts only bedGraph files, but has the advantage of using much less memory.
The wigToBigWig program can take up to 1.5 times as much memory as the wig file it is encoding, while bedGraphToBigWig and bedToBigBed use only about one-quarter as much memory as the size of the input file.
Once a BigBed or BigWig file is created, it can be viewed in the UCSC Genome Browser by using the custom track mechanism (Supplementary Material).
In brief the indexed file is put on a website accessible via HTTP, HTTPS or FTP, and a line describing the file type and data location in the form: track type=bigBed bigDataUrl=http://srvr/myData.bb is entered in the custom track section of the browser.
Additional settings in var=value format can be used to control the name, color, and other attributes of the track.
When the custom track is loaded and displayed, the Genome Browser fetches only the data it needs to display at the resolution appropriate for the size of the region being viewed.
While it may take a few minutes to convert the input text file to the indexed format, once this is done there is no need to upload the entire file, and the response time on the browser is nearly as fast as if the file resided on the local UCSC server.
Because the BigWig and BigBed files are binary, we have created additional tools that parse the files and describe the contents.
The bigWigSummary and bigBedSummary programs can quickly compute summaries of large sections of the files corresponding to zoomed-out views in the Genome Browser.
The bigWigInfo and bigBedInfo can be used to quickly check the version numbers, compression status and data ranges stored in a file.
The bigBedToBed, bigWigToWig and bigWigToBedGraph programs can convert all or just a portion of files back to text format.
3 IMPLEMENTATION The BigBed and BigWig readers and writers are written in portable C; other programs that can interface with C libraries can make use of the code directly.
For those working in languages that do not interface well with C, the Supplemental Information describes the file format in sufficient detail to reimplement it in another language.
Several layers of software are involved in enabling the remote access of the BigBed and BigWig files.
This section describes the software architecture, algorithms and data structures at a high level, and should be useful to anyone trying to understand the code enough to usefully modify it or to implement similar file formats that work well in a distributed data environment.
3.1 Data transfer layer Though BigBed and BigWig can be used locally, the primary design goal for this format was to enable efficient remote access.
This is done using existing web-based protocols that are generally already available at most sites.
Unlike typical web use, bigBed and bigWig files require random access.
At the lowest layer, we take advantage of the byte-range protocols of HTTP and HTTPS, and the protocols associated with resuming interrupted FTP transfers, to achieve random access to binary files over the web.
Web servers supporting HTTP/1.1 accept byte-ranges when the data is non-volatile.
OpenSSL provides SSL support for HTTPS via the BIO protocol.
FTP uses the resume command and simply closes the connection when sufficient data has been read.
2205 [15:37 30/7/2010 Bioinformatics-btq351.tex] Page: 2206 22042207 W.J.Kent et al.
3.2 URL data cache layer Since remote access is still slow compared to local access, and data files typically are viewed many times without changing, we implemented a cache layer on top of the data transfer layer.
Data are fetched in blocks of 8 Kb, and each block is kept in a cache.
The cache is implemented using two files for each file that is cached: a bitmap file that has a bit set for each file block in cache and a data file that contains the actual blocks of data.
The data file is implemented very simply using the sparse file feature of Linux and most other UNIX-like operating systems.
The cache software simply seeks to the position in the file where the block belongs and writes it.
The operating system allocates disk space only for the parts of the file that are actually written.
The cache layer is critical to performance.
Parts of the file, including the file header and the root block of the index, are accessed no matter what part of the genome is being viewed.
These parts need be transmitted only once.
In addition if multiple users view the same region of the genome, later users will benefit from the cache, as will a single user looking at the same region multiple times.
Though a cache can help convert remote access to local access, a minimum of one remote accessto check whether the file has changed at the remote siteis required even on a completely cached file.
Minimizing the number of cache checks is one of the motivations for keeping the index and the zoomed data in the same file as the primary data.
Even though a change check involves little in the way of data transfer, it does require a round trip on the network, which can take from 10 to 1000 ms depending on the network connectivity.
For similar reasons, though data are always fetched at least one full block at a time, the system will combine multiple blocks into a single fetch operation whenever possible.
3.3 Indexing The next layer handles the indexing.
It is based on a single dimensional version of the R tree that is commonly used for indexing geographical data.
The index size is typically less than 1% of the size of the data itself.
A BigBed file can contain overlapping intervals.
Overlapping intervals are not as easy to index as strings, points or non-overlapping intervals, but several effective techniques do exist, including binning schemes (Kent et al., 2002), nested containment lists (Alekseyenko and Lee, 2007) and R trees (Guttman, 1984).
R trees have several properties that make them attractive for this application.
They perform well for data at a variety of scales in contrast to binning schemes that typically have a sweet spot at a particular scale of data close to the smallest bin size.
R trees also minimize the number of seeks (and hence network roundtrips) compared to nested containment lists, another popular genomics indexing scheme.
The basic idea behind an R tree is fairly simple.
Each node of the tree can point to multiple child nodes.
The area spanned by a child node is stored alongside the child pointer.
The reader starts with the root node, and descends into all nodes that overlap the query window.
Since most child nodes do not overlap, only a few branches of the tree need to be explored for a typical query.
Though a separate R tree for each chromosome would have been simpler to implement, we elected to use a single tree in which the comparison operator includes both the chromosome and the position.
This allows better performance on roughly assembled genomes with hundreds or thousands of scaffolds, and also lets the files be applied to RNA as well as DNA databases.
To improve the efficiency of the single R tree, we store the chromosome ID as an integer rather than a name, and include a B+ tree to associate chromosome names and IDs in the file.
In the source code, the combined B+ tree and R tree index is referred to as a cirTree.
One additional indexing trick is used.
Because the stored data are sorted by chromosome and start position, not every item in the file must be indexed; in fact by default only every 512th item is indexed.
The software finds the closest indexed item preceding the query, and then scans through the data, discarding some of the initial items if necessary.
This may seem wasteful, since hundreds of thousands of bytes may be transferred in the same time that it takes to seek to a new position on disk, but in practice little time is lost and as a benefit the index is less than 1% of the size of the data.
3.4 Compression The data regions of the file (but not the index) are compressed using the same deflate techniques that are used in gzip as implemented in the zlib library, a very widespread, stable and fast library built into most Linux and UNIX installations.
The compression would not be very efficient if each item was compressed separately, and it would not support random access if the entire data area were compressed all at once.
Instead the regions between indexed items (containing 512 items by default) are individually compressed.
This maintains the same degree of random accessibility that was enabled by the sparse R tree index while still achieving nearly the same level of compression as compressing the entire file would.
The final layer of software is responsible for fetching and decoding blocks specified by the index.
It is only this final layer that differs between BigWig and BigBed.
4 RESULTS AND DISCUSSION The BigBed and BigWig files succeed in overcoming browser upload timeout limits.
By deferring the bulk of the data transfer to be on demand, the upload phase of BigWig and BigBed files now takes less than a second even on home and remote networks, well within the 300-s upload time limit at UCSC.
The on-demand connectivity requirements are modest, adding 0.51.0 s of data transfer time overhead depending on where the Big file is hosted (Supplementary Table 3).
BigBed and BigWig files are similar in many ways to BAM files (Li et al., 2009), which are commonly used to store mappings of short reads to the genome.
BAM files are also binary, compressed, indexed versions of an existing text format, SAM.
The samtools C library associated with SAM and BAM (http://samtools.sourceforge.net/) caches the BAM index, though not the data files.
Samtools also can fetch data from the internet via FTP and HTTP, but not HTTPS.
BAM files are not designed for wiggle graphs, and are more complex than BED files, but they do store alignment, sequence and sequence quality information very efficiently.
While this capability theoretically could be added as an extension to BigBed, we have adopted BAM for short read mapping to avoid a proliferation of formats.
BAM files are supported as custom tracks at UCSC, and we have added HTTPS support to BAM using the data transfer and data cache layers developed for BigBed and BigWig.
2206 [15:37 30/7/2010 Bioinformatics-btq351.tex] Page: 2207 22042207 BigWig and BigBed BigBed and BigWig files have been in use at genome.ucsc.edu since June 2009, and have proven to be popular.
As of February 2010, we have displayed data from nearly 1300 files using these formats.
The broader bioinformatics community has started to support these files as well, with Perl bindings available at http://search.cpan.org/lds/Bio-BigFile/ and a Java implementation in progress (Martin Deacutis, personal communication) for use in the Integrative Genome Viewer (http://www.broadinstitute.org/igv/).
Though the use of BigBed and BigWig requires access to the command line creation tools needed to create the files and a website or FTP site on which to place them, this is not an undue burden in the context of the informatics demands of a modern sequencing pipeline, and is clearly preferable to the long and uncertain uploads of large custom tracks in text formats.
ACKNOWLEDGEMENTS We would like to acknowledge James Taylor, Heng Li and Martin Deacutis for their testing and feedback on these formats, and Lincoln Stein for developing the Perl bindings.
Funding: This work was supported by the National Human Genome Research Institute (5P41HG002371-09, 5U41HG004568-02).
The open access charge was funded by the Howard Hughes Medical Institute.
Conflict of Interest: none declared.
ABSTRACT Summary: We develop a novel mining pipeline, Integrative Next-generation Genome Analysis Pipeline (inGAP), guided by a Bayesian principle to detect single nucleotide polymorphisms (SNPs), insertion/deletions (indels) by comparing high-throughput pyrosequencing reads with a reference genome of related organisms.
inGAP can be applied to the mapping of both Roche/454 and Illumina reads with no restriction of read length.
Experiments on simulated and experimental data show that this pipeline can achieve overall 97% accuracy in SNP detection and 94% in the finding of indels.
All the detected SNPs/indels can be further evaluated by a graphical editor in our pipeline.
inGAP also provides functions of multiple genomes comparison and assistance of bacterial genome assembly.
Availability: inGAP is available at http://sites.google.com/site/ nextgengenomics/ingap Contact: scs@bx.psu.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION The rapid development of promising new parallel sequencing technologies, known as Roche/454, Illumina and ABI/SOLID (Mardis, 2008a, b), has dramatically changed the nature of genetic studies, covering a wide range in high-throughput de novo genome sequencing, from microorganisms to living or ancient mammals and human genome re-sequencing and producing hundreds of thousands of reads with continuously decreasing cost.
These huge amount of reads can be used for varied purposes (Mardis, 2008a, b), among which single nucleotide polymorphism (SNP) identification is a common interest as these technologies provide the highest resolution.
Most current methods (Trapnell and Salzberg, 2009) focus on data from only specific sequencing platform and use only either Illumina and ABI/SOLID data, like SOAP (Li et al., 2008b) and MAQ (Li et al., 2008a), or Roche/454 data (Brockman et al., 2008).
None of them can integrate sequencing data from different platforms.
The pipeline we describe here, Integrative Next Generation Genome Analysis Pipeline (inGAP), is designed for this purpose.
Its workflow is shown in Figure 1.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First authors.
Fig.1.
A description of SNP/indel calling workflow of inGAP.
First, assigning reads to a reference genome.
Second, precise multiple alignments are performed for gapped regions.
Third, a Bayesian algorithm is used to call SNPs and indels.
inGAP can detect SNPs and indels by comparing sequence data generated by either Roche/454 and/or Illumina sequencing technologies, with a reference sequence, regardless read lengths and numbers.
Furthermore, it can deal with various genomes from prokaryotes to eukaryotes and detect genetic variations from highly divergent reads against distantly related reference genomes.
Extensive evaluations on both simulated and real datasets show that inGAP detects 97% and 94% of SNP and indels, respectively.
Additionally, we have incorporated genome assembly and multiple genome alignment softwares into inGAP.
To make inGAP user-friendly, all detected nucleotide changes can be searched and further edited, and genome assembly and multiple genome alignment operations can be completed using a graphical viewer.
2 METHODS As most popular technologies in genome sequencing, Roche/454 and Illumina are quite different on reads length and reads numbers they produce.
The read length from Roche/454 is much longer than that from Illumina and is more difficult to handle.
The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:36 2/12/2009 Bioinformatics-btp615.tex] Page: 128 127129 J.Qi et al.
inGAP maps Illumina reads to reference genomes through BWA (Li and Durbin, 2009) by default.
For Roche/454 data, reads mapping and gap opening are performed by BLASTN (Altschul et al., 1990) (for high-divergent mapping) or BLAT (Kent, 2002) (for close related mapping), MUSCLE (Edgar, 2004) is then applied to these mapped reads to obtain detailed multiple alignment for further assembly as shown in Figure 1.
After Roche/454 and/or Illumina reads are assembled, consistency between reads and reference are checked for initially indentifying candidates of SNPs and then evaluated by a Bayesian method.
SNPs candidates passing the evaluation are classified into synonymous/non-synonymous/non-coding ones when annotation information is available (see Supplementary Material for details).
3 RESULTS 3.1 Detecting SNPs and indels from simulated datasets To test the performance of our approach, we used inGAP to simulate 75 bp Illumina reads with different coverage (from 5 to 100) and various levels of divergence (from 0.1% to 1%) from a Helicobacter pylori J99 genome (NC_000921).
Simulated results are shown in Figure 2.
We also incorporated 1% substitutions and 0.2% indels to mimic sequencing errors (see Supplementary Fig.1).
We first tested whether genetic divergence between the target and reference genomes could affect the performance of SNP calling.
Approximate 0.4 million 75 bp reads (20) with 0.11% point mutations and 0.020.2% indels (ranging from 1 bp to 10 bp) were simulated from the complete genome of H.pylori J99.
As shown in Figure 2A, with the increase of genetic divergence, the sensitivity of SNP and indel detectionusing inGAP remain relatively constant as compared with those using MAQ.
Under a lower divergence level (0.1% point mutations and 0.02% indels), both inGAP and MAQ can identify >98% SNPs, and inGAP can also identify >94% indels with a high accuracy (99.4%, Fig.2B).
We then used this divergence rate to evaluate the performance of inGAP and MAQ under different coverage (Fig.2C and D).
inGAP performs slightly better than MAQ on the sensitivity of detecting SNPs, but exhibits a much higher PPV.
Compared to MAQ, inGAP can detect short indels from single-end date sets.
Moreover, it also outperforms MAQ in SNP prediction, because MAQ tends to falsely predict SNPs from gapped regions.
We also used simulated 454 reads to evaluate the performance of inGAP, and found a more promising result as longer reads can span certain short repeat regions.
As shown in Figure 2E and F, SNP discovery rate reaches 94% when using 5 coverage 454 data and sharply increases to 98% under 10 coverage.
It should be noted that inGAP could identify 98% of short indels (110 bp) with 99% accuracy using 454 reads.
By further exploring the undetected SNPs, we found that nearly half of them located in repeat regions and the other half was missed due to a low quality of multiple sequence alignment.
SNPs located in repeat regions can be partially recalled by using less stringent filtering parameters (e.g.
minimum alignment identity or matched read length) or longer reads.
SNPs missed by false alignments can be recovered by manually editing the problematic alignment.
inGAP provides a user-friendly graphical interface for checking and editing predicted SNPs.
Moreover, owing to its robustness and flexibility in mapping more divergent reads, we extended inGAP to assemble repetitive element from fragmented short reads.
Supplementary Figure 2 illustrated an assembly of a 3.5 Kb LINE/RTE element from 454 Fig.2.
Performance comparison between MAQ and inGAP on simulated datasets.
(A) Sensitivity on SNP/indel calling on different levels of divergent Illumina reads.
Green line shows the indels (110 bp) identified by inGAP.
(B) Positive predictive value (PPV) comparison between MAQ and inGAP based on different divergent Illumina reads.
(C) Performance on simulated Illumina reads with coverage ranging from 5 to 100.
(D) PPVs on simulated Illumina reads under different sequence coverage.
(E) Performance on simulated 454 reads with coverage ranging from 5 to 100.
(F) PPVs on simulated 454 reads under different sequence coverage.
sequenced mammoth genome sequences, where the RTE-2_MD from the opossum genome was used as a reference.
In this way, users can easily build a consensus sequence for each type of repetitive elements.
As described in our early study (Zhao et al., 2009), we have successfully built the consensus sequences for various types of interspersed repeats in the mammoth genome.
3.2 Application of inGAP in large-scale eukaryotic genomes Various real datasets have been used to evaluate the performance of inGAP.
The application of inGAP on eight strains of Salmonella Typhi is shown in Supplementary Table 1 (Holt et al., 2008).
We used a combined data from both Roche/454 and Illumina sequencing technologies to investigate crossover and gene conversion in yeast meiosis (Qi et al., 2009).
We identified over 46 000 single nucleotide differences between the two budding yeast strains, from which 91 crossovers and 21 gene conversions have been detected in four meiotic products of one tetrad.
To handle even larger reference genome (e.g.
human chromosomes), we suggest BWA as the reads aligner, which enable inGAP to map 10 million 35 bp Illumina reads on human chromosome 1 within 30 min and 2 Gb memory on a 8-core DELL machine.
128 [12:36 2/12/2009 Bioinformatics-btp615.tex] Page: 129 127129 inGAP 4 DISCUSSION inGAP is the first platform that allows users to evaluate the genetic variation of a sample, that contains multiple types of next-generation sequencing data.
It can also help with completing genome assembly and comparative genome analysis.
inGAP outperforms other software for the following aspects.
(1) It does not have any read length restriction.
It can handle 454 sequencing and/or Illumina sequencing and/or Sanger sequencing datasets.
(2) Besides SNPs, it can detect most small indels in either single-or paired-end datasets.
(3) It has a strong capability to identify variants based on a relatively divergent reference genome, which brings it to a much wider application other than re-sequencing projects.
(4) It provides a user-friendly graphic interface, through which users can browse, search, check, classify and even edit the identified variants.
ACKNOWLEDGEMENTS We greatly appreciate Dr Webb Miller and Aakrosh Ratan (Penn State University) for thoughtful readings of the manuscript.
We thank Huabin Hou (Wenzhou Medical College, China) for suggestions and software testing.
Funding: Gordon and Betty Moore Foundation (to S.C.S.).
Conflict of Interest: none declared.
ABSTRACT Motivation: How to find motifs from genome-scale functional sequences, such as all the promoters in a genome, is a challenging problem.
Word-based methods count the occurrences of oligomers to detect excessively represented ones.
This approach is known to be fast and accurate compared with other methods.
However, two problems have hampered the application of such methods to large-scale data.
One is the computational cost necessary for clustering similar oligomers, and the other is the bias in the frequency of fixed-length oligomers, which complicates the detection of significant words.
Results: We introduce a method that uses a DNA Gray code and equiprobable oligomers, which solve the clustering problem and the oligomer bias, respectively.
Our method can analyze 18 000 sequences of 1 kbp long in 30 s. We also show that the accuracy of our method is superior to that of a leading method, especially for large-scale data and small fractions of motif-containing sequences.
Availability: The online and stand-alone versions of the application, named Hegma, are available at our website:Contact: ichinose@i.kyoto-u.ac.jp; o.gotoh@i.kyoto-u.ac.jp Received on August 29, 2011; revised on October 25, 2011; accepted on October 26, 2011 1 INTRODUCTION The technological development of next-generation sequencing has enabled us to obtain genome-scale promoter sequences (Wakaguri et al., 2008).
The first step toward unraveling the regulatory mechanisms from such large-scale data is to identify cis-regulatory motifs.
Existing computational algorithms used for motif finding may be categorized into three classes: (1) motif discovery from promoter sequences in a single genome (Sandve and Drabls, 2006); (2) phylogenetic footprinting that uses promoter sequences from multiple species (Das and Dai, 2007); and (3) motif search relying on known motif models, such as JASPAR (Sandelin et al., 2004) and TRANSFAC (Wingender, 2004).
To predict the locations of motifs, each class adopts a distinct strategy: Class (1) tries to find particular words or sets of similar words significantly enriched in promoters; Class (2) aligns orthologous genomic sequences and extracts the sites that are well-conserved among species; and Class (3) finds the sites that match a list of known motifs cataloged To whom correspondence should be addressed.
in a library.
Although the latter two classes are applicable to genome-scale promoter sequences in principle, the high computational cost prohibits application of the first class to large-scale data, despite the fact that motif discovery is the only way if we have no prior knowledge of other species or known motifs.
Of the several different approaches adopted in motif discovery, word-based methods are much more scalable than other approaches (Das and Dai, 2007), such as expectation maximization (Bailey and Elkan, 1994) or Gibbs sampling (Lawrence et al., 1993).
In principle, a word-based method exhaustively counts all the oligomers in a given set of sequences and detects the ones that are represented more abundantly than the background frequencies.
However, there are two problems hindering the application of this method to large-scale data.
First, it is not trivial to cluster similar oligomers into fewer groups.
Fundamentally, a word-based method initially detects interesting oligomers without allowing any substitutions, whereas a motif is typically a set of similar oligomers that contain some variations among them.
Hence, we need to apply a clustering method to gather similar oligomers.
However, the computational cost rapidly increases with the number of initial oligomers or the degree of allowed variations.
Second, the detection of significantly abundant oligomers is complicated by the variable background frequencies of different oligomers with a fixed length.
For example, the background frequencies of AT-rich and GC-rich oligomers can differ extensively in human promoter sequences.
Moreover, the difference becomes more remarkable for longer oligomers.
Thus, we have to carefully evaluate the statistical significance of over-representation of particular oligomers in large-scale data.
Here, we report a new motif discovery method that can analyze tens of thousands of DNA sequences each 1 kbp long.
We solve the first problem by using a DNA Gray code [originally proposed by Gray (1947), see also Er (1984)].
The DNA Gray code is an ordering of oligomers in which adjacent oligomers differ from each other by only one nucleotide.
Since neighboring oligomers in the DNA Gray code are similar to one another, we can solve the first problem by searching only neighborhoods within the DNA Gray code.
To solve the second problem, we use equiprobable oligomers, the lengths of which are variably adjusted so that every oligomer should have an approximately equal background probability.
It is easily shown that the equiprobable oligomers can be naturally combined with the DNA Gray code.
We implement our motif discovery method in C to produce the computer program named Hegma and evaluate the performance of Hegma by using a known database, cisRED (Robertson et al., 2006).
The benchmark test indicates that in most situations Hegma The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[18:10 5/12/2011 Bioinformatics-btr606.tex] Page: 26 2531 N.Ichinose et al.
outperforms Weeder (Pavesi et al., 2004), the best existing word-based motif discovery tool (Tompa et al., 2005).
As Hegma is three to four orders of magnitude faster than Weeder, Hegma may be applicable to unprecedented scales of data analyses.
2 METHODS 2.1 DNA Gray code A Gray code is a coding system of binary numbers in which adjacent numbers differ by only one bit.
Although Gray has initially proposed this code as such binary numbers (Gray, 1947), we can easily extend it to quaternary numbers (Er, 1984) to be applied to a DNA sequence.
The DNA Gray code can be constructed iteratively from monomers to arbitrary length oligomers.
Consider a monomer code (A,G,C,T).
This code is obviously a Gray code because adjacent monomers differ by one nucleotide.
Note that we regard the last monomer to be adjacent to the first monomer, and this circularity holds for longer oligomers.
We prepare four copies of the monomer Gray code and concatenate them with each nucleotide, but in the cases of G and T, the copies are arranged in the reverse order.
This procedure yields the dimer Gray code as illustrated in Figure 1.
In the same manner as the dimers, we can construct the DNA Gray code of k-mers (k >1) by preparing four copies of the (k1)-mer Gray code, two of which are reversed and concatenating them to each nucleotide.
In general, if the (k1)-mer code is a Gray code, the k-mer code constructed by the above procedure is also a Gray code.
This fact can be understood from the following observations.
We can partition the k-mer Gray code into four regions in which the first nucleotides in each region are identical.
Inside each region, the oligomers are arranged in Gray code order because the first nucleotides are identical and the others are the (k1)-mer Gray code.
On the other hand, two oligomers at both sides of a boundary between neighboring regions are identical except for the first nucleotides because of the reverse copy.
Consequently, the k-mer code is inductively a Gray code as the monomer code is a Gray code.
The DNA Gray code has an ordered tree structure as a consequence of the construction process mentioned above (Er, 1984).
This implies that we can apply the depth-first search algorithm to the tree to naturally order oligomers of variable lengths.
This feature is important in combining the DNA Gray code with the equiprobable oligomers, as discussed later in Section 2.3.
The Hamming distance between oligomers located at a distance d in the DNA Gray code is smaller than or equal to d. In this regard, when we extract some consecutive oligomers from the DNA Gray code, those oligomers are similar to one another.
However, all similar oligomers are not necessarily in a neighborhood in the DNA Gray code, i.e.
two oligomers having a small Hamming distance can be located at distant positions.
Nevertheless, we can show that the property of the neighboring similarity is beneficial for efficient data processing compared with conventional methods (Section 3).
Fig.1.
Construction process of the DNA Gray code of dimers.
The ordinary and reverse copies in the second row are copied from the monomer Gray code in the first row.
The concatenation of the first and second rows yields the dimer Gray code shown in the third row.
2.2 Shift detection Two oligomers with a shift relation, for example ACGGT and CGGTC, are similar to each other in the sense of edit distance, although the Hamming distance between them is large.
Because of the large Hamming distance, we cannot immediately detect the similarity between such oligomers in the DNA Gray code.
Fortunately, however, we can detect the shift relations of the oligomers at a low cost by taking advantage of the feature that the DNA Gray code is left shift continuous.
Let S be a semi-infinite sequence, S =s0s1 si ,si {A,G,C,T}.
The left shift of the sequence is defined by: (S)=(s0s1 )=s1s2 .
(1) Note that the left shift is the inverse of the construction of the DNAGray code; in the construction process, we concatenate oligomers with each nucleotide, whereas we remove the first nucleotides from the oligomers in the left shift.
To explain the left shift continuity, we introduce a real-valued representation of the sequence in the DNA Gray code.
Let Gk = {g0,g1,...,gi,...,gN1} be a DNA Gray code with N =4k oligomers, where gi is an oligomer of length k, gi =si0si1 sik1.
The real-valued representation k of gi is defined by: xi =k(gi)= i 4k ,xi [0,1).
(2) In general, there is also a real-valued representation of a semi-infinite sequence S, x=(S) as k .
Our aim here is to show the function f that corresponds to the left shift in the real-valued domain x.
In order to understand the left shift function f , we consider the construction process of the DNA Gray code in the real-valued domain, as shown in Figure 2.
The copies and reverse copies in the construction process correspond to the linear maps that have positive and negative slopes in the real-valued domain, respectively.
Therefore, the process is expressed as shown in Figure 2a.
Since the left shift is the inverse process of the construction, we can obtain the left shift function as the inverse map, as shown in Figure 2b.
This function is equivalent to the composition map of the tent map well known in chaos theory (Alligood et al., 1997).
It should be noted that the function f is continuous.
The left shift continuity implies that the image mapped from a contiguous region in the DNA Gray code, which corresponds to a set of similar oligomers, is also contiguous.
If the functions were discontinuous, a contiguous region would be mapped to scattered regions.
The left shift continuity ensures that we can obtain a single region whenever a contiguous region is mapped.
Figure 3 illustrates two examples of contiguous regions r1 and r2, and their images r1 and r2.
The region r2 overlaps with the image r1 (Fig.3b), which corresponds to the left shifts of oligomers in r1 (Fig.3a).
Since this implies that r2 is included in the left shifts of r1, we can judge that those regions have a shift relation.
Thanks to the left shift continuity, a shift relation can be detected by mapping only two oligomers at the beginning and end of the region even though the contiguous region is composed of many oligomers.
To detect overlapped pairs in a set of contiguous regions, we compare the regions with a sorted list of their images.
We can compare those lists in (a) (b) Fig.2.
Construction process and left shift of the DNA Gray code in the real-valued domain.
(a) The construction process can be expressed as linear maps that have positive (A and C) and negative (G and T) slopes.
(b) The left shift function f can be understood as the inverse of the construction process.
26 [18:10 5/12/2011 Bioinformatics-btr606.tex] Page: 27 2531 Motif discovery using Gray code (a) (b) Fig.3.
Mappings from two contiguous regions (r1 and r2) to their images (r1 and r2).
(a) The relations between the contiguous regions and their images are indicated on the left shift function f .
(b) All contiguous regions are illustrated on the same unit line.
Since the region r2 overlaps with the image r1, there is a shift relation between r1 and r2.
Fig.4.
An example of equiprobable oligomers arranged in the order of Gray code.
We use the 0-th order Markov model with I(A)=2 and I(C)=3.
We fix the threshold parameter =8.
The height of a box corresponds to its information content.
a linear order of the number of regions.
Consequently, we can detect shift relations of oligomers quite efficiently.
2.3 Equiprobable oligomers The background probability is a model that represents an intrinsic property of DNA sequences regardless of the presence of motifs.
We can statistically detect an oligomer as a motif when the frequency of its occurrence is significantly higher than the background probability.
In this work, we use the m-th order Markov model of the given sequences as the model of the background probability.
As we mentioned in Section 1, a variation among the background probabilities causes statistical bias in the significance detection.
To overcome this problem, we propose equiprobable oligomers whose lengths are variable, but whose background probabilities are adjusted to be nearly identical to one another.
Let I(S) be the background information content of an oligomer S, where I(S)=log2 P(S) and P(S) is the background probability.
Let S be the oligomer in which the right-most nucleotide is removed from S. We define the equiprobable oligomer S such that it has the following property, I(S)< and I(S), (3) where is a threshold parameter.
As an example, we consider equiprobable oligomers that consist of only A and C with the 0-th order Markov model as the background probability.
In the 0-th order Markov model, the background information content I(S) of an oligomer S is expressed as the sum of the background information contents of individual nucleotides, i.e.
I(S)= I(s0s1 sk1)=k1i=0 I(si).
Figure 4 illustrates such equiprobable oligomers.
Each box corresponds to a nucleotide and its height is drawn to be proportional to the information content of that nucleotide.
Therefore, when the (downwardly) heaped boxes exceed the threshold , the column of those nucleotides becomes an equiprobable oligomer.
All the equiprobable oligomers do not have exactly the same probability; for example, I(AAAA)=8 and I(CCC)=9.
However, the equiprobability is considerably improved compared with fixed-length oligomers, especially in the cases of longer oligomers and a higher order Markov model.
The validity of the digitizing approximation is discussed in Section S.1 in Supplementary Material.
Consider two oligomers, S1 and S2, such that S1 is shorter than S2.
If S2 is an equiprobable oligomer and S1 matches a prefix of S2, S1 cannot be an equiprobable oligomer because I(S1) should be smaller than under the property of Equation (3).
This observation implies that the set of equiprobable oligomers is a prefix code in which no oligomer matches a prefix of any other oligomer.
Recall the feature that the DNA Gray code has the ordered tree structure.
In the prefix code, a code word is always located at a leaf of the tree.
Therefore, the equiprobable oligomers can be ordered on the tree and hence we can naturally combine the equiprobable oligomers with the DNA Gray code so that adjacent oligomers differ from each other by just one nucleotide up to the length of the shorter oligomer.
Algorithm 1 Display equiprobable oligomers with DNA Gray code procedure equigraycode(string S,boolean f ) if I(S) then print S else if f then equigraycode(S+A,true) equigraycode(S+G,false) equigraycode(S+C,true) equigraycode(S+T,false) else equigraycode(S+T,true) equigraycode(S+C,false) equigraycode(S+G,true) equigraycode(S+A,false) end if end if Algorithm 1 shows the recursive procedure that performs the depth-first search on the tree of the DNA Gray code.
By calling equigraycode(,true), one can display all of the equiprobable oligomers with the DNA Gray code.
If we use the i.i.d.
uniform distribution as the background model, we can obtain the DNA Gray code with a fixed length of/2, because I(S)=2|S| in this case.
Therefore, Algorithm 1 can generate the DNA Gray code as a special case.
2.4 Significance detection We have now obtained the DNA Gray code of equiprobable oligomers.
To detect significant motifs from a given set of sequences, we count the occurrences of equiprobable oligomers.
Let C be a set of occurrence counts of equiprobable oligomers: C ={c0,c1,...,ci,...,cM1}, (4) where M is the number of equiprobable oligomers and ci is the count of the i-th oligomer in the DNA Gray code.
We define a contiguous region [i,j] as a cluster if it satisfies the following conditions, i j,ci1 =cj+1 =0 and ck >0,k [i,j].
(5) The cluster is a set of similar oligomers that appear in the given sequences.
We detect the significance of the cluster by using its width w= ji+1 and the total count o=jk=i ck .
The null hypothesis is that the cluster is 27 [18:10 5/12/2011 Bioinformatics-btr606.tex] Page: 28 2531 N.Ichinose et al.
obtained from random sequences generated by the background model.
In the background model, the occurrence probability p of each oligomer can be approximated by p=1/M because oligomers are equiprobable.
Let q be the probability of an oligomer that occurs at least once.
Thus, q is expressed as q=1(1p)T , where T is the total number of oligomers in the given sequences.
The random width W against w can be understood as Bernoulli trials where there are W-successes with the probability q between two failures.
Therefore, the probability distribution of W is a geometric distribution represented by: P(W )=qW (1q)2.
(6) Since Ow, the random total count O against o is conditioned by the width w. If there is no constraint, the probability distribution of O is a binomial distribution with the success probability wp and the number of observations T .
The conditional probability distribution is represented by: P(O|w)=Bin(O)/Bin(Ow), (7) where Bin is the binomial distribution: Bin(O)= ( T O ) (wp)O(1wp)TO.
(8) Using these distributions, we define the p-value pv of a cluster by: pv=P(W w)P(Oo|w).
(9) Since there are many clusters in the set of occurrence counts C, a large number of significance tests must be involved.
To reduce the false discovery rate, we use the e-value ev instead of the p-value, which is adjusted by the number of equiprobable oligomers M as follows, ev=P(W w)P(Oo|w)M. (10) If ev is smaller than a significance level , the null hypothesis is rejected and hence the corresponding cluster is judged to be significantly enriched.
2.5 Summary of methods The flowchart shown in Figure 5 summarizes our motif discovery procedure.
The parameter that characterizes each process is presented beneath the description of the process.
(1) Threshold parameter : the threshold parameter is critical in our method because it regulates the probability of equiprobable oligomers p. Empirically, we can obtain good results when we set p=1/L, where L is the total sum of the lengths of the input sequences.
Therefore, in the application, is automatically adjusted in accordance with the input sequences, such that = log2(L) (empirically, =1).
The rationale behind this estimation is discussed in Section S.2 in Supplementary Material.
Fig.5.
Flowchart of the motif discovery.
Each box that corresponds to a process presents the description (upper) and the parameter (lower) within it.
(2) Order of Markov model m: the background Markov model is constructed from the input sequences that include the motifs themselves.
Since the regions occupied by the motifs are much smaller than the rest of the sequences, the background model can be properly estimated if m is small.
The default value of m is fixed at 3.
(3) Significance level : the significance level is not crucially influential in our method.
We set the default value at 0.01 as a typical value.
(4) Number of shifts: after finding significant clusters, we sort them in the ascending order of their e-values.
We pick up each cluster in this order and look for other clusters that have a shift relation with it.
The clusters thus found are merged into a single motif.
This process is recursively performed.
The depth of this recursion defines the number of shifts allowed.
We set the default value for the depth at 3.
2.6 Data and statistics As the benchmark data, we use the set of human promoter sequences in the cisRED database (Human v9.0, Robertson et al., 2006).
The cisRED database consists of a set of promoter sequences and a set of motifs defined in those sequences, where each motif is conserved among several species and annotated according to the known motif database TRANSFAC (Wingender, 2004).
The number of promoter sequences is 18 779.
The total number of nucleotides is 47 Mbp, of which valid (unmasked) nucleotides amount to 31 Mbp.
After removal of redundancy, the number of conserved motifs is 236 208 and the number of nucleotides occupied by the motifs is 2.3 Mbp.
By comparing the sites predicted by our method with those listed in the cisRED database, we assess the performance of our method at two distinct levels, the nucleotide level and the site level.
The statistics we use are essentially the same as those adopted by Tompa et al.
in their assessment strategy (Tompa et al., 2005).
At the nucleotide level, each dataset consists of pairs (i,p), where i is the sequence ID and p is the nucleotide position within the site.
We denote the sets of known sites and predicted sites by nK and nP, respectively.
At the site level, each set consists of triples (i,s,e), where i is the sequence ID, and s and e are the start and end positions of the site, respectively.
We denote the sets of known and predicted sites by sK and sP, respectively.
At the nucleotide level, the true positive nTP is simply defined by: nTP=|nK nP|, (11) where || implies the size of the set.
At the site level, the true positive sTP is expressed as: sTP= {usK|vsP;u.i=v.i,ov(u,v) len(u)/4}, (12) where ov(u,v)=min(u.e,v.e)max(u.s,v.s)+1 (overlap) and len(u)=u.e u.s+1 (length).
This expression implies that sTP is the number of known sites that overlap with the predicted sites by at least one-quarter of the length of the known site.
The false positive and the false negative are defined as follows, xFP=|xP|xTP,xFN =|xK|xTP, (13) where x=n (nucleotide level) or x=s (site level).
The true negative is defined only at the nucleotide level: nTN =LnFPnFN nTP, (14) where L is the number of valid nucleotides in the promoter sequences.
Of the above definitions, only the false positive at the site level sFP is different from that of Tompa et al.
(2005).
Tompa et al.
allowed overlaps between the predicted sites and removed such sites from sFP if each site overlapped with a known site.
In contrast, we use a slightly more stringent criterion to check whether the clustering of motifs is appropriately performed, i.e.
we include the overlaps of the predicted sites in sFP even if the sites overlap with a known site.
28 [18:10 5/12/2011 Bioinformatics-btr606.tex] Page: 29 2531 Motif discovery using Gray code Either at the nucleotide (x=n) or at the site (x=s) level, the sensitivity xSn and the positive predictive value xPPV are defined as usual: xSn=xTP/(xTP+xFN), (15) and xPPV =xTP/(xTP+xFP).
(16) To average these quantities to give a single statistic, we adopt the correlation coefficient nCC at the nucleotide level, which is defined by: nCC = nTP nTN nFN nFP (nTP+nFN)(nTN +nFP)(nTP+nFP)(nTN +nFN) .
(17) In a similar way, we adopt the average site performance sASP at the site level, which is defined by: sASP= (sSn+sPPV )/2.
(18) 3 RESULTS AND DISCUSSION 3.1 Performance evaluation with all motifs in cisRED To examine the performance of our method, Hegma, we adopt essentially the same evaluation scheme as that used by Tompa et al.
(2005).
To evaluate the effects of data size on the performance, we prepare sets of sequences that are randomly selected from the human promoter sequences of the cisRED database.
In the following results shown in Figure 6, we prepare 10 sets for each number of sequences.
Figure 6a indicates that nPPV at the nucleotide level is insensitive to the variation in the number of sequences.
In the default setting, our method adjusts the threshold parameter such that the equiprobable oligomers should have the probability p=1/L under the background (a) (b) Fig.6.
Prediction statistics at the nucleotide level (a) and the site level (b), as a function of the number of sequences.
The default parameter set described in Section 2.5 is used for calculation.
Each symbol indicates the average of 10 tests with the sequences randomly selected from the full data.
Error bars indicate the maximum and minimum values of the statistics.
The right-most statistics correspond to those for the full data: where nSn=0.27, nPPV = 0.11 and nCC =0.067 at the nucleotide level; sSn=0.34, sPPV =0.13 and sASP=0.23 at the site level.
model, as discussed in Section 2.5.
This adjustment maintains the null distribution at a constant precision, which accounts for the constant rate of false positive (or type I error) and hence nearly constant nPPV .
In contrast, nSn is improved as the number of sequences is increased.
This improvement can be explained by the general characteristics of statistical analysis, where a larger data size leads to more precise results.
The results at the site level are similar to those at the nucleotide level except that sPPV decreases for larger numbers of sequences (Fig.6b).
This decrease in sPPV originates from overlaps between predicted sites, which augment sFP under our definition.
Our method can detect a shift relation between overlapped sites and merge them.
If this process were perfectly performed, the overlaps of the predicted sites would be repressed.
However, we fail to eliminate all the overlaps partly because we restrict the size of shifts to 3 in the default setting.
We impose this restriction to avoid the risk of merging unrelated motifs.
Improved discrimination between related and unrelated motifs is one task to be explored in the future.
Figure 7 shows the memory usage and the calculation time.
Calculations are made on a computer with 3 GHz Intel Xeon with 16 GB memory running under Linux 2.6.
Both time and memory linearly increase with the number of sequences.
It is noteworthy that we need only 30 s for calculation of the full data (18 779 sequences, 31 Mb).
The memory usage of 1.1 GB is also sufficiently feasible for current conventional computers.
3.2 Performance evaluation with specific motifs We compare the performance of our method to that of Weeder (version 1.4.2, Pavesi et al., 2004), a representative word-based method based on exhaustive enumeration with a limited number of mutations.
We choose Weeder because it performed best in the assessment of Tompa et al.
(2005).
Almost all the conventional tools, including Weeder, assume that given promoter sequences are derived from coregulated genes.
This assumption implies that most of the given sequences have at least one specific motif that contributes to the specific regulation.
Therefore, we prepare a set of sequences in which the fraction of sequences holding the motif is variably specified.
We adopt the motif AhR as the specific motif, because it is the most frequent motif in the TRANSFAC annotations.
Let R and U be the sets of sequences with and without the motif AhR, respectively.
We select sequences from R and U according to a predefined percentage that we control.
For example, when the total number of sequences is 1000 and the Fig.7.
Dependence of memory usage and calculation time on the number of sequences.
Each value is the average of 10 trials.
For the full data, the memory usage is 1.1 GB and the calculation time is 30 s. 29 [18:10 5/12/2011 Bioinformatics-btr606.tex] Page: 30 2531 N.Ichinose et al.
(a) (b) Fig.8.
Performance comparison between Hegma and Weeder at the nucleotide level (a) and the site level (b).
The number of sequences is fixed at 1000.
Boxes show the average values of statistics of 10 sets of sequences.
Error bars show the maximum and minimum values of statistics.
The fractions of the motif-containing sequences are varied from 40% to 100%.
The parameter setting of our method is default.
See the text for the parameter setting of Weeder.
percentage of motif-containing sequences is 80%, we select 800 sequences from R and 200 sequences from U.
In the following results, we fix the number of sequences at 1000.
In order to evaluate the performance of single-motif detection, we regard only the known sites as the right sites of the motif AhR, even though the motifs may be present at other sites in the sequence.
We run Weeder under the following settings: the species code is HS; the minimal sequence percentage on which the motif has to appear is 5 (to increase sensitivity); and the top 20 000 (sufficiently large) motifs are reported.
We try the following pairs of motif length and maximal number of mutations: (6,1), (8,2) and (10,3).
Although motif length 12 is also allowed, we do not try it because of the prohibitively long calculation time.
We determine the positions of the predicted sites with the tool locator.out included in the Weeder tools.
Figure 8a shows the results at the nucleotide level.
When the percentage of motif-containing sequences is 100%, i.e.
all the sequences have the specific motif AhR, nCC of Weeder (0.093) is superior to that of Hegma (0.087).
However, Hegma outperforms Weeder under all other situations.
The performance of Weeder becomes worse as the percentage of motif-containing sequences decreases, whereas Hegma is little affected by this variation.
Since the average length of equiprobable oligomers in this evaluation is 10.7, our setting of the motif length of Weeder should be impartial.
Furthermore, Weeder also adopts statistical measures based on Z-score, in a similar way to our method.
Therefore, it is most likely that the equiprobable oligomers adopted in Hegma contribute to improving performance compared with the fixed-length oligomers used in Weeder.
Fig.9.
Average statistics for the 10 most frequent motifs at the nucleotide level.
Boxes show the average values for the statistics of motifs.
Error bars show the maximum and minimum values of the statistics.
The setting is the same as that in Figure 8.
The results at the site level (Fig.8b) are more remarkable than those at the nucleotide level.
At this level, Hegma outperforms Weeder under all situations, including the case that 100% of the sequences contain the motif, where sASP of Weeder is 0.23 and that of Hegma is 0.25.
We consider that the merge of shift-related motifs introduced in Hegma has effectively reduced sFP and hence improved sPPV , as mentioned in the previous subsection.
We repeat the same analysis as mentioned above for the 10 most frequent motifs in cisRED (AhR, aMEF-2, POU2F1, Pax-5, DEAF-1, CREB, HNF-1, DP-1, RSRFC4 and POU3F2).
Figure 9 summarizes the results for these 10 motifs at the nucleotide level by averaging their statistics.
The detailed results for individual motifs together with the results of non-parametric statistical tests are presented in Section S.4 in Supplementary Material.
Clearly, Hegma outperforms Weeder under all the situations tested.
The results at the site level are also similar to those at the nucleotide level (data not shown).
These observations imply that the performance of Hegma is more stable than that of Weeder regardless of the type of motif as well as the fraction of sequences that contain the motif.
An additional examination on a smaller ChIP-seq peak dataset also supports this conclusion as shown in Section S.3 in Supplementary Material.
The average calculation time per dataset (1000 sequences) for Weeder is 10 h, whereas that for our method is only 1.4 s when tested under the same condition mentioned in Section 3.1 and averaged over 40 trials.
Therefore, our method shows considerable advantage in calculation time as well.
3.3 Analysis of unannotated motifs In Section 3.1, we regard the predicted sites that do not match any cisRED annotation as false positives.
However, it is probable that some of them actually represent true motifs absent from the cisRED annotation.
We then extract such unannotated motifs from all significant motifs predicted by Hegma in the full data of the cisRED promoters such that >95% of the sites comprising each motif do not overlap with any annotated sites.
The number of all the predicted motifs is 7528 (composed of a total of 620 153 sites), of which the number of unannotated motifs is 1161 (36 443 sites).
Figure 10 illustrates four examples of the unannotated motifs with the smallest e-values in sequence logos (Schneider and Stephens, 1990).
The unannotated sites tend to be located in distal regions compared with all the predicted sites; the average position (SD) of the unannotated sites is 1140894 bp relative to the transcription 30 [18:10 5/12/2011 Bioinformatics-btr606.tex] Page: 31 2531 Motif discovery using Gray code Fig.10.
Four examples of unannotated motifs absent from the cisRED annotation.
Each motif is labeled according to the name of the most similar motif in the JASPAR database (Sandelin et al., 2004).
We selected these motifs as the ones with the smallest e-values: (1) ev = 6.21064, (2) 1.21040, (3) 1.91035 and (4) 8.61035. start sites, whereas that of all the predicted sites is 737837 bp (p-value of t-test: 0).
The unannotated sites are a subset of the predicted sites and its complementary set is associated with the cisRED annotation.
Therefore, this disparity suggests that the positions of the annotated sites in cisRED may have significant bias toward proximal regions.
These observations may be interpreted as follows; it may be difficult for a phylogenetic footprinting approach, including cisRED, to detect conserved motifs in the distal regions, where the marked sequence divergence or the existence of repetitive elements hinders reliable sequence alignment compared with more conserved proximal regions (Suzuki et al., 2004).
Therefore, our method can complement the phylogenetic footprinting approach to improve the overall sensitivity of motif discovery.
4 CONCLUSION We have developed a large-scale motif discovery tool, Hegma, and shown that Hegma is not only applicable to large-scale data, but also can stably detect motifs even if only a small fraction of the examined sequences contain the motifs.
Thus, Hegma is applicable to situations where the fraction of motif-containing sequences is uncontrollable, such as the detection of splicing enhancers or silencers in exon and intron sequences, or the detection of microRNA binding sites in UTR sequences.
A huge number of such sequences have already been collected in databases.
However, as our knowledge of those motifs is yet far from complete, it is difficult to know in advance the percentage of sequences holding the motifs.
We consider that the speed and precision of Hegma would facilitate discovery of novel motifs from a heap of sequence data.
Funding: Aihara Innovative Mathematical Modelling Project, Japan Society for the Promotion of Science (JSPS) through the Funding Program for World-Leading Innovative R&D on Science and Technology (FIRST Program), initiated by the Council for Science and Technology Policy (CSTP); Grants-in-Aid (No.
20651053, No.
221S0002 and No.
22310124) from the Ministry of Education, Culture, Sports, Science and Technology of Japan, in part.
Conflict of Interest: none declared.
ABSTRACT Motivation: Molecular association of phenotypic responses is an important step in hypothesis generation and for initiating design of new experiments.
Current practices for associating gene expression data with multidimensional phenotypic data are typically (i) performed one-to-one, i.e.
each gene is examined independently with a phenotypic index and (ii) tested with one stress condition at a time, i.e.
different perturbations are analyzed separately.
As a result, the complex coordination among the genes responsible for a phenotypic profile is potentially lost.
More importantly, univariate analysis can potentially hide new insights into common mechanism of response.
Results: In this article, we propose a sparse, multitask regression model together with co-clustering analysis to explore the intrinsic grouping in associating the gene expression with phenotypic signatures.
The global structure of association is captured by learning an intrinsic template that is shared among experimental conditions, with local perturbations introduced to integrate effects of therapeutic agents.
We demonstrate the performance of our approach on both synthetic and experimental data.
Synthetic data reveal that the multi-task regression has a superior reduction in the regression error when compared with traditional L1-and L2-regularized regression.
On the other hand, experiments with cell cycle inhibitors over a panel of 14 breast cancer cell lines demonstrate the relevance of the computed molecular predictors with the cell cycle machinery, as well as the identification of hidden variables that are not captured by the baseline regression analysis.
Accordingly, the system has identified CLCA2 as a hidden transcript and as a common mechanism of response for two therapeutic agents of CI-1040 and Iressa, which are currently in clinical use.
Contact: b_parvin@lbl.gov 1 INTRODUCTION Genome-wide association studies of expression and phenotypic data are becoming a routine methodology for identifying potential biomarkers.
While the literature is rich with supervised or unsupervised clustering of genomic information, methods for studying the relationships between genomic and phenotypic data remain relatively limited.
Existing association methods are typically based on the univariate correlation analysis, which either correlates a single gene to the resultant phenotype(s) or vice versa.
This is known as the gene-and phenotype-based approaches, respectively (Dryja, 1997).
More recently, (Yi et al., 2008) quantized large number of transcript data through clustering, and associated them with physiological responses or clinical metadata.
In contrast, another group of researchers have taken a new direction by first clustering morphometric data and then associating with the transcript data (Han To whom correspondence should be addressed.
et al., 2010).
However, in both cases, correlation is based on the independent, pairwise univariate analysis.
Pairwise univariate correlation analysis can quickly provide important association information, as well as candidates for further screening.
However, it treats the genes and the phenotypes as independent and isolated units, therefore the underlying interacting relationships between the units might be lost.
It is well-known that some transcripts act as regulatory nodes, driving other transcripts in a coordinated manner to determine the phenotypic profile.
Additionally, incubation with each therapeutic reagent simultaneously interferes with a subset of genes.
Here, we hypothesized that simultaneous incorporation of genome-wide expression data coupled with phenotypic data computed from multiple perturbation conditions, each targeting a different molecular region, can elucidate a common mechanism of response that may be hidden otherwise.
In fact, perturbation and molecular diversity of the model system have shown to be capable of reducing the samples needed for biological inference, thus enhancing robustness of biological conclusion (Ideker et al., 2001; Sachs et al., 2005; Tegnr et al., 2003).
Thus, we ask the following questions.
How can traditional univariate associations be modeled simultaneously and in the absence of a correlation threshold?
How can the inherent sparsity of association be formalized within an optimization framework?
How can one compensate for the lack of replicates due to the high experimental cost associated with gene expression profiling?
To address these issues, we have developed an integrated platform that simultaneously and systematically takes into account an ensemble of gene and phenotypic signatures.
Such an enterprise must incorporate an experimental design with sufficient degree of molecular diversity for increased computational robustness.
In this context, molecular diversity is achieved by using a panel of breast cancer cell lines that are well-characterized and readily available through American Type Culture Collection.
Our computational framework consists of two major steps.
First, a vector-valued, multitask regression formulation is adopted to model the relationships between transcripts and phenotypes under multiple experimental conditions.
In particular, the regression coefficients are factorized into two parts.
One part is a shared template that suggests a common mechanism of action under various treatments.
The second part is related to the perturbation that is induced locally in the transcript network under individual perturbation.
The regression has to be sparse, because only a subset of genes is typically involved in a specific phenotypic response.
Sparsity is enforced through L1-norm regularization, which inherently removes outliers and irrelevant associations.
The end result is a sparse regression matrix that captures intrinsic properties of genephenotype association.
This matrix is reordered for improved visualization of the gene phenotype grouping, where the reordering aims at an optimum permutation of rows and columns of the regression matrix such that The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i98 i97i105 K.Zhang et al.
the underlying saliency becomes apparent.
In this context, reordering reveals dominant association between subsets of genes (with the similar expression profile) and subset of phenotypic indices (with the similar measurements).
We have demonstrated the efficacy of our method with synthetic and experimental data, where the main purpose of synthetic data is to profile the robustness and precision of the proposed method.
Experimental data consist of baseline gene expression data for a panel of breast cancer cell lines, which are associated with cell-cycle inhibitor data.
The proposed method can be used as a complementary tool besides baseline regression techniques, to provide a richer and a more promising list of candidate molecular predictors for further biological verifications.
Section 2 presents our computational model and detailed optimization procedures.
Section 3 provides results on synthetic and experimental data.
Section 4 concludes with a discussion on the molecular predictors and system performance.
2 MODELS 2.1 Description of basic computational models In this section, we introduce our basic computational models for exploring the associations between genes and phenotypic responses.
To reduce excessive costs associated with the collection of gene expression data, we assumed that the gene expression were collected under a baseline (unperturbed) condition, as denoted by X0 RCN .
Here, C is the number of cell lines and N is the number of genes.
On the phenotypic side, assume that we obtained measurements Yd RCM s for d =0,1,2,....,D, where M is the number of phenotypic features, d =0 denotes the controlled, baseline condition and d = 1,2,...,D corresponds to the drug-perturbed conditions.
We used the linear regression model to measure the dependency between genes and phenotypes, as illustrated in Figure 1.
The design matrix X0 was mapped to the phenotype responses Yd RCM via a regressing matrix Td RNM , as X0Td Yd .
(1) The coefficient matrices Td s reflect the dependency (or correlation) between the genes and the phenotypes of interest, i.e.
its ij-th entry is the weight associated with the i-th gene in reconstructing the j-th feature in the phenotypic profile under the d-th condition.
There are a number of complexities in estimating T .
These complexities originate from low sample size, high dimensionality of the data and coupling between different perturbation conditions.
However, majority of the transcript data can be considered as Fig.1.
The linear regression model used to compute the sparse association between baseline gene expression data and phenotypic responses.
noisy background, as it believed that only a subset of genes are involved in each specific cellular process.
To address these issues, we propose a sparse, regularized multitask regression framework with co-clustering.
The novelty of our method involves: (i) leveraging the locality of the molecular interactions as a result of treatment with therapeutic agents, and modeling multiple treatments simultaneously; (ii) coupling it with a L1-regularized solution that enforces sparsity and simultaneously compensates for small sample size; and (iii) grouping associations with co-clustering.
First, a multitask regression framework is used to model the molecular interactions under multiple conditions in a systematic way.
The Multitask learning (Caruana, 1997; Lee et al., 2007; Xiong et al., 2007) is aimed at information sharing among learners from a set of different but related tasks, with the hope to boost the overall performance.
In this context, regression (1) under each experimental condition is deemed as a task.
As phenotypic profiles arise from the original gene regulatory network and its local perturbation, we can assume that phenotypic responses are triggered by different experimental conditions are lying on the same low-dimensional space, i.e.
Td =TPd for d =0,1,2,...,D. (2) In other words, task relatedness is enforced by requiring that Td s associated with each task are local perturbations of a shared subspace T. Here, TRNK represents the shared structure (related to the gene regulatory network), Pd RKM compensates for the perturbation of different experimental conditions and K is the dimension of the latent space in which the phenotypic responses are supposed to reside.
In our formulation, K is set to be equal to M for practical reasons, and Pd s are diagonal matrices.
The actual structure of Pd is an open problem at this point, and it is possible that a non-diagonal matrix can produce a better reconstruction result.
The structure of Pd and the choice of K is one of the topics for our continued research.
Nevertheless, the shared template matrix T has the potential to summarize association descriptor between N genes and M phenotypes.
An advantage of decomposing the Td matrices is a significant reduction in the number of variables for estimation.
Second, the L1 regularization technique is used to mathematically guarantee the robustness of the system against irrelevant genes.
The L1 regularization typically leads to sparse learning models, and has been independently discovered in several research areas such as regression shrinkage and variable selection (Tibshirani, 1996), basis pursuit (Donoho et al., 2001), compressive sensing (Donoho, 2006) and feature vector machine (Li et al., 2005).
By penalizing the L1-norm of the variables, part of the regression coefficients will be driven to zero with the level of sparsity controlled by the strength of regularization.
This is a desirable property considering the highly localized functionalities of genes as they relate to specific phenotypic signatures.
By combining the multitask learning frame with the L1 regularization, we established sparse multitask regression as follows: min TRNM PdRMM f =Dd=0X0TPd Yd2F +T1 .
(3) s.t.
PdF =1, for d =1,2,...,D. Here, F is the matrix Frobenius norm and 1 is the matrix L1-norm.
The first term enforces a fit between the gene expression i98 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i99 i97i105 Sparse multitask regression (b)(a) Fig.2.
The co-clustering procedure transforms a randomly displayed association table (a) of 50 genes and 40 phenotypes to a organized partition (b).
and the phenotypic signature under each condition, while the second term enforces sparsity on the shared template T. The constraints Pd=1 are used to prevent trivial solutions (i.e.
T approaches zero and Pd s approach infinity).
Alternatively, this can be achieved by penalizing PdF with a extra regularization parameter.
More recently, a heterogeneous multitask learning framework that considers both continuous (regression) and discrete (classification) variables was successfully used to discover genetic markers that jointly influence multiple correlated traits (Yang et al., 2009).
In comparison, our method considers pure regression setting only, where the phenotypic measurements are continuous.
Formulation (3) allows us to obtain condition-specific regression matrices Td s based on a common template T. Note that for each Td , its non-zero rows signify important genes under the d-th condition.
Therefore, template T, which is shared among multiple Td s, defines a combined list of genes that are important to the phenotypes studied under these conditions.
In other words, T is an integrated association descriptor that summarizes correlating relations between genes and phenotypes under multiple conditions; and we want to read out useful structures (such as the grouped correlation between subsets of genes and subsets of phenotypes) encoded in T. To achieve this goal, we performed co-clustering analysis (Hartigan, 1972) on T. Co-clustering analysis has been used to find clusters in various tabulated data such as the co-occurrence of documents/words (Dhillon, 2001), or the expression of genes under various conditions (Ding, 2003; Kluger et al., 2003; Tanay et al., 2002), by simultaneously grouping rows and columns of the association table.
However, it has rarely been applied to interpret associations between genes and phenotypes, where the association table is not directly available from raw data but instead has to be learned.
In fact, co-clustering can reorganize regression coefficients in a perceptually meaningful manner to bring more insights into our analysis.
This is illustrated by synthetic data, as shown in Figure 2.
For example, assume we have learned an association table of 50 rows (e.g.
genes) and 40 columns (e.g.
phenotypes) where it is difficult to observe any meaningful structures.
However, if we permute the rows and columns of the table by co-clustering (Dhillon, 2001), we will discover four dominant correlation groups, as shown in the Figure 2B.
Such a grouping can be regarded as a distinctive watermark of the genephenotypic association.
Furthermore, rows (genes) grouped into the same block are more likely to participate together in affecting corresponding columns (phenotype responses).
In summary, the sparse multitask regression has three advantages: (i) it allows us to reduce the number of variables from O(MND) to O(NM +DM2); (ii) the sparsity of T easily transfers to those of Td s due to the simple linear relation Td =TPd ; and (iii) as we shall see, the template matrix T is a platform from which explorative analysis can be carried out in identifying important, grouped correspondences between genes and phenotypic signatures.
2.2 Optimization procedures Formulation (3) is a vector-valued regression with intrinsic T and perturbation-specific Pd s. It can be solved by an alternating optimization strategy, i.e.
iteratively fixing Pd s and solving T, and then fixing T and solving Pd s. We will show that both T and Pd s subproblems are convex.
Thus a locally optimal solution of the problem (3) can always be guaranteed.
In the following, we present details on the alternating optimization (Parts I, II and III) and the co-clustering procedure (Part IV).
(I) Fix {Pd}Dd=0 and solve T: We will show that when Pd s are fixed, T can be solved through quadratic programming.
First, use the operator vec() :Rpq Rpq1 to denote the mapping that transforms a pq matrix into a pq1 vector via concatenating the columns in the matrix, and let ivec() be the inverse mapping.
Let t=vec(T)RMN1.
Then define a 3D matrix Ad RCMMN for d =0,1,2,...,D, such that Ad (i,j,:)=vec ( X0(i,:)Pd (:,j) ) .
(4) Here, X0(i,:) is the i-th row in X0, Pd (:,j) the j-th column in Pd and each (i,j)-pair locates an MN 1 vector denoted by Ad (i,j,:).
Now, computing T is equivalent to the following quadratic program min tRMN1 tQt2bt+t1 (5) where Q= D d=0 C i=1 M j=1 Ad (i,j,:)Ad (i,j,:) (6) b= D d=0 C i=1 M j=1 Yd (i,j)Ad (i,j,:).
(7) It can be easily verified that the residual termD d=0X0TPd Yd2F in (3) is identical to tQt2bt up to a constant that is independent of the optimization variables.
Note that the Hessian of the above quadratic programming problem is positive semi-definite: for any xRMN1 we have xQx = D d=0 C i=1 M j=1 xAd (i,j,:)Ad (i,j,:)x = D d=0 C i=1 M j=1 ( Ad (i,j,:)x )2 0.
On the other hand, the L1 regularization term t1 is a convex function.
Therefore, the problem is convex, and there exists a unique, globally optimal solution for the subproblem (5).
The main computational barrier is that the Hessian matrix Q is MN-by-MN , which can be very large and does not fit in a modern desktop computer.
However, this matrix is symmetric, positive-definite Hessian matrix Q and has very low rank in practice, i.e.
its eigen-spectrum decays very quickly to zero.
This is shown in Figure 3, where we chose N =1210 genes and M =3 phenotypes to construct the matrix Q (6) with size 36303630.
It is clear that i99 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i100 i97i105 K.Zhang et al.
Fig.3.
Spectrum of a 36303630 matrix Q, computed from our experimental data, indicates that only the largest 48 eigenvalues are strictly positive and the rest are insignificant.
The spectrum clearly reflects the low-rank nature of the matrix Q and the feasibility of low-rank approximation.
the spectrum of Q decays rapidly, with only the top 48 eigenvalues being strictly non-zero, thus substantiating the low-rank nature of the Q matrix.
As a result, the Hessian matrix can be represented by the low-rank approximation to alleviate prohibitive computational requirements.
To do this, we searched for a rank-R matrix L that best represents the Q matrix in a least square sense, minLRMNR Q LL2F , where RNM, LRMNR is a rectangular matrix with low row-rank and LL is called the rank-R approximation of Q.
This approximation QLL dramatically reduces memory usage from O(N2M2) to O(NMR).
Mathematically, the optimal rank-R matrix L is given by the eigenvectors of Q (Golub and Loan, 1996), which is computationally expensive.
We therefore pursued an approximate solution by adopting the sampling-based low-rank approximation scheme, known as the Nystrm method, which originated from the numerical treatment of integral equations of the second type (Baker, 1997).
The basic idea of the Nystrm method is to randomly sample R columns from the Q matrix, which, due to its symmetry, also corresponds to R rows.
Let E and E denote the sampled columns and its transpose, respectively, where E RMNR.
Let W RRR be the intersection of the selected rows and columns.
Then Q can be decomposed as QEW1E.
In our specific context, Q is represented as the sum of multiple outer products (6).
By utilizing this property, E and W can be computed efficiently as follows: E(p,q)= D d=0 C i=1 M j=1 Ad (i,j,p)Ad (i,j,q), W =E(I,I), 1pMN, q I, where I ={1,2,...,MN}R is the index of selected columns.
Given W and E, the low-rank approximation of Q is then expressed as QLL, where L=EW 12 .
(8) As W is a positive semi-definite (PSD) matrix, there exists theoretically a real square root of W .
In practice, we could encounter diminishing eigenvalues.
A robust way is to first perform the eigenvalue decomposition W =UU, remove those diminishing eigenvalues and then let W 1 2 =U 12 U.
The low-rank decomposition (8) allows us to rewrite the L1-regularized quadratic programming problem (5) into a standard least square problem (with L1 regularization), min tRMN1 Ltq2 +t1.
(9) Here, qRR1 can be determined by expanding the quadratic term in (9), comparing it with (3) and requiring Lq=b.
Formulation of (9) is a good approximation to the original problem (5) and it has been widely examined in statistics, optimization and machine learning.
We use the l1-ls solver (Kim et al., 2007) for large-scale L1-regularized least square problems, which are based on the truncated Newton interior-point method.
Empirically, it can solve large sparse problems with a million variables with high accuracy in a few tens of minutes on a modern desktop computer.
(II) Fix T and solve {Pd}Dd=1: By fixing T, entries of Pd s can be computed using simple scalar equations.
Let the i-th column of the matrix XT be denoted by XT(:,i) and the i-th column in Yd be Yd (:,i).
Its easy to verify that the i-th diagonal entry in Pd can be solved easily as Pd (i,i)=XT(:,i)XT(:,i)/Yd (:,i)22.
To guarantee that Pd s all have Norm 1, we will normalize them by Pd =Pd/PdF .
This can be deemed as iteratively projecting the solutions on the feasible region PdF =1.
Note that rescaling both T and Pd s with 1 does not affect the prediction performance of the multitask regression, but will reverse the signs of associations learned in T. To solve this problem, we require that the signs of the resultant matrix T should be maximally correlated with those of the standard correlation coefficients on the same set of genes.
From a practical standpoint, because Pd s are initialized with identity matrices, we have always observed that they continue to be PSD during the optimization procedure.
Empirically, our method converges rapidly in about 5 to 10 iterations on our current datasets.
(III) Initialization and parameter selection: By fixing one of the two groups of variables, T or Pd s (d =1,2,...,D), the other can be computed.
Here, we choose to initialize Pd s as identity matrices for d =1,2,...,D. Note that initialization of the Td s is usually much easier than that of T , where degrees of freedom are M2D and MN , respectively.
We used leave-one-out cross-validation to choose the hyperparameter since the sample size is very small.
This involves selecting one sample as a testing sample and the rest as training.
We repeated this process for each sample and computed the averaged predictor error on the testing sample at each grid point {103,102,101,1,10}.
(IV) Co-clustering: Template T is an intrinsic regression coefficient matrix linking the gene expression and phenotypic signature under the multiple conditions studied: the ij-th entry signifies the strength of the relationship between the i-th gene and the j-th phenotype.
To reveal the clustered structure in these associations, we used co-clustering to permute the rows and columns of T, so that the underlying saliency becomes apparent and can be visualized.
We have adopted the bipartite spectral clustering (Dhillon, 2001) for simultaneously clustering the genes and phenotypes.
Bipartite spectral clustering uses a bipartite graph where vertices are divided into two types, each from one dimension of the given contingency table (T).
In our case they are genes and phenotypes, denoted by G and P , respectively, and the number of vertices will be M +N .
The edge weights are determined by Wij = { |T(i,j)| vi G,vj P, 0 vi,vj G orvi,vj P. In other words, edges only exist between a gene vertex and a phenotype vertex.
By applying i100 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i101 i97i105 Sparse multitask regression spectral clustering on this bipartite graph, simultaneous groupings on gene and phenotype vertices can be computed.
Mathematically, we need to compute the singular value decomposition of the degree-normalized association matrix, S=D 1 2 l TD 12 r , where Dl is an N N diagonal degree matrix whose i-th entry is the summation of the i-th row in T, and Dr is a M M diagonal degree matrix whose i-th diagonal entry is the summation of the i-th column of T. Interestingly, the left and right singular vectors of S (corresponding to the second largest singular value) not only provide a partitioning of the rows and columns of T, but also provide a natural ordering (embedding) of the required row and column permutations.
3 RESULTS Our proposed method has been tested with both synthetic and experimental data.
The synthetic data is used for method validation and profiling against other known techniques.
Our studies with experimental data identified molecular predictors of cell cycle data from baseline gene expression data.
3.1 Evaluation with synthetic data In the synthetic case: (i) a data matrix X0 R50300 was created from the Gaussian distribution; (ii) a sparse intrinsic template TR3005 with 50 non-zero rows and a small set of randomly generated perturbation matrices Pd R55 were created for each d =1,2,...,D task; and (iii) the responses (e.g.
target values) were then determined by Yd =X0TPd +, where is the noise term.
We examined how well the system recovers Td s, and compared the proposed method with (i) independent L1-regularized regression, and (ii) independent L2-regularized regression, also known as regularized least squares (RLS).
First, we set D=10 and selected one of the tasks to visualize the regression qualities against the competing methods.
Reconstruction results are shown in Figure 4.
Notice that the L1 and L2 regressions (Fig.4c and d) contaminated the true regression coefficients.
In practical association analysis, this can lead to a number of false predictions.
In contrast, multitask regression (Fig.4b) reliably recovered the regression coefficients.
Second, we varied D from 1 to 50 and quantified the average per-task-error for each of the three methods, as shown in Figure 5.
It is clear that the error in multitask regression decreases monotonically with the number of tasks, while the errors in pure L1 and L2 regressions remain stationary.
Although this experiment demonstrates an improved error profile for multitask learning, we have not yet designed a synthetic experiment that maintains a correlation between transcripts.
3.2 Experimental design and quantification of biological endpoints We applied our method to a set of publicly available gene expression data for a panel of breast cancer cell lines collected with Affymetrix HG-U133A (Neve et al., 2006).
We used the following 14 cell lines: MCF12A, HCC38, HCC1428, AU5650, MDAMB415, SUM185PE, ZR75B, MCF7, MDAMB361, LY2, T47D, MDAMB436, MDAMB468 and ZR751.
From the original N =22215 probe sets, we chose 5706 by removing those with a variance of <0.3.
This is slightly above the noise level of the Affymetrix U133 platform.
Notice that the gene expression data were collected under baseline (e.g.
unperturbed) condition.
Our main (a) (b) (c) (d) Fig.4.
Reconstruction of the regression coefficient matrix indicates that multitask learning is more accurate when compared with L1-and L2-regularized regressions.
Td is a 300-by-5 matrix and each column is represented by a unique color.
(a) Ground-truth solution, (b) Multitask regression, (c) standard L1 regression and (d) regularized least square regression.
0 10 20 30 40 50 60 70 1.2 1 0.8 0.6 0.4 0.2 0 0.2 0.4 number of tasks er ro r/ pe r ta sk ( lo g) independent L2reg independent L1reg Multitask L1reg Fig.5.
Multitask learning has an improved error rate profile as the number of tasks is increased.
hurdle has been the prohibitive cost of collecting necessary data (e.g.
three conditions, 14 lines, and at least three biological replicates).
Thus, we assumed that perturbed expression data would be linearly predictable from the control data.
Cell cycle data where collected for cells exposed to three conditions: control condition (e.g.
DMSO solvent alone), the MEK inhibitor CI1040 and the tyrosine kinase inhibitor Iressa.
Both these inhibitors induce cell cycle arrest, but through different mechanisms.
Each cell line was plated in triplicate and incubated for 48 h with CI1040 and Iressa at 5.6 and 4.0 M, respectively.
Subsequently, samples were fixed and stained with Hoechst and BrdU, and 25 fields of view were imaged using the Celomics high-throughput system.
These images were uploaded into the BioSig imaging bioinformatics system (Parvin et al., 2003), and then analyzed for their morphometric and BrdU incorporation on a cell-by-cell basis (Raman et al., 2007; Wen et al., 2009).
Figure 6 shows a sample of images that have been registered with the BioSig and one segmented image.
Each segmented nucleus is represented using a multidimensional feature (Han et al., 2010) and stored in the database.
In our experiment, the pertinent features are total BrdU and i101 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i102 i97i105 K.Zhang et al.
(a) (b) Fig.6.
(a) Biological images are registered with BioSig and (b) each nucleus is segmented to quantify total DNA and BrdU incorporation on a cell-by-cell basis.
DNA content on a cell-by-cell basis.
By aggregating these features, within each well, percentages of cells being G1, S and G2 Phase can be quantified as a function of their treatment, as shown in Figure 7.
The main advantage of microscopy for evaluating cell cycle arrest is a significant reduction in the number of required cells.
Finally, outliers were removed.
Summary results are shown in Figure 8.
3.3 Evaluation with therapeutic agents First, we examined associations of gene expression and cell cycle data using independent L1-regularized regression that learns the regressing coefficients Td s separately for each experimental condition.
The results enabled us to contrast traditional L1 regression with multitask learning.
Predicted results are shown in Figure 9, where each subfigure corresponds to the regression matrix Td under one condition.
Here, zero rows in the regression matrix were removed, and the rows and columns of Td s have been reordered by the co-clustering procedure.
The positive and negative association Fig.7.
By aggregating total DNA and BrdU, on a cell-by-cell basis for all images in each well, the percentages of cells in G1, S, and G2 phase are quantified.
0 20 40 60 80 100 Cancel Cell Lines P er ce nt ag e of G 1 A rr es t M D A M B 436 H C C 1937 M C F 12A M D A M B 468 A U 565 LY 2 M D A M B 415 Z R 751 M C F 7 S U M 85P E Z R 75B M D A M B 361 H C C 38 H C C 1428 PBS CI1040 IRESSA Fig.8.
Percentage of each cell line being arrested in G1 phase with DMSO, CI1040, and Iressa treatment conditions.
between each genephenotype pair is encoded by green and red blocks, respectively.
Second, we applied the proposed multitask regression to learn a common template of correlation between genes and cell cycle data for the two inhibitors (e.g.
CI1040 and Iressa), as shown in Figure 10.
Again, we assumed that each therapeutic reagent would perturb a small molecular region in the cell cycle progression.
In this experiment, both CI1040 and Iressa induced cell cycle arrest by targeting different molecular moieties.
However, if there is a common mechanism of action, then we would like to infer that.
We observed that the genes identified by multitask regression (Fig.10) contained subset of genes that were identified separately by independent L1 regression, shown in Figures 9b and c. i102 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i103 i97i105 Sparse multitask regression (a) (b) (c) Fig.9.
The regression matrices (a) T0, (DMSO) (b)T1 (C11040), and (c)T2 (Iressa) learned by the independent regression using 14 cell lines and reordered by co-clustering.
Fig.10.
The intrinsic template T learned by the multitask regression using 14 cell lines and the two drug conditions (CI1040 and Iressa) and reordered by co-clustering.
However, there are certain genes that can only be predicted through the multitask regression.
These are hidden markers that are relevant to the effect of the therapeutic reagent and provide potential new hypothesis for further studies.
The total computation time on a modern desktop computer is approximately 6500 s. 4 DISCUSSION Our experiments with synthetic data have clearly demonstrated that multitask learning offers the following advantages over independent L1 regression: (i) regression is less noisy; (ii) regression error is reduced as a function of the number of tasks; and (iii) hidden variables are revealed since traditional L1 regression can push non-zero coefficients to zero and vice versa.
Therefore, the bulk of the discussion in this section is devoted to the experimental data by focusing on a few important genes and their independent analysis through Ingenuity Pathway Analysis (IPA) and Pathway Studio.
(I) CLCA2 is a hidden variable that has been identified through multitask regression and is shown to be negatively associated with the S phase.
We hypothesized that CLCA2 is a common mechanism of response for inhibitors CI1040 and Iressa.
This gene is known to be downregulated in breast cancer cell lines.
In addition to being a p53 client (Gruber and Pauli, 1999), its knockdown leads to increased invasiveness (Walia et al., 2009), and it is epigenetically regulated (Li et al., 2004).
It is also a tumor suppressor gene that may be a potential target for therapy.
It is likely that CLCA2 acts as a common molecular switch to inhibit DNA synthesis and initiate apoptosis as a result of treatment with either therapeutic agent.
Therefore, it not only serves as a therapeutic target, but can also be used in combination with other therapeutic targets used today for improved lethality.
(II) NLRP2 is regulated by NFB and is shown to be expressed in MDA-MB-436 and MCF-7 (Bruey et al., 2004) breast cancer cell lines.
This particular gene appears in both independent and multitask regression.
Furthermore, the Gene Ontology annotation indicates that NLRP2 is in involved in caspase activities and apoptosis.
We hypothesized that strong G1 arrest and complementary negative correlation with cells being in S is the result of treatment with the therapeutic agent.
This particular gene is reflected in multitask regression and independent regression analysis corresponding to CI1040 and Iressa.
It is also a potential common mechanism of response for further analysis.
(III) CDKN2A (also known as p16) expression is positively associated with G1 arrest in normal cells and tissues, but is negatively associated with the S phase in our analysis of the human i103 [11:08 12/5/2010 Bioinformatics-btq181.tex] Page: i104 i97i105 K.Zhang et al.
Fig.11.
Interaction of CSTA with JUN and FOS curated through IPA.
breast tumor cell lines (in both the independent regression of Fig.9b and the multitask regression of Fig.10).
This discrepancy is likely explained by the fact that most of the malignant cell lines in the panel have aberrations in downstream effectors of the product of this gene.
The aberrations result in continued proliferation in the presence of p16 expression that ordinarily would yield cell cycle arrest and senescence (Gauthier et al., 2007).
(IV) CSTA is involved in apoptosis and differentiation, and is normally regulated by JUN and FOS (Takahashi et al., 1998), whose gene products together constitute the AP1 transcription factor.
AP1 drives the expression of a number of genes that are necessary for cell cycle progression.
The relationships between these proteinprotein interactions are shown in Figure 11.
This gene appears in multitask and one of the independent regression analysis.
(V) CA2 is an example of the gene that is reported by both independent association of gene expression data with CI1040 (Fig.9b) and the multitask regression analysis (Fig.10).
CA2 is ordinarily involved in differentiation and apoptosis, overexpressed in MCF7 and MDA-MB-231 and negatively correlated with the S phase in the drug-treated cells.
SiRNA-mediated interference with human CA2 gene expression has been shown to decrease survival of MDA-MB-231 cell lines (Mallory et al.
, 2005).
Finally, we performed an independent analysis by using Ingenuity Pathway Analysis and Pathway Studio, scientific software that helps researchers more effectively search, explore, visualize, and analyze biological and chemical findings related to genes, proteins and small molecules.
We selected the set of genes that was correlated with the S phase, and uploaded them into IPA and Pathway Studio.
The IPA analysis indicated that this group of genes is largely involved in (i) cell cycle and signaling networks and (ii) cancer.
The net result is a more substantial support for gene-by-gene analysis.
Similar results have been obtained from Pathway Studio, which provides gene set enrichment analysis (GSEA) and identifies common regulators with the user-defined number of neighbors.
Gene enrichment analysis revealed that predicted gene groups are involved in response to toxin, drug, negative regulation of cell proliferation, negative regulation of peptidase activity where S phase is one of them and apoptosis among top-ranked groups.
Furthermore, a number of common regulators with high P-values were also inferred that are associated with the cell cycle machinery.
Figure 12 shows three regulators of MAPK, Jun/Fos, and GF, and their target entities.
Fig.12.
Three common regulators that have been inferred from a subset of genes associated with the S phase.
In summary, multitask learning has the potential to summarize a vast amount of data, compute biologically relevant markers and identify hidden variables that traditional regressors may fail to capture.
Although the technique is currently applied for integration of gene expression data with cell cycle data, it can also be used for other integrative biology applications.
ACKNOWLEDGEMENTS The content of this publication does not necessarily reflect the views or policies of the Department of Health and Human Services, nor does mention of trade names, commercial products or organizations imply endorsement by the U.S. Government.
Funding: U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research (contract DE-AC02-05CH11231); the National Institutes of Health (grants U54 CA112970 and CA58207).
Conflict of Interest: none declared.
Abstract In the past decades, advances in high-throughput technologies have led to the generation of huge amounts of biological data that require analysis and interpretation.
Recently, nonnegative matrix factorization (NMF) has been introduced as an efficient way to reduce the complexity of data as well as to interpret them, and has been applied to various fields of biological research.
In this paper, we present CloudNMF, a distributed open-source implementation of NMF on a MapReduce framework.
Experimental evaluation demonstrated that CloudNMF is scalable and can be used to deal with huge amounts of data, which may enable various kinds of a high-through-put biological data analysis in the cloud.
CloudNMF is freely accessible at http://admis.fudan.
edu.cn/projects/CloudNMF.html.
Introduction The explosion of biological data brought about by the high-throughput technologies poses a great challenge to bioinfor-matics research.
In order to learn the hidden structures of these high-dimensional data, nonnegative matrix factorization (NMF) [1] was introduced into biological research.
NMF u S).
eijing Institute of Genomics, tics Society of China.
g by Elsevier jing Institute of Genomics, Chinese A was quickly applied to various fields of biological data analy-sis, such as capturing expression pattern in microarray data [2], discovery of cancer subtypes [3], clustering of gene expression data [4,5], identification of histone modification modules [6], biological text mining [7,8], etc.
The intrinsic nature of the NMF method makes it very suitable for an integrative analysis of multi-dimensional genomics data [9].
Devarajan presented a comprehensive review of the application of NMF to computa-tional biology [10].
With the increasing dimensionality of biological data, it is foreseeable that the application of NMF to biological research will continue to grow.
For example, sequencing technologies are generating terabytes (TBs) or even petabytes (PBs) of data for a multi-dimensional analysis.
However, current implemen-tations of NMF in the biology area can only deal with matrices of thousands-by-thousands size.
For example, bioNMF [11], cademy of Sciences and Genetics Society of China.
Production and hosting Table 1 Algorithm for CloudNMF Input: nonnegative matrix A, dimension k, iteration number i Output: nonnegative matrices W and H 1: initiate W and H using random nonnegative values 2: for each iteration: 3: calculate X1 =W TA using two MapReduce steps 4: calculate Y1 =W TWH using two MapReduce steps 5: update H with H =H.\X1/Y1 using one MapReduce step 6: calculate X2 = AH T using two MapReduce steps 7: calculate Y2 =WHH T using two MapReduce steps 8: update W with W=W.\X2/Y2 using one MapReduce step 9: output W and H Liao R et al/ CloudNMF: NMF for Large-scale Biological Data 49 an implementation of NMF for bioinformatics analysis, can only handle matrices of 4096 512 (according to the documen-tation of bioNMF server), and thus would fail to process data with more attributes or samples.
Another implementation using R [12] fails to work when data size reaches gigabytes (GBs) in a standalone machine.
In their original papers, both implementations were used to analyze a microarray dataset represented by a 5000 38 gene expression data matrix [13].
However, a much more scalable implementation will be needed to deal with data of a significantly greater size such as protein protein interaction (PPI) data or sequencing data.
To facilitate biological data analysis in the Big Data era [14], we present CloudNMF, an open-source implementation of NMF in MapReduce framework.
The implementation was developed on the Hadoop platform and can enable the nonnegative factorization of sparse matrices up to million-by-million size.
Furthermore, CloudNMF is provided as a JAR file ready to be deployed anywhere.
In particular, Cloud-NMF can be easily deployed on Amazon Elastic MapReduce to utilize the power of cloud computing for biological data analysis.
Methods NMF was first introduced by Lee and Seung as a method for learning the substructure of data matrix [1].
It was defined as the factorization of a nonnegative matrix A into the multipli-cation of two other nonnegative matrices W and H, where A is a m nmatrix,W andH are m k and k nmatrices, where Figure 1 Using CloudNMF w k is the target dimensionality to be reduced to, which is a num-ber smaller than the minimum of m and n. NMF was aimed at minimizing the Euclidian distance between A and WH, and can be used as an effective technique for dimension reduction and unsupervised clustering.
In 2010, Liu et al.
proposed an algorithm to perform NMF in the MapReduce framework [15] and showed that the algorithm can be used to factorize huge nonnegative matrices up to millions-by-millions size.
However, this algorithm was aimed at Web applications, and no source code of the algorithm is available for public use.
Our work is the first open-source implementation of NMF in the MapReduce framework, targeted at dealing with the explo-sion of biological data.
Our work follows the method previously reported [15], which is based on the well-known iterative updating rule of W and H described by Lee and Seung [16].
H H : W TA WTWH 1 W W : AH T WHHT 2 Here, .\ denotes dot product and T denotes transpose of matrix.
Similar to the method used in [15], for each iteration, the updating of H and W are both factorized into five MapReduce steps; the computation of each step can be easily distributed into multiple machines to achieve speedup, please see Table 1 for the details of the algorithm.
The program was implemented using Java and was pack-aged as a JAR file which can run on local Hadoop clusters (Figure 1).
We offered a command-line interface for the pro-gram; the usage of the command-line interface is also provided in our website (http://admis.fudan.edu.cn/projects/Cloud-NMF.html).
Moreover, Amazon Elastic MapReduce service (http://aws.amazon.com/cn/elasticmapreduce/) offers on-de-mand computing clusters preinstalled with Hadoop, and pro-vides a web interface to run Hadoop JAR files using only a web browser (see http://docs.aws.amazon.com/ElasticMapRe-duce/latest/DeveloperGuide/CLI_JobFlowUsingCustom-JAR.html).
For those inexperienced users who find it hard to build their own Hadoop clusters, it is possible to upload their data and CloudNMF into the cloud and perform their analysis remotely (Figure 2).
ith a local Hadoop cluster 50 Genomics Proteomics Bioinformatics 12 (2014) 4851 Experimental evaluation In order to test the performance of our program, we applied the program to both real data and simulated matrices.
The PPI data matrix from the STRING database [17] was used for performance testing, which includes 108,133,799 protein interactions from 1134 species.
The dataset can be represented by a 1,349,909 1,349,909 matrix, where 1,349,909 is the num-ber of distinct proteins in the dataset.
Since the interactions be-tween proteins are both nonnegative and sparse, the dataset is quite suitable for the application of NMF.
Based on the STRING dataset, three submatrices of differ-ent sizes were generated.
The four datasets are described in Table S1 and the performance of CloudNMF for these four datasets is summarized in Table S2.
We also generated three simulated matrices of different sizes but containing the same number of nonzero elements to test the impact of matrix size on the performance of the program (Table S3).
The experi-ments were performed on an 8-machine Hadoop cluster, and each machine has a Duo Core CPU and 4 GB memory.
From Figure 3 we observed a very interesting feature of CloudNMF: the runtime actually increases in proportion to the number of nonzero elements (the number of PPIs) in the matrix (Figure 3A).
This may be attributed to the MapReduce implementation of the algorithm: only nonzero elements are stored and distributed for computation.
As the size of the ma-Figure 2 Using CloudNMF w Figure 3 Performance of CloudNMF A.
Performance of CloudNMF on four real datasets shows the linea elements in the matrix.
B.
Performance of CloudNMF on simulated elements shows that the runtime per iteration is linear to the logarithm trix grows, the computation time increases logarithmically (Figure 3B).
These features make the algorithm better to deal with sparse nonnegative matrices in comparison with the tradi-tional implementations.
Discussion CloudNMF is the first open-source implementation of MapReduce-based nonnegative matrix factorization, and is capable of handling significantly a greater size of data than existing NMF implementations in bioinformatics.
Besides being deployed in local Hadoop clusters, CloudNMF can also be easily used on cloud computing platforms such as Amazon Web Services via only a web browser.
Moreover, experimental results show that the algorithm can effectively deal with sparse matrices such as proteinprotein interaction networks.
CloudNMF also has some limitations.
Although the pro-gram achieved considerable performance when dealing with large-size matrices, with the high overhead of MapReduce par-adigm, it may be less efficient than existing implementations to deal with small-size matrices.
In addition, while bioinformatics analyses using NMF may involve many pre-processing or post-processing steps, we only implemented the basic NMF algorithm.
However, the code of CloudNMF is freely accessi-ble at our website; users can integrate the code into their own pipelines to perform more specific analyses.
ith Amazon Web Services r correlation of runtime per iteration with a number of nonzero matrices of different sizes but with the same number of nonzero of matrix size.
Note that the X-axis is on a logarithmic scale.
Liao R et al/ CloudNMF: NMF for Large-scale Biological Data 51 To sum up, CloudNMF is the first open-source implemen-tation of a MapReduce-based NMF algorithm and can be easily used to process large amounts of data.
With the explo-sion of biological data and the wide application of NMF to biological research, we expect that CloudNMF will play more important roles in bioinformatics in the upcoming Big Data era.
Authors contributions Ruiqi Liao drafted the manuscript and developed the software.
Yifan Zhang participated in the software development.
Shuig-eng Zhou proposed the idea of the software and revised the manuscript.
Jihong Guan revised the manuscript.
All authors have read and approved the final manuscript.
Competing interests The authors have no competing interests to declare.
Acknowledgements This work is financially supported by National High Technol-ogy Research and Development Program of China (863 Pro-gram; Grant No.
2012AA020403) and National Natural Science Foundation of China (Grant Nos.
61173118 and 61272380).
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.
2013.06.001.
ABSTRACT Motivation: Our focus has been on detecting topological properties that are rare in real proteins, but occur more frequently in models generated by protein structure prediction methods such as Rosetta.
We previously created the Knotfind algorithm, successfully decreasing the frequency of knotted Rosetta models during CASP6.
We observed an additional class of knot-like loops that appeared to be equally un-protein-like and yet do not contain a mathematical knot.
These topological features are commonly referred to as slip-knots and are caused by the same mechanisms that result in knotted models.
Slip-knots are undetectable by the original Knotfind algorithm.
We have generalized our algorithm to detect them, and analyzed CASP6 models built using the Rosetta loop modeling method.
Results: After analyzing known protein structures in the PDB, we found that slip-knots do occur in certain proteins, but are rare and fall into a small number of specific classes.
Our group used this new Pokefind algorithm to distinguish between these rare real slip-knots and the numerous classes of slip-knots that we discovered in Rosetta models and models submitted by the various CASP7 servers.
The goal of this work is to improve future models created by protein structure prediction methods.
Both algorithms are able to detect un-protein-like features that current metrics such as GDT are unable to identify, so these topological filters can also be used as additional assessment tools.
Contact: firas@u.washington.edu 1 INTRODUCTION During the fifth Critical Assessment of Techniques for Protein Structure Prediction (CASP) experiment (Moult et al., 1995), a high frequency of occurrence of knots were observed for certain targets among the protein structure prediction models built using the Rosetta homology-based structure prediction method (Bradley et al., 2003; Rohl et al., 2004a).
This required a significant effort in manual inspection to discard those models containing knots (Rohl et al., 2004a), but this step was necessary as the assessors in CASP4 had deemed knotted models impossible structures (Tramontano et al., 2001).
This was the motivation behind Knotfind, a rapid algorithm for knot detection which our group implemented during CASP6 in the context of the Rosetta homology-based method (Khatib et al., 2006).
We were interested in finding topological properties that were common in Rosetta models, but rare in real proteins.
To whom correspondence should be addressed.
Present address: Department of Biochemistry, University of Washington, Seattle, WA 98195, USA Present address: Merck Research Laboratories, 33 Avenue Louis Pasteur, Boston, MA 02115, USA.
In the CASP6 experiment, the assessors reported that knotted models were still being submitted and that such knotted models submitted for comparative modeling targets were rejected out of hand without additional assessment (Tress et al., 2005).
Knots in polypeptide chains are often difficult to detect simply by visual inspection, as evidenced by the fact that the assessors still accepted several knotted CASP6 comparative modeling models, most likely because it was not visually apparent that these models contained knots (Khatib et al., 2006).
We noticed the same phenomenon for a similar protein topology that occurs in protein structure prediction models.
After the CASP6 experiment, while analyzing models generated by the automated Robetta server (Chivian et al., 2003; Kim et al., 2004), which utilizes the Rosetta method, our group noticed an interesting topology for Target T0199.
By visual inspection, it would seem as though Robettas model 1 is knotted (Fig.1).
Following the orange region of the chain towards the red terminus in the backbone ribbon diagram, it seems as if the chain wraps itself around the cyan region, behind the blue region and next to the yellow region.
If one were to increase the tension in this protein chain, as described in the Knotfind algorithm (Khatib et al., 2006), it would correctly simplify the chain to a straight line, denoting an un-knotted chain.
This Robetta model does not contain a knot; it has what is more commonly known as a slip-knot.
Like untying shoelaces, which are commonly considered to be knotted, a slip-knot will simplify to a straight line if one pulls both ends of the chain.
It is very difficult to detect a slip-knot, and the Knotfind algorithm will simply report the polypeptide chain to be knot-free.
After noticing this particular case in CASP6, we set out to create a new algorithm that would detect this complex un-protein-like topology.
Although the Robetta model does not contain a mathematical knot, by visual inspection one can tell that its fold is not protein-like.
As seen in Figure 1, it seems as though the cyan region pokes through a small loop in the orange region.
Our goal was to be able to detect this computationally, since servers such as Robetta have no human intervention.
The model in Figure 2 is also from T0199, but is one predicted by the Rohl group (Group 079) at UCSC, also using the Rosetta homology-based method.
Although the chain does not contain a knot, it does seem that the red loop and cyan loop thread through one another.
Just looking at these two interconnecting loops led us to believe that it would be topologically unfavorable to pass a segment of the chain through such a small red loop and likewise wrapping such a red loop around another segment of the chain would also be unfavorable and thus un-protein-like.
In the Rosetta modeling process, however, such loops have no problem wrapping around one another or poking through small loops in the chain.
We refer to these slip-knots as pokes and set out to create a Pokefind algorithm that would be able to detect such topologically 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:56 15/5/2009 Bioinformatics-btp198.tex] Page: i282 i281i288 F.Khatib et al.
Fig.1.
A backbone ribbon diagram of Robettas model 1 for CASP6 target T0199.
Following the orange region to the red region, the chain seems to wrap around the cyan region, behind the blue and next to the yellow region.
It seems as if it would become tangled into a knot if one were to pull both ends of this chain.
That is not the case, however, because if the red and blue ends are pulled apart then the chain simplifies to a straight line.
This is commonly referred to as a slip-knot.
Fig.2.
One of the CASP6 decoys for target T0199 generated by the Rohl group using Rosetta.
This protein chain does not contain a knot even though the red and cyan regions thread through one another.
This chain is unknotted because the red and cyan regions are connected by the green region.
Increasing the tension between both terminal ends of the protein chain causes the white helix (which lies between the red and green regions) to simplify away.
This results in the red/green/cyan region easily simplifying to a straight line.
unfavorable conformations in our Rosetta models, referred to as decoys.
The goal of this work is to improve future models created by the protein structure prediction community.
Pokefind is an attempt to capture another topological property that distinguishes decoys from real protein structures.
Fig.3.
The longest closed loop found in CASP6 Rosetta decoys containing a poke we want Pokefind to report.
The closed loop of length 32 (shown in red) is being poked by the cyan segment of the chain.
This is the exact kind of poke that the Pokefind algorithm was designed for, because this chain does not contain a knot.
2 METHODS 2.1 Closed loops In 2003, Trifonov and Berezovsky (2003) reported that analysis of the closed loops in crystallized protein structures reveals that the contour length of 2050 residues is dominant, with the majority of 2530 residues.Trifonov et al.
cited a paper by Berezovsky et al.
from 2000 where for closed loops with ends within 7 from one another they reported how often they had observed different loop lengths.
The Pokefind algorithm defines these closed loops as any segment of the protein backbone where the ends are within 7.0 from one another and the ends span between 3 and 33 C atoms.
Any part of the remaining C trace of the chain that pokes through this closed loop is deemed to be a poke.
The maximum closed loop length of 33 residues was chosen so as not to detect entire domains that might poke through a large closed loop.
After examining our CASP6 Rosetta dataset (see Section 2.3), 32 residues was the longest closed loop containing what we believed to be an un-protein-like poke (Fig.3).
For the Pokefind algorithm, we wanted a method that would generate a surface out of the closed loop and detect anything that poked through this surface.
If a closed loop is non-planar, forming a U-shape for example, it would only make a surface of the actual U-shaped closed loop and not a surface that includes the entire space that the bent closed loop occupies.
Our solution to this problem was to break up this surface into as many small triangles as possible, and then inspect those triangles for pokes.
2.2 Poke-detection algorithm Pokefind begins by searching for the two closest C atoms in a given closed loop and connecting them.
Next it splits up the closed loop into two different areas based on this connection and runs again the exact same way on both sections.
It does this recursively until a section is left with only three points.
Once that occurs, it checks that triangle for any pokes by detecting if any line segments that lie outside the closed loop intersect the triangle.
Any line segment that pokes through the triangle is reported and Pokefind continues to solve the remaining sections (Fig.4).
i282 [09:56 15/5/2009 Bioinformatics-btp198.tex] Page: i283 i281i288 Pokefind: a novel topological filter for use with protein structure prediction Fig.4.
Example of the Pokefind algorithm.
A closed loop is shown in black, with C residues as red dots.
The two large red C residues are within 7 from one another, defining the closed loop.
Pokefind breaks up this closed loop into smaller sections, shown by the blue lines.
It then checks all these triangles to see if any line segments that lie outside the closed loop are poking through them.
Even if the turn at the top and the turn at the lower right are not in the same plane as the rest of the closed loop, Pokefind will still be able to correctly distinguish between pokes going through these turns and non-pokes that go through the concavity of a turn.
Pokefind is able to divide any closed loop into many smaller regions and analyze each region separately.
This is useful when the closed loop has many different topologies since Pokefind will not be looking at line segments that may poke the global fold of the closed loop, but rather will detect more local pokes within a single closed loop that may not be planar.
By triangulating the surface of the closed loop, this method will not report segments that poke through the concavity of a closed loop.
2.3 Pokefind training set The Rosetta decoy sets built during CASP6, including models from the Robetta server, use the Rosetta homology-based structure prediction method (Bradley et al., 2003; Chivian et al., 2005).
Predictions begin from an alignment to a parent protein of known structure and coordinates for the aligned regions are taken directly from the parent structure and serve as a fixed template.
Coordinates for structurally variable regions (SVRs), corresponding to both gaps in the alignment as well as regions of uncertain alignment, are constructed by assembling short fragments of known structure.
The selected fragments are combined using a Monte Carlo simulated annealing search by means of a knowledge-based potential function derived from the observed distributions of residues in known protein structure along with a gap penalty to ensure chain continuity in the final model (Rohl et al., 2004a, b).
The PISCES server was used to identify 9553 protein chains in the RCSB PDB (Berman et al., 2000) with less than 90% sequence identity, with X-ray structures of resolution better than 3.0 and no R-factor filtering (R 1.0) (Wang et al., 2003).
Running the Pokefind algorithm on both this PDB set and on our CASP6 Rosetta decoy set resulted in many pokes being reported in real proteins: 5543.
The most common types of pokes found in real proteins occur near the ends of the closed loop and barely poke through it (Fig.5).
Similar to a knotted protein chain that becomes unknotted if a few residues are trimmed from each end; if a poke occurs within a few residues of either end of the closed loop, we are less interested in it than a poke which is further down the protein chain.
We also noticed that shorter closed loops often had pokes in the decoys, but rarely in real proteins.
In the rare cases where real proteins with short Fig.5.
Example of the most commonly seen type of poke, taken from 1a8s.
The closed loop (shown in red) is being poked by a segment of the chain (in cyan) adjacent to one of the ends of the closed loop.
This poke barely punctures the plane created by the closed loop.
These are not the types of pokes that we are interested in reporting since they are very common in real proteins.
Fig.6.
Examples of real proteins with short closed loops containing pokes.
Two closed loops of length 14 are shown in red.
The poke on the left occurs in the real protein 1g8lA, where the middle strand (shown in green) pokes through the closed loop, between the two red strands.
The poke on the right is from the real protein 2viu, where the cyan strand pokes through the red closed loop.
In both of the real proteins, these short closed loops are being poked by strands that form a sheet with the closed loop.
closed loops contained pokes, most of these all had a similar topology.
The poke in the short closed loop would be the result of a beta sheet forming with a middle strand poking though the closed loop formed by the two outer strands (Fig.6).
While examining these rare pokes in real proteins, it became apparent that these occurrences are even more infrequent in Rosetta decoys.
Further inspection of pokes in real proteins revealed that these beta sheet topologies not only occur with short closed loops, but also with longer ones.
Figure 7 shows two different real proteins, with different closed loop lengths, that have strands poking through them.
The protein on the left, 1cex, has one green strand poking through the red closed loop.
2aqj, on the right, has two strands that form a poke; the green strand pierces through the red closed loop in one direction and pokes through in the opposite direction a few residues later with the blue strand.
These strands all form a beta sheet with the closed loops that they are poking, whereas this does not occur as often with pokes found in Rosetta decoys (see Section 2.5).
2.4 Sheet filter We implemented a sheet filter to detect and ignore most of the pokes that were found in real proteins.
This sheet filter uses the transitive property of strands to group them all together.
If two strands form a sheet with one very i283 [09:56 15/5/2009 Bioinformatics-btp198.tex] Page: i284 i281i288 F.Khatib et al.
Fig.7.
Examples of real proteins with closed loops being poked by strands.
Two real proteins, 1cex (on the left) and 2aqj (on the right) have closed loops that are poked by strands.
The closed loops are shown in red with the strands poking in green and blue.
These strands form a beta sheet with the closed loop that they poke through.
long strand which forms a sheet with three strands on the other side, then all six of these strands are considered to be in the same group.
The sheet filter then checks if a poke is in the same sheet group as any residue in the closed loop and filters the poke out if it is, so as to not report it as an un-protein-like poke.
The Undertaker program was used to establish whether a hydrogen bond exists between atoms to determine if a sheet is present (Karplus et al., 2005).
The detailed methods of how Undertaker models hydrogen bond geometry are explained in Archie and Karplus (2008).
The sheet filter was able to throw away 3977 of the 5543 pokes in real proteins.
One promising aspect is that it conveniently filters out all pokes that occur in knotted proteins.
None of the remaining 1566 pokes in real proteins are from proteins that contain knots.
This makes the sheet filter even more effective, because all knots that are found in real proteins will never be reported by Pokefind.
Pokefind will still report severely knotted models, but the types of knots that have been found in nature so far will not be reported.
Therefore, if a model is using a knotted region of a real protein as a template, Pokefind will not incorrectly classify that region as having a poke or as being un-protein-like; it will simply be ignored.
Of the 137 057 pokes found in 58 498 CASP6 Rosetta decoys, 27 548 pokes were filtered out by the sheet filter.
Although it may seem as though 20% of all pokes in Rosetta decoys are being filtered out incorrectlythat these 27 548 cases are all false negativesthis is not the case.
All Rosetta decoys created at UCSC during CASP6 were built using templates of known protein structures, so many of these sheet pokes in Rosetta decoys are actually template regions that are copied directly from real proteins.
27 327 of the 27 548 decoy pokes that were filtered out by the sheet filter occurred in template regions and of the remaining 221 sheet pokes that occurred solely in SVRs, none of those decoys had pokes that were 15 or more residues away from the closest end of the closed loop.
2.5 Co-pokes While examining Rosetta decoys having two conflicting SVRs, such as the decoy in Figure 3, we noticed another problematic topology that could easily be identified.
If two closed loops become intertwined and poke one another, then they can be classified as bad pokes.
This means that if a closed loop has a poke and that poke is part of another closed loop which is being poked by the initial closed loop, then both closed loops are poking one another resulting in what we call a co-poke (Fig.8).
After running the sheet filter, to ignore all the pokes that form beta sheets with the closed loops they are poking, we ran a co-poke identifier to classify two closed loops that poke one another as bad pokes.
Of the 137 057 pokes found in 58 498 CASP6 Rosetta decoys, 24 551 of these pokes had co-pokes.
With this simple identifier we are able to classify 18% of all decoys pokes as definitively having an incorrect topology, without any additional assessment.
Just as there are rare cases of deeply knotted proteins, our co-poke identifier discovered 37 co-pokes in real proteins.
That translates to only 0.67% of all real pokes being co-pokes.
This very low co-poke rate in real Fig.8.
Example of a co-poke.
This CASP6 Rosetta decoy contains two closed loops (shown in red and blue) that thread through one another.
The red closed loop is being poked by the blue loop and the blue closed loop is being poked by the red loop.
We have defined this topological feature of two closed loops poking one another as co-pokes.
proteins was very exciting, since the corresponding rate for co-pokes in Rosetta decoys was 18%.
Using these results, we looked at the ratio between the pokes per model in the decoys compared to the pokes per model in the reals as a measure of how un-protein-like a poke is.
In Figure 9 this ratio of pokes per model is shown at each step in the filtering process.
Initially, after running the Pokefind algorithm, the ratio of decoy pokes per model to real pokes per model is 4.04.
After implementing the sheet filter, that ratio increases almost three fold to 11.42.
The ratio for pokes thrown out by the sheet filter is very low at 1.13, whereas the ratio for co-pokes is 108.36, demonstrating how un-protein-like co-pokes are since they are observed at a much higher rate in decoys than in real proteins.
2.6 Assigning costs to different pokes We plotted the exact separations for pokes that were 29 residues or less away from their closed loops.
For example, if line segment 4546 pokes through a closed loop spanning residues 2040, the separation between the poke and the closest closed loop end would be five.
If line segment 1920 poked the same closed loop, it would have a separation of zero.
Figure 10 shows that due to very few data points in the reals, the ratios of decoy pokes per model to real pokes per model range from 3.55 to 192 when looking at the exact separation between a poke and the closed loop it is poking.
In order to smooth out the values in Figure 10 as much as possible, we manually grouped individual separations into bins.
Figure 11 shows a histogram of the smoothest manual binning.
For example, exact separations of zero, one and two are assigned an average ratio of 4.88, and separations of 29 or more are all given a ratio of 97.66 (including higher separation values not shown on the graph).
Using the ratios in Figure 11, we can assign how much worse a poke is the further it is from the closest end of the closed loop.
Energy is usually presented as a negative log probability, so to get an energy-like cost function we take the log of the sum of the ratios to assign each poke a cost.
For example, a decoy with a poke that is adjacent to the closed loop it is poking will have a cost of 0.69 (the log of 4.88) whereas a decoy with a poke that is 153 residues away from the closest end of its closed loop will have a cost of 2.78 (the log of 97.66).
A decoy containing both these pokes will have a cost of 2.01 (the log of 97.66 + 4.88).
Co-pokes had a ratio of 108.36, therefore any co-poke will be given a cost of 2.03.
Since pokes that were thrown out by the sheet filter had a ratio of 1.13, we i284 [09:56 15/5/2009 Bioinformatics-btp198.tex] Page: i285 i281i288 Pokefind: a novel topological filter for use with protein structure prediction Fig.9.
Chart showing how many pokes per model are separated out by the sheet filter (first arrow in red) and the co-poke identifier (bottom arrow in blue).
Numbers in black indicate the total number of pokes reported at each step, divided by the total number of models examined by Pokefind.
The numbers in red represent the ratio between the pokes per model in the decoys compared to the pokes per model in the reals.
Not all the pokes removed from decoys by the sheet filter are false negativesonly 0.0038 pokes per model are.
The remaining pokes were present in the templates used by the decoys (see Section 2.4).
Fig.10.
Graph plotting the ratio of decoy pokes per model to real pokes per model at exact poke to closed loop separation cutoffs.
The x-axis shows the exact separation between a poke and the closed loop it is poking for all pokes within 29 residues of a closed loop.
A separation of 10, for example, indicates that the poke is exactly 10 residues from the closest end of the closed loop it is poking.
The y-axis shows the ratio between the pokes per model in the decoys compared to the pokes per model in the reals.
Due to the small data set of pokes in real proteins, these ratios vary highly from one separation value to the next.
This led to a manually smoothed version of the graph, shown in Figure 11. assign these pokes a cost of 0.053, to differentiate them from completely poke-free proteins.
We use the log of the sum of the ratios rather than the sum of the logs because a decoy with one very bad poke, such as a co-poke, is much worse than many pokes that are adjacent to the end of a closed loop.
For example, a decoy with five pokes that are adjacent to the ends of the closed loops that they are poking is given a poke cost of 1.39, which is less than 2.03 (the poke Fig.11.
Histogram showing the ratio of decoy pokes per model to real pokes per model at various binned poke to closed loop separation cutoffs.
Using the data points from Figure 10, we manually grouped individual separations into bins and calculated the average ratios for those values.
This histogram represents the smoothest binning we were able to produce.
All separations of 29 and higher, including those not shown on this graph, are all given the same ratio value of 97.66. cost for a decoy containing a co-poke), whereas the decoy with five pokes would have a cost of 3.45 had we summed the log scores.
By assigning these various pokes different costs, we are reporting how un-protein-like each poke is.
Based on our analysis of CASP6 Rosetta decoys and our PDB set of real proteins, co-pokes are extremely rare in real proteins making up only 0.67% of all the real pokes, whereas 18% of all decoy pokes were co-pokes.
This difference indicates how un-protein-like co-pokes are, which is why we assign them the highest cost, compared to pokes that occur near the ends of a closed loop.
It is important to note that all our observations of pokes in decoy sets have been solely based on models built by our Rosetta group at UCSC during CASP6.
The decoy training set for all the Pokefind work was only the CASP6 Rosetta models built at UCSC, so we needed a completely independent decoy set to use as our test set.
It would not be sufficient to simply simulate another CASP6 experiment using the Pokefind algorithm, because the template regions would be the same and all our poke analysis was done on those exact same templates.
3 RESULTS 3.1 Pokefind test set We carefully investigated pokes in our CASP6 decoy training set and in real proteins to come up with a metric for how un-protein-like a given poke is.
To be certain that the poke costs we had assigned were adequate, we required a decoy test set that was completely unrelated from our decoy training set.
We selected two different datasets from CASP7 to use as a decoy test set.
We examined the models submitted by the various servers at CASP7containing predictions from 93 different structure prediction methodsas an independent decoy test set.
In addition to these 11 071 server models, we also looked at the Rosetta decoys built at the University of Washington, in the Baker Lab, during CASP7.
This Rosetta test set contained all 16 392 low energy decoys built for CASP7 using Rosettas loop modeling protocol and one round of full-atom relax.
Prior to running Pokefind on this CASP7 decoy test set, we ran it on the CASP7 solutions.
Two of the CASP7 targets contained knots in the solved structures, so we needed to determine whether any of i285 [09:56 15/5/2009 Bioinformatics-btp198.tex] Page: i286 i281i288 F.Khatib et al.
Fig.12.
Example of a CASP7 Rosetta decoy containing a co-poke.
This decoy for CASP7 target T0316 was built using Rosettas loop modeling protocol.
The red closed loop pokes the blue closed loop, while the blue closed loop pokes the red closed loop, resulting in a co-poke.
These closed loops are 169 residues apart from one another, denoted by the green region.
the solutions contained bad pokes.
Our CASP7 Rosetta decoy test set consisted of models built using Rosettas loop modeling protocol, therefore these were all decoys from the Template Based Modeling (TBM) category at CASP7, and we only included CASP7 server models for the exact same targets.
When running Pokefind on the solutions, we only looked at the solved structures from the same TBM category, ignoring the four targets that had not been solved: T0320, T0333, T0355 and T0386.
Most of the solved structures, 29 out of 39, had no pokes whatsoever.
Four of the 10 remaining proteins had pokes that were filtered out by the sheet filter; this included the two knotted CASP7 targets: T0332 and T0378.
Five of the remaining six proteins containing pokes that were within three residues of the end of the closed loop, and one solved structure had a poke 15 residues from the end of its closed loop.
None of the 39 real proteins contained co-pokes, but the same was not true for our CASP7 decoys test set.
An example of a CASP7 Rosetta decoy containing a co-poke is shown in Figure 12.
This prediction for target T0316 had a low Rosetta energy score of 28.63 despite the fact that this co-poke is very un-protein-like; the two closed loops that thread through one another (shown in red and blue in Fig.12) are 169 residues apart.
3.2 CASP7 results The histogram in Figure 13 shows the results of running the Pokefind algorithm on our CASP7 Rosetta test set, as well as the corresponding CASP7 server models test set and solved structures.
The red line represents all the CASP7 decoys built using the Rosetta loop modeling protocol, for which the corresponding target was successfully solved, with the associated total poke cost per decoy.
We wanted to look beyond Rosetta and ran Pokefind on the other methods that were used at CASP7.
The green line denotes all the CASP7 server predictions for the same solved targets, after filtering out models that contained missing density.
If server predictions with missing density are included, the histogram values for the green line are even higher.
The majority of decoys in our test set either did not contain any pokes (poke cost of zero) or contained pokes that were Fig.13.
Histogram showing the results of running Pokefind on CASP7 decoys.
The red line represents all CASP7 decoys built using Rosettas loop modeling protocol, for which there was a solved structure, with the associated total poke cost for each decoy.
The green line denotes all the template-based models with no missing atoms from CASP7 servers.
The blue crosses show the actual poke costs for the solved structures in the template based modeling category at CASP7, implying that any decoy with a higher poke cost contains an un-protein-like topological feature.
filtered out by the sheet filter (poke cost of 0.053), but there were still many decoys in the test set having higher poke costs than the actual solved structures.
None of the solved structures in the TBM CASP7 category had co-pokes, yet our test set of CASP7 Rosetta decoys contained 6335 co-pokes.
Clearly, these un-protein-like features were still being created by Rosettas loop modeling method, since there are many CASP7 Rosetta decoys with poke costs higher than those of the corresponding solved structures.
The blue crosses in Figure 13 show the actual poke costs for the solved structures in the TBM category at CASP7.
These results imply that any decoy with a poke cost greater than that of the rightmost blue cross in the figure contains an un-protein-like topological feature, since such pokes do not exist in the corresponding solved structures.
The histogram in Figure 13 shows that there are many files in our Rosetta test set that have a poke cost higher than the solved CASP7 targets, but this problem is not exclusive to Rosetta.
The results are similar for the CASP7 server predictions as well, shown in green in Figure 13.
There were 11 071 CASP7 server predictions, but only 5231 of them did not have any missing density and are shown in the histogram.
The green and red lines are similar, despite the fact that the Rosetta decoy set contained 16 392 files, 11 161 more than the server models.
Even with only 5231 predictions, there were more CASP7 server models with poke scores between 2.1 and 5.6 than Rosetta decoys.
An example of a server prediction with un-protein-like features, but no missing density, is shown in Figure 14.
This model was submitted as model 2 for target T0364 by the 3Dpro server and has a GDT_TS score of 76.701, which is the highest GDT_TS score for all submitted models for this target, including human predictions.
The solved structure for target T0364 does not contain any pokes, not even a poke that would be discarded by the sheet filter, but Figure 14 shows that this highest-ranked prediction contains a co-poke.
i286 [09:56 15/5/2009 Bioinformatics-btp198.tex] Page: i287 i281i288 Pokefind: a novel topological filter for use with protein structure prediction Fig.14.
Example of a high-ranking CASP7 server prediction containing unprotein-like pokes.
This prediction for CASP7 target T0364 by the 3Dpro server, submitted as model 2, has a GDT_TS score of 76.701, the highest GDT_TS score for all human and server submissions.
This unknotted prediction contains a co-poke, the blue and red closed loops thread through one another, showing that these unprotein-like features are not exclusive to the Rosetta method.
The solved structure for this target does not contain any pokes.
The blue closed loop is being threaded by the red segment of the chain while the red closed loop is being poked by the blue segment of the chain.
By visual inspection it looks as if this chain contains a knot, but that is not the case and Knotfind correctly reports this model to be unknotted.
3.3 Topological filters can detect what other metrics have not The particular example in Figure 14 highlights the need for topological filters, especially in the case of servers where there is no human intervention.
Just as Robetta was submitting knotted models in CASP6, many servers submitted models containing bad pokes in CASP7.
Even though certain models might obviously be un-protein-like by visual inspection, such as the prediction in Figure 14, servers have no human interference to detect such a feature.
Algorithms like Knotfind and Pokefind can be useful to discriminate between various models generated by servers.
Topological filters such as Knotfind and Pokefind are also useful additions to the standard metrics that are currently used to evaluate how protein-like a prediction is.
There is the example from CASP4, where a decent scoring model, with reasonable GDT and AL0 values, was deemed an impossible structure by the CASP4 assessors because it contained a trefoil knot (Tramontano et al., 2001).
This particular knot was identified by visual inspection, but many other submitted knotted models went unnoticed in CASP6 (Khatib et al., 2006).
Knotfind and Pokefind could be used by assessors in addition to the other metrics that are currently used.
When using Rosetta to evaluate the 9553 PDB chains taken from the PISCES server, incorporating the Pokefind algorithm added 571 seconds to the overall run time of 45 minutes on an Intel(R) Xeon(TM) CPU 2.80 GHz, compared to evaluating the chains using Rosetta without Pokefind.
The Knotfind algorithm is even faster, adding only 90 seconds to the same 45 minutes overall run time.
In CASP7, the assessors added a metric for hydrogen bond conservation, HBscore, in order to assess local atomic interactions (Kopp et al., 2007).
For the TBM CASP7 category, they combined GDT, AL0 and HBscore to determine which groups had submitted the best predictions.
The assessors showed examples of models with good GDT scores but low HB scores, compared to models with lower GDT scores and higher HB scores.
A model that has a decent GDT score, yet does not resemble a protein, is not a useful prediction.
This shows the importance of using different metrics that are not correlated with one another.
GDT and AL0 are highly correlated already, so combining useful uncorrelated metrics, such as HBscore, will help assessors in the future.
The average correlations with GDT, using Kendalls Tau, was 0.070 with Pokefind and 0.010 with Knotfind across all CASP7 server targets with no missing density.
Since both the Knotfind and Pokefind algorithms are uncorrelated with GDT, they could be useful additional metrics, just as HBscore was in CASP7, to detect un-protein-like features that GDT cannot identify.
Neither Rosettas energy function nor Undertakers cost function are correlated with Pokefind (unpublished data), which may explain why so many poked models are submitted to CASP.
If current protein structure prediction algorithms have no penalty for pokes, and the current metrics used by assessors do not penalize these un-protein-like features, then poked predictions will continue to be submitted.
This is especially true for server predictions, where there is no human intervention.
Protein structure prediction is regarded as one of the hardest problems in biology today.
We have introduced two novel algorithms that can be applied to structure prediction methods and can be used for assessment of predictions.
Both Knotfind and Pokefind are able to detect topological features that current metrics are unable to discover.
Implementing both these algorithms as metrics in future CASP assessments would force prediction methods to avoid creating these un-protein-like features in their models.
Removing these un-protein-like features will hopefully result in better models produced by the protein structure prediction community.
ACKNOWLEDGEMENTS We thank Josue Samayoa, David Bernick and Craig Lowe for the decoy structures predicted for CASP6, and Bin Qian and David Baker for CASP7 decoy sets.
Conflict of Interest: none declared.
Abstract The asparagine-X-serine/threonine (NXS/T) motif, where X is any amino acid except proline, is the consensus motif for N-linked glycosylation.
Significant numbers of high-resolution crystal structures of glycosylated proteins allow us to carry out structural analysis of the N-linked glycosylation sites (NGS).
Our analysis shows that there is enough structural information from diverse glycoproteins to allow the development of rules which can be used to predict NGS.
A Python-based tool was developed to investigate asparagines implicated in N-glycosylation in five species: Homo sapiens, Mus musculus, Drosophila melanogaster, Arabidopsis thaliana and Saccharo-myces cerevisiae.
Our analysis shows that 78% of all asparagines of NXS/T motif involved in N-gly-cosylation are localized in the loop/turn conformation in the human proteome.
Similar distribution was revealed for all the other species examined.
Comparative analysis of the occurrence of NXS/T motifs not known to be glycosylated and their reverse sequence (S/TXN) shows a similar distribu-tion across the secondary structural elements, indicating that the NXS/T motif in itself is not bio-logically relevant.
Based on our analysis, we have defined rules to determine NGS.
Using machine learning methods based on these rules we can predict with 93% accuracy if a particular site will be glycosylated.
If structural information is not available the tool uses structural prediction results resulting in 74% accuracy.
The tool was used to identify glycosylation sites in 108 human proteins with structures and 2247 proteins without structures that have acquired NXS/T site/s due to mder R).
eijing Institute of Genomics, tics Society of China.
g by Elsevier jing Institute of Genomics, Chinese Academy of Sciences and Genetics Society of China.
Production and hosting mailto:mazumder@gwu.eduLam PVN et al/ N-linked Glycosylation Sites 97 non-synonymous variation.
The tool, Structure Feature Analysis Tool (SFAT), is freely available to the public at http://hive.biochemistry.gwu.edu/tools/sfat.
Introduction Co-and post-translational modifications (PTMs) modify the function of proteins by the addition of specific chemical groups that affect their thermodynamic, kinetic and structural proper-ties.
Glycosylation, one of the many types of PTMs, contrib-utes to the diversification of proteins by the addition of structurally-diverse oligosaccharides.
This modification is widespread and involved in a wide variety of biochemical and cellular processes including protein folding, maintenance of cell structure, receptor-ligand interaction, cell signaling and cell-cell recognition [13].
The function of glycosylation in health and disease attracts significant attention with recent reports on the effects of non-synonymous variations on glyco-sylation [4], study of glycosylation in cellular pathophysiology [5], pharmacological significance of glycosylation in therapeu-tic proteins [6], the significance of glycosylation in the develop-ment of biopharmaceuticals [7] and carbohydrate-based vaccines [8].
N-linked glycosylation (NGS) occurs as a post-transla-tional modification and a co-translational process through which carbohydrates (glycans) are added to an asparagine (N) at the consensus motif asparagine-X-serine/threonine (NXS/T) in which X is any amino acid except proline [9].
There are reports of other NGS motifs such as asparagine-X-cysteine (NXC), but their frequency of occurrence is extre-mely low [10,11].
The attachment of the glycan is assisted by a hydrogen bond between the b-amide of asparagine as the hydrogen bond donor and the oxygen of threonine (serine) [12].
This process is catalyzed by the enzymatic action of N-glycosyltransferases which attach glycan to the unfolded pro-tein during protein synthesis [1].
It has been suggested that NGS may contribute to the correct folding of proteins; exper-imental evidence shows that interactions between the sugars and the amino acids in the native state stabilizes the folding of glycoproteins [13].
It has been concluded that the primary structure of the NXS/T tri-peptide is necessary, but not suffi-cient, for glycosylation [10].
The most probable explanation for this observation is that in addition to other factors such as the localization of the protein, the adoption of an appropri-ate conformation and solvent accessibility of this tri-peptide is required for the glycosylation reaction [14,15].
Studies by Beeley [16] and later by Bause et al.
[17] demon-strated a statistical probability for glycosylated asparagine res-idues to be located within a turn/loop conformation.
Availability of complete genomes, sensitive mass spectrometric tools, and bioinformatic methods has resulted in recent confir-mation of these findings in many eukaryotes [10,11].
The authors show that eukaryotic N-glycoproteins have invariant sequence recognition patterns, structural constraints and sub-cellular localization.
Their analysis suggests that a large num-ber of N-glycoproteins evolved after the split between fungi, plants and animals to support organismal development, body growth and organ formation specific to the corresponding clade [11].
It has been shown by Park and Zhang [18] in a comparative genomic study involving higher eukaryotes that the glycosylated asparagines evolve more slowly than the non-glycosylated counterparts in the same set of proteins.
The authors conclude that the solvent-accessible asparagines are most likely to be glycosylated and of biological importance [18].
A continued improvement of rule-based filters that pre-dict occupancy of the large number of N-glycosylation sequons is therefore important.
In this study, we performed a comprehensive structural analysis of potential N-linked glycosylation sites in Homo sapi-ens (human), Mus musculus (mouse), Saccharomyces cerevisiae (yeast), Drosophila melanogaster (fly) and Arabidopsis thaliana (plant) to refine the structural constrains of N-glycosylation with the aim to formulate basic rules improving prediction accuracy.
We then used these rules to predict N-glycosylation of NXS/T sequons created in the human genome by non-syn-onymous single nucleotide variation (nsSNV).
These rules were incorporated into an N-linked glycosylation prediction tool: Sequence Structure Feature Analysis Tool (SFAT).
Our analysis shows that current structural information is sufficient to develop such rules that are applicable to the entire prote-ome.
Such analyses can be used to prioritize targets for further validation in the laboratory.
Results and discussion Structural analysis of annotated and unannotated NXS/T motif The occurrence of the N-linked glycosylation sequence motif is not sufficient to determine if a particular site will get glycosyl-ated.
To better understand and describe the sequence and structural parameters that allow a specific site to be glycosyl-ated, and to see if these can be applied across evolutionarily distant organisms, we have performed a comprehensive analy-sis of the five following eukaryotic proteomes: human, mouse, fly, plant and yeast.
Table 1 provides details of the data sets used in this study.
Distribution of protein secondary structure elements in eukaryotes To understand the distribution of NGSs (annotated in Uni-ProtKB/Swiss/Prot) and unannotated NXS/T motifs, we have determined the distribution ofa-helix, b-sheet and loop/turn ele-ments in all the non-redundant protein structures.
The percent-age of amino acids in these structural elements was calculated for individual proteins and the percentage of a-helix, b-sheet and loop/turn conformations for the all the proteins with struc-tureswas then calculated.
The results show that the distributions of the three structural elements in all five species are very similar with a-helix being the highest and b-Sheet the lowest secondary structure conformation (Figure 1A).
More specifically, the fre-quency of a-helix, b-sheet and loop/turn conformation varies in the organisms studied, which is 3847%, 2128% and 3133%, respectively.
If asparagine is distributed evenly among all secondary structure elements, then one should expect to ob-serve similar frequencies of occurrences of the amino acid in the three secondary structure elements.
But this is not true as can be seen from the next analysis results.
Table 1 Structure datasets used in this study Organism Available structuresa No.
of annotated NXS/T sites No.
of unannotated NXS/T sites No.
of N sites Total length Sheet total length Helix total length Loop/turn total length Human 3094 2284 3779 30,762 1,627,531 377,793 713,587 536,151 Mouse 644 453 739 5984 91,718 31,568 24,182 35,968 Fly 103 42 103 1029 37,216 12,622 16,435 37,216 Plant 179 33 223 1834 136,158 30,062 62,978 43,118 Yeast 756 10 1163 16,745 191,581 41,428 87,412 62,741 Note: aStructures that have at least one asparagine in their sequence.
A A sp ar ag in e di st ri bu ti on (% ) B D is tr ib ut io n of s ec on da ry s tr uc tu re ( % ) P < 0.0001 0 10 20 30 40 50 60 70 80 90 100 Human Mouse Fly Plant Yeast-Helix-Sheet Loop/turn 0 10 20 30 40 50 60 70 80 90 100 Human Mouse Fly Plant Yeast-Helix-Sheet Loop/turn Figure 1 The distribution of secondary structure elements and asparagine A.
Distribution of secondary structural elements in proteins of human, mouse, fly, plant and yeast.
B.
Distribution of asparagine in secondary structural elements in proteins of human, mouse, fly, plant and yeast proteins.
P values are calculated with v2 test by comparing the occurrence of asparagine in secondary structural elements to the overall distribution of a-helix, b-sheet and turns/loops in all available structures in the species of interest.
98 Genomics Proteomics Bioinformatics 11 (2013) 96104 Distribution of asparagines in protein secondary structural elements It has been shown that NGS are more prevalent in turns [14,16].
This observation would not have functional implica-tions if the abundance of asparagines (N) in turns is similar to the abundance of NGS sites in turns.
There are 30,762 N-containing sites in 3094 proteins with crystallographic PDB structures for the human proteome; 5984 sites in 644 proteins for mouse, 1029 sites in 103 proteins for fly; 1834 sites in 179 proteins for plant and 16,745 sites in 756 proteins for yeast.
Figure 1B shows that asparagines are located preferentially in the loop/turn conformation with a frequency of 4450%.
Compared to the results shown in Figure 1A, the percentage of asparagines found in a-helix appears to be close to the ex-pected (40.20% vs. 42.10%).
However, the percentage of asparagines is higher in turns/loops and lower in b-sheets than expected.
These results prompted us to examine whether the distribution of potential NGS (annotated in UniProtKB/ Swiss/Prot) and unannotated NXS/T sites follows the same pattern.
Distribution of unannotated NXS/T motif in protein secondary structure elements All UniProtKB/Swiss-Prot records are manually curated.
Even the prediction results of every protein are manually checked before they are entered into the database.
Therefore, annota-tions available from UniProtKB records are considered gold standard in terms of functional annotation.
It is expected that unannotated NXS/T motifs not known to carry a glycan (and therefore functionally comparable to any N), should reflect the overall distribution of asparagines.
If we consider proteins with crystallographic structures, there are 3779 unannotated NXS/T motifs in human proteins, 739 sites for mouse, 1163 sites for yeast, 103 sites for fly and 223 sites for plant.
Accord-ing to Figure 2A, the percentage of asparagines in the unanno-tated NXS/T motif is slightly higher than that of all asparagines (Figure 1B) in a turn conformation.
It is possible that this is a function of the tripeptide property.
We therefore wanted to see if the distribution of annotated NXS/T motifs is significantly different than that of the unannotated NXS/T motifs.
Distribution of annotated NXS/T motif in protein secondary structure elements There are 2284 annotated NXS/T sites in 592 proteins which have PDB structures in the human dataset.
Among these, 1779 sites are in turn (78%), 222 sites in the a-helix (9.7%) and 283 sites in the b-sheet (12.3%) (Figure 2B).
This distribu-tion, based on analysis of the entire set of available structures, is consistent with recent results in the mouse showing 75% of NGSs in turns and 15% in b-sheets [10].
The same tendency is observed in our analysis for mouse, fly, plant and yeast.
In all A D is tr ib ut io n of u na nn ot at ed N X S/ T m ot if s (% ) D is tr ib ut io n of a nn ot at ed N X S/ T m ot if s (% ) B P < 0.0001P < 0.0001 0 10 20 30 40 50 60 70 80 90 100 Human Mouse Fly Plant Yeast-Helix-Sheet Loop/turn 0 10 20 30 40 50 60 70 80 90 100 Human Mouse Fly Plant Yeast-Helix-Sheet Loop/turn Figure 2 The distribution of asparagine in unannotated and annotated NXS/T motifs A.
Distribution of unannotated NXS/T motifs in secondary structural elements.
P values are calculated with v2 test by comparing the occurrence of asparagine in unannotated NXS/T motif to the distribution of all asparagines.
B.
Distribution of annotated NXS/T motifs in secondary structural elements.
P values are calculated with v2 test by comparing the occurrence of asparagine in annotated NXS/T motif to the distribution of asparagines in unannotated NXS/T motifs.
A B D is tr ib ut io n of A sn in h um an ( % ) D is tr ib ut io n of A sn in m ou se (% ) 0 10 20 30 40 50 60 70 80 90 100 Annotated N-glycosylation Unannotated with motif NXS/T All N-Helix-Sheet Loop/turn 0 10 20 30 40 50 60 70 80 90 100 Annotated N-glycosylation Unannotated with motif NXS/T All N-Helix-Sheet Loop/turn Figure 3 Distribution of asparagines in human and mouse proteins A.
Distribution of asparagines in human proteins.
B.
Distribution of asparagines in mouse proteins.
Lam PVN et al/ N-linked Glycosylation Sites 99 the species the percentages of asparagine that are part of the annotated motif is higher in loops/turns.
The distribution of annotated NXS/T sites is significantly different from the distri-bution of the unannotated NXS/T in all the species.
Represen-tative data were shown for human and mouse in Figure 3 (P < 0.0001).
Our results in mouse show a noticeable difference of the distribution of NXS/T sites in loops/turns (60% for annotated and 51% for unannotated), compared to the analysis of Zie-linska et al.
(75% and 71%, respectively) [10].
This is poten-tially due to the fact that we have analyzed a larger set of structures.
The occurrence of Asn-sequons in this type of sec-ondary structure is favored for three reasons: (i) loops/turns represent spatial arrangements of the peptide chain which fa-vor the hydrogen-bonded contact between the beta-amide of asparagines and the hydroxyl group of carbohydrates; (ii) turns constitute privileged conformations which guarantee accessibility of the sugar-acceptor sites due to their general location at the surface of proteins and (iii) these sites could be evolutionary selected because of functional importance.
Quantification and functional analysis of N-glycosylation sites in human and mouse proteome UniProtKB contains 15,828 sites for the entire human prote-ome where the sequon NXS/T is annotated as the N-linked glycosylation site.
Among them, 15,168 sites are found in se-creted and membrane proteins and 747 sites in cytoplasm and nucleus proteins (there are cases where the same protein can be found in two different places).
Thus, when comparing annotated NXS/T motifs found in the human proteome, approximately 96% of sites were found in secreted and mem-brane proteins and only 4.7% sites in cytoplasm, nucleus proteins.
For mouse, 95% of the proteins with annotated NXS/T motifs are secreted and membrane proteins, while only 3.8% are cytoplasm and nucleus proteins (in this case some proteins do not have location information).
In large-scale analysis of NGS sites, it was found that none of the identified glycoproteins are located in the mitochondria, cytosol or nucleus [11].
Kung et al.
[19] had identified mitochondrial glycoproteins using protein microarrays and it is possible that 100 Genomics Proteomics Bioinformatics 11 (2013) 96104 they are either errors in the curated data or mitochondrial gly-coproteins could not be captured because of the experimental protocols used.
It is also possible that some proteins could be present in more than one compartment at different time points, which might explain the differences in the results in Uni-ProtKB and the aforementioned studies.
Looking at the pro-portion of NXS/T sites that are annotated and the total number of NXS/T sites in the human proteome, the UniProt data suggest that only 27% of all NXS/T sites are N-glycosyl-ated.
Among secreted and membrane proteins, the number increases to 53% of all NXS/T sites (Table 2).
When compar-ing annotated NXS/T motifs to total NXS/T motifs for mouse, we find 21% of NXS/T motifs is annotated, which is lower than 27% for the human proteome, and this number increases to 36% if only secreted and membrane proteins are considered.
This could be due to the fact that not all of mouse proteome has been manually curated by UniProtKB/Swiss-Prot curators and extreme caution is employed by Uni-ProtKB/Swiss-Prot curators to ensure close to zero false posi-tives.
For cytoplasm/nucleus/mitochondria proteins, the numbers are similar for human and mouse (Table 2).
These findings show that there exist a very high number of NXS/T motifs in the proteome, but less than one third of them have so far have been documented as glycosylated for both hu-man and mouse.
A higher percentage of annotated NXS/T motifs are present in secreted and membrane proteins than cytoplasm and nucleus proteins, which is consistent with previously reported preferential N-glycosylation of proteins in the secretory pathways [10].
Comparison of the NXS/T and the reverse S/TXN site in the human proteome, which is not expected to carry glycans, reveals 58,781 NXS/T sites and 50,577 S/TXN sites (P = 0.9) (for additional details please see Table S1).
Similar distribution is observed in the mouse proteome (data not shown).
Additionally, for the human pro-teome we noticed that there are 28,527 and 22,568 NXS/T sites in secreted and membrane proteins and in cytoplasmic and nu-clear proteins, respectively, which is significantly different (P < 0.0001).
However, the number of the reverse motifs (S/ TXN) in secreted and membrane proteins and in cytoplasmic and nuclear proteins is comparable, which is 21,213 and 21,375, respectively.
It is important to note that similar results were obtained in terms of the relative cellular distribution of NXS/T and S/TXN sites, if only the proteins with PDB struc-tural information were considered (Table S1), which strongly supports the inclusion of just protein sequences with structural information for the type of analysis performed in this study.
Non-synonymous single nucleotide variation and polymorphic glycoproteins The structural analysis shows that NGS are over-represented on the surface of proteins.
We find that 91% of the annotated sites in humans and 93% in mouse are solvent-accessible, compared to 67% and 70% of the unannotated sites.
We used Table 2 Subcellular distribution of annotated NXS/T motifs in human Species Entire proteome (%) Secreted/m Human 27 53 Mouse 21 36 Note: Percentage of annotated NXS/T motifs against all NXS/T motifs in informatic tools to extract information on polymorphic N-gly-cosylation variants from the UniProtKB/Swiss-Prot database and dbSNP [20].
Previously we have shown using pathway and function enrichment analysis that a significant number of proteins that gain or lose the glycosylation motif are in-volved in kinase activity, immune response and blood coagula-tion [4].
However, it remains to be investigated whether a polymorphic site can indeed be glycosylated when there is gain of the NGS motif.
Our current analysis shows that of the 20,238 proteins in the complete human proteome (based on UniProtKB/Swiss-Prot), 3328 proteins contain polymorphic sites that create or abolish existing glycosylation sites (Fig-ure 4).
We employed machine learning techniques based on the rules developed from this study to examine the proteins that have crystallographic structure information available at NGS.
As a result, we identified 108 out of 221 polymorphic proteins with structures (Table S2), which have one or more gain of glycosylation that are expected to have some impact on protein function.
Based on UniProtKB/Swiss-Prot and Gene Ontology (GO) analysis, several of these proteins are in-volved in blood coagulation, cell adhesion, host-pathogen interaction, immunity and transport (Table S3).
The major molecular functions represented are hydrolases, receptors and transferases.
Out of 2299 proteins that do not have struc-tures, 2247 proteins are predicted to be glycosylated at the gain of glycosylation site by the SFAT tool (total NXS/T sites: 12,623; sites predicted as yes: 11,651 and sites predicted as no: 972).
Based on GO analysis using Panther tools [21], the over-represented GO biological processes in this dataset in-clude immune response, response to stimulus, signaling and blood coagulation, which agrees with the our results obtained previously [4].
N-linked glycosylation prediction tool The analysis that we have performed here represents an effi-cient way to explore the glycosylation potential of protein if the structure is known.
We have also extended the tool to work on proteins without structure, albeit with lower accuracy: 74% without structure compared to 93% with structure.
It is ex-pected that within the next decade, majority of proteins with NGS will have their structures solved, or it will be possible to generate high-quality homology models for these proteins based on related protein structures.
To provide easy compari-son of these structures and sequences we have web-enabled our tools developed for this study.
The tool can perform the fol-lowing tasks: (i) predict N-linked glycosylation sites, (ii) deter-mine the secondary structural elements of any site of interest (such as active site, metal binding site, N-linked glycosylation site or any other sited based on user-defined motif) and (iii) map UniProtKB and PDB sequence features.
The tool, Struc-ture Feature Analysis Tool (SFAT), is expected to be useful for and mouse proteome embrane (%) Cytoplasm/nucleus/mitochondria (%) 3 2.6 respective categories is shown.
20,238 human proteins 787,106 polymorphic sites 3328 N-linked glycosylation polymorphic proteins 1060 loss of N-linked glycosylation (LOG) sites 12,623 gain of N-linked glycosylation (GOG) sites 11,651 sites predicted by SFAT to be true GOG Figure 4 Identification of N-linked glycosylation (NLG) sites using SFAT Lam PVN et al/ N-linked Glycosylation Sites 101 researchers interested in site-specific quantitative structural analysis.
Figure 5 shows a snapshot of the SFAT interface.
Prediction of N-linked glycosylation sites The prediction is performed using the following four basic rules: (i) presence of Endoplasmic Reticulum targeting se-quence; (ii) not nuclear, mitochondrial or cytoplasmic; (iii) present in a loop or turn and (iv) exposed.
Data for the first two rules are extracted from the UniProtKB flat file.
The third piece of information is extracted from the PDB file and the rel-ative solvent accessibility is obtained from Define Secondary Structure of Proteins (DSSP) database [22].
When structure is not available, the last two pieces of information is predicted.
We identified 96 new (currently not annotated in UniProt) NGS in the human proteome that matches all four rules (3.6% of the total unannotated NXS/T sites).
Instead of using a set of strict rules for prediction which can potentially lead to large numbers of false negatives, we implemented these rules into a machine learning framework for better prediction accu-racy.
Using this approach our cross validation prediction mod-el showed the overall accuracy of 93%, and the precision for true positive of 90% for proteins with structures.
The accuracy Figure 5 Home page for N-linked glycosylation prediction tool SFAT User can either predict N-linked glycosylation sites, find the distributio and PDB sequence features.
and precision is 74% and 70%, respectively, when the tool is applied to proteins without structures.
The model was then used to predict NGS in the polymorphic glycoproteins.
The re-sults showed that for the gain of N-glycosylation, around 40% are predicted to be glycosylated.
There are several other NGS prediction tools currently available.
However, none of them use a rule-based method that is dependent on structural information.
EnsembleGly [23], a sequence-based method using ensembles of support vec-tor machine classifiers, has 94% accuracy; NetNGlyc (http://www.cbs.dtu.dk/services/NetNGlyc/) uses artificial neural net-works that examine the sequence context of Asn-X-Ser/Thr sequons with an overall accuracy of 76%.
In addition, GPP [24] uses the random forest algorithm and pairwise patterns to predict glycosylation sites with an accuracy of 90.8% for Ser sites, 92.0% for Thr sites and 92.8% for Asn sites.
It is important to note here that the authors used their own training and test datasets to determine the accuracy of their tools.
A di-rect comparison between different tools is thus difficult be-cause the tools were developed and tested on different training and test datasets.
Furthermore, the definition of neg-ative NGS is an open discussion, because it is difficult to prove definitively that a particular residue is not glycosylated under any conditions, although experiments can verify that a partic-ular residue can be glycosylated.
Determining the secondary structure elements of any amino acid site Knowledge of the distribution of a specific motif in the second-ary structure elements can be useful to predict the functional relevance.
To facilitate studies similar to the one described here, we implement within SFAT an option that can provide a distribution report of any motif of interest.
The input file is the UniProtKB flat file.
The tool gives the user multiple op-tions about different feature information in UniProtKB such as: N-linked glycosylation, active site or metal-binding site.
Alternatively, the user can define their motif of interest.
Instructions for how to determine the correct pattern can be found in the Help document.
The results are given in a down-loadable table.
A pie chart and plot graph are generated to illustrate the obtained results.
Mapping of UniProtKB and PDB features Sequences from PDB and UniProt may not be identical (PDB sequences can be shorter or longer compared to the n of a motif in secondary structural elements or map UniProtKB 102 Genomics Proteomics Bioinformatics 11 (2013) 96104 UniProtKB sequence).
Therefore, it is important to align them to explore the different feature annotations that are available on the sequence from UniProt or PDB.
Often this task is done manually and can lead to errors.
We have implemented a sim-ple alignment tool and the features are extracted from Uni-ProtKB flat files and PDB records and the user can easily identify the features of interest in the amino acid sequence.
This tool was used in this study to further analyze the poly-morphic glycosylation sites.
Conclusion Comparative structural study of asparagines in human, mouse, fly, plant and yeast showed that a high percentage of aspara-gines in NXS/T motifs implicated in N-glycosylation are local-ized within a turn/loop and are solvent-exposed at the protein surface.
The N-glycosylated proteins are typically not cytoplasmic, nuclear or mitochondrial.
We have incorporated these observations into an N-glycosylation prediction tool which combines structure-and sequence-based rules that sig-nificantly improve sequence-based prediction methods.
The tool was used to predict glycosylation sites of a set of polymor-phic human proteins.
Materials and methods By annotated glycosylation sites, we mean NXS/T sites that are indicated as the N-linked glycosylation site in Uni-ProtKB/Swiss-Prot [25] protein record, while unannotated gly-cosylation sites include all other NXS/T sites.
We consider these annotated glycosylation sites as potential NGS, as they have been manually curated by UniProtKB/Swiss-Prot cura-tors based on experimental evidence, similarity to experimen-tally-validated NGS in homologous proteins and/or in-depth sequence and functional analysis.
UniProtKB/Swiss-Prot hu-man proteome is considered the gold standard set of manu-ally-curated human proteins and in our opinion provides the best positive and negative datasets, since UniProtKB/Swiss-Prot curators have manually curated all entries for the human proteome.
All predictions are checked manually by curators to ascertain if there are any homologous sites in related proteins.
Predictions that are dubious are not included in the sequence feature annotation.
Datasets All data were collected from UniProtKB and Protein Data Bank (PDB) [26].
For the human proteome, the complete pro-teome available from UniProtKB/Swiss-Prot was used.
For mouse, the file rp-seqs-15.fasta.gz from http://pir.george-town.edu/rps/data/current/15/ was downloaded and parsed to obtain the complete proteome.
This was done because the complete proteome of mouse from UniProtKB has several po-tential splice variants as separate entries.
All other proteomes were obtained from UniProtKB using the complete proteome keyword tag.
The access dates for all data retrievals are be-tween 15th February and 15th June, 2012.
Experimentally-val-idated datasets were obtained from UniProtKB and supplementary materials in Zielinska et al.
[10,11].
Data analysis Python scripts were used to extract information from Uni-ProtKB flat file feature (FT) lines, cross-reference (DR) lines for PDB database and sequence (SQ) lines.
Information was also extracted from the PDB files in order to get secondary structure information.
For annotated N-linked glycosylation sites, all UniProt FT lines annotated as N-linked whether confirmed, potential or by similarity were retained.
The positions of these sites were retrieved and the correspond-ing sequences were checked for the NXS/T motif.
PDB IDs were extracted from UniProtKB flat files.
As there can be more than one PDB file mapped to any UniProtKB protein, the PDB structure which meets the following criteria was selected: the structure was determined by x-ray diffraction, highest res-olution, and the site of interest is contained within the solved structure.
Once selected, PDB files were downloaded from PDB and the positions aligned to the PDB sequence and the secondary structure was determined based on the secondary structure assignment in the PDB file.
Relative solvent accessibility was calculated using the infor-mation in DSSP database [22].
Based on the value of the rela-tive accessible surface area (ASA), the residues were grouped as buried (0.00.25) or exposed (0.251.0).
The choice of the threshold was based on previous studies [10,18,27].
For ma-chine learning using classification and regression tree (CART), we allow CART to automatically select the relative ASA based on the test set.
MUSCLE was used to perform pairwise align-ment to map UniProtKB protein sequences to PDB [28].
Pre-diction of the N-terminal targeting sequences was performed using Predotar [29].
Subcellular location analysis is performed based on UniProtKB keywords.
For sequences that do not have structural information, secondary structure and surface accessibility of the individual amino acid was predicted using NetSurfp [30].
P value was calculated using a binomial statistic based on methodology described earlier [31,32].
P value of 0.05 or less was considered significant.
Identification of polymorphic glycosylation sites Gain of glycosylation sites were identified by using variation data from UniProtKB FT lines and dbSNP [20,33].
Entries were first mapped to UniProtKB accessions using the ID map-ping service [34] followed by sequence mapping.
This resulted in a table with UniProtKB accession numbers and position of variation, the variation and the data source (Table S2).
Machine learning using CART This method consists of creating a model that predicts the va-lue or a class for a predictive variable based on several input variables.
The algorithms of this decision tree usually work top-down by choosing a variable at each step that is the next best variable to use in splitting the set of items.
Best is de-fined by how well the variable splits the set into homogeneous subsets that have the same value of the predictive variable [35].
As the number of unannotated sites is higher than that of the annotated sites in our analysis of the human proteome (2:3 ra-tio), the training dataset for our prediction model contains 200 experimentally-validated sites (noted as positive) and 300 unannotated sites (noted as negative).
Training classifiers are Lam PVN et al/ N-linked Glycosylation Sites 103 challenging because the positive and negative datasets are unbalanced with more negative sites than positive sites, which can result in poor classification of the minority class (in this case positive sites).
One solution is to change the distribution of major and minor classes during training by randomly select-ing a subset of the training data.
However, this approach does not then take into consideration all available information in the real dataset.
Therefore, we chose our training dataset to contain original data in a ratio that reflects the natural dataset.
First, a set of patterns is generated from the training data for each of the glycosylation sites and then used to generate a value for each instance.
Multiple runs are performed with each instance collecting weights to determine the positive or negative NGS class.
Each of the runs used dataset comprising randomly chosen positive and negative instances from the cross validation fold.
The accuracy of the prediction was eval-uated by cross validation (described below).
The variables that describe the dataset are selected based on the rules derived from this study: (i) ER targeting sequence (ii) sub-cel-lular location, (iii) secondary structure and (iv) exposed/buried.
To build the classificationmodel, the function rpart in statistical lan-guage R was used.
Twenty-fold internal cross validation was per-formed to validate the training dataset as described previously [24].
More specifically, the dataset was partitioned randomly into 20 sections and the training procedure was carried out using 19 of these while the 20th section provides a test dataset.
This was re-peated20 timesoneachoccasionwithadifferent sectionof thedata acting as the test set.
The evaluation of the model is based on the number of true positives (TP), false positives (FP), true negatives (TN), false negatives (FN).
The model was then used to predict NGS in the test dataset.
Authors contributions RM conceived, designed and coordinated the study and devel-oped a general outline for the algorithm.
PL developed the spe-cific algorithm, was responsible for software design and implementation and drafted the manuscript.
TN participated in the design and evaluation of the tool interface.
KK gener-ated the variation data.
RG tested the tool and helped design the study.
VSN and VSK performed the statistical analysis.
All authors read and approved the final manuscript.
Competing interests We declare that we have no competing interests.
Acknowledgements We want to thank Hayley Hamilton of the George Washing-ton University and Nagarajan Pattabiraman of MolBox LLC for critical reading of the manuscript and providing use-ful comments.
All computations were performed at High-per-formance Integrated Virtual Environment (HIVE) located at The George Washington University and co-developed by Drs.
Raja Mazumder and Vahan Simonyan.
Support for this work came from the George Washington University funds to RM.
RGs participation is supported by RO1 CA135069 and U01 CA168926.
This project is also supported in part by an appointment to the Research Participation Program at the Center for Biologics Evaluation and Research administered by the Oak Ridge Institute for Science and Education through an interagency agreement between the U.S. Department of Energy and the U.S. Food and Drug Administration.
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/ j.gpb.
2012.11.003.
ABSTRACT Motivation: Comparative genomics aims to understand the structure and function of genomes by translating knowledge gained about some genomes to the object of study.
Early approaches used pairwise com-parisons, but today researchers are attempting to leverage the larger potential of multi-way comparisons.
Comparative genomics relies on the structuring of genomes into syntenic blocks: blocks of sequence that exhibit conserved features across the genomes.
Syntenic blocs are required for complex computations to scale to the billions of nu-cleotides present in many genomes; they enable comparisons across broad ranges of genomes because they filter out much of the individ-ual variability; they highlight candidate regions for in-depth studies; and they facilitate whole-genome comparisons through visualization tools.
However, the concept of syntenic block remains loosely defined.
Tools for the identification of syntenic blocks yield quite different re-sults, thereby preventing a systematic assessment of the next steps in an analysis.
Current tools do not include measurable quality objectives and thus cannot be benchmarked against themselves.
Comparisons among tools have also been neglectedwhat few results are given use superficial measures unrelated to quality or consistency.
Results: We present a theoretical model as well as an experimental basis for comparing syntenic blocks and thus also for improving or designing tools for the identification of syntenic blocks.
We illustrate the application of the model and the measures by applying them to syntenic blocks produced by three different contemporary tools (DRIMM-Synteny, i-ADHoRe and Cyntenator) on a dataset of eight yeast genomes.
Our findings highlight the need for a well founded, systematic approach to the decomposition of genomes into syntenic blocks.
Our experiments demonstrate widely divergent results among these tools, throwing into question the robustness of the basic ap-proach in comparative genomics.
We have taken the first step towards a formal approach to the construction of syntenic blocks by develop-ing a simple quality criterion based on sound evolutionary principles.
Contact: cristinagabriela.ghiurcuta@epfl.ch 1 BACKGROUND Comparative studies have long been the mainstay of knowledge discovery in biology.
With the advent of inexpensive sequencing tools, pairwise sequence comparison became a major research tool; programs such as BLAST (Altschul et al., 1990) are used to identify regions with similar sequences in order to study prob-lems in genetics and genomics by using knowledge from better characterized organisms.
Such comparisons have been carried out on relatively short sequence fragmentsusually up to the length of a protein transcript, i.e.
a few thousand nucleotides.
Such work continues at a great pace today, but the rapidly increasing availability of complete genome sequences has led to the desire to compare entire genomes at once, the better to understand the large-scale architectural features of genomes and the evolutionary events that have shaped these features, such as segmental and whole-genome duplication, horizontal transfer, recombinations of various types and rearrangements.
Comparing entire genomes is not new: almost a century ago, Thomas Morgan and his students used chromosomal banding to build genetic maps of various strains of Drosophila melanogaster.
What is new today is the possibility of comparing complete genome sequences to each other.
Comparing even just two gen-omes is a major computational challenge when the two genomes have several billion nucleotides and when most of the sequence (490% in humans) is poorly understood and so lacks a suitable evolutionary model.
Consequently, researchers have approached the problem by defining (or searching for) conserved sequence markers (mostly belonging to the better understood coding re-gions of the genome).
These markers are then used to form large-scale patterns that can be evaluated for similarity and conserva-tion.
Such large-scale patterns, when used systematically, can be viewed as alternative representations of the genomes.
The sim-plest such representation uses the concept of syntenic blocks (SBs), large blocks of sequence that are well conserved (as testi-fied by commonality of markers and similarity of high-level pat-terns) across the species (or within a genome).
Working with such blocks facilitates comparative studies: (i) it confers robust-ness against variability across individuals and against various sources of error; (ii) it reduces the dependence on an accepted model of sequence evolution for each region and is less likely to suffer from homoplasy; (iii) it reduces the complexity of the ana-lysis of the genomic structures; (iv) it provides high-level features for further evolutionary studies; and (v) it identifies specific regions of interest for detailed studies and possible bench experiments.
In this article, we provide a concise overview of the existing notions of synteny in the literature and propose a formal, prin-cipled definition of SBs based on homologies.
We discuss how the quality of SBs can be measured against this definition and illustrate our approach with a comparison of three current tools for the construction of SBsCyntenator (Roedelsperger and Dieterich, 2010), DRIMM-Synteny (Pham and Pevzner, 2010) (DRIMM) and i-ADHoRE 3.0 (Proost et al., 2012) (i-ADHoRe).
We investigate the underlying heuristics and evalu-ate the results on a dataset of eight full genomes of various spe-cies of yeasts from the Yeast Gene Order Browser (YGOB) (Byrne and Wolfe, 2005), pointing out the issues that arise when working with SBs.
1.1 Early notions of synteny Little has been done so far towards a formal definition of SBs and/or SB families, nor have developers of algorithms and*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com [1] , , .
over paper syntenic block syntenic block 3 syntenic block [23] [20] , [21] 8 [5] syntenic block syntenic block XPath error Undefined namespace prefix software for producing SBs given any quantifiable goals.
Instead, identifying SBs has been a matter of application-dependent heur-istics, lacking any serious attempt at evaluating the quality of the approachessomething that in any case would have proved dif-ficult in absence of quality criteria.
The first mention of synteny as it is understood today was in an article of Renwick (1971) on human chromosome mapping, where the term is introduced to denote collocation of markers on the same chromosome.
Nadeau and Taylor (1984) gave an informal definition of syntenic seg-ments, in a paper that has since been cited by most researchers concerned with synteny.
Nadeau and Taylor gave a list of fea-tures viewed as supporting inclusion of markers in an SB, a list that includes conserved orientation, conserved adjacency and conserved position of homologous markers associated with the corresponding mapped chromosomes, a collection of features that loosely defines what is more commonly called today collinearity.
The study of rearrangements led to the definition of common intervals (Bergeron et al., 2002; Jahn, 2011), conserved regions of a chromosome within which the same set of genes can be observed, albeit not necessarily in the same order.
The concept is formally and precisely defined and captures many of the prop-erties informally associated in the literature with SBs.
The definition is given in terms of families of non-duplicated genes (or other families of unique sequences) and their ordering.
It does not take into account precise locations on the genome, nor the actual nucleotide sequences of these genes.
Around the same time, the need to compare entire genomes of the newly sequenced model species led The Mouse Genome Sequencing Consortium (2002) to propose SBs as sets of adjacent syntenic fragments (possibly shuffled in order and orientation) belonging to the same chromosome, where a syntenic fragment consisted of markers arranged in a conserved order.
In this view, syntenic fragments obey collinearity, whereas SBs need not do so.
Calabrese et al.
(2003), authors of the FISH synteny tool, defined their model based on segmental homology, in which the ordering of features belonging to two homologous segments is roughly conserved, some variation being allowed.
Pevzner and Tesler (2003) and later Bourque et al.
(2004), both working on the GRIMM-Synteny tool, removed constraints on conserved segments, thereby implicitly defining an SB in terms of conserved segments that can be disrupted by internal rearrangementsre-arrangements that the authors found to be far more common than expected and that therefore had to be largely ignored in constructing SBs.
In contrast, Van de Peer and his group, au-thors of the ADHoRe tool (Vandepoele et al., 2002), chose to emphasize collinearity and to break larger blocks into smaller ones as necessary to maintain this property.
These and other early tools are briefly reviewed in (Deonier et al., 2005).
1.2 Markers, syntenic blocks and genomic alignment Identifying SBs and aligning whole genomes both rely on iden-tifying markers, i.e.
short sequences that are highly conserved across the genomes and long enough to make their conservation statistically significant.
SB construction uses subsets from the set of markers: if a sufficiently dense region is identified in most of the genomes, those regions can be viewed as SBs.
Genomic align-ment uses the markers as anchors, i.e.
fixed references in the alignment.
Most SB finders use genes as markers; a few use k-mers, for a fixed value of k, to define a de Bruijn graph on the k-mers.
[de Bruijn graphs are widely used for genome assem-blysee Compeau et al.
(2011) for an excellent introduction in this context.
In such a graph, every k-mer found in the input sequences is represented by an edge connecting two vertices that are the k 1 prefix and k 1 suffix of the k-mer.
Thus a path of j edges through such a graph corresponds to an assembled se-quence of length k+ j 1 formed by ordering j k-mers, with each consecutive pair presenting a perfect overlap of length k 1; in particular, an Eulerian path through the graph corresponds to an assembly of all k-mers into a single sequence.]
Genomic align-ment may use a richer pool of markers, such as scaffold data, maximum unique matches (perfectly conserved sequence frag-ments of maximal length), genes and even assembly contigs.
Those that use markers in the sense of highly conserved sequence fragments define markers through a variety of criteria, such as Bayesian statistics in Pecan (Paten et al., 2009) or sequence simi-larity iterated through a refinement pipeline in ProgressiveMauve (Darling et al., 2010).
Just as most work on defining SBs focuses on two genomes at a time, so is whole-genome alignment usually done pairwise.
Biologists have long known that multi-way comparisons provide more information than pairwise comparisons, especially multi-way comparisons within a phylogenetic context.
However, com-paring several genomes at once introduces problems: finding good markers that are present in all, or almost all, genomes; choosing or inferring a number of parameters related to attri-butes difficult to measure, such as the level of evolutionary divergence among the genomes or the quality of the genome sequences used; assigning one-to-one correspondences among similar blocks so as to minimize the number of evolutionary events needed to explain the architecture of the modern genomes; whether to insist on the transitivity of relationships such as hom-ology and orthology (among markers, among genes, among SBs, etc.
); and many others.
1.3 Work to date Nadeau and Taylor (1984) defined synteny in terms of two or more pairs of homologous genes occupying the same chromo-somal segment, where homologous loci are based on similarity of function of the products of the corresponding genes.
They carefully distinguished synteny, which they were basing on con-servation of function, from conserved segments, based on con-servation of sequence.
More recent work has typically used conservation of sequence rather than conservation of function, but has also made use of orthology, presumably because orthol-ogy is viewed as a stronger indicator of conserved function than homology.
Zeng et al.
(2008) based their Orthocluster tool strictly on gene orthology and used many parametric constraints, such as pos-ition, overall number of genes in a block, allowed number of genes per block without orthologs, etc.
Their tool handles large-scale genomic events such as translocation, transposition, indels and duplication.
The restriction to orthology, however, means that the applicability of the tool is limited to collections of closely related organisms.
i10 C.G.Ghiurcuta and B.M.E.Moret syntenic block syntenic block in 1971, syntenic block , [3,13] syntenic block syntenic block [24] syntenic block syntenic block syntenic block [25] [9] , syntenic block that is, Syntenic block syntenic block that is, syntenic block (------.)
, [17] [8] syntenic block syntenic block [16] [27]  Cassis (Baudet et al., 2010), also based on orthology relation-ships, prunes considerably the list of orthologous gene pairs pro-vided as input, eliminating those that disrupt collinearity.
The remaining pairs are used to form blocks based on a statistical evaluation of their match to the collinear model.
Modern tools all attempt to handle the loss of collinearity, in recognition of the fact that collinearity (absence of rearrange-ments) is unlikely to be observed in collections of genomes of any significant size or degree of divergence.
Equally important and still challenging is the ability to deal with varying marker (most often gene) content: given reasonably divergent genomes, markers will have been variously lost or acquired over time.
In the multiple alignment tool ProgressiveMauve, Darling et al.
(2010) focused on a very principled approach to define and then to use the markers for the alignment process.
Its strat-egy is to identify highly conserved, sufficiently long sequences (anchors) throughout a concatenated multi-chromosomal genome and then, for each interval between consecutive anchors that exceeds a certain length, to search recursively for additional, less perfectly conserved anchors.
This recursive refinement continues until the anchor coverage has reached a sufficient dens-ity or the heuristic cannot retrieve any additional anchors.
ProgressiveMauve was designed as an alignment tool, not a syn-teny tool, but it generates a list of homologous, locally collinear regions that can be used as a basis for defining SBs.
Cyntenator (Roedelsperger and Dieterich, 2010) uses genes as markers and is based on a progressive alignment of profiles of gene-order data.
It allows gene duplication and loss and thus, in order to distinguish between orthologs and paralogs, takes into account gene family information as part of its scoring scheme.
Pairwise alignments produced at each stage are refined before being used in the next stage.
As is the case for most such tools, the blocks identified by Cyntenator are not formally character-ized, but indirectly defined through the algorithm.
i-ADHoRe 3.0 (Proost et al., 2012) also uses genes as markers; it includes heuristics to deal with rearrangement and duplication.
Duplicated genes are mapped onto a representative of the gene family.
The tool produces profiles of collinear regions based on homology maps of pairs of genomic regions and uses heuristics based on network flow to resolve conflicting relations between pairs of genes.
The tool provides three constraint models for generating SBs: collinear (conserving both order and orienta-tion), cloud (conserving neither order nor orientation, but content) and a sequential mixture of the two.
DRIMM-Synteny (Pham and Pevzner, 2010), the multi-way successor of the pairwise GRIMM-Synteny, is, like most synteny tools, based on genes, but follows an entirely different approach, as it is based on de Bruijn graphs.
A somewhat different version of de Bruijn graphs, called A-Bruijn graphs, is used in order to take into account the different characteristics of the problem, such as the use of gene orders rather than overlaps.
Thus a gene adjacency becomes an edge of the graph and is weighted by the number of its occurrences across the genomes.
SBs correspond to paths through the graph.
Sibelia (Minkin et al., 2013) follows up on DRIMM, in that it is also based on de Bruijn graphs, but, being designed for bac-terial genomes, it works directly from sequence data and so builds standard de Bruijn graphs from sequence k-mers.
It also adds an iterative refinement procedure that provides a range of granularity for the blocks.
The pipeline is executed individually for increasing sizes of the k-mers, until the output block is the whole genome.
At each iteration, a different set of blocks is generated and is placed as a node into a tree structure, with the root of the tree corresponding to the whole genome.
Table 1 lists the main features of the synteny tools we used.
1.4 Syntenic blocks, homology and granularity That blocks generated from the same data by different tools may differ enormously is due mostly to the lack of a formal definition for SBs: with no verifiable constraints and no measurable opti-mality criterion, one cannot meaningfully compare two collec-tions of SBs for the same data.
In part, the lack of such constraints and criteria can be attributed to the very different uses to which SBs are put.
For instance, using SBs to pinpoint a region of interest in the genomes works best if the blocks are small and highly conserved, whereas using SBs to study the evo-lution of the architecture of genomes does better with larger blocks and can tolerate much larger divergence in any given block among the genomes.
(Indeed, the larger the evolutionary divergence, the larger and sparser the SBs should be, to account for the lower number of high-quality markers.)
When large-scale (segmental or whole-genome) duplications are present, multiple instances of the same SB will be found within the same genome, as well as throughout other gen-omesthat is, SBs, like genes, can be grouped into families of homologs.
Identifying orthologies among the markers or genes is thus intertwined with identifying SBsarguing for a simultan-eous construction, which can take into account positions, re-arrangements and duplications and losses of markers and of blocks all at once.
Thus homology is at the root of any principled definition of SBs: the process of construction of SBs is simply the process of extending homologies among markers to homologies among blocks under a suitable model of evolution.
In such a manner, partitioning the genomes into SBs defines the necessary higher-level homology relationships that relate such blocks within and across genomes.
Since all genomes share a common ancestor, every single genome is trivially an SB by itself, albeit with a very low degree of conservation across a collection of genomes.
At the other extreme, if we had available a detailed history of all Table 1.
Major features or constraints of five synteny tools: ProgressiveMauve (PM), OrthoCluster (OC), Cyntenator (Cy), i-ADHoRe (i-A) and DRIMM (DR); presence is denoted by +, absence by and options by o PM OC Cy i-A DR Collinearity o o Framed blocks +   Overlapping content + + + Selective content + + + Across chromosomes + + + o Duplicated regions + + + + i11 Evaluating synteny for improved comparative studies [2] [8] syntenic block [23] [21] syntenic block [20] Syntenic block [15] Syntenic block , syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block that is, syntenic block syntenic block , syntenic block syntenic block syntenic block syntenic block evolutionary events at the sequence level, we could construct SBs consisting of a single nucleotide position.
In a similar vein, two or more adjacent SBs can be viewed as single, larger SBs, pre-sumably at the cost of some loss in conservation.
In other words, granularity is an important attribute and one can construct a hierarchy of decompositions into SBs, taking the form of a rooted directed acyclic graph where the trivial decomposition into a single block sits at the root and the equally trivial decom-position into individual nucleotide positions sits at the single leaf.
Children of a node in this dag are associated with decompos-itions of finer granularity than that associated with the node itself.
Under some mild constraints, this dag is in fact a lattice (or partially ordered set).
It is important to note that the lattice is determined by con-straints resulting from the definition of an SB, but the selection of a particular node in the lattice (a particular decomposition into blocks) is driven by other criteria (such as granularity) and thus determined by the application.
(Of all the various tools reviewed here, only Sibelia makes explicit mention of a hierarchy of SBs.)
2 METHODS 2.1 Homology, orthology and synteny Any definition of synteny must use homology or orthology.
Most synteny tools today use bothhomology as a matter of principle and orthology as a result of practical constraints.
In evolutionary biology, two structures (character positions in a sequence, markers of various types, genes, SBs) are homologous if they are descended from a common ancestral structure (Fitch, 2000); if, in addition, the branching at the last common ancestor was a speciation, the structures are also orthologous.
Thus homology is an equivalence relationship and, as such, determines equivalence classes, the homologous families of structures.
Orthology, in contrast, depends on the speciation point and so is context-dependent; in particular, it is generally not transitive.
(For instance, two gene duplicates within the same genome cannot be orthologous, but these two duplicates and a homologous gene in another species are orthologous if the duplication followed the speci-ation.)
Instead, it must be specified through hierarchies structured through the phylogeny (see Gabaldon and Koonin, 2013).
Homology and orthology cannot be observed, but only inferred.
In practice, homology for markers and genes is determined on the basis of sequence similarity, using tools such as BLAST.
Orthology is also initially determined through sequence similarity, but often verified through phylo-genetic analysis or by ascertaining functional similarity.
However, only rarely is position along the genome taken into accountexceptions are the database OrthoDB (Waterhouse et al., 2011), which also provides a hierarchy of orthology relationships, and the orthology tool MSOAR (Fu et al., 2007).
In practice, therefore, identifying homologies is much easier than identifying orthologies.
Synteny is defined both through families of homologous markers and through placement within the genome.
Therefore identifying SBs, in add-ition to prior knowledge of homologies, requires taking into account rearrangements and duplications that disperses the members of a hom-ologous family throughout the genome.
(Conversely, of course, produ-cing SBs makes direct statements about the evolutionary history of the genomes by ruling out some of the possible scenarios.)
Therefore, in principle, the identification of SBs should proceed from homologies (which have little direct dependence on location) rather than from orthol-ogies inferred without regard to location.
Computing gene clusters, for instance, is best done based on families of homologous genes instead of relations derived from orthologous groups (Jahn, 2011).
Practice may dictate otherwise.
Inferred homologies are neither sym-metric nor transitive in practice, as they depend on similarity thresholds.
In addition, since orthology is the stronger relationship, it is often preferred, at least for pairwise synteny, as it may provide higher quality markers and because it simplifies the task.
(Some synteny finders simply transform orthologous relationships into bijections, in spite of the fact that orthology is a many-to-many relation.)
When moving from pairwise to multi-way syntenies, orthologies become problematic: the more diverse the group of genomes, the more difficult it becomes to identify orthologies.
In practice, therefore, synteny tools rely on both homology and orthology, viewed largely as different degrees of sequence similarity.
2.2 Towards a formal definition for syntenic blocks Here we propose a fundamental constraint on the makeup of SBs, based on an evolutionary perspective.
We first formalize that constraint for pairwise synteny, then extend it to multi-way synteny.
We also propose a second constraint, which provides added refinement for bacterial genomes and also helps narrow searches when looking for conserved regions of interest.
Our definitions are made in terms of markers and homology state-ments among them.
Thus we regard each genome as a multi-set of mar-kersa multi-set rather than a set, as the same marker may occur more than once in the same genome.
Associated with each marker is a set of homology statements relating that marker to its homologs in other gen-omes or in its own genome; a homology statement is just an unordered pair of markers.
Ideally, these homology statements define an equivalence relation on the set of markers; in practice, of course, these statements come from a variety of sources (databases, direct analysis of sequence similarity, etc.)
and are unlikely to obey all the requirements of an equiva-lence relation.
Viewed abstractly, identifying SBs is a clustering problem: how do we partition the multi-set of markers into smaller multi-sets, so as to maxi-mize the similarity (as attested by multiple homology statements) between some of the smaller multi-sets, while minimizing their similarity to others?
Because our definition rests on homologies rather than orthologies, we expect to find homology statements connecting related SBs as well as some connecting unrelated SBsby and large, the first are more likely to be orthologies, while the second are more likely to be paralogies.
Our main constraint, then, is that, in order for two blocks to be homologous SBs, they must be connected through homology statements and that neither includes markers that, while unconnected in this manner to any-thing in the other blocks, are connected to markers in unrelated SBs.
We now formalize our definition for the basic version of SBs: SBs for two genomes, in which we restrict each to be a contiguous range of pos-itions within a chromosome.
DEFINITION 1.
We are given two genomes, GA with a set A of nA markers and GB with a set B of nB markers; the markers of GA are ordered along the chromosomes, as are the markers of GB.
Let H be a set of pairs of distinct elements of A [ Bthe homology statements.
We assume that every marker in A and B is part of at least one homology statement.
Let SA be a set of contiguous markers on one chromosome of GA and SB a set of contiguous markers on one chromosome of GB.
We say that SA and SB are homologous SBs if and only if, for any marker x 2 SA, there exists a marker y 2 SB such that {x, y} is a homology statement, and, for any marker u 2 SB, there exists a marker v 2 SA such that {u, v} is a homology statement.
We can further require that the two end markers form a conserved frame, thereby setting defined boundaries on the range of positions form-ing an SB.
DEFINITION 2.
Let SA and SB be homologous SBs as per Definition 1.
If the first marker of SA is a homolog of one of the two endmarkers (the first or last marker) of SB and the last marker of SA is a homolog i12 C.G.Ghiurcuta and B.M.E.Moret syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block [10] , , [12] [26] [11] syntenic block syntenic block syntenic block [13]-syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block Definition syntenic block syntenic block Definition syntenic block .
of the other endmarker of SB, we say that SA and SB are (homologous) framed SBs.
Many of the existing tools require that the homology between markers respect the ordering of the markers along the blocksa property usually referred to as collinearity.
Because genomes are subject to rearrange-ments, we do not require collinearity, but we can define it as follows using our notation.
DEFINITION 3.
Let SA and SB be two homologous SBs as per Definition 1.
We say that SA and SB are collinear SBs if the following condition, stated in the direction from SA to SB, holds in both directions: for any markers x and y in SA with x appearing before y, there exist markers u and v in SB, with u appearing before v, such that both {x, u} and {y, v} are homology statements.
Our requirement that each block be fully contained with a chromo-some may require that some evolutionary events, such as translocation, fusion and fission, all of which can move genomic material between chromosomes, be treated as block-splitting events.
For instance, if prior to such an operation, we would have identified regions A and B as hom-ologous SBs, but the operation moved part of region A, call it At (tail) to another chromosome, leaving only Ah (head) in the original location, then after the operation we may be unable to associate either of Ah or At with B, but we may be able to associate Ah with a first subregion Bh of B and At with a second subregion Bt of B, thereby producing two pairs of smaller SBs.
We extend pairwise synteny to multi-way synteny by taking advantage of the transitive nature of true homology: we simply require transitive closure of pairwise relationships.
DEFINITION 4.
We say that blocks A1, A2, .
.
.
, Ak are homologous SBs if and only if, for any i and j; 1 i5j k; Ai and Aj are pairwise hom-ologous SBs.
This definition is unambiguous whenever our set of homology state-ments defines an equivalence relation, since this property ensures transi-tivity.
In practice, however, neither transitivity nor symmetry will hold: our set of homology statements will typically be incomplete as not all homologies among markers are detectable and homology defined through sequence similarity (the most common type in practice) need not be symmetric.
The output of a synteny tool is a collection of families of homologous SBs (henceforth SBFs), each family tied together with homology state-ments.
We illustrate our definitions with a few cartoons.
Figure 1 shows the building blocks for our cartoons and also demonstrates the additional structure present in framed SBs.
Figure 2 illustrates the main character-istics used in our definitions.
The first two cartoons in the figure show SBs defined through one-to-one (Fig.2A) and one-to-many (Fig.2B) hom-ology statements.
Homology statements may connect markers in non-homologous SBs, as long as other homology statements connect these markers to markers in homologous SBFs.
The third cartoon (Fig.2C) gives an example of invalid blocks: the red marker has a homolog in a non-homologous SB, but none in the putative homologous SBs.
3 RESULTS AND DISCUSSION Our goal is to enable evaluations and comparisons of decompos-itions into SBs.
Such evaluations and comparisons have mostly been missing and, when present, have typically been limited to aspects such as coverage of the genome or number of blocks, neither of which has much to do with quality.
Our first step was to propose formal constraints that any decomposition into SBs should satisfy.
These constraints are not likely to be met except in ideal cases, so our second step is to measure compliance with the constraints, which is to say, to measure quality.
We therefore assemble a dataset of whole genomes to use in testing various methods; devise specific measurements of compliance with our definitions; and provide other insights and measures regarding the various tools tested.
3.1 The data Because we chose to include DRIMM in our evaluation, but could not reproduce its authors results, we decided to use their results directly.
Of the datasets used in the DRIMM study, only the yeasts combined complete results from the authors and public availability of the genomic data.
We thus used the gene data from the Yeast Gene Order Browser (version of April 2009) (Byrne and Wolfe, 2005) for the following eight yeast genomes: Candida glabrata (c), Eremothecium gossypii (g), Kluyveromyces lactis (l), Lachancea thermotolerans (t), Saccharomyces cerevisiae (s), Zygosaccharomyces rouxii (r), Kluyveromyces waltii (w) and Saccharomyces kluyveri (k).
The _genome.tab files were used to retrieve the complete list of genes for each of the organisms and (a) (b) (c) Fig.2.
Cartoons illustrating SBF structures on three genomes.
Colors at marker level denote families of homologous units.
(a) Three SBFs; in the SBF on the left, three markers are in one-to-one homology.
(b) Three SBFs; in the SBF on the left, three markers are in one-to-many homol-ogy, including an additional homologous marker in another SBF.
(c) Three putative SBFs; as shown, the red marker violates our definition, since it has a homology statement, but that homology connects it to a marker in a different SBF, while there is no homology connecting it to any marker within its own putative SBF Fig.1.
A cartoon for SBFs among three genomes G1, G2 and G3.
The horizontal strips correspond to the genomes; small colored boxes denote markers; each SBF is framed by a dashed rectangular outline; and hom-ologous SBFs are aligned vertically and enclosed in a thin solid box.
Colored lines between horizontal strips connect markers and denote se-lected homology statements.
Shown are an SBF of three framed homolo-gous SBFs (on the left) and, using the same homology statements, an SBF of three ordinary homologous SBFs (on the right) i13 Evaluating synteny for improved comparative studies syntenic block Definition syntenic block .
syntenic block , syntenic block syntenic block Definition syntenic block syntenic block syntenic blocks ( ) syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block devise [5] .
E. K. L. S. Z. K. , S. the associated NT.fsa file was processed in order to retrieve the sequences for these genes.
Table 2 summarizes the characteris-tics of the data.
All four tools require a list of homology statementsorthology statements for OrthoCluster.
We used Fasta36 (Pearson, 1998), with a cutoff of 105, to compile hom-ology statements for each gene, reflecting common practice.
We discarded any gene for which no homology statement was pro-duced and, because Cyntenator does not scale well with large gene-family sizes, we retained only the 10 best matches (homolog candidates) for each gene.
Computational constraints imposed by the tools meant that the number of markers could not be too large; moreover, a number of tools assume that the markers are genes; thus we used genes as markers.
3.2 The tools We used the results of the DRIMM study and ran OrthoCluster, Cyntenator and i-ADHoRe on the yeast dataset.
We had chosen DRIMM because it represented a very different approach to the problem (using de Bruijn graphs) and chose the other three be-cause all are of recent design and maintained, all support multi-way comparisons and all have clear statements about their design in the respective original publications.
Unfortunately, in spite of prompt support from the developers, OrthoCluster (Zeng et al., 2008) could not run within reasonable time on our dataset with-out removing so many genes and homology statements as to invalidate the exercise, so we had to exclude it from the study.
(We ran the tool for 2 weeks on a 48-core, 256 GB Dell Poweredge 815 without results.)
We ran Cyntenator with the parameter setting used by the authors in the original article: gap=0.3, mismatches=0.3, threshold=2 and filter=10 000.
The final output depends on, in effect, a guide tree (a phylogeny of the eight species), as it is obtained by running the tool on pairs of intermediate results the tool ran well on pairs, but not so well on triples, and almost never on larger subsets of genomes.
We eventually settled on the pattern described by the tree ((r, (w, (g, (k, (c, s))))), (l, t)).
We ran i-ADHoRe in collinear mode, with the following parameters: gap size=15, cluster gap=35, q value=0.9, prob-ability cutoff=0.001, anchor points=3, gg2 heuristic, no level 2 only and FDR as multiple hypothesis correction.
3.3 The output The output of all three tools is in the form of families of hom-ologous SBFs, where each family has at most eight blocks, each belonging to one of the eight genomes under comparison.
That we get no more than eight is due to the use of genes as markers: a large fraction of the genes are singletons (have no homolog within their own genome), thereby making it highly unlikely that a particular block structure would be found repeated within the genome.
A family has fewer than eight blocks when no homologous SB in that family can be identified in a particular genome.
Figure 3 gives an overall feel for the results of the study, showing how the blocks from one tool map onto those of an-other.
A very clear mapping pattern can be observed from both Cyntenator and DRIMM to a specific, small subset of the blocks generated by i-ADHoRe, as highlighted by the dark blue section on the ring of i-ADHoRe.
The number of blocks generated by i-ADHoRe is considerably higher than those generated by Cyntenator or DRIMM, so the blocks are smaller and the (blue) links thinner.
(This kind of mapping also illustrates the lattice concept discussed earlier: the thin links bind smaller blocks to a larger block made of these smaller blocks.)
3.4 Evaluation against our definitions Our main requirement is that markers within an SB have homo-logs within each of the other SBs in the family.
As we saw, this simple constraint is unlikely to be satisfied in practice, so we Fig.3.
SBFs defined by Cyntenator (purple), i-ADHoRe (blue) and DRIMM (green), mapped to each other in terms of gene content.
Each link bears the color of the tool, the output of which is mapped through the link onto the outputs of the other tools.
There are six pairwise com-parisons between the SBFs produced by the three tools.
The thickness of a link shows the level of similarity, measured by the overlap between the gene content of two SBFs relative to the SBF being mapped.
Each sector of the diagram is an ordering by size of all blocks generated by the cor-responding tool Table 2.
Characteristics of the data from YGOB Genomes Genes/genome Homolog pairs C.glabrata 5211 106 291 E.gossypii 4725 104 817 K.lactis 5086 113 075 L.thermotolerans 5111 94 262 S.cerevisiae 6600 140 851 Z.rouxii 5006 135 707 K.waltii 10825 194 234 S.kluyveri 5340 166 835 The genes for K.waltii are often contigs with various functions (ORFs, short com-plements with intron/exon annotation), which explains their abnormally high number.
i14 C.G.Ghiurcuta and B.M.E.Moret [18]--gene , 3 , [27] , 8 Q , 3 syntenic blocks ( ) 8 8 8 8 syntenic block syntenic block syntenic block relax the transitivity requirement and measure compliance with the resulting weakened constraint.
Our first measure relates to the families of SBFs: we compute the number of SBFs that include within one of their SBs a marker with no homolog within any block of the SBF.
This count is reported in the second column of Table 3.
Since this measure tolerates failures in transitivity, the number of SBFs not in perfect compliance with our definition may be much larger.
This first measure is an absolute count, although different tools produce different numbers of SBFs; moreover, it counts an SBF as a failure no matter how many markers in that SBF fail the test.
To address the first issue, we compute the percentage of failing markers in an SBFi.e.
markers that have homologs in other SBFs, but none in their own SBF.
We use two different base counts for normalization, to reflect fundamental differences between the tools with respect to selective use of markers: the first count is the total number of markers present in the SBF as generated by the tool, denoted E(X), while the second is the total number of markers present in the genome within the coordinates of the generated blocks, denoted E(X0).
Because DRIMM and i-ADHoRe eliminate markers from within SBs (within the co-ordinates of the block), something that Cyntenator does not do, the values of E(X) for DRIMM and i-ADHoRe may be signifi-cantly smaller than those of E(X0).
Figure 4 shows that i-ADHoRe generates more, and Cyntenator fewer, blocks with a very small fraction of markers lacking any homolog within their own SBF.
DEFINITION 5.
We define two scores, the first more forgiving than the second.
Relaxed Scoring uses a pairwise view of SBFs; for each block from an SBF, it counts the number of markers in that block that have at least one homolog within the SBF and normalizes it by the total number of markers present in the SBF.
Weighted Scoring attempts to quantify the deviation from our formal definition; for each block in an SBF, we count the number of markers in that block that have at least one homolog in each of the other blocks in the SBF and normalize this result by the number of blocks (minus 1) in the SBF and again by the total number of markers present in the SBF.
A perfect weighted score is 1, yet an SBF of n blocks with a weighted score of 1/(n 1) gets a perfect relaxed score.
These scores allow us to estimate the robustness of the homology state-ments, as they show how densely interconnected the SBs are through their homology statements.
A reduction from the first score to the second indicates that the tool has removed markers (to place them in other blocks) that fell within the blockso that the block produced is not contiguous.
Figure 5 gives histograms of the two measures for our experi-ments.
Since i-ADHoRe explicitly produces non-contiguous blocks, its two scores predictably differ significantly (by a third).
Like i-ADHoRe, DRIMM ignores many markers within a block, but in most cases it does not use them else-whereinstead, it eliminates them from the list of markers it uses.
As a result, its two base counts remain very close, but its two scores are very different.
Cyntenator and DRIMM yield similar distributions in both cases, but i-ADHoRE, which scores nearly perfectly under pair-wise scoring, scores poorly under weighted scoring.
i-ADHoRe does not place much emphasis on multi-way homologies: it keeps markers in its blocks even if these markers have just one hom-ology with one other block.
In contrast, Cyntenator progres-sively eliminates markers with few homology statements, therefore yielding blocks with strongly related markers.
DRIMM has much the same behavior under both scoring schemes, but its score drops by 80% when moving from pairwise to weighted scores, due to its dropping large numbers of markers from its working list.
That DRIMM scores poorly under both schemes, however, is due to a different set of goals: as stated by the authors, DRIMM aims at maximum genome coverage and simply ignores discordant homologies and other conditions that would cause Cyntenator or i-ADHoRe to break a block.
The yeast dataset contains several genes and ORFs that over-lap.
Such overlaps are discarded by DRIMM, but not by the other two tools; consequently, Cyntenator and i-ADHoRe occa-sionally output SBs with overlapping content (see Table 3).
Although we do not require collinearity, it remains desirable because it greatly simplifies the interpretation of the blocks.
Cyntenator makes this a formal constraint; in contrast, most of the blocks produced by DRIMM and i-ADHoRe are inter-rupted intervalsbetween the leftmost marker and the rightmost one, both tools pick and choose what to keep in the block.
The last column of Table 3 indicates the number of blocks affected by this selection.
The high proportion of blocks with selected con-tent explains in part the good scoring of i-ADHoRe.
In contrast, the very high proportion of such blocks, together with the 100% rate of homology violation, in DRIMM confirm the very Fig.4.
Histogram showing the percentage of markers from an SBF that do not have any homolog in that SBF.
The percentage is computed with respect to the total number of markers present in the SBF as generated by the tool and is supplemented by the E(X)/E(X0) ratio Table 3.
Characteristics of the SBFs generated by the tools SBFs w/o homologs in the SBF Content overlap Selective content DRIMM 509 509 0 455 Cyntenator 1106 583 39 0 i-ADHoRe 8088 278 2 7247 i15 Evaluating synteny for improved comparative studies syntenic blocks ( ) syntenic block `` '' that is, syntenic block Definition syntenic block--syntenic block &percnt; syntenic block `` '' different aim driving the tool.
A related issue is the handling of interchromosomal blocks: since genomic recombinations of vari-ous types can move parts of a conserved region to a different chromosome, one has to decide whether to split the conserved region into two SBs or to keep it as a single block.
Our definition requires a split, since it assumes that each block is contained within a chromosome; DRIMM and Cyntenator do the same, but i-ADHoRe allows blocks to span multiple chromosomes.
3.5 Quantifying the features of the blocks Comparing the blocks to each other is difficult, since explicit features of the blocks have not been defined a priori for any of the tools.
We chose to focus on three features: genome coverage in terms of used markers (the one measure commonly used in the original papers), overlap of blocks for each tool and agreement among blocks in terms of marker content.
We define marker coverage as the ratio of the total number of markers present in the blocks generated by a tool to the total number of markers present in the input within the generated block boundaries.
Figure 6 illustrates (qualitatively, not quantitatively) how the blocks generated by each tool cover a certain genomic area.
Figures 3 and 6 were generated using Circos (Krzywinski et al., 2009).
The three inner rings correspond to the three tools; each genome from our dataset corresponds to a cone in the figure, as indicated by the thin, labeled color indicator enclosing the diagram.
Block boundaries are drawn in thin black lines, so that dark areas represent short marker sets, thus small blocks and highly fragmented coverage.
Uncovered areas are white.
Our definition does not preclude using overlapping SBs, since it sets conditions on one SBF at a time.
In the lattice of decom-positions into SBFs, one may then choose to impose additional conditions to select good blocks.
DRIMM produces no overlap-ping blocks, because it does not reuse markers, whereas Cyntenator and (especially) i-ADHoRe do, which allows them to flag regions with ambiguous homologies or complex evolu-tionary histories.
Figure 7 illustrates the degree to which markers are reused by Cyntenator and i-ADHoRe.
While Cyntenator just reuses a few markers and not more than twice, i-ADHoRe reuses several of them up to 10 times, as depicted by the shape of the histograms.
We compute block similarity based on marker content: the markers of an SBF as generated by each tool are viewed as a single set and we compute the ratio between the overlap of two such sets relative to each of the sets, thereby yielding an asym-metric measure and six comparisons among the three tools.
Figure 8 shows that the distribution is skewed towards small valuesmost SBFs have a small overlap with other families.
Figure 8 also explains the types of links seen in Figure 3: most of the weight of the distribution is in the 1040% region, corres-ponding to overlaps with the many small blocks produced by i-ADHoRe and thus to the thin blue links of Figure 3, while the same small blocks are also responsible for the large spike at 100%, since many will completely overlap with the larger blocks.
4 DISCUSSION AND CONCLUSIONS We presented a review of the work to date on the definition and construction of SBs, pointing out the lack of a formal definition Fig.5.
Histograms of the two scores of Definition 5, illustrating the re-finement over the simple score used in Figure 4 Fig.6.
SBFs generated by DRIMM (inside ring), Cyntenator (middle ring) and i-ADHoRe (outside ring).
Each ring segment is a yeast genome.
Dark regions include many block boundariesthese SBFs have few markerswhile white regions have no identified SBFs.
Note the many contrasting outcomes from ring to ring: where one tool breaks a region into many small blocks, another produces a single block i16 C.G.Ghiurcuta and B.M.E.Moret syntenic block , [14] ) 3 3 l syntenic block ten 3 &percnt; syntenic block of SBs as well as the lack of clear objectives for the tools designed to construct these blocks.
The latter prevents us from evaluating each tool in terms of its own performance; the former prevents us from establishing a gold standard for evaluating the quality of SBs.
To remedy this situation, we proposed a simple set of homology-based criteria that SBs should satisfy.
These criteria do not identify unique solutionswe argued that a range of so-lutions should remain, since the specifics of the application should influence the selection of good blocks.
We based our definitions on homologies, because SBs are aimed at decompos-ing a genome into conserved regions (one of the few points on which all researchers agree) and conservation is embodied in homologies.
Since evaluating the quality of a decomposition into SBs is our main short-term goal, we defined new quality measures applic-able to all decompositions into SBs and applied them to the output of several synteny tools run on a dataset of eight yeast genomes.
This evaluation revealed very different behavior, as well as some reassuring commonalities, among the tools on the same dataset.
Almost all existing synteny tools use genes as markers.
Not only does such a choice restrict the usable range of granularity, but, at least in the case of most eukaryotic genomes, it discards most of the sequence data (close to 98% in the case of the human genome).
A sequence-based approach to the identification of markers, in the style of progressiveMauve or Sibelia, makes more sense in todays data environment.
Among choices that a user should be able to make are: (i) permissible degree of overlap of blocks; (ii) acceptable percentage of dropped markers; and (iii) granularity.
In addition, since the level of confidence in markers will vary, these choices should be further refined by taking into account the contribution of each shared, dropped or included marker.
Clearly, then, the next generation of tools needs a hier-archical organization of blocks, a measure of significance for blocks based on strong connections between markers in the same SBF, and user-defined (and application-motivated) con-straints and parameters.
Conflict of Interest: none declared.
ABSTRACT Motivation: We propose an efficient method to infer combinatorial association logic networks from multiple genome-wide measurements from the same sample.
We demonstrate our method on a genetical genomics dataset, in which we search for Boolean combinations of multiple genetic loci that associate with transcript levels.
Results: Our method provably finds the global solution and is very efficient with runtimes of up to four orders of magnitude faster than the exhaustive search.
This enables permutation procedures for determining accurate false positive rates and allows selection of the most parsimonious model.
When applied to transcript levels measured in myeloid cells from 24 genotyped recombinant inbred mouse strains, we discovered that nine gene clusters are putatively modulated by a logical combination of trait loci rather than a single locus.
A literature survey supports and further elucidates one of these findings.
Due to our approach, optimal solutions for multi-locus logic models and accurate estimates of the associated false discovery rates become feasible.
Our algorithm, therefore, offers a valuable alternative to approaches employing complex, albeit suboptimal optimization strategies to identify complex models.
Availability: The MATLAB code of the prototype implementation is available on: http://bioinformatics.tudelft.nl/ or http://bioinformatics.
nki.nl/ Contact: m.j.t.reinders@tudelft.nl; l.wessels@nki.nl 1 INTRODUCTION To explain complex biological phenomena it is of vital importance to measurein the same sampleall relevant (complementary) biological variables, and to measure these at a genome-wide scale.
For this reason, many multimodal screens have been performed that have complemented transcriptional profiling with, among others, copy number variation measurements, transcription factor binding assays, methylation status profiling or genotype calls (Bystrykh, 2005; Pollack et al., 2002; Shames et al., 2006; Visel et al., 2009).
A common aim in analyzing these multimodal datasets is to find associations between the biological variables measured to infer their regulatory role.
Consider, for instance, a study in which expression profiles and genome-wide genotype data were obtained in hematopoietic cells from a panel of fully homozygous recombinant inbred mouse strains (Fig.1A).
This genetical genomics approach To whom correspondence should be addressed.
enables the determination of expression quantitative trait loci (eQTLs) characterized by strong associations between the genotype and the observed expression levels (Jansen and Nap, 2001; Schadt et al., 2003).
In the absence of a strong direct association between the genotype and gene expression, real multi-locus interactions may still be present, due to epistatic interaction (Frankel and Schork, 1996; Michaelson et al., 2009).
Such interactions may not be detectable as (marginal) direct associations between the genotype and gene expression (Fig.1B).
To alleviate this, approaches which evaluate the joint association of multiple loci and a phenotype of interest are required.
Several approaches have been proposed to attack this problem.
These approaches differ mostly regarding the way the associations are modeled and the strategy employed to solve the combinatorial optimization problem.
Some approaches (Manichaikul et al., 2009; Wongseree et al., 2009) follow what could be loosely termed a two-stage approach, where all two-locus models are first evaluated, which, in stage two, are used in a greedy search to yield multi-locus models.
Approaches employing more advanced strategies to traverse the space of possible models are represented by a genetic programming approach (Nunkesser et al., 2007) and Markov Chain Monte Carlo (MCMC) approaches associated with Bayesian analyses (Mukherjee et al., 2009; Zhang and Liu, 2007).
Since two-stage approaches have been demonstrated to be suboptimal (Evans et al., 2006) and advanced search strategies such as MCMC are very sensitive to their implementation and parameter settings, and are not guaranteed to be optimal, an approach that finds a provably global solution to a selected model within reasonable time is highly desirable.
Of particular interest is the method of Ljungberg et al.
(2004) which is used for the pair-scan analysis that is available on the GeneNetwork on http://genenetwork.org.
Ljungberg et al.
(2004) stress the importance of performing a global search rather than relying on greedy searches by (pre)selecting markers based on their marginal effects.
To deal with the computational complexity associated with such an optimization problem, the authors present a method to find global optima of a linear regression problem for up to three predictors that is fast enough to be employed in permutation procedures.
In contrast to the class of additive models employed by Ljungberg et al.
(2004) (and many other approaches), we follow others (Kooperberg and Ruczinski, 2004; Mukherjee et al., 2009; Nunkesser et al., 2007) and employ Boolean combinatorial logic to explicitly incorporate interactions in the eQTL inference.
To this end, we infer combinatorial association logic (CAL) networks that combine the observed genotypes through and (), or () and xor The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i150 i149i157 J.de Ridder et al.
A B C Fig.1.
Schematic overview of data and association inference.
(A) A panel of BXD mice that is densely genotyped and expression profiled.
The genotype data can be considered as binary vectors by choosing a binary encoding of the alleles (in the figure D=true and B=false) and putting thresholds that divide the genome into loci such that each locus differs in at least one element from its neighbors.
The cartoon shows that good association is obtained between Locus 5 and Gene 7 because elevated expression is consistently observed in conjunction with the D allele of Locus 5.
(B) Interaction among genetic features may destroy direct associations between individual loci and genes.
The cartoon shows that configurations exist in which the gene expression can only be predicted by considering two loci simultaneously (using Boolean xor logic).
(C) By inferring CAL networks, interaction among genetic features is taken into account in the association inference.
Inferring CAL networks is achieved by selecting the input loci with the selection function S and combining these with the appropriate Boolean function B, such that the association (as measured by a scoring function f ) between the network output and the gene of interest is maximized.
() functions by searching for associations between the result of the Boolean operation and the gene expression.
The Boolean and function can be used if altered expression is consistently observed in combination with a particular combination of two alleles (which do not necessarily have to be equal), but remains unchanged in all other genotype configurations.
An example of a situation in which this may be observed is the case of two parallel pathways that only promote transcription of their downstream target when the genes in these pathways have specific alleles.
Conversely, we may also consistently observe differential transcription in the strains for which either one of two loci is of a certain genotype.
This may, for instance, be observed in case of a cascaded signaling pathway: a silencing mutation in one of the alleles can repress the entire pathway, regardless of which gene in the cascade contained this mutation.
Boolean or () and xor () are capable of capturing this behavior (Fig.1B).
Like the search for optimal predictors in the additive model, inferring optimal predictors of a Boolean function is a challenging computational problem, especially considering that more complex combinations of these functions are also possible.
Moreover, we noted that the objective function that needs to be optimized is highly discontinuous and nonlinear so that standard optimization techniques, such as genetic algorithms, simulated annealing and MCMC do not provide an optimal solution.
Nevertheless, an efficient andmost importantlyglobal solution is highly desirable, since this allows permutation procedures with which significance estimates of the discovered associations can be realized (Ljungberg et al., 2004).
In the following, we will mathematically prove that, under reasonable conditions, CAL network inference provides an efficient way to obtain globally optimal multi-locus models that associate multiple genomic loci with the expression of target genes.
We illustrate our approach on the genetical genomics dataset from Gerrits et al.
2009, and using these data show that 100% accuracy is achieved at runtimes that are a fraction of those required for exhaustive search.
Furthermore, we observe that using this approach complex associations are revealed that otherwise would have gone unnoticed.
As such, our approach offers a useful alternative to the commonly used additive models and suboptimal search strategies.
2 METHODS 2.1 CAL network search The construction of a CAL network that predicts the expression profile from a set of binary predictors can be formulated as an optimization problem.
Interesting logic networks are those for which maximal association between the network output and the gene expression is obtained.
Let g be the (T 1) vector, with T the number of samples, containing the expression values of a gene and L the (T L) matrix of binary predictors, e.g.
the genotypes, where L is the number of predictors.
A CAL network L is defined in terms of S(L;n) :BL BN , a selection function that selects N columns from L and B(I) :BN B, a Boolean logic function that specifies the network topology.
In the latter, (T N) matrix I is a concatenation of the columns selected by S, i.e.
I= (in(1),...,in(N)), where n is a (N 1) vector containing the indices of the selected columns.
Consequently, CAL network L maps the genotype matrix L to a (T 1) output vector y as follows: y=L(L;B,n)=B(S (L;n)).
(1) The association between g and y is quantified with an association measure f (g,y) f (g,y)=  |x0x1| (n01)s20+(n11)s21 n0+n12 ( 1 n0 + 1n1 ) if (n0 >)(n1 >) 0 otherwise.
(2) For notational convenience, we used x0 ={g() :y()=0, (1,...,T )} and x1 ={g() :y()=1, (1,...,T )}, i.e.
vector g is split into x0 and x1 according to the Boolean values in y.
Furthermore, x0 (x1), s20 (s 2 1) and n0 (n1) are defined as the sample mean, the sample variance and the number of elements in x0 (x1), respectively.
Note that Equation (2) is equal to the absolute value of the t-statistic, except when n0 or n1 becomes too small, which ensures high f-values are only obtained in Case x0 and x1 have at least elements.
The inference of CAL networks is a computationally challenging problem.
Primarily, because the feature selection problem, i.e.
finding the optimal vector n, critically depends on the number of features that are considered.
i150 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i151 i149i157 Combinatorial association logic networks In the case of genetic markers, this easily runs in the several hundreds to thousands.
Moreover, the optimal subset of markers is heavily dependent on how these markers are combined, i.e.
dependent on the optimal Boolean function B.All together, one frequently has to rely on greedy search strategies that easily get stuck in local optima or near exhaustive searches that are computationally too expensive, especially when employed in permutation procedures required to assess statistical significance.
Our solution to this problem hinges upon two observations.
First, in most practical datasets the sample size is relatively small, especially when compared to the number of features.
This means that we can limit ourselves to considering only small CAL networks with few inputs, since larger networks are prone to overfitting, which makes them less informative.
For this reason, and because most networks have many equivalent topologies that do not need to be evaluated due to symmetry, the set containing all unique and meaningful network topologies {Bj : j=1,2,} is relatively small (in the order of 10100, depending on the desired topology).
Consequently, the set of optimal input vectors {nj : j=1,2,}, associated with each Bj , can be found by fixing Bj and maximizing for each Bj separately nj =argmax n { f ( g,Bj(S(L;n)) )} .
(3) Second, we observe that Equation (3) still represents a complex optimization problem that can be significantly simplified by employing an approximation to the association measure, denoted by f .
In the following, we show that maximizing f is equivalent to maximizing f , but the maximization of the former can be very efficiently realized by using a branch and bound search.
Before defining f , we define the Boolean vector yopt as the solution for which f reaches a global maximum independent of the network topology, i.e.
yopt =argmaxy f (g,y).
Note that yopt can be easily determined by sorting the gene expression vector g and evaluating all positions for a threshold t that splits g into x0 and x1 (Fig.2A).
For f , we use the weighted Hamming similarity between yopt and the network output y f (yopt,y)= w()I(yopt()=y()) (4) A B C Fig.2.
Association versus approximated association.
(A) Example gene expression vector (circles) split in x0 and x1 according to yopt.
The magenta line denotes the association measure f , defined in Equation (2), as a function of a threshold t that splits the expression vector in x0 and x1.
The blue triangles indicate the error weights w() that result after optimizing them.
(B and C) 500 random samples that are generated by introducing up to seven bit-flips in yopt to show the relation between f and f .
The red dot indicates f and f values for yopt.
(B) shows the samples in case the weights are assumed equal.
Although the trend of the data is monotonically increasing, a large spread around this trend is observed.
(C) shows the same samples in case the weights are optimized, resulting in a near one-to-one relation between f and f .
where w()>0 denotes the weight for sample , and I() the indicator function, evaluating to 1 if the-th element of vectors yopt and y are equal.
For an example gene expression vector, Figure 2B shows 500 random samples of (f ,f ) pairs, in case all weights are equal to one.
Although the trend of this distribution is monotonically increasing, the spread around the trend is substantial.
This is undesirable because a maximum in f is only guaranteed to correspond to a maximum in f in case there is a direct one-to-one relation between them.
Clearly, this is not the case in Figure 2B, since each value of f corresponds to many values of f .
However, by optimizing the weights such that the difference between f and f is minimal, a near one-to-one relation can be obtained, as exemplified by Figure 2C.
With the proper adjustments, detailed below, it is thus ensured that maximizing f is equivalent to maximizing f .
The major advantage of maximizing f instead of f is that in the former each sample has an independent contribution to the association measure.
This can be readily exploited using a branch and bound search, so that it is possible to avoid the expensive evaluation of the association measure.
2.2 Optimizing Equation (3) Here, we show that optimizing Equation (3) can be achieved by first determining f =maxn(f ), where f was defined in Equation (4).
After this the search for f =maxn f is readily solved by searching in the neighborhood of f .
For a single sample , let V () be the set of input combinations such that y()=yopt()nV (), where y=L(L;B,n).1 Figure 3AC shows how V () can be inferred from L and the truth table of B.
For a set of samples C, the input combinations nV (C) for which all C reach the optimal output yopt are found by taking the intersection of all the individual sets of input combinations, i.e.
V (C) =C V ().
Note that, under the assumption that each sample has at least one non-zero locus, V () 	=.
In other words, for individual samples there always exists a combination of inputs for which the network can reach the desired optimal output yopt.
However, for an arbitrary combination of samples this is clearly not the case.
If we observe that V (C) =, this means that for the collection of samples in C there does not exist a valid combination of inputs.
Moreover, if V (C) =, all supersets of C will also result in the empty set.
Finally we note that, by choosing a convenient binary encoding, V () and V (C) can be computed very efficiently by means of bitwise xnor and and operations, respectively (see Fig.3D and the Supplementary Fig.S1 for details).
With these definitions in mind, we propose the following lemma: Lemma 1. f =max C C w() subject to: V (C) 	= (5) Proof.
Let C =argmaxC C w(), i.e.
C is the set of solutions for which f is obtained.
Since it is required that V (C) 	=, there must be at least one solution n such that yopt()=y() C. Since for C the optimum in f is obtained, it must also hold that yopt() 	=y()/C.
This means that Equation (4) can be rewritten as follows:  w()I(yopt()=y())= C w(), proving the statement in this lemma.
As argued by Lemma 1, Equation 4 is thus maximized by having as many samples in C as possible, while taking into account their respective weights w().
Before we will show that Equation (5) fits a branch and bound framework, we first make the observation that for the relation between f and f the following holds: (f (yopt,y1)< f (yopt,y2)) (f (g,y1)< f (g,y2)), (6) 1Since we optimize Equation 3 for each Bj separately, we omit its subscript if its meaning is inconsequential.
i151 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i152 i149i157 J.de Ridder et al.
A B C D Fig.3.
Computation of solution sets for each sample.
(A) Example data from Figure 1A.
(B) The topology and the truth table of the Boolean function B under investigation.
(C) Explanation by example of the calculation of V (), the set of all possible input combinations to B such that yopt()=y().
This panel shows how V (1) is determined.
Since yopt(1)=1, the rows from the truth table for which y=1 are applicable, i.e, r ={2,4,6,7}.
According to r =2, the desired output for =1 is obtained by selecting any of the loci that are 0 for inputs i1 and i2, and loci that are 1 for input i3.
Accordingly, for i1 we may select from the set: {l1,l2,l4}.
This can be efficiently calculated by taking the xnor (evaluates to 1 when both inputs are equal) between row =1 from the data matrix and the row r =2 from the truth table, as shown in (C).
Observe that the result is an efficient encoding of all the possible input combinations that satisfy yopt(1) while using r =2 from the truth table.
In general, we denote this set by V ()r , and its binary encoding by V ()r .
To determine the complete set of valid input combinations for =1, rows 4, 6 and 7 need to be considered in a similar fashion.
V (1) is now determined by taking the union of the subsets, i.e.
V (1) =V (1)2 V (1)4 V (1)6 V (1)7 , which, in binary form, may be represented by a concatenation of V (1)2 , V (1)4 , V (1)6 and V (1)7 .
(D) This panel shows the valid input combinations for =1 and =3 in binary representation (i.e.
V (1) and V (3)).
For any set of samples C the input combinations for which the output equals yopt can be obtained by taking the intersection of the individual sets.
In binary representation, this is equivalent to taking the row-wise cartesian product (row-wise product of all combinations of rows), as is shown in the panel.
where y1 and y2 are two Boolean vectors.
Note that, for =0, Equation (6) reduces to the requirement for strict monotonicity, and that for larger >0 this requirement is increasingly relaxed.
Even though this seems trivial, the value of this relation becomes clear by considering that if there exists a strong positive correlation between f and f , there may in fact exist a small for which Equation (6) is true.
Based on Lemma 1 and Equation (6), we observe that solutions that are suboptimal in terms of f may still be optimal in terms of f , since can be non-zero.
In the following, let {yi : i=1,2,} and {Ci : i=1,2,} be all the network outputs and the sample sets for the solutions for which holds that f  f (yopt,yi) f *, respectively.
Finally, let be chosen such that Equation (6) holds.
Our main theorem can now be formulated as follows: Theorem 2. n  Ci V (Ci) (7) Proof.
First, assume that Equation (6) holds for =0, and thus f (yopt,yi)= f i.
Furthermore, from Equation (6) it follows that in this case there exists a direct one-to-one relation between f and f .
Consequently, a maximum in f is guaranteed to correspond to a maximum in f and V (Ci) must contain n. This is true because from Lemma 1 it follows that V (Ci) 	=.
For non-zero values of , the one-to-one relation does not hold.
However, from Equation (6), it follows that all values of f for which the corresponding f lies outside the interval [f , f ] are strictly smaller than the value of f corresponding to f .
Thus, it must be the case that the maximum of f is constrained to solutions for which f lies in the interval [f , f ].
Therefore, the union of the sets of solutions that lie in this interval will contain n. From Theorem 2 it naturally follows that: Corollary 3. n =argmax n f (g,L(L,B,n))nV (Q), (8) where V (Q) =Ci V (Ci).
Notably, if there exists a small for which Equation (6) holds, the number of solutions in V (Q) is limited, and hence n is easily determined by an exhaustive search over all possible solutions in V (Q).
In the following, we show that in practice the set V (Q) is small by choosing w such that is small.
2.2.1 Estimating the weights Ideally, vector w is chosen such that is minimal.
For practical purposes, it is sufficient to choose w so that is small, which can be realized by minimizing the difference between f and f .
For this purpose, we sample the (f ,f ) relation by generating N random instances yn by introducing up to m random bit-flips in yopt (shown in Fig.2B and C).
The N corresponding association measures fn and Hamming similarities are collected in vector f =[f (g,y1),f (g,y2),]T and matrix F=[(yopty1)T ,(yopt y2)T ,]T , respectively.
In the latter, denotes the xnor operation, which evaluates to 1 in case its arguments are equal.
Notably, m (the number of bit-flips) should be chosen such that the region of interest of the distribution of f is sampled.
Since we are interested only in network outputs that associate well with the gene expression, we can choose m rather small to focus only on the right tail for which a good fit between f and f is obtained.
We found that smaller residuals were obtained by converting log-transformed f-values to z-scores, i.e.
f =z(lnf).
Furthermore, to deal with the intercept, the matrix F is mean centered, denoted by F. Using the vector f and matrix F we can find the weights w by constraint linear least squares minimization w=argmin w ||f Fw||2, subject to: w()w (9) where w > 0 is a small scalar that ensures each sample receives a non-zero weight.
Figure 2 illustrates a typical example showing that the trend of the relation is monotonically increasing, and the spread around the trend is marginal, indicating that Equation (6) indeed holds for a small .
2.2.2 Estimating The parameter can be estimated by randomly resampling the (f ,f ) relation using the obtained weights and measuring the spread around the trend in the data in the f direction (Fig.2C illustrates this schematically).
To this end, lowess smoothing was performed to obtain the the trend in the data (Cleveland, 1979).
Subsequently, the spread around i152 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i153 i149i157 Combinatorial association logic networks this trend was obtained by applying a sliding window in the f direction and defining as the maximum spread across all window positions.
2.2.3 Branch and bound search tree Equation (5) naturally fits a branch and bound framework with a backtracking search tree, in which each node corresponds to a particular set of samples C (shown in Supplementary Fig.S2).
Although this tree exhaustively represents all possible sample sets C, the search is very efficient since most nodes can be pruned from the search tree.
First of all, if V (C) becomes equal to the empty set, all child nodes of node C can be discarded because these will also result in the empty set.
Second, as a result of the search tree topology, for each node C we can define an upper bound f (C)up and lower bound f (C) low.
The upper bound f (C) up is defined as the value of f that would be obtained assuming all its subnodes do not result in the empty set (best case scenario) f (C)up = C w()+ Csub w(), (10) where Csub denotes the collection of all samples in the subnodes of C. The lower bound f (C)low is defined as the value of f that would be obtained assuming all subnodes will result in the empty set (worst-case scenario) f (C)low = C w().
(11) A vast reduction of the search space is realized by considering the following branch and bound principle: any node C can be pruned if there exists a node C , for which the following is true: f (C)up < f (C) low , under the condition: V (C) 	= (12) Thus, if we encountered a branch whose worst-case error is better than the best-case error of another branch, we can safely discard the latter.
After the complete search tree is traversed, the set V (Q) is determined by the union of all the nodes that resulted in a non-empty V (C).
In Equation (12), the parameter is included to ensure that set V (Q) includes n (Theorem 2).
An optimal leaf ordering is obtained when the samples are sorted based on their weight w().
This ensures that f (C)up decreases as quickly as possible, in effect pruning the tree early in the search.
Also, note that most V (C) will contain many duplicates when symmetries in the topology of B are considered.
By filtering these from V (C) before evaluating the succeeding node results in an additional search speedup.
2.2.4 Tolerance level A final, yet influential, search-space reduction is achieved by only considering solutions for which a certain minimum level of association is achieved.
This is realized by enforcing that f low can never be below a user defined tolerance level.
In other words, for this bounded f low, we can write: f low =max(f tol, f low).
As a result, branches for which f low f tol can be pruned even before the search is started.
The search procedure is explained by example in Supplementary Figure S2.
2.2.5 Estimating the false discovery rate Because our primary interest lies with the interpretation of the selected genotype markers and combinatorial logic, it is of critical importance to assess frequency of false positives among the networks called significant.
Due to the efficiency of the proposed method, it is possible to employ a permutation procedure to obtain a null-distribution for each Bj .
From this distribution, it is possible to estimate the false discovery rate (FDR) and the associated q-values by using the method proposed in Storey and Tibshirani (2003).
Not surprisingly, in many cases, multiple network topologies yield significant associations with the same gene.
The q-values, available for each of the solutions, provide a convenient way of performing selection of the most parsimonious model by accepting only the topology for which the q-value is minimal.
3 RESULTS 3.1 Genetical genomics dataset The genetical genomics dataset used to demonstrate our method contains genome-wide RNA transcript measurements performed on four related hematopoietic cell populations (Gerrits et al., 2009).
These were isolated from the bone marrow of 25 BXD recombinant inbred mouse strains that were derived by crossing C57BL/6J (B6) and DBA/2J (D2) (Peirce et al., 2004).
A typical analysis of these data includes determining eQTLs, i.e.
regions in the genome for which the genotype across strains associates well with RNA transcript levels.
We inferred associations only for the myeloid cell population, as for this cell type data for the largest number (T =24) of unique BXD strains were available.
The expression data were preprocessed as described in the Supplementary Methods.
Because the CAL networks inferred for highly correlated genes are equivalent, rather than starting the optimization for each gene separately, we constructed gene clusters and searched for CAL networks for the centroids of each gene cluster.
To ensure only tightly correlated probes were clustered, we employed a stringent cutoff (correlation distance cutoff 0.2).
This resulted in 6139 clusters that were used to determine eQTLs.
Genotype information for the strains was retrieved from The GeneNetwork (http://www.genenetwork.org/dbdoc/BXDGeno .html).
Genotype markers that were highly similar across strains and on the same chromosome were also grouped into clusters to prevent the algorithm from finding many combinations of genotype markers that are equivalent (such as the markers in linkage disequilibrium).
This resulted in 453 marker clusters (L=453).
The cluster centroids were defined as the majority vote of the individual markers in the cluster and were used as putative inputs to the network (see also the Supplementary Methods and Supplementary Figs.
S3 and S4).
For setting the tolerance level ftol no straightforward method exists.
Preferably, the tolerance level is set close to the final significance threshold to minimize the effort spent on finding optima for gene clusters that can never be significant.
We settled for a tolerance level equal to the 75th percentile of the f opt distribution (ftol =7.6), obtained by computing the f-values associated with each yopt.
Gene clusters for which the maximum f-score is below this tolerance level (i.e.
in case f opt < ftol) were not included in the CAL network search, to result in a set of 1525 high-potential gene clusters.
3.2 Algorithm performance From the methods section it follows that, under the condition that an appropriate value for is found, our algorithm produces an optimal solution.
We empirically validate this claim by comparing solutions of the proposed algorithm with the global optimum obtained with an exhaustive search.
To ensure realistic conditions, we do this using the real data described above.
For each gene expression vector, we performed our CAL network search as described with seven network topologies containing and, or and xor logic as well as a more complex combination of these Boolean functions.
A rather low tolerance level (ftol =4) was used, which turned out to capture most of the solution-space (>80% for all topologies).
The solutions obtained were compared with the optimal solutions determined by means of an exhaustive search for the same seven Boolean logic functions using Grid computing i153 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i154 i149i157 J.de Ridder et al.
A B C D Fig.4.
Algorithm performance in terms of accuracy and runtime under various conditions.
(A) Bargraph displaying accuracy for different network topologies and different values of the f-score.
For each of the network topologies the 75th percentile of the solution distribution is also given, showing that for solutions in the tail 100% accuracy is obtained.
For the two missing bars in the 4-6 and 5-6 bins no solutions were found.
(B) and C) Runtimes for different network topologies and dataset sizes.
The horizontal lines reflect runtimes for exhaustive search.
From bottom to top these represent the runtimes for: a single input network, two input network and three input network with one, two and four times the number of predictors, respectively.
facilities.
The accuracy is expressed as the percentage of times that the algorithm finds the same solution as the exhaustive search.
Figure 4A shows the resulting accuracy.
We observe that for solutions with f-scores between 5 and 6 already >95% accuracy is achieved, while for solutions with f-scores of 6, virtually 100% accuracy is achieved for each topology.
For comparison, Figure 4A also gives the 75th percentiles of the solution distributions for each topology.
Because solutions of interest (putatively significant solutions) are required to have f-scores substantially higher than the 75th percentile, we can conclude that our method achieves 100% accuracy for a reasonable operating range (solutions with f-scores between 4 and 5where the accuracy is below 95%are well the 75th percentile for all networks).
While comparing our method to the method presented in Mukherjee et al.
(2009), using simulated gene expression vectors and a predetermined random network (ground truth), we found that our method reaches higher true positive rates (see Supplementary Material).
These results illustrate the benefit of searching for solutions for each of the network topologies separately, and employing a significance estimate to enforce parsimony.
Obtaining the same accuracy as an exhaustive search is only useful if this is achieved for runtimes that are substantially lower.
To asses this, we randomly selected 200 gene expression vectors from the 1525 gene clusters and measured runtimes for both our CAL network search as well as the exhaustive search.
Figure 4BD shows these runtimes for a range of conditions.
The boxplots represent the results obtained with the CAL network search and the horizontal lines the runtimes for the exhaustive search.
Figure 4B compares runtimes for different network topologies.
Clearly, the branch and bound algorithm significantly outperforms the exhaustive search under all experimental conditions with differences in runtime of up to four orders of magnitude.
For the three input networks in particular, the runtime required for exhaustive search (>5 h per gene per network) prohibits any further permutation procedures.
The CAL network search, on the other hand, is able to find the solution in a matter of seconds, thereby enabling the large number of permutations required to obtain reliable significance estimates.
Compared to the variance in runtime of the exhaustive search, which was negligible, the variance of the CAL network search is quite high.
This is expected as our CAL network search finishes rapidly when a good solution presents itself early in the search, while more time is needed to conclude that no acceptable solution is present.
For a similar reason, the more complex networks, those containing xor logic, have higher median runtimes.
On no occasion, however, does this increase runtimes >100 s for any of the networks.
To evaluate performance as a function for dataset size we artificially increased the number of predictors and the number of samples (Fig.4C).
In addition, runtimes for different tolerance levels were examined (Fig.4D).
The number of predictors was increased by horizontally concatenating the original matrix L with copies of L containing 10% random bit-flips.
The sample size was increased by vertically concatenating matrix L as well as all gene expression vectors g with copies of L and g, respectively.
In case of the latter, normally distributed noise was added to the copies with noise =0.1g.
We observe that for both the exhaustive search as well as the CAL network search runtimes increase substantially as the number of predictors increase.
In case of the CAL network search, this is explained by the fact that many very good solutions are present due to the increased imbalance between the number of predictors and the sample size.
It is expected, yet not quantitatively established, that better performance is observed when this balance is restored.
The increase in runtime as a result of an increased number of samples is moderate, with a median runtime considerably lower than an exhaustive search for only two input networks.
Likewise, increasing the tolerance level only moderately speeds up the CAL network search, demonstrating that runtime is robust for the setting of this parameter.
3.3 Combinatorial eQTLs We performed the CAL network search for the set of 1525 high-potential gene clusters.
The complete search (e.g.
for all gene clusters and all topologies) was repeated 100 times using a permuted version of the gene-expression vectors.
For each topology, this resulted in a null-distribution containing 152 500 values, which was used to estimate q-values for each of the resulting solutions.
We considered network topologies with a maximum of three inputs listed in Supplementary Figure S5.
Notably, we included two single-input networks to account for direct positive and negative association, respectively, which is equivalent to positive association with the i154 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i155 i149i157 Combinatorial association logic networks A B C Fig.5.
(A) Bargraph with an overview of the number of gene clusters for which a significant (10% FDR) solution is found.
Network topologies are sorted according to the 10% FDR level (blue line).
(B) CAL networks significant at 10% FDR.
The color and shape of the symbols correspond to the symbols used in (C).
Small circles at the inputs of the networks denote negation, i.e.
for these inputs the mapping from allele to binary representation is switched.
We also indicate whether the best single marker coincides, for that gene cluster, with one of the inputs of the CAL network.
(C) Marker/probe-plot for the top CAL networks showing both the eQTLs (blue crosses) and ceQTLs (sets of colored symbols of various shapes).
The colors and shapes of the markers refer to the network topologies listed in (B).
Horizontal gray lines connect the inputs and the output of the CAL network.
Because probes were clustered, it occurs that the ceQTLs map to multiple probes in case these probes were part of the same cluster.
The numeric labels near the the colored symbols correspond to the input of the network.
Notably, some probes seem to be predicted by more ceQTLs than there are inputs to the CAL network reported.
This occurs when there are multiple combinations of markers that show the same association with the gene expression level of the network output, and can be explained by similarity among markers.
The cis-band (diagonal) is clearly visible, and in one occasion contains a ceQTL.
Overlap among ceQTLs from different networks is marked by red dashed lines, overlap between ceQTLs and eQTLs by black dashed lines.
D2 and B6 allele, respectively.
This ensures that the algorithm has the option of choosing the least complex model in case an eQTL is capable of explaining a significant portion of the variance in the expression of the gene cluster.
Figure 5A gives an overview of the number of gene clusters for which the output of a CAL network significantly (at the 10% FDR level) associated with its expression (red bars).
To obtain additional confidence in the significance threshold, we calculated q-values for 10 additional permutations of the whole dataset.
For none of the network topologies did the mean number of significant gene clusters across the 10 permutations exceed 0.6, indicating that the expected number of false discoveries is conservatively kept under control.
The yellow bars indicate the number of significant gene clusters after model size selection based on the q-value as detailed in Section 2.
It appears that most of the gene clusters for which association is observed can be explained by one of the single input networks.
For nine gene clusters (corresponding to 17 genes), however, a CAL network was capable of explaining significantly more of the variance than one of the single input networks or any one of the other CAL networks.
The network topologies, q-values and association scores of the significant CAL networks are given in Figure 5B.
Not surprisingly, for all gene clusters at the output of these networks, the combination of loci is vastly superior in explaining the variance in expression over any of the markers in isolation.
Interestingly, many of these genomic regions would have been missed, as in seven of the networks the best markers do not coincide with one of the inputs of the CAL network.
The sets of markers that were found as the optimal inputs for the seven topologies were mapped onto the genome.
Combinatorial eQTLs (ceQTL) were then defined as stretches of consecutive markers.
A genome map of the (c)eQTLs is given in Figure 5C, showing the eQTLs (red and blue crosses for positive and negative association, respectively) and ceQTLs (colored symbols) on the x-axis versus the genomic positions of the probes measuring expression on the y-axis.
The numbers near the ceQTL symbols correspond to the inputs of the CAL networks depicted in Figure 5B.
Before we zoom in on one of the CAL networks in more detail, some general observations can be made.
In particular, we note that in some cases overlap exists among the markers selected at the i155 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i156 i149i157 J.de Ridder et al.
Fig.6.
Input regions of the CAL network for Lilrb4 The line graphs give the f-score for association between the output gene and the individual markers (blue) and the network output (red).
The latter was computed by taking the maximum f-score of the network using the marker under evaluation for one input and any of the other markers for the second input of the network.
Where possible the IDs of the genetic markers are given, but some were omitted for readability.
The dot plots gives the expression values separated by network output (right) and the best markers in the inputs (left).
Finally, for one particular combination of markers the genotype for all strains is depicted as a Boolean heat map.
In these diagrams, the not gates were already incorporated.
inputs of the CAL networks and between other network inputs and eQTLs.
In seven instances, the identified ceQTLs coincide with eQTLs (connected by black dashed lines in the figure).
Some of these eQTLs are located in cis.
The finding of CAL networks that share one of their inputs (ceQTLs) with an eQTL suggests that the local genotype associated with the eQTL is involved in the regulation of a local gene (cis-regulation), but in addition collaborates with the other CAL input locus/loci to regulate the CAL network output gene(s).
Furthermore, two of the CAL networks (ranked sixth and ninth) share a ceQTL between the inputs (connected by red dashed lines).
It is not inconceivable that a gene present in this ceQTL is indeed involved in the regulation of the target genes of both networks, but that the interaction partners through which this regulation is established differs for both target genes.
Among the list of output genes of the nine most significant CAL networks is Lilrb4 (ranked third).
Lilrb4 encodes a leukocyte immunoglobulin-like receptor which is expressed on the surface of mast cells, neutrophils, and macrophages.
It plays a key role in counter-regulating the inflammatory response to prevent pathologic excessive inflammation (reviewed in Katz, 2007).
Figure 6 shows small regions around the ceQTLs that were selected as inputs for the CAL network of Lilrb4.
For each region, the association was measured between the expression of Lilrb4 and the individual markers (blue lines).
The red lines, on the other hand, give the association score for the network output.
Clearly, the association between the logical combination of inputs and the expression of Lilrb4 is markedly higher than considering any of the markers in isolation.
The regions for which the red curves reach their maximum correspond to the ceQTLs.
The Boolean heat map, displayed at the bottom of Figure 6, outlines the genotype of one particular combination of genetic markers in the ceQTLs across the BXD mouse strains.
The bottom two rows of this heat map give the optimal network output and predicted output, respectively.
For the Lilrb4 network the optimal network output is exactly recapitulated by the CAL network.
For Lilrb4 elevated expression is exclusively observed in case of B6 alleles in both the ceQTL regions of Chromosomes 7 and 19.
To focus our attention to the most interesting genes in the ceQTLs we performed a literature search using Ingenuity pathway analysis (IngenuitySystems, www.ingenuity.com).
Interestingly, we found a substantial number of interactions between genes localized in the ceQTLs and Lilrb4.
For example, the literature search revealed a link between Apba1 (located in the ceQTL region on Chromosome 19) and Lilrb4.
Both protein products have been described to bind ITGB3 (Calderwood et al., 2003; Castells et al., 2001).
In addition, the search revealed a link between Psenen (Chromosome 7 ceQTL) and Apba1 (Chromosome 19 ceQTL).
Both protein products have been described to bind PSEN1 and PSEN2 (Biederer et al., 2002; Steiner et al., 2002).
While literature is able to link the genes in the ceQTLs to Lilrb4 and thereby gives the first clues as to how the expression of Lilrb4 may be regulated, we do not exclude that other interactions (not yet represented in literature) exist.
In any case, the result of our method should provide a set of testable hypotheses that can be validated in the laboratory.
4 DISCUSSION Unravelling (transcriptional) regulatory networks by inferring complex associations, for instance, between genotype and gene expression, necessitates algorithms that take into account possible (allele-specific) interactions.
For this purpose, we have proposed a method to efficiently infer CAL networks, i.e.
small logic networks in which allele-specific interactions are modeled by Boolean functions.
To find the best possible fit of the model given the data, a computationally challenging optimization problem had to be solved.
i156 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i157 i149i157 Combinatorial association logic networks This was achieved by rewriting the optimization such that it could be effectively solved by a customized branch and bound algorithm.
Proof and empirical evidence for optimality of the solution, under appropriate conditions, was given.
At the same time, differences in runtimes of up to four orders of magnitude were observed when compared to exhaustive search.
Because the CAL network search is able to find the optimal solution in a matter of seconds a permutation procedure becomes feasible, which can be employed to obtain estimates of the FDR.
This is a major advantage as the resulting q-values allow selection of the most parsimonious model and enable ranking the network topologies in terms of their complexity.
We demonstrated our algorithm on a genetical genomics dataset, and found that, from the 1525 gene clusters (2913 genes) that resulted after selection of high potential genes, 9 gene clusters (17 genes) were significantly associated (at 10% FDR level) through a logical combination of genomic loci rather than a single eQTL.
Notably, without incorporating the complex interactions, these associations would have gone unnoticed.
Many of the discovered input regions were found to overlap eQTLs or were shared inputs of CAL networks explaining the expression of other genes, suggesting that these regions, indeed, are involved in transcriptional regulation.
ACKNOWLEDGMENTS The authors thank Daoud Sie and Leonid V. Bystrykh for their valuable input.
Funding: This work was part of the BioRange programme of the Netherlands Bioinformatics Centre (NBIC), which is supported by a BSIK grant through the Netherlands Genomics Initiative (NGI); Dutch Life Science Grid initiative of NBIC and the Dutch e-Science Grid BiG Grid, SARA-High Performance Computing and Visualisation; Netherlands Genomics Initiative (Horizon, 050-71-055); Dutch Cancer Society (RUG2007-3729); Netherlands Organization for Scientific Research (NWO) (VICI to G.d.H., 918-76-601); European Community (EuroSystem, 200720).
Conflict of Interest: none declared.
Abstract The advent of induced pluripotent stem cells (iPSCs) has revolutionized the concept of cellular reprogramming and potentially will solve the immunological compatibility issues that have so far hindered the application of human pluripotent stem cells in regenerative medicine.
Recent findings showed that pluripotency is defined by a state of balanced lineage potency, which can be artificially instated through various procedures, including the conventional Yamanaka strategy.
As a type of pluripotent stem cell, iPSCs are subject to the usual concerns over purity of differen-tiated derivatives and risks of tumor formation when used for cell-based therapy, though they pro-vide certain advantages in translational research, especially in the areas of personalized medicine, disease modeling and drug screening.
iPSC-based technology, human embryonic stem cells (hESCs) and direct lineage conversion each will play distinct roles in specific aspects of translational medi-cine, and continue yielding surprises for scientists and the public.
Introduction The reversion of differentiated cells to a state of pluripotency is a fascinating idea that has long been explored in cell biology, yet reversion to pluripotency simply through the over-expres-sion of a set of pluripotency-associated factors in somatic cells appeared to be impossible before Yamanaka and his col-leagues successfully reprogrammed mouse fibroblasts to plu-ripotent stem cells, the so-called induced pluripotent stem cells (iPSCs).
These cells exhibit the morphology and growth eijing Institute of Genomics, tics Society of China.
g by Elsevier jing Institute of Genomics, Chinese A properties of embryonic stem cells (ESCs) and express endog-enous ESC markers, such as Oct4 and Nanog [1].
This land-mark breakthrough quickly evoked the enthusiasm of both scientists and the public toward stem cells because of their far-reaching scientific value and numerous potential applica-tions.
In this review, we summarize recent advances in the field of reprogramming and iPSCs, in particular the new conceptual framework of cell fate determination and its potential applica-tions in translational research.
From somatic cell nuclear transfer to iPSCs Induced pluripotency has been studied for a very long time.
In the 1950s, Briggs and King established the technique of so-matic cell nuclear transfer (SCNT), or cloning, by trans-planting isolated nuclei into enucleated oocytes [2,3].
Using this system, they successfully cloned tadpoles from cell nuclei of late-stage embryos and tadpoles.
In the early 1960s, John Gurdon transplanted nuclei of adult frog intestinal cells into cademy of Sciences and Genetics Society of China.
Production and hosting mailto:byhu@ioz.ac.cnWu M et al/ Cell Fate Change Beyond iPSCs 289 unfertilized eggs and generated tadpoles [4,5].
Despite the pio-neering success of SCNT in the amphibian, it was not until the late 1990s that Ian Wilmut and colleagues cloned the first mammal, Dolly the sheep [6].
This work demonstrated that dif-ferentiated somatic cells indeed retain the genetic information that is necessary for the generation of a multicellular organism, and during development, reversible epigenetic rather than irre-versible genetic changes are imposed on the genome, most pos-sibly by factors in the oocytes.
Another remarkable breakthrough in life science accompa-nying SCNT was the derivation of ESCs from the inner cell mass (ICM) of mouse and human blastocysts [79].
In an opti-mal culture condition that enables the long-term maintenance of pluripotency, ESCs, the in vitro counterparts of ICM cells, can be propagated indefinitely [10].
This has allowed the in-depth dissection of pluripotency circuitry and identification of the master pluripotency genes Oct4, Nanog and Sox2, which have been employed to generate iPSCs.
Just as ESCs, the properties of differentiated cell lineages are determined by master genes necessary for establishing and maintaining cellular identity.
Products of these master genes drive the expression of cell type-specific genes while suppressing lineage-unrelated genes.
Ectopic expression of these master genes can induce a cell fate change.
In Dro-sophila, ectopic expression of the transcription factor Anten-napedia in the head region results in the formation of legs instead of antennae.
Overexpression of skeletal muscle deter-minant gene MyoD in the mouse fibroblasts results in the formation of myocytes [11,12].
Upon overexpression of the myeloid transcription factor C/EBPa, primary B and T cells efficiently convert to functional macrophages in mice [13,14].
These findings suggest that transcription factors play key roles in cell fate determination, and that ectopic expression of such factors can switch the cell fates of differentiated cells.
Yamanaka and Takahashi devised a screening system that could activate a dormant drug resistance allele that was inte-grated into the ESC-specific Fbxo15 locus and selected from a pool of 24 candidate pluripotency-associated genes.
They found that only four of the factors, Oct3/4 (also known as Pou5f1), Sox2, Klf4 and c-Myc, were needed to generate ESC-like colonies from fibroblasts of both embryonic and adult mice.
They termed these reprogrammed cells induced pluripotent stem (iPS) cells [1].
However, it was later demon-strated by Yamanakas group and other investigators that these iPSCs were not fully reprogrammed, since iPSCs selected through this approach failed to produce adult chimaeras.
It was soon recognized that Fbxo15 was not an ideal selection gene, and thus later, with the use of Nanog or Pou5f1 instead of Fbxo15 for selection, germline-competent iPSCs very similar to ESCs were generated in multiple labs [1517].
A few years after iPSCs were initially developed, the last skeptics were fi-nally convinced by a stringent verification of iPSC pluripoten-cy: individual iPSCs were able to generate viable mice in a tetraploid compensation assay [18].
At the same time, human iPSCs (hiPSCs) were induced using the same or a similar set of transcription factors, and subsequently were widely used for disease modeling and drug screening [1921].
The stem cell research field was then boosted by the emergence of hot topics such as probing the mechanisms of reprogramming, increasing reprogramming efficiency and improving therapeutic safety [2225].
All roads lead to cell fate change During mammalian development, cells gradually lose potential and become progressively differentiated to fulfill the special-ized functions of somatic tissues.
The traditional Wadding-tons concept of the epigenetic landscape described a progressively restricted and educative hierarchical model of cell differentiation potential during normal development.
According to this model, the pluripotent state resides above the differentiated somatic states, and lineage differentiation and commitment are unidirectional and irreversible.
Pluripo-tency-associated factors and lineage specifiers have divergent roles in maintaining identities of pluripotent or differentiated states [26].
SCNT and transcription factor-based reprogram-ming experiments demonstrated that a terminally-differenti-ated somatic cell fate can be reversed, yielding a pluripotent state.
During reprogramming or direct lineage conversion, the cells need to overcome the epigenetic hierarchy or the bar-riers between the lineages.
It was not until recently that Shu et al.
revealed that balanced overexpression of transcription factors that control ectoderm and mesendoderm lineage spec-ification can also reprogram the mouse fibroblasts into iPSCs.
They proposed a seesaw model to explain their findings: when all specification forces are well balanced at an appropri-ate level, the reprogrammed cells are allowed to assume a plu-ripotent state [27].
This is in agreement with other findings that preventing lineage specification is sufficient for pluripotency induction.
Although the precise mechanism by which lineage specifiers coordinate the induction of pluripotency is still under investigation, the insights that have already emerged in this re-gard have enhanced our understanding of the true nature of pluripotency.
Based on a careful analysis of the literature on direct reprogramming, Ladewig et al.
proposed an epigenetic disc model of cell fate change, which seems more adaptable to so-matic cell fate conversion, including iPSC induction [28].
In this model, the pluripotent state locates in the central area of a flat disk, represents just one of many possible states of a cell, and is metastable, requiring certain conditions for long term maintenance.
In the case of a cell fate change, a cell has multiple choices in terms of its destination, and can proceed through a shortcut to one cell fate or alternative routes to reach a different cell fate.
The non-hierarchical epigenetic disc model extends our understanding of cell fate change and will facilitate the development of optimized approaches for cell differentiation, reprogramming and trans-differentia-tion.
Although pluripotency induction seems feasible accord-ing to this model, it reminds us that a wide variation in pluripotency might exist among different iPS cell lines, which needs to be carefully considered when they are used for re-search and discovery.
Since reprogramming factors such as Oct4, Sox2, c-Myc and Klf4 regulate specific signaling pathways, it is conceivable that different combinations of small molecules can be used to reprogram somatic cells.
Although the complete chemical reprogramming approach remains to be further explored for reprogramming of human somatic cells, chemically induced pluripotent stem cells (CiPSCs) have already been generated from mouse somatic cells, using a combination of seven small molecule compounds [29].
These findings increase our under-standing about the establishment of cell identities and open 290 Genomics Proteomics Bioinformatics 11 (2013) 288293 up the possibility of generating functionally desirable cell types for regenerative medicine, using specific chemicals or drugs, in-stead of genetic manipulation and difficult-to-manufacture biologics.
As the reprogramming strategies are improved, we will be equipped to tackle challenges that have hampered the use of iPSCs in clinical and translational medicine.
hiPSCs can and cant Similar to hESCs, iPSCs have the ability to proliferate indef-initely and differentiate into any cell types of the body.
These features make iPSCs an attractive complement to hESCs in many aspects of research and translation, in par-ticular disease modeling, and a potential source of cells for personalized regenerative medicine.
However, as a novel type of cell still at an early stage of scientific study, plenty of issues exist that limit the application of hiPSCs.
The tech-niques for reprogramming are far from optimized; mutations during reprogramming may cause abnormalities in the iPSC lines; and the differentiation potential of iPSC lines may vary.
At the top of the task list for promoting the applica-tion of iPSCs is refining the reprogramming technique, for example, using small molecules to generate genomic non-integrative iPSCs.
Ideal model for studying human development hiPSCs, like ESCs, are invaluable tools for studying human development.
Because their in vitro differentiation faithfully recapitulate what occurs in in vivo development, and iPSC lines usually retain the same genetic information with their donors.
hiPSCs provide certain advantages in the study of neural development, especially early neural system development.
The development and optimization of protocols for directed differentiation have made it easy for investigators to differen-tiate iPSCs into many subtypes of neurons with the course of differentiation mimicking the endogenous human neural devel-opment process.
The hiPSCs can be converted to neuroepithe-lial cells (NE cells), and these hiPSC-derived NE cells can then pattern efficiently to region-specific neural progenitors along the anterior-posterior axis, which can further differentiate into functional neurons including forebrain glutamatergic neurons, midbrain dopaminergic neurons and spinal motor neurons [3032].
Most of these protocols have been developed based on our knowledge of developmentally-relevant signals identi-fied in animal models.
hiPSCs and animal models complement each other, thus promoting the understanding of the mecha-nisms of developmental processes.
As for disease-based iPSCs, especially those developmental disease-based iPSCs, they are ideal tools for studying the early events relevant to the devel-opment of the specific diseases.
By studying molecular defects or mutations that are readily observable in iPSC-derived cells, we are able to investigate the important roles of the affected molecules and identify how particular molecular events affect normal development.
For example, using iPSCs generated from the fibroblasts of a patient with Rett syndrome (RTT) as models, investigators identified an unexplored critical win-dow at the early stage of neural development, during which subtle alterations in the nervous system, found to be caused by MeCP2 mutations, play important roles in the initiation of RTT [33].
Feasible system for disease modeling and drug screening Because of the limitations of animal models, human specific as-pects of diseases are hard to clarify.
Mechanistic findings and therapeutic approaches for animal models usually failed to be translated into a human context.
Patient-specific iPSCs pro-vide a unique platform to study human genetic diseases in vitro, particularly for inherited developmental disorders.
Through differentiation of iPSCs along specific lineages, some of the phenotypes of mono-gene diseases have been recapitu-lated in a dish.
In addition, for some of the more complex polygenic disorders, patient-specific iPSCs also proved to be useful models of disease progression.
As a model system, iPS-Cs and in vitro differentiation can be used to explore pathogen-esis, develop early diagnostic tools and discover potential treatment approaches.
A variety of patient-specific hiPSCs from Parkinsons disease (PD) [34], Alzheimers disease (AD) [3537], Huntingtons disease (HD) [38] and schizophrenia [39] patients have been obtained and have all been shown com-petent to model the disease progression in vitro.
In most cases, iPSCs were differentiated into disease-relevant subtypes of cells exhibiting certain disease features.
Using hiPSCs from familial and sporadic AD patients, researchers have success-fully established the AD model and revealed stress phenotypes associated with intracellular Aa in neurons/astrocytes and dif-ferential drug responsiveness [40].
Similarly, from somatic cells of a late stage pancreatic ductal adenocarcinoma (PDAC) pa-tient, iPSCs were generated and re-differentiated into pancre-atic tissue [41].
These disease-specific iPSC-derived pancreatic cells mimic the progression of early to mid-stage pancreatic cancer, releasing protein which later was identified as a bio-marker of early-stage cancer progression.
Identification of bio-markers for such cancers will eventually facilitate the early detection and successful treatment of these diseases, and thus potentially reduce associated mortality.
Usually, conventional drug discovery is costly and time-consuming.
In addition, a large proportion of candidate drugs that have passed animal tests fail testing in the following stages, largely because of efficacy and safety issues when used in humans.
Thus most animal model-based pre-clinical studies lead to uncertain results in clinical trials, which is a huge prob-lem for the pharmaceutics industry.
To help overcome this is-sue, iPSCs and the differentiated derivatives that recapitulate disease phenotypes can be used for stem cell-based drug screening.
Compared to other systems such as animal models, hiPSCs offer unique advantages.
They directly provide infor-mation on how drugs affect human cells; iPSC-based screening is much easier to operate on a large scale; disease-specific iPS-Cs have higher sensitivity and accuracy; and iPSC-based screening is cost effective.
In an attempt to identify effective drugs for the treatment of PD, investigators found that only 16 out of 44 compounds shown effective in animal models were able to protect human stem cell derived-dopaminergic neurons from rotenone-induced cell death [42], a result indicating the need to use disease-relevant human neurons for drug screen-ing.
In a high-throughput drug screen, 8 out of 6912 small mol-ecule compounds tested on neural crest precursors derived from familial dysautonomia (FD) iPSCs proved able to rescue phenotypes of the disease to a level similar to that observed in cells with wt-IKBKAP, the gene that is responsible for FD.
Among these compounds, SKF-86466 could induce IKBKAP Wu M et al/ Cell Fate Change Beyond iPSCs 291 transcription through modulation of the levels of intracellular cAMP and PKA-dependent CREB phosphorylation.
SKF-86466 was also able to rescue the expression of IKAP protein and disease-specific loss of autonomic neuronal marker expres-sion [43].
In another study, researchers employed the stem cell-based drug screening techniques and found that the survival of motor neurons was greatly improved upon treatment with a compound called kenpaullone, which is much cheaper and more effective than olesoxime and dexpramipexole, two drugs that are currently used to treat amyotrophic lateral sclerosis (ALS) patients [44].
Uncertain cell-based therapy Another exciting aspect of iPSCs is the possibility that custom-tailored pluripotent cells can be generated for autologous cell transplantation, as has been indicated by a compelling study showing that sickle cell anemia model mice can be rescued by transplantation of hematopoietic progenitors differentiated from autologous iPSCs [45].
In the central nervous system, transplantation of hiPSC-derived oligodendrocyte progenitor cells (OPCs) into the neonatal brains of myelin-deficient shi-verer mice resulted in a robust myelination of the hypomyeli-nated shiverer brain and substantially increased host survival with no evidence of either tumorigenesis or heterotopic non-glial differentiation [46].
These studies indicate that trans-plantation of iPSC derivatives for customized therapeutic regeneration is feasible.
Although iPSCs have possible applications in personal-ized clinical intervention, there are still challenges in using iPSCs for translational applications.
One problem in using iPSC-derived cells for transplantation is that residual undif-ferentiated cells increase the risk of teratoma formation.
An-other obstacle is the lack of protocols for efficiently generating therapeutically-sufficient numbers of purified line-age-specific cells.
Adding uncertainty to the application of iPSC-derived cells in regenerative medicine is the fact that cells differentiated in vitro are less mature than those that develop in vivo, and might not be able to integrate into the host tissues upon transplantation.
In addition to these problems that all types of pluripotent stem cells have, incomplete reprogramming or genetic aberrations that accrue during iPSC derivation pose issues such as de novo immuno-genicity and genomic instability.
Because of this, even the reliability of an iPSC-based drug screen would not be so so-lid as to be unchallengeable.
ES age, iPSC decade and the post-iPSC era Stem cell research promotes novel therapeutic innovations in regenerative medicine, which are an important complement to conventional medical interventions.
It is conceivable that pluripotent stem cells such as ESCs and iPSCs, somatic stem cells and functional cells obtained through other approaches would be recognized as key players in regenerative medicine.
The late 1990s and early 2000s represent the age of hESCs, as these cells were recognized during those years as offering a great promise both to the scientific community and the public.
However, in addition to ethical dilemmas, issues such as researchers poor understanding of the nature of true plu-ripotency, risks of tumor formation and immune rejection upon allograft transplantation were not easily solvable and enthusiasm for stem cells started to vanish as the public lost their patience after years of waiting.
Nonetheless, the repro-gramming of human fibroblasts back to a pluripotent state with only a few transcription factors or small molecules was a great breakthrough in the stem cell field, and launched a new decade of stem cell research.
iPSCs provide another important avenue to study pluripotency, and can be used to develop systems for disease modeling, drug discovery and cell-based therapy.
iPSCs have some advantages over hESCs such as the absence of ethical concerns and presum-ably immune rejection issues.
Nevertheless, as a new type of stem cell, they still must be studied using hESCs as a refer-ence for a complete understanding of their nature and real-ization of their application potential.
In fact, in the context of cell replacement therapy, hiPSCs are subject to the same requirement that applies to hESCs: they need to be reliably differentiated in large quantities and be functional before they can be used for therapy.
Compromising the potential use of iPSCs for cell replacement therapy are the suboptimal procedures for iPSC production, mutations during repro-gramming and uncertainties over the genomic stability of the differentiated derivatives.
In this regard, other avenues, such as direct lineage conversion or transdifferentiation, might be more promising for personalized regenerative med-icine in the future.
We expect in a post-iPSC era there will be more advances in concepts and breakthroughs in transla-tional research, addressing reprogramming, pluripotency and cell fate change.
Conclusion Ever since the use of Oct4, Sox2, Klf4 and c-Myc (OSKM) to reprogram somatic cells into pluripotent stem cells, break-throughs in the iPSC field have been reported frequently and these advances greatly challenge our conventional understand-ing of cell fate determination.
Pluripotency, which has long been considered as being atop the epigenetic potency valley, is probably a balanced state of counteracting differentiation cues.
Pluripotency factors, which were thought to prevent dif-ferentiation by inhibiting the action of lineage specifiers, are not indispensable for reprogramming somatic cells back to a pluripotent state.
Other lineage specifiers, when employed appropriately, are also able to generate iPSCs.
In this regard, pluripotency represents just one of the states among many.
iPSCs could be very useful in modeling diseases and screening drugs, and clarification of the molecular mechanism of repro-gramming and cell fate determination is also important to effi-ciently produce the desired specific type of cells for cell-based replacement therapy.
However, as a type of pluripotent stem cells still needing further investigation, iPSCs are not likely ideal for this purpose.
Other techniques that are being devel-oped based on the theory of reprogramming and fate change, such as direct lineage conversion or reprogramming of somatic cells into lineage specified progenitors, might fulfill the prom-ises of personalized cell-based therapy.
Competing interests The authors declared that no competing interests exist.
292 Genomics Proteomics Bioinformatics 11 (2013) 288293 Acknowledgements This work was supported by grants from the Ministry of Sci-ence and Technology of China (Grant No.
2013CB966904, 2014CB964600 and 2012CBA01307) and the Strategic Prior-ity Research Program of the Chinese Academy of Sciences (Grant No.
XDA01040109).
ABSTRACT Motivation: One of the most deadly cancer diagnoses is the carcinoma of unknown primary origin.
Without the knowledge of the site of origin, treatment regimens are limited in their specificity and result in high mortality rates.
Though supervised classification methods have been developed to predict the site of origin based on gene expression data, they require large numbers of previously classified tumors for training, in part because they do not account for sample heterogeneity, which limits their application to well-studied cancers.
Results: We present ISOLATE, a new statistical method that simultaneously predicts the primary site of origin of cancers and addresses sample heterogeneity, while taking advantage of new high-throughput sequencing technology that promises to bring higher accuracy and reproducibility to gene expression profiling experiments.
ISOLATE makes predictions de novo, without having seen any training expression profiles of cancers with identified origin.
Compared with previous methods, ISOLATE is able to predict the primary site of origin, de-convolve and remove the effect of sample heterogeneity and identify differentially expressed genes with higher accuracy, across both synthetic and clinical datasets.
Methods such as ISOLATE are invaluable tools for clinicians faced with carcinomas of unknown primary origin.
Availability: ISOLATE is available for download at:Contact: gerald.quon@utoronto.ca; quaid.morris@utoronto.ca Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION While most cancerous tumors present at their cancer site of origin (CSO), 4% of all new tumors do not (American Cancer Society, 2001).
Without knowledge of this site, treatment regimens are highly limited in their specificity and result in high mortality rates (Blaszyk et al., 2003; Shaw et al., 2007).
In an effort to identify CSO, patients routinely undergo extensive clinical examination, radiology and histoimmunological testing (Hainsworth and Greco, 1993).
However, these drastic interventions fail to identify the site of origin more than half of the time (Blaszyk et al., 2003).
To whom correspondence should be addressed.
Gene expression profiling provides a precise, high-resolution molecular fingerprint of a tumor that also offers insight into the underlying transcriptional activity that gave rise to its aberrant behavior (Liotta and Petricoin, 2000).
To date, a number of supervised classification methods have been used to categorize tumors according to their site of origin based on gene expression profiles, including support vector machines (Ramaswamy et al., 2001; Su et al., 2001; Tothill et al., 2005), decision trees (Dennis et al., 2005; Shedden et al., 2003), K-nearest neighbors (Bridgewater et al., 2008; Giordano et al., 2001; Horlings et al., 2008), neural networks (Bloom et al., 2004) and others (Buckhaults et al., 2003; Dennis et al., 2002; Varadhachary et al., 2008).
These studies all share a similar three-step strategy: transcriptional profiles of many tumors with known sites of origin are used to identify individual marker genes whose expression levels discriminate cancers of different origin; then the expression levels of these marker genes in each tumor are used to train a classifier that is subsequently used to classify new tumors not previously labeled with a site of origin.
These microarray-based models have shown great diagnostic potential for identifying the site of origin of patients with carcinomas of unknown primary origin: accuracies of >80% were commonly reported for some types of carcinomas.
However, because these methods are supervised classification methods, they require large amounts of transcriptionally profiled tumors with identified origin upon which to train.
While this data may be available for mature tumors from common sites of origin, there are many less well-characterized cancers or poorly differentiated tumors that often have very little or no data available upon which to train.
Reported prediction accuracy on these underrepresented tumors is little better than random performance (Ramaswamy et al., 2001; Shedden et al., 2003; Su et al., 2001).
Classifier performance also depends critically on the CSO-specific marker genes identified in the preprocessing step, making downstream analysis highly sensitive to the marker set (Tothill et al., 2005).
Unsupervised methods that neither rely on previously collected training data nor prescreen for marker genes are therefore of high value.
However, to the best of our knowledge, such methods are not currently available.
It is often of interest to not only identify the site of origin, but also to identify the genes differentially expressed in the cancer cells with respect to the site of origin.
Ideally, the tumor expression profile can be directly compared with that of the CSO to identify The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:03 22/10/2009 Bioinformatics-btp378.tex] Page: 2883 28822889 ISOLATE: identifying the primary origin of cancer Fig.1.
Multiple samples taken from even the same tumor can be composed of different mixing proportions of component sources, giving rise to significantly different tumor expression profiles compared with the expression profile of the homogeneous cancer cell population.
Methods for de-convolving sample heterogeneity and removing the contributions of non-cancerous cell populations to the measured expression profile aim to re-construct the expression profile of the homogeneous cancer cell population.
those differentially expressed genes.
However, tumors are not homogeneous masses of cancer cells, but are mixtures of cell populations with varying levels of heterogeneity, not only between tumors of the same cancer but also even across the samples from the same tumor (Dennis et al., 2005).
Contaminating cell populations can include surrounding healthy tissues (such as the site of origin or the local site of a metastatic tumor) and supporting stroma (Masters and Lakhani, 2000).
Sample heterogeneity contributes significantly to the large diversity of expression profiles observed even from similar tumor samples, and in many cases contaminating non-cancer cells can dominate the expression profile (Golub et al., 1999; Liotta and Petricoin, 2000; Reya et al., 2001), as illustrated in Figure 1.
While methods exist for de-convolving heterogeneous expression profiles into their individual component profiles and inferring the so-called mixing proportions (also known as coefficients), based on techniques like Independent Components Analysis (ICA) (Hyvarinen, 2001; Lahdesmaki et al., 2005; Venet et al., 2001), they have been developed independently of the models for identifying CSO and therefore are currently applied as a preprocessing step.
The advent and rapidly decreasing cost of high-throughput sequencing (HTS) methods for expression profiling promises much higher reproducibility and a wider dynamic range of detectable gene expression than microarrays (Marioni et al., 2008; Mortazavi et al., 2008).
HTS methods are quickly becoming feasible for highly accurate characterization of the transcriptome profile of tumors.
However, the digital counting of sequence tags in HTS methods leads to a different observation of noise process compared with the analog measurement of probe intensity in microarrays.
This change requires an update to the statistical models used to analyze these data.
In this article, we present ISOLATE, a model for the Identification of Sites of Origin by LATEnt variables.
ISOLATE is the first method that simultaneously identifies sites of origin in an unsupervised fashion and addresses sample heterogeneity using HTS cancer expression profiling.
ISOLATE is designed to achieve three goals: identification of the site of origin from a set of profiled candidate sites, de-convolution of heterogeneous expression profiles into their individual components and identification of differentially expressed genes.
We demonstrate on both synthetic and clinical datasets that ISOLATE achieves higher accuracy on all of these goals than a similar ICA-based unsupervised strategy that mirrors existing tools.
The high accuracy levels achieved by ISOLATE demonstrate the feasibility of unsupervised methods for complementing traditional immunohistological and supervised classification models for identifying the site of origin of and characterizing tumors from carcinomas of unknown primary origin.
2 APPROACH OVERVIEW ISOLATE is designed to achieve three goals, illustrated in Figure 2: identification of the CSO, identification of the differentially expressed genes in the homogeneous cancer cell population and characterization of the cellular composition of each heterogeneous tumor sample (by estimation of the mixing proportions of their component cell populations).
We compared ISOLATE with an ICA-based strategy that can be applied using existing tools to address the three challenges.
In this study, we use Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to implement the ICA strategy.
LDA is an equivalent model to ICA (Shashanka et al., 2008) but with an observation noise model more appropriate for digital HTS data.
Both ISOLATE and LDA model the expression profile of a heterogeneous tumor sample as a weighted mixture of expression profiles of source cell populations (representing candidate sites of origin or contaminants), all of which have been previously characterized except for the homogeneous cancer cell population.
The set of source cell populations are herein called the Source Panel.
Candidate sites of origin and potential contaminating cells can be treated similarly in the context of LDA and ISOLATE and are hence both referred to as sources.
ISOLATE differs from LDA in that it explicitly models the similarities in expression profile between the cancer cell population and the site of origin by representing the homogeneous cancer expression profile as a sparsely perturbed version of the profile of its site of origin.
Tumor cells display functional, developmental and morphological similarities to their site of origin (Lobo et al., 2007; Sell and Pierce, 1994).
This similarity is also reflected at the gene expression level, both between the primary tumor and the site of origin (Khan et al., 2001; Ross et al., 2000), and between the primary and metastatic tumor (DArrigo et al., 2005; Weigelt et al., 2003).
By explicitly modeling the cancer cell expression profile as a perturbation of the site of origin profile, our model is a more precise representation of cancer that naturally leads to the identification of differentially expressed genes as those whose expression was perturbed to produce the tumor expression profile.
ISOLATE then uses the estimate of the homogeneous cancer profile in conjunction with the Source Panel to decompose each tumor sample.
A key feature of ISOLATE is that it recognizes the interdependence of the solutions of all three goals and iteratively solves all of them simultaneously.
In contrast, the ICA strategy 2883 [11:03 22/10/2009 Bioinformatics-btp378.tex] Page: 2884 28822889 G.Quon and Q.Morris Fig.2.
Outline of the ISOLATE and LDA methods for de-convolving cancer gene expression data and identifying the site of origin.
The input to each method consists of the expression profile of a heterogeneous tumor sample(s), as well as the Source Panel representing previously profiled cell populations that may act either as contaminants or as candidate sites of origin.
Each method performs three tasks: identification of the site of origin, identification of those genes differentially expressed in the cancer cells and characterization of the cellular composition of each heterogeneous sample by estimating the mixing proportions of each component cell population.
The ICA-based strategy operates serially by first de-convolving the heterogeneous sample without constraining the cancer profile to be derived from the Source Panel, then predicts the site of origin and differentially expressed genes.
This is in stark contrast to ISOLATE, which solves all three problems cooperatively.
first iteratively decomposes the tumor samples while estimating the profile of the homogeneous cancer cell population.
Then it compares the estimated homogeneous cancer profile to the Source Panel and identifies the parent site as the most similar profile, and finally identifies differentially expressed genes by comparing the estimated cancer profile to that of the identified site of origin.
This makes the ICA-based strategy for identifying the site of origin sensitive to imperfect de-convolution of the tumor expression profiles and often leads to misidentification of the site of origin as the surrounding tissue.
The following sections describe how we generated the synthetic datasets and collected and processed the clinical datasets that we used to test our model.
We also describe statistical inference with ISOLATE and LDA.
3 METHODS 3.1 Synthetic data collection We measured the performance of ISOLATE on a comprehensive set of synthetic data for which the correct answer is known.
Our strategy for generating data is shown in Figure 3 and summarized below, with more details provided in following sections.
Each experiment is defined by five parameters: the number of genes whose expression is perturbed in the cancer cells, their multiplicative perturbation factor, the number of heterogeneous tumor samples profiled, the number of sources in the Source Panel and the level of biological variability of the expression profiles of the same sources between different cancer patients.
First, using human kidney and liver data from Marioni et al.
(2008), we generate expression profiles for each source, both for the (a) training profiles that make up the Source Panel, and (b) for a template healthy patient.
From the template healthy patient, (c) we Fig.3.
Overall experimental strategy for generating the heterogeneous tumor samples from three sources (i.e.
candidate sites of origin) to input into the LDA and ISOLATE models.
The sources color-matched between the Source Panel and the template healthy patient differ only by technical variability in their expression profiles.
Yellow represents cancer cells, while orange represents the site of origin.
randomly select one component source as the site of origin, from which we perturb the expression profile to construct a cancer cell expression profile.
The original template of healthy source expression profiles together with the cancer cell expression profile make up a template cancer patient, from which we (d) generate one or more unique cancer patients by adding variability to the template cancer patient independently for each cancer patient.
(e) One heterogeneous tumor sample is generated from each individual using a unique set of mixing proportions to combine the source profiles of the cancer patient.
Finally, we use the Source Panel and the heterogeneous tumor 2884 [11:03 22/10/2009 Bioinformatics-btp378.tex] Page: 2885 28822889 ISOLATE: identifying the primary origin of cancer samples as input into the LDA and ISOLATE models, to (f) identify the CSO, (g) de-convolve the heterogeneity of each tumor sample, and identify differentially expressed genes.
3.1.1 Dataset Human liver and kidney transcriptome profiling data from a single human male was obtained from Marioni et al.
(2008) who sequenced each tissue seven times, split across two runs of an Illumina Genome Analyzer and at two concentrations, 1.5 pM and 3 pM.
All reads were mapped to the genome using the Illumina ELAND algorithm, and only uniquely mapped reads were retained.
A gene copy number is computed by counting the number of reads mapped to each known transcript, then computing the median number of copies for each gene over all of its respective transcripts.
We discarded all genes for which there was not even one copy in all of the runs of both tissues, leaving 13 061 genes.
Gene abundances (also called the expression profile) were computed from gene copy numbers by dividing each copy number by the sum of all gene copy numbers.
3.1.2 Generating a new source expression profile We first applied a differential expression test (Lu et al., 2005) to identify the top 40% of all genes that were most likely to be constitutively expressed across all of the kidney and liver datasets and deemed these to be candidate house-keeping genes (Zhu et al., 2008).
We then randomly selected two runs from either the kidney or liver datasets, and permuted the expression levels of their non-house-keeping genes randomly in the same order.
One run is used in the Source Panel (Fig.3a) as previously profiled abundances in LDA and ISOLATE, while the other is used in the template healthy individual (Fig.3b).
3.1.3 Generating a cancer cell expression profile To generate a cancer cell expression profile given the expression profile of the site of origin, the number of genes to perturb and their perturbation factor, we first randomly selected the set of genes to become differentially expressed, then randomly perturbed the abundances of each gene in that set either up or down (with equal probability), then renormalize the abundances to sum to 1 to make gene abundances correspond to parameters of a multinomial distribution.
This cancer expression profile and the healthy tissue profiles in the template healthy individual combine to make the sources in the template cancer patient (Fig.3c).
3.1.4 Generating a cancer patient from the template cancer patient We use the template cancer patient to obtain a cancer patient profile by adding biological variability to each source expression profile.
We represent biological variability by resampling the expression levels of a fraction of genes from the entire set of expression levels observed in that sources original expression profile.
The expression profile is subsequently rescaled to sum to 1.
3.1.5 Generating a heterogeneous tumor sample from a cancer patient We first determine what proportion of each source will compose the tumor sample, then we generate the sequence reads that are observed in the sample.
For the tumor sample d, the mixing proportions of the sources d are drawn from a Dirichlet distribution with parameters ={sc ,s1 ,s2 ,...,sS }, where sc indicates the cancer source.
In our experiments, for all non-cancer sources i =c, si =1, and sc =3 by default.
Larger values of sc will result in tumor samples containing larger proportions of cancer cells.
Once the mixing proportions d are generated, for each transcript read to generate, we randomly select a source using the mixing proportions d , then randomly select a transcript from which to generate a read using the multinomial distribution specified by the expression profile of the chosen source.
Each tumor sample was generated with 1 675 078 reads, the average number of reads collected per experiment in Marioni et al.
(2008), though the results were not sensitive to the total number of reads generated per tumor sample (data not shown).
3.2 Clinical data processing Both the ISOLATE and LDA strategies require a fully profiled Source Panel and heterogeneous tumor samples, but owing to the current lack of such data available, we took advantage of the vast quantities of microarray data available and chose to digitize such datasets to make them compatible with our model.
We downloaded a total of 93 tumor expression profiles from Su et al.
(2001), consisting of 10 kidney, 6 liver, 24 lung, 23 ovary, 6 pancreatic and 24 prostate-originating tumors collected using Affymetrix U95a GeneChip arrays.
Following the procedure of Su et al.
(2001), for each tumor sample, raw intensity values were thresholded at 20.
Mappings from the probe identifier to Ensembl gene identifiers were downloaded from the Affymetrix web site, and multiple probes matching the same Ensembl gene identifier were averaged together to produce a single measurement for each gene.
The resulting array intensities were rounded to the nearest integer and treated as transcript counts from a HTS experiment.
As a Source Panel, we downloaded a separate set of microarray data collected using Affymetrix Human Genome U133A arrays (Su et al., 2004), giving us a healthy profile version of those same six tissues.
Intensities for replicate array measurements were averaged together for each respective tissue, and using the provided annotation files, each probe was mapped to its respective Ensembl gene identifier, and multiple probes matching the same Ensembl gene identifier were averaged together.
The total set of common genes profiled in the Source Panel and the tumor profiles were 8667 genes.
For these 8667 genes, their raw averaged intensities in each source of the Source Panel were divided by the total intensity measured to produce a proper multinomial distribution over the profiled genes.
3.3 Inference with the LDA model The input to the LDA model (Blei et al., 2003), for both the synthetic and clinical datasets, consists of the expression profiles over all the genes in each source.
Each profile is represented by a vector from the set {s}Ss=1 that contains one vector for each of the S sources from the Source Panel.
Also input to the LDA model are D sets of reads {td,n}Dd=1 originating from transcriptome profiling experiments of D heterogeneous tumor samples that each generate Nd reads.
LDA estimates the expression profile sc of all genes in the cancer source sc and performs de-convolution by inferring hidden variables {zd,n} (one for each read td,n) that indicate from which of the S+1 sources (S from the Source Panel, and 1 from the cancer source) the transcript most likely originates.
In doing so, LDA estimates the fraction of each cancer sample d (the mixing proportions), d,s, coming from each of the S+1 sources.
The full model is specified below: d Dirichlet() (1) zd,n Multinomial(d ) (2) td,n|zd,n =sMultinomial(s) (3) The model was trained using the same variational Expectation Maximization (EM) framework used in Blei et al.
(2003) with 100 iterations, and rerun S times with random parameter initializations.
The initialization that resulted in the highest log likelihood of the data is chosen.
The model parameters estimated include , d for all tumor samples d, and cancer abundances sc .
Using the output of LDA, we predict the site of origin by choosing the source (from the Source Panel) whose expression profile has the least KullbackLiebler divergence from the estimated cancer expression profile sc .
To rank genes in order of differential expression, we applied a two-class differential expression test (Lu et al., 2005) to compare the expression profile of the predicted site of origin against the set of reads the cancer cells are responsible for in each tumor sample (zd,n =sc), and sorted the genes based on the resulting P-value.
Lastly, the mixing proportions (heterogeneity) of each sample d are estimated directly from the learned parameters of the model, d .
2885 [11:03 22/10/2009 Bioinformatics-btp378.tex] Page: 2886 28822889 G.Quon and Q.Morris 3.4 Inference with the ISOLATE model ISOLATE maintains the same probabilistic framework as LDA [Equations (1 3)], but introduces the following key constraints on the learned parameters sc , where :,g is a column vector of abundances of gene g across all S non-cancer sources: sc,g =T :,gg (4) g Gamma(,) (5) T =1 (6) is a (S1)-dimensional parameter, where s =1 denotes that source s is the site of origin.
g is the estimated perturbation (multiplicative) factor that describes how much the cancer cells perturb the expression of gene g relative to the site of origin described by .
Since we expect many genes to maintain similar expression levels to that of the CSO, we put a Gamma prior on g [Equation (5)], with mean E[g]=1 to emphasize that we expect many genes to not have perturbed expression.
ISOLATE uses the same variational EM framework as LDA (Blei et al., 2003) with 100 iterations, and rerun using S different initializations to test different candidate CSO.
There is exactly one initialization per source s where the value s is set to 1, and the remaining entries set to 0.
The initialization that resulted in the highest log likelihood of the data is chosen.
To rank genes in order of differential expression, we sorted the genes based on the distance of g from the value 1.
That is, the farther g is from 1, the more perturbed its abundance is from that of the site of origin.
Finally, mixing proportions of each sample d is estimated directly from the learned parameters of the model, d .
3.5 Performance metrics The error rate in identifying the primary site of origin is the fraction of experiments in which the CSO was incorrectly identified.
For the synthetic datasets, the reported error is averaged over the 20 datasets generated for each specific setting of the parameters.
The model heterogeneity error is computed by averaging, over all tumor samples, the mean absolute error of the mixing proportions d of the cancer cells and the true site of origin.
We only measure the error with respect to these two sources because we found that almost all of the error in the mixing proportion estimates is from these two sources.
Finally, we assess the error of the identification of differentially expressed genes for the synthetic datasets by using the ranks of the genes (in order of differential expression as defined by each model) and our knowledge of which genes are truly differentially expressed to compute an area under the receiver operator curve (ROC), where larger values correspond to higher accuracy.
Error is computed as [1 (Area under ROC)].
4 RESULTS 4.1 Synthetic datasets We have evaluated the relative performance of ISOLATE and LDA as a function of realistic parameter settings to demonstrate their robustness to different conditions.
We also compared naiveLu, a simple method for identifying differentially expressed genes by simply applying a differential expression test directly without accounting for sample heterogeneity.
We do these comparisons because of the dearth of clinical data and the difficulties associated with defining gold standards therein.
We are also able to query a larger variety of experimental conditions.
In the absence of analytical estimates of performance, which are likely impossible due to the complexity of our models, these comparisons provide the best support for our claims of improved performance over LDA.
We varied the following parameters: the number of differentially expressed genes, the perturbation factor by which their expression levels are differentially expressed in cancer, the number of heterogeneous tumor samples, the number of sources in the Source Panel, and the (biological) variability between our profiled Source Panel and the corresponding profiles used to generate the tumor samples (see Section 3).
This variability represents the expected differences between the normal source profiles in our Source Panel, which will likely come from different individuals, and the corresponding source profiles for the patient from which the tumor sample is drawn.
Biological variability, which could represent either biological variation or technical noise, is a key parameter because it limits our ability to detect differentially expressed genes, as seen below.
In the following, we vary only a single parameter from the default; the default parameters we use are 100 perturbed genes, 3 tumor samples, 10 sources, a perturbation factor of two, perturbation scale prior =10 [see Equation (5)], and a biological variability of 0.16 (16%), which empirically leads to 14% of genes differentially expressed, as measured by Lu et al.
(2005).
This level of differential expression between simulated individuals is similar to reported variation between unrelated individuals (Sharma et al., 2005).
4.1.1 Identification of differentially expressed genes One of the principal objectives of identifying CSO is to identify genes that are differentially expressed in the cancer cell population with respect to healthy cells of the site of origin.
We tested each models ability to identify the differentially expressed genes, defined as those genes whose expressions were perturbed to differentiate the cancer source from the site of origin source, and measured the performance by the area under the ROC curve (see Section 3).
Figure 4a demonstrates that ISOLATE consistently achieves higher accuracy at identifying differentially expressed genes than LDA across all three parameters at almost all settings.
Surprisingly, the performance of both LDA and ISOLATE seem to stay constant despite increasing the number of tumor samples available.
Figure 5a illustrates that beyond a variability level of 15%, increasing the amount of data does not improve ISOLATE performance.
Because 15% is near the average variability between unrelated individuals (Sharma et al, 2005), this result suggests that ISOLATEs identification of differentially expressed genes can be improved by analyzing multiple tumor samples from the same individual but not necessarily by analyzing multiple samples from different individuals.
Both ISOLATE and LDA improve performance as the perturbation factor increasesa direct result of its increasing differentiation from the site of origin source and hence easier de-convolution of sample heterogeneity though ISOLATE improves at a much faster rate.
The performance of naiveLu, which does not address heterogeneity, illustrates that de-convolution clearly improves the identification of differentially expressed genes.
The ISOLATE performance gain over LDA is not just simply due to a difference in the specific method that ISOLATE uses to compute differential expression: we computed differential expression using the same method as for LDA (ISOLATE-Lu) and see that its performance is still better than LDA in many cases.
4.1.2 Identification of CSO Figure 4b compares LDA and ISOLATE based on how often they are able to correctly identify the site of origin.
ISOLATE consistently outperforms LDA across all datasets.
Most importantly, while ISOLATE is robust against the number of sources in the Source Panel, the performance of LDA diminishes rapidly after six sources.
This makes LDA and other ICA-like techniques impractical for considering many 2886 [11:03 22/10/2009 Bioinformatics-btp378.tex] Page: 2887 28822889 ISOLATE: identifying the primary origin of cancer Fig.4.
Performance of ISOLATE and LDA on synthetic datasets.
Each row represents a different parameter tuned: the number of heterogeneous tumor samples, the number of sources (non-cancer) in the Source Panel and the perturbation factor of the differentially expressed genes (manipulating the number of perturbed genes within the range of 50500 genes did not result in changes in performance to either model and are not shown).
Each column represents a different performance metric applied to each dataset.
(a) Differential expression error, (b) origin error and (c) heterogeneity error are as defined in Section 3.
Two additional models are plotted in (a): ISOLATE-Lu is the performance achieved when applying the same method as ICA for identifying differentially expressed genes (Lu et al., 2005) to the output of the ISOLATE model, and naiveLu is the performance achieved when directly comparing the heterogeneous tumor expression profiles to the site of origin to identify differentially expressed genes, without accounting for sample heterogeneity.
potential candidates for the CSO, an important feature given the potentially large set of candidate CSO to query.
The difference between ISOLATE and LDA also illustrates the improvement in CSO identification, sometimes as staggeringly as 70%, achieved by solving for both cell population mixture coefficients and CSO simultaneously within the same framework.
From Figure 5b, we see that ISOLATE achieves high accuracy at identifying CSO under even high variability conditions, while LDA accuracy varies quite widely even under low variability conditions.
ISOLATE is therefore able to capture the underlying signal of the site of origin even despite large amounts of noise in the expression profiles.
Most importantly, even when looking at very small sets of tumor samples, ISOLATE performs as well as it does with many more samples, an important feature given the cost of profiling tumors in a diagnostic setting.
4.1.3 Correction of sample heterogeneity Figure 4c illustrates that when considering Source Panels containing fewer than 10 profiles, ISOLATE achieves better de-convolution of heterogeneity than LDA.
However, their performance is nearly identical regardless of the perturbation factor applied to the differentially expressed genes, suggesting that the amount of tumor sample data is far more important for de-convolution than the difference in expression profiles of the cancer cells and the site of origin.
As expected, as Fig.5.
Performance of ISOLATE and LDA on synthetic datasets under different biological variability conditions.
Each column represents a different performance metric, and each row a different model (top, ISOLATE; bottom, LDA).
Here, we co-vary the biological variability added to each tumor sample independently, and the number of tumor samples made available to each model.
The performance metrics of (a) differential expression error, (b) origin error and (c) heterogeneity error are as defined in Section 3.
Fig.6.
An example of heterogeneity error for a single tumor sample.
Here, we illustrate a representative tumor sample whose mixing proportions of component sources were estimated by ISOLATE (top row) and LDA (bottom row), compared with the actual values (middle row).
Each of the four columns on the left represent a source from the Source Panel (Sources 13, as well as the site of origin source), while the right-most fifth column represents the cancer cells.
The area of each square is proportional to the fraction of the sample composed of that particular source.
Whereas ISOLATE estimated mixing proportions fairly close to the truth, the LDA estimate of the CSO and cancer sources were quite erroneous.
the number of tumor samples increases, both LDA and ISOLATE increase in performance.
Figure 5c illustrates that ISOLATE achieves better accuracy for the same number of data points and level of variability, although it takes more samples for a given level of biological variability than LDA to achieve its maximum performance.
Most of the performance loss in LDA appears to be due to confusion of the contributing expression signatures from the cancer cells and the site of origin source (Fig.6 illustrates an example).
This is a problem that ISOLATE is able to mitigate because of the constraints it places on the learned cancer expression profile.
2887 [11:03 22/10/2009 Bioinformatics-btp378.tex] Page: 2888 28822889 G.Quon and Q.Morris Fig.7.
The performance of ISOLATE, LDA and another Kullback-Leibler (KL) divergence-based measure on the clinical dataset of 93 tumor samples.
Each sample is predicted independently of all other samples in the dataset.
The number of samples in each class is shown beside the class name, and classes are in decreasing order of size, from left to right, with the overall performance shown in the leftmost column.
The black line shows random performance.
4.2 Clinical dataset We used ISOLATE and LDA to predict the site of origin of 93 tumor samples from Su et al.
(2001).
Heterogeneity and differential expression error could not be measured on these datasets as the true values are not known.
Each tumor, regardless of its site of origin, is predicted independently of all other tumors, to reproduce clinical diagnostic conditions.
As a benchmark besides LDA, we constructed a predictor that chooses the source from the Source Panel whose Kullback-Leibler (KL)-divergence is least with respect to the tumor samples expression profile (KL).
To set the perturbation scaling prior of Equation (5), we tried several values of (100, 101, 102, 103, 104, 105), and performed 2-fold cross-validation by choosing the that maximized performance of half the data, in order to predict the other half of the dataset, and vice versa.
The optimal value of was 105 for both halves of multiple splits of the data, and so was used to generate the results shown in Figure 7.
Over the entire dataset of 93 tumor samples, ISOLATE achieves the highest performance of 65.59% accuracy, compared with 52.69% of both LDA and the KL measure.
On a class-by-class basis, ISOLATE ties or performs better than LDA and ISOLATE in the larger classes, only performing worse when predicting tumors originating from the pancreas, the smallest class.
Note that though previously reported performance of supervised classification methods is higher on some of these cancer types, ISOLATE achieves the observed performance considering each tumor separately without reference to any of the other tumor samples and without any training, mirroring clinical settings for the CSO identification of tumors underrepresented among previously profiled samples.
5 DISCUSSION We have developed ISOLATE to provide a molecular diagnostic tool to aid in identifying the site of origin of tumors of poorly characterized cancers, situations in which classical supervised models perform poorly.
ISOLATE simultaneously de-convolves tumor expression profiles, and identifies the CSO and genes differentially expressed in the cancer cells, three tasks that were previously solved independently.
Our experiments detail the performance of ISOLATE under a wide range of realistic experimental conditions for synthetic and digitized clinical microarray data, showing that solving all three tasks simultaneously leads to greater predictive performance than solving them individually.
ISOLATE, unlike previous methods for classifying cancers of unknown primary origin, is an unsupervised classification algorithm, which provides it with several inherent advantages.
It does not require a large training set of tumors of known primary origin, and in our clinical validation we only use data from a single tumor, a particularly important feature given the difficulty and cost of procuring many high-quality tumor samples in a diagnostic setting.
Because it is an unsupervised algorithm, ISOLATEs performance is also less sensitive to the number of candidate sites of origin, as suggested by our synthetic data validation, in contrast to supervised learning methods that have difficulty with more than 10 classes (Su et al., 2001).
By its construction, ISOLATE is also less prone to overfitting than supervised learning algorithms, and as such, does not require a prescreening stage to identify marker genes upon which cancers could be discriminated.
Despite our use of microarray data for our clinical validation, we recommend that ISOLATE be used exclusively with HTS expression profiles, which we believe support more accurate tumor diagnosis.
HTS methods promise a substantial reduction in sample-to-sample variability, which our synthetic data-based validation shows limits the accuracy.
Also, because HTS measurements are not probe-based, they are both less platform-specific, allowing easier integration of data from multiple labs, and less sensitive to polymorphisms in transcript sequence that are common in highly polymorphic cancer genomes.
For these reasons, we have tailored ISOLATEs statistical model for HTS gene expression data.
The successful application of ISOLATE or other expression-based models will depend on the availability of expression profiles for a wide range of human tissues, in order to consider them as potential sites of origin.
With the costs of high-throughput expression profiling dropping quickly and the number of studies using these technologies to profile tumors increases (Jones et al., 2008; Parsons et al., 2008), soon it will be practical to collect a compendium of expression data from many of the individual tissues of humans along with multiple tumor samples, as is currently available for microarrays (see, e.g.
Su et al., 2004).
Molecular-based diagnostic tools for identifying cancer sites of origin represent an important class of tools that can potentially facilitate faster, more accurate diagnoses leading to the successful identification of primary sites.
ISOLATE will be an invaluable tool for exploring new, uncharacterized cancers of unknown primary origin for which little expression data are available, or clinically ambiguous samples for which more traditional models cannot classify with high accuracy.
ACKNOWLEDGEMENTS The authors thank Yoav Gilad for providing the processed read data from Marioni et al.
(2008).
Funding: CFI/ORF equipment grants (to Q.M.
); NSERC operating grant (to Q.M.
); NSERC PGS Doctoral Scholarship (to G.Q., in part).
2888 [11:03 22/10/2009 Bioinformatics-btp378.tex] Page: 2889 28822889 ISOLATE: identifying the primary origin of cancer Conflict of Interest: none declared.
ABSTRACT Motivation: Proteins exhibit complex subcellular distributions, which may include localizing in more than one organelle and varying in location depending on the cell physiology.
Estimating the amount of protein distributed in each subcellular location is essential for quantitative understanding and modeling of protein dynamics and how they affect cell behaviors.
We have previously described automated methods using fluorescent microscope images to determine the fractions of protein fluorescence in various subcellular locations when the basic locations in which a protein can be present are known.
As this set of basic locations may be unknown (especially for studies on a proteome-wide scale), we here describe unsupervised methods to identify the fundamental patterns from images of mixed patterns and estimate the fractional composition of them.
Methods: We developed two approaches to the problem, both based on identifying types of objects present in images and representing patterns by frequencies of those object types.
One is a basis pursuit method (which is based on a linear mixture model), and the other is based on latent Dirichlet allocation (LDA).
For testing both approaches, we used images previously acquired for testing supervised unmixing methods.
These images were of cells labeled with various combinations of two organelle-specific probes that had the same fluorescent properties to simulate mixed patterns of subcellular location.
Results: We achieved 0.80 and 0.91 correlation between estimated and underlying fractions of the two probes (fundamental patterns) with basis pursuit and LDA approaches, respectively, indicating that our methods can unmix the complex subcellular distribution with reasonably high accuracy.
Availability: http://murphylab.web.cmu.edu/software Contact: murphy@cmu.edu 1 INTRODUCTION To investigate the subcellular localization of proteins at a proteome-wide scale, we need to be able to characterize all observed patterns.
Identification of subcellular localization patterns from fluorescence images using supervised machine learning methods has become an established method, with excellent results in its field of application.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
However, this method is, by design, limited to hard assignments to classes predefined by the researcher.
Some researchers have explored using unsupervised learning technologies (Garca Osuna et al., 2007; Hamilton et al., 2009), which do not require the researcher to specify classes.
These methods still result in each protein being assigned a single label.
However, not all proteins can be thus characterized.
In particular, there are many proteins that exhibit mixed patterns, i.e.
patterns that are composed of more than one location.
For example, while some proteins locate in the nucleus and others locate in the endoplasmic reticulum, there is a third group that locates in both of these locations.
A simple class assignment does not adequately represent the relationship between these three possibilities.
One alternative is to assign multiple labels to a single pattern.
In one large-scale study of the yeast proteome, a third of proteins were annotated with multiple locations, which demonstrates that this is not a problem confined to special case proteins (Chen et al., 2007; Huh et al., 2003).
However, this approach fails to quantify the contribution of each element and shows the need for a system that directly models the mixture phenomenon.
We have previously presented some methods that address this pattern unmixing problem in a supervised setting: given images of fundamental patterns (e.g.
nuclear and endoplasmic reticulum in the above example) and mixed images, map mixed images into a set of coefficients, one for each fundamental pattern (Peng et al., 2010; Zhao et al., 2005).
These methods were observed to perform well on both synthetic and real data in recovering the underlying mixture coefficients (which had been kept hidden from the algorithm).
However, the supervised approach still requires the researcher to specify the fundamental patterns of which other patterns are composed.
For example, for the quantitative analysis of translocation experiments as a function of time or drug concentration, the extreme points could be easily identified as the patterns of interest.
However, they are still inapplicable to proteome-wide studies where it would be a difficult (and perhaps impossible) task to identify all fundamental patterns that are present.
We note that the set of fundamental patterns that can be identified depends both on the specific cell type and the technology used for imaging, high-resolution confocal microscopes being able to distinguish patterns that lower resolution systems cannot.
Therefore, it is necessary to tackle the unsupervised pattern unmixing problem: given a large collection of images, where none has been tagged as being a representative of a fundamental pattern, map all images into a set of mixture coefficients automatically derived from the data.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:41 12/5/2010 Bioinformatics-btq220.tex] Page: i8 i7i12 L.P.Coelho et al.
Fig.1.
Overview of unmixing methods.
(a) The algorithms use a collection of images as input in which various concentrations of two probes are present (the concentrations of the Mitotracker and Lysotracker probes are shown by increasing intensity of red and green, respectively).
Example images are shown from wells containing only Mitotracker (b), only Lysotracker (c) and a mixture of the two probes (d).
(e) Objects with different size and shapes are extracted and object features are calculated.
(f) Objects are clustered into groups in feature space, shown with different colors.
(g) Fundamental patterns are identified and the fractions they contribute to each image are estimated.
In this article, we present and compare methods to address this problem using a test dataset previously created to test supervised unmixing methods (Peng et al., 2010).
2 METHODS 2.1 Object typing 2.1.1 Overview All the methods developed for this problem so far are based on a bag of objects model, where an image is interpreted as a collection of regions of above-background fluorescence.
Each object is then characterized by a small set of object features, and objects are clustered into groups (object types).
Patterns are then defined as distributions over these groups.
This is illustrated in Figure 1.
The intuition is to capture patterns such as the fact that lysosomes are small mostly circular objects, while mitochondria consist of stringy objects.
The methods need to be robust to stochastic variation, however, as mitochondrial patterns are also observed to contain circular objects and agglomerations of lysosomes may appear as a single stringy object.
In fact, the algorithms need to capture not only the fact that mitochondrial patterns are composed of stringy objects, but also that the proportions of different types of objects are present in statistically different proportions.
2.1.2 Image preprocessing and segmentation Images are first preprocessed to remove uneven illumination.
The illumination bias is estimated by fitting a plane to the average pixel intensity at each location across the whole collection of images.
Every image pixel is then divided by this illumination estimate to regularize across the whole image.
Images are segmented by using the model-based method of Lin et al.
(2003) on the nuclear channel, which was previously found to give the best results for images in the unmixing test dataset (Coelho et al., 2009).
The segmentation is extended to the whole field by using the watershed method with the segmented nuclei as seeds.
2.1.3 Object detection In our previous supervised unmixing work, objects were simply defined as contiguous pixel regions above a global threshold.
In the work described here, we use both a global threshold, using the Ridler Calvard method (Ridler and Calvard, 1978), and a local threshold, the mean pixel value of a 1515 window centered at the pixel.
We have found that the global threshold achieves a good separation of the general cell areas from the background, while, inside those regions, local thresholding is better at capturing detail.
Objects that are smaller than 5 pixels are filtered out.
2.1.4 Object features Each object is characterized by a set of features, previously defined as SOF1 (subcellular object features 1).
This is a combination of morphological features for describing the shape and size of the object and features which capture the relationship to the nuclear marker (Zhao et al., 2005): (1) Size (in pixels) of the object.
(2) Distance of object center of fluorescence to DNA center of fluorescence.
(3) Fraction of object that overlaps with DNA.
(4) Eccentricity of object hull.
(5) Euler number of object.
i8 [10:41 12/5/2010 Bioinformatics-btq220.tex] Page: i9 i7i12 Quantifying the distribution of probes (6) Shape factor of convex hull.
(7) Size of object skeleton.
(8) Fraction of overlap between object convex hull and object.
(9) Fraction of binary object that is skeleton.
(10) Fraction of fluorescence contained in skeleton.
(11) Fraction of binary object that constitutes branch points in the skeleton.
2.1.5 Object clustering In order to be able to reason about object types, objects are clustered into groups using k-means on the z-scored feature space.
Multiple values of k are tried and the one resulting in the lowest BIC (Bayesian information criterion) score is selected.
Based on this clustering, each object can be assigned a numerical identifier, its cluster index, which serves as its type.
After this step, the algorithms diverge in how they handle the cluster indices.
2.2 Basis pursuit In this model, each image is represented by a vector x(i) such that entry x(i) represents the fraction of objects in condition i that have type (if there are multiple images for the same condition, a common situation, they are counted together).
We have one vector per input condition (i.e.
i=1,...,C, where C is the number of conditions), and the size of this vector is the number of clusters that was automatically identified in the clustering step (i.e.
=1,...,k).
Using fractions instead of the direct object counts normalizes for the different number of cells in each image and different cell sizes.
In this model, bases (fundamental patterns) are represented as a set of vectors in the same space and a mixture is defined by a set of coefficients j for each b(j) (j=1,...,B, where B is the number of basis vectors, and each b(j) is of the same dimension as the x(i)s): x(i) = j b(j)(i)j +(i), (1) where (i) encapsulates both the stochastic nature of the mixing process and the measurement noise.
Given a set of observations, the task is to identify the bases b(j) and coefficients (i), which minimize the squared norm of the error terms i(i)2.
Without additional constraints, principal component analysis (PCA) is the simplest solution to this problem.
However, this is unsatisfactory as it could result in negative mixtures, which are not meaningful.
Independent component analysis (ICA) suffers from the same problem.
Therefore, we add a non-negativity constraint on the vector and use non-negative matrix factorization (NNMF) possibly with sparsity constraints to solve the problem (Hoyer et al., 2004; Lee and Seung, 1999).
An additional constraint can be helpful to obtain more meaningful results: require the basis vectors to be members of the input dataset (i.e.
for all j, there is some i, such that b(j) =x(i)).
This condition, which encapsulates the expectation that the input dataset is large enough to contain both fundamental and mixed patterns, requires a search method.
Some preliminary results showed that this model was still too sensitive to the trend, i.e.
to the average value of xi,j across the dataset (data not shown).
If one basis vector was allocated to handle this trend, good fits were obtained but poor interpretability.
We found that removing the mean from the data led to more meaningful results.
In this detrended dataset, x(i)j may take negative values, but the mixing coefficients i,j are still constrained to be non-negative.
Thus, the final optimization problem is: min b(j), ||(i)||2 (2) x(i) =x(i) x (3) (i) = x(i)  j b(j)(i)j (4) Subject to the constraint, that for all j, there exists an i, such that b(j) = x(i).
In order to find the best basis, we resort to simulated annealing as an optimization method.
In this class of methods, the number of fundamental patterns B must be prespecified by the user.
PCA and ICA were also performed on detrended data, but NNMF could not be (as the detrended data contains negative numbers, it cannot be the product of two positive matrices).
Before applying NNMF, we therefore removed very frequent objects (those that appeared in more than 90% of the images).
The intuition is that very frequent objects also correspond to the background.
2.3 Latent Dirichlet allocation Topic modeling in text using latent Dirichlet allocation (LDA) is a popular technique to solve an analogous class of problems (Blei et al., 2003).
In this framework, documents are seen as simple bags of words and topics are distributions over words.
Observed bags of words can be generated by choosing mixture coefficients for topics followed by a generation of words according to: pick a topic from which to generate, then pick a word from that topic.
In our setting, we view object classes as visual words over which to run LDA.
This is similar to work by other researchers in computer vision which use keypoints to define visual words (Csurka et al., 2004; Philbin et al., 2008; Zhu et al., 2009).
The process of generating objects in images to represent mixtures of multiple fundamental patterns follows the Bayesian network in Figure 2.
The generative process is as follows: for each of M images, a mixture i is first sampled (conditioned on the hyper-parameter ).
i is a vector of fractions of the fundamental pattern distributions b. Ni objects are sampled for each image in two steps: select a basis pattern according to i and then an object is sampled from the corresponding object type distribution.
To invert this generative process, we used the variational EM algorithm of Blei et al.
(2003) to estimate the model parameters of fundamental patterns and mixture fractions .
It should be noted that this is an approximation approach liable to getting trapped in local maxima and returning non-optimal results.
Therefore, we ran the algorithm multiple times with different random initializations and chose the one with the highest log-likelihood.
We choose the number of fundamental patterns B to maximize the log likelihood on a held-out dataset (using cross-validation to obtain more accurate estimate).
Fig.2.
LDA for unmixing.
represents the prior on the topics, is the topic mixture parameter (one for each of M images), z represents the particular object topic which is combined with , the topic distributions to generate an object of type w. i9 [10:41 12/5/2010 Bioinformatics-btq220.tex] Page: i10 i7i12 L.P.Coelho et al.
3 RESULTS 3.1 Dataset In order to validate the algorithms, we used a test set that was built to evaluate pattern unmixing algorithms (Peng et al., 2010).
In this dataset, u2os cells were exposed to different concentrations of two fluorescent probes with differing localization profiles (mitochondrial and lysosomal) but similar fluorescence.
The probes were image using the same fluorescence filter and therefore could not be distinguished.
This simulates the situation in which a fluorophore is present in two different locations.
For each probe, eight concentrations were used, for a total of 64 combinations.
In parallel to the marker image, a nuclear marker was imaged to serve as a reference point.
3.2 Computation time Most of the computation time is dominated by segmenting the images (30 s per image in our implementation) and computing features (10 s per image).
However, this is an embarrassingly parallel problem and can be computed on multiple machines simultaneously.
The clustering takes increasing time for different numbers of clusters, but we limited each clustering run to 1 h (while relying on multiple initialization as a guard against local minima).
Again, we note that the runs for multiple k can easily be run in parallel.
Both basis pursuit and LDA then take only on the order of minutes to run.
3.3 Basis pursuit We measured how well the identified coefficients (i)j correlated with the underlying fractions, which were estimated as linearly proportional to the ratio of the relative concentration of the mitochondrial probe to the sum of the relative concentration of the mitochondrial and lysosomal probes (relative concentration is defined as fraction of the maximum subsaturating concentration).
Using PCA, the correlation coefficient between predicted fractions and the underlying relative concentrations was 0.20.
NNMF performed better on this metric, achieving a correlation coefficient of 0.65.
Independent component analysis performed very poorly, returning correlations on the order of less than 0.10.
This is not unexpected as the independence assumptions that underly ICA fail to hold even as an approximation.
However, we are also interested in having the basis vectors line up with the underlying fundamental patterns and, in this regard, NNMF performs poorly.
One of the patterns corresponded roughly to the total concentration and they did not align well with the fundamental patterns in the data (data not shown).
The fully constrained basis pursuit algorithm performed better.
It achieved a 0.80 correlation with the underlying relative concentration.
It identified as a basis a vector that has the maximal concentration of the mitochondrial probe (and some lysosomal probe, at a relative concentration of 19%) and another that consists of the maximal concentration of the lysosomal probe and 20% mitochondrial probe.
Table 1 shows that the identified pattern 0 matches the mitochondrial probe, while pattern 1 matches the lysosomal probe.
The results above were obtained by specifying B=2 as an input to the algorithm.
For different values of B, we obtain decreasing reconstruction error as plotted in Figure 3.
As it is clear in this figure, Table 1.
Unmixed coefficients for images of fundamental patterns and mixed samples using basis pursuit with B=2 Mitochondrial (%) Lysosomal (%) Pattern 0 99 18 Pattern 1 1 82 For the two fundamental patterns, we display the average coefficient for the inferred fundamental patterns.
0 1 2 3 4 5 6 7 8 9 B 0.10 0.11 0.12 0.13 0.14 0.15 0.16 0.17 R ec on st ru ct io n er ro r Fig.3.
Average squared reconstruction error as a function of the number of patterns B for basis pursuit.
This is the value of i2 in (2).
For B=0, we show the total variance, i.e.
ix(i)2 most of the contribution to the reconstruction comes from the first two or three vectors.
Therefore, we can expect that a researcher would be able to estimate B=2 or B=3.
3.4 LDA To estimate the number of fundamental patterns using the LDA approach, we measured the log likelihood of the dataset for different numbers of bases using cross-validation.
The results are shown in Figure 4.
We can see that the best result is obtained for B=3, although the underlying dataset only has two fundamental patterns.
Table 2 shows the average coefficients inferred for pure pattern inputs after the algorithm had been applied on the whole dataset.
Pattern 1 obviously corresponds to the lysosomal component, while pattern 2 corresponds to the mitochondrial component.
Pattern 0 appears to be a non-significant pattern capturing the new object types arising in the mixture patterns.
The overall correlation coefficient is 0.95 with pattern 0 removed.
Using the LDA approach with B=2, which is the ground truth, the overall correlation coefficient between estimated and actual pattern fractions was found to be 0.91.
3.5 Comparisons Figure 5 shows the results of one inferred fraction as a function of the underlying concentrations (the plots for the other fraction, not shown, are, of course, symmetric as they sum to 1).
Figure 6 plots all the estimates in a single plot as a function of the underlying concentration fractions.
i10 [10:41 12/5/2010 Bioinformatics-btq220.tex] Page: i11 i7i12 Quantifying the distribution of probes 4 DISCUSSION We have described two approaches for performing unsupervised unmixing of subcellular location patterns, and demonstrated good performance with both on a test dataset acquired by high-throughput microscopy and previously used for testing supervised methods.
In our supervised work, we had presented two methods, one based on a linear mixture, whose adaptation to the unsupervised case results in the basis pursuit method described here, and another based on multinomial mixtures, which results in the LDA model.
The newer LDA model led to slightly better results than the basis pursuit method.
This model has the apparent disadvantage that it does not return examples of the underlying patterns, which could Fig.4.
Log likelihood as a function of the number of fundamental patterns.
Table 2.
Unmixed coefficients for fundamental patterns and mixed samples for the discovered patterns (using LDA method) Mitochondrial (%) Lysosomal (%) Pattern 0 0.0 0.0 Pattern 1 8.8 99.9 Pattern 2 91.2 0.1 For the two fundamental patterns, we display the average coefficient for the three discovered fundamental patterns.
potentially make interpretation harder.
However, we observed that this was, empirically, not a major issue as the identified bases were indeed well aligned with the underlying (hidden) concentrations as opposed to forming a complex mixture with a difficult interpretation.
The methods are comparable in terms of computational cost as it is the image processing, feature computation and, particularly, the k-means clustering that has the highest cost (the clustering is done over objects and even this evaluation set of 12 K images resulted in 750 K objects).
Once the clustering is done, both algorithms are very fast.
Therefore, in their current forms, the LDA algorithm is superior.
It is notable that both unsupervised methods led to higher correlation with the underlying coefficients than the supervised methods.
A possible cause of this is the appearance of new object types in the mixture patterns.
Under the unsupervised framework, with massive clustering, these objects might be assigned labels different from the ones of the fundamental patterns, while in the Fig.5.
Comparison of results for different unmixing methods.
The inferred fraction of pattern 1 is displayed as different intensities of gray (black corresponding to pure pattern 1).
The design matrix, which was kept hidden from the algorithms is shown on the top left, for comparison; the other three panels are results of computation.
Fig.6.
Estimated concentration as a function of the underlying relative probe concentration.
Perfect result would be along the dashed diagonal.
In LDA unmixing with 3 fundamental patterns, fractions of the two major patterns are normalized and plotted over ground-truth.
i11 [10:41 12/5/2010 Bioinformatics-btq220.tex] Page: i12 i7i12 L.P.Coelho et al.
supervised version they are forced to be one of the object types present in the fundamental patterns.
To prove this conjecture, we assumed that such new types of objects really exist and applied the outlier removal technique of Peng et al.
(2010) to perform supervised unmixing again, in the hope of removing the influence of these objects.
The correlations increased to 0.91 and 0.88 with linear and multinomial unmixing approaches, respectively, which are comparable with the unsupervised results.
Based on the results presented here, we plan to apply the unsupervised unmixing methods to large-scale image collections with the goal of identifying both the set of all fundamental patterns and of quantitating for the first time the fraction of all proteins that are present in each.
ACKNOWLEDGMENTS The authors thank Ghislain Bonami, Sumit Chanda and Daniel Rines for providing images as well as many helpful discussions.
Funding: National Institutes of Health (grant GM075205); Fundao para a Cincia e Tecnologia (grant SFRH/BD/37535/2007 to L.P.C., partially); fellowship from the Fulbright Program (to L.P.C.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Inferring the taxonomic profile of a microbial community from a large collection of anonymous DNA sequencing reads is a challenging task in metagenomics.
Because existing methods for taxonomic profiling of metagenomes are all based on the assignment of fragmentary sequences to phylogenetic categories, the accuracy of results largely depends on fragment length.
This dependence complicates comparative analysis of data originating from different sequencing platforms or resulting from different preprocessing pipelines.
Results: We here introduce a new method for taxonomic profiling based on mixture modeling of the overall oligonucleotide distribution of a sample.
Our results indicate that the mixture-based profiles compare well with taxonomic profiles obtained with other methods.
However, in contrast to the existing methods, our approach shows a nearly constant profiling accuracy across all kinds of read lengths and it operates at an unrivaled speed.
Availability: A platform-independent implementation of the mixture modeling approach is available in terms of a MATLAB/Octave toolbox at http://gobics.de/peter/taxy.
In addition, a prototypical implementation within an easy-to-use interactive tool for Windows can be downloaded.
Contact: pmeinic@gwdg.de; thomas@gobics.de Supplementary Information: Supplementary data are available at Bioinformatics online.
Received on January 28, 2011; revised on April 8, 2011; accepted on April 15, 2011 1 INTRODUCTION Metagenomics provides a holistic approach to the analysis of microbial communities that overcomes the necessity of isolating single organisms for cultivation (Beja et al., 2000; Rondon et al., 2000).
Investigating a mixture of genetic material from the whole spectrum of organisms, researchers can now obtain comprehensive descriptions even of highly diverse communities.
Further, comparative metagenomics offers new possibilities for studying the distinguishing characteristics of a wide range of ecosystems, which are shaped by specific combinations of microorganisms.
In particular, research on the human microbiome has begun to elucidate the community structures associated with the human body.
Important medical perspectives, for instance, arise To whom correspondence should be addressed.
from comparing gut microbiome profiles to differentiate between healthy and diseased states (Turnbaugh et al., 2009).
To investigate the taxonomic composition of metagenomes, many studies focus on sequencing the 16S rRNA gene (Hugenholtz, 2002), which currently provides the best resolution in terms of the available number of operational taxonomic units in the reference databases (Stach and Bull, 2005).
Besides the selectivity of primers (Hong et al., 2009), another difficulty for quantitative analysis arises from the varying copy number of the 16S rRNA gene (Kunin et al., 2008).
A pure 16S analysis, however, completely neglects the functional potential encoded in the metagenome.
In contrast, whole metagenome sequencing allows simultaneous taxonomic and functional profiling, which provides a deeper insight into the structure of a microbial community.
The taxonomic profiling of whole metagenome sequencing reads is a challenging task, and several techniques have been developed to extract the phylogenetic signal encoded in the sequenced material.
To date, all approaches rely on the classification of sequencing reads.
In most cases, a supervised binning of sequences according to an assignment to predefined taxonomic categories is performed.
Although an unsupervised binning is possible for longer contigs (Teeling et al., 2004), the comparison of metagenomes becomes difficult with the inclusion of unlabeled bins.
Methods for the classification of sequencing reads have been based either on homology using sequence similarity or on genomic signatures in terms of oligonucleotide composition.
Homology-based methods include the taxonomic evaluation of BLAST hits (Huson et al., 2007; Kosakovsky Pond et al., 2009; Meyer et al., 2008) and phylogenetic analyses of particular marker genes (von Mering et al., 2007; Wu and Eisen, 2008) or protein domains (Krause et al., 2008; Schreiber et al., 2010).
Signature-based approaches have been realized based on the correlation of oligonucleotide frequencies (Teeling et al., 2004), machine learning techniques (Diaz et al., 2009; McHardy et al., 2007) and probabilistic models (Brady and Salzberg, 2009; Rosen et al., 2008).
All these methods are highly dependent on read length.
For homology-based approaches, the number of significant similarity hits decreases considerably for shorter reads (Wommack et al., 2008).
Additionally, the estimation of genomic signatures in sequencing reads becomes increasingly difficult for decreasing read lengths.
For sequence lengths below 1000 bp, earlier approaches showed a sharp breakdown in accuracy (McHardy et al., 2007), which has been improved with recent tools (Brady and Salzberg, 2009).
All methods require a minimum read length in order to be applicable.
In many cases, this condition restricts the use of ultra-short read techniques for metagenome profiling.
As a consequence The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:32 30/5/2011 Bioinformatics-btr266.tex] Page: 1619 16181624 Mixture models for taxonomic profiling of metagenomes of the length-dependent classification performance, the varying accuracy of taxonomic profiling methods particularly complicates the comparison of metagenomes.
With a rapidly increasing number of sequenced samples, comparative metagenomics faces the problem that many samples are difficult to compare due to different sequencing platforms with varying read lengths and platform-specific sequencing errors.
In particular, the read length is highly variable across different platforms and generations of sequencing technologies.
Another source of sequence length variability arises from different stages of assembly.
Depending on the number of reads and the diversity of the community, for many samples, a varying number of assembled contigs exists.
Although homology and signature-based methods perform significantly better on longer contigs, the sample-specific distribution of contig lengths introduces a bias towards more abundant species, which complicates the comparability of samples.
We here present a novel method for taxonomic profiling of metagenomes that is based on mixture modeling.
Instead of a classification of sequencing reads based on a read-specific estimate of oligonucleotide frequencies, our method performs an analysis of the total oligonucleotide composition of a sample.
The discrete distribution of oligonucleotides is modeled by a mixture of organism-specific oligonucleotide distributions as obtained from sequenced genomes.
Taxonomic profiling then means to obtain the organism weights of that mixture from an approximation of the metagenomic distribution.
We show that under a varying read length this mixture approach provides a more stable estimation of taxonomic composition than methods based on read classification.
In particular, this advantage implies a better comparability of samples across different sequencing platforms.
Another advantage of our approach is the computational speed; it is the first profiling approach that allows the analysis of large volumes of sequence data within a few minutes on a single laptop.
2 METHODS Genomic signatures in terms of oligonucleotide distributions have widely been used for genome-based characterizations of microbial organisms (Bohlin et al., 2009).
We here propose the analysis of the oligonucleotide distribution of a metagenome for the taxonomic characterization of the corresponding microbial community.
2.1 Computation of compositional parameters We model the oligonucleotide distribution of a metagenome by a linear mixture of organism-specific genomic signatures from a reference database.
Given the N oligonucleotide probabilities of the metagenomic and genomic signatures as N-dimensional vectors y and xi together with M positive organism weights wi, the metagenomic signature y arises from a convex combination of the database signature vectors xi: y= M i=1 wi xi (1) The organism weights are the free parameters of the model and provide the basis for all kinds of taxonomic profiling tasks.
To obtain a profile on a particular taxonomic level, all the weights of organisms belonging to the same category on that level are summed to yield the corresponding profile value.
A key question is how to determine the unknown mixture weights if only the metagenomic and genomic signatures are given.
In general, it will be impossible to exactly reconstruct the metagenomic signature by some limited amount of genomic database signatures because a large fraction of organisms in the underlying community will not be covered by the available genomes.
Therefore, the mixture weights have to be chosen to yield a close approximation of the metagenomic distribution according to some distance measure.
The most common way would be to apply the EM algorithm (Dempster et al., 1977) to minimize the Kullback Leibler divergence between the metagenomic distribution and the mixture approximation.
However, the EM algorithm requires an initial estimate of the weights and only converges to a local optimum.
Therefore, we here consider a weighted L2-distance measure which gives rise to a convex optimization problem.
More specifically, in the N-dimensional space of oligonucleotide probabilities we minimize the normalized squared Euclidean distance between the metagenomic and model signatures.
With xij and yj denoting the probability of oligonucleotide j for database organism i and the target (meta)genome, respectively, we use the following error function: E(w) = M i=1 N j=1 (wi xij yj)2 2j (2) s.t.
M i=1 wi =1 (3) with weight vector w containing positive weights wi 0.
The standard deviation j of dimension j can be estimated from oligonucleotide frequencies observed for the M database genomes.
Minimization of the above error gives rise to a convex quadratic programming problem (QP), which can be solved by standard optimization tools.
We used a corresponding function from a MATLAB SVM toolbox (Canu et al., 2005).
In general, the solution is unique if the dimensionality of the signature vectors exceeds the number of database organisms used for the approximation.
For uniqueness, the database signature vectors have to be linearly independent such that no organism-specific oligonucleotide signature can be perfectly (without error) reconstructed by a convex combination of the other signature vectors.
This kind of non-redundancy condition geometrically means that all signature vectors have to be vertices of their convex hull.
In practice, however, a redundancy elimination of signature vectors on the organism level is actually not necessary.
For very close signatures, the solution is not unique only for the associated weights of the corresponding organisms.
This ambiguity does not affect less specific phylogenetic levels, since here the organism-specific weights of closely related organisms are aggregated.
In contrast to homology-based methods, the use of many closely related organisms does not imply a profiling bias toward these organisms.
2.2 FOU error and profile divergence To map the value of the above approximation error E to an interpretable scale between 0 and 1, we compute an additional error measure which we refer to as the fraction of oligonucleotides unexplained (FOU).
We define the FOU as the total one-sided error of predictions in the oligonucleotide frequency space FOU= 1 2 N j=1 |yj yj| (4) where yj is the relative frequency of oligonucleotide j from the observed metagenomic signature and yj is the corresponding prediction resulting from the estimated combination of genomic signatures.
The FOU measures the fraction of metagenomic DNA that cannot be explained by the mixture of genomic signatures.
It equals the sum of deviations resulting from underpredicted relative frequencies that are lower than the corresponding observed metagenomic frequencies.
The same error is necessarily obtained from the sum of deviations resulting from overpredicted frequencies due to the unit sum of all probabilities of a signature.
In general, the FOU error of the Taxy method cannot be used for quantification of phylogenetic novelty because the divergence between the 1619 [12:32 30/5/2011 Bioinformatics-btr266.tex] Page: 1620 16181624 P.Meinicke et al.
observed profile and the mixture model is forced to be minimized by the approximation method.
Only in cases where even on higher phylogenetic levels no related organisms/signatures exist in the database, an increased FOU may indicate novel organisms.
Analogously to analyses based on sequence similarity, this case will be hard to distinguish from a degradation of sequence quality.
On the other hand, the unusual case where all metagenomic sequences refer to the known database organisms can easily be detected by a vanishing FOU.
Because the FOU error is defined as a distance between discrete distributions, also the divergence between two taxonomic profiles on the same phylogenetic level can be measured in that way.
In this case, the absolute deviations in Equation (4) arise from the taxon-specific fractions on a particular level.
In order to be able to compare discrete distributions, the corresponding categories have to be the same for all profiles and the weights of a profile have to sum up to some unique constant.
In particular, when read classification methods are used, the comparison cannot include unclassified reads, i.e.
reads that have not been assigned to some taxonomic category.
In the following, we use the profile divergence to measure the deviation of a predicted profile from a reference profile.
Similar to the FOU error, also the profile divergence can be interpreted in terms of a percental deviation.
2.3 Genomic and metagenomic signatures The oligonucleotide probabilities can be estimated by counting exact DNA word matches of a certain length.
For a word length k, the estimate comprises 4k relative frequencies according to the number of different DNA words.
In the case of fragmented data, the estimate was established by summing up the oligonucleotide counts of all individual reads, contigs or chromosomes.
To ensure that read orientation did not affect the taxonomic prediction, oligonucleotide counts of the reverse complement were added to every metagenomic and reference signature.
This scheme implies a loss of information, which approximately halves the number of distinguished oligonucleotides.
Finally, the oligonucleotide counts of each signature were normalized to relative frequencies.
For the genomic reference signatures, we chose the KEGG organism database (Kanehisa and Goto, 2000) as of March 2010, providing 1013 fully sequenced prokaryotic genomes.
The NCBI taxonomy database was used for taxonomic annotation of the genomic signatures.
As outlined in Section 2.1 , the high number of reference organisms suggests a minimum word length of k =6 to provide a unique solution for the mixture weights.
In this case, the combination of two read orientations implies 2000 non-redundant dimensions within a 4096-dimensional signature vector.
For most metagenomic datasets, also longer words may be considered.
Because the memory requirements for storing all reference signatures increase with k, we limited the word length to a maximum value of k =8 to enable the computation on most of the current notebook and desktop architectures.
For the experimental analyses in the following study, we used a medium word length of k =7.
3 RESULTS 3.1 Profile comparison First, we evaluated our method, which we refer to as Taxy in the following, on the Northern Schneeferner glacial ice sample (Simon et al., 2009).
This sample was used in Schreiber et al.
(2010) to compare the predictions of the Treephyler tool with the results of CARMA (Krause et al., 2008), Phymm (Brady and Salzberg, 2009) and a classical 16S analysis.
For the comparison with Taxy, we additionally used the homology-based web tool Galaxy (Kosakovsky Pond et al., 2009) for taxonomic profiling analysis.
As suggested by the original study, we used an intermediate level between phylum and class rank for comparison.
The sequence data from the glacier sample comprise 1 076 539 pyro-sequencing reads with an average 200 bp read length.
The approximation error (FOU error, see Section 2) of Taxy based on heptamer signatures was 0.021 and 0.02 for the QP and EM method, respectively.
These low values indicate a good approximation of the metagenomic oligonucleotide distribution by the reference signatures.
As shown in Figure 1, the taxonomic distribution predicted by Taxy was largely congruent with CARMA and Treephyler.
For Alphaproteobacteria, Taxy showed a clearly lower level, which was closer to the 16S analysis.
The profile divergence of the Taxy-based abundances from the 16S profile was 32.7 percentage points (p.p.).
CARMA, Treephyler, Phymm and Galaxy diverged by 24.0, 25.3, 48.9 and 83.1 p.p., respectively, from the 16S-based prediction.
Here, Galaxy showed large peaks for the Gammaproteobacteria and Firmicutes phyla, which can be explained by the overrepresentation of associated organisms in nucleotide databases.
The maximum common difference between the tool-based predictions and the 16S profile occurred in the Bacteroidetes phylum.
The Taxy prediction differed from the 16S result by 19.6 p.p.
in this phylum, while CARMA, Treephyler, Phymm and Galaxy differed by 17.5, 18.0, 26.9 and 30.8 p.p., respectively.
In many phyla, Taxy constituted a compromise between the Phymm and the CARMA/Treephyler-based predictions.
While Taxy was not as close to the 16S profile as CARMA and Treephyler, it was closer to the 16S level than Phymm and Galaxy.
However, the large Bacteroidetes prediction divergence of all tools with respect to the 16S analysis highlights the difficulty of establishing a gold standard for the taxonomic profiling of metagenomic data.
Note that the comparison with a 16S analysis is not unquestionable due to the varying copy number of the corresponding marker gene.
An additional problem arises from a possible bias of the 16S primers which has been reported to favor the amplification of Bacteroidetes 16S rRNA in human gut samples (Gill et al., 2006).
Finally, marker gene counts measure organism frequency rather than genomic DNA content, which is measured by the above tools.
In addition, we used the simHC dataset introduced in Mavromatis et al.
(2007) to measure the accuracy of taxonomic profiling methods on a simulated high-complexity community (see Supplementary Material).
Besides the limited realism of a simulated metagenome, another problem arises from possible overlaps between reference or training organisms used by the profiling tools and the database organisms which have been used to construct the simulated metagenome sequences.
To reduce this overlap, we removed all reference/training organisms belonging to genera which are present in the simHC data from the tools.
Because we were not able to fully exclude these organisms from the CARMA and Treephyler prediction engines only Taxy, Phymm and, in addition, homology-based results from the Galaxy server (Kosakovsky Pond et al., 2009) were used in this evaluation.
Besides the removal of all genus-level overlaps with simHC, we chose the remaining set of 654 reference organisms to be equal for all three tools to ensure comparability of the corresponding methods.
In Figure 2, the estimated profiles at class level for all three methods are shown together with the original profile according to the known composition of the simulated metagenome.
In most taxonomic categories, the predictions agree well with the original profile.
An exceptional deviation can be observed for the Galaxy predicted fraction of Alphaproteobacteria, which exceeds the original fraction by 26.1 p.p.
To investigate whether this peak 1620 [12:32 30/5/2011 Bioinformatics-btr266.tex] Page: 1621 16181624 Mixture models for taxonomic profiling of metagenomes Fig.1.
Phylum/class-level taxonomic profiles of the Norther Schneeferner metagenome as obtained from Taxy, CARMA, Galaxy, Phymm and Treephyler in comparison with a 16S rRNA profile.
Fig.2.
Class-level taxonomic profiles of the simHC simulated metagenome as obtained from Taxy, Galaxy and Phymm in comparison with the original profile according to the known fractions of taxa.
arises from the particular configuration of the homology search step within Galaxy, we used different BLAST parameter settings and repeated the analysis.
However, the high deviation for the Alphaproteobacteria class was observed for all configurations.
In contrast, the Taxy and Phymm predictions for Alphaproteobacteria were close to the original.
In total, the profile divergences of Taxy, Phymm and Galaxy were 20.7, 22.5 and 33.84 p.p., respectively.
We also analyzed the more general phylum level, where the corresponding values were 17.9, 18.3 and 17.2 p.p.
The divergences for Taxy, Phymm and Galaxy on the more specific order level were 57.6, 46.8 and 56.4 p.p., which indicates that an estimation of the taxonomic distribution on this level is generally difficult within the chosen simHC setup.
3.2 Read length dependence For comparative metagenome analyses, it is highly desirable that read length does not affect the estimation of taxonomic composition.
Therefore, we also investigated the variation of profiling results with respect to a varying read length.
For that purpose, we compared the read length dependence of Taxy, Galaxy and Phymm on the hypersaline microbial mat samples introduced in Kunin et al.
(2008).
1621 [12:32 30/5/2011 Bioinformatics-btr266.tex] Page: 1622 16181624 P.Meinicke et al.
Table 1.
Profile divergence between results obtained from full and fragmented reads of the hypersaline microbial mat samples Method Read length (bp) Mean Max.
Min.
350 0.21 0.29 0.18 Taxy 175 0.34 0.49 0.22 80 0.64 0.79 0.47 350 5.51 7.95 3.54 Galaxy 175 6.09 8.01 4.26 80 8.17 13.99 4.98 350 2.55 4.32 2.00 Phymm 175 6.30 10.91 5.04 80 10.32 19.32 5.29 Statistics in terms of the mean, maximum and minimum values over all 10 depth-specific samples for the Taxy, Galaxy and Phymm results.
The dataset includes 129 147 unassembled Sanger sequencing reads (700 bp read length) from 10 samples according to different depth layers of the mat.
For reasons of space, an analysis of the taxonomic profiles of all depth layers as estimated by the three methods on full-length reads can be found in the Supplementary Material.
We studied the effect of read length dependence by measuring the divergence between profiles obtained from full length data and different versions of fragmented data.
We chose three different fragment lengths (350, 175 and 80 bp) to reflect the range of read lengths provided by current sequencing technologies.
The fragmentation was implemented through a simple read splitting, which cut the original reads into fragments that approximately met the desired read lengths (see Supplementary Material).
Because the fragments did not overlap, this scheme implied a loss of oligonucleotide information around the fragment border.
For a word length of 7 bp and an average read length of 80 bp, about 10% of the original heptamers in the full-length reads were lost.
The results in Table 1 indicate that, on average, the phylum level divergence of the Taxy tool was at least one order of magnitude lower than the corresponding Galaxy and Phymm results.
Thereby, the overall profile divergences of Galaxy and Phymm were rather similar.
All methods exhibited a correlation between the fragment length and profile divergence.
Both Galaxy and Phymm exhibited a stronger than average divergence in the top three layers, while Taxy did not diverge above average in these layers (see Supplementary Table S1).
The Taxy method showed a very even and predictable small divergence due to the above-mentioned loss of oligonucleotide information in the fragments.
In contrast, the Galaxy and Phymm profiles showed a large variation of the divergence with maximum deviations of 14.0 p.p.
(Galaxy) and 19.3 p.p.
(Phymm) for the 80 bp fragments in the first layer.
The length dependence of fragment classification methods in particular can be problematic if partially assembled data have to be analyzed.
With todays high-throughput sequencing technologies, even microbial communities with a medium complexity allow to assemble a large fraction of the original reads into longer contigs.
On the one hand, the classification of longer contigs is more reliable than assigning the original short reads to taxonomic categories.
On the other hand, a significant bias may arise from the fact that the probability that two reads can be assembled increases with the abundance of the corresponding organism.
We analyzed the profile divergence between partly assembled data and simulated short read data for a human gut sample (Kurokawa et al., 2007) where we compared Taxy with WebCARMA (Gerlach et al., 2009) and the NBC web server tool (Rosen et al., 2011) on phylum level (see Supplementary Material).
Because of the widely varying sequence length, for all methods, we compared the amount of DNA (bp) attributed to phylum level categories and not the number of sequences assigned to these categories.
While WebCARMA and NBC showed a large deviation for the most abundant phyla, reaching 12.9 and 21.8 p.p.
in the Bacteroidetes phylum, the deviations of our mixture approach were below 0.15 p.p.
in all categories.
3.3 Run time The Taxy runtime for the analysis of the 239.7 MB Northern Schneeferner dataset on a single core of an AMD Opteron (2.4 GHz) processor was 7.5 s. The single core run times for Treephyler, Phymm and CARMA were 12 h, 30 h and extrapolated 696 h, respectively.
Considering the computational cost for analysis of the hypersaline microbial mat data, the Taxy run time on a single core of a 2.66 GHz Intel processor for the analysis of all 10 sets (84.35 MB) was 9 s, while Phymm and the Galaxy analyses required about 69 h (CPU time) and 95 min, respectively.
The hardware requirements for a Taxy analysis are exceptionally low: we were able to process the complete 1.7 GB sequence file from the Sargasso Sea sample (Venter et al., 2004) on a notebook with a single core 1.4 GHz Pentium (M) CPU and 760 MB RAM under Octave 3.2.4 in 95 s. 3.4 Implementation The taxonomic profiling algorithm described above was implemented using the MATLAB programming language.
A MATLAB toolbox containing the computational routines, precomputed oligonucleotide signatures for 1013 reference organisms and documentation can be downloaded from http://gobics.de/peter/taxy.
The toolbox allows the profiling of a given metagenome sample (in multiple FASTA format) on different taxonomic levels (phylum, class, order, family, genus).
The output comprises histogram bar plots of the sample-specific taxonomic composition as well as comma-separated value (CSV) files for detailed analysis of the profiles with spreadsheet software such as Microsoft Excel.
The toolbox code is also executable with recent versions of Octave (3.0 and above, http://www.gnu.org/software/ octave/)), a freely available MATLAB-like software environment.
The toolbox was tested under Microsoft Windows and Linux and can easily be used on other platforms.
In addition, we provide an implementation of the proposed method as part of the freely available Taxy tool for Windows.
The Taxy tool prototype includes the precomputed taxonomic profiles of 256 metagenomes based on sequence data obtained from the CAMERA web site (Seshadri et al., 2007).
Here, the 256 samples with the corresponding profiles can also be used for comparative analysis.
Furthermore, the graphical user interface of the Taxy tool allows the user to inspect the sample metadata and the taxonomic profile as estimated by the mixture modeling method (see also Supplementary Material).
4 DISCUSSION As do all other methods for taxonomic profiling of whole metagenome sequences, our method crucially depends on the range 1622 [12:32 30/5/2011 Bioinformatics-btr266.tex] Page: 1623 16181624 Mixture models for taxonomic profiling of metagenomes of microbial reference genomes available in current databases.
The phylogenetic coverage of these genomes directly determines the limits for the achievable taxonomic resolution.
Because genome databases still suffer from a significant bias toward certain culturable organisms, an important impact on profiling performance is expected from recent efforts to broaden the range of sequenced organisms (Wu et al., 2009).
Obviously, all profiling methods will largely benefit from a more even sampling of the microbial world.
In several cases, it may be useful to include eukaryotic organisms in the analysis of the taxonomic composition.
In particular, the inclusion of a known host genome provides a straightforward way to identify the fraction of host-specific DNA in a sample.
In this context, the modular architecture of Taxy allows an easy integration of eukaryotic genomes in the database of signature vectors.
Preliminary results with 28 fully sequenced eukaryotic organisms added to the database show that Taxy was able to benefit from eukaryotic signatures in the analysis of an insect herbivore microbiome dataset (Suen et al., 2010), which is characterized by a high proportion of eukaryotic DNA (see Supplementary Material).
The main advantage of the Taxy approach over all existing methods is the inherent read length invariance of the composition estimates.
First of all, this property makes it possible to fully utilize ultra-short reads from all high-throughput sequencing technologies.
Secondly, without losing comparability, it allows the use of datasets with heterogeneous sequence lengths, which for instance arise from a combination of raw reads and assembled contigs.
In this case, the method is also robust with respect to erroneous assemblies because no taxonomic assignment of contigs is actually performed.
Finally, Taxy facilitates the comparability of data obtained from different sequencing platforms.
This advantage is of particular importance because the heterogeneity of sequencing technologies and the associated read lengths is still increasing.
Read length invariance, however, cannot cope with the variability of metagenomic protocols, which affect the preparation of samples before sequencing and which can severely degrade the comparability of data.
Other sources of variability, for instance, include the cloning bias of Sanger sequencing or particular sequencing errors.
Another consequence of the read length invariance is that the prediction performance cannot be assessed in terms of sensitivity and specificity as in sequence classification methods.
Because no single read is actually assigned to a taxonomic category, it is impossible to measure the performance in terms of detection accuracy.
Instead, the compositional parameters describing the abundance of taxonomic units are directly predicted from the overall oligonucleotide distribution.
For many problems of quantitative metagenome analysis, direct predictions of the taxonomic composition will be sufficient, but in some cases, a more detailed investigation is necessary.
For example, further analysis of reads from a particular phylogenetic group would require a sequence classification method for the identification of the corresponding reads.
Therefore, Taxy complements the current range of profiling methods rather than replacing any of the existing methods.
Furthermore, the organism-specific weights as obtained from a Taxy analysis can be used as priors in a probabilistic fragment classification framework such as the NBC approach (Rosen et al., 2008).
Currently, a number of web-based metagenome analysis systems exist, which provide the user with a comfortable platform for comparative metagenomics and taxonomic profiling: MG-RAST (Meyer et al., 2008), Galaxy (Kosakovsky Pond et al., 2009), IMG/M (Markowitz et al., 2008) and CAMERA (Seshadri et al., 2007).
Although these platforms are of great value for metagenome analyses, they show the typical disadvantages of web-based tools, such as restrictions on user-supplied data or long response times.
Several platforms are based on a BLAST (Altschul et al., 1990) engine, which matches the supplied sequence data against particular databases.
BLAST analyses usually involve a number of parameters that have a measurable effect on the results.
The optimal choice of BLAST parameters depends on the complexity and size of the sample and on the sequence length distribution.
As a consequence, the specific adjustment of parameters such as E-value, word length, minimal alignment length and percent identity for each metagenomic dataset can complicate a BLAST-based comparative analysis.
The same difficulties are encountered when using tools like MEGAN (Huson et al., 2007), which rely on prior results from a costly BLAST analysis.
As our Galaxy results on the glacial ice sample and on the simHC data demonstrate, also the taxonomical distribution of the reference database may affect the estimation of profiles.
On the other hand, read classification methods based on BLAST offer the adjustment of a similarity-based rejection criterion, which allows to exclude parts of the data from taxonomic profiling.
This can be a great advantage if sequence quality is low and it suggests the combination of different methods rather than favoring one single approach.
The particular utility of Taxy arises from a quick overview of the taxonomic distribution of large datasets, which can be a good starting point for any kind of computational metagenome analysis.
Besides the inherent read length independence of Taxy, which significantly simplifies comparative analysis, there is another striking advantage that qualifies the method as an excellent early stage data mining tool for metagenomics: the computational speed is orders of magnitude faster than that of any of the existing taxonomic profiling methods.
Therefore, large amounts of data can be processed without having access to extensive computational facilities.
All computations can be performed on a local standard PC requiring at most a few minutes for even the largest datasets.
This efficiency makes it possible to already obtain a first estimate of the sample composition, long before extensive computations on external servers or computer clusters may provide a more detailed picture of the community structure.
ACKNOWLEDGEMENT We would like to thank Peter Gumrich for programming the Taxy tool for Windows and Christian Opitz, Stefanie Mhlhausen and Alexander Kaever for additional technical support.
We further thank two anonymous reviewers for their helpful comments.
Funding: Grants from the Deutsche Forschungsgemeinschaft (ME 3138, Compositional descriptors for large scale comparative metagenome analysis to P.M. in part) and (LI 2050, Development of machine learning methods for functional characterization of the peroxisome T.L.
in part).
Conflict of Interest: none declared.
ABSTRACT Motivation: A large number of experimental studies on ageing focus on the effects of genetic perturbations of the insulin/insulin-like growth factor signalling pathway (IIS) on lifespan.
Short-lived invertebrate la-boratory model organisms are extensively used to quickly identify ageing-related genes and pathways.
It is important to extrapolate this knowledge to longer lived mammalian organisms, such as mouse and eventually human, where such analyses are difficult or impossible to perform.
Computational tools are needed to integrate and manipulate pathway knowledge in different species.
Results: We performed a literature review and curation of the IIS and target of rapamycin signalling pathways in Mus Musculus.
We com-pare this pathway model to the equivalent models in Drosophila mel-anogaster and Caenorhabtitis elegans.
Although generally well-conserved, they exhibit important differences.
In general, the worm and mouse pathways include a larger number of feedback loops and interactions than the fly.
We identify functional orthologues that share similar molecular interactions, but have moderate sequence similarity.
Finally, we incorporate the mouse model into the web-ser-vice NetEffects and perform in silico gene perturbations of IIS com-ponents and analyses of experimental results.
We identify sub-paths that, given a mutation in an IIS component, could potentially antagon-ize the primary effects on ageing via FOXO in mouse and via SKN-1 in worm.
Finally, we explore the effects of FOXO knockouts in three dif-ferent mouse tissues.
Availability and implementation: http://www.ebi.ac.uk/thornton-srv/ software/NetEffects Contact: ip8@sanger.ac.uk or thornton@ebi.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 21, 2014; revised on June 24, 2014; accepted on July 15, 2014 1 INTRODUCTION The insulin/insulin-like growth factor signalling pathway (IIS) and the target of rapamycin (TOR) signalling pathway have been shown to be important regulators of ageing across species via the transcription factor FOXO (Kenyon, 2011).
The mechanisms by which FOXO increased activity leads to lifespan extension are still unclear.
However, it is thought that lifespan extension is achieved through cell-cycle arrest by FOXO in the absence of insulin signalling (van der Horst and Burgering, 2007).
In addition, identification of FOXO transcriptional tar-gets has revealed a second tier of transcription factors regulating a variety of downstream responses (Alic et al., 2011).
Ageing via the IIS pathway has been intensively studied at the level of in-vertebrate model laboratory organisms.
With their short life-spans, well-described genomes and a variety of mutants already available, Drosophila melanogaster (Clancy et al., 2001) and Caenorhabtitis elegans (Kenyon et al., 1993) provide excellent frameworks for fast identification of genetic determinants of ageing.
Relating results from fly and worm to a longer lived mammalian model, such as Mus musculus (Bl uher et al., 2003), is critical for the understanding of the ageing processes in human, but it is often difficult owing to the large evolutionary distance between invertebrates and mammals.
The general flow of the pathway is as follows.
The insulin and insulin growth factor receptors can be activated by two different insulin molecules or two insulin-like growth factor molecules.
On activation, the two receptors can activate the insulin receptor substrates (IRS1-4) by tyrosine phosphorylation.
The role of IRS1 especially has been well examined and found to propagate the signal further downstream, via the PI3K complex.
The phospholipid products of PI3K [phosphatidylinositol-3,4,5-tri-phosphate (PIP3)], once produced, can activate phosphoinosi-tide-dependent kinase-1 (PDK1) that leads to the activation of AKT/protein kinase B-like proteins (AKT1-3, with AKT1 being well studied) and serum and glucocorticoid-inducible kinases (SGK1-3).
AKT1 and the SGK1-3 kinases inhibit the activity of the Forkhead transcription factors FOXO by retaining them in the cytoplasm.
The IIS pathway is generally well conserved, with the main building blocks (INSR, PI3K, PDK1, AKT, FOXO) present in both mammals and invertebrates.
The TOR pathway is also well conserved with its main building blocks present (TOR complexes 1 and 2, RHEB, S6 kinase).
Important differences also exist.
There are seven known insulin molecules in the fly, as opposed to 40 in the worm.
In the mouse, there are two insulin and two insulin-like growth factors.
Flies and worms possess a single in-sulin receptor each, whereas mice possess two insulin-like growth factor receptors in addition to the insulin receptor.
In mice, we *To whom correspondence should be addressed.
yPresent address: Mouse Informatics Group, Welcome Trust Sanger Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.
due Up ( ( )-Transcription Factors very-forty XPath error Undefined namespace prefix observe more copies of certain proteins, such as four copies of AKT (Akt1-4) instead of one in the fly and two in the worm and four genes encoding FOXO (Foxo1, Foxo2, Foxo4 and Foxo6) in contrast to a single FOXO in flies and worms (foxo and daf-16).
The TOR pathway is remarkably similar between flies and mice, but exhibits differences with the worm (Ivanov et al., 2013; Riesen et al., 2014).
Previously, we developed the web-service NetEffects to solve the problem of relating gene expression results to their effects at the protein signalling level of the IIS pathway and the ageing phenotype (Papatheodorou et al., 2012).
This is a common problem of many studies on ageing that use whole organism mutants to study changes in lifespan and uncover protein interactions within the signalling pathways by use of transcriptomic datasets.
NetEffects uses Answer Set Programming, a logic-based method for inference that uses manually curated maps of the IIS pathway and its relationship to lifespan as prior knowledge.
Given a gene mutation and the resulting genome-wide differential gene expression, NetEffects will deduce signalling effects and how they can influence lifespan according to the prior knowledge.
Our applications on fly and worm datasets (Ivanov et al., 2013; Papatheodorou et al., 2012) have revealed consistent homeostatic mechanisms across both long-and short-lived mutants.
2 METHODS The signalling network model of the IIS and TOR pathways was built using GraphML, using the editor yEd (http://www.yworks.com).
This enabled a computationally readable representation of the pathways, as well as a graphical visualization that relates molecular topology to cellular components.
During the curation process, we used the insulin pathway available in KEGG (Kanehisa et al., 2012) as a starting point and the rest of the connections were built by literature review.
Literature searches were performed using PubMed (http://www.ncbi.
nlm.nih.gov/pubmed/) by querying mouse gene names and identifiers.
Only connections with experimental support, rather than just suggestive or hypothetical, were used.
Proteinprotein interactions from yeast two-hybrid system or other screens were omitted.
Supporting literature for each relationship is available by clicking on the pathway connections within NetEffects.
All graphs from the comparative analyses across the fly, worm and mouse pathways were produced by custom-made R (http://www.r-pro-ject.org/) scripts.
These cross-species analyses were based on genomic sequence-based orthologous relationships, downloaded from Ensembl Compara (http://www.ensembl.org).
Orthology relationships that were not predicted by Compara but were suggested by the topology and con-nectivity of the genes in the pathway models, were sought in TreeFam (http://www.treefam.org/), OrthoDB (http://cegg.unige.ch/orthodb7) and Phylome (http://phylomedb.org/).
The mouse pathway was incorporated to the web-service NetEffects, built using PHP (PHP: Hypertext Preprocessor), JavaScript and Perl.
Proteins on the pathway have been annotated with Ensembl identifiers to enable the import and analysis of experimental datasets.
The theoretical perturbations option of NetEffects was used to query the pathway and produce inferences on the possible paths to longevity from different mutations.
The Experimental Results Analysis option was used to analyse the gene expression datasets.
Raw files of the expres-sion datasets in Paik et al.
(2007) were analysed using the Limma package in R (Smyth, 2005).
3 RESULTS AND DISCUSSION 3.1 A model of the insulin and TOR signalling pathways for M.musculus We curated a model of the IIS, TOR and neighbouring pathways in the mouse, providing access to the underlying literature as clickable connections on the pathway within the web-service.
We found the IIS pathway to be well connected and to include several points of cross-talks with neighbouring pathways.
This enables it to respond to signals from a variety of sources rather than just extracellular insulin molecules.
IRS and SHC1 bind to GRB2, which then activates the MAPK/ERK pathway.
In add-ition to propagating insulin signals, IRS also receives feedback from other pathways, such as TOR by inhibition from S6 kinase (gene name Rps6kb1), JNK by inhibition from JNK1 and Wnt signalling through an inhibition by GSK3-beta.
SHC1 is phos-phorylated by the activated Insulin receptor, propagating the signal to MAPK/ERK via GRB2, SOS1 and SOS2.
AKT1 pro-vides another point of cross-talk with the TOR pathway, by directly inhibiting PRAS40, which then inhibits RPTOR, inhibit-ing TSC2.
AKT1 can also be activated by TOR complex 2.
Previous work on mouse mutants of the IIS and TOR signal-ling pathways have clearly shown a role for the insulin pathway in the regulation of lifespan, with null S6K (Selman et al., 2009) mice and null IRS-1 (Selman et al., 2008) mice showing signifi-cant lifespan extension when compared with wild-type.
Although there is so far no experimental confirmation of lifespan regula-tion by mammalian FOXOs, results from mice lacking one or more of FoxO1, FoxO3 and FoxO4 have revealed ageing-related phenotypes, such as reduced bone mass (Ambrogini et al., 2010) and the development of ageing-related diseases like thymic lymphomas, hemangiomas (Paik et al., 2007).
These results sug-gest that mammalian FoxOs play a protective role against age-related diseases.
In addition, Willcox et al.
(2008) provide evi-dence for FoxO3A genetic variation being associated with life-span in a large, well-phenotyped cohort of humans through a casecontrol study of five candidate genes.
3.2 Comparison of insulin and TOR signalling in fly, worm and mouse With the availability of thoroughly curated pathway models for each of the three species, we are now able to make comparisons of their components and connections.
Supplementary Table S2 summarizes the similarities and differences between the IIS and TOR pathway molecules across the three different species.
Figure 1 presents on the mouse IIS and TOR pathway model the occurrence of fly and/or worm orthologues, also present in the species-specific models.
Functional orthologues were also identified, where sequence similarity across species is moderate but molecular interactions and experimental evidence suggest that these pairs are indeed orthologues.
Such cases include ist-1, a worm orthologue to the IRS (OrthoDB); drr-2, a worm orthologue to eukaryotic translation initiation factors 4H (Ching et al., 2010) and 4B (Phylome Orthology); unc-51, worm orthologue to Ulk1/Atg1; let-363, worm orthologue to Mtor (TreeFam) and age-1, worm orthologue to Pik3ca (TreeFam).
Gene Deptor encodes a protein associated with the mammalian TORC1 that is absent from the 3000 I.Papatheodorou et al.
, in order se ; Ivanov etal., 2013visualisation in order `` T P '' `` '' &amp; S P M very , , to ,-5 insulin receptor substrate fly and worm genomes.
Glatter et al.
(2011) hypothesized that the gene appeared later in vertebrate evolution.
In general, the IIS and TOR pathways in the mouse appear considerably larger and with more cross-talk (see Supplementary Section S3).
This effect is partly because of the fact that the IIS and TOR pathways in the mouse have been more thoroughly studied, as well as to the different extents of the curation of the neighbouring pathways within each organism.
Being used as a model organism for studies on human diseases such as cancer and diabetes and with the availability of a large number of murine cell lines, more interactions within and between the IIS, TOR and their neighbouring pathways have been discovered.
Cross-talk points between the IIS and TOR pathway include interactions between AKT1 and TORC2 in all three organisms, as well as RPS6KBA (S6 kinase) and IRS1 in mouse and fly.
In the mouse, both AKT and IRS involve several paralogous copies.
The IRS genes in the mouse are also involved in the cross-talk between JNK and IIS pathways, an interaction that is conserved across species.
In some cases, the interactions be-tween neighbouring pathways are not conserved due to the lack of orthologues in worms or flies or both.
For example, there is no orthologue for TSC2 in the worm which is inhibited by AKT in flies and mice.
There are also cases where orthologues in the invertebrates exist, but the interactions have not been observed experimentally as exemplified by the interaction of IIS and MAPK/ERK pathways via GRB2 and SOS1.
Finally in the case of MAPK/ERK to TOR cross-talk, facilitated by the acti-vation of RPS6KA1 by MAPK1 in the mouse, we found ortho-logues in both other organisms but no interaction in the fly.
In the worm, there is evidence for a proteinprotein interaction between them (see Supplementary Section S3D for the complete table of cross-talk points).
3.3 Paths to FOXO-mediated longevity with NetEffects We incorporated the mouse pathway model into the web service NetEffects (Papatheodorou et al., 2012) to enable computational analyses.
We can now compare the effects on FOXO-mediated ageing across the three species.
Using the theoretical perturb-ations functionality, we tested the paths to FOXO-mediated longevity in the mouse and worm from already known mutants in flies (see Supplementary Section S4 for full results).
Knocking out Ins1 or Ins2 in mouse results in a path consistent with those obtained when doing the equivalent test in the fly and worm models (Table 1).
This leads to increased lifespan through inhib-ition of AKT, which then allows translocation of FOXO into the nucleus.
However, in mouse and worm, we also encounter paths that reduce longevity.
In the mouse, this path involves the IGF1-receptor and RACK1 (gene Gnb2l) that leads to enhanced AKT phosphorylation and activation.
This effect, however, appears to be cell-type-specific and probably also context-specific, as Fig.1.
The model of the IIS and TOR pathways in M.musculus.
Colours indicate the existence of orthologues in flies, worms, both or none.
A larger version of this model is available in Supplementary Figure S1 3001 Mouse model of IIS and TOR hypothesised s s due to--s s .-in order `` '' s sdescribed in Kiely et al.
(2005).
In the worm, there is also an effect that might be antagonizing the FOXO-mediated lifespan increase, but is mediated by LET-60 (Kras orthologue) and the transcription factor SKN-1.
Similar effects were produced when Igf1r was mutated.
Mutation of Insr had similar results to the equivalent manipulations in the fly and worm.
We also tested known mouse mutants in the IIS pathway that affect lifespan.
The Irs1/ mutant (Selman et al., 2008) results in a long-lived phenotype, as expected from previous knockout ex-periments on the fly orthologue chico (Clancy et al., 2001).
In contrast, the Irs2/ mutant is short-lived (Selman et al., 2008).
NetEffects infers similar paths for both Irs1/ and Irs2/ mu-tants, as they exhibit similar connections to other components of the IIS and TOR pathways (Table 1).
The shortest path that leads to increase in lifespan involves inhibition of AKT.
The shortest paths leading to a reduction of lifespan require increased activity of GSK3B leading to inhibition of FOXO via SIRT1 and E2F1.
Functional experiments, as well as genome-wide gene expression in the two mutants could show whether Irs2/ mutant mice reduce their lifespan via a different route, and whether the activity of GSK3B plays a role.
We also tested ist-1, the func-tional worm orthologue for Irs1 and Irs2, where a knockout experiment with lifespan analysis has not been performed.
In addition to the FOXO-mediated lifespan extension, we obtained a sub-path of the same antagonistic effect via SKN-1 as in the INS1/2 tests shown above.
Partial support for the opposing effect of LET-60 (RAS) via SKN-1 to FOXO-mediated lifespan extension comes from a study on long-lived age-1 (PI3K) worms, where downregulation of let-60 and skn-1 genes was observed (Tazearslan et al., 2009).
Finally, we analysed the expression datasets in cells derived from three different tissues of null and conditional alleles in the three main Foxo genes (FoxO1/+; FoxO3/; FoxO4/).
The datasets were generated by Paik et al.
(2007).
We analysed the Foxomutants in liver and lung endothelial cells and thymus cells.
According to the authors, liver cells presented cancer-related phenotypes, whereas lung cells did not present a detectable phenotype.
With NetEffects we can show that 16 genes within the pathway model are differentially expressed in liver, one in lung and three in thymus (excluding the three Foxo genes that have been knocked out).
Almost all of the shortest paths starting from these differentially expressed genes correspond to negative feedback to the mutation of Foxo genes, as shown by the pre-dicted impact on lifespan (Supplementary Section S5).
This sug-gests that the function of the three Foxo genes plays a greater part in the liver and thymus rather than in lung endothelial cells.
Similar negative feedback was identified in our previous analyses of Foxo null mutants in the fly Papatheodorou et al.
(2012).
4 CONCLUSION The molecular basis of nutrient signalling is largely comparable across a large evolutionary space, despite striking differences in the presence or number of copies of certain components between species.
The pathways, in all organisms except worm focus only on FOXO-mediated lifespan, thereby ignoring any effects through different transcription factors.
However, using the mouse pathway, we can identify neighbouring signalling path-ways with the potential to influence the signal transduction of the IIS through the identified cross-talk points.
Comparison of the pathways in a systematic and qualitative way has the po-tential to explain differences in effects on lifespan across species and help design experiments that will evaluate the pathway flux.
The richness and detail of the mammalian model can inform the interpretation of results in invertebrates, where molecular interactions have not been so extensively studied.
Being able to compare the pathways side by side, we recorded all differences and identified functional orthologues.
By use of NetEffects, we were able to suggest possible paths affecting FOXO-mediated Table 1.
Shortest paths to FOXO-mediated longevity, derived from NetEffects, where DAF-16 is the FOXO orthologue in C.elegans 3002 I.Papatheodorou et al.
s--&Unicode_x2215;--, ,--&Unicode_x2215;----&Unicode_x2215;----&Unicode_x2215;----&Unicode_x2215;--knock-&Unicode_x2215;--&Unicode_x2215;+--&Unicode_x2215;----/--1 3lifespan given a single mutation and how these differ across spe-cies, thus generating hypotheses for further investigation.
ACKNOWLEDGEMENTS The authors thank Dobril Ivanov and Matthias Ziehm for useful discussions.
Funding: This work was funded by the Wellcome Trust Strategic Award WT081394MA (I.P., J.M.T.)
and by the European Molecular Biology Laboratory (EMBL) (R.P., J.M.T.).
Conflict of interest: none declared.
ABSTRACT Summary: We have developed ClueGO, an easy to use Cytoscape plug-in that strongly improves biological interpretation of large lists of genes.
ClueGO integrates Gene Ontology (GO) terms as well as KEGG/BioCarta pathways and creates a functionally organized GO/pathway term network.
It can analyze one or compare two lists of genes and comprehensively visualizes functionally grouped terms.
A one-click update option allows ClueGO to automatically download the most recent GO/KEGG release at any time.
ClueGO provides an intuitive representation of the analysis results and can be optionally used in conjunction with the GOlorize plug-in.
Availability: http://www.ici.upmc.fr/cluego/cluegoDownload.shtml Contact: jerome.galon@crc.jussieu.fr Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Since the number of genes that can be analyzed by high-throughput experiments by far exceeded what can be interpreted by a single person, different attempts have been initiated in order to capture biological information and systematically organize the wealth of data.
For example Gene Ontology (GO) (Ashburner et al., 2000) annotates genes to biological/cellular/molecular terms in a hierarchically structured way, whereas Kyoto encyclopedia of genes and genomes (KEGG) (Kanehisa et al., 2002) and BioCarta assigns genes to functional pathways.
Several functional enrichment analysis tools (e.g.
Boyle et al., 2004; Huang et al., 2007; Maere et al., 2005; Ramos et al., 2008; Zeeberg et al., 2003) and algorithms (e.g.
Li et al., 2008) were developed to enhance data interpretation.
As most of these tools mainly present their results as long lists or complex hierarchical trees, we aimed to develop ClueGO a Cytoscape (Shannon et al., 2003) plug-in to facilitate the biological interpretation and to visualize functionally grouped terms in the form of networks and charts.
Other tools like BiNGO (Maere et al., 2005) or PIPE (Ramos et al., 2008) assess overrepresented GO terms To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.
and reconstruct the hierarchical ontology tree, whereas ClueGO uses kappa statistics to link the terms in the network.
Compared with the approach of Ramos et al.
(2008) which creates an in silico annotation network based on pathways and protein interaction data and maps the gene list of interest afterwards, ClueGO generates a dynamical network structure by already initially considering the gene lists of interest.
ClueGO integrates GO terms as well as KEGG/BioCarta pathways and creates a functionally organized GO/pathway term network.Avariety of flexible restriction criteria allow for visualizations in different levels of specificity.
In addition, ClueGO can compare clusters of genes and visualizes their functional differences.
ClueGO takes advantage of Cytoscapes versatile visualization framework and can be used in conjunction with the GOlorize plug-in (Garcia et al., 2007).
2 METHODS AND IMPLEMENTATION ClueGO has two major features: it can be either used for the visualization of terms corresponding to a list of genes, or the comparison of functional annotations of two clusters.
2.1 Data import Gene identifier sets can be directly uploaded in simple text format or interactively derived from gene network graphs visualized in Cytoscape.
ClueGO supports several gene identifiers and organisms by default and is easy extendable for additional ones in a plug-in like manner (Supplementary Material).
2.2 Annotation sources To allow a fast analysis, ClueGO uses precompiled annotation files including GO, KEGG and BioCarta for a wide range of organisms.
A one-click update feature automatically downloads the latest ontology and annotation sources and creates new precompiled files that are added to the existing ones.
This ensures an up-to-date functional analysis.
Additionally ClueGO can easily integrate new annotation sources in a plug-in like way (Supplementary Material).
2.3 Enrichment tests ClueGO offers the possibility to calculate enrichment/depletion tests for terms and groups as left-sided (Enrichment), right-sided (Depletion) or two-sided (Enrichment/Depletion) tests based on the hypergeometric distribution.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[18:02 30/3/2009 Bioinformatics-btp101.tex] Page: 1092 10911093 G.Bindea et al.
Furthermore it provides options to calculate mid-P-values and doubling for two-sided tests to deal with discreetness and conservatism effects as suggested by (Rivals et al., 2007).
To correct the P-values for multiple testing several standard correction methods are proposed (Bonferroni, Bonferroni step-down and Benjamini-Hochberg).
2.4 Network generation and visualization To create the annotations network ClueGO provides predefined functional analysis settings ranging from general to very specific ones.
Furthermore, the user can adjust the analysis parameters to focus on terms, e.g.
in certain GO level intervals, with particular evidence codes or with a certain number and percentage of associated genes.
An optional redundancy reduction feature (Fusion) assesses GO terms in a parentchild relation sharing similar associated genes and preserves the more representative parent or child term.
The relationship between the selected terms is defined based on their shared genes in a similar way as described by Huang et al.
(2007).
ClueGO creates first a binary gene-term matrix with the selected terms and their associated genes.
Based on this matrix, a termterm similarity matrix is calculated using chance corrected kappa statistics to determine the association strength between the terms.
Since the termterm matrix is of categorical origin, kappa statistic was found to be the most suitable method.
Finally, the created network represents the terms as nodes which are linked based on a predefined kappa score level.
The kappa score level threshold can initially be adjusted on a positive scale from 0 to 1 to restrict the network connectivity in a customized way.
The size of the nodes reflects the enrichment significance of the terms.
The network is automatically laid out using the Organic layout algorithm supported by Cytoscape.
The functional groups are created by iterative merging of initially defined groups based on the predefined kappa score threshold.
The final groups are fixed or randomly colored and overlaid with the network.
Functional groups represented by their most significant (leading) term are visualized in the network providing an insightful view of their interrelations.
Also other ways of selecting the group leading term, e.g.
based on the number or percentage of genes per term are provided.
As an alternative to the kappa score grouping the GO hierarchy using parentchild relationships can be used to create functional groups.
When comparing two gene clusters, another original feature of ClueGO allows to switch the visualization of the groups on the network to the cluster distribution over the terms.
Besides the network, ClueGO provides overview charts showing the groups and their leading term as well as detailed term histograms for both, cluster specific and common terms.
Like BiNGO, ClueGO can be used in conjuntion with GOlorize for functional analysis of a Cytoscape gene network.
The created networks, charts and analysis results can be saved as project in a specified folder and used for further analysis.
3 CASE STUDY To demonstrate how ClueGO assesses and compares biological functions for clusters of genes we selected up-and down-regulated natural killer (NK) cell genes in healthy donors from an expression profile of human peripheral blood lymphocytes (GSE6887, Gene Expression Omnibus).
For upregulated NK genes ClueGO revealed specific terms like Natural killer cell mediated cytotoxicity in the group Cellular defense response.
Downregulated in NK cells compared with the reference (a pool of all immune cell types) were genes involved in the innate immune response (Macrophages), but also in the adaptive immune response (T and B cell).
The common functionality refers to characteristics of leukocytes (chemotaxis), besides other terms involved in cell division and metabolism (Fig.1).
Fig.1.
ClueGO example analysis of up-and down-regulated NK cell genes in peripheral blood from healthy human donors.
(a) GO/pathway terms specific for upregulated genes.
The bars represent the number of genes associated with the terms.
The percentage of genes per term is shown as bar label.
(b) Overview chart with functional groups including specific terms for upregulated genes.
(c) Functionally grouped network with terms as nodes linked based on their kappa score level (0.3), where only the label of the most significant term per group is shown.
The node size represents the term enrichment significance.
Functionally related groups partially overlap.
Not grouped terms are shown in white.
(d) The distribution of two clusters visualized on network (c).
Terms with up/downregulated genes are shown in red/green, respectively.
The color gradient shows the gene proportion of each cluster associated with the term.
Equal proportions of the two clusters are represented in white.
1092 [18:02 30/3/2009 Bioinformatics-btp101.tex] Page: 1093 10911093 ClueGO 4 SUMMARY ClueGO is a user friendly Cytoscape plug-in to analyze interrelations of terms and functional groups in biological networks.
A variety of flexible adjustments allow for a profound exploration of gene clusters in annotation networks.
Our tool is easily extendable to new organisms and identifier types as well as new annotation sources which can be included in a transparent, plug-in like manner.
Furthermore, the one-click update feature of ClueGO ensures an up-to-date analysis at any time.
ACKNOWLEDGEMENTS We thank A Van Cortenbosch for the name of the tool.
Funding: INSERM; Ville de Paris; INCa; the Austrian Ministry for Science and Research, Project GEN-AU; BINII; the European 7FP Grant Agreement 202230 (GENINCA).
Conflict of Interest: none declared.
ABSTRACT The Red Queen said, It takes all the running you can do, to keep in the same place.
Lewis Carrol Motivation: Newly solved protein structures are routinely scanned against structures already in the Protein Data Bank (PDB) using Internet servers.
In favourable cases, comparing 3D structures may reveal biologically interesting similarities that are not detectable by comparing sequences.
The number of known structures continues to grow exponentially.
Sensitivethorough but slow search algorithms are challenged to deliver results in a reasonable time, as there are now more structures in the PDB than seconds in a day.
The brute-force solution would be to distribute the individual comparisons on a massively parallel computer.
A frugal solution, as implemented in the Dali server, is to reduce the total computational cost by pruning search space using prior knowledge about the distribution of structures in fold space.
This note reports paradigm revisions that enable maintaining such a knowledge base up-to-date on a PC.
Availability: The Dali server for protein structure database searching at http://ekhidna.biocenter.helsinki.fi/dali_server is running DaliLite v.3.
The software can be downloaded for academic use fromContact: liisa.holm@helsinki.fi 1 INTRODUCTION Comparative analyses of protein sequences and structures are a cornerstone of bioinformatics.
When sequence and structure similarities have an evolutionary origin, it is often possible to infer similarities in the biological functions of the proteins, which would be difficult to predict directly.
Structure comparisons have a longer look-back time than sequence comparison and have led to the identification of many super-families of distantly related proteins.
Many measures have been proposed to quantify structural similarity.
The Dali method uses a weighted sum of similarities of intra-molecular distances, which correlates with expert classifications in the sense that the structures of homologous proteins typically get higher similarity scores than the structures of evolutionarily unrelated proteins (Sierk and Pearson, 2004).
This property is useful to a biologist using structure comparison to learn more about her query protein: the biologically informative neighbours are found at the top of the match list with relatively few false leads.
To whom correspondence should be addressed.
The Dali method has been used to systematically scan new structures against the Protein Data Bank (PDB) for some 15 years (Holm and Sander, 1994).
The overall strategy is to screen the structure database with many different methods, starting with fast but unreliable ones and ending with the most sensitive but slow methods.
This ensures that no significant similarity is missed.
The search space is pruned between methods; if a strong match has been found, then subsequent methods only compare the query structure to the neighbours of the strong match.
This strategy requires that all the neighbours of the known structures are precomputed in all versus all fashion within a representative subset of structures.
The size of the structure set has grown by two decades since the system was introduced, and all versus all comparison is a quadratic problem in the number of structures.
Recently, the paradigm of all versus all comparisons became untenable when the weekly PDB updates began to take more than a week to process.
DaliLite is a standalone package of the Dali algorithm.
The first release of DaliLite (Holm and Park, 2000) contained all the functionality of the Dali server at EBI except the site-specific, complicated database update protocol.
The main DaliLite program is a wrapper that calls a variety of methods for protein structure comparison.
New workflows can thus be easily implemented by rewiring the regulatory logicbut keeping the basic algorithms unchanged.
In DaliLite v.3, we introduce new options for database searching (DaliLite quick) and database updates (DaliLite update).
The new protocols improve server throughput and vastly simplify the updates, making the complete system portable.
The key change from earlier is that we abandon the all versus all matrix of similarities in favour of a connected graph of similarities.
The nodes of the graph represent protein structures and edges represent structural alignments.
Whereas before each representative structure was directly linked to all its structurally similar neighbours, we now require only that there is a path of continuous structural similarity through the graph.
The structural neighbours of a query structure are collected by walks through the graph.
Not only need the graph be less densely connected than the all versus all matrix, thus saving computational effort, but also there is the added benefit that the incremental updates of the structural similarity graph and the choice of structural representatives are completely decoupled.
2 METHODS 2.1 PDB clustering The PDB is highly redundant.
The structures of some proteins and their mutants have been determined in various conditions, though the structures 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Searching protein structure databases with DaliLite v.3 Table 1.
Comparison of DaliLite v.3, the SSM server and SCOP Query Dali q time (min) Dali q P Dali q Z AUC Dali l time (min) Dali l Z AUC SSM Q AUC SCOP T SCOP class 1c52 14 241 0.885 297 0.822 0.674 219 a.3.1.
1sfxA 12 401 0.757 357 0.733 0.386 331 a.4.5.
1azu 14 529 0.967 369 0.969 0.516 289 b.6.1.
1wk2A 7 49 0.800 201 0.966 0.197 60 b.122.1 3be7A 51 542 0.990 1861 0.968 0.843 135 c.1.9.
1qlwA 30 414 0.908 1211 0.897 0.576 456 c.69.1.
2baa 18 726 0.693 486 0.354 0.025 979 d.2.1 1wotA 13 500 0.558 251 0.642 0.009 179 d.218.1 AUC, area under the curve of reliability (TP/P) versus coverage (TP/T), where Ts (true) are members of the same SCOP (Murzin et al., 1995) superfamily as the query structure, Ps (positive) are the top n matches from the ranked list for varying n and TPs (true positive) are the intersection of sets T and P. Only PDB entries classified in SCOP v.1.73 were evaluated and each PDB entry was counted once.
SSM server (http://www.ebi.ac.uk/msd-srv/ssm/cgi-bin/ssmserver; Krissinel and Henrick, 2004) parameters were set to 10 : 10 and highest precision.
The DaliLite search was performed using the list (Dali l) or quick option (Dali q with MAX_HITS = 1000) and reporting matches with Z > 2. remain the same for classification purposes.
We use a representative subset at 90% sequence identity level (PDB90), derived from the current set of PDB sequences using CD-HIT (Li and Godzik, 2006).
The PDB contains over 100 000 structures (chains), which is reduced to about 20 000 PDB90 representatives.
Further clustering of similar folds at lower levels of sequence identity was not cost effective.
2.2 Structural similarity graph The structural similarity graph and alignment data are stored in a relational database (MySQL).
The graph is updated incrementally.
If a new structure has strong similarity to structures already in the graph, one edge is sufficient to connect the new structure to the graph in the proper neighbourhood.
If there is no strong match, we compare the new structure to all existing structures and add edges for all significant similarities.
Similarity is measured by Dali Z-scores.
Significant similarities have a Z-score above 2; they usually correspond to similar folds.
Strong matches have sequence identity above 20% or a Z-score above a cutoff that depends on the size of the query protein.
The Z-score cutoff was empirically set to n/10 4, where n is the number of residues in the query structure.
We additionally require that the complete structure is covered by structural alignments; a segment of the query structure longer than 80 residues without any structural matches always disqualifies a strong match.
2.3 Database searching The database search option DaliLite quick compares a query structure to all structures in the PDB, as organized in the structural similarity graph.
To initiate a transitive search of structures in the graph, the query structure must be attached to some structural neighbours.
Fast feature filters are often successful in finding near neighbours.
We currently use sequence comparison by Blast, GTG sequence motifs (Heger et al., 2007) and secondary structure triplets to rank the structures in PDB90.
We convert the feature filter scores to Z-scores in order to combine the ranked lists.
The top 100 structures are compared using the normal Dali procedures.
If a strong match is found, we move to the next step (transitive alignment).
Otherwise, the query structure is compared against all 20 000 structures in PDB90.
The entry points connect the query structure to one or more structures in the structural similarity graph.
These are direct (first shell) neighbours of the query.
Structures in the second shell are compared in batches of 100, selecting those with the strongest connections first.
Connection strength is the lesser Z-score along the path from query to the first neighbour to the second neighbour.
The transitive alignment (via first neighbour) between the query structure and second neighbour is used as starting point for refinement, skipping the costly alignment optimization from scratch.
The expansion is repeated until the connection strength drops below a Z-score cutoff of 2, or a maximum number of matches have been reported (default: MAX_HITS = 500).
3 RESULTS The utility of a protein structure database search method (i.e.
similarity measure and optimization algorithm) must depend on its ability to report back interesting matches.
As an illustration, we chose query and target structures representing diverse super-families from the four main structural classes in SCOP: cytochromes c and winged helix DNA-binding domains from the all-alpha class, cupredoxins and PUA-like domains from the all-beta class, metallo-dependent hydrolases and alpha/beta hydrolases from the alpha/beta class, and lysozyme-likes and nucleotidyltransferases from the alpha + beta class (Table 1).
Match lists were evaluated using the AUC, where the maximum value of one indicates perfect sensitivity and selectivity.
Compared to optimizing the alignment from scratch (DaliLite list), the new transitive search mode (DaliLite quick) is about 30 times faster, without affecting AUC much (we removed all pre-existing edges from the query structures to the structural similarity graph).
Compared to the SSM servers Q-score, the higher AUC values in Table 1 indicate superior discrimination of homologous proteins from unrelated proteins by Dalis Z-score.
In conclusion, Dali remains a useful tool for structural bioinformatics.
The Dali server has been running DaliLite quick for a number of months now, with a throughput of 50 user queriesa mixture of redundant and unique structuresper day per CPU.
Funding: Academy of Finland (grants #109849 and #1105210).
Conflict of Interest: none declared.
ABSTRACT Motivation: Large-scale methods for inferring gene trees are error-prone.
Correcting gene trees for weakly supported features often re-sults in non-binary trees, i.e.
trees with polytomies, thus raising the natural question of refining such polytomies into binary trees.
A feature pointing toward potential errors in gene trees are duplications that are not supported by the presence of multiple gene copies.
Results: We introduce the problem of refining polytomies in a gene tree while minimizing the number of created non-apparent duplications in the resulting tree.
We show that this problem can be described as a graph-theoretical optimization problem.
We provide a bounded heur-istic with guaranteed optimality for well-characterized instances.
We apply our algorithm to a set of ray-finned fish gene trees from the Ensembl database to illustrate its ability to correct dubious duplications.
Availability and implementation: The C++ source code for the al-gorithms and simulations described in the article are available at Contact: lafonman@iro.umontreal.ca or mabrouk@iro.umontreal.ca Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION With the increasing number of completely sequenced genomes, the task of identifying gene counterparts in different organisms becomes more and more important.
This is usually done by clus-tering genes sharing significant sequence similarity, constructing gene trees and then inferring macro-evolutionary events such as duplications, losses or transfers through reconciliation with the phylogenetic tree of the considered taxa.
The inference of accur-ate gene trees is an important step in this pipeline.
While gene trees are traditionally constructed solely from sequence align-ments (Guidon and Gascuel, 2003; Ronquist and Huelsenbeck, 2003; Saitou and Nei, 1987), recent methods incorporate infor-mation from species phylogenies, gene order and other genomic footprint (Akerborg et al., 2009;Boussau et al., 2013; Durand et al., 2003; Rasmussen and Kellis, 2011; Szollosi et al., 2013; Thomas, 2010; Wapinski et al., 2007).
A large number of gene tree databases are now available (Datta et al., 2009;Flicek, 2012; Huerta-Cepas et al., 2011; Mi et al., 2012; Schreiber et al., 2013).
But constructing accurate gene trees is still challenging; for example, a significant number of nodes in the Ensembl gene trees are labelled as dubious (Flicek, 2012).
In a recent study, we have been able to show that 30% of 6241 Ensembl gene trees for the genomes of the fishes Stickleback, Medaka, Tetraodon and Zebrafish exhibit at least one gene order incon-sistency and thus are likely to be erroneous (Lafond et al., 2013).
Moreover, owing to various reasons such as insufficient differ-entiation between gene sequences and alignment ambiguities, it is often difficult to support a single gene tree topology with high confidence.
Several support measures, such as bootstrap values or Bayesian posterior probabilities, have been proposed to detect weakly supported edges.
Recently, intense efforts have been put towards developing tools for gene tree correction (Berglund-Sonnhammer et al., 2006; Chaudhary et al., 2011; Chen et al., 2000; Doroftei and El-Mabrouk, 2011; Gorecki and Eulenstein, 2011a,b; Nguyen et al., 2013; Swenson et al., 2012; Wu et al., 2012).
A natural approach is to remove a weakly supported edge and collapse its two incident vertices into one (Beiko and Hamilton, 2006), or to remove dubious nodes and join resulting subtrees under a single root (Lafond et al., 2013).
The resulting tree is non-binary with polytomies (multifurcating nodes) repre-senting unresolved parts of the tree.
A natural question is then to select a binary refinement of each polytomy based on appropri-ate criteria.
This has been the purpose of a few theoretical and algorithmic studies conducted in the past years, most of them based on minimizing the mutation (i.e.
duplication and loss) cost of reconciliation (Chang and Eulenstein, 2006; Lafond et al., 2012; Vernot et al., 2009; Zheng et al., 2012).
In the present article, we consider a different reconciliation criterion for refining a polytomy, which consists in minimizing the number of non-apparent duplication (NAD) nodes.
A dupli-cation node x of a gene tree (according to the reconciliation with a given species tree) is a NAD if the genome sets of its two subtrees are disjoint.
In other words, the reason x is a duplication is not the presence of paralogs in the same genome, but rather an inconsistency with the species tree.
Such nodes have been flagged as potential errors in different studies (Chauve and El-Mabrouk, 2009; Flicek, 2012; Scornavacca et al., 2009).
In particular, they correspond to the nodes flagged as dubious in Ensembl gene trees.
We introduce the polytomy refinement problem in Section 2, and we show in Section 3 how it reduces to a clique decompos-ition problem in a graph representing speciation and duplication relationships between the leaves of a polytomy.
We develop a bounded heuristic in Section 4, with guaranteed optimality in well-characterized instances.
In Section 5 we exhibit a general methodology, using our polytomy refinement algorithm, for cor-recting NAD nodes of a gene tree.
We then show in Section 6 that this approach is in agreement with the observed corrections of Ensembl gene trees from one release to another.
*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com footprints 1 , 4 , 12 , 31 , 33, 36] [ 10 , 20, 24 , 28] `` '' [14] about , , [22] due [ 3, 8, 30, 37, 32] [2] `` '' [21] l , [ 5, 23, 34, 38] paper [ 29, 14] `` '' XPath error Undefined namespace prefix 2 THE POLYTOMY REFINEMENT PROBLEM Phylogenies and reconciliations.
A phylogeny is a rooted tree that represents the evolutionary relationships of a set of elements (such as species, genes, .
.
. )
represented by its nodes: internal nodes are ancestors, leaves are extant elements and edges repre-sent direct descents between parents and children.
We consider two kinds of phylogenies: species trees and gene trees.
A species tree S describes the evolution of a set of related species, from a common ancestor (the root of the tree), through the mechanism of speciation.
For our purpose, species are identified with gen-omes, and genomes are simply sets of genes.
As for a gene tree, it describes the evolution of a set of genes, through the evolution-ary mechanisms of speciation and duplication.
Therefore, each gene g, extant or ancestral, belongs to a species denoted by s(g).
The set of genes in a gene tree is called a gene family.
A leaf-label corresponds to a genome in a species tree, and to a gene belong-ing to a genome in a gene tree.
Given a phylogeny T, we denote by l(T) the leaf-set and by V(T) the node-set of T. Given a node x of T, we denote by l(x) and call the clade of x, the leaf-set of the subtree of T rooted at x.
We call an ancestor of x any node y on the path from the root of T to the parent of x.
In this case we write y5x.
Two nodes x, y are unrelated if none is an ancestor of the other.
For a leaf subset X of T, lcaTX, the lowest common ancestor (LCA) of X in T, denotes the farthest node from the root of T, which is an an-cestor of all the elements of X.
In this article, species trees are assumed to be binary: each internal node has two children, representing its direct descendants (see S in Fig.1).
For an in-ternal node x of a binary tree, we denote by x and xr the two children of x.
DEFINITION 1.
(Reconciliation) A reconciliation between a binary gene tree G and a species tree S consists in mapping each internal or leaf node x of G (representing respect.
an ancestral or extant gene) to the species s(x) corresponding to the LCA in of the set fsl; for all l 2 lxg.
Every internal node x of G is labelled by an event E(x) verifying: Ex= Speciation (S) if s(x) is different from sx and sxr, and Ex= Duplication otherwise.
We define two types of duplication nodes of a gene tree G. A Non-Apparent-Duplication (NAD) is a duplication node x of G such that [x2lxx  \ [y2lxry  =;.
A duplication that is not an NAD is an apparent duplication (AD) node, i.e.
a node with the left and right subtrees sharing a common leaf-label.
Therefore, any internal node x of G is of type S, AD or NAD.
The gene trees we consider might be non-binary.
We call polyt-omy a gene tree with a non-binary root (see F in Fig.1).
DEFINITION 2.
(Binary refinement) A tree HT is a refinement of a tree T if and only if the two trees have the same leaf-set and T can be obtained from HT by contracting some edges.
When HT refines T, each node of T can be mapped to a unique node of HT so that the ancestral relationship is preserved.
HT is a binary refinement of T if and only if HT is binary and is a refinement of T. In this article, as only binary refinements are considered, we omit the term binary from now.
Problem statement.
The general problem we address is the fol-lowing: Given a non-binary gene tree G and a species tree S, find a refinement of G containing theminimumnumber ofNADswith respect toS.
Such a refinement ofG is called aminimum refinement of G w.r.t.
S. Hence, we aim at refining each non-binary node of G. We first show that each such non-binary node of G can be refined inde-pendently of the other non-binary nodes.
THEOREM 1.
Let fGi; for 1 i ng be the set of subtrees of G rooted at the n children fxi; for 1 i ng of the root of G. Let HminGi;S be a minimum refinement of Gi w.r.t.
S. Let G0 be the tree obtained from G by replacing each Gi by HminGi;S. Then a minimum refinement of G is a minimum refinement of G0.
It follows from Theorem 1 that a minimum refinement of G can be obtained by a depth-first procedure iteratively solving each polytomy Gx, for each internal node x of G. In the rest of this paper, we consider G as a polytomy, and we denote by F the forest fG1;G2; .
.
.Gng obtained from G by removing the root.
For simplicity, we make no difference be-tween a tree Gi of F and its root.
In particular, sGi corresponds to srootGi, where rootGi is the root of Gi (Fig.1).
We are now ready to define the main optimization problem we consider.
Minimum NAD polytomy refinement (MinNADref) problem: Input: A polytomy G and a species tree S; Output: In the set HG of all refinements of G, a refinement H with the minimum number of NAD nodes.
Such a refinement is called a solution to the MinNADref problem.
3 A GRAPH-THEORETICAL CHARACTERIZATION We show (Theorem 2) that theMinNADref Problem reduces to a clique decomposition problem on a graph that represents the impact, in terms ofNADcreation, of joining pairs of trees fromF .
The join graph of a polytomy.
We first define a graph R based on the notion of join.
A join is an unordered pair fG1;G2g where G1;G2 2 F .
The join operation j on fG1;G2g consists in joining the roots of G1 and G2 under a common parent; we denote by (a) ((a,f),(g,h)) (g) (h,l) (b,e) (i,m) (d) ((c,d),(j,k)) (k) F : ((a , f),( g ,h))((c ,d),( j,k)) (a)(d)(g) (k) (b , e) (i , m) (h , l) R :: a b c d e f g h i j k l m (a) (d) (g) (k) (b,e) (i,m)(h,l) ((a,f),(g,h)) ((c,d),(j,k)) S : n H :: ((a , f),( g , h))((c ,d),( j , k)) (a)(d)(g) (k) (b , e) (i , m) (h  , l) Fig.1.
A forest F , a species tree S and the corresponding graph R. Each gene tree G of F is attached to its corresponding node s(G) in S. In R, joins of type AD are represented by green lines.
All other lines are the joins of type S. Non-trivial AD-components (AD-components containing at least two nodes) are represented by green ovals.
Red lines in R repre-sent a vertex-disjoint clique W of RS.
Here, RAD [W has a single con-nected component, which leads to the binary refinement H of F with no NAD.
After the joins of W are applied (red edges in H), the speciation-free forest can be joined with four joins AD (green vertices in H) i520 M.Lafond et al.
which , a paper which , paper G1;2 the resulting join tree.
We call the join type of j=fG1;G2g, and denote by jtG1;G2, the reconciliation label of the node created by joining G1 and G2 (i.e.
the root of G1;2), where jtG1;G2 2 fS;AD;NADg, respectively, for speciation, AD and NAD, w.r.t.
the species tree S. We denote by R=V;E the join graph of F , defined as the unoriented complete graph on the set of vertices V=F , where each edge (join) is labelled by the corresponding join type (Fig.1).
We denote by RS and RAD the subgraphs of R defined by the edges of type, respectively, S and AD.
We call a connected component of RAD an AD-component.
Let F0 be the new forest obtained by replacing the two trees G1 and G2 of F by the join tree G1;2.
The rules given below, follow-ing directly from the definition of speciation and duplication in reconciliation, are used to update the join type jtG1;2;T for any T 2 FnfG1;G2g.
Ruleset 1 (1) If jtG1;T=AD or jtG2;T=AD, then jtG1;2;T=AD; (2) Otherwise, if jtG1;T=NAD or jtG2;T=NAD, then jtG1;2;T=NAD; (3) Otherwise, if lca(T) is not a descendant of lcaG1;2, then jtG1;2;T=S; (4) Otherwise, jtG1;2;T=NAD.
Clique decomposition of the join graph.
Let a join sequence J=J1; J2; .
.
.
; JjJj be an ordered list of joins.
We denote by FJ; i the forest obtained after applying the first i joins of J, starting with F .
Note that FJ; 0=F , and that Ji 2 J is a join on FJ; i 1.
Let J denote the set of all possible join se-quences of size jF j 1.
Clearly, applying all joins of a sequence J 2 J yields a single binary tree, and there exists a gene tree H 2 HG with d NADs if and only if there exists a join sequence J 2 J with d joins of type NAD.
We refine this property by showing that there is a solution to the MinNADref problem where all duplication nodes are ancestral to all speciation nodes (see the treeH of Fig.1 for an example).
The proof (not shown) makes abundant use of Ruleset 1.
LEMMA 1.
There exists a binary refinement H 2 HG with d NADs if and only if there exists a join sequence J 2 J with d joins of type NAD such that, if Ji 2 J is the first join not of type S in J, then all following joins Jj, for j4i, are of type AD or NAD.
We define a speciation tree as a gene tree in which every internal node is a speciation node.
We deduce from the previous lemma that we can obtain a solutionH to the MinNADref prob-lem by creating a forest of speciation trees first, then successively joining them with joins of type AD or NAD.
As the nodes of R corresponding to the leaves of a given speciation subtree ofH are pairwise joined by speciation edges, they form a clique in RS (in Fig.1 the cliques in red are selected and the corresponding joins are applied to compute refinement H).
The next theorem makes the link between the number of NADs ofH and the cliques of RS.
For a set W of vertex-disjoint cliques of RS, we denote by RAD [W the graph defined by the union of the edges of RAD and W. THEOREM 2.
A solution to the MinNADref Problem has d NADs if and only if, among all graphs RAD [W where W is a set of vertex-disjoint cliques of RS, at least one has d+ 1 connected com-ponents and none has less than d+ 1 connected components.
The proof of Theorem 2 is constructive.
Given an optimal set W of vertex-disjoint cliques of RS, it leads to an optimal refine-ment H. Unfortunately, it can be shown that, given an arbitrary graph with two edge colours AD and S, finding if there exists a set W yielding a given number of connected components is an NP-hard problem (proof not shown).
However, R is constrained by the structure of a species tree, which restricts the space of possible join graphs.
An arbitrary complete graph R with edges labelled on the alphabet {S, AD, NAD} is said to be valid if there exists a species tree and a polytomy whose join graph is R. We characterize below the valid graphs in terms of forbidden induced subgraphs.
The proof is partially based on well-known results on P4-free graphs (Corneil et al.
1985).
THEOREM 3.
A graph R is valid if and only if RS is fP4; 2K2g-free, meaning that no four vertices of RS induce a path of length 4, nor two vertex-disjoint edges.
Although we have not been able to find an exact polynomial-time algorithm for the MinNADref problem, this very con-strained structure of the R graph yields a bounded heuristic for this problem with good theoretical properties described in the next section.
REMARK 1.
The P4-free property, which was already introduced in relation with reconciliations in (Hellmuth et al., 2013), is of special interest, as many NP-hard problems on graphs have been shown to admit polynomial time solutions when restricted to this class of graphs.
Unfortunately we can prove that, given an arbitrary P4-free graph on which we add AD edges, finding an optimal W is still NP-hard (proof not shown).
However, the added 2K2-free restric-tion imposes a rigid structure on the graph at hand, and we con-jecture that there exists a polynomial time algorithm to find an optimal W. 4 A BOUNDED HEURISTIC We first describe a general approach based on the notion of useful speciations, followed by a refinement of this approach with guaranteed optimality criteria.
DEFINITION 3.
Let J=J1; .
.
.
; JjJj be a join sequence.
A join Ji= fG1;G2g of J is a useful speciation if jtG1;G2=S and G1, G2 are in two different AD-components of the R graph obtained after applying the J1; .
.
.
; Ji1 joins.
Hence, if R has c AD-components, finding a zero NAD solu-tion becomes the problem of finding a join sequence with c 1 useful speciations.
For example, the graph R in Figure 1 has five AD-components (three trivial and two non-trivial), and thus the four useful speciations represented by the red lines lead to a 0 NAD solution (the binary tree H).
In the general case, the prob-lem we face is to select as many useful speciations as possible, as the resulting AD-components will have to be connected by NAD joins.
If we define a speciation-free forest as a forest F such that no edge of its join graph R is a speciation edge, following Lemma 1, we would like to first compute a set of useful speciations that i521 Polytomy refinement , apparent duplication non-apparent duplication ; [9] that [19] 5 3 2 4  results in a speciation-free forest whose join-graph has the least number of AD-components.
DEFINITION 4.
A lowest useful speciation is a useful speciation edge fG1;G2g of RS such that sG1;2 is not the ancestor of any sGi;j, for fGi;Gjg being another useful speciation edge of RS.
Lowest useful speciations fit naturally in the context of bottom-up algorithms where speciations edges that correspond to lower vertices of S are selected before speciations edges cor-responding to ancestral species.
The theorem below shows that proceeding along these lines ensures that the resulting join se-quence contains at least half of the optimal number of useful speciation.
THEOREM 4.
Let s be the maximum number of useful speciations leading to a solution to the MinNADref problem.
Then any algo-rithm that creates a speciation-free forest through lowest useful speciations makes at least ds=2e useful speciations.
This theorem implicitly defines a heuristic with approximation ratio 2 on the number of useful speciations that visits S in a bottom-up way, making useful speciations (which would thus be lowest useful speciations) whenever such an edge is available.
We now describe an improved version of this general heuristic principle.
A detailed example is given in Figure 2.
The main idea is to consider a bottom-up traversal of the species tree S, and for each visited vertex s, to find a useful set of speciation edges by finding a matching in a bipartite graph.
More precisely, for a node s 2 VS, we consider the complete bipartite graph B=X [ Y; fxyjx 2 X; y 2 Yg such that the left (respectively right) subset X (respectively Y) contains all the trees Gi of F where sGi is on the left (respectively right) subtree of s. Consider the two partitions ADX and ADY of X and Y, respect-ively, into AD-components.
The key step of our heuristic is to find a matching M EB of useful speciations between ADX and ADY, called a useful matching.
For example, in Figure 2, the bipartite graph and matching illustrated for Step 3 corres-pond to node l and that of Step 4 to node m of S. Notice that not all edges of B correspond to useful speciations.
Indeed it is possible that for some x 2 X and some y 2 Y, although {x, y} is a speciation edge, x and y are in the same AD-component of R due to another tree z not in B such that {x, z} and {z, y} are AD-edges.
For example in Figure 1, although fa; gg is a join of type S, the trees (a) and (g) are in the same AD-component ofR due to the tree a; f; g; h. For a vertex x of X (respectively y of Y), denote by AD(x) (respectively AD(y)) the component of ADX (respectively ADY) containing x (respectively y).
We indicate the fact that AD(x) and AD(y) belong to the same AD-component in R by adding two dummy genes b1 in AD(x) and b2 in AD(y), and a bridge fb1; b2g in EB.
Such bridges will be included in every matching, prevent-ing to include non-useful speciation edges.
An instance P of the problem associated with a vertex s of S is denoted by P=X;Y;ADX;ADY;B where X, Y, ADX, ADY are defined as above and B is the set of bridges induced by R. The graph corresponding to P, i.e.
the complete bipartite graph on sides X and Y to which we added the bridge edges B, is denoted by BP.
The whole method is summarized in Algorithm 1 MinNADref(F ;S) and illustrated on a simple example in Figure 2.
Algorithm 1: MinNADrefF ;S. for each node s of S in a bottom-up traversal of S do Let P=X;Y;ADX;ADY;B be the problem instance corresponding to s; Find a useful matching M of BP of maximum size (Algorithm MaxMatching below); Apply each speciation of M, and update F end for For each connected component C of RAD, join the trees of C under AD Nodes; If there is more than one tree remaining, join them under NAD nodes.
Finding a useful matching of maximum size can be done in polynomial time by Algorithm 2.
For an instance P=X;Y;ADX;ADY;B, the algorithm progressively increments the set M of speciation edges, eventually leading to a useful matching of maximum size.
At a given step, let GP;M be the graph with vertices X [ Y and edges EP;M=EAD [M, where EAD is the set of AD edges of R connecting vertices of X [ Y.
Components ADXi 2 ADX and ADYj 2 ADY are linked if there is a path in GP;M linking a vertex of ADXi to a vertex of ADYj , and not linked otherwise.
Algorithm 2:MaxMatchingX;Y;XX;ADY;B. D=;; M=B; while D 6 X [ Y do Find C 2 ADX [ ADY of maximum cardinality with vertices not included in D, if any; assume w.l.o.g.
C=ADXi 2 ADX; Fig.2.
A species tree S and a forest F of binary trees forming the polytomy.
The trees of F are placed on S according to their LCA.
The i, k, l and m nodes of S are annotated with the forest obtained after running Algorithm 2 on these nodes.
Their corresponding complete bi-partite matching instances are illustrated at the bottom.
AD joins are represented by dotted lines, useful matching are represented by plain lines (we omit drawing all the other edges of the complete bipartite graphs).
Note that there is a bridge induced by M between (F, K) and I at step 4.
In the fourth step, we obtain a single connected component, which allows, in a final step, to connect all the subtrees by AD nodes (final tree is on the top of the figure) i522 M.Lafond et al.
prior to , that .
.
.
due due .
( . )
.
.
, for each x 2 C that is not incident to an edge in M do if there is an y 2 Y such that AD(y) is not linked to C then Find such y with AD(y) of maximum cardinality; Addtheverticesxandy toDandaddthe speciationedge{x,y} toM; end if end for Add remaining vertices of C to D; end while THEOREM 5.
Given an instance P=X;Y;ADX;ADY;B, Algorithm 1 finds a useful matching M of maximum size.
Algorithm 1 is a heuristic, as it may fail to give the optimal solution (refinement with minimum number of NADs), as in Figure 1 for example.
In this example, a bottom-up approach would greedily speciate a and d, which cannot lead to the optimal solution.
However, we prove in Theorem 6 that if transitivity holds for the duplication join type, then Algorithm 1 is an exact algorithm for the MinNADref problem.
The example of Figure 1 does not satisfy this property, as fa; a; f; g; hg is a join of duplication type (AD), fa; f; g; h; gg is a join of duplication type but fa; gg is a join of speciation type.
THEOREM 6.
(1) Let s be the maximum number of useful speci-ations leading to a solution to the MinNADref problem.
Then, Algorithm 1 makes at least ds=2e useful speciations.
(2) If, for every node s of S the instance P corresponding to s has no bridges, then Algorithm 1 outputs a refinement of the input polytomy with the maximum number of useful speciations.
The following corollary provides an alternative formulation of the optimality result given by the above theorem.
COROLLARY.
Algorithm 1 exactly solves the MinNADref problem for an input F ;S such that each AD-component of the corres-ponding graph R is free from S edges (i.e.
there is no S edge between any two vertices of a given AD-component).
5 GENE TREE CORRECTION The polytomy refinement problem is motivated by the problem of correcting gene trees.
Duplication nodes can be untrusted for many reasons, one of them being the fact that they are NADs, pointing to disagreements with the species tree that are not due to the presence of duplicated genes.
Different observations tend to support the hypothesis that NAD nodes may point at errone-ous parts of a gene tree (Chauve and El-Mabrouk, 2009; Swenson et al., 2012).
For example, the Ensembl Compara gene trees (Vilella et al., 2009) have all their NAD nodes labelled as dubious.
In (Chauve and El-Mabrouk, 2009), using simulated datasets based on the species tree of 12 Drosophila species given in (Hahn et al., 2007) and a birth-and-death process, starting from a single ancestral gene, and with different gene gain/loss rates, it has been found that 95% of gene duplications lead to an AD vertex.
Although suspected to be erroneous, some NAD nodes may still be correct, due to a high number of losses.
However, in the context of reconciliation, the additional damage caused by an erroneous NAD node is the fact that it significantly increases the real rearrangement cost of the tree (Swenson et al., 2012).
Therefore, tools for modifying gene trees according to NADs are required.
We show now how Algorithm 1 can be used in this context.
In (Lafond et al., 2013), a method for correcting untrusted duplication nodes has been developed.
The correction of a du-plication node x relies on pushing x by multifurcation, which transforms x into a speciation node with two children being the roots of two polytomies.
Figure 3 recalls the pushing by multi-furcation procedure.
These polytomies are then refined by using an algorithm developed in (Lafond et al., 2012), which optimizes the mutation cost of reconciliation.
In the context of correcting NADs, we use the same general methodology, but now using AlgorithmMinNADref for refining polytomies.
Removing all NADs of a gene tree can then be done by iteratively applying the above methodology on the highest NAD node of the tree (the closest to the root).
6 RESULTS Simulated data.
Simulations are performed as follows.
For a given integer n, we generate a species tree S with a random number of leaves between 0:5n and 3n.
We then generate a forest F=G1; .
.
.
;Gn of cherries by randomly picking, for each cherry Gi 2 F , one node si 2 S and two leaves, one from each of the two subtrees rooted at si.
Any leaf of S is used at most once (possibly by adding leafs to S if required), leading to a set of cherries related through joins of type S or NAD.
Then, for each pair fGi;Gjg with join type NAD, we relate them through AD with probability 1/2 (or do nothing with probability 1/2), by adding a duplicated leaf.
For each pair S;F, we compared the number of NADs found by Algorithm MinNADref with the minimum number of NADs returned by an exact algorithm exploring all possible binary trees that can be constructed from F .
We generated a thousand random S and F for each n 4.
We stopped at n=14, as the brute-force algorithm is too time-costly beyond this point.
Over all the explored datasets simulated as described above, Algorithm MinNADref was able to output an optimal solution, i.e.
a refinement with the minimum number of NADs.
Therefore, the examples on which the heuristic fails seem to be rare, and the algorithm performs well on polytomies of reason-able size.
We then wanted to assess how the NAD minimization criter-ion differs from the rearrangement cost minimization criterion.
We generated 960 random instances with forests of sizes ranging between 5 and 100 (10 instances for each 5 n 100).
We a b c d S :G : a1 c3c2 b1 d2d1 x c1 b2 a1 b1 b2 c1 c3c2 d2d1 G* :s sl sr Fig.3.
A gene tree G and a species tree S, from which we obtain G by pushing x by multifurcation.
Here, x is a NAD, and is pushed by taking the forest of maximal subtrees of G that only have genes from species in the sl subtree (green), then another forest for the sr subtree (red) in the same manner.
Both these forests are joined under a polytomy, which are then joined under a common parent, so the root of G is a speciation i523 Polytomy refinement , [ 7, 30 ] [35] `` '' [7]-[18] due [30] [21] [23] , compared the output of Algorithm MinNADref with that of Algorithm MinDLref, given in (Lafond et al., 2012), which com-putes refinement minimizing the duplication+loss (DL) cost of reconciliation with the species tree.
Both algorithms gave the exact same refinement for only 12 instances (1.25%).
As ex-pected, Algorithm MinNADref always yielded a refined tree with a lower or equal number of NADs than the tree given by AlgorithmMinDLref, but always had a higher or equal DL-cost.
However, in many cases, minimizing the DL-score did not min-imize the number of NADs, as in 377 instances (39.3%), Algorithm MinNADref yielded strictly less NADs than Algorithm MinDLref.
Ensembl Gene Trees.
Next we tested the relevance of the proposed gene tree correc-tion methodology, by exploring how Ensembl gene trees are cor-rected from one release to another.
As the Ensembl general protocol for reconstructing gene trees does not change between releases, the observed modifications on gene trees are more likely due to modifications on gene sequences.
We used the Ensembl Genome Browser to collect all available gene trees containing genes from the monophyletic group of ray-finned fishes (Actinopterygii), and filtered each tree to preserve only genes from the taxa of interest (ray-finned fish genomes).
We selected from both Releases 74 (the present one) and 70 the 1096 gene trees that are present in both with exactly the same set of genes from the monophyletic group of fishes, and with less NAD nodes in Release 74.
We wanted to see to what extent our general principle of correcting an NAD by transforming it to a speciation node is observed by comparing Rel.70 to Rel.74.
Such a transformation requires to preserve the clade of the corrected NAD node x of the initial tree, meaning that l(x) should also be the leaf-set of a subtree in the corrected tree.
For490% of these trees (993 trees), the highest NAD node clade was preserved in Rel.74.
Moreover, among all such nodes that were corrected, i.e.
were not NAD nodes in Rel.74 (641 trees), almost all were trans-formed into speciation nodes (630 trees), which strongly supports our correction paradigm.
To evaluate our methodology for correcting NADs, we applied it to the highest NAD node of each of the 1096 afore-mentioned trees of Rel.70.
Figure 4 illustrates a comparison be-tween the corrected trees (Rel.70C, C standing for Corrected) obtained by our methodology and those of Rel.
74.
Pairwise comparisons are based on the normalized RobinsonFoulds (RF) distance (number of identical clades divided by the total number of clades).
The yellow curve shows a good correlation betweenRel.70C andRel.74, with65% exhibiting480% similar clades between Rel.70C andRel.74.
If we reduce the set of trees to those for which the highest NAD node is also transformed to a speciation node in Rel.74 (630 trees), the correlation is even better (blue curve of Fig.4), with 44% of trees being identical (277 over 630 trees) and 80% exhibiting 480% similar clades between Rel.70C and Rel.74.
Now, to specifically evaluate Algorithm MinNADref, we further restricted the set of trees to those giving rise to a non-trivial polytomy (i.e.
polytomy of degree42) after the pushing by multifurcation, which leads to a set of 117 trees.
Overall, the results for these trees (red curve in Fig.4) are close to those observed for all trees (yellow curve) detailed above.
We then wanted to evaluate our correction of the 117 afore-mentioned trees compared with trees in Rel.74.
Figure 5 provides an evaluation of the corrected trees (yellow curve) compared with those in Rel.
74 (blue curve) based on the normalized RF dis-tance with the initial trees in Rel.70.
Overall, the initial tree is closer to our correction than to the one of Rel.74.
Therefore, even though gene trees of Rel.74 are likely to have stronger stat-istical support with respect to the gene sequences provided in Rel.74, our correction removes NADs while respecting as much as possible the given tree topology.
Finally, we considered the reconciliation mutation cost as another evaluation criterion.
Among the 117 trees of Rel.70C, 30 are identical to the corres-ponding trees in Rel.
74, and 60% have a lower mutation cost, which tend to support our correction compared with the tree in Rel.74.
As for the 40% remaining trees, half of them have more NADs than the corresponding tree in Rel.74, which suggests that applying our correction to all NAD, instead of just the highest one, would help to obtain better results.
Fig.4.
Normalized RF-distance between corrected gene trees (by modi-fication of the highest NAD) from Rel.
70 and corresponding gene trees in Rel.
74.
Blue curve: transformation of the highest NAD into a speci-ation.
Red curve: trees with a non-trivial polytomy after pushing by multifurcation.
Yellow curve: all trees & ' Fig.5.
Normalized RF-distance between corrected trees (yellow curve) and Rel.
74 trees (blue curve) and original Rel.
70 trees i524 M.Lafond et al.
[ 23] more than , In order `` ''-about more than about more than in order , very to to st to  Finally, we evaluated the effect of NAD correction on the tree likelihood.
For this purpose, we selected the 1891 Ensembl Rel.74 gene trees of the considered monophyletic group containing at least one NAD, and we corrected each NAD individually.
The sequences were aligned using ClustalW (Larkin et al., 2007) and the likelihood values were computed with PhyML (Guidon et al., 2003).
For a tree T and a NAD node x, denote by Tx the tree obtained after correcting x.
For each T and each x, we computed the log-likelihood ratio Lx=logLHT=logLHTx.
Among the 4454 NAD nodes found in the considered set of trees, 95.4% of the L(x) ratios were between 0.98 and 1.02.
Although the cor-rection algorithm is not expected to outperform the Ensembl protocol in terms of likelihood as it ignores sequences, we found that the likelihood of the tree has been improved (L(x)41) after correction for 43.9% of the NAD nodes.
Moreover, 1180 (62.4%) trees contained at least one NAD node improving the likelihood.
7 CONCLUSION The present work is dedicated to the polytomy refinement prob-lem.
While the mutation cost of reconciliation has been used pre-viously as an optimization criterion for choosing an appropriate binary tree, here we use an alternative criterion, which is the mini-mization of NADs.
The tractability of the MinNADref Problem remains open, as is the problem to select, among all possible so-lutions, those leading to a minimum reconciliation cost.
Although developing a gene tree correction tool is not the purpose of this article, we show how our algorithm for polytomy refinement can be used in this context, by developing a simple algorithm allowing to correct a single NAD.
This algorithm has been applied to trees of a previous Ensembl release, and the corrected trees have been compared with the trees of the current Ensembl release.
A good correlation between the two sets of trees is observed, which tends to support our correction paradigm.
While minimizing NADs cannot be a sufficient criterion for gene tree correction, it should rather be seen as one among others, such as statistical (Wu et al., 2012), syntenic (Lafond et al., 2013) or based on reconciliation with the species tree (Chaudhary et al., 2011; Lafond et al., 2013; Swenson et al., 2010), that can be integrated in a methodological framework for gene tree correction.
Funding: N.E.-M. and M.L.
are supported by Fonds de recherche du QuebecNature et technologies (FRQNT).
C.C.
and N.E.-M. are supported by the Natural Sciences and Engineering Research Council of Canada (NSERC).
R.D.
is sup-ported by the MIUR PRIN 20102011 grant Automi e Linguaggi Formali: Aspetti Matematici e Applicativi, code H41J12000190001.
Conflict of interest: none declared.
ABSTRACT Motivation: Shotgun sequence read data derived from xenograft material contains a mixture of reads arising from the host and reads arising from the graft.
Classifying the read mixture to separate the two allows for more precise analysis to be performed.
Results: We present a technique, with an associated tool Xenome, which performs fast, accurate and specific classification of xenograft-derived sequence read data.
We have evaluated it on RNA-Seq data from human, mouse and human-in-mouse xenograft datasets.
Availability: Xenome is available for non-commercial use fromContact: tom.conway@nicta.com.au 1 INTRODUCTION Xenograft models are an important tool for many areas of biomedical research, including oncology, immunology and HIV pathology.
A typical scenario, drawn from oncology research, is that of a human prostate cancer grown in an immunocompromised mouse model.
Doing so allows researchers to investigate aspects of the cancer that are not necessarily preserved in cell lines, and it allows investigations into the interactions between the cancer and the surrounding stromal tissue.
The mouse may be biopsied or harvested and samples of cancer and/or stroma collected at various time points during an experiment.
Difficulties arise, when sequencing the genome or transcriptome of the samples because host (mouse) material (i.e.
DNA/RNA) will inevitably comingle with the graft (human) material.
If a sufficiently careful section is taken, it has been generally assumed that the level of host contamination is low enough that it may be ignored.
This may be a dangerous assumption, however, since the level of gene expression is non-uniform.
If the overall level of host contamination in a graft sample is measured to be 10% overall, it may still be the case for a given gene that the host homologue accounts for most or all of the expression.
Contamination may be minimized by physical or biochemical techniques such as conservative sectioning, cell sorting or laser capture micro-dissection, but these techniques can be a significant source of technical bias, or in some cases may require infeasibly large amounts of starting material.
Further, in the case of transcriptomic investigation, classifying host and graft in vitro may fail to adequately capture the interactions between them.
An alternative strategy is to sequence an acknowledged mixture of host and graft, then use in silico methods to classify the individual sequence reads.
This is the approach discussed here.
We demonstrate a simple technique, based on an analysis of sequence To whom correspondence should be addressed.
reads using Tophat, and a more precise technique based on a k-mer decomposition of the host and graft reference sequences, Xenome.
In both cases, the primary goal of the analysis is to classify reads into four classes: reads attributable to the host, reads attributable to the graft, reads which could be attributed to both and reads which are attributable to neither.
To the best of our knowledge, there are no results in the literature examining the classification of high-throughput sequencing short reads from xenograft models.
The studies we know of are concerned with microarray expression profiles or alternative methods for estimating the amount of host material or cell types in the samples.
For example, Lin et al.
(2010) investigate the use of species-specific variation in gene length and a multiplex PCR to ascertain the relative amount of mouse and human DNA.
Wang et al.
(2010) use microarray gene profiling data and in silico techniques to estimate the quantity of various tissue components.
In Samuels et al.
(2010), there is an analysis of a mouse xenograft model using microarray data.
They conclude that if there is more than 90% human DNA then the expression profiles are not unduly skewed.
They also describe an experimental method for removing homologous genes based on cross-hybridization analysis of the probes.
Ding et al.
(2010) use short read sequencing to study a cancer genome and identify mutations/deletions.
They estimate tumour cellularity using pathological assessment, and state that their xenograft is 90% tumour cells.
They also map NOD/SCID (mouse) genomic data to human and mouse genomes, reporting 3.17% and 95.85% mapping rates, respectively, and so apply no correction for the murine cells.
We note that in the context of non-uniform RNA-Seq data ignoring the contribution of the murine expression can lead to biases.
Tools such as Tophat serve a different purpose than that of Xenome.
The former aligns reads to a reference, and we can use those alignments for a variety of purposes, including the classification task we present here.
In contrast, Xenome only performs the classification task itself.
This is an important distinction, since an alignment must assign the read to zero or more positions in the genome; the classification merely has to decide if the read was more likely to arise from the genome than not.
For the remainder of the article, we will assume, unless otherwise stated, that sequence reads arise from RNA-Seq.
However, the techniques we present are applicable to genomic DNA sequences (including ChIP-Seq and MeDIP-Seq) and also to other mixtures of DNA species.
2 METHODS Under the assumption that a graft sample has only a low level of host material contamination, the simplest analysis is to use a regular mapping-based RNA-Seq analysis tool, such as Tophat and assume that either the observed expression is dominated by the graft, which has the greatest number The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i173 i172i178 Xenome Fig.1.
A Venn diagram showing the different classes that a given k-mer may belong to.
The marginal host (and marginal graft) partitions are for those host (and graft) k-mers that are Hamming distance 1 from a k-mer in the graft (and host) reference of input cells, or that the homology between the host species and graft species is such that reads arising from host material will tend to map poorly, and the resultant inferred level of gene expression will be negligible.
In some cases, these assumptions may be true, but in the case of human cancer xenografts in mice, for example, the second assumption is false for many transcripts, and a more precise technique is desirable.
Therefore, we have developed two techniquesone based on the existing RNA-Seq resequencing tool Tophat (Trapnell et al., 2009), and one based on k-mer decompositions of the host and graft references.
For genomic DNA, another resequencing/alignment tool could just as well be used.
2.1 A Tophat-based method A more precise analysis may be performed by using Tophat (Trapnell et al., 2009) to analyse the reads.
First, Tophat is used to process the read set with the graft genome as reference.
Secondly, Tophat is used to process the read set with the host genome as reference.
Lastly, the accepted alignments from the Tophat mappings are post-processed to partition the reads into four classes: host, graft, both or neither.
Tophat provides mapping quality scores in its output, but they only reflect whether or not the read mapped to multiple locations.
If the quality scores reflected a measure of certainty that the read maps to the given location, a more sophisticated approach would be to extend the classification to assign reads that map with high certainty to one genome and low certainty to the other to the appropriate specific class rather than both.
We do not pursue this further here.
An implementation of this method may be achieved easily with Tophat, SAMtools (Li et al., 2009) and some simple scripts.
As will be apparent in the results presented in Section 3, although very few reads are misclassified (i.e.
classified as host instead of graft, or vice versa), a significant proportion of the reads, even in a pure graft or pure host sample, fall into the both class.
If these ambiguous reads were uniformly distributed in their origin across the genome, this would have only a small impact, but as we will elucidate in Section 4, the ambiguous reads are non-uniformly distributed.
As a result, a significant number of genes cannot have their expression unambiguously pinned to the host or the graft, though at least compared with a single analysis, the set-based analysis makes clear which reads may be clearly associated with the host or the graft, and does not assume that all gene expression in the sample is explained by the graft.
2.2 A k-mer-based method Our method proceeds in two phases: constructing a reference data structure, then classifying reads with respect to that reference data structure.
The reference structure is built from the sets of k-mers in a pair of reference sequences, which we will refer to as the host and the graft.
2.2.1 Definitions Since in most sequencing protocols the reads are a mixture of forward and reverse complements with respect to the reference sequence, we cannot assume the orientation of k-mers drawn from reads will match the orientation of k-mers drawn from the reference.
We could always consider both orientations, but that would entail a lot of double handling of information, so instead we normalize or canonicalize k-mers.
Definition 1.
(k-mer canonicalization).
Consider a k-mer x.
We denote its reverse complement by x.
A canonical k-mer x is defined by a choice function C giving a deterministic choice between x and x: x : x=C(x)=C(x) In principle, we can choose any such function: min or max being obvious candidates, and the results of our method are identical for all choices.
In Section 4, we will present our specific choice which has important performance ramifications.
This definition can be extended to a set of k-mers S in the obvious way: S ={x :xS} Definition 2.
(Marginal inclusion).
Consider a set of canonical k-mers S. We say that a k-mer x is has marginal membership of S if x does not exist in S, but has a Hamming distance 1 neighbour y such that y is a member of S. To aid our computation of marginal inclusion, we define the function M: M(x,S)={y :yHam1(x)} S where Ham1(x) is the set of Hamming distance 1 neighbours of x.
Note that{ y :yHam1(x) }={y :yHam1(x)} 2.2.2 Reference construction For both host and graft reference sequences, we construct the set of canonical k-mers (H and G respectively).
From these we compute a complete set of canonical reference k-mers S = H G. Note that x S,x= x.
The sets of canonical k-mers tend to be large: with k =25, there are 2.4 billion in the human genome, 2.1 billion in the mouse genome, and 4.5 billion in the uniononly 12 million are shared.
A naive representation (2 bits per base, packed), would use 50 bits per 25-mer, or about 26 GB.
As discussed in our previous work (Conway and Bromage, 2011), information theory gives a lower bound for the memory usage.
For a domain of 4k possible k-mers, the minimum number of bits required to represent a set of n k-mers is log2 ( 4k n ) In the case of the union S above, this is about 10 GB, or less than half what is required for the naive representation.
As also discussed in our previous work, succinct data structures have been developed to give concrete representations that approach this theoretical lower bound.
The one we use, due to (Okanohara and Sadakane, 2006), works very well, requiring about 13 GB.
For each reference k-mer, we determine whether it occurs in the host, the graft, both, or if it occurs in one and has a marginal occurrence in the other.
These classes are denoted h, g, b or m, respectively (Fig.1).
More formally, we compute the function K for each x S: K(x)=  b if x G x H g if x G x/ H (H M(x,S)=) h if x/ G x H (GM(x,S)=) m if ( x G x/ H (H M(x,S) 	=) ) ( x/ G x H (GM(x,S) 	=) ) K may be extended to project not just k-mers to classes, but also a set of k-mers Q to a set of classes in the obvious way: K(Q)={K(x) : x Q} These classes are pre-computed and stored as a sequence of 2-bit values corresponding to the k-mers in the succinctly store reference set.
i173 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i174 i172i178 T.Conway et al.
The class m denotes that the k-mer exists in H and has marginal membership of G or vice versa.
That is, they are marginally distinctive, in the sense that a single polymorphism or sequencing error may cause a k-mer in that class to change from being marginally host to being marginally graft.
Since the marginal set is symmetric (every marginal host k-mer has a corresponding marginal graft k-mer and vice versa), and represents k-mers that are not very discriminating, we combine both marginal sets into a single marginal set with the class m. 2.2.3 Classification Classification proceeds by taking each read r and constructing the set of canonical k-mers that may be derived from the read Qr .
We then map the set of k-mers to a set of classes by the function K described above.
k-mers that do not occur in the reference, S, are ignored.
The k-mer classes that occur for a given read determine the classification of the read as a whole according to the following function: C(Qr)=  graft if gK(Qr)h/K(Qr) host if g/K(Qr)hK(Qr) ambiguous if gK(Qr)hK(Qr) both if K(Qr){g,h}=K(Qr) 	= neither if K(Qr)= The read classes graft and host denote cases where there is at least one k-mer which unambiguously comes from that k-mer class, and there are no contradictory k-mers (i.e.
unambiguously from the opposing reference).
The ambiguous class corresponds to the case where there are k-mers which appear to be contradictoryunambiguously host, and unambiguously graft.
The both class represents cases where there are only k-mers which are either unambiguously common to both the host and graft or k-mers which may belong to either if they contain a single polymorphism or sequencing error.
The last class, neither, represents those cases where there were no matching k-mers.
2.2.4 Implementation concerns In Section 2.2, we introduced an abstract canonicalization function C. The most commonly used concrete function is min (or max), which selects the lexicographically/numerically smaller of the k-mers x and x: Cmin(x)=min(x,x) We use the following definition, assuming some reasonable hash function f : Chash(x)= x if f(x)< f(x) min(x,x) if f(x)= f(x) x if f(x)> f(x) Assuming f is a reasonable hash function, this definition effectively makes a random, but deterministic choice between x and x.
We use this definition, rather than the more common lexicographic one, because lexicographic canonicalization leads to the set of canonical k-mers being non-uniformly distributed across the set of all possible k-mers.
There are two ways in which a more uniform distribution of k-mers improves performance of Xenome.
The first is that the succinct bitmap representation that we use (due to Okanohara and Sadakane, 2006) performs better on a uniform distribution of bits (that is, a uniform distribution of k-mers).
The second is that it improves the performance of our intermediate hash table, from which the succinct bitmap data structure is built.
The way the hash table is used is that as the input sequences are read, they are decomposed into k-mers which are stored in the hash table, which is of a fixed (controlled by a command line parameter) size.
When the hash table fills (that is, unresolvable collisions arise), it is sorted and written out to disk.
When all source k-mers have been read, the sorted runs are merged and the main succinct bitmap is constructed.
The specific representation used is a succinct cuckoo hash table, broadly similar to (Arbitman et al., 2010).
The succinct representation relies on the fact that the location of the slot where a key x (in this case, a k-mer) is stored contains some of the information present in the key.
Consider an idealized hash table with load factor 1 (i.e.
the number of values stored in the table equals the number of slots in the hash table), with no collisions.
If the width of the hash table is 2J , then J bits of the key are implied by the slot chosen.
For keys of N bits, therefore, the entries of the hash table need store only N J bits.
The simplest way this may be realized would be to just use J bits of the key as the slot number, and store the remainder in that slot, but of course, this is likely to have an unacceptable number of collisions.
Instead, we use an invertible hash function [based on a single-stage Feistel network, see Luby and Rackoff (1988)] to turn a key x into a slot number s and a stored component v in such a way that given s and v we can recover x (assuming a hash function f ): Ff (x)= ( x mod 2J )f ( x 2J ), x 2J F1f (s,v)=sf (v)+v2J The size of the key data stored in the hash table is, then, 2J (N J ) bits.
If 2J 2N , this is within (1+o(1))log2(2N2J ) bits, and hence succinct.
[Of course, idealized hash tables are not possible if the set of keys is not known in advance.
Hence we use cuckoo hashing (Pagh and Rodler, 2004), in which collisions are resolved (as far as possible) using multiple hash functions.]
To make the sorting of keys more efficient, our hash function preserves the N J most significant bits of the keys, which are then stored in the hash table (along with a few bits to determine which hash function was used).
This means that we can perform an initial bucketing without inverting the hash functions; the buckets can then be processed independently, using multiple threads if required.
Now consider a set of random k-mers.
For hash-based canonicalization, given a good hash function, there is an equal probability of observing any base in the most significant position in x.
For lexicographic canonicalization, there is a probability of 410 that it is a, 3 10 that it is c, 2 10 that it is g, and only 1 10 that it is t. For a set of N random k-mers, the expected entropy at the most significant base is 2.0 bits and 1.85 bits, respectively.
This matters, because hash functions are frequently vulnerable to poor behaviour in the presence of highly correlated sets of keys.
In real, biologically derived sets of k-mers, the set of k-mers is non-random, so there is likely to be less entropy to begin with, and therefore loss of entropy due to canonicalization is exacerbated.
This is especially problematic in the case of the above family of invertible hash functions, since it is precisely the most-significant bits of the key which are passed to the underlying hash function f .
The upshot of this is that Cmin leads to highly correlated k-mers, which in turn lead to a high probability of unresolvable collisions even when the hash table is nearly empty, resulting in a large number of short runs.
Since the set of canonical k-mers that result from Chash are less correlated, the hash table is unlikely to encounter unresolvable collisions until it is almost full (8085% in practice).
2.3 Xenomeusage and workflow To use the xenome tool, first it must be invoked to construct the reference data structures.
A typical invocation will give the FASTA filenames for the host and graft genomes, a filename prefix for the index files, and optionally, the amount of working RAM to use (in Giga Bytes), and the number of threads to use: $ xenome index-M 24-T 8-P idx \-H mouse.fa-G human.fa This will run for some timeon the human/mouse references, around 46 h on an 8-core server.
This need only be done once for a given pair of references, and a given k-mer size.
The k-mer size defaults to 25, which seems to work well.
With the reference data structures built, read data may be segregated.
To make the output files easier to identify, command line flags can be used to i174 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i175 i172i178 Xenome name the host and graft files.
If paired data is being classified, the pairs flag should be givenpairs are classified by computing K(Q) over all the k-mers in the pair.
$ xenome classify-T 8-P idx--pairs \--graft-name human--host-name mouse \--output-filename-prefix XYZ-i XYZ_1.fastq-i XYZ_2.fastq This yields the following set of files: XYZ_ambiguous_1.fastq XYZ_ambiguous_2.fastq XYZ_both_1.fastq XYZ_both_2.fastq XYZ_human_1.fastq XYZ_human_2.fastq XYZ_mouse_1.fastq XYZ_mouse_2.fastq XYZ_neither_1.fastq XYZ_neither_2.fastq If the total size of the index files is larger than RAM,xenomewill perform poorly, but the flag-M can be supplied with the maximum desired working set size, and the classification will be done in multiple passes each using less memory.
On a single server with 8 AMD Opteron cores running at 2 GHz and with 32 GB of RAM Xenome processes 15 000 read pairs per second.
Having classified the reads, typical usage would be to then run Tophat and Cufflinks to perform intron-aware gapped alignments and compute gene expression.
Instead of running Tophat on all of the reads, having separated the reads according to their origin, we can run it with the human genome just against the human fraction of the reads, and against the mouse genome on the mouse fraction of the reads.
It may well be desirable to combine the both and ambiguous fractions with the human fraction to run against the human genome, and also with the mouse fraction to run against the mouse genome.
If this is done, attention should be paid to homologous genes, since it is possible that the human and mouse homologues may be represented by the same reads.
In some cases, the correct attribution of the gene expression may be apparent from the nature of the experiment.
For example, a sample from a human prostate cancer grown in mouse may show ambiguous expression in MYH genes which would be reasonably attributed to the stromal mouse tissue.
3 RESULTS Our analysis examines two questions: whether or not it is technically feasible to separate host and graft reads in silico; and whether or not the fast technique we have proposed (Xenome) yields a worthwhile improvement over the mapping (Tophat) based technique.
In the first experiment, we take a sample of human cDNA sequence data (SRR342886), and a sample of mouse cDNA sequence data (SRR037689) and analyse them both with Tophat and Xenome, and compare the results.
This allows us to evaluate the degree to which sequences are misclassified (assigned to human rather than mouse or vice versa), and the specificity of the classificationthe proportion of sequences which are not classified as both.
The use of pure human or mouse cDNA gives an experiment where the correct assignment of reads is known.
The second experiment runs the same analysis on sequence data from a human prostate cancer xenograft growing in a mouse host [BM18, see McCulloch et al.
(2005)].
In this case, however, we not only classify the reads, but use the Tophat mappings to compute approximate levels of gene expression [measured in fragments per thousand bases of transcript per million mapped reads, or FPKM Trapnell et al.
(2010)] and use human species-specific quantitative RT-PCR (qRT-PCR) on selected genes to validate the results.
In this instance, we have no gold standard by which we can judge the Fig.2.
Summary of the results with Human cDNA.
Each of the classes of reads is divided into those reads assigned to the class only by Xenome (Xenome), only by the Tophat analysis (Tophat) or by both Xenome and the Tophat analysis (Concordant) results, but the qRT-PCR will give some degree of validation, and known aspects of the biology of the cancer can give some qualitative corroboration.
Tophat uses a global analysis, combining the results of all the read mappings to locate exons, junctions and so on.
In contrast, Xenome performs pre-computation on the two reference genomes, then classifies each read independently.
Therefore, for each of the three sets of reads, we ran Tophat with the human reference genome and again with the mouse reference genome, then, as described in Section 2.1, the mappings were post-processed to determine which reads belonged to each of the four classes.
Each of the four partitions of the sets of reads was then partitioned with Xenome to allow us to easily determine which reads were classified as the same by both procedures, and which were classified differently.
Figures 2, 3 and 4 summarize the results.
For the three samples, the proportion of reads receiving the same classification were 82, 87 and 84%, respectively.
As can be seen from the human and mouse only figures, both techniques are accurate, in as much as they misclassify only a small proportion of the reads (the worst case being the Tophat-based analysis of the human cDNA which misclassified 1.2% of the readsall the other analyses misclassified 0.20.3%).
The main difference between the Tophat-based and Xenome analyses is that the latter yields better specificitythe fraction of reads classed as both is significantly smaller in the Xenome analysis.
To check for false positives for the human cDNA dataset, we also used BLAT (Kent, 2002) to map the 1.1 million reads which Xenome classed as human but which were not mapped to either genome by Tophat.
Most of them were successfully mapped with high quality to the human reference by BLAT (about 90%).
BLAT also mapped about 18% of them, with very variable quality, to the mouse reference.
This supports our confidence in the accuracy of the Xenome algorithm.
From this we can conclude that the in silico classification of sequences is feasible and accurate.
i175 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i176 i172i178 T.Conway et al.
Fig.3.
Summary of the results with Murine cDNA Fig.4.
Summary of the results with BM18 xenograft cDNA The second experiment, using RNA-Seq data from a prostate cancer xenograft into a mouse demonstrates that the classification works on a real mixture.
As described above, we performed the same process to partition the reads into classes, then we used the refGene genome coordinates as calculated by Tophat to assign reads to genes, from which we computed an expression level using the fragments per kilobase of transcript per million mapped reads formula (Trapnell et al., 2010): FPKM= f 10 9 zN where f is the number of fragments (reads, for single ended data or pairs for paired data), z is the combined length of the exons Fig.5.
Validation of the in silico classification of xenograft RNA-Seq data with qRT-PCR.
The horizontal axis shows log10 FPKM for the Xenome-derived gene expression for the 18 test genes.
The vertical axis shows the Ct value for each gene relative to the Ct of actin.
There were two RNA-Seq samples processed (biological replicates), and four replicates of the qRT-PCR.
For each gene, an ellipse is shown centered on the mean log10 FPKM in the x-axis, and on the mean relative Ct in the y-axis.
The horizontal and vertical radii show the variance in the samples of the gene and N is the total number of mapped fragments.
Although this quantification is peripheral to the technique we are presenting, we have computed expression levels for the purposes of comparing with some qRT-PCR data for the same biological data.
The qRT-PCR data were available for 18 genes:ABCG2,ALDH1A1, CD177, DLL1, DLL3, GLI1, GLI2, HES1, JAG1, JAG2, LGR5, NANOG, NOTCH1, NOTCH2, NOTCH3, PTCH1, PTCH2 and SMO.
Figure 5 shows the log10FPKM versus the difference of the Ct for each target gene and the Ct for actin (which was used as a housekeeping gene).
With the exception of NANOG, the two methods correlate reasonably well (the Pearsons correlation coefficient is 0.80).
We have investigated the NANOG data, and cannot explain the low FPKM .
Whether this is a sequencing issue or a biological variation in the mice is unknown, but the low level of expression does not appear to be related to the behaviour of Xenome or Tophat.
4 DISCUSSION We have presented a simple read classification method based on Tophat, and our refined classification approach, Xenome.
Xenome can be used to efficiently and effectively partition the read set for subsequent processing by tools such as Tophat.
What is not apparent from the results above is the relative behaviour at the level of a single gene.
It should be expected that the distribution of ambiguously mapped reads (classed as both) should be non-uniform, since some genes in the two genomes are more highly conserved than others.
i176 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i177 i172i178 Xenome Fig.6.
An in silico analysis showing the degree of ambiguity in HG19 refGene, according to the k-mer based analysis used by Xenome.
In this analysis, k =25 The first result we present in Figure 6 on this point is an in silico analysis showing the proportion of each human gene (ignoring introns) covered by k-mers that are not classed as human.
It is clear that the vast majority of genes contain few or no k-mers that are not classed as human.
The fraction of k-mers which are ambiguous gives a worst-case view of how Xenome might be expected to perform.
In order for a read to be classified as both, all of its k-mers must be of the both class, or there must be at least one k-mer from each of the two genomes (which happens less than 2% of the time in the samples we have tried).
Conversely, a single host or graft k-mer is sufficient to classify the read into the respective class.
Therefore for a read to be classified as both, the reference must contain a sufficiently long run of consecutive k-mers of class both and/or SNPs and sequencing errors must eliminate all the distinctively host or graft k-mers.
The second result we report on this point, presented in Figure 7 is the relative proportion of reads which are classified as both on a per-gene basis.
What is evident in this figure is that although there are many genes for which the proportion of both reads is tightly correlated between Tophat and Xenome, there are a large number of genes for which the Tophat-based analysis has significantly more both reads.
There are 15 591 genes for which there were at least 20 mapped reads in the BM18 xenograft sample.
Of these, there were 65 for which Xenome assigned both or ambiguous to at least half the reads mapping to the gene; there were 498 for which the Tophat-based analysis assigned both to at least half the reads mapped to the gene.
For the most highly conserved genes, there is not much that can be done with this data directlyfurther signal processing or other data would be required to determine the relative expression in the host and graft.
While we have developed Xenome with RNA-Seq on human/ mouse xenografts in mind, we anticipate it will be an effective tool for other similar mixtures.
For example, capturing the differential methylation around genes between host and graft using MeDIP-Seq may shed light on the interraction between the two.
Fig.7.
A plot showing the distribution of human genes with respect to the proportion of xenograft reads which are classed as both by the Tophat-based analysis and the Xenome analysis.
The reads considered are only those mapped by Tophat since Xenome does not yield mappings, so cannot be used to assign reads to genes.
Only genes for which at least 20 reads mapped were considered.
The horizontal axis corresponds to the number of reads classified as both or ambiguous by Xenome as a proportion of all the reads that might possibly be human (i.e.
both, ambiguous or human).
The vertical axis corresponds to the number of reads classified as both by the Tophat-based analysis, once again, as a proportion of all the reads that might possibly be human The need for our technique is substantially motivated by the fact that for a xenograft to be viable there must be very strong homology between the host and graft organisms.
This leads to a situation where there is a high probability that a read may map to either genome, and it is this problem that Xenome specifically addresses.
A side benefit is that the classification is done independently for each read, and results in groups of reads in each class; each group may then be processed independently with further tools [such as Tophat, Cufflinks (Roberts et al., 2011) or others].
Given that many kinds of analysis require global processing of the input data, being able to process a coherent subset of the data can lead to a time/space gain.
This benefit extends beyond the sphere of xenografts.
For example, there are situations where a parasite or pathogen cannot be cultured independently (for example Chlamydia, and some fungi), so samples will generally contain a mixture of host and pathogen.
In some examples, although the pathogen can be cultured independently, there are phenotypic differences between organisms growing in culture and those growing in a host.
In both cases, there are benefits to being able to classify the two groups of reads, even though straight mapping based approaches will be less sensitive to cross-talk than xenograft data.
Precise alignment and alignment-free methods represent different points along a spectrum of possible classification techniques.
Fundamentally, both rely on establishing homology between a read and the host and/or graft references.
By substituting different algorithms for establishing homology (e.g.
various alignment algorithms, k-mer spectrum methods, etc.
), different sensitivity and specificity might be achieved.
Although our current technique is built on simple set-based classification, there is clearly scope to develop statistical models i177 Copyedited by: ES MANUSCRIPT CATEGORY: [11:35 29/5/2012 Bioinformatics-bts236.tex] Page: i178 i172i178 T.Conway et al.
which allow for a more subtle classification procedure.
These could, for example, be based on the frequency of k-mers of different classes, giving rise to the computation of the likelihood that a read originated from either the host or graft genomes.
Indeed, if such likelihoods were used from read alignments (Tophat unfortunately does not produce such scores), a unified model allowing either our current k-mer based model or alignments could be used.
A further extension could take into consideration the established homology between genes in the host and graft organisms.
Where reads are ambiguous, non-ambiguous reads associated with the same gene homologues could be used to help disambiguate the classification.
EM-based methods such as those described in Newkirk et al.
(2011); Chung et al.
(2011); Hormozdiari et al.
(2010) would be a good basis for such an extension.
We note however, that this would require significant conceptual changes to Xenome since it requires relatively more precise alignments.
It is instructive to consider a specific example of a gene where the Tophat based and Xenome analyses are very different.
For the gene MYH3, in the human cDNA dataset, there are 29 reads classed as human by the Tophat-based analysis and 2 713 classed as human by Xenome.
All the reads that are thus assigned by Xenome but not by the Tophat-based analysis were classed as both by the latter.
This is because there is a high level of conservation in this gene between the two species, and the reads therefore aligned to both.
Were Tophat to yield meaningful alignment quality scores, a statistical approach of the kind hinted at above may perform similarly to the k-mer based approach.
ACKNOWLEDGEMENT National ICT Australia (NICTA) is funded by the Australian Governments Department of Communications; Information Technology and the Arts; Australian Research Council through Backing Australias Ability; ICT Centre of Excellence programs.
Funding: Prostate Cancer Foundation of Australia (EDW) and the Victorian Governments Operational Infrastructure Support Program.
EDW is supported by an Australian NHMRC Career Development Award [#519539] in part.
Conflict of Interest: none declared.
ABSTRACT Motivation: The computational identification of transcription factor binding sites is a major challenge in bioinformatics and an important complement to experimental approaches.
Results: We describe a novel, exact discriminative seeding DNA motif discovery algorithm designed for fast and reliable prediction of cis-regulatory elements in eukaryotic promoters.
The algorithm is tested on biological benchmark data and shown to perform equally or better than other motif discovery tools.
The algorithm is applied to the analysis of plant tissue-specific promoter sequences and successfully identifies key regulatory elements.
Availability: The Seeder Perl distribution includes four modules.
It is available for download on the Comprehensive Perl Archive Network (CPAN) at http://www.cpan.org.
Contact: martina.stromvik@mcgill.ca Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION The binding of transcription factors to relatively short and variably degenerate regulatory DNA sequences (cis-regulatory elements) is central to the regulation of gene expression (Orphanides and Reinberg, 2002).
While several sequenced genomes are nearly deciphered in terms of the protein-coding gene repertoire, the inventory and comprehensive characterization of cis-regulatory elements remains elusive.
Motif discovery has motivated the development of numerous tools and algorithms, and the use of various motif models and statistical approaches (Guha Thakurta, 2006).
Motif discovery can be broadly divided into sequence-driven and pattern-driven methods.
The former methods typically involve building a position-weight matrix (PWM) from sequence data, and local search techniques such as expectationmaximization or Gibbs sampling are used to optimize the log likelihood ratio until convergence or a maximum number of iterations is reached.
Though routinely fast, those methods are not guaranteed to yield the best solution, or global optimum (Stormo, 2000).
Enumerative methods, on the other hand, are guaranteed to find a global optimum but have the drawback of being computationally expensive and limited to short motifs.
Searching a set of sequences for patterns that are overrepresented relative to a given background model may converge towards To whom correspondence should be addressed.
motifs that are prevalent in the genome thus not likely to represent regulatory elements.
Sinha (2003) introduced the notion of discriminative motif discovery in which a motif is treated as a feature that leads to good classification between positive sequences deemed to contain common cis-regulatory elements and a set of background sequences.
In this work, we present the Seeder algorithma novel, exact discriminative seeding DNA motif discovery algorithm inspired by Keich and Pevzner, 2002; Pizzi et al., 2005.
The major benefits of the Seeder algorithm are (i) the use of intuitive and reliable statistics for the choice of motif seeds and (ii) a data structure that significantly accelerate the computation of motifs and background models.
The algorithm is benchmarked against popular motif finding tools and demonstrates greater performance.
The algorithm is applied to the analysis of Arabidopsis thaliana seed-specific (the plant structure seed, not to be confused with motif seed) promoters and identifies motifs with high similarity to seed-specific cis-regulatory elements experimentally characterized in Brassica napus, a closely related species.
2 METHODS 2.1 The Seeder algorithm Our algorithm starts by enumerating all nucleotide combinations (words) of a given length, usually six.
For each word, it calculates the Hamming distance (HD) between the word and its best matching subsequence (we call this distance the substring minimal distanceSMD) in each sequence of a background set.
This data is used to produce a word-specific background probability distribution for the SMD.
For each word, it then calculates the sum of SMDs to sequences in a positive set.
The P-value for this sum is calculated using the word-specific background probability distribution.
The word for which the P-value is minimal is retained, and a seed PWM is built from the closest matches to this word found in every positive sequence.
The seed PWM is extended to full motif width and sites maximizing the score to the extended PWM are selected, one in each positive sequence.
A new PWM is built from those sites and the process is iterated until convergence, or a maximum number of iterations is reached.
2.1.1 Input data and parameters Our algorithm takes as input a set B = {B1, , Bm} of m background sequences of length L, a set P = {P1, , Pn} of n positive sequences of length L, the length k of the motif seed and the length l of the full motif to discover.
2.1.2 Substring minimal distance The HD between two strings of equal lengths is the number of positions at which symbols differ (Hamming, 1950).
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
F.Fauteux et al.
We define the SMD d(w,w) between a short nucleotide sequence w and a longer sequence w as the minimal HD between w and a |w|-length substring of w. 2.1.3 Background model A discrete random variable Y (w) is associated with each word w of seed length k, corresponding to the SMD between w and a randomly selected background sequence from B.
This w-specific distribution function is obtained empirically from B; for each word w, we set gw(y) = Pr[Y (w) = y] = |{Bi: d(w,Bi) = y}|/m, for y = 0, , k. 2.1.4 Seed position weigth matrix For each word w, the sum of SMDs to the positive sequences S(w)=j d(w,Pj) is computed.
Under the background model, the distribution function of this sum of n independent and identically distributed (i.i.d.)
random variables is gn w (y), the n-fold self-convolution of gw(y) (Grinstead and Snell, 1997).
The P-value (p) for word w with sum S(w), which is the probability of obtaining a sum lower or equal to S(w) under the assumption that Pjs are random in respect to w, is p(S(w))= S(w) y=0 gn w (y) (1) The word w for which the P-value p (S(w)) is minimal is retained.
For each positive sequence in P, the set of one or more subsequences of length k having the SMD to w are retained.
A PWM P0 is built from this set of selected subsequences using standard procedures and pseudocounts proportional to n (Wasserman and Sandelin, 2004), with the modification that when a sequence contains more than one match, each match (subsequence) weight is reduced proportionally.
The subsequence associated with the highest score to P0 is retained in each sequence, and the seed PWM Ps is built from this optimal set of n subsequences, as described above.
2.1.5 Full length motifs The seed PWM Ps is of width k, smaller than the full motif width.
It is extended to full motif width l by adding null weights at (lk)/2 positions upstream and downstream.
The full length PWM is then refined by iterating the following process.
(i) Sites (one per sequence in P) maximizing the score to the extended weight matrix are selected and (ii) a revised full length PWM is built from those sites.
This process is repeated until convergence (i.e.
the sites maximizing the PWM score are fixed in all sequences) or for at most a default number of 10 iterations, which we observed to often be sufficient for the convergence of significant seeded motifs.
2.1.6 N-fold self-convolution Our implementation of the n-fold self-convolution uses the binary expansion of n (Sundt and Dickson, 2000), and is an adaptation of the square and multiply algorithm (Gordon, 1998) while convolutions per se are computed using the input side algorithm (Smith, 1997).
2.1.7 Multiple hypothesis testing correction For each motif predicted, a list of 4kP-values is generated thus prompting for a multiple testing correction.
This is carried out by generating a list of q-values from the list of P-values associated with words of seed length k, using the general algorithm for estimating q-values described in (Storey and Tibshirani, 2003).
The statistical significance of a motif is evaluated with the q-value of the sum S(w), which is the expected proportion of false positives incurred when calling the sum significant (i.e.
not likely to have occurred if the positive sequences were randomly selected).
2.1.8 Searching both strands Because transcription factor binding sites (TFBS) can be located either on the forward or the reverse strand, motifs are typically searched for on both strands.
This is easily achieved with Seeder: one simply redefines the SMD so as to consider matches one both strands (for both the background and positive sequences) and perform PWM matching similarly.
Fig.1.
SMD index generation.
The SMD index generation is illustrated for the word CAG.
N, top-level tree node nucleotide numerical value; d, level.
2.1.9 Multiple motifs When the user asks to retrieve more than one motif, the sites identified in the preceding run(s) are masked and the motif-finding process is repeated.
The positions of the sites are obtained by scanning each sequence (plus strand first) until the highest scoring subsequence is found.
2.2 Data structures The calculation of SMDs using direct string comparison approaches requires a considerable amount of operations and this probably explains in part why this quantity has not been more often exploited for DNA motif discovery.
We have designed a data structure based on the organization of the matrix of HDs between words of length 6 (see Fig.4, supplied as supporting information).
This structure, called the SMD index (Fig.1), allows very efficient lookup, in a given sequence, for a subsequence minimally distant to a given word, hence improving the efficiency of the SMD computation.
2.2.1 SMD index generation Each nucleotide is mapped to a numerical value (A,C,G,T0,1,2,3).
For a given word w = w1,w2, ,wk of length k, a list of indices is generated equivalent to a tree structure with levels d = 0, , k 1.
At each new level of the tree, each node is expanded into four nodes, one for each possible nucleotide N {0,1,2,3} at that position.
An index id = N + (4 id1) is assigned to each new node, where id1is the index of the parent node.
At the final level, the tree has nodes and indices corresponding to all possible nucleotide sequences of length k. For a given node at a given level d, the HD is one more than that of the parent, except for the node corresponding to nucleotide wd+1, where the HD is unchanged (Fig.1).
The SMD index is precomputed for every word w of seed length k and HDs between 0 and 3, which requires a marginal amount of memory and appreciably accelerates the process.
2.2.2 SMD calculation The number of occurrences of every word of length k in each sequence in P is stored using base 4 indexing (word count array).
The SMD between w and sequence Pj is obtained by looking up elements in word count array of Pj , in order of increasing HD to w, until a nonzero count is found.
2304 Discriminative seeding motif discovery 2.3 Benchmarking of motif discovery tools The performance of the Seeder algorithm was compared with that of popular motif discovery tools using benchmarks designed for robust assessment of motif discovery algorithms (Sandve et al., 2007).
In the benchmark suites, binding site sequences from the Transfac database (Wingender et al., 1996) are represented either in their original genomic context sequences (Model RealMR, Algorithm RealAR) or in sequences generated with a third-order Markov model (MM) (Algorithm MarkovAM).
The reverse complement of sequences is used in cases where the original binding site appears on the negative strand, so all sites within the benchmark suites appear in the forward sequence.
The MR suite contains motifs that, according to Sandve et al.
(2007), are harder to distinguish from the local background using common motif models (consensus, PWM and mismatch).
The AM and AR suites each contain 50 datasets and a total of 810 sequences of mean length 1300 nucleotides, and the MR suite contains 25 datasets and a total of 410 sequences of mean length 1250 nucleotides.
2.3.1 Parameter settings In order to be representative of common usage where parameter adjustment is nominal while providing homogeneous instructions to different software, sequences were scanned in the forward orientation, searching for one motif of width 12 with one occurrence (site) per sequence.
Other parameters were left to default values.
We ran Seeder v. 0.01 (this article), Weeder v. 1.3.1 (Pavesi et al., 2004), BioProspector v. 1 (Liu et al., 2001), MEME v. 3.5.4 (Bailey and Elkan, 1994), the Gibbs Motif Sampler v. 3.03.003 (Lawrence et al., 1993) and Motif Sampler v. 3.2 (Thijs et al., 2001) on each dataset.
The DIPS algorithm (Sinha, 2006) was not included in the benchmark study because it was associated with prohibitive runtime requirements under our computational conditions.
Background models were generated separately for each suite using all sequences within the suite.
Background distributions for words of length 6 were generated using the Seeder::Background module.
Frequency files (expected values for 6-mers and 8-mers) used by Weeder were generated using a custom Perl script.
A sixth-order MM was generated for MEME using a custom Perl script, and for Motif Sampler using the INCLUSive CreateBackgroundModel program (Thijs et al., 2002).
The default (third-order) MM was generated for BioProspector using the genomebg program provided with the software.
2.3.2 Evaluation of motifs versus known binding sites The predictions were evaluated using the suite of tools described in (Sandve et al., 2007) (http://tare.medisin.ntnu.no).
The predictions were scored using the nucleotide-level Pearsons correlation coefficient (nCC) (Tompa et al., 2005).
Differences between scores were assessed using paired t-tests ( = 0.05).
2.4 Motif discovery in the promoters of Arabidopsis seed-specific genes A background set of 22 032 nuclear protein-coding gene promoters (500 bp upstream of the transcription start site) was generated using the TAIR (release 7) loci upstream sequences dataset (sequences preceding the 5 end of each transcription unit) and the protein-coding with transcript support listing (loci with supporting cDNA or ESTs deposited in Genbank), downloaded from the TAIR ftp server (ftp://ftp.arabidopsis.org).
Tissue-specific promoter sequence sets were assembled according to marker gene data from Schmid et al.
(2005).
The Seeder algorithm was used to perform motif prediction in seed-specific promoters using a seed length of six and a motif length of 12, and the protein-coding with transcript support gene promoters as a background.
3 RESULTS 3.1 Performance of motif discovery tools Figure 2 shows the differences between scores of different motif discovery tools on the benchmark suites of Sandve et al.
(2007).
Fig.2.
Average benchmarking scores and pairwise differences between motif discovery tools.
Average nucleotide-level Pearson correlation coefficient (nCC) and pairwise differences ( nCC) for six motif discovery tools tested on three benchmark suites.
Error bars correspond to 95% confidence intervals.
Stars indicate significant differences ( = 0.05) between scores.
On the AM suite, the performance of each tool was statistically equivalent.
Interestingly, the tool that performed the best (though by a nonsignificant margin), BioProspector, models background sequences using a third-order MM, the same type as that used by Sandve et al.
(2007) to generate the AM background sequences.
Seeder, BioProspector, Weeder, MEME and the Gibbs Sampler scored equally on the AR suite, which contains binding sites in their original sequence.
The MR suite also contains binding sites in their original sequence, but in this case the binding sites have a composition that is more similar to that of the surrounding background sequence.
This suite was assembled for the purpose of testing novel motif models (Sandve et al., 2007).
Seeder scored significantly higher on the MR suite than any other algorithm tested.
At first glance, it may seem surprising that the performance of some tools is actually higher on the MR suite than on AR suite.
However, although the similarity of motifs to their local background does complicate the task of motif-finding approaches using local background models, this does not overly affect those based on global background models.
It nonetheless appears that our discriminative approach to seed selection yields a nonnegligible advantage to Seeder.
Having said that, it should be noted that for a number of individual datasets the scores obtained by other tools are higher than that of Seeder, which highlights the complementary of these programs.
2305 F.Fauteux et al.
A B Fig.3.
Arabidopsis seed-specific motifs.
Sequence logos of motifs overrepresented in the promoters of A. thaliana seed-specific marker genes.
(A) Full-length forward motifs.
(B) Reverse complement of motifs.
3.2 Arabidopsis seed-specific motifs The Seeder algorithm was used to discover motifs (on both strands) in a set of 57 promoter sequences of A. thaliana seed-specific marker genes identified by expression data analysis (Schmid et al., 2005).
The computation of the background distributions (motif seed length of 6) took 35 min using a single Intel 86 processor, and motif computation took 3.5 min per motif reported.
This example shows that most of the computing time is used to compute the background model, particularly when using genome-scale background datasets.
The Seeder::Background module was therefore designed to precompute background models which can be reused for any number of motif finding operations.
The top two predictions (q-value < 0.01) were compared to known plant motifs in the PLACE database (Higo et al., 1998) using the STAMP web server (Mahony and Benos, 2007).
The first motif (Fig.3, m1) (q-value = 4.4 109, information content = 7.4) and the second motif (Fig.3, m2) (q-value = 1.1 103, information content = 7.6) are similar to two experimentally characterized cis-regulatory elements found in the napA promoter in B. napus, the RY repeat (CATGCA) (E = 6.32 108) and the G-box (CACGTG) (E = 2.92 105) (Ezcurra et al., 1999).
The function of these regulatory elements was shown by substitution mutation analysis using promoterreporter gene fusions, leading to a strong reduction of the napA promoter activity in seeds (Ezcurra et al., 1999).
The second motif is also highly similar to a sequence (ACGTGTC) (E = 4.70 1011) overrepresented in the promoters of A. thaliana genes downregulated during seed germination (Ogawa et al., 2003).
4 CONCLUSION We have described a novel algorithm for DNA motif discovery and demonstrated its capacity to discover motifs in real biological datasets.
Advantages of the algorithm over other approaches include (i) the enumerative-guaranteed optimality of seed selection; (ii) a background model based on empirical distribution of SMDs; and (iii) efficient data structures that make background and motif computations relatively fast at moderate seed lengths.
We have benchmarked the algorithm against popular motif finding tools and demonstrated its performance to be equal or better than that of other tools on biological datasets.
We note however that, although the Sandve et al.
(2007) benchmarks proved extremely useful for our performance analysis, it would be ideal to have suites designed specifically for discriminative motif-finding algorithms.
Tompa et al.
(2005) recommend biologists to use a few complementary tools, and to consider the top few predicted motifs of each tool.
Based on the benchmarks results presented in this study, we recommend the inclusion of Seeder in the biologists DNA motif discovery toolbox.
The present implementation of Seeder allows for motif searches in the mode one occurrence per sequence (oops).
This assumption is deeply engrained in the algorithm and statistics for the selection of the motif seed and the construction of the seed PWM.
Of course, once a good seed PWM has been selected, other search modes [e.g.
zero-or-one occurrence per sequence (zoops) or any-number of repetitions (anr)] could be implemented using the type of frameworks previously implemented in tools like MEME or BioProspector.
We have applied the algorithm to the analysis of A. thaliana seed-specific promoters and found that the top two motifs were similar to experimentally characterized cis-regulatory elements found in the promoters of B. napus seed-storage protein genes.
This was unanticipated, considering the array of gene families and functions found in the seed-specific gene set from (Schmid et al., 2005).
ACKNOWLEDGEMENTS We thank G.K. Sandve (Norwegian University of Science and Technology, Trondheim, Norway) for helpful comments, and the Perl Monks (http://perlmonks.org) for support in the development of the Perl modules.
We also thank the reviewers for their constructive comments.
We also acknowledge support from Fonds qubcois de recherche sur la nature et les technologies (FQRNT) and Centre SVE.
Funding: Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant No.
283303 (to M.V.S.
); NSERC Postgraduate Scholarship (PGS D) (to F.F.).
Conflict of Interest: none declared.
Abstract Proteome-wide Amino aCid and Elemental composition (PACE) analysis is a novel and informative way of interrogating the proteome.
The PACE approach consists of in silico decompo-sition of proteins detected and quantified in a proteomics experiment into 20 amino acids and five elements (C, H, N, O and S), with protein abundances converted to relative abundances of amino acids and elements.
The method is robust and very sensitive; it provides statistically reliable differ-entiation between very similar proteomes.
In addition, PACE provides novel insights into prote-ome-wide metabolic processes, occurring, e.g., during cell starvation.
For instance, both Escherichia coli and Synechocystis down-regulate sulfur-rich proteins upon sulfur deprivation, but E. coli preferentially down-regulates cysteine-rich proteins while Synechocystis mainly down-regulates methionine-rich proteins.
Due to its relative simplicity, flexibility, generality and wide applicability, PACE analysis has the potential of becoming a standard analytical tool in proteomics.
arev RA).
icine, University of Wisconsin A. eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A Introduction Modern proteomics analysis provides the identities and the rel-ative abundance changes for thousands of proteins per a single LCMS/MS experiment [1,2].
However, since many proteins have multiple functions and the exact function of many pro-teins is not yet known, this information is not always easy to rationalize.
Pathway analysis [3,4] provides mapping of the proteome onto more than 160 known signaling pathways and dozens of metabolic pathways.
Nonetheless, molecular cademy of Sciences and Genetics Society of China.
Production and hosting mailto:Roman.Zubarev@ki.se220 Genomics Proteomics Bioinformatics 11 (2013) 219229 pathways are often overlapping and inter-related, such a map-ping is rarely unequivocal.
A similar problem plagues the pop-ular gene ontology (GO) mapping.
Ideally, an aggregate analysis of the proteome state would involve mapping onto a reasonably small number orthogonal, i.e., non-overlapping and mutually independent, classification factors that have clear physico-chemical interpretations.
Although mutually orthogo-nal (extreme) pathways have been constructed for microor-ganisms [5,6], such constructs are usually artificial, i.e., do not have clear counterparts at the molecular level.
However, methods to reduce the proteome to a manageable number of orthogonal entities do exist.
For example, proteins can be broken down into their constituent amino acids (AAs).
Since amino acids in protein sequences are, in general, not mutually interchangeable (the evidence for which is their sur-vival of the evolutionary pressure), they represent an orthogo-nal set for global proteome analysis.
And since all organisms try to minimize the cost of protein synthesis by adjusting their AA content to specific growth conditions [7], it is reason-able to assume that changes in these conditions will be reflected in the abundances of the component AAs.
Thus, a proteome-wide AA composition analysis can provide an aggregate fin-gerprint characterizing the specific state of a given organism.
Unfortunately, the current methods for AA analysis all possess significant drawbacks.
Edman degradation [8], for in-stance, is limited with regard to the size of polypeptide which can be interrogated.
Meanwhile, acid hydrolysis [9,10] fol-lowed by quantification with either ninhydrin [1113] or mass spectrometry (MS) [1417] is limited by exposing proteins to harsh chemical treatment, which in turn completely destroys unstable AAs, e.g., tryptophan.
Even a short hydrolysis dura-tion leads to deamidation of asparagine and glutamine to aspartic acid and glutamic acid, respectively [10,18].
As will be shown below, the AA and element analyses of whole proteomes can provide valuable information on the ongoing metabolic processes.
Here, we present a novel, non-destructive method of performing such analysis on quantita-tive data obtained in expression proteomics experiments.
The entire Proteome-wide Amino aCid and Elemental composition (PACE) analysis is performed in silico, and as it can be applied to previously acquired data, it can provide fresh insights from earlier results without a requirement of new experiments.
In addition, this method is platform-independent, i.e., can be used for data generated with any mass spectrometric, and even non-mass-spectrometric (e.g., laser fluorescence or antibody-based) quantitative proteomics platforms.
What relevant biological insights can PACE mapping pro-vide?
At a very basic level, it can answer the question of whether two given proteomes are different better than any other known statistical method while providing a quantitative estimate of this difference and associated P value.
PACE map-ping also yields a fingerprint of the dominant metabolic pro-cesses and, in some cases, even reveals their character.
For instance, PACE analysis confirms that single-cell organisms deprived of a single element (e.g., sulfur) during growth exhibit depletion of this element in their proteins [7].
Analyzing both our own and published data with PACE, we investigated the question of whether this depletion is proteome-wide or is in-stead concentrated in a few highly abundant proteins.
We also used PACE to reveal which AA residues get depleted and to what degree.
Processes not involving nutrient depletion (e.g., cold or heat stress) also leave a specific mark in the PACE domain, which subsequently can be used as a fingerprint for their recognition.
As a novel and informative way of interro-gating the proteome, which combines relative simplicity, flexi-bility and wide applicability, PACE has the potential of becoming a standard analytical tool in proteomics.
Results Distribution of PACE signal in the proteome Until very recently, proteomics analyses were unable to reveal the entire expressed proteome due to the high dynamic range of protein expression.
Thus, in any real-life experiment, a subset of the total expressed proteome is sampled, representing the most abundant part of the proteome.
To investigate whether the partial nature of the proteomics data affects the PACE dia-gram, we analyzed a deep proteomics (>50% of the ex-pressed proteome) literature dataset of the model cyanobacterium Synechocystis sp.
PCC 6803 [19].
The total list of2000 quantified proteinswas randomly split into twohalves, and a PACEAA (Figure 1) and elemental histogram (Figure S1) were produced for each of the half-proteomes.
The visual simi-larity between the two histograms is confirmed by correlation analysis (Figure 2;R2 P 0.8 for both correlations).
This example demonstrates that the PACE signal is distributed throughout the whole proteome, and the partial nature of real-life proteo-mics data does not affect the PACE analysis fatally.
Detection of small differences between proteomes Toanswer the question as towhether the observed proteome dif-ferences between two cellular states are statistically significant, one typically needs to use principal component analysis (PCA) or a similar statistical method to differentiate two groups, each consisting of multiple replicate analyses.
In the absence of a pri-ori knowledge of statistics associated with protein abundances (each protein being, strictly speaking, a separate statistical en-tity), there is no easy method to assign statistical significance to a difference, if only two proteomics datasets are available.
However, this task becomes solvable with PACE analysis, as the following example demonstrates.
In this example, a pair of measured proteomes (lists of500 protein identities and respec-tive abundances; T1 and T2) represents two technical replicates of the same proteome B1, while a third measured proteome (B2) represents a separate biological replicate.
The protein abun-dances of the same proteome analyzed repeatedly (technical rep-licates) are affected by random, statistically independent errors in the measured abundances of individual proteins, while non-identical but biologically similar proteomes (biological repli-cates) vary in a fundamentally different way, where abundances of the proteins within the same pathway are statistically linked.
A simple comparison through the correlation coefficientR gives similar values when T1 and T2 are compared (R2 = 0.9999) as well as for the similarity between T2 and B2 (R2 = 0.9989), and provides no estimate for P values of the differences (Fig-ure 2A).
The failure of standard approaches to robustly differen-tiate between the biologically unique samples as compared to technical replicates of the same sample is further demonstrated by unsupervised PCA of the data (Figure 2A).
Here, the PCA model yields a nonsensical negative Q2 value, illustrating the inability to separate these datasets from each other.
Figure 1 Robustness of the PACE method The effect of randomly splitting the sample and control proteomes into two equal parts: the resulting PACE histograms of sample/ control comparison are very similar.
Figure 2 PACE detects minute differences A.
Principle component analysis (PCA) on three measured proteomes, of which two Biorep 1-tech rep 1(B1_T1) and B1_T2 are technical replicates, and B2_T1 is another biological replicate.
B.
PACE analysis on the same data.
left: B1_T1 vs. B1_T2; right: B1_T1 vs. B2_T1.
PCA is unable to distinguish either the technical replicates or the biological replicates from each other with statistical significance, while upon performing PACE analysis, the biological replicates are able to be teased apart with statistical significance, thus illustrating the power of PACE to identify minute but real biological variability.
Good DM et al/ Proteome-wide Amino Acid Mapping 221 In contrast, PACE analysis of the same data allows a straightforward statistical testing of the T2/T1 and B2/T1 dif-ferences (Figure 2B).
To illustrate the method of testing, imag-ine two measured proteome datasets, A and B, the comparison of which gives a PACE AA histogram A/B.
Let us define the PACE difference D as a standard deviation of the 20 AA abundance values in A/B from zero.
Since the null hypothesis is that A and B represent the same proteome, the true value of D is zero if the null hypothesis is accepted.
Thus, the question of whether A and B represent biologically different proteomes is reduced to testing whether DA/B, which is the observed value of D, is consistent with its true value being zero.
To address the latter issue, one needs to find the probability to obtain DA/B or larger value by pure chance, i.e., to calculate P value.
Assum-ing the half-normal distribution of D (assumption arising due to the fact that D is always non-negative), P value can be cal-culated as P = 1 erf(DA/B/[p 1/2Dm]), where erf is the error function and Dm is the mean value of D. The latter quantity 222 Genomics Proteomics Bioinformatics 11 (2013) 219229 can be estimated by repeated random permutation of the pro-tein abundances between A and B (this method of randomiza-tion does not require a priori knowledge of the statistical properties of individual protein abundances).
In the example above, P 0.06 (no statistical significance) for the comparison between T1 and T2, whereas P 0.007 (good statistical signif-icance) between T1 and B2.
Thus for T1 and T2 comparison, the null hypothesis (common origin) remains valid, while for T1 and B2 it should be rejected.
Therefore, PACE analysis provides a statistical evaluation of small differences between just a few measured proteome datasets, in a situation where standard statistical methods fail.
Sulfur assimilation by Escherichia coli Sulfur is an essential nutrient and can be a growth-limiting fac-tor in freshwater environments [7].
It is also unique among the six elements most important for lifeC, H, N, O, S and P, in that it is mostly protein-related, which makes it most suitable for studying proteomics effects of element availability.
More-over, sulfur is unique among the five most protein-related ele-mentsC, H, N, O and S, in that it is not found within the polypeptide backbone, but instead only in the side chains of two AAs cysteine and methionine.
Therefore, the impact due to changes in the availability of sulfur should be easily traceable not only in the element analysis, but also at the level of the AA content of the proteome.
Indeed, there is ample evidence in the literature of the im-pact that sulfur has on the proteome.
In response to decreased sulfur levels in water, the cyanobacterium Calothrix sp.
PCC 7601 initiates the production of a methionine-and cysteine-de-pleted form of its most abundant protein phycocyanin [7].
The cyanobacterium Fremyella diplosiphon behaves in a similar way.
This response occurs over the physiological range of Figure 3 Effect of sulfur depletion and nitrogen depletion on E. coli A.
Growth curves of E. coli with respect to the level of nitrogen and sul of the observed proteome changes for nitrogen depletion versus sulfur sulfate concentrations likely to be encountered by the organ-ism in its natural environment, which can be viewed as a form of environmental accommodation [20].
Although phycocyanin does not take part in sulfur fixation, its elevated expression is believed to affect the sulfur budget of cyanobacterial cells [5].
Other microorganisms, such as bacteria and yeast, can also re-spond to sulfur and carbon deprivation by reducing the num-ber of sulfur and carbon atoms in the sulfur assimilatory pathway and carbon assimilatory pathway, respectively [21].
One question which has as of yet remained unanswered by previous research is whether sulfur deprivation affects the whole proteome, or depletion in methionine and cysteine is only observed in the most abundant protein(s).
Another rele-vant question is to what extent each of these two AAs is af-fected.
To answer these questions, we grew E. coli strain BL21 under conditions when low sulfur or low nitrogen con-centrations started to reduce the growth rate (Figure 3).
Prote-omes of the microbes in their exponential growth phases were extracted and subjected to quantitative proteomics measure-ments.
PACE analysis followed based on 500 quantified proteins.
Not completely unexpectedly [16,20], sulfur depletion led to an overall reduction of sulfur content in the proteome, while nitrogen depletion led to reduction of nitrogen (Figure 3B).
At the AA level of analysis (left panel), the relative effects of sulfur starvation vary for cysteine and methionine, with cys-teine being relatively more depleted.
This effect can partially be explained by the fact that, in our PACE analysis, the N-terminal methionine has always been considered present, while in reality many proteins lack this residue.
It is, however, unlikely that the observed large differences between the cys-teine and methionine peaks are solely due to this phenomenon (vide infra).
In addition, it is likely that the cysteine/methionine depletion is contained throughout the proteome, and not fur content within their minimum growth media.
B.
PACE analysis depletion.
Figure 4 PACE analysis of sulfur depletion and nitrogen depletion on Synechocystis PACE analysis of the observed proteome changes in Synechocystis resulting from depletion of sulfur as compared to depletion of nitrogen.
The P value for sulfur depletion peak is 8 107, while for nitrogen enrichment peak, P is less than 3 107 for the element domain.
Good DM et al/ Proteome-wide Amino Acid Mapping 223 simply in a few abundant proteins.
If the latter were true, then the error bars would be much larger.
In the nitrogen depletion, it is notable that not all nitrogen-rich AAs in the proteome are affected equally.
For example, both lysine and arginine show no statistically significant differ-ence between N and S starvations, while both glutamine and asparagine are quite depleted in nitrogen starvation as com-pared to sulfur starvation.
This may be a manifestation of the fact that many E. coli strains preferentially catabolize these two AAs upon nitrogen starvation in glucose-ammonia mini-mal media [22].
Carbon/nitrogen assimilation by a cyanobacterium Cyanobacteria are the only prokaryotes capable of oxygenic photosynthesis and they play a crucial role in the global car-bon/nitrogen balance.
Wegener et al.
have performed a large-scale proteomic analysis of the widely studied model cya-nobacterium Synechocystis sp.
PCC 6803 under different envi-ronmental conditions [19].
We have PACE-analyzed their dataset of approximately 2000 proteins (53% of the predicted proteome) and their abundance changes in response to envi-ronmental stress.
Most remarkable in the study was the impact of nitrogen deficit (shortage of nitrate) during growth.
To ac-count for the observed proteome changes, the authors sug-gested that the cyanobacterium resorts in these conditions to an unusual pathway in nitrogen accommodation.
As an alternative method to pathway analysis, nitrogen assimilation can be investigated through PACE analysis.
In some microorganisms, proteins involved in the assimilation of carbon and sulfur are depleted in these respective elements compared to the rest of the proteome.
Therefore, Baudouin-Cornu et al.
predicted that oligotrophic organisms could adapt to the permanent scarcity of an element by diminution of the content of that element in all proteins [22].
This prediction has been confirmed in yeast, which adapts to sulfur scarcity by reducing the content of sulfur-rich proteins in the proteome [23].
However, no net reduction of carbon in the proteome has been reported in yeast, due to its acute response to carbon lim-itation in relation to yeast limited by other nutrients (N, S or P) [22].
If the nitrogen effect in cyanobacterium is similar to the sulfur effect observed in yeast, one could predict that a nitrogen deficit should lead to down-regulation of nitrogen-rich proteins.
To test this hypothesis and also to investigate the sulfur effect in an organism other than yeast, we performed PACE analysis of the dataset from Wegener et al.
[19].
The ele-mental histogram (Figure 4) shows the proteome changes in the cyanobacterium grown on a nitrogen-depleted medium as compared to a sulfur-depleted medium.
Here, the sulfur peak is strongly positive, while the nitrogen peak is significantly neg-ative.
The value of the latter on the arbitrary scale is 3.73, while random permutation of protein identities and abun-dances gives an average of 0.51.
Assuming normal statistics, the P value of the nitrogen peak is less than 3 107.
Similarly, the P value for the sulfur depletion peak is 8 107.
Thus, the effect of down-regulation of sulfur-and nitrogen-rich proteins upon the corresponding starvation, which has been previously seen in yeast [22], exists in other organisms as well.
At the AA level, sulfur depletion affected methionine in the proteome much more significantly than cysteine, in contrast to the situation observed in E. coli (compare Figures 3 and 4).
Nitrogen depletion caused the most significant down-regulation Figure 5 PACE elucidates similarities between heat shock and cold shock response A.
Comparison of PACE analyses of changes within the Synechocystis proteome due to heat shock and cold shock compared to standard growth conditions.
B.
Linear correlation between cold-and heat-shock responses in the AA space.
224 Genomics Proteomics Bioinformatics 11 (2013) 219229 of glutamine (Q)-and arginine (R)-containing proteins, while ly-sine (K) remained unaffected and asparagine (N) content some-what increased (Figure 4).
Therefore, it appears that the scarcity of nitrogen in the media caused a shortage of arginine, an alter-native source of nitrogen for cell growth [19].
Conversion of arginine into succinate also releases, besides glutamate and ammonia (which is also assimilated into glutamate), CO2, whose carbon is then fixed by ribulose 1,5-bisphosphate carboxylase oxygenase (RuBisCO) [19].
This process may explain the ob-served excess of carbon-containing proteins under nitrogen star-vation conditions (Figure 4).
Interpretation of the proteomics data at the level of individ-ual proteins has been less than straightforward [19].
Classifica-tion of differentially regulated proteins according to known cellular functions yielded little insight, as the results were not correlated with observed physiological responses.
Moreover, a large number of proteins with unknown functions showed significant differential regulation during both depletion and recovery phases, as did many proteins associated with common housekeeping functions.
Most proteins related to photosynthe-sis and pigment biosynthesis did not show significant changes in their abundance, although some proteins with several criti-cal functions were differentially regulated.
For example, heme oxygenase was down-regulated during nutrient depletion con-ditions [19].
This demonstrates one pitfall of straightforward interpretation of protein expression levels.
That is, although the majority of environmental perturbations had little impact on levels of proteins involved in photosynthesis, the slow growth and chlorosis indicated that the efficiency of photosyn-thetic reactions was nevertheless significantly affected by these perturbations [19].
In contrast to that complex picture arising due to the intricacy of cellular mechanics and the limited Figure 6 PACE analysis of arginine deprivation on human carcinoma cell line A431 The effects of arginine deprivation on sensitive human A431 epidermoid carcinoma cells 24 h (A) and 48 h (B) after growth in arginine-free media.
Good DM et al/ Proteome-wide Amino Acid Mapping 225 knowledge of the functional roles of proteins, PACE analysis provided an aggregate, easily interpretable view on the effect of nutrient deprivation on the proteome.
Fingerprinting of cellular response Another important aspect of PACE analysis is to provide a fin-gerprint of the responses of an organism to varying environ-mental and/or other stresses.
Figure 5 demonstrates how the Synechocystis proteome responds to heat or cold stress as com-pared to normal growth in the control BG11 media.
A striking similarity (R2 0.9, corresponding to P < 0.0001) of the AA domain response to these two seemingly opposite stressors was revealed.
This similarity is also observed on the elemental level (Figure S2).
One may hypothesize that this could be the result of each of these stresses being thermal in nature.
How-ever, in E. coli, heat shock and cold shock protein are tightly controlled not to be expressed simultaneously [24].
Thus the similarity in the AA and elemental domains does not necessar-ily extend to the level of individual proteins.
Therefore, the above PACE observation is intriguing and invites a more de-tailed research.
Effect of arginine deprivation on A431 human cells Specific AA deprivation can selectively target subsets of hu-man cancers.
To study the effect of arginine deprivation, hu-man A431 epidermoid carcinoma cells were exposed to varying time intervals with arginine-deprived media.
Figure 6 provides the first-ever view on the effect of such treatment on the proteomes after 24 h and 48 h of arginine deprivation.
Not surprisingly, a significant drop in nitrogen is observed for both depletion periods.
Another expected result was the down-regulation of the proteins rich with arginine.
Also as ex-pected, and again supporting the robustness of PACE analysis, the AA response patterns for each of the time points are quite similar, with a relative change of each being in the same direc-tion (either up-or down-regulated) within the experimental error.
Perhaps far more interesting than the expected results, how-ever, are the responses of those AAs which do not seem to be affected by such deprivation.
For example, though the overall level of nitrogen was reduced, only arginine was found to be down-regulated among the nitrogen-rich AAs.
This speaks to the selectivity of arginine deprivation.
Discussion Searching for a mutually independent limited set of parameters with which to quantitatively characterize the difference(s) be-tween proteomes, we have discovered that proteome-wide ami-no acid and elemental composition analysis (PACE-analysis) possesses the required features.
Mapping the whole proteome onto 20 AAs provides a large parameter space and thus high specificity, while also exhibiting maximum sensitivity, i.e., detecting statistically significant differences between two identical biological proteomes, which conventional methods based on individual proteins fail to uncover.
Recently, Choi et al.
have introduced an interesting approach to finding statis-tically significant differences in protein abundances that works with a small number of replicates [25].
The difference in the ap-proaches is that Choi et al.
assume that different proteins in the same proteome are statistically related, but they do not take into account the identities of individual proteins.
In Figure 7 PACE work-flow Shown here is a graphical description of the work-flow for PACE analysis.
The quantitative proteomics data are loaded and protein sequences are identified in the corresponding protein database.
For each sequence found, an array is created with the number of each AA or element contained within that protein.
These arrays for all proteins are summed together, using as weighing factors for relative protein abundances in n-th power (scaling factor).
The summed arrays for sample and control can then be compared, resulting in either a relative or absolute difference.
226 Genomics Proteomics Bioinformatics 11 (2013) 219229 contrary, PACE analysis considers AA composition of each protein and explicitly utilizes intrinsic correlations between the abundances of proteins that share common compositional features.
These two approaches are complementary, and a sit-uation is conceivable (e.g., when all protein abundances differ by less than 50%) when PACE can detect a difference that the approach of Choi et al.
will miss.
Mapping the same dataset onto five bio-elements (C, H, N, O and S) reduces the specificity but provides clear insight into metabolic assimilation of nutrients, and can give important clues in the case of a deficit of a valuable element.
Finally, PACE, being an in silico analysis, is applicable to a wide range of emerging and already published data, thus extending useful-ness of such an approach.
Good DM et al/ Proteome-wide Amino Acid Mapping 227 Materials and methods PACE analysis The PACE approach is illustrated in Figure 7.
In the simplest case, proteomics data contain a list of protein identities and their relative abundances Asi and Aci for proteomes of sam-ple and control, respectively.
In order to avoid a systematic bias due to the differences in total protein amounts, total pro-tein abundances in all proteomes are normalized to the same value prior to PACE analysis.
Another required input is pro-tein sequence database.
For each protein i in the list, the PACE algorithm finds its AA sequence in the database and reduces it to an occurrence histogram of 20 AA residues, (1aai .
.
.
20aai).
Then, the occurrence histograms for individual proteins are summed together to a total histogram (1AAi .
.
.
20AAi).
Sum-mation occurs with a weight Wi, i.e., AAi = Wi aai, where Wi A1=ni 1 Here, A is the relative abundance of protein, and n (>0) is the power factor, whose function is to reduce the effect of large proteome dynamic range (P7 orders of magnitude) and ensure that contribution of each protein to the total weight is not neg-ligible.
Typically, the value of n was in the range of 35, reflect-ing the dynamic range of the measured proteome.
Note that in PACE analysis, proteins are not separated into up-/down-reg-ulated and unchanged; all protein signals are utilized, regard-less of their intensity or statistical significance, as statistical evaluation of the results is performed at a later stage.
The total histograms (1AAs/c .
.
.
20AAs/c) for sample and control are then compared in relative terms: jkr jAAs=jAAc 1 1000 2 as well as absolute terms, jka jAA jAAc 1000 3 and expressed in promil (0.001).
Each resultant dataset con-tains 20 numbers, both positive and negative, that show the change (relative or absolute) of abundances for respective AAs in the proteome of sample and control compared to control.
A similar procedure is used for elemental composi-tion analysis, with lEs/c (l = 1. .
.5) replacing jAAs/c.
The magni-tudes and the error bars for the total histogram were calculated from of a set of results, each obtained from PACE analysis of a unique samplecontrol pair of replicates.
For instance, if there are two replicates for sample and control, then the four pairwise comparisons (S1/C1, S1/C2, S2/C1 and S2/C2) will give a set of four values for each histogram column.
The average of this set will be reported as the column magnitude, while standard error will be represented as its error bar.
E. coli growth and analysis E. coli BL21 stock cells were cultured in M9 minimal media.
To observe varying stress responses of the organism due to deple-tion of certain elements, specific forms of theM9media deficient in carbon (glucose), nitrogen (NH4Cl) or sulfur (MgSO4) were employed: control (none of the elements depleted), 5% of control, 1% of control, and 0% (100% depletion).
All samples were incubated in a Bioscreen C Automated Microbiology Growth Curve Analysis System (Growth Curves USA, Piscata-way, NJ) with a growth time of 24 h at 39 C. Growth was auto-matically recorded through use of optical density measurements taken at wavelength of 600 nm (OD600).
Three biological repli-cates were run for each condition.
At the end of culture, 3 mL of E. coli containing media were collected for each condition and spun down at 5000 g. The resulting pellet was rinsed with PBS and re-pelleted.
Lysis and digestion were performed as outlined previously [26].
Briefly, lysis buffer (8 M urea, 75 mM NaCl, 50 mM Tris, one tablet of Complete Mini protease inhibitors cocktail [Roche Diagnostics, Bromma, Sweden] and 10 mM sodium pyrophosphate) was added in a volume ratio of 3:1 buffer to cell pellet.
Samples were probe-sonicated on ice 3 60 s with 90 s pause (6 s run, 3 s pause; amplitude 40%), vortexed and then centrifuged at 20,000 g for 20 min at 4 C. Protein concentration was determined using BCA assay (Thermo Scientific, Rockford, IL, USA) and 20 lg of each sample were taken for overnight trypsin digestion, following the method previously described [26].
Resulting peptides were cleaned using C18 Zip-Tips (Millipore, Billerica, MA, USA) and samples were analyzed by LCMS/MS employing an EASY nLC (Thermo Scientific, Odense, Denmark) coupled to a Velos Orbitrap mass spectrometer equipped with electron transfer dissociation (ETD) [27,28] (Thermo Scientific, Bre-men, Germany).
Survey mass spectra were acquired at 60,000 resolving power and a data-dependent top-10 method was employed, with each precursor ion being fragmented by both ETD and collision-activated dissociation (CAD) in the linear ion trap, with subsequent detection there.
Resulting .raw data were converted to Mascot generic for-mat (.mgf) files using in-house software and ETD spectra were cleaned [29,30] prior to database searching with Mascot.
CAD and ETD spectra were not separated prior to searching against a concatenated version of the SwissProt E. coli database.
The parameters employed were: peptide tolerance 10 ppm, frag-ment ion tolerance 0.6 Da, a maximum of three missed cleavages, fixed modification of carbamidomethyl on cysteine and a variable modification of oxidation on methionine.
Search results were downloaded to a local computer as .dat files and subsequently filtered to a <1% false discovery rate (FDR) using the target-decoy strategy [31].
These filtered files were then merged and the retention times for sequenced pep-tides were aligned using in-house software.
This merged file was then re-searched in Mascot against a forward-only data-base.
The resulting .dat file was used by the Quanti quantifica-tion algorithm [32] for label-free quantification with a minimum of three proteotypic peptides employed for calcula-tion of abundance of each protein.
PCA was performed using SIMCA-P+software (Umetrics, Sweden).
Arginine deprivation A431 epidermoid carcinoma cells (ATCC; CRL-1555) have previously been shown to be sensitive to depletion of the con-ditionally-essential amino acid arginine [33], likely due to inac-tivity of argininosuccinate synthetase (ASS1).
Therefore, we chose here to employ these cells under such deprivation condi-tions to provide a model system for investigating the ability of PACE to tease out important biological information from experiments focused on in vitro studies of human cell lines.
228 Genomics Proteomics Bioinformatics 11 (2013) 219229 A431 cells were cultured in Dulbeccos Modified Eagles Medium (DMEM; VWR, Solna, Sweden) supplemented with 10% fetal bovine serum (heat-inactivated at 57 C for 1 h), 1% L-glutamine, 1% streptomycin/penicillin and 1% sodium pyruvate.
Cells were cultured in a humidified atmosphere with 5% CO2 at 37 C. Arginine-depleted media was obtained by adding arginase (20 units for 40 mL of media).
After cell split-ting and establishing solid growth, their growth media were re-placed with the depleted media (time = 0).
Control cells were grown in full media.
Cells were harvested after 24 and 48 h in the depleted media.
Upon reaching 75% confluency, cells were trypsin-released, rinsed with PBS and pelleted prior to lysis (lysis, sample clean-up and LC/MS/MS analysis were performed as described above).
Authors contributions DMG and RAZ designed experiments.
DMG wrote the PACE software.
AM performed E. coli experiments; HB and DMG performed human cell line experiments.
DMG and RAZ wrote the manuscript.
All authors read and approved the final manuscript.
Competing interests The authors claim no competing interests in the work presented here.
Acknowledgements This work was supported by grants from the Swedish Research Council (Grant No.
2009-4103) as well as the Knut and Alice Wallenberg Foundation to RZ.
DMG is thankful for a Wenner-Gren post-doctoral fellowship.
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.2013.
07.002.
ABSTRACT Reverse phase protein arrays (RPPAs) are a powerful high-throughput tool for measuring protein concentrations in a large number of samples.
In RPPA technology, the original samples are often diluted successively multiple times, forming dilution series to extend the dynamic range of the measurements and to increase confidence in quantitation.
An RPPA experiment is equivalent to running multiple ELISA assays concurrently except that there is usually no known protein concentration from which one can construct a standard response curve.
Here, we describe a new method called serial dilution curve for RPPA data analysis.
Compared with the existing methods, the new method has the advantage of using fewer parameters and offering a simple way of visualizing the raw data.
We showed how the method can be used to examine data quality and to obtain robust quantification of protein concentrations.
Availability: A computer program in R for using serial dilution curve for RPPA data analysis is freely available at http://odin.mdacc.tmc.
edu/zhangli/RPPA.
Contact: lzhangli@mdanderson.org 1 INTRODUCTION The reverse phase protein array (RPPA) is an emerging high-throughput technique in proteomics (for reviews, see Borrebaeck and Wingren, 2007; Charboneau et al., 2002; Lv and Liu, 2007; Poetz et al., 2005; Sheehan et al., 2005).
This technology has been successfully applied in a number of basic and clinical studies (Amit et al., 2007; Aoki et al., 2007; Fan et al., 2007; Pluder et al., 2006; Sahin et al., 2007; Tibes et al., 2006; Yokoyama, et al., 2007).
A single array slide can be used to measure hundreds of samples for a protein.
The protein level across the slide is detected by binding of a highly specific and sensitive primary antibody followed by detection using amplification linked to fluorescence, dye deposition, near infrared or nanoshells.
Because protein concentrations can vary over many orders of magnitude in patient or cell line samples, it is desirable to have accurate measurements of protein concentrations over a wide dynamic range.
To extend the dynamic range of the measurements, each sample is diluted multiple times successively To whom correspondence should be addressed.
and spotted on an RPPA slide so that if a protein concentration in the original sample is close to saturation, the sample can still be measured at diluted spots.
Multiple methods are available for analysis of RPPA data (Hu et al., 2007; Kreutz et al., 2007; Mircean et al., 2005).
Typically, the methods are based on modeling the response curve, which describes the relationship between the observed signal and the protein concentration.
Mircean et al.
(2005) realized that since it is the same protein being measured for all the samples spotted on an RPPA slide, the same response curve should be suitable for all these samples.
Based on this assumption, Microean et al.
proposed a robust linear-square method to quantify the protein levels.
However, an obvious drawback of the method is that it fails to recognize saturation effects for proteins at high levels.
Recently, Hu et al.
(2007) developed an alternative method using a non-linear, non-parametric approach to model the response curve.
In this study, we show an alternative approach to RPPA data analysis.
Instead of modeling the response curve, we construct a new model, serial dilution curve, which characterizes the relationship between signals in successive dilution steps.
The advantage of this approach is two fold: (i) the signals in successive dilutions can be related to each other in explicit formula in which the underlying unknown protein concentrations do not appear.
This allows a low-dimensional non-linear optimization to estimate the key parameters of the map between protein concentration and signal intensity.
The estimated map can then be applied to the observed signals to estimate the underlying abundances; (ii) it leads to an intuitive display of raw data, which is very useful for checking data quality and interpreting the model.
2 METHODS 2.1 Serial dilution curve Our new method is based on the recognition that the relationship between signals in successive dilution steps uniquely determines the response curve.
Typically, a response curve is a monotonic, s-shaped curve.
It can be described by the Sips model (Sips, 1948): S =a+bx/[1+x/(M a)] (1) where a is the background noise; b is the response rate in the linear range; M is the maximum or saturation level, x is the concentration of the protein.
Sips model has been widely used to describe adsorption including binding 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Reverse phase protein array data of DNA (Glazer et al., 2006) and proteins on solid surface (Vijayendran and Leckband, 2001).
Generally, =1 applies to conditions in which the free energy of binding of the solute molecules can take a range of values instead of a unique value (Sips, 1948), i.e.
there is some hereterogeneity in the solute molecules or the surface receptors.
When the range of the free energy of binding shrinks to a singular point, approaches to 1, in which case it is equivalent to the conventional Langmiur model.
With RPPA technology, one can only determine the relative protein concentration.
Thus, x can be chosen on an arbitrary scale.
For simplicity, we set x on a scale (i.e.
a physical unit of x) so that b = 1.
Thus, S =a+x/[1+x/(M a)] (2) On this scale, protein concentration equals the background subtracted signal (Sa) when = 1 and saturation effect can be ignored.
Starting from Equation (2), we can see that if the protein concentration is diluted from x to x/dk at the k-th dilution step, where d > 1, the expected signal would be: Sk =a+(x/dk)/[1+(x/dk)/(M a)] (3) Combine the cases for Sk+1 and Sk and eliminate x, we have: Sk =a+d(Sk+1 a)/[1+(d 1)(Sk+1 a)/(M a)] (4) Equation (3) describes Sk as a function of Sk+1, which we call the serial dilution curve, with three unknown parameters: a, M and (d is known).
These parameters have graphical interpretations from the plot.
As shown in Figure 1, the curve has two intersection points with identity line: one at background level, Sk =Sk+1 =a, the other at the saturation level, Sk = Sk+1 = M. At the left side in Figure 1, the saturation effect is of no concern and the relationship between Sk and Sk+1 is approximately linear, Sk a d(Sk+1 a).
(5) Thus, d corresponds to the slope in the linear range in the serial dilution plot.
Equation (4) suggests a new model for displaying and analyzing RPPA data.
It is important to note that Equation (4) does not contain protein concentration.
Thus, it permits an appealing way of displaying the raw data without model specification or parameterization.
Based on the plot like Figure 1, we can infer the parameters (a, M and ) from the graph or through model fitting without knowing the protein concentrations in the samples.
Fig.1.
Serial dilution plot.
Each point in the serial dilution plot is composed of an observed signal Sk at dilution step k (on x-axis) and a corresponding signal Sk + 1 of the same sample at the dilution step k + 1 (on y-axis).
The curve was produced using Equation (3).
The curve has two intersection points with the identity line: (a, a) and (M, M).
Model fitting with Equation (4) is relatively simpler than that with model fitting with Equation (2), which involves much more unknown parameters as in the existing methods of RPPA data analysis.
Altogether, the number of unknown parameters in the model with Equation (2) is three plus the number of protein samples (each dilution series count as one sample), which can be in the hundreds.
In contrast, Equation (4) only involves three unknown parameters.
2.2 Parameterization of the serial dilution curve To find the optimal parameters, we used a weighted non-linear regression model using Equation (4) as the model and taking a, D=d, M as parameters.
We assumed the observed signals have multiplicative errors except for the signals close to zero.
The weight used in the regression model is 1/(m + |S|), where m =5, which is taken as the minimal error from signal quantification from the scanner used to obtain RPPA data.
The starting values of a, D and M were taken to be max(m, min(S)), d, max(S), respectively.
The nls function implemented in R-language (Ihaka and Gentleman, 1996) was used to optimize the parameters.
The m is set to be the lower bound of a.
2.3 Estimating protein concentrations Given the parameters in Equation (4) and signals of a dilution series of a particular sample (let these be S0,S1,S2, ... ,SK ), to obtain protein concentration x in the original undiluted sample, we used the following procedure.
First, if all these signals are greater than M/r, the protein concentration x is marked to be saturated.
This threshold value of M/r is set according to an approximate estimate of the 95% confidence interval (CI) of the signals at the saturated spots.
Under multiplicative error model, assume that the error rate of the observed signals is = 10%, and the saturation level is M, we expect the CI to be [M/(1+ 2 ), M(1+2 )] = [M/1.2,1.2M].
Similarly, at background level a, we expect the 95% CI to be [a/(1+2 ), a(1+2 )] = [a/1.2,1.2a].
In general, r should be >1 and can be reduced if precision of signals is improved.
If all the signals except one are >M/r and the exception is not SK , x is also marked to be saturated.
Similarly, if all the signals are <ar, x is marked to be undetected.
If all of them except one are >M/r and the exception is not S0, x is also marked to be undetected.
The minimum and maximum of x are set to be xmin =[1/(ara)1/(M a)]1/ and (6) xmax =[1/(M/ra)1/(M a)]K/, (7) respectively.
The above steps were taken to stabilize the protein concentration estimates for out of linear-range measurements.
If x is not marked saturated or undetected, we proceed to make an estimate of x.
We choose to remove signals >M/r or <ar.
Then, we convert each of the remaining signals Sj to xj as xj =dj[1/(Sj a)1/(M a)]1/ (8) where j denotes the j-th dilution step.
To remove outliers among xjs, we identify an outlier among xjs as |xj median(x)| >3 mad(x) where mad(x) is the median absolute deviation of x.
Here, x is the vector of all xjs.
Note that the outliers can also be identified from the serial dilution plot as points far away from the dilution curve (e.g.
Fig.3A).
Finally, we give the estimate of the dilution series as a weighted average of xjs: x = (xjwj) wj (9) where wj = 1( xj a a )2 + ( xj M M )2 + ( xj )2 (10) 651 L.Zhang et al.
the partial derivatives are derived and computed according to Equation (8); a, M and are standard deviations of a,M, , respectively, which are obtained from the nls function in R. The estimated error of x is obtained from (wj)1/2.
3 RESULTS To test the utility of the serial dilution curve for analyzing RPPAdata, we first applied the method to simulated data, which was composed according to the Sips model [See Equation (2) in Section 2], with background level a = 100, saturation level M = 50 000 and = 1, dilution factor d = 2.
We added multiplicative noise (error rate = 0.15) to nominal signals and generated data as shown in Figure 2A.
The multiplicative error model has been previously suggested (Kreutz et al., 2007).
The samples were diluted to 1/2, 1/4 and 1/8 of their original concentrations serially.
Figure 2B shows the serial dilution plot, which contains all data in the dilution series.
Each point in the serial dilution plot is composed of an observed signal at dilution step k (on y-axis) and a corresponding signal of the same sample at the dilution step k+1 (on x-axis).
We found that our algorithm was able to recover the true parameters from the simulated signals accurately.
The values of a, M and were found to be 98 5, 49 800 520, 1.05 0.01, respectively.
The estimated protein concentrations are also accurate (Figure 2C), except for the cases which are clearly out of the linear range.
The lower and the upper bound of the range were calculated using Equations (6) and (7) and shown as dashed lines in Figure 2C.
Note that setting the lower and upper bound helps to stabilize the estimates of protein concentration on logarithm scale, so that small changes in observed signals do not incur large changes in the estimates.
Compare Figure 2A and C, one can also see that the linear range is much wider in the latter, showing that the dilution series can greatly expand the linear response range of the measurements.
We have also tested our algorithm with experimental data.
Figure 3 shows a typical example of RPPA dataset.
The experimental methods used to produce the array data were described by Fan et al.
(2007).
From the serial dilution plot (Fig.3A), we notice many outliers (marked by red plus signs) near both x-and y-axis.
Inspection of the original scanned image revealed that these outliers A B C D E Fig.2.
Computer simulations.
(A) Computer generated data with serial dilutions.
Red, yellow, green, blue represent undiluted concentrations, 1/2, 1/4, 1/8 original concentrations, respectively.
(B) Serial dilution plot.
The blue line shows the estimated serial dilution curve.
(C) The estimated versus the true concentrations.
The dashed lines show the upper (shown in green) and lower (shown in blue) bounds of the estimated concentrations according to Equations (5) and (6).
The red line shows the identity lines.
(D) Estimated error rates.
CV = estimated error/estimated concentration.
(E) Signal versus estimated concentrations.
Red, yellow, green, blue represent undiluted concentrations, 1/2, 1/4, 1/8 original concentrations, respectively.
652 Reverse phase protein array data A B C D Fig.3.
Example of a practical dataset.
The measured protein is beta actin, which serves as a control standard for measurements.
(A) Serial dilution plot.
Points shown in red were regarded as outliers or saturated (circled).
(B) Signal versus estimated concentration.
The signals of undiluted samples are shown in red, 1/2 diluted samples in green and 1/4 diluted samples in blue.
(C) Estimated error rates.
CV = estimated error/estimated concentration.
Each point represents result from one serial dilution.
(D) Estimated protein concentrations from replicated dilution series of the same samples.
were produced by a faulty background subtraction method that extracted signals from the scanned image.
The image quantification method took median pixel intensities from local regions outside the spotted area as the background level.
However, occasionally the protein samples seemed to spill over the spotted area, which caused grossly overestimated background levels, which in turn led to grossly underestimated signals.
Figure 3A also showed that all the signals are bounded below 65 000 (the points close to the upper bound are marked by the red circles).
This was caused by imaging software that set the maximum pixel intensity to be 65 536.
Thus, the real signals must have been truncated for these spots.
We therefore removed the points shown in red in Figure 3Abefore fitting the serial dilution curve.
The estimated parameters are a = 5, M = 63 602, = 0.57.
The estimated protein concentrations were shown in Figure 3B.
Sometimes RRPA experiment may fail to yield meaningful measurements of proteins.
In Figure 4, we show an example that has quality problems.
The experimental methods used to produce the array data was described by Tibes et al.
(2006).
Using methods as described in Section 2.2, the background was estimated to be 1000, saturation level: 4751, dilution factor: 1.11.
The black line is the identity line and the blue line is the serial dilution curve.
The serial dilution curve (blue) is very close to the identity line (black), indicating that after dilution, the signals tend to stay at the same levels as before.
This implies that the dilution had failed to produce the expected reduction of signals.
The exact cause of this effect is unclear.
From our observations, such pattern often occurs in the slides that have faint signals.
Furthermore, because the serial dilution curve is approximately linear, the saturation level cannot be accurately determined.
To evaluate data quality on an array, we find the following two measures to be most important according to our empirical experience.
(i) V1 = Percentage of data points in linear range (as defined by the interval [ar, M/r]) of all data points on the array, where a is the background level, M is the saturation level, r is the threshold value (as described earlier).
High V1 value indicates good quality of data.
When V1 is low, the data points are out of the linear range, in which cases extra manipulation of protein 653 L.Zhang et al.
Fig.4.
Example of data with quality problems.
This is a serial dilution plot.
The measured protein is GAPDH.
The red symbols show the outliers.
The background is estimated to be 1000, saturation level: 4751, dilution factor: 1.11.
The black line is the identity line and the blue line is the serial dilution curve.
concentration in the samples is needed prior to hybridization on arrays.
Alternatively, the level of antibody can be adjusted so that more data points will may fall in the linear range.
In addition, note that the distribution of the data points can also inform the significance of non-linear effects.
When most data points are far below the saturation level, the serial dilution curve approaches a straight line, in which case the saturation level is uncertain (for example, see Fig.4).
(ii) V2 = median CV on an array, where CV = estimated error/estimated protein concentration.
V2 represents estimated error rate.
High precision of protein concentration measurements is represented by low V2 values.
4 DISCUSSION Graphical display of data plays a very important role in data analysis.
For RPPA data, it is conventional to plot the observed signals against the estimated protein concentrations.
However, because the estimated protein concentrations depend on the models as well as the estimated parameters, when the signals seem to fit poorly to the estimated concentrations, it is not clear whether it is due to a suboptimal model or to noisy data.
Making the serial dilution plot per se requires no model selection or parameter fitting.
The plot presents the entire set of observables on an array in their original values.
From the plot one can identify the background level, saturation level, which signals are in the linear range, and which signals are outliers (as in Fig.3A).
Fitting a serial dilution curve needs only three parameters, which is much simpler than fitting the response curve, which requires estimating the protein concentrations as additional parameters.
From simulated RPPA data, we showed that our algorithm can yield robust and accurate estimates of protein concentrations.
From practical RPPA data, we saw some of the data points did not follow the serial dilution curve.
There may be multiple causes of the abnormal points, such as saturation or failure of binding.
It should be noted that the response curve in RPPA technology is sensitive to a large number of factors, including the amount and duration of sample incubation, specific and non-specific interactions of reporter molecules and surface chemistry in the microarrays (Seurynck-Servoss et al., 2007).
These factors complicate the interpretation of RPPA data.
Non-parametric models (Hu et al, 2007) take fewer assumptions about the hybridization kinetics in RPPA technology.
Hence, the non-parametric models are more flexible, and in some cases they may fit better with observed RPPA data.
The disadvantage of non-parametric models is that the parameters are less interpretable, while the parameters in Sips model are physically meaningful and can be used to optimize the conditions for RPPA experiments.
We believe the method developed in this study will have broad utility in RRPA applications.
Funding: M. D. Anderson Cancer Center start-up fund; MDACC Institutional Research Grant (to L.Z.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Similarity searching and clustering of chemical compounds by structural similarities are important computational approaches for identifying drug-like small molecules.
Most algorithms available for these tasks are limited by their speed and scalability, and cannot handle todays large compound databases with several million entries.
Results: In this article, we introduce a new algorithm for accelerated similarity searching and clustering of very large compound sets using embedding and indexing (EI) techniques.
First, we present EI-Search as a general purpose similarity search method for finding objects with similar features in large databases and apply it here to searching and clustering of large compound sets.
The method embeds the compounds in a high-dimensional Euclidean space and searches this space using an efficient index-aware nearest neighbor search method based on locality sensitive hashing (LSH).
Second, to cluster large compound sets, we introduce the EI-Clustering algorithm that combines the EI-Search method with JarvisPatrick clustering.
Both methods were tested on three large datasets with sizes ranging from about 260 000 to over 19 million compounds.
In comparison to sequential search methods, the EI-Search method was 40200 times faster, while maintaining comparable recall rates.
The EI-Clustering method allowed us to significantly reduce the CPU time required to cluster these large compound libraries from several months to only a few days.
Availability: Software implementations and online services have been developed based on the methods introduced in this study.
The online services provide access to the generated clustering results and ultra-fast similarity searching of the PubChem Compound database with subsecond response time.
Contact: thomas.girke@ucr.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on August 26, 2009; revised on November 19, 2009; accepted on February 16, 2010 1 INTRODUCTION Software tools for mining the available small molecule space play an important role in many bioscience and biomedical areas.
To whom correspondence should be addressed.
They are ranging from drug discovery, chemical biology and chemical genomics to medicinal chemistry (Haggarty, 2005; Oprea, 2002; Oprea et al., 2007; Savchuk et al., 2004; Strausberg and Schreiber, 2003).
Currently, the structures of over 20 million distinct small molecules are available in open access databases, like PubChem (Austin et al., 2004), ChemBank (Seiler et al., 2008), NCI (Ihlenfeldt et al., 2002), ChemMine (Girke et al., 2005), ChemDB (Chen et al., 2007) and ZINC (Irwin and Shoichet, 2005).
To analyze these important data resources, efficient structure similarity search tools are essential for retrieving chemical and bioactivity information from databases (Chen and Reynolds, 2002; Sheridan and Kearsley, 2002).
In addition, they are often useful for predicting bioactive small molecules (Cao et al., 2008; Cheng et al., 2007).
A variety of structure similarity search methods are available (reviewed by Willett et al., 1998).
Unfortunately, they are often not fast enough for systematic analyses of very large compound collections with millions of compounds.
This is because most of these methods sequentially compare a query structure against all entries in the database and then rank the results by a chosen scoring system, and thus the cost to perform similarity searches grows linearly with the size of the compound database.
Therefore, more efficient and sophisticated search methods need to be developed to utilize the available chemical space efficiently.
Clustering of compound sets is essential on practically all stages of the discovery process of bioactive compounds (reviewed by Downs and Barnard, 2002).
Commonly, structure similarity-based clustering utilizes the pairwise similarity measures generated by the above compound search methods to partition the data into discrete groups of similar compounds.
An example is JarvisPatrick clustering, which is among the most widely used clustering methods in cheminformatics (Willett, 1987).
Alternatively, they can be used to build hierarchical trees that represent the similarity relationships among all items in a compound dataset.
One of the main challenges in this area is the difficulty to cluster the millions of compound structures that are currently available in the public domain.
This is because many cluster analysis approaches multiply the complexity of a chosen similarity search method by the number of compounds in the dataset.
They often require the calculation of all-against-all similarities for the compounds under investigation and the computational cost grows quadratically with the size of the dataset.
Therefore, novel clustering methods need to be developed for exploring this vast chemical space efficiently.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 954 953959 Y.Cao et al.
A few methods have been proposed to accelerate the nearest neighbor searching in compound databases.
Agrafiotis and Lobanov (1999) reported an algorithm based on k-dimensional (or k-d) trees for diversity analyses.
This problem is relevant to clustering (Bentley, 1975) and the method can possibly be adopted to nearest neighbor searching.
It works by representing each chemical compound as a multidimensional vector and organizing the compound dataset in a k-d tree structure to speedup diversity analyses.
However, the vector representation used is very limited and the k-d tree cannot handle high-dimensional vector data.
Later the authors proposed a new data structure called-tree to perform guided nearest neighbor search of compounds in general metric space (Xu and Agrafiotis, 2003).
A-tree organizes the database in recursively partitioned Voronoi regions and represents these partitions as a tree.
The method greatly reduces the number of similarity comparisons required for the nearest neighbor search by applying an efficient branch and bound strategy.
As a distance-based indexing method, the-tree approach does not require the compounds to be represented as multidimensional vectors.
However, it requires the similarity measure used to be a metric.
Swamidass and Baldi (2007) used upper bounds on similarity measures for structural fingerprints to reduce the number of molecules that need to be considered in the nearest neighbor searches.
Later, Baldi et al.
(2008) employed tighter bounds to achieve further time savings.
These latter two methods have been designed to be used only with fingerprint-based similarity measures.
A variety of data structures and algorithms have been proposed to accelerate the nearest neighbor search in multidimensional Euclidean space (reviewed by Bohm et al., 2001).
These methods, often referred to as multidimensional access methods (MAMs), have not been widely used in similarity searching and clustering of chemical compounds.
The reason for this may be the popularity of non-Euclidean similarity coefficients in chemical structure similarity searching, which are not immediately compatible with MAMs.
Moreover, Fu et al.
(2000) reported that the embedding of the generic metric space into multidimensional space can introduce considerable inaccuracy in the nearest neighbor search applications.
In this article, we introduce the embedding and indexing (EI)-Search and EI-Clustering methods for ultra-fast similarity searching and clustering of very large datasets using an EI strategy.
First, we introduce an efficient embedding algorithm.
Subsequently, we describe the design of EI-Search and EI-Clustering, and test their efficiency and accuracy.
Finally, experimental test results for three large compound datasets are presented and discussed.
2 METHOD 2.1 Embedding compounds in Euclidean space The foundation of our algorithms is the usage of embedding techniques to build multidimensional vector representations of compound structures, which can be used to approximate compound dissimilarities by the inter-vector distances.
Embedding objects in Euclidean space offers many benefits, such as the possibility of accelerating the nearest neighbor search.
In addition, they are useful for all-pair query approaches used in data visualization, clustering and data mining (Faloutsos and Lin, 1995).
The problem of geometric embedding has previously been studied and applied to the nearest neighbor search in metric space.
Three of the most widely used methods in this area are multidimensional scaling (MDS; Kruskal and Wish, 1978), stochastic proximity embedding (SPE; Agrafiotis, 2003; Agrafiotis and Xu, 2002; Smellie et al., 2006) and FastMap (Faloutsos and Lin, 1995).
MDS is used to discover structures in datasets by representing the relationships among its objects as spacial distances in a low-dimensional display plane.
It is computationally expensive because it depends on the availability of all pairwise dissimilarities among the objects in a dataset.
As a result, it becomes quickly infeasible for compound databases with more than a few thousand entries.
Many variances of MDS have been proposed to solve the problem for large datasets.
Chang and Lee (1973) selected from the whole dataset a smaller number of representative objects, called pivots, and applied classic MDS to this subset.
The remaining objects were then embedded to the same Euclidean space based on their distances to the pivots.
SPE is an alternative to MDS.
As a self-organizing algorithm, SPE starts with an initial assignment of data points to coordinates and carries out iterative pairwise refinement steps by adjusting randomly selected pairs of coordinates to better approximate the corresponding dissimilarity values.
SPE is very efficient and is reported to scale linearly with the size of the dataset.
FastMap is another fast alternative of MDS with linear-time complexity.
It gains its computational efficiency by directly calculating the induced vectors rather than iterative improvement steps used by most MDS implementations.
However, the proper choice of the set of pivot objects can be complicated by the presence of large numbers of outlier compounds that are not similar to any other compounds in a compound library.
This study presents an alternative embedding method that is accurate and robust enough to process very large compound datasets.
The initial steps of the embedding procedure are similar to the method from Chang and Lee (1973), but it differs significantly in its final optimization steps.
The method starts by dividing all compounds into two sets: a small reference compound set and a much larger target compound set.
The reference compound set is a user-definable parameter that can be generated by maximum diversity or random selection methods of compounds in a given library.
Traditional MDS is applied to obtain the coordinates of the induced reference vectors for the reference compounds.
Subsequently, an induced target vector is obtained for each target compound by computing the vector coordinates that can best preserve its dissimilarity to all reference compounds.
More specifically, for the i-th target compound oi, the following stress function is minimized: stress= |R| j=1 ( d(oi,rj) d(xi,rj) )2 .
(1) In this equation, d(oi,rj) is the dissimilarity value between target compound oi and reference compound rj .
The variable rj is the coordinate of the j-th induced reference vector obtained by applying MDS to the reference compounds.
The unknown coordinate of the i-th target vector is xi, and d(xi,rj) gives the Euclidean distance between xi and rj .
By minimizing the stress function with a global optimization algorithm, the coordinate xi can be computed so that it best preserves the dissimilarities from target compound oi to all reference compounds.
Two very important parameters in our modified version of the MDS algorithm are the number of dimensions D and the reference compound set.
Large D values will not increase the minimum value of the stress function, and thus will never negatively impact the embedding quality.
To maximize the embedding quality, our method can be used with conservatively large D values of over 100, but at the expense of longer computation times for both the embedding and the downstream similarity searches.
Often D values >100 may not be necessary for many types of molecular descriptors (e.g.
physicochemical), due to their frequently high redundancy and correlation among each other (Agrafiotis and Xu, 2002).
It might also be possible to take advantage of low intrinsic dimensionality of some similarity measures, using methods such as ISOMAP (Tenenbaum et al., 2000) and locally linear embedding (LLE; Roweis and Saul, 2000).
However, our tests using ISOMAP with atom pair descriptor also showed that D values >100 may still be necessary for some molecular descriptors to achieve satisfactory accuracy.
954 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 955 953959 EI-Search and EI-Clustering The reference compound set is the second important parameter for our approach.
The size of the set, R, directly controls the complexity of the stress function.
Therefore, the larger the reference compound set, the longer the embedding time.
Furthermore, both the size and the composition of the reference set have an influence on the embedding quality.
This is because the induced reference vectors serve as similarity landmarks to position each target compound in the embedding space according to its dissimilarity profile to all reference compounds.
To minimize ambiguous placements, the reference set should ideally be chosen as diversely as possible and cover the entire chemical space of the target compounds.
However, our benchmark tests for structure similarity searching and clustering, have shown that randomly selecting reference compounds is sufficient to obtain robust results (Section 3.5).
This is partly because R is usually much larger than the number of D. Due to this redundancy, choosing one or several suboptimal reference compounds will not have a great impact on the embedding quality.
Moreover, it is important to identify the smallest values for R and D, which can achieve satisfactory accuracy.
Increasing either value will result in longer processing time.
This does not only apply to the embedding of the compound database, but also to every query structure used in similarity searches, because both components have to go through the embedding process.
This slightly offsets the time saving gained by the other parts of the algorithm.
Agrafiotis et al.
(2001) presented a neural network-based method that could reduce the embedding time of our method significantly.
However, in our tests we were not able to achieve embedding qualities with this method that were sufficient for similarity search and clustering applications (Supplementary Table S-1).
Compared to the quadratic time complexity of traditional MDS methods, our algorithm offers large time savings.
Given similar distributions of pairwise dissimilarities and a fixed number of reference compounds, the running time of the above embedding algorithm is roughly linear to the number of compounds.
Because each target compound is processed independently, our method can be easily parallelized on computers with multiple CPU cores or compute clusters.
This is a very desirable feature when processing very large compound databases.
The related SPE method is in its current form less effective for processing very large datasets because of the challenges involved in implementing a parallelized version of this algorithm.
Our tests with a publicly available implementation of the SPE algorithm show that its unparallelized compute times on very large datasets with millions of compounds are too long to obtain embeddings of high enough qualities to perform accurate nearest neighbor searches with acceptable recall rates (Supplementary Table S-2).
Nevertheless, SPE is a powerful algorithm that can be easily utilized as an alternative embedding method by our EI-Search and EI-Clustering tools.
2.2 EI-Search After the compounds are embedded into the D-dimensional Euclidean space, structure similarity searches can be performed by a nearest neighbor search in the embedding space.
For this, the query compound is embedded into the same D-dimensional Euclidean space.
Subsequently, the corresponding induced vector is used to perform a nearest neighbor search in the embedding space.
This search method offers great time savings and flexibility for similarity searches because of two major reasons.
First, compared to other similarity measures used for comparing compound structures, Euclidean distances can be calculated very efficiently.
Second, many MAMs can be employed in the nearest neighbor search to further improve its time efficiency.
Although many MAMs have been proposed to speedup the nearest neighbor search problem, most of them are affected by the dimensionality problem, often referred to as the curse of dimensionality.
This is the phenomenon that for various geometric search problems, including the nearest neighbor search, the best algorithms known have performance trends that degrade exponentially with an increase in dimensionality (Weber et al., 1998).
Our tests also confirmed that the SR-tree method (Katayama, 1997) becomes slower than sequential scans of the dataset as soon as D is set to values >100.
However, our tests also indicated that the dissimilarities between compound structures can only be preserved reliably with large D values of at least 100.
Thus, it is important for our method to combine high-dimensional embedding and an indexing method that are both insensitive to the dimensionality problem.
It has become increasingly popular to use the approximate nearest neighbor search in high-dimensional space to avoid the dimensionality problem.
One of these approximation approaches utilizes a spatial index using locality sensitivity hashing (LSH) to perform the fast nearest neighbor search in Euclidean space.
LSH uses a family of hashing functions to map database objects into buckets.
It is designed to join related items based on a given similarity measure with high probability.
Accordingly, many hashing functions vary with respect to their similarity measures.
For example, Gionis et al.
(1999) introduced in the original LSH paper a bit sampling-based LSH approach that uses the Hamming distance measure.
Datar et al.
(2004) proposed an LSH scheme for p-stable distributions along with Euclidean distances.
However, so far no hashing functions have been developed for the similarity measures that are commonly used for comparing compound structures.
This limitation can be addressed by embedding compounds in Euclidean space and building for them a spatial index using induced vectors in embedding space.
Taking advantage of the above embedding and the LSH-based spatial indexing approaches, we designed an efficient approximate compound structure similarity search algorithm.
This algorithm is named EI-Search after its two key components: embedding and indexing.
The algorithm first preprocesses the compound dataset by embedding it into a high-dimensional Euclidean space and generating a spatial index using LSH.
When searching for k compounds that are most similar to a given query compound, a two-step approach is employed to reduce the error introduced in the embedding process and the approximate nearest neighbor search.
First, the query compound is embedded in the same Euclidean space.
The resulting induced vector is then used in an index-assisted nearest neighbor search of the embedding space to retrieve a candidate set consisting of k vectors that are most similar to it.
The relaxation ratio is a user-defined parameter that controls the trade-off between processing time and search accuracy.
Larger values for result in larger candidate sets and possibly higher accuracy, but at the cost of longer search times.
Second, a refinement step applies exact structure similarity searches to the candidate set obtained in the first step.
This allows the selection of the final k compounds that are most similar to the query structure.
Compared to commonly used structure similarity search methods, EI-Search has several advantages.
The most important ones are its time efficiency and compatibility with a wide range of similarity measurements.
This makes the method potentially useful for accelerating similarity searches of a variety of data objects that are of relevance to many life science and non-life science areas.
2.3 EI-Clustering In addition to similarity searching, Euclidean space representations can be used for clustering large datasets very efficiently.
For example, spatial join can be used to perform single linkage clustering by finding all vector pairs, which are separated by a distance below a given threshold (Brinkhoff et al., 1993).
The nearest neighbor information required for JarvisPatrick clustering can also be obtained very efficiently in Euclidean space by using an efficient algorithm for the all-nearest-neighbors problem (Vaidya, 1989).
In this article, we introduce a new clustering method, EI-Clustering, that takes advantage of the accelerated search speed provided by EI-Search to cluster very large sets of chemical compounds under the JarvisPatrick clustering framework.
JarvisPatrick clustering requires a nearest neighbor table, which consists of p nearest neighbors for each compound in the dataset.
This information is then used to join compounds into clusters that share at least m nearest neighbors.
The values for p and m are user-defined parameters.
In case of EI-Clustering, the EI-Search method generates the nearest neighbor information for each compound in the dataset.
The resulting nearest neighbor table is then used as direct input for JarvisPatrick 955 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 956 953959 Y.Cao et al.
clustering.
When clustering very large datasets, EI-Clustering is particularly efficient due to its fast computation of the nearest neighbor table.
3 EVALUATION 3.1 Implementation We implemented the EI-Search and EI-Clustering algorithms as C++ programs.
Internally, they use the L-BFGS-B library (Zhu et al., 1997) for the global optimization step in the embedding procedure.
For the LSH-based spatial indexing, we utilized the lshkit library that implements the MPLSH algorithm, a variant of LSH that requires fewer hash tables and offers large space savings as well as shorter query time (Lv et al., 2007).
In addition, we wrote a reusable C++ library for calculating atom pair descriptors.
We were particularly interested in testing atom pair descriptors because of their superior performance in structure similarity searching (Chen and Reynolds, 2002).
To also include in our tests one of the most widely used fingerprint methods, we employed the fingerprint descriptors used by PubChem and utilized a fingerprint implementation provided by the NIH Chemical Genomics Center (2009).
JarvisPatrick clustering was performed with a custom program implemented in C++.
3.2 Datasets and testing platform To test the performance of the proposed methods, benchmark comparisons for similarity searching and clustering were performed using the publicly available compound structures from NCI and PubChem.
The NCI dataset consisted of 260 071 compounds.
After removing entries that did not generate any usable atom pair descriptors, 260 027 compound structures were used from this collection.
From PubChem, we used the structures from the PubChem Compound dataset.
From this collection, we selected two sets: one subset consisting of 2.3 million compounds with at least five non-hydrogen bonds (PubChem Subset) as well as the entire PubChem Compound library with over all 19 million compounds (PubChem Compound).
The tests were performed on a Linux computer equipped with Xeon CPUs clocked at 2.4 GHz and 16 GB of RAM.
For parallelized computations, a computer cluster was used with the exact same hardware configuration per compute node.
Compute times are given as total CPU hours throughout the text.
3.3 Time efficiency of embedding First, we evaluated the running time of our modified MDS algorithm for different parameters.
For this, the compounds from all three datasets were embedded into a high-dimensional Euclidean space.
3.3.1 Time efficiency with respect to the size of the dataset Our results show that the time required for embedding increases almost linearly with the number of compounds in the library (Table S-3 in Supplementary Materials).
The average time to embed one compound varies only slightly across the three datasets and is below 0.3 s using a practical parameter set.
Because the embedding algorithm processes each compound independently after applying MDS to the reference compound set, it is trivial to further reduce its computation time by using a compute cluster.
In our experiments, it took around 10 min to process the 260 027 compounds of the NCI dataset using 87 CPUs.
Similarly, the 19 million compounds of the PubChem Compound library could be processed in less than a day using 80 CPUs.
3.3.2 Time efficiency with respect to the embedding parameters As discussed above, the number of dimensions D and the number of reference compounds R are the two main factors that will affect the embedding time.
To estimate their impact, we randomly selected from the NCI library seven reference compound sets with sizes ranging from 240 to 800 compounds.
These reference sets were used in independent test runs of our embedding algorithm where D was set to a fixed value to study the influence of R on the total CPU time.
Similarly, to model the impact of D, the value of R was fixed to three times the value of D and the total CPU time was collected using D values ranging from 120 to 260.
These ranges were chosen to ensure high-quality embedding, but also to keep the computation within manageable time limits.
Our test results (Supplementary Fig.S-1) indicate that the total CPU time of our method grows linearly with R, and exponentially with D. Based on this time behavior, the values for R and D should be chosen as small as possible, but large enough to maintain an embedding quality that is sufficient for the downstream similarity search and clustering steps (see below).
3.3.3 Time efficiency and global optimization parameters Solving the global optimization problem accounts for most of the embedding time.
Therefore, the parameter choice of this step has a great impact on the time efficiency of the embedding step of our method.
Using less stringent termination conditions for the L-BFGS-B algorithm, will typically reduce the embedding time, but at a cost of embedding quality.
For example, the average time to embed one compound could be easily cut into half with a small loss in accuracy (Supplementary Table S-3).
Although the preprocessing of new datasets is time consuming, our method is relatively flexible with respect to adding new entries to an already preprocessed library.
Because all entries are embedded independently, one can easily add new ones without repeating this process for the entire dataset.
This meets the work flow requirements of many large compound databases, where minor updates occur frequently, but major revisions are rare.
3.4 Quality of embedding As pointed out by previous studies, embedding metric repre-sentations of objects in Euclidean space may reduce the accuracy of the nearest neighbor searches (Fu et al., 2000).
However, when the parameters for our embedding method are chosen properly, the method is accurate enough to be used in the prescreening step of EI-Search (as shown in Section 3.5).
As a performance test of the embedding step, we randomly selected 10 million compound pairs from the NCI dataset, computed their atom pair-based Tanimoto similarity coefficients and compared them with the corresponding distance values obtained from the induced vectors.
The two datasets were highly correlated, as indicated by a Pearsons correlation coefficient of 0.79.
Additionally, the agreement among the datasets was evaluated by grouping the Tanimoto coefficients into 10 similarity intervals from 0 to 1 using increments of 0.1.
Subsequently, the distributions of the vector distances for each interval were plotted in the form of box plots.
In this representation, a low degree of overlap among the boxes from adjacent intervals indicates a strong agreement between the two 956 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 957 953959 EI-Search and EI-Clustering methods.
The box plots obtained from our tests show only a moderate overlap among adjacent intervals, and only minor to no overlap among more distant intervals (Supplementary Fig.S-2).
This indicates a strong agreement among the two methods for the majority of the compounds with some inconsistencies at the interval boundaries, but for a much smaller number of cases.
For example, for high Tanimoto similarities ranging from 0.8 to 1, the similarities of the corresponding vectors pairs (1distance) fall all into a very narrow range 0.751.0.
The two methods also agree very well in the low similarity range.
This is indicated by the fact that 98.17% of all compound pairs with a Tanimoto coefficient <0.4 are placed into a very similar range (<0.38) in vector space.
Based on these results, our embedding method appears to preserve well-separated ranges of Tanimoto coefficients in a robust manner.
3.5 Accuracy of EI-Search To further examine the accuracy of the EI-Search method for compound similarity searching, we implemented an EI-Search program and tested it with different parameters.
The accuracy of EI-Search is measured by the recall rate.
When retrieving the k most similar compounds to a query structure, the recall rate is defined as the percentage of compounds obtained with EI-Search that are also returned by sequential search methods.
To evaluate the impact of the different parameters used by EI-Search, the program was run using different values of D, R, k and .
In each run, the recall rates for 1000 random queries from the NCI dataset were calculated.
First, we compared the recall rates when searching for the k most similar compounds with different R and values while the values of k and D were fixed.
(Supplementary Fig.S-3a shows the results for constant k and D values of 100 and 140, respectively.)
The results indicate that increasing from 1 to 50 results in a significant improvement of the recall performance, while this effect starts to plateau off at values of >20.
With respect to the number of reference compounds R, the recall rate increases from values 240 to 420.
Based on these results, a good empirical choice for R is between two and three times the value of D. Another observation is the fact that the effect of R diminishes for values >20.
In other words, large enough values can compensate a suboptimal choice of R. For example, when was set to 20, and k to 100 and D to 140, the best and worst recall rates were 97.87% and 97.35%, respectively.
This corresponds to a difference of only 0.52%.
Similarly, we investigated the correlation between the recall rates and the values of D and .
According to the results from the previous tests, the R values were always set to three times the values of D. When searching for the k most similar compounds the recall rates were collected for different D and values while the value of k was fixed.
Supplementary Figure S-3b shows the results for a k value of 100.
These results indicate that the recall rates consistently improve with the number of dimensions.
This effect is much stronger for smaller values.
For example, when k was set to 100 and to 1, then the recall rate could be improved from 58.35% to 65.57% for D values of 120 and 260, respectively.
For large values above 20 this effect is again much less pronounced.
While larger values will result in an increase in processing time, their impact is less severe than increasing the value of D. Accordingly, we chose in the subsequent experiments the D values as small as possible and the Table 1.
Performance tests of EI-Search Dataset NCI PubChem Subset PubChem Compound Descriptor type Atom pair Atom pair Atom pair Fingerprint Average search time (s) Sequential search 0.800 11.570 93.121 19.658 EI-Search 0.067 0.170 0.427 0.499 Recall of EI-Search (%) Mean 99.95 99.60 97.38 96.32 SD 0.44 1.82 5.61 11.54 Search times and recall rates are listed for searching three large compound sets with EI-Search and the sequential search methods.
The same descriptor type was used for each comparison pair.
The experiments were performed on the same hardware using the same embedding and relaxation parameters (R = 300, D = 120 and = 30).
The LSH parameters were supplied by lshkit.
values as large as necessary to maintain both high accuracy and time efficiency of the method.
3.6 Time efficiency of EI-Search While maintaining high recall rates, EI-Search was able to greatly reduce the time for performing structure similarity searches in large compound databases.
To examine the time efficiency of EI-Search, 1000 random queries were performed on each of the three datasets using first the EI-Search program and then exhaustive sequential searches with the atom pair and fingerprint similarity search programs.
For each comparison we used for EI-Search the same descriptor type as for the sequential search methods.
The query compounds were randomly selected from the dataset.
To obtain realistic search results, the 100 most similar compounds with a minimum similarity of 0.5 were retrieved for each query.
Atom pair descriptors combined with Tanimoto coefficients were used as similarity measure.
For the PubChem Compound library, we also included in the tests the Tanimoto similarities of PubChems fingerprints.
According to the above parameter optimization results, we used in all tests the following settings: R = 300, D = 120 and = 30.
The obtained search times and recall rates are listed in Table 1.
Several conclusions can be drawn from Table 1.
First, in comparison to the highly accurate atom pair method, EI-Search achieves in the atom pair descriptor tests very high recall rates ranging from 97.38% to 99.95%.
In comparison to the fingerprint method, the recall rate of the corresponding EI-Search is slightly lower with 96.32%.
The reason for this reduction may be the fact that fingerprints provide less accurate similarity measures than atom pairs (Chen and Reynolds, 2002).
This could result in less robust rankings of the nearest neighbor search results, and therefore a slightly lower recall rate is reasonable.
Second, EI-Search provides significant time savings for the nearest neighbor searches.
For example, the average time required to search >19 million compounds of the PubChem Compound dataset is reduced for the atom pair approach from >93 to <0.5 s, and for the fingerprint method from >19 to <0.5 s. This corresponds to accelerations by our EI-Search method of over 200 and 40 folds, respectively.
Third, while the search time for the exhaustive sequential search methods increases linearly with the size of the databases, this increase is less than linear for the EI-Search method.
For instance, searching 957 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 958 953959 Y.Cao et al.
Table 2.
Performance tests for EI-Clustering Dataset NCI PubChem Subset PubChem Compound Similarity measure Atom pair Atom pair Atom pair Fingerprint Total clustering time (h) JarvisPatrick 72.9 7355.6 N/A N/A EI-Clustering 3.5 92.2 1517.2 2869.71 Jaccard coefficient 0.9913 0.9887 N/A N/A The table compares the time and accuracy performance of EI-Clustering with Jarvis Patrick clustering when using exhaustive search methods for generating the required nearest neighbor information.
The compute time is given in hours of total CPU time.
The agreement among the clustering results is given in the last row in form of Jaccard partition coefficients.
Clustering of the PubChem Compound dataset was not possible with the exhaustive search methods due to their insufficient performance on this large dataset.
the PubChem Subset dataset with EI-Search takes on average only 2.5 times longer than searching the NCI dataset, that has one-ninth of the size of PubChem Subset.
Finally, another outstanding feature of EI-Search is the fact that its search speed is much less impacted by the complexity of the similarity measure used for database searching than this is the case for the exhaustive methods.
For instance, the switch from the fingerprint similarity measure to the more computationally expensive atom pair similarity measure results only in a minor increase of EI-Searchs query time, while it is a >4-fold increase for the exhaustive sequential search methods.
3.7 Accuracy and time efficiency of EI-Clustering To test the performance of our EI-Clustering method for partitioning large compound sets, we clustered all three compound sets with the JarvisPatrick algorithm.
The required nearest neighbor tables were generated with EI-Search and the exhaustive sequential search methods.
EI-Search was run with the same embedding and searching parameters (R = 300, D = 120 and = 30) as in the previous section.
The LSH parameters were slightly changed to achieve higher accuracy.
The process of generating the nearest neighbor tables was parallelized on a computer cluster.
For each clustering result, the total search time was calculated for all utilized CPUs.
To measure the agreement among the clustering results, we computed the Jaccard partition coefficient for each pair of clustering results (Table 2).
Jaccard coefficients close to zero indicate low similarities and values close to one indicate high similarities among the evaluated cluster sets.
The results in Table 2 show that the EI-Clustering method can dramatically reduce the processing time of the Jarvis Patrick clustering approach while maintaining a high level of agreement with the results obtained by JarvisPatrick clustering with exhaustive nearest neighbor search methods (Jaccard coefficients >0.98).
The total CPU time to process over 2.3 million compounds from the PubChem Subset could be reduced from over 306 days to just 4 days.
This is a major improvement, because it makes it feasible to cluster millions of compounds in a few days on a regular workstation or in a few hours when a small computer cluster is available.
By running EI-Search on 80 CPUs on a computer cluster, we were able to cluster the entire PubChem Compound library in only a day.
Because EI-Clustering spends most of the time running EI-Search, which runs in time sublinear in the size of the dataset, the compute time of EI-Clustering is subquadratic to the size of the dataset.
Therefore, EI-Clustering scales much more efficiently to larger datasets than traditional methods.
The superior speed of EI-Clustering comes at the cost of a larger memory footprint compared to the other methods.
Most of the memory is consumed by its LSH index.
For example, when clustering the 19 million PubChem Compound library, the LSH index requires around 13 GB of memory.
Considering the performance of todays research workstations, this memory requirement appears to be manageable.
3.8 Availability of the programs and data The EI-Search and EI-Clustering programs can be downloaded from http://chemmine.ucr.edu/ei/.
The same web site features an online service using EI-Search for ultra-fast similarity searching of the PubChem Compound library with subsecond response time.
In addition, the site provides access to the EI-Clustering results of the entire PubChem Compound library that are based on atom pair and PubChem fingerprint descriptors.
4 CONCLUSIONS AND FUTURE WORK In this study, we have presented EI-Search and EI-Clustering as efficient methods for accelerating structure similarity searches and clustering of very large compound datasets.
The acceleration is achieved by applying embedding and indexing techniques to represent chemical compounds in a high-dimensional Euclidean space and to employ ultra-fast prescreening of the compound dataset using the LSH-assisted nearest neighbor search in the embedding space.
Our tests show that the method can dramatically reduce the search time of large databases, by a factor of 40200 folds when searching the 100 closest compounds to a query.
Recently published acceleration methods achieved only a 5.5-fold reduction in search time when using a Tanimoto threshold of 0.8 and up to 20-fold with a relatively restrictive threshold of 0.9 (Baldi et al., 2008; Swamidass and Baldi, 2007).
Another limitation of these methods is their narrow utility spectrum that is currently restricted to fingerprint-based searches.
In contrast to this, the EI-Search framework is designed to be useful for a wide spectrum of similarity measures.
After embedding, EI-Search will run in most cases with comparable time efficiencies independent of the complexity of the similarity measure.
This can be particularly useful for accelerating searches that use much more accurate, but computationally very expensive similarity measures, such as maximum common substructures or 3D approaches (Cao et al., 2008; Raymond et al., 2003; Willett, 2005).
By taking advantage of the fast similarity search speed of EI-Search, we developed EI-Clustering into an effective clustering method for very large datasets.
The method accelerated the clustering of the three test datasets used in this study by 2080 folds.
Most importantly, the EI-Clustering made it feasible to cluster datasets of almost 20 million entries within acceptable time limits.
Due to its subquadratic running time, the EI-Clustering method should scale well enough to cluster even larger datasets with tens or even hundreds of millions of objects.
In the future, we will expand the performance and utility spectrum of the EI-Search and EI-Clustering methods on several levels.
First, several statistical methods will be employed to further improve the embedding algorithm by dynamically optimizing its parameters and the selection of the most effective reference compounds.
Second, 958 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 959 953959 EI-Search and EI-Clustering the EI-Search method will be tested and optimized for the usage of a variety of more complex similarity measures that are available for compound structures.
Finally, additional clustering algorithms, besides JarvisPatrick clustering, will be incorporated into the EI-Clustering method.
ACKNOWLEDGEMENTS We acknowledge the support from the Bioinformatics Core Facility, the Center for Plant Cell Biology (CEPCEB) and the Institute for Integrative Genome Biology (IIGB) at UC Riverside.
Funding: National Science Foundation (grants IOB-0420033, IOB-0420152, IGERT-0504249 and IIS-0711129).
Conflict of Interest: none declared.
Abstract A resource that provides candidate transcription factor binding sites (TFBSs) does not currently exist for cattle.
Such data is necessary, as predicted sites may serve as excellent starting locations for future omics studies to develop transcriptional regulation hypotheses.
In order to gen-erate this resource, we employed a phylogenetic footprinting approachusing sequence conserva-tion across cattle, human and dogand position-specific scoring matrices to identify 379,333 putative TFBSs upstream of nearly 8000 Mammalian Gene Collection (MGC) annotated genes within the cattle genome.
Comparisons of our predictions to known binding site loci within the PCK1, ACTA1 and G6PC promoter regions revealed 75% sensitivity for our method of discovery.
Additionally, we intersected our predictions with known cattle SNP variants in dbSNP and on the Illumina BovineHD 770k and Bos 1 SNP chips, finding 7534, 444 and 346 overlaps, respectively.
Due to our stringent filtering criteria, these results represent high quality predictions of putative TFBSs within the cattle genome.
All binding site predictions are freely available at http://bfgl.
anri.barc.usda.gov/BovineTFBS/ or http://199.133.54.77/BovineTFBS.
Introduction The detection of functional transcription factor binding sites (TFBSs) remains an elusive goal in the post-genome world [1].
Much of the difficulty in TFBS discovery comes from the short length of the sequencing reads as well as their degenerate nature.
Additionally, transcription factors (TFs) can often bind sequences completely dissimilar to their canonical TFBS motif and some TFBSs can be lineage-specific, thereby iu GE).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A confounding comparative evolutionary discovery [2].
Given these difficulties, experimental discovery and annotation re-mains the most reliable method for TFBS discovery; however, such validation is often unavailable for individual TFs.
The proliferation of genome sequencing and assembly has made possible the use of comparative genomics approaches for TFBS discovery.
The reasoning behind the use of comparative genomics as a means of TFBS discovery is that conserved se-quence upstream of a gene is likely to contain essential TFBSs due to selective pressures to conserve the sequence across sev-eral different species [3].
Since TFs typically bind to non-cod-ing regions of the genome, accurate identification of their binding sites could provide important context to the recent in-flux of genetic variation discovery studies that often identify variants outside of coding regions.
In silico prediction methods for the detection of TFBSs can be classified by the order in which they apply sequence homol-ogy among several related species.
The first methodtermed cademy of Sciences and Genetics Society of China.
Production and hosting 196 Genomics Proteomics Bioinformatics 11 (2013) 195198 the alignment-free methoduses motif detection algorithms on unaligned genomic sequence prior to comparisons of sequence homology [4].
By contrast, phylogenetic footprinting uses con-served sequence alignments across several animal species as a starting point for TFBS motif detection [4].
Both techniques are subject to unique benefits and disadvantages based on their starting approaches.
In our previous study, we identified novel TFBSs upstream of the phosphoenolpyruvate carboxykinase (PEPCK or PCK1) promoter and applied a TFBS prediction algorithm to detect TFBSs upstream of all genes available in the human genome [5].
In this study, we applied a phylogenetic footprinting approach using the transcription factor binding site locator (TFLOC) algorithm initially developed to detect conserved TFBSs within multiple genome alignments for the University of California, Santa Cruz (UCSC) genome browser [6].
Implementation TFLOC uses a position-specific scoring matrix (PSSM) algo-rithm to identify putative TFBSs across multiple genome align-ment files through the generation of a similarity matrix score for each putative position [5].
The PSSMs that we used were derived from the JASPAR CORE, FAM and phyloFACTs databases, which contain freely available consensus TFBS scoring matrices that were experimentally determined or statis-tically predicted [7].
We chose the Btau4.0 reference assembly for our analysis for two reasons: (1) it is currently the most extensively-annotated cattle reference assembly and (2) simul-taneous comparative alignments against other mammalian genomes already exist for Btau4.0 (downloaded from: http://hgdownload.cse.ucsc.edu/goldenPath/bosTau4/multiz5way/).
We chose the 1000 bp upstream multiple alignment file (maf) for our analysis as proximal TFBSs tend to be found within 1000 bp of the transcription start site (TSS) of a gene [8].
After downloading the 1000 bp maf from the UCSC genome brow-ser, we removed alignments from Platypus (Ornithorhynchus anatinus) and mouse (Mus musculus) due to their large se-quence divergences from cattle (Bos taurus).
Unfortunately, promoter sequence for similar genes could not be found within the genomes of some animals.
For example, platypus and mouse only shared promoter sequence synteny with cattle 1437 times (1437/8740; 16%) and 7649 times (7649/8740; 88%), respectively, compared to 8440 times (97%) for human (Homo sapiens) and 8165 times (93%) for dog (Canis lupus familiaris).
By focusing on multiple alignments containing se-quences only from cattle, human and dog, we were able to investigate 7764 locations that had homology among three Table 1 Performance of TFLOC predictions at various score thresho T 1 2 3 Known sites 44 44 4 Predicted sites 24 31 3 False negatives 20 13 1 50% Overlapping predictions 24 40 6 Total predictions 93 179 28 Sensitivity (%) 55 70 7 Specificity (%) 25 22 2 species as opposed to only 1335 locations if we included align-ments that contained sequences from all five species.
Application Computational TFBS prediction methods are often marred by high false positive rates (FPRs), so we initially sought to define stringent filters for the algorithm in order to focus on highly-likely TFBS motifs.
Similar to Liu et al.s approach [5], we tested the fit of raw TFLOC prediction scores for all surveyed PSSMs to a Gaussian distribution and found that 176 out of 315 of the motifs (55.9%) had significant deviations from a standard distribution.
We also identified 8 different distribu-tion types for TFLOC prediction scores, similar again to the previous report [5].
For all subsequent predictions, we consid-ered non-Gaussian distributions of TFLOC scores by using fine-tuned filtering values for each PSSM.
The final filter val-ues were derived from an empirical test consisting of compar-isons between well-characterized TFBSs identified within the PCK1, ACTA1 and G6PC promoters and TFLOC predictions.
To estimate the sensitivity and specificity of our predictions, we sought to use these promoter regions with relatively high numbers of coordinate-converted TFBS positions.
If we in-clude promoter regions with fewer characterized TFBS posi-tions, the specificity estimation could be artificially penalized due to a lack of experimental TFBS information rather than a real flaw in our algorithm.
Based on 44 characterized sites upstream of the human PCK1, ACTA1 and G6PC genes that could be converted to Btau4.0 reference coordinates using the liftOver tool [9] as a standard [1013], we measured the overlap of predictions at incremental cutoff values (Table 1).
We defined sensitivity as the number of true positives divided by the number of true positives in addition to the false nega-tives.
Specificity was defined as the percentage of predictions that overlapped known sites by at least 50%, similar to the cri-terion described previously [5].
A cutoff value of 0.04% was chosen for future TFBS predictions as it produced superior sensitivity and specificity while making fewer overall predic-tions than the higher cutoff values (Figure 1).
While it is very likely that some TFBSs with in vivo activity may have been ex-cluded due to this stringent filter, our analysis focused on puta-tive, high percent similarity binding sites that likely have functional significance due to their conservation across species.
We also compared TFBS detection results from our method to results from another phylogenetic footprinting tool, Mul-tiTF [14].
Using the same 1 kb upstream regions from PCK1, ACTA1 and G6PC, we aligned the sequences with the Mulan webtool (http://mulan.dcode.org/) and loaded the alignments lds hreshold (0.01%) MultiTF 4 5 6 4 44 44 44 44 2 33 33 33 29 2 11 11 11 15 6 78 81 84 88 4 359 416 452 215 2 75 75 75 66 3 22 20 19 41 Figure 1 Comparison of known and predicted sites upstream of the PCK1 locus on the Btau4.0 reference assembly The chromosome position (Chr13: 59,379,17959,379,654) is listed at the top of the diagram, with vertical gray bars serving as scale bar markers.
Known PCK1 TFBSs are represented by black bars (previously identified in [11]) and blue bars (identified in [5]) in the top track.
TFBS predictions made by TFLOC using a 3-way alignment of human, dog and cow are depicted in the following three tracks.
Predictions from JASPAR CORE, JASPAR FAM and JASPAR PHYLOFACTS were represented by red, grey and green bars, respectively.
Additional UCSC tracks include gap locations, RefSeq annotated genes, cow mRNAs mapped to the reference genome, and 5-way multiz alignment & conservation.
Bickhart DM and Liu GE/ TFBS Prediction in the Cattle Genome 197 in MultiTF for TFBS detection using the default settings.
Only 29 of the 44 known TFBSs in the three genes were detected by MultiTF (66% sensitivity) compared to the 33 sites identified in our method (75% sensitivity).
Both methods made a similar number of predictions within the analyzed regions (361 predic-tions for MultiTF and 359 for TFLOC).
Therefore, differences in predicted sites may be attributed to the use of different TFBS PSSMs, as our method used the JASPAR databases [7], while MultiTF uses the TRANSFAC database [15].
Although both methods provide high degrees of sensitivity for TFBS detection in promoter regions, TFLOC was able to detect four more experimentally-validated TFBSs at the 0.04% cutoff filter than MultiTF.
Our analysis predicted 379,333 TFBSs upstream of 7764 MGC annotated loci within the Btau4.0 reference assembly.
Many of the placed MGC annotations (683 loci) on Btau4.0 lacked sequence conservation in either dog or in human, so we were unable to predict TFBSs in these regions.
Another portion ofMGCupstreamalignments (293 loci) were removed as the up-stream region fell within gap or repeat regions of the Btau4.0 assembly.
Despite these losses, we were able to predict TFBSs at 80% (7764 out of 9706) of the currently-annotated MGC loci in the Btau4.0 assembly and 89% (7764 out of 8740) of the MGC loci present in the maf alignment.
We then checked for previously-annotated variants that might overlap with our predictions by comparing our TFBS loci with the 9 million plus SNP variant calls within the cattle genome that are present in the dbSNP variant repository (http://www.ncbi.nlm.nih.gov/ projects/SNP/) [16].
Since the variants present in dbSNP have coordinates on the UMD3.1 reference assembly, we used the UCSCs liftOver tool to convert the SNP coordinates to the Btau4.0 assembly (>98% conversion rate).
We identified 7534 TFBS predictions that overlapped with variant SNP loci (Table S1).We also compared our TFBS loci with SNPs present on the Illumina BovineHD 770k and Affymetrix Bos 1 SNP chips and identified 444 and 346 intersections, respectively.
Gi-ven the potential for SNP variants to cause changes within the sequences of TFBSs and theoretically impact the binding affin-ities of TFs, we counted the number of SNPTFBS intersections that were within conserved nucleotides (monomorphic) of the TFBS consensus sequence (Table S1).
We found a high number of SNPTFBS intersections that changed conserved TFBS consensus sequences (5598 in dbSNP; 243 in Bos 1 and 327 in BovineHD 770k).
These SNPTFBS intersections were also identified upstream of 1887 MGC annotated genes.
Several of these overlaps occurred upstream of essential genes, such as the CTCF binding site of HLA-DMA (encoding histocompati-bility antigen, DM alpha chain), the NKX3_1 binding site of LYZ1 (encoding lysozyme) and the FOXF2 binding site of HSP40/DNAJB4 (encoding heat shock protein 40/DnaJ homo-log, subfamily B, member 4).
Conclusion In this study, we identified 379,333 putative transcription fac-tor binding sites (TFBSs) within the promoter regions of 7764 annotated genes in the cattle genome.
Intersections of known 198 Genomics Proteomics Bioinformatics 11 (2013) 195198 SNP sites from dbSNP (5598 sites), the Bos 1 array (243 sites) and the BovineHD 770k array (327 sites) with our predicted TFBSs revealed interesting overlaps.
It is feasible that future GWAS, QTL mapping and whole genome sequencing studies are able to investigate our identified SNPTFBS intersections to link variants within our TFBS predictions to phenotypes.
Currently, our predictions represent high priority regions of interest for future surveys such as RNA-seq, which can tag dif-ferences of expression with animal genotypes.
All TFBS pre-dictions and SNP marker intersections are freely available at http://bfgl.anri.barc.usda.gov/BovineTFBS/ or http://199.133.
54.77/BovineTFBS.
Authors contributions DMB and GEL designed the procedures, carried out the experiments, wrote the draft and corrected the manuscript.
Both authors read and approved the final manuscript.
Competing interests The authors declare no competing interests.
Acknowledgements We thank Reuben Anderson and Alexandre Dimtchev for technical assistance.
GEL was supported by the National Research Institute and the Agricultural and Food Research Initiative (Grant No.
2007-35205-17869 and 2011-67015-30183) from the United States Department of Agriculture Cooperative State Research, Education and Extension Ser-vice (now the National Institute of Food and Agriculture) and Project from the US Department of Agriculture Agri-cultural Research Service (ARS) (Grant No.
1265-31000-098-00).
Mention of trade names or commercial products in this article is solely for the purpose of providing specific infor-mation and does not imply recommendation or endorsement by the US Department of Agriculture.
The US Department of Agriculture is an equal opportunity provider and employer.
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.2012.
10.004.
ABSTRACT Motivation: Systematic analysis of connection between proteins, their cellular function and phenotypic manifestations in disease is a central problem of biological and clinical research.
The solution to this problem requires the development of new approaches to link the rapidly growing dataset of genedisease associations with the many complex and overlapping phenotypes of human disease.
Results: We analyze genetic skin disorders and suggest a manually designed set of elementary phenotypes whose combinations define diseases as points in a multidimensional space, providing a basis for phenotypic disease clustering.
Placing the known gene disease associations in the context of this space reveals new patterns that suggest previously unknown functional links between proteins, signaling pathways and disease phenotypes.
For example, analysis of telangiectasias (spider vein diseases) reveals a previously unrecognized interplay between the TGF-signaling pathway and pentose phosphate pathway.
This interaction may mediate glucose-dependent regulation of TGF-signaling, providing a clue to the known association between angiopathies and diabetes and implying new gene candidates for mutational analysis and drug targeting.
Contact: grishin@chop.swmed.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Rigorous quantitative analysis of disease phenotypes is a key problem on our way to understanding systemic effects of human gene mutations.
Such understanding would enable statistical prediction of clinical manifestations for genome abnormalities, inference of causative genes from complex disease phenotypes, as well as deeper insights into molecular mechanisms of pathophysiology.
This tremendous task requires the development of new approaches to link the rapidly growing dataset of genedisease associations with the many complex and overlapping phenotypes of human disease.
Previously reported approaches to this problem ranged from considering diseases as individual entities connected through shared To whom correspondence should be addressed.
causative genes (Goh et al., 2007) or co-occurrence in the same patient (Rzhetsky et al., 2007), to more detailed classifications involving the comparison of disease phenotypes, usually based on ontologies of phenotypic terms derived from natural-language phenotype descriptions through automated or semi-automated text analysis (Robinson et al., 2008; van Driel et al., 2006).
These analyses may include additional high-throughput data on protein associations (Lage et al., 2007; Wu et al., 2008), improving prediction of new connections between diseases and proteins involved.
Here we suggest a different approach to quantitative gene-phenotype analysis.
By focusing on the set of genetic skin disorders, we are able to manually analyze the corresponding descriptions of phenotypic manifestations and design a set of elementary phenotypic features whose combinations define any given disease as a point in a multidimensional space.
Placing the known genedisease associations in the context of this space reveals new patterns that suggest previously unknown functional links between disease phenotypes, proteins and signaling pathways.
In particular, analysis of telangiectasias (spider vein diseases), reveals a previously unrecognized interplay between the TGF-signaling cascade and pentose phosphate pathway (PPP), which may mediate glucose-dependent regulation of TGF-signaling in diabetes.
2 METHODS The database of 560 genodermatoses, with their phenotypic representations, affected organ systems and associated genes (see Supplementary Material) was previously compiled by expert analysis of OMIM database (Feramisco et al.
2009).
The elementary phenotypic features were manually selected as general clinical manifestations that can occur independently in different diseases and, when combined, can cover phenotype of any included genodermatosis (Feramisco et al., 2009).
Correlation matrix R of elementary phenotypes is calculated based on the set of genodermatoses represented as vectors in the phenotype space: Rij = corr(Pi,Pj), where P = XT is the matrix composed of phenotype vectors Pi that corresponds to the transposed matrix X of disease vectors Xi = {xk}, k = 1,N (N is the dimension of phenotype space).
corr(Pi, Pj) is Pearsons correlation coefficient of two vectors.
Principal components of the set of disease points in the phenotype space are derived as eigenvectors of covariance matrix C = XXT, with largest The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2892 28912896 R.I.Sadreyev et al.
eigenvalues corresponding to the dimensions of strongest correlations within the data set X (Jolliffe, 2002).
Statistical significance of genephenotype associations is estimated from the observed counts of co-occurrence of a given phenotype with mutations in a given gene (Supplementary Tables S1S5).
Fishers exact test is used as a more appropriate alternative to chi square association test in the cases where counts in some cells of contingency tables are low (<5).
The significance level is adjusted for the multiple testing of all genephenotype associations using Bonferroni correction: = 0/n = 1.2106, where 0 = 0.05 and n = n(genes) n(phenotypes).
Phenotypic clustering is performed using UPGMA agglomerative method.
3 RESULTS Using the Online Mendelian Inheritance in Man (OMIM) compendium of human mendelian inheritance (Amberger et al., 2009), we previously compiled a database of genetic skin diseases, their phenotypic representations, affected organ systems and associated genes (Feramisco et al., 2009).
This database (see Supplementary Material) includes 560 diseases associated with 501 protein-coding genes, with 16% of diseases linked to two or more mutated genes and 18% of genes linked to two or more disease entities.
3.1 Phenotypic categorization of genodermatoses Based on the manual analysis of disease phenotypes, we define a minimal set of elementary phenotypic features whose combinations cover all included genodermatoses, so that phenotypic manifestation of each skin disease can be represented as a combination of several elementary features.
This set includes 42 elementary dermatologic features forming 18 groups: cornification phenotypes, pigmentation phenotypes, etc.
and 29 elementary systemic phenotypes forming 17 groups (Feramisco et al., 2009) (see Supplementary Material).
For example, the group of pigmentation features includes hyper-and hypo-pigmentation, caf au lait, poikiloderma and nevi (birthmarks and moles).
Most of the genodermatoses are characterized by more than one elementary phenotype, with the average number of assigned elementary phenotypes being 2.4 per disease (Feramisco et al., 2009).
Distributions of the numbers of dermatologic and systemic phenotypes per disease and the numbers of diseases sharing an elementary phenotype are shown in Supplementary Figure S1.
As a combination of elementary features, each disease can be represented by a point in multi-dimensional space defined by 71 basis vectors corresponding to the features (Fig.1).
To check for the independence of these features in the phenotype space, we calculate their correlation matrix based on the set of all disease points (see Methods section for details).
This matrix (Fig.2, see also Supplementary Material) does not show major correlation patterns in the incidences of different features in composite disease phenotypes.
The highest non-diagonal correlation coefficient is 0.605 for poikiloderma versus alopecia/hypotrichosis phenotypes; the second highest is 0.505.
When the distribution of disease points is analyzed in the plane (Supplementary Fig.S4A) or 3D-space (Supplementary Fig.S4B) of highest-correlated phenotypes, every possible combination of these phenotypes is found in a significant number of diseases.
In addition, we perform principal component analysis (PCA) of the disease set and visualize the distribution of points along principal components.
Elongated or skewed distribution of points Fig.1.
Representation of diseases as points in the phenotype space.
As an example, a 3D space of elementary phenotypic features Ph1Ph3 is shown, with two diseases (D1, D2) defined by the combinations of these features.
The similarity between these composite disease phenotypes is determined by the distance between points D1 and D2.
projections onto the subspace of first two or three principal components, which correspond to the largest correlations within the dataset, would indicate a major correlation in the occurrence of phenotypic features.
Neither the projection on the plane of top two components (Fig.2B) nor the projection in the space of top three components (Supplementary Fig.S2) reveal such correlations.
The corresponding projections for the separate dermatologic and systemic phenotype sets, along with phenotype correlation matrices are shown in Supplementary Figure S3.
These results suggest that our set of phenotypic features is largely independent.
In phenotypic space, the similarity between two diseases can be determined by the distance between the corresponding points (Fig.1).
We use these distances to group diseases by phenotypes.
Several tested distance metrics (Manhattan block, Euclidean distance, etc.)
produce similar results, thus we further use Euclidean distances for simplicity.
The distances range from zero for disorders with the same sets of phenotypic features to 3.9 for the most distant disorders that have 15 elementary phenotype differences.
Based on this disease representation, we analyze (i) phenotypes shared among diseases associated with a protein or a group of proteins (Fig.3A); and (ii) functional links between proteins associated with phenotypically similar diseases (Fig.3B).
This analysis can provide significant insights into molecular mechanisms of pathogenesis.
First, associations between proteins and disease phenotypes can be dissected in a statistically rigorous manner and previously unknown links can be revealed.
Second, similarity in phenotypic manifestation may suggest common mechanisms of action for different proteins or signaling systems, and point to potential functional interactions.
Finally, inferred phenotypic association of a protein group or pathway provides a set of new protein candidates that may cause similar diseases of yet unknown molecular mechanism (Fig.1).
Our approach is different from previously reported approaches to the representation of disease phenotypes (Lage et al., 2007; Robinson et al., 2008; van Driel et al., 2006) in two essential points.
First, we base our analysis on the data of high quality, with the phenotype descriptions being manually curated by an expert.
Second, as opposed to building ontology of all phenotypic terms 2892 [12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2893 28912896 Phenotypic categorization of genetic skin diseases Fig.2.
Selected elementary phenotypic features are largely independent.
(A) Correlation matrix of phenotypic features, based on the whole dataset of 560 disease phenotypes.
The matrix is shown as a grayscale grid, with each square representing the absolute value of Pearsons correlation coefficient between two phenotypic features (see Methods).
The values are in the range between 0.0 (white) and 1.0 (black).
The matrix is symmetric, with ones on the diagonal.
(B) Projection of the disease set on the plane of first two principal components in phenotypic space does not show any general correlations in the occurrence of elementary phenotypes among the diseases.
Fig.3.
(A) Inference of proteinphenotype associations.
Decomposing disease phenotypes into elementary features allows for statistical analysis of co-occurrence between mutations in specific proteins and resulting elementary phenotypes.
Significant correlations between defects of a protein (filled circle) and a manifested phenotypic feature (filled square) may suggest causation.
(B) Inference of protein and pathway associations.
Diseases are clustered by phenotypic presentation (upper plane) and corresponding groups of disease-associated proteins are considered (lower plane).
Left, when a cluster of diseases corresponds to mutations in functionally related proteins (e.g.
proteins from the same signaling cascade), other proteins of this functional group (open circles) may be suggested as new potential candidates for the association with the same class of diseases.
Right, new relations between different protein groups may be inferred from their mapping to the same phenotypic cluster of related diseases.
and automatically tracing their relationships through shared parents, we are able to decompose complex phenotypes into elementary unrelated features that can be combined in an independent fashion.
3.2 Proteinphenotype associations Decomposition of disease phenotypes into elementary features often reveals that an individual protein or a group of proteins is predominantly associated with a certain phenotypic feature (Fig.3A).
The statistical significance of such associations can be estimated from the frequencies of co-occurrence of mutations in a specific protein with an elementary phenotype.
Our results confirm many previously known associations.
For example, mutations in keratin I and collagens I and VII are significantly associated with disease phenotypes of hyperkeratosis (Supplementary Table S1, Fishers exact test P-value: P = 1.1106), atrophy/aplasia/fragile skin (Supplementary Table S2, P = 9.2108) and bullous epidermal cohesion (blistering phenotype, Supplementary Table S3, P = 1.5109), respectively.
In addition, our results suggest previously unknown associations.
For example, the link between mutations in the subunits of laminin 5(332) and mucosal phenotype group (Supplementary Table S4, P = 7.2107), has been detected.
As a major component of the basement membrane, laminin 5(332) is an essential structural component of the dermalepidermal junction and is involved in cell adhesion and signal transduction.
Our analysis suggests that various defects in laminin 5(332) consistently affect the integrity of mucous membranes.
Extending our analysis to groups of functionally related proteins, we find statistically significant 2893 [12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2894 28912896 R.I.Sadreyev et al.
Fig.4.
Phenotypic clustering of diseases sharing the phenotype of telangiectasia (spider veins) corresponds to a clear functional grouping of associated proteins and suggests the relation between TGF-signaling cascade and pentose phosphate metabolic pathway.
The cladogram of agglomerate hierarchical clustering is shown, the nodes marked by the numerical code of the disease (117) with the name of the associated protein in parentheses.
Filled circles, proteins associated with PPP.
Filled squares, proteins involved in TGF-pathway.
Open triangles, proteins involved in DNA excision repair.
Open circles, other proteins.
The following diseases are shown: 1: arterial tortuosity syndrome; 2: transaldolase deficiency; 3: Osler-Rendu-Weber syndrome 2; 4: telangiectasia, hereditary hemorrhagic, of Rendu, Osler and Weber; 5: juvenile polyposis/hereditary hemorrhagic telangiectasia syndrome; 6: UV-sensitive syndrome; 7: xeroderma pigmentosum with normal DNA repair rates; 8: xeroderma pigmentosum, complementation group E; 9: xeroderma pigmentosum, complementation group D; 10: De Sanctis-Cacchione syndrome; 11: osteogenesis imperfecta, type IV; 12: hypotrichosis-lymphedema-telangiectasia syndrome; 13: Kindler syndrome; 14: ataxia-telangiectasia; 15: Bloom syndrome; 16: congenital disorder of glycosylation, type Ie; 17: Rothmund-Thomson syndrome.
associations for gap junction proteins, connexins, which show a highly significant connection to hyperkeratosis (Supplementary Table S5, P = 1.7109).
Unsurprisingly, most of the elementary phenotypic features are associated with a wide range of protein functions.
However, some features are associated with more functionally uniform groups of proteins.
For example, bullous (blistering) phenotype is mainly caused by mutations in structural proteins (collagens, keratins), filament-associated proteins, or cell adhesion molecules.
3.3 Functional associations between proteins through their phenotypic representation Similarity of disease phenotypes caused by mutations of different proteins may point to functional relationship between these proteins.
In addition to many known protein relations, our dataset allows for the inference of previously unknown functional links.
As an example, Figure 4 shows phenotypic clustering of telangiectasias (spider vein diseases) that suggests an association between the TGF-cascade and the PPP.
The cladogram based on phenotypic composition reveals two major groups of similar, tightly clustered diseases.
The first group is associated exclusively with the proteins of DNA excision repair (ERCC2, ERCC6, DDB2, POLH), whereas the second group is associated with proteins that are connected to two distinct systems: TGF-cascade (SMAD4, ENG, ACVR1) and PPP (transaldolase TALDO1 and glucose transporter GLUT10).
Involvement of TGF-in vascular anomalies has been reported for several diseases (Coucke et al., 2006; Loeys et al., 2005; Tille and Pepper, 2004).
The detected phenotypical similarity to the PPP-associated disorders suggests that PPP is implicated in angiogenesis through the same pathophysiologic mechanism.
In particular, we hypothesize a cross-talk between PPP and TGF-cascade, which may mediate the connection between glucose metabolism and abnormal vascular development in angiopathies.
Among other implications, this hypothesis provides a potential explanation of the known link between diabetes and angiopathies (Miles et al., 2007; Simo et al., 2006), as well as suggests new potential angiopathy-linked proteins.
4 DISCUSSION The role of proteins and their interactions in the living organism is a central focus of molecular and cellular biology, with both fundamental and clinical implications.
In this respect, the catalogued links between protein mutations and their phenotypic manifestations in disease are, in a sense, the results of a grand mutagenesis experiment that may prove useful for finding new associations and generating new hypotheses.
Here, we present an analysis of phenotypes expressed in genetic skin disorders and the corresponding causative genes.
Although our method involved initial manual curation of elementary phenotypes, it can be readily generalized to other disease sets with available phenotype descriptions.
The construction of the set of elementary phenotypes can start from all phenotypic terms derived from textual annotations of diseases of interest, further filtered by manual expert analysis and/or numerical selection of the maximal subset of independent phenotypic features, based, for example, on the analysis of correlation matrix (Fig.2A) or on PCA.
However, we believe that the set of phenotypes manually curated by an expert provides a more informative categorization, since it (i) involves additional knowledge not reflected in the brief disease annotations; and (ii) is based on well-defined clinical terms and thus is more accessible and relevant for clinical research community.
As an example, analysis of such a dataset can link genes to specific clinical manifestations (Fig.3A) that are easily recognized by medical practitioners, which facilitates data accumulation and hypothesis testing.
Involvement of a protein in the development of a specific disease phenotype is an important piece of functional information, which may lead to (i) better understanding of proteins function and molecular mechanism of disease; (ii) hypotheses about phenotypic effects of mutations in functionally related proteins; and (iii) phenotype-based prediction of potential proteins involved in similar diseases with unknown genetic causation.
The presented proteinphenotype associations, inferred from a relatively restricted statistical sample of OMIM, can be validated on larger datasets.
This validation may be performed in at least two directions.
First, phenotypic effects of mutations in a specific gene can be analyzed on a wider scale: most directly, in larger 2894 [12:28 22/10/2009 Bioinformatics-btp538.tex] Page: 2895 28912896 Phenotypic categorization of genetic skin diseases clinical studies or in animal models.
Second, for a considerable set of characterized skin diseases, causative genes are still unknown.
With many of these diseases included in OMIM, an interesting further direction would be to test the patients for the mutations in the genes that have statistical associations with the manifested phenotypes.
Similarity in phenotypic effects of different proteins may lead to hypotheses about previously unknown functional associations (Fig.3B).
We present a potentially interesting example of such an association, the interaction between PPP and TGF-pathway suggested by phenotypic clustering of telangiectasias (Fig.4).
Utilizing glucose-6-phosphate as the substrate, PPP produces the major fraction of cells NADPH, which plays a central role in maintaining intracellular redox potential by serving as a co-factor in the reduction of glutathione (Berg et al., 2001).
As an example, PPP enzyme glucose-6-phosphate dehydrogenase is shown to play an important role in oxidative stress (Leopold et al., 2003; Park et al., 2006), which is thought to be the main mechanism of its involvement in cardiovascular disease (Matsui et al., 2005; Rajasekaran et al., 2007; Wiesenfeld et al., 1970).
Changing the concentration of reduced glutathione alters redox state and affects, among other systems, TGF-pathway (Maulik and Das, 2002; Shan et al., 1994).
Altered redox state and the resulting oxidative stress are known to stimulate angiogenic response (Maulik and Das, 2002; Ushio-Fukai, 2006), and are shown to be involved in at least one disorder with telangiectasia phenotype (Nicotera et al., 1989).
In addition, the effect of GLUT10 deficiency on TGF-signaling in the arterial wall is shown in arterial tortuosity syndrome (Coucke et al., 2006).
We hypothesize that (i) defects in PPP may cause abnormal vascular development by altering intracellular redox state; and (ii) this effect may be mediated by TGF-signaling cascade.
This hypothesis has several important implications.
First, the reported effects of glucose concentration on TGF-cascade (Hua et al., 2003; Isono et al., 2000; Zhu et al., 2007) may be mediated by PPP.
Furthermore, our hypothesis may explain the observed connection between microangiopathies and diabetes (Miles et al., 2007; Simo et al., 2006), suggesting that abnormal angiogenesis may be caused by changes in PPP activity due to the disruption of intracellular glucose homeostasis.
Finally, our hypothesis suggests that other proteins of PPP, as well as of TGF-pathway, may be associated with angiopathies, providing a new set of potential candidates for mutational analysis and drug targeting.
This hypothesis can be readily tested in various experiments, including (i) analysis of redox-state and TGF-beta responses to different levels of glucose concentration in the patients with telangiectasia phenotype carrying TALDO1 and GLUT10 mutations, or in corresponding animal models; (ii) analysis of these responses, as well as general phenotypic effects caused by mutations in other PPP proteins (in animal models or clinical studies); (iii) analysis of TGF-response to expression changes or up-and down-regulation of PPP proteins in vivo and in culture; (iv) sequencing genes of PPP and TGF-pathway in patients with telangiectasias of unknown genetic background; and (v) further experimental investigation of the role of PPP activity in the association between angiopathies and diabetes, in diabetes patients and animal models.
In conclusion, our manually curated decomposition of disease phenotypes is based on the set of elementary phenotypic features that serve as basis vectors in a multidimensional space, as opposed to previously reported automated (Lage et al., 2007; van Driel et al., 2006) or semi-automated (Robinson et al., 2008) ontologies of phenotypic terms.
The potential value of this approach is shown by confirming known and revealing previously unknown genephenotype, genegene and pathwaypathway associations.
ACKNOWLEDGEMENTS The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing high-performance computing resources.
Funding: National Institutes of Health (GM67165 to N.V.G.
); Welch Foundation (I1505 to N.V.G.
); American Cancer Society (to H.T.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Functional genomics research has expanded enormously in the last decade thanks to the cost reduction in high-throughput technologies and the development of computational tools that generate, standardize and share information on gene and protein function such as the Gene Ontology (GO).
Nevertheless, many biologists, especially working with non-model organisms, still suffer from non-existing or low-coverage functional annotation, or simply struggle retrieving, summarizing and querying these data.
Results: The Blast2GO Functional Annotation Repository (B2G-FAR) is a bioinformatics resource envisaged to provide functional information for otherwise uncharacterized sequence data and offers data mining tools to analyze a larger repertoire of species than currently available.
This new annotation resource has been created by applying the Blast2GO functional annotation engine in a strongly high-throughput manner to the entire space of public available sequences.
The resulting repository contains GO term predictions for over 13.2 million non-redundant protein sequences based on BLAST search alignments from the SIMAP database.
We generated GO annotation for approximately 150 000 different taxa making available 2000 species with the highest coverage through B2G-FAR.
A second section within B2G-FAR holds functional annotations for 17 non-model organism Affymetrix GeneChips.
Conclusions: B2G-FAR provides easy access to exhaustive functional annotation for 2000 species offering a good balance between quality and quantity, thereby supporting functional genomics research especially in the case of non-model organisms.
Availability: The annotation resource is available atContact: aconesa@cipf.es; sgoetz@cipf.es Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 25, 2010; revised on January 4, 2011; accepted on February 1, 2011 1 INTRODUCTION Functional genomics research has gained importance in the last decade thanks to the fast improvement and cost reduction in To whom correspondence should be addressed.
high-throughput technologies.
Beyond traditional model species, also non-model organisms are in the genomics race and today it is hard to find a biological domain without a functional genomics initiative.
This expansion would not have been that successful without the accompanying development of computational tools that generate, standardize and share information on gene and protein function.
The Gene Ontology (GO) project is one such standard.
GO is a collaborative effort aiming at the establishment of a controlled vocabulary that provides biologically meaningful annotations for gene (products) across species (Ashburner et al., 2000).
There are two main aspects of this project: (a) the definition of a comprehensive ontology of functional terms and (b) the generation of an annotation database containing the assignment of GO terms to genes and proteins (The Gene Ontology Consortium, 2008).
Annotation for each organism in the GO database is supplied and maintained by the respective consortium member.
Evidence codes (ECs) are added to each individual annotation to reflect the information source used in a GO term assignment.
ECs indicate if the annotation is supported by some (and which) experimental evidence, whether it was transferred (and how) from related genes or if it was generated by other prediction methods.
The large majority of GO annotations is centralized in the Gene Ontology Annotation (GOA) Database (Camon et al., 2004).
This resource contains high-quality functional annotations for proteins mostly obtained from the UniProt Knowledgebase (The Uniprot Consortium, 2007).
However, the great majority (95%) of GO corresponds to automatically transferred annotations based on InterProScan (Quevillon et al., 2005) results.
Currently, only a small number of assignments have experimental evidence or are revised by expert curators.
While the GO project is improving the ontology definition and quality of the GOA database (Barrell et al., 2009), it is still far from providing extensive functional annotation for the wealth of sequence data that populate public databases.
However, thanks to the structured and universal nature of GO, large-scale annotation using an automated process is conceivable and could be feasible given that adequate computational resources are available.
Such an annotation effort complement current established annotation initiatives by generating preliminary functional labels for sequences not yet covered by any of the reference projects of the GO consortium.
Examples of functional data-intensive resources in the field of functional genomics such as the Integr8 (Kersey et al., 2005) and PEDANT (Riley, 1993) The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:17 17/3/2011 Bioinformatics-btr059.tex] Page: 920 919924 S.Gtz et al.
projects, which offer genome-oriented views on different types of annotation data, can be found.
An example of a specialized resource in the area of functional plant genomics is the PlexDB (Wise et al., 2007), a project to support transcriptomics studies on plants and plant pathogens.
Although these resources provide comprehensive information on many genomic aspects, they do not represent a universal resource where up-to-date large-scale functional data on species can be readily accessed and analyzed.
The Blast2GO project started in 2005 to build an universal bioinformatics platform for the functional annotation and analysis of novel sequence data (Conesa et al., 2005).
The tool was designed to provide high-throughput and quality assignments with a user interface strongly based on visualization and intuitive use (Gtz et al., 2008) (Conesa and Gtz, 2008).
So far, Blast2GO has been used in nearly 300 functional profiling projects involving a wide range of model and non-model species, from bacteria and fungi, through arthropods and mollusks, to plants, avian, fish and mammals, which evidences the strong need for functional annotations beyond that provided by public sequence databases.
During the last 5 years of Blast2GO, we have witnessed many cases of redundant annotation of public data by different labs and limitations for keeping results updated.
These facts, in combination with the annotation gaps mentioned above, have encouraged us to consider the generation of an extensive repository of automatic functional annotation making use of the widely accepted functional annotation engine Blast2GO.
Lastly advances in supercomputing and distributed computing have paved the way for undertaking highly intensive genome analysis tasks as demonstrated by many examples in different research fields such as proteomics (Marti-Renom et al., 2007), phylogenomics (Huerta-Cepas et al., 2007; Sjlander, 2004) and sequence analysis (Rattei et al., 2008).
In this work, we have made use of these technologies to create the Functional Annotation Repository (B2G-FAR), a web-based large-scale resource of precomputed functional annotations.
Generated by pipelines of intensive computing, B2G-FAR is a centralized repository of automatically annotated sequence data, which can directly be downloaded and used, saving processing time and avoiding redundant processing.
The resource covers all biological kingdoms and is structured in such a way that easy download and processing of data for a species of interest is possible.
The B2G-FAR follows the philosophy of Blast2GO development: (i) high throughput: over 25 million sequences have been processed and 2000 species are now present in the B2G-FAR database; (ii) the application to functional genomics projects: the repository contains GO term predictions for both, gene products and probe set collections of Affymetrix GeneChips; and (iii) user friendly: annotations are provided in the form of plain text files for direct download.
The main benefit is obtained in the case of non-model species where functional information and bioinformatics resources are limited.
2 METHODS 2.1 Construction The data contained in the B2G-FAR repository consists of two parts: species-centered functional annotations obtained through the Simap2GO project and a collection of microarray probe set annotations.
Although generated by different pipelines, both data sources implement the same basic annotation scheme as established by Blast2GO (Gtz et al., 2008): (i) sequence similarity search (BLAST) (Altschul et al., 1990) to obtain a list of potential Fig.1.
The B2G-FAR annotation pipeline.
The scheme shows how the different data-sources are related and contribute to the generation of the B2G-FAR annotations, all passing through the Blast2GO annotation algorithm.
homologues;, (ii) mapping of the corresponding GO terms and their evidence codes for all BLAST hits; and (iii) the annotation step applying the Blast2GO annotation rule taking into account GO evidence codes, the GO hierarchy and the degree of sequence similarity (an extensive explanation of the Blast2GO annotation rule can be found in the Supplementary Material).
In a final step, protein domains, families and motifs are identified through InterProScan and the corresponding GO terms are merged with the already transferred annotations taking care of redundancies and GO hierarchy.
Simap2GO data and processing: Simap2GO is the result of the collaboration between the SIMAP (Rattei et al., 2008) and Blast2GO projects.
SIMAP is a database first published in 2005 (Arnold et al., 2005), which provides an all-against-all sequence similarity matrix containing precalculated pairwise comparisons of almost all publicly available proteins.
Computations are accomplished by volunteer grid computing based on the Berkeley Open Infrastructure for Network Computing (BOINC).
By May 2010, SIMAP contained over 29 million non-redundant sequences covering the content of all major public sequence resources.
The Simap2GO project, with the ambitious goal of functionally annotating this comprehensive sequence space, adopted and applied successfully the Blast2GO annotation methodology to the SIMAP resource.
Blast2GO is primarily based on sequence similarity and therefore perfectly suited to be run on the SIMAP database.
The computation of the annotations is run in parallel for the entire sequence space.
For each uncharacterized sequence, GO annotated (only non-electronic annotations) homologues are retrieved from the SIMAP sequence similarity matrix.
The applied Blast2GO method selects the GO terms above a given confidence threshold according to the established annotation rule.
Here, the algorithm takes into account the actual sequence similarity, the original GO evidence code and the location of the potentially transferred GO term within the hierarchy (see the Supplementary Material for a more detailed description of the Blast2GO annotation strategy and annotation parameters).
Apart from the annotation transfer through potential homologues, Simap2GO makes use of the InterPro domain information also provided in a precalculated manner by SIMAP.
This information is merged with the already predicted assignments to improve and increase the GO term assignment.
The sequence annotations generated in this way are stored in the B2G-FAR database and the routine continues with the next unprocessed sequence until completion (see Fig.1).
One entire annotation process takes approximately 3 days with 10 CPUs running in parallel and with direct access to the SIMAP database.
The annotation of the same amount of sequence data 920 [13:17 17/3/2011 Bioinformatics-btr059.tex] Page: 921 919924 B2G-FAR without precalculated alignments would have taken over half a year on a 150 CPU cluster.
All annotations are further processed to summarize and present species-centered information online.
Available charts and data files are given in the Supplementary Table S1.
Micorarray probe set data: the probe-set collection of 17 Affymetrix GeneChip designs corresponding to non-model species was annotated with Blast2GO.
As GeneChip probe sets do not necessarily target protein sequences available in public databases, their functional annotation cannot be recovered by Simap2GO and therefore has been computed using local resources.
FASTA files containing the target sequences of the probe sets were downloaded from the official Affymetrix web site.
The annotation pipeline started by splitting source FASTA files into smaller chunks and launching them against a distributed BLAST setup.
A 150 CPU cluster at the CIPF Bioinformatics and Genomics Department was used to run BLAST searches against the NCBI non-redundant (NR) database.
Simultaneously, protein domain information was obtained through a local installation of InterProScan (Quevillon et al., 2005).
Once BLAST and InterProScan searches were completed, results for every species were gathered and processed within Blast2GO for automatic function prediction.
Charts were generated during the annotation process and are provided online in Supplementary Table S3.
Finally, to assess the coverage of annotation results, each GeneChip was compared with the GO information provided by Affymetrix.
All currently annotated and available datasets are listed in the Supplementary Table S2.
2.2 Contents B2G-FAR presents contents in a user-friendly data sheet concept based on Wiki technology.
All the given information (annotations, data files, images) is generated beforehand by the B2G-FAR annotation pipeline and is summarized on automatically generated web pages.
This facilitates fast access to data files, images and charts describing genome-wide information.
Annotation data can be further visualized and analyzed through its upload into the Blast2GO application (see below: Download and query options).
B2G-FAR is periodically updated every 6 months.
Species annotations: by applying the above-described steps, we could assign GO terms to 14 million sequences that represents 56.4% of the entire SIMAP database (excluding metagenomic data).
The remaining sequences are entries without significant alignments (35.7%) or that did not surpass the annotations quality threshold (7.7%).
Sequences from 150 000 taxa were functionally annotated and the 2000 most represented species are now available through B2G-FAR.
Table 1 contains the numbers of annotated sequences compared with the whole SIMAP dataset and the available source annotations by the GO.
Species can be accessed directly by their scientific name or NCBI taxa ID through a search function.
For every species, several precalculated files and statistical charts are available.
These include a GO annotation flat file and its corresponding GO-Slim version.
Statistical charts provide information about GO annotation distributions, GO level distributions or about the most abundant functional terms within one of the three GO categories.
Microarray annotations: This section is organized as annotation sheets for each probe-set collection corresponding to the 17 non-model Affymetrix GeneChips.
Model species Affymetrix chips were purposely not included in the repository as there already exist extensive functional annotation projects.
The annotation sheet contains detailed information on the Blast2GO annotation process from the BLAST step up to the augmentation by ANNEX [a data mining procedure to annotate from links between molecular function and biological process/cellular component GO terms (Myhre et al., 2006)] and InterProScan.
In contrast to the previous section, which provides only final annotation records, the microarray probe-set annotation sheets include a great variety of descriptive charts that offer a comprehensive view of the functional information contents gathered throughout the annotation pipeline.
Likewise, Blast2GO project files are provided.
The charts and files included in the annotation sheets are listed in the Supplementary Table S3.
Table 1.
Simap2GO annotation coverage: the table shows the number of Blast2GO-annotated sequences in relation to the whole SIMAP dataset (May 2010) and the number of GO sequences which has been used as annotation source/reference dataset Data source Unique sequences Whole Simap 29 906 548 Simap without metagenomes 25 099 929 Simap protein sequences annotated by Blast2GO 14 175 984 Sequences which do not surpass the annotation threshold 1 938 862 Sequences without sequence alignment 8 985 083 GO annotation source sequences (only sequences with non-electronic annotations) 465 677 Only sequences with at least one non-electronic annotations (non-IEA) were used (GO-Lite data-set).
Additionally, the number of sequences which could not be annotated is given, i.e.
sequences without sequence alignments and sequences whose annotations did not surpass the annotation threshold.
Download and query options: in both Species and Microarray sections, final annotation files are provided in plaintext format as GO and GO-Slim data.
The text file format allows direct upload into the Blast2GO application for further analysis of annotation results as well as integration in other applications accepting GO annotation data.
Additionally, all species annotations are available in the standard GO annotation format.
Some descriptive charts are included in B2G-FAR for a quick overview of the results.
Dynamic access to the data is provided by the Blast2GO Java application.
This guarantees optimal reutilization and synchrony within Blast2GO developments.
For example, new query options have been incorporated into Blast2GO to support diverse access to B2G-FAR data (see online tutorial available as Supplementary Material).
Annotated sequences can be queried and filtered by their name/id, description, GO code and GO name, either as exact or contains matches.
Existing Blast2GO functions such as the generation of summary charts, single or combined graphs, annotation pies and enrichment analysis can be performed for the sequences selected by the user.
Moreover, the .annot files from B2G-FAR are fully compatible with the Babelomics suite (Al-Shahrour et al., 2008; http://www.babelomics.org) for functional profiling analysis, where additional statistical methods for pathway analysis [FatiGO (Al-Shahrour et al., 2004) and FatiScan (Al-Shahrour et al., 2007)] are available.
This is especially interesting in the case of microarray probe files or when a functional enrichment needs to be assessed with experimental data involving any of the non-model species included in the repository.
Comparison of B2G-FAR annotations with GO annotations: the quality of the Blast2GO annotation method has been extensively assessed and proved in previous works (Conesa and Gtz, 2008; Conesa et al., 2005; Gtz et al., 2008).
However, we performed an additional evaluation of the annotation process to provide B2G-FAR users with a general feeling of the performance and nature of the annotations contained in the repository.
We selected 10 000 random sequences from B2G-FAR which were also present in the GO database and compared their annotations.
We recorded the number of exact GO term matches, more specific or more general terms (different specificity levels of the annotation), other branch or other GO category (true novel annotations) as described previously (Gtz et al., 2008).
Results are given in Table 2.
The comparison study revealed that most of the original GO annotations (93.5%) were contained in the B2G-FAR repository as exact matches and more specific/general terms and only a small fraction (6.5%) were lost (other GO branch and category annotations) during the annotation process, presumably due to GO version differences or the removal of root category terms in the B2G-FAR repository.
When comparing in the opposite direction, we observed that 49% of the B2F-FAR annotations were represented as exact matches in the GOA, and an additional 13% of terms are provided as more specific concepts.
The remaining 38% 921 [13:17 17/3/2011 Bioinformatics-btr059.tex] Page: 922 919924 S.Gtz et al.
Table 2.
Functional annotation of 10 000 random sequences from the GO and B2G-FAR compared against each other (annotation score 70, evalue 1E10, GOw=5, 5 BLAST hits) Compared GO versus FAR FAR versus GO Compared terms 46 414 (GO) 61 176 (B2G-FAR) Exact GO term match 29 446 29 446 More specific GO terms 510 7960 More general GO terms 13 457 156 Other GO branch 1126 16 193 Other GO category 1875 7421 Comparisons are given as reference database versus comparing database, and numbers refer to the reference database.
are terms in other branches and in other main GO categories.
To have an impression on the nature of these novel B2G-FAR annotations, we checked manually 20 randomly selected sequences for which differences between the two databases were found (see manual_evaluation.xls in Supplementary Material).
Curation of the novel GO terms implied contrasting against scientific papers and established functional databases, such as UniProt, Tair, Saccharomyces Genome Database, Entrez, etc.
From these 20 sequences one (AT5G35370.1) resulted to have doubtful sequence identity and was not considered in further computations.
The remaining 19 sequences accounted for 109 novel GO terms, 9 of which could not be verified from the available literature.
One sequence (Cyclin CLB2 of S.cerevisiae) obtained 4 presumably false GO functions due to sequence similarity to a paralogue with different functional specification.
The remaining 96 GO terms (88%) were confirmed from literature data and assessed as valid annotations.
These results evidence the quality of the GO term assignments contained in B2G-FAR.
3 UTILITY We illustrate the utility of the B2G-FAR on two examples of functional genomics studies taken from the literature and show how B2G-FAR can speed up or facilitate new data analyses.
The first example is in the field of next-generation sequencing (NGS).
These methods are rapidly extending within the genomics community as they greatly outperform both in sensitivity and accuracy hybridization-based approaches.
B2G-FAR can support functional assessment in NGS research.
In a pioneering study, Holt and colleagues analyzed genome variation and evolution in Salmonella typhi using NGS (Holt et al., 2008).
The authors applied 454 and Solexa technologies to resequence 19 different S.typhi strains and isolates.
The authors carried out a phylogenetic analysis of SNPs variance and identified genome insertions, deletions and modified genes across strains.
However, although the impact of genomic changes on certain coding regions was discussed, no genome-wide functional analysis of strain variations was attempted.
By typing S.typhi on the B2G-FAR species search box, we can readily locate the annotation file for this species, which contains GO assignments for 3917 genes (Supplementary Fig.S1).
GeneBank IDs included in the annotation file provide the means for matching functional and genomic variation data.
This annotation file can be opened with the Blast2GO software and by uploading each list of strain-specific varying genes, Blast2GO functions can be used to interrogate data for significant functional differences between isolates at the genome level, and to obtain the functional profiling of the genomic alterations or to locate mutated genes in metabolic pathways.
This example illustrates how readily available functional data can complement the analysis of experimental results with little additional effort.
The second example relates to the use of Affymetrix probe-set annotation data available at the repository.
B2G-FAR offers an annotation coverage which is substantially higher than the NetAffx GO annotations provided by the manufacturer and also has fast and reliable access to functional data for these GeneChips.
The study by Espinoza et al.
(2007) can serve as an illustrative example for this section of the repository.
The article presents a transcriptomics analysis of viral infection in wine grape cultivars using the Affymetrix Grape GeneChip.
In this study, authors generated functional annotations for up-and downregulated gene groups through similarity-based function transfer from Arabidopsis thaliana by WU-BLAST, GO terms being directly transferred for all retrieved alignments.
The obtained annotation was summarized to reflect the abundance of distinct functional classes within regulated genes.
Although valid, this basic functional description does not allow the identification of those functional categories which are specifically activated at viral infection.
For this, a functional comparison to the whole genome represented in the array would be required, which implies that functional data for all probes would be needed.
This information, absent in the article and presumably costly for the authors to obtain, is readily available from the B2G-FAR site.
The B2G-FAR annotation file for the Grape GeneChip contains 54 841 GO terms and covers 11 971 probe sets.
The list of differentially expressed genes provided in the article as Supplementary Material was used in Blast2GO to perform a GO term enrichment analysis based on the B2G-FAR annotation file.
The analysis indicated a significant overrepresentation of chloroplast genes in the Camnre downregulated gene set (adjusted P-value: 1.2105) (see Supplementary Fig.S2a) and only a slight enrichment of membrane, L-arginine and L-glutamate import and other membrane transport activities (P-values: 6103) for the upregulated gene set (see Supplementary Fig.S2b).
4 DISCUSSION The major purpose of B2G-FAR is to offer biologists easy access to functional information.
B2G-FAR has been conceived as a repository of automatic annotations generated by Blast2GO using high-throughput computing technologies to save annotation time to the functional genomics community.
The B2G-FAR is species centered, which means that data can readily be obtained for any of the 2000 organisms present in the database.
The Blast2GO annotation strategy has shown to render good recall values for sequence similarity function transfer methods and to match functional assignments by curated computational analysis (Gtz et al., 2008).
The B2G-FAR retains these quality levels: we showed that the majority of B2G-FAR assignments are identical or functionally related to GO Database annotations for sequences present in this database and, additionally, novel predictions are generally supported by the available literature.
It should be stressed, however, that the quality of B2G-FAR is closely linked to the completeness and accuracy of the GO and InterPro databases.
B2G-FAR complements the GO effort by offering high-throughput automatic annotations on a species basis.
Compared with GOA, where automatically generated annotations are to a big extent based 922 [13:17 17/3/2011 Bioinformatics-btr059.tex] Page: 923 919924 B2G-FAR Fig.2.
Comparison between NetAffx and Blast2GO generated annotations for GeneChips contained in B2G-FAR.
on protein domains, B2G-FAR combines both sequence similarity-based annotations through Blast2GO together with domain-based information through InterProScan.
In this way, GO term assignments could be increased in number and the amount of available annotated sequences could be nearly doubled.
Comparing the generated Affymetrix GeneChip annotations to the current GO annotation available at the NetAffx site, the B2G-FAR resource increased the coverage of functional annotations from an average of 7.89% (NetAffx) to 40.89% (Blast2GO) (Fig.2).
Only the Bovine and Chicken NetAffx annotations were richer than the ones generated by Blast2GO due to intensive proteome annotation efforts of GOA in collaboration with the International Protein Index (Barrell et al., 2009).
Currently, most of the GeneChips of non-model species processed in B2G-FAR contain sufficient annotation coverage for a successful evaluation of microarray results in terms of pathways and biological functions.
Moreover, the compatibility of B2G-FAR file formats with functional profiling tools make functional assessment methods readily accessible for a much larger diversity of organisms.
Finally, B2G-FAR should not be understood as a competitive annotation source to annotation projects as carried out within the GO consortium, nor as a replacement to high-quality manual annotation of single-gene products, but as a complementing resource.
Although automated annotation is by nature more error prone than manually curated one, B2G-FAR offers novel valuable information, making functional data accessible to a large users community working on different species.
5 CONCLUSIONS B2G-FAR provides easy access to exhaustive functional information for a broad range of species encompassing most organisms under genome investigation.
The repository is simple in architecture and still offers many analysis possibilities through the proximity to the Blast2GO software.
In its current form, the resource is species centric.
Future developments will consider multispecies scenarios such as metagenomics data or comparisons across taxa.
6 AVAILABILITY AND REQUIREMENTS The annotation resource is freely available at http://b2gfar.bioinfo.cipf.es, is based on the DokuWiki framework and works with any common web browser.
There are no other requirements or plugins needed to use the repository.
Data files can be downloaded and unzipped or directly uploaded into the Blast2GO application through Java WebStart technology.
Therefore, Java has to be installed.
For both, B2G-FAR and Blast2GO, tutorials and quick-start sections are provided online.
Funding: Spanish Ministry of Science and Innovation (MICINN) (grants BIO2008-04638-E, BIO2008-05266-E, BIO2008-04212, BIO2009-10799 and CEN-20081002) and the PlanE Program; GVA-FEDER (PROMETEO/2010/001); Red Tematica de Investigacion Cooperativa en Cancer (RTICC), ISCIII, MICINN (grant RD06/0020/1019, in part).
Further financial support was granted by the European Science Foundation (ESF) with the activity entitled Frontiers of Functional Genomics.
Conflict of Interest: none declared.
ABSTRACT Motivation: Species trees provide insight into basic biology, including the mechanisms of evolution and how it modifies biomolecular function and structure, biodiversity and co-evolution between genes and species.
Yet, gene trees often differ from species trees, creating challenges to species tree estimation.
One of the most frequent causes for conflicting topologies between gene trees and species trees is incomplete lineage sorting (ILS), which is modelled by the multi-species coalescent.
While many methods have been developed to estimate species trees from multiple genes, some which have stat-istical guarantees under the multi-species coalescent model, existing methods are too computationally intensive for use with genome-scale analyses or have been shown to have poor accuracy under some realistic conditions.
Results: We present ASTRAL, a fast method for estimating species trees from multiple genes.
ASTRAL is statistically consistent, can run on datasets with thousands of genes and has outstanding accuracy improving on MP-EST and the population tree from BUCKy, two statistically consistent leading coalescent-based methods.
ASTRAL is often more accurate than concatenation using maximum likelihood, except when ILS levels are low or there are too few gene trees.
Availability and implementation: ASTRAL is available in open source form at https://github.com/smirarab/ASTRAL/.
Datasets studied in this article are available at http://www.cs.utexas.edu/users/phylo/ datasets/astral.
Contact: warnow@illinois.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Species tree estimation is difficult in the presence of gene tree conflict, which can result from incomplete lineage sorting [ILS, modelled by the multi-species coalescent (Kingman, 1982)] as well as other causes.
ILS is equivalent to deep coalescence, which occurs with high probability whenever the time between speciation events is short relative to the population size (Maddison, 1997).
When ILS is present, gene trees can differ from each other and from the species tree, presenting substantial challenges to phylogeny estimation methods (Degnan and Rosenberg, 2009; Edwards, 2009).
For example, the standard approach, concatenation (which concatenates the multiple se-quence alignments for different genes together into one super-alignment, and then estimates a tree on the super-alignment) can return incorrect trees with high confidence (Kubatko and Degnan, 2007).
Furthermore, under some conditions, even the most probable gene tree topology may not be identical to the species tree topology (Degnan, 2013; Degnan and Rosenberg, 2006, 2009), a condition called the anomaly zone.
However, there are no anomalous rooted three-taxon species trees (Degnan and Rosenberg, 2009) and no anomalous un-rooted four-taxon species trees (Allman et al., 2011; Degnan, 2013), a key fact that underlies the design of some summary methods and their proofs of statistical consistency.
While several methods are established to be statistically consistent under the multi-species coalescent model, MP-EST (Liu et al., 2010) and the population tree returned by BUCKy (Larget et al., 2010) are among the leading methods.
BUCKy-pop is more computation-ally intensive but can work with unrooted gene trees, while MP-EST requires rooted gene trees but easily scales to hundreds of gene trees and so has been used in many phylogenomic studies (Song et al., 2012; Zhao et al., 2013; Zhong et al., 2013).
Other statistically consistent species-tree estimation methods include BEST (Liu, 2008) and *BEAST (Heled and Drummond, 2010), which co-estimate gene trees and species trees from input sequence alignments; however, these methods are extremely com-putationally intensive on datasets with 100 genes (Bayzid and Warnow, 2013; Smith et al., 2014), so that only summary meth-ods are feasible for use on genome-scale datasets.
However, even the best coalescent-based summary methods have not been reliably more accurate than concatenation (Bayzid and Warnow, 2013; DeGiorgio and Degnan, 2010), and performance on biological datasets has in some cases resulted in species trees that were less well resolved and biologic-ally feasible than concatenation (Kimball et al., 2013; McCormack et al., 2013).
Hence, the choice between coales-cent-based estimation and concatenation is highly controversial (Springer and Gatesy, 2014).
We present ASTRAL (Accurate Species TRee ALgorithm), a new coalescent-based species tree method.
ASTRAL provides a statistically consistent estimation of the true species tree from unrooted gene trees, under the multi-species coalescent model.
Furthermore, ASTRAL runs in polynomial time and can analyse genome-scale datasets in minutes.
We evaluate ASTRAL in comparison with two statistically consistent methods (MP-EST and BUCKy-pop), two simple summary methods [MRP (Ronquist, 1996) and the greedy con-sensus] and concatenation under maximum likelihood (CA-ML) using RAxML (Stamatakis, 2006), on a collection of biological and simulated datasets.
We explore genome-scale analyses, analysing datasets with hundreds to thousands of genes, which are too large for BUCKy-pop and *BEAST to analyse.
*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com https://github.com/smirarab/ASTRAL/)), , `` '' ; Degnan, 2013 `` '' 3 4---or more ; Bayzid and Warnow, 2013 analyze to ( ), analyzing analyze XPath error Undefined namespace prefix ASTRAL is more accurate than the other summary methods under all the simulated model conditions we explore.
As expected, the relative accuracy of ASTRAL and concatenation depends on the amount of ILS, with ASTRAL having an advan-tage when ILS levels are at least moderate, and concatenation having an advantage when ILS levels are low.
Thus, ASTRAL enables highly accurate large-scale phylogenomic estimation, even in the presence of high levels of gene tree conflict because of ILS.
2 APPROACH The input to ASTRAL is a set of unrooted gene trees; ASTRAL finds the species tree that agrees with the largest number of quar-tet trees induced by the set of gene trees.
This optimization problem is NP-hard (Jiang et al., 2001), and so ASTRAL has two versions: an exact version that is guaran-teed to return the globally optimal tree, and a heuristic version that can be used on large datasets.
For the heuristic version, ASTRAL constrains the search space to reduce the running time, by including a set X of bipartitions (splits of the leaf set into two disjoint sets) as part of the input, and requiring that the output species tree T draw its bipartitions from X .
Thus, for every edge e in T, the deletion of e splits the leaf set into two parts, and that bipartition must be in X .
Finding a tree that has the optimum score but draws its bipartitions from the set X can be solved in polynomial time (Theorem 1).
Thus, ASTRAL can be used to find optimal trees for small enough numbers of spe-cies, or heuristically for larger numbers of species.
We formalize this approach as the Maximum Quartet Support Species Tree (MQSST) problem: Input: set T of unrooted gene trees, each leaf-labelled by species set S, and set X of bipartitions on S. Output: tree T on species set S that draws its bipartitions from X such that P q2QT wq; T is maximized, where Q(T) is the set of quartet trees induced by T and wq; T is the number of the trees in T that induce quartet topology q.
The default mode sets X to be all bipartitions from the input set of unrooted gene trees; however, X can be any set of bipartitions.
We note that MQSST takes into account the relative fre-quency of all three alternative quartet topologies and weights them accordingly.
Thus, if the dominant (i.e.
most frequent) quartet topology is much more frequent than the alternatives, trees that do not induce the dominant topology are penalized, but if the three alternative quartet topologies all have frequencies close to 1/3, that quartet will contribute little to the optimization problem.
This approach is in contrast to some other quartet-based methods such as BUCKy-pop that first try to find the dominant quartet topologies and then summarize them.
Estimation of the dominant quartet tree is susceptible to error (because of insufficient gene sampling and estimation error), and the MQSST accounts for this.
ASTRAL uses a dynamic programming (DP) approach to solve the MQSST optimization problem, so that it does not need to explicitly enumerate the set of all possible quartet trees.
For a given unrooted binary tree T and four leaves i; j; k; l in the tree, the induced subtree of T connecting the four leaves will have exactly two nodes u and v that have degree42.
Thus, a quartet tree on i; j; k; l induced by an unrooted binary tree is associated to the pair of nodes {u, v} defined in this way.
Furthermore, given any node x of the tree, it is easy to count the number of quartets that are associated to pairs {x, y} (for some other node y), as we now show.
Deleting x from the tree T separates it into three parts, A, B and C; this is called a tripar-tition and is denoted AjBjC.
We pick one of these sets (say A), and pick two leaves from it, and then pick one leaf from each of the remaining sets.
Therefore, if a, b and c give the sizes of A, B and C, respectively, then the number of quartets mapped to u is a 2 !
bc+a b 2 !
c+ab c 2 !
= abca+b+c 3 2 .
Therefore, we can associate the quartet tree on i; j; k; l induced by T with two tripartitionsone associated with the internal node u and the other associated with the internal node v, where the quartet tree is associated with the pair {u, v}.
Our algorithm uses a DP approach that is similar to the DP algorithm first introduced in Hallett and Lagergren (2000) for constructing species trees from sets of gene trees, minimizing the total number of duplications and losses, and subsequently used to construct species trees minimizing deep coalescence (Yu et al., 2011).
Instead of explicitly calculating quartet trees, we use the set X to generate a set of tripartitions, and then for each tripar-tition, we calculate the number of quartet trees induced by the input set of gene trees that would be associated to that triparti-tion and therefore would be satisfied by any species tree that includes that tripartition.
Thus, the species tree can be con-structed by calculating a score for individual tripartitions based on a recursive formula that defines the DP.
Recall that X is a set of bipartitions that can be used in the output tree T; we define X to be the set of subsets of S that appear as parts of these bipartitions (i.e.
A 2 X if and only if the bipartition AjS A 2 X ).
Then, the recursion in the DP finds a way of dividing each set A 2 X into A0 and A A0 (each of which must be in X) such that the number of quartets satisfied by an optimal rooted tree on A0 and A A0, in addition to those satisfied by the tripartition A0jA A0jS A, is maximized.
Thus, the recursion is given by CA= max A0A;A02X CA0+CA A0+WA0jA A0jS A where WAjBjC counts the number of gene tree quartets asso-ciated to tripartition AjBjC (which we call the weight of the tripartition).
The function C(X) denotes the total contribution to the support of the best rooted tree TX on taxon set X, where each quartet tree in the set of input gene trees contributes 0 if it con-flicts with TX or only intersects it with one leaf, and otherwise contributes 1 or 2, depending on the number of nodes in TX it maps to.
We set the boundary condition to be Cfxg=0.
At the end of the algorithm, C(S) gives the final score, and backtracking gives the final tree.
Because each quartet is associated to exactly two nodes, our described DP counts each quartet tree induced by gene trees exactly twice, and hence, the final score needs to be divided by two to get the quartet score.
The weight of a tripartition is calculated by counting the number of quartet trees mapped to each node of each gene i542 S.Mirarab et al.
due to , , &bull; &bull; ' due to greater than two , `` '' , , , :-dynamic programming ( ) , dynamic programming , dynamic programming : Since dynamic programming tree that is also mapped to that tripartition.
For calculating this, we just need to find the intersection of clusters of the tripartition and all the tripartitions from all gene trees (see Supplementary Materials).
For the special case of A=S, we set WA0jA A0jS A=0.
THEOREM 1.
ASTRAL finds an optimal solution to MQSST, and runs in On2x2k time, where n is the number of species, x is the number of bipartitions in X and k is the number of gene trees.
If X is the set of bipartitions from the input gene trees, then x=Onk, and so ASTRAL runs in On4k3 time.
Because of space constraints, we provide the proof in the Supplementary Materials.
THEOREM 2.
ASTRAL is a statistically consistent estimator of the species tree topology under the multi-species coalescent model, even when run in default modeso that X is the set of bipartitions from the input gene trees.
Proof Sketch: Let T be the species tree.
Given a candidate spe-cies tree T, let wT q;T be the number of trees in T that induce a topology identical to T for a quartet q of taxa.
Unrooted quartet trees do not have anomaly zones (Degnan, 2013); therefore, given a large enough number of gene trees, each quartet topology induced by the species tree will have higher probability than either of the two alternative topologies, and hence appear with greater frequency in T with high probability.
Therefore, for every quartet q and every possible tree T, wT q;T wT q;T with high probability.
By extension, if Q is the set of all quartets of taxa, the score CT T= X q2Q wT q;T attains its (unique) max-imum value when T=T with high probability.
CT T is the score optimized in MQSST; hence, when ASTRAL is run exactly it solves MQSST and so is statistically consistent.
The con-strained default version of ASTRAL is also statistically consist-ent because when a large enough number of gene trees is given, then with high probability at least one of the gene trees will be topologically identical to the species tree, T, and so the set X will contain all the bipartitions from T. When this occurs, ASTRAL run in its default mode will return T. (Note also that X may contain all the bipartitions from T even without having T among its gene trees.)
Note that the MQSST optimization problem could be expressed as finding amedian tree, where instead of finding a species tree that maximizes the total number of quartet trees that it satisfies, we would seek a species tree that has a minimum total distance to the input gene trees, where the distance is the number of quartet trees that it violates.
Then, Theorem 2 asserts that the median tree (under this definition) is a statistically consistent estimator of the species tree, under the multi-species coalescent model.
3 EXPERIMENTS Overview.
We explore performance on a collection of biologi-cal and simulated datasets.
We compare the estimated species trees to the model species tree (for the simulated datasets) or to the scientific literature (for the biological datasets), to evaluate accuracy.
Tree error is measured using the RobinsonFoulds (RF) (Robinson and Foulds, 1981) rate; because all trees esti-mated here are completely bifurcating, this is the same as the missing branch rate (proportion of internal edges in the model tree missing in the estimated tree).
100-taxon simulated datasets.
We briefly describe the process used to generate these data, and direct the reader to the original publication (Yang andWarnow, 2011) for details.
The 100-taxon model species tree was created by a birthdeath process, and 25 genes evolved within the species tree under the multi-species coa-lescent, producing ultrametric gene trees.
Nucleotide sequences with 1000 sites were evolved down each gene tree under a process with GTRGAMMA substitutions as well as insertions and dele-tions, using ROSE (Stoye et al., 1998).
True alignments were used to generate estimated gene trees using RAxML.
37-taxon mammalian simulated datasets.
We simulated this collection of datasets based on a 37-taxon mammalian dataset with 447 genes studied in Song et al.
(2012).
First, we used MP-EST to estimate a species tree on the biological dataset from Song et al.
(2012), and then used it as a model species tree, with branch lengths in coalescent units.
We evolved gene trees down the model tree under the multi-species coalescent model using Dendropy (Sukumaran and Holder, 2010), and then rescaled the gene trees to deviate from the molecular clock and produce branch length patterns observed in the biological data-set.
We then evolved sequences with 500 and 1000 sites down each gene tree under the GTR model of site evolution, using GTR parameters estimated on the biological dataset.
This pro-duces the default model condition that has the amount of ILS estimated for this dataset by MP-EST.
We varied this protocol by scaling the model species tree branch lengths up (2 and 5) or down (0.2 and 0.5) to modify the amount of ILS (so that longer branch lengths reduces ILS, and shorter branch lengths increases ILS).
The default model tree conditions (including the number of genes, sequence length distribution and amount of ILS) were set to produce a dataset called the mixed condition that most resembled the biological dataset.
The average bootstrap support (BS) in the biological data was 71%, and so we generated sequence lengths that produced esti-mated gene trees with BS values bracketing that value500bp alignments produced estimated gene trees with 63% average BS and 1000bp alignments produced estimated gene trees with 79% BS.
The mixed dataset of 400 genes was produced using 200 genes with 63% BS and 200 genes with 79% BS, and had average BS of 71%like the biological data.
For each model condition (specified by the ILS level, the number of genes and the sequence length), we created 20 repli-cates, except for the 1600-and 3200-gene model conditions where we created 10 and 5 replicates, respectively.
We then used RAxML to produce estimated gene trees on the simulated sequence alignments, and we generated 200 ML bootstrap repli-cates for the mixed dataset.
Biological datasets.
We analysed three biological datasets: the mammalian dataset from Song et al.
(2012), containing 37 spe-cies and 447 genes, the plant dataset from Zhong et al.
(2013), containing 32 species and 184 genes, and also the amniota data-set from Chiari et al.
(2012), containing 16 species and 248 genes.
Methods.
We compare ASTRAL with MP-EST, BUCKy-pop (the population tree from BUCKy), MRP (a supertree method), i543 ASTRAL: genome-scale species tree estimation are supplementary Due to supplementary-, &squ;--`` '' `` '' X X X X , `` ''--bootstrap support `` ''-, five analyzed , the Greedy Consensus and concatenated analysis using maximum likelihood (CA-ML), as computed by RAxML.
For 100-taxon datasets and the mixed mammalian datasets, we ran summary methods using three different procedures: using maximum likelihood gene trees as input (bestML), using all bootstrap replicates of all genes as input (All BS) and using the site-only multi-locus bootstrapping (MLBS) procedure (Seo, 2008).
For MLBS, we used the greedy consensus of 200 replicate species trees, each computed on an input consisting of one bootstrap replicate tree per gene.
BUCKy-pop uses a distri-bution of gene trees as input, which we approximate using boot-strap gene trees; thus, BUCKy-pop can only be run with a procedure analogous to All BS.
In subsequent analyses, where we study the impact of various model parameters, we only study the bestML approach.
For the biological datasets, we used the multi-locus bootstrapping procedure (Seo, 2008) to obtain BS values.
For the simulated datasets, we set X to be the set of biparti-tions from the input set of trees.
On the amniota dataset, as the number of taxa is small, we ran the exact version of ASTRAL.
The mammalian biological dataset has a large number of genes that contain all the species, so we used the default setting for ASTRAL.
However, the plant dataset has fewer genes and sub-stantial missing data, and so we extended X to include biparti-tions from species trees estimated using MP-EST, CA-ML and MRP, as well as trees published by Chiari et al.
(2012) (see Supplementary Materials) This ad hoc approach can improve ASTRALs ability to find near-optimal solutions, when the exact version is not feasible.
4 RESULTS Results on mammalian simulated datasets.
The first experiment (Fig.1) shows results on the mixed mammalian dataset, which most closely resembles the biological dataset studied in Song et al.
(2012).
We compare ASTRAL, MP-EST, Greedy, MRP, BUCKy-pop and CA-ML and three types of inputs to sum-mary methods.
For MRP, MP-EST and ASTRAL, using bestML input trees produced more accurate species trees than using bootstrap replicates, either as one input (All BS) or using MLBS.
The purpose of using bootstrap replicates is to take gene tree uncertainty (resulting from insufficient sequence length, for example) into account, but these results indicate that for this model condition, these two simple approaches do not improve species tree estimation.
However, it is possible that other model conditions [perhaps smaller numbers of genes, as studied in Knowles et al.
(2012)] or other ways of addressing gene tree uncertainty might show some advantage over the BestML approach.
Therefore, we use bestML input trees in the remaining experiments.
For the mixed model condition and using bestML trees, ASTRAL is the most accurate of these methods, MP-EST the next most accurate, followed by the other summary methods, and finally by CA-ML.
ASTRAL with any of the three sets of inputs is also more accurate than BUCKy-pop; however, differ-ences between ASTRAL on All BS and BUCKy-pop are rela-tively small.
The next experiment explored variants of the basic mamma-lian simulation, exploring the impact of changes to the ILS level (by scaling the species tree branch lengths), number of genes and gene sequence length, on the absolute and relative performance of various methods using bestML input.
ASTRAL was generally more accurate than all the other summary methods (Fig.2).
However, for a few cases, ASTRAL and one or more summary methods had similar accuracy; for example, on 800 true gene trees from default ILS levels, all summary methods (except for Greedy) produced the true species tree.
Furthermore, ASTRAL was more accurate than CA-ML, except when the amount of ILS is low.
The relative performance between ASTRAL and CA-ML depended on the amount of ILS, so that CA-ML was more accurate than ASTRAL under low levels of ILS, and otherwise ASTRAL was more accurate than CA-ML.
Some observed trends were expected: all summary methods gave improved accuracy as the sequence length in each gene increased from 500 to 1000bp; using true gene trees gave the best results (Fig.2a); species tree error rates generally reduced as the number of genes increased (Fig.2b); and species tree error rates increased as ILS levels increased (Fig.2c).
However, some other observed trends were surprising.
For example, unlike the other methods, Greedy did not continue to improve with increased numbers of gene trees, but could be more accurate than many other summary methods (including MP-EST but not ASTRAL) when the number of gene trees and gene sequence lengths were both small (Fig.2a).
In addition, we observed that MRP, a simple supertree method that is not known to be statistically consistent, was in some cases more accurate than MP-EST.
For example, while MP-EST was always at least as accurate as MRP on true gene trees or on estimated gene trees with high ILS, there were cases (Fig.2a Fig.1.
Species tree estimation error on the default mammalian datasets with 37 genes and 400 genes (half with 500bp and half with 1000bp and with 71% mean BS).
We show the missing branch rates for estimated species trees computed using summary methods (MRP, MP-EST, greedy, BUCKy-pop and ASTRAL) as well as concatenation using RAxML.
Results are shown for running summary methods on maximum likeli-hood gene trees (bestML) and on the set of all bootstrap replicates from all genes (All BS), as well as the greedy consensus of running summary methods on individual bootstrap replicates from all genes (MLBS).
CA-ML is run on the true alignment.
Average and standard error shown based on 20 replicates i544 S.Mirarab et al.
, , ince , supplementary and c) where MRP was more accurate than MP-EST (although the differences are small).
Analyses with large numbers of species.
We evaluated the feasi-bility of using ASTRAL on datasets with large numbers of taxa using the 100-taxon simulated datasets, with 25 genes and 10 replicates.
Because there is no single outgroup, the estimated trees are not rooted, and so we could not use MP-EST.
ASTRAL had no difficulty analysing these data (completing in 51 s).
ASTRAL had average missing branch rate of 6.1%, better than MRP and Greedy (6.4%), but not as good as CA-ML (5.7%); differences are not statistically significant (P40.1; paired Wilcoxon test).
Results on biological datasets.
Song et al.
(2012) analysed a dataset with 447 genes across 37 mammalian species using MP-EST.
Two of the questions of greatest interest were the placement of bats (Chiroptera) and tree shrew (Scandentia), where their MP-EST analysis differed from the concatenated analyses they performed.
In our analysis of this dataset, we noted the distance of esti-mated gene trees to other gene trees; this produced a distribution with two clear outliers (see Supplementary Materials).
We also identified 21 genes with mislabelled sequences [easily confused taxon names, subsequently confirmed by the authors of Song et al.
(2012)].
We removed all 23 outliers from the dataset, and reanalysed the reduced dataset.
We used a multi-locus bootstrapping procedure with 100 repli-cates, with both site and gene resampling, to be consistent with Song et al.
(2012).
We re-estimated the gene trees using RAxML on the gene sequence alignments produced by Song et al.
(2012).
We recomputed the MP-EST tree, obtaining a tree topologically identical to the MP-EST tree reported in Song et al.
(2012), but with lower bootstrap for the placement of Scandentia (62% in our analysis).
CA-ML analyses of the full and reduced datasets were topologically identical and had similar branch sup-port.
Thus, the CA-ML and MP-EST trees on the reduced data-set still differed in the placement of both Scandentia and Chiroptera.
We compare ASTRAL to MP-EST in Figure 3.
Both ASTRAL and MP-EST trees placed Chiroptera as the sister to all other Laurasiatheria except Eulipotyphyla, whereas CA-ML placed Chiroptera as the sister to Cetartiodactyla.
The ASTRAL tree placed Scandentia as sister to Glires with 74% support and thus agrees with the CA-ML tree but differs from the MP-EST tree.
(A) (B) (C) Fig.2.
Species tree estimation error on the simulated mammalian datasets.
We show the missing branch rates for estimated species trees computed using summary methods (MRP, MP-EST, greedy and ASTRAL) as well as CA-ML.
Summary methods are run on RAxML bestML gene trees.
We also show performance of summary methods on the true gene trees.
Subfigure (A) shows results under default levels of ILS, varying the number of genes and gene tree resolution; (B) shows results under increased ILS levels, varying the number of genes, and on both true gene trees and estimated gene trees and (C) shows results on 200 genes, varying the amount of ILS from very low (5 species tree branch lengths) to very high (0.2 species tree branch lengths) i545 ASTRAL: genome-scale species tree estimation 2( ) analyzing under one econd p analyzed supplementary analyzed-in order while , Plant dataset.
We analysed a plant dataset from Zhong et al.
(2013) of 32 species and 184 genes using ASTRAL, adding bipar-titions to X (see Supplementary Materials).
The question of greatest interest is the sister group to land plants.
Previous analyses have inferred many different possible sister clades, including the following four major hypotheses: Zygnematales, Coleochaetales, Zygnematales+Coleochaetales and Charales.
Zhong et al.
(2013) used MP-EST to analyse their data and inferred Zygnematales as the sister with 64% BS.
A reanalysis of the same data using STAR was performed by Springer and Gatesy (2014), who obtained Zygnematales+Coleochaetales with 44% BS.
We analysed this dataset using ASTRAL and obtained a tree that generally has high BS on the branches (i.e.
with the excep-tion of four branches, all branches have support at least 86%, and most have 100% support).
However, one edge had low support (only 18%).
After collapsing the single branch with low support, we obtained a tree (see Supplementary Materials) in which the Charales+ land plants hypothesis is rejected with moderately high support (86%); however, it is not determined whether Zygnematales, Coleochaetales or Zygnematales+Coleochaetales are the sister group to land plants (the branch that distinguishes between these three hypoth-eses is the one with 18% support).
Thus, ASTRALs analysis of this dataset can be seen as suggesting that this dataset is insuffi-cient to completely resolve the sister relationship to land plants.
However, the most interesting question is whether Charales are sister to land plants, and the ASTRAL tree rejects that hypoth-esis with 86% support.
Amniota dataset.
Chiari et al.
(2012) assembled a dataset of Amniota to resolve the position of turtles relative to birds and crocodiles.
Most recent studies favour an Archosaurus hypoth-eses that unites birds and crocodiles as sister groups (Hugall et al., 2007).
The MP-EST analyses by Chiari et al.
(2012) resolved this relationship differently when AA and DNA gene trees were used; thus, AA had 99% support for the Archosaurus clade, but DNA rejected Archosaurus with 90% support.
We analysed the same dataset using the exact version of ASTRAL and found that both AA and DNA recover Archosaurus; how-ever, while ASTRAL on AA gene trees recovered Archosaurus with 100% support, ASTRAL on DNA gene trees had only 55% support for Archosaurus.
Running time.
Comparisons between coalescent-based methods reveal substantial differences in running time.
For example, on the mammalian dataset from Song et al.
(2012) with 37 taxa and 421 genes, MP-EST (run with 10 random starting points) used 83min per bootstrap replicate, while ASTRAL used 7 s. Analyses of the simulated mammalian datasets allow us to explore the limits of BUCKy-pop, as well as obtain other com-parisons.
We examine running times under moderate ILS, gene sequences of length 500 bp, and with 400 and 800 genes and with bestML input trees (except for BUCKy-pop).
BUCKy-pop strictly runs in serial, using a Bayesian Markov Chain Monte Carlo (MCMC) technique, which can take a long time and substantial memory to reach convergence.
On the 37-taxon mammalian simulated datasets, BUCKy-pop ran to completion for datasets with up to 400 genes (where it took 5h), but failed to complete (due to memory issues) on the 800-gene dataset.
MP-EST completed relatively quickly100minfor both the 400-gene and 800-gene datasets.
We ran MP-EST with 10 random starting points, so this time could be reduced by using just one starting point, but with a potential decrease in accuracy.
ASTRAL completed in 3.3 s on the 400-gene dataset and in 5.3 s on the 800-gene dataset.
Thus, ASTRAL is dramatically faster than the other methods and able to run on these phyloge-nomic datasets in reasonable time frames.
However, BUCKy is used with 200 bootstrapped gene trees for each gene and out-puts support values.
Running ASTRAL and MP-EST using MLBS to obtain support values would increase their running times if run in serial, but ASTRAL would still be much faster than BUCKy (e.g.
11min on the 400-gene dataset rather than 5h).
In addition, parallelizing MLBS is trivial because each boot-strap replicate is independent.
See Supplementary Materials for more information about running times under different model conditions.
5 DISCUSSION AND CONCLUSIONS This study introduced ASTRAL, a method for estimating species trees from unrooted gene trees.
We proved that ASTRAL is statistically consistent under the multi-species coalescent model.
In our study, ASTRAL was more accurate than MP-EST and BUCKy-pop, two leading coalescent-based methods, and improved or matched the accuracy of CA-ML under many conditions, except when the amount of ILS was low, where concatenation was more accurate.
Results on the biological datasets show that statistically consistent coalescent-based meth-ods can differ in terms of support for established clades, and produce different resolutions of biologically interesting relationships.
Fig.3.
Analysis of the Song et al.
mammals dataset using ASTRAL and MP-EST.
We show the result of applying ASTRAL and MP-EST to 424 gene trees on 37-taxon mammalian species.
MP-EST is based on rooted gene trees; ASTRAL is based on unrooted gene trees, and then rooted at the branch leading to the outgroup.
Branch support values in black are for both methods, those in red are for ASTRAL and values in blue are for MP-EST.
See Supplementary Materials for trees with full resolution i546 S.Mirarab et al.
z supplementary analyze ,-analyzed , very very supplementary analyzed T utes econds approximately ours-about utes-econds , econds , , s , utes ours since supplementary materials concatenation under maximum likelihood very The differences in performance are the result of different algorithmic techniques, which can result in greater or lesser robustness to missing data (Springer and Gatesy, 2014) and gene tree estimation error (Bayzid and Warnow, 2013).
Hence, the choice of coalescent-based method matters.
This study also showed that concatenation can be more accurate than coa-lescent-based estimation, provided that the amount of ILS is low enough.
However, the best coalescent-based methods can be more accurate than concatenation under biologically realistic conditions.
This study suggests the possibility that some of the observed discrepancies between previous coalescent-based analyses and concatenation in previous studies (Springer and Gatesy, 2014) might be the result of the choice of coalescent-based method, and that improved coalescent-based analyses might not only help to identify alternate relationships but might also confirm prior hypotheses produced using concatenation.
The algorithmic design of ASTRAL can be improved.
When run in default mode, ASTRALs accuracy is limited by the bipar-titions in the input gene trees.
Including estimated species trees in X enlarges the search space and allows ASTRAL to produce highly accurate species trees, but other less ad hoc approaches for expanding X should also be developed.
The running time we have given is polynomial and fast enough to run on genome-scale datasets, but improved algorithmic designs with better asympto-tic performance could also be developed.
Using bootstrap replicate gene trees instead of best ML gene trees did not improve species tree estimation accuracy on the simulated mixed mammalian datasetand in fact made species tree estimations less accurate for MRP, MP-EST and ASTRAL.
This suggests the possibility that the topological error in boot-strap gene trees is large enough to offset any improvement in species tree estimation obtained by taking gene tree uncertainty into account.
However, it is possible that an improvement might be obtained under other conditions, or that using a sample of gene trees estimated by a Bayesian MCMC analysis might be better-suited to coalescent-based species tree estimation methods than maximum likelihood bootstrap trees, as suggested by DeGiorgio and Degnan (2014) [although see Yang and Warnow (2011)].
Knowles et al.
(2012) found varying impact in species tree topology estimation through taking gene tree esti-mation error into account, but only examined small numbers of species and genes; thus, to some extent, the results we obtained might be because of the large number of genes and perhaps species in our studies.
In summary, advances in algorithmic strategies for coalescent-based estimation can enable highly accurate species tree estimation in the presence of massive ILS.
ASTRAL provides one such advance, but new and more accurate coalescent-based methods are needed to enable these analyses, especially for genome-scale datasets where missing data and extremely low phylogenetic signal in individual genes may be a substantial problem.
ACKNOWLEDGMENTS The authors thank the anonymous reviewers for their sugges-tions for improvements to the manuscript.
Funding: This work was supported by a generous allocation on Texas Advanced Computing Center (TACC).
This research was supported by the National Science Foundation [0733029 and 1062335 (to T.W.
), 10735191 (through iPLANT), and 1216898 (to M.S.S.
)]; by the University of Alberta, Musea Ventures and Prof. G. K.-S. Wong; and by a Howard Hughes Medical Institute (HHMI) graduate student fellowship (to S.M.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Homologous protein families share highly conserved sequence and structure regions that are frequent targets for comparative analysis of related proteins and families.
Many protein families, such as the curated domain families in the Conserved Domain Database (CDD), exhibit similar structural cores.
To improve accuracy in aligning such protein families, we propose a profile profile method CORAL that aligns individual core regions as gap-free units.
Results: CORAL computes optimal local alignment of two profiles with heuristics to preserve continuity within core regions.
We benchmarked its performance on curated domains in CDD, which have pre-defined core regions, against COMPASS, HHalign and PSI-BLAST, using structure superpositions and comprehensive curator-optimized alignments as standards of truth.
CORAL improves alignment accuracy on core regions over general profile methods, returning a balanced score of 0.57 for over 80% of all domain families in CDD, compared with the highest balanced score of 0.45 from other methods.
Further, CORAL provides E-values to aid in detecting homologous protein families and, by respecting block boundaries, produces alignments with improved readability that facilitate manual refinement.
Availability: CORAL will be included in future versions of the NCBI Cn3D/CDTree software, which can be downloaded atContact: fongj@ncbi.nlm.nih.gov.
Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Homologous protein families contain core regions that reflect conservation in molecular evolution.
Many protein family alignments in Pfam (Finn et al., 2006), SMART (Letunic et al., 2006) and SUPERFAMILY (Wilson et al., 2007) exhibit conserved regions including blocks, or ungapped regions, within an alignment.
The Conserved Domain Database (CDD) (Marchler-Bauer et al., 2009) models protein domains explicitly as series of blocks.
For NCBI-curated domains, the blocks represent structural core motifs based on structure superpositions as well as conserved sequence regions and motifs.
Comparative analysis of proteins and protein families through sequence alignment is invaluable for grouping homologs, To whom correspondence should be addressed.
subdividing diverse families into sub-families, tracing evolutionary histories and identifying conserved functional sites.
In recent years, alignment methods that compare two profiles, the statistical models that represent protein families, have been shown to improve alignment quality and homolog recognition over sequencesequence methods such as BLAST (Altschul et al., 1997) and sequenceprofile methods such as PSI-BLAST (Altschul et al., 1997; Schaffer et al., 2001).
Numerous profile alignment methods have been assessed in Edgar and Sjolander (2004), Heger and Holm (2001), Ohlson and Elofsson (2005), Ohlson et al.
(2004), Panchenko (2003), Rychlewski et al.
(2000), Soding (2005), Yona and Levitt (2002) and others.
While many alignment methods focus on detecting remote homologs in order to expand coverage of functional inference, obtaining high-quality alignments remains difficult even for closely-related families.
According to structure superpositions, corresponding core regions in many homologous domains differ by fewer insertions and deletions than inferred by general alignment programs, reflecting the stability of the structural core of the protein family.
To better capture this property, we propose a method CORAL (CORe ALigner) to align core regions from two protein families without indels within blocks, which we will refer to as the core constraint.
CORAL is implemented through a common dynamic programming engine for optimal pair-wise alignment (Needleman and Wunsch, 1970; Smith and Waterman, 1981).
Several other algorithms to align sequence or sequence profiles to core regions have been effective for detecting similarities or assigning domains.
These algorithms include a profileprofile method using Gibbs sampling (Panchenko, 2003), and SALTO (Kann et al., 2005) and GLOBAL (Kann et al., 2007) which employ additional block-based constraints.
SALTO aligns a consecutive sub-set of complete blocks and GLOBAL aligns a sub-set (including full or empty set) of contiguous columns within every block.
All of these methods disallow indels in alignments of blocks and exclude sequence regions outside blocks.
Additionally, LAMA (Pietrokovski, 1996) and CYRCA (Kunin et al., 2001) were developed to align individual blocks that represent sequence motifs (Henikoff et al., 2000).
Block shift and extension operations have also proved useful to improve multiple sequence alignments (MSAs) through REFINER (Chakrabarti et al., 2006).
Here, we present the CORAL algorithm and benchmark its performance on curated domains in CDD against other widely used profile methods COMPASS (Sadreyev and Grishin, 2003), HHalign (Soding, 2005) and PSI-BLAST.
Reference alignments are inferred from structure superpositions from the VAST database 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1863 18621868 CORAL (Gibrat et al., 1996; Madej et al., 1995) and the SABmark benchmark set (Van Walle et al., 2005), and from a comprehensive set of expert-determined mappings, and homology is defined by CDD relationships.
In particular, CORAL outperforms all other methods in the quality of alignments.
We also discuss the role of profile alignment in modeling protein families.
2 METHODS 2.1 Core regions dataset MSAs representing protein family core regions were taken from the curated domains in CDD.
Sequence regions outside the cores are not aligned in CDD and are not considered in this study.
Here, we use the terms domain and protein family interchangeably.
NCBI-curated domains have been organized into hierarchical domain families.
A superfamily, which indicates common evolutionary descent, contains one or more domain families.
We define related domains with respect to CDD to be those in the same family and unrelated domains to be those in different superfamilies, in order to minimize false positives (FPs).
A set of 100 domains, chosen randomly from different superfamilies, was reserved for parameter optimization (dataset opt100).
Similarity between domains was estimated as the fraction identity of their consensus sequences with pair-wise sequence alignments computed by MUSCLE 3.6 (Edgar, 2004).
The consensus sequences express only columns in the MSA with <50% gap content, including the most conserved columns, and hence report higher similarity values than using full length protein sequences.
2.2 Reference alignments To test alignment accuracy, we construct three benchmark datasets.
The first reference set is based on superpositions of the 3D structures that annotate curated domains in CDD v2.14.
We gather structural neighbors from the VAST database that satisfy the default significance cutoff of P-value < 0.0001, such that folds are described by a continuous sequence region that overlaps the sequence fragment in the domain model by 90%.
To ensure that the structure alignments involve core regions, aligned core positions are required to comprise 80% of all structurally aligned positions and 50% of the respective profiles.
This procedure yields structure alignments for 2385 domain pairs within 91 CDD families.
A second set of structure alignments is taken from the superfamilies set in SABmark, that is, alignments of SCOP domains with a common evolutionary origin.
CDD domains are mapped onto the SCOP domains using RPSBLAST (Marchler-Bauer et al., 2002).
Due to time of testing, a later version of CDD (v 2.16) was used for this benchmark set.
SCOP folds are filtered for live sequences in Entrez and at least 50% overlap with the extent of the domain hit, resulting in structure alignments for 1627 domain pairs in 128 SCOP superfamilies.
The two structural reference sets differ in coverage across and within domain families; classification by CDD versus SCOP; and curator-optimized versus RPSBLAST-computed structure-domain alignments.
Athird benchmark set provides comprehensive coverage over homologous domains in CDD.
In NCBI-curated hierarchies, the MSAs of a parent domain and its sub-family contain overlapping fragments from at least one protein sequence.
The shared sequence identifies aligned columns between the two MSAs and reflects the curators assertion of how the sub-family should be mapped to its parent.
Transitivity over each hierarchy extends the guide alignment to all pair-wise comparisons in multi-domain families.
Guide alignments include 57 786 domain pairs over 212 CDD families.
2.3 Alignment algorithm We describe the profile alignment algorithm with core constraint in terms of required modifications to the canonical algorithm for local alignment (Smith and Waterman, 1981).
The problem is to align profiles A = a1 an and B = b1 bm with n and m columns, respectively, where each profile has been sub-divided into blocks.
Let table H contain the maximum similarity score of two profile segments ending in ai and bj in entry Hi,j Scoring functions S(ai, bj) to compute the similarity between profile columns ai and bj are described in the next paragraph.
To prevent gaps within blocks, the affine gap penalty is replaced with a large negative value if the last aligned column before the gap is not a block end.
To ensure that the endpoints of the optimal alignment fall on the N-and C-terminal of some blocks, Hi,j may be re-initialized to S(ai, bj) (replacing initialization to 0) if ai or bj is the first column in its respective block and traceback through Hi,j is required to terminate at that position.
Traceback may begin from the maximum Hi,j such that at least one of ai and bj is the end of its respective block.
These changes preserve the O(nm) running time.
The optimal scores from H are normalized into Z-scores as follows.Alarge set of random alignments was simulated using all curated domains, each aligned with 100 domains from different superfamilies.
Alignment scores were binned by the sum of lengths of the profiles.
Regression curves were fitted for the means and SDs over the bins.
The length-dependent values from the regressions were used to compute Z-score.
2.4 Scoring functions Much of the previous work on profileprofile alignment algorithms sought advances through new scoring functions for comparing profile columns.
Probabilistic methods are believed to be the most effective (Mittelman et al., 2003; von Ohsen et al., 2003) and are applied in state-of-the-art aligners such as prof_sim (Yona and Levitt, 2002), COMPASS and HHsearch.
CORAL uses a symmetrical log-odds function similar to Picasso (Heger and Holm, 2001) and COMPASS (Sadreyev and Grishin, 2003): SLO ( a,b )= k Qak log ( Rbk )+ k Qbk log ( Rak ) To compute similarity between aligned columns a and b, Qa and Qb represent vectors of weighted observed frequencies of amino acids k in the respective columns.
Likewise, R is the vector of the frequency ratios of weighted frequency for each amino acid over the background frequency of the amino acid.
Q and R are defined as for PSI-BLAST (Altschul et al., 1997; Schaffer et al., 2001).
Surveys of scoring functions (Edgar and Sjolander, 2004; Mittelman et al., 2003; Panchenko, 2003) have suggested that probabilistic methods offer incremental improvements over simpler functions such as sum of pairs (Gotoh, 1993), dot product and Pearson correlation coefficient.
Consequently, we also test the symmetrical dot product function: SDP ( a,b )=Qa Rb +Qb Ra In Section 3, the two methods will be denoted as CORAL LO and CORAL DP, respectively.
The public release of CORAL will use the better performing log-odds function.
2.5 Parameter optimization A local alignment requires that the expected column score be negative and some column score(s) be positive.
To satisfy these conditions, a constant shift value is added to each column score.
To initialize the search space for potential shift values, we computed the distributions of column scores for correctly aligned columns in all related domains in CDD and for all pairs of columns in a sampling of unrelated domains.
A second parameter, the gap penalty, is necessary to distinguish significant alignments.
Shift values between the means of each distribution and small gap weights were tested systematically over combinations of both parameters.
Performance was assessed for alignment accuracy and homolog sensitivity following the testing procedures and metrics described in Section 3.
Over the opt100 dataset, performance was fairly robust over a range of parameter values.
We assigned shift values of 0.15 and 6.6 for the two scoring functions, respectively, and gap weights of 0.1 and 0.5, respectively.
1863 [18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1864 18621868 J.H.Fong et al.
2.6 Statistical significance To approximate the statistical significance of each alignment, we turn to the extreme value distribution (EVD) which has been shown empirically to fit optimal ungapped alignments of random sequences (Karlin and Altschul, 1990).
It is frequently used with gapped sequences and profile alignments.
Supposing that the alignment scores follow an EVD, the E-value for every alignment can be computed from the alignment score z and parameters and as E =e(z).
To determine and , normalized alignment scores from the random alignments described above were fitted to the cumulative density function,F(x) = exp(exp((x))).
Parameters were computed separately for each scoring function SLO and SDP .
The goodness of fit is illustrated for CORAL LO in Supplementary Figure S1.
3 RESULTS 3.1 Alignment accuracy The quality of CORAL alignments between CDD-curated domains was evaluated against the reference alignments described in Section 2 and compared with alignments from COMPASS 3.0, HHalign 1.5.1.1 and PSI-BLAST.
COMPASS is a high-performance implementation of the standard sum-of-scores optimal local alignment and its comparison with CORAL implies a lower bound in improvement that can be attributed to the core constraint.
COMPASS was run with default parameters and with reduced gap penalties.
To promote longer alignments, the gap open penalty was reduced arbitrarily default from 10 to 3 and the gap extension penalty default from 1 to 0.1.
HHalign was run in local and global modes using one domain alignment as query and the other as template.
To compute probabilities and E-values for HHalign, each HMM was calibrated against the cal.hhm database from the download site.
For every pair of domains, a PSI-BLAST alignment was computed between one domain and each sequence from the MSA of the other domain, and vice versa, using the NCBI Toolkit.
The sequence profile alignment with smallest E-value was used as the PSI-BLAST alignment.
CORAL and COMPASS held a speed advantage over the other methods, requiring than a 10th of a second for most inputs.
HHalign required 510 s, largely because of the calibration step.
The following metrics are used to evaluate alignment accuracy.
To measure extent of reconstructing a reference alignment, we compute Sdev, the ratio of the number of correctly aligned positions to the number of aligned columns in the reference alignment.
Sdev is the same as the developers score of (Sauder et al., 2000).
To measure correctness, we compute Smod, the ratio of the number of correctly aligned positions to the number of aligned columns in the evaluated alignment where at least one of each two aligned columns is present in the reference alignment.
This is analogous to the modelers score (Sauder et al., 2000), modified to include only the profile columns that can be determined to be correct or not.
The two previous measures are summarized through a balanced score, Sbalanced = (Sdev +Smod)/2.
To more directly illustrate the trade-off between alignment accuracy and alignment length, we estimate the latter as Scov, the number of aligned positions divided by the length of the shorter profile.
Results from multiple structure alignments for the same domain pair are averaged over the domain pair.
First, we analyze overall performance over CDD families and SCOP superfamilies, both referred to as families for brevity.
An average Sbalanced for every family is taken over its domain pairs (Fig.1).
CORAL produced high-quality alignments for more families than the other methods: 44% of domain families average Fig.1.
Distribution of balanced scores from three benchmark sets: (A) VAST structure superpositions; (B) SABmark structure alignments; and (C) curator-inferred guide alignments.
SABmark alignments are grouped by SCOP superfamily and the others by CDD family.
The balanced score is an average of accuracy over computed alignment and completeness in reconstructing the reference alignment.
Sbalanced 0.8 compared with 41% by the best non-CORAL method according to the VAST benchmark, 37% versus 28% according to the SABmark benchmark and 57% versus 45% by guide alignments.
In nearly all of these families, the alignments with Sbalanced 0.8 were both accurate and complete.
Under the three highest performing methods (CORAL LO, CORAL DP and HHalign global), over 96% of domain families with Sbalanced 0.8 had both Sdev 0.8 and Smod 0.8 with respect to all benchmark sets.
Comparison of Sbalanced over the domain pairs present in more than one benchmark set reveals high consistency among the reference alignments.
For pair-wise comparison of the reference sets, we identified domain pairs present in both benchmark sets.
Sbalanced scores for the common domain pairs, averaged over domain families, were 0.0260.032 lower according to the different 1864 [18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1865 18621868 CORAL alignment methods in VAST alignments than the corresponding guide alignments and 0.0320.047 lower in SABmark alignments than the corresponding guide alignments.
Approximately 2% of domain pairs had balanced score at least 0.01 higher by VAST structure alignments, 30% of domain pairs had balanced score 0.01 higher for guide alignments and the remaining two-thirds of domain pairs had negligible differences between those two references.
We hypothesize that guide alignments are more accurate because they are the outcome of manual curation that reviews both structure-based alignments and patterns of sequence conservation, where the latter may overrule structure superimposition.
Nearly 19% of the domain pairs evaluated with SABmark were assigned to different CDD superfamilies (but the same SCOP superfamilies).
Average Sbalanced score over these pairings was less than half that from domains in the same CDD superfamily, resulting in a larger fraction of families with low-balanced score than from the other reference alignments for all alignment methods (Fig.1).
CORAL returned highest average Sbalanced score for domains in different CDD superfamilies as well as domains from the same CDD superfamilies.
The higher Sbalanced scores for both CORAL methods over the other methods suggest that the core constraint played a significant role in improving performance for several families.
For some families, including Macro and PDZ, all members benefited from the core constraint.
Domain families that benefited the most and the least using CORAL are listed in Supplementary Table S1.
No correlation was observed between average similarity within families and improvement, or lack thereof, from using CORAL.
Better alignments generally came about because CORAL prevented spurious intra-block gaps and shifted blocks that were misaligned by COMPASS and HHalign into the right positions.
The families with most negative effect from CORAL, phosphofructokinase (PFK) and Rieske, illustrate the case where long blocks must be split to enable a completely correct CORAL alignment.
One example is the alignment of two Rieske domains: non-heme iron oxygenase family/nathphalene 1,2-dioxygenase sub-family (cd03535) and small sub-unit of Arsenite oxidase family (cd03476).
Families such as the kinesin/myosin motor domains contain dissimilar sub-groups such that domains within a sub-group are aligned with much higher accuracy than domains from different sub-groups.
To account for varying difficulty, domain pairs were grouped by sequence identity.
The distribution of sequence identity is shown in Supplementary Figure S2 with mean percent identity 29.6% and SD 10.1%.
We partitioned alignments into four similarity ranges: 020%, 2030%, 3040% and 40%.
Results from guide alignments are provided in Figure 2 and referred to in the remainder of the section; results from VAST and SABmark alignments illustrate similar trends and are provided in Supplementary Figure S3.
Sdev and Smod results within each similarity range are consistent across most alignment methods (Fig.2), pointing to the inherent ease or difficulty of aligning particular domains.
CORAL has highest Sdev over all similarity ranges.
Although HHalign local and COMPASS with default arguments have higher Smod at <30% identity, CORAL yields higher Sbalanced value for every similarity range.
Over the entire dataset, CORAL gives an average balanced score of 0.80 and 0.77 for the log odds and dot product functions, respectively, compared with 0.74 for HHalign and 0.75 for COMPASS.
The shorter alignments correlate with higher alignment accuracy (Smod), Fig.2.
Alignment accuracy in terms of the (A) Sdev; (B) Smod; and (C) Scov metrics based on curator-optimized (guide) reference alignments.
These metrics indicate completeness in reconstructing the reference alignment, accuracy over the computed alignment and the localglobal trade-off in the resulting alignment, respectively.
but are less informative as they exclude more homologous regions.
CORAL and COMPASS parameters may be set to permit near-global alignments using a local alignment algorithm.
PSI-BLAST performance deteriorated rapidly as sequence similarity decreases.
Almost half of all domain pairs from the same family had no significant PSI-BLAST alignment.
Aligning the consensus sequences by pairwise BLAST led to a similar outcome, showing that these families are not as easy to align despite the high-reported sequence identities.
The default significance cut-off for PSI-BLAST is restrictive and many domain pairs may not satisfy the cut-off due to low-sequence similarity or short profile lengths.
Domain pairs with no PSI-BLAST results were assigned value 0 for all metrics (following the regular definitions of Sdev and Scov, and replacing the otherwise undefined Smod term), leading to a large number of families with low Sbalanced score.
1865 [18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1866 18621868 J.H.Fong et al.
Fig.3.
Recognizing homologs: ROC curve plotting percentage of TP identified before the n-th FP.
3.2 Homology recognition Next, we evaluated the accuracy of CORAL and its E-values at detecting related domains.
Although we do not propose to identify homologous protein families from core regions alone, given the evolutionary signal present in the more variable loop regions, a scoring system helps to distinguish more similar and better-aligned core regions.
Related and unrelated domains are defined with respect to CDD families/superfamilies, as described in Section 2.
A test set of 100 domains was taken from different superfamilies.
Each domain is aligned with all domains within the same family (with a minimum of two related domains) and with 100 randomly selected unrelated domains, using the alignment methods described in the previous section.
PSI-BLAST was omitted to avoid handling missing data.
The distribution of sequence identity between related domains in this test set is similar to the distribution over the entire CDD (Supplementary Fig.S2).
Figure 3 shows performance measured as the fraction of true relationships (true positive, TP) that score higher than the i-th highest scoring false relationship (FP), averaged over the test set.
Scores refer to the E-values for CORAL and COMPASS and probabilities for HHalign, which performed much better than its E-values.
To assess sensitivity, we measure the area under curve (AUC), ROCn =1/ni=1...n ti, for each sample domain where ti is the fraction of TPs before the i-th FP.
Standard error over ROCn values is computed as SE=/n.
ROC curves and AUC values reveal that the CORAL and HHalign methods detect homologs from core regions at similar rates, and better than COMPASS.
Average ROC100 and SE ranges overlapped for all CORAL and HHalign methods and were: 0.9620.009 for CORAL LO, 0.963 0.008 for CORAL DP, 0.966 0.008 for HHalign local and 0.957 0.011 for HHalign global.
There was a statistically significant difference between the distribution of ROC100 values for HHalign local, the highest curve in Figure 3, from the closest methods HHalign global and CORAL according to the Wilcoxon signed-rank test (P-values 0.010.02), but not between the CORAL methods and HHalign global (all pair-wise P-values > 0.05).
3.3 Alignment in protein family modeling The problem of aligning conserved core regions was conceived by the need to automate domain curation and develop tools for analyzing individual families.
Many domain families contain diverse members that are difficult to align.
Sequence similarity, for example via characteristic motifs, can make it clear that sequence fragments are related by common descent.
More powerful tools are needed to obtain an accurate alignment across the full domain model and to determine domain boundaries.
In defining diverse domain families, two important and interrelated tasks for each domain are step 1: to build a MSA and step 2: to split off sub-families when applicable for increasing functional specificity, starting with a less-diverse sub-set of sequences from the current domain.
These tasks are common to many approaches to subfamily identification (see e.g.
Brown et al., 2007), although, here we describe steps in the CDD curation pipeline.
Typically, the higher degree of conservation in child models allows curators to extend blocks and/or define additional blocks beyond the base core structure of their parent.
Aligning the child and parent domains requires the selection of a representative sequence to provide the guide alignment between a new sub-family and its parent domain.
A badly aligned representative compromises the overall alignment of the child with respect to the parent, which may amplify noise present in the parent and misrepresent evolutionary distance and diversity within the superfamily.
Cleaning up the child model by itself further propagates overall error, which may be difficult to detect.
By iterating steps 1 and 2, the child alignment is refined, its core structure may be extended or revised, and realigning the child and parent may help to refine the core structure of the parent as well.
When subfamilies are covered by 3D structure, structure superposition helps to provide high-quality guide alignments.
Profile alignments augment this information and may substitute for superpositions when structures are not known.
The structure alignment may differ markedly from the guide alignment, as in the alignment of the eukaryotic translation factor 5A domain and the Hex1/S1-like RNA-binding domain (Fig.4).
In this case, CORAL validates the structural alignment and extends the aligned region.
A third major step in CDD curation is annotating domain models with function and functional sites following the literature and analysis of 3D structures.
The alignment of related protein families helps to confirm the locations of functional sites, which may be placed at nearby positions in parent and child domains as shown in Figure 4 for RNA-binding sites.
4 DISCUSSION Here, we showed that profileprofile alignment with well-structured alignment constraints can achieve high-alignment accuracy and work well in detecting homologous relationships between conserved core regions of domain families.
The core constraint exploits relationships between profile columns, prohibiting insertions or deletions within blocks, rather than pursuing improvements through refinement of the column scoring function.
Our proposed method is a simple interpretation of a framework in which gap penalties vary according to local conservation, requiring only two different gap penalties.
The core constraint may be incorporated into other alignment algorithms as well.
We benchmarked CORAL on core regions from NCBI-curated domains in CDD.
Blocks in curated domains reflect sequence and structural conservation and approximate the structural core of the family.
However, curators may define blocks to be longer or shorter than in structure alignments, and merge, split or delete the blocks suggested by structure alignments.
They may also introduce 1866 [18:14 3/7/2009 Bioinformatics-btp334.tex] Page: 1867 18621868 CORAL Fig.4.
VAST structure, guide and CORAL alignments between the eukaryotic translation factor 5A domain (cd04468; eIF5A) and the Hex1/S1-like RNA-binding domain (cd04469; S1_Hex1) are illustrated using sequence fragments from 1X6O and 1KHI and structure superpositions.
The domains share a parent (EF-and S1-like RNA-binding domain) and 28% identity.
Aligned positions in the reference alignments are underlined.
The structure alignment is believed to be the most accurate.
Misaligned regions in the guide and CORAL alignments are colored blue.
RNA-binding sites are highlighted in yellow on both sequence and structure alignments.
The structure superposition is colored red for identical residues, purple for other aligned residues and grey for unaligned residues.
additional blocks to record conserved features and sites outside the structural core, such as binding sites and motifs.
CORAL E-values identify 70% of all domain pairs from the same hierarchy with E-value < 0.05 compared with 3.0% of domain pairs from different superfamilies.
Ranking scores from the same family, as in the homology recognition test, achieves even higher performance.
In general, the CDD superfamily classification used to define homologs is comparable in specificity to SCOP superfamilies, the basis for remote homology in previous benchmark studies (Marchler-Bauer et al., 2009).
Nevertheless, that curated domains in CDD are easier to classify is unsurprising, because many previous studies aligned noisier profiles constructed by PSI-BLAST and the hierarchical organization of CDD families suggests that many domains have similar conserved cores.
Constructing high-quality alignments between well-defined core regions, in contrast, benefits tremendously from the core constraint.
CORAL aligns more families with high-balanced score, produces better alignments with respect to the balanced score than COMPASS or HHalign across all similarity ranges, and returns higher developers score for almost all groups of data.
Possibly even more importantly, by respecting block boundaries, it produces alignments that may be easier to revise.
Automated alignments of sequences or profiles with low similarity often require manual correction to produce optimal results.
Reducing error to a small number of block shifts simplifies manual analysis.
Although the core constraint reduces the space of possible alignment solutions, it does not necessarily constrain the alignment to only one good solution.
Our results demonstrate that weak sequence similarity between corresponding core regions increases errors in all methods.
Additionally, even in the more constrained setting of global alignment, differences in profile and block lengths permit more than one possible alignment between many blocks.
The clear shortcoming of the core constraint is that at some level of divergence, core regions cannot be aligned correctly without insertions or deletions, hence methods without the core constraint are more suited to remote homolog recognition and alignment.
One solution to ameliorate shift errors is to split long blocks into shorter units, randomly or by inspecting the block structure or preliminary alignments of core regions.
The curated domain models already contain breaks within blocks where the sequences naturally split.
In unreported experiments, we have aligned the curated domains using this alternative block definition with similar and slightly worse overall performance.
Further development of this algorithm will allow for cases where additional blocks have been inserted into a sub-family model relative to its parent.
CORAL will be made available to the public as an alignment tool bundled into a future release of the NCBI Cn3D/CDTree software.
This user-friendly implementation will provide fast and accurate alignment of core regions, along with access to protein family alignments from CDD.
While we only tested alignments between pre-computed protein family models, core regions may be inferred from the continuous regions of any protein family alignment.
However, the effective use of CORAL requires high overlap between the conserved regions of two families, for example, in the case of a common structural core, and additional processing may be needed to identify putative conserved core regions.
The core constraint may also be incorporated into profile alignment algorithms with more sophisticated scoring methods to improve on both CORAL and the original method for aligning conserved cores.
ACKNOWLEDGEMENTS We thank Anna Panchenko and John Spouge for helpful discussions.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine.
Conflict of Interest: none declared.
ABSTRACT Motivation: Phospholipid scramblases (PLSCRs) constitute a family of cytoplasmic membrane-associated proteins that were identified based upon their capacity to mediate a Ca2+-dependent bidirectional movement of phospholipids across membrane bilayers, thereby collapsing the normally asymmetric distribution of such lipids in cell membranes.
The exact function and mechanism(s) of these proteins nevertheless remains obscure: data from several laboratories now suggest that in addition to their putative role in mediating transbilayer flip/flop of membrane lipids, the PLSCRs may also function to regulate diverse processes including signaling, apoptosis, cell proliferation and transcription.
A major impediment to deducing the molecular details underlying the seemingly disparate biology of these proteins is the current absence of any representative molecular structures to provide guidance to the experimental investigation of their function.
Results: Here, we show that the enigmatic PLSCR family of proteins is directly related to another family of cellular proteins with a known structure.
The Arabidopsis protein At5g01750 from the DUF567 family was solved by X-ray crystallography and provides the first structural model for this family.
This model identifies that the presumed C-terminal transmembrane helix is buried within the core of the PLSCR structure, suggesting that palmitoylation may represent the principal membrane anchorage for these proteins.
The fold of the PLSCR family is also shared by Tubby-like proteins.
A search of the PDB with the HHpred server suggests a common evolutionary ancestry.
Common functional features also suggest that tubby and PLSCR share a functional origin as membrane tethered transcription factors with capacity to modulate phosphoinositide-based signaling.
Contact: agb@sanger.ac.uk 1 INTRODUCTION Biological membranes have an asymmetric distribution of lipids.
A variety of proteins have been identified which create and maintain this distribution.
Phospholipid scramblase (PLSCR) proteins were initially identified as being able to mediate the collapse of this To whom correspondence should be addressed.
asymmetric distribution, by accelerating transbilayer movement of membrane phospholipids in response to elevated [Ca2+].
This activity was first purified in the human PLSCR1 protein (Zhou et al., 1997), named PLSCR1 for phospholipid scramblase 1.
Subsequently, further members of the gene family have been identified in the human genome (Wiedmer et al., 2000).
More recently members of this family have been shown to have a role in cellular signaling.
When palmitoylated, PLSCR1 was shown to be raft-associated, to physically interact with ligand-activated EGF receptors, and to serve as an adapter to promote interaction between c-Src, Shc and the EGFR receptor kinase, thus serving to enhance receptor transactivation (Sun et al., 2002).
In rat, PLSCR1 has been shown to be involved in mast cell activation through a Lyn-dependent pathway (Amir-Moazami et al., 2008) with PLSCR2 possibly playing an antagonistic role in mast cell activation (Hernandez-Hansen et al., 2005).
It has also been shown that PLSCR1 when not palmitoylated is avidly imported into the nucleus by the importin-/ nuclear chaperones, where it functions as a DNA-binding protein with transcriptional activity (Ben-Efraim et al., 2004).
One identified gene target of nuclear PLSCR1 is the inositol 1,4,5-trisphosphate receptor type 1 gene (IP3R1), nuclear PLSCR1 serving to increase both IP3R1 transcription and protein expression (Zhou et al., 2005).
Mouse knockouts of each of PLSCR1 and PLSCR3 have been studied.
Deletion of PLSCR1 affects myelopoiesis in response to G-CSF, causing a defect in emergency granulopoiesis (Zhou et al., 2002).
In contrast, deletion of PLSCR3 in mouse gives rise to abnormal triglyceride and cholesterol accumulation in white fat, with subsequent manifestations of the metabolic syndrome (Wiedmer et al., 2004).
Deletion of PLSCR orthologues in Drosophila was found to cause abnormality in flight that was apparently related to a defect in motor neurons affecting the distribution and size of neutransmitter storage vesicles at the synapse (Acharya et al., 2006).
PLSCR3 and other members of the PLSCR family have also been reported to be mitochondrial proteins, and proposed to mediate both lipid transport and pro-apoptotic functions (Liu et al., 2003a, b; Van et al., 2007).
The seemingly disparate and enigmatic biology that has been observed for the PLSCR family of proteins has to date precluded the 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
A.Bateman et al.
development of a logically consistent molecular model for how these highly conserved and presumably related proteins might collectively function in the cell.
In addition to the disparity of molecular interactions and biologic functions that have been ascribed to the PLSCR proteins, a unifying concept of the functional properties of the PLSCR family has also been hindered by the absence of any solved or predicted molecular structure for its members, precluding an attempt to gain insight into protein function from protein structure.
Whereas site-directed mutagenesis has revealed selected residues that are either targets of post-translational modification (thiolesterification and phosphorylation), or that are required to observe PLSCR1 binding to lipid, proteins or DNA, there is virtually no information as to how the functional domains might be organized, or how their 3D structure might be related to other proteins with defined function.
In order to gain new insight into the potential structure of the PLSCR protein family, we undertook extensive similarity searches to identify structural homologues for the phospholipid scramblase family.
2 METHODS Profileprofile comparisons were carried out using the Pfam database of protein families (release 23).
The profile for the Scramblase family (accession PF03803) was compared with the 10 339 other family profiles using the SCOOP software (Bateman and Finn, 2007) and the PRC software using default parameters.
The remote homology detection server HHpred (Soding, 2005) was used to search for homologs of human PLSCR1 in the Protein Data Bank with default parameters.
An alignment of human PLSCR1 with the DUF567 family member At5g01750 from Arabidopsis thaliana (PDB-code: 1zxu) was generated with HHpred (Soding, 2005) using its maximum accuracy (MAC) alignment algorithm (Soding et al., 2005) and adjusted manually.
Structural models were built with the Modeller software (Sali and Blundell, 1993) with default parameters.
Loops were modeled with the MODloop server (Fiser and Sali, 2003).
Verify3d (Luthy et al., 1992) was used to check the model quality.
3 RESULTS Profileprofile comparisons provide a powerful way to link homologous protein families.
Using the Pfam database of protein families we were able to scan for similarities of the Scramblase family to over 10 000 other protein families.
A potential relationship of the Pfam Scramblase family to the uncharacterized Pfam protein family DUF567 was initially identified with the SCOOP software (Bateman and Finn, 2007) with a score of 47.7.
The next highest scoring match was DUF512 with a score of 15.3 which is not considered significant.
In addition the PRC (Profile Comparer) software identified the relationship between the same two Pfam families with an E-value of 0.0267.
The next highest scoring match was to the RdRP_2 family (PF00978) with a non-significant E-value of 2.39.
We also removed the N-terminal proline-rich stretch of 70 amino acids from human PLSCR1 and searched through the Protein Database with the HHpred server (Soding et al., 2005).
The first match was to At5g01750 (PDB-code: 1zxu), a member of the DUF567 family from A.thaliana.
At E-value 2e-11 and probability for homology of 99.5% this result is strongly indicative of a homologous relationship.
The second match was again to 1zxu, but with a different alignment, indicating the presence of internal sequence symmetry (E-value 0.04, 95%).
The third hit was to Tubby-related protein 1 (PDB: 2fim_A), with E-value 14 and probability for homology of 42%.
The DUF567 family, defined by the Pfam database (Finn et al., 2008), contains only uncharacterized proteins.
It is found in plants, fungi, eubacteria and some archaea.
Within the plants DUF567 is greatly expanded, with A.thaliana containing 21 members of the family.
Despite being functionally uncharacterized, the family has had a representative structure solved by the CESG structural genomic project (http://www.uwstructuralgenomics.org/) for the Arabidopsis protein At5g01750 (UniProt:Q9LZX1) (PDB-code: 1zxu).
Although no function is known for this protein, taking its sequence and PSI-BLAST searching (Altschul et al., 1997) it against the NR database at NCBI with default E-values we find that by round four several members of the Scramblase family are identified, including PLSCR1, PLSCR3 and PLSCR5.
This confirms that the scramblase family is related to the DUF567 family and therefore identifies a structural template for modeling of the scramblases.
An alignment of representatives of the scramblases to the DUF567 family is shown in Figure 1.
The structure of At5g01750, and therefore by similarity the Scramblase family, is a 12-stranded-barrel that encloses a central C-terminal-helix, see Figure 2.
This C-terminal helix is thought to be a transmembrane helix.
However, the structural model suggests that the hydrophobic nature of this helix is due to its packing in the core of the protein domain and that this is not a transmembrane helix.
It can be seen that most of the sequence conservation lies within the secondary structures as well as the-hairpin turns between strands 23 and 45 providing further support for the model.
The structural model allows an improved understanding of previous results.
For example, the DNA-binding motif defined by deletion experiments (Zhou et al., 2005) was identified as spanning residues 86118 in human PLSCR1.
This deletion would have removed the first-strand of the domain potentially leading to misfolding of the-barrel with observed loss of DNA binding.
Thus, it remains unresolved whether the observed results from truncation mutation identify the actual DNA-binding residues in PLSCR1, or, cause misfolding of the DNA-binding motif.
Also it has been hypothesized that PLSCR contains an EF-hand like Ca2+-binding motif (D273D284), and mutation of select residues within that motif were observed to abrogate both Ca2+ binding to PLSCR1 and the expression of its Ca2+-dependent PL scramblase activity (Zhou et al., 1998).
The proposed motif overlaps with one of the core-strands of the-barrel and thus is structurally incompatible with an EF-hand structure.
As well as providing a structural model for the scramblases, the structure of At5g01750 shows the same fold as found in the C-terminal domain of the Tubby protein (Boggon et al., 1999).
A possible homologous relationship between the tubby-like proteins and phospholipid scramblases is indicated by the fact that a tubby family protein (PDB-code: 2fimA) appears as second best match in a database search of PLSCR1 through the PDB using HHpred, albeit with marginal statistical significance.
The tubby family of proteins is only found in eukaryotic species, whereas the scramblase/DUF567 family are found in eukaryotes and eubacteria, which suggests that the scramblases have a more ancient evolutionary origin.
We hypothesize that the tubby family of proteins evolved from an ancestral scramblase-like protein.
The scramblase structure appears to have fewer elaborations between the-strands compared with the tubby family, which also suggests their more basic ancestral nature.
160 PLSCRs and tubby-like proteins Fig.1.
Multiple alignment showing members of the scramblase and DUF567 families (upper block) and the tubby-like family (lower block).
Members within each family were aligned with the multiple alignment program PROMALS (Pei and Grishin, 2007).
The scramblase and DUF567 sequences were aligned with each other using HHpred (Soding, 2005) in local MAC alignment mode (Soding et al., 2005), while keeping the family alignments frozen.
The resulting alignment was merged with the alignment of tubby-like proteins by using the structural alignment of 1zxu and 1c8z from TMalign (Zhang et al., 2005) as a guide and again keeping sub-alignments frozen.
Red boxes represent regions that are part of the structural alignment.
Alignment columns for which scramblase and tubby family members exhibit similar amino acids are colored according to the chemical nature of the residue class.
Various sequence features of the PLSCR1 sequence are indicated by colored, bold letters: transcriptional activation domain (magenta), Cysteine palmitoylation motif (orange), non-classical nuclear localization signal (green), Ca2+-binding motif (blue), predicted transmembrane helix (red).
If scramblases and tubby proteins are distantly related, we might expect to find some common functional features.
There are a number of interesting similarities between the two families of proteins that support an evolutionary connection.
First, most members of the scramblase and tubby family possess an N-terminal stretch of 100250 natively unfolded residues that might function as activation domains or proteinprotein interaction domains.
Both scramblases and tubby are localized to the inner side of the cell membrane, 161 A.Bateman et al.
Fig.2.
3D structural model of PLSCR1 computed by homology modeling.
PLSCR1 forms a closed, symmetric-barrel of 12-strands wrapped around a very hydrophobic C-terminal helix.
Various sequence features of PLSCR1 are highlighted in color: transcriptional activation domain (magenta), Cysteine palmitoylation motif (orange), non-classical nuclear localization signal (green), Ca2+-binding motif (blue), predicted transmembrane helix (red).
in the case of scramblases by palmitoylation and in the case of Tubby by phosphoinositol binding.
In both cases these localizations appear to be reversible and disruption of the membrane binding gives rise to a nuclear localization.
Members of each family have been shown to be DNA binding (Boggon et al., 1999; Zhou et al., 2005), although no specific target has yet been identified for Tubby.
Figure 1 shows two positions that conserve positively charged residues between the scramblases and tubby proteins, one between strands 3 and 4 and one at the end of strand 11.
These two positions are both at the same end of the barrel and might form part of a DNA-binding site.
One of the binding targets of PLSCR1 is the promoter of the inositol 1,4,5-trisphosphate receptor.
It is notable that calcium release mediated by inositol 1,4,5-trisphosphate is part of the signaling cascades mediated by this protein.
Furthermore, it has been shown that Tubby can bind to phosphatidylinositol 4,5-bisphosphate.
We tentatively suggest that scramblase genes may also share inositol polyphosphate-binding activity.
It is also possible that tubby proteins may possess scramblase activity.
4 CONCLUSIONS In conclusion, we have demonstrated a structural model for the family of scramblase proteins and have identified an evolutionary link with the tubby-like proteins.
The members of this new superfamily are both endofacial plasma membrane and nuclear distributed, with distinct plasma membrane and nuclear functions.
Nuclear trafficking of these proteins requires liberation from membrane attachment and import by nuclear chaperones, and results in binding to chromatin and altered gene transcription.
In addition to common structural ancestory, the possibility of conserved function as modifiers of polyphosphoinositide-based cell signaling is also suggested.
Funding: This work was supported by the Wellcome Trust [grant number WT077044/Z/05/Z]; Heart, Lung, Blood Institute, National Institutes of Health (HL036946, HL063819 and HL076215 to P.J.S.
and T.W.).
Conflict of Interest: none declared.
ABSTRACT Motivation: As ArrayExpress and other repositories of genome-wide experiments are reaching a mature size, it is becoming more meaningful to search for related experiments, given a particular study.
We introduce methods that allow for the search to be based upon measurement data, instead of the more customary annotation data.
The goal is to retrieve experiments in which the same biological processes are activated.
This can be due either to experiments targeting the same biological question, or to as yet unknown relationships.
Results: We use a combination of existing and new probabilistic machine learning techniques to extract information about the biological processes differentially activated in each experiment, to retrieve earlier experiments where the same processes are activated and to visualize and interpret the retrieval results.
Case studies on a subset of ArrayExpress show that, with a sufficient amount of data, our method indeed finds experiments relevant to particular biological questions.
Results can be interpreted in terms of biological processes using the visualization techniques.
Availability: The code is available fromContact: jose.caldas@tkk.fi 1 INTRODUCTION The most common approach for searching in microarray databases is based on metadata such as annotations and descriptions of the arrays and genes (Zhu et al., 2008).
Such searches are naturally useful, given that the quality of annotations is good, the search task is reasonably well focused to match the capabilities of the search engine and, most importantly, known things are sought for.
The annotations can naturally only contain known things.
An alternative is to search with an interesting gene or gene set as the query, resulting in datasets where the query genes are correlated (Hibbs et al., 2007) or differentially expressed (Parkinson et al., 2009).
In this work, we develop methods for performing searches having an experiment as the query.
The simplest method would be content-based search, where the query would be one microarray and the set of most similar microarrays would be retrieved (Fujibuchi et al., 2007; Hunter et al., 2001).
The obvious problem is how to choose the distance measure, with which the similarity of the expression profiles will be assessed.
To whom correspondence should be addressed.
The search problem is related to the natural suggestion that analysis of a new dataset would benefit from putting it in the context of all earlier datasets (Tanay et al., 2005).
In that study, the authors develop a method for extracting a set of biclusters from earlier studies and evaluating the activity of those biclusters in a new experiment.
In another holistic analysis paper (Segal et al., 2004), a module map of gene modules versus clinical conditions was formed by first finding differentially expressed gene sets, then combining them into modules and finally identifying modules differentially expressed over a set of arrays having the same annotation.
More recently, a tool called the Connectivity Map was developed for relating diseases and chemicals via common gene expression profiles (Lamb et al., 2006).
These ideas can naturally be extended by incorporating more biological knowledge into the model, for instance in the form of regulatory networks, partly assumed and partly learned from data.
Of course, the computational complexity will increase accordingly.
What we would like to do is to take the idea of extracting information about biological processes from the gene expression compendium, and to use it in the search process to focus the search on biologically relevant things.
This we would like to do in an at least partly data-driven way, in order to be able to find unexpected things in addition to the already known things available for metadata searches.
Moreover, out of all potentially biologically relevant things, we would like to focus on the ones that were differentially activated as a result of the experimental setup.
Finally, the models used for the compendium need to be reasonably simple to keep the searches scalable, but they still need to be able to extract relevant things.
We will need four elements to make the searches successful: (i) a model for the activity of biological processes across the compendium, which should be able to make the miscellaneous experiments and data types stored in the database commensurable, (ii) a way of performing searches given the model, having one experiment as the query and (iii) ways of visualizing the search results.
As an additional insight we would like to ensure that (iv) the retrieved experiments would be relevant in the sense that the same biological processes were activated by the experimental treatment in them, as in the query experiment.
For (i), we would like to specify the model such that it will both incorporate some prior knowledge about biological processes and learn new things from data.
Both steps need to be simple to keep the computational load manageable.
We chose the simplest form of prior biological knowledge available, used in some of the earlier holistic analyses as well: gene sets extracted from earlier analyses.
The gene sets will be incorporated by using Gene Set 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:05 15/5/2009 Bioinformatics-btp215.tex] Page: i146 i145i153 J.Caldas et al.
Enrichment Analysis (GSEA) (Subramanian et al., 2005) in a new way.
Each experiment, both the query and the earlier ones in the compendium, will be encoded as a vector containing the number of differentially expressed genes in each set.
This step makes the different experiments commensurable.
Moreover, when the differential expression is measured for the main experimental variable compared to control, the encoding focuses on the changes each experiment targeted [item (iv) above].
We use the so-called topic models or discrete principal component analysis (Blei et al., 2003; Buntine and Jakulin, 2004), which have earlier been successfully used in textual information retrieval.
The topic models are suitable for finding latent components from count data, such as texts assumed to be bags of words.
Being probabilistic models they can infer the underlying components taking the uncertainty in the data properly into account.
For gene expression, we change the counts of words to counts of differentially expressed genes in gene sets, one word type corresponding to one gene set.
Each experiment will then correspond to an activity profile over the components, and each component will correspond to a distribution over the gene sets.
The differences from earlier applications of topic models to discretized genomic data (Flaherty et al., 2005; Gerber et al., 2007) are the use of gene sets to bring in biological knowledge, focusing to effects elicited by the experimental treatments and the application to retrieval.
Given a topic model, there are well-justified methods for doing information retrieval (Buntine and Jakulin, 2004; Griffiths and Steyvers, 2004) for texts, where the query is simply another document.
The same principles apply for querying with a new experiment here, and we borrow them for item (ii).
Finally, we will need to visualize both the components to interpreting the biological findings, and the retrieval results in order to be able to browse the collection.
We will both apply earlier methods (Venna and Kaski, 2007) and develop new ones.
2 METHODS 2.1 Gene expression dataset We obtained 288 pre-processed human gene expression microarray experiments from the ArrayExpress database (Parkinson et al., 2009).
By an experiment, we mean a set of microarrays from a particular paper.
Each experiment is associated with a collection of experimental factors describing the variables under study, e.g.
disease state or gender.
Each microarray in an experiment takes on a specific value for each of the experimental factors, e.g.
disease state = normal and gender = male.
We have focused on experiments having the experimental factor disease state, and decomposed them into sub-experiments, or comparisons, of healthy tissue against a particular pathology.
This yielded a total of 105 comparisons that included a wide range of pathologies such as several cancer types, as well as neurological, respiratory, digestive, infectious and muscular diseases (although the only significantly frequent broad category was cancer, with 27 comparisons).
We also systematically transformed the remaining experiments in the dataset into collections of simpler comparisons.
For each experimental factor in an experiment, we chose to compare either two values of that experimental factor (e.g.
disease A versus disease B), or one value versus all others (e.g.
control versus all treatments).
In experiments with more than one experimental factor, the factors whose values are not being compared provide a context for the comparison.
For example, when comparing two values of disease state, e.g.
normal versus cancer, we can get different comparisons for gender = male and for gender = female.
For each comparison, we generated all possible combinations of contextual factors.
We kept all comparisons that had at least six microarrays assigned to each phenotype, mapping probesets to HUGO gene symbols (Eyre et al., 2006) and collapsing equivalent probesets by their median.
The total number of obtained comparisons, including the 105 healthy versus disease comparisons mentioned above, was 768.
2.2 Topic model 2.2.1 GSEA this tests if a set of genes is coordinately associated with the difference between two phenotypes in a microarray experiment.
Here we give a very brief description; for more details and for software we used, see the original papers (Mootha et al., 2003; Subramanian et al., 2005).
GSEA starts by computing a ranked list of the genes in the experiment, according to how well each gene discriminates between the two phenotypes.
This can be achieved by using metrics such as fold change or signal-to-noise ratio.
Then, a weighted KolmogorovSmirnov (KS) running statistic, deemed the enrichment score (ES), is computed over the list.
The ES, after normalization, is used to compute significance measures such as the family-wise error rate and the false discovery rate (FDR) q-value.
The computation of the statistic also produces a subset of the genes in the set.
That subset, called the leading edge subset, constitutes a tentative core for the gene set.
We used GSEA to bring in biological knowledge in the form of the pre-defined gene sets.
In effect, we quantified the differential expression within each set as a count.
In brief, a sub-experiment essentially consists of a collection of microarrays that is divided into two sample categories, or phenotypes.
Designate those phenotypes, respectively, by A and B.
In order to assess which gene sets were differentially expressed in either of the phenotype switching directions AB and BA, we ran GSEA for both switching directions.
The gene sets whose enrichment was assessed were taken from the Molecular Signatures Database (Subramanian et al., 2005); in particular, we used a collection of canonical, manually compiled pathways (collection C2-CP).
We collapsed the results from both GSEA runs together, sorting gene sets according to the magnitude of their normalized ES (NES).
We then collected the 50 gene sets with the highest absolute NES.
This choice was motivated by previous observations that several gene sets that do not reach a standard FDR q-value of 0.25 are still effectively relevant to the condition under study, and that these are overall consistent among laboratories conducting similar microarray experiments (Subramanian et al., 2005).
Finally, we obtained the size of the leading edge subset of each of those 50 gene sets.
For each comparison, running the above procedure generates a collection of significant gene sets, each associated with an integer value (the size of its leading edge subset for that particular comparison).
This representation can be seen as analogous to the so-called bag-of-words model for text documents.
In textual information retrieval, it is common to represent a document by how many times each word in the vocabulary appears in that document.
The order of the words is therefore omitted, and hence the name bag-of-words.
The procedure described above effectively generates a bag-of-words representation for each comparison in the dataset.
This allows us to conceptually regard each comparison as a document having several words from a vocabulary.
In our context, the vocabulary is the collection of canonical pathways, and each gene set found to be significant is a word.
In essence, the above procedure generates a representation of differential expression that is amenable to probabilistic modeling with topic models, and for topic model-based information retrieval tools.
2.2.2 Topic models These are probabilistic unsupervised models for finding latent components in documents, alternatively called Latent Dirichlet Allocation (LDA; Blei et al., 2003) or discrete Principal Component Analysis (dPCA; Buntine and Jakulin, 2004).
Provided a corpus in bag-of-words representation, it models each document as a probability distribution over so-called topics.
A topic, the central concept, is itself a probability distribution, but over words in the vocabulary.
The model is a generative hierarchical model, which can be specified by formulating the generative process from i146 [10:05 15/5/2009 Bioinformatics-btp215.tex] Page: i147 i145i153 Retrieval of relevant experiments which the data are assumed to arise.
More formally the generative process goes as follows: the distribution over topics for each document d, and the distribution over words for each topic t, are specified, respectively, by the random variables (i.e.
parameters of a hierarchical model) d and t , d Dirichlet(), t Dirichlet().
Here and are scalar hyperparameters for symmetric Dirichlet probability distributions, and they regulate the sparsity of the model.
Each word is assumed to come from exactly one topic.
For word i in document d, a topic is chosen using the documents topic probability distribution.
This amounts to sampling from a scalar variable zd,i, zd,i|d Multinomial(d ).
After choosing a topic zd,i, the corresponding word wd,i is sampled from the topics distribution over words, wd,i|zd,i,zd,i Multinomial(zd,i ).
The above formulation corresponds to a variant by Griffiths and Steyvers (2004).
Topic models have been successfully used in several text modeling applications; in bioinformatics, they have been used at least for finding components of haploinsufficiency profiling data (Flaherty et al., 2005) and of discretized gene expression data (Gerber et al., 2007).
We use topic models to model the experiments that have been pre-processed by GSEA.
The relationship to text document modeling is that we are conceptualizing each experiment as a document.
In this conceptualization, each word is a gene set, and each topic is a probability distribution over gene sets.
A topic aims at representing a biological process.
It specifies an ordering on gene sets, the ordering meaning how likely it is that a gene set is differentially expressed.
By considering the top gene sets in a topic, one can obtain a biological picture that is broader and more holistic than the one described by a single gene set.
Finally, by having a probability distribution over topics, a comparison effectively assigns different weights to biological processes.
In the remainder of the article, we will use the terms experiment and document, as well as gene set and word interchangeably.
In the models we chose the hyperparameters to be at =1 and =0.01, and fixed the number of topics at T =50.
For computing the models we used the same approach as Griffiths and Steyvers (2004).
We used so-called collapsed Gibbs sampling to find assignments of the words of each document to the topics, by first analytically integrating out the parameters and to the obtained joint probability of the corpus and the word-to-topic assignments, P(w,z)= P(w,z, ,)d d. The values of the z were then sampled by Gibbs sampling, from the conditional probability distribution P(zd,i|z(d,i),w), where z(d,i) is obtained by discarding zd,i from z.
We sampled iteratively for a total of 2000 scans.
On an Intel 1.73 GHz Core 2 Duo CPU, this took about 23 min.
Computations were performed using the Topic Modeling Toolbox (http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm).
We repeat the procedure for a total of eight parallel samplers.
Out of the samples, we chose for interpretation the sample having the highest probability, and estimated the parameter values and based on the assignments of words to the topics.
The formulas for the conditional distribution, variable estimation and estimate selection are omitted for brevity.
2.3 Probabilistic search The topic model represents each experiment as a distribution over topics.
It is then natural to measure similarity of experiments in terms of distances between their distributions over the topics.
Suitable distance measures for distributions include the (symmetrized) KullbackLeibler divergence, JensenShannon divergence or Hellinger distance; unfortunately all of these have problems with sparsity, which necessarily results when the dimensionality is high.
The most straightforward way of retrieving experiments, given a new experiment as a query, would be to rank the documents to be retrieved according to their distance from the query.
There is, however, a more natural and well-performing way of doing information retrieval in a probabilistic model such as this one (Buntine and Jakulin, 2004).
Essentially, we compute the probability that the gene sets in a query experiment were generated by another experiment.
In more precise terms, this amounts to computing P(wq|d )= wwq T t=1 d,tt,w, where wq is the collection of words in a query experiment q and T is the number of topics in the model.
The above equation states that, for each word in the query, we compute the overall probability that it was generated by any topic, given the topic proportions in the potentially relevant experiment.
By repeating the same query for all experiments, we obtain a ranked list that is ordered by the relevance of each experiment to that query.
The computation of all queries took <5 s. 2.4 Visualization 2.4.1 Relationship between comparisons, topics and gene sets Visualization of the topic model is essential to understand the biological findings of our analysis.
We want to gain insight into the structure of our gene expression compendium and the biological processes recorded in it.
In order to do so we need to examine the topic composition of the experiments as well as the gene set composition of the topics.
The results obtained from GSEA and the topic model are essentially two matrices Pt and Pg containing the topic probabilities across the experiments and the gene set probabilities across the topics.
The connection between Pt and Pg are the topics.
Accordingly, we can consider the matrices a disjoint union of two complete bipartite graphs where the probabilities in the matrix represent edge weights.
We layout the resulting graph by placing the nodes for experiments, topics and gene sets in three separate columns, where the middle column contains the nodes for the topics and is shared by the two subgraphs.
We have to select a subset of edges for the visualization since the two bipartite graphs are complete.
Rather than making a hard selection, we use a reduced line width and color opacity of the edges based on the corresponding weights.
With this strategy we emphasize those edges representing a high probability and virtually remove those standing for lower probabilities.
Each topic is assigned a distinct color and all edges connecting the topic are drawn in this color.
This makes it easier for the viewer to follow the edges from the topic to the corresponding experiments or gene sets.
At the same time the links having a particular color are easily distinguishable and provide an overview interpretation of that particular topic, in terms of both its distribution over gene sets and over experiments where this topic plays a role.
Clutter is reduced by rearranging gene sets and topics so that the number of intersecting edges is low.
We found that a suitable heuristic for achieving this is to compute a complete linkage hierarchical clustering of the gene sets and of the experiments to obtain a partial ordering for both.
As a distance measure, we used the symmetrized KullbackLeibler divergence between the corresponding distributions.
Further we sort the topics by the index of the maximum value in the corresponding column of Pg.
Additionally, we use Bzier curves instead of straight lines to connect topics with experiments and gene sets.
The Bzier curves form edge bundles, which further reduces clutter.
In order to increase the space available to plot experiment and gene set names, we plot them circularly instead of along a straight line.
Figure 1 shows the resulting visualization.
The complete visualization is readable on an interactive display; to keep it readable also on paper, we selected a subset of topics for which the sum of probabilities given the i147 [10:05 15/5/2009 Bioinformatics-btp215.tex] Page: i148 i145i153 J.Caldas et al.
Fig.1.
Visualization of the topic model.
A subset of 13 topics, 211 gene sets and 105 experiments is shown.
For details and a discussion see the text.
i148 [10:05 15/5/2009 Bioinformatics-btp215.tex] Page: i149 i145i153 Retrieval of relevant experiments Fig.2.
The experiment collection visualized as glyphs on a plane.
Topic colors in all glyphs match topic colors in Figure 1.
(A) NeRV projection of the 105 experiments, each shown as a glyph.
(B) The slices of each glyph show the distribution of topics in the experiment.
The experiment labels are from left to right: asthma, Barretts esophagus and high-stage neuroblastoma.
(C) Enlarged region from (A) where glyphs have additionally been scaled according to their relevance to the query with the malignant melanoma experiment shown in the center.
A detailed description of this experiment is included in Section 3. documents is the highest.
In detail, we selected the top 10 topics in the subset of the 105 main experiments and top 10 topics in the completed dataset, and took the union, resulting in a set of 13 topics.
We additionally reduced the number of gene sets on the visualization by choosing the most probable 25 for each topic, and taking the union over all topics.
Based on a quick inspection, the probabilities typically leveled off beyond the 25.
This gave in total 211 gene sets for the visualization of the 13 selected topics.
2.4.2 Visualizing retrieval results To complement the standard ranked lists, retrieval results can be presented on a projection display showing all the data items.
Assuming that the projection is good, the display is useful in putting the retrieval result into the context of the whole set of experiments.
Clusters and outliers in the retrieval results may become obvious, results of different queries can be easily compared, and the whole collection can be interactively browsed while simultaneously seeing the retrieval results.
To visualize retrieval results, we project all experiments to a two-dimensional display using a new projection method that has recently been shown to outperform the alternative methods, in the task of retrieving similar data points (here experiments) given the display.
The method called Neighbor Retrieval Visualizer (NeRV; Venna and Kaski, 2007) has been developed specifically for visualizing data in retrieval tasks and for explorative information visualization.
NeRV needs to be given the relative cost of misses and false positives of the true similarities between the data points.
We chose to penalize false positives, resulting in a display that is trustworthy in the sense that if two points are similar in the visualization they can be trusted to have been similar before the projection also.
As other multidimensional scaling methods, NeRV starts with a pairwise distance matrix between all experiments.
In this article, we used the symmetrized KullbackLeibler divergences between the topic distributions of the documents.
The pure projection of the experiments shows only their relative similarity, and for further interpretation the display needs to be coupled with the topic content of the documents.
It is possible to include this important information by including glyphs in the projections to represent the distribution of topics (Yang et al., 2007).
Including the glyphs has the additional advantage that a non-linear projection of a large dataset to a two-dimensional space cannot preserve all similarities, and the imperfectnesses will be detectable based on the glyphs.
We designed glyphs to represent the probability distribution over the topics of a document by dividing a square into vertical slices that each stand for a topic.
The width of the slice represents the probability of the topic.
This is illustrated in Figure 2B in the top row.
While this is sufficient for comparing the shape of the probability distributions of documents, we also color the strips with a distinct color representing the topic, as shown in Figure 2B in the bottom row.
The coloring has the additional distinctive purpose that it connects the topics of the glyphs visually with the same topics in the display of Figure 1, which can be used for interpreting them.
3 RESULTS 3.1 Inferred topics By analyzing the most probable gene sets for each topic, we can infer its underlying biological theme.
The most probable gene sets in most of the topics learned by the model are coherent, and the topics taken together describe a wide range of processes.
We focus our analysis on the same most prominent topics shown in the visualizations, based on their sum of probabilities over documents being the highest.
The top five gene sets for each of the 13 topics are shown in Table 1. i149 [10:05 15/5/2009 Bioinformatics-btp215.tex] Page: i150 i145i153 J.Caldas et al.
Table 1.
Top five gene sets for the 13 most probable topics 2 5 11 Cell cycle (BIOCARTA) Purine metabolism (KEGG) G protein signaling Cell cycle (KEGG) Pyrimidine metabolism (KEGG) Biopeptides pathway G1 to S cell cycle (REACTOME) Purine metabolism (GENMAPP) NFAT pathway DNA replication (REACTOME) Pyrimidine metabolism (GENMAPP) CREB pathway G2 pathway DNA replication (REACTOME) GPCR pathway 15 18 19 Gluconeogenesis Apoptosis (GENMAPP 1) Valine leucine and isoleucine degradation Glycolysis Apoptosis (KEGG) Propanoate metabolism (KEGG) Glycolysis and gluconeogenesis (KEGG) Apoptosis (GENMAPP 2) Fatty acid metabolism Glycolysis and gluconeogenesis (GENMAPP) Apoptosis (GENMAPP 3) Propanoate metabolism (GENMAPP) Fructose and mannose metabolism Death pathway Valine leucine and isoleucine degradation 24 26 27 IL2RB pathway mTOR pathway Hematopoietic cell lineage PDGF pathway Sphingolipid metabolism Complement and coagulation cascades EGF pathway eIF4 pathway Inflammation pathway Gleevec pathway RAS pathway NKT pathway IGF-1 pathway IGF-1 mTOR pathway Dendritic cell pathway 32 35 44 Epithelial cell signaling in H. pylori Infection Integrin pathway mRNA processing (REACTOME) Cholera infection (KEGG) Met pathway RNA transcription (REACTOME) Photosynthesis ERK pathway Translation factors ATP synthesis AT1R pathway Folate biosynthesis Flagellar assembly ECM pathway Basal transcription factors 50 Oxidative phosphorylation (KEGG) Oxidative phosphorylation (GENMAPP) Glycolysis and gluconeogenesis IL-7 pathway Gamma hexachlorocyclohexane degradation An acronym for the source of the gene set was included either to distinguish between gene sets with similar names, or when the gene sets name already includes a mention of that source [KEGG (Kanehisa and Goto, 2000), GENMAPP (Salomonis et al., 2007), BIOCARTA (http://www.biocarta.com) or REACTOME (Vastrik et al., 2007)].
The topics are related to diverse themes such as cell cycle (topic 2), DNA replication (topics 2 and 5), organic compound metabolism (topics 5 and 19), G protein signaling (topic 11) glycolysis (topic 15), apoptosis (topic 18), cell growth and proliferation (topics 24 and 26), cell differentiation (topic 27), infection (topic 32), cell communication (topic 35), DNA replication (topic 44) and oxidative phosphorylation (topic 50).
In some topics, some of the top gene sets are almost identical.
This stems from the fact that those gene sets are highly overlapping, therefore being put into similar topics with similar probabilities.
Although Table 1 is illustrative of the variety of topics found by the model, understanding each topic may require looking beyond the top five words.
For instance, in topic number 2, gene sets until the eighthposition are not deeply informative of the process the topic is representing, beyond the fact that it is related to cell cycle and DNA replication.
However, the gene set at the ninth position, ATR BRCA Pathway, contains a signaling system involving genes BRCA1 (breast cancer 1, early onset), and BRCA2 (breast cancer 2, early onset).
These genes are involved in the cellular response to DNAdamage, and their mutations have been found to increase breast cancer susceptibility (Tutt and Ashworth, 2002).
We investigated which experiments have the highest probability for this topic.
The top four results are for cancer-related comparisons: normal tissue versus sporadic basal-like breast cancer, vulvar intraepithelial neoplasia, breast carcinoma and esophageal carcinoma.
As the only two breast cancer experiments in the dataset appear among those four top experiments, these results indicate that topic number 2 has relevance not only simply for cell cycle and DNA replication, but also for breast cancer.
As another interesting example, we analyzed the top gene sets in topic number 44.
One of the gene sets corresponds to genes involved in folate biosynthesis.
Folate has an important role in DNA and RNA synthesis, and low folate levels are known to promote a number of pathologies (Au et al., 2009; Glynn and Albanes, 1994; Hoffbrand et al., 1968).
We again computed which experiments had the highest probability for this topic.
The top four results pertained to comparisons between normal tissue versus Crohns disease, chronic lymphocytic leukemia, and chronic myelogenous leukemia, as well as a comparison between patients with normal tissue and cancer patients with acute radiation toxicity.
Folate deficiency has been observed both in patients with Crohns disease (Hoffbrand et al., 1968) and in patients with leukemia (Au et al., 2009).
i150 [10:05 15/5/2009 Bioinformatics-btp215.tex] Page: i151 i145i153 Retrieval of relevant experiments Fig.3.
(A) Average Precision for cancer queries for the top 10 results.
Queries are sorted by the average precision given by the topic model.
Error bars represent the 99% confidence interval of the random permutation results.
(B) Interpolated average precision at 11 standard recall levels (given as percentages).
The solid line corresponds to our method; the dashed line corresponds to the baseline.
Once again, the model manages to assign experiments to meaningful topics and, moreover, is able to relate experiments according to the mechanisms shared between them.
The assignment between topics and experiments is not disjoint as in clustering, assumed to underlie even smoothed clustering, but instead each experiment can genuinely belong to several topics.
The previous two examples illustrate that the topic model is in fact finding topics that correspond to meaningful biological processes.
By combining gene sets into topics, a holistic model of the differential activation of biological processes is created.
Our approach also seems to be robust, as the topic model was inferred from a collection of experiments from different sources, and as the above examples show, similar comparisons from different laboratories and samples do seem to match to the same biological processes.
We point out that the methods we combined are themselves robust, GSEA against laboratory and sample variations, and the topic model against noise in the input data.
3.2 Visualizing the model A major strength of our topic model visualization in Figure 1 is that it connects gene sets to experiments while making the connection by compressing the relationships through the topics.
This enables us to interpret topic distributions of experimentsand thus experiments themselvesefficiently.
Furthermore, the visualization allows us to begin the exploration of the model with an experiment, a topic or a gene set.
The larger structure of the model becomes evident immediately as well, namely that topics hardly ever share their top gene sets, while topics are shared across experiments with similar probabilities quite frequently.
We can also observe that some experiments have what we could call a primary topic that is indicated by a wider-than-average edge connecting the experiment to a topic.
For instance, in Figure 1 we can identify an instance of a high-stage neuroblastoma experiment where topic 19 seems to be the primary topic.
The glyph on the right in Figure 2B confirms this.
The visualization also reveals how gene sets are distributed across topics and that there is a range of different distributions.
We find that for example topic 50 has very high probabilities for two gene sets and much lower probabilities for the remaining gene sets, while topic 24 has rather uniformly distributed probabilities for a wide range of gene sets.
Figure 2A shows a NeRV projection of the experiments including glyphs describing the probability distribution over the top 13 topics.
While the visualization of the topic model in Figure 1 provides some insight into the structure of the experiment space, the projection immediately provides us with an overview of clusters and outliers.
We find only a few distinct clusters in our subset of 105 experiments, but this is not surprising given the range of biological questions that have been investigated in those experiments.
The glyphs reveal how topic usage is changing across documents and explain for instance which topics are shared by experiments forming a cluster.
The change in topic usage is gradual in most parts of the projection but seems abrupt in others.
This could indicate imperfectness in the projection where not all similarities have been preserved by the dimensionality reduction.
3.3 Searching for experiments We evaluated the performance of the method quantitatively in retrieving relevant experiments, given a query experiment.
For that purpose, we queried with cancer experiments and considered all other cancer experiments to be relevant, and all non-cancer experiments to be irrelevant.
We chose cancer because it had the largest number of experiments in our corpus and, more importantly, experiments from several laboratories and on different cancer types.
For other diseases, the number of experiments either was too small or came from a single larger experiment, making retrieval too easy.
In total, we queried the system with each of the 27 experiments comparing normal versus cancerous tissue.
As a result we obtained a ranked list of experiments, sorted by the probability of the query given the experiment and the model, as discussed in Section 2.
We computed the average precision, a standard summary statistic for evaluating retrieval performance, over the top 10 retrieved experiments.
To give a baseline, we additionally computed the average precision over randomly ranked results.
By randomizing 1000 times we get an estimate of the confidence intervals.
We also computed the average of the precisionrecall curves for all queries, for both our method and the random baseline.As shown in Figure 3A, in more than half the queries, the average precision is above 0.8, and in 20 of the 27 queries the topic model-based retrieval is above the confidence interval of the random baseline.
As seen in Figure 3B, the precisionrecall curve shows that the trade-off between precision and recall in our method is reasonable and well above the i151 [10:05 15/5/2009 Bioinformatics-btp215.tex] Page: i152 i145i153 J.Caldas et al.
Fig.4.
NeRV projection of the 105 experiments, portraying the outcome of querying the model with a melanoma experiment.
Both glyph size and color saturation encode the relevance of each experiment to the query.
The bigger the glyph and the more saturated the red the higher the relevance of the experiment to the query.
The query itself is represented by the biggest glyph.
random baseline.
We further studied the false positives in the top 10 ranked experiments, and on average 20% of those were found to be cancer related (e.g.
benign tumor), showing an advantage of our method over standard annotation-based searches and suggesting that the actual retrieval performance is actually better than the reported quantitative figures suggest.
We will finally illustrate the potential of the probabilistic relevance search with two case studies.
We first queried with an experiment comparing normal tissue against malignant melanoma.
The top two results were comparisons of normal tissue to bladder carcinoma and vulvar intraepithelial neoplasia.
The next two results were hyperparathyroidism and a study of intra-pulmonary airway epithelial cells from non-smokers versus current smokers.
The remaining top10 results were from comparisons of normal tissue against bladder carcinoma (twice), infiltrating ductal carcinoma, prostate cancer, breast carcinoma and esophageal adenocarcinoma.
It is clear that cancer experiments have a high preponderance in the top results, given the melanoma query.
Interestingly, a study of intra-pulmonary airway epithelial cells from smokers was included in the top results.
Although the annotation is not fully clear as to what the actual pathology is, it is plausible that it might be a cancer-related one.
This highlights the capability of our method for hypothesis generation.
Finally, it is known that hyperparathyroidism is associated with a higher cancer incidence (Nilsson et al., 2007), a relation that is highlighted by the melanoma query.
Figure 2C visualizes the topic distributions for experiments found relevant to the melanoma query.
The visualization not only highlights the most relevant experiments, but also the relation between them.
In particular, a subset of the carcinoma experiments appears to become separate from the glandular-related pathologies (esophageal adenocarcinoma, primary hyperparathyroidism and prostate cancer).
Alternatively, Figure 4, which is also a NeRV projection with glyphs, distinguishes the relevance of each experiment by changing the glyph size and color saturation accordingly.
As another case study, we queried with an experiment on myelogenous leukemia.
The top result was Crohns disease.
Although it is a digestive system disease, it has some commonalities with the query, as described in the previous section.
The second result was chronic lymphocytic leukemia.
Finally, the remaining results were on ischemic cardiomyopathy, post-traumatic stress disorder, multiple invasive and transitional cell carcinomas and chronic obstructive pulmonary disease.
Although the top 10 results span a large class of diseases, some of which are hard to connect to the query pathology, this case study highlights the fact that the method is capable of extracting meaningful top results, both by ranking a disease that is very similar to the query above other diseases that are broadly similar (chronic lymphocytic leukemia), and by ranking highly a disease which, although not immediately identifiable as similar to the query, effectively shares properties with it (Crohns disease).
4 DISCUSSION We have introduced methods for retrieval of relevant experiments, given a sample experiment as a query.
The retrieval is based on methods for modeling and visualizing differential gene set expression in a large body of gene expression microarray experiments.
The probabilistic model combines two recent approaches that have been shown to be effective.
The model was able to cluster gene sets into components, called topics, that exhibit a high biological coherence and that are meaningfully related to particular experiments.
The probabilistic nature of the model allows for a precise formulation of retrieval, in which the model is queried with the differential expression in gene sets of an experiment and it returns a ranked list of relevant experiments.
We showed that, querying the model with cancer experiments, we obtain a performance significantly better than random, measured by average precision.
More importantly, the average precisions were on average at the good value of 82%.
The random baseline is 40%.
We complemented the quantitative analysis with two case studies.
The model was able to associate melanoma with several cancer types.
We also demonstrated how the model finds hypothetical connections between experiments, by selecting an experiment of epithelial tissue in non-smokers versus current smokers as being highly relevant to cancer experiments, which naturally makes sense a posteriori.
We also showed that the model finds relations between Crohns disease and leukemia, and also between hyperparathyroidism and cancer, which were confirmed in the literature.
Finally, given a query experiment on leukemia, the model was able to extract, from a set of cancer experiments, precisely another leukemia experiment as being the most relevant.
The result indicates that the model not only manages to partition the experiments into general classes, but also allows going into finer-grained distinctions.
We also provide a concise visual description of one of those case studies, highlighting the consistence in topic distributions between similar experiments.
As for future work, there are two complementary directions.
In the current system, we intentionally kept the system simple and fast by using a simple way of bringing in prior biological knowledge and a reasonably simple probabilistic model.
Since already such a simple i152 [10:05 15/5/2009 Bioinformatics-btp215.tex] Page: i153 i145i153 Retrieval of relevant experiments system proved to be very useful in retrieving relevant experiments, it could next be scaled up to large experiment collections.
An alternative direction is to include more detailed models, making the retrieval results and analyses more accurate and informative but increasing computational complexity.
They should be useful for smallish collections.
Straightforward extensions are available by including recent methods that attempt to improve on GSEA (such as Oron et al., 2008).
On the other hand, there have been a wide variety of extensions of topic models over the recent years, for instance allowing topics to be correlated (Blei and Lafferty, 2007) or form a hierarchical structure (Blei et al., 2003).
ACKNOWLEDGEMENTS We would like to thank Misha Kapushesky for his expertise and advice, especially on converting microarray experiments into binary comparisons.
J.C., S.K.
and A.F.
belong to the Finnish Centre of Excellence on Adaptive Informatics Research.
Funding: TEKES (grant no.
40101/07); PASCAL 2 Network of Excellence, ICT 216886 (partially to J.C., A.F.
and S.K.).
Portuguese Foundation for Science and Technology (doctoral grant to J.C.); European Molecular Biology Laboratory (EMBL) (PhD fellowship to N.G.).
Conflict of Interest: none declared.
ABSTRACT Motivation: The classification of biological entities in terms of species and taxa is an important endeavor in biology.
Although a large amount of statements encoded in current biomedical ontologies is taxon-dependent there is no obvious or standard way for introducing taxon information into an integrative ontology architecture, supposedly because of ongoing controversies about the ontological nature of species and taxa.
Results: In this article, we discuss different approaches on how to represent biological taxa using existing standards for biomedical ontologies such as the description logic OWL DL and the Open Biomedical Ontologies Relation Ontology.
We demonstrate how hidden ambiguities of the species concept can be dealt with and existing controversies can be overcome.
A novel approach is to envisage taxon information as qualities that inhere in biological organisms, organism parts and populations.
Availability: The presented methodology has been implemented in the domain top-level ontology BioTop, openly accessible at http://purl.org/biotop.
BioTop may help to improve the logical and ontological rigor of biomedical ontologies and further provides a clear architectural principle to deal with biological taxa information.
Contact: stschulz@uni-freiburg.de 1 INTRODUCTION The classification of biological entities according to their morphological, genetic, evolutionary and functional characteristics is a fundamental organizing principle since Carolus Linnaeus established conventions for naming living organisms (Ereshefsky, 2001).
One century later, the distinction of species received its theoretical underpinning with Charles Darwins theory of evolution (1859) and was finally demystified by the spectacular advances of molecular biology in the late 20th century.
Although these changes have drastically challenged the basic assumptions of Linnaeus biological theory and have given rise to an ongoing debate about the concept of biological species and taxa Hey (2006), his main organizing principle remains the same.
All biology is, in some way, related to the concept of biological taxa.
Taxa are hierarchically structured labels or categories used for biological classification, such as species, family, class, etc.
All organisms, populations, tissues, cells, cell components and biological macromolecules that are under scrutiny of experimental or descriptive biologists are related to some hierarchy of taxa and most biological discoveries have their scope related to one To whom correspondence should be addressed.
species or taxon.
Table 1 gives an exemplary overview of the hierarchical order of taxa.
The basic taxon is the species.
Several species are grouped together by a genus.
Several genera constitute a family, several families an order, several orders a class and then several classes a phylum or division.
Finally, the top-most level, the kingdom distinguishes between animals and plants.
Similar to the several criteria that are discussed to delineate the concept of species, no clear principles exist that govern the division of superordinate taxa.
For instance, orders can be further split into superorders and suborders.
Even more, the number of taxonomic divisions is variable, and there are also divisions without rank name.
The importance of species and biological taxa is evidenced by many sources.
Biological taxa constitute 3497 out 24 766 descriptors of MeSH1, the indexing vocabulary of Medline.
In the Open Biomedical Ontologies (OBO) collection2 (Smith et al., 2007), 30 out of 66 ontologies are taxon specific, with taxa ranging from species such as Homo Sapiens or Caenorhabditis elegans, genera such as Plasmodium over families such as Poaceae to classes such as Mammalia.
Due to the sheer number of taxa there is no universal authoritative source, but every important subfield within biology has been independently maintained by curators, so-called systematists, and for a long time the field of biological systematics has been considered an important research discipline.
A converging effort in unifying taxon information for whole biology is the Catalogue of Life3 targeted for complete coverage of all 1.75 million known species by 2011.
In the mentioned OBO collection, nearly half a million taxon entries of medical interest is available in computer-processable form via the rapidly growing NCBI Taxonomy (Wheeler et al., 2008).
To sum up, biological taxa constitute an overarching and systematic ordering principle that is relevant in practically all biological subject areas.
In this article, we will show how the realm of biological systematics can be embedded into an ontological framework.
It is structured as follows: We start with a summary introduction of domain ontologies in general, as well as in the context of the biology, addressing the OBO ontologies and the BioTop biomedical top-domain ontology.
Then we provide a formal account of different aspects of the conceptualization of biological taxa and demonstrate how this is implemented in BioTop.
Finally, we briefly describe our tentative implementation supporting our claim that an overarching ontological framework for biology must have a conclusive and practical account of biological taxa.
1Medical Subject Headings, http://www.nlm.nih.gov/mesh 2Open Biomedical Ontologies, http://www.obofoundry.org 3Catalogue of Life, http://www.catalogueoflife.org 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[19:58 18/6/03 Bioinformatics-btn158.tex] Page: i314 i313i321 S.Schulz et al.
Table 1.
Biological taxa with examples Taxon (rank) Asian elephant Chimpanzee Drosophila Species Elephas maximus Simia troglodytes Drosophila melanogaster Genus Elephas Pan Drosophila Subfamily Drosophilinae Family Elephantidae Hominides Drosophilidae Superfamily Elephantoidea Order Proboscidea Primates Diptera Class Mammalia Mammalia Insecta Subphylum Vertebrata Vertebrata Phylum Chordata Chordata Arthropoda Kingdom Animalia Animalia Animalia 2 BIOMEDICAL ONTOLOGIES 2.1 The foundations of biomedical ontology It is mainly the information explosion in biology and the necessity to process huge amounts of research data that have stimulated the proliferation of biomedical ontologies.
Rubin et al.
(2008) give an overview of the broad range of biomedical information services that can be supported by domain ontologies, with the Gene OntologyAshburner et al.
(2000) and the OBO collection as the most prominent examples.
Whereas this tenet used to be addressed in the past mainly by what had been termed biomedical terminologies (with the UMLS4 as prototypical example), more recently we have seen a steady growth in the usage of the term ontology.
Due to the lack of a clear notion of what an ontology actually constitutes (Kusnierczyk, 2006) there is a tendency for either insupportable expectations or general rejection of this term.
In this article, we detach the concept of terminology from the one of ontology subscribing to the following definitions: According to ISO (2000), a terminology is defined as a set of terms representing the system of concepts of a particular subject field.
Terminologies relate the senses or meanings of linguistic entities.
In contrast, according to Quine (1948), Ontology (in singular and upper case) is the study of what there is.
In our understanding, ontologies (plural and lowercase) are formal theories that attempt to give precise formulations of the types of entities in reality, of their properties, and of the relations between them (Guarino, 1998).
In contradistinction to terminology, formal ontologies strive for describing (as much as possible) what the consensus in a given scientific domain is, independently of human language.
Their constituent nodes are referred to as types, kinds or universals.
As they are well suited to hierarchically order and classify particular entities (e.g.
a given piece of tissue, a cell under a microscope, an amount of biological substance, an animal, a particular population of bacteria, etc.
), they are also referred to as classes, a parlance we will use in the following, in accordance with the more recent language use in current biomedical ontology engineering and research.5 Although the question whether certain entities really exist are subject to major philosophical disputes, we contend that at any 4Unified Medical Language System (UMLS): http://umlsinfo.nlm.nih.gov 5We follow a general trend and restrict the use of the word concept to the realm of terminologies, where it denotes artifacts that represent meanings of linguistic expressions.
We avoid it in relation to formal ontologies.
given stage in the development of science, there is a consensus core of scientific understanding of reality, and in our view, it is this which should serve as starting point for developing science-based ontologies.
Examples of statements belonging to this consensus core are that: primates are vertebrates, cells contain cytoplasm, aspirin tablets contain a derivative of salicylic acid, ADP is phosphorylated in mitochondria or that certain biochemical compounds have a clearly delineated composition.
2.2 Top-level ontologies It is widely recognized that the construction of formal ontologies should obey principled criteria.
To this end, several top-level ontologies have been devised, such as DOLCE (Gangemi et al., 2002), BFO (Smith et al., 2005), or GOL (Heller and Herre, 2004).
These ontologies mainly coincide in their fundamental division between continuants (also called endurants, e.g.
material objects) and occurrents (also called perdurants, e.g.
events, processes).
Orthogonal to this distinction, there is also a coincidence in clearly separating concrete entities or particulars (e.g.
the chimpanzee named Washoe, the elephant named Clyde, or the 3rd author of this article) from the classes they instantiate (e.g.
Chimpanzee, Asian Elephant, Human).
To this end, we introduce the irreflexive, anti-transitive and asymmetric instantiation relation instance_of which relates particulars to classes.
In addition, we need a formal relation for subsumption between classes.
Here we follow the OBO standard and introduce, for this purpose, the taxonomic subsumption relation Is_a by means of instance_of6 just as proposed by Smith et al.
(2005): Is_a (A,B)=def x : (instance_of (x,A) instance_of (x,B)) In the following discussion, we are proposing several possible alternative solutions for an ontological account of species.
2.3 Domain top-level ontologies Whereas top-level ontologies contain only a restricted set of highly general classes, such as the aforementioned Continuant, Occurrent, Function or Object, which are not tied to any particular domain of interest, a domain top-level ontology contains all the classes that are essentially needed to describe a certain domain, like Organism, Tissue, Cell and also Species in the case of biology.
Those more specific classes are in turn a specialization of the top-level classes as expressed in the formula Is_a (Cell, Object).
2.4 BioTopa domain top-level ontology Recently, two separate implementations to encode the top-level of the biomedical domain into ontologies have been created, namely, BioTop7 (Stenzhorn et al., 2007) and the Simple Top Bio (Rector et al., 2007).
At the moment, efforts set forth by the authors are ongoing to converge these two implementations.
The goal of BioTop is to provide classes and classificatory criteria to categorize the foundational kinds of biology, without any restriction to granularity, species, developmental stages or states 6Throughout this article, we use capitalized initial letters for the names of relations between universals, as well as for the names of universals.
Particulars are highlighted by lower case or by quoted names, bold face is used for relations between particulars.
7Available at http://purl.org/biotop i314 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i315 i313i321 Ontology of biological taxa of structural well-or ill-formedness (Schulz and Hahn, 2007).
The initial impetus for creating the BioTop ontology was the idea of redesigning and expanding the GENIA ontology (Ohta et al., 2002) in a comprehensive and formally sound way, i.e.
to adhere to the fundamental principles of formal rigor, explicitness and precision of ontological axioms.
In BioTops initial development, no definitive commitment existed towards any existing upper ontology, except for the distinction between continuants and occurrents (cf.
Section 2.2).
The primary focus at this stage was set on representing continuants from the area of interest.
In the continued development, however, the focus was broadened to include the representation of biological processes, functions and qualities.
Additionally, BioTop was aligned with the BFO upper level ontology.
BioTop is implemented in OWL DL,8 an official Semantic Web standard published by the World Wide Web Consortium (W3C).
By using this language, our ontology can benefit from a large amount of support tools for editing, automatic classification, etc.
OWL DL is also one of the languages accepted by the OBO consortium.
The significance of this lies in the fact that, in our view, the high-level BioTop classes can serve as a bridge to link and interface the domain-specific ontology classes in the OBO collection.
Using such interfacing facility can both potentially reveal overlaps or design errors in OBO ontologies and also create synergetic effects.
2.5 The difficult concept of species Before we embark on a more general ontological account of biological taxa, we first turn to the most basic taxon, namely, species.
Both biologists and philosophers disagree on the proper definition of the term species and its ontological status (Ereshefsky, 2001).
It had been principally the criterion of similarity between organisms and organism groups that guided Linnaeus classificatory efforts.
Although there are rarely any two individuals with exactly identical characteristics, we made the following observations in regard to the similarity of organisms.
From a diachronic point of view, there are generally significant but relatively minor differences between an organism and its offspring due to sexual or asexual reproduction and spontaneous mutations.
However, the distance increases with the number of generations and so todays organisms have little in common with their ancestors.
The genetic and phenotypic modifications can be assumed to lie on a mainly continuous scale, and the boundary of the emergence of a new species cannot be drawn by unambiguous criteria, a phenomenon that is ubiquitous in biology (Schulz and Johansson, 2007).
No obvious distinguishing feature exists that is apt to clearly divide the species Homo sapiens from Homo erectus and nothing indicates any sort of qualitative leap.
As a corollary of this, the parallel evolution of independent lines of organisms increases their genetic and phenotypic distance.
Under a synchronic viewpoint, this manifests itself as groups of organisms with clear criteria of species identity.
In contrast to the diachronic view, the distinguishing features do not lie on a continuous scale but they are clearly discrete.
For instance, the boundary between the species Homo sapiens and Simia troglodytes (chimpanzee) can be clearly drawn, as there are no organisms existing in the middle.
Even under the diachronic perspective, the distinction between groups of organisms with diverging characteristics may be blurred, 8Web Ontology Language (OWL): http://www.w3.org/TR/owl-features e.g.
by the distinction of subgroups of the same species.And different species may even form hybrids and merge to a new species.
All these peculiarities claim for a non-arbitrary conceptualization of what constitutes exactly a species.
There are different types of species concepts, from which the concept of biological species as a group of organisms that can interbreed and produce fertile offspring (Mayr, 1969), has found the widest acceptance.
Nevertheless, this definition provides only necessary but not sufficient criteria.
A defined population of organisms (e.g.
the Asian elephants living in Thailand) certainly fulfills this criterion although they do not form a species of their own since they can mate and produce fertile offspring with elephants from Cambodia, for instance.
Abbreviating the ability of producing fertile offspring by , according to the biological species concept, the pertinence of biological organisms to the same species is expressed by the predicate : ( o1,o2 )= def (t :(o1,o2,t ))(o,t1,t2 : ( ( o1,o,t1 )(o2,o,t2 ))) The shortcomings of Mayrs definition are well known (Grene and Depew, 2004, ch.
10): first, it only allows the comparison of organisms living at the same time.
Second, the definition depends on the dispositional criterion , the verification of which remains speculative in many cases.
Third, the definition fails with infertile individuals, as well as with species in extinction of which only female or male individuals remain.
Fourth, it fails in the numerous cases of asexual reproduction such as bacteria.
It is therefore neither easily applicable, nor generally valid, in spite of its theoretical soundness (Hull, 1997).
So it is not surprising that other species concepts compete with Mayrs one.
The 22 different conceptualizations of species identified and discussed by Mayden (1997) bear witness on the intensive discussions and disagreements among theoretical biologists and philosophers.
For our practical purpose of biomedical ontologies the formalization of species ormore generallyof biological taxa that we propose, is intended to be neutral to the different and conflicting species conceptualizations.
It departs from the principle that biological taxa are something that regardless of its existence in nature or its (fiat) attribution by biologists has a highly ranked importance in biology and therefore requires to be accounted for in biomedical ontologies.9 In the following, we will analyze the ontological status of biological taxa and propose and critically assess alternative solutions.
3 CONCURRENT ACCOUNTS OF BIOLOGICAL TAXA 3.1 Biological taxa as meta-properties The above restriction to a two-leveled ontological framework (i.e.
dividing the world exhaustively into particulars and universals) has often been challenged.
(Gangemi et al., 2001) contend that there is a fundamental difference between instances in an ontology on the one hand and domain entities (particulars, cf.
Section 2.1) on 9The approach should be flexible enough to support even classification schemes that contradict classic taxonomic principles such as carnivore and herbivore.
The authors are aware of the fact that this may challenge some of the philosophical foundations underlying Basic Formal Ontology (BFO).
i315 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i316 i313i321 S.Schulz et al.
the other hand.
They argue that we can extend a Theory A (which follows the two-level assumption) by a meta-Theory B.
Whereas Theory A describes domain entities (particulars) that instantiate universals (classes), B takes As universals as instances of so-called meta-properties.
Indexing the instantiation relation by theory level (using subscripts in the formulae) we may state in Theory A that instance_ofA(x,y) and then place this in the context of Theory B with instance_ofB (y,z) To give a concrete example: instance_ofA(Clyde, Elephas maximus) instance_ofB (Elephas maximus, Species) Due to the algebraic property of antitransitivity (as claimed by (Gangemi et al., 2001), we can then coherently reject the hypothesis that our elephant Clyde is an instance of Species.
There are several arguments against this solution.
Let us consider the second-level predications instance_ofB (Elephas maximus, Species) on the one hand and instance_ofB (Elephas maximus, Genus Elephas) on the other hand.
Whereas the first one asserts that the class Elephas maximus is an instance of a Species, the second one states that the species class Elephas maximus as a member of the genus Elephas.
In the same right as we have stated instance_ofB (Elephas maximus, Species) we could then assert in a third-level predication (instance_ofC) instance_ofC (Genus Elephas, Genus) Clyde would then be a second-level instance of Species and a second-level instance of Genus Elephas, as well as, in virtue of the latter, a third-level instance of Genus.
Given instance_ofC (Species, Taxon) and instance_ofC (Genus, Taxon), Clyde would finally act simultaneously both as third and fourth-level instance of Taxon.
Together with the argument that Clyde might also directly instantiate Genus Elephas and the fact that some taxonomic levels (such as subfamilies) are sometimes skipped, it is very obvious that this solution leads to an obscure and inconsistent picture.
Another shortcoming of this approach lies in the fact that it lacks a transitive hierarchical relation between taxa of different levels that would be able to express in simple terms (e.g.
that all Indian elephants are vertebrates).
From a computational viewpoint, there is also an important performance argument.
For example, efficient reasoning algorithms which have been developed for description logics (Baader et al., 2003) and are coherent with the Semantic Web standard OWL DL do not provide support for reasoning capabilities about instances of instances.
3.2 Biological taxa as hierarchies of classes We could simplify the above approach (and render it well-suited for description logics-based reasoning) by conflating the level of classes with the one of the meta-level classes.
Given the definitions above and a division of all entities in either particulars or classes, it may appear straightforward to use the Is_a relation for expressing that Chimpanzees, Indian Elephants, Humans, etc.
are species, or that Genus Pan, Genus Elephas and Genus Homo are genera: Is_a (Elephas maximus, Species) Is_a (Simia troglodytes, Species) Is_a (Genus Elephas, Genus) Is_a (Genus Pan, Genus), just as Is_a (Elephas maximus, Genus Elephas) Is_a (Simia troglodytes, Genus Pan) The weakness of this solution, however, immediately derives from the above definition of the Is_a relation.
So given that instance_of (Clyde, Elephas maximus) instance_of (Washoe, Simia troglodytes) we can infer that instance_of (Clyde, Genus Elephas) instance_of (Washoe, Genus Pan) as well as that instance_of (Clyde, Species) instance_of (Washoe, Species) instance_of (Clyde, Genus) instance_of (Washoe, Genus) We finally end up with all taxa in a specialization hierarchy, having individual organisms as instances.
This neither captures the nature of a biological organism, nor the intended meaning of Species or Genus, since neither Clyde nor Washoe or any other individual animal is an instance of the class Species.
Nevertheless, we could consistently do this excluding the terms species, genus, etc.
This would reduce the instances of taxa (Elephant, Elephantidae, Vertebrates) to classes of organisms and we would no longer be able to account for the meaning of terms like Genus or Species in a description logic-based framework.
However, the resulting assertions such as Clyde is an instance of Mammalia (on par with Clyde is an instance of Elephant) would collide with the plural meaning of the taxon terms.
3.3 Biological taxa as populations Several authors have argued in favor of the inclusion of collectives into an ontological framework (Bittner et al., 2004; Rector et al., 2006; Schulz et al., 2006a) .
BioTop has embraced these aspects by introducing the relation has_granular_part, an irreflexive and intransitive subrelation of the OBO Relation Ontology relation has_part (Schulz et al., 2006b).
This allows us to relate a collective entity to each of its constituent elements, without, however, resorting to set theory.
For instance, has_granular_part (PopulationofThaiElephants,Clyde) asserts that there is a collective entity Population of Thai Elephants that is constituted by granular parts like our elephant Clyde and a number of other individuals similar to Clyde.
It permits to define i316 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i317 i313i321 Ontology of biological taxa Is_A W ashoe Genus Pan Quality Family Elephantidae Quality Order Primates Quality Family Hominides Quality Class Mammalia Quality Subphylum Vertebrata Quality Genus Elephas Quality Simia troglodytes Quality Elephas maximus Quality Order Proboscidea Quality Phylum ChordataQuality Kingdom Animaliaa Quality Is_A Is_A Is_A Is_A Is_A Is_A Is_AIs_A Is_A Is_A Is_A Clydes taxon quality Washoes taxon quality instance_ofinstance_of inheres_in Clyde Washoe P ar tic ul ar s U ni ve rs al s (C la ss es ) inheres_in Fig.1.
Taxon qualities inhering in individual organisms.
collectives in terms of granular parts such as x : instance_of (x, ElephantPopulation) y1,y2,...yn : instance_of (y1,y2,...,yn, Elephant) has_granular_part (x,y1,y2,...,yn) z : (instance_of (z, Elephant) has_granular_part (x,z)) Note that Population of Thai Elephants is a particular collective and an instance of the universal collective ElephantPopulation.
The union of all possible instances of ElephantPopulation, namely, Total ElephantPopulation would then be the maximal population of elephants every individual elephant is a granular part of.
x : instance_of (x, Elephant) has_granular_part (Total ElephantPopulation,x) Yet, TotalElephantPopulation is a particular entity.
Our proposal here is to consider it as an instance of Species.
In the same way, we could introduce other populations in different degrees of abstraction such as TotalVertebratePopulationwhich would then be an instance of Phylum.
It may be practical for many purposes to equate biological taxa with biological populations although the meaning of Elephantidae or Vertebratae, in practice, goes further.
Especially in molecular biology, species information is not only attributed to whole organisms, but also to organism parts, their constituting cells and derived cell lines.
As an example, individual cells from the HELA cell line are considered human cells, but their existence is not dependent on any human population.
The interpretation of biological taxa as populations is therefore not adequate for such cases.
We can use the OBO relation derives_from in order to express that a HELA cell is a human cell: x : instance_of (x, HELA Cell) y : instance_of (y, Human) derives_from (x,y) has_granular_part (TotalHumanPopulation,y) 3.4 Biological taxa as qualities Most top-level ontologies coincide in granting qualities a prominent status.
For instance, BFO describes the class Quality as A dependent continuant that is exhibited if it inheres in an entity or categorical property.
Examples: the color of a tomato, the ambient temperature of air, the circumference shape of a nose, the mass of a piece of gold, the weight of a chimpanzee.10 DOLCE introduces qualities as the basic entities we can perceive or measure: shapes, colors, sizes, sounds, smells, as well as weights, lengths, electric charges (Masolo, 2003) and also makes reference to the relationship of inherence.
The position of the class Quality in BFO makes clear that qualities are dependent entities, i.e.
they can only exist in dependence on the entities they inhere in.
Our proposal here is to interpret the relation of a biological object to a given taxon as the ascription of a quality.
For example, the quality of belonging to the species Homo sapiens is a quality that inheres in any human organism, tissue or cell.
The quality of belonging to the phylum Chordata is a quality that inheres in any biological object that is part of or derived from an organism the species of which belongs to the phylum Chordata.
Figure 1 depicts a segment of our proposed subclass hierarchy of taxon qualities.
The hierarchy exhibits two organizational principles: generalization versus specialization on one side, and the relevance to an organizational level on the other.
Every instance of a material biological object has one inherent taxon quality.
Since, e.g.
every human is a hominid, every inhering instance of the class Homo sapiens Quality is also an instance of Family Hominides Quality, etc.
The introduction of qualities is helpful for 10SNAP Continuant Definitions: http://www.ifomis.org/bfo/manual/snap.pdf i317 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i318 i313i321 S.Schulz et al.
Fig.2.
Taxon qualities inhering in individual organism and their location in Taxon Regions consistent with the DOLCE upper level ontology.
ontological definitions such as x : instance_of (x, Human) instance_of (x, Organism) y : instance_of (y, HomosapiensQuality) inheres_in (y, x) x : instance_of (x, Vertebrate) instance_of (x, Organism) y : instance_of (y, VertebrateQuality) inheres_in (y, x) Based on a hierarchy of qualities, such definitions permit inferences such as that every human is a vertebrate or that every human population is part of some vertebrate population.
In addition, it allows for linking organism parts with qualities such as x,: instance_of (x,VertebrateHeart) instance_of (x, Heart) y : instance_of (y,VertebrateQuality) inheres_in (y, x) If the import of the taxon concept should be extended from biological organisms to their parts, as argued in Section 3.3 (e.g.
human leukocyte), the attribution of qualities to organism parts or derivatives can easily be axiomatized by the so-called right identity rules (with being the relation concatenation symbol): part_of (x, y) inheres_in (z, y) inheres_in (z, x) derives_from (x, y) inheres_in (z, y) inheres_in (z, x) 3.5 Biological taxa as Qualia An alternative approach to a subclass hierarchy based on the DOLCE upper ontology (Masolo, 2003) is represented in Figure 2.
Since DOLCE is inspired by trope theory (Goodman, 1951), which distinguishes between qualities and their values (i.e.
Qualia) this proposal introduces another layer of abstraction.
Each quality type has an associated quality space (i.e.
Region) in which it is located.
As in BFO, qualities are dependent entities which are inherent in their respective particulars.
Compared to the representation depicted in Figure 1 only few taxon qualitiesone for every taxonare organized in a flat hierarchy and are related to corresponding value regions.
The subsumption hierarchy of taxon qualities of the former approach is represented as a partonomic hierarchy of the Taxon Regions in the latter, e.g.
the Species Region is part of the Class Region which is itself part of the Kingdom Region.
The variety of features is represented as subclasses of the basic Taxon Regions, e.g.
Mammalia Class Region Is_a Class Region.
The main advantages of this approach are a clearer separation of hierarchies and the possibility to make explicit assertions on the specialized Taxon Regions without uncontrolled inheritance of restrictions.
Its disadvantage lies in a higher complexity.
3.6 Synthesizing different taxon accounts We have proposed four mutually dependent kind of ontologically relevant entities that describe different aspects of what is meant i318 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i319 i313i321 Ontology of biological taxa by biological taxa on the one hand, and that are expressible in a description logics-based framework on the other.
The totality of organisms belonging to one taxon (e.g.
all Gram-positive bacteria, all primates or all humans).
This entity is a particular one that instantiates the class Maximal Biological Population.
For each taxon there is one such instance.
Population classes, the instances of which are defined as parts of some instance of Maximal Biological Population.
For example, Elephant Population in Thailand is an instance of the class Elephas Maximus Population, the latter being a subclass of Elephas Population and so on.
For each taxon there is one such population class.
Taxon quality classes that are instantiated by each and every particular object to which a taxon can be ascribed.
There is one such taxon quality class for each taxon.
Because taxon classes are arranged in an Is_a hierarchy, the quality of a subordinate taxon is also the quality of a superordinate taxon.
For example, an instance tqClyde of Elephas Maximus Quality can be ascribed to the elephant Clyde.
tqClyde is equally an instance of Genus Elephas Quality, of Family Elephantidae Quality, and so on.
Taxon quality regions that are represented by a mereological inclusion hierarchy.
In contrast to the third approach, every taxon-relevant entity has an inherent quality instance from each taxonomic level.
4 IMPLEMENTATION We extended BioTop by the notion of biological taxa following the quality approach discussed in Section 3.4. bfo:Entity  bfo:Continuant bfo:DependentContinuant  bfo:SpecificallyDependentContinuant  bfo:Quality   biotop:ContinuantQuality   biotop:TaxonQuality The class biotop:TaxonQuality has the following restrictions11: biotop:TaxonQuality implies inheres_in.
(has_part.biotop:NucleicAcid)AND inheres_in.
(has_part.biotop:NucleicAcid) So we claim the existence of genetic information as a limiting and necessary condition for those entities biological taxa can be ascribed to.
In the inverse direction, we claim the inherence of taxon qualities to the classes biotop:Cell, biotop:Organism, biotop:Tissue,biotop:OrganismPart, biotop:NucleicAcid, e.g.
biotop:Cell implies inv_inheres_in.biotop:TaxonQuality The class biotop:TaxonQuality is then the interface to a specialized ontology such as the NCBI taxon ontology.
For demonstration purposes we created taxdemo, a small example ontology.12 11For the Description Logics notation cf.
(Baader et al., 2003), or12Available at http://purl.org/biotop taxdemo:TaxonQuality  biotop:TaxonQuality  taxdemo:KingdomAnimaliaQuality  taxdemo:PhylumChordataQuality   taxdemo:ClassMammaliaQuality   taxdemo:OrderPrimatesQuality    taxdemo:FamilyHominidaeQuality    taxdemo:GenusHomoQuality     taxdemo:HomoSapiensQuality In parallel, the taxonomic ranks (TaxonQuality, KingdomQuality, etc.)
are indirectly represented as a second hierarchy.
taxdemo:TaxonQuality biotop:TaxonQuality  taxdemo:KingdomQuality  taxdemo:KingdomAnimaliaQuality taxdemo:KingdomBacteriaQuality taxdemo:KingdomVirusesQuality  taxdemo:PhylumQuality  taxdemo:PhylumChordataQuality  taxdemo:ClassQuality  taxdemo:ClassMammaliaQuality  taxdemo:OrderQuality  taxdemo:OrderPrimatesQuality  taxdemo:OrderProboscideaQuality  taxdemo:FamilyQuality  taxdemo:FamilyHominidesQuality  taxdemo:FamilyElephantidaeQuality  taxdemo:GenusQuality  taxdemo:GenusHomoQuality taxdemo:GenusPanQuality taxdemo:GenusElephasQuality  taxdemo:SpeciesQuality  taxdemo:HomoSapiensQuality  taxdemo:ElephasMaximusQuality This allows us to define population as a plurality of organism of the same species as follows: taxdemo:Population IMPLIES has_granular_part.biotop:OrganismAND =1 inv_inheres_in.taxdemo:SpeciesQuality These criteria are not met by mixed groups of individuals, e.g.
a group of different primates which coincide only at the level of taxdemo:OrderQuality The flexibility of our approach becomes obvious when we use taxon information for parts of the organisms.
For instance, the class HumanLeukocyte can be defined as taxdemo:HumanLeukocyte EQUIVALENT TO taxdemo:Leukocyte AND inv_inheres_in.taxdemo:HomoSapiensQuality If we define taxdemo:AnimalCell EQUIVALENT TO taxdemo:Cell AND inv_inheres_in.taxdemo:KingdomAnimaliaQuality i319 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i320 i313i321 S.Schulz et al.
then taxdemo:HumanLeukocyte can be classified as taxdemo:AnimalCell, provided that the ontology supports: taxdemo:HomoSapiensQuality Is_a taxdemo:KingdomAnimaliaQuality together with taxdemo:Leukocyte Is_abiotop:Cell It is obvious that this kind of reasoning can be of great advantage for biological fact retrieval from databases or for semantically enriched information extraction from texts.
From a computational perspective, however, we acknowledge that there still is a bottleneck with regard to the use of inverses (such as inheres_in versus inv_inheres_in) and qualified number restrictions (such as =1) in description logics reasoners.13 We admit that the meaning of the taxonomic rank classes SpeciesQuality, GenusQuality, KingdomQuality, etc.
is somewhat counterintuitive, since every instance of SpeciesQuality is also an instance of GenusQuality and so on.14 They are, therefore, not suited to comprehensively represent the meaning of Species as disjoint from Genus, Kingdom, etc.
Such a reading would require the meta-class representation as discussed in Section 3.1, discarded due to computational reasons.
In our framework, the only way to have an instantiable Species (Genus, Kingdom) class would be to collect all maximal populations (cf.
Section 3.3) with identical species-(genus-, kingdom-) level qualities as instances of Species (Genus, Kingdom) which, again, would only partially match the meaning of Species (Genus, Kingdom).
We refrained from implementing the solution discussed in Section 3.5, because its more differentiated approach to the representation of qualities is not supported by the BFO upper ontology, currently in use for BioTop.
5 RELATED WORK Literature on the ontology of taxa roughly falls into two categories: the conceptualization of the nature of species on the one hand, and the ontological status of taxa on the other.
In both cases, the focus lies mainly on species whereas higher taxa are seldom addressed.
The first line of scientific discussion is characterized by numerous publications that started with the seminal book of Mayr (1942), who compared several approaches to delineate the nature of species15 and propagated the popular concept of species as a group of organisms that interbreed and produce fertile offspring.
Hull (1997) casts doubt on the monistic assumption that there is one single and ideal way to define species and hypothesizes a trade-off between theoretical significance and practical applicability of species concepts.
He classifies the existing species concepts into three categories, namely, (i) similarity-based (which, of course, hinges on some unambiguous notion of phenic or genetic resemblance), (ii) biological and evolutionary (which includes Mayrs and other proposals such as Hennig, 1966) centering around the behavior (i.e.
mating, reproduction) of biological organisms and (iii) phylogenetic, focusing the historic development of species.
Mayden (1997) 13See frequently updated list at http://www.cs.man.ac.uk/sattler/ reasoners.html 14An instance of HomoSapiensQuality would be an instance of KingdomQuality, too.
15For an overview of earlier approaches see Hey (2006).
performed an extensive literature review and identified 22 distinct species concepts.
In contradistinction to Hull, he propagates the cladistics-based evolutionary significant unit (Evolutionary Species Concept, Simpson, 1961), rooted in the philosophical principle of identity: An evolutionary species is an entity composed of organisms that maintains its identity from other such entities through time and over space and that has its own independent evolutionary fate and historical tendencies.
According to (Goodman, 1951) this concept of species is the most acceptable and most compatible with other species concepts that are rather criterion-based detection protocols than theoretically underpinned concepts.
He argues that no criterion that presumes to delineate natural boundaries can overcome the generic vagueness (Hull, 1965) of species concepts.
Our approach advocates neutrality towards the conceptualization of species and is apt to coexist with both monistic and pluralistic approaches.
We are aware of the fact that in the latter case species qualities with multiple parents may be taken into account, due to different categorizations according to conflicting species concepts.
The second line of discussion is on more abstract grounds, and scrutinizes the ontological nature of species, regardless of the species concepts subtleties as exposed above.
A fundamental question in here is whether speciesseen as single evolving lineage that act as units of evolutionare classes or individuals, the latter being advocated by Ghiselin (1974) and Hull (1978), with the consequence that every single organism is a spatiotemporal part of its species.
This theory comes close to our view of species as the totality of organisms belonging to one specific species, which can be generalized from species to taxa.
We prefer this mereological approach over the set-theoretical one (also pointed out by (Ereshefsky, 2007), because the view of a group of organisms as mathematical sets (that are not localized in space and time) is rather counterintuitive.
The conceptualization of species as universals or natural kind conflicts with the fact that there are relatively few essential properties that are shared by all individuals of a species (including developmental stages and malformations).
Boyds (1999) Homeostatic Property Cluster Theory tries to overcome this, but is still too much committed to similarity-based criteria according to (Ereshefsky, 2007).
The approach pursued in this article, namely, introducing theory-neutral species qualities that are extensible to general taxon qualitiesseems to be rather novel.
6 CONCLUSION We have proposed an ontological approach to biological taxa in the context of the domain top-level ontology BioTop.16 It is essentially based upon the assumption that every biological organism, population or biological matter has some inherent taxon quality.
Since it does not raise further reaching ontological claims, our approach largely bypasses the ongoing dispute on species concepts.
This enables us to delineate biological populations in terms of shared taxon qualities and to formulate taxon-specific axioms in the framework of description logics.
Our proposal is fully embedded into the standards of Open Biological Ontology and is in line with a major top-level ontology, BFO.
Our account of taxon qualities (i.e.
the preference of the 16BioTop, together with a tentative taxon-specific extension is available at[19:58 18/6/03 Bioinformatics-btn158.tex] Page: i321 i313i321 Ontology of biological taxa simpler approach described in Section 3.4 over the more complex solution found in Section 3.5) also demonstrates how fundamental ontology design decisions depend on the choice of the underlying top-level model.
As our approach represents taxon qualities as a simple is_a hierarchy, the import of subsets of existing taxonomy databases such as the NCBI taxonomy is straightforward and scalable.
These data can automatically be transformed into an OWL subtype hierarchy and linked to the BioTop node TaxonQuality.
ACKNOWLEDGEMENTS The authors would like to thank Alan Rector (Manchester), Elena Beiwanger (Jena), Udo Hahn (Jena), Eric van Mulligen (Rotterdam) and Lszl van den Hoek (Rotterdam), as well as Olivier Bodenreider (Bethesda), for fruitful discussions.
Funding: This work was supported by the EC STREP project BOOTStrep (FP6 028099).
Conflict of Interest: none declared.
Abstract Pluripotency-associated factors and their rivals, lineage specifiers, have long been consid-ered the determining factors for the identity of pluripotent and differentiated cells, respectively.
Therefore, factors that are employed for cellular reprogramming in order to induce pluripotency have been identified mainly from embryonic stem cell (ESC)-enriched and pluripotency-associated factors.
Recently, lineage specifiers have been identified to play important roles in orchestrating the process of restoring pluripotency.
In this review, we summarize the latest discoveries regarding cell fate conversion using pluripotency-associated factors and lineage specifiers.
We highlight the value of the seesaw model in defining cellular identity, opening up a novel scenario to consider pluri-potency and lineage specification.
Introduction Understanding how cellular identity is established is a major goal for modern biology.
The programming and reprogram-ming of cellular identity elicit tremendous scientific and public interest.
The groundbreaking work of Takahashi and Yama-naka established a precedent with the generation of induced pluripotent stem cells (iPSCs) by the forced expression of only (Deng H).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A four transcription factors Oct4, Sox2, Klf4 and c-Myc [1].
Similar to embryonic stem cells (ESCs), iPSCs can proliferate and self-renew indefinitely under appropriate conditions and give rise to all types of cells in the body, which bestows these cells with many potential uses in regenerative medicine.
Pa-tients with degenerative diseases such as diabetes and cancer, along with aging individuals could all benefit from iPSC-based therapies [2].
The discovery of iPSCs Somatic cells can be reprogrammed by nuclear transfer [3] or by fusion with ESCs [4], suggesting that oocytes and ESCs contain factors that can reprogram somatic cells into stem cells.
Inspired by this discovery, Yamanaka and his colleagues selected 24 genes that are specifically expressed in ESCs, which cademy of Sciences and Genetics Society of China.
Production and hosting mailto:hongkui_deng@pku.edu.cn260 Genomics Proteomics Bioinformatics 11 (2013) 259263 also play important roles in the maintenance of ESC identity, as candidate factors to induce pluripotency in mouse somatic cells.
By transducing all 24 candidate genes together, G418-resistant colonies were generated by using Fbx15bgeo/bgeo as a selection marker for pluripotency.
These cells were further identified to possess ESC properties.
To determine which of the 24 candidates were essential, Yamanaka and his colleagues tested the effects of the withdrawal of individual factors from the 24-candidate gene pool on the generation of G418-resistant colonies.
Ultimately Oct4, Sox2, Klf4 and c-Myc were identi-fied to be pivotal for the induction of pluripotency in mouse somatic cells [1].
Furthermore, these factors were proven to be able to induce pluripotency in human somatic cells as well [5].
Other laboratories have also been working on inducing plu-ripotency in somatic cells.
By using Lin28 and Nanog together with Oct4 and Sox2, the Thomson laboratory also indepen-dently discovered a set of four reprogramming factors that are highly enriched in ESCs [6].
These first proof-of-principle studies opened the realms of iPSC research to a future in regenerative medicine.
Discovering novel pluripotency regulators for the induction of pluripotency It is coherent to test whether there are novel ESC-associated factors that can regulate iPSC induction.
Several reports have suggested factors that are important for the maintenance of ESC identity can also facilitate the induction of pluripotency.
For example, Nr5a2, an orphan nuclear receptor that is en-riched in ESCs, can replace Oct4 in the induction of pluripo-tency [7].
Esrrb, another orphan nuclear receptor that plays a pivotal role in the maintenance of pluripotency, can replace Klf4 and c-Myc [8].
PRDM14 and NFRKB were identified as novel determinants of human ESC identity and can substitute for Klf4 in reprogramming [9].
Recently, by a single-cell anal-ysis of the reprogramming process, Lin28, Sall4, Esrrb and Dppa2 were identified as a completely novel set of reprogram-ming factors [10], which are different from the factors initially identified by Yamanaka et al.
These factors are all important for the maintenance of ESC identity [10].
In addition to the highly expressed factors in ESCs, the maternal factor Glis1 in oocytes was reported to be a novel facilitator of reprogram-ming [11].
Direct reprogramming into iPSCs by lineage specifiers For years, it was generally believed that ESCs are maintained by a shield of pluripotency factors.
These factors function in concert with each other to prevent ESCs from differentiating into any lineage, thus preserving the ESCs at an undifferenti-ated state [12,13].
A more challenging perspective has been put forward recently.
Pluripotency factors might as well func-tion as classical lineage specifiers that direct ESCs to differen-tiate into a specific lineage and inhibit their commitment to mutually exclusive lineages [14].
Consistent with the notion, in ESCs, Oct4 promotes the differentiation of mesendoderm (ME) and primitive endoderm, while suppressing differentiation of the ectoderm (ECT) [1517]; Sox2 inhibits ME differentiation but promotes neural ECT differentiation [16,17].
Shu et al.
provided the first proof-of-principle report showing that modulating lineage-specifying forces can restore the pluripotency of mouse somatic cells [18].
When screening for factors that may substitute for Oct4 in the induction of pluripotency, Shu et al.
found that GATA3, which is known to regulate ME commitment and specification, can substitute for Oct4.
Subsequent analysis of other lineage specifiers that mainly function in ME differentiation and early embryonic patterning, which are generally not enriched in ESCs, found that GATA6, SOX7 and PAX1, among others, were also able to substitute for Oct4 to induce pluripotency, whereas ectodermal specifiers could not.
All Oct4 substitutes were also able to attenuate the upregulated expression of ECT-associated genes that is triggered by the expression of Sox2, Klf4 and c-Myc (SKM), whereas knockdown of the key ectodermal marker Dlx3 promoted SKM-only reprogram-ming [18].
These findings suggest that a novel function of Oct4/ GATA3 is to suppress ECT differentiation during reprogramming.
Accordingly, the ECT lineage specifiers SOX1, SOX3, RCOR2 and GMNN can replace Sox2 during reprogramming.
Similarly, Sox2 and its substitutes attenuate the expression of ME-specific markers induced by expression of Oct4, Klf4 and c-Myc (OKM) [1820].
Strikingly, co-expression of GATA6 and GMNN can substitute for Oct4 and Sox2 to reprogram mouse fibroblasts into iPSCs in the presence of Klf4 and c-Myc [18].
More recently, Montserrat et al.
showed that lineage speci-fiers can also be used to reprogram human fibroblasts into iPS-Cs.
The authors found that GATA3 can replace OCT4 and the ECT specifier, ZNF521, can replace SOX2.
Lastly, they showed that GATA3, together with ZNF521, OTX2 and PAX6, can substitute for both OCT4 and SOX2 for human iPSC induction in the presence of KLF4 and c-MYC [21].
A seesaw model for cell fate conversion A binodal model for cell fate determination, such as GATA1 and PU.1, RUNX2 and PPARc, has been examined in various instances of pluripotent stem or progenitor cells that assume a binary cell fate decision [22].
Such circuit hints at the concept of a balanced pluripotent state.
Inspired by these insights, Shu et al.
proposed a new model, termed the seesaw model, in which the pluripotent state has a precarious balancing equi-librium that results from continuous mutual competition be-tween rival lineage specification forces (Figure 1).
This model comprises two coupled modules the canonical pluripotency module and the lineage-antagonism module.
The former mod-ule is represented by the mutual activation of Oct4 and Sox2, whereas mutual inhibition of the ME and ECT genes repre-sents the latter module [18].
Both the canonical pluripotency module and the lineage-antagonism module are incorporated leading into the integrated seesaw model.
The novelty of the seesawmodel is the proper combination of the two modules.
This model led to unexpected insights and scenarios of cell fate conversion.
The activation of the cross-activating pluripotency module is important for the reestablishment of the pluripotency network to achieve successful reprogramming.
The self-activating pluripotency module gets activated when all of the lineage-specifying forces are Pluripotent stateLineage A inducer (e.g., ME inducer) Pluripotent state Pluripotent state Lineage A inducer (e.g., ME inducer) Lineage B inducer (e.g., ECT inducer) Lineage A state Lineage B state Pluripotent state Pluripotent state Pluripotent state Lineage B inducer (e.g., ECT inducer) Pluripotent state Lineage B inhibitor (e.g., ECT inhibitor) Lineage A inhibitor (e.g., ME inhibitor) Lineage A inhibitor (e.g., ME inhibitor) Lineage B inhibitor (e.g., ECT inhibitor) Figure 1 A seesaw model for cell fate conversion A modified diagram of the seesaw model [18].
Blue clouds indicate the regions that the cell states are likely to sample with noise.
The pluripotent state (red ball) is located near the balance region.
When the seesaw is balanced between the two differentiation potentials, the cell has a higher probability of entering the pluripotent state.
ME stands for mesendoderm and ECT stands for ectoderm.
Shu J and Deng H/ Lineage Specifiers in the Induction of Pluripotency 261 counteracted at the dynamic balance point of the seesaw.
In other words, no particular lineage-specifying activity is dominant in inhibiting the pluripotency module.
In this case, the pluripo-tent state becomes achievable, eliciting the Oct4 and Sox2 self-activating module to coordinate with other pluripotency factors, thus collaboratively restoring the pluripotency network.
Once the cross-activating pluripotency module is activated, the ME and ECT lineage fates are blocked by Sox2 and Oct4, respec-tively.
As a result, the pluripotent state is maintained [18].
This innovative model can illustrate the aforementioned points and predicts novel strategies for cell fate conversion, including strategies for the direct conversion of somatic cells into iPSCs by pluripotency factors or lineage specifiers along with the direct conversion of somatic cells into specific lineages by lineage specifiers or pluripotency factors.
Direct reprogramming into other cell types by lineage specifiers or pluripotency factors The direct reprogramming strategy for cell fate conversion has beenwidely adapted for some other cell types in addition to iPSCs.
The direct conversion of fibroblasts into myoblasts by over-expressing MyoD was reported in 1987 by Davis and colleagues [23].
Recently, increasing numbers of different cell types have been obtained by direct conversion, termed transdifferentiation.
For example, a combination of three neuronal lineage-specific tran-scription factors, Ascl1, Brn2 and Mytl1, is sufficient to induce neurons from fibroblasts [24].
The cardiac-specific transcription factors Gata4, Tbx5 and Mef2c can induce fibroblast transdiffer-entiation into cardiomyocytes [25].
It is therefore assumed that the more specific and closer the endogenous regulatory network is to the factors, the more efficient the conversion will be [26].
The seesaw model also predicts that inhibiting the mu-tual antagonistic lineage-specifying forces could convert one cell type into another.
Furthermore, consistent with the seesaw model, repro-gramming factors have been reported to directly produce line-age-committed cells.
Two pivotal pluripotency factors, Oct4 and Sox2, were reported to regulate ESC differentiation into different germ layers and to induce direct conversions between different cell types beyond iPSCs.
Previous studies have shown that a twofold increase in Oct4 expression induces ESCs toward mesendodermal specification [15], whereas high levels of Sox2 trigger the neuroectodermal commitment of ESCs.
Recently, it was reported that Oct4 and Sox2 also orchestrate a germ-layer fate selection.
Oct4 inhibits neuroectodermal differentiation and promotes mesen-dodermal differentiation, whereas Sox2 promotes neuroecto-dermal differentiation and inhibits mesendodermal differentiation [16,17].
More recently, overexpression of Oct4 in fibroblasts was shown to lead to transdifferentiation into hematopoietic cells of a mesendodermal lineage [27], whereas overexpression of Sox2 directly converted fibroblasts into neural stem cells.
Additionally, decreased expression of Oct4 among the four Yamanaka factors can result in the direct conversion of fibro-blasts into neural stem cells [2830].
These discoveries suggest that pluripotency factors, such as Oct4 and Sox2, can regulate not only pluripotency but also lineage specification.
Conclusion and outlook After the discovery of the famous Yamanaka factors, a set of transcription factors consisting of Oct4, Sox2, Klf4 and 262 Genomics Proteomics Bioinformatics 11 (2013) 259263 c-Myc, regenerative biology has stepped into a new era.
Increasing numbers of pluripotency-related factors have been identified either to replace the Yamanaka factors or to boost the process.
Meanwhile, direct transdifferentiation has been successfully demonstrated through a similar strategy, by using lineage-specific transcription factors for specifying each lineage fate.
Recently, discoveries and notions related to the seesaw model for cell fate conversion have introduced a novel scenario for cell fate conversion causing us to re-evaluate the characteristics of pluripotency factors and lineage specifiers, which are two rivals in the conventional conception of the development of cellular identity.
Increasing evidence suggests that pluripotency factors are also lineage specifiers.
For example, Oct4 specifies ME differentiation while inhibiting ECT differentiation, and induces hematopoi-etic transdifferentiation from fibroblasts.
Sox2 directs ECT differentiation while prohibiting ME commitment, and in-duces direct transdifferentiation from fibroblasts into neural stem cells.
Overexpression of Nanog, Esrrb, or Tbx3 pro-motes mesendodermal determination [14].
Lastly, the lineage specifiers depicted as pluripotency rivals, such as GATA3 and PAX6, have been identified to be able to restore pluripo-tency in somatic cells.
Based on these discoveries, we should reconsider the defini-tions of pluripotency and lineage specification and present a novel perspective for understanding the determinants of cellu-lar identity, which is one of the most important topics in mod-ern biology.
Competing interests The author has declared that no competing interests exist.
Acknowledgements This work was supported by the National Basic Research Pro-gram of China (973 Program, Grant No.
2012CB966401), the Key New Drug Creation and Manufacturing Program (Grant No.
2011ZX09102-010-03), the National Science and Technol-ogy Major Project (Grant No.
2013ZX10001003), the Ministry of Science and Technology (Grant No.
2011DFA30730 and 2013DFG30680) and 111 Project.
ABSTRACT Motivation: Transcription factors (TFs) are proteins that regulate gene activity by binding to specific sites on the DNA.
Understanding the way these molecules locate their target site is of great importance in understanding gene regulation.
We developed a comprehensive computational model of this process and estimated the model parameters in (N.R.Zabet and B.Adryan, submitted for publication).
Results: GRiP (gene regulation in prokaryotes) is a highly versatile implementation of this model and simulates the search process in a computationally efficient way.
This program aims to provide researchers in the field with a flexible and highly customizable simulation framework.
Its features include representation of DNA sequence, TFs and the interaction between TFs and the DNA (facilitated diffusion mechanism), or between various TFs (cooperative behaviour).
The software will record both information on the dynamics associated with the search process (locations of molecules) and also steady-state results (affinity landscape, occupancy-bias and collision hotspots).
Availability: http://logic.sysbiol.cam.ac.uk/grip Contact: n.r.zabet@gen.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on November 14, 2011; revised on February 2, 2012; accepted on March 12, 2012 1 INTRODUCTION It is well established now that transcription factor (TF) find their target site through facilitated diffusion, a combination between 1D random walk on the DNA and 3D diffusion in the cytoplasm (Berg et al., 1981; Elf et al., 2007).
Once bound to the DNA, TFs perform three main types of movements: (i) sliding , (ii) hopping and (iii) jumping (Mirny et al., 2009).
The first two mechanisms, sliding and hopping, assume that the TF performs small movements on the DNA without releasing into the cytoplasm, whereas the third assumes a 3D diffusion in the cytoplasm before rebinding.
With few exceptions, most of the theoretical efforts have been invested into analytical solutions of the facilitated diffusion mechanism.
If one wants to consider real DNA sequences and dynamic crowding on the DNA (mobile roadblocks), then this rules out analytical solutions.
Computational methods and, in particular, stochastic simulations overcome these limitations and To whom correspondence should be addressed.
provide a more accurate mechanistic representation of the underling biological process.
In particular, these type of stochastic simulations can be used to answer question related to how TFs perform the search process.
For example, one could investigate whether molecules prefer to hop or to slide and what is the contribution of these two alternative movements on the DNA to the overall 1D random walk in a crowded environment.
Building on the comprehensive model constructed in (N.R.Zabet and B.Adryan, submitted for publication), we developed GRiP (gene regulation in prokaryotes), a program that allows stochastic simulation of the search process of TFs for their target sites on the DNA.
The analyzed systems can be large.
For example, Escherichia.coli K-12 has a 4.6 Mbp genome and there are 104 DNA binding proteins (agents).
To produce results within relative short time, previous software had to either rely on coarse grain models (Wunderlich and Mirny, 2008) or to consider small subsystems (Chu et al., 2009).
GRiP represents a new and efficient implementation of the TF search process, which considers a highly detailed model of 1D diffusion and, at the same time, it simulates at least 4 times faster than previous software (Barnes and Chu, 2010; Chu et al., 2009).
Consequently, by allowing genome-wide stochastic simulations of a highly detailed model of facilitated diffusion, GRiP can highlight possible biases in the results, where the level of details was insufficient (coarse grain models) or the size of the analyzed system was too small.
A few studies, such as Das and Kolomeisky (2010), addressed the problem of facilitated diffusion through simulations focusing on the 3D diffusion rather than the 1D case.
The 3D diffusion is time and resource consuming, especially for simulations at the genome level.
van Zon et al.
(2006) showed that the model based on the zero-dimensional Chemical Master Equation can reliably represent the rate at which TFs associate non-specifically with the DNA, as long as the model takes into account that once a molecule unbinds from the DNA, it has a high probability of fast rebinding in close proximity.
This suggests that there is no need to simulate the 3D diffusion explicitly, but rather have this replaced by a simple arrival rate and ensuring that the model incorporates the fast rebinding probability in the unbinding rate, a strategy which we also adopt.
2 DESCRIPTION We implemented the target finding process as a hybrid model mixing agent-based methods with event driven stochastic simulation algorithms (Gillespie, 1977).
The software is implemented in Java 1.6, which ensures high portability.
The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: APPLICATIONS NOTE [12:59 9/4/2012 Bioinformatics-bts132.tex] Page: 1288 12871289 N.R.Zabet and B.Adryan In the simulator, each TF molecule is represented as an agent able to perform certain actions, whereas the DNA molecule is modelled as a string of base pairs (A, T, C, G).
There is no measure of distance between the molecules, but the TF molecules can be either free in the cytoplasm or bound on the DNA at certain positions.
The free TF molecules have only one action available, namely to bind to the DNA.
The cytoplasm is assumed to be a perfectly mixed reservoir from where the free TF molecules can find the DNA at exponentially distributed times.
To simulate the 3D diffusion we use the Direct Method implementation of Gillespie Algorithm (Gillespie, 1977) which generates a statistically correct trajectory of the Master Equation.
The model considers volume exclusion, allowing only one TF to cover certain base pair at any specific time point.
A bound molecule will occupy a number of consecutive base pairs on the DNA.
The size on the DNA of each TF molecule is computed as the number of base pairs of the DNA binding motif added to the number of obstructed base pairs on the left side of the molecule and the number of obstructed base pairs on the right side.
A feature which was not considered by previous models (Barnes and Chu, 2010; Chu et al., 2009) is TF orientation on the DNA.
If TFs are not symmetric, the user can set TF molecules to have two orientations on the DNA, which can lead to different affinities depending on the molecule orientation.
Whenever a TF binds to the DNA, the system selects a random orientation.
This can be changed only after the TF molecule unbinds and rebinds to the DNA, including during hops.
The simulator supports the definition of multiple TF species, which are classified in two types: (i) non-cognate TFs and (ii) cognate TFs.
The cognate ones are the TFs that are of interest and that we can follow, whereas the non-cognate ones main purpose is to simulate the other proteins on the DNA, which might interfere with the search process of the cognate TFs.
For efficiency reasons, we pre-calculate the affinities of each TF species, both cognate and non-cognate, and store them in individual arrays.
The non-cognate binding energy is randomly generated using a Gaussian distribution with the mean and variance provided as inputs for each non-cognate species.
For cognate TFs, there are several ways in which the binding energy can be computed, but this work is restricted to three well known ones: (i) mismatch energy (Gerland et al., 2002); (ii) position frequency matrix (PFM) and information theory (Stormo, 2000); and (iii) PFM and binding energy (Berg and von Hippel, 1987).
In all scenarios, we assume that each position in the DNA binding motif is approximately independent and additive.
A bound TF molecule can perform, with user-defined probabilities, one of the following actions: (i) slide left; (ii) slide right; (iii) hop to a position that is Gaussian distributed around original position with a user-defined variance; or (iv) unbind from the DNA.
We assume reflecting boundaries.
In the case the molecule unbinds, there is a certain probability that it will rebind fast near the original place.
Finally, the model allows cooperative behaviour between TF molecules and this can be either mediated by DNA (binding of one molecule to a certain site on the DNA can alter the affinity between another molecule and a different site) or represented as direct TFTF interaction (two molecules bound to the DNAand in physical contact can have different affinities for their current positions compared with the case where they are not in contact); for more details see (N.R.Zabet and B.Adryan, submitted for publication).
The simulation speed is sensitive to the number of agents in the system.
This mainly comes from the fact that the events queue becomes larger with increasing number of molecules in the system and, consequently, higher queues require higher maintenance time.
For 106 TF molecules and the genome of E.coli K-12 (4.6 Mbp), we can simulate 4 105 events per second on a Mac Pro 2x2.26 GHz quad-core Intel Xeon with 32 GB memory running Mac OSX 10.6.8.
3 DISCUSSION GRiP is a highly versatile program which comes with both command-line interface and graphical user interface.
Furthermore, being written in Java, the software can be run on any machine where the Java Runtime Environment 1.6 (or higher) is installed.
The program takes as input a parameters file, which can specify, among many other parameters, three additional data files, namely: (i) the DNAsequence file (from a FASTAfile); (ii) TF file (a csv file with TF-specific characteristics) and, optionally; (iii) TF cooperativity file (a csv file).
Note that, if either the DNA sequence file or the TF file are not provided, then the simulator can randomly generate that data (DNA sequence or TF species).
Once started, the simulation runs until the time in the cell reaches a predefined stop time, or until all target sites are reached (if the stop time is set to 0).
As output, the simulator can print information on: (i) the position of TF molecules on the DNA (or proportion of bound molecules to the DNA); (ii) computed affinity landscapes for each TF species; (iii) measured occupancy bias for each TF species; (iv) statistical information related to TF species (such as residence time, sliding lengths, actual sliding lengths, binding events etc.
); (v) simulation speed; (vi) stored sliding lengths for each species; and (vii) statistics on collisions (total number, total number per species and hot spots on DNA).
GRiP can simulate 1 s of E.coli K-12 and lacI using biologically plausible parameters between 1 h and 4 h (depending on the simulation parameters, the machine on which the simulation is run and even on the interface of the application, GUI or command line), which means that one can simulate up to 10 min of a bacterial cell within a month; for details see Supplementary Material.
Funding: Medical Research Council [G1002110 to N.R.Z.]
and the Royal Society [B.A.].
Conflict of Interest: none declared.
ABSTRACT Summary: Community curationharnessing community intelligence in knowledge curation, bears great promise in dealing with the flood of biological knowledge.
To exploit the full potential of the scientific com-munity for knowledge curation, multiple biological wikis (bio-wikis) have been built to date.
However, none of them have achieved a substantial impact on knowledge curation.
One of the major limitations in bio-wikis is insufficient community participation, which is intrinsically because of lack of explicit authorship and thus no credit for commu-nity curation.
To increase community curation in bio-wikis, here we develop AuthorReward, an extension to MediaWiki, to reward com-munity-curated efforts in knowledge curation.
AuthorReward quanti-fies researchers contributions by properly factoring both edit quantity and quality and yields automated explicit authorship according to their quantitative contributions.
AuthorReward provides bio-wikis with an authorship metric, helpful to increase community participation in bio-wikis and to achieve community curation of massive biological knowledge.
Availability: http://cbb.big.ac.cn/software.
Contact: zhangzhang@big.ac.cn Supplementary information: Supplementary data are available at Bioinformatics online.
Received on April 1, 2013; revised on May 8, 2013; accepted on May 13, 2013 1 INTRODUCTION Biological knowledge is generated at ever-faster rates and dis-persed among researchers and across literatures.
As each new biological study has become increasingly dependent on the avail-ability of existing knowledge, comprehensive and up-to-date col-lection of biological knowledge across a wide variety of research fields is of critical significance in life sciences (Clark, 2007).
Traditionally, biological knowledge has been aggregated through expert curation, conducted manually by dedicated ex-perts.
However, with the burgeoning volume of biological data and increasingly diverse densely informative published litera-tures, expert curation becomes more and more laborious and time consuming, increasingly lagging behind knowledge creation.
Accordingly, community curationharnessing community intel-ligence for knowledge curationhas gained significant attention as a solution to this issue (Salzberg, 2007; Waldrop, 2008; Zhang et al., 2011).
A successful example that engages community in-telligence in knowledge aggregation is Wikipedia that features up-to-date content, huge coverage and low cost for maintenance.
Spirited by the extraordinary success of Wikipedia, multiple bio-logical wikis (bio-wikis) have been built to date (Supplementary Table S1).
However, bio-wikis have not achieved a substantial impact on community curation of biological knowledge (Finn et al., 2012).
One of the major limitations in bio-wikis is insufficient partici-pation from the scientific community, which is intrinsically be-cause of lack of explicit authorship and thus no credit for community-curated contributions (Finn et al., 2012; Howe et al., 2008).
A valuable attempt has been made to motivate community contributions in wikis by means of social rewarding techniques (Hoisl et al., 2007), but it does not provide explicit authorship for any wiki page.
Although authorship has been introduced in a non-MediaWikibased system (Hoffmann, 2008), it only links every sentence to its author but does not provide a quantitative measure of authorship, and most import-ant, it is inapplicable to extant bio-wikis that are largely built on MediaWiki (a free, open source and widely used wiki engine, which is adopted by Wikipedia).
Several initiatives based on se-mantic web technologies have already emerged for biological knowledge management (Antezana et al., 2009).
However, they do not promise to manage or quantify authorship of the free text in bio-wikis.
To increase community curation in bio-wikis, here we develop AuthorReward, an extension to MediaWiki, to reward community-curated efforts in bio-wikis by contribution quantification and explicit authorship.
2 ALGORITHMS MediaWiki allows anyone to develop customized functionalities by packaging a bunch of codes as MediaWiki extensions.
Thus, AuthorReward is implemented as an extension to MediaWiki.
Although MediaWiki itself includes an infrastructure for individ-ual contributions to be recognized, it only records the revision history and provides no explicit authorship.
*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com A wiki page contains a collection of knowledge on a specific subject, where multiple researchers are most likely to collabora-tively provide edits.
AuthorReward aims to provide a viable quantification for researchers contributions in bio-wikis.
A major concern to automated authorship has been ensuring that authorship cannot be manipulated by spurious, short-lived edits (Supplementary Text S1).
For any wiki page p, we assume there are a series of edit versions v0, v1, v2, .
.
., vn, where version v0 is empty and n40.
AuthorReward counts multiple successive ver-sions edited by a researcher as one version.
Thus, any neighbor-ing versions, vi 1 and vi (where 1 i n), are edited by different researchers.
The edit distance between vi and vj, termed as d(vi, vj) (where i5j), is computed by the Levenshtein distance (LD) (Levenshtein, 1966) that measures the minimum number of edit operations (insertions, deletions and substitutions) required to transform one string into the other.
In AuthorReward, the contribution score of version vi, CS(vi), is formulated straightfor-wardly as CSvi cdvi1, vn dvi, vn, 1 where c is the scale factor, d(vi 1, vn) is the edit distance between vi 1 and vn and d(vi, vn) is the edit distance between vi and vn.
In Equation (1), CS(vi) factors edit quality as well as edit quantity in an implicit manner; the edit quantity of version vi, QTY(vi), amounts to the edit distance between vi and its previous version vi 1, viz., d(vi 1, vi) [Equation (2)], and the edit quality of version vi, QAL(vi), corresponds to whether the edit persists in comparison with the last version vn [Equation (3)].
QTYvi dvi1, vi 2 QALvi dvi1, vn dvi, vn dvi1, vi 3 According to the triangle inequality, QAL(vi) ranges from 1, when the edits were entirely reverted, to 1, indicating that the edits were totally preserved in the last version.
Therefore, QAL(vi), in other words, measures how long the edit lasts in the latest version; a high (or low) quality score is given for ver-sion vi, if it is long-lived (or short-lived).
Consequently, CS(vi) can be expressed by QTY(vi) multiplied by QAL(vi), namely, CS(vi)QTY(vi)QAL(vi).
Thus, CS(vi) is not easily gamed, providing a viable quantification for researchers contributions.
Considering that one researcher may provide many discon-tinuous edits across the evolution of a wiki page, and thereby contribute multiple versions in one wiki page, the contribution score of researcher r in page p, S(r, p), is quantified as the sum over all contributed versions, Sr, p X vi2Er, p CSvi, 4 where E(r, p) is a set of versions contributed by researcher r in page p. As a consequence, the total contribution of researcher r in a bio-wiki is termed as the sum of multiple contribution scores in all participated pages, Sr X p2P Sr, p, 5 where P is a set of pages in which researcher r provides edits.
3 APPLICATION AND FEATURES To test the functionality of AuthorReward, we installed it in RiceWiki (http://ricewiki.big.ac.cn).
For testing purposes, we chose the semi-dwarfing gene (sd1), which is one of the most important genes deployed in modern rice breeding and is also known as the green revolution gene affecting plant height of rice.
There were nine researchers collaboratively annotating the sd1 gene, providing 87 versions as of August 23, 2012 (Supplementary Table S2; http://ricewiki.big.ac.cn/index.php/ Os01g0883800).
As testified on the sd1 gene (Supplementary Fig.S1), AuthorReward is capable of yielding sensible quantitative contri-butions and providing automated explicit authorship, consistent well with perceptions of all participated contributors.
Moreover, AuthorReward features good compatibility with any MediaWiki-based system and simple installation, consequently possessing a broad scope for its application and providing a consistent appearance and functionality as Wikipedia.
4 CONCLUSION AuthorReward provides bio-wikis with an authorship metric, fea-turing robust contribution quantification and automated explicit authorship.
When contribution is appropriately quantified and authorship is duly rewarded, it is possible to exploit the full potential of the scientific community in knowledge curation.
Although AuthorReward does not contribute directly to the integration of biological knowledge, it provides a standard prac-tice to reward community-curated efforts, which in return can increase community participation in bio-wikis for knowledge curation.
Thus, our intention here is to produce an automated, simple and robust authorship metric and no automated measure will be able to gauge scientific content.
AuthorReward can be used in combination with semantic web technologies, potentially promising a significant advance for harnessing community intel-ligence for knowledge curation.
In addition, social rewarding techniques (e.g.
peer rating) can be used together with AuthorReward for contribution evaluation.
Moreover, it is likely in the long term to integrate community-curated efforts across multiple bio-wikis for each researcher, which accordingly requires close collaborations among bio-wikis and standardized mechanisms for individual identity recognition (e.g.
OpenID at AuthorReward provides a standard practice to reward commu-nity-curated efforts in bio-wikis, and it is of interest to the sci-entific community intending to perform knowledge curation collectively and collaboratively in bio-wikis and also other domain wikis.
ACKNOWLEDGEMENTS The authors thank Jun Yu, Lina Ma, Gang Wu, Hao Wu, Chao Xu, Jian Sang and Ang Li for their valuable comments on this work.
Funding: National Programs for High Technology Research and Development (863 Program; 2012AA020409); the 100-Talent Program of Chinese Academy of Sciences (Y1SLXb1365); 1838 L.Dai et al.
National Natural Science Foundation of China (60803050, 61132009); USA National Institutes of Health P01 (GM068067).
Conflict of Interest: none declared.
ABSTRACT Summary: The analysis of gene regulatory networks (GRNs) is a central goal of bioinformatics highly accelerated by the advent of new experimental techniques, such as RNA interference.
A battery of reverse engineering methods has been developed in recent years to reconstruct the underlying GRNs from these and other experimen-tal data.
However, the performance of the individual methods is poorly understood and validation of algorithmic performances is still missing to a large extent.
To enable such systematic validation, we have developed the web application GeNGe (GEne Network GEnerator), a controlled framework for the automatic generation of GRNs.
The theoretical model for a GRN is a non-linear differential equation system.
Networks can be user-defined or constructed in a modular way with the option to introduce global and local network perturbations.
Resulting data can be used, e.g.
as benchmark data for evaluating GRN reconstruction methods or for predicting effects of perturbations as theoretical counterparts of biological experiments.
Availability: Available online at http://genge.molgen.mpg.de Contact: hache@molgen.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Inferring gene regulatory networks (GRNs) from experimental data is a challenging task becoming increasingly important with routine practical use of corresponding experimental techniques, such as RNA interference combined with microarray or next generation sequencing.
Various computational algorithms for reconstructing GRNs from experimental data have been developed in the last decades (see Supplementary Material for an overview).
Besides the algorithmic developments, the actual assessment of methods performances remains a challenge, primarily due to the lack of experimental benchmark data.
However, such systematic validation is crucial, since it shows strengths and weaknesses of the methods and their suitability for the specific problem domain (time series or perturbation experiments, noisiness of data, etc.).
Availability of experimental data, with a few exceptions, such as the network described by Davidson et al.
(2002), is still the major bottleneck for GRN reconstruction.
Hence, generating simulated data derived from theoretical considerations is still the method of choice for constructing benchmark datasets and for conducting To whom correspondence should be addressed.
performance studies on individual methods.
These theoretical models should reflect features and complexity of real regulatory processes.
They allow performance analysis under well-defined conditions using appropriate network characteristics, network complexity, noise levels, missing data or other hidden information.
This knowledge can aid further algorithmic developments and guide improvements of experimental as well as analytical methods.
There are some tools that provide such forward GRN modeling approaches, such as SynTReN (den Bulcke et al., 2006), RENCO (Roy et al., 2008) or SynBioSS (Hill et al., 2008).
However, despite of their usefulness they lack some features such as automatic generation of different network types, manipulation of network structure, simulation of global and local perturbation and visualization of simulation results in a single framework, specialized for GRNs (Supplementary Material).
To meet the above mentioned requirements, we have developed the GRN generator GeNGe (GEne Network GEnerator), a web application to model GRNs of different types.
The GRNs are used to set up a deterministic ordinary differential equation (ODE) system.
The gene regulatory model system is composed of instances of mRNAs and proteins acting as transcription factors (TFs) and their corresponding target genes.
Non-linear kinetics based on the logic described by Schilstra and Nehaniv (2008) are used to describe the influence of sets of independently or jointly binding TFs on the expression of a gene.
Various dynamics can be modeled, such as oscillation and bistability (Supplementary Material).
Global perturbations (network noise) or local perturbations of a single or multiple network nodes can be simulated and the resulting time series are visualized in order to display the perturbation effects.
All results can be downloaded and used for validation of reverse engineering methods or studies of the network dynamics.
Moreover, GeNGe offers features for the topological characterization of GRNs.
Network parameters are computed, such as in-and out-degree distributions, average path lengths and clustering coefficients.
Furthermore, by varying parameters of the kinetic laws or by choosing different kinetics, in silico analyses can be performed, e.g.
on the effects of knock-downs (partial knock-downs) of a single gene or groups of genes.
The results can be used to define critical network nodes and suitable candidates for perturbation experiments and thus guide future experimental work.
2 FUNCTIONALITY The workflow of GeNGe is divided into three levels (Fig.1).
In the first level, the network level, networks are added to a network 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:51 31/3/2009 Bioinformatics-btp115.tex] Page: 1206 12051207 H.Hache et al.
Fig.1.
Flowchart of the simulation process.
It is divided into three levels, the network level, to generate a network topology; the kinetic level, to select kinetic laws of the dynamic model; and the simulation level, to set the parameter values and simulate time series with local or global perturbation.
repository that will be used for further analyses and simulations.
GeNGe provides several pre-defined GRNs, such as a part of the developmental network in sea urchin described by Davidson et al.
(2002), artificial networks and network motifs.
Furthermore, the upload of user-defined networks, in form of tables or adjacency matrices, is supported.
Various artificial networks can be generated such as random networks, scale free networks and networks composed of small regulatory network motifs (Barabsi and Oltvai, 2004; Bollobs et al., 2003; Lee et al., 2002).
Network parameters can be adjusted by the user to generate networks with specific topological characteristics.
Furthermore, it is possible to change any network by adding or deleting nodes and edges as well as associated regulation strengths.
TFs are assumed to bind independently on the DNA.
Nevertheless, sets of jointly binding TFs can be specified.
The networks can be visualized and diverse topological measures are calculated, e.g.
in-and out-degree distributions, average path lengths and clustering coefficients.
In the next level, the kinetic level, kinetics of the model are specified.
Degradation of mRNA and protein can be modeled by a linear or a MichaelisMenten kinetic.
The translation is described by a linear kinetic law.
For the transcription dynamic, different non-linear kinetic laws can be selected.
In the third level, the simulation level, parameters of individual kinetic laws can be specified or set randomly.
Based on the network topology, the kinetics and the parameters, an ODE system of the network is set up and exported to PyBioS simulation engine via a web-services based API (Wierling et al., 2007).
Besides unperturbed time series analysis, global perturbations (such as Gaussian noise) as well as single or multiple local network perturbations (e.g.
knock-downs) can be introduced and the resulting steady states of the system are computed.
The procedures can be repeated with different settings and used in an iterative way.
All resulting time series can be visualized.
For knock-down experiments, the ratio of each network node of the knock-down and control simulations is calculated and visualized in the network graph.
All results, including the networks in the format of Systems Biology Markup Language (SBML), time series and simulation parameters Fig.2.
Example workflow in GeNGe.
(A) Pre-defined network Simple Oscillator is selected.
(B) A kinetic schema for transcription and degradation is specified.
(C) Local perturbations (knock-down) of gene lacI of degree 80% is selected.
(D) Simulated time courses of the mRNA and proteins for control (blue) and knockdown (red) can be visualized or downloaded.
can be downloaded for further analyses.
More details about the network and data generator is given in the Supplementary Material.
3 EXAMPLE An Example workflow in GeNGe is shown in Figure 2 which is adapted from the synthetic repressilator by Elowitz and Leibler (2000).
The pre-defined network Simple Oscillator is added to the network repository.
A local perturbation of gene lacI is introduced with a knock-down degree of 80%.
The control and knock-down time series are calculated.
The oscillations of the mRNAs and proteins are still observable with this rate of knock-down, however, there are changes in frequency and amplitude.
A breakdown of the oscillations is observable at 97% rate of knock-down (data not shown).
Whereas the effect of local perturbations is straightforward in small networks, in larger networks the impact of such perturbations on the steady state of the system is less obvious and thus simulations can guide experimental work in selecting the most promising candidates.
An example given in the Supplementery Material shows that knock-downs of genes encoding highly connected TFs with many targets have not always a large impact on the global system state.
In contrast, TFs with critical positions within the network can have a large downstream effect even if they have only a few direct targets.
ACKNOWLEDGEMENTS We thank Maria Schilstra for valuable comments on the kinetics.
Funding: Max Planck Society and the EU under its sixth Framework Programme [grants EMBRACE (LSHG-CT-2004-512092), AnEUploidy (LSHG-CT-2005-037627), the seventh 1206 [12:51 31/3/2009 Bioinformatics-btp115.tex] Page: 1207 12051207 GeNGe Framework Programme project APO-SYS (HEALTH-F4-2007-200767)]; German Federal Ministry of Education and Research (BMBF) through the NGFN Plus research initiative (Mutanom project (01GS08105)).
Conflict of Interest: none declared.
ABSTRACT Motivation: There is a strong demand in the genomic community to develop effective algorithms to reliably identify genomic variants.
Indel detection using next-gen data is difficult and identification of long structural variations is extremely challenging.
Results: We present Pindel, a pattern growth approach, to detect breakpoints of large deletions and medium-sized insertions from paired-end short reads.
We use both simulated reads and real data to demonstrate the efficiency of the computer program and accuracy of the results.
Availability: The binary code and a short user manual can be freely downloaded from http://www.ebi.ac.uk/kye/pindel/.
Contact: k.ye@lumc.nl; zn1@sanger.ac.uk 1 INTRODUCTION Amajor part of the genetic difference between individuals is encoded in the form of structural variations, such as insertions, deletions and duplications.
In previous studies, hundreds of large-scale structural variants have been identified using arrayCGH (Iafrate et al., 2004; Sebat et al., 2004).
Polymorphic transposon insertions (Bennett et al., 2004) and other short indel polymorphisms (Mills et al., 2006) have been discovered using capillary sequencing reads.
Clone-end capillary sequencing provided a means to interrogate the intermediate size structural variations and particularly, the method is able to find relatively large insertion events (Kidd et al., 2008).
Whole genome complete sequencing created a more detailed catalog of structural variations for single human individuals (Levy et al., 2007; Wheeler et al., 2008).
For example, the de novo assembly of the Venter genome played an essential role in detecting long insertions, deletions and structural rearrangements.
Recent efforts on variant detection have been fueled up by next-gen high throughput sequencing.
Using the Illumina platform for a complete sequencing of a human individual, Bentley et al.
(2008) reported 4-million SNPs and 0.4-million short indels of size 116 bp.
However, large indel events were not extensively studied To whom correspondence should be addressed.
by the authors.
For the next-gen sequencing data, there are a number of ways to detect long variants, notably assembly (complete de novo or using unmapped reads only), read splitting, read coverage depth analysis, inconsistencies of insert sizes through paired-end mapping, etc.
Among these, de novo assembly from short reads perhaps offers the best chance for long indels and structural rearrangements.
There are a number of short read assemblers based on de Bruijn graphs.
However, assembly of short read data is most successful when applied to small genoms like bacterial genomes (Chaisson and Pevzner, 2008; Zerbino and Birney, 2008).
Above the eukaryotic level, it would be problematic due to repetitive genome regions.
Given the current read length of 3575 bp in the next-gen sequencing platforms, lack of high quality de novo assembly looks like to continue in the near future.
Therefore, the need is eminent in the genomic community to develop read mapping related algorithms in order to reliably identify structural variants.
In this article, we present Pindel, a method that uses pattern growth algorithm to identify the break points of large deletions (1 bp10 kb) and medium sized insertions (120 bp) from 36 bp paired-end short reads.
We will start with introducing pattern growth for string matching.
Then Pindel, the procedure of computing medium sized insertions and large deletions from paired-end short reads, will be illustrated.
We test our Pindel program with simulated paired-end short reads on human chromosome X.
We also report the results of Pindel using the whole human genome data of NA18507, sequenced on an Illumina platform.
Finally Runtime and peak memory usage are analyzed to demonstrate the efficiency of Pindel.
2 METHODS 2.1 Pattern growth for exact string matching In our previous study, we followed the principles of the pattern growth data structure as presented in PrefixSpan, a sequential pattern mining algorithm (Pei et al., 2004), and introduced various constraints during the data mining process to mine biological meaningful patterns from unaligned protein sequences (Ye et al., 2007).
Since the data structure of pattern growth and its extensions to the analysis of protein sequences have been well documented before, here we only briefly introduce how to use pattern growth to find The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[21:54 21/10/2009 Bioinformatics-btp394.tex] Page: 2866 28652871 K.Ye et al.
minimum and maximum unique substrings of a given pattern against a sequence database.
In our particular application those unique substrings must start from either the leftmost or the rightmost base of pattern P. For simplicity, we only describe the procedure of finding minimum and maximum unique substrings starting from the leftmost base of the pattern P. The inputs of the algorithm are a dataset S which consists of a series of non-empty sequences of the alphabet {A, C, G, T}, a pattern P to search for the locations of its minimum and maximum substrings.
The output of the algorithm consists of all substrings (and their locations) starting from the leftmost base of P that appear exactly once in S. The algorithm works as follows.
Here a is a substring that starting from the leftmost base of P and Sa is the so-called projected database that contains all sequences that contain the substring a, where the last element of each occurrence of a is marked (the so-called a-locations).
The computation of Sa from Sa requires, for each a-location, the check whether or not the base on its right-hand side equals the newly appended item b.
In case of equality this gives an a-location in Sa .
Sequences without any a-location are removed from the projected database.
The main call is unique (, S), where is the empty substring; note that each base in database S is a-location, including the position before each sequence.
This call creates a projected database that marks all occurrences of the first base b of pattern P. Let us use a simple example to explain the procedure above.
Suppose we define a database S of genome sequence as ATCAAGTATGCTTAGC and the pattern P is ATGCA.
In the first step, we scan the whole database for A, the first base of pattern P. The locations of A (red A in ATCAAGTATGCTTAGC) are stored in a projected database of A.
In the second step, we look for T as it is the second base in pattern P at the right side of As identified previously.
The projected database for AT then only contains two locations (ATCAAGTATGCTTAGC).
When we search for the third base G of pattern P at the right sides of AT, we found that ATG appears exactly once in the database S (ATCAAGTATGCTTAGC).
Thus we know that ATG is the minimum unique substring of pattern P in the database S. After we examine the fourth and fifth base of pattern P, we notice that ATGC is also unique in the database S but ATGCA isnt.
In this case we know that ATGC is the maximum unique substring of pattern P in this particular database S. 2.2 General procedure of Pindel In the Pindel program, we aim to compute the precise break points as well as the fragments inserted or deleted compared to the reference genome from paired-end reads.
In the preprocessing step, we first use SSAHA2 (Ning et al., 2001) to map all the reads to the reference genome.
Then the mapping results are examined to keep those paired reads that only one end can be mapped.
For each of those read pairs, the mapped end must be uniquely located in the genome with no mismatch bases while the other end cannot be mapped to anywhere in the genome under a given threshold alignment score (s = 20 for 36 bp reads).
For each of those pairs, our Pindel program uses the mapped end to determine the anchor point on the reference genome and the direction of the unmapped read.
Knowing the anchor point, the direction to search for the unmapped read and the user defined Maximum Deletion Size (Max_D_Size) parameter, a sub-region in the reference genome can be located, where Pindel will break the unmapped read into 2 (deletion) or 3 (short insertion) fragments and map the two terminal fragment separately.
2.3 Detecting large deletions When we map paired-end reads to the reference genome, for the majority of the reads, both ends can be mapped to the reference genome.
However, a small portion of them might have only one end mapped to the reference genome.
One of the possibilities is that the unmapped read mate spans the break point of a deletion event in the test sample compared to the reference genome as shown in Figure 1(a).
Thus, those unmapped reads actually carry the information about the precise break points of the deletion event.
If we can find a proper position to split the read into two fragments, which can (a) (b) Fig.1.
Detecting deletion events.
(a) When mapping paired-end reads to the reference genome, some reads may not be mapped even allowing a few mismatches because they are just across the break points of deletion events.
If we can find a proper position to break the read into two fragments and map them separately, we will be able to compute the exact break points and the fragment deleted compared to the reference.
If find more supporting evidences can be found, the possibility of the deletion event will be higher in the test sample.
For simplicity, we only depicted one mapped read (green arrow); (b) The procedure to break the unmapped read into two parts at appropriate position and mapped them separately to the reference genome.
The location and direction of the mapped read (green) define the local region to break the unmapped read into two fragments and map them separately.
The 3 end of the mapped read is defined as anchor point.
Then pattern growth is used to search for minimum and maximum unique substrings from the 3 end of unmapped reads within the range of two times of insert size starting from the anchor point.
Using pattern growth again to search for minimum and maximum unique substrings from the 5 of unmapped read within the range of read length + user defined maximum deletion size starting from the already mapped 3 end of the unmapped read.
The computed minimum and maximum substrings from both 3 and 5 are examined to see whether a complete unmapped read can be assembled.
All possible solutions are stored in a database for sorting according to the break point coordinates.
A deletion event is reported if at least two reads support it.
be mapped back to the reference separately, we will be able to compute the exact positions of the break points and thus the fragment deleted compared to the reference.
If we collect multiple reads that support the same incidence, we will be more confident about the deletion event in the test sample.
Although for short reads, half of a read length might be too short to be mapped uniquely in a whole genome scale as large as humans, the location and the direction of the mapped read reduces the search space dramatically.
As shown in Figure 1b, when we have one read mapped (the anchor point) but the other one is not mappable, we only need to consider the local region at one side of the anchor point.
The computational procedure for each unmapped read is described as following (Fig.1b): (1) Read in the location and the direction of the mapped read from the mapping result obtained in the preprocessing step; (2) Define the 3 end of the mapped read as anchor point; 2866 [21:54 21/10/2009 Bioinformatics-btp394.tex] Page: 2867 28652871 Pindel: detecting break points of indels with pattern growth (3) Use pattern growth algorithm to search for minimum and maximum unique substrings from the 3 end of the unmapped read within the range of two times of the insert size from the anchor point; (4) Use pattern growth to search for minimum and maximum unique substrings from the 5 end of the unmapped read within the range of read length + Max_D_Size starting from the already mapped 3 end of the unmapped read obtained in step 3; (5) Check whether a complete unmapped read can be reconstructed combining the unique substrings from 5 and 3 ends found in steps 3 and 4.
If yes, store it in the database U.
Note that exact matches and complete reconstruction of the unmapped read are required so that neither gap nor substitution is allowed.
After processing all unmapped reads, sort the database U according to the breakpoint coordinates in the reference and output every deletion event supported by at least two reads.
User has to specify the parameter Max_D_Size and also change the minimum lengths for unique substrings reported in step 3 and 4, denoted as Min_C and Min_F, respectively.
2.4 Detecting medium sized insertions In Section 2.3, we explain how to compute the precise break points of deletion events and the deletion size could be rather large as long as we find unique matches for the two parts of the unmapped read.
It is, however, difficult to infer the fragment for large insertions directly from the read sequence.
In this case we aim to compute the precise break points and the fragment inserted in the medium sized range (<=20 bases for 36 bp reads).
The computational procedure is very similar to that used for searching deletions.
The only difference is in step 4 where the search range for the unique occurrence of minimum and maximum unique substrings from the 5 end of the unmapped read is read length minus one.
In this case, we certainly cannot reconstruct the whole read and the extra bases are an inserted fragment compared to the reference genome as shown in Fig.2.
Fig.2.
Detecting short insertion events.
The procedure to split the unmapped read into three parts at appropriate position and mapped the terminal two separately to the reference genome.
The location and direction of the mapped read (green) define the local region to split the unmapped read.
The 3 end of the mapped read is defined as anchor point.
Then pattern growth is used to search for minimum and maximum unique substrings from the 3 end of unmapped reads within the range of two times of insert size starting from the anchor point.
Using pattern growth again to search for minimum and maximum unique substrings from the 5 of unmapped read within the range of read length 1, starting from the already mapped 3 end of the unmapped read.
The computed minimum and maximum substrings from both 3 and 5 are examined to see whether they are adjacent to each other.
The middle fragment is the inserted fragment.
All possible solutions are stored in a database for sorting according to the break point coordinates.
An insertion event is reported if at least two reads support it.
2.5 Simulated data 2.5.1 Simulating paired-end reads on human X chromosome In order to evaluate our Pindel program, we first simulated indels on human X chromosome and examined how well Pindel can detect those simulated indels in the presence of SNPs and sequencing errors.
The sequence of human X chromosome was obtained at ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/.
We put 20 instances for each indel length we have chosen.
The insertion sizes vary from 120 bp while the deletion sizes range from 110, 100, 1, 10, 100 kb to 1 Mb.
SNPs are simulated at a rate of 0.001 and sequencing error rate is set to 0.005.
Using the above setting, we simulated 30 coverage of 36 bp paired-end short reads with an average insert size of 200.
The indels are randomly placed and their locations are recorded to facilitate comparison.
2.6 Real data Recently the genome of a male Yoruba from Ibadan, Nigeria (YRI, sample NA18507), was sequenced (Bentley et al., 2008).
The 4-billion paired-end reads (40-fold depth, 135 Gb of sequence) were obtained from the NCBI short-read archive, accession SRA000271 (ftp://ftp.ncbi.nlm.nih.gov/pub.TraceDB/ShortRead/SRA000271).
The list of 378 287 short indels (116 bp) in their study was kindly provided by Illumina.
3 RESULTS 3.1 Implementation of Pindel The Pindel program was implemented in C++ to identify break points of medium-sized insertion and large deletions using the principle of pattern growth.
Currently no parallelization has been implemented in Pindel so that it only runs on a single CPU.
The input for Pindel consists of the reference genome sequence and a file of one-end-mapped read pairs.
The output contains information about each indel event at the base level.
In Figure 3, for example, a deletion event (D one the start of the first line) as reported by Pindel is depicted.
The indel size and its break point coordinates on the reference genome are given.
There are 15 reads supporting such an event.
For 9 out of 15 reads, their mapped partners are located upstream of the deletion event (+ sign) while six are located downstream (-sign).
The coordinates of mapped reads are given as well.
Currently we report an indel event if there are at least two reads supporting it.
3.2 Simulation on human chromosome X In order to evaluate performance of Pindel, we first examine how well it can retrieve randomly placed indels on human chromosome Fig.3.
An example output of Pindel.
The type and size of deletion are specified first (D 321).
Then the chromosome ID, coordinates of the break points and the number of reads supporting every event are given.
The mapping directions of the mapped reads and their 3 coordinates on the reference are reported for each supporting read.
2867 ftp://ftp.ncbi.nlm.nih.gov/genomes/H_sapiens/Assembled_chromosomes/ ftp://ftp.ncbi.nlm.nih.gov/pub.TraceDB/ShortRead/SRA000271 [21:54 21/10/2009 Bioinformatics-btp394.tex] Page: 2868 28652871 K.Ye et al.
(a) (d) (e) (f) (i)(h)(g) (b) (c) Fig.4.
Simulation of paired-end reads from human chromosome X.
(af) True positive rates per each deletion size are displayed in the presence of sequencing errors and SNPs when Max_D_Size is increased from 10 to 1 000 000 bp.
The impact of sequencing errors and/or SNPs on the overall true positive rates (g) and false discovery rates (h) when Max_D_Size is set to different values from 10 to 1 000 000.
(i) The impact of sequencing errors and/or SNPs on the true positive rates for detecting medium sized insertions from size 1 to 20 bp.
X in the presence of SNPs and sequencing errors.
For deletion, user must specify the maximum size of deletion events (Max_D_size).
As shown in Figure 4, when we increase Max_D_size from 10 bp to 100 kb, we are able to recover about 80% of deletions (Fig.4ae) with <2% false discovery rate (Fig.4h).
It should be noted that the maximum size cannot be too high as this significantly increases the false positives as well as decreases the rate of true positives (Fig.4f and h), especially when the maximum size reaches 1 Mb.
We also investigated the effect of sequencing errors and SNPs on the performance of detecting deletions by Pindel (Fig.4g and h) and we found that data with SNPs and/or sequencing errors only slightly affects the rate of false negatives and has little effects on accuracy of variant detection due to the stringent filtering of read alignment in the preprocessing step, i.e.
no mismatch is allowed.
As for insertions, we can correctly detect around 80% of the insertions of 116 bases in the presence of SNPs and sequencing errors.
For 36 bp paired-end reads, the probability for Pindel to detect insertion events longer than 16 bp is decreasing as the size of insertion events goes up (Fig.4i).
3.3 Real data (NA18507) The newly sequenced data (Bentley et al., 2008) of a male Yoruba from Ibadan, Nigeria (YRI, sample NA18507), provides us a chance to further examine Pindel with high depth, high quality and most importantly real paired-end read data.
After preprocessing the paired-end reads with SSAHA2, we obtained 56 161 333 pairs of reads which only have one end mapped uniquely to the human reference while the other end couldnt be mapped.
In order to compare the prediction of Pindel with the short indel calling 2868 [21:54 21/10/2009 Bioinformatics-btp394.tex] Page: 2869 28652871 Pindel: detecting break points of indels with pattern growth Fig.5.
Plots of deletion size distribution for NA18507 from 1 to 10 000 bp.
(a) The frequency per each deletion size from 1 to 10 000 bp.
Adjacent dots are connected.
There is a peak around 300 bp, which may contain hundreds of putative SINEs.
(b) Sum of frequencies for each 20 bases is plotted.
The peaks for putative SINEs and LINEs are visible.
(116 bp, indels_NA18507) published together with the sequencing data, we first constrained our Pindel to pick up indels of length 116 bp.
As a result, Pindel predicted 146 843 deletions and 142 908 insertions.
We found that 133 974 deletions (91.2% of 146 843) and 124 559 insertions (87.2% of 142 908) are in the indel list of Bentley et al.
If we take the prediction of Bentley et al.
as true positive, the overall false negative rates of Pindel for deletions and insertions of length 116 bp will be 31.6%.
Tables of comparison between predictions of Bentley et al.
and Pindel are available atSince our Pindel is able to identify deletions of size up to 10 kb from 36 bp paired-end reads, we removed the constraints on the indel size and rerun Pindel.
The result was compared with the database of genomic variants [DGV, http://projects.tcag.ca/variation/, (Iafrate et al., 2004)].
For the range of 100 bp1 kb, Pindel predicted 1399 deletions, 949 of which (67.8%) were listed in DGV.
In the range of 110 kb, Pindel predicted 1138 deletions, 494 of which (43.4%) were listed in DGV.
The deletion size distribution for NA18507 from 1 to 10 000 bp is shown in Figure 5a, where the frequency per each deletion size is depicted as a dot while adjacent dots are connected to illustrate the trends.
There is a peak around 300 bp, which contain hundreds of putative SINEs.
In Figure 5b, when we sum up the frequencies for every 20 bases, there is an additional peak at around 6 k which most likely corresponds to putative LINEs.
We further characterized the 665 deletion events, whose sizes range from 300 to 350 bp.
Six hundred thirty seven of 665 deletion events (95.8%) were reported as SINEs by RepeatMasker and the majority of those 637 events were flagged as potential members of the AluY family.
3.4 Runtime and memory usage for processing data of NA18507 The Pindel program is efficient in time and memory.
In order to reduce memory requirement, each time Pindel loads one chromosome into memory and scan the entire read alignment file, examining reads associated with the current chromosome.
After Pindel processes all the reads, it outputs the results and then frees memory for the next chromosome.
As shown in Figure 6a, when Max_D_Size is smaller than 1 k, the most time-consuming step is loading the reference sequence and scanning the read file.
When we set Max_D_Size to 10 k, it took Pindel only 16 125 s (<4.5 h) to process the complete data of NA18507 on a single CPU.
Because Pindel processes one chromosome at a time, the maximum memory consumption is well controlled.
As shown in Figure 6b, when we increase Max_D_Size from 10 bp to 10 kb, the peak memory usage only increases slightly from 1429 to 1542 MB simply because more indel events are found and stored in memory before writing back to the hard disk.
4 DISCUSSION 4.1 Pattern growth: memory and speed Currently major genome-wide sequence analysis programs use either hash-tables or suffix tree related data structure to index sequences to allow efficient query (Ning et al., 2001; Schulz et al., 2008).
Indexing entire genomes as large as the human one requires significant amount of computer memory.
Since our Pindel program only processes and breaks and unmapped reads in the defined local regions according to the position of the mapped anchor read, using pattern growth to directly search for unique minimum and maximum unique substrings is apparently more memory efficient than indexing the entire genome with hash-tables or suffix trees (Fig.6b).
In our applications, the pattern growth method is very efficient in searching for deletions up to 10 kb long, taking only 4.5 h on a single CPU to process data derived from 40 coverage paired-end reads of human genome.
It should be noted that Pindel does need read mapping and this process is the main bulk of computational CPU time and memory usage.
We are also working on the adaption of the SAM (http://samtools.sourceforge.net) format so that Pindel 2869 [21:54 21/10/2009 Bioinformatics-btp394.tex] Page: 2870 28652871 K.Ye et al.
Fig.6.
Runtime and memory consumption for Pindel applied to the NA18507 data on a single CPU for mining indels with different Max_D_Size (10 bp, 100 bp, 1 kb and 10 kb).
(a) The user runtime for Pindel is divided into three categories: (i) loading the reference genome and the reads into memory.
(ii) Break unmapped reads and map them separately using pattern growth.
(iii) Sort break points according to coordinates and write the results on hard disk.
(b) Maximum memory consumption for Pindel to process the NA18507 data with different Max_D_Size (10 bp, 100 bp, 1 kb and 10 kb).
scans SAM or BAM files for variants regardless of the alignment tools.
4.2 Sensitivity and specificity As one can see in the results from both simulation and real data, Pindel misses a fraction of true positives for several reasons.
First of all, repeats in the reference genome may prevent unique mapping of anchor points.
Pindel also requires unique exact occurrence of the terminal fragments of unmapped reads within a certain region related to the anchor point.
Whenever missing anchor points or repeats occurring around the variant, Pindel will not report this event at all.
The probability of finding a random substring that is identical to one of the terminal fragments increases as we enlarge the parameter Max_D_Size (Fig.4ag).
In the current version of Pindel, we only consider perfect matching and mismatch is not allowed.
As a consequence, SNPs or sequencing base errors in the regions of anchor or indel points may lead to the miss of true positives because there might be insufficient supporting reads.
We are investigating inexact matching with pattern growth to overcome the issue of SNPs and sequencing errors.
Throughout this manuscript we have used a fixed cutoff value for the number of supporting reads >=2, for the detection of deletions and insertions from paired-end short read data.
Obviously this is a simplification as the actual parameters like insertion or deletion size, and the number or paired-end reads are not taken into account.
For example in the simulation on chromosome X we have seen a drop in true positive rate for searching of large deletions, mostly due to the accumulated number of random matches.
It was very surprising to us that the simple cutoff we used showed such promising results over a broad range of search parameters.
Clearly, a fixed cutoff will be inappropriate once we introduce mismatches in the string-matching step as every mismatch may increase the number of random matches by orders of magnitudes dependent on read length.
Therefore, we are currently investigating a statistical score that captures random matches due to different search parameters, inexact matching, and the total number of reads.
5 CONCLUSIONS In this study, we present Pindel, a computational approach to detect breakpoints of large deletions and medium sized insertions from paired-end short reads.
As far as we know, Pindel is one of the first programs to compute deletion events as large as 10 kb with base level precision from 36 bp paired-end short reads.
Due to its high performance in sensitivity, specificity and efficiency in memory and speed, Pindel has been proved to be a promising approach to address the structural variations between individuals from next-gen high throughput sequencing.
ACKNOWLEDGEMENTS The authors thank E. Birney, M. Fritz, M. Schuster, K. Walter and Y. Zhang for comments.
Funding: NGI/EBI fellowship 050-72-436 from Netherlands Genomics Initiative.
Conflict of Interest: none declared.
ABSTRACT Motivation: Due to different experimental setups and various interpretations of results, the data contained in online bioinformatics resources can be inconsistent, therefore, making it more difficult for users of these resources to assess the suitability and correctness of the answers to their queries.
This work investigates the role of argumentation systems to help users evaluate such answers.
More specifically, it looks closely at a gene expression case study, creating an appropriate representation of the underlying data and series of rules that are used by a third-party argumentation engine to reason over the query results provided by the mouse gene expression database EMAGE.
Results: A prototype using the ASPIC argumentation engine has been implemented and a preliminary evaluation carried out.
This evaluation suggested that argumentation can be used to deal with inconsistent data in biological resources.
Availability: The ASPIC argumentation engine is available from http://www.argumentation.org.
EMAGE gene expression data can be obtained from http://genex.hgu.mrc.ac.uk.
The argumentation rules for the gene expression example are available from the lead author upon request.
Contact: kcm1@hw.ac.uk 1 INTRODUCTION Biologists have access to an ever increasing number and range of online data resources (Bateman, 2007).
Many of these resources contain inconsistent data.
This is not surprising as biology is a complex science in which countless parameters affect the outcome of every experiment.
Added to this is the human element that causes two identical results to be evaluated differently by different people.
The consequence is that two seemingly identical experiments can produce contradictory outcomes.
These experiments may be stored in one or more of the online resources that service a particular field.
If both of these experiments are published by the same resource, it becomes inconsistent.
However, if each experiment is published by a different resource, then the inconsistency is between resources and becomes harder to detect.
Regardless of where it occurs, inconsistency confuses users, forcing them to research further in order to answer their query.
In McLeod and Burger (2007) it was suggested that argumentation could be one solution to this problem.
By using all the resources in a field, arguments could be created for and against potential answers to a query.
These arguments could be presented to the user, providing To whom correspondence should be addressed.
them with a powerful set of knowledge that could be used to identify the most likely solution to the query.
This case study created a prototype using a third-party argumentation engine from ASPIC, Argumentation Services Platform with Integrated Components (www.argumentation.org), to generate arguments for the data held in the EMAGE developmental mouse gene expression database (Davidson et al., 1997).
Future work will extend this to include data from a complementary developmental mouse gene expression database, GXD (Ringwald et al., 2001).
Section 2 starts with a discussion of argumentation.
It is followed in Section 3 by an examination of the gene expression resources EMAGE and GXD in order to explain the need for argumentation.
In Section 4, the argumentation engine is introduced, and Section 5 describes how the knowledge in the domain was interpreted for use with the ASPIC argumentation engine.
Subsequently, in Section 6, the creation of arguments by ASPIC is discussed.
The study continues with a preliminary evaluation of the prototype in Section 7 and a discussion of the work in Section 8 before the conclusions are presented in Section 9.
2 ARGUMENTATION An argument is a reason to believe that something is true.
Arguments can be used to support or attack statements.
Argumentation (Carbogim et al., 2000; Pollock, 2002) is the use of computers in the process of arguing, either for helping humans to argue or by actually using the computers to conduct the argument.
As an approach argumentation mimics a human process and appears intuitive to human users (Williams and Williamson, 2006).
The actual form of an argument will depend on the theory being implemented.
Commonly, an argument is viewed as being a series of inference rules that are chained together in a manner similar to logic programming: there are a number of statements (premises) that if true, imply that the conclusion is true.
A premise of a rule may be satisfied by the conclusion of another rule.
So in order for the first rule to be satisfied, all the premises of the second rule must also be satisfied.
Eventually, premises will be satisfied because they are known to be true: they appear in a knowledge base that holds all currently accepted knowledge for the domain.
As the knowledge changes, new arguments will be formed.
These new arguments may contradict existing arguments, thus creating conflict.
Conflict between arguments is usually represented in two ways.
The first is rebuttal, where two arguments have opposite conclusions: e.g.
it is raining outside versus it is not raining outside.
Undercut is the second form of conflict.
It is an attempt to show that another argument is not valid because the premises do not imply the conclusion.
For example, an argument that someone will get wet 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
http://www.argumentation.penalty 0org [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i305 i304i312 Towards the use of argumentation in bioinformatics because it is raining outside is undercut by the knowledge that the person has an umbrella.
Argumentation provides a means of resolving this conflict.
The arguments can be weighed and compared, with the strongest argument(s) winning.
Thus, the conclusion supported by the strongest argument(s) wins.
Argumentation has been used in areas such as medicine, law (Bench-Capon and Prakken, 2006) and practical reasoning (Rahwan and Amgoud, 2006).
Medical uses of argumentation vary from systems that provide advice on administering drugs (Hurt et al., 2003), to those that help clinicians plan the management of chronic illness through the provision of decision support (Glasspool et al., 2006).
Argumentation has also been used to generate explanations of diagnosis, produced by other computational means, for the benefit of patients (Williams and Williamson, 2006).
In contrast to medical-informatics, bioinformatics has produced little work on argumentation, although Jefferys et al.
(2006) used argumentation to successfully evaluate the output of a protein prediction tool.
This work showed clearly that argumentation could be applied to bioinformatics tools, but what about bioinformatics data resources?
3 ON THE NEED FOR ARGUMENTION IN BIOLOGY Bioinformaticians have access to an ever-increasing range of online resources (Bateman, 2007), many of which publish experimental results for a particular field.
For example, the results of in situ gene expression experiments for the developmental mouse are published in both EMAGE (Davidson et al., 1997) and GXD (Ringwald et al., 2001).
Genes are a set of instructions that tell the body what to build, e.g.
a particular set of genes results in the creation of a nose and a different set of genes produces whiskers.
In situ gene expression experiments are designed to identify the genes that are active in a particular anatomical structure.
For that structure, the experiment sets out to determine if the gene is active (expressed) or not active (not expressed).
EMAGE and GXD take their knowledge of embryonic anatomical structures from a common anatomy, EMAP (Baldock and Davidson, 2008), though GXD has additional structures for the adult mouse.
An in situ gene expression database, such as EMAGE or GXD, allows its users to find the conclusions of gene expression experiments.
These conclusions link a gene to a structure, with a level of expression (i.e.
expressed or not expressed).
The database also provides provenance data such as: who the research team was, details of where the experiment was published, the images showing the experimental result and details of the mouse experimented on.
When using such a resource, the user will start by asking for the genes (not) expressed in a particular structure, or for the list of structures where a specific gene is expressed.
The complex nature of biology means that it is possible for experiments to produce conclusions that seem to be contradictory, e.g.
one experiment may suggest the gene Hoxb1 is expressed in the Neural Ectoderm (EMAP:151) and a second report that it is not.
There are many reasons why this could be the case.
For example, the experiments though very similar may be slightly different, e.g.
using different probes may have produced different results, the results may have been analysed differently, e.g.
different interpretations of the original gene expression images generate different experimental conclusions, and there is always the possibility of a genuine error, e.g.
when entering the data into the database.
In addition to internal inconsistencies, resources covering the same field may contradict each other.
For example, although EMAGE and GXD have a high level of duplication (in terms of data), their contents are not identical.
To illustrate this, consider the gene Bmp4 and the structure Future Brain (EMAP:1199).
At the time of writing this article, GXD contains only one experiment for this combination, and it suggests that Bmp4 is not expressed.
EMAGE has this experiment, but in addition it contains another three experiments, all of which indicate that Bmp4 is expressed in the Future Brain.
With all the available evidence, the most likely conclusion is that the gene is expressed; however, if the user relies on a single resource, in this case GXD, a wrong conclusion may be drawn.
Because these resources are incomplete, it is vital that they are both used, in order to generate as many arguments as possible.
However, this highlights a number of issues, both practical and theoretical, which require consideration.
An example of a practical issue would be in identifying experimental results that are duplicated in the other resource.
If an argument is created from data in EMAGE, there is no point in creating an argument based on the same data in GXD.
Theoretical issues include determining whether or not the knowledge used to create arguments for EMAGE can be successfully applied to a similar resource such as GXD.
These issues are not considered in this study but will be the subject of future work.
Regardless of location or reason, contradictions are confusing for users, and require them to investigate the data more fully, often to the extent of re-reading the original paper in which the result was published.
It would be useful to conduct an investigation of the data automatically, presenting the findings to the user in a manner that they could analyse easily.
It is hoped that argumentation may provide a mechanism to achieve this.
4 ARGUMENTATION ENGINE Many different types of argumentation software exist.
Some are used for visualization and explanation of arguments, e.g.
Araucaria (Reed and Rowe, 2001), some for decision support (Fox, 2001) and some for collaborative decision support (Gordon and Karacapilidis, 1997).
However, an inference tool that generates and evaluates arguments is used in this study.
This case study intended to create a web-based tool that dynamically pulled data on-demand from EMAGE to conduct argumentation.
For this reason, it required a robust inference tool that could be integrated readily into applications.
Few tools are available that meet these requirements.
Many different theories for argumentation have been proposed, but few have a robust implementation that can be integrated freely into another application.
For example, Gordon (1993) produced an implementation of his theory, but it is not available publicly.
Oscar is an implementation from Pollock (2002) which is available for download.
Unfortunately, it is programmed in LISP making it difficult to integrate.
The original argumentation engine concept and theory was produced by Dung (1995), but it did not include an implementation.
ASPIC had the goal of standardizing argumentation theory in Artificial Intelligence and developing a suite of tools that could be used in standard application areas such as dialogue, decision-making i305 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i306 i304i312 K.McLeod and A.Burger and machine learning.
The foundation of this implementation is a JAVA tool that generates arguments using inference.
ASPICs argumentation engine is still in development.
Consequently, it is not as robust as might be desired, though it is reliable enough to work for more complex examples than those presented here.
Crucially, the engines design ensures that it can be integrated into other projects.
Although the code is not open source, the engine is available publicly.
The theory behind the engine is based on the work of Dung (1995).
Dungs system is abstract, in the sense that the notion of an argument was not defined.
ASPIC, however, defines an individual argument in the form of an inference tree: inference rules are chained together to form an argument that is organized in a tree structure (Fig.1).
The sole form of attack in the system is rebuttal.
ASPIC mimics an undercut by allowing the user to assign a name to a rule and then create a second rule that rebuts the name (Fig.2).
This succeeds because the name is automatically treated as a premise to its rule, and therefore the second rule rebuts a premise of the first rule.
Input to the engine is: a set of knowledge that models the domain being argued about, a set of rules used to infer new knowledge in the domain (when instantiated a rule forms an argument for the knowledge generated), a set of parameters that configure the behaviour of the engine, and a query that the user wishes the engine to argue about.
Once a query is submitted, the engine generates arguments that support and attack the query before evaluating them.
Output is the arguments, their status and the relationships between them.
In terms of status, the engine records whether or not an argument is true (w.r.t current knowledge) and for relationships the engine provides a list of which arguments attack which other arguments.
This information can be presented visually, in a graph, or in textual form.
ASPICs argumentation engine can be used via a supplied Graphical User Interface (GUI), or programmatically through a JAVA Application Programming Interface (API).
The engine has a fixed knowledge syntax, so an argument must conform to the Conclusion Premise 1 Premise 3 Rule: Premise 1 <-Premise 3 Premise 2 Rule 1: Conclusion <-Premise 1 & Premise 2 Fig.1.
Arguments in ASPIC are stored in a tree structure.
The earlier argument has the conclusion Outcome and three contributing subarguments.
Rule 1 provides the inference rule used to reach the conclusion.
This inference rule states that Outcome is true if both Premise 1 and Premise 2 are true.
Premise 2 is known to be true.
Premise 1 is the conclusion of another argument, and is only true when Premise 3 is true.
[ID_1] Conclusion <-Premise 1 & Premise 2 To undercut this rule: ID_1 <-Premise 3 & Premise 4 Fig.2.
Undercutting an argument in ASPIC.
The first rule states that Conclusion is true when Premise 1 and Premise 2 are both true.
This rule is assigned the name ID_1.
The second rule states that when Premise 3 and Premise 4 are both true, the rule called ID_1 cannot be applied.
specification created by the designers.
When using the GUI, input to the engine has the form of first-order logic.
The chosen logic is similar to PROLOG (Bratko, 2000) and features weak and strong negation.
The JAVA API is designed around the logic, with the methods reflecting the underlying language by using terminology such as Variable, Term, Consequent and Antecedent.
It is the API that the rest of this article deals with.
5 FORMALIZATION OF KNOWLEDGE Argumentation takes place in a particular domain.
That domain could be some everyday area such as planning how to travel to London, or it could be something more specialized such as in situ gene expression.
The argumentation engine is given two forms of knowledge from the domain of gene expression.
The first documents the current state of the domain, i.e.
what is believed to be true, which in this case is the results of gene expression experiments.
The second type, is the knowledge of how to interpret the first.
This knowledge came from the EMAGE curator, and was converted into inference rules that the engine uses to infer new arguments.
The domains state will change continually as new experiments are submitted to EMAGE daily.
However, the knowledge of how to interpret that experimental data changes far less often.
Therefore, it is safe to gather expert knowledge in advance and store it for use later.
Due to the high rate of change, the experimental knowledge must be obtained when it is to be used.
This on-demand creation of knowledge is achieved by pulling data through EMAGEs SOAP-based web service and subsequently converting it.
Knowledge can be strict or defeasible.
Strict knowledge is definitely true, e.g.
London is the capital of the UK.
Defeasible knowledge may be true, but an element of doubt remains, e.g.
it is raining, therefore I will get wet.
Associating knowledge with a real number between 0 and 1 indicates the users degree of belief in an item of knowledge.
If no degree of belief is specified, the piece of knowledge is assumed to be strict (have confidence equal to 1).
This confidence score is how ASPIC assigns a strength to an argument: the higher the score the stronger the argument.
In addition, each piece of knowledge can be assigned a description: a piece of natural language text that describes the knowledge.
The description can hold a simple explanation of a rule or fact.
It is also possible to assign a description to the conclusion of a rule.
Consequently, an argument can be viewed as a series of logic or natural language statements.
5.1 Expert knowledge to inference rules Inference rules are used by ASPIC to infer new arguments.
They model the inference processes of the domain being investigated.
Once captured the inference processes need to be converted into ASPICs chosen logic for use in the engine.
The example featured attempts to argue over the accuracy of data stored in EMAGE.
As such, new arguments are inferred from the contents of the database according to processes suggested by the EMAGE curation team.
This team is responsible for maintaining the quality of the resource by reviewing the experiments submitted for inclusion in EMAGE.
Expert knowledge was gathered in advance during a series of meetings.
These meetings started with informal discussions and i306 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i307 i304i312 Towards the use of argumentation in bioinformatics moved onto using concrete examples to illustrate how the curator processes information.
Although ASPICs engine uses a first-order logic, biologists tend to prefer natural language.
In order to provide a bridge between the two, the notion of an argument schema (Walton, 1996b) is used.
This allowed the experts reasoning to be captured in a semi-formal way using natural language.
A schema provides a natural language inference rule that documents an inference that can be assumed to be true unless shown otherwise (defeated by a counter-argument).
A schema also provides a collection of Critical Questions that highlight exceptions to, and extra conditions on, the use of the inference rule.
All the knowledge needed to create a formal logic inference rule is documented in a manner that can be easily understood by biologists.
For example, when an EMAGE curator evaluates an experiment, they record their confidence in the experiment as a score between 0 and 3, with 3 indicating a high level of confidence.
The curators confidence is made public because a high-quality experiment is more likely to produce a correct result than an experiment the curator has less confidence in.
Intuitively, it can be suggested that if the curator has high confidence in the experiment, the user can have high confidence in the result of the experiment.
This would lead to something like the following schema based on Waltons schema for an Expert (Walton, 1996a): EMAGE is a leading resource on mouse in-situ gene expression EMAGE has C confidence in experiment E suggesting gene G is expressed in structure S Therefore we may be C confident that G is expressed in S Assuming that anyone who uses the system automatically accepts the initial premise that EMAGE is an expert resource, it is possible to simplify the above schema and represent it in a PROLOG-like syntax (with capital letters indicating variables that unify and lower case letters indicating constants), so the basic rule is: expressed(Gene, Structure) <-experiment(Id, Gene, Structure, expressed), confidence(Id, Confidence).
The problem with this rule is that the confidence EMAGE has in the experiment is not passed to ASPIC.
There should be a direct link between EMAGEs confidence and the strength of the argument; therefore, it is necessary to add a degree of belief.
In the instance of EMAGE having high confidence the argument should be strong and thus have a high degree of belief, for example 0.8.
This can be set when passing the inference rule to ASPIC using its JAVA API.
A selection of further rules can be seen in Table 1.
These rules use notions such as Theiler Stages, Spatial Annotation and Textual Annotation.
The Theiler Stages are the 26 developmental phases of a mouse embryo.
Each experiment must be mapped to one of these stages.
The results of gene expression experiments (2D section images) can be described with respect to the EMAP anatomy ontology or spatially mapped into the 3D embryo models (one per Theiler Stage) of EMAP.
These are referred to as Textual Annotation and Spatial Annotation, respectively.
Rules 3, 4 and 5 from Table 1 are all variations of the schema discussed earlier in this section.
5.2 State of domain knowledge ASPIC refers to each item of knowledge (or belief) referring to the current state of the domain as a fact; like inference rules these can be strict or defeasible.
The EMAGE resource provides the setting for this case study, so the facts given to the argumentation engine correspond directly to the data held in EMAGE.
The contents of EMAGE can be abstracted to knowledge about an experiment and its conclusion.
The conclusion is literally that a gene was (not) expressed in a particular anatomical structure.
The experimental information states: who performed the experiment, Table 1.
Some of the rules defined by the EMAGE curator ID Description 1 If a gene G, is expressed in a structure S, in Theiler Stage (T 1) and also in Stage (T +1), then G is very likely to be expressed in S in Stage T .
2 If the user, after examining the image of the experimental result, is confident that the gene G, is expressed in the structure S, then G is very likely to be expressed in S. 3 If a spatial annotation SA, suggests a gene G is expressed in structure S, and the curator has high confidence in SA, then we may have high confidence that G is expressed in S. 4 If a textual annotation TA, suggests a gene G is expressed in structure S and the curator has high confidence in TA, then we may have high confidence that G is expressed in S. 5 If a textual annotation TA, suggests a gene G is expressed in structure S and the curator has medium confidence in TA, then we may have medium confidence that G is expressed in S. 6 If the user does not trust the research team that conducted experiment E, then all spatial and textual annotations based on that experiment should have a low level of confidence.
7 If a spatial annotation SA and a textual annotation TA disagree, then always trust TA.
8 If two experiments disagree on whether, or not, a gene G is expressed in structure S and the user believes the experiments are examining different parts of S, then G is likely to be expressed in part of S. 9 If two experiments disagree on whether, or not, a gene G is expressed in structure S and the user believes the experiments are examining different parts of S, then G is likely to be not expressed in part of S. Rules 17 are relatively straightforward.
However, Rules 8 and 9 may require further explanation.
They state that if two experiments are examining different parts of the same structure both results can be correct regardless of their conclusion.
For example, consider two experiments on the human hand.
The first experiment may find a particular gene expressed in the thumb, and the second conclude that the same gene is not expressed in the index finger.
These experiments show that the gene is both expressed and not expressed in the hand.
i307 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i308 i304i312 K.McLeod and A.Burger what type of experiment it was, how the gene was detected, what kind of mouse the experiment was performed on (its species, its age, whether it was normal or abnormal) and it provides photographs of the result taken by the researchers.
Consequently, the minimum requirement is to provide ASPIC with knowledge on genes, anatomical structures, the relationship between the two and some details of the experiment that established the relationship.
A fact is treated as a simplified inference rule, i.e.
a rule without premises.
One possible way of saying that an experiment in EMAGE, with the associated identifier EMAGE:772, reported the gene Hoxb1 was expressed in the Neural Ectoderm (EMAP:151) is: experiment( EMAGE:772, Hoxb1, EMAP:151, expressed).
Not all of the facts can be generated easily, due to the impossibility of automatically processing the experimental images.
These images are taken by the researchers at the end of the experiment, and are stored in EMAGE as part of an experiments provenance.
Image analysis is a vital part of evaluating the quality of the result: it is done manually by the EMAGE curator.
Consequently, in this study, the images are presented to the human user and they are asked specific questions such as: these images are from two experiments that examine the same structure, do they appear to investigate the same area?
These questions are straightforward for a regular user of EMAGE to answer, but are more challenging for someone with less experience.
6 GENERATING ARGUMENTS Arguments are generated from the contents of the knowledge base, in response to the user posing a query.
The results of the query are returned for the user to examine.
6.1 Query The query is the conclusion that the user wishes ASPIC to argue about.
It will take the form of a fact, and will conform to the earlier discussion in all but one respect: it will not have a degree of belief associated with it.
So in this case, an example would be: expressed(Hoxb1, EMAP:151).
Once the query has been created its status is determined by the argumentation engine.
6.2 Evaluating a query ASPIC uses a dialogue game to determine the status of a query (ASPIC, 2004).
The knowledge given in the query can be undefeated (true with respect to current knowledge), defeated (false with respect to current knowledge), or unknown.
The game features two computer players, the Proponent (PRO) who attempts to prove the query, and the Opponent (OPP) who tries to stop PRO.
The game starts with PRO creating an argument to support the query (an argument whose conclusion is identical to the query).
This process starts by searching for a rule with an appropriate conclusion.
Once found, rules with conclusions that are identical to the premises are sought.
If the premises cannot be satisfied in this manner, the facts are examined to determine if they satisfy the premises.
OPP now attempts to defeat PROs argument.
To succeed, OPPs argument must rebut part of PROs and have a higher degree of belief.
OPP starts by trying to construct arguments that rebut the conclusion of PROs argument.
If that cannot be done, OPP attempts to rebut the premises.
If OPP succeeds in defeating PROs argument, PRO will attempt to counter OPPs argument by defeating it.
This process of attack and counter-attack continues until one player (PRO or OPP) fails to defeat the others argument.
If PRO is stopped, they try a new line of defence by creating a new argument, to support the conclusion that OPP has defeated, if they fail OPP wins.
However, if OPP fails, they try to defeat one of PROs previous arguments, if they cannot do so PRO wins.
For an example we shall use a simplified set of data for Hoxb1 in EMAP:151, ignoring the distinction between textual and spatial annotations (see Section 5.1).
EMAGE has two relevant experiments.
The first suggests that Hoxb1 is expressed in EMAP:151 and the second that it is not.
The EMAGE curator has medium confidence in the first experiment, and a high level of confidence in the second.
In the game, PRO starts by using the first experiment to create the argument in Figure 3 (based on a variation of the schema in Section 5.1), which OPP defeats by creating the argument in Figure 4, based on the second experiment (and a different variation of the schema in Section 5.1).
The next argument of PRO depends on the information provided by the user.
Since it is impossible for the system to evaluate the images of experimental results the user is asked to help.
They are given a number of questions to answer, for example: are the two experiments dealing with the same part of the structure?
This question relates to Rules 8 and 9 from Table 1, and is asked because it is possible for a gene to be expressed in one part of an anatomical structure but not expressed in another part of it (e.g.
a gene may be expressed in the index finger but not the thumb, as the index finger and thumb are two separate parts of the hand, the gene is both expressed and not expressed in the hand).
If the user answers the question by suggesting that the experiments are examining different parts of EMAP:151, then it is possible that Hoxb1 is both Hoxb1 is expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is expressed The EMAGE curator has medium confidence in the experiment If the curator has medium confidence in the experiment, then we may   have medium confidence in the experiment and its result Degree of belief = 0.4 Fig.3.
Argument for Hoxb1 being expressed in EMAP:151. i308 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i309 i304i312 Towards the use of argumentation in bioinformatics Hoxb1 is not expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed The EMAGE curator has high confidence in the experiment If the curator has high confidence in the experiment, then we may have high confidence in the experiment and its result Degree of belief = 0.8 Fig.4.
A counter argument to the argument in Figure 3.
Because the EMAGE curator has more confidence in the experiment used in this argument than the experiment used in Figure 3, this argument has a higher degree of belief and so defeats the argument from Figure 3.
Hoxb1 is expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed EMAGE has an experiment suggesting Hoxb1 is expressed The experiments look at different parts of the same structure If the experiments look at different parts of the same structure, they can both be correct, so the gene is expressed.
Degree of belief = 0.9 Fig.5.
A second argument, based on Rule 8 from Table 1, showing Hoxb1 is expressed in EMAP:151.
Hoxb1 is not expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed EMAGE has an experiment suggesting Hoxb1 is expressed The experiments look at different parts of the same structure If the experiments look at different parts of the same structure, they can both be correct, so the gene is not expressed.
Degree of belief = 0.9 Fig.6.
A second argument, based on Rule 9 from Table 1, showing Hoxb1 is not expressed in EMAP:151. expressed and not expressed in EMAP:151.
This leads PRO to produce the argument in Figure 5 by using Rule 8 from Table 1.
As this argument defeats OPPs previous argument (based on the EMAGE experiment suggesting Hoxb1 was not expressed) PROs first argument is reinstated because it is no longer attacked.
OPP must counter PROs argument and does so with the same logic as PRO (Rule 9 from Table 1): the experiments are using different parts of EMAP:151, so Hoxb1 can be both expressed and not expressed, and therefore it is not expressed (Fig.6).
Currently there are two arguments of equal strength that contradict each other (Figs 5 and 6) .
The outcome of this conflict depends on which type of game semantics is used.
ASPIC provides two game semantics, skeptical and credulous, for the user to choose between.
When a skeptical game is played, if there is any doubt about the acceptability of an argument it is rejected.
In this case, there is doubt about the acceptability of both arguments, and so they are both rejected.
The credulous game is implemented in such a way that even if there is doubt about one of PROs arguments it is accepted, whilst OPPs argument is rejected if there is any doubt.
So here PROs argument that Hoxb1 is expressed is accepted, with OPPs counter argument being defeated.
It is left to the user to decide which game is most suitable for their situation.
Adopting credulous semantics, PROs last argument is accepted.
Because of this, both of OPPs arguments are defeated, leaving both of PROs arguments undefeated.
OPP must try to find another argument that defeats one of PROs two arguments.
However, there are no more arguments available, and so OPP fails.
This game has been won by PRO.
PRO starts a second game with the argument from Figure 5 (as the two experiments are looking at different parts of EMAP:151 Hoxb1 can be both expressed and not expressed, and therefore it is expressed).
The same arguments as before are constructed, once again PRO wins.
PRO can construct no more arguments to support the query so the game is over.
The results are given to the programmer to manipulate as they wish.
The results come in two separate parts.
The first is a series of yes and no.
Each one represents an argument that PRO has constructed to support the query.
As PRO won both games, the results from this example are yes and yes.
The second part of the results is called the proof.
Essentially it is all the arguments used in the game.
If calculated the status of the argument is also recorded.
In this example, the two arguments provided by PRO are undefeated with both of OPPs arguments being defeated.
The programmer can present the arguments to the user in any way they wish.
However, when communicating with biologists it makes more sense to use a natural language form similar to that used in this section (Fig.4) by using the descriptions attached to rules and facts.
i309 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i310 i304i312 K.McLeod and A.Burger Fig.7.
Screen shot from the prototype (top-left) with simplified presentation in a mock prototype (bottom-right).
Although this evaluation of a query may seem complicated, it is essentially the type of thought-process naturally deployed by a human user.
The apparent complexity relates primarily to the need to formalize this process for computational purposes.
Fortunately, the details of this formalization need not be communicated to the end user and our initial evaluation (see Section 7) adds weight to our view that the underlying argumentation reasoning is accessible and helpful to biologists.
7 EVALUATION Once the implementation of the above system was completed, a preliminary evaluation was undertaken.
This informal study involved demonstrating the system to the EMAGE curator, and recording the feedback given.
Overall the system was well received.
The tool was deemed easy to use, and the arguments were presented in far less time than the curator had expected.
The arguments made sense to the curator, and they covered the majority of the points the curator wished to see.
The curator felt that the arguments would be enough for most people to evaluate the data from EMAGE, and thus determine whether or not a gene was expressed in a particular structure.
As such the system was a success.
Although feedback from the curator was positive, four issues clearly require to be tackled.
The first is the presentation.
The examples discussed above are simplified in order to improve clarity.
However, the prototype displayed arguments in a rudimentary manner using a slightly amended version of a method ASPIC provided for the task (see top-left of Fig.7).
This resulted in a confusing output.
Much of this output was redundant as it restated what had already been given.
For example, in the first argument, the five lines quality_author(EMAGE:772) through to You have confidence in the research team said who the research team was twice, and that the user had confidence in this team twice.
Consequently, the test user was presented with a simplified version of these arguments in a mocked-up prototype (see bottom-right of Fig.7).
Feedback from the curator suggested simplifying further the presentation of the arguments.
For example, subarguments were indented to show that they were separate from the main argument but still contributed to it.
However, the curator did not understand the relationship.
Instead, he suggested the information should be presented in a simple paragraph comprising two or three sentences.
i310 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i311 i304i312 Towards the use of argumentation in bioinformatics The second key issue highlighted by the evaluation is trust.
When using EMAGE, a user must trust the researcher to have performed and evaluated the experiment correctly, in addition the user must trust that all mistakes were detected and corrected by the journal that published the experiment, and finally they must trust the curator of EMAGE (or GXD) to have mapped correctly the researchers findings to the EMAP ontology for inclusion in the database.
For example, if the research team suggested that Bmp4 was expressed in the presumptive infundibulum, then the curation team needed to map this structure to its equivalent in the EMAP anatomy (infundibular recess of third ventricle).
These trust issues should have been made clear to the user by explicitly asking them if they had confidence in each of the above groups.
The system did not do this.
The third issue related to the screen that asked the user for help in processing information.
As mentioned in Section 5.2 the user was asked to analyse some information when the system could not do so.
The curator felt that this screen presented the user with too many tasks to undertake.
One possible solution would be to use the image analysis already undertaken by the authors, journal and EMAGE curation team.
The final issue raised by the curator related to GXD.
The system worked with data from EMAGE.
In real life, the curator would advise anyone with doubts over data in EMAGE to examine GXD (and vice versa), he felt that extending the system to include arguments based on data in GXD was vital.
The goal of this work was to assess the usefulness of argumentation in bioinformatics.
Overall it was obvious that much work remained.
However, it was also evident that the current prototype system was the first step on the way to a useful and interesting tool.
8 DISCUSSION This work concentrates on two resources publishing in situ developmental mouse gene expression information.
However, other resources that perform this function exist, for example, the Mouse Atlas of Gene Expression, MAGE (http://www.mouseatlas.org).
Therefore, it would be beneficial to extend the system to include this and other related resources.
Unfortunately, this is not a simple task.
MAGE uses its own ontology to describe the mouse anatomical structures.
This ontology does not have a mapping to the EMAP ontology used by EMAGE and GXD.
One structure in EMAP may correspond to parts of several structures in the MAGE ontology, and vice versa.
Although work is progressing on a cross-linked mammalian ontology that will hopefully link EMAP to MAGE, currently there is no automatic mechanism to do this.
At present this makes it impossible to use these resources together in this system.
If MAGE had used the EMAP ontology, there would be no reason why it could not be included in the system.
Data from MAGE would need to be pulled and converted for use within ASPIC.
Likewise, there would be a need for an evaluation of the current inference rules to determine if they could be applied to MAGE.
It is probable that several extra inference rules would be required.
With the new rules in place ASPIC would be able to argue as before.
However, with futher knowledge at its disposal, it would be possible to create extra arguments and thus have a more complex argumentation process.
Although this would be unlikely to have a significant effect in the example discussed here, it is possible that the integration of a large number of resources (or resources with a larger number of expert generated inference rules) might cause the argumentation process to run too slowly to be useful.
In such a situation, it might be necessary to balance the inclusion of each resource against the usefulness of the information it provides for the arguments.
Alternatively, it might prove helpful to investigate the other argumentation engines that are beginning to appear e.g.
ArgKit (http://www.argkit.org).
Of course, in the context of the Internet, the argumentation workload can be distributed across more than one site.
We envision domain-specific argumentation engines, e.g.
one or more sites for in situ gene expression argumentation, that communicate with each other.
Efforts are already underway to develop an Argumentation Interchange Format (Chesevar et al., 2006) to facilitate such interactions.
In addition, we note that there is a potential issue with scalability in terms of formalizing enough relevant domain knowledge for the purposes of argumentation.
As with most semantics-based applications, it is unrealistic to expect that all relevant domain knowledge will be captured.
However, the experience with the Semantic Web so far shows that even a little semantics goes a long way (Wolstencroft et al., 2005), and we believe that this applies equally to argumentation.
Argumentation has been used within this work to resolve inconsistencies across biological data resources.
A variety of other mechanisms to integrate data and resolve inconsistency exist.
For example, data reconciliation (a.k.a.
data fusion) uses a function to turn multiple possible values into a single value, e.g.
computing the average of four numbers (Motro and Rakov, 1998).
A second possible mechanism would create multiple query plans for the resources, then select the best according to information quality criteria (Naumann, 1996).
Our work is not an attempt to replace these mechanisms.
We are not concerned with automatically resolving conflict, but instead wish to determine whether or not argumentation can enable biologists to resolve the differences themselves.
9 CONCLUSION This case study explored the usefulness of argumentation in helping biologists work around conflicting information presented by an online biological database, in this case a developmental mouse gene expression database called EMAGE.
By investigating the reasoning processes of an EMAGE curator, a series of rules for assessing the quality of an experiment were produced.
These rules were used by the ASPIC argumentation engine to generate arguments on the validity of the data provided by EMAGE.
This enabled arguments for and against each experimental result to be produced and presented to the user.
Following an implementation of the system, an evaluation was undertaken with the EMAGE curator.
The evaluation showed that the basic concept was correct: arguments could be used to highlight issues and help the user determine if data was valid.
However, it also stressed the importance of presenting the arguments in an appropriate manner, and here further work must be undertaken.
This is not the only work needed, in particular an effort must be made to extend the system so that it can create arguments based on the data held in another developmental mouse gene expression database, GXD.
Only then it will be possible to make an accurate assessment of the full worth of argumentation in a bioinformatics setting.
i311 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i312 i304i312 K.McLeod and A.Burger ACKNOWLEDGEMENT This work could not have been completed without the support of the EMAGE curation team.
Funding: Funding by the EU projects Sealife (FP6-2006-IST-027269) and REWERSE (FP6-2006-IST-506779) is acknowledged.
Conflict of Interest: none declared.
ABSTRACT Motivation: Next-generation DNA sequencing machines are generating an enormous amount of sequence data, placing unprecedented demands on traditional single-processor read-mapping algorithms.
CloudBurst is a new parallel read-mapping algorithm optimized for mapping next-generation sequence data to the human genome and other reference genomes, for use in a variety of biological analyses including SNP discovery, genotyping and personal genomics.
It is modeled after the short read-mapping program RMAP, and reports either all alignments or the unambiguous best alignment for each read with any number of mismatches or differences.
This level of sensitivity could be prohibitively time consuming, but CloudBurst uses the open-source Hadoop implementation of MapReduce to parallelize execution using multiple compute nodes.
Results: CloudBursts running time scales linearly with the number of reads mapped, and with near linear speedup as the number of processors increases.
In a 24-processor core configuration, CloudBurst is up to 30 times faster than RMAP executing on a single core, while computing an identical set of alignments.
Using a larger remote compute cloud with 96 cores, CloudBurst improved performance by >100-fold, reducing the running time from hours to mere minutes for typical jobs involving mapping of millions of short reads to the human genome.
Availability: CloudBurst is available open-source as a model for parallelizing algorithms with MapReduce at http://cloudburst-bio.sourceforge.net/.
Contact: mschatz@umiacs.umd.edu 1 INTRODUCTION Next-generation high-throughput DNA sequencing technologies from 454 Life Sciences, Illumina,Applied Biosystems and others are changing the scale and scope of genomics.
These machines sequence more DNA in a few days than a traditional Sanger sequencing machine could in an entire year, and at a significantly lower cost (Shaffer, 2007).
James Watsons genome was recently sequenced (Wheeler et al., 2008) using technology from 454 Life Sciences in just 2 months, whereas previous efforts to sequence the human genome required several years and hundreds of machines (Venter et al., 2001).
If this trend continues, an individual will be able to have their DNA sequenced in only a few days and perhaps for as little as $1000.
To whom correspondence should be addressed.
The data from the new machines consists of millions of short sequences of DNA (25250 bp) called reads, collected randomly from the target genome.
After sequencing, researchers often map the reads to a reference genome to find the locations where each read occurs, allowing for a small number of differences.
This information can be used to catalog differences in one persons genome relative to a reference human genome, or compare the genomes of closely related species.
For example, this approach was recently used to analyze the genomes of an African (Bentley et al., 2008) and an Asian (Wang et al., 2008) individual by mapping 4.0 and 3.3 billion 35 bp reads, respectively, to the reference human genome.
These comparisons are used for a wide variety of biological analyses including SNP discovery, genotyping, gene expression, comparative genomics and personal genomics.
Even a single base pair difference can have a significant biological impact, so researchers require highly sensitive mapping algorithms to analyze the reads.
As such, researchers are generating sequence data at an incredible rate and need highly scalable algorithms to analyze their data.
Many of the currently used read-mapping programs, including BLAST (Altschul et al., 1990), SOAP (Li et al., 2008b), MAQ (Li, et al., 2008a), RMAP (Smith et al., 2008) and ZOOM (Lin et al., 2008), use an algorithmic technique called seed-and-extend to accelerate the mapping process.
These programs first find sub-strings called seeds that exactly match in both the reads and the reference sequences, and then extend the shared seeds into longer, inexact alignments using a more sensitive algorithm that allows for mismatches or gaps.
These programs use a variety of methods for finding and extending the seeds, and have different features and performance.
However, each of these programs is designed for execution on a single computing node, and as such requires a long running time or limits the sensitivity of the alignments they find.
CloudBurst is a new highly sensitive parallel seed-and-extend read-mapping algorithm optimized for mapping single-end next generation sequence data to reference genomes.
It reports all alignments for each read with up to a user-specified number of differences including both mismatches and indels.
CloudBurst can optionally filter the alignments to report the single best non-ambiguous alignment for each read, and produce output identical to RMAPM (RMAP using mismatch scores).
As such CloudBurst can replace RMAP in a data analysis pipeline without changing the results, but provides much greater performance by using the open-source implementation of the distributed programming framework MapReduce called Hadoop (http://hadoop.apache.org).
The results presented below show that CloudBurst is highly scalable: the running times scale linearly as the number of reads increases, and with near linear speed improvements 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1364 13631369 M.C.Schatz Fig.1.
Schematic overview of MapReduce.
The input file(s) are automatically partitioned into chunks depending on their size and the desired number of mappers.
Each mapper (shown here as m1 and m2) executes a user-defined function on a chunk of the input and emits keyvalue pairs.
The shuffle phase creates a list of values associated with each key (shown here as k1, k2 and kn).
The reducers (shown here as r1 and r2) evaluate a user-defined function for their subset of the keys and associated list of values, to create the set of output files.
over a serial execution of RMAP for sensitive searches.
Furthermore, CloudBurst can scale to run on large remote compute clouds, and thus map virtually any number of reads with high sensitivity in relatively little time.
1.1 MapReduce and Hadoop MapReduce (Dean et al., 2008) is the software framework developed and used by Google to support parallel distributed execution of their data intensive applications.
Google uses this framework internally to execute thousands of MapReduce applications per day, processing petabytes of data, all on commodity hardware.
Unlike other parallel computing frameworks, which require application developers explicitly manage inter-process communication, computation in MapReduce is divided into two major phases called map and reduce, separated by an internal shuffle phase of the intermediate results (Fig.1), and the framework automatically executes those functions in parallel over any number of processors.
The map function computes keyvalue pairs from the input data, based on any relationship applicable to the problem, including computing multiple pairs from a single input.
For example, the map function of a program that counts the number of occurrences of all length k substrings (k-mers) in a set of DNA sequences could emit the keyvalue pair (k-mer, 1) for each k-mer.
If the input is large, many instances of the map function can execute in parallel on different portions of the input and divide the running time by the number of processors available.
Once the mappers are complete, MapReduce shuffles the pairs so all values with the same key are grouped together into a single list.
The grouping of keyvalue pairs effectively creates a large distributed hash table indexed by the key, with a list of values for each key.
In the k-mer counter example, the framework creates a list of 1s for each k-mer in the input, corresponding to each instance of that k-mer.
The reduce function evaluates a user-defined function on each keyvalue list.
The reduce function can be arbitrarily complex, but must be commutative, since the order of elements in the keyvalue list is unstable.
In the k-mer counting example, the reduce function is called once for each k-mer with its associated list of 1s, and simply adds the 1s together to compute the total number of occurrences for that k-mer.
Each instance of the reduce function executes independently, so there can be as many reduce functions executing in parallel as there are distinct keys, i.e.
k-mers in the input.
As an optimization, MapReduce allows reduce-like functions called combiners to execute in-memory immediately after the map function.
Combiners are not possible in every application because they evaluate on a subset of the values for a given key, but when possible, reduce the amount of data processed in the shuffle and reduce phases.
In the k-mer counting example, the combiner emits a partial sum from the subset of 1s it evaluates, and the reduce function sums over the list of partial sums.
Computations in MapReduce are independent, so the wall clock running time should scale linearly with the number of processor cores available, i.e.
a 10-core execution should take 1/10th the time of a 1-core execution creating a 10 speedup with complete parallel efficiency.
In practice, perfect linear speedup is difficult to achieve because serial overhead limits the maximum speedup possible as described by Amdahls law (Krishnaprasad, 2001).
For example, if an application has just 10% non-parallelizable overhead, then the maximum possible end-to-end speedup is only 10 regardless of the number of cores used.
High speedup also requires the computation is evenly divided over all processors to maximize the benefit of parallel computation.
Otherwise the wall clock running time will be limited to the time for the longest running task, and reduce overall efficiency.
MapReduce tries to balance the workload by assigning each reducer 1/N of the total key space, where N is the number of cores.
If certain keys require substantially more time than others, however, it may be necessary to rebalance the workload using a custom partition function or adjusting how keys are emitted.
MapReduce is designed for computations with extremely large datasets, far beyond what can be stored in RAM.
Instead it uses files for storing and transferring intermediate results, including the inter-machine communication between map and reduce functions.
This could become a severe bottleneck, so Google developed the robust distributed Google File System (GFS) (Ghemawat et al., 2003) to efficiently support MapReduce.
GFS is designed to provide very high-bandwidth for MapReduce by replicating and partitioning files across many physical disks.
Files in the GFS are automatically partitioned into large chunks (64 MB by default), which are replicated to several physical disks (three by default) attached to the compute nodes.
Therefore, aggregate I/O performance can greatly exceed the performance of an individual memory storage device (e.g.
a disk drive), and chunk redundancy ensures reliability even when used with commodity drives with relatively high-failure rates.
MapReduce is also data aware: it attempts to schedule computation at a compute node that has the required data instead of moving the data across the network.
Hadoop and the Hadoop Distributed File System (HDFS) are open source versions of MapReduce and the GFS implemented in Java and sponsored by Amazon, Yahoo, Google, IBM and other major vendors.
Like Googles proprietary MapReduce framework, applications developers need only write custom map and reduce functions, and the Hadoop framework automatically executes those functions in parallel.
Hadoop and HDFS are used to manage production clusters with 10 000 + nodes and petabytes of data, including computation supporting every Yahoo search result.
A Hadoop cluster of 910 commodity machines recently set a performance record by sorting 1 TB of data (10 billion 100 bytes records) in 209 s (http://www.hpl.hp.com/hosted/sortbenchmark/).
1364 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1365 13631369 CloudBurst In addition to in-house Hadoop usage, Hadoop is becoming a de facto standard for cloud computing where compute resources are accessed generically as a service, without regard for physical location or specific configuration.
The generic nature of cloud computing allows resources to be purchased on-demand, especially to augment local resources for specific large or time-critical tasks.
Several organizations offer cloud compute cycles that can be accessed via Hadoop.
Amazons Elastic Compute Cloud (EC2) (http://aws.amazon.com) contains tens of thousands of virtual machines, and supports Hadoop with minimal effort.
In EC2, there are five different classes of virtual machines available providing different levels of CPU, RAM and disk resources with price ranging from $0.10 to $0.80 per hour per virtual machine.
Amazon offers preconfigured disk images and launches scripts for initializing a Hadoop cluster, and once initialized, users copy data into the newly created HDFS and execute their jobs as if the cluster was dedicated for their use.
For very large datasets, the time required for the initial data transfer can be substantial, and will depend on the bandwidth of the cloud provider.
Once transferred into the cloud, though, the cloud nodes generally have very high-internode bandwidth.
Furthermore, Amazon has begun mirroring portions of Ensembl and GenBank for use within EC2 without additional storage costs, thereby minimizing the time and cost to run a large-scale analysis of these data.
1.2 Read mapping After sequencing DNA, researchers often map the reads to a reference genome to find the locations where each read occurs.
The read-mapping algorithm reports one or more alignments for each read within a scoring threshold, commonly expressed as the minimal acceptable significance of the alignment, or the maximum acceptable number of differences between the read and the reference genome.
The algorithms generally allow 110% of the read length to differ from the reference, although higher levels may be necessary when aligning to more distantly related genomes, or when aligning longer reads with higher error rates.
Read-mapping algorithms can allow mismatch (mutation) errors only, or they can allow insertion or deletion (indel) errors, for both true genetic variations and artificial sequencing errors.
The number of mismatches between a pair of sequences can be computed with a simple scan of the sequences, whereas computing the edit distance (allowing for indels) requires a more sophisticated algorithm such as the Smith Waterman sequence alignment algorithm (Smith et al., 1981), whose runtime is proportional to the product of the sequence lengths.
In either case, the computation for a single pair of short sequences is fast, but becomes costly as the number or size of sequences increases.
When aligning millions of reads generated from a next-generation sequencing machine, read-mapping algorithms often use a technique called seed-and-extend to accelerate the search for highly similar alignments.
This technique is based on the observation that there must be a significant exact match for an alignment to be within the scoring threshold.
For example, for a 30 bp read to map to a reference with only one difference, there must be at least 15 consecutive bases, called a seed, that match exactly regardless of where the difference occurs.
In general, a full-length end-to-end alignment of an m bp read with at most k differences must contain at least one exact alignment of m/(k+1) consecutive bases (Baeza-yates et al., 1992).
Similar arguments can be made when designing spaced seeds of non-consecutive bases to guarantee finding all alignments with up to a certain numbers of errors (Lin et al., 2008).
Spaced seeds have the advantage of allowing longer seeds at the same level of sensitivity, although multiple spaced seeds may be needed to reach full sensitivity.
In all seed-and-extend algorithms, regions that do not contain any matching seeds are filtered without further examination, since those regions are guaranteed to not contain any high-quality alignments.
For example, BLAST uses a hash table of all fixed length k-mers in the reference to find seeds, and a banded version of the Smith Waterman algorithm to compute high-scoring gapped alignments.
RMAP uses a hash table of non-overlapping k-mers of length m/(k+1) in the reads to find seeds, while SOAP, MAQ and ZOOM use spaced seeds.
In the extension phase, RMAP, MAQ, SOAP and ZOOM align the reads to allow up to a fixed number of mismatches, and SOAP can alternatively allow for one continuous gap.
Other approaches to mapping include using suffix trees (Kurtz et al., 2004; Schatz et al., 2007) to quickly find short exact alignments to seed longer inexact alignments, and Bowtie (Langmead et al., 2009) uses the BurrowsWheeler transform (BWT), to find exact matches coupled with a backtracking algorithm to allow for mismatches.
Some BWT-based aligners are reporting extremely fast runtimes, especially in configurations that restrict the sensitivity of the alignments or limit the number of alignments reported per read.
For example, in their default high-speed configuration, SOAP2 (http://soap.genomics.org.cn/), BWA (http://maq.sourceforge.net) and Bowtie allow at most two differences in the beginning of the read, and report a single alignment per read selected randomly from the set of acceptable alignments.
In more sensitive or verbose configurations, the programs can be considerably slower (http://bowtie-bio.sourceforge.net/manual.shtml).
After computing end-to-end alignments, some of these programs use the edit distance or read quality values to score the mappings.
In a systematic study allowing up to 10 mismatches, Smith et al.
(2008) determined allowing more than two mismatches is necessary for accurately mapping longer reads, and incorporating quality values also improves accuracy.
Several of these programs, including RMAPQ (RMAP with quality), MAQ, ZOOM and Bowtie, use quality values in their scoring algorithm, and all are more lenient of errors in the low-quality 3 ends of the reads by trimming the reads or discounting low-quality errors.
Consecutive or spaced seeds dramatically accelerate the computation by focusing computation to regions with potential to have a high-quality alignment.
However, to increase sensitivity the length of the seeds must decrease (consecutive seeds) or the number of seeds used must increase (spaced seeds).
In either case, increasing sensitivity increases the number of randomly matching seeds and increases the total execution time.
Decreasing the seed length can be especially problematic because a seed of length s is expected to occur L/4s times in a reference of length L, and each occurrence must be evaluated using the slower inexact alignment algorithm.
Therefore, many of the new short read mappers restrict the maximum number of differences allowed, or limit the number of alignments reported for each read.
2 ALGORITHM CloudBurst is a MapReduce-based read-mapping algorithm modeled after RMAP, but runs in parallel on multiple machines with Hadoop.
It is optimized for mapping many short reads 1365 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1366 13631369 M.C.Schatz Fig.2.
Overview of the CloudBurst algorithm.
The map phase emits k-mers as keys for every k-mer in the reference, and for all non-overlapping k-mers in the reads.
The shuffle phase groups together the k-mers shared between the reads and the reference.
The reduce phase extends the seeds into end-to-end alignments allowing for a fixed number of mismatches or indels.
Here, two grey reference seeds are compared with a single read creating one alignment with two errors and one alignment with zero errors, while the black shared seed is extended to an alignment with three errors.
from next-generation sequencing machines to a reference genome allowing for a user specified number of mismatches or differences.
Like RMAP, it is a seed-and-extend algorithm that indexes the non-overlapping k-mers in the reads as seeds.
The seed size s=m/(k+1) is computed from the minimum length of the reads (m) and the maximum number of differences or mismatches (k).
Like RMAP, it attempts to extend the exact seeds to count the number of mismatches in an end-to-end alignment using that seed, and reports alignments with at most k mismatches.
Alternatively, like BLAST, it can extend the exact seed matches into end-to-end gapped alignments using a dynamic programming algorithm.
For this step, CloudBurst uses a variation of the LandauVishkin k-difference alignment algorithm (Landau et al., 1986), a dynamic programming algorithm for aligning two strings with at most k differences in O(km) time where m is the minimum length of the two strings.
See Gusfields (1997) classical text on sequence alignment for more details.
As a MapReduce algorithm, CloudBurst is split into map, shuffle and reduce phases (Fig.2).
The map function emits k-mers of length s as seeds from the reads and reference sequences.
The shufffle phase groups together k-mers shared between the read and reference sequences.
Finally, the reduce function extends the shared seeds into end-to-end alignments allowing both mismatches and indels.
The input to the application is a multi-fasta file containing the reads and a multi-fasta file containing one or more reference sequences.
These files are first converted to binary Hadoop SequenceFiles and copied into the HDFS.
The DNAsequences are stored as the keyvalue pairs (id, SeqInfo), where SeqInfo is the tuple (sequence, start_offset) and sequence is the sequence of bases starting at the specified offset.
By default, the reference sequences are partitioned into chunks of 65 kb overlapping by 1 kb, but the overlap can be increased to support reads longer than 1 kb.
2.1 Map: extract K-mers The map function scans the input sequences and emits keyvalue pairs (seed, MerInfo) where seed is a sequence of length s, and MerInfo is the tuple (id, position, isRef, isRC, left_flank, right_flank).
If the input sequence is a reference sequence, then a pair is emitted for every k-mer in the sequence, with isRef = 1, isRC = 0, and position set as the offset of the k-mer in the original sequence.
If the given input sequence is a read, then isRef = 0, and a pair is emitted for the non-overlapping k-mers with appropriate position.
Seeds are also emitted for the non-overlapping k-mers of the reverse complement sequence with isRC = 1.
The flanking sequences [up to (m s + k) bp) are included in the fields left_flank and right_flank.
The seeds are represented with a 2 bit/bp encoding to represent the four DNA characters (ACGT), while the flanking sequences are represented with a 4 bit/bp encoding, which also allows for representing an unknown base (N), and a separator character (.).
CloudBurst parallelizes execution by seed, so each reducer evaluates all potential alignments for approximately 1/N of the 4s seeds, where N is the number of reducers.
Overall this balances the workload well, and each reducer is assigned approximately the same number of alignments and runs for approximately the same duration.
However, low-complexity seeds (defined as seeds composed of a single DNA character) occur a disproportionate number of times in the read and reference datasets, and the reducers assigned these high-frequency seeds require substantially more execution time than the others.
Therefore, CloudBurst can rebalance low-complexity seeds by emitting redundant copies of each occurrence in the reference and randomly assigning occurrences in the reads to one of the redundant copies.
For example, if the redundancy is set to 4, each instance of the seed AAAA in the reference will be redundantly emitted as seeds AAAA-0, AAAA-1, AAAA-2 and AAAA-3, and each instance of AAAA from the reads will be randomly assigned to seed AAAA-R with 0R3.
The total number of alignments considered will be the same as if there were no redundant copies, but different subsets of the alignments can be evaluated in parallel in different reducers, and thus improve the overall load balance.
2.2 Shuffle: collect shared seeds Once all mappers have completed, Hadoop shuffles the keyvalue pairs, and groups all values with the same key into a single list.
Since the key is a k-mer from either the read or reference sequences, this has the effect of cataloging seeds that are shared between the reads and the reference.
2.3 Reduce: extend seeds The reduce function extends the exact alignment seeds into longer inexact alignments.
For a given seed and MerInfo list, it first partitions the MerInfo tuples into the set R from the reference and set Q from the reads.
Then it attempts to extend each pair of tuples from the Cartesian product R Q using either a scan of the flanking bases to count mismatches, or the LandauVishkin k-difference algorithm for gapped alignments.
The evaluation proceeds block-wise across subsets of R and Q to maximize cache reuse, and using the bases flanking the shared seeds stored in the MerInfo tuples.
If an end-to-end alignment with at most k mismatches or k differences is found, it is then checked to determine if it is a duplicate alignment.
This is necessary because multiple exact seeds may be present within the same alignment.
For example, a perfectly matching end-to-end 1366 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1367 13631369 CloudBurst alignment has k + 1 exact seeds, and is computed k + 1 times.
If another exact seed with smaller offset exists in the read the alignment is filtered as a duplicate, otherwise the alignment is recorded.
The value for k is small, so only a small number of alignments are discarded.
The output from CloudBurst is a set of binary files containing every alignment of every read with at most k mismatches or differences.
These files can be converted into a standard tab-delimited text file of the alignments using the same format as RMAP or post-processed with the bundled tools.
2.4 Alignment filtration In some circumstances, only the unambiguous best alignment for each read is required, rather than the full catalog of all alignments.
If so, the alignments can be filtered to report the best alignment for each read, meaning the one with the fewest mismatches or differences.
If a read has multiple best alignments, then no alignments are reported exactly as implemented in RMAPM.
The filtering is implemented as a second MapReduce algorithm run immediately after the alignments are complete.
The map function reemits the end-to-end alignments as keyvalue pairs with the read identifier as the key and the alignment information as the value.
During the shuffle phase, all alignments for a given read are grouped together.
The reduce function scans the list of alignments for each read and records the best alignment if an unambiguous best alignment exists.
As an optimization, the reducers in the main alignment algorithm report the top two best alignments for each read.
Also, the filtration algorithm uses a combiner to filter alignments in memory and reports just the top two best alignments from its subset of alignments for a given read.
These optimizations improve performance without changing the results.
3 RESULTS CloudBurst was evaluated in a variety of configurations for the task of mapping random subsets of 7.06 million publicly available Illumina/Solexa sequencing reads from the 1000 Genomes Project (accession SRR001113) to portions of the human genome (NCBI Build 36) allowing up to four mismatches.
All reads were exactly 36 bp long.
The test cluster has 12 compute nodes, each with a 32 bit dual core 3.2 GHz Intel Xeon (24 cores total) and 250 GB of local disk space.
The compute nodes were running RedHat Linux AS Release 3 Update 4, and Hadoop 0.15.3 set to execute two tasks per node (24 simultaneous tasks total).
In the results below, the time to convert and load the data into the HDFS is excluded, since this time was the same for all tasks, and once loaded the data was reused for multiple analyses.
The first test explored how CloudBurst scales as the number of reads increases and as the sensitivity of the alignment increases.
In this test, sub-sets of the reads were mapped to the full human genome (2.87 Gbp), chromosome 1 (247.2 Mbp) or chromosome 22 (49.7 Mbp).
To improve load balance across the cores, the number of mappers was set to 240, the number of reducers was set to 48, and the redundancy for low-complexity seeds was set to 16.
The redundancy setting was used because the low-complexity seeds required substantially more running time than the other seeds (>1 h compared with <1 min), and the redundancy allows their alignments to be processed in parallel in different reducers.
Figure 3 shows the running time of these tasks averaged over three runs, and shows Fig.3.
Evaluation of CloudBurst running time while scaling the number of reads and sensitive for mapping to the (A) full human genome; (B) chromosomes 1; and (C) 22 on the local cluster with 24 cores.
Tinted lines indicate timings allowing 0 (fastest) through four (slowest) mismatches between a read and the reference.
As the number of reads increases, the running time increases linearly.
As the number of allowed mismatches increases, the running time increases superlinearly from the exponential increase in seed instances.
The four mismatch computation against the full human genome failed to complete due to lack of available disk space after reporting 25 billion end-to-end alignments.
that CloudBurst scales linearly in execution time as the number of reads increases, as expected.
Aligning all 7M reads to the full genome with four mismatches failed to complete after reporting 25 billion mappings due to lack of available disk space.
Even allowing zero mismatches created 771M end-to-end perfect matches from the full 7M read set, but most other tools would report just one match per read.
Allowing more mismatches increases the runtime superlinearly, because higher sensitivity requires shorter seeds with more chance occurrences.
The expected number of occurrences of a seed length s in a sequence of length L is (L s +1)/4s, so a random 18 bp sequence (k =1) is expected to occur 0.04, 0.003 and 0.001 times in the full genome and chromosomes 1 and 22, respectively, while a 7 bp sequence (k =4) is expected to occur >17 500, >15 000 and >3000 times, respectively.
Consequently, short seeds have drastically more chance occurrences 1367 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1368 13631369 M.C.Schatz Fig.4.
CloudBurst running time compared with RMAP for 7M reads, showing the speedup of CloudBurst running on 24 cores compared with RMAP running on 1 core.
As the number of allowed mismatches increases, the relative overhead decreases allowing CloudBurst to meet and exceed 24 linear speedup.
and correspondingly more running time even though most chance occurrences will fail to extend into end-to-end matches.
The second test compared the performance CloudBurst on 24 processor cores with a serial execution of RMAPM (version 0.41) on 1 core with the full read set to chromosomes 1 and 22.
RMAP requires a 64 bit operating system, so it was run on 1 core of a 64 bit dual core 2.4 GHz AMD Opteron 250 with 8 GB of RAM running RedHat Enterprise Linux AS Release 3 Update 9.
CloudBurst was configured as before, except with the alignment filtration option enabled so only a single alignment was reported for each read identical to those reported by RMAPM.
Figure 4 shows the results of the test, and plots the speedup of CloudBurst over RMAP for the different levels of sensitivity.
The expected speedup is 24, since CloudBurst runs in parallel on 24 cores, but CloudBursts speedup over RMAP varies between 2 and 33 depending on the level of sensitivity and reference sequence.
At low sensitivity (especially k =0), the overhead of shuffling and distributing the data over the network overwhelms the parallel computation compared with the in-memory lookup and evaluation in RMAP.
As the sensitivity increases, the overhead becomes proportionally less until the time spent evaluating alignments in the reduce phase dominates the running time.
The speedup beyond 24 for high-sensitivity mapping is due to implementation differences between RMAP and CloudBurst, and the additional compute resources available in the parallel environment (cache, disk IO, RAM, etc.).
The speedup when mapping to the full genome did not improve as the level of sensitivity increased because of the increased overhead from the increased data size.
This effect can be minimized by aligning more reads to the genome in a single batch, and thus better amortize the time spent emitting and shuffling all of the k-mers in the genome.
The next experiment compared CloudBurst with an ad hoc parallelization scheme for RMAP, in which the reads are split into multiple files, and then RMAP is executed on each file.
In the experiment, the full read set was split into 24 files, each containing 294k reads, and each file was separately mapped to chromosome 22.
The runtimes were just for executing RMAP, and do not consider any overhead of partitioning the files, remotely launching the program, or monitoring the progress, and thus the expected speedup should be a perfect 24.
However, the runtimes of the different files varied considerably depending on which reads were present, and the corresponding speedup is computed based on the runtime for the longest running file: between 18 and 41 s with a 12 speedup for zero mismatches, 2667 s with a 14 speedup for one mismatch, 3498 s with a 16 speedup for two mismatches, 132290 s with a 21 speedup for three mismatches and 13791770 s with a 29 speedup for four mismatches.
The superlinear speedup for four mismatches was because the total computation time after splitting the read set was less than the time for the full batch at once, presumably because of better cache performance for RMAP with fewer reads.
This experiment shows the ad hoc scheme works well with speedups similar to CloudBurst, but fails to reach perfect linear speedup in most cases because it makes no special considerations for load balance.
In addition, an ad hoc parallelization scheme is more fragile as it would not benefit from the inherent advantages of Hadoop: data-aware scheduling, monitoring and restart and the high-performance file system.
4 AMAZON CLOUD RESULTS CloudBurst was next evaluated on the Amazon EC2.
This environment provides unique opportunities for evaluating CloudBurst, because the performance and size of the cluster are configurable.
The first test compared two different EC2 virtual machine classes with the local dedicated 24-core Hadoop cluster described above.
In all three cases, the number of cores available was held constant at 24, and the task was mapping all 7M reads to human chromosome 22 with up to four mismatches, with runtimes averaged over three runs.
The first configuration had 24 Small Instance slaves running Hadoop 0.17.0, priced at $0.10 per hour per instance and provides one virtual core with approximately the performance of a 1.01.2 GHz 2007 Xeon processor.
The second configuration had 12 High-CPU Medium Instance slaves, also running Hadoop 0.17.0 and priced at $0.20 per hour per instance, but offers two virtual cores per machine and have been benchmarked to have a total performance approximately five times the small instance type.
The running time for the High-CPU Medium Instance class was 1667 s, and was substantially better per dollar than the Small Instance class at 3805 s, and even exceeds the performance of the local dedicated cluster at 1921 s. The final experiment evaluated CloudBurst as the size of the cluster increases for a fixed problem.
In this experiment, the number of High-CPU Medium Instance cores varied between 24, 48, 72 and 96 virtual cores for the task of mapping all 7M reads to human chromosome 22.
Figure 5 shows the running time with these clusters averaged over three runs.
The results show CloudBurst scales very well as the number of cores increases: the 96-core cluster was 3.5 times faster than the 24-core cluster and reduced the running time of the serial RMAP execution from >14 h to 8 min (>100 speedup).
The main limiting factor towards reaching perfect speedups in the large clusters was that the load imbalance caused a minority of the reducers running longer than the others.
This effect was partially solved by reconfiguring the parallelization settings: the number of reducers was increased to 60 and the redundancy of the low-complexity seeds was increased to 24 for the 48-core evaluation, 144 and 72 for the 72-core evaluation and 196 and 72 for the 96-core evaluation.
With these settings, the computation had better balance across the virtual machines and decreased the wall clock time of the execution.
1368 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1369 13631369 CloudBurst Fig.5.
Comparison of CloudBurst running time (in seconds) while scaling size of the cluster for mapping 7M reads to human chromosome 22 with at most four mismatches on the EC2 Cluster.
The 96-core cluster is 3.5 faster than the 24-core cluster.
5 DISCUSSION CloudBurst is a new parallel read-mapping algorithm optimized for next-generation sequence data.
It uses seed-and-extend alignment techniques modeled after RMAP to efficiently map reads with any number of mismatches or differences.
It uses the Hadoop implementation of MapReduce to efficiently execute in parallel on multiple compute nodes, thus making it feasible to perform highly sensitive alignments on large read sets.
The results described here show CloudBurst scales linearly as the number of reads increases, and with near linear parallel speedup as the size of the cluster increases.
This high level of performance enables computation of extremely large numbers of highly sensitive alignments in dramatically reduced time, and is complementary to new BWT-based aligners that excel at quickly reporting a small number of alignments per read.
CloudBursts superior performance is made possible by the efficiency and power of Hadoop.
This framework makes it straightforward to create highly scalable applications with many aspects of parallel computing automatically provided.
Hadoops ability to deliver high performance, even in the face of extremely large datasets, is a perfect match for many problems in computational biology.
Seed-and-extend style algorithms, in particular, are a natural fit for MapReduce, and any of the hash-table based seed-and-extend alignment algorithms including BLAST, SOAP, MAQ or ZOOM could be implemented with MapReduce.
Future work for CloudBurst is to incorporate quality values in the mapping and scoring algorithms and to enhance support for paired reads.
We are also exploring the possibility of integrating CloudBurst into RNA-seq analysis pipeline, which can also model gene splice sites.
Algorithms that do not use a hash table, such as the BWT based short-read aligners, can also use Hadoop to parallelize execution and the HDFS.
Implementing algorithms to run in parallel with Hadoop has many advantages, including scalability, redundancy, automatic monitoring and restart and high-performance distributed file access.
In addition, no single machine needs to have the entire index in memory, and the computation requires only a single scan of the reference and query files.
Consequently, Hadoop based implementations of other algorithms in computational biology might offer similar high levels of performance.
These massively parallel applications, running on large compute clouds with thousands of nodes, will drastically change the scale and scope of computational biology, and allow researchers to cheaply perform analyses that are otherwise impossible.
ACKNOWLEDGEMENTS I would like to thank Jimmy Lin for introducing me to Hadoop; Steven Salzberg for reviewing the manuscript; and Arthur Delcher, Cole Trapnell and Ben Langmead for their helpful discussions.
I would also like to thank the generous hardware support of IBM and Google via the Academic Cloud Computing Initiative used in the development of CloudBurst, and the Amazon Web Services Hadoop Testing Program for providing access to the EC2.
Funding: National Institutes of Health (grant R01 LM006845); Department of Homeland Security award NBCH207002.
Conflicts of Interest: none declared.
ABSTRACT Motivation: Many enzymes are not absolutely specific, or even promiscuous: they can catalyze transformations of more compounds than the traditional ones as listed in, e.g.
KEGG.
This information is currently only available in databases, such as the BRENDA enzyme activity database.
In this article, we propose to model enzyme aspecificity by predicting whether an input compound is likely to be transformed by a certain enzyme.
Such a predictor has many applications, for example, to complete reconstructed metabolic networks, to aid in metabolic engineering or to help identify unknown peaks in mass spectra.
Results: We have developed a system for metabolite and reaction inference based on enzyme specificities (MaRIboES).
It employs structural and stereochemistry similarity measures and molecular fingerprints to generalize enzymatic reactions based on data available in BRENDA.
Leave-one-out cross-validation shows that 80% of known reactions are predicted well.
Application to the yeast glycolytic and pentose phosphate pathways predicts a large number of known and new reactions, often leading to the formation of novel compounds, as well as a number of interesting bypasses and cross-links.
Availability: MATLAB and C++ code is freely available at https://gforge.nbic.nl/projects/mariboes/ Contact: d.deridder@tudelft.nl Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION In biotechnology, much effort is spent on altering metabolism, mainly of industrially relevant microorganisms such as bacteria, yeasts and fungi.
In most cases, the aim is to increase existing product yield or to introduce and optimize a pathway to a new product.
To be able to perform such metabolic engineering, one needs a full description of the metabolism of the species of interest: to select desired functions (enzymes) needed to introduce a new To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First authors.
pathway, to unravel metabolic regulation, to find bottlenecks in metabolism, and to reveal undesired bypasses.
Missing functions or gaps in this metabolic network description make metabolic engineering difficult; but even when the main pathways are known, missing bypasses or cross-links may pose problems.
It is therefore essential to have a full overview of all possible metabolic reactions in the cell.
The metabolic networks of model organisms are mostly sufficiently characterized and annotated (e.g.
Feist and Palsson, 2008; Herrgard et al., 2008).
For newly sequenced species, metabolic functions are usually derived by looking for genes homologous to known enzymes in other species (e.g.
Pireddu et al., 2006).
At a certain stage, homology does not suffice to complete the metabolic network, i.e.
to fill the remaining gaps in a network, to connect dead ends, or to create links between fragmented (sub)networks and pathways.
In such cases one needs to perform an extensive manually search for functions or pathways (Feist et al., 2009).
To complete metabolic networks, enzyme functions can also be inferred from metabolome data, such as mass spectra (MS).
Although high-resolution techniques and advanced pathway extraction tools are available (Breitling et al., 2006; Gipson et al., 2008), it is still not always possible to uniquely identify compounds, as the MS peaks are not sufficiently accurate.
Even when measurements are perfect, structural isomers cannot be distinguished by mass alone.
Alternatively, metabolic networks can be completed by exploiting enzyme functionality information.
The key idea is that (at least some) enzymes are known to be aspecific, i.e.
able to perform the associated chemical transformation on compounds other than the one traditionally associated with that enzyme (DAri and Casadess, 1998).
Some enzymes can even perform slightly different transformations (OBrien and Herschlag, 1999).
Modeling this aspecificity is important for biotechnology and poses significant bioinformatics challenges; for example, predicting aspecificity based on mining the available enzyme characterization data (Nobeli et al., 2009).
A number of researchers have explored the idea of predicting metabolic reactions based on an analysis of the basic biochemical transformations performed by enzymes (Hatzimanikatis et al., 2005; The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
https://gforge.nbic.nl/projects/mariboes/[09:39 26/10/2009 Bioinformatics-btp507.tex] Page: 2976 29752982 M.J.L.de Groot et al.
Li et al., 2004).
Specifically in the field of the biotransformation of xenobiotics (substances foreign to a biological system), several such systems have been developed.
These tools, such as METEOR, META (Klopman et al., 1999), CATABOL (Dimitrov et al., 2004), UM-PPS (Ellis et al., 2008), etc., mainly consist of manually supplied reaction rules and heavily depend on user selection of feasible predicted pathways.
In Oh et al.
(2007), instead of manually created rules, xenobiotic reaction possibilities were derived using measures of structural similarity between compounds, which were represented as graphs.
Chemical transformations were captured in so-called reaction patterns (RDM, or Reaction centre-Difference-Matched patterns).
A given query compound is assumed to be converted by an enzyme when its RDM pattern is present and the compound is sufficiently similar to known substrates of all enzymes with the same RDM pattern.
To develop their system, Oh et al.
(2007) used the KEGG database (Kanehisa and Goto, 2000), which describes only enzymatic reactions with well-known metabolic function.
The limited amount of data available means that generalization can only take place per RDM pattern, rather than per enzyme.
The latter would be desirable, as enzymes with identical RDM patterns can be specific to different types of substrates.
We present a novel system for metabolite and reaction inference based on enzyme specificities (MaRIboES), building on the work of Oh et al.
(2007).
We generalize an enzymatic transformation by training a classifier on the list of activities found for that enzyme in the Braunschweig Enzyme Database (BRENDA; Barthelmes et al., 2007).
This unique database contains a large number of enzyme activities reported in literature, found by detailed enzyme characterization including non-metabolically relevant compounds, toxic compounds, etc.
We demonstrate the potential of our method by performing both an internal validation and an application to extend the glycolysis and pentose phosphate pathways of Saccharomyces cerevisiae, in which we predict a number of reactions (bypasses and cross-links), many of which lead to the formation of novel compounds.
2 METHODS Our system takes four steps to get from the enzyme activity data in BRENDA to a trained classifier for each enzymatic transformation.
First, reaction and compound data are extracted from BRENDA and preprocessed.
Second, enzymatic reactions are defined in terms of reaction patterns (changes in molecular structure), which are then used to derive enzymatic transformations between reactant pairs.
Third, sets of candidate compounds for each transformation (having the reaction pattern) are characterized by a set of structural, stereo and fingerprint features.
Finally, a selection of these features is used to build a classifier for each enzymatic transformation, predicting for novel compounds whether that transformation is likely to occur.
2.1 Extracting data from BRENDA We downloaded version 0702 of BRENDA (August 22, 2008) and selected all information available for 3016 enzymes present in S.cerevisiae.
In this way, we obtained a list of reactions known to be catalyzed by each enzyme.
A total of 3360 compounds were involved in these reactions, 1146 of which are known as natural substrates or products.
We also gathered the chemical structures (mol-files) of the substrates and products of all enzymatic reactions; these were available for 2399 of the 3360 compounds (August 2008).
Fig.1.
Illustration of an RN pattern.
Here, a phosphate group is added to D-glucose, obtaining D-glucose 6-phosphate.
The RN pattern, O1a-O2b:*-P1b+O1c+O1c+O3b, fully describes the chemical transformation, using the atom types defined in Hattori et al.
(2003).
R-and N-atoms are indicated; all other atoms are I-atoms.
BRENDA contains a number of duplicates, i.e.
compounds with the same composition and structure but different names (e.g.
glucose and D-glucose).
To remove these, we employed the following strategy.
First, we examined which pairs of compounds had the same binary and discrete fingerprints (see Section 2.3).
For these pairs, we subsequently calculated similarity in structure and stereochemistry.
If two compounds were completely equal in structure and stereochemistry, we treated these compounds as identical.
This left 1914 unique molecular structures, 753 of which correspond to natural substrates or products.
Of several compounds (particularly monosaccharides), we found both the linear and the ring variant in the database.
This poses problems for the definition of reactant pairs based on compound similarity (see Section 2.2), particularly if a substrate is in linear form and the corresponding product in ring form.
Therefore, we first automatically transformed all compounds in linear form which are likely to be in ring form in solution, into their ring variants.
To accomplish this, we first detected whether a compound had a ketone or aldehyde group.
If so, we assessed whether it was indeed a Fisher projection of a saccharide.
If this was also the case, we adjusted the bonds such that the compound became cyclic, and subsequently generated the Haworth projection using MarvinView (ChemAxon, 2009).
2.2 Inferring enzymatic transformations 2.2.1 Defining enzymatic transformations We defined chemical structure transformation patterns similar to the proposal in Oh et al.
(2007).
They represented molecules as graphs and looked for differences between the substrate and product of an enzymatic reaction (the reactant pair).
First, these graphs were aligned (see below), to obtain a matched and unmatched part.
The boundary atom between the matched and unmatched part is called the reaction center (R-atom); the atom(s) adjacent to the R-atom in the unmatched parts are the difference atom(s) or D-atom(s); and the atom(s) adjacent to the R-atom in the matched region are the matched atom(s) or M-atom(s).
Unlike Oh et al.
(2007), we focus on describing all changes in molecular structure due to a reaction, i.e.
all unmatched atoms rather than just those connected to the reaction center.
Therefore, we distinguish two different atom types besides the reaction center (R-atom): identical or I-atoms, all matched atoms except the R-atom; and non-identical or N-atoms, all unmatched atoms (not to be confused with iodine (I) or nitrogen (N) atoms).
The RN pattern describing the entire transformation between a reactant pair (Fig.1) then consists of an RN pattern for the substrate (RNs) and one for the product (RNp).
2.2.2 Inferring enzymatic transformations The representation of chemical and biological molecules by means of graphs permits the use of a maximum common subgraph (MCS) algorithm to identify the chemical structure transformation pattern between a reactant pair (Gardiner et al., 1997).
Many existing algorithms convert the MCS problem into a maximum clique finding problem, by introducing an association graph (Hattori et al., 2003).
Due to the nature of chemical structures, this association graph usually is very 2976 [09:39 26/10/2009 Bioinformatics-btp507.tex] Page: 2977 29752982 MaRIboES dense, making clique finding computationally prohibitive.
Cao et al.
(2008) proposed an algorithm that directly operates on the chemical structure graphs themselves.
Still, although much more computationally efficient than algorithms based on clique finding, it cannot always infer the MCS in reasonable computation time without progressive optimization (human intervention), especially when the MCS is large.
The main bottleneck of the algorithm is that the common subgraph is extended by only one node at a time.
As a consequence, many different ways exist in which the same common subgraph can be constructed.
We adjusted their method to speed up the process (van Berlo et al., 2009), enabling us to detect all maximal substructures common to a pair of molecules, rather than only the maximum one (a maximal common subgraph is any complete common subgraph not contained in any other complete common subgraph; the MCS is the largest of these).
A natural score for a common subgraph thus found would be its size, the number of matched atoms (R-and I-atoms).
However, to reflect the prior knowledge that most enzymes affect a molecule at only one point, we can assign a lower weight to R-atoms than to I-atoms.
Furthermore, as many reactions add a phosphate group to a molecule as a single, elementary unit (by extracting it from ATP), it would be desirable to count this group as a single atom rather than four.
Hence, we adopted the following similarity score (SS) for weighting the different maximal common subgraphs found between graphs G1 and G2: SS(G1,G2)=w1|R|+w2|I|+w3|PO3| (1) Here, |R| denotes the number of R-atoms, |I| the number of I-atoms and |PO3| the number of aligned phosphate groups.
For the reasons given above, the weight vector w= (w1,w2,w3) was set to (0.5,1,3), as this favors substructures that (i) include long backbones (and/or few phosphate groups) and (ii) contain few reaction centers.
As a result, the MCS will not necessarily lead to the highest similarity score.
This emphasizes the need for an algorithm that can identify all maximal common subgraphs.
Like Hattori et al.
(2003), we used the Jaccard coefficient (also known as the Tanimoto coefficient) to adjust the similarity score for the size of the two aligned graphs G1 and G2: JC(G1,G2)= G opt(G1,G2) G1+G2Gopt(G1,G2) (2) where Gopt(G1,G2) is the highest scoring maximal subgraph common to G1 and G2 according to (1) and G indicates the number of nodes in G. 2.2.3 Defining reactant pairs As an enzymatic reaction usually involves multiple substrates and products, we employed an iterative procedure to find all reactant pairs.
We first selected from all possible substrateproduct combinations the one resulting in the highest JC.
Second, we removed the corresponding Gopt from both the substrate and product of this reactant pair.
This procedure was iterated until all atoms in all substrates and products were part of some Gopt.
Figure 2 illustrates the procedure for the hexokinase reaction with D-glucose as substrate, showing that the highest JC is obtained when ATP is aligned with ADP.
The corresponding Gopt is removed from both molecules.
In the second step, the reactant pair consisting of D-glucose and D-glucose 6-phosphate yields the highest JC.
In the final step, the remaining phosphate groups of ATP and D-glucose 6-phosphate are aligned.
In this way, we infer a complete reaction definition.
Note that we can obtain multiple RN patterns for a single enzyme, as enzymes can catalyze similar reactions and reverse reactions are taken into account as well.
Let te denote a chemical transformation (i.e.
an RN) as accomplished by enzyme e. We define Te as the set of all possible chemical transformations that can be accomplished by enzyme e, and the set Et as the set of all enzymes that can perform the same chemical transformation t. 2.3 Characterizing candidate compounds In general, the RN patterns in Te are quite consistent between the different reactions listed for e in BRENDA.
This allowed us to infer possible Fig.2.
Reactant pairs inferred for a hexokinase reaction in which D-glucose is converted into D-glucose 6-phosphate.
(a) The best alignment is between ATP andADP, (b) the next best between D-glucose and D-glucose 6-phosphate and (c) finally, the remaining parts of ATP and D-glucose 6-phosphate are aligned (atoms already matched are no longer considered), resulting in Gopt being the phosphate group.
R-and N-atoms are indicated, all other atoms are I-atoms.
new substrates for an enzyme e, by searching for candidate compounds, containing a particular RNs-or RNp-pattern.
Let cte denote a particular candidate compound c for the chemical transformation t as accomplished by enzyme e, and Cte the set of all such candidates.
We divided this set into substrates or products involved in reactions listed in BRENDA as catalyzed by e, Pte (positive examples), and possible new candidates N t e (negative examples).
The end goal is to construct a classifier to predict whether or not a particular candidate compound c can be transformed by a chemical transformation te of an enzyme e, based on a number of features of that compound.
We expect that c is more likely to be transformed if it is similar to the compounds in the set of positive examples Pte.
Therefore, we characterize c by the following potentially useful features: Structural similarity: just as the MCS algorithm was applied to determine the chemical transformation of a reactant pair, we used it to infer the Gopt between c and each of the (other) positive examples.
We used the largest JC to construct a structural similarity feature, i.e.
we employ the similarity to the closest positive example as a feature.
Stereo dissimilarity: we believe that for several enzymes, stereo (3D) information can be an important feature for determining whether or not a compound can be transformed by an enzyme e. Therefore, we inferred whether the stereochemistry of c matched that of the positive examples.
To this end, for all positive examples we used the alignment as obtained by the MCS algorithm and counted the number of times a stereo bond differs, as illustrated in Figure 3.
We selected 2977 [09:39 26/10/2009 Bioinformatics-btp507.tex] Page: 2978 29752982 M.J.L.de Groot et al.
Fig.3.
Comparison of D-glucose and D-galactose results in one stereo difference.
MCSs are rotated in 3D such that their major axes are aligned in the xy plane; stereo differences can then be easily identified by checking the z-coordinates of atoms.
the minimum of these for constructing a stereo-based distance feature, i.e.
we employ the difference with the closest positive example as a feature.
Binary and discrete fingerprints: fingerprinting is one of the most widely applied methods for measuring similarity.
A fingerprint of a compound is usually a binary vector in which each element is nonzero if the corresponding feature (e.g.
it is an alcohol) holds for that compound, and zero otherwise.
Sometimes these fingerprints are discrete instead of binary.
In this case, each element denotes the number of times a particular feature (e.g.
a certain type of atom) is present in the compound.
We used the 204 binary and 57 discrete fingerprints as defined by Checkmol (Haider, 2003).
2.4 Constructing a classifier As a final step of our algorithm, we constructed a classifier based on the available features, which outputs a posterior probability pt,qe of a compound q being transformed by transformation te.
For this, we used a Parzen kernel density estimation-based classifier (Duin, 2009) as, in our experience, it works well in situations where there are only few examples.
First, all 263 features (1+1+204+57) were individually normalized by applying mean-variance normalization.
Subsequently, they were ordered based on their mutual information with the labels (negative or positive), i.e.
the amount of information that can be inferred about the labels by observing the features.
We prefer this option over using a wrapper approach (Wessels et al., 2005), since wrapper approaches have exponential time complexity and often do not improve classification performance (Lai et al., 2006).
Stereo dissimilarity is only predictive in conjunction with any of the other features.
For example, if two molecules have exactly the same 2D-structure (e.g.
glucose and galactose), then stereo similarity enables us to distinguish these compounds.
Therefore, although mutual information between stereo similarity and the labels is often low, we always included it as a feature.
Next, the optimal number of features kopt was found using leave-one-out cross-validation (LOOCV).
For k =1,...,263, we trained a classifier on the first k of the ordered features, determined the ROC (receiver operator characteristic; Duda et al., 2001) curve and used the area under this curve (AUC) to evaluate the performance of the classifier.
The smoothing parameter s of the Parzen density estimate was also optimized on the training data, using the leave-one-out Lissack and Fu estimate (Duin, 2009).
This means that at least three positive examples and three negative example should be available, to allow estimation of the two parameters kopt and s. This was the case for 137 RN pattern-enzyme combinations, of 78 unique RN patterns and 57 enzymes.
To prevent overfitting, we employed the same smoothing parameter s for both the negative and positive class and for each feature.
3 EXPERIMENTAL SETUP 3.1 Validation In a first experiment, we assessed whether we could predict the correct enzymatic transformation te for a particular query compound q.
To test prediction performance, we first determined whether this compound could be transformed by enzyme e; that is, whether it contains one of its RN patterns, corresponding to one of its enzymatic transformations.
If so, then q serves as either a positive example for te (if q is listed in BRENDA as the substrate or product of a reaction catalyzed by e) or a negative example (if not).
We then removed q from the training set, learned the prediction rule based on the remaining training samples (Pte\q and Nte\q), and applied the resulting rule to query compound q to assess whether it is likely to be transformed by that particular enzymatic transformation of that enzyme.
It was necessary that, after removing the query compound q, at least three positive and three negative examples remained for training (see Section 2.4).
More formally, let tqe denote a chemical transformation t accomplished by enzyme e, that can be applied to query compound q, and for which the training set except query compound q contains at least three positive as well as negative examples.
Let TqE denote the full set of chemical transformations that can be applied to query compound q by any of the enzymes in set E. Note that this set may contain multiple chemical transformations for one enzyme but also a single chemical transformation for multiple enzymes.
Let lt,qe be a label equal to one if query compound q is a positive example for the chemical transformation t as performed by enzyme e and zero otherwise, and let pt,qe be the posterior probability as calculated by applying the corresponding prediction rule to the query compound.
We can then order the posterior probabilities to analyze whether transformations predicted as the most likely indeed correspond to transformations for which q is a positive example, i.e.
if pt,qe correlates with l t,q e .
3.2 Application to central metabolism of S.cerevisiae In a second experiment, we focused on the well-described central metabolism of S.cerevisiae, specifically glycolysis and the pentose phosphate pathway.
We used all compounds known to be involved in these pathways as input for all chemical transformation classifiers.
For each compound-transformation pair (q,te), this leads to a posterior probability pt,qe of q being a substrate for te (note that if q does not contain the RN-pattern, no probability can be calculated).
Each reaction for which this posterior exceeded 0.9 was then automatically predicted to occur.
It is hard to base decisions on these posteriors alone, as some transformations yield many more predictions than others, i.e.
are far less specific.
Therefore, for each transformation te, we also ranked the n predicted compounds by the posterior probability pt,qe .
We then applied a hypergeometric test with the null hypothesis that the top j was not enriched for true positives, using the remaining nj compounds as background, for j=1,...,n. Compound j was then predicted to be transformed by te if the corresponding null hypothesis was rejected (i.e.
if the Bonferroni-corrected P < 0.05).
At the end of this prediction step, we checked whether each predicted product (found by applying the chemical transformation to the substrate) is already listed in BRENDA or KEGG, by looking for an identical compound (see Section 2.1).
If no match was found, the predicted new compound was given a unique identifier (new...).
The compounds were translated to SMILE strings (ChemAxon, 2009) and searched for using the ChemSpider search engine (ChemZoo, 2007).
If found, the relevant compound name was assigned; 2978 [09:39 26/10/2009 Bioinformatics-btp507.tex] Page: 2979 29752982 MaRIboES otherwise, the compound was annotated manually or given a standard IUPAC name (ChemAxon, 2009).
We re-iterated this entire procedure twice, using the compounds predicted to be formed in the previous iteration as new inputs.
4 RESULTS AND DISCUSSION 4.1 Validation 4.1.1 Most predictions are reliable For any compound q, we would like transformations te for which l t,q e =1 (i.e.
for which q is known substrate of enzyme e) to be predicted with high probability.
To verify this, q can be left out during classifier training.
After training, we can then test whether the correct transformations are indeed highly ranked.
Figure 4 lists the 130 compounds thus tested.
For each compound q, a bar indicates a ranked list of all transformations TqE , from high (left) to low (right) probability transformations.
Black elements indicate transformations for which q is a known substrate or product; dark gray elements indicate either different transformations of enzyme e (members of Te, see Section 2.2), or identical transformations t by a different enzyme (members of Et).
For 77 compounds, an actual enzyme reaction annotated in BRENDAto perform this particular conversion (i.e.
a black element) is found as the most likely transformation.
For an additional 27 compounds, this is the second most likely transformation and for 26 compounds, the true conversion ranks lower.
For 13 of the latter 53 compounds, related enzymes or transformations are predicted as most likely (dark gray elements).
In the 26 compounds for which the actual transformations were not ranked highly, cofactors (e.g.
ADP, UDP, etc.)
are overrepresented.
These compounds play a very generic role in many transformations, and the RN patterns hence occur in many transformation classifiers, increasing the chances of misclassification.
We conclude that, as 80% of the results (77 + 27 out of 130) are good, our system produces reliable predictions even given the relatively limited amount of training data available.
4.1.2 Features selected reflect enzyme specificities In a second experiment, we investigated whether the features used for building the prediction rule differed between different enzymatic transformations.
For this, we compared all prediction rules corresponding to the same transformation t but accomplished by different enzymes, Et .
Four of these results are shown in Supplementary Figure 1.
The figures show that not only the type, but also the number of features used in the prediction rule can be quite different between enzymatic transformations.
Structural similarity based on MCS frequently seems to be most relevant, and hence is a highly predictive feature.
This agrees with the findings of Oh et al.
(2007).
We do find that for different enzymes that perform the same conversion, different subsets of fingerprint features are selected.
This indicates that specificity is governed by different structural features for each individual enzyme.
It also demonstrates that it is apparently beneficial (in terms of predictive power) to use more than just structural similarity.
4.2 Application to central metabolism of S.cerevisiae To demonstrate how our system can yield practical predictions, we applied it repeatedly to the glycolysis and pentose phosphate Fig.4.
Classification performance for different query compounds q.
Each row represents the results for a particular q, showing the different candidate transformations in TqE ordered from the most likely (left) to the most unlikely (right).
Black elements: chemical transformations tqe listed in BRENDA (lt,qe =1).
Dark gray: either different transformations of q by the same enzyme e (in Te) or identical transformations t by a different enzyme (in Et); light gray: chemical transformations tqe that could in principle be applied to compound q, but for which we have no biological evidence (lt,qe =0).
pathways of S.cerevisiae, to see which new reactions and possibly new compounds would be predicted.
The reactions in these pathways were used in training and hence not considered as new predictions.
We performed three iterations; in the second and third, we used only compounds predicted to be produced in the previous iteration as input.
Table 1 gives an overview of the number of predicted novel reactions and compounds; Supplementary Figure 2 gives a detailed overview of all outcomes.
As there are far too many predictions to discuss, below we focus on some key findings.
4.2.1 Many predicted reactions are known to exist We predict a large number of new reactions.
The corresponding chemical transformations were not present in the dataset we used to train our system, either because BRENDA does not list them or because 2979 [09:39 26/10/2009 Bioinformatics-btp507.tex] Page: 2980 29752982 M.J.L.de Groot et al.
Table 1.
Number of novel reactions and compounds predicted by the system when applied to the glycolysis and pentose phosphate pathways in S.cerevisiae, in three iterations Iteration Novel Of which Novel Of which in reactions in KEGG compounds BRENDA predicted predicted or KEGG 1 70 17 62 11 2 109 37 58 7 3 84 8 41 1 Total 263 62 161 19 it lists less than three conversions with accompanying structure information (mol-file), too little to train the classifier on.
Strikingly, a large number of these predicted reactions are listed in KEGG (see Table 1, left columns and the green arrows in Supplementary Figure 2).
This demonstrates that our system is able to generalize well, and indicates that it is potentially useful in, for example, metabolic network reconstruction and metabolic engineering.
4.2.2 Enzymatic alternatives to autocatalytic reactions are found Some predicted reactions occur in pathways described in literature as autocatalytic, i.e.
not requiring enzymes.
For example, an autocatalytic pathway has been reported from dihydroxyacetone phosphate and glyceraldehyde 3-phosphate to hydroxypyruvaldehyde (Thornalley et al., 1984).
However, we find leads suggesting possible enzyme catalyzed conversions (Fig.5a).
In the first iteration of our system, dihydroxyacetone phosphate is predicted to be transformed into dihydroxyacetone (an existing reaction); in the second iteration, this is further transformed into hydroxypyruvaldehyde.
A similar path is predicted from glyceraldehyde 3-phosphate via glyceraldehyde to hydroxy-pyruvaldehyde.
Perhaps the corresponding enzymes are required to decrease the activation energy for these reactions only under in vivo conditions and hence may have been missed in Thornalley et al.
(1984).
4.2.3 Interesting bypasses and cross-links are suggested Some existing bypasses are predicted.
For example, an isoenzyme conversion from fructose 1,6-bisphosphate to fructose 6-phosphate is predicted, as well as a longer bypass via fructose 1-phosphate and fructose (Fig.5b).
The reactions involved are all found in KEGG (RPAIRS RP00242, RP00680 and RP00210).
Interestingly, an even longer bypass is predicted through 1-keto-D-fructose and 1-keto-D-fructose 1-phosphate.
Another example is the predicted formation of lactate and subsequently lactoyl-CoA from both pyruvate and phosphoenolpyruvate.
Although this is not an annotated path in S.cerevisiae, this cross-link may be interesting for sterol biogenesis and propionate metabolism, in which lactoyl-CoA is involved (KEGG PATHWAYS KO00643 & KO00640).
Missing such bypasses or cross-links could cause problems when applying flux analysis, as the flux through the known reactions may be overestimated, leading to incorrect conclusions on possible bottleneck reactions.
4.2.4 Predictions of obscure metabolites make sense Our system predicts the production of a number of compounds that are Fig.5.
Example pathways constructed using the MaRIboeS algorithm.
(a) Pathway containing reactions originally reported as autocatalytic.
(b) Prediction of possible pathways around phosphofructokinase.
(c) Predictions from D-xylulose 5-phosphate.
The direction of the arrows indicates the reaction direction.
Predictions from the first, second and third iterations are indicated by closed, open and square arrowheads, respectively.
EC numbers denote the enzyme for which an associated classifier predicted the reaction.
In (c), numbers indicate the estimated Gibbs free energy of the reaction (rGo, kcal/mol).
not yet listed as part of a traditional pathway (Table 1, right columns), but that have been described in literature.
This suggests that the corresponding enzymatic transformations may have been overlooked.
Figure 5c shows an example of D-xylulose 5-phosphate, which is predicted to be transformed into four compounds: D-xylulose (a known reaction, KEGG RP01652), D-xylulose 1,5-bisphosphate, 2-keto-D-xylulose 5-phosphate and a diphosphate.
The latter three compounds are not known to exist, but the reactions seem possible from an energy point of view (the estimated Gibbs free energy of each reaction (Jankowski et al., 2008) is shown in the figure).
For D-ribulose 5-phosphate a reaction similar to the second one, to D-ribulose 1,5-bisphosphate, is predicted as well.
This compound is a known substrate for glyoxylate and dicarboxylate metabolism (KEGG pathway KO00630).
This suggests that the predicted formation of D-xylulose 1,5-bisphosphate may be valid.
It has been described before as being produced from D-xylose in algae (Wu et al., 1970); as a side product of an enzymatic reaction involving a misprotonation (Edmondson et al., 1990); as an inhibiting factor for growth on D-xylose and a strong competitive inhibitor of Rubisco (Andersson, 2008).
2980 [09:39 26/10/2009 Bioinformatics-btp507.tex] Page: 2981 29752982 MaRIboES In light of metabolic engineering efforts in which the fermentation of pentoses is engineered in S.cerevisiae (Hahn-Hgerdal et al., 2007; Wisselink et al., 2009), it may be wise to attempt to circumvent production of this compound.
Another example of a predicted obscure metabolite is hydroxy-pyruvaldehyde phosphate, produced directly from dihydroxyacetone phosphate and through a reaction involving a kinase acting on hydroxypyruvaldehyde (Fig.5a).
This compound has been reported as a substrate of glyoxalase in S.cerevisiae (Weaver and Lardy, 1961); as a product of transaldolase (Christen and Gasser, 1976); and in erythrocytes when provided with glucose (Cogoli-Greuter and Christen, 1981).
It has also been reported to react with hydrogen peroxide acting as an antioxidant (Cogoli-Greuter and Christen, 1981), which may point to an interesting application.
Not all predictions are easily explained.
For example, phosphorylation of phosphate groups is performed on nucleotides; our system predicts this to occur as well on, for example, D-xylulose 5-phosphate, D-ribulose 5-phosphate, phosphoenolpyruvate and glycerol 1-phosphate.
Although these compounds have not yet been described, perhaps they play a (minor) role in metabolism.
Other predicted compounds are seemingly instable (e.g.
containing two neighboring keto groups).
5 CONCLUSIONS We have described MaRIboES, a system to predict possible enzymatic transformations as well as the resulting output compounds, given a set of input compounds.
Our work significantly extends that of Oh et al.
(2007).
First, we generalize chemical transformations based on experimental data available in BRENDA, rather than pathway descriptions in KEGG.
This allows us to include non-metabolically relevant conversions and to predict for each individual enzyme rather than for classes of enzymes.
Second, we added both stereochemistry similarity and molecular fingerprints.
Stereo information is essential, as many enzymes are known for their chiral specificity.
Fingerprint features were often selected for prediction, indicating they are useful.
Our system was validated using a metabolome-wide leave-one-out procedure.
For over 80% of the compounds, we predict the enzyme associated with the compound as the first or second most likely one.
Next, we applied it to metabolites in the glycolysis and pentose phosphate pathways of S.cerevisiae.
Besides reactions leading to well-annotated metabolites, we predict formation of novel compounds, for which we can find some confirmation in other organisms.
We also predict enzymatic alternatives for reactions thought to be autocatalytic, interesting bypasses within and cross-links between pathways.
We foresee a number of applications besides the ranking of possible substrates for enzyme characterization.
First, automated metabolic network reconstruction could be improved, by using MaRIboES to calculate function-based rather than sequence-based similarity between enzymes.
Second, prediction of possible bypasses and cross-links can benefit metabolic engineering, by charting alternative routes and identifying potentially competing compounds.
Finally, the predicted compounds may help to interpret metabolomics data, by listing possible candidates for unidentified masses.
MaRIboES performance could be improved by including some estimate of stability (not yet readily available) of the compound predicted to be formed, and by comparing estimates of the activation energy of a chemical reaction to those of natural substrates.
However, it would benefit most from a proper characterization of more enzyme activities (see Supplementary Figure 3).
Although current research invests heavily in high-throughput analyses of genome expression, proteome levels and modifications, physical interactions and metabolites, it would be wise not to forget that these all rely on basic principles unraveled by looking at details rather than the big picture.
Funding: The Netherlands Bioinformatics Center (to MdG) and the Kluyver Centre for Genomics of Industrial Fermentation (to RvB), both supported by the Netherlands Genomics Initiative.
Conflict of Interest: none declared.
ABSTRACT Summary: Popular methods for 3D protein structure similarity search-ing, especially those that generate high-quality alignments such as Combinatorial Extension (CE) and Flexible structure Alignment by Chaining Aligned fragment pairs allowing Twists (FATCAT) are still time consuming.
As a consequence, performing similarity searching against large repositories of structural data requires increased com-putational resources that are not always available.
Cloud computing provides huge amounts of computational power that can be provi-sioned on a pay-as-you-go basis.
We have developed the cloud-based system that allows scaling of the similarity searching process vertically and horizontally.
Cloud4Psi (Cloud for Protein Similarity) was tested in the Microsoft Azure cloud environment and provided good, almost linearly proportional acceleration when scaled out onto many computational units.
Availability and implementation: Cloud4Psi is available as Software as a Service for testing purposes at: http://cloud4psi.cloudapp.net/.
For source code and software availability, please visit the Cloud4Psi project home page at http://zti.polsl.pl/dmrozek/science/cloud4psi.
htm.
Contact: dariusz.mrozek@polsl.pl Received on January 13, 2014; revised on June 5, 2014; accepted on June 12, 2014 1 INTRODUCTION Cloud computing has emerged as a result of increased require-ments for the public availability of computing power and new technologies for data processing.
It has become a mechanism that allows for the control of the development of hardware and software resources by introducing the idea of virtualization.
In practice, cloud computing allows the public to run applica-tions and services on a distributed network using a virtualized system and its resources, and at the same time, allows to abstract away from the implementation details of the system itself.
Apart from the natural applications of cloud computing architecture in business, the concept is also becoming increasingly popular in scientific research, including life sciences, because the theoret-ically infinite resources of the cloud allow computationally inten-sive problems to be solved.
Cloud-based solutions were proposed for various areas of bioinformatics, including automated sequence analysis (Angiuoli et al., 2011), identification of peptide sequences from spectra in mass spectrometry (Lewis et al., 2012), mapping next-generation sequence data to the human genome and other reference genomes, for single nucleotide polymorphism discovery, genotyping and personal genomics (Schatz, 2009), 3D ligand binding site comparison and similarity searching of a structural proteome (Hung and Hua, 2013), sequence alignment, clustering, assembly, display, editing and phylogeny (Krampis et al., 2012).
The 3D protein structure similarity searching is a computa-tionally complex and time-consuming process that requires enhanced computational resources.
This is the motivation behind efforts to develop new methods in the area and build scalable platforms, such as the presented Cloud for Protein Similarity (Cloud4Psi) platform, that allow for the completion of the task much faster by distributing computation on many machines.
2 IMPLEMENTATION 2.1 Algorithms Cloud4Psi enables researchers to search for protein structure similarities by using a combination of three algorithms: jCE, jFATCAT-rigid and jFATCAT-flexible (Prlic et al., 2012).
These are new versions of the combinatorial extension (CE) (Shindyalov and Bourne, 1998) and flexible structure alignment by chaining aligned fragment pairs allowing twists (FATCAT) (Ye and Godzik, 2003) algorithms, which are well-established among the scientific community.
jFATCAT and jCE work on the basis of matching protein structures using aligned fragment pairs.
The limitation of the original CE and FATCAT algo-rithms is that they compute sequence order-dependent align-ments.
The jCE algorithm, an enhanced version of CE, solves the problem by handling circular permutations (Bliven and Prlic, 2012).
The jFATCAT-flexible algorithm, in turn, eliminates the drawbacks of many existing methods that treat proteins as rigid bodies, not flexible structures.
The research conducted by the authors of FATCAT has shown that rigid representation causes many similarities, even when strong, to be omitted.
The flexible version of jFATCAT allows for the entry of twists in protein structures while matching their fragments, which pro-vides better alignments in a number of cases.
All three algorithms used by Cloud4Psi are publicly available through the Protein Data Bank (PDB) (Berman et al., 2000) Web site for those who want to search for structural neighbors.
Moreover, these algorithms are used for precalculated all-to-all 3D-structure comparisons for the whole PDB, which are updated on a weekly basis (Prlic et al., 2010).
*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com (AFPs) w-XPath error Undefined namespace prefix 2.2 Cloud4Psi in the Microsoft Azure Model Cloud4Psi was developed for use in the Microsoft Azure public cloud.
Any application that runs in the Microsoft Azure model is composed of a set of roles performing some tasks.
Basic types of roles include Web roles [for graphical user interface (GUI)] and Worker roles (for computations).
Roles use computational units provided by the Microsoft Azure cloud.
These computational units have various sizes from A0 to A9, determining their computational capabilities and associated costs of their usage.
Description of standard sizes of computational units is provided in the Windows Azure Cloud Specification manual (Microsoft, 2014, http://msdn.microsoft.com/en-us/library/windowsazure/dn 197896.aspx).
Cloud4Psi consists of several types of roles and storage modules responsible for gathering and exchanging data between roles (Fig.1).
Users can initiate the similarity search and receive the search results through the Web role.
The Web role provides a GUI through a user-friendly Web site and also has an additional logical layer.
The logical layer is responsible for converting the parameters received from the user into a form of a request mes-sage that is placed in the Input queue.
The role also has access to the Storage Tables that provide results from the ongoing or fin-ished similarity searches and also access to the virtual hard drive (VHD) for PDB files; this is for when the user decides to send his/her own PDB file for comparison by the Cloud4Psi.
Users search requests are consumed by the Manager role (Worker role), which schedules distributed computations on many Searcher roles.
The Manager role also divides the whole reposi-tory of PDB files into packages that are sent to Searcher roles through the Output queue, and manages associated computa-tional loads between Searcher roles.
Searcher roles (which are also Worker roles) bear the computational load associated with the process of protein comparison.
They perform parallel batch comparisons of a given protein structure with proteins contained in the package retrieved from the Output queue.
This role type is scaled out (scaled horizontally) by adding more computational units and scaled up (scaled vertically) by using computational units of different sizes during the similarity searching process.
After finishing computations for a package, Searcher roles enter results for their integration into a table in the Storage Tables service of the Microsoft Azure cloud and retrieve another package from the Output queue.
3 RESULTS Scalability of the Cloud4Psi was tested in the Microsoft Azure cloud by increasing the number of Searcher roles from 1 to 18 (horizontal scaling) and by increasing the size, i.e.
computational capabilities, of the Searcher roles from A0 to A4 (vertical scal-ing).
Horizontal scaling was performed with the use of A1-sized Searcher roles.
A1 size (formerly small size) represents the cheap-est computational unit providing one unshared core and is the smallest sized unit recommended for production workloads (Microsoft, 2014).
During the vertical scaling, we gradually enlarged the sizes of the Searcher roles from A0 (formerly extra small, with one shared central processing unit (CPU) core) to A4 (formerly extra large, eight CPU cores).
Tests were performed on a repository containing 1000 protein structures from the PDB.
Package size was set to 10 protein structures.
A single comparison of a pair of protein structures takes on average 11 s for jCE and 14 s for jFATCAT (both variants).
The time required for computation depends on the size of the given Fig.1.
Architecture of the Cloud4Psi: Web role provides a front-end for users of the system; Manager role mediates the distribution of the searching process, which is executed by Searcher roles.
Search requests and packages are transferred through Input and Output queues.
Roles have access to various storage resources inside the cloud, including Binary large object (BLOB) Storage (VHD with PDB files) and Storage Tables (containing results of similarity searches) 2823 Structure similarity searching ( utilize ,(Graphical User Interface) w , their one , ten econds econds protein structure.
In Figure 2a, we have shown the n-fold speedup for jCE and jFATCAT algorithms when scaling the Cloud4Psi system horizontally.
We noticed that using two in-stances of the Searcher role increased the speed of the similarity searching by almost 2-fold.
Adding more Searcher roles propor-tionally accelerated the process.
However, when using 48 Searcher roles, the dynamics of the acceleration slowed down.
Finally, the acceleration ratio (n-fold speedup) reached the level of 15.17 for jCE, and 15.87 for jFATCAT, when the number of instances of the Searcher roles was increased from 1 up to 18.
In Figure 2b, we have shown the n-fold speedup when scaling the system vertically.
Good results were obtained for Searcher roles of sizes A1 and A2 (A0 is not recommended for production environments).
Above size A2, i.e.
two computing units, we noticed that the dynamics of the acceleration significantly slowed down.
Acceleration for eight CPU cores (A4 size) reached only 4.12 for jFATCAT and 5.73 for jCE.
When analyzing the results of the scalability tests, we observed that both types of scaling used experienced a slowdown in the acceleration dynamics.
For the horizontal scaling, this was caused by several factors, including concurrent accesses to the VHD while retrieving protein structures processed by par-ticular Searchers, and concurrent accesses to the Storage Table while storing partial results of similarity searches.
For the vertical scaling, additional slowdown was brought about by sharing the same CPU and additional coordination of computations carried out in parallel on the same computing unit.
Results of the per-formed tests showed that horizontal scaling with the use of A1 roles provided better n-fold speedup than vertical scaling.
Moreover, horizontal scaling was easier for deployment and more elastic; the working system could be scaled automatically according to the current needs.
For this reason, the available version of the Cloud4Psi runs on A1 roles and is scaled horizontally.
4 DISCUSSION Although Clou4Psi can be configured to work on various num-bers of Searcher roles, results returned by the system are inde-pendent of the configuration of the cloud platform.
This guarantees reproducibility of results.
Regardless of the number of instances of the Searcher role used at a particular moment, the system returns the same set of results for the given protein struc-ture.
This applies to the number of results returned, values of similarity measures for returned proteins and the content of the result set.
The list of the database proteins returned for the given structure may change only when the repository of macromolecu-lar structures that Searchers operate on is updated.
In case of software components updates, the results of searches performed with the previous software version are still available after provid-ing a proper token number.
Cloud4Psi provides storage of the results for at least 1 month.
5 CONCLUDING REMARKS Cloud4Psi represents an excellent Software as a Service solution, which is still rare in the domain of bioinformatics.
The system-implements the Microsoft Azure role-based and queue-based model, which also provides many operational benefits.
Taking into account that the range of resources provided by the Microsoft Azure cloud is theoretically unlimited, Cloud4Psi is a highly scalable and high-performance solution for protein similarity searching and function identification.
ACKNOWLEDGEMENTS We would like to thank Microsoft Research for providing us with free access to the computational resources of the Windows Azure cloud under the Windows Azure for Research Award program.
Further development of the system will be carried out by the Cloud4Proteins non-profit scientific group (http://zti.polsl.pl/ dmrozek/science/cloud4proteins.htm).
Funding: This work was supported by the European Union from the European Social Fund (grant agreement number: UDA-POKL.04.01.01-00-106/09).
Conflict of Interest: none declared.
Fig.2.
Acceleration (n-fold speedup) of the similarity searching as a function of (a) the number of Searcher roles, (b) the size of the Searcher roles for jCE (solid line) and jFATCAT (dashed line) algorithms 2824 D.Mrozek et al.
employ two more than eight one , virtual hard drive employ , one (SaaS)-
ABSTRACT Motivation: RNA-Seq technique has been demonstrated as a revolu-tionary means for exploring transcriptome because it provides deep coverage and base pair-level resolution.
RNA-Seq quantification is proven to be an efficient alternative to Microarray technique in gene expression study, and it is a critical component in RNA-Seq differential expression analysis.
Most existing RNA-Seq quantification tools require the alignments of fragments to either a genome or a tran-scriptome, entailing a time-consuming and intricate alignment step.
To improve the performance of RNA-Seq quantification, an align-ment-free method, Sailfish, has been recently proposed to quantify transcript abundances using all k-mers in the transcriptome, demon-strating the feasibility of designing an efficient alignment-free method for transcriptome quantification.
Even though Sailfish is substantially faster than alternative alignment-dependent methods such as Cufflinks, using all k-mers in the transcriptome quantification impedes the scalability of the method.
Results: We propose a novel RNA-Seq quantification method, RNA-Skim, which partitions the transcriptome into disjoint transcript clus-ters based on sequence similarity, and introduces the notion of sig-mers, which are a special type of k-mers uniquely associated with each cluster.
We demonstrate that the sig-mer counts within a cluster are sufficient for estimating transcript abundances with ac-curacy comparable with any state-of-the-art method.
This enables RNA-Skim to perform transcript quantification on each cluster inde-pendently, reducing a complex optimization problem into smaller op-timization tasks that can be run in parallel.
As a result, RNA-Skim uses 54% of the k-mers and510% of the CPU time required by Sailfish.
It is able to finish transcriptome quantification in510 min per sample by using just a single thread on a commodity computer, which represents 4100 speedup over the state-of-the-art alignment-based methods, while delivering comparable or higher accuracy.
Availability and implementation: The software is available at http://www.csbio.unc.edu/rs.
Contact: weiwang@cs.ucla.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION RNA-Seq technique has been demonstrated as a revolutionary means for examining transcriptome because it provides incom-parable deep coverage and base pair-level resolution (Ozsolak and Milos, 2010).
Though RNA-Seq sequencing exhibits itself as an efficient alternative to Microarray techniques in gene ex-pression study (Wang et al., 2009), it also brings unprecedented challenges, including (but not limited to) how to rapidly and effectively process the massive data produced by the proliferation of RNA-Seq high-throughput sequencing, how to build statis-tical model for accurate quantification of transcript abundances for transcriptome, etc.
Most of current RNA-Seq tools for RNA-Seq quantification contain two main steps: an alignment step and a quantification step.
Various aligners [TopHat (Trapnell et al., 2009), SpliceMap (Au et al., 2010), MapSplice (Wang et al., 2010)] are devised to infer the origin of each RNA-Seq fragment in the genome.
The alignment step is usually time-consuming, requiring substantial computational resources and demanding hours to align even one individuals RNA-Seq data.
Because there are multiple variations of RNA-Seq sequencing techniques, e.g.
single-end sequencing and paired-end sequencing, to facilitate the discussion in this article, we simply refer to the read or the pair of reads from a RNA-Seq fragment as a fragment.
More importantly, a signifi-cant percentage of the fragments cannot be aligned without am-biguity, which yields a complicated problem in the quantification step: how to assign the ambiguous fragments to compatible tran-scripts and to accurately estimate the transcript abundances.
To tackle the fragment multiple-assignment problem, an expect-ation-maximization (EM) algorithm (Pachter, 2011) is often used to probabilistically resolve the ambiguity of fragment as-signments: at each iteration, it assigns fragments to their com-patible transcripts with a probability proportional to the transcript abundances, and then updates the transcript abun-dances to be the total weights contributed from the assigned fragments, until a convergence is reached.
The EM algorithms simplicity in its formulation and implementation makes it a popular choice in several RNA-Seq quantification methods [Cufflinks (Trapnell et al., 2010), Scripture (Guttman et al., 2010), RSEM (Li and Dewey, 2011), eXpress (Roberts and Pachter, 2013)].
Because all fragments and all transcripts are quantified at the same time in the EM algorithm, it usually re-quires considerable running time.
Some studies [IsoEM (Nicolae et al., 2011), MMSEQ (Turro et al., 2011)] reduced the scale of the problem by collapsing reads if they can be aligned to the same set of transcripts.
It is also worth mentioning that RNA-Seq quantification is an important first step for differential ana-lysis on the transcript abundances among different samples (Trapnell et al., 2012).
The alignment step is a vital step in the RNA-Seq assembly study (Trapnell et al., 2010) and has become the computational bottleneck for RNA-Seq quantification tasks.
If users are only interested in RNA-Seq quantification of an annotated transcrip-tome, aligning RNA-Seq fragments to the genome seems cum-bersome: not only do the RNA-Seq aligners take a long time to align fragments to the genome by exhaustively searching all*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Since employed very ( ) Since ( ) XPath error Undefined namespace prefix possible splice junctions in the fragments, they may also generate misaligned results owing to repetitive regions in the genome or sequencing errors, introducing errors in the quantification results (Zhang et al., 2013).
From another perspective, the annotation databases of tran-scriptome, e.g.
RefSeq (Pruitt et al., 2007) and Ensembl (Flicek et al., 2011), play an increasingly important role in RNA-Seq quantification.
For example, TopHat/Cufflinks supports a mode that allows users to specify the transcriptome by supplying an annotation database (a GTF file).
RSEM (Li and Dewey, 2011) uses bowtie (Langmead et al., 2009)a DNA sequence alignerto align fragments directly to the transcriptome.
Aligning RNA-Seq fragments to transcriptome avoids the com-putation to detect novel splice junctions in fragments and elim-inates the non-transcriptome regions in the genome from further examination, and thus reduces the total running time of the quantification method and the number of erroneous alignments in the results.
To further improve the performance, the utility of k-mers was recently proposed.
The concept of k-mersshort and consecu-tive sequences containing k nucleic acidshas been widely used in bioinformatics, including genome and transcriptome assembly (Fu et al., 2014; Grabherr et al., 2011), error correction in sequence reads (Le et al., 2013), etc.
Because the number of k-mers in the genome or transcriptome is enormous when k is large (e.g.
k 25), the need to store all k-mers impedes their counting.
Most of existing methods save memory usage during the computation by using sophisticated algorithms and advanced data structures [bloom filter (Melsted and Pritchard, 2011), lock-free memory-efficient hash table (Marcais and Kingsford, 2011), suffix array (Kurtz et al., 2008)] or relying on disk space to com-pensate memory space (Rizk et al., 2013).
Thanks to the recent advances in both annotated transcrip-tome and algorithms to rapidly count k-mers, the transcriptome-based alignment-free method, Sailfish (Patro et al., 2013), requires 20 times less running time and generates comparable results with alignment-dependent quantification methods.
Sailfish is a lightweight method: it first builds a unique index of all k-mers that appear at least once in the transcriptome, counts the occurrences of the k-mers in the RNA-Seq fragments and quantifies the transcripts by the number of occurrences of the k-mers through an EM algorithm.
Regardless of being alignment-dependent or alignment-free, all methods need to recover the fragment depththe number of fragments that cover a specific locationacross the whole transcriptome as one of the initial steps.
However, none of the existing methods exploit the strong redundancy of the fragment depth in RNA-Seq data.
More specifically, Fig.1 shows a strong correlation between the fragment depth of any two locations that are a certain distance apart on the transcriptome, varying the distance from 1 to 100bp.
Even when the two locations are 20 bp away from each other, the Pearson correlation score is still as high as 0.985.
In other words, if an RNA-Seq quantifica-tion method that is able to recover the fragment depths for every 20 bp and quantify the abundance levels based on such informa-tion, there should be no significant accuracy loss in the result.
Recently, Uziela and Honkela (2013) developed a method that simply counts the number of alignments that covers the locations of hybridization probes used in the gene expression studies.
Though these probes only represent a sparse sampling on every transcript in the transcriptome, the method still provides reason-ably accurate results.
The observation and the method inspire us to ask the following question: what is the minimum information we need to achieve comparable accuracy in RNA-Seq quantifi-cation to the state-of-the-art methods?
More specifically, does there exist a subset of k-mers that can provide accurate transcrip-tome quantification?
And if so, how do we identify and use them to quantify transcriptome efficiently?
To answer these questions, we introduced a special type of k-mers called sig-mers, which only appear in a (small) subset of transcripts in the transcriptome.
Based on these sig-mers, we developed a method, RNA-Skim, which is much faster than Sailfish and also maintains the same level of accuracy in the results.
RNA-Skim includes two stages, preparation and quanti-fication.
In the preparation stage, RNA-Skim first partitions transcripts into clusters and uses bloom filters to discover all sig-mers for each transcript cluster, from which a small yet in-formative subset of sig-mers is selected to be used in the quanti-fication stage.
In the quantification stage, a rolling hash method (Karp and Rabin, 1987) is developed to rapidly count the occur-rences of the selected sig-mers, and an EM algorithm is used to properly estimate the transcript abundance levels using the sig-mer counts.
Because no sig-mer is shared by two transcript clus-ters, the task can be easily divided into many small quantification problems, which significantly reduces the scale of each EM pro-cess and also makes it trivial to be parallelized.
While RNA-Skim provides similar results to those of alternative methods, it only consumes 10% of the computational resources required by Sailfish.
In this article, we first describe the RNA-Skim method, then discuss how we compared RNA-Skim with other methods, fol-lowed by the experimental results using both simulated and real data.
2 METHOD In this section, we introduced the notion of sig-mers, which is a special type of k-mers that may serve as signatures of a cluster of transcripts, distinguishing them from transcripts in other clusters in the transcriptome that do not contain these k-mers.
Fig.1.
This figure shows the correlations of the fragment depth of any pair of locations as a function of the distance between the two locations from 1 to 100bp.
This figure is generated based on the alignments re-ported by TopHat on a real RNA-Seq data i284 Z.Zhang and W.Wang due Since of ( ) to very , base-pair ase-airs ase-air ase-airs in order , are employed Since  2.1 sig-mer In this article, an annotated transcriptome consists of a set of T tran-scripts: =ft1; :::; tTg.
A transcript t is an RNA sequence composed of a string of four bases A, U, C and G. In this article, we use the corresponding four DNA nucleotide bases A, T, C, G to represent.
The length of a transcript sequence may vary from 100 to 10 000bp.
A partition of a given transcriptome groups all transcripts into P dis-joint non-empty subsets or clusters, denoted by =f1; :::; Pg.
For example, one commonly adopted partition of transcriptome is to group transcripts into genes based on their locations on the genome.
For any transcript t, we use t to denote the cluster to which t belongs.
A substring of length k from a transcript sequence, its reverse se-quence, its complimentary sequence or its reverse and complimentary sequence is called a k-mer of the transcript.
We define a function k-mer() to represent the set of all k-mers from a single transcript or a cluster of transcripts, denoted as k-mer(t) or k-merp, respectively.
For simplicity, if a string s is a k-mer of transcript t, we say s 2 k-mert.
In this case, s 2 k-mert is also true.
DEFINITION.
Given a length k, a transcriptome and its partition , if a k-mer s only exists in one cluster p and never appears in other clusters n p, we call it a sig-mer of cluster p. And for any given cluster p, we denote all of its sig-mers as p. That is, p=fsjs 2 k-merp;8q 2 n p; s 2 k-merqg: Sig-mers characterize the uniqueness of each cluster.
It is obvious that the number of sig-mers heavily depends on the transcriptome partition.
If transcripts with similar sequences are assigned to different clusters, k-mers from these transcripts may not qualify as sig-mers.
Consequently, fewer sig-mers may be identified, and in the worst case, some cluster may not have any sig-mers.
2.2 Workflow of RNA-Skim Because sig-mers are unique to only one cluster of transcripts, if a sig-mer occurs in some RNA-Seq reads, it indicates the sig-mers corresponding transcripts may be expressed.
Therefore, its occurrence in the RNA-Seq data may serve as an accurate and reliable indicator of the abundance levels of these transcripts.
We proposed a method, RNA-Skim, for quan-tifying the transcripts using the sig-mer counts in RNA-Seq data.
Because no sig-mer is shared between transcript clusters, the problem reduces to quantifying transcript abundances using sig-mer counts within each clus-ter, which can be solved much more efficiently and can be easily paralle-lized.
This is different from Sailfish that uses all k-mers in the transcriptome.
In fact, RNA-Skim can be considered as a generalization of Sailfish: if the whole transcriptome is treated as a single cluster that includes all transcripts, all k-mers become sig-mers, and RNA-Skim de-generates to the exact formulation of Sailfish.
The workflow of RNA-Skim includes two stages: preparation and quantification.
In preparation, RNA-Skim clusters the transcripts based on their sequence similarities, finds all sig-mers for each transcript cluster and selects a subset of sig-mers to be used in the quantification stage.
In quantification, RNA-Skim quickly counts the occurrences of sig-mers and quantifies transcripts within each cluster.
The preparation stage of RNA-Skim does not require RNA-Seq read data and thus can be com-puted once as an offline process and be repeatedly used in the quantifi-cation stage.
2.3 Preparation stage In the preparation stage, RNA-Skim only requires users to supply a transcriptome (including all transcript sequences and gene annotations) and specifies a desired sig-mer length to be used in RNA-Skim.
Transcriptome Partitioning A straightforward way to partition tran-scripts is based on their genome locations from an annotation database.
However, the result of this partitioning approach may not be optimal because some transcripts of different genes may have similar sequences or share common subsequences.
To minimize the number of common k-mers shared between clusters, RNA-Skim uses a sequence similarity-based algorithm to generate a partition of transcriptome, instead of using any existing partition.
We first define the k-mer-based similarity, which is used as the sequence similarity in the algorithm.
DEFINITION.
The k-mer-based similarity of two sets of sequences i and j is defined as the higher of the two ratios: the number of common k-mers divided by the total number of k-mers in i, and the number of common k-mers divided by the total number of k-mers in j: k-mer-Similarityi; j= 1 max jk-meri \ k-merjj jk-merij ; jk-meri \ k-merjj jk-merjj !
: 2 Transcripts from the same gene are likely to be similar to each other.
To avoid unnecessary computation, RNA-Skim operates at the level of genes rather than transcripts.
However, calculating the exact similarity between a pair of genes requires generating all k-mers appearing in each gene and taking the intersection of the two sets.
This is computationally expensive.
To expedite the computation, RNA-Skim uses the data struc-turebloom filter (Bloom, 1970)coupled with a sampling-based ap-proach to approximate the similarity between two genes.
The bloom filter is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set, without the need of storing the set explicitly.
A bloom filter includes a vector of bits and several inde-pendent hash functions.
Initially, all bits are set to 0.
When an element is added to the bloom filter, the bits based on the hash values of the element are set to 1.
The bloom filter reports an element is in the bloom filter if its corresponding bits are all set to 1.
A bloom filter may yield a small number of false positives, but no false negatives.
The false-positive rate is bounded if the number of elements in the set is known.
It can be maintained efficiently when new members are added to the set.
RNA-Skim first builds a bloom filter for the set of k-mers of each gene.
Then, it randomly samples two subsets of k-mersnoted as Xi and Xjfrom the pair of genes, and the k-mer-Similarityi; j is approximated by maxjXi \ k-merjjjXij ; jk-meri \ Xjj jXjj (our experiments show that we only need a small number (e.g.
10) of k-mers from each gene to achieve approximation with high accuracy).
After we calculate the approximated similarities for every pair of genes, an undirected graph is built with each node representing a gene.
There is an edge between two nodes if the similarity of the two corresponding genes exceeds a user-specified threshold .
Each connected component of this graph naturally forms a cluster of nodes; each cluster of nodes forms a cluster of genes and transcripts of the genes.
Sig-mers discovery By definition, the sig-mers are essentially the k-mers occurring in only one cluster of transcripts.
A brute force way to find all sig-mers is, for every k-mer in the transcriptome, to determine whether the k-mer that appears in one cluster also appears in some other cluster.
Because the number of possible k-mers is in the order of billions, without any sophisticated data structure and data compression algorithms, stor-ing the k-mers alone will easily take at least tens of gigabytes of memory space, which is way beyond the capacity of any commodity computer.
RNA-Skim again uses bloom filters to reduce memory usage.
Three types of bloom filters are used: a bloom filter BF:ALL for checking whether a given k-mer has been examined, a bloom filter BF:DUP for checking whether a given k-mer appears in more than one cluster and a bloom filter BF:Sp for each cluster p for checking whether a given k-mer is in k-merp.
i285 RNA-Skim paper , ase-airs , Since Since , y In order very es employs 1 tiliz employ or not or not, First, for each cluster p, all distinct k-mers in it are enumerated: RNA-Skim enumerates all k-mers for every transcript in the cluster, and adds them to BF:Sp; if a k-mer is already in BF:Sp, it will be ignored.
Second, every distinct k-mer in p is added into BF:ALL, and if it is already in BF:ALL (that is, it was added when RNA-Skim examined other clusters), it is added into BF:DUP.
Therefore, if a k-mer occurs in multiple clusters, it is added in BF:DUP.
Last, every k-mer of the tran-scriptome is enumerated again, and if the k-mer is not in BF:DUP, it is reported as a sig-mer, as it only occurs in one cluster.
Because bloom filters may have false-positive reports, but never have false-negative reports, through this approach, some genuine sig-mer strings may be missed, but a non-sig-mer will never be labeled as a sig-mer.
Figure 2 shows the pseudocode of our algorithm.
Sig-mers selection RNA-Skim does not use all sig-mers because they are still numerous.
Instead, RNA-Skim selects a subset of sig-mers for the quantification stage.
We used a simple approach to select sig-mers from all sig-mers found by the previous step: for every transcript, sig-mers are evenly chosen based on the sig-mer locations such that any two sig-mers are at least 50 base pair away from each other in the given transcript.
Because some sig-mers may appear in multiple transcripts in the same cluster, for every selected sig-mer, all transcripts are re-examined, and the ones that contain the sig-mer are also recorded.
Through this approach, we can guarantee that every transcript is associated with some sig-mers (as long as there exist some sig-mers).
A good sig-mer coverage is crucial for accurate quantification of transcript abundance.
The final output of the preparation step includes the partition of the transcriptome, selected sig-mers and their associating clusters and transcripts.
2.4 Quantification stage The quantification stage requires users to provide RNA-Seq data (e.g.
FASTQ/FASTA files) and the selected sig-mers associated with tran-scripts containing them from the preparation stage.
Sig-mer counting Because the number of sig-mers used in RNA-Skim is much smaller than the number of k-mers typically used by other k-mer-based approaches, all sig-mers can be stored in a hash table in memory.
The number of occurrences of all sig-mers can be counted by enumerating all k-mers in the RNA-Seq reads and looking up the k-mers in the hash table to update the corresponding counters.
RNA-Skim basically follows this scheme with a tweak on the hash function to further speed up the computation.
In a straightforward implementation of the previously described algo-rithm, every k-mer incurs an O(k) operation to calculate its hash value, and this hash operation can be further reduced to O(1) by the Robin Karp pattern matching algorithm (Karp and Rabin, 1987).
The Robin Karp pattern matching algorithm requires a special hash function rolling hashthat only uses multiplications, additions and subtractions.
In rolling hash, the hash value Hr of the first k-mer in the RNA-Seq read r is calculated by Hr0; :::; k 1=r0 hk1+r1 hk2+:::+rk 1 h0; where h is the base of the hash function, ri is the ith character in s and the character hash function maps a character to an integer value.
One way to calculate the hash value for the (sequentially ordered) second k-mer r1; :::; k is Hr1; :::; k=r1 hk1+r2 hk2+:::+rk h0: But thanks to the structure of the rolling hash function, Hr1; :::; k can be calculated in a much faster way: Hr1; :::; k=Hr0; :::; k 1 r0 hk1 h+rk h0; which only requires one subtraction, three multiplications and one add-ition.
We can look up the hash value in the hash table, and if it is in the hash table, its associated counter is incremented accordingly.
Because RNA-Skim uses this specially designed hash function, we implemented our own hash table in RNA-Skim using open addressing with linear probing.
The base h is arbitrarily set to be a prime number 37, and the function maps every character to its actual ASCII value.
Quantification Because every cluster of transcripts has a unique set of sig-mers, which are the k-mers that never appear in other transcript clus-ters, every cluster can be independently quantified byRNA-Skim, resulting in a set of smaller independent quantification problems, instead of one huge whole transcriptome quantification problem in other approaches.
Formally, if p is a cluster of transcripts, the set of sig-mers of P is denoted by Sp, a sig-mer is denoted by s (s 2 Sp), the set of all occurrences of sig-mers is denoted by Op, an occurrence of a sig-mer in the RNA-Seq dataset is denoted by o (o 2 Op) and the sig-mer of the occurrence is denoted by zo.
From the previous steps, we obtained cs (the number of occurrences of the sig-mer s in the RNA-Seq data), ys;t (binary variables indicating whether the sig-mer s is contained in the sequence of transcript t) and bt (the number of sig-mers that are contained by transcript t).
C is the number of occurrences of all sig-mers (C= X s cs).
Same as in the previous study (Pachter, 2011), we define )=ftgt2p where t is the proportion of all selected sig-mers that are included by the reads from transcript t, and X t=1.
For an occurrence o; po 2 t represents the probability that o is chosen from transcript t, in a genera-tive model, po 2 t=yzo;t t bt 3 Therefore, the likelihood of observing all occurrences of the sig-mers as a function of the parameter ) is L= Y o2Op X t2p po 2 t= Y o2Op X t2p yzo;t t bt 4 = Y s2Sp X t2p ys;t t bt cs : 5 Fig.2.
The pseudocode to find all sig-mers i286 Z.Zhang and W.Wang s since Since-Since , Since--, Since Since , or not , This is in spirit similar to the likelihood function used in other studies, except that this is the likelihood of observing sig-mers rather than frag-ments (Li and Dewey, 2011) or k-mers (Patro et al., 2013).
Thus, we also used an EM algorithm to find that maximizes the likelihood: it alter-nates between allocating the fraction of counts of observed sig-mers to transcripts according to the proportions and updating given the amount of sig-mers assigned to transcripts.
RNA-Skim also applies the same technique used in Patro et al.
(2013), Nicolae et al.
(2011) and Turro et al.
(2011) to collapse sig-mers if they are contained by the same set of transcripts.
(See the Supplementary Material) RNA-Skim reports both Reads Per Kilobase per Million mapped reads (RPKM) and Transcripts Per Million as the relative abundance estimations of the transcripts, and both metrics are calculated by the way used in Sailfish (Patro et al., 2013).
So far, we have described both preparation and quantification stages in RNA-Skim.
In the last, a toy example is provided to illustrate how each stage works in RNA-Skim in Figure 3.
3 SOFTWARE FOR COMPARISON RNA-Skim is implemented in C++ with heavy usage of the open-source libraries bloomd (Dadgar, 2013), protobuf (Google, 2013) and an open-source class StringPiece (Hsieh, 2013).
The parameter settings will be discussed in the Section 5.
We compared RNA-Skim with four different quantification methods: Sailfish (0.6.2), Cufflinks (2.1.1), RSEM (1.2.8) and eXpress (1.5.1) in both simulated and real datasets.
TopHat (2.0.10) and Bowtie (1.0.0) are used as the aligners when needed.
For Sailfish, we set k-mer size to be 31 because this value gives the highest accuracy in the simulation study, among all k-mer sizes supported by Sailfish (k 31).
For other software, we followed the experiments in Patro et al.
(2013) to set the param-eters.
Input to Cufflinks was generated by TopHat, which used Bowtie (bowtie1), allowing up to three mismatches per read (-N 3 and read-edit-dist 3).
Both TopHat and Cufflinks were pro-vided with a reference transcriptome.
RSEM and eXpress dir-ectly used Bowtie to align the reads to the transcriptome, with the argument (-N 3) to allow up to three mismatches per read.
The eXpress was executed in the streaming mode, to save the total quantification running time.
For simulation study, we used the estimations without bias correction for Sailfish, Cufflinks and eXpress.
For real datasets, the estimations with bias correction are used for these three methods.
For RSEM, since it does not provide an option to control the bias correction, we did not differentiate its usage in the simulation and real data studies.
Other parameters were set to default values for these methods.
All methods were run on a shared cluster managed by the Load Sharing Facility (LSF) system.
The running time and CPU time of these methods are measured by LSF.
Each cluster node is equipped with Intel(R) Xeon(R) 12-core 2.93GHz CPU and at least 48GB memory.
All files were served by the Lustre file system.
4 MATERIALS All materials including both simulated and real data are based on the mouse population and consist of paired-end reads with 100bp length per read.
We used C57BL/6J downloaded from Ensembl (Build 70) as the reference genome in all experiments.
All methods studied in this article were provided with 74215 protein-coding annotated transcripts from the Ensembl database.
The simulation Fig.3.
An illustration of how RNA-Skim works on a toy transcriptome of five transcripts i287 RNA-Skim , (TPM) Results , LSF ( ase-airs datasets, including 100 mouse samples with the number of reads varying from 20 millions to 100 millions, were generated by the flux-simulator (Griebel et al., 2012) with its default error model enabled.
For real datasets, we used the RNA-Seq data from 18 inbred samples and 58 F1 samples derived from three inbred mouse strains CAST/EiJ, PWK/PhJ and WSB/EiJ.
The RNA-Seq data was sequenced frommRNA extracted from brain tissues of both sexes and from all six possible crosses (including the reciprocal).
5 RESULTS In this section, we first compared alternative partition algorithms and how they impact sig-mer selections in RNA-Skim and then furnish a comparison with four alternative methods on both simulated and real data.
At last, we demonstrated that RNA-Skim is the fastest method among all considered methods.
5.1 Similarity-based partition algorithm We compared the result of our similarity-based partition algo-rithm with those from two alternative ways to partition tran-scripts: transcript-based partition (every cluster contains a transcript) and gene-based partition (every cluster contains the transcripts from an annotated gene).
The similarity threshold in our partition algorithm was set to be 0.2 (more details are provided later on the parameter choice).
Table 1 compares these partitions on the same transcriptome.
The number of clus-ters generated by our similarity-based partition is 20% fewer than the number of genes.
The average number of transcripts per cluster is20% more than the average number of transcripts per gene.
Most clusters only contain transcripts from a single gene, though the largest cluster contains 6107 transcripts.
These transcripts in the largest cluster share a substantial number of k-mers (e.g.
from paralogous genes), which need to be examined altogether to accurately estimate their abundance levels.
Failing to consider them together (e.g.
by using transcript-based or gene-based partitions) will compromise the number of sig-mers that help distinguish transcripts and hence impair the accuracy of transcriptome quantification.
Even though this clus-ter contains many transcripts, it represents510% of the total number of transcripts.
We used these three types of partitions as the input to the sig-mer discovery method.
To evaluate the goodness of a partition, we measured the proportion of each transcript that is covered by sig-mers and plot the cumulative distribution of all transcripts sorted in ascending order of their sig-mer coverage in Figure 4, with varying k-mer sizes.
For any transcript, the higher the sig-mer coverage is, the more accurate the abundance estimation will be.
Our similarity-based partition is the best: almost all tran-scripts have at least 80% sig-mer coverage, which pushes the curves to the upper left corner of the plot regardless of the k-mer size.
The gene-based partition is slightly worse: 95% of transcripts have at least 80% sig-mer coverage.
The gene-based partition tends to result in low sig-mer coverage for genes sharing similar sequences.
The transcript-based partition is the worst for an obvious reason: transcripts from the same genes may share exons and thus the number of sig-mers that can distinguish a transcript may be very small.
We also observed that using longer k-mer improves the sig-mer coverage.
In the end, RNA-Skim selects 2 586388 sig-mers to be used in the quantification stage, and these sig-mers count for53.5% of 74 651 849 distinguished k-mers used by Sailfish.
Because RNA-Skim uses a much smaller set of sig-mers, it is able to use the rolling hash methoda very fast but memory-inefficient meth-odto count sig-mers in RNA-Seq reads.
5.2 Simulation study Figure 5 compares the performance of the five methods on the simulated data using four metrics: Pearsons correlation coeffi-cient, Spearmans rank correlation coefficient, significant false-positive rate (SFPR) and significant false-negative rate (SFNR).
For brevity, we use Pearson (Truth), Spearman (Truth), SFPR and SFNR to denote these metrics, respectively.
The Pearsons correlation coefficient is calculated in a logarithmic scale, using all transcripts whose true and estimated abundance values are 40.01 RPKM.
This calculation is the same as that used by Sailfish (Patro et al., 2013).
The Spearmans rank correlation is calculated on the set of transcripts whose true abundance values are40.01 RPKM.
The SFPR and SFNR are calculated to assess the estimation distributions on the set of transcripts excluded by Fig.4.
The distribution sig-mer coverages across all transcripts an as-cending order of the sig-mer coverage.
The upper the curve is, the better the corresponding partition is Table 1.
This table compares three different partitions Type Number of clusters Average number of transcripts per cluster Size of the largest cluster Transcript 74 215 1 1 Gene 22 584 3.29 39 RNA-Skim 18269 4.06 6107 Sailfish 1 74 215 74215 Note: If the partition contains only one cluster of all transcripts, RNA-Skim degen-erates to Sailfish.
We thus listed it in the table for comparison.
i288 Z.Zhang and W.Wang , 6 about , , in order , less than about a total of , , less than , , Since larger than larger than significant false positive rate significant false negative rate the previous metrics: if a transcripts estimation is40.1 RPKM, but its true abundance value is50.01 RPKM (a 10-fold suppres-sion), we call it a significant false positive; similarity, if a tran-scripts estimation is50.01 RPKM, but its true abundance value is40.1 RPKM (a 10-fold amplification), we call it a significant false negative.
There are two reasons that we chose SFPR and SFNR instead of the regular false-positive rate and false-negative rate: first, we prefer the transcripts with relatively large abun-dance values (40.1 RPKM) because they are accountable for 99% the RNA-Seq data; second, owing to the noisy nature of RNA-Seq, for the transcripts with small abundance values (50.01 RPKM), it is difficult to calculate accurately, e.g.
both RSEM and Sailfish set the default minimal abundance value to be 0.01 RPKM.
For RNA-Skim, we varied the sig-mer length from 20 to 95 bp.
Other methods are presented as horizontal lines for comparisons.
Despite the small differences by individual metrics, Figure 5 shows that these five methods exhibit comparable performance: no method outperforms the remaining methods across all metrics and the maximal difference by any metric is within 0.05.
Figure 5(a) and 5(b) show two concave curves of Pearson (Truth) and Spearman (Truth) for RNA-Skim by varying its sig-mer length.
There are two factors explaining the concave curves.
First, when the sig-mer length increases, sig-mers become more distinct and the sig-mer coverage increases, which improves the correlations between the truth and estima-tion.
Second, for any fixed read length, when we increased the sig-mer length, the probability that a sig-mer is contained by a single read drops, causing the decrease in the number of sig-mers observed in the RNA-Seq data, which may exacerbate the cor-relations.
In summary, there is a clear trade-off on the sig-mer length.
Empirically, the best sig-mer length is between 55 to 60, and we thus used 60 in other experiments.
For the same reason, in Figure 5(c) and 5(d), we found that the increase in the sig-mer length affects positively on SFPR, but negatively on SFNR.
When the sig-mer length equals 30, it has similar SFPR with Sailfish, but worse SFNR score than Sailfish, indicating that the complete set of k-mers still has its advantage than a small set of sig-mers.
However, RNA-Skim is able to use much longer k-mers that Sailfish does not support, so RNA-Skim using longer k-mers can have a better SFPR than Sailfish.
Other methods also follow the same inverse correlation: while Sailfish and eXpress are the worst in SFPR among these five methods, they are the best two in SFNR.
Figure 6 shows the Pearson (Truth), Spearman (Truth), SFPR and SFNR as a function of the number of sig-mers used in RNA-Skim.
In Figure 6(a), 6(b) and 6(d), when the number of sig-mers increases, the three metrics improve substantially, though at different paces.
Figure 6(c) shows no significant change in SFPR for different numbers of sig-mers.
This obser-vation suggests that we should use as many sig-mers as possible given available memory space.
To ensure RNA-Skim to have similar memory usage to that of other methods, RNA-Skim uses 2.58 million sig-mers.
This is also the default setting in other experiments in this article.
Table 2 shows that the metrics do not vary much when using different similarity thresholds.
In the simulation study, we varied the similarity threshold from 0.06 to 0.28 and observed at most 0.005 change across all metrics.
Owing to limited space, the de-tailed results for the thresholds between 0.06 and 0.28 are omitted.
Figure 7 shows a strong and clear linear correlation between the estimated RPKM scores by RNA-Skim and the true RPKM scores on one simulated sample.
In simulation study, we note that the accuracy of RNA-Skim depends on the sig-mer length and the number of sig-mers, but is insensitive to the threshold .
When these parameters are chosen properly, RNA-Skim produces similar results to those by other methods.
5.3 Study using real RNA-Seq data Because the flux simulator cannot simulate RNA-Seq data with bias effects, and there might also be other unknown factors in the real RNA-Seq data that the simulator fails to capture, we also compared RNA-Skim with other methods on real data.
Because we do not know the ground truth on real data, we computed the Pearson correlation and Spearman correlation between the re-sults produced by RNA-Skim and one other method, referred to as Pearson (methods) and Spearman (methods) to distinguish from the previous computed correlations between RNA-Skim result and the ground truth.
Figure 8 shows that the distributions of the Pearson (methods) and Spearman (methods) are not significantly different between real data and simulated data.
For example, the differences be-tween the mean values of the correlations on both simulated and real data are no more than 0.02 across all methods.
This (a) (b) (c) (d) Fig.5.
(a), (b), (c) and (d) plot Pearson (Truth), Spearman (Truth), SFPR and SFNR of RNA-Skim as a function of sig-mer length, respectively.
For comparison, we also plotted that of the other four methods as the horizontal lines.
The reported values are the average across 100 simulated samples.
The red crosses indicate the sig-mer length (i.e.
60 bp) used in other experiments in this article i289 RNA-Skim larger than less than smaller than larger than larger than due less than very , ase-airs ( ) Fig.5( ) , ( ) ( ) , ( )( )( ) Due Since Since  consistency suggests that the result from RNA-Skim may have similar correlations with the unobserved truth.
The slightly wider distribution of the correlations in real data (than that in simu-lated data) suggests the real data may exhibit more diversity than simulated data.
(For the comparison with gene expression data, please see the Supplementary Material).
5.4 Running time For the preparation stage (including transcriptome partitioning and sig-mer selection), RNA-Skim takes 3h to finish on the mouse transcriptome by using a single thread.
Most time is spent on calculating the k-mer-based similarities between different pairs of genes.
It takes 10min to finish sig-mer discovery and selection.
It is worth noting that these steps only need to be run once for one population beforehand, and after sig-mers are se-lected and their connections with transcripts are established, the result can be repeatedly used on quantifying the transcriptome of many samples.
Therefore, the running time for the preparation stage is less critical than the running time of the quantification stage, and the one-time investment of 3 h is acceptable.
For the quantification stage, we compared both the running time and the CPU time of these five methods on a real sample with 44 millions of paired-end reads.
The running time is the (a) (b) (c) (d) Fig.6.
(a), (b), (c) and (d) plot Pearson (Truth), Spearman (Truth), SFPR and SFNR as a function of the number of sig-mers used in RNA-Skim, respectively.
For comparison, we also showed that of the other four methods as horizontal lines.
The reported values are the average across 100 simulated samples.
The red crosses indicate the number of sig-mers (i.e.
2.58 million sig-mers) used in other experiments in this article (a) (b) (c) (d) Fig.8.
(a), (b), (c) and (d) show the distributions of the Pearson (methods) and Spearman (methods) correlations between the results from RNA-Skim and the results from each of the remaining methods on both simulated and real data Fig.7.
The scatterplot of the estimated RPKM scores by RNA-Skim versus the true RPKM scores.
Both axes are in a logarithmic scale, and all transcripts whose true RPKM or estimated RPKM is 50.01 are omitted Table 2.
This table shows that the four metrics do not change much for different similarity threshold  Pearson Spearman SFP SFN 0.06 0.9438 0.9242 0.0692 0.0233 0.28 0.9440 0.9237 0.0698 0.0235 i290 Z.Zhang and W.Wang about ours about utes ours elapsed time between the start and end of a method, and the CPU time is the total time a method uses on each core of the CPU.
For a single thread method, the running time is exactly the same as the CPU time.
And for a multi-threading method run-ning on a multi-core CPU, the running time is typically shorter than the CPU time.
RNA-Skim is submitted as a single thread method.
Sailfish, Cufflinks with TopHat as the aligner and RSEM with Bowtie as the aligner are submitted with multi-threading enabled and requiring eight threads.
eXpress is an online algorithm, and it can quantify a streaming input of align-ments generated by Bowtie in real time.
Bowtie and eXpress use six and two threads for alignment and quantification, respectively.
Table 3 summarizes the running time of all five methods.
RNA-Skim is the fastest, 11 faster than the second best method, Sailfish, on the CPU time.
Even when Sailfish uses eight threads, RNA-Skim is 1.6 faster on the running time by just using one thread.
Because the aligner usually consumes lots of computation time, RNA-Skim has4100 times speedup on the CPU time compared with Cufflinks, RSEM and eXpress.
Overall, these results demonstrate that RNA-Skim provides comparable accuracy with other methods on both simulated and real data, using a much shorter running time.
6 DISCUSSION AND CONCLUSION We introduced RNA-Skim, a lightweight method that can rap-idly and efficiently estimate the transcript abundance levels in RNA-Seq data.
RNA-Skim exploits the property of sig-mers, significantly reducing the number of k-mers used by the method and the scale of the optimization problem solved by the EM algorithm.
Based on our benchmark, it is at least 10 faster than any alternative methods.
To the best of our know-ledge, the design principle of almost all existing methods is to use as much data as possible for RNA-Seq quantification.
Our re-sults are encouraging, in the sense that they demonstrate a dif-ferent, yet promising, direction of building a much faster method by discovering and using only informative and reliable features the counts of sig-mers in RNA-Seq data.
Currently, the annotation databases are incomplete and still under development.
Aligners and alignment-dependent RNA-Seq methods are commonly used to allow unknown transcript discovery, which will further improve the completeness and ac-curacy of the annotation databases.
The performance of tools like Sailfish and RNA-Skim depends on the quality of the annotation database.
Their accuracy is likely to improve when annotation databases become complete or nearly complete in the future.
They will become better choices when we have a better understanding of transcriptome and transcript discovery task be-comes less important.
Because RNA-Skim is still under development, there are sev-eral directions to further improve its performance.
(i) RNA-Skim uses a simple hash table implementation without any optimiza-tion on the memory usage.
We will investigate advanced data structures enabling better memory utilization.
(ii) Currently, the sig-mer selection algorithm in RNA-Skim only ensures uniform coverage.
In the future, we will explore variable selection tech-niques to select fewer but more informative sig-mers.
(iii) The current version of RNA-Skim does not have built-in bias correc-tion capability, even though it already produces results compar-able with the state-of-the-art methods with bias correction on real data.
We plan to incorporate bias correction in the next version of RNA-Skim, which is likely to improve the perform-ance further.
In addition, we also plan to support multi-thread implementation and deploy RNA-Skim in differential expression analysis.
We are optimistic that, when we add the multi-thread-ing capability to RNA-Skim, the running time will be further improved.
ACKNOWLEDGEMENTS We would like to thank those center members who prepared and processed samples as well as those who commented on and encouraged the development of RNA-Skim, in particular, Leonard McMillan, Vladimir Jojic, William Valdar and Yunjung Kim.
We also would like to thank three anonymous reviewers for their thoughtful comments.
Funding: This work was funded by NIH R01HG006703, NIH P50 GM076468-08 and NSF IIS-1313606.
Conflict of Interest: none declared.
ABSTRACT Motivation: Understanding the patterns of association between polymorphisms at different loci in a population (linkage disequilibrium, LD) is of fundamental importance in various genetic studies.
Many coefficients were proposed for measuring the degree of LD, but they provide only a static view of the current LD structure.
Generative models (GMs) were proposed to go beyond these measures, giving not only a description of the actual LD structure but also a tool to help understanding the process that generated such structure.
GMs based in coalescent theory have been the most appealing because they link LD to evolutionary factors.
Nevertheless, the inference and parameter estimation of such models is still computationally challenging.
Results: We present a more practical method to build GM that describe LD.
The method is based on learning weighted Bayesian network structures from haplotype data, extracting equivalence structure classes and using them to model LD.
The results obtained in public data from the HapMap database showed that the method is a promising tool for modeling LD.
The associations represented by the learned models are correlated with the traditional measure of LD D. The method was able to represent LD blocks found by standard tools.
The granularity of the association blocks and the readability of the models can be controlled in the method.
The results suggest that the causality information gained by our method can be useful to tell about the conservability of the genetic markers and to guide the selection of subset of representative markers.
Availability: The implementation of the method is available upon request by email.
Contact: maciel@sc.usp.br 1 INTRODUCTION The detection of linkage disequilibrium (LD), the non-random association of alleles at different loci in a population, and the assessing of its intensity, extent and distribution is a fundamental step in many genetic studies.
In association studies, for example, the search for the locus (or loci) responsible of a particular trait or disease is narrowed to regions of high LD (LD blocks) where genotyped genetic markers were observed to be associated with the studied phenotype (Mueller, 2004).
In population genetic, LD patterns has been widely used to study the evolutionary and demographic processes in a variety of animal and plant populations, such as admixtures, migration and natural selection (Tishkoff et al., 1996; Zhang et al., 2004).
LD information was also useful to learn more about the architecture of the human genome and its biology of recombination (Pritchard and Przeworski, 2001).
To whom correspondence should be addressed.
A variety of coefficients have been proposed to quantify the intensity of LD (see Mueller, 2004 for a review).
Pairwise LD measures were the first ones reported for this purpose, which measure the overall allele association between two loci.
Popular examples of such measures are D and r2 (Hedrick, 1987).
Subsequently, multi-locus LD coefficients were proposed to measure simultaneous allele associations among multiple loci.
Classical examples are the IA index and the coefficient H (Mueller, 2004; Sabatti and Risch, 2002).
Recently, information theory was used to develop new LD coefficients.
Some examples are: the coefficent , based in entropy (Nothnagel et al., 2002); the normalized mutual information (MIR) coefficient (Zhang et al., 2009); and the normalized relative entropy (ER) coefficient (Liu and Lin, 2005).
This active search of LD coefficients has been accompanied with the development of various tools that display LD measures in a comprehensive way.
Examples of those tools are: GOLDsurfer (Pettersson et al., 2004), Haploview (Barrett et al., 2005) and LdCompare (Hao et al., 2007).
This remarkable interest in developing new measures and tools for studying LD can be explained by the dramatic increase of public genotype data [from the HapMap project (The International HapMap Consortium, 2003), for example], which at the same time enabled association studies in a whole-genome scale that presented new statistical and computational challenges due to the vast quantity of data collected.
Although some of the aforementioned LD measures and tools were useful in characterizing LD at various genomic regions of several populations, they are limited to provide a static view of the LD structure in the studied region.
In an attempt to go beyond these measures some generative models (GMs) were proposed (Hudson, 2001; Li and Stephens, 2003; Maniatis et al., 2002; McVean et al., 2002).
GMs are useful because they model the process that generate the observed data, providing the machinery to do inferences and simulations of yet unobserved situations or to help understanding the underlying generative process.
Models based in coalescent theory (Kingman, 2000) have been the most appealing GMs because they relate LD to evolutionary factors, such as recombination rate, migration and mutation.
However, the parameter estimation and inference in such models is still computationally challenging and only applicable to short regions (Nicolas et al., 2006).
A more practical alternative to model LD is to learn probabilistic graphical models (PGMs) directly from the observed genotype or haplotype data, as proposed by Thomas and Camp (2004).
PGM are a type of GMs that have proven to be useful in modeling a variety of complex real-world problems mainly due to the following reasons: intuitive interpretability of the models, their ability to encode the joint probability distribution with a reduced number of parameters (Heckerman et al., 1995) and the existence of efficient methods to learn PGM from data and to make inferences.
Because the learning The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i633 i632i637 Modeling associations between genetic markers using Bayesian networks of PGM from data is an empirical approach, the learned models are not intended to describe LD in terms of evolutionary factors, but rather to represent the current LD structure in an accurate, compact and understandable way, which is important in genetic association studies.
In the present article, we propose the use of Bayesian networks (BN) to model LD.
BN (Pearl, 1988) are a type of PGM that encode a joint probability distribution via a directed acyclic graph (DAG), where nodes represent random variables and edges represent dependencies between them.
We choose BN because we were interested in learning the causality of the associations, which we found to give additional information about the LD structure.
Our approach is different from the previous works of Thomas and Camp (2004) and Thomas (2005), which propose the use of Markov networks to represent dependences between proximal loci and several runs of simulated annealing algorithm to learn optimal models.
BN models are more naturally interpretable than Markov networks, since the components (factors) of the factorized joint probability distribution are associated to the model nodes instead of cliques.
The learning of the BN structures is based on the K2GA algorithm (Larranaga et al., 1996), which finds the optimal BN structure(s) through the combination of a global search [using a genetic algorithm] GA on the space of topological orderings and a local search (using the K2 greedy search method) on the subspace delimited by each ordering.
The learning method computes the strength of each edge.
The learned BN structures are grouped into equivalence structure classes (set of BN with the same set of dependence/independence relationships), which are represented by a partially DAG (PDAG).
The identification of LD associations and LD blocks are performed in these models.
The method was tested in public haplotype data from the HapMap database in three different segments located in ENCODE regions.
The results showed the plausibility of the method in modeling LD, representing associations and LD blocks consistent with standard tools.
The effects of pruning weak edges on the formation of association blocks are studied.
The correlation of the traditional measure D with the associations represented by the PDAGs is also studied.
2 APPROACH 2.1 Data We consider that we have in hand haplotype data obtained from a given population.
This data are formed by m haplotypes, each one consisting of N marker loci sorted by their physical location in the chromosome.
Let xij denote the allele state of the j-th marker in the haplotype i, which can be one of the allele set (1,2,...,rj), where rj is the number of alleles of marker j.
The haplotypes may be experimentally determined or inferred from genotype data by a phasing algorithm [e.g.
fastPHASE (Scheet and Stephens, 2006); BEAGLE (Browning and Browning, 2007)].
In the context of BN, the j-th marker is modeled by a random variable Xj .
Thus, an haplotype xi= (xi1,...,xiN ) is regarded as a realization of a N-dimensional random variable X= (X1,...,XN ).
2.2 Bayesian networks Bayesian networks (Neapolitan, 2003; Pearl, 1988) are a type of PGM that can represent the conditional dependencies and independencies between a set of random variables X= (X1,...,XN ) via a DAG.
A BN is composed by a model structure S and a set of model parameters .
The model structure is the qualitative part of a BN and is formed by a DAG S= (V ,E), where V is the set of nodes representing the random variables X and E is the set of edges representing the conditional dependencies between the random variables of X.
The model parameters are the quantitative part of a BN that in conjunction with the DAG define completely its joint probability distribution.
The model parameters are formed by N parameter vectors = (1,...,N ) defining the conditional probability distributions of each variable, i.e.
i defines the conditional probability distribution of Xi, P(xi|pai,i), where xi represents the instantiation of the variable Xi and pai represents the instantiation of the parent variables of Xi, Pai.
All BN have the property that each variable is conditionally independent on its non-descendants given its parent variables (Markov property).
For example, the BN {abc} and {abc} represent the same independences (a and c are independent given b) and, therefore, are indistinguishable.
The BN {abc} differs from the previous because it represent diferent independencies (a and c are marginally independent and all other pairs are dependent).
This property allows the computation of the joint probability of any instantiation of X, x, as the product of the local conditional probabilities: P(x)=N i=1P(xi|pai,i).
2.3 Structural learning of BN The induction of BN structures from data is a NP-hard problem (Chickering et al., 2004).
We need a heuristic method to find the structure(s) that best reflect the dependency/independence relations contained in the data.
To this end, we take the idea of the K2GA algorithm (Larranaga et al., 1996) to learn model structures from haplotype data, as described below.
The main idea of K2GA is to find optimal BN structures by searching on the space of topological orderings using a combination of global search (using a GA) and a local search [using the greedy search K2 heuristic (Cooper and Herskovits, 1992)].
A topological ordering (TO) is an ordering of the system variables X such that i,j, if Xj comes before Xi in the ordering, namely (...,Xj,...,Xi,...), then Xi can only have node Xj as a parent node.
A TO, therefore, delimits a subspace of structures.
A GA is used to evolve a population of TOs.
Each TO in the evolving population is evaluated by the K2 heuristic.
Such evaluation consists in finding the best structure within the subspace of structures spanned by the TO.
The data provided to K2 are: the TO, a dataset D containing the observations of the variables X, and an upper bound u on the number of parents a node may have.
For each node Xi, (i=1,...,N), K2 iteratively attempts to add up to u edges from Anc(Xi), the ancestors of Xi according to the TO.
In each iteration one node is selected from Anc(Xi) and added to the set of parent nodes of Xi, Pai.
The selected parent is one whose addition most increases the structure score.
K2 stops to adding parents to the node Xi when u parents were added or when no more ancestors in Anc(Xi) can increase the structure score.
The posterior probability of the structure given the data (Heckerman and Chickering, 1995), known as BDe metric, is used as the structure score, which is computed as follows: P(S|D)=Ni=1g(i,Pai), (1) g(i,Pai)= qi j=1 (ij) (ij+Nij) ri k=1 (ijk+Nijk) (ijk) i633 [11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i634 i632i637 E.Villanueva and C.D.Maciel where ri is the number of states of Xi, qi is the number of different instantiations that the parents Pai can take; Nijk is the number of cases in D where Xi takes its k-th state and its parents Pai take their j-th state; Nij is the sum of Nijk over k; ijk is a prior on Nijk , calculated as/riqi when a uniform prior is considered with equivalent sample size ; and ij is the sum over k of ijk .
When the logarithm of the structure score [Equation (1)] is applied, this reduces to a sum of node scores, log(g(i,Pai)), which only depends on the node Xi and its parents Pai.
This modularization of the structure score is advantageous to K2, which selects the set of parents for each node Xi based in the maximization of the node score instead of the maximization of the whole structure score.
The strength of any edge is measured as the increase on the structure score that the edge provokes when it is added to the structure.
For example, if a new parent node Xj is added to the set of parents of Xi, i.e.
an edge XjXi, the edge strength is computed as log(g(i,PaiXj)) log(g(i,Pai)).
This strength is equivalent to the logarithm of the Bayes factor.
The GA used to evolve TOs starts with an initial population of TOs (called individuals in the context of the GA) generated randomly.
Then, each individual is evaluated with K2, which returns the score of the best structure as the fitness of the individual.
The following operators are then sequentially performed to create the next generation (the evaluated population is referred as the current generation).
(i) Selection: pairs of individuals are selected from the current generation to form a mating pool.
The selection is based on the individuals fitness using the roulette wheel algorithm.
(ii) Crossover: produces two new individuals (offspring) for each pair of the mating pool.
The order-based crossover operator OX2 (Larranaga et al., 1996) is used for this purpose.
This operator is only applied when a random number generated uniformly in the interval [0,1] is less than , the crossover rate parameter, otherwise the resulting offspring is replicated from the mating individuals.
(iii) Mutation: this is applied to each offspring individual resulting of the previous step.
The displacement mutation (DM) operator (Larranaga et al., 1996) is used for this purpose.
Like crossover, this operator is only applied when a random number generated uniformly in the interval [0,1] is less than , the mutation rate parameter, otherwise the offspring individual is left unchanged.
(iv) Scoring: evaluates the resulting offspring individuals after mutation with the K2 heuristic.
(v) Replacement: creates the next generation by replacing the worst individuals of the current generation with the best individuals of the offspring population, provided that the replacing individuals are better than the replaced individuals.
The new generation is set as the current generation and a new loop is started.
The algorithm terminates when a predefined number of generations is reached.
2.4 Obtaining BN structure equivalence classes for modeling LD As pointed by Chickering (2002), it is more appropriate to learn equivalence classes of network structures than single structures.
An equivalence class represents a group of structures that encode the same set of dependence/independence relationships.
We took advantage on the ability of the above learning method in generating several optimal BN structures (structures with the same maximum fitness found along the evolutionary process) to get equivalence classes.
All the optimal BN structures are analyzed and Fig.1.
Example illustrating the construction of a PDAG from two optimal DAGs.
The nodes with the same color are associated with each other forming an association block.
grouped according to their topological connection, i.e.
structures in one group only differ in edge directions.
Each group is represented by a PDAG constructed by superimposing all the DAGs of the group, as shown in Figure 1.
The resulting PDAGs can serve to characterize LD.
Two genetic markers are associated if they are not marginally independent, which imply that the corresponding nodes are connected in a PDAG by a directed path or by a common ancestor node.
In the example of Figure 1 all the pair of nodes with the same color are associated.
Additionally, the pairs (3, 4) and (3, 5) are also associated.
We define an association block in a PDAG as the set of the largest number of consecutive markers (consecutive in terms of its physical location) that are associated with each other.
In Figure 1, two association blocks can be found, identified with different colors.
3 RESULTS The described method was implemented in the C++ programming language and tested in a 64-bit 2-core (2.1 GHz) computer with 4 GB of RAM, running a Linux operating system.
The graph drawing was performed using the Graph Visualization Software (www.graphviz.org).
To test the ability of the proposed approach in characterizing LD, we used public data registered in the HapMap database (The International HapMap Consortium, 2003).
We chose three genomic segment located in ENCODE1 regions of different chromosomes and different populations, as shown in Table 1.
ENCODE regions were chosen due to their high density of genotyped SNPs markers and to the availability of existing studies in such regions (The International HapMap Consortium, 2005).
The segments were selected increasing their size and complexity of the LD patterns (Figs 4, 5 and 6), aiming to test the method in different situations.
The datasets were obtained from the HapMap repository, version III release 2, via the Genome Browser web application (http://hapmap.ncbi.nlm.nih.gov) using the option Download Phased Haplotype Data.
The SNP markers with minor allele frequency (MAF) < 0.03 were discarded, since they have little polymorphism.
The following set of parameters were introduced to the learning method in all tests: population size =20, couples in the mating pool =10, crossover rate =0.95, mutation rate =0.05, number of replacement individuals in each generation =10, number of generations = 300, maximum number of parents a node may have u=#SNPs1.
For clarity and space, we first detail the results for the ENm010_CHB+JPT dataset and then present summarized results for the other datasets.
Figure 2 shows the evolution of the best fitness and average fitness of the BN structures learned from the ENm010_CHB+JPT 1Encyclopedia of DNA Elements (ENCODE Project Consortium, 2004) i634 [11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i635 i632i637 Modeling associations between genetic markers using Bayesian networks Table 1.
Datasets used to test the method Dataset name ENCODE region Chromosome band Genomic segment (kp) HapMap population Number of SNPs (MAF > 0.03) Number of haplotypes ENm010_CHB+JPT ENm010 7p15.2 27 07027 126 CHB+JPT 15 340 ENr131_CEU ENr131 2q37.1 235 065235 122 CEU 31 234 ENr321_YRI ENr321 8q24.11 118 797118 895 YRI 47 230 These datasets were obtained from the HapMap repository (version III, release 2) using the Genome Browser web application (http://hapmap.ncbi.nlm.nih.gov).
Only SNPs with MAF > 0.03 were considered.
Fig.2.
Evolution of the maximum and average fitness of BN structures learned from the ENm010_CHB+JPT dataset.
dataset.
Only the first 20 generations are shown, since the curves remain unchanged in subsequent generations.
As can be observed, the first optimal BN structure is found at the fifth generation.
Six generations later the entire population converged to only optimal BN structures (the average fitness stabilizes at its maximum value).
This can be considered a fast convergence, since it is reached in approximately the first 4% of the total number of generations.
We use a predefined high number of generations as a stopping criterion because we were interested in exploring the topological diversity of optimal BN structures to get equivalence classes (PDAGs).
All the optimal learned DAGs (a total of 5800 DAGs from generations 11 to 300) were analyzed.
Twelve different DAGs were identified in that set, which were grouped into four different PDAGs, each having 31 edges.
One of these PDAGs is shown in Figure 3a.
The other PDAGs are similar to this figure, differing only in the location of three edges.
For each of these PDAGs was determined the set of pairwise marker associations.
No difference was found between these four sets of pairwise associations.
As can be observed, two association blocks were identified in the PDAG, which are consistent with the two LD blocks (Block1 and Block 2) found in the triangular LD plot (Fig.4) by the Haploview tool using the Strong LD Spine block definition (Barrett et al., 2005).
With the aim to knowing how the associations and association blocks are affected with the elimination of weak edges, we perform two experiments in the PDAG of Figure 3a.
In the first experiment, the edges with strengths lower than the first quartile (the lower quartile) were eliminated.
The resulting PDAG is shown in Figure 3b with 23 edges.
There was no alteration either in the set of pairwise associations or in the association blocks (these blocks are also indicated with line segments at the top of the LD plot of Figure 4).
Fig.3.
(a) A PDAG (original) learned from ENm010_CHB+JPT dataset.
PDAGs resulting of pruning edges at the first quartile (b) and at the second quartile (c) of (a).
The node numbers are the codes assigned by the Haploview tool in the segment.
The edge labels are the edge strengths.
i635 [11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i636 i632i637 E.Villanueva and C.D.Maciel Fig.4.
LD plot for the genomic segment of the ENm010_CHB+JPT dataset.
The segmeted lines All, Q1 and Q2 at the top represent the association blocks found in the original PDAG, the PDAG with the first quartile of edges removed and the PDAG with the second quartile of edges removed, respectively.
Table 2.
Structural learning results for the three analyzed datasets Dataset Convergence generation # PDAGs Processing time (seg) ENm010_CHB+JPT 11 4 7 ENr131_CEU 43 4 143 ENr321_YRI 166 3 735 In the second experiment, the edges with strengths lower than the second quartile (the worst half of edges) were eliminated.
The resulting PDAG is shown in Figure 3c.
In this case the number of pairwise associations fell from 63 to 49, but the association blocks remained unchanged.
This preservation of the association blocks with the removal of significant number of edges can be due to the high compaction of the LD blocks.
Asummary of results of executing the method in all datasets can be found in Table 2.
It can be observed that the convergence generation increases with the number of markers, as well as the processing time.
This is an expected result, since the search space is bigger and more complex.
Like the results in the ENm010_CHB+JPT dataset, the four PDAGs obtained from the ENr131_CEU dataset and the three PDAGs obtained from the ENr321_YRI dataset represent the same set of pairwise associations and the same association blocks.
These blocks are illustrated as segmented lines with the name All at the top of Figures 5 and 6, respectively.
As can be noticed, the association blocks represented in the PDAGs considering all edges tend to be generous, grouping large quantity of nodes into few blocks, including in such blocks markers with moderate to low D values.
When the lower quartile of edges is removed, the association blocks are splitted into more compact blocks (line segments Q1).
When the second lower quartile of edges is removed the association blocks (line segments Q2) tend to be more segmented and similar to that found by the Strong LD Spine block definition.
It is interesting to know the relationship between the learned associations represented by the PDAGs and the classical measure of LD D. For this end, Figures 7a, b and c shows the distributions Fig.5.
LD plot for the genomic segment of the ENr131_CEU dataset.
Fig.6.
LD plot for the genomic segment of the ENr321_YRI dataset.
of the associations with respect to 10 equally spaced intervals of D for the three datasets, respectively.
It is possible to observe a clear trend in the three datasets to represent associations with high D values (in the interval [0.91.0]).
When edges are pruned at first quartile, the distributions of the represented associations are almost unchanged.
When the second quartile of edges are removed the association distributions are moderately altered, predominantly in the lower intervals of D. This means that weak edges are important components of weak D associations, and vice versa.
The learned PDAGs showed another interesting information: the markers most central in the blocks tend to have more outgoing edges (e.g.
markers 4 and 24 in all PDAGs of Fig.3) and the markers on the block boundaries tend to have predominantly incoming edges (markers 1, 13, 19 and 27 in all PDAGs of Fig.3).
An explanation for this behavior is that genetic markers near the center of the blocks are highly conserved in the population (had minimal recombination), which is reflected in the learned models by their tendency to be more causative than dependent nodes.
This suggests that the causal information gained by BN models can be useful to tell about the conservability of the genetic markers in the analyzed population.
The learned causal information can also be useful to guide the selection of an informative subset of tags markers, since nodes that have predominantly outgoing edges can explain the allele status of their dependent markers, being good candidates as tag markers.
i636 [11:11 28/8/2010 Bioinformatics-btq392.tex] Page: i637 i632i637 Modeling associations between genetic markers using Bayesian networks Fig.7.
Distributions of the PDAG associations with respect to 10 equally spaced intervals of D for the three analyzed datasets: (a) ENm010_CHB+JPT; (b) ENr131_CEU; (c) ENr321_YRI.
4 CONCLUSIONS In this article a novel application of Bayesian networks to model associations between genetic markers was proposed.
The method is based on learning optimal BN structures (weighted) from haplotype data, extracting equivalence structure classes (PDAGs) and using them to model LD.
The results obtained in public data from the HapMap database showed that our approach is a promising tool for modeling LD.
All the association blocks represented in the learned PDAGs were consistent with LD blocks found by standard tools.
It was shown that by pruning weak edges is possible to control the granularity of the association blocks and the clarity and interpretability of the models.
The associations represented by the PDAGs were shown in correlation with the traditional measure of LD D. It was suggested that the causality information learned by our approach can be useful to infer about the conservability of the genetic markers and to guide the selection of informative tagsmarkers.
Our current developments are in improving the efficiency of the method and parallel implementations for extending greatly the number of markers that can be considered up to whole-genome scales.
Funding: Coordenao de Aperfeioamento de Pessoal de Nvel Superior (CAPES), the Brazilian government agency for the development of human resources.
Conflict of Interest: none declared.
ABSTRACT Motivation: Genome-wide measurement of transcript levels is an ubiquitous tool in biomedical research.
As experimental data continues to be deposited in public databases, it is becoming important to develop search engines that enable the retrieval of relevant studies given a query study.
While retrieval systems based on meta-data already exist, data-driven approaches that retrieve studies based on similarities in the expression data itself have a greater potential of uncovering novel biological insights.
Results: We propose an information retrieval method based on differential expression.
Our method deals with arbitrary experimental designs and performs competitively with alternative approaches, while making the search results interpretable in terms of differential expression patterns.
We show that our model yields meaningful connections between biological conditions from different studies.
Finally, we validate a previously unknown connection between malignant pleural mesothelioma and SIM2s suggested by our method, via real-time polymerase chain reaction in an independent set of mesothelioma samples.
Availability: Supplementary data and source code are available fromContact: samuel.kaski@aalto.fi Supplementary Information: Supplementary data are available at Bioinformatics online.
Received on July 5, 2011; revised on October 10, 2011; accepted on November 13, 2011 To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
Present address: Center for Biomedical Informatics, Harvard Medical School, Boston, MA 02115, USA.
1 INTRODUCTION DNA microarrays are a frequently used high-throughput tool for measuring gene expression, which is reflected in the continuously increasing amount of data available in public repositories such as the Gene Expression Omnibus (GEO) (Barrett et al., 2009) or ArrayExpress (Parkinson et al., 2009).
The thousands of gene expression studies in these repositories make it increasingly challenging to retrieve datasets that are relevant to the user.
At the same time, the availability of these collections gives us the opportunity to develop retrieval methods that take into account the gene expression data from these studies to deliver biologically meaningful results and provide insights into the molecular mechanisms at work in the deposited studies.
There are two possible types of solutions for the task of retrieving relevant studies from databases.
Knowledge-driven approaches are based on the metadata used to describe the deposited gene expression studies.
Various forms of string matching algorithms have been applied to retrieve studies based on a textual query (Zhu et al., 2008).
Advanced solutions incorporate controlled vocabularies or ontologies for semantic query expansion (Malone et al., 2010).
Given high-quality annotations, the likelihood of biological relevance of the results is high, but methods using this paradigm are limited to retrieving studies annotated with a known label.
Moreover, these approaches are fundamentally limited by the fact that the text-based description of a study and its results contains only a fraction of the information in the actual gene expression data.
Data-driven or content-based approaches to information retrieval or meta-analysis (Caldas et al., 2009; Engreitz et al., 2011; Fujibuchi et al., 2007; Hu and Agarwal, 2009; Huang et al., 2010; Hunter et al., 2001; Kapushesky et al., 2009; Kupershmidt et al., 2010; Lamb et al., 2006; Segal et al., 2004) have a high potential for discovering novel and biologically meaningful relationships between the studied tissues, organisms and biological conditions, since similarities between studies are derived from shared expression patterns.
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:39 20/12/2011 Bioinformatics-btr634.tex] Page: 247 246253 Information retrieval of transcriptomics data Fig.1.
Flowchart outlining the key steps of the REx information retrieval framework.
MSigDB is the molecular signature database, NDCGis the normalized discounted cumulative gain measure.
Differential expression is a natural encoding for a study, as it describes the biological variation between the studied conditions.
It is also a very useful basis for data-driven retrieval in heterogeneous collections of gene expression studies, and meta-analysis in general, as it addresses issues such as inter-platform incommensurability.
Data-driven information retrieval or meta-analysis methods typically consist of the following four components: (i) a decomposition of the experimental design of studies into differential expression (pairwise comparison) of genes or gene sets; (ii) a method to measure the significance of differential expression [e.g.
fold-change, t-test or gene set enrichment analysis (GSEA); Subramanian et al., 2005], which serves as a basis for encoding the studies; (iii) a method to extract biological patterns of interest from the encoded studies; and (iv) a relevance measure between studies, conditions or microarrays.
Supplementary Table S1 describes the various existing approaches for each of the components.
Depending on their scope, most existing methods include only a subset of the components.
For instance, the well-known meta-analysis module map method (Segal et al., 2004) does not include an approach for computing the relevance between studies.
Conversely, several information retrieval methods do not make use of any method to extract shared expression patterns (Fujibuchi et al., 2007; Hu and Agarwal, 2009; Hunter et al., 2001; Kupershmidt et al., 2010; Lamb et al., 2006).
Three challenges that are particularly significant in the context of large and highly heterogeneous gene expression repositories but that so far have not been addressed are the decomposition of studies with an arbitrary experimental design, facilitation of the biological interpretation of the retrieval results and the systematic evaluation of the retrieval performance.
For instance, most methods are designed to deal only with studies comparing case versus control.
Other methods are able to handle studies with arbitrary designs, but decompose studies into comparisons in ways that induce study-specific bias and hinder the interpretation of the retrieval results.
As an example, comparing two phenotypes (e.g.
normal versus disease) in a multifactorial study while ignoring additional experimental variables may introduce confounding factors.
In this article, we propose REx (data-driven Retrieval of Experiments), which extends our earlier data-driven information retrieval method (Caldas et al., 2009).
An overview of the key steps of the method is provided in Figure 1.
First, for the decomposition of studies into pairwise comparisons, we introduce an approach that takes into account the fact that a comparison depends not only on the phenotypes being compared, but also on the phenotypes which are held constant in the comparison, i.e.
the context.
In each comparison, the other experimental factors need to have the same values in order to avoid confounding factors.
Unlike in our previous work, this approach is applicable to any type of experimental factor.
The underlying data-driven modeling has also been extended.
Our proposed unsupervised learning model enables the detection of associations between studies and the interpretation of these associations in terms of recurrent patterns of differential expression.
The new model additionally takes into account correlations in the activity of gene expression patterns; moreover, while the earlier method worked purely on the level of gene sets, we now additionally model the activity of the specific genes in the sets to increase accuracy and enable more specific interpretations.
Finally, we propose a novel ontology-based approach for evaluating the retrieval results, to deal with the wide range of biological and medical subject areas spanned by the studies in the repository.
We apply REx to a collection of 1092 studies taken from the ArrayExpress repository, involving three species (human, mouse and rat) and corresponding to a total of 6925 phenotype comparisons (in our previous feasibility study, we applied our method to <800 comparisons derived from human studies).
We show that the inferred differential expression patterns correspond to functionally coherent core intersections of gene sets.
We also demonstrate that the numerical retrieval performance of our method is competitive with existing approaches.
In a series of case studies, we point out that connections between conditions found by our method have been confirmed in independent studies.
These case studies illustrate how conditions can be connected on a molecular level, and provide evidence for the validity of our approach.
In an experimental validation study, we explored a connection found by our method that hints at a potential role of the basic helix loophelix transcription factor Single-minded homolog 2, short isoform (SIM2s) in malignant pleural mesothelioma (MPM), which has not been previously described in the literature.
Using real-time polymerase chain reaction (RT-PCR), we were able to detect significant SIM2s underexpression in an independent set of MPM tumors, indicating that SIM2s may effectively have a role in MPM.
This shows that our data-driven information retrieval approach can indeed be used to obtain novel biological insights from large and heterogeneous collections of transcriptomics data.
247 [17:39 20/12/2011 Bioinformatics-btr634.tex] Page: 248 246253 J.Caldas et al.
2 METHODS 2.1 Data 2.1.1 Gene expression studies Datasets from transcriptomics studies in human (Homo sapiens), mouse (Mus musculus) and rat (Rattus norvegicus) were obtained from the ArrayExpress Archive on 26 October 2009 by selecting all datasets that include a preprocessed expression matrix and sufficiently curated annotation.
The datasets fulfilling these criteria are also included in the ArrayExpress Atlas database (Kapushesky et al., 2009) and the same underlying data were used to construct our collection.
A total of 1092 microarray datasets were retrieved.
Out of these, 479 were from human, 445 were from mouse and 168 were from rat studies.
2.1.2 Gene sets For our analysis, we used the canonical pathway gene set collection (C2.CP) provided by the Molecular Signature Database (Version 2.5) (Subramanian et al., 2005).
This collection contains 639 gene sets that represent pathways from a range of public databases.
2.1.3 Tumor specimens and RT-PCR Tumor tissue specimens were obtained from 10 MPM patients who were diagnosed with mesothelioma tumor at Royal Brompton and Harefield NHS Trust, UK.
Of those, six were epithelial and four were biphasic MPMs.
As a control, we used a microscopically normal scraped pleural tissue lining of the lung of a 39-year-old, previously healthy male patient operated at the Helsinki University Central Hospital for a non-neoplastic intrabronchial inflammatory polyp.
We then measured the expression levels of MMP2, MMP3, MMP14, SNAI1, SNAI2, MYOM2, SIM2l and SIM2s via RT-PCR.
We provide the full details of our experimental procedure in Supplementary Material S1.
2.2 Information retrieval framework 2.2.1 Study decomposition The collected and preprocessed datasets were decomposed into binary comparisons between two conditions, denoted by A and B, to be able to determine differentially expressed genes and gene sets.
We applied the following criteria: (1) All samples for the conditions A and B are annotated with exactly one of two different factor values that belong to the same experimental factor.
(2) If there are additional experimental factors used in the study, the factor values of each of those must be the same for all samples associated with conditions A and B.
These factor values form the context of the comparison.
(3) For each condition there must be at least three samples.
(4) Neutral factors are removed before studies are decomposed into comparisons.
Neutral factors are factors that would not result in meaningful comparisons and have a very large number of associated factor values within a study.
The factors age (without stratification) or individual are examples for such cases.
The full list of neutral factors is shown in Supplementary Material S2.
We extracted all possible comparisons according to these rules, which resulted in a total of 6925 comparisons.
Of those, 1976 are from human studies, 2137 are from mouse studies and 2812 are from rat studies.
The extracted comparisons were further classified into whether they are interpretable or not.
We define a comparison to be interpretable if either A or B can be considered as a control or normal state in the experiment.
Such conditions are, for example, wild-type strains when different genotypes are being compared, a mock treatment when the effects of drugs are analyzed or healthy tissues when cancers are studied.
The assumption is that the effects observed in an interpretable comparison can be attributed to the non-control condition.
In order to identify interpretable comparisons, we assembled a list of control factor values by manually classifying all factor values used in the collection of datasets.
The full list of control factor values is shown Fig.2.
Plate diagram of the proposed graphical model.
Rectangles indicate sets of variables, with the cardinality of the set marked in the bottom right corner.
Gray nodes correspond to observed data.
in Supplementary Material S3.
We were able to classify a total of 908 comparisons as interpretable, with 325 coming from human, 429 coming from mouse and 154 coming from rat studies.
The number of interpretable comparisons is almost nine times higher than in our earlier study, where only 105 interpretable comparisons were used.
Furthermore, in our earlier work we only considered comparisons of disease against some control as interpretable, whereas here we considered interpretable comparisons derived from a wide range of different experimental factors.
2.2.2 Differential expression We use the signal-to-noise ratio as a measure of differential expression of each gene in each comparison.
We then apply GSEA version 2.04 (Subramanian et al., 2005) to test for the overrepresentation of pre-defined gene sets among the most up-or downregulated genes, and collect the 50 gene sets with the highest normalized score, ignoring the direction of differential expression.
Unlike in previous work (Caldas et al., 2009), we also consider the most differentially expressed genes in each gene set, a subset known as the leading edge subset (Subramanian et al., 2005).
We provide additional details in Supplementary Material S4.
2.2.3 Unsupervised learning method We propose a latent variable mixture model for analyzing the GSEA results.
Patterns of gene set and gene differential expression are represented as mixture components and GSEA comparisons are encoded as soft combinations of those components.
The model structure is shown in Figure 2.
We assume there are T mixture components, or submodules, with the t-th submodule consisting of two vectors of Bernoulli distributions, t and t .
The vector t has length equal to the number of gene sets and models the binary activation status of each gene set; the vector t has length equal to the total number of genes in the dataset and models the leading edge subset of each gene set.
The activation status of a gene set j in a given GSEA comparison and the composition of its leading edge subset are assumed to be generated by first picking a submodule t; then, the binary activation status of gene set j is a sample from a Bernoulli distribution parameterized by t,j , while for each gene g in that gene set we generate its leading edge subset membership by sampling from a Bernoulli distribution with parameter t,g.
In order to model correlations between submodules, we incorporate a two-level submodule selection procedure (Li and McCallum, 2006); we assume that each GSEA comparison i has a discrete distribution over the so-called modules, parameterized by a vector i; each module m has a discrete distribution over submodules, parameterized by a vector m. The selection of a submodule t is made by first choosing a module 248 [17:39 20/12/2011 Bioinformatics-btr634.tex] Page: 249 246253 Information retrieval of transcriptomics data m using i and then choosing submodule t using m. The variables u and v in Figure 2 indicate the chosen module and submodule, respectively.
Finally, we endow each i and m with conjugate symmetric Dirichlet prior distributions, and each t,j and t,g with conjugate symmetric Beta prior distributions, parameterized by , , and , respectively.
The conjugate prior distributions are primarily chosen for the purpose of analytical tractability, as it allows us to derive a collapsed Gibbs sampler for inference and estimation, which has been shown to work well in latent variable mixture models (Griffiths and Steyvers, 2004).
For reasonably uninformative priors, such as the ones used in this article, this choice does not markedly decrease generality.
We use a collapsed Gibbs sampler (Griffiths and Steyvers, 2004) to compute approximate posterior distributions for u and v, as well as estimates for , , and given the observed GSEA results and a pre-defined number of modules and submodules.
The relevance of a GSEA comparison r to a query q is computed as the expected probability that the parameters of comparison r generated the data in comparison q.
Using a general probabilistic formulation, this amounts to computing rel(q,r) def=  P(xq|r )P(|X)d , where X is the input data and is the collection of random variables upon which inference is performed (Buntine et al., 2004).
Finally, our model allows computing for each comparison the marginal probability that each gene set is active.
Using the inferred estimates for the model variables, the marginal probability of a gene set being active in a given comparison is given by the following expression: P(gene set s is active|comparison i)= M m=1 T t=1 immtts (1) The full details of our model are described in Supplementary Material S5.
2.3 Performance evaluation In our previous work, the evaluation of retrieval results relied on a manual classification of comparisons into cancer-related and not cancer-related (Caldas et al., 2009).
This was possible because the number of comparisons was fairly small.
For the REx method described here, we developed a scalable approach that employs an ontology-based relevance score to evaluate the performance of the method.
The Experimental Factor Ontology (EFO; Malone et al., 2010) is a representation of the relationships between experimental factor values used in the studies in ArrayExpress and essentially a directed, acyclic graph with a root.
Each experimental factor value corresponds to a path between the root and a downstream node, with more specific terms generally being further away from the root.
For evaluation purposes, and to compare our method to other information retrieval methods, we used the EFO as an external gold standard, based on which the relevance of a retrieved comparison given a query is measured.
This approach is a systematic solution for evaluating retrieval results from a large, heterogeneous collection of studies that contains data on a wide range of subjects, that would otherwise require a large number of experts from different fields to evaluate the results; this expert knowledge is partially encoded in the ontology.
To evaluate retrieval performance with the EFO, we used an expert-curated mapping to associate the experimental factor values that define interpretable comparisons with terms in the EFO (Release 1.7), if possible.
The mapping is also used for the ArrayExpress Atlas and available as a table in the ArrayExpress database.
When the non-control condition of an interpretable comparison can be mapped to the EFO, we call the comparison an evaluable comparison.
A total of 219 evaluable comparisons were identified based on the mapping from the ArrayExpress Atlas, with 137 coming from human studies, 39 coming from mouse studies and 43 coming from rat studies.
To compute the similarity between terms in the EFO and thus between comparisons in our collection, we employed a modified version of the Jaccard coefficient (Manning et al., 2008), which yields a graded relevance score between 0 and 1.
We then applied the normalized cumulative discounted gain (NDCG) measure (Jrvelin and Keklinen, 2002) to evaluate REx based on the modified Jaccard coefficient.
The approach is described in detail in Supplementary Material S6.
2.4 Module and submodule interpretation We used a statistical significance approach to compute a collection of gene sets and genes with a high activation probability for each module and submodule.
Here, we describe the procedure only for submodules; for modules, the only difference is that it is first necessary to compute module-to-gene-set and module-to-gene probabilities by standard marginalization.
For submodule k, we first computed the probability that the submodule activates both gene set s and gene g via the product k,sk,gs,g, where s,g asserts if gene g belongs to gene set s. We then assessed which genes have a significantly high probability of being activated relative to other genes.
This was done by using a one-tailed Wilcoxon rank-sum test, where the samples being compared are all the (gene set, gene) joint probabilities that involve a particular gene versus all other joint probabilities.
An equivalent approach was used for gene sets.
Significance was assessed at the standard q-value threshold of q<0.05.
This allows obtaining for each submodule a list of significantly probable genes and gene sets.
To further bind the two lists, we pruned the list of significant gene sets by removing those which are not overrepresented in the list of significant genes, as assessed by a hypergeometric test with a cut-off of q<0.05.
3 RESULTS AND DISCUSSION 3.1 Case studies We retrieved the top 25 most relevant results for each of the 908 interpretable comparisons in our collection and created HTML-based reports for each of these queries.
The full list of reports is available online at http://www.ebi.ac.uk/fg/research/rex.
Using these reports, we performed a series of case studies in order to obtain a qualitative evaluation of the retrieval performance of REx.
In each case study, we interpreted the retrieval results for one or more query comparisons with the help of the reported most relevant gene sets and the literature.
Due to space constraints, the details of the case studies are described and discussed in Supplementary Material S7S9.
In summary, we were able to use REx to identify links between conditions such as malignant melanoma and cardiomyopathies, or between pancreatic cancer, insulin signaling, diabetes mellitus and inflammation.
REx also identified a set of comparisons from different studies that were all related to the central nervous system.
3.2 RT-PCR experimental validation: SIM2s expression in MPM We queried the database with a comparison of MPM versus normal in human pleura.
The top 25 most relevant comparisons are presented in Supplementary Table S2.
The top two retrieved comparisons come from the same study and test the effect of potassium and thapsigargin in human cerebrovascular smooth muscle cells.
Both potassium and thapsigargin lead to elevated levels of Ca2+, by activating Ca2+ influx channels and depleting intracellular Ca2+ storage, respectively (Pulver-Kaste et al., 2006).
Abnormal levels of Ca2+ can promote tumor cell proliferation and resistance to apoptosis (Feng et al., 2010), which potentially explains the connection to MPM.
A hallmark for epithelial and biphasic MPM is the expression 249 [17:39 20/12/2011 Bioinformatics-btr634.tex] Page: 250 246253 J.Caldas et al.
of the calcium binding protein calretinin, which is used in the identification of the tumors, although it remains unclear what might be its putative role in carcinogenic processes (Henzi et al., 2009).
The third most relevant comparison is an investigation of an RNAi knockdown of SIM2s in a human colon carcinoma cell line at 18 h. SIM2, located on Chromosome 21, encodes a basic helixloophelix transcription factor and has two splicing isoforms, SIM2s (short) and SIM2l (long).
Due to its chromosomal location, SIM2 has been associated with Down syndrome (Trisomy 21).
For instance, overexpression of SIM2 has been shown to induce a partial Down syndrome phenotype in mouse (Chrast et al., 2000).
Due to the fact that individuals with Down syndrome have a higher risk for leukemia but a lower risk for solid tumors than the general population (Hasle et al., 2000), there are genes on Chromosome 21 that are likely candidates for tumor suppressors or oncogenes (Laffin et al., 2008).
SIM2s has been found to be overexpressed in colon and prostate cancer (Aleman et al., 2005; Halvorsen et al., 2007), and underexpressed in breast cancer (Kwak et al., 2007).
The connection found by REx suggests that SIM2s may be differentially expressed in MPM.
To the best of our knowledge, SIM2s has not yet been identified as having a role in MPM.
Interestingly, Sim2 expression was found in the mesothelium of mice during embryonic development, whereas Sim2 mutant mice died within 3 days of birth from breathing failure due to the defects in the structural components surrounding the pleural cavity, such as pleural mesothelium tearing.
After severe dyspnea, disruption of the pleural mesothelium basement membrane was observed in Sim2 mutants (Goshu et al., 2002).
In the MPM study analyzed by our model (Gordon et al., 2005), SIM2s was slightly underexpressed in comparison to a pleural control (fold-change=0.87).
We tested via RT-PCR measurements whether SIM2s underexpression could be observed in an independent set of 10 MPM patients.
This set consisted of six epithelial MPM and four biphasic MPM [both histological subtypes are included in the original MPM study (Gordon et al., 2005) analyzed by our model].
We also quantified the expression of genes known to be closely related to SIM2s, namely its transcriptional targets MYOM2 (Woods et al., 2008), MMP3 (Kwak et al., 2007), MMP2 and SNAI2 (Laffin et al., 2008).
Finally, we also measured the expression of MMP14, which has been recently observed to be differentially expressed in MPM (Crispi et al., 2009), as well as the expression of SNAI1 and SIM2l.
The log-ratio results are presented in Figure 3.
SIM2s was significantly underexpressed (P<0.05) in MPM patients in comparison to a pleural control.
MMP3 and SIM2l expression was detected in all MPM specimens (except MMP3 in one biphasic sample) but not in the pleural control.
While this implied differential expression of those genes in MPM, the lack of expression in the pleural control precluded us from obtaining numerical fold-change values.
Although we did not confirm significant overexpression of MMP14 reported earlier (Crispi et al., 2009), the expression levels of MMP14 were significantly correlated with the expression of MMP2 (r =0.74, P<0.05), in accordance with the fact that MMP14 is required for MMP2 activation (Crispi et al., 2009).
However, we did observe significant overexpression of SNAI2 (P<0.05).
Overexpression of SNAI2 and MMP3 is consistent with their potential role as repressive transcriptional targets of SIM2s (Kwak et al., 2007; Laffin et al., 2008).
Finally, overexpression of MYOM2 is consistent with the fact that it can be activated by both Fig.3.
Bar plots of MPM versus pleura log-ratio gene expression values obtained via RT-PCR.
The height of the bars represent the log-ratio expression of the corresponding genes and error bars indicate SD.
short and long isoforms of SIM2 (Woods et al., 2008); in the analyzed MPM samples, we observed SIM2l overexpression.
The fact that we observed statistically significant SIM2s underexpression in an independent set of MPM patients suggests that SIM2s may be a relevant gene in MPM.
Currently, no known role for SIM2s in MPM has been described.
However, it has been observed that SIM2s RNAi silencing in MCF-7 cells induces an epithelialmesenchymal transition (EMT)-like phenotype and estrogen receptor (ER)-negative tumors in mouse via an MCF-7 xenograft assay (Laffin et al., 2008).
Overexpression of EMT-related genes, including SNAI2, has been recently observed in mixed MPM (Casarsa et al., 2011), which is consistent with our RT-PCR results.
The importance of estrogen signaling in MPM is an open question, although recent studies indicate that ER levels have prognostic value in MPM (Pinton et al., 2009).
The GADD45A gene, which has been observed to be upregulated in the SIM2s depletion study analyzed by our model (Aleman et al., 2005), is a transcriptional target of ER (Paruthiyil et al., 2011).
It is thus tempting to hypothesize that SIM2s expression may be connected to the estrogen signaling network.
An important line of evidence comes directly from REx.
The top three gene sets reported for both the SIM2s and MM studies are metabolism of xenobiotics by cytochrome p450, androgen and estrogen metabolism and arachidonic acid metabolism.
Cytochrome p450 (CYP) enzymes are known to mediate estrogen metabolism (Tsuchiya et al., 2005).
The genes in the xenobiotics and arachidonic acid metabolism gene sets significantly overlap, as per a one-tailed Fishers exact test (P<0.05).
Together, our results and existing work indicate that SIM2s may have a relevant role in MPM, potentially via the EMT network and estrogen signaling.
3.3 Functionally coherent differential expression patterns We computed for every module and submodule a group of top gene sets and genes, as described in Section 2.
We assessed the functional 250 [17:39 20/12/2011 Bioinformatics-btr634.tex] Page: 251 246253 Information retrieval of transcriptomics data profile of each group of genes by testing for the overrepresentation of Gene Ontology (GO) (Ashburner et al., 2000) biological process terms.
Supplementary Figures S1 and S2 display the associations between enriched functional categories as described by gene sets and modules and submodules, respectively.
Modules are enriched on a wide span of biological processes such as apoptosis (e.g.
module 1), metabolism (e.g.
module 31), neoplasia (e.g.
module 38), respiration (e.g.
module 13), toll-like receptor signaling (e.g.
module 28) and transcription (e.g.
module 8).
There is also an overall trend for modules to focus either on metabolic gene sets or disease-related gene sets, although modules from one group typically include gene sets from the other group.
Next, we studied how the modules combine submodules.
Supplementary Figure S3 displays a heatmap of the distribution of submodules within the modules.
It shows that each module is primarily focussed on a small number of submodules, with some of the submodules being effectively used by several modules.
It also shows that while some submodules are predominant in at least one module, other submodules act as module fine-tuners, not being highly probable in any module.
These results demonstrate that our latent variable model is able to extract meaningful patterns of differential co-expression of gene sets and map them to core subsets of the most differentially expressed genes in those gene sets.
3.4 Retrieval performance We evaluated REx quantitatively by using each comparison in turn as a query, and measuring how well related comparisons were retrieved using the NDCG based on the EFO.
This complements the qualitative evaluation through case studies and experimental validation.
To put our quantitative results into context, we also computed the NDCG for other retrieval approaches, namely a Term Frequency Inverse Document Frequency (TF-IDF) model (Manning et al., 2008) with cosine similarity based on a count representation for the GSEA results as described in previous work (Caldas et al., 2009), a Spearmans rank correlation approach based on the fold-change ratios of the expression data, a Pearsons correlation approach using the inferred distributions over modules of the comparisons, our own earlier method (Caldas et al., 2009) and a random baseline.
The box plots of the NDCG results are shown in Figure 4.
For succinctness, we show only the NDCG results of the best-performing combination of modules and submodules; the results for alternative number of modules and submodules are shown in Supplementary Figure S4.
For the random baseline, we computed for each query comparison the median NDCG over 1000 random permutations of all other comparisons.
In order to obtain a rigorous measure of the difference in performance between methods, we ran a two-tailed Wilcoxon signed-rank test over the NDCG values of every pair of methods, correcting for multiple hypothesis testing via a q-value threshold of q<0.05.
The random baseline performs significantly worse than all non-random approaches (q<0.05).
The difference between Pearsons correlation and the remaining non-random approaches is also significant (q<0.05), as is the difference between Spearmans correlation and the remaining approaches (q<0.05).
To confirm whether this difference corresponds to worse or better performance, we repeated the same procedure but Fig.4.
Data-driven retrieval performance, NDCG results.
The box plots summarize the distribution of NDCG results for 219 interpretable query comparisons.
LDA corresponds to our earlier method (Caldas et al., 2009).
this time using a one-tailed Wilcoxon signed-rank test.
Pearsons correlation performed significantly worse than all other non-random approaches, while Spearmans correlation performed significantly better.
We then analyzed the magnitude of the difference in NDCG values between our method and Spearmans correlation.
The NDCG values obtained by REx are on average 99% (SD=0.03) of the NDCG values obtained using the Spearmans correlation approach.
The difference in performance between REx and the best-performing approach seems to be consistent, albeit small.
Although Spearmans rank correlation performs slightly better than REx, all evaluated retrieval methods have essentially a very similar performance according to the gold standard derived from the EFO.
The advantage of REx is that it readily provides key information for the interpretation of the results, as illustrated by our case studies.
The fact that our proposed model-based relevance measure outperforms other approaches that use the GSEA encoding and performs slightly worse than Spearmans correlation with a fold-change encoding, suggests that the proposed relevance measure itself is sensible, but the current GSEA encoding may be suboptimal.
Finally, the retrieval results obtained by our method are robust with regard to the number of modules and submodules.
Changing the number of modules and/or submodules yields retrieval results that are significantly correlated with the reported ones.
We also found that for multiple choices in the number of modules and submodules, the structures inferred by the model yield comparison-to-gene-set probabilities that are significantly correlated with the comparison-to-gene-set probabilities in the final model, which justifies the similarity in the query results.
The details are described in Supplementary Material S10.
4 CONCLUSION We proposed a method, REx, for performing data-driven information retrieval in a heterogeneous, large repository of transcriptomics studies.
By associating studies with shared patterns of differential gene set and gene expression, REx facilitates the analysis of the 251 [17:39 20/12/2011 Bioinformatics-btr634.tex] Page: 252 246253 J.Caldas et al.
retrieval results.
We also proposed an ontology-based approach to evaluate the retrieval results, which will become more precise, as more and more phenotypes are mapped to the ontology.Additionally, we carried out case studies showing that the method yields biologically meaningful results.
In one of the case studies, REx suggested a novel connection between differential expression of SIM2s and MPM, which we validated by observing significant SIM2s underexpression in an independent set of MPM tumor samples.
These results show that REx can be used to drive biological discovery.
Finally, we pointed out that both REx and existing work are particular cases of a general framework that unifies meta-analysis and information retrieval.
As the main aim of information retrieval and meta-analysis methods is to join multiple, heterogeneous datasets in order to obtain robust and novel findings, one particularly important direction of research is how to extend the current framework to alternative or multiple data types.
Generally, REx can be applied to functional genomics data types for which GSEA is a suitable analysis method (Subramanian et al., 2005).
While not demonstrated here, the method is immediately applicable to gene expression data generated with sequencing technologies (RNA-seq) once gene-level measurements have been derived.
The same applies to datasets obtained from quantitative proteomics studies, e.g.
using mass spectrometry, which can also be analyzed with GSEA.
REx can also be applied to collections of metabolite profiling studies, but there are some practical challenges that would have to be addressed, such as the need for an appropriate collection of metabolite sets that is analogous to the gene set collection in the Molecular Signature Database.
Although we did not consider integration of multiple data types (e.g.
Guan et al., 2010), our proposed information retrieval and meta-analysis framework provides a sound basis for that task.
For instance, since our method is primarily based on the activation of gene sets, studies with different data types can readily be merged as long as the same collection of gene sets can be used, i.e.
transcriptomics and proteomics data sets could be integrated and used for retrieval without major changes to the method.
There is a wide spectrum of practical applications for REx.
For instance, implemented in repositories of gene expression data, the method could be used to complement existing knowledge-based approaches for study retrieval.
When considering this scenario, where new studies are frequently added to a repository, the unsupervised learning algorithms employed by REx would benefit from the ability to perform online learning, which is an interesting and relevant area of future research.
With such algorithms in place, the links between studies provided by REx could also serve as navigational aids in exploratory settings to guide users to relevant studies in very large repositories.
Overall, as we have showed in this article, the relatively unexplored paradigm of data-driven information retrieval in transcriptomics data offers the possibility of obtaining novel biological findings based on existing data, and holds the potential to ultimately accelerate biomedical research in areas as diverse as drug repurposing or biomarker development.
ACKNOWLEDGMENTS We thank Pivi Tuominen and Tiina Marjomaa for excellent technical assistance; Ele Holloway for advice on ArrayExpress data curation; and Leo Lahti for helpful comments.
Funding: Finnish Funding Agency for Technology and Innovation (40101/07); Pattern Analysis, Statistical Modeling and Computational Learning Network of Excellence (ICT 216886); Portuguese Science and Technology Foundation (SFRH/BD/35974/2007 to J.C.); Academy of Finland (115372 to E.K.
); Finnish Doctoral Programme in Computational Sciences (doctoral fellowship to A.F.
); Finnish Center of Excellence in Adaptive Informatics Research (J.C., A.F.
and S.K.)
and the European Molecular Biology Laboratory (doctoral fellowship to N.G.).
Conflict of Interest: none declared.
ABSTRACT Motivation: G-quadruplexes are stable four-stranded guanine-rich structures that can form in DNA and RNA.
They are an important component of human telomeres and play a role in the regulation of transcription and translation.
The biological significance of a G-quadruplex is crucially linked with its thermodynamic stability.
Hence the prediction of G-quadruplex stability is of vital interest.
Results: In this article, we present a novel Bayesian prediction framework based on Gaussian process regression to determine the thermodynamic stability of previously unmeasured G-quadruplexes from the sequence information alone.
We benchmark our approach on a large G-quadruplex dataset and compare our method to alternative approaches.
Furthermore, we propose an active learning procedure which can be used to iteratively acquire data in an optimal fashion.
Lastly, we demonstrate the usefulness of our procedure on a genome-wide study of quadruplexes in the human genome.
Availability: A data table with the training sequences is available as supplementary material.
Source code is available online at http://www.inference.phy.cam.ac.uk/os252/projects/quadruplexes Contact: os252@cam.ac.uk; jlh29@cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Understanding biological sequences and predicting the functional elements they determine are widely studied themes in computational biology.
Examples of well-established problems are gene finding and the prediction of protein structure from its amino acid sequence.
Computational methods addressing such challenges helped to gain insights into interesting biological phenomenon.
However, other information encoded in the DNA sequence remains to be explored.
Recently, it has been found that particular G-rich DNA (and RNA) sequences are capable of forming stable four-stranded structures known as G-quadruplexes (Burge et al., 2006; Huppert, 2008; Neidle and Balasubramanian, 2006).
G-quadruplexes have been shown to be relevant in a number of biological processes (Patel et al., 2007).
They are an important component of human telomeres (Oganesian and Bryan, 2007), and play a role in regulation of transcription (Qin and Hurley, 2008; Siddiqui-Jain et al., 2002) as well as translation (Kumari et al., 2007).
Structurally, intramolecular G-quadruplexes consist of a square arrangement of four guanines (a tetrad) in a planar hydrogen bonded form.
At the centre of the tetrads is a monovalent cation, e.g.
K+, that further stabilizes the structure.
The core guanines are linked by three nucleic acid To whom correspondence should be addressed.
N1 N N N O N2 H H H N N N N7 O 6 N H H H N N N N O N H H H N N N N O N H H H K+ Loop 1 Loop 3 Loop 2(a) (b) Fig.1.
(a) Hydrogen bond pattern in a G-tetrad.
A monovalent cation occupies the central position.
(b) Schematic diagram of an intramolecular G-quadruplex, with three G-stacks.
sequences (loops) of varying composition and topology.
Figure 1 shows a schematic picture of a G-quadruplex together with the hydrogen bond pattern.
An obvious challenge is to predict which sequences will form these G-quadruplexes.
A necessary condition for G-quadruplex formation is the presence of core guanines and loop sequences.
These basic requirements can be used to identify putative G-quadruplexes using a simple pattern-based rule, matching sequences of the form d(GNG N1NL  L1 GNG N1NL  L2 GNG N1NL  L3 GNG ), (1) where GNG are the guanine cores that can occur with different numbers of G-stacks, NG =2,3,4.
The symbol N denotes any nucleotide.
The loop sequences (L1, L2, L3) have varying length, where NL =7 is a typical choice for the maximum length.
For very long loops, G-quadruplexes are unlikely to form as their stability decays with the total sequence length (Bugaut and Balasubramanian, 2008; Hazel et al., 2004).
Similar rules have been widely used in the literature (e.g.
Huppert and Balasubramanian, 2005) and demonstrated to work well in practice.
However, they are not exhaustive, for example some structures with much longer loops can be formed (Bourdoncle et al., 2006).
The most important limitation of pattern-based sequence rules is that they do not predict the thermodynamic stability, a key property of the G-quadruplex.
In order for the G-quadruplex to have a biologically meaningful role, it needs to be stable enough to form a structure at body temperature.
Furthermore, it has been speculated that G-quadruplexes that are metastable at body temperature carry the most significant role, as their influence on transcriptional processes can be active or inactive depending on other factors.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i375 i374i382 Predicting the stability of G-quadruplexes This motivates the problem of predicting the G-quadruplex melting temperature as a proxy for stability from its sequence alone.
In contrast to simpler systems such as DNA duplexes (SantaLucia, 1998), sequence differences in G-quadruplexes affect thermo-dynamic stability in a non-linear fashion, hence rendering this prediction task challenging.
The nearest neighbour approaches that have been so successful for predicting duplex stability, such as from (SantaLucia, 1998), are not applicable to folded-back structures such as G-quadruplexes.
It is relatively straightforward to experimentally determine the thermodynamic stability for specific G-quadruplexes using ultraviolet (UV) melting (Mergny et al., 1998).
In a UV melting experiment, the absorbance of a guanine-rich oligonucleotide is recorded as a function of the temperature.
This allows the melting temperature of the G-quadruplex to be deduced.
However, no one has managed to extrapolate generalized energy parameters to each component of the structure.
Instead, empirical rules and intuition have been built up based on small-scale studies with a few dozen G-quadruplex sequences.
Various details have been discovered, establishing the importance, in particular, of the loops that join the core guanines together (Bugaut and Balasubramanian, 2008; Hazel et al., 2004; Lane et al., 2008).
Although it is still in the early days of our understanding of G-quadruplex stability, it is clear that both loop length and loop composition are important.
The stability of G-quadruplexes is also strongly influenced by the surrounding solution providing the monovalent cation that sits inside the structure, typically between the G-tetrad stacks (Fig.1).
For instance, K+ is strongly favoured over Na+ or Li+ and hence leads to more stable structures.
In this work, we propose a computational prediction method for the stability of G-quadruplexes based on Gaussian process (GP) regression.
This includes a special purpose covariance function that allows sequence features potentially affecting the G-quadruplex stability to be flexibly incorporated.
The inference procedure automatically determines the relevance of sequence features and yields predictions with error bars.
Using a heavy-tailed likelihood, our model gains additional robustness with respect to outliers.
The presented framework can also handle experimental data that merely set a maximum or minimum range on the melting temperature rather than an explicit value.
This situation occurs if a structure is found to be stable at all experimentally accessible temperatures.
We demonstrate the accuracy of the prediction method on previously unseen sequences and compare it to alternative methods.
Finally, we consider an active learning procedure and apply the methodology to assess the stability of G-quadruplexes in gene promoters, comparing them to other G-quadruplexes.
2 QUADRUPLEX PREDICTIONS USING GP The prediction of G-quadruplex stability can be cast as a regression problem.
For a given training dataset with observed G-quadruplexes, D={xn,tn}Nn=1, the task is to infer a latent function f :x t, mapping from a G-quadruplex input x to its melting temperature t. The main determinant of G-quadruplex stability is the sequence information.
However, the cation nature and salt concentration also have an effect on the stability of the resulting G-quadruplex.
Our G-quadruplexes were measured at different concentration levels, which must be taken into account when making predictions.
Fig.2.
Bayesian network representation of a GP regression model.
The model relates observed independent input/output pairs {xn,tn}Nn=1.
The thick lines couple the latent function value {fn}, illustrating the smoothness assumptions introduced by the GP prior.
The parameters K and L denote hyperparameters of the kernel and likelihood, respectively.
We assume that inputs x={s,c} consist of the quadruplex sequence s and a vector of log-salt concentrations c. To apply the GP machinery, all we need is a positive definite covariance function defined (kernel) between pairs of G-quadruplex inputs.
Given a training dataset D the posterior distribution over latent function values f is P(f |HGP ,D,K,L)N (f |0,KX,X(K)) N n=1 pL(tn |fn,L), (2) where K and L are hyperparameters of the kernel (K) and the likelihood (L), respectively.
We use X to denote the set of all training inputs, X={x1,...,xN }.
The covariance matrix KX,X(K) is derived from the covariance function k(x,x |K) which specifies how function values at two inputs x,x covary.
The noise model pL(tn |fn,L) relates function values fn and the corresponding noisy observations tn.
For simplicity let us first assume standard Gaussian noise, pL(tn |fn,L)=N (tn |fn, 2) with noise level .
In this case, the predictive distribution for an unseen input x is a Gaussian again (Rasmussen and Williams, 2006), where t N (,v) and =K,X [ KX,X(K)+ 2I ]1 t (3) v =K,K,X(K) [ KX,X(K)+ 2I ]1 KX,X (K).
A Bayesian network representation of this model is shown in Figure 2.
A comprehensive introduction to GPs can be found in Rasmussen and Williams (2006).
Hyperparameters: a GP is a non-parametric model.
The only explicit parameters of the model are hyperparameters L and K, all other parameters can be integrated out and are not represented explicitly.
In a GP model the posterior probability of the hyperparameters is P(K,L|HGP,D)P(t|HGP,X,K,L)P(K,L).
(4) The log of the first term, L(K,L) (marginal likelihood), can again be computed in closed form for a Gaussian-noise model (Rasmussen and Williams, 2006).
Gradient-based optimizers can be used to then determine the most probable setting of the hyperparameters {K,L}=argmax K, L (L( K, L)+logP( K, L)).
(5) i375 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i376 i374i382 O.Stegle et al.
2.1 Covariance function and hyperpriors An important design choice for using a GP is a suitable covariance function.
We use a product of covariance functions to combine kernels evaluated on the sequence s and solution concentrations c k({s,c},{s,c})=kc(c,c)ks(s,s), (6) where ks is the sequence kernel and kc the concentration kernel.
The product expresses the belief that both kernels must assign high similarities for covariation of function values.
The squared exponential concentration kernel decays exponentially with log-concentration difference kc(c,c)=A2c exp ( 1 2 i ( ci ci )2 lic 2 ) , (7) where Ac determines the typical amplitude of deviations from the mean and {ci} are log salt concentrations in mM of Na+, K+, NH+4 and Mg2+, respectively.
These are the four most common stabilizing cations for G-quadruplexes; the nature of the anion does not seem to play a role.
The lengthscale parameters lc determine the significance of the associated concentration parameters where large lengthscales correspond to less relevant parameters and short length scales to more relevant ones.
To make the lengthscale comparable, the individual input dimensions are linearly rescaled such that observed training inputs fall into a set range, here 5 to 5.
The sequence kernel, ks, is a sum of two covariance functions.
The first covariance is designed to specifically incorporate existing beliefs about characteristic sequence features that are likely to determine the stability of the G-quadruplex (Lane et al., 2008).
For flexibility, we consider G-quadruplexes that contain either two, three or four stacked tetrads and hence have the equivalent number of guanines in each run.
From the raw sequence information of a G-quadruplex with the form d(GNG N1NL  L1 GNG N1NL  L2 GNG N1NL  L3 GNG ), (8) a set of features f is extracted: Ltotal total length of the sequence (in bases) NG number of G-tetrad stacks (2, 3 or 4) L1 length of the first loop (from the 5 end, in bases) L2 length of the second loop L3 length of the third loop FA relative frequency of adenine in the sequence FC relative frequency of thymine FT relative frequency of cytosine The loop lengths determine the number of bases between the guanine stacks, N1NL .
The relative frequency of the adenine, thymine and cytosine are calculated as FA = NALtotal , where NA denotes the total number of adenines in the sequence (similarly for thymine and cytosine).
Again, a squared exponential kernel is used to combine these features kf(f,f )=A2f exp 1 2 i ( fi f i )2 lif 2 , (9) where fi denotes the i-th of the eight sequence features.
The parameters have the same interpretation as for the concentration kernel.
As before, input dimensions are rescaled and the lengthscale parameters lf was adjust the relevance of the sequence features.
The second sequence covariance function is ignorant to the biological meaning of the G-quadruplex sequence and merely treats it as character string.
We can construct a spectrum kernel (Leslie et al., 2002), that is sensitive to common k-mers present in two sequences s and s ks(s,s)=A2s k(s)k(s), (10) where k(s) maps the sequence s to a vector of counts with the number of occurances for each k-mer in s. The number of possible k-mers in a nucleotide sequence scales as 4k and hence only small orders k are practical.
In experiment,1 we consider k-mers up to an order of k =4.
Due to this low order of k, this spectrum kernel is local in that it is not sensitive to long common substrings.
In contrast, the feature kernel captures global sequence characteristics and hence both sequence kernels complement each other.
Finally, all three kernels are combined in k({s,c},{s,c})=kc(c,c) [ kf(f,f )+ks(s,s) ] .
(11) The relative weights of the individual kernels are controlled by the amplitude parameters Ac,Af and As.
Hyperpriors: priors on all kernel-and likelihood-hyperparameters {K,L} are Gamma distributed.
The prior on the expected amplitudes of the squared exponential kernels Af and Ac is (2,10) with an expected value of 20.
The amplitude of the string kernel has a prior As (2,0.5).
The prior on the noise level is (2,0.5), which corresponds to an a priori uncertainty of 1C about the measured G-quadruplexes melting temperatures.
The lengthscale parameters of the feature and concentration kernels have a prior of (4,10), which favours long lengthscales (mean 40) encouraging irrelevant features to be switched off.
2.2 Robust likelihood The presentation of the GP model so far makes the simplifying assumption that observation noise is Gaussian.
For our full model, we use a heavy-tailed noise model which acknowledges that a small fraction of the data points can be extremely noisy (outliers) while others are measured with considerably more precision.
The two model (Jaynes and Bretthorst, 2003) reflects this belief, pL(tn |fn,L)=0N (tn |fn, 2)+(10)N (tn |fn, 2inf ).
(12) Here, 0 represents the probability that a datum is a regular observation and (10) is the probability of an outlier observation.
The variance of the outlier component, 2inf , is much larger than for regular observations, 2, which allows the model to effectively discard outlier observations.
When using this likelihood model, the posterior in Equation (2) is no longer computable in closed form.
To overcome this problem, we use Expectation Propagation (EP) (Minka, 2005) for approximate inference.
The goal of EP is to approximate the exact posterior with 1Source code for the mapping from strings to k-mer count vectors is taken from the Shogun toolbox (Sonnenburg et al., 2006).
i376 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i377 i374i382 Predicting the stability of G-quadruplexes a tractable alternative of the form Q(f |D,K,L)N (f |0,KX,X (K)) N n=1 gn ( fn |Cn,n,n ) , (13) where gn ( fn |Cn,n,n ) denote approximate factors.
Following Rasmussen and Williams (2006) we choose unnormalized Gaussians gn ( fn |Cn,n,n )=Cn exp( 1 22n (fn n)2 ) , (14) which results in a GP for the approximate distribution again.
The idea of EP is to iteratively update one approximate factor at a time, leaving all other factors fixed.
This is achieved by minimizing the KullbackLeibler (KL) divergence, a distance measure for distributions (Kullback and Leibler, 1951).
The update for the i-th approximate factor is performed by minimizing KL [ N (f |0,KX,X(K)) n =i gn(fn |Cn,n,n) exact factor  pL(ti |fi,L) N (f |0,KX,X(K)) n =i gn(fn |Cn,n,n)gi ( fi |Ci,i,i )  approximation ] (15) with respect to the i-th factors parameters i,i and Ci.
This is done by matching the moments between the two arguments of the KL divergence which can then be translated back into an update for factor parameters.
There is no convergence guarantee for EP but in practice it is found to converge for the likelihood model we consider (see also Kuss et al., 2005).
The fact that the mixture of Gaussian likelihood is not log-concave represents a problem as it may cause invalid EP updates, leading to a covariance matrix that is not positive definite.
We avoid this problem by damping the updates as suggested by Kuss et al.
(2005) and Seeger (2005).
EP also yields an approximation of the log marginal likelihood which can be used to determine the setting of hyperparameters L(K,L) ln df N (f |0,KX,X(K)) N n=1 gn(fn) = 1 2 N n=1 ( ln2n +lnCn ) 1 2 ln KX,X(K)+ 1 2 tT ( KX,X(K)+ ) t, (16) where =diag({n}Nn=1).
In addition to the noise level (Section 2.1), the robust likelihood includes a parameter inf and the mixing proportion 0.
The parameter 0 is optimized together with the remaining hyperparameters.
The noise level of outliers, inf , is set to 10 4.
After convergence of EP, we obtain a GP as approximate posterior distribution (Equation 13).
Predictions from this model follow analogous to the standard GP (Equation 3).
A comprehensive overview on EP approximations for GP models can be found in Rasmussen and Williams (2006); robust GP regression has been previously applied to biological time series in Stegle et al.
(2008).
2.3 Constrained likelihood In addition to normal observations of sequence/temperature pairs, our G-quadruplex measurements also include a small fraction of sequences where only a bound on the melting temperature was determined.
For example, if a G-quadruplex is so stable that it does not complete its melting transition within the experimentally accessible range (typically 1085C), one can only deduce that the melting temperature is larger than this threshold value.
Such observations can be included using a theta likelihood function.
For instance, for an observed lower bound tn pL(tn |fn,L)	(fn tn), (17) where 	(x)= { 1 x>0 0 x<0 .
These non-Gaussian likelihood terms can be dealt with using an EP approximation similar to the one used in (12), where exact likelihood terms are approximated by Gaussian approximate site functions.
2.4 Active learning In addition to predicting G-quadruplex melting temperatures, it is possible to use the GP framework for experimental design, i.e.
to choose which of a set of candidates to measure.
Suppose that we would like to optimally expand a training dataset D, such that we can make most informative predictions about a test set Dtest.
A naive approach would be to randomly draw a subset of the sequences in Dtest, measure their melting temperatures and use them as additional training data.
Alternatively we can consider active learning, choosing this set using an information criterion as proposed by MacKay (1992), or in the context of GP discussed by Seo et al.
(2000).Apractical objective function is the mean marginal information gain over the set of interest, here Dtest ={xm,tm}Mm=1.
If the predictions are Gaussian, the mean marginal entropy is entirely determined by the predicted variances 2tm SM = 1 2 M m=1 log 2tm .
(18) To decide which sequence to measure and add to the training data, we iterate through all candidate test inputs xm Dtest, choosing the one which minimizes SM .
The mean entropy SM can be efficiently evaluated as predictive uncertainties of a GP, 2tm , only depend on the training inputs (Equation 3) and hence candidate sequences can be scored before knowing their melting temperature (Seo et al., 2000).
Once a measurement has been taken, the new input/target pair {x, t} is added to the training dataset and hyperparameters are optimized again.
3 EXPERIMENTS To evaluate the proposed method, we applied the GP predictor to a meta dataset summarizing major G-quadruplex experiment data available as of today.
In total, this dataset consists of 260 G-quadruplex structures which have been experimentally tested with varying salt concentrations.
All of the considered sequences were of the form described by the pattern in Equation (8).
Hence the covariance function as introduced in Section 2.1 was applicable.
i377 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i378 i374i382 O.Stegle et al.
Fig.3.
Accuracy of GP predictions for a representative 50:50 training/test split (260 total measurements).
(a) True measured melting temperatures (green) and marginal GP predictions with 2 SDs error bars (blue).
(b) Prediction errors .
(c) Z-Scores for the predicted values, | | .
3.1 Predictive performance on observed data To assess the accuracy of the GP method, the model was trained on subsets of all 260 G-quadruplexes.
Subsequently, the trained model was used to predict melting temperatures of G-quadruplexes in the remaining test set, and predictions were compared with the true observed melting temperatures.
This predictive test was repeated for different training/split ratios and multiple random splits.
3.1.1 Mean prediction We first investigated how well we were able to predict real data using our model.
Figure 3a shows marginal GP test predictions versus the true melting temperatures for a representative 50:50 training/test split.
The plot illustrates that the GP has estimated appropriately sized error bars.
A histogram view of the differences of the true melting temperatures and the predictions is shown in Figure 3b.
The results show that most of the experimental data was predicted within a 5C error margin, a reasonable standard of accuracy.
Indeed, across 100 random 50:50 training/test splits, on average 80% of the predictions were within 5C of the experimentally determined values.
We then compared the performance of our model with alternative methods.
This comparison includes the proposed GP model (GP robust), a simpler variant of the model without the robust and constrained likelihood (GP standard), Bayesian linear regression on the sequence features f (Linear regression, Bishop 2006) and a support vector machine (SVM, Fan et al., 2005).
The SVM was applied with the same kernel as used in the GP models.
For the standard GP, linear regression and the SVM, sequences where the data only supplied an upper or lower bound on the melting temperature (i.e.
the sequence was too stable to measure under these conditions) had to be excluded.
In total, this reduced the size of the training dataset from 260 to 256 sequences.
Figure 4a, shows the root mean squared error on the test dataset for different algorithms as a function of the relative test set size.
As expected, the performance of all algorithms decreased with growing test set and therefore shrinking training set sizes.
The GP methods outperformed the SVM, and linear regression.
Our robust GP model performed marginally but consistently better than the standard GP.
3.1.2 Variance prediction As a second criterion, we assessed the mean log probability of the test data under the predictive distribution given by different models.
Bigger predictive probability indicates that a method not only is accurate in estimating the mean but also yields appropriately sized error bars.
For this analysis, the results from the support vector machine had to be excluded as the method does not yield a predicted uncertainty.
The results in Figure 4b mirror the comparison of the root mean squared errors.
However, using this probabilistic performance measure, the robust GP performed significantly better than the standard GP variant.
This suggests that the robust likelihood model helps to ensure appropriate predictive uncertainties.
The quality of these error bars is also supported by Figure 3c, which shows Z-scores of test predictions for a 50:50 training/test split.
The number of data points within a 2 SDs margin is in line with the expected number hence showing that the robust GP model knows what it knows.
This is an important and powerful feature for making useful predictions, and will be relevant in the genome-wide G-quadruplex study in Section 4.
3.2 Determining causal features of the G-quadruplex sequence To understand the mechanisms of G-quadruplex stability it is useful to be able to analyse which sequence features play a role in determining the stability of a G-quadruplex.
Such insights can be gained from observing the optimized hyperparameters of the feature kernel kf.
As the lengthscale parameter l i f indicates the relevance of a particular feature i, this can be regarded as a form of feature selection.
A related approach has been described by Chu et al.
(2005) who used GP for biomarker discovery in microarray experiments.
The string covariance function ks(s,s) explains part of the sequence similarity and thus makes the relevances of the sequence feature kernel difficult to interpret.
Hence the string covariance was excluded for this evaluation.
Figure 5 shows the inverse lengthscale parameters of the sequence kernel optimized on the full G-quadruplex dataset.
The results were averaged over 100 independent optimizations with random starting points.
The results show that the relevance of features varied significantly.
The most important features were the length of the middle sequence (L2), the total loop length (Ltotal) and the number of guanine stacks (NG).
Among the parameters for base composition frequency, the adenine frequency appeared to be most important.
Both observations are in line with previously observed characteristics of G-quadruplexes (Lane et al., 2008).
However, it had been expected that L1 and L3 would also have a large effect.
In this context, it is interesting to note the strong fluctuation of the significances of the outer loop lengths L1 and L3 as indicated by the error bars in Figure 5.
A possible explanation for this effect is that there are dependencies between these parameters such that either one or the other feature is needed to explain G-quadruplex stability.
Obviously, there is an underlying relationship between Ltotal, NG and L1...3.
As a result of this interaction, independent i378 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i379 i374i382 Predicting the stability of G-quadruplexes Fig.4.
Comparative predictive performance of different algorithms evaluated as a function of the relative test-set size (260 total measurements).
(a) Root mean squared error on the test set.
(b) Mean log probability of the test data under the predictive distribution.
Error bars show 1SD estimated from 100 random training/test splits.
Fig.5.
Optimized inverse lengthscale hyperparameters.
The plot shows empirically estimated means and 1 SD error bars estimated from 100 restarts of the optimization procedure.
Larger bars indicate more important parameters.
restarts might then explore different modes of the hyperparameters posterior distribution.
To better understand the posterior over hyperparameters, we employed a Hamiltonian Monte Carlo sampler (e.g.
MacKay, 2003) to draw samples from this distribution.
Figure 6 shows the correlations between hyperparameters of the feature kernel as a Hinton diagram.
The correlation coefficients have been calculated from 500 MCMC samples (500 burn-in).
This figure shows that the relevances of L1 and L3 were indeed anti-correlated.
This observed anti-correlation can be explained by positive correlations between the corresponding features in the training dataset, causing that either L1 or L3 is sufficient to predict the melting temperature.
A strong positive correlation of hyperparameters was observed between the loop length L2 and the number of G-stacks NG.
4 GENOME-WIDE ANALYSIS OF G-QUADRUPLEX CANDIDATES We applied the GP predictor to human genome-wide G-quadruplex candidates downloaded from the quadruplex.org database (Wong Fig.6.
Correlations between inferred hyperparameters illustrated as Hinton diagram.
Correlation coefficients were estimated from 500 Monte Carlo sample.
The size of the squares denote the strength of the correlation, where white squares indicate positive correlation and black squares negative correlation.
et al., 2008).
The database contains candidate structures extracted from sequence information using the pattern-based rule from Equation (8), considering quadruplexes with three or more G-stacks (NG 3).
Using this rule a total of 359 548 G-quadruplex candidates with precisely 3 loops have been identified genome-wide, from a total of 373 k predicted sequences, some of which contain several possible G-quadruplexes, and hence cannot be predicted with the available data.
Following Huppert and Balasubramanian (2007), we also extracted those G-quadruplexes found in the promoters of human genes, looking at the 200 bp upstream of the transcription start site.
Again restricting to 3-loop G-quadruplexes there were 10 987 quadruplexes in human promoter regions.
All computational predictions for these G-quadruplexes were made for a solution containing 100 mM K+, which roughly approximates physiological conditions and has become something of a standard for experimentation.
i379 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i380 i374i382 O.Stegle et al.
Fig.7.
Average predictive uncertainty for promoter G-quadruplexes as a function of the number of additional measurements.
Compared are two random measurement sequences (black) and the active learning strategy (red).
The red and black cross indicate the average predictive uncertainty after physically measuring 10 actively (red) or randomly (black) chosen G-quadruplexes.
4.1 Active learning for promoter G-quadruplexes Given the large number of genomic sequences and the relatively small number of data points, it is necessary to be efficient with data collection, so as to maximize the information derived from each new experiment.
We therefore developed a method of active learning such that we can predict which experimental data (i.e.
melting temperatures of sequences) would be most useful to collect.
As a preliminary case study of the usefulness of active learning, we considered the set of promoter G-quadruplexes and applied the active learning strategy outlined in Section 2.4.
Given the training dataset, we selected the subset of the 10 most informative G-quadruplexes in promoter regions, assessed by the marginal information gain.
The melting temperatures of the corresponding sequences were experimentally determined and added to the training set.
As an alternative, we did the exact same experiment but selected 10 randomly chosen sequences instead.
Again the sequences were experimentally characterized and added to the training set.
In each case, the sequences were prepared at 4 M concentration in a TrisHCl buffer at pH 7.4 with 100 mM KCl.
A Varian Cary 300 spectrophotometer was used to measure the absorbance at 295 nm over repeated slow heating/cooling cycles (Mergny et al., 1998).
Melting temperatures were determined by the derivative method.
Figure 7 shows the average predictive uncertainty for all promoter quadruplexes as a function of the number of additional measurements.
Results for the physical measurements are indicated as red and black crosses.
Lines show the expected uncertainties obtained from the model without conducting any physical measurement.
It is apparent that very few additional measurements can significantly reduce the predictive uncertainty.
This observation can be explained by the sequence homology present in the G-quadruplexes found across the genome (Huppert and Balasubramanian, 2005; Todd et al., 2005).
The active selection performed significant better than the randomly selected sequences.
Active learning allows a feedback cycle to be developed, where after each set of data is added, new learning can be performed to optimize the next data collection, resulting in efficient experimentation.
Fig.8.
Predictive uncertainty for genome-wide G-quadruplex candidates shown in standard deviations in degree celsius.
The average uncertainties resulting after real measurements were higher than the model expectations.
This discrepancy is because the theoretical calculations are approximations based on fixed hyperparameters, whereas for the physical measurements the hyperparameters were re-optimized (Section 2.4).
However, we did clearly observe a substantial reduction in uncertainty using the experimental data.
These results are supportive and encouraging that active learning in the context of G-quadruplex structures is a helpful tool, although clearly more than 10 further data points are required to make a substantial difference to the predictive power of the model.
4.2 Study of genome-wide G-quadruplex candidates We also performed predictions on all 360 k G-quadruplexes genome-wide.
The predictive uncertainty for those G-quadruplexes varied significantly.
Figure 8 shows a histogram of the predictive uncertainty in SD for the entire set of all G-quadruplex sequences.
For 90% of the sequences this uncertainty was <14C.
At a more stringent cut off level, still 63% of the sequences could be determined within 10C and 6% within 5C.
This highlights the need for further data collection and the active learning methodology previously described, as well as highlighting the usefulness of predictive uncertainties.
4.2.1 Quadruplexes in promoters Previous analysis of G-quadruplexes suggests that G-quadruplexes are likely to play a widespread regulatory role, supporting experimental demonstrations.
It has been shown that G-quadruplexes are over-represented inside promoter regions compared to elsewhere in the genome, by about an order of magnitude (Huppert and Balasubramanian, 2007).
However, so far it has not been possible to assess whether these quadruplex structures have different stabilities.
Here, we use the developed GP predictor to investigate whether there are systematic differences of G-quadruplex stability inside and outside of promoter regions.
Figure 9 directly compares the predictive mean melting temperature for G-quadruplex structures inside promoter regions with G-quadruplexes elsewhere in the genome.
For this analysis, we restricted the considered sequences to those that could be predicted with at most a 5C standard deviation error margin yielding a total of 17 006 G-quadruplexes out of which 235 were in promoter regions.
The plots suggest that the statistics of melting temperature might indeed be different for promoter G-quadruplexes.
The significance of the difference i380 [10:02 15/5/2009 Bioinformatics-btp210.tex] Page: i381 i374i382 Predicting the stability of G-quadruplexes Fig.9.
Mean predictions of the melting temperature in 100 mM KCl for genome-wide G-quadruplex candidates with a predicted uncertainty <5C.
(a) Histograms for promoter and non-promoter quadruplexes.
(b) Cumulative distribution functions.
between the two distributions, melting temperatures of promoter G-quadruplexes and non-promoter quadruplexes, was assessed by a KolmogorovSmirnov test.
A two-sided test on the predicted mean temperatures for promoter and non-promoter G-quadruplexes found the difference was significant (P = 4.05105).
This result suggests that G-quadruplexes found in gene promoters are likely to be more stable than those found in the bulk of the genome.
5 DISCUSSION AND CONCLUSION We have here presented a robust and sensitive method for inferring the stability of G-quadruplexes from the sequence information.
Our approach is robust with respect to outliers, allows constraints to be incorporated as observations and automatically determines relevant sequence features.
We have further demonstrated how active learning can be used to perform experimental design to guide the choice which sequences of a set of candidates to measure.
We demonstrated as proof of principle that we can apply this approach to determine features of biologically important G-quadruplexes, selecting as our example G-quadruplexes found in the 200 bp region upstream of known human gene transcription start sites, a region containing much promoter activity.
We have shown previously that G-quadruplexes are concentrated in this region (Huppert and Balasubramanian, 2007), and a number of individual studies have confirmed that these can have transcriptional regulatory ability (Qin and Hurley, 2008).
From the results shown here, we can now conclude that the G-quadruplexes in promoters are likely to be more stable than in the genome as a whole, further supporting the hypothesis that they play an important general role in transcriptional control.
The precise mechanistic details of how G-quadruplexes regulate transcription are not entirely clear, but the current model is that their formation disrupts the binding of the normal transcriptional machinery (Qin and Hurley, 2008).
This approach can be further extended to other regions where G-quadruplexes are found to investigate other functional roles.
Several interesting and fruitful extensions to our proposed method could be considered.
The sizes of currently available G-quadruplex datasets is very limited.
As more data becomes available it would be possible to apply more general sequence kernels characterizing similarity of the loop sequences.
Such an approach might yield novel insights into how the sequence composition influences the stability of G-quadruplex structures.
We are currently in the process of scaling available G-quadruplex data to significantly larger datasets using the active learning approach proposed in this work to efficiently explore the phase space available.
Once the amount of available data goes beyond 1000 examples, it would be helpful to explore sparse approximations to the proposed GP scheme (for instance Snelson and Ghahramani, 2006).
We will also arrange a data store for other researchers to contribute experimental data they have collected.
We plan to have discussions with other researchers to establish a standard for experimental measurements, as well as standards for the quality and style of data provided, which should include measurements of G(37C), H and S as well as the melting temperature.
This would allow us to predict these parameters in addition to the melting temperature alone.
We intend to provide a web-enabled version of these predictions.
Links to these resources, source code and Supplementary Material are available online.2 The field of G-quadruplexes has grown rapidly in recent years, and we anticipate that the ability to predict their thermodynamic properties will be useful to many in the field, and accelerate the rate of discovery of new functional roles for these fascinating structures.
Conflict of Interest: none declared
Abstract Motivation: Local compositionally biased and low complexity regions (LCRs) in amino acid se-quences have initially attracted the interest of researchers due to their implication in generating artifacts in sequence database searches.
There is accumulating evidence of the biological signifi-cance of LCRs both in physiological and in pathological situations.
Nonetheless, LCR-related algo-rithms and tools have not gained wide appreciation across the research community, partly due to the fact that only a handful of user-friendly software is currently freely available.
Results: We developed LCR-eXXXplorer, an extensible online platform attempting to fill this gap.
LCR-eXXXplorer offers tools for displaying LCRs from the UniProt/SwissProt knowledgebase, in combination with other relevant protein features, predicted or experimentally verified.
Moreover, users may perform powerful queries against a custom designed sequence/LCR-centric database.
We anticipate that LCR-eXXXplorer will be a useful starting point in research efforts for the elucida-tion of the structure, function and evolution of proteins with LCRs.
Availability and implementation: LCR-eXXXplorer is freely available at the URL http://repeat.biol.
ucy.ac.cy/lcr-exxxplorer.
Contact: vprobon@ucy.ac.cy Supplementary information: Supplementary data are available at Bioinformatics online.
1 Introduction During the past 30 years, the main focus of research related to re-gions of local compositional extremes (low complexity regions; LCRs) was their identification for the purpose of sequence masking (Altschul et al., 1994; Wootton and Federhen, 1993; Ye et al., 2006) for eliminating spurious hits in database searches (Promponas et al., 2000; Tsoka et al., 1999).
Several studies have been published show-casing the abundance and importance of such regions at the molecu-lar/structural (e.g.
Radivojac et al., 2006; Tamana et al., 2012), functional (e.g.
Andrade et al., 2001; Haerty and Golding, 2010), organismic (e.g.
Miskinyte et al., 2013; Pizzi and Frontali, 2001) and habitat level (e.g.
Nandi et al., 2003).
Despite the apparent biological importance of LCRs theres a distinct lack of tools or ser-vices capable of helping biologists to study them in depth.
Most of the methods capable of detecting LCRs were developed for the sole purpose of masking them and are meant to be used from the com-mand line as part of a sequence analysis or search pipeline.
While some tools, such as SEG (Wootton and Federhen, 1993), CAST (Promponas et al., 2000) or BIAS (Kuznetsov and Hwang, 2006) do offer more advanced reports as an option, their results are mostly meant to be parsed by a computer software and not a biologist.
In this work, we present LCR-eXXXplorer, an online service to search, visualize and share LCRs in protein sequences.
We highlight VC The Author 2015.
Published by Oxford University Press.
2208 This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Bioinformatics, 31(13), 2015, 22082210 doi: 10.1093/bioinformatics/btv115 Advance Access Publication Date: 20 February 2015 Applications Note Altschul etal., 1994; Tsoka etal., 1999; ; Miskinyte etal., 2013 low complexity regionsits unique features that may facilitate research efforts towards understanding the biological roles of proteins with LCRs.
2 Functionality 2.1 General description LCR-eXXXplorer is built upon a customized instance of GBrowse (Stein et al., 2002) modified to properly work with protein se-quences.
It currently contains 545 000 sequences (retrieved from UniProt/SwissProt) annotated with over 16 million LCR-related an-notations.
Along with information about sequence complexity, LCR-eXXXplorer displays external annotations from UniProt, as well as predicted disordered and binding regions by utilizing IUPRED (Dosztanyi et al., 2005) and ANCHOR (Dosztanyi et al., 2009; Meszaros et al., 2009) respectively.
Data are stored in a MySQL database, using a database schema based on the SeqFeature schema internally used by GBrowse (see Supplementary Methods and Supplementary Fig.S1).
2.2 Key functionality A basic keyword-based search functionality (allowing wildcards) is available for retrieving protein sequences with matching UniProtKB Accession(s)/Entry Name(s) or gene name(s).
Moreover, the Advanced Search option (specifically implemented for this process as a custom-made GBrowse plug-in) facilitates more fine-tuned queries.
Using the basic search mode, users are able to retrieve up to 500 entries using simple keyword search (e.g.
with a single UniProt identifier or accession number).
An Advanced Search may be initi-ated by querying a suitable combination of UniProt fields (e.g.
gene or protein name, source organism) or LCR properties (e.g.
type of LCR, percent of masked residues)yet, only the AND Boolean op-erator is currently supported for combining search criteria.
Under this mode, batch search functionality is also available using a list of UniProt accession numbers: this feature enables users to take advan-tage of the powerful UniProt search engine and come up with a list of entries specifying complex search criteria.
Results can be displayed in the browser (with a limit of 15 000 entries) or downloaded in a plain text tab-delimited formatted file providing statistics on the LCR content for further processing (with a limit of 50 000 entries).
Different options of masking protein sequences are provided for each individual sequence from the graphical GBrowse protein details view and sequences are available in FASTA format.
The Downloads section offers LCR-eXXXplorer the option of downloading the complete set of sequences in FASTA formatted files masked for LCRs, the complete set of annotations in GFF3 format or a CSV formatted table with LCR statistics for each sequence in the database.
Users may also search for data in LCR-eXXXplorer using BLASTP (Ye et al., 2006) powered by the user-friendly SequenceServer (Priyam et al., manuscript in preparation).
Three underlying databases (unmasked, SEG or CAST masking with de-fault parameters) are provided, with the masked databases being a unique feature of this service; this configuration is shown to improve database search results (Kirmitzoglou, 2014; Kirmitzoglou et al., in preparation).
Furthermore, users may initiate BLASTP searches against the sequence databases hosted at the NCBI web servers (http://www.ncbi.nlm.nih.gov/) using as input query the currently displayed sequence; several options of applying masking using any combination of amino acid residue types and detection algorithm are available.
The main strength of LCR-eXXXplorersetting it apart from similar servicesis its visualization capabilities.
Displaying LCRs in a protein sequence is more informative when information regard-ing other functional or structural features is also shown (Supplementary Fig.S2).
By taking advantage of the underlying GBrowse capability to display features stored on a remote web ac-cessible server, LCR-eXXXplorer incorporates selected annotations from UniProt into the main browser interface.
UniProt annotations displayed in LCR-eXXXplorer are of two major types: (i) general annotations associated with the protein sequence (e.g.
protein name, gene ontology terms, PDB accession IDs) and (ii) position-specific annotations, which may include domains, sites, secondary structure etc.
These annotations are fetched from UniProt/SwissProt on-the-fly for the protein sequence of interest.
This is facilitated by a cus-tom-designed cgi-bin script and the retrieved features are further post-processed to a format suitable for the LCR-eXXXplorer.
Using the same underlying mechanism, LCR-eXXXplorer can display tracks generated by another instance of GBrowse, a Distributed Annotation System (DAS) server or valid GFF3 files gen-erated by the user.
The only requirement is that the remote tracks must use the same coordinates system, which in the case of LCR-eXXXplorer is the protein sequence itself.
Thus, users may practically display results from any LCR-detection tool (or any other protein sequence analysis tool) alongside the data provided by LCR-eXXXplorer.
2.3 Comparison to similar services Two services for providing access to protein sequence LCR-related data are currently available online.
The one most closely related to LCR-eXXXplorer is LPS-annotate (Harbi et al., 2011), which iden-tifies LCRs based on the LPS algorithm (Harrison and Gerstein, 2003), compared to SEG.
These LCR annotations are accompanied with disordered region predictions by DISOPRED (Buchan et al., 2010).
Even though LPS-annotate is an invaluable resource for re-searchers interested in compositionally biased proteins, its main drawback is the lack of any effective visualization options.
Moreover, the underlying database (according to data available at the LPS-annotate website) has not been updated since 2009.
Recently, the HRaP server (Lobanov et al., 2014) was developed, specializing in the study of homopolymeric repeats, which comprise a highly specialized case of LCRs, thus it is not further discussed herein.
A detailed presentation of web-based services pro-viding information related to LCRs is presented in Kirmitzoglou (2014).
3 Future Developments The current version of the LCR-eXXXplorer web server offers sev-eral tools for facilitating research on proteins with LCRs, including BLAST search and interactive visualization by exploiting inherent GBrowse features.
Given the genuine interest of our research group in LCR-containing proteins, we plan to expand this service in the near future.
More specifically, we are in the process of automating the LCR-eXXXplorer update procedure to regularly synchronize with UniProt updates.
Moreover, the customizations performed on differ-ent GBrowse modules require some additional work (and appropri-ate documentation) for enabling full programmatic access to our service through the REST interface already available for GBrowse.
LCR-eXXXplorer 2209 545000 ,) &ndash; 15000 50000 `` '' &ndash; &ndash; low complexity regions d An important improvement destined for the next version of LCR-eXXXplorer is enabling full support of Boolean queries against fields in the underlying database.
The modular (both in terms of data and software) architecture of LCR-eXXXplorer enables easy incorporation of novel datasets (e.g.
complete genome sequences) and LCR detection tools in future versions.
Funding The Cyprus Research Promotion Foundation is the local agency that funded the projects (with money provided from the Republic of Cyprus and the EU European Regional Development Fund (ERDF) (The CyTera project, NEA !PODOMH/RTPATH/0308/31 and PENEK/ENIRX/0308/77).
Conflict of interest: none declared.
ABSTRACT Summary: We present a novel public health database (GENI-DB) in which news events on the topic of over 176 infectious diseases and chemicals affecting human and animal health are compiled from surveillance of the global online news media in 10 languages.
News event frequency data were gathered systematically through the BioCaster public health surveillance system from July 2009 to the present and is available to download by the research community for purposes of analyzing trends in the global burden of infectious diseases.
Database search can be conducted by year, country, disease and language.
Availability: The GENI-DB is freely available via a web portal atContact: collier@nii.ac.jp Supplementary information: Supplementary data are available at Bioinformatics online.
Received on October 5, 2011; revised and accepted on February 23, 2012 1 INTRODUCTION Systems which gather information about disease outbreak events from informal digital sources such as news media are now seen as having high value to national and transnational public health agencies (Heymann and Rodier, 2001).
Although agencies in wealthy countries have a sophisticated array of indicator sources such as over-the-counter sales or sentinel networks, not all countries possess the resources to implement or maintain such systems.
With concerns about newly emerging diseases such as A(H5N1), there has been increasing attention on epidemic intelligence (EI) systems that can complement indicator networks by detecting events on a global scale so that they can be acted on close to source.
While there are several surveillance systems offering alerts and news browsing (Hartley et al., 2010), to the best of our knowledge there are only a few databases where researchers, government health officials, physicians and public health practitioners can look for historical event data which are updated in real time.
ProMED (Madoff and Woodall, 2005), a human network organized through the International Society for Disease Surveillance, is an excellent source with its wide-coverage open access database of human disease reports from 1995 onwards.
Reports are gathered manually and reviewed by experts in a staged process before being sent out via email and stored in an online database.
ProMED reports on both human and animal health events although its coverage of animal health is more limited in scope.
The World Health Organization To whom correspondence should be addressed.
(WHO) offers a more specialized service through its weekly EuroFlu bulletins which are archived online as well as a news bulletin service through its Global Alert and Response site.
Additionally, the World Animal Health Information Database (WAHID) provides access to reports of exceptional events submitted by member states of the OIE (World Organisation of Animal Health).
GENI-DB is a complementary service that provides additional support to those interested in understanding the context of ongoing disease outbreaks as well as analyzing the global burden of infectious diseases.
We developed the GENI-DB database as a free, structured and searchable source on news event statistics reported in the global media.
Information gathering has been done fully automatically without human intervention by the BioCaster EI system (Collier et al., 2008) from thousands of sources in 10 languages.
Our experiments have shown that aggregated event counts from news can provide valuable early warning alerts that in some cases are more timely than ProMED (Collier, 2010).
An additional advantage is that since BioCaster is a single system with a common reporting standard, this allows users to obtain comparative estimates of disease outbreaks across disease conditions and geographic areas.
2 METHODS The GENI-DB database and web server is implemented on a 24 2.66 GHz Xeon core server running on Ubuntu Linux version 9.04, Apache (version 2.2.11), PHP (version 5.2.9) and MySQL (version 14.14) and is viewable in all major web browsers and operating systems, e.g.
Safari, IE, Firefox and Chrome on Linux/Windows/Apple OS.
The database is freely available for users to view and download data 24/7.
Updates to the database take place once every hour during normal operation but this can be shortened to 20 min as required during public health emergencies.
The BioCaster system comprises a modularized text mining pipeline running on a dedicated cluster linked to the backend of the web server.
The modules consist of efficient natural language processing algorithms for web scraping, language detection, machine translation (Koehn et al., 2007), classifying documents into relevant or non-relevant (Bow toolkit: www.cs.cmu.edu/mccallum/bow) as well as dedicated modules for identifying terms and their relationships (Simple Rule Language editor: http://code.google.com/p/srl-editor).
These modules are implemented in various programming languages and glued together using Perl scripts.
Various modules are integrated with a sophisticated knowledge model of the domain defining semantic categories for diseases, species, symptoms, agents etc.
and the relationships between them.
These relationships are assembled automatically into an event report comprising a slot filler template with a minimum fill of a country, province, disease, species and time element (Collier et al., 2008).
One event report is generated for each relevant news event.
Reports cover 176 infectious diseases including under-specified types such as Unclassified disease.
At various points in the pipeline staged filtering heuristics are applied to ensure a minimum level of quality.
For example, events with no identifiable province are entered into the database The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
collier@nii.ac.jp Copyedited by: TRJ MANUSCRIPT CATEGORY: APPLICATIONS NOTE [14:28 29/3/2012 Bioinformatics-bts099.tex] Page: 1187 11861188 GENI-DB but considered to be of lower quality and therefore not shown in the output of GENI-DB.
Currently BioCaster surveillances 27 000 news items per day from Google News as well as various public and NPO sources such as the ProMed-mail, Hong Kong SAR Communicable Disease Watch list, the OIE alert lists, the European Media Monitor alerts and AlertNet.
One important change that has occurred in the system was in December 2011 when the freely available Google Translate service was deprecated.
Up to this point, BioCaster used this service to translate articles to English in order to assess topical relevance.
We have endeavored to work around this by implementing the freely available MOSES machine translation system and have currently trained translation engines for Arabic, Russian, French, Portuguese and Spanish to English.
Chinese, Dutch, German, Italian, Korean and Vietnamese to English are expected to be ready by April 2012.
However we have not been able to recover Thai to English due to lack of parallel texts needed to train the system.
It is still too early to assess the quality impact on performance but we hope to report on this in future publications.
Domain modeling is encapsulated through the BioCaster ontology, a freely available public health applications ontology designed to integrate laymens language of disease reporting across 12 languages (http://code.google.com/ p/biocaster-ontology).
3 RESULTS GENI-DB is a useful source for exploring media reporting patterns as well as following disease outbreaks.
Figure 1 illustrates how aggregated multilingual reporting can be used to visualise media coverage and timeliness in different languages for the porcine foot-and-mouth epidemic in South Korea during 20102011.
Ranking diseases by the number of reports (Table 1) and countries by the number of outbreak events detected per unit of population (Tables S2 and S3 in Supplementary Material) gives an indication of both the incidence of disease but also the characteristics of online media reports.
Ranking countries according to the language of the report also yields some interesting trends in media focus, supporting our view that multilingual reports are necessary to maximize sensitivity.
For example, Haiti features in the top three reported countries between July 2009 and July 2011 for most languages except for Chinese where it appeared ranked at seven behind Japan, Taiwan, USA and France.
In French both Angola and Canada were more widely reported than USA.
A recent quantitative study by Lyon et al.
(2011) provides further insights into the volume, geographic coverage, timeliness and sources of BioCasters information and a comparison against two other systems: HealthMap and EpiSpider.
Within the GENI-DB database we have non-zero event counts for 170 states.
Several states have unexpectedly low counts either because there were very few open source reports during the two year period (e.g.
for sub-Saharan Africa or central Asia) or because of technical limitations in the system such as missing languages (e.g.
Polish), out of vocabulary names for provinces (e.g.
Egypt), failure to normalise diacritics to Roman and non-registration of small island states.
These issues are now being addressed by adding automated detection for alternative Arabic Romanizations, extending our place names ontology to include all world states and provinces as well as a greater number of minor cities (populations under 100 000).
Additionally, we are constantly looking at how we can improve place name disambiguation from evidence in the text which is one of the greatest technological challenges we face.
Several caveats need to be kept in mind when interpreting the data.
Perhaps the most important is that the data have been sourced and Fig.1.
Porcine foot-and-mouth outbreak in South Korea 20102011.
Daily news event counts are shown for several languages as denoted by the ISO 639-1 codes.
Event frequencies are given in stacked bar graphs and on the left-hand axis.
The line graph and right-hand axis show cumulative event frequencies.
Table 1.
Disease event frequency by species in GENI-DB between 15th July 2009 and 28th July 2011 Rank Human disease Reports Animal disease Reports 1 Unclassified influenza 20 982 Unclassified influenza 7519 2 Cholera 19 936 Foot-and-mouth 3827 3 Influenza A(H1N1) 17 759 Influenza A(H5N1) 2202 4 Dengue fever 14 064 Influenza A(H1N1) 1351 5 Measles 6378 West Nile fever 815 6 E-coli 2557 Anthrax 658 7 Anthrax 2123 Rabies 595 8 Influenza A(H5N1) 1946 Herpes 583 9 HFMD 1788 Brucellosis 568 10 Malaria 1716 Eastern equine encephalitis 512 analyzed automatically, i.e.
no human moderation has taken place.
In this respect, the events reported in the database are as is.
De-duplication has not been attempted, except to exclude articles with the same URL, since the frequencies of reports may have something useful to say about the degree of concern felt about an event.
4 CONCLUSIONS The goal of GENI-DB is to offer a complementary service to extant databases helping provide insights and overcome information overload on experts.
The database provides opportunities for comparisons against other sources as well as material for generating synthetic datasets.
We hope that by making GENI-DB available, the data can aid in analysis of global trends, progress the state of the art in automated event alerting as well as helping those interested more generally in the patterns of media reporting on public health.
ACKNOWLEDGEMENT We would like to thank all those who have contributed toward creating and maintaining the BioCaster system.
1187 Copyedited by: TRJ MANUSCRIPT CATEGORY: APPLICATIONS NOTE [14:28 29/3/2012 Bioinformatics-bts099.tex] Page: 1188 11861188 N.Collier and S.Doan Funding: Japan Science and Technology Agency (JST) Sakigake fund.
Conflict of Interest: none declared.
ABSTRACT Summary: The Unstructured Information Management Architecture (UIMA) framework and web services are emerging as useful tools for integrating biomedical text mining tools.
This note describes our work, which wraps the National Center for Biomedical Ontology (NCBO) Annotatoran ontology-based annotation serviceto make it available as a component in UIMA workflows.
Availability: This wrapper is freely available on the web at http://bionlp-uima.sourceforge.net/ as part of the UIMA tools distribution from the Center for Computational Pharmacology (CCP) at the University of Colorado School of Medicine.
It has been implemented in Java for support on Mac OS X, Linux and MS Windows.
Contact: chris.roeder@ucdenver.edu Received on February 1, 2010; revised on April 12, 2010; accepted on May 9, 2010 Integration and ease of installation are increasingly important concerns as the number of biomedical text mining tools grows in size and complexity.
Many tools are deployed as web services to avoid complex installation.
The National Center for Biomedical Ontology (NCBO)s Annotator (Jonquet, 2009) is one such tool.
It integrates many biomedical ontologies into a concept annotation service available on the web.
Instead of installing the tool locally, users outside of the NCBO can simply embed a web service client in their applications.
However, challenges for a bioinformatician extend beyond using a single service.
Pipelines of tools are often assembled to accomplish a greater goal like identifying and annotating protein protein interaction events.
These tools work with different data formats for input and output, making the creation of a processing pipeline cumbersome.
The Unstructured Information Management Architecture (UIMA) (Ferruci, 2006) is a framework for integrating such tools into a common data representation and interface.
It provides a mechanism for running the tools in unison and extensions for scaling to larger processing loads.
UIMA is commonly used in the biomedical natural language processing (NLP) community, and recognition of ontology concepts is an important component of many text mining applications.
Hence, adaptation of the NCBO Annotator to UIMA will likely result in broad adoption of the Annotator in biomedical text mining research.
Once a tool has been adapted to UIMA, it can be used in many different assemblies or pipelines with other tools also adapted To whom correspondence should be addressed.
to UIMA.
The Center for Computational Pharmacology (CCP) at the University of Colorado School of Medicine has a collection of such adaptations or wrappers (Baumgartner, 2008), and has successfully used UIMA in the development of systems for participating in the BioCreative and BioNLP shared tasks.
The CCP has now also adapted the NCBO Annotator to UIMA, making it available to UIMA projects.
The NCBO Annotator automatically processes a piece of raw text to annotate (or tag) it with relevant ontology concepts and return the annotations (Jonquet, 2009).
The Annotator accesses over 200 biomedical ontologies from the Unified Medical Language System (UMLS) Metathesaurus (http://www.nlm.nih.gov/research/umls/) and the NCBO BioPortal (http://bioportal.bioontology.org/).
The biomedical ontologies used by the Annotator can be thought of as enriched term lists that include relationships and synonyms.
One of the ontologies available is the Gene Ontology (GO), (http://www.geneontology.org/) which can be used to find references to cell components, biological processes and molecular functions.
The Annotator identifies ontology terms in submitted text and returns formally described annotations in the form of a Uniform Resource Identifier.
For example, if mitochondrion appeared in the submitted text, the Annotator would return the start and end character indexes of the word, the GO ontology id, 39917, and an id for the concept within GO, GO:0005739.
Such direct matches are found using the MGREP tool (Xuan, 2007).
The Annotator enables the use of the hierarchical structure of the ontologies as well to provide more functionality: given a particular parameter setting, it can climb the ontologys is-a hierarchy and return more general concepts that relate to a particular term.
For instance, it could return the ancestors of mitochondrion, i.e.
intracellular membrane-enclosed organelle, intracellular organelle, organelle and cell component.
Such matches are called is-a matches.
They are considered indirect matches because MGREP does not match mitochondrion with them, rather they are found through relationships in the ontology.
The UMLS and BioPortal also allow the Annotator to search between ontologies using mappings to produce a broader range of results including those from different forms of the term such as plurals (mitochondria would match mitochondrion for example).
The UIMA wrapper and the Annotator are actively being used in ongoing research that performs concept matching in text.
The ultimate utility of this wrapper is to use the Annotator in more elaborate pipelines where the generated semantic annotations are used as input to downstream processing, for instance to help identify complex relationships between biological entities.
For example, the CCPs work in BioNLP09 (Cohen, 2009) used similar methods to The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:00 16/6/2010 Bioinformatics-btq250.tex] Page: 1801 18001801 UIMA wrapper for the NCBO annotator create annotations that were then referenced in OpenDMAP (Hunter, 2008) patterns for identifying protein interactions.
To make the functionality of the Annotator easily available to other projects, it is deployed as a web service.
A web service is a software component that makes functionality available over the web in order to be used by computer programs.
In this case, the Annotator is accessible to both humans and computers, respectively through the BioPortal user interface and the BioPortal web service application programming interface.
The ubiquity of the web and the simple protocols involved make it easy to access.
All that is required is an HTTP library and an XML parser.
Other output formats are available that are even easier to parse, though do not include as much detail.
The Annotator allows for control of term matching by exposing parameters available in the web service.
For example, climbing the inheritance hierarchy can be limited with the levelMax parameter.
A full description of parameters is available online (http://www.bioontology.org/wiki/index.php/Annotator_Web_service).
All options described there are supported by the UIMA wrapper with the exception of outputFormat which is set to XML so that the results can be parsed internally to the wrapper itself.
Integrating the Annotator with UIMA requires a type system specification that defines the Java classes used to store annotations.
This wrapper uses the CCPs type system, making it compatible with any of the other wrappers available in the CCPs collection, BioNLP-UIMA (Baumgartner, 2008), freely available online (http://bionlp-uima.sourceforge.net).
Using this wrapper in combination with other UIMA tools will involve some adaptation between type systems.
Since no standard type system has emerged (Hahn, 2008), different UIMA adaptations are likely to use different type systems.
The Julie Labs JCORE (Hahn, 2008) and the Tsujii labs U-Compare system (Kano, 2009), for example, use different type systems.
Each has used different methods for adapting between type systems that apply here as well.
JCORE makes use of a pair of type system adapters that convert from the local type system to the foreign one and back.
Such a converter can be re-used whenever another tool from the same foreign type system is used.
U-Compare makes use of both a shared type system where comparable annotations share a base class (Kano, 2008), and adapter pairs.
Included with U-Compare is a pair of adapters for using CCP wrappers.
The topic of type system design is explored more fully in Verspoor (2009).
Annotations are retrieved from the web service by the wrapper using the HttpComponents (http://hc.apache.org/) from Apache.
They are unmarshalled from the returned XML by JAXB (https://jaxb.dev.java.net/).
Then they are represented in the primary UIMA data structure, the Common Analysis Structure or CAS, using the structures available in the CCP type system (Verspoor, 2009).
A CCPTextAnnotation delineates the span of the matched concept and holds references to other objects describing the annotation such as a CCPAnnotator that shows it came from the NCBO Annotator.ACCPClassMention object, named after the ontology and the term id, GO:000557, for example, contains references to CCPSlotMention objects that contain the attribute values.
The attributes are ontology, concept and type.
The type attribute tells what kind of a match the Annotator used to find the concept.
Possible values are DIRECT, MAPPING and IS-A.
DIRECT is for direct matches from the text to an ontology term found with MGREP.
MAPPING is a match found in a second ontology based on a correspondence to a term in an initial ontology.
More general concepts found through an ontologys hierarchy are marked with a type of IS-A.
IS-A matches have a fourth attribute that tells how far up the hierarchy the term is, named LEVEL.
The wrapper limits the size of the requests it makes on the web service by using sentence annotations created by an analysis engine upstream in the processing pipeline (we currently use the LingPipe toolkit for sentence analysis), and sending one sentence at a time.
This keeps requests below the 300 word limit.
The NCBO reports that with 10 simulated users, requests of 280 words take <5 s. Our tests have been limited to five concurrent requests to leave some bandwidth for others.
On the rare occasions the Annotator is unavailable, the wrapper waits for a short timeout and throws an exception that stops the pipeline.
The wrapper is available as part of the CCPs BioNLP-UIMA distribution.
It requires Java 1.6 and includes build scripts that run on Macintosh and Linux variants.
Windows support is forthcoming.
Scripts download required third party jars and run the build.
A sample pipeline is included ready to run after installation.
Users outside our lab have installed the software quickly and easily.
Further pipelines can be built using a UIMA-supplied GUI.
The NCBO Annotator provides annotations from a wealth of ontologies.
Packaged as a web service, it is readily available to NLP researchers.
With a UIMA adaptation provided by the CCP, it is now also available to the world of UIMA tools and pipelines.
ACKNOWLEDGEMENTS The authors would like to thank the anonymous reviewers and Kevin Livingston for helpful discussion and review.
Funding: National Institutes of Health (5R01-GM083649-02, 2R01-LM008111-04A1, 2R01-LM009254-04 to L.H.
); National Center for Biomedical Ontology (U54 HG004028).
Conflict of Interest: none declared.
ABSTRACT Motivation: Analyses and algorithmic predictions based on high-throughput data are essential for the success of systems biology in academic and industrial settings.
Organizations, such as companies and academic consortia, conduct large multi-year scientific studies that entail the collection and analysis of thousands of individual experiments, often over many physical sites and with internal and outsourced components.
To extract maximum value, the interested parties need to verify the accuracy and reproducibility of data and methods before the initiation of such large multi-year studies.
However, systematic and well-established verification procedures do not exist for automated collection and analysis workflows in systems biology which could lead to inaccurate conclusions.
Results: We present here, a review of the current state of systems biology verification and a detailed methodology to address its shortcomings.
This methodology named Industrial Methodology for Process Verification in Research or IMPROVER, consists on evaluating a research program by dividing a workflow into smaller building blocks that are individually verified.
The verification of each building block can be done internally by members of the research program or externally by crowd-sourcing to an interested community.
www.sbvimprover.com Implementation: This methodology could become the preferred choice to verify systems biology research workflows that are becoming increasingly complex and sophisticated in industrial and academic settings.
Contact: gustavo@us.ibm.com Received on November 16, 2011; revised on February 8, 2012; accepted on March 5, 2012 1 BACKGROUND AND PHILOSOPHY OF SYSTEMS BIOLOGY VERIFICATION 1.1 What is verification?
In the past two decades molecular biology has experienced an increase in the amount and diversity of data that are produced to answer key scientific questions.
Systems biology has emerged as a new paradigm for the integration of experimental and computational To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first three authors should be regarded as joint First Authors.
efforts.
This uses algorithmic analyses to interpret the data and mathematical models are built to predict yet unmeasured states of the biological system.
However, algorithms and models are not unique and the determination of the right algorithm and model leading to the true interpretation of the natural phenomena under study becomes a fundamental question that falls within the realm of the philosophy of science.
Popper postulated (Popper, 1959) that a hypothesis, proposition, theory or in the case of systems biology a model, is scientific only if it is falsifiable.
In Poppers thesis, a theory can be proven wrong by producing evidence that is inconsistent with the theory.
In contrast, a theory cannot be proven correct by evidence because other evidence, yet to be discovered, may exist that will falsify the theory.
Conversely, according to the verificationist school, a scientific statement is significant only if it is a statement of logic (such as a mathematical statement deduced from axioms) or if the statement can be verified by experience (Ayer, 1936).
Statements that do not meet these criteria of being either analytic or empirically verifiable are judged to be non-sensical.
The McGraw-Hill Concise Dictionary of Modern Medicine (2002) defines verification as: The process of evaluating a system, component or other product at the end of its development cycle to determine whether it meets projected performance goals (http://medical-dictionary.thefreedictionary.com/verification).
For systems biology, a fundamental question to address is how to verify the correctness of a model that integrates vast amounts of data into a representation of reality.
These data are not only high-dimensional but noisy given the biological variability, sample preparation inconsistencies and measurement noise inherent to the sensor instrumentation.
While the concept of verification may be applied to different contexts with slightly different meanings, here we always use verification as checking for the truth or correctness of either data (i.e.
whether the data represents what we wish to measure) or the correctness of a theorys predictions.
1.2 Crisis in peer-review/slow and low throughput The quality of a scientific prediction or the accuracy of a scientific model is the subject of rigorous scrutiny, usually by the researchers themselves or by colleagues in the peer-review process that is at the heart of scientific publishing (Spier, 2002).
As stated by the editors of the journal Science (Alberts et al., 2008), The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1194 11931201 P.Meyer et al.
peer review is under increasing stress [] The growth of scientific publishing is placing a burden on the entire scientific enterprise.
Papers today are more interdisciplinary, use more techniques, and have more authors.
Many have large volumes of data and supplementary material.
The coming of age of systems biology and its computational methods such as data-interpreting algorithms are challenging the peer-review process as large numbers of simultaneous predictions are generated, but only a small minority is tested.
In the best cases, a very small sampling of predictions are verified using sound experimental assays and methods and then are presented as representative confirmation of the soundness of the entire set of predictions.
Typically, this verification method lacks sufficient rigor, objectivity and a clear characterization of the relative strengths and weaknesses of the algorithms (Dougherty, 2010; Jelizarow et al., 2010; Mehta et al., 2004).
The same lack of rigor in verification of model predictions can be found in many areas of science where complex systems are measured, analyzed and modeled.
For example, in systems biology, high-throughput data are collected and analyzed together with insufficient verification.
Specifically, false positive and, equally important, false negative rates, are rarely considered a requisite for verification of the analysis for publication.
Consider that the first experimentally-generated, genome-wide interactomes in yeast (Gavin et al., 2006; Ito et al., 2001; Uetz and Hughes, 2000) showed minimal overlap, generating some concerns within the scientific community that the data and methodologies were unreliable.
Later work showed that high quality interactome maps could be generated by including controls and quality standards in data collection, careful verification of all interacting pairs and validation tests using independent, orthogonal assays (Dreze et al., 2010).
Similarly, Genome-Wide Association Studies (GWAS) generate a high rate of false positives as correlations are found for single nucleotide polymorphisms with no direct effect on the phenotype.
The community responded by defining a quality-control process and software package for analysis (Purcell et al., 2007).
Similar problems are found in other fields including protein structure prediction (Moult et al., 1995), prediction of docking between proteins (Wodak and Mendez, 2004), text mining from scientific literature (Hirschman et al., 2005) and biological network inference (Stolovitzky et al., 2007).
In these cases the response has been to set up community-based efforts, as discussed below.
1.3 Proposed community approaches for science verification The difficulties in verifying complex science with traditional methods is driving changes in the methods of evaluation.
Advances in web technology (called web 2.0) have allowed communities to stay tightly in touch to develop their interests, even when they are geographically dispersed.
The journal Nature developed in 2006 an experiment allowing an online public review of manuscripts that in parallel were undergoing peer-review (http://www.nature.com/nature/peerreview/).
Faculty of 1000 is an annotation service that allows researchers to locate outstanding or influential papers from the whole body available that can completely overwhelm the individual.
Faculty of 1000 has domain experts cull, rate and summarize both the importance of the papers findings and context within the field and hence is a good example of new practices in research evaluation that go far beyond simple indexing and content annotation (as in PubMed, for example).
The journal PLoS ONE and now even mainstream sites like Twitter have become places where manuscripts are publicly criticized (Mandavilli, 2011).
We think that these changes in research evaluation, while valuable, will not have sufficient rigor and consistency for the needs of research workflows verification.
2 COMMUNITY APPROACHES FOR SCIENCE VERIFICATION 2.1 Community consensus as criteria of science done right A natural evolution of allowing community feedback has been the development of crowd-sourcing, a modality of distributed problem-solving.
Challenges are broadcasted to potential interested stakeholders (solvers) in the form of an open call for participation.
Participants submit solutions for the challenges, and the best solutions are typically chosen by the crowd-sourcer (the entity that broadcasted the challenge).
The top performing participants are sometimes rewarded either with monetary awards, prizes, certificates or with recognition.
We think that such directed community approaches could complement and enhance the peer-review process.
Most importantly, we think that these could serve as a tool to verify the scientific results and fulfill the ultimate goal of scientific research that is to advance our understanding of the natural world (Meyer et al., 2011).
Community-based approaches to verify scientific research can be considered a more focused attempt to tap the consensus building that historically occurs in scientific progress.
Kuhn understood progress in science as an eminently social process, in which the scientific worldview is dominated by the paradigm embraced by the scientific community at any given time (Kuhn, 1962).
When the number of anomalies accumulated under the current paradigm generates distrust, the community may adopt a new paradigm that now guides how research is conducted.
In this view, the scientific community, and not just nature itself, needs to be taken into account when considering what is accepted as verified science.
For our purposes, we abbreviate the typical definition of verification given in the first paragraph to: science done right, where the right refers to the accepted best practices of the scientific community or similar criteria.
Accepted best practices means that there is a consensus in the community as to the proper collection and analysis of a data modality.
Obviously, a modality must already be accessible to a wide community for the consensus to form.
For newly developed modalities, crowd-sourcing provides a means to a rapid consensus as to the best collection and analysis methodologies.
2.2 Summary of community approaches for verification in other fields Recent practices involving a new form of research quality control have become well-established during the last decade and a half.
These efforts have merged the need of scientific verification of methods used in research, with the widespread practice of crowd-sourcing, to create a sort of collaboration-by-competition communities.
The practice of this idea has been sufficiently well-established to become the business model of for-profit companies.
In this section, we summarize three relevant community-based 1194 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1195 11931201 IMPROVER Table 1.
Additional information for the eight community-based efforts described in the paper.
The last row describes other efforts not discussed in the main text Name Domain and Regularity Website KDD Cup Knowledge discovery and machine learning in various domains.
http://www.sigkdd.org Knowledge Discovery and Data Mining.
Every year since launch in 1997.
InnoCentive The name mixes Innovation and Incentive.
http://www.innocentive.com/ Crowd-sourcing for problems of commercial interest.
Founded in 2001.
New challenges are released on a rolling schedule.
Netflix Prize The name comes from the sponsoring company, Netflix.
http://www.netflixprize.com//index Prediction of user ratings for films, based on previous ratings.
Only challenge so far, released in 2006, lasted 3 years to complete.
CASP Critical Assessment of Techniques for Protein Structure Prediction.
http://predictioncenter.org/ Protein 3D structure prediction assessment.
Every 2 years since 1994.
CAPRI Critical Assessment of PRedicted Interactions.
Assessment of predictions of http://www.ebi.ac.uk/msd-srv/capri proteinprotein docking or protein-DNA interaction from 3D structure.
Goes by Round 22 since 2001.
Starts whenever an experimentalist offers an adequate target.
Predicted structures are submitted 68 weeks later.
DREAM Dialogue for Reverse Engineering Assessments and Methods.
http://www.the-dream-project.org/ Assessment of quantitative modeling in systems biology.
Every year since 2006.
BioCreAtIve Assessment of Information Extraction Systems in Biology.
Evaluating text mining http://www.biocreative.org and information extraction systems applied to the biological literature.
http://biocreative.sourceforge.net Every 2 years beginning in 2004.
FlowCAP Flow Cytometry Critical Assessment of Population Id Methods.
http://flowcap.flowsite.org/ Evaluation of automated analysis of flow cytometry data.
http://groups.google.com/group/flowcap Only one iteration on 2010, second one on planning phase.
Others efforts TunedIT: http://tunedit.org/, RGASP-RNAseq Genome Annotation Assessment Project: www.sanger.ac.uk/PostGenomics/encode/RGASP.html Pittsburgh brain competition: http://pbc.lrdc.pitt.edu/ CAMDA Critical Assessment of Microarray Data Analysis: http://camda.bioinfo.cipf.es/camda2011/ Genome Access Workshop evaluation of statistical genetics approaches: http://www.gaworkshop.
verification approaches with overlapping objectives but different focus areas.
Some relevant details of these efforts are listed in Table 1.
Knowledge Discovery and Data Mining Cup (KDD Cup) is an annual competition organized by the Association for Computing Machinery (ACM) Special Interest Group on Knowledge Discovery and Data Mining, the leading professional organization of data miners (Fayyad, 1996).
KDD goals are to achieve a better understanding and analysis of data in many knowledge domains, such as medical informatics, consumer recommendations, diagnostics from imaging data and Internet user search query categorization.
InnoCentive, a spin-off of Eli Lilly, was founded in 2001 to match problems in need of solutions with problem solvers.
The main entry point of InnoCentive is a web portal where solutions to scientific and business problems are solicited on behalf of organizations seeking innovations.
An example of a recent challenge is Solutions to Respond to Oil Spill in the Gulf of Mexico.
InnoCentive works with seekers to design the challenge, score/judge solutions and manage the intellectual property transfer.
There is usually a cash award to the winning solver.
Netflix Prize was a competition to produce a better algorithm to substantially improve the accuracy of predictions about how much a customer is going to enjoy a movie based on their past movie preferences.
The results were measured against the predictions proposed by Cinematch, the algorithm then used by Netflix for customer preference prediction.
In 2009, the $1M Grand Prize was awarded, and the description of the best performing algorithm (if not the source code) was made publicly available.
2.3 Summary of community approaches for verification in the bio-sciences In this section, we summarize five different verification approaches in the bio-sciences, with overlapping objectives but different scientific focus.
A summary of these efforts is listed in Table 1.
CASP (Critical Assessment of protein Structure Prediction) is used to objectively test structure prediction methods against experimentally found structures in a worldwide-community context (Moult et al., 1995; Moult, 1996; Shortle, 1995).
Even though the primary goal of CASP is to advance the methods of predicting protein 3D structure from its amino acid sequence, the pioneering efforts started by CASP have inspired other similar collaboration-by-competition challenges, such as those listed below.
CAPRI (Critical Assessment of PRediction of Interactions) is a community-wide experiment designed on the model of CASP (Wodak and Mendez, 2004).
Both CASP and CAPRI are blind prediction experiments that rely on the willingness of structural biologists to provide unpublished experimental 1195 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1196 11931201 P.Meyer et al.
structures as targets.
CAPRI is a blind test of the ability of proteinprotein docking algorithms to predict the mode of association of two proteins based on their 3D structure.
DREAM (the Dialogue for Reverse Engineering Assessment and Methods) is a community-based effort whose goal is to help improve the state of the art in the experimental design, application and assessment of systems biology models.
DREAM organizers do this through annual reverse-engineering and modeling challenges and conferences (Prill et al., 2010; Stolovitzky et al., 2007; Stolovitzky et al., 2009).
The challenges, based on either new or pre-existing but obfuscated datasets, test participants in biological network inference and model predictions.
Overall, a handful of best-performer teams are identified in each challenge, while some teams make predictions equivalent to random.
As observed in many DREAM challenges, the aggregation of the predictions of all the teams improves the predictive power beyond that of any single method (G.Stolovitzky, personal communication), providing a sort of community wisdom that truly gives meaning to the notion of collaboration by competition.
BioCreAtIve is the Critical Assessment of Information Extraction systems in Biology.
Patterned on CASP, BioCreAtIve is a community-wide project for assessing the application of information retrieval, information extraction and text mining to the biomedical literature.
An example of a BioCreAtIve task is the recognition of gene names in sentences.
Tasks are released biannually, with associated workshops for dissemination of the methods applied to the tasks by the participating researchers.
Results and level of participation in BioCreAtIve I and II are detailed in (Hirschman et al., 2005; Morgan, Lu et al., 2008), where the lessons learned and the remaining opportunities in this important area of systems biology are also discussed.
FlowCAP is a community-based effort to develop new methods for flow cytometry applications.
The motivation for the project comes from the rapid expansion of flow cytometry applications that have outpaced the functionality of traditional analysis tools used to interpret flow cytometry data.
Hence, scientists are faced with the daunting prospect of manually identifying interesting cell populations in 20 dimensional data from a collection of millions of cells.
For this reason a reliable automated approach to flow cytometry analysis is becoming essential.
FlowCAP is a community-based project to assess the interpreting flow cytometry data and automated gating of single-cell multi-variate data compared with gold standards based on manual gating.
2.4 Lessons from community approaches for verification in the biosciences The discussion in the previous section supports the notion that different communities have embraced crowd-sourcing and collaborative-competition as an aid toward science verification and problem solving.
The value of these efforts is well-demonstrated by the level of acceptance by their respective communities.
The main goals of approaches such as CASP or DREAM are, within their respective areas of focus, to determine the state of the art in predictive models, to identify progress over time, to reveal bottlenecks that stymie progress and to show where effort may best be focused.
For all these efforts, clear gold standards and metrics are necessary to quantify and score the entries of the participants.
Three kinds of gold standards are commonly used.
In one case, evoking the classical machine learning paradigm, some of the data is released as a training set whereas the remainder of the data is withheld as a gold standard test set.
The second case consists of using an established method, a technology or a database accepted by the community as a reference.
The third case consists of combining numerous datasets, algorithms or techniques, to get a closer estimate of the ground truth.
A complication is that gold standard datasets are typically hard to obtain, and in many cases, are presently unobtainable in biology.
For example, in protein structure prediction or macromolecular interactions, unpublished experimental structures can be hard to obtain, depending on the willingness of structural biologists to share their pre-publication data.
On the other hand, the complete connectivity of a signaling network in a cell may be unobtainable with todays technology.
Therefore, gold standards for signaling networks are lacking.
There are solutions to this, however, such as requesting participants to train their network models to be consistent with measured levels of phospho-proteins provided in a training set, while testing the resulting models on their ability to predict levels of phospho-proteins under previously unseen perturbations provided in the test set (Prill et al., 2011).
Establishing a performance metric for scoring a challenge is another far-from-trivial task, which is central to challenge design.
There is no unique or perfect scoring metric.
The three main steps involved in evaluation are: (i) identification of a suitable metric (such as the area under the ROC and root mean square between prediction and measurement); (ii) simulation of a null distribution for the chosen metric by evaluation of randomly sampled predictions; and (iii) assignment of a P-value for a prediction with respect to the null distribution for the metric.
The choice of a useful scoring metric involves complexities that may not be as straightforward as ones intuition might suggest.
Consider the case of CASP in which participants predictions are compared with measured 3D structures.
Early experience with matching only-carbon position rather than side chains led to artifacts and over-fitting that were later addressed by more complex metrics than in averaged structure similarities over multiple spatial scales (Ben-David et al., 2009).
The invariance of the metric under different transformations of the data is another issue to take into account when scoring.
For example, when testing a model prediction that spans a large dynamic range (such is the case in phosphoproteomics and gene expression measurements), a root mean square of the differences between predicted and measured variables may depend on the scale of interest.
For example, the sum of differences squared in linear scale could overemphasize the difference over the large scales, whereas the sum of differences squared after log transforming the data amplifies the differences at the smaller values of the predictions.
The results of such different measures of proximity could yield different best performers.
Thus, aggregation of metrics plays an important role to balance the different biases imposed when choosing a metric.
Even in the simple case of binary classification, metrics such as area under the ROC curve, may be misleading if the positive and negative sets are very unbalanced, and it may need to be 1196 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1197 11931201 IMPROVER complemented with the area under the precision recall curve (Davis and Goadrich, 2006; Stolovitzky et al., 2007).
Other typical performance metrics involve the correlation between the predicted values and the gold standard values.
Potential correlation methods include rank correlation, linear correlation, correlation of the log-values and mutual information.
Combined community predictions can yield meta-predictions that are robust and often more accurate than any of the individual predictions.
In CASP, Meta-servers that poll the results of automatic servers are among the best performers.
Similar observations have been made for some of the DREAM challenges (Marbach et al., 2010; Prill et al., 2010).
Lessons from DREAM suggest that in the absence of first principle understanding, algorithms should be simple to avoid over-fitting to a particular dataset.
In general, there is no one-size-fits-all algorithm, as the DREAM results have shown that the best algorithm depends on the subtleties of the data or on the system studied.
For example, gene network reconstruction algorithms that may work very well in prokaryotes do not translate to eukaryotes, and data based on gene deletions have to be treated differently than data based on gene overexpression in network inference tasks.
The community-wide acceptance of these crowd-sourcing methodologies can be thought of in the context of the discussions between verificationists and falsificationists on when a theory is correct or not.
Instead of choosing between validation and refutation the option is finding a practical solution that is accepted by the community.
Of course, this acceptance is not arbitrary as the scientific community is the guardian of rigor and good science.
The community acceptance of the efforts described here gives credibility to the use of the same techniques and challenges to check theories, hypothesis and models.
How we can use this credibility to implement a methodology to verify systems biology results will be discussed next.
3 PROCESS OF VERIFICATION IN INDUSTRIAL RESEARCH 3.1 IMPROVER methodology: research workflow and building blocks Among the lessons that we extracted from the community approaches described in the previous section, the notion that challenges can be used for science verification is paramount.
In this section, we embrace that concept and present a methodology for process verification that can be used in industrial research workflows and other settings.
We call this methodology IMPROVER, for Industrial Methodology for Process Verification in Research.
IMPROVER evaluates the robustness of a research workflow by dividing it into building blocks that are relatively small and verifiable (Meyer et al., 2011).
A building block is the small functional unit of a research pipeline that has a defined input (data, samples or materials), resulting in a defined output (data analyses, samples, high-throughput data or materials).
Functionally, a building block is a discrete research operation at the small end of the scale that is amenable to verification.
Similar divide and conquer approaches are employed in other fields.
Typically, however, building blocks are developed around rigidly defined criteria in which the output is a known function of the input.
In contrast, IMPROVER building blocks need to accommodate a priori A B Fig.1.
Organization of a research workflow by decomposition into building blocks amenable to verification.
(A) Research pipelines are indicated by the gray arrows, whereas the orange blocks are the more specific building blocks necessary to execute the pipeline.Aconcatenation of research pipelines forms a research workflow.
Each of the building blocks in this diagram can be verified by the challenges indicated by the black arrows emerging from the orange blocks.
(B) Example of a research pipeline including the challenges discussed in Section 3.
For the internal challenge example, levels of RNA extracted from tissue or cells are measured with 2 different technologies, one of which is used as reference.
For the external challenge example, gene expression data from patients and control subjects are used to test whether a disease signature can be extracted and verified.
unknown inputoutput functions.
The development of appropriate scoring metrics is a key element to the verification methodology that helps identify the strength or weakness of a building block when a precise knowledge of an inputoutput relationship is not possible.
The verification can be done internally by members of a research group, or externally by crowd-sourcing to an interested community.
IMPROVER is, therefore, a mix of internal/non-public as well as external/public assessment tests or challenges.
The concepts of research workflow and building blocks are clarified in Figure 1.
The chain resulting from linking together the building blocks is a research pipeline (Fig.1A).
The integration of several pipelines forms a research workflow.
Note that there is no unique way of parceling a research pipeline into modules and building blocks.
In general, however, any decomposition will ultimately have some interdependence on natural functional boundaries and the ability to isolate and verify the building block.
In order to be verified, a research building block has to be recast into a challenge (similar to the challenges of the crowd-sourcing efforts discussed in the previous section), that may be assessed internally or broadcasted externally to stakeholders in the interested community.
In both cases, the challenge construction has critical features such as producing the gold standard datasets that will be used as an anchor against which to compare the predictions of a challenge 1197 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1198 11931201 P.Meyer et al.
output, and the scoring methodology to assess the performance of the predictions.
Although IMPROVER has some commonalities with other crowd-sourcing methods, fundamental differences exist.
Here we briefly highlight the differences between DREAM and IMPROVER.
DREAM is a forum to organize the academic systems biology research community around challenges.
These challenges are chosen by the DREAM organizers in collaboration with the community and are mostly structured to tackle independent problems in systems biology, with no specific link between challenges.
DREAM challenges are widely advertised to the community, and its results are publicly announced.
Conversely, IMPROVER challenges are designed following the interests of a research organization.
These challenges, in turn, are designed to verify building blocks that work synergistically in a research workflow.
Challenges performed to verify these building blocks can help the organization determine a way forward with respect to a previously laid plan: if the task that a building block was supposed to perform at a given level of accuracy is not verified, then the building block has to be modified.
If a building block is verified, then its outcomes can be trusted with a higher degree of confidence.
Examples of building block tasks and possible challenges to verify them are shown in Figure 1B.
IMPROVER can pose its challenges internally, that is within the organization, or externally, to a wide community.
3.2 Internal challenges An organization will use internal assessment challenges to verify in-house data generation, analysis and interpretation, either because of proprietary concerns or because the scope does not require a community effort.
An IMPROVER challenge internal to an organization could help researchers identify building blocks that need either improvement or replacement with a new technology.
As it will be described for external challenges, internal assessment challenges should be scored by an objective third party, who will not participate in the challenge but that could be from another group within the same company or institution.
An internal challenge could be designed to evaluate the quality of data used for an external challenge.
While data production can be ensured by Good Laboratory Practices (OECD 1998), the robustness of the technology used to collect the data may evolve in time, and therefore the quality of the data collection process itself may need to be verified (exemplified by the Noise Level in Gene Expression Data challenge in Fig.1B).
Consider that an organization must decide if the output data from the Gene Titan System for gene expression profiling fromAffymetrix is of sufficient quality to consider its adoption.
This technology allows researchers to process hundreds of samples in one experiment with minimal hands-on time, thus considerably increasing gene expression profiling throughput.
An internal challenge is then constructed to compare the Gene Titan platform with the more established standard using Affymetrix single cartridge technology.
A first verification challenge could consist of profiling a gold standard mRNA references sample, containing known quantities of spiked RNA.
These reference samples, when hybridized on both technology arrays, would allow for the comparison of the sensitivities and error levels of both technologies.
What is essential here is that the assessment be done by an objective third party who knows the composition of the reference sample that is unknown to the experimenter.
In general, the IMPROVER internal challenge contribution to a research workflow will result in an understanding of the limitations of the methodology used in a pipeline.
This understanding could be used to improve the results expected from a building block, thus increasing the robustness and value for the larger research pipeline.
3.3 External challenges/the first IMPROVER challenges An external challenge can be designed to achieve multiple goals when aimed at verifying a building block within a pipeline.
First, a public challenge invites novel approaches to a problem, not considered by the internal researchers.
Second, a blended prediction aggregated from the entire community of predictions is often more accurate than any individual prediction (G.Stolovitzky, personal communication).
Third, the public discourse centered on a challenge, including conference presentations and papers on the best-performing methods, can rapidly build a consensus in the community as to which approaches are the most fruitful for a given task.
Fourth, if despite wide participation, no single team manages to achieve a good performance at solving the challenge, then the building block can be considered as non-verified, increasing the risk of failure of that building-blocks pipeline.
Wide participation by the community is particularly important.
While financial incentives are only one approach to increase participation, other incentives could be just as attractive, including the opportunity to verify the algorithm predictions against newly collected experiments, bragging rights for the best algorithm, the ability to publish and to drive the field for purely academic interests.
We illustrate the concept of an IMPROVER external challenge using as an example the search for robust signatures to perform diagnosis of diseases based on commonly available transcriptomics data.
There are examples of gene expressions signature in use today, such as Oncotype DX and MammaPrint, two FDA approved tests that provide prognostic value and can guide treatment in subsets of breast cancer patients (Paik et al., 2004; van de Vijver et al., 2002).
While diagnostic signatures exist in limited cases, the wide availability of high-throughput transcriptomics data makes plausible the discovery of diagnostic signatures for a multitude of diseases.
The community has recognized the need for robust genomic and gene expression signatures as important enablers for personalized medicine, as patients could directly benefit from treatments tailored to the individual (Subramanian and Simon, 2010).
While there has been a clear need for diagnostic signatures, efforts to discover such signatures in commonly available transcriptomics data have generally fallen short of expectation.
There are many reports in the literature in which the lists of differentially expressed genes purported to distinguish between two biological conditions showed little overlap when the data were taken from different cohorts or when experiments were performed in different laboratories with different platforms (Ioannidis, 2005).
Hence, the discovered signatures do not generalize and perform poorly when classifying datasets other than the ones used to develop the methods.
Even with good control over data collection and patient selection, signature discovery can be inhibited by inherent variability in gene expression.
One proposed method to discover robust classifiers in spite of inherent variability is to separate driver genes from the passenger genes (Lim et al., 2009).
The driver genes (sometimes 1198 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1199 11931201 IMPROVER referred to as master regulators) are upstream controllers that are proposed to be better indicators of disease state than the downstream regulated genes that can show more inherent variability.
The first set of IMPROVER challenges, termed the Diagnostics Signature Challenge, addresses the problem of diagnostics from transcriptional data in a biomedical context.
(This challenge is being organized at the time of this writing.)
The need to find biomarkers that stratify a population into segments characterized by a given phenotype is felt not just in biomedicine but also in other contexts such as the pharmaceutical industry, where a similar IMPROVER challenge could be deployed.
We consider four prevalent diseases: multiple sclerosis (MS), psoriasis, lung cancer and chronic obstructive pulmonary disease.
The building block that this challenge is designed to verify is Find Gene Expression Signature (Fig.1B).
In other words, what needs to be verified is the hypothesis that transcriptomics data contains enough information for the determination of these human disease states.
In a context such as the pharmaceutical industry, a test of validity of the notion of transcriptomics-based signatures would be a pre-requisite to attain the research pipeline goal of finding a product (such as a drug) tailored for each individual (Fig.1B).
We will now describe the operational steps for the Diagnostic Signature Challenge taking out of the four diseases, MS as an example.
MS is an inflammatory disease, believed to be an autoimmune disease that affects the central nervous system.
The trigger of the autoimmune process in MS is unknown, but it is believed that MS occurs as a result of some combination of genetic, environmental and infectious factors (Compston and Coles, 2008), and possibly other factors such as vascular problems (Minagar et al., 2006).
The symptoms of the disease result from inflammation, swelling and lesions on the myelin and in 85% of patients start with a relapse-remitting stage of MS (RRMS).
Finding a robust genetic signature would be of great importance, as diagnosis by a neurologist usually involves ruling out other nervous system disorders with invasive and expensive tests (NINDS Multiple Sclerosis Information Page, http://www.ninds.nih.gov/disorders/ multiple_sclerosis/multiple_sclerosis.htm) and recently drugs can delay the progression of MS when RRMS, is diagnosed early on (Rudick et al., 2006).
IMPROVER organizers will procure from the public literature, a training set of gene expression data from peripheral blood mononuclear cells (PBMCs) corresponding to MS and healthy patients (Fig.2).
In this challenge, the test set corresponds to an unpublished cohort of 129 samples whose labels will be hidden from the participants.
This set of samples obtained from patients that were determined as healthy or RRMS by a physician will constitute the gold standard.
A wealth of additional useful gene expression data is also available through databases such as the Gene Expression Omnibus or ArrayExpress.
Participants can use the training set, open literature information and any other publicly available data.
With this data at hand, participants will generate the transcriptomics-based molecular signature that can differentiate between healthy and RRMS patients.
Participants will be asked to submit for each sample a confidence of the prediction to belong to the RRMS class.
The confidence of the classification should have a value between 1 and 0, 1 being the most confident and 0 the least confident.
After predictions from participants are collected via website submissions, the results will be scored using metrics such as the Area Under the Precision versus Recall (AUPR) curve.
Precision Fig.2.
Schematic diagram of MS Disease signature challenge organization.
A dataset with both gene expression and corresponding clinical diagnoses or prognosis forms the basis of the challenge.
The test data contains the gene expression data generated only and is transmitted to the participants via a web portal.
There are three participants shown, the actual challenges could involve many more.
The participants generate predictions-based gene signatures that are submitted back via the website.
A trusted party will blindly score and rank the prediction by comparing to the gold standard dataset that contains both the gene expression data and actual clinical outcomes.
is defined as the fraction of correct positive set predictions, and recall is the proportion of correct positive set predictions out of all patients in the positive set.
Other metrics for binary classification assessment will also be evaluated.
Teams will be ranked according to their overall performance based on those metrics.
Figure 2 illustrates how the MS disease signature challenge will be organized in order to verify through the IMPROVER methodology whether a robust MS gene signature can be found.
A diagnostic signature for those phenotypes can be accepted as existing, and the building block Find a Transcriptomics-based signature for control versus RRMS verified, only if there is at least one participating team who classified in the correct class a statistically significant number of subjects.
A subsequent verification of the molecular signature discovered by the best performer could be further tested by evaluating its performance in a similar, but biologically independent dataset.
Finally, if no team managed to distinguish the RRMS patients from healthy donors from PBMC transcriptomics data, then we can assert that the building block failed verification, and an alternative way of classification should be explored.
If the building block was verified, an obvious by-product of the challenge is the identification of the best diagnostic signature and the corresponding discovery algorithm for each of the diseases.
Other expected advantageous outcome of the IMPROVER challenge is that it enables a fair comparison of competing methods, as the IMPROVER format requires blind prediction by the participants and blind scoring of the submissions (Fig.2).
This approach will alleviate many of the problems that produce overestimation of results when the authors of an algorithm compare their own method with other existing methods (Norel et al., 2011).
For example, over-fitting and information leakage between training and test datasets are two common pitfalls that can be avoided.
A final advantage of the methodology is that it allows for an assessment of the performance of submissions across both participants and diseases.
This will provide an unparalleled opportunity to assess whether the diagnostic signature discovery approaches can be applied across 1199 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1200 11931201 P.Meyer et al.
different diseases.
Such a controlled assessment is harder to reach with traditional scientific approaches, as it requires a wide variety of participants using different methodologies on the same data and scored under the same metrics.
3.4 Gold standard and metrics of performance A foremost concern in designing a challenge for IMPROVER is to obtain a gold standard dataset against which a set of predictions can be scored in order to verify a building block.
While designing a challenge to verify a building block, the possibility exists that a gold standard cannot be defined or is considered suboptimal as an adequate database, unpublished good quality data or an accessible expert in the field is unavailable.
In this case, the rationale behind the challenge has to be altered and the challenge must be redesigned before the building block can be verified.
Redesigning a challenge can be laborious as it might imply obtaining data for a new gold standard and change assumptions that simplified the underlying biology and favored a good challenge formulation.
A building block can be considered as verified if the predictions made within the challenge are close enough to the known gold standard.
For each challenge, a quantitative metric of performance must be defined.
Like the gold standard, the performance metric is central and should be an integral part of the challenge formulation.
This performance metric can also be used to assign a risk that the verification was a fluke (e.g.
computing a P-value).
It is also possible that a challenge results in lack of verification: none of the participating teams could find an acceptable solution to the problem.
There is generally no a priori reason why one metric should be better than the others.
As a rule of thumb, aggregating the several metrics into one overall metric may have advantages and provide less arbitrary performance metric.
In other cases, however, the nature of the problem guides the choice of metric.
For example, the large dynamic range of gene expression data suggest a performance metric in which the values are represented in logarithmic scale.
4 CONCLUSION AND FUTURE DIRECTIONS The great opportunities made possible by the emergence of high-throughput data in all realms of science and technology have also resulted in the problem of extracting knowledge from these massive datasets.
The proliferation of algorithms to analyze this data creates the conundrum of choosing the best algorithms among the multiple existing ones.
Crowd-sourcing efforts that take advantage of new trends in social networking have flourished.
These initiatives, summarized in Section 2, match discipline-specific problems with problem solvers, who are motivated by different incentives to compete and show that their solution is the best.
In this way, the best method available to solve a given problem can be found in an unbiased context.
Interestingly, these crowd-sourcing methodologies also have an epistemological value, shedding light to the question of when a theory is correct or not.
Instead of tasking a researcher to self-assess (a process suspect of biases) the truth of a model or methodology, the alternative is finding how it fares in an unbiased and rigorous test.
The community acceptance of the efforts described in the first part of this article gives some credibility to the use of similar approaches to verify the sometime elusive results attained in systems biology research.
Extrapolating the idea of using challenges for verification of scientific results, we propose the IMPROVER methodology to assess the performance of a research workflow in contexts such as industrial research.
A main concept in IMPROVER is the formalization of a process to determine a go or no-go decision for the research pipeline in an industrial context (internal and external challenges), as well as better methods inspired by the community participation (external challenges).
If the results are positive, that is, if the pipeline passes all the challenges and there is active community participation, then the credibility of the data, analysis and of the subsequent results would be enhanced in the eyes of the scientific community and regulatory agencies.
The challenge-based approach creates a metric for comparison between possible solutions to a challenge designed to verify a building block.
Superior performance by one methodology could promote acceptance by the community of the best performer methodology as a reference standard.
IMPROVER could offer a complement and enhancement to the peer-review process in which the results of a submitted paper are measured against benchmarks in a double-blind challenge, a process that can well be called challenge-assisted peer-review.
The IMPROVER approach could be applied to a variety of fields where the outputs of a research project are fed into the input of other projects, such as is the case in industrial research and development, and where the verification of the individual projects or building blocks is elusive, as is the case in systems biology.
ACKNOWLEDGEMENTS We thank Robert J. Prill and Alberto de la Fuente for useful discussions and Hugh Browne and Jennifer Galitz McTighe for a careful reading of the manuscript.
Funding: IBM and PMI authors performed this work under a joint research collaboration funded by PMI.
Conflict of Interest: none declared.
ABSTRACT Motivation: The highly coordinated expression of thousands of genes in an organism is regulated by the concerted action of transcription factors, chromatin proteins and epigenetic mechanisms.
High-throughput experimental data for genome wide in vivo proteinDNA interactions and epigenetic marks are becoming available from large projects, such as the model organism ENCyclopedia Of DNA Elements (modENCODE) and from individual labs.
Dissemination and visualization of these datasets in an explorable form is an important challenge.
Results: To support research on Drosophila melanogaster transcription regulation and make the genome wide in vivo protein DNA interactions data available to the scientific community as a whole, we have developed a system called Flynet.
Currently, Flynet contains 101 datasets for 38 transcription factors and chromatin regulator proteins in different experimental conditions.
These factors exhibit different types of binding profiles ranging from sharp localized peaks to broad binding regions.
The protein DNA interaction data in Flynet was obtained from the analysis of chromatin immunoprecipitation experiments on one color and two color genomic tiling arrays as well as chromatin immunoprecipitation followed by massively parallel sequencing.
A web-based interface, integrated with an AJAX based genome browser, has been built for queries and presenting analysis results.
Flynet also makes available the cis-regulatory modules reported in literature, known and de novo identified sequence motifs across the genome, and other resources to study gene regulation.
Contact: grossman@uic.edu Availability: Flynet is available at https://www.cistrack.org/flynet/.
Supplementary information: Supplementary data are available at Bioinformatics online.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
1 INTRODUCTION Metazoan genomes contain thousands of protein-coding and noncoding RNA genes, whose expression needs to be precisely controlled.
Approximately 310% of the proteins in the metazoan known proteome are sequence specific transcription factors (TFs) (Kummerfeld and Teichmann, 2006), which bind to specific cis-regulatory DNA sequences and modulate the expression of their target genes.
These cis-regulatory sequences are organized into cis-regulatory modules (CRM) containing one or more binding sites for a particular set of TFs.
One example of CRMs are enhancers that determine a specific temporal-spatial expression pattern of their target gene (Wang et al., 2007).
The various proteins that form the chromatin participate in the regulation of genes (Sims and Reinberg, 2008).
For example, the histones forming the nucleosomes can be post-translationally modified to create a chromatin environment that will repress or activate the genes around them.
The different associations of TFs with their cis-regulatory elements on the DNAcan trigger, counteract or modulate these regulatory states of genes.
Although detailed studies of individual genes have identified many of the components and basic principles that control transcription, we still lack an understanding of the global architecture of transcription regulatory networks (Babu et al., 2004).
Drosophila melanogaster has been used extensively as a model organism to identify components and basic principles of transcription regulation.
However, even after decades of research only 661 CRM sequences corresponding to 235 Drosophila genes and 778 transcription factor binding sites (TFBSs) are annotated in the Drosophila Cis-Regulatory Database (http://www.comp.nus.edu .sg/bioinfo/Drosophila/) that combines information from sources such as RedFly (Halfon et al., 2008), DNAse footprint database (Bergman et al., 2005) and Drosophila Cis-Regulatory Database (Narang et al., 2006).
Chromatin Immunoprecipitation (ChIP) followed by microarray hybridization on the whole genome tiling arrays (ChIP-chip; Iyer et al., 2001; Ren et al., 2000) or followed by massively parallel DNA sequencing (ChIP-seq) (Johnson et al., 2007), are now established as powerful methods to identify all of the genomic The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
https://www.cistrack.org/%EF%AC%82ynet/%00[09:43 26/10/2009 Bioinformatics-btp469.tex] Page: 3002 30013004 F.Tian et al.
regions bound by a protein of interest in a given condition (Keles, 2007).
Genome wide proteinDNA interaction data and epigenetic marks are now available for many transcription factors and chromatin regulators for D.melanogaster as well as other species that are providing details on transcription regulation (Kim and Ren, 2006).
Moreover, the National Human Genome Research Institute sponsored model organism ENCyclopedia Of DNA Elements (modENCODE; http://www.modencode.org) Project aims to identify the majority of the sequence-based functional elements in the Caenorhabditis elegans and D.melanogaster genomes.
It is important therefore to develop tools for storing, organizing and analyzing these data sets and to make them available to the scientific community in a usable format.
We have built Flynet as a part of our data management and visualization efforts for the modENCODE project, whose goal is to map the genome wide associations of a large set of the Drosophila sequence-specific TFs and chromatin regulator proteins.
Flynet is the first public database for D.melanogaster in vivo protein DNA interaction data identified on the whole genome tiling arrays using ChIP-chip as well as ChIP-seq for a variety of transcription factors and chromatin regulator proteins in different experimental conditions.
It also makes available known CRMs, well-known and de novo identified sequence motifs across the genome, and a list of transcription factors and chromatin regulator proteins in D.melanogaster genome, their domain assignments and their orthologs and paralogs across 12 Drosophila genomes in the form of multiple sequence alignments.
In the following sections we describe the query interface, system architecture, and AJAX based genome browser, as well as tools and resources available as a part of Flynet.
2 METHODS 2.1 Flynet system architecture The Flynet data system is designed to be a general system for storing, annotating, and visualizing in vivo DNA-protein interaction datasets.
Flynet includes code for processing, integrating and indexing the data from the several primary data sources.
The Flynet database is implemented in MySQL.
The user interface is written in Perl and uses Perls Common Gateway Interface module (CGI.pm) and Cascaded Style Sheets (CSS).
Flynet provides two types of front-end tools that let the user interact with the stored data: an AJAX based genome browser (Skinner et al., 2009) and a web tool that provides a table-based view of the data and a mechanism for downloading data.
The data in Flynet is stored in a system we call Cistrack (https://www.cistrack.org).
Cistrack manages cis-regulatory data and was built using an integrated set of tools that we have developed called the Chicago Utilities for Biological Sciences or CUBioS.
CUBioS includes: (i) a database; (ii) a light weight database browser; (iii) a cloud for storing auxiliary files, archiving the data and computing over the data; (iv) utilities for uploading and annotating the data and (v) Web 2.0 widgets for accessing, analyzing and visualizing the data.
The in vivo protein DNA interaction data are processed using different analysis algorithms (see Methods).
The processed data are stored in the Flynet system in three different formats: (i) as ChIP enriched region data for downloading; (ii) as parsed and tiled data for visualization by the genome browser and (iii) as tables in MySQL to support SQL queries.
The Flynet data system includes the chromosome, start and end co-ordinates of the ChIP-enriched region, peak location for the binding region, P-value, false discovery rate and various other attributes computed by the analysis.
There are seven tables in the Flynet module of Cistrack database that store information about the datasets, binding sites, experimental metadata, motifs, annotations and pre-computed putative target genes (PTGs).
The first table contains basic information about the datasets.
The TFBS table stores the binding information for each TF.
The annotation tables contain annotations from several sources, including Redfly, Flybase and the Gene Ontology.
A pre-computed PTG table allows users to search for co-regulators of a gene of interest.
2.2 Analysis of in vivo protein DNA interaction data We analyzed peak-based ChIP-chip data generated on Affymetrix genomic tiling arrays as follows.
Affymetrix BPMAP files were remapped using xMAN (Li et al., 2008) and the latest version of the fly assembly (UCSC dm3/Flybase release 5.8).
The remapped BPMAP files are also processed to remove probe redundancy so that each 25-mer probe is mapped no more than once in any 1 kb window along the genome.
The UCSC (http://genome.ucsc.edu/) dm3 RepeatMasker and simple repeat files were downloaded and used to create a Repeat Library file for use with MAT (Johnson et al., 2006).
MAT was run with the remapped BPMAP files, the Repeat Library files generated specifically for the Drosophila genome and appropriate parameters for Bandwidth, MaxGap and Minprobe.
Bandwidths were taken according to the average DNAfragments lengths from the original publications when reported (Johnson et al., 2006).
The Repeat Library file is available for download from the Flynet resource page.
Peak-based data from two color arrays were analyzed using the MA2C and CisGenome packages with appropriate parameters for the UCSC dm3 assembly.
ChIP-seq data were analyzed with the MACS package using appropriate parameters for the UCSC dm3 assembly.
We used the following procedure for identification of binding regions for factors that identify broad regions.
The ChIP data were first quantile normalized, replicate information was merged, and fold change for each probe on the array was calculated.
The data was smoothed with an appropriate window and given as input for the HMM based segmentation.
HMM segmentation using expectation maximization was used to identify the regions (Shah et al.
in preparation).
2.3 Determination of PTGs We extracted transcription start site (TSS) information of 15 145 genes from Flybase release 5.8.
PTGs for a transcription factor were simply defined as genes whose TSS is closest to the transcription factor binding sites (Zhang, 1998).
We also employed insulator information to correctly assign transcription factor binding sites to their target genes (Negre et al., in preparation).
2.4 Motif discovery and scanning methods For each factor position, weight matrices were obtained from known databases or enriched motifs were identified using MEME (Bailey and Elkan, 1994), AlignACE (Hughes et al., 2000) and MDscan (Liu et al., 2002).
All programs were run with default parameters except for MEME, which was restricted to a maximum of 3 iterations and a maximum motif width of 25.
The motifs were then evaluated using the motif instance pipeline described in (Kheradpour et al., 2007) in order to identify motifs specifically enriched in the insulator regions and to compare the motifs discovered by the different programs.
Each motif was scanned at two PWM cutoffs corresponding as determined by TFM (Touzet and Varre, 2007).
Motifs were then ranked by their enrichment at several conservation levels (from 0.0 to 1.0 confidence).
3 RESULTS 3.1 Flynet user interface Flynet is available at https://www.cistrack.org/flynet/.
It provides users with the ability to search and browse in vivo DNA protein interaction data and related resources.
The Flynet search page allows users to query the database using a transcription factor of interest (TF and DNA associated proteins) 3002 [09:43 26/10/2009 Bioinformatics-btp469.tex] Page: 3003 30013004 Flynet Fig.1.
A snapshot of Flynet JBrowse genome browser showing small peaks for the insulator CTCF (Illumina platform) and regions of histone modification H3K27me3 (Embryo stage E0-4 and S2 cells).
JBrowse allows for faster and smoother navigation through the genome without requiring the reloading of the page.
The annotation tracks in the left panel could be dynamically added and removed by dragging.
or by a target gene of interest.
On the Search page, users can first select single or multiple factors of interest.
Next, users can review the associated metadata (e.g.
data source, antibody, platform, analysis method, and publication) and use this information to refine the selection, after which it is then possible to browse the selected tracks in the browser.
Alternatively, the search by genes page in Flynet allows users to query and retrieve the available data by Flybase identifier, CG identifier or gene symbol, and then to browse the putative regulators and the known Drosophila transcriptional CRMs from RedFly.
Flynet provides lists of genes that are regulated by a transcription factor by two means: 1) using the closest transcription start site to a TFBS and 2) refining the search criteria using the presence of genome wide binding sites of insulator proteins (Negre et al., in preparation).
Flynet also provides an advanced search page that allows users to query by selecting genomic regions and False Discovery Rates (FDR) thresholds for filtering results and identifying results in the genomic repeat regions.
The download all option allows users to download the publicly available data matching the specific query and provides quantitative information like enrichment score, enrichment ratios, FDR levels and peak positions.
Flynet utilizes JBrowse (http://www.jbrowse.org/), which allows users to select single or multiple factors from the available datasets and to browse them in a dynamic HTML environment that renders data tracks on the client side (Skinner et al., 2009).
The use of an JAX-based browser offers several advantages over the existing static HTML based browsers, including a faster and smoother navigation through the genome without requiring the reloading of the page.
Moreover, the browser makes it possible to view the portion of the genome and annotations by dragging and double clicking on the annotations.
Users can also dynamically add or remove annotation tracks to generate customized views using the browser (for example as shown in Fig.1).
3.2 Flynet contents The present version of Flynet contains in vivo protein DNA interaction data for 101 datasets for 38 transcription factors (general and sequence specific TFs) and 8 chromatin regulators, including histone modifications in different experimental conditions.
The data sources for these are experiments performed on whole genome tiling arrays in one color (Affymetrix) and two colors (Agilent and Nimblegen), as well as massively parallel sequencing using Illumina Genome analyzer.
Raw data for D.melanogaster TFs and chromatin regulatory proteins include those downloaded from repositories like GEO (Barrett et al., 2009) andArrayExpress (Parkinson et al., 2007), as well as those coming from our high-throughput experimental pipeline for identification of transcription factor binding sites as a part of the modENCODE project.
These transcription factors show different binding behaviors ranging from sharp peak to broad regions and require different analysis methods and parameter optimizations.
Flynet stores relevant experimental metadata including the gene synonyms, data source (experimental conditions referring to appropriate development stage or cell line), antibody name, array platform, analysis method and literature reference.
In addition to the experimental data and metadata, the UCSC dm3 genome assembly, annotations of 15 145 D.melanogaster genes from Flybase release 5.8, 162 727 PTG records and 665 CRM records from Redfly 2.0 are also integrated into Flynet.
In the resources section, we make available a list of transcription factors and chromatin regulator proteins in D.melanogaster genome, their domain assignments, and their orthologs and paralogs across 12 Drosophila genomes in the form of multiple sequence alignments and a collection of position weight matrices.
4 DISCUSSION Flynet is a web-accessible database of in vivo protein DNA interactions integrated with effective searching and advanced browsing capabilities.
To our knowledge Flynet is the first public database for Drosophila with the explicit goal of making accessible high-throughput data from genome wide studies on vivo protein DNA interactions and integrating it with other available data.
Specialized databases providing a list of sequence specific TF (Adryan and Teichmann, 2006), collections of CRM and TFBS information from literature (Halfon et al., 2008), Drosophila cis-regulatory element database (Narang et al., 2006), DNAse I footprint database (Bergman et al., 2005) and position weight matrices (Sandelin et al., 2004; Wingender et al., 1997) are available.
Flynet integrates knowledge from some of these sources, includes recent experimental data (Georlette et al., 2007; Isogai et al., 2007; Kwong et al., 2008; Lee et al., 2008; Li et al., 2008; Matsumoto et al., 2007; Misulovin et al., 2008; Schwartz et al., 2006), and allows users to browse and compare this data easily.
For the fly genome, gene centric databases like Flybase and Flymine (Lyne et al., 2007) provide genomic and protein sequences, annotations, GO terms, protein structure, proteinprotein interactions and pathway information, as well as various types of functional genomics data, including gene expression profiling and phenotypic information from various screens.
However, these databases do not contain the large amount of publicly available ChIP-chip and ChIP-Seq data in an analyzed format.
3003 [09:43 26/10/2009 Bioinformatics-btp469.tex] Page: 3004 30013004 F.Tian et al.
Flynet, on the other hand, takes a TF-centric approach and specializes in transcriptional regulation information.
It includes data being produced as the part of modENCODE project, as well as data deposited in GEO and ArrayExpress.
In fact, 41 of the 101 datasets (40%) currently in Flynet are non-modENCODE data sets.
In this way, Flynet provides users with ability to view and analyze modENCODE data, other published data, de novo computed motifs and CRM information along the genome.
Flynet data is analyzed by experts in a uniform manner using state-of-the art methods, undergoes a manual check for quality, and is based on the latest version of the fly genome.
Methods for analyzing ChIP-chip and ChIP-seq data are still evolving.
In fact, we have developed an HMM based segmentation algorithm for handling region based data for K9 and K27 tri-methylation of Histone H3.
We plan to reanalyze the available data with better methods as they are developed.
The data residing in Flynet includes data generated using different experimental platforms and under various experimental conditions, including different developmental stages, different cell lines and different antibodies.
Because of the variety of data included in Flynet, similarities and differences in genome wide occupancy of these factors can be analyzed, as well as the strengths and weaknesses of different data generation platforms.
For example, information on insulator binding regions in Flynet can be used for determining likely target genes for transcription factor binding sites.
Flynet will be updated with new data, as it becomes available.
We will also plan to update Flynet with every major update of D.melanogaster genome assembly and genome annotations.
The data residing in Flynet will be helpful in identification of new CRMs and the comparative analysis of similarities and differences in genome wide in vivo proteinDNA interactions and epigenetic marks.
ACKNOWLEDGEMENTS The authors would like to thank Ian Holmes laboratory for developing the AJAX based genome browser used in this project and making it available as an open source project.
They also like to thank Manolis Kellis laboratory for providing sequence motifs to display as an annotation track.
Funding: Chicago Biomedical Consortium with support from The Searle Funds at The Chicago Community Trust, NIH through grants P50 GM081892 and U01 HG004264, and the Chinese Student-Exchange Program.
Conflict of Interest: none declared.
Abstract Protein trafficking or protein sorting in eukaryotes is a complicated process and is carried out based on the information contained in the protein.
Many methods reported prediction of the subcellular location of proteins from sequence information.
However, most of these predic-tion methods use a flat structure or parallel architecture to perform prediction.
In this work, we introduce ensemble classifiers with features that are extracted directly from full length protein sequences to predict locations in the protein-sorting pathway hierarchically.
Sequence driven fea-tures, sequence mapped features and sequence autocorrelation features were tested with ensemble learners and their performances were compared.
When evaluated by independent data testing, ensemble based-bagging algorithms with sequence feature composition, transition and distribution (CTD) successfully classified two datasets with accuracies greater than 90%.
We compared our results with similar published methods, and our method equally performed with the others at two levels in the secreted pathway.
This study shows that the feature CTD extracted from protein sequences is effective in capturing biological features among compartments in secreted pathways.
Introduction Eukaryotic cells contain complex compartments called organ-elles enclosed within membranes.
Protein trafficking or protein sorting is a biological process where newly formed proteins get dan G).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A sorted and delivered to various organelles in the intracellular and secretory pathways [1].
Prediction of these protein locali-zation sites in the pathways from the full length amino acid sequence is a complex process, which has not been fully eluci-dated yet.
In 1982, Nishikawa et al.
[2] reported that amino acid composition correlates with localization sites and each localization site in a cell has a unique set of functions.
Hence protein localization prediction has implications both for the function of the protein and its possibility of interacting with other proteins in the same compartment [3,4].
Major protein sorting pathways can be divided hierarchi-cally into secretory and intracellular types [5,6].
In a secretory pathway, all non-secretory proteins are delivered to the endo-plasmic reticulum (ER) and then transported to other related cademy of Sciences and Genetics Society of China.
Production and hosting mailto:geetha@sctimst.ac.in386 Genomics Proteomics Bioinformatics 11 (2013) 385390 locations, which is controlled by ER signal sequences located in the N-termini.
On the other hand, in an intracellular path-way, proteins with organelle-specific signal sequences are im-ported into the nucleus or mitochondria, according to their signal sequence type.
The remaining proteins lacking sorting signals are located in the cytosol [7,8].
The success of computational prediction relies on the extraction of biological features from the sequence and the computational technique used [913].
A wide variety of meth-ods have been tried throughout the years in order to predict the subcellular localization of proteins from full length se-quence features.
Methods reported differ in terms of input data and the technique employed to make the prediction about subcellular location.
According to studies reported by Naka-shima and Nishawa [14], intracellular and secretory proteins differ significantly in their amino acid compositions and in res-idue pair frequencies.
Therefore, in this study simpler and less expensive methods that can extract features from full length protein sequence were given priority.
The main advantage of our feature extraction methods over existing techniques is that features are extracted from the full length protein sequence based on various coding schemes without referencing external databases.
For computation, we used hierarchical ensemble learning [1519] (Figure 1) by mimicking the protein trafficking phenomenon which is incorporated from the location descrip-tions provided by the Gene Ontology (GO) Consortium [20] with the sequence features as input.
Results and discussion Two basic ensemble based classifiers, bagging and AdaBoost M1 were trained to classify the location compartment of pro-teins in the intracellular and secretory pathways using the Wai-kato environment for knowledge analysis (WEKA) [21].
Two tests were carried out with two datasets for performance evalu-ation.
These include a 6-fold cross validation test, which means randomly partitioning the dataset into equally sized training and test sets, training on 5 sets and testing with the 6th set and averaging the results, and an independent data test, which means training on one set and testing with another set by divid-ing the dataset into two random groups.
The performance eval-uation parameters specificity (Sp), sensitivity (Sn), accuracy Level 0 Level 1 Level 2 Figure 1 Hierarchical structures of compartments in protein trafficking Adopted from [1519].
Level 0, root of hierarchy; Level 1, first division; Level 2, second division.
(Acc), Mathews correlation coefficient (MCC), positive predic-tive value (PPV), negative predictive value (NPV) and receiver operating characteristic (ROC) were calculated at all levels for comparing our results with the published results.
Tables S1 and S2 show the average of the classifier perfor-mance parameters obtained from the two datasets at various levels of the pathway hierarchy in 6-fold cross validation and independent data test.
These results were compared with the similar work of LOCtree [15] in Table S3.
Table S4 shows the comparison of our classifier performance parameters with the LocTree2 [16] dataset for 5-fold cross validation.
Comparison with existing methods Our method provides a hierarchical system for the prediction of protein subcellular localization with features generated exclu-sively from the full length sequence without using any server generated inputs.
Similar classification work was reported by LOCtree [15] and LocTree2 [16].
LOCtree used the amino acid composition (20 units), composition of the 50 N-terminal resi-dues (20 units) , amino acid composition from three secondary structure states and SignalP server [22] outputs as a feature vec-tor on a support vector machine, whereas LocTree2 used the profiles created by BLAST-ing [23].
Although the results reported by LOCtree [15] are not di-rectly comparable to ours in terms of features, selection of data, sizing of the data, and method of accuracy calculation, PPV, NPV and MCC reported by our method proved to be better at Level 0 and Level 1 of the hierarchy in the secreted pathway.
The overall accuracy mentioned in LOCtree [15] is the PPV re-sult based on the 6-fold cross validation experiments from a sin-gle dataset.
At Level 0, our independent data testing results based on AdaBoost M1 and bagging reported average accura-cies above 95% (Table S3) between the intracellular and secre-tory pathways with four of the sequence features.
Bagging reported accuracy above 91% for classifying proteins between the secretory and organelle pathways with independent data testing.
Because there is no result published for independent data tests by LOCtree [15], results obtained by this method can-not be compared.
For the 6-fold cross validation test (Table S3), our method reported accuracies above 92% at Level 0 for both bagging and AdaBoost M1 with an average MCC of 0.87, which was reported as 0.73 when using the LOCtree method.
At Level 1, AdaBoost M1 and bagging reported PPVs above 90% with MCC above 0.70 while LOCtree reported an MCC of 0.55.
Classifier bagging with sequence feature CTD performed bet-ter than LOCtree in differentiating the cytoplasm and mito-chondrial pathways at Level 2.
LocTree2 is developed using a different hierarchical path-way and hence we could do the testing only for two levels using a LocTree2 dataset under 5-fold cross validation.
Our method reported accuracies above 88% at Level 0 (Table S4) for all fea-tures under bagging while LocTree2 reported 90%.
For level 1, bagging with feature vector CTD reported an accuracy of 82%, which is also comparable to that reported by LocTree2, 83%.
Conclusion Previous protein localization prediction methods have been implemented using standard machine learning algorithms with Govindan G and Nair AS/ Hierarchical Prediction of Secreted Protein Trafficking 387 parallel architecture as a common practice in computer science [2426].
Here novel systems of ensemble learners using hierar-chical architecture from features extracted directly from full length protein sequences that can predict localization have been tested and the results have been compared.
Our testing results at the secretory pathway of hierarchy show that the prediction accuracy can be significantly im-proved by using the classifier bagging with feature vector CTD.
The system achieved an overall accuracy above 90% with this sequence signature using bagging on independent data tests, suggesting that the native protein localization for each compartment is imprinted onto the features extracted from protein sequence.
Feature generation methods described in this paper works independently and no server/external data reference is required for its extraction.
Methods are based on the composition of amino acid.
Additionally, this hierarchical structure has provided insights into the sorting process, such as the accurate distinction between the intracellular and secretory pathways.
However, we observed that, as one descends the hierarchical path, the prediction accuracy progressively de-creases as the classification task complexity increases.
The best scoring decisions reported are at the top, and the worst are at the bottom.
Thus, hierarchical model classification is unable to correct a prediction mistake made at the top node.
This study supports the hypothesis reported by Nakashima and Nishawa [14] that intracellular and secretory proteins dif-fer significantly in their amino acid compositions.
Both classi-fiers performed well using three sequence features at the top levels of hierarchy.
In the future, this classification method could be potentially extended to any level in the hierarchy using these sequence fea-tures and with the location descriptions provided by the Gene Ontology Consortium [20].
This method can predict the final localization of the protein as well as the mechanism underlying such localization.
Our result may aid the development of more accurate predictors of protein function.
G C A T G G T G C G A A A C T T T G G C T G Zero skip-c0TG= 4, c0GC=3, c0AT=1 G C A T G G T G C G A A A C T T T G G C T G One skip-c1TG =3, c1GC =1, c1AT =1 G C A T G G T G C G A A A C T T T G G C T G Two skips-c2TG=3, c2GC =1, c2AT =2 Figure 2 Amino acid di-peptide (GC, TG, AT) count with skips in a sample sequence c0 indicates count of dipeptides with zero skip, c1 indicates count of dipeptides with one skip and c2 indicates count of dipeptides with two skips.
Materials and methods Dataset construction Two datasets (Table S5) were compiled for this study, which are denoted as ASN_G 1756 (Human) and ASN_G 1008 (Eukaryote).
ASN_G (Human) is collected from a manually curated database for the subcellular localizations of proteins in human [27] and ASN_G (Eukaryote), which is from eSLDB [17], is a database for eukaryotic organisms.
These are the only two manually curated public databases with experimental annotations reported in www.psort.org [28] for eukaryotes.
ASN_G (Human) and ASN_G (Eukaryote) is maintained by the Rost lab of Columbia University Bioinformatics Centre and the Bologna Biocomputing Group, University of Bologna, respectively.
These experimentally annotated proteins were finalized by verifying with UniProt (www.uniprot.org, release 2011-02 SeptOct) and by selecting the sequences that had a determined single subcellular location.
Entries in the subcellu-lar location that were annotated as putative, potential, possible and by similarity were eliminated to remove se-quences with ambiguous and uncertain annotations.
We used the Cluster Database at High Identity with Toler-ance (CD-HIT-2D) [29] web server to eliminate sequences in both datasets that displayed a similarity greater than or equal to 30%.
The program (CD-HIT) takes a fasta format sequence database as input and produces a set of non-redundant repre-sentative sequences as output by removing the highly similar sequences.
For comparing our results with the LocTree2, we down-loaded 1682 sequences from the LocTree2 publication site [16] and generated a dataset with 1677 sequences (Table S7) after verifying the subcellular localizations with UniProt (March 2013).
Sequence feature formation The features extracted from protein full length sequence can be classified into three groups.
The first group consists of se-quence driven features, which are generated directly from se-quence through converting the protein sequence into a numeric sequence by replacing each amino acid with equiva-lent numeric values, counts, etc.
The second group consists of sequence mapped features, which are generated by mapping amino acids into sub groups and the third group contains se-quence autocorrelation features, which are obtained from cal-culations based on three types of spatial autocorrelation (Moreau-Broto, Moran and Geary).
Sequence driven features There are two composition features considered, which include amino acid dipeptide composition (dipeptide descriptors) and composition of physico-chemical properties (amino acid in-dex).
Properties of dipeptides are determined by the amino acids forming the dipeptide.
Dipeptide composition, which gives a fixed pattern length of 400 (20 20), encapsulates the global information about each protein sequence and the order it contains [30].
For example, in the sample protein sequence GCATGGTGCGAAACTTTGGCTG, 400 pairs of dipeptide occurrence frequency with no skips c0, are calculated by count-ing its presence in the sequence with no gaps.
In Figure 2, the count of c0GC is 3, one skip c1GC is 1 and two skips c2GC is 1.
The dipeptide count, cNxx, counts pairs with N skips between them.
The feature vector using the dipeptide occurrence fre-quency count for a protein sequence is represented as three separate numeric counts of its dipeptide c0, c1 and c2, each having 400 components.
The final feature vector of 1200 com-388 Genomics Proteomics Bioinformatics 11 (2013) 385390 ponents is formed by concatenating the corresponding vectors c0, c1 and c2.
The Amino Acid Index (AAindex-1,2,3) is a database of numerical indices representing various physico-chemical and biochemical properties of amino acids and pairs of amino acids [31].
Physico-chemical properties derived from the AAindex1 database having 544 indices are used to compute the features.
Feature vector having 544 components is represented as {f1 f2 f3 .
.
.
f544} where f1 is the physico-chemical property value for all residues of the sequence divided by the length of the sequence.
Sequence mapped features (CTD descriptors) Structural variation in the R groups of amino acids is con-sidered as the main factor for its difference in properties.
From side chains we can classify amino acids into four groups (1) non-polar and neutral, (2) polar and neutral, (3) acidic and polar, and (4) basic and polar.
The 20 amino acids forming the protein sequence can also be divided into several groups based on their other properties like (5) charge, (6) hydrophilicity or hydrophobicity, (7) size, and (8) functional groups.
Twenty amino acids can be mapped into 13 groups by replacing each amino acid code with its group code.
From the mapped sequence, features called composition, transition and distribution (CTD) can be calcu-lated.
Composition is determined as the number of amino acids of a particular property divided by total number of amino acids, whereas transition is determined as the number of transition from a particular property to different property divided by (total number of amino acids 1).
Distribution is the chain length within which the first, 25%, 50%, 75% and 100% of the amino acids of a particular property are located.
According to the property types, amino acids are divided into three groups and are marked as numeric indices 1, 2 and 3 (Table S6).
Properties whose attributes can be grouped perfectly into three sets like charge, hydrophobicity, normalized van der Waals volume, polarity, polarizability, secondary structure and solvent accessibility are used for this mapping [3235].
For example, according to secondary structure property grouping, the sample protein sequence HEAMRQLTIFVCYWNSPDDG is coded as 222222233 33333111111.
In this example with the property of second-ary structure, the total count of the coil is 6, the helix is 7 and the strand is 7.
Hence the composition is calculated as 6/20, 7/20 and 7/20, where 20 is the total length of the se-quence.
Three numbers of composition descriptors are formed from three groups.
The transition from class 1 to 2 is the percentage frequency with which class 1 is followed by class 2 or class 2 is followed by class 1 in the encoded sequence, likewise the transition from class 3 to class 1 or class 1 to class 3, etc.
For the sample se-quence, the sum of transition from 2 to 3 and 3 to 2 is 1.
Hence transition = 1/19.
The distribution descriptor describes the distribution of each property in the sequence.
Five distribution descriptors are formed for each group, including the position percentages in the sequence for the first residue, 25% of the residues, 50% of the residues, 75% of the residues and 100% of the residues.
Fifteen distribution descriptors are formed from three groups.
In total 21 CTD descriptors are formed from a sequence.
For this study, CTD calculation is performed for 7 proper-ties for each protein sequence after dividing each sequence into three equal segments.
In total, 21 3 attributes for a sequence and 441 attributes for 7 properties compose the final feature vector.
Sequence autocorrelation features (autocorrelation descriptors) Sequence autocorrelation-based features are based on the To-blers first law of geography everything is related to every-thing else but nearby things are more related than distant things [36].
Sequence autocorrelation-based features also as-sume that the disturbances in each area are systematically re-lated to those in adjacent areas [37].
Spatial autocorrelation is the correlation of the variable with itself through space.
Spatial autocorrelation measures the degree to which near and distant things are related, which is positive when nearby things are similar and negative when they are dissimilar.
This concept helps to analyze the dependency among the features of se-quences in each location.
Autocorrelation features are calculated based on the distri-bution of amino acid properties along the sequence.
Thirty nine amino acid indices related to hydrophobicity are used for calculation after replacing each amino acid with its equiv-alent normalized index as Pi.
Three autocorrelation descriptors are used as features, including normalized Moreau-Broto auto-correlation descriptors [38], Moran auto-correlation descrip-tors [39] and Geary autocorrelation descriptors [40].
The Moreau-Broto autocorrelation descriptor is defined as MBd XN-d i1 PiPid where d 1; 2; 3 upto Max:lag where d is the lag of the autocorrelation, N is the length of the sequence, and Pi and Pi+d are the amino acid index value of the selected property at position i and i + d, respectively.
Max.lag is the maximum value of the lags.
The normalized Moreau-Broto autocorrelation descriptors are defined as MB(d)/(N d).
The Moran autocorrelation descriptor is defined as Moran d 1 N-d PN-d i1Pi PPid P 1 N PN i1Pi P 2 d 1; 2; 3 .
.
.
; 30 P PN i1Pi N where Pi and Pi+d have the same meaning as above.
The Geary autocorrelation descriptor is defined as Geary d 1 2N-d PN-d i1Pi Pid 2 1 N1 PN i1Pi P 2 d 1; 2; 3 .
.
.
; 30: where P, Pi and Pi+d have the same meaning as above.
3510 attributes from 39 amino acid properties with 30 lags compose the sequence feature vector for autocorrelation.
Computational techniques used Among prediction algorithms, ensemble learning is a process by which multiple models such as classifiers are generated and combined to improve overall prediction accuracy [41].
Multiple learners (base learners) are trained to solve the same Govindan G and Nair AS/ Hierarchical Prediction of Secreted Protein Trafficking 389 problem by averaging over multiple classification models with different input feature vectors.
These ensemble techniques re-duce the small sample size problem which is critical in biolog-ical applications.
This method reduces the over fitting of data.
The three most popular classifiers based on the ensemble meth-od, are bagging [42], AdaBoost M1 [43] and Random Forest [44].
In this study, two methods bagging and AdaBoost were used to predict protein trafficking at all levels of protein sort-ing pathway.
Bagging is the name derived from bootstrap aggregation.
This method uses multiple versions of a training set on differ-ent models by using the bootstrap (sampling with replace-ment).
The outputs of the models are combined (average or vote) to create a single output.
AdaBoost M1 adopts an adap-tive sampling by using all instances of each iteration.
In bag-ging, each classifier has the vote of the same strength, whereas AdaBoost M1 assigns different voting strengths to classifiers based on their accuracy.
Performance evaluation parameters The classifier performance evaluation parameters specificity, sensitivity, accuracy, MCC [45], PPV [46], NPV [46] and ROC [47] were calculated at all levels as per the below equa-tions.
Specificity (Sp) is determined as (TN)/(TN + FP), where TN indicates true negative and FP means false positive.
Sensi-tivity is defined as (TP)/(TP + FN), where TP means true po-sitive and FN means false negative.
Accuracy is defined as (TP + TN)/(TP + TN + FP + FN).
PPV and NPV is calcu-lated as (TP)/(TP + FP) and (TN)/(TN + FN), respectively.
MCC is calculated as TPTNFPFN sqrtTPFNTPFPTNFNTNFP.
Authors contributions GG collected the dataset, conducted the data analysis, did ma-chine learning experiments and wrote the manuscript.
ASN conceived the original idea of using ensemble classifiers for the prediction of protein localization hierarchically.
Both authors read and approved the final manuscript.
Competing interests The authors declared that no competing interests exist.
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.2013.0 7.005.
ABSTRACT Motivation: High-throughput sequencing (HTS) technologies are transforming the study of genomic variation.
The various HTS technologies have different sequencing biases and error rates, and while most HTS technologies sequence the residues of the genome directly, generating base calls for each position, the Applied Biosystems SOLiD platform generates dibase-coded (color space) sequences.
While combining data from the various platforms should increase the accuracy of variation detection, to date there are only a few tools that can identify variants from color space data, and none that can analyze color space and regular (letter space) data together.
Results: We present VARiDa probabilistic method for variation detection from both letter-and color-space reads simultaneously.
VARiD is based on a hidden Markov model and uses the forward-backward algorithm to accurately identify heterozygous, homozygous and tri-allelic SNPs, as well as micro-indels.
Our analysis shows that VARiD performs better than the AB SOLiD toolset at detecting variants from color-space data alone, and improves the calls dramatically when letter-and color-space reads are combined.
Availability: The toolset is freely available atContact: varid@cs.toronto.edu 1 INTRODUCTION High-throughput sequencing (HTS) technologies are revolutionizing the way biologists acquire and analyze genomic data.
HTS machines, such as 454/Roche, Illumina/Solexa and AB SOLiD are able to sequence up to a full human genome per week, at a cost hundreds fold less than previous methods.
The resulting data consists of reads ranging in length between 35 and 400 nt, from unknown locations in the genome.
Analysis of these datasets poses an unprecedented informatics challenge due to the sheer number of reads that a single run of an HTS machine can produce, the shortness of the reads, and the various technologies different sequencing biases and error rates.
The two basic steps in the discovery of variants in the human population from reads coming from any of these technologies are: first, the mapping of reads to a finished (reference) genome, and second the identification of variation by analysis of these mappings.
In the last few years, there have been many approaches proposed for mapping reads from HTS technologies (Campagna et al., 2009; Langmead et al., 2009; Li and Durbin, 2009; Li et al., 2008a, b, 2009; Lin et al., 2008; Rumble et al., 2009 among many others; see To whom correspondence should be addressed.
Fig.1.
Color-space description: Parts (a) and (b) show the correspondence between di-nucleotides and their color space representation with a translation matrix and the corresponding Finite State Automaton.
In part (c), we show the effect of SNPs on the color-space representation of the read, as well as effect of sequencing errors on the trivial translation of the read from color to letter space.
The first letter shown in the reads is actually the last letter of the linker, which helps us lock-in on one of the four possible translations of a color-space read.
Dalca and Brudno, 2010; Flicek and Birney, 2009 for reviews) that utilize a wide variety of approaches.
Compared to this multitude of mapping tools, there have only been a handful of toolsets for single nucleotide polymorphism (SNP) and small (15 bp) indel discovery.
The main challenge in detecting these variants is using the error rates of the sequencing platform, the potentially incorrect mappings, and the varying coverage to determine the likelihood that a position represents a heterozygous or homozygous variant with respect to some reference genome.
We use the term heterozygous to refer to the case when a single donor allele differs from the reference, and homozygous to refer to the case when both donor alleles differ from the reference, and are the same as each other.
Tri-allelic SNPs, when the two donor alleles differ from each other and from the reference, The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:07 12/5/2010 Bioinformatics-btq184.tex] Page: i344 i343i349 A.V.Dalca et al.
are rare.
This variation detection task is further complicated by the different types of errors and data representation methods used by various technologies.
For example, while the predominant error type in Illumina sequencing is the misreading of a base pair, in 454/Roche the most common mistake is insertion/deletion errors in a homopolymer (same base repeating multiple times).
The AB SOLiD system introduced a dibase sequencing technique, where two nucleotides are read at every step of the sequencing process together as one color.
Only four dyes are used for the 16 possible dibases (Fig.1a), and the predominant error is the miscall of a color (colors are usually written as numbers 03).
Most tools for variation detection (Li et al., 2008a, 2009; Marth et al., 1999) combine a detailed data preparation step, in which the reads are filtered, realigned and often rescored, with a nucleotide or heterozygosity calling step, typically done using a Bayesian framework.
The typical parameters considered are the sequencing error rate, the SNP rate in the population (the prior) and the likelihood of misalignment (mapping quality).
Most of the tools for SNP calling analyze one base of the reference genome at a time and do not use adjacent locations to help call SNPs (positions are considered independent).
AB SOLiDs dibase sequencing presents several unique challenges for SNP and indel identification.
While typical, letter-space reads represent the DNA sequences directly as a string of As, Cs, Gs and Ts, one can think of dibase encoding as the output of a Finite State Automaton: consider each color as the shift from one letter to the next, so even though only four colors are generated, we can derive each subsequent letter if we know the previous one (Fig.1b).
Sequencing starts at the last letter of the molecule that connects to the DNA (the linker), which is known, thus enabling the translation of the whole read from color space into letter space.
It is important to note, however, that if one of the colors in a read is misidentified (e.g.
due to a sequencing error), this will change all of the subsequent letters in the translation (Fig.1c).
For this reason, simply translating the reads to letter-space would be impractical.
While this error profile may at first seem detrimental, it can actually be advantageous when we need to decide if a particular difference between a read and the reference genome is due to an underlying change in DNA or a sequencing error: all SNPs will change two adjacent colors, while the probability that two adjacent colors are both misread is small, as error probabilities at adjacent positions are independent.
Simultaneously, non-SNP genomic variants (e.g.
polymorphisms at adjacent residues and micro-indels) have more complicated color-space signatures, complicating variation discovery.
Some tools for color-space SNP calling first map the reads in color space by translating the reference, but then translate the multiple alignment back to nucleotide space in order to call SNPs (Li and Durbin, 2009; Li et al., 2008a).
McKernan et al., 2009 describe Corona Lite, a consensus technique where each valid pair of read colors votes for an overall base call.
Currently, there are no methods that can simultaneously take full advantage of both color-and letter-space data to call variantsan important, consideration since the advantages and disadvantages of the various platforms are quite disparate.
By combining these data sources, it is possible to exploit the strengths of multiple HTS technologies to improve on the accuracy of current SNP callers.
Here, we present VARiD a probabilistic approach for variant identification from either or both letter-and color-space data simultaneously.
We represent both types of data as emissions from a hidden Markov model (HMM), while the underlying genotypes of the sequenced genome are the hidden states.
By applying the forwardbackward algorithm on the HMM we generate, for every base of the genome, a probability distribution over the possible bases.
In our testing, VARiD performs more accurately than ABs Corona Lite pipeline for just color-space data, while its ability to incorporate letter-space data allows for more accurate determination of genomic variants using multiple read types, simultaneously.
2 ALGORITHMS In this section, we introduce our application of a HMM to the process of detecting variation from mapped reads.
We begin by describing a simplified version of the model, and then describe the details of the full model and pipeline.
2.1 A hidden Markov model for variation detection An HMM is a statistical model where the states of the system are hiddenthat is, not observable directlyand respect a Markov progression.
The observables are emissions from the hidden states.
For a detailed introduction to HMMs, we refer the reader to Chapter 3 of Durbin et al.
(1999).
The structure of an HMM is defined in terms of the possible hidden states and the permitted transitions and hidden states and the permitted transitions between these.
The model is parameterized by the emissions and transition probabilities.
In the context of variation detection, we define the following HMM model (illustrated in Fig.2): States: the unknown states in the HMM indicate the possible donor genotypes at each position in the genome.
As we will model color-space, as well as letter-space data, and color-space sequencing corresponds to the change between adjacent nucleotides, the HMM will have states that correspond to pairs of consecutive positions.
Overall, there are 16 possible states: {AA, AC, AG, AT, CA, , TG, TT}, illustrated in green in Figure 2a.
Transitions: as each state corresponds to a pair of nucleotides, two adjacent states will overlap by one nucleotide: for example, the state at positions (5, 6) will be followed by the state at positions (6, 7), thus sharing the nucleotide at position 6.
(a) (b) Fig.2.
Illustration of the simplified VARiD HMM.
In (a) emissions, states and transitions are illustrated, and in (b) we illustrated in detail how one can transition from one state to the next.
Note that Y is shared in the illustration (b), and hence we can only transition from a state ending in, say, letter A to a state starting with A. i344 [12:07 12/5/2010 Bioinformatics-btq184.tex] Page: i345 i343i349 VARiD: Variation detection for HTS data Fig.3.
This figure illustrates the concept of emissions in our problem: at the top, we have two adjacent positions in the unknown genome.
We also have six aligned readsthree color-space, three letter-space.
The exact aligned colors to this pair, and the exact aligned letters to the second letter in this pair represent the six emissions observed for this state.
We can proceed to compute the probability that these emissions came from a state AA, AC, .
We show such a computation for the state CC.
This example is also described in the text, see Equation (5).
Consequently the transitions are constrained so that states that end with some nucleotide Y can only transition to states that start with the same nucleotide Y, thus forcing transitions that obey the overlap between adjacent states (Fig.2b).
Using this constraint and the frequency of each nucleotide, we define our transition probabilities: P(transition SZXY)= (1) p(XY |SZ)= { frequency(Y ) if X = Z 0 otherwise For example, the state (TA) will have probability of 0 to transition in any state not starting with A due to our constraint, and the probability of transition to state (AY), where Y is one of {A, C, G, T}, is equal to nucleotide Ys frequency.
Emissions: given that the states of the model correspond to the donor genotypes, the emissions are the donor reads at these loci, generated by either letter-or color-space sequencing technologies (Fig.3).
The genotype state at some position (,+1) can emit one color and one letter (we arbitrarily choose the second, +1 letter as the emission).
As the states overlap, the first nucleotide is emitted by the previous state.
Since the emissions are (mapped) reads, and since platforms and mappers are prone to error, a state corresponding to the di-nucleotide CA will emit color 1 with high probability, although it may emit other colors with some error probability .
Similarly, CA will emit the letter A with high probability, but may emit other letters with some error .
We define the probability of emitting one particular color c or letter from the state CA as (Fig.4): P(emission=c|state = CA)= (2) q(c|CA)= { 13 if c is 1 if c is 0, 2 or 3 Fig.4.
Possible emissions of the states AA and TT, with the respective probabilities.
Here, and are the error probabilities in color space and letter space.
In the complete VARiD model, these errors will vary with their position in a read.
and P(emission=|state = CA)= (3) q(|CA)= { 13 if is A if is C, G or T Similar emission probabilities follow for all states.
Since in general more than one read will cover a position, and we may have reads from different technologies, we combine the above definitions to get the emission probabilities for our HMM: P(emissions=E|state=s)= q(E|s)= ( colors cE q(c|s) )( letters E q(|s) ) (4) where E is a set of letter and color emissions at that position.
For the example illustrated in Figure 3, P(emissions = {0,0,1,A,A,C}|state = CC)=( (13)21 )( (13)12 ) (5) Genotyping: we formulate the problem of variation detection from letter-and color-space sequencing as the problem of finding the maximum likelihood state for each genotypes position, given the emissions generated by the HMM.
To obtain the most likely state at each position we use the forward backward algorithm.
This algorithm first computes, for each state, the probability of being in this state having observed all of the emissions prior to this position (forward probability), and the probability of starting in this state if we are to observe all the remaining emissions (backward probability).
Combining the forward and backward probabilities for a specific location, one gets the overall likelihood of each state at that location given all the observed emissions (Fig.5).
We detect variants by comparing the most likely state with the reference nucleotide at this position.
2.2 VARiD: algorithm for variation identification In the previous subsection, we described a simplified HMM for variation detection that can use both color-and letter-space data.
This i345 [12:07 12/5/2010 Bioinformatics-btq184.tex] Page: i346 i343i349 A.V.Dalca et al.
simple HMM, however, calls only a single nucleotide per position, and cannot detect events such as micro-indels or heterozygous SNPs.
In this section, we describe the full VARiD variation identification algorithm, including the expanded HMM utilized to address the above shortcomings, and the use of base and mapping quality values to parameterize the emission probabilities.
We also describe the post-processing methods utilized in VARiD to filter some types of spurious calls.
A summary of the VARiD pipeline and model is given in Figure 6.
2.2.1 Extensions to the HMM Insertions and Deletions: in order to detect micro-indels, the model must include gaps in the state definitions.
Due to the nature of color-space sequencing, the expanded model needs to maintain the last letter before the current gap was started.
For example, the A--G subsequence, represented by the states {(A ), ( ), ( G)}, should emit the color 2 of AG on the last state, which is accomplished by maintaining four gap types, gapA, gapC, gapG and gapT, with the rule that a gapX state can only follow the letter X or another gapX state.
Thus, in addition to the 16 basic states there are also 24 gap states: 4 states (X, gapX), 4 states (gapX, gapX), and 16 states (gapX, Y), where X and Y are nucleotides {A, C, G or T}, giving a total of 40 states.
Fig.5.
An example of the resulting probabilities given by the forward backward algorithm: in this case, the state AT will be most likely and the nucleotide T will therefore be proposed.
These states allow for deletions with respect to the reference.
The model requires no changes for insertions with respect to the reference (i.e.
gaps in the reference), as the state sequence only describes the donor.
Heterozygous SNPs: to allow for heterozygous variant detection, we build an expanded set of states by taking the cross-product of the state space with itself.
Each state represents both alleles at a position and thus corresponds to a pair of dibases, e.g.
(AC/AG) or (A-/TG).
After expanding the states for indels and diploid states, there are a total of 402 =1600 states in the HMM.
Similar to the transition probabilities above, only a small fraction of the possible transitions are allowed: states where the second nucleotides in the two alleles are A and G, for example, can only transition to states where the first nucleotides are A and G, and the transition probabilities in such cases are based on nucleotide frequencies.
An example of resulting states and transitions is shown in Figure 7a.
Emission probabilities: While the simple model described above used constant errors and to parameterize color-and letter-space emissions, respectively, in practice the error rates vary with the position in the read, and most platforms also generate a quality score for each position in the read to indicate the likelihood of error.
VARiD can use both of these sources of information, either converting a quality value into an error likelihood (assuming it is on the standard Phred scale) or using pre-specified error likelihoods for every position in a read.
In the results presented below we use the second approach, as in our experience with the AB SOLiD data the quality values proved less informative than the read position.
The per-position error frequencies are maximum likelihood estimates obtained from the alignments of the color-space reads.
For the 454 data, we use a fixed error probability of 0.5%, also inferred from the mappings.
First color: the first color in a color-space read is encoded relative to the last letter of the linker that connects the DNA to the slide.
This will cause the first color in a read to be different from the corresponding color in other reads, which are encoded relative to the previous DNA letter.
To address this, we translate through the first color of the read, thus obtaining the first-sequenced DNA letter, and use this letter as an emission.
For example, if a read began T2312, it will be converted to Fig.6.
A summary of the steps involved in the described pipeline.
The purple sections are inputs, outputs or steps performed with previous software.
The blue parts illustrate steps described in this manuscript.
i346 [12:07 12/5/2010 Bioinformatics-btq184.tex] Page: i347 i343i349 VARiD: Variation detection for HTS data Fig.7.
Diagram showing the expansions of the model.
(a) we show examples of the expanded states that allow for gaps and heterozygous calls, as well as examples of allowed and not allowed transitions.
(b) we note that adding a cleaning post-processing step is needed because of situations such as these: here we have six reads at two adjacent positions; when the colors of these reads are added up, it seems like we could call a heterozygous SNP represented by the allele combination such as redyellow, bluegreen, although the bluegreen combination is actually not present in any read.
Instead of incorporating a higher order model which would incur complexity costs, we simply check the (generally few) proposed SNPs and disregard cases such as these.
C312.
The C character becomes the corresponding letter-space emission, while the remaining colors are unaffected.
This modification allows VARiD to be used with color-space data only, by providing some letter-space emissions, as well as with letter-and color-space reads together.
2.2.2 Post-processing The HMM that VARiD utilizes is memoryless: the information about the specific reads that generated certain letters and colors is not maintained.
This leads to the possibility that a valid path through the state space is not supported by any reads.
Figure 7b depicts an example that may result in a heterozygous SNP prediction: four counts of red and two counts of blue for the first position, and four yellow, and two green for the second.
Red:yellow and blue:green are considered valid adjacent color changes that typically support a SNP.
In this case, however, there are no individual reads that support the blue:green combination, indicating that this combination is actually unlikely to appear in the genome and hence is unlikely to be a heterozygous variant.
While the proper approach to fixing this problem would be to use a higher order HMM, this would be computationally inefficient.
We instead supplement the current probabilistic model with a post-processing step, where we verify that a statistically likely fraction of the reads directly support each heterozygous SNP call.
This approach is fast, as putative SNPs are rare.
2.2.3 Running time The running time of the typical forward backward algorithm is O(nt), where n is the length of the sequence and t is the number of permitted transitions.
While t <k2, where k is the number of states, in the VARiD HMM k =1600 and it is necessary to utilize sparse matrix operations to efficiently implement the forwardbackward algorithm.
Overall, the running time of VARiD is linear in the length of the genome.
Furthermore, it is possible to parallelize VARiD over larger intervals by splitting the reference into smaller segments or windows, with the requirement that they be slightly overlapping.
The overlapping regions can then be easily reconciled.
VARiD required 4 min on a single Intel P4 Xeon 3.2GHz machine to predict variants in the 80 kb of the human genome that we analyze in the next section.
3 RESULTS To test VARiD, we utilized the dataset of Harismendy et al.
(2009), who sequenced several regions of the human genome, spanning a total of 260 kb, from four individuals (NA17156, NA17275, NA17460 and NA17773), both with the AB SOLiD platform and the 454/Roche Pyrosequencer.
To validate the SNP calls, the authors also resequenced 80 kb from the same regions with Sanger sequencing.
From the original high-coverage datasets, we generated reduced coverage, randomly selected subsets from the individuals with different degrees of coverage.
To analyze the AB SOLiD data we ran the SOLiD System Analysis Pipeline Tool (Corona Lite 4.2.2 with the 35_3 schema) on the color-space data, as well as VARiD with both the AB Pipeline mappings as well as SHRiMP (Rumble et al., 2009) mappings, for all of the read subsets.
For the 454 data, we ran VARiD and gigaBayes (Marth et al., 1999) on the letter-space reads (using Mosaik and SHRiMP as the mapping tools).
Finally, we tested our prediction pipeline on various color-and letter-space subsets combined.
We compared the variants called by each method with the Sanger validation set to compute the following statistics: Number of true positive (TP): SNPs that the predictor detects that are also in the validation set; Number of false positive (FP): SNPs the predictor calls variant that are not in the validation set; Precision: the number of true positives as a fraction of all predictions, 100TP/(TP+FP); Recall: the fraction of true positives as a fraction of the validated SNPs, 100TP/(TP+FN); F-measure: the harmonic mean of precision (P) and recall (R): 2PR/(P+R).
The results of our analysis are illustrated in Figures 810, where we present results of color space only, results of letter space only and results for combinations of the two sequence types, respectively.
In Figure 8, we present results from variation identification with VARiD and the Corona Lite SNP caller (http://www.solidsoftwaretools.com/gf/project/mapreads) using the color-space data.
We ran VARiD both with the alignments produced i347 [12:07 12/5/2010 Bioinformatics-btq184.tex] Page: i348 i343i349 A.V.Dalca et al.
Fig.8.
Results illustrating performance of VARiD and Corona Lite on various coverage rates of color-space AB SOLiD reads.
In the first of the three sections, we ran VARiD on various datasets aligned with the SHRiMP tool, in the second we ran it with AB mapper output and finally in the third we ran the Corona Lite pipeline on the AB mappings.
In general, the results show that variation detection is difficult even with high coverage of color space, and the results are dependent on the coverage and the mapping package usedfor example, VARiD with SHRiMP mappings tends to have slightly lower precision, but higher recall, leading to higher F-measure scores, especially at lower coverages, while VARiD with AB mappings has higher precision, but also lower recall.
Fig.9.
Results of running VARiD (SHRiMP alignments), VARiD (Mosaik alignments) and gigaBayes (Mosaik alignments) on all individuals of our datasets, using the 454/Roche data at various coverages.
VARiD with SHRiMP mappings and gigaBayes have similar precision and recall at lower (10) coverage, while VARiD with Mosaik alignments performs slightly worse.
However, at high coverage (20), VARiD with SHRiMP mappings has 70% precision to gigaBayes 56%, and has 83% recall to gigaBayes 64%, thus showing overall improvement.
Fig.10.
These numbers show the improvements we can obtain when combining reads from various platforms.
Comparing at cost, for example, we can look at combining 50 of AB SOLiD color-space data with 5 of 454/Roche data.
Comparing to the equivalent cost of 454/Roche (10) we achieve 7% more precision and 9% higher recall in the combined run.
Similarly, comparing to the equivalent cost of AB SOLiD color-space data (100), we obtain 6% better precision and 3% better recall.
Another example can be found by looking at the CS-100 and LS-10 combination, and comparing with 200 of CS or 20 of LS in Figures 8 and 9. by the AB pipeline for the Corona caller and with alignments generated by SHRiMP.
While the results as a whole demonstrate the difficulty of calling variants from color-space data, even at high coverages, a direct comparison of the two SNP calling pipelines shows that at low-coverage (10) VARiD outperforms the Corona pipeline when using the same set of mappings generated by ABs own mapping tool, while at higher coverage VARiD has better precision and worse recall (and a lower F-measure).
The VARiD + SHRiMP pipeline has slightly lower precision than Corona and VARiD + AB mapper, but a significantly better recall, leading to a higher F-measure score.
In Figure 9, we compare results of running the VARiD framework on the 454/Roche letter-space data using the Mosaik alignments as well as using the SHRiMP alignments, compared to gigaBayes using Mosaik alignments.
At low coverage (15), the gigaBayes SNP caller produces the best results, having higher precision with similar recall.
At higher coverages (1020), VARiD outperforms gigaBayes with higher recall and higher precision, regardless of the mapper used to generate the alignments.
Figure 10 shows the main advantage of the VARiD pipeline: its ability to combine color-and letter-space reads.
In determining useful combinations of the SOLiD and 454/Roche subsets for running on the VARiD framework together, we considered the cost and accuracy of each platform.
The 454/Roche contains a relatively high indel count, but has much more accurate base calls.
At the same time, the 454 platform is 10 times more expensive.
Therefore, we considered combining read coverages with 10-fold more AB SOLiD than 454 data.
For example, we may combine 50 of color-space reads with 5 letter-space, giving us the equivalent of 100 of AB SOLiD or 10 of 454 in terms of cost.
Of course, the best trade-off i348 [12:07 12/5/2010 Bioinformatics-btq184.tex] Page: i349 i343i349 VARiD: Variation detection for HTS data will vary depending on the costs of the platforms and their respective accuracies.
In Figure 10, we consider the various possible coverage combinations between the AB SOLiD data and the 454/Roche.
In general, the performance of VARiD on a certain coverage of color-space data can be greatly improved with just a small number of 454 reads.
More concretely, comparing at cost we can look at 50 coverage of color space with 5 coverage of 454 data: when combined, we find 84% precision and 77% recall.
Looking at the cost equivalent coverage of just 454 data10gives 79% lower precision and recall.
Similarly, for the cost equivalent coverage of AB SOLiD data100will again perform worse.
Combining the data thus shows significant improvement over predicting variation from letter or color space only.
4 DISCUSSION The various HTS technologies that have emerged in the past few years have different data representations, advantages, biases and features.
In this work, we introduced a novel probabilistic framework for variation identification, which can use both letter-and color-space data simultaneously.
We have shown in our results that when using only color-space dataa data type for which very few genomic analysis tools existthe model outperforms the AB SOLiD toolkit Corona Lite, and performs on par with gigaBayes predictions for letter-space data alone.
More importantly, when the color-and letter-space data are combined, the VARiD framework allows for a significant performance increase, demonstrating that a method that can take into consideration multiple technologies, combining their different advantages and compensating for their different weaknesses can achieve higher accuracy variant predictions than are possible from any single data type.
ACKNOWLEDGEMENTS We thank Adrian Dalca Sr for help with the implementation.
Funding: National Sciences and Engineering Research Council (NSERC) of Canada; Mathematics of Information Technology and Complex Systems (MITACS) grant; Life Technologies (Applied Biosystems).
Conflict of Interest: none declared.
ABSTRACT Motivation: Cryo-electron tomography allows the imaging of macro-molecular complexes in near living conditions.
To enhance the nominal resolution of a structure it is necessary to align and average individual subtomograms each containing identical complexes.
However, if the sample of complexes is heterogeneous, it is necessary to first classify subtomograms into groups of identical complexes.
This task becomes challenging when tomograms contain mixtures of unknown com-plexes extracted from a crowded environment.
Two main challenges must be overcomed: First, classification of subtomograms must be performed without knowledge of template structures.
However, most alignment methods are too slow to perform reference-free clas-sification of a large number of (e.g.
tens of thousands) of subtomo-grams.
Second, subtomograms extracted from crowded cellular environments, contain often fragments of other structures besides the target complex.
However, alignment methods generally assume that each subtomogram only contains one complex.
Automatic meth-ods are needed to identify the target complexes in a subtomogram even when its shape is unknown.
Results: In this article, we propose an automatic and systematic method for the isolation and masking of target complexes in subto-mograms extracted from crowded environments.
Moreover, we also propose a fast alignment method using fast rotational matching in real space.
Our experiments show that, compared with our previously proposed fast alignment method in reciprocal space, our new method significantly improves the alignment accuracy for highly dis-torted and especially crowded subtomograms.
Such improvements are important for achieving successful and unbiased high-throughput reference-free structural classification of complexes inside whole-cell tomograms.
Contact: alber@usc.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Cryo-electron tomography enables the 3D imaging of macromol-ecular complexes at nanometer-scale resolution in near native conditions (Best et al., 2007; Lucic et al., 2005; Medalia et al., 2002).
Tomograms of individual cells are essentially 3D represen-tations of the entire proteome providing a snapshot of the dis-tributions of protein complexes (Beck et al., 2011).
However, comprehensive detection of individual complexes in cell tomo-grams is challenging because of the inherent low signal-to-noise ratio (SNR), missing data, nonisotropic resolution and the fact that individual macromolecules are difficult to recognize in a highly crowded environment (Beck et al., 2009; Best et al., 2007; Bohm et al., 2000; Frangakis et al., 2002; Medalia et al., 2002; Nickell et al., 2006).
Most methods for detecting complexes in cell tomograms rely on a template structure, which serves as a reference in searching for a similar pattern in the tomogram (e.g.
Beck et al., 2009).
However, for an unbiased detection and classification of all the different complexes in a cellular tomogram, template-free meth-ods are needed (Xu et al., 2011).
Such an analysis is challenging and relies on three main steps: First, the locations of potential complexes are detected by using particle-picking methods and a subregion surrounding the potential complex is extracted (i.e.
the subtomogram).
Second, the subtomogram regions corresponding only to the target complex must be detected, which allows mask-ing out all background regions, which in turn contain noise and in case of crowded subtomograms also fragments of surrounding structures.
Third, reference-free structural classification of the masked subtomograms is performed, which is typically based on an iterative process of subtomogram alignments, classifica-tions and averaging (e.g.
Amat et al., 2010; Bartesaghi et al., 2008; Chen et al., 2013; Forster et al., 2008; Volkmann, 2010; Xu et al., 2012).
Finally, the averaging of aligned subtomograms of the same complexes will enhance the nominal resolution of the resulting density maps.
In this article, we address two main challenges in reference-free subtomogram classifications.
First, we propose an automatic method for adaptive masking of target regions in crowded sub-tomograms without the knowledge of the shape of the complex.
Such a method is particularly important for subtomograms ex-tracted from crowded environments, such as the cell cytoplasm.
In such a case, subtomograms will also contain fragmental re-gions of other complexes owing to the high particle density in the tomogram.
Masking out these regions is of great importance in the subsequent classification process.
Unlike automatic masking methods based on voxel weighting (Xu et al., 2012) or dimension reduction methods such as Principle Component Analysis (Heumann et al., 2011), our method is highly scalable because it is independent of the classification process and does not in-volve iterative processing of a large number of subtomograms.
Second, we propose a new method for fast rotational matching of subtomograms in real space.
The 3D subtomogram alignment is computationally the most intensive step in the classification process.
Currently, most alignment methods are based on max-imizing the overlap similarity of two subtomograms through exhaustive search over all rigid transforms (rotation and*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com mailto:alber@usc.edutranslation) of one subtomogram with respect to the other (e.g.
Amat et al., 2010; Forster et al., 2008).
Such methods scan ex-haustively through each rotational angle, and find the best cor-responding translation using Fast Fourier Transform (FFT) (e.g.
Amat et al., 2010; Forster et al., 2008).
These methods are com-putationally intensive, which limits their applicability in refer-ence-free classifications.
To increase alignment accuracy, local search methods have been developed to refine initial alignments (e.g.
Bartesaghi et al., 2008; Hrabe et al., 2012; Xu and Alber, 2012).
To increase computational efficiencies, two approximate alignment methods have been developed, which separate the translational from the rotational subtomogram alignments (Bartesaghi et al., 2008; Xu et al., 2012).
These methods are based on similarity scores defined in the reciprocal space (i.e.
Fourier space).
To separate translational and rotational search, these methods use an approximate score that introduces transla-tional invariance by eliminating the phase information of the complex coefficients in the reciprocal space and use only the magnitude of the Fourier coefficients (Bartesaghi et al., 2008; Xu et al., 2012).
By expressing the structural information in Spherical Harmonics (SH) Expansion, it is possible to formulate the rotational alignment as a fast rotational matching, which simultaneously calculates alignment scores of all rotations using FFT.
This procedure significantly increases the alignment speed.
For example, our previously published method (Xu et al., 2012) achieved between hundreds to thousands fold speedup (de-pending on subtomograms size and rotational angle interval) compared with the standard scanning-based alignment method (Forster et al., 2008) by using a translational invariance approxi-mation of a similarity score equivalent to a popular score pro-posed by (Forster et al., 2008).
However, formulating the fast rotational matching in recipro-cal space has some limitations.
In reciprocal space, the majority of informative signals of a complex is usually contained in a rela-tively small amount of high magnitude Fourier coefficients (Amat et al., 2010).
These coefficients are often occupying relatively few voxels in the reciprocal space located within a small region cen-tered at the origin.
Therefore, the SH expansion in the fast rota-tional matching may be hampered by interpolation errors, which reduces the accuracy in the alignment process.
In contrast, in real space, the signal covers a wider area within a subtomogram, and the SH expansion is expected to be more accurate.
It is therefore beneficial to formulate fast rotational matching in real space.
In addition, real space fast rotational matching uses the full constrained alignment score instead of a translation invariant approximation when the method is expressed in reciprocal space (Bartesaghi et al., 2008; Xu et al., 2012).
Here, we devise an improved alignment method, which formu-lates the fast rotational matching of subtomograms in real space.
To achieve this goal, we first identify a keypoint in the target complex whose relative location is invariant to rigid transform-ations and serves as the center of rotation in the fast rotational matching of the two subtomograms.
A natural choice is the center of mass of a complex.
However, detection of the center of mass is not trivial and cannot be approximated by the geo-metrical or mass density center of the subtomogram.
The reason for this complication is that tomograms are images of the crowded cellular environment (Beck et al., 2009), and a subto-mogram contains not necessarily mass density of only one single complex.
When a potential complex is detected and its subtomo-gram extracted as a rectangular cube then this subtomogram typically contains also fragments of other surrounding structures that occupy parts of the subtomogram.
Then, the center of mass estimation and subtomogram alignment are affected by the ex-istence of the additional mass density in the subtomogram.
It is necessary to focus only on the regions that contain the actual target complex.
Even when a subtomogram does not contain any surrounding structures, its mean intensity is often close to back-ground intensity owing to the suppression of low frequency signal owing to the Contrast Transfer Function (CTF) effect, which also makes the estimation of the center of mass of the complex inaccurate.
Applying our automatic target complex segmentation method, it is possible to estimate a center of mass based only on the target complex regions in the subtomogram (i.e.
the constrained center of mass).
Once the constrained center of mass is detected, we design a fast subtomogram alignment method that uses real space signals and takes into account missing wedge corrections using reciprocal space signals.
Moreover, the alignment focuses exclusively on the target complex regions in each subtomogram, which significantly decreases the influence of background noise and surrounding structures to the alignment process.
Our experiments show that our new approach significantly increases the alignment accuracy compared with our previous proposed fast alignment method (Xu et al., 2012) for highly dis-torted subtomograms.
Most importantly, the new method is highly robust to the influence of surrounding structures inside crowded subtomograms.
2 METHODS This section begins by describing the automatic segmentation of the target complex and then focuses on the fast rotational alignment in real space.
2.1 Automatic segmentation of the target complex in a subtomogram The automatic target complex segmentation method consists of several steps (Figs 1, 2 and 7): (i) automatic scale selection is performed to max-imally enhance the detection of structural boundaries in a subtomogram; Fig.1.
Flow chart for segmentation of target complex regions and the real space alignment of subtomogram using only target complex regions i275 Crowded subtomogram segmentation and alignment (ii) segments are defined that describe the boundaries between structural elements; and (iii) a classification of the detected structural segments into candidate regions is performed by using a statistical model based clustering.
2.1.1 Automatic scale selection for optimal smoothing and bound-ary enhancement In this article, we assume that in a subtomogram high image intensity corresponds to high electron density.
When a sub-tomogram is renormalized so that its mean intensity is zero, the boundary of a complex tends to have a negative intensity surface (Fig.3, yellow arrow).
This property is due to the CTF effects in the electron tomog-raphy imaging process, which suppresses low frequency signals.
We use this negative intensity surface as a main characteristic when identifying the segments at the boundary of structural elements, such as the target complex and those structures captured at the outer regions of the sub-tomogram.
To reduce the influence of noise, smoothing of the subtmo-grams is needed.
However, too much smoothing will weaken the negative intensity boundary.
To find the optimal degree of smoothing, we use the scale-space representation method and propose an automatic scale selec-tion method to find the optimal degree of smoothing that maximally enhances the boundary surface while minimizing the noise.
Scale-space (Witkin, 1983) is a framework for multi-scale signal representation.
An image is represented as a one-parameter family of smoothed images, the scale-space representation, parameterized by the size of the Gaussian smoothing kernel (Lindeberg, 1994): gx : 1 2232 e x>x 22 Given a subtomogram, f, a Gaussian smoothing can be expressed as the convolution between f and g : ~f : f g In the smoothed subtomogram ~f , all features with a size smaller than are filtered out, whereas others are preserved.
Our objective is to deter-mine the optimal scale to enhance the formation of a negative intensity surface while minimizing the influence of noise.
To do so, we scan a range of feasible values.
For a given smoothed subtomogram with scale , we find all local minima with negative intensities in ~f and denote this set as Smin.
We then define a local surface score for each minima in Smin, to identify those minima that are likely part of a negative intensity surface.
The surface score is calculated as follows: we calculate the Hessian matrix Hx, whose elements are second-order derivatives of ~f evaluated at x 2 R3, Hijxk @ 2 ~f x @xi@xj xxk , 8k 2 Smin, i, j 1, 2, 3 where xk is the corresponding location of a voxel k. For any k 2 Smin, let 1, 2, 3 be the ordered eigenvalues of Hxk such that j1j j2j j3j.
Then, we can construct a local surface score s ~f xk similar to (Martinez-Sanchez et al., 2011) as follows: s ~f xk j1j ffiffiffiffiffiffiffiffiffiffiffiffi j23j p if 140, 0 if 150: We find the value that maximally enhances the surface scores of all the minima so that the most negative minima tends to have a strong surface score.
To do so, we calculate an accumulative surface score from all the minima.
First, the minima are ordered in ascending order in ~f such that ~fxk1  ~fxk2  .
.
.
~fxkjSmin j 50 where k1, .
.
.
, kjSminj 2 Smin.
Then the accumulative surface score saccui is defined as saccui Xi j1 s ~f xkj Because f ~fxki g is in ascending order with respect to i, this score saccui forms a monotonically increasing function of ~fxki .
We can measure the Area Under Curve, AUC , using fsaccui g and f ~fxki g. AUC tends to be large when the most negative minima have large surface scores.
To meas-ure how much the AUC reflects true surface signals rather than random noise, we also randomly permutate all voxels in f and repeat the afore-mentioned procedure to calculate a permutated AUC, AUCperm .
To determine the optimal Gaussian filtering value , we scan through a range of different s to find the value that maximizes AUC AUC perm For simplicity, in the following sections, we denote ~f : ~f (Figs 2C and 7B) and also denote ~fi : ~fxi for any voxel i.
2.1.2 Detecting structural, boundary and background segments in the subtomogram After identifying the optimal scale , we now determine those local minima that have strong surface scores and are likely to separate different structural elements (i.e.
the target complex and surrounding structures).
We define them as those local minima with both high surface scores and low intensity values.
To do so, first we select the minima whose intensity is significantly smaller than the average intensity, i.e.
Smin intensity fk 2 Smin : ~fk5mediani2Smin ~fi cmin intensity factor madi2Smin ~fig A B C D E F G Fig.2.
Four typical examples for target complex segmentation based on simulated data.
Each row is an example, whereas each subfigure is a slice through the x-z plane in the 3D image.
(A) Ground truth.
(B) Simulated subtomograms at SNR 0.005 and tilt angle range 50 (Section 3.1).
(C) After Gaussian smoothing with automatic scale selection.
(D) After Watershed segmentation.
(E) After boundary segment detection.
(F) Selected segment cluster from model based clustering.
(G) Detected target complex regions Fig.3.
A subtomograms image features.
The boundary feature is a sur-face that has strong negative intensity (yellow arrow).
A local maxima in Smax structural boundary is highlighted by a red cross, which is inside the macromolecular complex.
Its corresponding structural boundary segment is highlighted by a red circle.
A local maxima in Smax background boundary is highlighted by a green cross, which is inside a background region.
Its corresponding background boundary segment is highlighted by green circle.
See Section 2.1.2 for the details of these two types of local maxima i276 M.Xu and F.Alber where cmin intensity factor is a constant parameter to control the significance level, and mad is the Median Absolute Deviation, which is a robust measure of variability.
In addition, we also collect those minima whose surface score is significantly larger than the average scores of all minima, i.e.
Smin surface fk 2 Smin : s ~f xk4mediani2Smin s ~f xi cmin surface factor madi2Smin s ~f xig where cmin surface factor is a constant parameter.
All minima with strong surface scores are defined as the intersection of both groups.
Smin boundary : Smin intensity \ Smin surface We denote these minima as the boundary surface minima.
We then determine all local intensity maxima Smax in ~f, which are likely to be inside structural elements.
We first perform Watershed segmenta-tion (Beucher and Lantuejoul, 1979; Volkmann, 2002) using Smax as seeds (Figs 2D and 7C).
Each segment corresponds to one local maximum, and therefore the terms local maximum and segment are used interchange-ably.
We then select all the segments that border with local minima defined in Smin boundary and denote the corresponding set of maxima as Smax boundary.
The actual separations between structural elements and background regions are always at the boundary between segments in Smax boundary.
The segments in Smax boundary can be divided into two types: a structural boundary segment is located in the target complex or the surrounding structures (Fig.3, inside red circle), and it contains a maximum with high intensity value (Fig.3, highlighted by red cross); a background boundary segment is located in the regions that do not contain structural elements (Fig.3, inside green circle), and which contain a max-imum with low intensity value (Fig.3, highlighted by green cross).
To separate these two types of boundary segments, we perform k-means clustering on Smax boundary (with cluster number 2), resulting in the two groups Smax structural boundary and Smax background boundary.
Smax structural boundary are segments containing local maxima with higher intensity values and are defined as structural boundary segments (Figs 2E and 7D), whereas Smax background boundary are defined as background boundary segments.
After focusing on boundary segments (i.e.
those that border with sur-face minima), we now characterize the remaining segments as either part of structural elements or background regions.
To do this, we first deter-mine a cutoff intensity value cmaxima cutoff as the largest intensity value in all the background boundary segments Smax background boundary.
All remain-ing segments that contain a local maxima with an intensity value smaller that cmaxima cutoff are assigned as background regions.
Finally, the remaining segments whose maxima have intensity values larger than cmaxima cutoff are defined as structural segments and are there-fore assumed to be either part of the target complex or other structural elements, i.e.
Smax structural : fk 2 Smax : ~fi4cmaxima cutoffg.
2.1.3 Combining structural segments into complexes using model based clustering We now cluster structural segments in Smax structural according to the intensity values and location of their local maxima so that each cluster of segments will approximately correspond to one con-secutive structural region.
We choose a model-based clustering approach because it allows an automatic determination of the optimal set of clus-ters.
To perform the clustering, we assume that the location and intensity of local maxima from clusters follows a probabilistic mixture model, where each component probability distribution corresponds to a cluster (Fraley and Raftery, 2002).
We model the probability of independently observing both location and intensity of a local maxima i 2 Smax structural from a cluster k as: pik p ~fi xi;k,k 1 where p ~fi is a probability proportional to the intensity ~fi, and is a probability density function in the form of a multivariate normal distri-bution with mean k and covariance k. For simplification, we assume that the clusters are spherical with different sizes, i.e.
k kI.
The main difference between our model and the standard model-based clustering (Fraley and Raftery, 2002) is the inclusion of the image intensity related term p ~fi in Equation (1).
Our simulation experiments show that the inclusion of image intensity improves the clustering performance (Supplementary Document).
The likelihood for the mixture model with K clusters is: L1, .
.
.
, K;1, .
.
.
,K; 1, .
.
.
, Kjx Y i2Smax structural XK k1 kpik where x represents the collection of observed local maxima, and k is the probability of cluster k (i.e.
cluster mixing probability).
A model-based clustering determines the parameters fkg, fkg, fkg that maximizes L. We use a scale-space representation to estimate cluster centers fkg.
Given a scale  2, we obtain the set S, max of local maxima with positive intensities in ~f .
The local maxima j1, .
.
.
, jjS,maxj are ordered with decreasing intensity values, i.e.
~fx, j1  .
.
.
~f x, jjS, max j  40, where x, jk is the corresponding location of local maxima jk on ~f .
We choose cluster centers as the first K local maxima, i.e k x, jk , 8k 1, .
.
.
,K Similar to the standard model based clustering (Celeux and Govaert, 1995; Fraley and Raftery, 2002), an Expectation Maximization algorithm is used to estimate fkg and fkg.
The expectation maximization is defined by two iterative steps: E-step: update an estimated posterior probability zik that the local maximum i belongs to cluster k: zik  kpikPK j1 jpij M-step: update the cluster mixing probability k, and variances k, given the new parameters in the previous step: k  nkPK j1 nj where nk : X i2Smax structural zik and k  trWk 3nk where Wk X i2Smax structural zikxi kxi k> The Expectation Maximization algorithm iterates until there is no signifi-cant change in the likelihood L. The clustering of local maxima leads to clusters of the corresponding segments (Figs 2F and 7E).
Using statistical modeling for clustering enables the determination of an optimal set of clusters through model selection.
By varying the scale and cluster number K, we obtain the optimal values for scale and cluster set using the Bayesian Information Criterion (Fraley and Raftery, 2002): BIC 2 logL K log jSmax structuralj The final target complex is then represented by combining multiple seg-ments that fulfill the following criteria: (i) they belong to same cluster in the model based clustering; (ii) they are in consecutive contact to each other; and (iii) the maximum intensity values _fij of the voxels between i277 Crowded subtomogram segmentation and alignment two consecutive segments i and j is larger than a threshold.
The thresh-old is computed as cboundary cutoff factorstdi, j2Smax structural _fij, where cboundary cutoff factor is a constant parameter.
The target complex region is then selected as the combined segments that contain at least one structural boundary segment (defined by Smax structural boundary) and are located closest to the center of the subto-mogram.
To prevent that small features of the target complex are excluded, a dilation operation is performed to define the target complex region A (Figs 2G and 7F).
2.2 Real space fast subtomogram alignment using the target complex region and its center of mass Fast rotational matching in real space relies on an initial alignment of the center of masses of the two subtomograms.
Therefore, we describe first how the center of mass of each target complex is calculated before we describe the fast rotational matching approach.
2.2.1 Calculation of center of mass for target complex region The determination of the center of mass in subtomograms is not trivial, pri-marily owing to high noise levels, the suppression of low frequency signals in the reciprocal space and the existence of surrounding structures inside a subtomogram.
Here, the center of mass is calculated only based on the regions in the target complex A.
We set a thresholded region Athresholded on the filtered subtomogram ~f, i.e.
Athresholded fx 2 A : ~fx4mean ~f 0:5std ~fg.
Then, we use Athresholded to calculate the center of mass.
We define another intensity function ~fcmx ~fx miny2Athresholed ~fy if x 2 Athresholed, 0 if x=2Athresholed: We then calculate the constrained center of mass cccm of ~fcm.
2.2.2 Fast rotation alignment We represent two subtomograms f and g as two integrable functions f, g : R3 !
R. They have been trans-formed such that (i) their mean intensities are adjusted to zero, (ii) the voxel intensities outside the target complex region A is set to zero, and (iii) they are translated according to cccm so that their constrained centers of mass are aligned.
The constrained center of mass is used as rotational centers in our fast rotational alignment.
To correct for the missing wedge distortions, we only keep the Fourier coefficients within the overlap of nonmissing wedge regions of both subtomograms.
Following Bartesaghi et al.
(2008), Forster et al.
(2008) and Xu et al.
(2012), we use a binary missing wedge mask function as M : R3 !
f0, 1g, which defines valid and missing Fourier coefficients in reciprocal space.
For example, with a tilt angle range , the missing wedge mask function can be defined as M	 : Ij	3 jj	1 j tan	, where I is the indicator function.
GivenM, the real space subtomogram that excludes any coefficients inside of any of the two missing wedge regions is defined as, f <F1MfF f g <F1MgFg where < denotes the real part of a complex function; F is the Fourier transform operator; andMf andMg are missing wedge masks for f and g, respectively.
Given a 3D rotation R, we can calculate a correlation Rf, g R f RgffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiR jF fj2 RM2g q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiR M2f RjFgj2 q 2 where R is the rotation operator such that Rex : eR1x for any function e : R3 !
C, and R 2 R3	3 is the rotation matrix corres-ponding to 3D rotation R. As in Xu et al.
(2012), the correlation measure R in Equation (2) is equivalent (up to a constant factor.)
to a popular constrained correlation score with missing wedge correction (Forster et al., 2008).
The fast rotational alignment is based on sampling of R simultan-eously over all rotations R. To do so, we apply a fast 3D volumetric rotational matching (Kovacs and Wriggers, 2002; Xu et al., 2012).
It can be seen that Equation (2) can be formulated as being composed of three rotational correlation functions of the form R : R p Rq, where p and q are component functions; e denotes the complex conjugate (when e is a real valued function, the complex conjugate e e.) of a function e. Specifically, we can represent R as R 0Rffiffiffiffiffiffiffi 1R q ffiffiffiffiffiffiffi 2R q R p0 Rq0ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiR p1 Rq1 q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiR p2 Rq2 q 3 where p0 : f, q0 : g, p1 : jF fj2, q1 :M2g, p2 :M2f , and q2 : jFgj2.
When represented in spherical coordinates, the p and q components of these three functions R can be approximated by an SH expansion.
Following Garzon et al.
(2007), Kovacs and Wriggers (2002) and Xu et al., (2012), the p and q components in Equation (3) can be expressed as: pr,,  XB1 l0 Xl ml C p lmrYlm, qr,,  XB1 l0 Xl ml CqlmrYlm, where B is the bandwidth, and Clmr are the coefficients associated with the complex-valued spherical harmonic function Ylm, with degree l and order m, where r, and are the radial distance, co-latitude and longitude, respectively, which define the position in spherical coordinate.
When a suitable parameterization of the 3D rotational group is achieved, the rotational correlation function R : R p Rq of all sampled rotations R can be represented as an inverse FFT of a 3D array of the sum of integrals as follows (Garzon et al., 2007; Kovacs and Wriggers, 2002; Xu et al., 2012): R F1m, n,m0 X l dlmnd l nm0 Z C p lmrC q lm0 rr 2dr " # where dlmh are real coefficients defining the elements of the Wigner small d-matrix evaluated at 90 .
As a consequence of the aforementioned formulation, the cross-correl-ation functions can be efficiently sampled by using FFT simultaneously over all sampled rotations (Kovacs and Wriggers, 2002; Xu et al., 2012).
The sampling is given as twice the bandwidth B used in the SH trans-formations.
Therefore, R can be efficiently computed over all sampled rotations R. Similar to Bartesaghi et al.
(2008) and Xu et al.
(2012), the set of candidate rotations are then obtained by identifying the local maxima of  with respect to the rotational degrees of freedom R. To obtain the optimal translation, a fast translational search is per-formed for each candidate rotation over the full correlation function R by using FFT.
Finally, the best combination of rotation and transla-tion is chosen.
3 RESULTS We assess the performance of our method by using simulated subtomograms of a phantom model.
In addition, we test our method by experimentally determined structures of five com-plexes as well as experimental ribosome subtomograms extracted from a whole-cell tomogram.
i278 M.Xu and F.Alber 3.1 Generation of simulated tomograms from phantom and experimental structures We follow a previously applied methodology for generating sub-tomograms from initial structures by simulating the tomographic imaging process as realistically as possible, allowing for the in-clusion of noise, tomographic distortions due to missing wedge and electron optical factors such as CTF and Modulation Transfer Function (MTF) (Beck et al., 2009; Forster et al., 2008; Nickell et al., 2005; Xu and Alber, 2012; Xu et al., 2011, 2012).
A density map of the complexes (Fig.4) are generated by applying a low pass filter at 4 nm resolution to the atomic struc-tures using the PDB2VOL program of the Situs 2.0 package (Wriggers et al., 1999) with voxel length of 1nm.
In addition, a density map of a phantom model is generated (Fig.4A).
These density maps are used as base maps, both of size 643 voxels, and they are randomly rotated and translated to generate density map instances in uncrowded conditions.
To generate a crowded environment, two additional structures (randomly chosen from the phantom model or the ribosome complex) are randomly ori-ented and placed into the density map 32 voxels away from the center of the map.
The density maps of the target complex and neighboring struc-tures are used as input for simulating electron micrograph images at different tilt angles.
Our simulated subtomograms therefore contain a wedge-shaped region in reciprocal space for which no structure factors have been measured (i.e.
the missing wedge effect), resulting in distortions in the density map as observed in the experimental measurements.
To generate realistic micro-graphs, noise is added to the images according to a given SNR level (ranging between 0.05 and 0.001), defined as the ratio be-tween the variances of the signal and noise (Forster et al., 2008) (Fig.4A).
The resulting image is convoluted with a CTF, which describes the imaging in the transmission electron microscope in a linear approximation (Frank, 2006; Nickell et al., 2005).
We also con-volute the density map with the corresponding MTF of a typical detector used in tomography.
Typical experimental acquisition parameters (Beck et al., 2009) were used: voxel grid length 1 nm, the spherical aberration 2	 103m, the defocus value 4	 106m, the MTF corresponded to a realistic electron detector (McMullan et al., 2009), defined as sinc!=2 where !
is the fraction of the Nyquist frequency.
Finally, we use a backprojection algorithm (Nickell et al., 2005) to generate a tomogram from the individual 2D micrographs generated at the various tilt angles (Beck et al., 2009; Xu et al., 2011, 2012; Xu and Alber, 2012).
3.2 Segmentation of the target complex in simulated subtomograms To determine the optimal scale for surface enhancement, we vary from 1.0nm to 3.0nm.
At high noise levels (e.g.
SNR 0:01), the optimal is usually obtained at 2.0nm.
For the detection of boundary segments, we set the signifi-cance levels cmin intensity factor cmin surface factor 1.
For deter-mining the target complex, we choose cboundary cutoff factor 0, ignoring all boundary segment maxima with negative intensities in f _fijg.
For combining segments, we scan from 2 to 10 nm and scan the cluster number K from 1 to 5.
To assess the target complex segmentation, we calculate the overlap between the determined target complex A and the ground truth Atrue complex.
We also calculate the overlap between A and regions occupied by the surrounding structures in the crowded ground truth density map Asurrounding structure.
Then, we calculate the median of the following three overlap scores across 100 simulated subtomograms at each distortion level and for each of the benchmark structures: otrue positive rate jA \ Atrue complexj jAtrue complexj ofalse positive rate jA \ Actrue complexj jActrue complexj osurrounding structure jA \ Asurrounding structurej jAsurrounding structurej where Ac denotes the complementary set of A.
Our results show that for all distortion levels, the median of otrue positive rate is at least 0.9, and ofalse positive rate is at most 0.004, demonstrating a good performance in segmenting the areas of the corresponding target structures.
In contrast, the median of osurrounding structure is close to zero for most of the distortion levels, indicating that our target complex segmentation method can successfully exclude surrounding structures in the subtomograms.
Only at high dis-tortion levels, the median of osurrounding structure starts to deviate from zero (Supplementary Tables S7S12).
A B Fig.4.
Structures used for simulating the tomographic image process.
(A) Top: a phantom model that consists of four elliptical Gaussian func-tions as branches and one spherical Gaussian function to connect the elliptical functions.
Bottom: Ribosome complex (PDB ID: 2AW7, 2AWB).
Left: isosurface of the two structures.
Right: Slices of the cor-responding x-z plane in the simulated tomograms with different degree of distortions, i.e.
different SNRs and tilt angle ranges.
(B) The isosurfaces of four additional benchmark complexes (Xu et al., 2012) i279 Crowded subtomogram segmentation and alignment 3.3 Assessment of constrained center of mass detection The constrained center of mass is expected to be invariant to rigid transforms of the corresponding structures.
The assessment of the rotational invariance of our constrained center of mass is performed as follows.
We randomly select 100 pairs of subtomo-grams for each of the benchmark structures.
For each pair of subtomograms i and j, we calculate dccm inverse, ij : jjcccm inverse, i cccm inverse, jjj2 as a measure of the degree of invariance, where cccm inverse, i and cccm inverse, j are the corresponding locations of the constrained center of masses in the original complex that is used to generate the instances by random rigid transforms.
The smaller dccm inverse, ij, the higher is the degree of invariance, indicating a good performance.
The performance is expressed as the median of fdccm inverse, ijg, which is listed in Figure 5 and Supplementary Tables S1, S4, S13 and S16.
We can see that the estimation error is generally smaller than 3, indicating high accuracy even at high distortion levels.
As can be seen in the following section, the degree of invariance is sufficiently small to allow successful pairwise fast rotational alignments in real space.
3.4 Fast rotational alignment of subtomograms Our fast alignment method relies on fast rotational matching.
In the following two subsections, we calculate the rotational align-ment errors for the pairs of complexes, both for the crowded as well as uncrowded cases.
The rotational alignment error is cal-culated as follows: Suppose R is the rotation matrix calculated for an alignment, and Rtrue is the true rotation matrix.
The ro-tation alignment error can then be measured as drot err jjR RtruejjF, where jj jjF is Frobenius norm.
3.4.1 Uncrowded subtomograms First, we test our alignment method with uncrowded subtomograms, which contain one com-plex without surrounding structures.
We compare the alignment errors between our new alignment method presented here and our previous method that relied on fast rotational matching in reciprocal space (Xu et al., 2012) (left column of Fig.6 and Supplementary Tables S2 and S5).
It is evident that our new method improves the alignment accuracy on highly distorted subtomograms.
3.4.2 Crowded subtomograms Next, we compare the two align-ment methods (Xu et al., 2012) with crowded subtomograms, which also contain fragments of additional complexes (right column of Fig.6 and Supplementary Tables S14 and S17).
It can be clearly seen that when considering crowded subtomograms without excluding surrounding structures, our previous method (Xu et al., 2012) generally fails in finding the correct alignment.
By contrast, our new alignment method aided by target complex seg-mentation is only marginally affected by crowding and the exist-ence of the surrounding structures.
Our new method clearly outperforms our previous method when dealing with subtomo-grams from crowded cellular environments.
Similar improvement in performance is also observed by testing the crowded subtomo-grams of four additional benchmark complexes (Supplementary Tables S19S22) (Fig.4B) used in (Xu et al., 2012).
We have also measured the correlation between dccm inverse, ij and drot err (Supplementary Tables S3, S6, S15 and S18) and Fig.6.
Rotational alignment error for the phantom model and the ribo-some complex inside uncrowded and crowded subtomograms simulated at different distortion levels.
The error is measured in terms of the median drot err across 100 pairwise alignments.
The nodes on the solid black grid correspond to the alignment performance of our new method.
The nodes in the red dashed grid correspond to the alignment performance of our previous method (Xu et al., 2012).
The green regions correspond to the distortion levels at which both methods can successfully perform align-ments.
The yellow regions correspond to the distortion levels at which our new method outperforms our previous method and correctly aligns the subtomograms, whereas our previous method (Xu et al., 2012) fails.
The gray region corresponds to the distortion levels at which both methods fail.
For both alignment methods, we choose a rotation angle interval 5 , i.e.
the bandwidth B 36 Fig.5.
The performance of the center of mass invariance, expressed as the median of fdccm inverse, ijg across 100 pairwise comparisons of subto-mograms simulated at different distortion levels.
The surface region color is rescaled hue saturation value (HSV) color and proportional to the surface height i280 M.Xu and F.Alber tested the performance with respect to the levels of crowdedness (Supplementary Figs S1S3).
At low to median distortion levels, we observe moderate correlation between the aforementioned two quantities.
At high distortion levels, the correlations are small, showing that the alignment is more affected by high levels of distortions.
3.5 Segmenting target complexes in subtomograms extracted from experimental whole-cell tomogram of the human pathogen Leptospira interrogans We tested our method also on experimental subtomograms extracted from a whole-cell tomogram of Leptospira interrogans in undisturbed conditions (Beck et al., 2009).
Segmenting of complexes in such subtomograms is significantly more challen-ging because the macromolecular crowding is high and because the SNR levels and resolution of these subtomograms is typically low.
Beck et al.
(2009) have identified several complexes in the tomogram using template matching (Best et al., 2007).
The tem-plates were generated by using density maps of structures in the Protein Databank and convoluted with CTF andMTF.
As a test case, we use two subtomograms with high template matching scores to the ribosome template.
For each of these matches, we extracted a subtomogram of size 493 voxels that was large enough to contain an instance of the complex and certain amount of surrounding structure fragments.
The results shown in Figure 7 demonstrate that our method can successfully segment the target ribosome complex while sur-rounding structures are clearly excluded.
The ribosome positions agree with those detected in the template matching.
We further extended the study to the subtomograms corresponding to the top 20 template matching scores.
Overall, 18 of them showed successful segmentation (Supplementary Fig.S4).
We also ana-lyzed the target complex segmentation performance with respect to the level of crowdedness.
The crowdedness of the Leptospira interrogans subtomograms ranges from 0 to 60%.
The true-posi-tive and false-positive rates plotted in Supplementary Figure S4 shows that, in most situations, our method can correctly identify the target complex region.
Only at high crowdedness level, (i.e.
450%), our method fails to correctly identify the target complex regions.
3.6 Analysis of computational costs We implemented the methods in MATLAB and carried out our tests on a computer cluster consisting of 2.32.7GHz computer nodes.
On average, the target complex detection step takes 14.8 s. In this step, the major proportion of computation time was used for the scale selection (8.6 s on average).
In practice, we can de-termine a fixed scale parameter from a set of training subto-mograms and directly apply it to all subtomograms.
Then, on average, only 6.2 s would be used for detecting the target com-plex.
The computer memory consumption is small.
Because a subtomogram is typically small (a subtomogram of size 643 cor-responds to at most 2M memory), our detection step uses no more memory than 20 times the size of a single subtomogram.
The most computational intensive step in the classification process is the alignment.
The computational cost of our new method is essentially the same as our previous method (Xu et al., 2012) and therefore allows high-throughput subtomogram classifications.
4 CONCLUSION Cryo-electron tomograms provide useful information for simul-taneously detecting the native structures and their spatial cellular locations of a large number of macromolecular complexes.
However, the high level of macromolecular crowding and distor-tions in the extracted subtomograms makes such analysis challenging.
In addition, the availability of a large number of subtomograms containing potential complexes requires high-throughput analysis techniques.
In this article, we propose an automatic segmentation technique that isolates the target com-plex inside a crowded subtomogram.
We further detect the rigid transform invariant center of mass of the target complex and propose an improved fast alignment method using fast rotational matching in real space.
The new method shows good perform-ance in segmenting target complexes in crowded subtomograms and performing fast rotational alignments using both simulated and experimental subtomograms.
This performance is a neces-sary condition for high-throughput structural classifications of complexes in whole-cell tomograms.
This work represents a step toward high-throughput and reference-free Visual Proteomics analysis of highly crowded whole-cell cryo-electron tomograms.
ACKNOWLEDGEMENTS The authors thank Dr Martin Beck for providing the experimen-tal tomograms for testing.
They thank the anonymous reviewers for their constructive suggestions.
Funding: Human Frontier Science Program RGY0079/2009-C, the Arnold and Mabel Beckman foundation; NIH [R01GM096089 and U54RR022220] and NSF CAREER [1150287] (to F.A.).
F.A.
is a Pew Scholar in Biomedical Sciences, supported by the Pew Charitable Trusts.
Conflict of Interest: none declared.
A B C D E F Fig.7.
Segmentation of target complex regions in Leptospira interrogans subtomograms.
Each row shows the segmentation process for a ribosome instance.
Each subfigure in a row consists of the consecutive slices through the y-axis of the 3D image.
(A) Original extracted subtomo-grams.
(B) Images after Gaussian smoothing with automatic scale selec-tion.
(C): Images after Watershed segmentation.
(D) Images after structural boundary segment detection.
(E) Images of the selected seg-ment clusters resulting from the model-based clustering.
(F) Images of the final segmented target complexes.
Because the contrast is low, for the detection of significant boundary segments, we relaxed significance level parameters by setting cmin intensity factor cmin surface factor 0:5.
When determining the target complex region, we set cboundary cutoff factor 2 i281 Crowded subtomogram segmentation and alignment
ABSTRACT Motivation: RNA-seq has become the method of choice to quantify genes and exons, discover novel transcripts and detect fusion genes.
However, reliable variant identification from RNA-seq data remains challenging because of the complexities of the transcriptome, the challenges of accurately mapping exon boundary spanning reads and the bias introduced during the sequencing library preparation.
Method: We developed RVboost, a novel method specific for RNA variant prioritization.
RVboost uses several attributes unique in the process of RNA library preparation, sequencing and RNA-seq data analyses.
It uses a boosting method to train a model of good quality variants using common variants from HapMap, and prioritizes and calls the RNA variants based on the trained model.
We packaged RVboost in a comprehensive workflow, which integrates tools of vari-ant calling, annotation and filtering.
Results: RVboost consistently outperforms the variant quality score recalibration from the Genome Analysis Tool Kit and the RNA-seq variant-calling pipeline SNPiR in 12 RNA-seq samples using ground-truth variants from paired exome sequencing data.
Several RNA-seqspecific attributes were identified as critical to differentiate true and false variants, including the distance of the variant positions to exon boundaries, and the percent of the reads supporting the variant in the first six base pairs.
The latter identifies false variants introduced by the random hexamer priming during the library construction.
Availability and implementation: The RVboost package is imple-mented to readily run in Mac or Linux environments.
The software and user manual are available at http://bioinformaticstools.mayo.
edu/research/rvboost/.
Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 30, 2014; revised on August 11, 2014; accepted on August 20, 2014 1 INTRODUCTION RNA-seq has become popular with the decreasing cost and its potential to quantify exon/transcript levels over a large dynamic range, discover novel transcripts, identify various splicing mechanisms and detect fusion genes (Costa et al., 2013) (Asmann et al., 2011).
However, while the variant identification from DNA sequencing is becoming a routine practice, the vari-ant detection from RNA-seq remains challenging because of the complexity of the transcriptome, the ambiguities in mapping exon boundary spanning reads and the artifacts introduced in RNA-seq library protocols (Piskol et al., 2013a).
Because expressed genetic variants have more immediate impact on the protein function compared with the DNA variants, we were motivated to develop a reliable RNA-seq variant prioritization method.
In general, variant detection from massive parallel sequencing data involves two steps.
First is variant calling, which outputs all positions with any evidence of alternative alleles compared with reference.
An essential next step is variant prioritization and filtering to obtain reliable variants of high confidence.
For DNA sequencing data, the most widely used variant prioritization method is a mixture model-based classifier, variant quality score recalibration (VQSR), within the Genome Analysis Toolkit (GATK) (DePristo et al., 2011).
VQSR integrates multiple attributes/annotations of the variants, all of which are based on features of sequencing, including the depth of coverage, strand bias, mapping qualities and variant position bias toward the end of the reads.
VQSR uses variants reported in HapMap as the training source to calculate a filtering criterion, and then predicts true novel variants.
Another method SNPiR proposes a series of arbitrary hard thresholds to filter and reduce the number of false variants (Piskol et al., 2013b).
After careful examination of the RNA-seq variant detection process, we proposed to include RNA-specific attributes/anno-tations for the variant prioritization model in addition to the features included in GATK.
Furthermore, we observed that the Gaussian mixture model and the parameter selection used in VQSR are not ideal for modeling these features and proposed to use a boosting method that uses generalized linear models as its base learners.
This method, called the RNA Variant Boosting (RVboost), is a ranking machine to (a) train a model based on common variants in HapMap and (b) rank the variants accordingly.
*To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com very , due to , to to all of which that , s `` '' is which (GLM) : ; XPath error Undefined namespace prefix XPath error Undefined namespace prefix We implemented a comprehensive workflow using the frame-work of GATK that incorporates RVboost to facilitate reliable RNA-seq variant prioritization (Fig.1).
This workflow outputs called variants with detailed annotations in both the standard variant calling format (VCF) and the tab-delimited text format.
We have shown that RVboost outperforms VQSR in 12 RNA-seq samples with paired exome sequencing data.
We also intro-duced a key concept of train-set quantile score, or the train-Q score, to help users determine their preferred precision/recall trade-off.
2 FEATURES 2.1 Attributes selection After testing, six attributes were included in RVboost.
We kept three attributes fromGATKs Unified Genotyper, which are rou-tinely used in VQSR: (i) Quality score over depth, (quality by depth, QD); (ii) Positional bias (ReadPosRankSum); and (iii) Fishers exact test-based Strand bias (FS).
In addition, we added three novel attributes that are specific and unique forRNA-seq: (i) the percent of variant-supporting reads with variant positions in the first six bases of the reads (PctExtPos).
This is to model the false variants introduced during the random hexamer priming of the cDNA synthesis step during the RNA-seq library protocol.
The mismatches allowed between the hexamer primers and the RNA templates resulted in substantial amount of false variants (Fig.1 of the Supplementary Material); (ii) distance to the exon intron boundary (DJ); and (iii) the uniqueness of the read map-ping in the genome and transcriptome (ED).
More details are available in Supplementary Section 1.1.
2.2 Input, output and major modules RVboost takes an aligned RNA-seq BAM file [e.g.
the BAM file generated by TopHat (Trapnell et al., 2009)] and processes it through three major components (Fig.1): (i) Unified Genotyper from GATK for raw variant calling in the target region and generation of the annotations including all GATK classic annotations and the three novel attributes described above in Section 2.1; (ii) annotation of each variant with additional attributes, including all functional annotations from SnpEff (Cingolani et al., 2012), and whether the variant position is a known RNA-editing site according to a RNA-editing data-base (Ramaswami and Li, 2014); (iii) Variant prioritization and ranking using RVboost.
This module includes two components: a novel boosting method to train the variant classifying model using high-confidence variants (e.g.
common variants in HapMap as GATK recommended); and ranking of the likely true variants using a confidence score (details in the method section below).
The output is a VCF file of all variants, with full annotations.
Users can also generate a text file with selected attributes.
2.3 Description of the variant prioritization methods With the selection of six attributes, we formulated the variant prioritization as a ranking problem where only likely true vari-ants (e.g.
common variants from HapMap) are used for model training.
We describe it as a mathematical process to find a good F(.
), which outputs ranking score y from data X with minimum error defined by a loss function L(.
): FoptX=arg min F Ly;FX; 1 where elements of y is 1 or 0 to indicate likely true or false variants, respectively, X is a variant by attribute matrix used to rank true variants and arg min stands for the argument of the minimum error of the loss function.
Different from mixture model-based VQSR, in which the construction of F(.)
requires explicitly the number of Gaussian kernels and the percent of worst variants that are used as negative sets, we proposed to use a more flexible boosting method to rank variants.
Boosting methods construct such a function F(X) by additive combinations of M base learners h(X) (e.g.
linear regression models) FX= XM m=1 mhX; m, with corresponding combination coefficients m and parameters m of m-th learner (Friedman, 2001; B uhlmann andHothorn, 2007).
By leveraging implemented boosting methods in R package gbm, we chose three boosting options for which response variable values range from 0 to 1: adaboost with AdaBoost exponential loss function, bernoulli with logistic regression loss function, and huberized with mod-ified Huber loss function (Ridgeway, 2005).
It is often found that these three distribution options lead to similar performance and converge well before 20 000 iterations.
Hence, we chose AdaBoost model and 20 000 iterations as default settings for RVboost.
2.4 Expected recall rate and train-set quantile score After training, the user needs to choose a prioritization score as the threshold to call true variants.
In practice, it is often difficult to interpret a prioritization score derived from a complex computational model and its implications for precision/recall trade-off.
To address this problem, we explicitly defined a mono-tonic transformation independent of ranking methods, which depends on training set of likely true variants train-Qj=eCDFtrain-setscorej 2 where score[j] is score generated by method from high to low indicating the likelihood of the j-th variant to be a true variant.Fig.1.
The overall workflow of RVboost 3415 RVboost tiliz `` '' 3 1 ( 2 3 3 6-<inlinemediaobject><imageobject><imagedata fileref= <inlinemediaobject><imageobject><imagedata fileref= <inlinemediaobject><imageobject><imagedata fileref= `` '' ing ",0,0,2 ",0,0,2 ",0,0,2 ",0,0,2 ",0,0,2 ",0,0,2 very , , `` '' eCDFtrain-set is the empirical cumulative density function learnt from the training dataset.
The train-Q score is intuitive to users, as it directly uses the expected recall rate from the provided training dataset.
For example, a cutoff of the train-Q scores of 20% means that using this cutoff, 80% of the variants within the training set will be retained, i.e.
we have a 80% expected recall.
Through our comparison studies, we suggest a moderate expected recall rate, e.g.
90 or 95%, instead of an aggressive 99%, which is the recommended default by VQSR.
2.5 Comparison studies We compared specificity and sensitivity of RVboost to VQSR (GATK version 1.6.9) and SNPiR (Piskol et al., 2013b), using the concordance between RNA and DNA variants from eight follicular lymphoma tumor samples and four replicates of MCF-7 cell lines (details described in Supplementary Material 1.4).
To make unbiased comparisons, we evaluated the recall/ precision on a subset of novel variants that (i) are not in the positive training set and (ii) have at least 10-fold coverage in both RNA-seq and exome-seq.
We regarded the genotype calls from exome-seq as the ground truth and computed precision/ recall accordingly, under the assumption that RNA-editing sites are a small percentage of RNA-seq variants (Piskol et al., 2013a).
Overall, RVboost consistently outperforms both VQSR and SNPiR in all the tested samples in terms of AUC (Area Under the Curve) of precision/recall curves, and demonstrates superior precision in low train-Q score cutoffs, or equivalently, with high expected recall rates (details in Supplementary Material 2.2).
We also investigated the contribution of individual attribute to dis-tinguish true versus false variants, suggesting that the percent of reads supporting the variants in the first six base pairs and QD are the most informative features (details in Supplementary Material 2.3).
3 CONCLUSIONS We developed RVboost, a software package designed to reliably prioritize and call variants from RNA-seq data.
The output of our workflow provides comprehensive annotations to facilitate biological understanding.
Variant prioritization is based on a proposed boosting method, which not only outperforms two other methods (SNPiR and VQSR) in overall performance, but also provides great flexibility to users for adjusting of the preci-sion/recall trade-off, and it is superior to ad hoc hard-threshold approaches, such as SNPiR.
The major modules are wrapped as a comprehensive package.
Funding: Support for this work was provided by gift from Everett and Jane Hauck to the Center for Individualized Medicine at Mayo Clinic Jacksonville Florida, funds from the 26.2 with Donna Foundation and the proceeds of the National Marathon to Fight Breast Cancer and NIH P50 CA097274.
Conflict of interest: none declared.
ABSTRACT Summary: We present bammds, a practical tool that allows visualiza-tion of samples sequenced by second-generation sequencing when compared with a reference panel of individuals (usually genotypes) using a multidimensional scaling algorithm.
Our tool is aimed at deter-mining the ancestry of unknown samplestypical of ancient DNA dataparticularly when only low amounts of data are available for those samples.
Availability and implementation: The software package is available under GNU General Public License v3 and is freely available together with test datasets https://savannah.nongnu.org/projects/bammds/.
It is using R (http://www.r-project.org/), parallel (http://www.gnu.org/-software/parallel/), samtools (https://github.com/samtools/samtools).
Contact: bammds-users@nongnu.org Supplementary information: Supplementary data are available at Bioinformatics online.
Received on April 10, 2014; revised on June 16, 2014; accepted on June 23, 2014 1 INTRODUCTION Population structure plays an important role in determining the evolutionary history of a group.
A great deal has been learned from single nucleotide polymorphism (SNP) array technology providing unmatched information of the population structure of several species [for humans, see (Novembre and Ramachandran, 2011)].
The advent of new sequencing plat-forms, which can deliver millions to billions of sequencing reads within days, has shifted the focus from SNP array data to whole-genome shotgun (WGS) data.
While the cost has stead-ily decreased (Sboner et al., 2011), obtaining many high-depth genomes remains prohibitive for many laboratories, in particular when working with ancient DNA (aDNA) samples where it is often desirable to screen many samples of potential interest while keeping the cost at a minimum.
Methods based on non-parametric multidimensional statistics (more specifically principal components analysis, PCA) were first applied to genetic data more than 30 years ago (Menozzi et al., 1978).
PCA has since become a standard tool in population gen-etics (Patterson et al., 2006; Wang et al., 2014) owing in particu-lar to (i) the low computational demand of such analyses, (ii) the appealing graphical result and (iii) its ease of use.
Here, we describe a tool that allows to assign an ancestry to low-depth mapped WGS data when compared with an existing reference panel of genotype data using multidimensional scaling (MDS) based on genetic distances, a related method that pro-vides results similar to those of PCA (Cox and Cox, 2000).
2 METHODS In what follows, we assume that WGS data have been mapped to a reference genome and that files in BAM format are available (Li et al., 2009).
Calling genotypes for low-depth data is a challenging task (Nielsen et al., 2011), particularly for aDNA, as ancient damage (Briggs et al., 2007) and contamination are not incorporated into sequence data error models.
To avoid calling genotypes, we sample a read at every position for the WGS data, similar in spirit to previous aDNA approaches (Green et al., 2010).
Specifically, for the reference set of individuals, we randomly sample one of the alleles from each individual, and for the WGS data, we choose an allele from a randomly selected read covering that site.
If no read covers that site or if the sampled allele is not the minor or the major allele in the reference panel, we then assume that the data for this site are missing for that sample.
In other words, the data in both the reference panel and the WGS samples become either one allele (A, C, G or T) or missing data.
For site k, let dkij =1 if individuals i and j have a different randomly chosen allele and 0 if that allele is the same or if one of the individuals has missing data.
Assume that the number of sites in the reference panel is K. Denote ~Kij as the number of sites where neither of individual i and j have missing data.
Then, the allele-sharing distance between individuals i and j*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com https://savannah.nongnu.org/projects/bammds/population structure of , e.g., ) that c high 1 2 3 low to s ; low Note that i is s <inlinemediaobject><imageobject><imagedata fileref= <inlinemediaobject><imageobject><imagedata fileref= <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) IG(IG) IG(IG) allele IG(IG) IG(IG) XPath error Undefined namespace prefix XPath error Undefined namespace prefix is as follows: dij= 1 ~Kij XK k=1 dkij A matrix D=dij of allele-sharing distances between all pairs of individ-uals is computed.
We then apply classical MDS to this matrix [e.g.
(Cox and Cox, 2000)].
Our implementation has three major features: it is user friendly and is intended to be used by biologists with limited familiarity with a UNIX system, it is flexible in terms of formats of the reference panel and in terms of the visual output, it runs in 20 min on a machine with four 2.2GHz cores with a reference panel including 4600 000 SNPs and 950 individuals, making it practical to screen samples of an ongoing experiment pro-gressively as additional data are produced.
We first tested bammds through simulations using publicly available modern and ancient human data.
For the WGS data, we used 10 modern human genomes from HGDP cell lines, published in Meyer et al.
(2012), an Australian aboriginal genome (Rasmussen et al., 2011) and the Anzick-1 genome (Rasmussen et al., 2014).
We mapped and processed the data identically for all genomes (see Supplementary data).
We used a public reference panel that we make available in the Supplementary data, i.e.
HGDP (Li et al., 2008), which includes4600 000 SNPs and 950 individuals subdivided into 53 populations and 7 geographic regions (Africa, Eastern-, Western-, Central-and South Asia, Europe, Oceania and Native America).
For each genome, we sampled 3 104, 3 105, 3 106, 1:5 107, 3 107 and 1:5 108 reads (which corresponds to a depth of coverage around 0.001, 0.01, 0.1, 0.5, 1 and 5, assuming 100bp sequence reads).
For each sub-sampled genome, we ran bammds with the HGDP reference panel.
We summarized the simulation results using dimension 1 and 2 only of the MDS output, as we expect this to be the common usage.
For each population in HGDP, we defined its centroid (or center of gravity) based on the coordinates of its members for those two dimensions.
We then evaluated the results using two criteria: (i) by assessing which population was the closest when comparing the position of the WGS sample with the population centroid, and (ii) by determining if the position of the genome is within a two-dimensional 99% confidence region.
We built the confi-dence region by assuming that the points follow a bivariate normal dis-tribution centred around the centroid of the population to which it belongs (population ellipse).
We present a practical example on how to use the tool to determine whether a library is heavily contaminated by processing a newly sequenced 10 000 year BP old phalange (Gus) from Argentina that clusters with the Europeans (Supplementary data).
OOOOOOOOOOO O OOO A AA A AAA AAA A AAA AA A RR R RR R R RRR RRRR RRRR R RR R RR R B BBBBBBBB B B BBBBBBBBBBB BB F FFFFFFFF F FFFF FFFFFFF F FFFFFFI IIIIIIIIII IS SSS SSSSSSSSSSS SSS SSSSSSSSSS TTTT T T T MM M M M M MM M M M M MMMMM MM M M M M MMM M B B B B B B BBB BB B B B BB BB B B B BB BB B B B B B B B B B B B B B B BB BB B B DDDD DD D DD D DDDD DD DD D DDDDD D D DDD D D DDDD D DDDDD D PPP P P P P PP P P P PP P P P PPPP PPPP P PP P P P P P PP PPPP P P P P P PP B B B B B B B BB B B B B B B B BBB BBBB B BBB BBB B BB BB BB B B B B B B B B B B B B BBBBBBBB B B B BB BBB B BBBBB B BB H H HH H H H HHH H HH H H H HH H HH H KKK K KK KKKKKKKKKKKKKKKKK M MM M M M M M MM M M M M M M M M M M M M MM M PP PP P PP PPPP PP P P PPP P PP P S S SS S S S S S S SSSSS S S S S SS S S S MMMMMMMMMMMPPPP PPPPPPPPPPPPP C C C CCCC KKKKKKKK KKKKKSSSSSSSS M M M M M M M M M M M M M M M M MM M M M PPPP P PP P P P P P PP BB B B BBBB BBB B BB BB B BB M MMMM MMM MMMMM M MMMM M M MM YYYY Y YYYYY YYYYYYYYYYY BBB B BB BBBBB B B BBBBB B BBB MM M MMMMM MMMMM SSSSS HHH H HHH HH HHH HHH HHHHHHHHHHH H HHHHHHHHHHHHH HHHH D D DDDDDDDD D DD D D DDDDH HHH HHH HH LLLLLLLLM MMMMMMMMM O O OOOOOOO SSSSSSSSS STTTTTTT T TT TTTT TT T TT T X XXX X X XX X YYYY Y YY Y YY MMM M M M MM MMN NNNN NNN U U U U U U U UU U C C C C C C C CCC JJJJJ JJJJJJ JJJ JJJ J JJJJJJJJJ J YYY Y Y Y YY Y YYYY Y Y YYYYYY Y Y Y Y M F P S H Y K A N D M F P S H Y K A N D O A R B F I S T M B D P B B B H K M P S U M P C K S M P B B M Y B M S H H D D H L M O S T T X Y M N C J Y Mbuti French Papuan Sardinian Han Yoruba Karitiana San Mandenka Dai Orcadian Adygei Russian Basque French Italian Sardinian Tuscan Mozabite Bedouin Druze Palestinian Balochi Brahui Burusho Hazara Kalash Makrani Pathan Sindhi Uygur Melanesian Papuan Colombian Karitiana Surui Maya Pima BantuSAfrica BantuKenya Mandenka Yoruba BiakaPygmy MbutiPygmy San Han HanNChina Dai Daur Hezhen Lahu Miao Oroqen She Tujia Tu Xibo Yi Mongola Naxi Cambodian Japanese Yakut Fig.1.
First two dimensions of an MDS plot including the ten 0.1X modern human genomes and the HGDP SNP data 2963 bammds IG(IG) allele multidimensional scaling ( e.g, ) , utes ' is 8 ten , 8 9<inlinemediaobject><imageobject><imagedata fileref= <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) X X X X X X 0 since 1 to 2 e `` '' if ' yrs `` ''3 RESULTS The graphical result with all 10 modern individuals at a depth of 0.1 can be seen on Figure 1.
We find in the simulations that for all but two cases, we re-cover the geographic region as the first hit for as few as 30 000 reads (0.001, Table 1).
In the remaining two cases, the Sardinian and the Karitiana individual, a depth of 0.1 and 0.01, respectively, is enough.
The true nearest population was also identified in most cases within the three closest centroids for a depth above 0.01 (7/10 cases).
For the second criteria, we find that in 9/10 of the cases, the WGS sample was within the popu-lation ellipse at 0.5 and above.
Only in one case (San individ-ual) was a depth of 1 necessary to be placed within the population ellipse.
For the ancient data, we get similar results for the Aborigine, which is assigned to the correct geographic region (Oceania) as a first hit with a depth of 0.001 and above.
At a depth higher than 0.01, we also recover the expected population as the closest population.
For the Anzick-1 individual, presumably be-cause of increased damage, a depth of 1 is needed to recover the geographic region as the first hit.
On the other hand, a Native American population is among the three closest populations from a depth of 0.1 and above.
The results for Gus are given in Supplementary data.
4 CONCLUSION The tool we present in this article is based on classical MDS, a technique that originated in the 1930s and is commonly used in other fields [see, e.g.
(Borg and Groenen, 1997) and citations therein].
We present a tool that was designed to be practical to assess the ancestry of mapped WGS data for samples sequenced at low depth, assuming that a relevant reference panel in terms of ancestry is provided.
We show through simulations that useful ancestry information can be recovered for as few as 30 000 readscorresponding to a fraction (1/60 in early 2014) of a HiSeq 2000 lane (www.illumina.com) for a sample with 1% en-dogenous content (or 1/4800 of a lane for a typical modern sample).
ACKNOWLEDGMENTS The authors thank Mara C. Avila-Arcos, Amhed Missael Vargas Velazquez, Morten E. Allentoft, Hannes Schroeder, Kerttu Majander, Maanasa Raghavan and Johannes Krause for helpful discussions and testing, and the National high-throughput DNA Sequencing Center for assistance with the sequencing.
Funding: A.-S.M.
was supported by a Swiss NSF, J.V.M.-M. by the Consejo Nacional de Ciencia y Tecnologa (Mexico) and M.D.
by the US NSF (DBI-1103639).
GeoGenetics members were supported by the Lundbeck Foundation and the Danish National Research Foundation (DNRF94).
Conflict of Interest: none declared.
Abstract Proteinpeptide interactions, where one partner is a globular protein (domain) and the other is a flexible linear peptide, are key components of cellular processes predominantly in signal-ing and regulatory networks, hence are prime targets for drug design.
To derive the details of the proteinpeptide interaction mechanism is often a cumbersome task, though it can be made easier with the availability of specific databases and tools.
The Peptide Binding Protein Database (Pep-Bind) is a curated and searchable repository of the structures, sequences and experimental observa-tions of 3100 proteinpeptide complexes.
The web interface contains a computational tool, protein inter-chain interaction (PICI), for computing several types of weak or strong interactions at the pro-teinpeptide interaction interface and visualizing the identified interactions between residues in Jmol viewer.
This initial database release focuses on providing proteinpeptide interface informa-tion along with structure and sequence information for proteinpeptide complexes deposited in the Protein Data Bank (PDB).
Structures in PepBind are classified based on their cellular activity.
More than 40% of the structures in the database are found to be involved in different regulatory pathways and nearly 20% in the immune system.
These data indicate the importance of protein peptide complexes in the regulation of cellular processes.
PepBind is freely accessible at http://pepbind.bicpu.edu.in/.
hur PP).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A Introduction Functional analyses of proteins involve the exploration of their interactions with other molecules, which plays vital roles in dif-ferent pathways.
Nearly 60% of the interaction pathways such as signal transduction, apoptotic, immune system and other pathways contain domains with bound peptides [1].
These inter-actions are prevalent in Src homology 2 (SH2) domain, major histocompatibility complex (MHC), antibodies, proteases, cal-modulin, PapD chaperone and OppA (oligopeptide permease cademy of Sciences and Genetics Society of China.
Production and hosting 242 Genomics Proteomics Bioinformatics 11 (2013) 241246 A) structures, with variable sequence specificity and binding affinity [2].
Proteinpeptide interactions require only a small interface and can occur in many interaction networks.
Hence, these are attractive drug targets both for small molecules and inhibitory peptides [35].
This implies that synthetic peptides can be designed to alter specific interactions in disease or other pathways [1,6,7].
Out of the structures deposited in the Protein Data Bank (PDB) [8], every month around 20 new entries are shown to exhibit interactions with small peptides.
As the num-ber of new and interesting proteinpeptide complex structures continue to expand, our understanding of these proteinpeptide recognition events should improve.
To understand and analyze the proteinpeptide interactionmechanisms, a reliable database of proteinpeptide complexes is necessary.
A number of se-quence-based proteinpeptide interaction databases are avail-able, such as ELM [9], PhosphoELM [10], DOMINO [11], SCANSITE [12], PepBank [13], APD [14], ASPD [15] and BIO-PEP [16].
Structural data are also available on proteinpeptide complex structures in peptiDB [17] and PepX [18].
While pep-tiDB is a set of 103 curated PDB files for non-redundant proteinpeptide complexes, PepX contains 1431 non-redundant X-ray structures clustered based on their binding interfaces and backbone variations.
Previous studies report heterogeneity of domains or proteins to bind multiple peptides (e.g., at least 13 different types of peptides have been reported to bind to SH3do-mains [19]).
For detailed analysis of interactions of similar pro-teins with different peptides, an enormous amount of data concerning proteinpeptide complex structures are needed.
To address this problem, we have created the Peptide Binding Pro-tein Database (PepBind), which contains 3100 available protein Figure 1 Interactions between the complex of Apopain with the tetrap Hydrogen bonds (A), hydrophobic interaction (B) and ionic interact colored in brown.
structures from the PDB, irrespective of the structure determi-nation methods and similarity in their protein backbone.
Different kinds of interactions have been noted in the stabil-ization of proteinpeptide binding.
Analyses of various interact-ing interfaces between linear peptide and protein domains help us in distinguishing transient and permanent complexes [20 22].
It has been demonstrated that protein-peptide interfaces contain more hydrogen bonds per 100 A2 solvent accessible sur-face area (ASA) (i.e., 50% more than proteinprotein interac-tions and 100% more than intrinsically-unstructured regions to protein interactions) [17].
The importance of other interac-tions such as interactions between nonpolar hydrophobic amino acid residues and ionic interactions in the structure and function of proteins is also well known [23,24].
Knowing the importance of proteinpeptide interface hydrogen bonds and other kinds of interactions, we developed and integrated a web-based interac-tion tool, protein inter-chain interaction (PICI), which calcu-lates all the interface hydrogen bonds along with other interactions (such as disulfide bonds, hydrophobic interactions and ionic interactions) in tertiary structures of proteinpeptide complexes and can be visualized with an integrated Jmol [25] viewer.
Although a similar tool, Protein Interaction Calculator (PIC) [26], has been available, this tool calculates interface inter-actions specific for the peptide chain of a proteinpeptide com-plex structure and visualizes them in a single web page along with highlighted interacting residues on sequences.We have also developed a binding prediction server built in PepBind (http://pepbind.bicpu.edu.in/PepBind_prediction_beta.php) to predict the possible protein domains in the PepBind database that may bind the user-defined peptide sequence.
eptide inhibitor ACE-DVA-ASK (PDB ID: 1CP3) ions (C) were identified by PICI server.
Interacting residues are Das AA et al/ PepBind: Proteinpeptide Database with PICI Tool 243 Results The PepBind database provides researchers with residue and atomic-level information about sequences and structures of proteinpeptide complexes and their interfaces, helping in the analysis of proteinpeptide interactions by computing various interface interactions and by providing structural information both interactively on screen and in a text format (Figure 1).
The PepBind database also maintains a repository of structure coordinate files, PDBML [27] data files and proteinpeptide interaction files generated by PICI tool.
The database is up-dated on a regular basis to serve as a resource for structural, functional and proteinpeptide interaction studies of peptide-binding proteins.
Researchers can also submit proteinpeptide complexes to the database, which will be uploaded to PepBind after manual verification.
Database statistics As shown in Table 1, current version of PepBind contains structural information for a total of 3100 proteinpeptide com-plexes.
Based on cellular activity, 1745 complexes of all the 3100 proteins (56.3%) are involved in regulatory pathways, along with inhibitory complexes.
Our study shows 1278 struc-tures (41.2%) in the database play major roles in hormonal activity, gene regulation, transcription and signal transduction pathways along with transferases.
Furthermore, 600 structures (19.3%) in the database are found to function in the immune system.
It has been found that 252 proteins (8.1%) are struc-tural, contractile and membrane proteins involved mainly in transport (5.2%) and cell adhesion (1.9%).
In addition, 953 (30.7%) structures have protease or other hydrolase activities, Table 1 Contents of the PepBind database Cellular activity No.
of complexes (%) Cell cycle 90 (2.9) Structural proteins 126 (4.0) Cell adhesion 59 (1.9) Transporta 163 (5.2) Calmodulin (CaM) 42 (1.3) Apoptosis 125 (4.0) Signaling 626 (20.2) Hormones 84 (2.7) Transferasesb 415 (12.7) Transcription 268 (8.6) Gene regulation 38 (1.2) Inhibitory complex 663 (21.4) MHC 340 (10.9) Immunoglobulin (Ig) 250 (8.0) Antibiotics 15 (0.5) Other immune system proteins 98 (3.1) Proteases 687 (22.1) Other hydrolases 266 (8.5) Others 326 (10.5) Note: There are totally 3100 proteinpeptide complexes in PepBind.
Since categories.
a Transporters, channels and pumps; b Transferases along with while 10.5% structures in the database are associated with pro-teins involved in other cellular activities.
Web interface The user interface has been developed for browsing through all the contents of the database as a list or by different categories (Figure 2).
For the ease of users to search and access data, we have integrated many search tools (Figure 2A) into the web interface.
Using the simple search function, users can retrieve information about proteinpeptide complexes using their PDB ID or protein name.
Our keyword search tool scans all the fields of all the tables in PepBind for the matched word and re-turns a list of all protein structures related to the query.
Using the advanced search function, users can filter search based on peptide length, cellular activity of proteins, structure determi-nation methods (e.g., X-ray diffraction, nuclear magnetic res-onance and electron microscopy) and authors contributing to solving protein structure.
All these search options with their parameters are joined by AND operator for an intensive search.
Additionally, to find any protein sequences homolo-gous to the sequence submitted, we provide BLAST searching [28] against PepBind/PDB/SwissProt.
The web interface for the output result has been designed to show all the chains present in the protein structure (Figure 2B).
Each chain is linked to the PICI web tool for analyzing its interactions with other chains of the protein.
This tool shows the interaction details by highlighting the corresponding inter-acting residues in the displayed sequence along with the Jmol visualization tool for the identified interactions between the residues (Figure 2C).
Different tab viewers have been designed for various types of interactions.
The protein detail page shows information about protein complex on a single web page under Functional category No.
of complexes (%) Structural, contractile and membrane proteins 252 (8.1) Regulatory proteins 1278 (41.2) Inhibitory complexes 663 (21.4) Immune system 600 (19.3) Proteases and other hydrolases 953 (30.7) Others 326 (10.5) some proteins are multi-functional, there are overlaps among different kinase, phosphomutase, transaldolase and transketolase.
Figure 2 Snapshots of PepBind output A.
Search page with search parameters.
B.
Result summary page showing all the chains with their sequence.
C. Jmol showing protein peptide interface and sequence viewer showing protein chains with identified residues highlighted.
D. Detailed result page displaying summary of the protein and other tab options.
244 Genomics Proteomics Bioinformatics 11 (2013) 241246 different tabs (Figure 2D), such as summary, sequence and source, gene ontology, methodology, Ramachandran plot, citation and external links.
While the sequence and source tab displays amino acid sequence in different colors as per their biochemical properties along with source organism data, the Ramachandran plot tab shows the Ramachandran plot im-age developed by the MolProbity [29] server, and the Gene Ontology tab shows GO functional annotation [30].
For a structure similarity search, we take advantage of the web ser-vice of PDB, which employs the FATCAT algorithm [31] to recognize homologous domains available at PepBind, SCOP [32] and PDP [33].
Discussion Proteinpeptide interactions are the key components of cellu-lar processes such as signal transduction, protein trafficking, defense mechanisms and enzyme regulation.
Various databases are available on protein interactions.
They can be grouped as protein-small molecule, protein-nucleic acid and protein protein interaction databases.
However, the retrieval of struc-tural and functional information of proteinpeptide interac-tions in biological processes is tedious due to the lack of specific databases to provide such details.
The establishment of the PepX database has resolved the difficulty of unavailabil-ity of a proteinpeptide interaction database, whereby authors have classified the proteins based on backbone variations and binding interfaces.
While in PepX, grouping is based solely on 3D similarity, PepBind complements PepX by providing inter-face information for both the peptide and protein chains of the complexes along with their cellular functions and options for sequence and structure similarity searches.
PepBind is inte-grated with the Jmol viewer to visualize the interface residues along with the interaction files generated by the PICI tool.
Furthermore, PepBind provides BLAST search and structure similarity search for protein chains.
It also provides a predic-tion service for binding of user-given peptides to possible pro-tein domains present in the PepBind database.
Links to other related databases and servers for the queried protein are provided for further analysis of the structures.
These resources include PDB [8], PDBsum [34], Pfam [35], Das AA et al/ PepBind: Proteinpeptide Database with PICI Tool 245 CASTp [36], OCA Browser (http://bip.weizmann.ac.il/oca/), PSI/KB (http://sbkb.org/kb/), SRS [37], MMDB [38], PQS [39], SCOP [32], CATH [40], Proteopedia [41], Jena Library [42] and UniProt [43].
Currently our interaction tool PICI is capable of analyzing inter-chain interactions like hydrogen bond, disulfide bridge, hydrophobic interaction and ionic interaction.
Keeping in view the importance of other weak interactions in stabilizing the protein structure, we plan to improve our tool to study inter-actions such as aromatic-aromatic interactions [44], cation-pi interactions [45] and aromatic-sulfur interactions [46].
In addi-tion, the current interaction tool capabilities will be extended to user-submitted structures, allowing for examination of interfaces in complexes currently not present in the PepBind.
Methods Data collection and curation Files for atomic coordinate (pdb files version 3.30), sequences (fasta files) and other data (pdbml files version 4.0) of 3100 proteinpeptide complexes in the PDB were downloaded fol-lowing a thorough manual screening of all the available struc-tures in the PDB.
Because PepBind intends to be a comprehensive collection of proteinpeptide complexes from the PDB, the database contains all the available proteinpep-tide complexes, irrespective of their sequence or structure redundancy.
Classification of all the collected structure data was done in three steps: (I) an automated program to scan the amino acid sequences and classify them based on length of the bound peptide, (II) manual curation for the cellular activity of the complexes through study of the literature and (III) an automated program to read the data file and group the complexes as per their structure determination methods.
Functionality has been analyzed through literature studies and classified as proteins involved in different cellular activities and grouped in 19 categories.
Database schema and implementation The PepBind database consists of a series of server-side scripts written in the PHP programming language with HTML and JavaScript for user interface functions, which runs on the Apache 2.2 web server, using MySQL 5.1 as a database back-end.
Atomic coordinate information from the PDB and other related information from other remote databases and web servers were mined through an automated program and stored in a file repository for further processing.
We developed sets of PHP scripts for operating with the available data and process them for easy integration in the database and front-end user interface.
The first set of scripts reads the PDBML files [27], extracts the data, and inserts them into the database tables; the second set sorts these data with respect to each attri-bute and the third set generates web pages with specific infor-mation about individual complexes.
Utilities and tools The PICI tool for depicting potential hydrogen bonds and other interactions between the short peptide and core protein was developed and integrated into PepBind.
This tool parses the structure coordinate files, removes the hetero atoms and water molecules, and predicts the interaction based on coordi-nate distance between atoms of amino acid residues of small peptide and the protein.
For structures determined by NMR, the first model in the file is taken for calculation by PICI tool.
For the two atoms A(x1, y1, z1) and B(x2, y2, z2), linear distance D is calculated as per the Euclidean distance equation D(A, B) = p {(x1 x2)2 + (y1 y2)2 + (z1 z2)2}.
Various potential interactions are calculated based on stan-dard and published criteria.
The hydrogen bond is detected if the distance between oxygen or nitrogen atoms of the peptide and the protein domain is 63.5 A [47].
Interactions between hydrophobic residues (such as alanine, valine, leucine, isoleu-cine, methionine, phenylalanine, tryptophan, proline and tyro-sine) [48] have been predicted if they fall within 5 A range.
Apart from these interactions, ionic residue (arginine, lysine, histidine, aspartic acid and glutamic acid) pairs falling within 6 A contribute to ionic interactions.
The tool with integrated Jmol viewer shows various interactions between the peptide and the amino acid residues of the interacting protein chains.
Moreover, it highlights the positions of interacting amino acid residues on the displayed sequence (Figure 2D).
This tool also generates an interaction file for each type of interactions.
A sequence modification tool has been developed and incorporated into the result page, which can read the protein sequence file and color the amino acid sequence (using single letter code) of protein according to their biochemical proper-ties (such as green for non-polar hydrophobic amino acids, yel-low for uncharged polar amino acids, blue for positively charged amino acids, red for negatively charged amino acids and black for non standard amino acids).
A web-based predic-tion server has been provided to find the protein domains pres-ent in the database that likely bind to the user-given peptide.
The sequence search tool present in the web interface allows users to BLAST search the queried sequence in the database using various parameters.
All data related to structure, sequence and interface interac-tions currently in the PepBind database have been made avail-able for further analysis.
These files along with the complete list of the PepBind dataset can be downloaded freely from our database.
A reporting tool has been integrated to generate the result in a printer-friendly PDF file.
Authors contributions PPM, RK and MSK conceived and designed the project.
AAD collected the data, developed the database, developed the tools and designed the website.
OPS developed the BLAST search script.
AAD and OPS wrote the manuscript.
All authors read and approved the final manuscript.
Competing interests The authors have no competing interests to declare.
Acknowledgements This work is supported by the Department of Biotechnology (Grant No.
BT/BI/03/015/2002) and Department of 246 Genomics Proteomics Bioinformatics 11 (2013) 241246 Information Technology (Grant No.
DIT/R&D/15 (9)2007), Government of India.
ABSTRACT Motivation: Testing for correlations between different sets of genomic features is a fundamental task in genomics research.
However, searching for overlaps between features with existing web-based methods is complicated by the massive datasets that are routinely produced with current sequencing technologies.
Fast and flexible tools are therefore required to ask complex questions of these data in an efficient manner.
Results: This article introduces a new software suite for the comparison, manipulation and annotation of genomic features in Browser Extensible Data (BED) and General Feature Format (GFF) format.
BEDTools also supports the comparison of sequence alignments in BAM format to both BED and GFF features.
The tools are extremely efficient and allow the user to compare large datasets (e.g.
next-generation sequencing data) with both public and custom genome annotation tracks.
BEDTools can be combined with one another as well as with standard UNIX commands, thus facilitating routine genomics tasks as well as pipelines that can quickly answer intricate questions of large genomic datasets.
Availability and implementation: BEDTools was written in C++.
Source code and a comprehensive user manual are freely available at http://code.google.com/p/bedtools Contact: aaronquinlan@gmail.com; imh4y@virginia.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on November 24, 2009; revised on January 11, 2010; accepted on January 21, 2010 1 INTRODUCTION Determining whether distinct sets of genomic features (e.g.
aligned sequence reads, gene annotations, ESTs, genetic polymorphisms, mobile elements, etc.)
overlap or are associated with one another is a fundamental task in genomics research.
Such comparisons serve to characterize experimental results, infer causality or coincidence (or lack thereof) and assess the biological impact of genomic discoveries.
Genomic features are commonly represented by the Browser Extensible Data (BED) or General Feature Format (GFF) formats and are typically compared using either the UCSC Genome Browsers (Kent et al., 2002) Table Browser or using the Galaxy (Giardine et al., 2005) interface.
While these tools offer a convenient and reliable method for such analyses, they are not amenable to large and/or ad hoc datasets owing to the inherent need to interact with a remote or local web site installation.
Moreover, complicated To whom correspondence should be addressed.
analyses often require iterative testing and refinement.
In this sense, faster and more flexible tools allow one to conduct a greater number and more diverse set of experiments.
This necessity is made more acute by the data volume produced by current DNA sequencing technologies.
In an effort to address these needs, we have developed BEDTools, a fast and flexible suite of utilities for common operations on genomic features.
2 FEATURES AND METHODS 2.1 Common scenarios Genomic analyses often seek to compare features that are discovered in an experiment to known annotations for the same species.
When genomic features from two distinct sets share at least one base pair in common, they are defined as intersecting or overlapping.
For example, a typical question might be Which of my novel genetic variants overlap with exons?
One straightforward approach to identify overlapping features is to iterate through each feature in set A and repeatedly ask if it overlaps with any of the features in set B.
While effective, this approach is unreasonably slow when screening for overlaps between, for example, millions of DNA sequence alignments and the RepeatMasker (Smit et al., 19962004) track for the human genome.
This inefficiency is compounded when asking more complicated questions involving many disparate sets of genomic features.
BEDTools was developed to efficiently address such questions without requiring an installation of the UCSC or Galaxy browsers.
The BEDTools suite is designed for use in a UNIX environment and works seamlessly with existing UNIX utilities (e.g.
grep, awk, sort, etc.
), thereby allowing complex experiments to be conducted with a single UNIX pipeline.
2.2 Language and algorithmic approach BEDTools incorporates the genome-binning algorithm used by the UCSC Genome Browser (Kent et al., 2002).
This clever approach uses a hierarchical indexing scheme to assign genomic features to discrete bins (e.g.
16 kb segments) along the length of a chromosome.
This expedites searches for overlapping features, since one must only compare features between two sets that share the same (or nearby) bins.
As illustrated in Supplementary Figure 1, calculating feature overlaps for large datasets (e.g.
millions of sequence alignments) is substantially faster than using the tools available on the public Galaxy web site.
The software is written in C++ and supports alignments in BAM format (Li et al., 2009) through use of the BAMTools libraries (Barnett et al., http://sourceforge.net/projects/bamtools/).
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:28 19/2/2010 Bioinformatics-btq033.tex] Page: 842 841842 AR.Quinlan and I.M.Hall 2.3 Supported operations Table 1 illustrates the wide range of operations that BEDTools support.
Many of the tools have extensive parameters that allow user-defined overlap criteria and fine control over how results are reported.
Importantly, we have also defined a concise format (BEDPE) to facilitate comparisons of discontinuous features (e.g.
paired-end sequence reads) to each other (pairToPair), and to genomic features in traditional BED format (pairToBed).
This functionality is crucial for interpreting genomic rearrangements detected by paired-end mapping, and for identifying fusion genes or alternative splicing patterns by RNA-seq.
To facilitate comparisons with data produced by current DNA sequencing technologies, intersectBed and pairToBed compute overlaps between sequence alignments in BAM format (Li et al., 2009), and a general purpose tool is provided to convert BAM alignments to BED format, thus facilitating the use of BAM alignments with all other BEDTools (Table 1).
The following examples illustrate the use of intersectBed to isolate single nucleotide polymorphisms (SNPs) that overlap with genes, pairToBed to create a BAM file containing only those alignments that overlap with exons and intersectBed coupled with samtools to create a SAM file of alignments that do not intersect (-v) with repeats.
$ intersectBed-a snps.bed-b genes.bed > out.bed $ pairToBed-abam reads.bam-b exons.bed > out.bam $ intersectBed-abam reads.bam-b repeats.bed-v | samtools view-> reads.noRepeats.sam Other notable tools include coverageBed, which calculates the depth and breadth of genomic coverage of one feature set (e.g.
mapped sequence reads) relative to another; shuffleBed, which permutes the genomic positions of BED features to allow calculations of statistical enrichment; mergeBed, which combines overlapping features; and utilities that search for nearby yet non-overlapping features (closestBed and windowBed).
BEDTools also includes utilities for extracting and masking FASTA sequences (Pearson and Lipman, 1988) based upon BED intervals.
Tools with similar functionality to those provided by Galaxy were directly compared for correctness using the knownGene and RepeatMasker tracks from the hg19 build of the human genome.
The results from all analogous tools were found to be identical (Table 1).
2.4 Other advantages Except for the novel paired-end functionality and support for alignments in BAM format, many of the genomic comparisons supported by BEDTools can be performed in one way or another with available web-based tools.
However, BEDTools offers several important advantages.
First, it can read data from standard input and write to standard output, which allows complex set operations to be performed by combining BEDTools operations with each other or with existing UNIX utilities.
Second, most of the tools can distinguish DNA strands when searching for overlaps, which allows orientation to be considered when interpreting paired-end mapping or RNA-seq data.
Third, the use of BEDTools mitigates the need to interact with local or public instances of the UCSC Genome Browser or Galaxy, which can be a major bottleneck when working with large genomics datasets.
Finally, the speed Table 1.
Summary of supported operations available in the BEDTools suite Utility Description intersectBed* Returns overlaps between two BED files.
pairToBed Returns overlaps between a BEDPE file and a BED file.
bamToBed Converts BAM alignments to BED or BEDPE format.
pairToPair Returns overlaps between two BEDPE files.
windowBed Returns overlaps between two BED files within a user-defined window.
closestBed Returns the closest feature to each entry in a BED file.
subtractBed* Removes the portion of an interval that is overlapped by another feature.
mergeBed* Merges overlapping features into a single feature.
coverageBed* Summarizes the depth and breadth of coverage of features in one BED file relative to another.
genomeCoverageBed Histogram or a per base report of genome coverage.
fastaFromBed Creates FASTA sequences from BED intervals.
maskFastaFromBed Masks a FASTA file based upon BED coordinates.
shuffleBed Permutes the locations of features within a genome.
slopBed Adjusts features by a requested number of base pairs.
sortBed Sorts BED files in useful ways.
linksBed Creates HTML links from a BED file.
complementBed* Returns intervals not spanned by features in a BED file.
Utilities in bold support sequence alignments in BAM.
Utilities with an asterisk were compared with Galaxy and found to yield identical results.
and extensive functionality of BEDTools allow greater flexibility in defining and refining genomic comparisons.
These features allow for diverse and complex comparisons to be made between ever-larger genomic datasets.
ACKNOWLEDGEMENTS We thank Royden Clark for helpful algorithmic advice.
Funding: Ruth L. Kirschstein National Research Service Award from the National Institutes of Health [1F32HG005197-01 to A.R.Q.
]; a Burroughs Wellcome Fund Career Award to I.M.H.
; National Institutes of Health Directors New Innovator Award [DP2OD006493-01 to I.M.H.].
Conflict of Interest: none declared.
ABSTRACT Motivation: The availability of modern sequencing techniques has led to a rapid increase in the amount of reconstructed metabolic networks.
Using these models as a platform for the analysis of high throughput transcriptomic, proteomic and metabolomic data can provide valuable insight into conditional changes in the metabolic activity of an organism.
While transcriptomics and proteomics provide important insights into the hierarchical regulation of metabolic flux, metabolomics shed light on the actual enzyme activity through metabolic regulation and mass action effects.
Here we introduce a new method, termed integrative omics-metabolic analysis (IOMA) that quantitatively integrates proteomic and metabolomic data with genome-scale metabolic models, to more accurately predict metabolic flux distributions.
The method is formulated as a quadratic programming (QP) problem that seeks a steady-state flux distribution in which flux through reactions with measured proteomic and metabolomic data, is as consistent as possible with kinetically derived flux estimations.
Results: IOMA is shown to successfully predict the metabolic state of human erythrocytes (compared to kinetic model simulations), showing a significant advantage over the commonly used methods flux balance analysis and minimization of metabolic adjustment.
Thereafter, IOMA is shown to correctly predict metabolic fluxes in Escherichia coli under different gene knockouts for which both metabolomic and proteomic data is available, achieving higher prediction accuracy over the extant methods.
Considering the lack of high-throughput flux measurements, while high-throughput metabolomic and proteomic data are becoming readily available, we expect IOMA to significantly contribute to future research of cellular metabolism.
Contacts: kerenyiz@post.tau.ac.il; tomersh@cs.technion.ac.il 1 INTRODUCTION Modern genome-sequencing capabilities have led to the generation of genome-scale metabolic network reconstructions for many microorganisms, giving rise to more than 50 highly curated metabolic reconstructions that have been published to date (Duarte et al., 2004; Feist and Palsson, 2008).
A metabolic network reconstruction is composed of a set of biochemical reactions, and the associations between these reactions and their enzyme-coding genes.
The constraint-based modeling (CBM) computational framework serves to analyze the functionality of these genome scale To whom correspondence should be addressed.
models, enabling the prediction of various metabolic phenotypes in microorganism such as growth rates, nutrient uptake rates, by-product secretions and gene essentiality (Price et al., 2004).
CBM has been used for a variety of applications including the comparative metabolic analyses over multiple organisms (Blank et al., 2005; Lee et al., 2009), drug discovery (Gordana et al., 2005), metabolic flux analysis (Rantanen et al., 2008), studies of network evolution (Fong et al., 2005) and metabolic engineering tasks (Pharkya et al., 2004).
Using metabolic models as scaffolds for the analysis of high throughput data such as transcriptomics, proteomics and metabolomics suggests the possibility of inferring condition-dependent changes in the metabolic activity of an organism.
Developing computational methods capable of predicting metabolic flux by integrating these data sources with a metabolic network is hence a major challenge of metabolic network modeling.
Previous investigations have already utilized CBM to integrate high-throughput molecular datasets with a metabolic network in a qualitative manner: The methods developed by kesson et al.
(2004) and Becker and Palsson (2008) use gene expression data to identify genes that are absent or likely to be absent in certain contexts.
They then search for metabolic states that prevent (or minimize) the flux through the associated metabolic reactions.
A recent method by Shlomi et al.
(2008) considers data on both lowly and highly expressed genes in a given context, as cues for the likelihood that their associated reactions carry metabolic flux and employs CBM to accumulate these cues into a global, consistent prediction of the metabolic state.
The lack of dependency on a cellular objective is a marked advantage of this approach as the latter is difficult to define for multi-cellular organisms.
While transcriptomics and proteomics data provides important insight into hierarchical regulation of metabolic flux (representing the control over the maximum activity of enzymesi.e.
vmax), metabolomics may provide information on an additional level of regulation called, metabolic regulation (Rossell et al., 2006).
The latter denotes the effect of metabolite concentrations on actual enzyme activity through mass action, kinetic and allosteric effects.
A previous CBM method for integrating metabolomic data with a metabolic network model, thermodynamic-based metabolic flux analysis (TMFA) (Henry et al., 2007), derives constraints on reaction directionality from metabolite concentration data based on thermodynamic principles.
Another method by Cakir et al.
(2006) integrates quantitative metabolome data with genome-scale models to identify reporter reactions, defined as the set of reactions that respond to genetic or environmental perturbations through coordinated variations in the levels of surrounding metabolites.
Currently, however, there is no CBM method that enables the The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:46 12/5/2010 Bioinformatics-btq183.tex] Page: i256 i255i260 K.Yizhak et al.
integration of quantitative metabolomic data with a metabolic network model to directly infer the metabolic fluxes themselves.
In this article we introduce a novel CBM method, integrative omics-metabolic analysis (IOMA), for integrating quantitative proteomic and metabolomic data with genome-scale metabolic network models to predict metabolic flux.
This is achieved primarily by considering a mechanistic model of reaction rates.
The method is formulated as a quadratic programming (QP) problem (Nocedal and Wright, 2006) geared to find a feasible, steady-state flux distribution, such that: (i) a set of stoichiometric mass-balance and enzymatic directionality constraints are satisfied; (ii) the flux through a core set of reactions for which measured proteomic and metabolomic data exists is as consistent as possible with flux estimations derived via Michaelis Menten-like kinetic rate equations for these reactions.
The latter involves the estimation of missing enzyme kinetic constants, by searching for optimal parameters as part of the optimization problem, as described below.
To examine the predictive performance of IOMA, we applied it to predict metabolic flux for red blood cells (RBC) for which a detailed kinetic model is available for validation (utilizing it to simulate metabolic flux changes following gene knockouts based on artificially generated proteomic data).
As a further validation, we applied IOMA to predict metabolic flux for Escherichia coli under different gene knockouts, utilizing available metabolomic and proteomic data as input, and available experimental fluxes for validation (Ishii, 2007).
A comparison of IOMAs performance to that of the commonly used methods of flux balance analysis (FBA) (Fell and Small, 1986; Kauffman et al., 2003; Varma and Palsson, 1994) and minimization of metabolic adjustment (MOMA) (Segre et al., 2002) shows the significant advantage of IOMA in both validation tests.
2 METHODS 2.1 CBM of metabolic network A metabolic network consisting of n metabolites and m reactions can be represented by a stoichiometric matrix, denoted by N , where the entry nij represents the stoichiometric coefficient of metabolite i in reaction j (Price et al., 2004).
A CBM model imposes mass balance, directionality and flux capacity constraints on the space of possible fluxes in the metabolic networks reactions through the set of linear equations: Nv=0, (1) vlb v vub, (2) where v stands for the flux vector for all of the reactions in the model (i.e.
the flux distribution).
The exchange of metabolites with the environment is represented as a set of exchange reactions, enabling for a pre-defined set of metabolites to be either taken up or secreted from the growth media.
The steady-state assumption represented in Equation (1) constrains the production rate of each metabolite to be equal to its consumption rate.
Enzymatic directionality and flux capacity constraints define lower and upper bounds on the fluxes and are embedded in Equation (2).
In the following, flux vectors satisfying these conditions will be referred to as feasible stead-state flux distributions.
2.2 The IOMA method To associate quantitative measurements of protein and metabolite levels with metabolic fluxes, we used the following Michaelis Menten-like rate equation to estimate the flux v in a reaction transforming a set of substrates S to a set of products P: v=k+cat e sis ( si km,si +si ) kcat e pip ( pi km,pi +pi ) , (3) where Si and Pi denote the concentrations for the i-th substrate and i-th product, respectively, and e denotes the enzyme concentration.
km,si and km,pi denote the dissociation constants for the i-th substrate and i-th product, respectively.
Enzyme turnover rates in the forward and backward directions are denoted by k+cat and k cat, respectively.
Given the substrate and product metabolites concentration and their dissociation constants, the following saturation values for the enzyme in the forward and backward directions can be computed as following: a+ = sis ( si km,si +si ) , a = pip ( pi km,pi +pi ) .
Given the above definitions, the rate Equation (3) takes the following form: v=e(a+kcat akcat).
(4) To account for proteomics data that reflect relative protein levels compared to some reference state (rather than absolute protein concentrations), Equation (4) can be rewritten as following: v= e eref ( a+v+max avmax ) , (5) where eref denotes the enzyme concentration in the reference state, and v+max and vmax are equal to the corresponding enzyme turnover rates multiplied by eref.
Hence, in order to predict the metabolic flux through a certain enzyme given relative concentration level and saturation coefficients (computed given absolute metabolite levels and metabolite dissociation constants), the corresponding vmax value of the enzyme is also required.
We describe a new CBM method that addresses the problem of predicting genome-scale metabolic flux distributions, vj , for a set of k growth conditions (j=1, ... ,k), given (Fig.1): (i) relative enzyme concentrations for a core set of enzymes (denoted E), under the various conditions, eji/e ref i , where ( j=1, ... ,k) and iE; (ii) absolute substrate and product metabolite concentrations for enzymes in E; (iii) metabolite dissociation constants.
Concentrations and dissociation constants together enable us to compute enzyme saturation values, aj+i and a j i where j = 1, ... ,k, and iE.
The method is formulated as a QP problem that searches for k flux distributions, such that: (i) each flux distribution satisfies stoichiometric mass-balance and reaction directionality constraints, (ii) the fluxes through the core reactions (for which proteomic and metabolomic data is given) are as consistent as possible with the estimated rates calculated via Equation (5).
The latter is facilitated by searching for the v+max,i and v max,i parameters for all enzymes iE as part of the QP optimization.
IOMAs QP problem is formulated as following: min vj ,v+max,vmax,ji iE var ( i ) , s.t.
Nv j =0, j=1,...k, (6) v lb v vub, j=1,...k, (7) v ji = eji erefi ( aj+i v + max,i aji vmax,i +ji ) , j=1,...k, (8) where Equation (6) represents mass-balance constraint for the k-th flux distributions, Equation (7) represents reaction directionality and flux capacity constraints, and Equation (8) represents the estimated fluxes for the core reactions based on the proteomic and metabolomic data.
To account for missing concentration levels of substrate and product metabolites for some enzymes, the presence of noise in both the proteomic and metabolomic data, and the simplifying assumptions employed in the rate equation formalism, the i256 [11:46 12/5/2010 Bioinformatics-btq183.tex] Page: i257 i255i260 Integrating quantitative proteomics and metabolomics Fig.1.
The figure illustrates the associations between variables in IOMAs optimization problem imposed by the various constraints.
Rows represent enzymes and columns represent growth conditionsi.e.
the j-th column representing the flux distribution for the j-th condition (denoted by vj .
Mass-balance [Equation (6)] and reaction directionality [Equation (7)] create dependencies between fluxes through different enzymes in one condition, irrespectively of all others conditions (i.e.
associating fluxes in one column).
The enzyme-kinetic constraint [Equation (8)] associates between fluxes through one enzyme in different growth conditions (via the enzymes parameters v+max and vmax, which are condition-invariant), irrespectively of all other enzymes (i.e.
associating fluxes in one row).
The latter constraint is defined only for a core set of enzymes for which metabolomic and proteomic data is available (marked in blue).
The union of both types of row and column constraints in IOMAs optimization indirectly associates between many additional fluxes through various enzymes in different conditions.
error ji variables were added to Equation (8), guaranteeing a feasible solution for the QP problem.
The optimization is hence formulated to minimize the total sum of variance in the error variables for each enzyme across the k conditions.
We chose to minimize the variance of the error variables (and not their total sum) to account for potential metabolic regulation mechanisms that are not explicitly incorporated in the model (e.g.
allosteric regulation) and may systematically affect the metabolic flux.
The application of IOMA in this article considers additional constraints by utilizing additional datasets given as input for each knockout condition: first, for a knockout condition j of an enzyme-coding gene that is not backed up by isozymes in the model, the flux through the corresponding reactions was constrained to zero via the following constraint: vjko =0, j=1,...,k. Second, the organisms growth rate (denoted vGR) in a knockout condition j is given and is used to constrain the biomass yield rate: vjbiomass =vGR, j=1,...,k. Third, experimentally measured exchange fluxes (uptake and secretion rates for several metabolites) were further used to constrain the predicted flux distributions via a two-step procedure: (i) we applied QP for each condition j ( j = 1, ... ,k) in order to find a feasible steady-state flux distribution vj for which the Euclidean distance to the given exchange fluxes is minimized; (ii) we added the following constraints to IOMAs optimization problem, so that the exchange fluxes are fixed at the values predicted in (i): vjex = vjex, j=1,......k. The commercial CPLEX solver was used for solving QP problems, on 64-bit machines running Linux.
Matlab implementation of IOMA can be found at www.cs.technion.ac.il/tomersh/tools.
Fig.2.
Precision, recall and accuracy of predicted changes in fluxes between the wild-type and knockout strains in the RBC model, obtained by IOMA (blue) and MOMA (red).
The average and standard deviation of the precision, recall and accuracy are shown across the 50 simulation runs in two scenarios.
(a) No flux data is given as input for MOMA and IOMA, hence MOMA relies on random sampling of possible wild-type flux distributions to predict knockout effects.
(b) Exchange fluxes are given as input to both methods, and are used by MOMA to obtain a more reliable prediction of the wild-type flux distribution.
In both test scenarios, IOMAs predictions are significantly more accurate.
3 RESULTS 3.1 Predicting gene knockout effects in the red blood cell model As a first validation of IOMA, we applied it to predict metabolic fluxes in human erythrocytes.
For this metabolic system, a detailed kinetic model (Jamshidi, 2001) is readily available for validation, by simulating the steady-state metabolic flux after gene knockouts.
This model consists of four basic pathways: glycolysis, pentose-phosphate pathway (PPP), adenosine nucleotide metabolism, and the Rapoport-Leubering shunt, accounting for 48 metabolites, 39 internal reactions and nine exchange reactions.
The set of differential equations in the model, describing the dynamics of metabolite concentration, were solved via the ode15s solver in Matlab (The Mathworks, Inc.).
To generate synthetic proteomic and metaoblomic data that can be used as input to IOMA, and the corresponding flux data for validation, we utilized the RBC kinetic model in the following way: a gene knockout was modeled by restricting the flux through its corresponding reaction to zero.
Random protein levels were drawn from a uniform distribution, reflecting up to 5-fold increase or decrease in concentration compared to the wild-type condition.
These protein levels were then used to determine the values in the kinetic model.
To apply IOMA to predict metabolic fluxes for all gene knockouts given a CBM model of RBC metabolism, we provide it with the randomly generated proteomic data and the corresponding steady-state metabolomic data (identified by the RBC kinetic model) for a core set of 10 reactions whose rate equations (in the kinetic model) are based on Michaelis Menten-like kinetics.
The performance of IOMA is evaluated in terms of predicting significant changes in flux between the wild-type strain and each of the knockouts, considering a threshold of 0.001 to define a significant increase or decrease in flux.
Repeating the analysis 50 times with random proteomic data provided an average precision of 0.95, recall of 0.93, and an overall accuracy of 0.91 (where accuracy is the rate of true predictions) (Fig.2a).
Similar results were obtained assuming a normal distribution for protein levels with an average precision of 0.96, recall of 0.94, and an overall accuracy of 0.92. i257 [11:46 12/5/2010 Bioinformatics-btq183.tex] Page: i258 i255i260 K.Yizhak et al.
IOMAs prediction accuracy is insensitive to the specific choice of threshold, with thresholds in the range 0.00010.1 yielding <2.5% change in the prediction accuracy.
Notably, applying other flux prediction methods such as FBA and MOMA in this setup, given only metabolomic and proteomic data is problematic: FBA depends on a definition of a cellular objective function (commonly assumed to be the maximization of biomass yield rate in microbes), which is not available in the RBC model.
MOMA depends on data regarding the wild-type flux distribution, which in this test scenario was not given as input.
Utilizing a sampling technique (Becker et al., 2007) to predict a set of 100 feasible flux distributions for the wild-type strain and applying MOMA to predict the knockout effects starting from each of them resulted in a markedly lower average accuracy of 0.45 (Fig.2a).
To enable further comparison with MOMA, we considered a second test scenario in which the fluxes through exchange reactions (representing the uptake and secretion of metabolites from the growth environment; which are easier to measure experimentally) as identified by the kinetic model, are also given as input to IOMA and MOMA.
The flux distribution of the wild-type strain is computed via QP, by searching for a feasible flux vector, satisfying mass-balance and reaction directionality constraints, minimizing the Euclidian distance between the predicted exchange fluxes and those given as input.
MOMA was applied starting from this predicted wild-type flux distribution, utilizing also the given flux rates through the exchange reactions for all knockout conditions (via additional constraints in MOMAs QP formulation).
The additional flux data through exchange reactions is also utilized in a similar manner by IOMA as described in the Methods section.
The results show that while MOMAs performance significantly improves in this test scenario, reaching an average accuracy of 0.82, IOMA still achieves a statistically significant higher accuracy of 0.91 (Fig.2b) (Wilcoxon P-value = 1.61 105).
To test the robustness of IOMA to noise in the proteomic and metabolomic data, we repeated the first test scenario (the more challenging one, without using metabolite uptake and secretion rates as input), while adding random noise to both data sources given as input to IOMA.
The noise was drawn from a normal distribution with mean zero and SD = 1050% of the true proteomic and metabolomic levels (as obtained from the kinetic simulations).
The results show that IOMA still achieves a high accuracy of 0.95 and 0.84, for SD levels of 1050%, respectively, testifying for IOMAs robustness to noisy measurements.
3.2 Predicting metabolic fluxes in E.coli via the integration of experimental metabolomic and proteomic data As a second validation of IOMA, we applied it to the genome-scale E.coli metabolic network model of Feist et al.
(2007) accounting for 1260 metabolic genes, 2382 reactions and 1668 metabolites, to predict metabolic flux distributions for wild-type E.coli K-12 and 23 single-gene knockouts, which cover most viable glycolysis and PPPs knockouts, grown in glucose minimal medium.
As input we utilized experimentally measured absolute protein (mg-protein/g-dry cell weight) and metabolite (mM) concentrations for a core set of 11 reactions, uptake and secretion rates for nine metabolites, and measured growth rates, all taken from the E.coli multi-omics database (Ishii, 2007).
Metabolites dissociation constants (km) were Fig.3.
Precision, recall and accuracy of predicted changes in flux between wild-type and following gene knockouts in the E.coli model, obtained by IOMA (blue), FBA (green) and MOMA (red).
obtained from the BRENDA database (Bennett et al., 2009).
To validate the predicted flux distributions, we utilized experimentally measured fluxes in these knockout strains for 26 reactions in E.colis central metabolism, also taken from the multi-omics database.
To assess the accuracy of IOMA versus that of MOMA and FBA, we compared their predictions of significant increase or decrease in flux for the 26 measured reactions between the wild-type and knockout strains (considering the same threshold of 0.001 to define significant changes in flux as done above).
As shown in Figure 3, IOMA achieves a prediction accuracy of 0.54 which is markedly higher than that achieved by FBA and MOMA (0.44 and 0.38, respectively).
The prediction accuracy is significantly high compared to random predictions, with a hypergeometric P-value of 1.26104 and 5.12108 for the prediction of increased and decreased fluxes compared to the wild-type strain, respectively.
Notably, IOMAs prediction accuracy is insensitive to the specific choice of threshold (with thresholds in the range of 0.00010.1 yielding <5% change in the prediction accuracy of the various methods).
An example flux distribution predicted by IOMA following the knockout of an enzyme in the non-oxidative branch of the PPP, talB (Transaldolase B), is shown in Figure 4.
The figure shows that IOMA correctly predicts an increase in flux through most of the enzymes in glycolysis and a decreased flux through enzymes in PPP following the knockout of talB.
FBA is only partially correct, predicting an increased flux through both glycolysis and PPP, while MOMA falsely predicts both the increased glycolysis and decreased PPP fluxes.
To demonstrate the added value of utilizing both proteomics and metabolomics to infer metabolic flux, we compared IOMAs performance when given both proteomics and metabolomics, to its performance when only one of the sources is given as input.
To utilize IOMA only with proteomic data, we considered zero saturation coefficients (a+ and a) for all enzymes, while for utilizing it only with metabolomic data, we considered a constant expression level for each enzyme across the various conditions (e/eref = 1).
We find that, quite expectedly, when only a single data i258 [11:46 12/5/2010 Bioinformatics-btq183.tex] Page: i259 i255i260 Integrating quantitative proteomics and metabolomics Fig.4.
An example flux distribution predicted by IOMA following the knockout of talB (Transaldolase B).
The network shows E.colis glycolysis and PPP.
Green (red) edges represent a measured increase (decrease) of flux between the wild-type and knockout strains.
The letters F, M and I, represent predictions made by FBA, MOMA and IOMA, respectively, with green (red), representing a predicted increase (decrease) in flux.
Predictions of no significant change in flux are not shown.
As evident, IOMA correctly predicts the measured pattern of increased flux throughout glycolysis and decreased flux throughout the PPP pathways, with only two mismatches, while FBA and MOMA perform significantly worse.
source is used, the accuracy achieved is lower than that obtained when using both data sources together, with the prediction accuracy obtained when using only proteomic or metabolomic down to 0.45 and 0.42, respectively.
Notably, using either data source alone still improves the prediction accuracy upon FBA and MOMA.
Gene expression is significantly easier to measure than proteomics and was previously shown to significantly correlate with protein levels (Lee et al., 2000; Tuller et al., 2007).
Here, we find an average correlation of 0.65 (with 19 knockouts yielding a significant P-value <0.05) between measured gene and protein expression levels across all gene knockouts in the employed dataset (Ishii, 2007).
Considering this high correlation, we further explored the predictive performance of IOMAs given gene expression data instead of proteomic data.
The analysis shows that IOMAobtains an accuracy of 0.49 when utilizing gene expression and metabolomics with a hypergeometric P-value of 3.15104 and 8.63108 for the prediction of increased and decreased fluxes compared to the wild-type strain, respectively (higher than the accuracy obtained with FBAand MOMAas reported above).
4 DISCUSSION This study presents a novel approach for integrating quantitative proteomic and metabolomic data with a genome-scale metabolic network model to predict flux alterations under different perturbations, based on a mechanistic model for determining reaction rate.
The method predicts feasible flux distributions while accounting for missing concentration levels of substrate and product metabolites for some enzymes, for potential noise in both the proteomic and metabolomic data, and for the simplifying rate equation formalism used.
IOMA is shown to successfully predict changes in fluxes both in E.colis central metabolism under various genetic perturbations and in a simulated RBC kinetic model, displaying a significant improvement versus the commonly used FBA and MOMA methods.
Metabolic fluxes are the most informative and direct indices of the metabolic and physiological state of cells/tissues, and hence, inferring their state in different biological contexts is probably the holy grail of metabolic modeling.
However, in a somewhat spiteful way, while we are facing an ever increasing availability of numerous pertaining high-throughput omics data including transcriptomic, proteomic and metabolomic measurements, the measurement of fluxes is still very challenging and limited to a small fraction of the reactions present in cells.
Hence, there is an emerging need to continue and develop new computational methods for robustly inferring the flux state, while capitalizing on these other available omics measurements.
In this context, IOMA presents an important step forward in this direction, which hopefully will be followed upon by others.
IOMA profits from the absolute quantification of metabolites levels (in contrast to fold changes), that are becoming available, while absolute quantification of proteins is not necessary.
Apart from the specific kind of reaction rate laws utilized in this work, IOMA can be used with a variety of rate laws including different types of regulation or enzyme saturation.
The only restriction is that the rate laws can be represented in the form of Equation (5), where estimates of the terms a+ and a can be recomputed based on available data.
Future work for improving flux predictions, could possibly utilize existing information on the thermodynamic constants of reactions to further constraint the models solution space, following Henry et al.
(2007).
Another potential application of IOMA is the prediction of metabolic flux alterations associated with human metabolic disorders (as means for predicting potential clinical biomarkers).
Encouragingly, genome-scale human metabolic models have already shown their value in this highly important clinical task [e.g.
in the case of inborn errors of metabolism (Shlomi and Cabili, 2009)], but as the methods used up until now have been simple and straightforward, there is certainly much room for improvement ahead, to which methods like IOMA are bound to serve as solid starting points.
Funding: Grant from the Israel Science Foundation (ISF) to T.S.
; European Commission [BaSysBio, grant number LSHG-CT-2006-037469] to W.L.
; Fellowship from the Edmond J. Safra Bioinformatics program at Tel-Aviv University.
Conflict of Interest: none declared.
i259 [11:46 12/5/2010 Bioinformatics-btq183.tex] Page: i260 i255i260 K.Yizhak et al.
ABSTRACT Motivation: Finding protein-protein interaction (PPI) information from literature is challenging but an important issue.
However, keyword search in PubMed is often time consuming because it requires a series of actions that refine keywords and browse search results until it reaches a goal.
Due to the rapid growth of biomedical literature, it has become more difficult for biologists and curators to locate PPI information quickly.
Therefore, a tool for prioritizing PPI informative articles can be a useful assistant for finding this PPI-relevant information.
Results: PIE (Protein Interaction information Extraction) the search is a web service implementing a competition-winning approach utilizing word and syntactic analyses by machine learning techniques.
For easy user access, PIE the search provides a PubMed-like search environment, but the output is the list of articles prioritized by PPI confidence scores.
By obtaining PPI-related articles at high rank, researchers can more easily find the up-to-date PPI information, which cannot be found in manually curated PPI databases.
Availability: http://www.ncbi.nlm.nih.gov/IRET/PIE/ Contact: sun.kim@nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.
Received on August 15, 2011; revised on December 2, 2011; accepted on December 17, 2011 1 INTRODUCTION Researchers keep track of protein-protein interaction (PPI) information by searching literature online or using PPI database services.
When using PPI databases, well-summarized information can be obtained.
But, newly discovered evidence may be missed due to the rapid growth of the biomedical literature and time-consuming manual curation process (Blaschke et al., 2005).
For online literature search, people commonly find relevant information in PubMed by exploring a combination of keywords, e.g.
protein names, journal names or author names.
This step can be time consuming; however, the interactive query-retrieval process by manual effort can reach better and more focused PPI information.
Automatic article recommendation for PPI information can be, therefore, positioned between PPI database services and manual literature search because To whom correspondence should be addressed.
it provides more effective retrieval by suggesting PPI informative articles (PPI articles) from simple user queries.
PPI extraction tasks require several prerequisite steps such as gene mention, gene normalization, PPI article filtering or PPI experimental method extraction.
Although article filtering is an essential step among those tasks, it has been often neglected by previous protein-interaction extraction systems.
Improving PPI article classification enables better literature navigation for biologists (Krallinger et al., 2009), effective assistance for curators in manually updating repositories (Dowell et al., 2009) and better literature-mining system development for text mining researchers (Leitner et al., 2010).
PIE the search is an online web service to assist in finding PPI articles from PubMed.
PIE the search provides the following novel features distinguished from other PPI services: First, it navigates PPI-specific articles for biologists and curators.
Second, it provides a compact PubMed-search environment to help easy access for PubMed users.
Third, users can easily find the up-to-date PPI information, which has not been curated in PPI databases.
Since the proposed system implements the recent competition-winning approach in BioCreative (BC) III (Krallinger et al., 2010), it also guarantees the state-of-the-art performance among various methods.
2 SYSTEM AND FUNCTIONALITY Figure 1 shows the overall architecture of the PIE the search system.
The web interface module manages the whole process of PPI article prediction for users.
For user queries, PubMed IDs are first retrieved through online PubMed services.
PPI confidence scores are calculated for retrieved articles, and articles are re-ranked based on scores.
For protein name queries, this process does not guarantee highly ranked articles that contain query-specific PPIs.
However, it is still likely to have useful PPI information related to protein queries.
The prediction module learns and classifies PubMed articles (Kim and Wilbur, 2011).
To effectively capture PPI patterns from biomedical literature, our approach utilizes both word and syntactic features in a machine learning framework.
Dependency parsing, gene mention tagging and term-based features are utilized along with a Huber classifier.
Since PIE the search is designed to provide only compact, but necessary features for PPI article search, its use is very straightforward, especially for PubMed users.
It accepts PubMed input formats including All Fields, Author, Journal, MeSH Terms, Publication Date, Title and Title/Abstract with Boolean operations (AND, OR and NOT).
However, the output is the list of articles prioritized by PPI confidence scores.
Search results can be sorted by either PPI scores or dates.
With the date sorting, only articles with PPI scores > 0.1 will appear.
For convenient use, there are no page changes The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:20 9/2/2012 Bioinformatics-btr702.tex] Page: 598 597598 Kim et al.
Fig.1.
Overall architecture of PIE the search.
between search results and detailed article information.
One typical routine in PubMed search is to follow full paper links after article information pages.
Hence, full paper and PubMed links are also shown on the same page.
In addition, some gene/protein names which contributed for PPI prediction are underlined and linked to Entrez and Entrez Gene.
Another feature of PIE the search is a Keyword Cloud.
Key noun phrases used for PPI article prediction are pooled and listed based on frequencies.
This function helps users view abstracts in a nutshell.
3 RESULTS AND DISCUSSION The prediction module in PIE the search is trained by all available BC datasets except for the BC3 test set (Kim and Wilbur, 2011).
The performance of the PIE system was evaluated on the BC3 test set in terms of F1, MCC1 and AUC iP/R1 measures (Krallinger et al., 2010).
This test set contains 910 PPI and 5090 non-PPI articles, which is unbalanced reflecting a real-world situation.
The proposed system provides 0.6258 F1, 0.5610 MCC and 0.6834 AUC iP/R, whereas the medians of BC3 participant results are 0.5353 F1, 0.4563 MCC and 0.5367 AUC iP/R.
The PIE system significantly outperformed the other approaches on all measures.
Since PIE the search is a web-based ranking system, the performance at top-ranked articles is more important than overall classification performance.
At rank 10 (P@10), 100 (P@100) and 200 (P@200), our system achieves 100, 94 and 91.50% precision, respectively.
Our system also produces over 95% precision at 10% recall.
Even though the ratio between PPI and non-PPI articles in the PubMed database is more skewed than the BC3 test set, the ranking performance of PIE the search shows its usefulness as a PPI article search engine.
To understand how accurately our system ranks PPI articles and how users respond to this service, we further performed manual evaluation.
A total of 10 biologists were asked to judge the top 10 search results from PubMed and PIE the search using their own queries.
Each user performed searches for five different queries, and precision was calculated based on their assessments.
As a result, PubMed achieved 25.40% precision on average.
Meanwhile, PIE the search achieved 81.60 and 75.89% precision on average for results sorted by PPI scores and dates options, respectively.
For the question of how satisfactory is this service as a PPI article search tool, the biologists responded with a rating of 4.4 on average on a 1 (bad) to 5 (good) scale.
The Supplementary Material describes the manual evaluation in detail.
1MCC and AUC iP/R measures are further explained in the Supplementary Material.
While most PPI extraction services provide PPI-centered information, PIE the search pursues a more general strategy for biologists and curators, i.e.
a topic-specific search service by ranking PPI articles in PubMed.
Even though it places more responsibility on users to choose useful query terms, it increases the chance to get the correct PPI articles.
If one wants to find core PPI information from gene or protein names, other extraction services may be a good choice.
However, PIE the search provides more up-to-date PPI information by directly searching PubMed with guaranteed high classification performance.
Taking a user-friendly perspective, the interface adopts a very easy and compact search scenario.
Moreover, the PIE system provides a batch access through CGI programs, which can help other bio-text mining researchers develop similar prediction systems or perform performance comparisons.
A tutorial of how to use PIE the search can be found at the homepage.
4 CONCLUSION PIE the search is a web service designed for searching PPI articles from PubMed, which employs word and syntactic features in a machine learning framework.
Compared to previous PPI article classification approaches, this method actively utilizes syntactic information.
The Priority Model (Tanabe and Wilbur, 2006) and Huber classifiers are also a distinctive choice for effectively handling PubMed data.
PIE the search is already practical as a ranking system since it provides high classification performance at top-ranked articles.
The web service is freely accessible and the local PubMed database in PIE is being updated monthly.
ACKNOWLEDGEMENTS The authors would like to thank all the participants for their contribution to the manual evaluation.
The authors also would like to thank Natalie Xie for valuable comments on web implementation of the system.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine (to S.K.
and W.J.W.
); Basic Science Research Program through the National Research Foundation of Korea (NRF) (NRF-2011-0002437) (to D.K.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Biological data generation has accelerated to the point where hundreds or thousands of whole-genome datasets of various types are available for many model organisms.
This wealth of data can lead to valuable biological insights when analyzed in an integrated manner, but the computational challenge of managing such large data collections is substantial.
In order to mine these data efficiently, it is necessary to develop methods that use storage, memory and processing resources carefully.
Results: The Sleipnir C++ library implements a variety of machine learning and data manipulation algorithms with a focus on heterogeneous data integration and efficiency for very large biological data collections.
Sleipnir allows microarray processing, functional ontology mining, clustering, Bayesian learning and inference and support vector machine tasks to be performed for heterogeneous data on scales not previously practical.
In addition to the library, which can easily be integrated into new computational systems, prebuilt tools are provided to perform a variety of common tasks.
Many tools are multithreaded for parallelization in desktop or high-throughput computing environments, and most tasks can be performed in minutes for hundreds of datasets using a standard personal computer.
Availability: Source code (C++) and documentation are available at http://function.princeton.edu/sleipnir and compiled binaries are available from the authors on request.
Contact: ogt@princeton.edu 1 INTRODUCTION Whole-genome assays have now become pervasive, and the resulting wealth of data represents a new opportunity for biological discovery.
A single genome-scale dataset can capture a snapshot of cellular function; integrative analysis of hundreds or thousands of genome-scale datasets can provide even more extensive systems-level insights regarding gene interactions under diverse conditions (Troyanskaya, 2005).
Integrated approaches have already resulted in important biological discoveries (Hong et al., 2008; Myers and Troyanskaya, 2007), and the breadth and depth of possible analyses will only increase as additional experimental data is collected.
As the amount of data to be analyzed continues to increase, computational efficiency becomes a greater concern.
Specialized resources exist to enable very high-throughput computing for To whom correspondence should be addressed.
specific applications (Pekurovsky et al., 2004; Swindells et al., 2002), but few computational options exist allowing researchers to quickly mine large collections of genome-scale datasets.
To address this need, we have created the Sleipnir library for computational functional genomics.
The library contains algorithms and data types for efficiently manipulating and mining very large biological data collections.
The core C++ library can be integrated into computational systems to provide rapid analysis of functional genomic data.
Additionally, a variety of tools are provided that use the library to perform common tasks: microarray processing, Bayesian and support vector machine (SVM) learning and so forth.
Even when analyzing individual datasets, Sleipnir often outperforms existing utilities in processing time, memory usage or both (Table 1).
Tools provided with Sleipnir address common data manipulation requirements, in many cases processing hundreds of datasets on a standard desktop computer.
Additionally, the core Sleipnir library can be easily employed to efficiently apply new algorithms to complex biological data.
2 METHODS The Sleipnir library contains a wide variety of tools for consuming standard biological data formats, manipulating and normalizing data and performing machine learning and prediction.
These are discussed extensively in the user and developer documentation included with the library (http://function.princeton.edu/sleipnir) and are presented here in summary.
Sleipnir provides C++ classes to parse pairwise interaction data and standard microarray file formats.
Microarray data can be converted into pairwise similarity/distance scores using a variety of measures, discretized, normalized, randomized for bootstrapping or synthetic data production, split or merged, imputed or clustered.
To facilitate functional enrichment analysis, gene function prediction and gold standard generation from known gene functions and relationships, Sleipnir provides a uniform interface to several organism-independent function annotation catalogs.
Information from organism-specific annotations can be merged with these functional annotations.
Sleipnir also includes collections of data structures for dealing with common biological entities: gene identifiers, sets of genes, groups of related files, etc.
Other utility classes include resources for multithreading, a ready-made network client/server class and a variety of mathematical and statistical tools.
Sleipnir provides several tools for rapid machine learning and data mining.
The SMILE Bayesian network library (Druzdzel, 1999) and the SVM Light (Joachims, 1999) library are used to learn and evaluate Bayesian or SVM models from very large collections of biological data.
Arbitrary Bayesian structures are allowed, with parameters learned either discriminatively or generatively (Greiner and Zhou, 2005) from data in a context-specific 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[23:34 18/6/03 Bioinformatics-btn237.tex] Page: 1560 15591561 C.Huttenhower et al.
Table 1.
Sleipnir efficiency on integration and single dataset tasks Implementation Peak RAM (KB) Time (s) Bayesian learning (500 genes, 15 datasets) Sleipnir 1376 <1 GeNIe 6832 4 BNT 593 180 15 Bayesian inference (500 genes, 15 datasets) Sleipnir 1216 1 BNT 273 992 >600 Missing value estimation (10% missing, k =10) Sleipnir 27 232 195 knnimpute 115 708 368 Hierarchical clustering Sleipnir 83 188 156 Cluster 3.0 176 836 154 MeV 198 292 361 K-means clustering (k =100) Sleipnir 8780 114 Cluster 3.0 28 544 102 MeV 198 292 361 Memory usage and runtimes for Sleipnir and a number of other common tools for Bayesian analysis and biological data manipulation (de Hoon et al., 2004; Druzdzel, 1999; Murphy, 2001; Saeed et al., 2003; Troyanskaya et al., 2001).
All microarray operations were performedon the 300 conditions and 6153 genes of (Hughes et al., 2000) using Euclidean distance.
Bayesian operations were performed on simulated data using a binary gold standard and five randomly distributed values per dataset.
Tests were run in a single thread on a 2 GHz Intel Core 2 Duo.
In every case, Sleipnir demonstrates a substantial advantage in speed, memory usage or both.
manner (Huttenhower et al., 2006); extremely fast-customized learning and evaluation implementations are used for naive structures.
3 RESULTS While Sleipnirs efficiency in integrating and mining biological datasets is most critical for very large data collections, it is also practical for single dataset tasks and smaller analyses (Table 1).
When compared to several common tools for microarray manipulation or Bayesian learning, Sleipnir consistently demonstrates a substantial advantage in runtime, memory usage or both.
These improvements arise from a variety of optimizations but are broadly attributable to the flexibility allowed by C++ in manipulating large quantities of individual data (microarray values, interaction pairs, etc.)
What Sleipnir trades off in generality (e.g.
with respect to BNT) or robustness to malformed input (e.g.
with respect to MeV), it gains in speed, memory management and overall scalability, allowing it to efficiently manipulate large data collections.
The Sleipnir library is particularly useful for large integration tasks involving hundreds of diverse biological datasets; example applications of Sleipnir in such settings include Huttenhower et al.
(2006) and Myers and Troyanskaya (2007).
A schematic of such a task is shown in Figure 1, where Sleipnir was used to learn 200 context-specific Bayesian classifiers each integrating 186 Saccharomyces cerevisiae datasets.
Conditional probability tables were learned for each dataset within each context, entailing 75 000 probability distributions.
The resulting Bayesian classifiers were used to infer context-specific functional relationship networks, each consuming 90 MB of disk space and calculated in 16.3 min.
Sleipnir also supports an online mode for functional relationship inference Fig.1.
Sample application of the Sleipnir library to integrate 186 heterogeneous genomic datasets in S.cerevisiae within 200 biological contexts.
White boxes indicate externally generated data, grey boxes data generated by Sleipnir, arrows processing performed by Sleipnir, and black bubbles highlight time-consuming tasks.
Times were generated on a 2 GHz Intel Xeon CPU; peak RAM usage was 200 MB.
Sleipnir is extensively parallelizable, and running these tasks on four cores reduces processing time by an optimal 4-fold to 13 h each for Bayesian learning and inference.
in which no additional disk space is consumed and individual context-specific functional relationships can be produced in as little as 100 ns.
Parallelization on four processor cores reduces the total learning and evaluation time by an optimal 4-fold speedup (13 h each for Bayesian learning and inference).
Every stage of this complex data integration and machine-learning task was performed using Sleipnir and its associated tools.
4 DISCUSSION The Sleipnir library for computational functional genomics provides a wide range of data processing and machine learning algorithms optimized for integrating very large collections of heterogeneous biological data.
These include algorithms for data integration, machine learning by Bayesian networks or SVMs, and data types for manipulating microarrays, gene identifiers, functional annotations and other common biological entities.
Several tools are provided with the core library to perform common tasks, and most algorithms are multithreaded or parallelizable for distributed computing.
The Sleipnir library enables computational biologists to efficiently integrate thousands of genomic datasets and to rapidly mine them for biological knowledge.
ACKNOWLEDGEMENTS We thank Matthew Hibbs and Chad Myers for helpful discussions and code reviews.
Funding: This research is supported by NSF CAREER DBI-0546275, NIH R01 GM071966, NIH T32 HG003284 and NIGMS Center of Excellence P50 GM071508.
O.G.T.
is an Alfred P. Sloan Research Fellow.
Conflict of Interest: none declared.
ABSTRACT Summary: NAViGaTOR is a powerful graphing application for the 2D and 3D visualization of biological networks.
NAViGaTOR includes a rich suite of visual mark-up tools for manual and automated annotation, fast and scalable layout algorithms and OpenGL hardware acceleration to facilitate the visualization of large graphs.
Publication-quality images can be rendered through SVG graphics export.
NAViGaTOR supports community-developed data formats (PSI-XML, BioPax and GML), is platform-independent and is extensible through a plug-in architecture.
Availability: NAViGaTOR is freely available to the research community from http://ophid.utoronto.ca/navigator/.
Installers and documentation are provided for 32-and 64-bit Windows, Mac, Linux and Unix.
Contact: juris@ai.utoronto.ca Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION The availability of proteinprotein interaction (PPI) data is increasing rapidly through literature-derived databases (Bader et al., 2003; Breitkreutz et al., 2002; Hermjakob et al., 2004a; Peri et al., 2004; Xenarios et al., 2000; Zanzoni et al., 2002), high-throughput detection methods (Barrios-Rodiles et al., 2005; Rual et al., 2005) and computational predictions (Brown and Jurisica, 2005; Persico et al., 2005).
These data, collectively referred to as the interactome, are critical to our understanding of both normal cellular processes and disease mechanisms.
Visualizing the interactome, along with integrating orthogonal data types, may aid in the understanding of cell function, help elucidate hidden relationships within the data and help prioritize functional studies.
Several biological graph visualization tools are currently available, implementing a diverse range of approaches and algorithms (Breitkreutz et al., 2003; Chin et al., 2008; Hu et al., 2004; Ju and Han, 2003; Macpherson et al., 2009; Paananen and Wong, 2009).
Cytoscape (Shannon et al., 2003), in particular, has been widely adopted by the biological community for its ease of use and extensibility through open source plug-in development.
To whom correspondence should be addressed.
While many of these tools are effective and widely used, there are several critical areas where these applications require improvement (reviewed in Suderman and Hallett, 2007).
Scalability is essential to visualize the tens of thousands of known PPI, which is a challenge for current layout algorithms and software.
Biological graph drawing software must also be able to handle richly annotated data, including genomic and proteomic profiles, KEGG pathways (Kanehisa and Goto, 2000), Gene Ontology (GO) annotations, data in PSI-MI (Hermjakob et al., 2004b) and BioPAX formats (http://www.biopax.org/), in addition to the vast quantity of microarray data that is currently available.
NAViGaTOR builds upon these earlier efforts, addressing known issues in existing software.
NAViGaTOR uses a combination of hardware-based graphics acceleration and highly optimized layout algorithms to enable interactive visualization of large networks.
It supports community-based data interchange formats, such as PSI-MI, BioPAX and GML, facilitating interoperability with existing software tools.
Additionally, NAViGaTOR includes a rich suite of built-in analysis and visualization functions, which can be extended through an application programming interface (API).
Here, we describe the implementation of NAViGaTOR, and highlight how this tool improves upon existing network visualization packages.
2 SOFTWARE 2.1 Implementation NAViGaTOR has been implemented in Java (v1.6), providing platform-independence, and uses JOGL (https://jogl.dev.java.net/) to enable OpenGL hardware-accelerated graphics rendering.
At present, the core code-base is closed-source to ensure stability, but future enhancements will extend the plug-in API to an OSGi-compliant (http://www.osgi.org/Main/HomePage) framework that enables community-driven extensibility.
2.2 Features NAViGaTOR enables interactive visualization and analysis of complex graphs that are typical in systems biology studies.
Graphs can be loaded from PSI-MI XML, BioPax, GML and tab-delimited text files, or through online databases such as I2D (http://ophid .utoronto.ca/i2d) and cPATH (http://cbio.mskcc.org/cpath/).
Both The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[14:42 10/11/2009 Bioinformatics-btp595.tex] Page: 3328 33273329 K.R.Brown et al.
Fig.1.
Screen capture of the NAViGaTOR user interface.
Labels indicate the various tools and descriptive regions of the interface.
A graph is shown in the Graph Panel, with edges adjusted automatically by Edge Filters.
Filters can be used to automatically control visual attributes of both nodes and edges.
2D and 3D network views are fully interactive, allowing the user to manually manipulate the graph, or to use automated layouts such as circular, linear or concentric circular on subsets of nodes or entire graphs.
A spreadsheet view supports selecting and deselecting nodes, edges and paths based on any attributes.
Nodes and edges can be grouped into subsets, which can be collapsed or expanded to simplify views, or manipulated through set operations.
Network analysis tools provide information about node and edge connectivity, shortest paths, identify hubs, cliques and articulation points and summarize network statistics.
NAViGaTOR can also use a multi-threaded implementation to efficiently generate random networks for enrichment analyses.
Fully annotated graphs can be exported to six different graphics formats, including PDF and SVG.
In summary, NAViGaTOR provides a network analysis platform that is rich in the features essential to many biological applications, and yet is extensible through a plug-in interface to include additional features when required.
See Figure 1 and the Supplementary Materials for examples of the NAViGaTOR interface and rendered networks.
2.3 Advances NAViGaTORs ability to handle larger datasets is facilitated through optimized layout algorithms, hardware-based graphics acceleration and a reduced memory footprint relative to other software.
NAViGaTOR performs an initial layout using Graph Drawing with Intelligent Placement (GRIP; Gajer and Kobourov, 2002), which performs network layout in near linear time, and then continuously updates the layout of the graph using a multi-threaded grid-variant (Fruchterman and Reingold, 1991) of a force-directed layout algorithm.
When benchmarked against the force-directed algorithms in Cytoscape and VisANT, NAViGaTOR consistently provided graphs rendered in significantly shorter time (Fig.2; Supplementary Fig.3.3).
Only the yFiles Organic plug-in for Cytoscape rendered in similar time to NAViGaTOR, although the resulting graph was poorly optimized (compare Supplementary Fig.3.5C to Supplementary Fig.3.4C).
OpenGL enables NAViGaTOR to take advantage of hardware-based acceleration to render larger graphs in both 2D and 3D.
Additionally, the data structures within NAViGaTOR were designed to maintain a small memory footprint in order to provide greater scalability for large datasets.
When compared against Cytoscape and VisANT, NAViGaTOR had a memory footprint approximately half that of Cytoscape, although a 1238% larger footprint than VisANT (Supplementary Fig.5.1).
The NAViGaTOR user interface includes unique tools to help simplify the hairball, which is a common challenge in many PPI 3328 [14:42 10/11/2009 Bioinformatics-btp595.tex] Page: 3329 33273329 NAViGaTOR 0 500 1000 1500 2000 2500 3000 yF ile s O rg an ic Fo rc e-dir ec te d RS FD P Fo rc e-dir ec te d Un do O N Un do O FF Fo rc e-dir ec te d Re lax ed e leg an t Fo rc e-dir ec te d Layout Algorithm/Status T o ta l V is u al iz at io n T im e (s ) Rendering Loading Cytoscape Interviewer3 NAViGaTOR VisANT Osprey Fig.2.
Performance comparisons between applications.
The Reactome Caenorhabditis elegans BioPax file was used to test the performance of several graph visualization applications in loading and visualizing the graph.
Only Cytoscape and NAViGaTOR were able to load the BioPax file directly; Interviewer3 required a GML export from NAViGaTOR, VisANT required a PSI-MI XML v1.0 file, and Osprey required a tab-delimited text file.
Stacked bars were used to show the cumulative loading and rendering time, or the total time to view a graph.
networks.Alpha blending is a technique to deemphasize unimportant areas of the network and focus on important areas by fading out selected nodes and edges.
Automated filters let users map node and edge properties, such as color, size, shape and opacity to any numeric or text attribute.
For instance, nodes can be scaled by degree or betweenness centrality, and colors can be mapped to GO ontology categories.
Rectangle and lasso selection, and the unique ability to (de)select a connected neighborhood of nodes by dragging out its radius in edges, allow users to easily select specific subsets of nodes and edges, while other tools allow the selected subset to be rotated, scaled or laid out along a line or circle.
Combined with pan/zoom navigation, users can quickly explore or simplify complicated networks.
3 FUTURE DEVELOPMENT NAViGaTOR has evolved from an in-house visualization tool to a more versatile, comprehensive platform.
While the current version of NAViGaTOR includes a plug-in API, NAViGaTOR 3.0 will adopt a more formal open plug-in interface compliant with the OSGi framework.
This framework will allow for community-driven development through small, tightly coupled bundles while protecting the core code-base of the application.
NAViGaTOR will also serve as a platform to explore novel ways for biologists to interact with graphs, as well as new ways to encode and display information in biological networks.
ACKNOWLEDGEMENTS We would like to acknowledge Richard Lu and Frederic Breard for supporting the I2D database, which provides PPI data and annotations for NAViGaTOR, Rick Valenzano for researching and implementing the GRIP layout, and Uzma Khan for helping with updating and improving the NAViGaTOR web site (http://ophid.utoronto.ca/navigator/).
Funding: Genome Canada via the Ontario Genomics Institute; Canada Foundation for Innovation (grant nos 12301 and 203383); Canada Research Chair Program in part; Ontario Research Fund Research Excellence.
Conflict of Interest: none declared
ABSTRACT The advent of high-throughput DNA sequencers has increased the pace of collecting enormous amounts of genomic information, yielding billions of nucleotides on a weekly basis.
This advance represents an improvement of two orders of magnitude over traditional Sanger sequencers in terms of the number of nucleotides per unit time, allowing even small groups of researchers to obtain huge volumes of genomic data over fairly short period.
Consequently, a pressing need exists for the development of personalized genome browsers for analyzing these immense amounts of locally stored data.
The UTGB (University of Tokyo Genome Browser) Toolkit is designed to meet three major requirements for personalization of genome browsers: easy installation of the system with minimum efforts, browsing locally stored data and rapid interactive design of web interfaces tailored to individual needs.
The UTGB Toolkit is licensed under an open source license.
Availability: The software is freely available at http://utgenome.org/.
Contact: moris@cb.k.u-tokyo.ac.jp 1 INTRODUCTION Browsing genomic information has been central to life science analysis since large-scale genomes became available for many species including mammals, invertebrates, plants and insects.
To support the task of analyzing genomic information, two categories of genome databases are available.
The first category includes generic genome database species maintained by large organizations, such as Ensembl (Hubbard et al., 2009), UCSC (Kuhn et al., 2009) and NCBI (Wheeler et al., 2007).
The other category includes species-specific genome databases, such as SGD (Hong et al., 2007), FlyBase (Wilson et al., 2008) and Wormbase (Rogers et al., 2007), which utilize general-purpose genome browsers, such as GBrowse (Stein et al., 2002).
Both types of genome browser represent centralized resources because of the high cost of building and maintaining web database servers.
Due to the limited number of experts available to maintain the servers, updates of these centralized web servers are likely to be slow, which is tolerated because the amount of genomic data output by Sanger sequencers is relatively small, yielding on the order of 10 million nt per week.
To whom correspondence should be addressed.
Today, however, the rate at which genome information is produced has outperformed the pace of centralized web browser maintenance.
Indeed, the recent advent of high-throughput DNA sequencers (e.g.
Solexa/Illumina, SOLiD/ABI and 454/Roche) allows even small groups of people to collect huge amount of genomic data in a fairly short period, billions of nucleotides per week, requiring the data analysis to be done as quickly as possible.
In particular, generating tracks for investigating personalized data becomes a crucial step in conducting specific analysis.
The UCSC Genome Browser, for example, offers the function of managing custom tracks that upload and display personal data in local disks to the UCSC Genome Browser together with well-annotated existing tracks.
These functions in traditional genome browsers are useful but suffer from three major drawbacks.
First, the users have to upload their novel and confidential data to the server, though they want to keep these datasets in their local files before publication.
Second, although anonymization of the data is partially supported by UCSC, uploading large volumes of data (e.g.
1 GB of Solexa reads data) to the remote web server still needs enourmous amount of time.
In order to avoid the costs of data uploads, installing a private version of these genome browsers is desirable; however, the task is extremely hard for non-programming biologists because it demands expertise in programming.
Third, personalization of web interface is limited and time consuming, though personalizing web interface through a variety of rearrangements is an important step toward finding new ideas.
For example, Affymetrix developed the Integrated Genome Browser (IGB) (Affymetrix, http://www.affymetrix.com/partners_programs/ programs/developer/tools/affytools.affx.)
for integrating various types of biological data provided by DAS (Distributed Annotation System) (Dowell et al., 2001) servers.
We developed the (University of Tokyo Genome Browser) UTGB Toolkit to provide solutions to these three major requirements: browsing locally stored data, ease of system installation with minimum effort and custom web interface design.
Installing the UTGB Toolkit locally on ones own computer is quite easy for inexperienced users because the system can be installed with a few steps, and also avoids uploading confidential data to the remote server.
Furthermore, the UTGB Toolkit packages ready-to-use functions, such as a stand-alone web server, HTML rendering functionality, database engines, into one component so that these functions can be made available at private sites 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1857 18561861 UTGB toolkit for personalized genome browsers resize drag and drop Structure of gene Frequency of transcription start sites Nucleosome position Genetic variation Specificity of short reads Repeat masker A B C D removal Fig.1.
Rapid creation of tailored interface.
(A) A number of tracks in a UTGB genome browser.
A long track with a large amount of information can be resized to a shorter track interactively using the scroll bar, which allows the user to browse the content of the original long track.
It is also quite easy to eliminate tracks irrelevant to a particular analysis.
(B) The resulting tracks can be reordered to facilitate the further analysis.
(C) Tracks can be rearranged using the interactive drag-and-drop interface.
(D) A genome browser tailored to the analysis of nucleosome positioning surrounding transcriptional start sites and its effect on genetic variation.
The track for specificity of short reads is useful in assessing the uniqueness of short-read alignments on the genome.
It takes <1 min to perform all the steps.
immediately after the installation.
Figure 1 shows how the UTGB Toolkit facilitates personalization of the web interface in the study of epigenomics.
Studying epigenomics involves searching for meaningful combinations from a variety of genome-wide data resources such as DNA methylation, nucleosome positions, transcription start sites, gene expression and evolutionary conservation (Kasahara et al., 2007; Sasaki et al., 2009), which can now be observed via high-throughput sequencers.
Our toolkit makes it quite easy to develop a genome browser with drag-and-drop functions for rearranging, juxtaposing and resizing relevant tracks side-by-side interactively to highlight how epigenetic controls are responsible for gene expression and evolution.
2 METHODS 2.1 Genome browser interface To enhance the portability of the system, the UTGB Toolkit is implemented in Java, a portable programming language, such that its machine codes run on top of the Java Virtual Machine.
Thus, the UTGB Toolkit is executable on most commonly available platforms, including Mac OS X, Windows, Linux, Solaris and FreeBSD.
In addition, UTGB Toolkit is designed such that the browser interface runs on most common web browsers, such as IE, Safari, Firefox and Opera, although it is still necessary to settle discrepancies between these web browsers.
To achieve this goal, the interface is compiled into JavaScript code via the Google Web Toolkit (GWT) compiler, which is capable of subsuming the differences between JavaScript engines in the individual web browsers.
2.2 Portable web server for quickly browsing local resources UTGB Toolkit contains a portable web server so that the genome browser can be launched from the users personal computer.
To avoid manual installation of the web server program (e.g.
Apache), the UTGB Toolkit has an embedded Tomcat web server engine.
The Tomcat server in the UTGB Toolkit works as a stand-alone web server that is not resident on the system.
UTGB Shell launches an instance of Tomcat with the utgb server command, and then deploys the genome-browser program on the local Tomcat server.
1857 [14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1858 18561861 T.L.Saito et al.
The portable web server is useful not only for avoiding the installation process, but also for browsing locally stored data resources without losing data privacy.
With the web server running on the user machine, no need exists to upload confidential data, such as personal genomic data, to a remote database center.
Although anonymization of the data is supported in the UCSC genome browser, the cost of uploading large volumes of data is still prohibitive.
As a solution to this problem, the user can utilize the UTGB running on the local machine to simultaneously display local tracks, whose data are kept on the local hard disk, and publicly available tracks.
2.3 Ensuring portability via the embedded database engine The database management system (DBMS) is an essential component of the genome browser used to provide genomic data for drawing tracks.
However, its installation and setup are quite complicated tasks even for database experts.
To avoid problems in setting up database engines, we embedded the SQLite (http://www.sqlite.org/) database engine into the UTGB Toolkit.
Connections to other DBMS, such as MySQL, PostgreSQL, are also supported in the UTGB Toolkit through JDBC (Java Database Connection) (http://java.sun.com/products/jdbc/).
However, unlike these DBMS that use several files to store the database contents, SQLite is portable in that it uses a single file with a universal format, which can work across several operating systems.
To make the DBMS available in any OS environment, we developed an SQLite JDBC connection library, which packs natively compiled SQLite binaries (SQLite is written in C) for operating systems such as Windows, Mac OS and Linux.
To support other operating systems for which the SQLite binary is not available, our SQLite JDBC library also contains a pure Java SQLite database engine, which works in any environment that supports Java.
Therefore, even if the computer has no DBMS, running the UTGB browser with database support is possible.
In addition, with the embedded SQLite database engine, we can easily port the genome browser to other OS environments; e.g.
we can make a clone of the genome browser simply by copying the browser code and database files.
This portability of the genome browser is a novel feature made possible by the UTGB Toolkit.
2.4 Server-side programming support The UTGB Toolkit is designed to accommodate various requirements of data visualization, as visualization for genomic data tends to be different for each scientific study.
Standard graphical representations provided by existing database centers do not always fulfill user needs.
The UTGB Toolkit allows developers to use their own data visualization programs, e.g.
CGI-based graphic image generators, HTML content renderer.Although many variations in data visualization exist, a common implementation pattern is used in writing web-based graphic generation programs.
For example, a typical pattern of server-side graphic drawing is as follows: the web server receives a user request from the browser, and then issues a query to the database.
The results of the query are translated to class objects (e.g.
gene objects), and finally, image data for visualizing these gene objects are returned to the browser.
This pattern contains three major processes: web request handling, database connection and database object mapping.
Here, we describe libraries for supporting implementation of these common tasks, and how the UTGB Toolkit eases server-side programming.
2.4.1 Web Action The web request handler in the UTGB Toolkit is called a web action, which is a Java class for receiving HTTP requests.
Web actions in the UTGB Toolkit enable developers to rapidly begin coding web interfaces; the utgb action command in UTGB Shell generates a web action instantly.
Each web action is directly mapped to a web server URL.
For example, a web action named sequence corresponds to the URL, http://(server_base_url)/sequence.
Parameter values attached to the URL (e.g.
sequence?name=chr1&start=100) are passed to the web action, and these values are automatically assigned to corresponding variables in the web action class.
Data types of the request parameters, such as integer and string, are detected automatically from the class definitions of web action class, and parameter values in the URL, which are merely a string type, are automatically converted into appropriate data types.
Individual web actions correspond to the genome browser web API.
With the web action mechanism, it is possible to avoid writing repetitive code for request handling and data type conversion.
2.4.2 Database connection The second core component is database connection support.
In general, web database development with Java requires a database server installation and complex configuration files.
Inclusion of the embedded SQLite database engine removes the requirement for database installation, and connections to SQLite databases or other DBMS are immediately available within the web action codes.
UTGB Toolkit supports both local SQLite database files and remote databases connected through JDBC.
These databases can be used by specifying their system types and database location (file names or URLs) in the config/track-config.xml file.
2.4.3 Object database mapping Relational database engines serve table-formatted data, and an impedance mismatch occurs between table data and their memory representations in computer programs.
To use the database data in a program, it is necessary to convert the table data into a more usable format, such as array or class objects in main memory.
The UTGB Toolkit provides the BeanUtil library, which supports translation of table data into class objects.
Similar to the web action handling, table format data are converted to appropriate class objects by investigating the Java class definitions.
Our matching algorithm translates the table data to a set of class objects by comparing column names in the table data and parameter names in the class definition.
The matching algorithm automatically converts the data types between table and class objects using the reflection mechanism in the Java language, which provides the information of parameter names and types in the class definitions.
Thus, no manual mapping configuration is required to bind table data to class objects.
2.5 Importing biological data The UTGB Toolkit supports visualiztion of commonly used biological data formats, such as BED, DAS (Dowell et al., 2001), etc.
To create a track for displaying biological features mapped onto the genome, the user has to specify the data files to load (see the documentation at http://utgenome.org/toolkit/ for details).
The UTGB parses these biological data files in a stream manner and generates graphics that display the features in the region specified in the genome browser.
Access to DAS data is also supported in the UTGB Toolkit for utilizing existing biological data resources in conjunction with the users own tracks.
We are continuing the effort of improving the UTGB Toolkit to support other biological data formats, such as AGP, GFF, PSL, AXT (alignment data), etc.
3 RESULTS 3.1 UTGB framework for browsing various data sources A notable feature of the UTGB Toolkit is its framework design for integrating multiple web resources (Figure 2).
The interface of the UTGB browser consists of tracks, which display individual data resources.
To manipulate a set of tracks at the same time, we provide the notion of the track group, which holds variables that are shared between multiple tracks.
For example, the user can relocate the window positions of several tracks by changing their track group state, e.g.
by clicking the scroll button.
The genome browsers generated by using the UTGB Toolkit can display both public and private data sources on a local machine simultaneously.
1858 [14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1859 18561861 UTGB toolkit for personalized genome browsers Web Browser Web Server Data Resources Web Resorces Web Resources Web Server JavaScript codes generated from Java programs Track group paremters:-start = 1000, end = 2500-sequence_length = (db query)-... interactive  communication HTML/Text/XML Databases Database Query API PostgreSQL, MySQL,  SQLite, etc.
Data Files GET/POST (HTTP) Requests interactive  communication Web Resource  Adapters Interactive communication via AJAX between browser and web servers Track Content Arbitrary web contents, including HTML, texts, images, etc.
generates Track Track Track Group Track Content Track Content Track Content generates generates Fig.2.
Illustration of the UTGB framework.
The UTGB framework has a two-sided design, client-(browser) and server-side code.
For the client side, the UTGB Toolkit generates a web browser interface that consists of a set of tracks.
Individual tracks communicate with the web servers, and produce track contents from the received data in the form of, for example, graphics or table data.
Track groups, which manage a set of tracks, hold common parameters shared among tracks, such as window location on the genome sequence.
On the server side, arbitrary web data sources (e.g.
HTML, text, XML, database query results, etc.)
can be used to generate track contents using mini-browser (iframe) tracks or web resource adapters.
Advanced users can implement tracks in Java, which are compiled into JavaScript code, to provide a more sophisticated user interface.
3.2 Fast and flexible genome browser interface revision The interface of the UTGB genome browser displays a list of tracks, each of which can be dragged to a different location, allowing the user to customize the browser interface online.
The track interface of the UTGB Toolkit is implemented using JavaScript technology, which can dynamically rewrite HTML components in the browser window, so track relocation can be performed without accessing HTML pages on the server.
This feature greatly reduces user frustration when employing traditional genome browsers that reload the entire pages after the track relocation.
Reloading of track contents is independent for each track in the browser.
Therefore, even if the response time of the server providing a track contents is slow, other track contents served by other servers can be displayed immediately after the arrival of the data.
Therefore, the users can continue to relocate sequence positions on the UTGB genome browser seamlessly, eliminating the latency caused by the slowest server to display tracks.
3.3 UTGB shell for quick genome browser development To facilitate rapid genome browser development, we developed the UTGB Shell, a command-line user interface of the UTGB Toolkit.
Figure 3 shows the overview of the UTGB Toolkit and what can be done with the UTGB Shell.
The UTGB Shell has several user commands that support development of a personalized genome browser.
For example, the utgb create command creates a new genome browser in a few seconds, and the utgb server command launches a web server on the local machine, which enables immediate use of the genome browser.
For developers, track programs that use, for example, database searches, data visualization, can be implemented with minimal programming effort because utgb create command generates a genome browser code that already has features such as database connection support, a rich user interface and standard track implementations.
3.4 Stand-alone and web mode Genome browsers generated by the UTGB Toolkit work in two modes, stand-alone and web modes.
In the stand-alone mode, the genome browser runs as a user program on the local computer, so a privileged account is not required to run the genome browser program.
This stand-alone mode is useful in testing the behavior of the browser before publishing.
The web mode is for publishing the genome browser on a server machine.
The browser code is sent to the Tomcat engine, which is a standard web server program for running web applications written in Java.
With utgb deploycommand in the UTGB Shell, we can immediately deploy the genome browser on the Tomcat server.
Even if the server machine already is running another web server, such as Apache, these server programs can coexist by bypassing HTTP requests received by the Apache server to the Tomcat engine through the proxy module, and users can use the genome browser contents as if they were served by the Apache web server.
Detailed information regarding such settings is available from http://utgenome.org/.
3.5 Personalization of genome browser Maintenance of biological databases consists of data conversions to accommodate site-specific data formats, and data submission to appropriate sites.
In general, however, this process is not suited to visualizing the huge amounts of data that are now common in the era of large-scale genome analysis.
In addition, site-specific data formats and their graphical representations may not be ready 1859 [14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1860 18561861 T.L.Saito et al.
Fig.3.
Overview of the UTGB Toolkit.
The UTGB Toolkit supports development of personalized genome browsers in various ways.
The UTGB Shell generates code templates for handling web requests from the genome browser interface (web action), database access support through SQLite JDBC and mapping support from SQL query results to the specified class objects (BeanUtil).
Developers can generate track graphics by using the library included in the UTGB Toolkit or their own programs.
The generated graphics (or arbitrary HTML contents) can be displayed as track contents in the genome browser interface.
To browse both of the locally and remotely stored data, these steps can be performed in a local user machine by launching a local web server from the UTGB Shell.
or extendable to publish a variety of research results.
The UTGB Toolkit tackles these problems by providing a web browser interface to display tracks hosted by multiple web servers in a single window.
A set of standard tracks supporting visualization of data resources is already available.
The framework design of the UTGB Toolkit provides users with flexibility to browse their own data using both preinstalled and their own visualization programs.
To enhance the user experience, we also incorporated the modern web application technologies, such as the Google Web Toolkit (GWT), AJAX and client-side graphic drawing into our toolkit.
These technologies make several features of UTGB, such as drag-and-drops, flexible resizing and smooth scrolling, available to both users and web application developers.
3.6 Efficiency of client-side resource integration The Ensembls Genome Browser (Hubbard et al., 2009) is composed of a single set of image data integrating images of several tracks using server-side programs.
However, this architecture suffers from a major drawback: when the user clicks a browser button to move the location to display, the genome browser must redraw the whole contents already shown on the page.
This type of implementation, which we call server-side integration, is problematic as the server program has to perform similar database queries and repeatedly draw graphics even for slight window relocation.
Without a caching mechanism, this type of implementation cannot work efficiently.
The major reason why the Ensembl Genome Browser merges images is that the HTTP protocol is stateless; i.e.
the browser cannot remember the presented HTML data when the user clicks a link and moves to the next page.
To display the next page, the browser must retrieve the entire contents again from the server as stateless web browsers do not allow partial updating of the genome tracks.
With recent advances in browser technology, it has become possible to change the HTML contents dynamically after loading HTML and image data using the JavaScript language.
Several other technologies are available for drawing graphical contents in the browser, such as Flash, Adobe Air and Microsoft Silverlight.
However, these extensions of the web browser sometimes do not work; e.g.
if the browser does not have the required plug-ins to run these extensions or if plug-in installation is not allowed for non-privileged users.
Therefore, we chose JavaScript which is commonly supported in most modern web browsers, including Internet Explorer (IE), Firefox, Safari and Opera.
These browsers already have an embedded JavaScript engine, and therefore no additional installation process is required.
Scripts written in JavaScript language can update the displayed browser content in situ, and enable the web browsers to remember the state of the web page.
Therefore, modern web browsers already have the capability to partially modify displayed content while retaining necessary information in memory on the browser (client) side.
The UTGB Toolkit fully utilizes this browser (client)-side memory to preserve track contents, which enables the genome browser to support drag-and-drop and resizing of tracks without reloading.
The utilization of client-side memory has attracted a great deal of attention not only for the genome browsers, but also for developing general web applications.
In September 2008, Google launched a new web browser, Google Chrome.
This browser has its own implementation of the JavaScript engine called V8 with major performance improvements in the JavaScript garbage collection mechanism, which efficiently reuses browser-side memory.
The 1860 [14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1861 18561861 UTGB toolkit for personalized genome browsers open-source web browser Firefox 3 is also continuing to develop the JavaScript engine, and has achieved faster performance than the previous version, Firefox 2.
This trend of improving the JavaScript engine lends marked support for client-side resource integration.
3.7 Server-side and client-side graphic drawing With regard to graphical visualization of genomic data, the UTGB Toolkit provides several ready-to-use graphical representations, such as drawing genes and graph representations.
In the UTGB Toolkit, these visualization supports are available in server-side code, and we are continuing to develop other types of data visualization.
The UTGB Toolkit also supports browser-side graphic drawing with the canvas tag, which will be a standard of browser-side graphic drawing in HTML 5, the next version of HTML.
Several web browsers, including Google Chrome, Firefox, Safari and Opera, support drawing graphics with the canvas tag, with IE being the only major browser not providing support for this tag (as of 2008).
However, IE can draw graphics using Vector Markup Language (VML), which has similar functionality to canvas for drawing graphics.
A team at Google has developed GWT Canvas, a graphic drawing library, which hides differences between VML in IE and the canvas tag, so developers can draw graphics in both IE and other browsers that support this tag.
The UTGB Toolkit uses GWT Canvas to draw custom user tracks.
The capability of drawing graphics in the browser has an impact on genome browser design because it simplifies the role of the server; the server machine has no need to generate graphics, but rather its role is to serve data objects that represent genomic information such as genes and CDS.
The client (browser) receives these data objects and draws their graphical representation on behalf of the server.
This two-sided genome browser design greatly reduces the workload on the server machine and simplifies server-side programming; the server needs only to perform database queries and publish the query results.
3.8 Availability of the UTGB Toolkit The UTGB Toolkit is freely available from the UTGB Project Page (http://utgenome.org/).
All source code is managed using the source revision control system Subversion, so the latest code is made available immediately.
All of our source code is licensed under the Apache License version 2.0, which is an open-source license allowing free use and distribution for both personal and commercial purposes.
The Apache License is applied on a file basis; i.e.
modified source files must be licensed under the Apache License Version 2.0, while users are free to apply any license they wish to source code generated by the UTGB Toolkit and user-developed source code that simply uses the UTGB Toolkit.
4 DISCUSSION In general, database integration takes various forms: a portal, compound and fully integrated.
A portal collects links to internal or external data sources (e.g.
PubMed and ENCODEdb); search engines, such as Google and Yahoo, also belong to this category.
The second type, compound, displays several database contents at the same time in the browser window.
The interface of the UTGB can support this type of data visualization.
The IGB (Affymetrix, http://www.affymetrix.com/ partners_programs/programs/developer/tools/affytools.affx.)
is also a compoond browser and supports the integration of genome data resources provided by DAS protocol using the Java-based GUI.
Another example is iGoogle (http://www.google.co.jp/ig), a web service consisting of user-customizable gadgets, sub windows that can display, for example, news or a calendar.
The third type comprises fully integrated databases that merge several databases into a single DBMS to support queries across these databases; this type of integration is common in centralized database centers such as NCBI and Ensembl.
These various types of database integration each have their own benefits: portals can be used as directories to data resources, compound browsers can collect resources provided by several organizations and full integration is necessary to process queries that cannot be evaluated without integration of databases.
For example, a query listing all SNPs surrounding user-selected genes requires both SNP and gene locus databases.
Without integration of these databases, the user must click the browser buttons numerous times to navigate through unintegrated database browsers.
Our UTGB framework is not restricted to a certain pattern of database integration, and all of three of the above types can be used as back-end databases.
However, to achieve faster query performance and maintenance of the integrated databases, further implementation effort are required, for example, integrated query support, query optimization and user-friendly database management interfaces.
Funding: Japan Science and Technology Agency (JST).
Conflict of Interest: none declared.
ABSTRACT Motivation: Phosphorylation is a crucial post-translational protein modification mechanism with important regulatory functions in biological systems.
It is catalyzed by a group of enzymes called kinases, each of which recognizes certain target sites in its substrate proteins.
Several authors have built computational models trained from sets of experimentally validated phosphorylation sites to predict these target sites for each given kinase.
All of these models suffer from certain limitations, such as the fact that they do not take into account the dependencies between amino acid motifs within protein sequences in a global fashion.
Results: We propose a novel approach to predict phosphorylation sites from the protein sequence.
The method uses a positive dataset to train a conditional random field (CRF) model.
The negative training dataset is used to specify the decision threshold corresponding to a desired false positive rate.
Application of the method on experimentally verified benchmark phosphorylation data (Phospho.ELM) shows that it performs well compared to existing methods for most kinases.
This is to our knowledge that the first report of the use of CRFs to predict post-translational modification sites in protein sequences.
Availability: The source code of the implementation, called CRPhos, is available from http://www.ptools.ua.ac.be/CRPhos/ Contact: kris.laukens@ua.ac.be Suplementary Information: Supplementary data are available at1 INTRODUCTION Protein phosphorylation is an essential type of post-translational modification that consists of the addition of a phosphate (PO4) group to serine (S), threonine (T), tyrosine (Y) and to a lesser extent histidine (H) residues.
The process is catalyzed by a group of enzymes called kinases, and can be reverted by phosphatases.
Phosphorylation has important implications on the function of a protein.
If an enzyme gets phosphorylated its activity may be stimulated or inhibited, for example, leading to altered metabolic fluxes in the case of a metabolic enzyme, or resulting in the modulation of a regulatory effect if the substrate protein plays a regulatory role.
The human genome encodes more than 500 different kinases, many of which have been related to cancer and To whom correspondence should be addressed.
other diseases (Manning et al., 2002).
They regulate a diverse range of biochemical pathways and biological functions and are often indispensable signal integrators in a living system.
Being one of the most important reversible mechanisms of post-translational modification, phosphorylation is a prevalent subject of research in biochemistry.
A first step towards elucidating the phosphorylation network consists of the determination of the phosphorylated residues in a substrate protein for a given kinase.
Revealing the exact position of a phosphorylation in a sequence is essential to get irrefutable evidence for the assignment of a protein as a kinase substrate.
It also provides powerful clues for biomedical drug design or other biotechnological applications.
Phosphorylation sites on substrates are usually experimentally determined by mass spectrometry-based techniques (reviewed by Jensen, 2004).
This has led to several databases of phosphorylation sites, often tied to specific species, such as The Phosphorylation Site Database (Gnad et al., 2007), Phospho.ELM (Diella et al., 2004, 2008), PhosphoSite (Hornbeck, 2004) and PhosPhAt (Heazlewood et al., 2008).
Performing such experiments, however, remains time consuming, labor intensive and expensive.
These disadvantages have been anticipated by the bioinformatics community with the development of predictive models that are trained with experimentally annotated and known phosphorylation sites.
These models can be used to predict potential target sequences and thus significantly reduce the number of sequences that need to be verified by mass spectrometry.
Several computational models have been built and applied with varying success to predict phosphorylation sites, including hidden Markov models (HMMs) (Huang et al., 2005b), neural networks (Blom et al., 1999, 2004; Ingrell et al., 2007), group-based scoring method (Xue et al., 2005; Zhou et al., 2004), Bayesian decision theory (Xue et al., 2006), support vector machines (SVMs) (Kim et al., 2004; Plewczynski et al., 2005, 2008; Wong et al., 2007) and algorithms to identify short protein sequence motifs on recognized substrates (Neuberger et al., 2007; Obenauer et al., 2003).
Particularly the flanking sequence (typically 4, +4) around the potential sites (S/Y/T) is often used to develop these models.
Apart from the protein sequence, some additional information has also been integrated, including disorder information (Iakoucheva et al., 2004), structure information (Blom et al., 1999) and the distribution of the phosphorylated sites (Moses et al., 2007).
The majority of the computational models dedicated to predicting phosphorylation sites use the experimentally validated 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
T.H.Dang et al.
database Phospho.ELM (Diella et al., 2004, 2008) for training and for the evaluation of their performance.
Due to the fact that for some particular kinases in Phospho.ELM only a small number of phosphorylated sites is known, the annotated Swiss-Prot database (Boeckmann et al., 2003) is often used in complement to increase the size of the training and testing dataset.
In this article, we introduce a novel machine learning scheme that overcomes several disadvantages associated with existing methods.
The model is based on conditional random fields (CRFs) (Lafferty et al., 2001) and allows prediction of phosphorylated sites for each specific kinase separately.
The positive and negative datasets are flanking sequences of amino acids around the potentially phosphorylated residues.
Information about the chemical classes that individual amino acids belong to is also incorporated.
The CRF model is trained from only the positive training dataset.
The key idea of this approach is to generate the probability distribution for the positive data samples.
This derived distribution takes the probability values of the positive training dataset, calculated from the corresponding learned CRF model, as its values.
Within a set of protein sequences, the number of truly phosphorylated sites is always small compared to the number of non-phosphorylated sites.
To overcome this difficulty, we apply Chebyshevs Inequality from statistics theory to find high confidence boundaries of the derived distribution.
These boundaries are used to select a part of the negative training data, which is then used to calculate a decision threshold based on a user-provided allowed false positive rate.
To evaluate the performance of the method, k-fold cross-validations were performed on the experimentally verified phosphorylation dataset.
This new method performs well according to commonly used measures.
2 METHODS CRFs were introduced initially for solving the problem of labeling sequence data that arises in scientific fields such as bioinformatics and natural language processing.
In sequence labeling problems, each data item xi is a sequence of observations {xi1,xi2,...,xiT }.
The purpose of the technique is to make a prediction of the sequence labels, that is, yi = {yi1,yi2,...,yiT }, corresponding to this sequence of observations.
So far, in addition to CRFs, some probabilistic models have been introduced to tackle this problem, such as HMMs (Freitag and McCallum et al., 2000) and maximum entropy Markov models (MEMMs) (McCallum, et al., 2000).
In this section, we review and compare these models, before motivating and discussing our choice for the CRFs scheme.
2.1 Review of existing models An HMM is one of the most common methods for performing sequence labeling.
It is a generative model that maximizes the joint probability distribution p(X,Y ), where X and Y are random variables whose values take on all observation sequences and corresponding label sequences, respectively.
To calculate the joint probability, HMMs need to enumerate all possible observation sequences.
This is intractable when the number of atomic observations becomes large.
Moreover the interacting range between positions in a sequence is often long.
First-order HMMs relax these strict constraints by working with two assumptions.
The first one is the fact that a prediction of a future observation only depends on the present one (or on the immediate previous one).
As a result we have p(Xt+1|Xt,Xt1,...,X1) = p(Xt+1|Xt).
The second assumption is the time invariant or stationary: p(Xt+1|Xt) = p(X2|X1).
These limitations of HMMs in particular and generative models in general are the motivation behind the introduction of conditional models.
By maximizing the conditional probability p(Y |X ) from the training dataset, conditional models do not explicitly model the observation sequences.
Furthermore, these models remain valid if dependencies between arbitrary features exist in the observation sequences, and they do not need to account for these arbitrary dependencies.
The probability of a transition between labels may not only depend on the current observation but also on past and future observations.
MEMMs (McCallum et al., 2000) are a typical group of conditional probabilistic models.
Each state in a MEMM has an exponential model that takes the observation features as input, and outputs the distribution over the possible next states.
These exponential models are trained by an appropriate iterative scaling method in the maximum entropy framework.
On the other hand, MEMMs and non-generative finite state models based on next-state classifiers are all victims of a weakness called label bias (Lafferty et al., 2001).
In these models, the transitions leaving a given state compete only against each other, rather than against all other transitions in the model.
The total score mass arriving at a state must be distributed and observed over all next states.
An observation may affect which state will be the next, but does not affect the total weight passed on to it.
This will result in a bias in the distribution of the total score weight at a state with fewer next states.
In particular, if a state has only one out-going transition, the total score weight will be transferred regardless of the observation.
A simple example of the label bias problem has been introduced in the work of Lafferty et al.
(2001).
2.2 Conditional random fields CRFs are discriminative probabilistic models that not only inherit all advantages of MEMMs but also overcome the label bias weakness.
While MEMMs use exponential models of the current state to calculate the conditional probabilities of the next states, CRFs use a single exponential model for the conditional probability of all training labels, given the observation sequence.
Therefore, the weight of an arbitrary feature can be learned from its global interactions with all the other features.
This means that the weights of all the features within CRFs can be traded-off against each other.
CRFs have been applied to some common problems in natural language processing, such as NP (noun phrase)-chunking, POS (part of speech)-tagging and text segmentation (Sha and Pereira, 2003), and the experimental results are significantly better than those from HMMs and MEMMs.
In CRFs, the dependencies between the label components of a random variable Y are represented by an undirected graph G = (E,V ).
Let C be a set of cliques in graph G. Suppose that there exists a set of K feature functions fk(c,X) predefined in each clique c C, where k = 1...K .
According to the HammersleyClifford theorem, the conditional probability of a label sequence given the observation sequence is calculated as follows (Sha and Pereira, 2003): p ( Y |X) = 1 Zo cC ( c,X ) (1) Here Zo is the normalization function defined over all possible label sequences and (c,X) is called the potential function of clique c. This is a non-negative real-valued function and is defined as follows: ( c,X ) = ek fk(c,X) (2) The parameters k are learned globally from a labeled training dataset.
Although the graph G of Y may have a general structure for the problem of modeling the sequence the most simple and important structure is the linear chain structure.
Several authors have previously applied CRFs with a linear structure and obtained good performances (Lafferty et al., 2001; Sha and Pereira, 2003).
Within a linear structure, each clique is an edge with two end points.
The conditional probability formula can then be rewritten as follows: p ( Y ,X ) = 1 Zo exp  eE,k khk ( e,Y |e ,X )+ vV ,k kgk ( v,Y |v ,X ) (3) 2858 Conditional random fields for phosphorylation site prediction In this formula Y |e ,Y |v are components of the random variable Y corresponding to the edges and vertices of graph G, respectively.
The function gk and hk are the respective feature functions for the stateobservation pair and the statestate pair.
These are real-valued functions but are often defined as Boolean functions.
In the domain of phosphorylation site prediction, these feature functions, g1 for example, can be defined as follows: g1 = [ 1 if AA3 = "R"and AA2 = "K"and L ( AA0 ) = "Phos" 0 otherwise (4) Here AA3 = "R" means The amino acid three positions left from current AA is R and L ( AA0 ) = "Phos" means The label of the current amino acid is phosphorylated.
As explained in the Section 3.1, the statestate pair feature functions (hk in formula 3) are not declared in our implementation.
Several authors have proposed methods to efficiently induce such feature functions from datasets (Lafferty et al., 2001; McCallum, 2003; Pietra et al., 1997).
The weights of the CRFs are learned from the training dataset {xi,yi} to maximize the conditional log likelihood of label sequences {yi} (Sha and Pereira, 2003).
L = i logp ( xi,yi ) = i [ c k k,c fk ( c,xi )logZo(xi) ] (5) This likelihood function in CRFs is convex when the training label sequences (i.e.
a series of the labels phosphorylated and non-phosphorylated) make the state sequences (i.e.
a series of amino acids) unambiguous (McCallum, 2003).
In the case of phosphorylation site prediction this means that the training labels do corroborate the substrate specificity of the kinase.
This situation happens often in practice.
It guarantees that the global maximum value of the log likelihood of the conditional probability L will be found.
2.3 Proposed algorithm In this section, we introduce an algorithm that has all of the advantages of the CRFs discussed in the above section.
The algorithm follows a novelty detection approach, as previously successfully implemented in gene prioritization by De Bie et al.
(2007).
It builds a CRF model M+ for all training data objects that belong to the positive class.
In this application, we designed the features or patterns according to the motifs described in the biochemical literature on phosphorylation site prediction (reviewed by Kobe et al., 2005).
All patterns used are listed in the Supplementary Material.
If this set of features and patterns is well designed, the probabilities p(+|x,M+) that a positive training data object x is labeled as positive (+) are guaranteed to be the global maximum.
This is due to the convex characteristic of the conditional log likelihood function in CRFs.
They will distribute mainly near the largest probability value 1.
Furthermore, according to Chebyshevs Inequality (Ewens and Grant, 2001), given a random variable X and a real number n > 0, p(|X E(X)|n )1/n2.
Here E(X)and 2 denote the expected value and the variance of variable X, respectively.
This means that the confidence degree of a value of X belonging to the range [E(X)n , E(X)+n ] is larger than (11/n2).
For example, with n = 3, the confidence degree is >89%.
From now this interval will be referred to as the n-confidence interval.
When applied to the distribution of the probability values p(+|x,M+), the expected value can be estimated by the average value of all values p(+|x,M+), with x being the positive training data objects.
The n-confidence interval is enlarged by increasing the value n until the upper bound equals 1.
This interval is used in the proposed algorithm to overcome the difficulty that the number of examples in the positive training dataset is very small.
Due to the guarantee of obtaining the global maximum of the CRFs, the n-confidence interval is expected to contain all values p(+|x,M+) of all real positive data objects.
Moreover, the negative training dataset may contain some phosphorylated residues that have not yet been experimentally verified as such.
These negative data will then get high probabilitieswithin the n-confidence intervalof being labeled as positive, and will not be considered during the process of controlling the false positive rate of the obtained classifier.
Algorithm Input: Positive training dataset D+ and Negative training dataset D. Predefined False Positive Rate (PFPR) of the obtained predictor.
Output: A predictor including a model M+ and a decision threshold so that the observed False Positive Rate is expected to equal PFPR.
(1) Generate the positive CRF model M+ from the positive training data set D+.
(2) Initialize an empty array Thres.
(3) For each data object xD+ (4) Calculate probability of predicting x as positive (+) given the model M+, P+ = p(+|x,M+) (5) Calculate the n-confidence interval of the distribution of P+ so that the up bound equals 1.
(6) For each data object yD (7) Calculate probability of predicting y as positive (+) given the model M+, P = p(+|y,M+) and insert into array Thres if P/ n-confidence interval.
(8) Sort the array Thres according to ascending order.
(9) = Thres[(length(Thres)1)PFPRlength(Thres)] (10) Return (Model M+, Decision threshold ) A new data object will be classified as positive if the probability of classifying it as positive given the model M+is greater than or equal to the threshold .
In all experiments, we used the open source software tool CRF++ {{http://crfpp.sourceforge.net/}} to build the model.
3 RESULTS AND DISCUSSION 3.1 Implementation We used the Phospho.ELM (Diella et al., 2008) (version 0707) database to experimentally evaluate our approach.
This dataset has been used as a benchmark to test the performance of most computational phosphorylation prediction models previously published.
Phospho.ELM contains experimentally verified phosphorylation sites in eukaryotic proteins, manually curated from the literature.
It stores information about substrate proteins with the exact positions of the residues that are experimentally verified to be phosphorylated by a given kinase.
For each potentially phosphorylated residue (S, T or Y), we extracted the nine amino acid sequence, including the central residue, surrounding it (from 4 to +4).
All of these sequences of which the central residue was annotated as phosphorylated by a given kinase were considered as the positive set, whereas all remaining 9mer sequences on the same substrate proteins, were considered as negative examples.
Following Kim et al.
(2004), we discarded highly homologous sequences (over 70% identity) from the positive and negative training dataset to avoid overestimation on accuracy when cross-validating.
Such bias appears if the testing data are highly homologous to the training data.
The number of positive and negative samples for different kinases, after removing the redundancies, is shown in Table 1.
There are clearly much more negative samples than positive ones.
Apart from the amino acid itself, the chemical/structural group that an amino acid belongs to is used as an additional feature for each residue.
Twenty amino acids were grouped into eight different clusters (Table 2) according 2859 T.H.Dang et al.
Table 1.
The size of positive and negative datasets for some common protein kinases, obtained from Phospho.ELM version 0707 Protein kinase Positive size Negative size Abl (Proto-oncogene tyrosine-protein kinase) 45 1209 ATM (Ataxia telangiectasia mutated) 55 1882 CaM-KII (Calcium/calmodulin-dependent 50 1829 protein kinases) CDK (Cyclin-dependent kinases) 104 1990 CK1 (Casein kinases 1) 42 1051 CK2 (Casein kinases 2) 226 3875 DNA-PK (DNA-dependent protein kinase 20 632 catalytic subunit) EGFR (Epidermal growth factor receptor) 44 823 Fyn (Proto-oncogene tyrosine-protein kinase) 48 1409 GSK-3 (Glycogen synthase kinases 3) 32 866 InsR (Insulin receptor) 44 724 Met (Hepatocyte growth factor receptor) 13 132 mTOR (FK506 binding protein 12-rapamycin 13 50 associated protein 1) PKA (cAMP-dependent protein kinase) 310 8823 PKB (Protein kinases B) 79 3563 PKC (Protein kinase) 227 4428 Src (Proto-oncogene tyrosine-protein kinase) 141 2681 Syk (Tyrosine-protein kinase) 45 680 Table 2.
The chemical classes to which the 20 amino acids belong, based on Wong et al.
(2007) Group name Amino Acids Sulfur C, M Aliphatic 1 A, G, P Aliphatic 2 I, L, V Acid D, E Base H, K, R Aromatic F, W, Y Amide N, Q Small hydroxy S, T to their common chemical/structural properties (Wong et al., 2007).
For each position in the positive sequence data, a set of Boolean value feature functions was declared, including functions for amino acids (e.g.
formula 4), for chemical groups (e.g.
formula 6) and for combinations of amino acids and chemical groups (e.g.
formula 7).
g2 = [ 1 if G3 = "Sulfur"and G2 = "Base"and L ( AA0 ) = "Phos" 0 otherwise (6) g3 = [ 1 if A3 = "R"and G2 = "Base"and L ( AA0 ) = "Phos" 0 otherwise (7) Here G3 = "Sulfur"means The chemical group of the amino acid (AA) three positions left from the current AA belongs to the cluster Sulfur and L(AA0) = "Phos" means The label of the current amino acid is phosphorylated.
N R K Q S W F D H Amide Base Base Amide SmallHydroxy Aromatic Aromatic Acid Base S((N, Amide), (R, Base), (K, Base), (Q, Amide), (W, Aromatic),  (F, Aromatic), (D, Acid), (H, Base)) Transformed to an object S Fig.1.
Method for transforming an amino acid sequence to a data object of the central amino acid.
When applying the algorithm (Section 2.3) to build a predictive model from the positive (i.e.
central residue is phosphorylated) and negative (i.e.
central residue is not phosphorylated) sequence data, the conditional probabilities in Steps 4 and 7 are probabilities of the central residues in the sequence data having the label Phos (i.e.
phosphorylated).
These probabilities are equivalent to the total sum of the probabilities of all possible label sequences of which the central labels are Phos, assigned by a CRF given the flanking sequence of amino acids.
This increases the computational complexity of the algorithm due to the required enumeration of all possible surrounding labels.
To tackle this problem, we introduce a transferring method that is applied to the sequence data as follows.
Each nine-residue long amino acid sequence is represented in an equivalent form, where the center residue (S, Y or T) is a data object and the surrounding residues themselves and their corresponding features become the new features (Fig.1).
The information about the positions of the residues is conserved, thus the CRF model still has the ability to exploit the meaning of residue positions if suitable feature functions (gk) are used.
The statestate feature functions (hk , formula 3) are not further declared since the dependencies between labeling information of the surrounding amino acids is omitted in this new representation.
3.2 Evaluation To evaluate the performance of the algorithm, k-fold cross-validation was used for the model trained from the large datasets, whereas Jackknife cross-validation was applied when the models were trained with less than 30 positives.
Each cross-validation was performed 20 times, and after each round we calculated Sensitivity (Sn) = TP/(TP+FN) and Specificity (Sp) = TN/(TN+FP).
Here TP, TN, FP and FN are true positive, true negative, false positive and false negative values, respectively.
The average values after 20 runs were used as the final measure of the performance for the model.
For each kinase-specific phosphorylation predictor, the ROC (receiver operating characteristic) curve, which shows the tradeoff between sensitivity and specificity, was generated from the final average.
The ROC curves obtained from different k-fold cross-validations (k = 2, 4, 6, 8, 10) were approximately the same (data not shown).
For the sake of clarity, all shown ROC curves are the result from 10-fold cross-validation (Fig.3 and Supplementary figures, blue lines).
All ROC curves, except CDK1 and PKB, reach 100% sensitivity with a specificity of at least 20%.
Because the number of positives is much smaller than the number of negatives, this implies a significant reduction in the number of required validations, even if no false negatives are desired.
2860 Conditional random fields for phosphorylation site prediction Fig.2.
Relation between expected and observed specificity values of obtained predictor.
All lines are generated using linear regression.
We also validated whether the observed specificity value of a classifier generated from the method is close to the expected value.
For each value of an expected specificity, a 4-fold cross-validation procedure was implemented 20 times.
The average observed specificity was calculated and compared with the expected value (Fig.2).
These values were identical for kinases with a negative training dataset larger than 1500.
For kinases with a smaller negative training set, the smallest regression coefficient was 0.97, for the mTOR kinase, of which the number of negative training sequences was only 50.
As a consequence, the algorithm can return any desired point (classifier) on the ROC curve based on taking into account an expected specificity value as input.
The model proposed in this article uses the positive dataset for training, and uses the negative data to calculate a decision threshold.
In order to demonstrate the efficiency of this approach, we also tested a conventional approach, using both the positive and negative data for training a CRF model.
For this experiment, the nine amino acid protein sequences from both the positive and the negative dataset were taken as input to the learning algorithm of the CRFs.
The derived ROC curves are shown in red in Figure 3 and in the Supplementary Material.
For most kinases, this conventional approach results in a slightly worse ROC curve, indicating that our approach outperforms the application of CRFs trained on both positive and negative data.
3.3 Comparison The derived ROC curves allow for easy comparison of our method with reported performance measures from other methods.
We followed two different approaches.
The approach applied by most authors of phosphorylation site prediction methods, is the direct comparison of obtained results with previously reported performances (Huang et al., 2005a; Kim et al., 2004; Zhou et al., 2004).
If available, performance values, reported in literature as pairs of sensitivities/specificities, were shown as colored dots on the ROC plots for each kinase method (Fig.3 and Supplementary Fig.1).
These values can be considered worse or better, depending on whether these dots fall below or above the CRPhos ROC curve, respectively.
In most cases, CRPhos yielded a performance that is comparable or better than other methods.
(SVMs-based approaches applied in Predphospho (Kim et al., 2004) and KinasePhos 2.0 (Wong et al., 2007) do perform better in some instances (e.g.
both in CK2, KinasePhos 2.0 in PKC, PredPhospho in CDK), but worse in other cases (both in PKA, PredPhospho in PKC).
However, both predictors have been validated on data of which the size of the negative and positive subset has been equalized, in contrast to this article.
Compared with PPSP (Xue et al., 2006), CRPhos performs better for the majority of the kinases, but worse or similar for a few.
From all kinases, only the prediction for CK2 by CRPhos is generally worse than those by other prediction methods, although even then CRPhos achieves both sensitivity and specificity values above 80%.
NetphosK could only be compared for PKA and ATM, yielding worse and better performance, respectively.
Except for CK2, CRPhos performs similar or better than the other methods, including GPS (Zhou et al., 2004), Scansite (Obenauer et al., 2003) and KinasePhos 1.0 (Huang et al., 2005a).
There is a chance that the version of the dataset, which is different for previously published models, affects the above comparison.
An ideal solution to perform an unbiased comparison is running new cross-validations on all existing methods using the same dataset that we used.
This is practically hard to achieve since trainable versions of most tools are not available.
An alternative solution consists of testing and comparing our method and other existing ones on the same testing dataset.
There is however a high chance to get a biased comparison if some testing data are already learned by one of the methods.
To eliminate this problem, a more rigorous approach was recently deployed by Wan et al.
(2008).
They generated a subset of Phospho.ELM, called MetaPS06, which contains the phosphorylation sites that were only recently added, after publication of existing prediction models.
This MetaPS06 set does not overlap with any previously used training data.
By testing this dataset against different prediction tools, Wan and Colleagues (2008) obtained comparable performance measurements that represent the predictive power of each tool.
To generate equivalent performance values, we removed from Phosphos.ELM version 07 all phosphorylated sites originated from Phospho.ELM version 06 (with annotation data <12/31/2004), as described (Wan et al., 2008).
For this experiment the removed dataset was used to train the CRPhos model, whereas the remaining fraction was used for testing.
The results (Fig.4) demonstrate that the performance of CRPhos remains better than the performance of most other methods.
Unlike other methods, CRPhos learns the model only from the golden positive dataset and not from the un-golden negative dataset.
This negative dataset could contain some real phosphorylated (positive) data that have not yet been experimentally validated.
This may cause a bias in the prediction by models that are trained from both positive and negative data.
Moreover, we also cross-validated our model using the older versions of Phospho.ELM.
versions 06 & 1206.
Supplementary Figure 2 demonstrates that this has almost no effect on the performance.
A significant advantage of the method described in this article lies in the fact that it is able to generate predictions for all possible specificity values.
Any classifier, defined by a point in the ROC curve, can be readily obtained, whereas other approaches are only able to generate one classifier with a fixed sensitivity/specificity.
2861 T.H.Dang et al.
Fig.3.
ROC curves of our method for some well-studied kinases, using 10-fold cross-validation (CRPhos).
CRF* stands for the equivalent curve for a CRF model learned from both the positive and negative training dataset.
For comparison, corresponding performance measures reported in literature are shown: PPSP (Xue et al., 2006), Scansite (Obenauer et al., 2003), NetPhosK (Blom et al., 2004), KinasePhos 1.0 (Huang et al., 2005a), KinasePhos 2.0 (Wong et al., 2007), GPS (Zhou et al., 2004) and PredPhospho (Kim et al., 2004).
4 CONCLUSION In this article, we introduced a novel approach based on CRFs to predict kinase-specific phosphorylation sites.
Upon validation with a real dataset of phosphorylation sites, the method yielded accurate predictions that were similar or better than predictions obtained with existing methods.
This is consistent with the theoretical advantages of CRFs, including the convergence to the global maximum of the log likelihood conditional probability and the capability of capturing all amino acid motifs and their interactions in a global fashion.
Our approach employs Chebyshevs Inequality to find the confidence interval for the distribution of the real positive data.
As a result, it overcomes the difficulty that, in reality, the size of the experimentally verified positive data is very small compared to that of the negative data.
Moreover, the use of Chebyshevs Inequality also allows eliminating the noisy negative data, which may contain target sites that have not yet been experimentally assigned as positive.
Finally, this method allows obtaining an optimal prediction for any given allowed false positive rate.
This gives the end-user extra flexibility, especially when applied in situations where either incomplete detection, or false positives are undesired.
ACKNOWLEDGEMENTS The authors are grateful to Koen Smets for valuable feedback on the article.
They also wish to thank Taku Kudo for releasing the CRF++ tool under an open source license.
Francesca Diella and the Phospho.ELM team are gratefully acknowledged for providing the Phospho.ELM dataset and for offering useful suggestions.
Funding: SBO grant (IWT-600450) of the Flemish Institute supporting ScientificTechnological Research in industry (IWT); the EU project Inductive Queries for Mining Patterns and Models (IQ).
Conflict of Interest: none declared.
2862 Conditional random fields for phosphorylation site prediction Fig.4.
Performance of CRPhos with the testing dataset that is created according to the scheme in Wan et al.
(2008).
The remaining dataset after removing this testing data from Phospho.ELM v.07 was used to train CRPhos.
The performance measure of other existing methods, reported by Wan et al.
(2008), are shown for comparison.
ABSTRACT Summary: We developed a Python package, ProDy, for structure-based analysis of protein dynamics.
ProDy allows for quantitative characterization of structural variations in heterogeneous datasets of structures experimentally resolved for a given biomolecular system, and for comparison of these variations with the theoretically predicted equilibrium dynamics.
Datasets include structural ensembles for a given family or subfamily of proteins, their mutants and sequence homologues, in the presence/absence of their substrates, ligands or inhibitors.
Numerous helper functions enable comparative analysis of experimental and theoretical data, and visualization of the principal changes in conformations that are accessible in different functional states.
ProDy application programming interface (API) has been designed so that users can easily extend the software and implement new methods.
Availability: ProDy is open source and freely available under GNU General Public License from http://www.csb.pitt.edu/ProDy/.
Contact: ahb12@pitt.edu; bahar@pitt.edu Received on December 26, 2010; revised on March 9, 2011; accepted on March 27, 2011 1 INTRODUCTION Protein dynamics plays a key role in a wide range of molecular events in the cell, including substrate/ligand recognition, binding, allosteric signaling and transport.
For a number of well-studied proteins, the Protein Data Bank (PDB) hosts multiple high-resolution structures.
Typical examples are drug targets resolved in the presence of different inhibitors.
These ensembles of structures convey information on the structural changes that are physically accessible to the protein, and the delineation of these structural variations provides insights into structural mechanisms of biological activity (Bakan and Bahar, 2009; Yang et al., 2008).
Existing computational tools and servers for characterizing protein dynamics are suitable for single structures [e.g.
Anisotropic Network Model (ANM) server (Eyal et al., 2006), elNmo (Suhre and Sanejouand, 2004), FlexServ (Camps et al., 2009)], pairs of structures [e.g.
open and closed forms of enzymes; MolMovDB (Gerstein and Krebs, 1998)], or nucleic magnetic resonance (NMR) models [e.g.
PCA_NEST (Yang et al., 2009)].
Tools for systematic retrieval and analyses of ensembles of structures are not publicly accessible.
Ensembles include X-ray structures for a given protein and its complexes; families and subfamilies of proteins that belong to particular structural folds; or a protein and its mutants resolved in the presence of different inhibitors, ligands or substrates.
The To whom correspondence should be addressed.
analysis of structural variability in these ensembles could open the way to gain insights into rearrangements selected/stabilized in different functional states (Bahar et al., 2007, 2010), or into the structure-encoded dynamic features shared by protein family or subfamily members (Marcos et al., 2010; Raimondi et al., 2010; Velazquez-Muriel et al., 2009).
The lack of software for performing such operations is primarily due to the non-uniform content of structural datasets such as sequence variations at particular regions, including missing or substituted residues, short segments or loops.
We developed ProDy to analyze and retrieve biologically significant information from such heterogeneous structural datasets.
ProDy delivers information on the structural variability of target systems and allows for systematic comparison with the intrinsic dynamics predicted by theoretical models and methods, thus helping gain insight into the relation between structure, dynamics and function.
2 DESCRIPTION AND FUNCTIONALITY 2.1 Input for ProDy The input for ProDy is the set of atomic coordinates in PDB format for the protein of interest, or simply the PDB id or sequence of the protein.
Given a query protein, fast and flexible ProDy parsers are used to Blast search the PDB, retrieve the corresponding files (e.g.
mutants, complexes or sequence homologs with user-defined minimal sequence identity) from the PDB FTP server and extract their coordinates and other relevant data.
Additionally, the program can be used to analyze a series of conformers from molecular dynamics (MD) trajectories inputted in PDB file format or programmatically through Python NumPy arrays.
More information on the input format is given at the ProDy web site tutorial and examples.
2.2 Protein dynamics from experiments The experimental data refer to ensembles of structures, X-ray crystallographic or NMR.
These are usually heterogeneous datasets, in the sense that they have disparate coordinate data arising from sequence dissimilarities, insertions/deletions or missing data due to unresolved disordered regions.
In ProDy, we implemented algorithms for optimal alignment of such heterogeneous datasets and building corresponding covariance matrices.
Covariance matrices describe the mean-square deviations in atomic coordinates from their mean position (diagonal elements) or the correlations between their pairwise fluctuations (off-diagonal elements).
The principal modes of structural variation are determined upon principal component analysis (PCA) of the covariance matrix, as described previously (Bakan and Bahar, 2009).
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[16:12 12/5/2011 Bioinformatics-btr168.tex] Page: 1576 15751577 A.Bakan et al.
Fig.1.
Comparative analysis of p38 dynamics from experiments (PCA) and theory (ANM).
(A) Overlay of 150 p38 X-ray structures using ProDy.
An inhibitor is shown in space-filling representation.
(B) Network model (ANM) representation of p38 (generated using NMWiz and VMD).
(C) Comparison of the principal mode PC1 (from experiments; violet arrows) and the softest mode ANM1 from theory (green arrows) and (D) overlap of the top five modes.
(E) Distribution of X-ray structures (blue) and ANM-generated conformers (red) in the subspace spanned by PC1-3.
The green ellipsoid is an analytical solution predicted by the ANM.
2.3 Protein dynamics from theory and simulations We have implemented classes for Gaussian network model (GNM) analysis and for normal mode analysis (NMA) of a given structure using the ANM (Eyal et al., 2006).
Both models have been widely used in recent years for analyzing and visualizing biomolecular systems dynamics (Bahar et al., 2010).
The implementation is generic and flexible.
The user can (i) build the models for any set of atoms, e.g.
the substrate or inhibitor can be explicitly included to study the perturbing effect of binding on dynamics, and (ii) utilize user-defined or built-in distance-dependent or residue-specific force constants (Hinsen et al., 2000; Kovacs et al., 2004).
ProDy also offers the option to perform essential dynamics analysis (EDA; Amadei et al., 1993) of MD snapshots, which is equivalent to the singular value decomposition of trajectories to extract principal variations (Velazquez-Muriel et al., 2009).
2.4 Dynamics analysis example Figure 1 illustrates the outputs generated by ProDy in a comparative analysis of experimental and computational data for p38 kinase (Bakan and Bahar, 2011).
Figure 1A displays the dataset of 150 X-ray crystallographically resolved p38 structures retrieved from the PDB and optimally overlaid by ProDy.
The ensemble contains the apo and inhibitor-bound forms of p38, thus providing information on the conformational space sampled by p38 upon inhibitor binding.
Parsing structures, building and diagonalizing the covariance matrix to determine the principal modes of structural variations takes only 38 s on Intel CPU at 3.20 GHz.
Figure 1C illustrate the first principal mode of structural variation (PC1; violet arrows) based exclusively on experimental structural dataset for p38.
As to generating computational data, two approaches are taken in ProDy: NMA of a representative structure using its ANM representation (Figure 1B; color-coded such that red/blue regions refer to largest/smallest conformational mobilities); and EDA of MD trajectories provided that an ensemble of snapshots is provided by the user.
The green arrows in Figure 1C describe the first (lowest frequency, most collective) mode predicted by the ANM, shortly designated as ANM1.
The heatmap in Figure 1D shows the overlap (Marques and Sanejouand, 1995) between top-ranking PCA and ANM modes.
The cumulative overlap between the top three pairs of modes is 0.73.
An important aspect of ProDy is the sampling of a representative set of conformers consistent with experimentsa feature expected to find wide utility in flexible docking and structure refinement.
Figure 1E displays the conformational space sampled by experimental structures (blue dots), projected onto the subspace spanned by the top three PCA directions, which accounts for 59% of the experimentally observed structural variance.
The conformations generated using the softest modes ANM1-ANM3 predicted to be intrinsically accessible to p38 in the apo form, are shown by the red dots.
The sizes of the motions along these modes obey a Gaussian distribution with variance scaling with the inverse square root of the corresponding eigenvalues.
ANM conformers cover a subspace (green ellipsoidal envelope) that encloses all experimental structures.
Detailed information on how to generate such plots and figures using ProDy is given in the online documentation, along with several examples of downloadable scripts.
2.5 Graphical interface We have designed a graphical interface, NMWiz, to enable users to perform ANM and PCA calculations from within a molecular visualization program.
NMWiz is designed as a VMD (Humphrey et al., 1996) plugin, and is distributed within the ProDy installation package.
It is used to do calculations for molecules loaded into VMD; and results are visualized on the fly.
The plug-in allows for depicting color-coded network models and normal mode directions (Fig.1B and C), displaying animations of various PCA and ANM modes, generating trajectories, and plotting square fluctuations.
2.6 Supporting features ProDy comes with a growing library of functions to facilitate comparative analysis.
Examples are functions to calculate, print and plot the overlaps between experiment, theory and computations (Fig.1D) or to view the spatial dispersion of conformers (Fig.1E).
For rapid and flexible analysis of large numbers of PDB structures, we designed a fast PDB parser.
The parser can handle alternate locations and multiple models, and read specified chains or atom subsets selected by the user.
We evaluated the performance of ProDy relative to Biopython PDB module (Hamelryck and Manderick, 2003) using 4701 PDB structures listed in the PDB SELECT dataset (Hobohm and Sander, 1994): we timed parsers for reading the PDB files and returning C-coordinates to the user (see documentation).
The Python standard Biopython PDB parser evaluated the dataset in 52 min; and ProDy in 11 min.
In addition, we implemented an atom selector using Pyparsing module for rapid access to subsets of atoms in PDB files.
This feature reduces the user programming effort to 1576 [16:12 12/5/2011 Bioinformatics-btr168.tex] Page: 1577 15751577 ProDy access any set of atoms down to a single line of code from several lines composed of nested loops and comparisons required with the current Python packages for handling PDB data.
The implementation of atom selections follows that in VMD.
Full list of selection keywords and usage examples are provided in the documentation.
ProDy also offers functions for structural alignment and comparison of multiple chains.
3 DISCUSSION Several web servers have been developed for characterizing protein dynamics, including elNmo (Suhre and Sanejouand, 2004), ANM (Eyal et al., 2006) and FlexServ (Camps et al., 2009).
These servers perform coarse-grained ENMbased NMA calculations, and as such are useful for elucidating structure-encoded dynamics of proteins.
FlexServ also offers the option to use distance-dependent force constants (Kovacs et al., 2004), in addition to protocols for coarse-grained generation and PCA of trajectories.
ProDy differs from these as it allows for systematic retrieval and comparative analysis of ensembles of heterogeneous structural datasets.
Such datasets includes structural data collected for members of a protein family in complex with different substrates/inhibitors.
ProDy further allows for the quantitative comparison of the results from experimental datasets with theoretically predicted conformational dynamics.
In addition, ProDy offers the following advantages: (i) it is extensible, interoperable and suitable for use as a toolkit for developing new software; (ii) it provides scripts for automated tasks and batch analyses of large datasets; (iii) it has a flexibleAPI suitable for testing new methods and hypotheses, and benchmarking them against existing methods with minimal effort and without the need to modify the source code; (iv) it allows for producing publication quality figures when used with Python plotting library Matplotlib; and (v) it provides the option to input user-defined distance-dependent force function or utilize elaborate classes that return force constants based on the type and properties of interacting residues [e.g.
based on their secondary structure or sequence separation (Lezon and Bahar, 2010)].
4 CONCLUSION ProDy is a free, versatile, easy-to-use and powerful tool for inferring protein dynamics from both experiments (i.e.
PCA of ensembles of structures) and theory (i.e.
GNM, ANM and EDA of MD snapshots).
ProDy complements existing tools by allowing the systematic retrieval and analysis of heterogeneous experimental datasets, leveraging on the wealth of structural data deposited in the PDB to gain insights into structure-encoded dynamics.
In addition, ProDy allows for comparison of the results from experimental datasets with theoretically predicted conformational dynamics.
Finally, through a flexible Python-based API, ProDy can be used to quickly test and implement new methods and ideas, thus lowering the technical barriers to apply such methods in more complex computational analyses.
Funding: National Institutes of Health (1R01GM086238-01 to I.B.
and UL1 RR024153 to A.B.).
Conflict of Interest: none declared.
ABSTRACT Summary: The new version of the TRITON program provides user-friendly graphical tools for modeling protein mutants using the external program MODELLER and for docking ligands into the mutants using the external program AutoDock.
TRITON can now be used to design ligand-binding proteins, to study proteinligand binding mechanisms or simply to dock any ligand to a protein.
Availability: Executable files of TRITON are available free of charge for academic users at http://ncbr.chemi.muni.cz/triton/ Contact: triton@chemi.muni.cz Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Automated proteinligand docking is an effective method for predicting the conformation of a ligand bound to a receptor and the structure of the complex (Sousa et al., 2006).
Although the method is typically used for screening a library of potential inhibitors (Kitchen et al., 2004), it can also be useful in computational protein design (Kraemer-Pecore et al., 2001).
A possible strategy in designing ligand-binding proteins is the combination of docking methods with computational site-directed mutagenesis.
In this approach, the 3D structure of a wild-type protein is used as a template for modeling protein mutants with amino acid residue substitutions (deletions, insertions) at a defined site, typically at the binding site of the protein.
Subsequently, ligands are docked into these protein mutants to investigate their binding properties such as ligand-binding modes and affinities.
Protein mutants with predicted high binding affinity or desired ligand specificity can then be studied experimentally.
In silico design can substantially reduce the amount of experimental work and it can assist in experimental protein design studies.
Since computational site-directed mutagenesis combined with subsequent modeling of mutant properties is accompanied by the processing of high amounts of input and output data for computational programs, development of user-friendly graphical software, which would automate these operations, is highly desirable.
2 METHODS Our idea with TRITON software development was to create a user-friendly graphical tool that would automate and simplify utilization of computational To whom correspondence should be addressed.
software suitable for computational protein design.
In the previous version of TRITON, we have implemented computational site-directed mutagenesis methodology to study enzymatic reactions (Prokop et al., 2000).
Here, we present a new version of TRITON which is focused on studies of the properties of proteinligand complexes.
The computational site-directed mutagenesis was transferred from the previous version and mutant protein structures are modeled by external software MODELLER (Sali and Blundell, 1993).
Additionally, ligand-binding modes and affinities of these mutants are calculated by docking software AutoDock (Morris et al., 1998).
Preparation of input data for these external programs and visualization of the output is performed by TRITON.
The program and the methodology were tested on docking monosaccharide ligands into PA-IIL lectin of Pseudomonas aeruginosa and its mutants S22A, S23A and G24N (see Supplementary Material for details).
3 IMPLEMENTATION Program TRITON is a user oriented software with graphical interface that enables visualization of molecular structures, preparation of input files for computational software and analysis of output data.
Computational data are organized in hierarchically structured projects.
For each calculation, a separate project is created.
Projects are displayed in the form of a tree list in the main window of the program (Supplementary Fig.1) which enables fast access to input and output data.
For user-friendly preparation of input data, TRITON offers wizards that lead the user step by step in the process of input structures, parameters and other data specifications.
In the current version, four wizards are available: Mutagenesis for modeling mutants by MODELLER, Docking for proteinligand docking by AutoDock, Reaction for calculation of reaction pathways by MOPAC and Optimization for optimization of structure geometry by MOPAC.
Specific tools for analysis of output data of calculations are also implemented.
Here, we will describe only mutagenesis, which has partially been improved from the previous version of TRITON, and docking, which is a new option not included in the previous version of TRITON.
3.1 Mutagenesis The Mutagenesis wizard assists in specification of input structure of a protein wild-type in PDB format (which is used as a template for homology modeling by MODELLER).
One-, two-or multiple-point mutations are possible by specification of residues to be mutated and the required substitutions.
Also parameters for MODELLER have to be set.
Multiple preconfigured versions of MODELLER can be used.
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
M.Prokop et al.
Computations can be run directly from the graphical interface of TRITON on a local computer.
For each mutant, a separate project is generated with related input data files.
After finishing computations, input and output data are accessible from each project.
They can be visualized using standard tools described below.
3.2 Proteinligand docking The Docking wizard is used for specification of input data for proteinligand docking calculations.
First, input structure of the receptor protein is specified in the wizard.
Then superfluous molecules, e.g.
crystallographic waters or unwanted ligands, can be removed.
Hydrogen atoms have to be added to protein residues if they are not present in the input file.
Next, partial atomic charges have to be set.
Two types of charges are implemented: united atom charges (Weiner et al., 1984) are recommended for AutoDock version 3 and Gasteiger PEOE charges (Gasteiger and Marsili, 1980) should be used with AutoDock version 4.
Subsequently, solvation parameters and atom types are set automatically, although they can also be modified manually.
A docking box, which defines the area where ligand moves during the docking procedure, is also specified.
Some sidechains of the binding site can be marked to be flexible if required.
In the next part of the wizard, the user specifies a file containing the structure of the ligand.
Hydrogens can be added to the ligand structure and atom types can also be specified.
Then, active bonds, i.e.
bonds to be rotated during the docking procedure, are marked.
After completing the above specifications, the docking method and parameters for AutoDock are set.
Calculation can be run automatically on the local computer or manually on a remote computer.
After finishing the calculation, output data are displayed in the Output Data folder of the project (Supplementary Fig.1).
TRITON loads output structures into the main window and displays a dialog box where binding modes can be chosen from a list which is sorted by model or cluster number or by calculated binding energy.
Visualization of affinity maps help to investigate which areas have high affinity of specified ligand atoms toward the receptor.
Additionally, a graph depicting electrostatic interactions of individual ligand atoms with receptor residues can be generated.
Structures of calculated proteinligand complexes can be saved in PDB format.
If a new calculation with different parameters is required, it is possible to use the project cloning function.
In this case, input structures and settings are copied to the new project from the existing user-specified project.
The parameters and settings can then be modified in the wizard as required.
3.3 Graphical tools Program TRITON offers the basic tools needed to manipulate 3D molecular structures.
It can handle files in PDB and Mol2 formats as well as AutoDock input files (PDBQ, PDBQS, PDBQT) and MOPAC input and output files.
Structures can be visualized as a 3D model in various representations (wire, stick, ball and stick, CPK) and colors.
The source file from which the structure was loaded can be displayed in interactive form, i.e.
with the possibility to click on atoms in the list provided.
An interactive list of residues is provided for fast selection, coloring and other operations on the protein structure.
Another tool enables adding polar and non-polar hydrogen atoms automatically to the standard protein residues or manually to atoms of non-standard residues.
Measurement of distances and angles between atoms is also possible, as well as structure superposition, visualization of hydrogen bonds, working with alternate sidechain locations, preparation of simple animations, etc.
Detailed description of TRITON features can be found in online manual: http://ncbr.chemi.muni.cz/triton/manual.pdf.
3.4 System requirements, availability Program TRITON is written in C++ language and it uses an OpenGL interface for 3D rendering.
It is ported to a Linux operating system and a version for MS Windows is under development.
Executable files are available free of charge for academic users at http://ncbr.chemi.muni.cz/triton/ 4 CONCLUSION The new version of TRITON allows for protein mutant modeling and the docking ligand molecules into them.
Implemented methodology can be used to design protein receptors with desired ligand binding affinity or specificity.
It can also be used to investigate mechanisms of proteinligand interactions and to assess the role of individual residues in the binding.
Program TRITON substantially simplifies the work related to input data preparation and consequently decreases the probability of human errors.
Thanks to its user-friendly interface, TRITON is helpful not only for computational chemists but also for experimentalists providing assistance and direction in preparation of laboratory work.
ACKNOWLEDGEMENTS Funding: This work was funded by the Ministry of Education of the Czech Republic (MSM0021622413 to M.P.
and J.K., LC06030 to Z.K.
); Grant Agency of the Czech Republic (GA303/06/570 to M.W.
and J.A.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Exact sequence search allows a user to search for a specific DNA subsequence in a larger DNA sequence or database.
It serves as a vital block in many areas such as Pharmacogenetics, Phylogenetics and Personal Genomics.
As sequencing of genomic data becomes increasingly affordable, the amount of sequence data that must be processed will also increase exponentially.
In this context, fast sequence search algorithms will play an important role in exploiting the information contained in the newly sequenced data.
Many existing algorithms do not scale up well for large sequences or databases because of their high-computational costs.
This article describes an efficient algorithm for performing fast searches on large DNA sequences.
It makes use of hash tables of Q-grams that are constructed after downsampling the database, to enable efficient search and memory use.
Time complexity for pattern search is reduced using beam pruning techniques.
Theoretical complexity calculations and performance figures are presented to indicate the potential of the proposed algorithm.
Contact: s.abhilash@samsung.com; ajit.b@samsung.com 1 INTRODUCTION Decreasing costs of sequencing personal genome have opened up many avenues of research.
Several efforts related to Personal Healthcare and Pharmacogenetics are attempting to use the information in an individuals genomic data towards personalization of healthcare.
An important component of these solutions is the search for specific subsequences in a given genome.
For example, Cetuximab (Eric et al., 2009)an Epidermal Growth Factor Receptor (EFGR) inhibitor used to treat various types of cancer is ineffective if certain mutations in the KRAS gene (which lies in Exon 2 of Chromosome 12) exist.
Thus, a search for appropriate mutations conducted on genetic data would be vital in prescribing the most effective treatment.
Exact sequence search also finds application in fields such as Evolutionary Biology and Phylogenetics where certain subsequences of DNA are mined from genomic data of various species of organisms to understand their origin, relatedness and descent.
In the above context, the challenge is to be able to perform fast pattern searches in whole genomes (a complete human genome contains 3 billion base pairs) and databases spanning Giga to Tera Bytes or more.
Prevalent search methods (Altschul et al., 1990; Charras and Lecroq, 2004; Gusfield 1997; Kurtz et al., 2004; Li and Durbin, 2010; Lipman and Pearson, 1985; Ma et al., 2002; Ning et al., 2001) use techniques that have proven to be efficient for To whom correspondence should be addressed.
existing genomic sequences and databases, but do not scale up well for large datasets such as those of whole human genomes.
Fast and efficient search methods that scale up well for large databases are therefore of great value.
In this article, we address the problem of searching for all occurrences of pattern P in a text T, where T is the reference sequence whose length can vary from several million (in case of individual chromosomes) to several billion (in case of complete genomes) bases.
P is the pattern which is a few tens to a few hundred bases in length.
2 RELATED WORK Many methods have been developed for the task of pattern search.
These include FASTA (Lipman and Pearson, 1985; Pearson and Lipman, 1988), BLAST (Altschul et al., 1990), PatternHunter (Ma et al., 2002), MUMMER (Kurtz et al., 2004), SSAHA (Sequence Search and Alignment using Hashing Algorithm) (Ning et al., 2001), Fast String Matching Algorithms (Lecroq, 2007) and BWA-SW (Burrows Wheeler AlignmentSmith Waterman) (Li and Durbin, 2010).
Though Smith Waterman-based methods such as FASTA mines approximate matches by employing dynamic programming techniques, they are computationally very intensive.
BLAST and its variants are an improvement over FASTA in that they use certain seeds for basic anchoring, which are then extended to exact or approximate matches.
However, apart from being probabilistic in nature, BLAST type algorithms require large amounts of memory and computing time for pattern search in large sequences such as whole genomes.
PatternHunter (Ma et al., 2002) is a similar seed-based technique but is still inefficient for applications that involve whole genomes or large databases.
Recent suffix tree-based methods (Gusfield, 1997) such as Mummer (Kurtz et al., 2004) that yield exact matches have a very low-search time complexity.
They represent all suffixes of the text as a plurality of inter-mingled linked lists.At times when the knowledge about genomes gets updated frequently, updating the suffix tree in place becomes tedious as the inter-mingling of linked lists is very sensitive to changes in the text data.
Also, as every node in the tree is required to hold tree-related information such as pointers to their parents and children apart from text-based information, even the best implementation of suffix trees require 15.4 bytes per base (Kurtz et al., 2004), which scales up to 46 GB of memory to store the preprocessed Human Genome.
Deterministic Finite Automaton (DFA)-based methods (Charras and Lecroq, 2004; Gusfield 1997) such as BWA-SW (Li and Durbin, 2010) combine DFA and dynamic-programming-based The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:28 28/8/2010 Bioinformatics-btq364.tex] Page: i415 i414i419 A fast algorithm for exact sequence search in biological sequences using polyphase decomposition alignment methods.
As this method involves Finite Automaton, the method does not scale well for large sequences, even for the best case of exact matches.
Also, because they use dynamic programming, the memory requirements of the method are huge.
Hashing-based methods such as SSAHA (Ning et al., 2001) and those proposed by Lecroq (2007) propose substring matching and Q-gram hashing method to greatly improve the time complexity.
To summarize, efficient biological pattern-search algorithms must mitigate two problems.
First, that of random access into text, without which the time complexity of the algorithm shoots up to an unacceptable O(LT ) (Charras and Lecroq, 2004; Knuth et al., 1977; Melichar, 1995), where T is of the order of several billion bases.
This can be mitigated by employing mechanisms such as suffix trees and hash tables.
Hashing methods are considered because changing data locally is an easy task when information in the corresponding sequence gets updated.
The second problem is that of memory constraints.
Random access algorithms (Kurtz et al., 2004; Lecroq, 2007; Ning et al., 2001) come with an undesirable space complexity of O(LT ), where LT is the length of the text T is of the order of several billion bases.
In our work, we propose an efficient methodology that employs down-sampled version of T and polyphase decomposition of P to determine potential areas of exact match.
These, in conjunction with hash tables and the use of Q grams to successively refine potential-search regions results in superior space and time complexity.
Note that the present method differs from the existing methods (Lecroq, 2007; Ning et al., 2001) in that, firstly, we consider down-sampled substrings of both text and the pattern that result in large memory savings.
Secondly, the Q-grams have traditionally been used only for the purposes of string matching, but in the proposed method, their locations have been used to progressively localize the potential locations of exact match.
We now describe the proposed algorithm.
3 PROPOSED METHOD We first explain various terminologies we use in the exposition that follows.
For this purpose, we use an example sequence, given by: S = actgcttctact.
(1) Let the length of the sequence S be denoted by LS .
Also, S[n] is used to represent the n-th base of the sequence S. For example, S[0] = a and S[1] = c. Downsampling (Vaidyanathan, 1993) a sequence by a factor of M means that we pick every M-th base from sequence S to form a new sequence SM given by: SM[n]=S[Mn],0n LS M (2) where,  indicates the largest integer less than the argument.
For example, for the sequence S and M =3, S3 = agta M-channel polyphase decomposition (Vaidyanathan, 1993) gives M possible down-sampled sequences for different integer-phase shifts.
The generalized form of polyphase decomposition is given by: SMi[n]=S[Mn+i],0n LS M ,0 iM 1.
(3) Note that SM = SM0.
The polyphase decomposition of the sequence S in Equation (1) for M =3 yields: S30 = agta, S31 = cccc and S32 = tttt A Q-gram of a sequence S is denoted by QS(n) and is made up of Q consecutive bases starting from position n. Thus for sequence S as in Equation (1) example Q-grams are: QS(0) = actg QS(1) = ctgc QS(2) = tgct QS(8) = tact Contiguous Q-grams of a sequence S is the set: CQS ={QS(0)QS(1)QS(LS Q)} (4) where the cardinality of the set CQS is |CQS|=LS Q+1.
Note that there are Q 1 common bases between any two consecutive Q-grams in CQS.
For example, given S as in Equation (1) and Q=4, CQS = {actg, ctgc, tgct, gctt, cttc, ttct, tcta, ctac, tact}.
NQS is the set of non-overlapping Q-grams of sequence S given by: NQS = { QS ( 0 ) QS ( Q ) ...QS ( LS Q )} .
(5) Note that: QS(i)and QS(j)NQS, QS ( i )QS(j)= if i 	= j where denotes the null set.
That is the Q-grams in NQS are pair-wise and non-overlapping.
For S as in Equation (1) and Q=4, NQS = {actg, cttc, tact}.
We now describe the proposed method which is composed of two stages, namely: (1) Preprocessing stage.
(2) Pattern-search stage.
The detailed description of each stage is presented in the following subsections.
3.1 Preprocessing stage A block diagram for the preprocessing stage is shown in Figure 1.
In this step, text T is downsampled and processed into a hash table to support random access into the text.
A hash table is a data structure that efficiently links keys to corresponding values called buckets (Cormen et al., 1990).
In our case, the key refers to each distinct Q-gram while bucket refers to the list of locations of that Q-gram in the text.
Here, T is first downsampled by a factor of M to give TM and a hash table HTMQ is then constructed using contiguous Q-grams of TM.
The bucket of a given Q-gram q is denoted by HTMQ[q].
The details of the preprocessing algorithm are given below.
This is followed by an example on sample text.
AlgorithmA (1) Downsampled the text T by a factor of M to yield TM.
(2) Generate the set of contiguous Q-grams for TM, namely, CQTM.
(3) For each Q-Gram in CQTM, consult the key field of the hash table HTMQ.
i415 [10:28 28/8/2010 Bioinformatics-btq364.tex] Page: i416 i414i419 A.Srikantha et al.
Fig.1.
Block diagram of the preprocessing stage.
Table 1.
A sample Q-gram hash table Key: Q(=3) gram Bucket: Locations list agt 0, 3, 6 gta 1, 4, 7 tag 2, 5 taa 8 aac 9 aca 10 (4) If the Q-Gram exists in the key field, append the new position in the bucket HTMQ[Q-gram].
(5) Else, add a new Q-gram key and its position in the corresponding bucket in HTMQ.
Example: Consider the sequence T = acc gat tag aag ggt tta aga gtc tca acc aga cta agc.
For M =3 and Q=3, the result of the algorithm is given below.
(1) T3 = agtagtagtaaca.
(2) CQT3:{agt(0), gta(1), tag(2), agt(3), gta(4), tag(5), agt(6), gta(7), taa(8), aac(9), aca(10)}.
Note that we have also mentioned the indices for each Q-gram in CQT3 in parentheses.
(3) Finally the HTMQ is as given in Table 1.
3.2 Pattern-search stage The idea behind the pattern-search algorithm is that given T and P, if P occurs in T at unknown locations, it is necessary that at least one downsampled polyphase PMi of P occurs in TM.
Note that the reverse need not be true, that is, a certain PMi occurring in TM does not guarantee that P occurs in T. Therefore, we first mine for all occurrences of PMi in TM and search around the resulting indices for an exact match in T. Figure 2 presents the block diagram of the procedure.
The definitions of crucial variables are given below: (1) PPAll: this is the array that holds the locations of exact matches for all polyphases of P. The elements of PPAll denoted by PPAll(n).
(2) PP: this is the array that holds the locations of exact matches of a particular polyphase of P. The elements of PP are denoted by PP(n).
The algorithm breaks each polyphase into non-overlapping Q-grams and bases its search on a successive refinement principle by employing beam pruning technique.
That is, matches to the first non-overlapping Q-gram are first found.
If these matches extend to the next Q-gram, then these locations are retained.
This process is carried out for all the Q-grams in the given polyphase.
At the end of this process, those locations where all the Q-grams match represent the locations where the polyphase PMi matches TM.
These locations are then mapped to the original text T where the final search takes place.
In this manner, the algorithm successively refines the search regions and thus speeds the search process.
The algorithm is presented below followed by an example on a sample pattern.
AlgorithmB (1) Generate PMi 0 iM 1 from P using Polyphase Decomposition.
(2) Initialize PPAll = {} (an empty array).
(3) For each polyphase PMi, do (4) and (8).
(4) Generate NQPMi (set of non-overlapping Q-grams of polyphase PMi).
(5) PP = HTMQ[NQPMi(0)] (this represents all positions in TM where an exact match is found for the first non overlapping Q-gram of PMi).
(6) For all NQPMi(j)NQPMi, j>0 (the rest of the entries in NQPMi) do (7).
(7) Delete from PP, PP(k) such that PP(k) + jQ/HTMQ[NQPMi(j)] NQPMi(j) should have occurred at location PP(k) + jQ if there was an exact match.
But, if that is not the case, there is no chance of an exact match of polyphase PMi in TM at location PP(k).
Hence, it must be pruned from the array PP.
(8) Append all PP (k) PP into PPAll.
That is, make note of all locations of exact match of PMi in TM in the array PPAll that holds the locations of exact match for all PMis in TM.
(9) Translate PPAll(n) to corresponding location in original text T and verify exact match of P in T in that location.
Note that Steps (4) and (8) deal with extracting exact locations of PMi in TM.
Step (7) is a beam-pruning method to prune out non-exact-match locations of PMi from the PP.
Example: Consider P = aag ggt tta aga gtc tca.
Also M, Q and T as are the same as those used in the previous illustration.
We show the steps of the algorithm for one of the polyphases: PM0.
The results of the other two polyphases are presented in Step (8).
i416 [10:28 28/8/2010 Bioinformatics-btq364.tex] Page: i417 i414i419 A fast algorithm for exact sequence search in biological sequences using polyphase decomposition Fig.2.
Block diagram of the pattern-search stage.
(1) PM0 = agtagt PM1 = agtgtc PM2 = gtaaca.
(2) PPAll = {} (an empty array) (3) Iterating for PM0 for Steps (4) and (8).
(4) NQPM0 = {agt(0), agt(1)} (note that we have mentioned the indices of the Q-gram in NQPM0 in parenthesis).
(5) PP = HTNQ[NQPM0(0)] = [0,3,6].
(6) Iterating over {agt(1)} for Step (7).
(7) Prune only PP(2).
Because PP(2)+1Q=6+3=9/ HTNQ[NQPM0(1)].
(8) PPAll = {0,3}, (a) result of PM1: PP = {}; (b) result of PM2: PP = {7}.
Therefore, PPAll = {0,3,7}.
(9) Translating the locations of PPAll to those in T gives {0,9,19}.
Note that we have considered the actual polyphase the exact match has come from.
For example, as PPAll[2] = 7 come from the second polyphase, the mapping of this location to a location in T would be PPAll[2] M 2=19.
Exact matches are now verified starting from these locations in T. In this example, the exact match occurs at index 9 in T. Note that if the length of PMi is not an integral multiple of Q, then certain trailing nucleotides will be missed by the NQPMi.
One of the solutions in order to avoid this loss is to verify exact match of PMi in TM for all PP(n) just before Step (8).
4 ALGORITHM ANALYSIS We now present the space and time complexity analysis of the proposed method.
4.1 Space complexity of hash table From Algorithm A, because every overlapping Q-gram in TM contributes to an entry in the hash table, the algorithms space complexity is O(LTM ) where the length of TM, LTM =LT/M due to downsampling.
Also, because constructing the hash table is a sequential process, its time complexity is O(LTM ).
Note that there are two types of memory in a computermain memory, which is fast, costly and scarce and the secondary memory, which is slow, cheap and abundant.
An efficient design of an algorithm is an optimal balance between its speed and its main memory requirements.
In our design, we have retained a smaller, down-sampled version of the text T in the main memory, which we consider for space complexity calculations.
For the purposes of exact match verification in Step (9) of Algorithm B, the original text T can reside in the cheaper secondary memory, and relevant sections (with length <<LT ) can be paged into the main memory when required.
Also note that if we set M =1, the HTMQ reduces to its nave version and its main memory requirements are then O(LT ).
4.2 Time complexity of pattern search We will now discuss the time complexity for pattern search (Algorithm B).
In our analysis, we model the base distribution as an iid process and following uniform distribution.
This is reasonable over large databases because it has been shown that DNA sequences at best have weak long-range correlations (Bernaola et al., 2002).
The complexity of processing a particular polyphase as described in Steps (4) and (8) is as given below.
Step 4 is the generation of the NQPMi list.
This step which generates non overlapping Q-grams is computationally simple and its effect on the search complexity can be neglected.
Step 5 is a look up from the hash table.
Because this step is only a main memory lookup (Cormen et al., 1990), it contributes O(1) to the search complexity.
Also, It follows from our assumption that the distribution of bases is uniform and iid, the probability of the existence of any given Q-gram = 1/4Q.
Thus, the expected number of any Q-grams in TM = LTM (1/4Q).
That is, the expected number of matches for the first Q-gram in any given polyphase is LTM/4Q.
Step 7 is the beam pruning procedure, where entries from the array PP are removed based on the entries in the bucket HTMQ[NQPMi(j)].
This translates to traversal of both arrays, namely, PP and HTMQ[NQPMi(j)].
The complexity of this step then depends on the lengths of each array.
The bucket: following the above explanation, the bucket has an average of LTM/4Q entries.
The PP Array: note that for any j, the PP Array holds the locations of a string of length jQ.
Therefore, the expected length of the PP Array for any j is LTM/4jQ.
As a result, employing binary search, the complexity of the beam pruning procedure for a single polyphase is O((LTM/4Q)log(LTM/4Q)).
Also, note that Steps (1), (2) and (8) are computationally simple and their contribution to the search complexity can be neglected.
Now, the complexity of the algorithm until Step (9) is the complexity of beam pruning procedure for all M polyp-hases = O(M(LTM/4Q)log(LTM/4Q))) = O((LT/4Q)log(LTM/4Q)).
The complexity of Step (9) is negligible as its of order O(LP), LP <<LT .
Thus the overall time complexity for the search procedure: O((LT/4 Q)log(LT/M4 Q)).
A comparison of complexities of various search algorithms are given in Table 2.
As can be seen from the table, the proposed method has superior complexity as compared to most existing methods.
Also, while i417 [10:28 28/8/2010 Bioinformatics-btq364.tex] Page: i418 i414i419 A.Srikantha et al.
Table 2.
Comparison of theoretical complexities of various pattern/homology search algorithms Search algorithm Space complexity Time complexity BLASTa O(LPLT )b O(LPLT ) FASTAa O(LPLT ) O(LPLT ) Finite automaton O(LP) O(LT ) Knuth Morris Pratt O(LP) O(LT ) Suffix tree based O(LT ) O(LP) BWA-SW O(LP) O(L0.628P LT ) SSAHA O(LT ) O((LT/4Q)log(LT/4Q)) Proposed O(LT/M) O((LT/4Q)log(LT/M4Q)) aMethods also yield approximate matches.
bLP <<LT <<LT .
The bold line corresponds to the complexity of the proposed algorithm.
Table 3.
Hash table size for M Q combinations (MB) Q=8 Q=9 Q=10 Q=11 M =7 117.0 118.3 120.3 129.1 M =15 55.0 55.3 57.6 65.0 M =23 35.9 36.4 38.4 45.3 M =31 26.7 27.2 29.2 35.2 M =39 21.3 21.7 23.7 28.9 the suffix-tree approach would possibly have the advantage of a slightly better time complexity when compared to the proposed method, this advantage could be offset by its huge memory requirements.
The space savings would be particularly useful in processing huge sequences such as whole genomes.
Also, note that because the algorithm works on polyphase decomposition with little inter-dependency between each polyphase, it lends itself to easy parallelization thereby speeding up its operation.
4.3 Experimental analysis The algorithm was implemented using Python.
The text T considered is the Human Chromosome 1 (LT = 250 M bases) (Genome Reference Consortium, UCSC) the pattern P is randomly chosen segment of 300 bases (LP = 300) from T .
All experiments were conducted on a PC with a 2 GB RAM and Intel 2.4 GHz Quad core processor.
The variation of the size of the hash table with varying M and Q is presented in Table 3.
It can be seen that the size of the hash table decreases with increasing M as expected.
Also, it can be seen that the size of the hash table increases slightly with increasing Q.
The variations in time taken to process the pattern and generate a list of locations for post processing are presented in Table 4.
Note that the combination of parameters M and Q must satisfy the condition MQ < LP .
Otherwise, at least one of the polyphases will contain <Q bases, and thus cannot be looked up through the hash table.
The blocks whose M Q combination is infeasible for LP =300 are marked with .
It can be seen that as M and Q increases, the search time decreases as expected.
Also, as M increases, the number of potential exact matches that must be post-processed also increases.
Table 5 provides this data.
Table 4.
Search times for various M Q combinations (in micro seconds) Q=8 Q=9 Q=10 Q=11 M =7 1830 277 27 30 M =15 288 33 7 3 M =23 105 9 1.4 0.5 M =31 108 8  M =39   Table 5.
Number of matches to be post processed versus M M =7 M =15 M =23 M =31 Number of matches 1 1 2 330 Thus, it can be seen from the data presented that a large M results in a smaller hash table, but also generates larger number of potential matches that must be post processed.
Also, a larger Q speeds up the polyphase search, but demands larger hash table size.
Therefore, values M and Q must be carefully chosen.
For example, for setting parameters M and Q for a pattern of length 300, Table 3 inspires us to use a highest value of M =39 (because this gives the smallest hash table).
However, the numbers in table Table 4 suggest that it would be reasonable to select the higher values of Q and set M 23 (because higher M Q combinations are either slow or infeasible) and further consultation with Table 5 indicates that a combination of M =23 and Q=11 is practical (because the number of matches that must be post processed are near minimal).
Thus an optimal choice of parameters would be M =23 and Q=11, which requires 45.3 MB for the hash table (Text size = 250 M bases).
Also, with respect to the pattern (Pattern size = 300 bases) related searching time, 0.5 ms are required to generate the list of exact matches of polyphases in the downsampled text.
Another 1 ms is required to verify if the exact polyphase match translates to exact match in text.
Thus a total of 1.5 ms were required for mining exact matches.
5 CONCLUSION In this article, we presented a method for fast exact sequence search that relies on downsampling and polyphase representations to expedite the search process.
We also computed the complexity of the algorithm and showed it to be better than existing methods.
Because the proposed method uses polyphase representations, and because searching for exact matches in multiple polyphases does not have any data or functional inter-dependency, the algorithm can be parallelized.
This would further reduce time complexity.
Implementation of a parallel version of the algorithm will be a topic of further work.
The proposed algorithm addresses the problem of finding exact matches to a substring.
Our future research will extend the utility of this algorithm to finding approximate matches.
Conflict of Interest: none declared.
i418 [10:28 28/8/2010 Bioinformatics-btq364.tex] Page: i419 i414i419 A fast algorithm for exact sequence search in biological sequences using polyphase decomposition
ABSTRACT Motivation: Pre-mRNA cleavage and polyadenylation are essential steps for 30-end maturation and subsequent stability and degradation of mRNAs.
This process is highly controlled by cis-regulatory elements surrounding the cleavage/polyadenylation sites (polyA sites), which are frequently constrained by sequence content and position.
More than 50% of human transcripts have multiple functional polyA sites, and the specific use of alternative polyA sites (APA) results in isoforms with variable 30-untranslated regions, thus potentially affecting gene regulation.
Elucidating the regulatory mechanisms underlying differen-tial polyA preferences in multiple cell types has been hindered both by the lack of suitable data on the precise location of cleavage sites, as well as of appropriate tests for determining APAs with significant dif-ferences across multiple libraries.
Results: We applied a tailored paired-end RNA-seq protocol to spe-cifically probe the position of polyA sites in three human adult tissue types.
We specified a linear-effects regression model to identify tissue-specific biases indicating regulated APA; the significance of differences between tissue types was assessed by an appropriately designed permutation test.
This combination allowed to identify highly specific subsets of APA events in the individual tissue types.
Predictive models successfully classified constitutive polyA sites from a biologic-ally relevant background (auROC99.6%), as well as tissue-specific regulated sets from each other.
We found that the main cis-regulatory elements described for polyadenylation are a strong, and highly in-formative, hallmark for constitutive sites only.
Tissue-specific regu-lated sites were found to contain other regulatory motifs, with the canonical polyadenylation signal being nearly absent at brain-specific polyA sites.
Together, our results contribute to the understanding of the diversity of post-transcriptional gene regulation.
Availability: Raw data are deposited on SRA, accession numbers: brain SRX208132, kidney SRX208087 and liver SRX208134.
Pro-cessed datasets as well as model code are published on our website: Contact: uwe.ohler@duke.edu 1 INTRODUCTION Almost all eukaryotic mRNAs undergo a post-transcriptional processing step called polyadenylation, in which they acquire a polyA tail at their 30-end.
After transcription, the 30-most seg-ment of the newly made RNA is cleaved off at specific sites (polyA sites) by a set of RNA regulatory proteins, which is fol-lowed by the synthesis of the polyA tail by the addition of ad-enine (A) residues in a non-templated fashion (Andreassi and Riccio, 2009).
Around 90 protein factors regulate this process, with CPSF (cleavage and polyadenylation specificity factor), CstF (cleavage simulator factor), CFI (cleavage factor I), CFII (cleavage factor II), PAP (polyA polymerase) and PABII (polyA binding protein) playing a crucial role (Beaudoing et al., 2000; Ji and Tian, 2009; Shi et al., 2009; Tian et al., 2005).
PolyA sites are essential for 30-end maturation, stability and degradation of mRNAs.
Furthermore, polyadenylation defines the extent of the 30-untranslated region (30-UTR) of mRNAs, which spans from the stop codon up to the polyA tail and con-tains many post-transcriptional regulatory sequence elements such as microRNA (miRNA) target sites.
In addition, alternative polyadenylation (APA) events arise from the presence of more than one particular functional cleavage/polyadenylation (polyA) site.
The specific use of different polyadenylation sites can play a direct role in gene regulation.
For instance, eliminating large parts of a 30-UTR by using the more proximal polyA site enables a transcript to escape from miRNA regulation of its longer iso-form.
In proliferating cells, proximal polyA sites are therefore favored over distal ones, resulting in the production of mRNAs with shorter 30-UTR and fewer miRNA-binding motifs (Ji and Tian, 2009; Sandberg et al., 2008).
APA can influence mRNA nuclear export, cytoplasmic localization and non-miRNA mediated changes in mRNA stability and translational efficiency (Majoros and Ohler, 2007; Mayr and Bartel, 2009; Moore, 2005).
As such, it is important to identify not just alternative but specifically regulated alternative events, such as tissue-specific APA.
Based on earlier analysis of expressed sequence tags (ESTs), over 50% of the human and more than 30% of mouse genes were observed to have multiple polyadenylation sites, which results in mRNA isoforms different in their 30-UTR and/or coding sequences (Tian et al., 2005).
Initial studies on ESTs and tiling microarrays also indicated a bias in the regula-tion of polyA sites in certain human tissues (David et al., 2006; Tian et al., 2005; Zhang et al., 2005a).
The introduction of high-throughput sequencing technology has vastly expanded the opportunities to explore APA.
Recent deep sequencing of mRNA populations from multiple tissue types has shown that 86% of human genes exhibit variants due to APA sites (Wang et al., 2008).
In addition, several protocols relevant for studying polyadenylation have been developed.
*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com These protocols are designed to capture the 30-end of mRNAs using specific primers, then sequence these fragments using second-and third-generation sequencing technologies (Jan et al., 2010; Mangone et al., 2010; Ozsolak et al., 2010; Shepard et al., 2011).
However, with one exception (Derti et al., 2012), these approaches have been applied on small samples or non-mammalian genomes, leaving human normal tissues unexplored.
A thorough analysis of the polyadenylation process in adult tissue types, showing differential gene expression, would help us understand tissue-specific APA regulation.
Although genome-wide APA profiling enables us to discover genes with multiple polyA isoforms at a genome-wide scale, it introduces major chal-lenges.
Without adequate methodology to specify the significance of APA biases in different tissues, we may confuse the mere presence of multiple APA with their specific up-or downregula-tion across conditions.
A clean definition of truly specific sets is necessary to investigate which features allow for successful dis-crimination via computational models, and to suggest candidate regulatory features for future studies.
In this article, we address several of these challenges by using data from a new RNA-seq protocol applied to sequence the 30-UTR end of mRNAs from different adult normal tissue types.
Using a linear model, we distinguish between constitutive, alternative and alternatively regulated polyadenylation sites.
Our linear regression model takes into account different library depth, expression of each gene in each tissue, as well as inter-actions between tissues and genes.
As is still the case with many deep sequencing datasets, we do not have multiple replicates at our disposition that can be used to identify significantly differing APAs across tissues.
Instead, significance of differences between samples from different tissue types is assessed by an appropri-ately designed permutation test.
We then use the flanking se-quence region around polyA sites to build predictive models both for the discrimination of constitutive polyA sites from gen-omic background, as well as to distinguish between regulated APA sets from different tissues.
2 RESULTS 2.1 A paired-end sequencing strategy for identifying polyadenylation sites To precisely map polyA sites at genome-wide scale, we made use of several new libraries generated by a tailored sequencing ap-proach, PA-seq.
This protocol yields paired-end tags, with one tag located directly at the cleavage site, and its pair mapping to a more upstream location, typically in the 30-UTR of the same transcript, Figure 1 (see Section 5).
We used PA-seq to monitor the differential usage of polyade-nylation sites in three different human adult tissue types: brain, liver and kidney.
Each tissue was sequenced at varying depth.
We obtained 2.8 million raw paired reads from liver, 8 million from kidney and 3.5 million from brain.
Of those paired reads, 85% mapped to the human genome (hg19).
Non-redundant read pairs, i.e.
those that showed differences in at least one of the paired end tags, were grouped for each unique 30 position, denot-ing a polyA site.
These sites were filtered to exclude 30 locations that mapped to genomic regions with high A content, to exclude possible contaminations by internal priming.
PA-seq reads were then clustered into clusters (PAS clusters) analogous to an algo-rithm previously developed for the analysis of capped 50 mRNA tags (Ni et al., 2010).
We used the total sum of the non-redun-dant read pairs of all of the 30 tags in each PAS cluster as a measure of the PAS usage, and considered PAS clusters covering narrow genomic regions and with five or more reads for all fur-ther analyses (see Section 5).
Table 1 summarizes the data for all libraries.
To differentiate between PAS clusters that are constitutively used versus those with more than one polyA site, we grouped all overlapping PASs of the same transcript from the three tissue types together.
Each PAS cluster was referred to by the mode of its median (see Section 5).
If the gene has one PAS cluster, we refer to it as a constitutive gene; if it has more than one PAS cluster, we refer to it as an alternatively polyadenylated gene.
Overall, we identified PAS clusters for 11 454 genes: around 7278 are constitutive and 4176 are alternative polyadenylated.
From genes that are expressed in the three tissue types, 2171 are constitutive genes and 1965 are alternative polyadenylated genes.
Alternative-polyadenylated genes had 5357 different PAS clusters; this is the set included in our analysis.
2.2 Characterization of tissue-specific regulated polyadenylation sites Previous research on APA has shown that most of human genes have multiple polyadenylation sites, with many of them being tissue-specific.
Testing the statistical significance of differential preferences for APA usage for a gene between tissues has been previously investigated by applying Fishers exact tests, chi-square tests or linear trend test (Beaudoing and Gautheret, 2001; Fu et al., 2011; Zhang et al., 2005a).
Applied on a gene with multiple PAS, measured across multiple conditions, Fishers test will detect a significant difference of the pattern from the null assumption, but further tests are needed to pinpoint exactly which PAS, in which tissue, deviates from constitutive expres-sion.
A popular approach for identifying specific events across multiple tissues/sites has therefore been introduced based on Fig.1.
Summary of PA-Seq Protocol: Total mRNA is randomly frag-mented and reversed transcribed with a modified oligo(dT) primer, which synthesizes with the polyA tail.
cDNA fragments are then captured and sequenced using multiplexed paired-end sequencing on Illumina i109 Modelling of tissue-specific alternative polyadenylation Shannon entropy (Schug et al., 2005).
Entropy values close to zero represent events specific to a single tissue; values increase as the relative usage spreads more across tissue types, or when the relative contribution of the tissue to the overall usage decreases.
However, entropy does not directly reflect significance, as sam-ples with vastly different levels of evidence (e.g.
read coverage) may lead to similar entropy values.
To avoid these shortcomings, we specified a linear effects re-gression model for the read counts of each PAS cluster in each tissue type, motivated by previous applications to detect signifi-cant changes in gene expression (Marioni et al., 2008) and alter-native splicing patterns (Blekhman et al., 2010).
We controlled for fixed effects including different tissue depth, expression of each gene in each tissue, as well as any interaction between tis-sues and genes.
The resulting residual for a given PAS cluster in a given tissue reflects evidence that this PAS cluster is specific, and highly used, in the tissue.
We then needed to quantify whether for a given PAS cluster, an observed difference in read counts in a specific tissue is sig-nificant, i.e.
more pronounced than what would be expected owing to random variation.
Given that the libraries were sequenced without experimental replicates, we applied permuta-tion tests on the read counts of PASs for each gene in our libraries, to determine a tissue-specificity threshold (see Section 5).
With three libraries at our disposal, we separated tissue-specific PAS clusters into two groups: clusters that are highly used in one individual tissue (individual), and clusters that are highly used in two tissue types simultaneously (overlap-ping).
Figure 2a shows the test statistics for assessing the over-lapping tissue-specificity applied to both original and permuted data.
By applying our linear model on alternative-polyadenylated genes, our strict selection led to 234 tissue-specific individual PAS clusters, and 214 tissue-specific clusters overlapping in two tissue types [at P50.01; false-discovery rate (FDR)50.25] (Fig.2b).
To study the biased usage of APA in different tissues, we calculated a variability index (VI) between each pair of tis-sues.
A low VI between two tissues indicates strong concordance in their usage of PAS clusters (see Section 5).
Confirming expect-ations, liver and kidney showed the highest correlation, while brain and kidney were the lowest.
To illustrate the difference of our model compared with pre-vious approaches, we calculated the Shannon entropy for the subset of PAS clusters that showed significant tissue specificity for both the individual and overlap PAS, Figure 3a.
While the Shannon entropy is less than one for about 480 PAS clusters, our model identified only half of these as significantly tissue specific, with few additional PASs that had higher entropy.
This is mainly because Shannon entropy does not take the abundance of evi-dence into account.
For example, in Figure 3b, while the residual for the most proximal PAS site of the gene HDLBP (on negative strand) in brain indicates its outlier character, it is based on 17 tags (510% of the total) and thus not large enough to be sig-nificantly brain specific.
Additional data would be needed to confirm the specific trend.
In turn, our model characterized spe-cific PAS clusters that would have been characterized as non-specific due to higher entropy values.
As an example, Figure 3c shows two PAS clusters for the gene BDH1 (on negative strand).
The distal cluster is used in the three tissue types, while the prox-imal is used in kidney and brain only.
Using the linear model, the distal cluster was detected as significant in liver, given that the other cluster, the proximal one, shows higher usage in the other two tissues (more than 2-folds).
2.3 Modeling constitutive polyadenylation sites Because the PAPs responsible for synthesizing the polyA tail lack substrate specificity, it necessitates the presence of specific signals in the sequences around polyA sites that control mRNA poly-adenylation (reviewed by Tian and Graber, 2012).
One of the known main cis-regulatory elements is a conserved hexamer with consensus AWUAAA, located 1035nt upstream of the polyA Table 1.
Summary of PA-seq generated data, filtering steps and clustering in each tissue library PA-seq reads and clustering Liver Kidney Brain Raw read pairs with identifiable linker sequences 2 851 978 8044 879 3 533 285 Read pairs mapped 2 449 567 7198 135 2 711 473 Non-redundant read pairs no priming 649 410 1353 072 1 320 265 Non-redundant read pairs 2 distinct 50 tags 545 708 1190 344 1 001 479 Different polyA sites 57 396 99 482 132 616 PAS clusters 8537 12 477 15 727 PAS clusters with NPa 7439 10 291 13 205 aNPNarrow Peak.
(a) (b) Fig.2.
(a) Test statistic for the residual of the original (red) and permuted data (blue) for calculating overlap-significant PAS sites.
(b) Number of tissue-specific PAS clusters found in each tissue: total individual sites: 234 (90 68 76), total overlap sites: 214 (31 100 83) i110 D.Hafez et al.
site, referred to as polyadenylation signal (Beaudoing et al., 2000).
The sequence composition at the cleavage site itself is not well characterized, but a dinucleotide preference CA was found in vitro (Chen et al., 1995).
The sequence around polyA sites are usually G/U-rich with a remarkable downstream elem-ent (DSE), located within 30 nt downstream of the cleavage site.
Upstream of polyA sites are upstream elements (USE) that are usually also U-rich, while some G-rich sequences have been re-ported as well.
These elements are largely located in the region (100,100) nt around polyA sites (Tian and Graber, 2012).
Most early attempts for the computational prediction of polyA sites considered only samples containing the canonical PAS signal.
Position weight matrices (PWM) for DSE and USE along with the PAS signal were used as input features for hidden Markov model (HMM) or support vector machines (SVM) (Hajarnavis et al., 2004; Legendre and Gautheret, 2003; Liu et al., 2003; Salamov and Solovyev, 1997; Tabaska and Zhang, 1999).
After the characterization of 15 putative regula-tory elements surrounding PAS signals (Hu et al., 2005), pos-ition-specific scoring matrices for the identified motifs and structural patterns of mRNA, have been later used as input fea-tures (Ahmed et al., 2009; Akhtar et al., 2010; Chang et al., 2011; Cheng et al., 2006; Shao et al., 2009).
Most recently, the appli-cation of artificial neural network and random forests techniques have been proposed (Kalkatawi et al., 2012).
These models were largely trained on low-abundance pooled EST data from varying human tissues; none of them examined tissues independently.
While the use of curated quality controlled data from collections such as PolyA_DB (Zhang et al., 2005b) made it possible to design models with high accuracy, studies typically restricted their dataset to only include transcripts with PAS signals and results were sometimes hard to interpret owing to negative data not matched to the problem faced by the RNA processing machinery (such as using random genomic locations).
Modeling constitutive and/or APA sites specifically has so far rarely been investigated.
The exact motifs responsible for APA are frequently still unknown, especially when it comes to tissue-regulated APA.
Calculating PWM scores as features of classi-fiers, as in the case of constitutive sites with known motifs, will likely not reflect all of the regulatory elements.
It is thus more applicable to use a sparse sequence-based classifier that uses a broad definition of the feature space.
String kernels transform the input sequences into a higher-dimensional feature space, ef-fectively looking for similarities among substrings, and have been proven to be successful in the prediction of alternative splicing and transcription start sites (Sonnenburg et al., 2006, 2007).
Here, we build an SVM, using all of the information available in the sequences flanking the polyA sites, by applying two string kernels, the spectrum kernel (Leslie et al., 2002) and the weighted degree kernel with shifts (WD) (shogun toolbox; version 2.0.0) (Ratsch et al., 2005).
While the spectrum kernel highlights the global similarities between sequences as it counts the number of occurrences of similar motifs, the WD kernel counts the number of matching substrings of similar lengths at the same position but allowed to be shifted within a specified window size around that position.
To investigate whether local sequence features around polyA sites are sufficient to explain polyadenylation, we first examined whether PAS clusters for constitutive genes could be classified from non-polyA sites.
We focused on (100,100) nt around polyA sites, given that the known constitutive elements are located in this region, and that it has additionally been shown to exhibit a biased nucleotide composition (Legendre and Gautheret, 2003; Tian et al., 2005).
As the polyadenylation ma-chinery scans transcribed sequences for cleavage locations, it is not appropriate to use random genomic locations as negative set.
Within transcripts, the highly distinct higher order nucleotide composition in coding sequences renders them inappropriate.
Fig.3.
(a) Histogram of Shannon entropy Q-values for all PAS clusters (range from 0 to 9).
Red bars represent entropy values for individual-significant PAS detected by our model, blue bars represent overlap-significant PAS.
Individual sites cluster at 02 entropy, and overlap sites cluster at the upper end of the range.
(b) Example showing that Shannon entropy does not take the abundance of the evidence into account for calling sites significant.
The usage (count) of each PAS cluster is marked in each tissue, followed by the entropy values then the test statistics resulted from our linear model.
Tissue specificity is determined by low entropy values but high test statistics above a certain threshold.
The proximal site in brain (17 tags; 510% of total) is classified as specific (entropy 0.46).
However, this relatively low tag number compared with the overall expression of the gene and the total library depth is not enough to call this site brain-specific.
Entropy values for this proximal site in liver and kidney represent pseudo-counts (not shown in figure).
(c) Example of a specific PAS cluster detected by our linear model and not by Shannon entropy (BDH1 gene on negative strand).
The distal site (65 tags) is the only PAS site for this gene used in liver.
Given the relatively low expression level of the gene and the liver-low library depth compared with brain and kidney, this site is classified as significant (test statistic of our model is marked by red circle).
Shannon entropy values do not reflect this relative usage i111 Modelling of tissue-specific alternative polyadenylation Instead, we built a biologically motivated and challenging nega-tive dataset: for each PAS, we randomly selected 10 positions in the 30-UTR sequence between the transcript stop codon and the PAS, but not including the last 100 nucleotides.
We retrieved the flanking (100,100) regions around these positions to create our negative dataset.
In total, we extracted 2171 positive ex-amples, and 21 710 negative examples.
Because we needed to set multiple hyper-parameters for the SVM and kernels, like order, shift and the classification penalty, we randomly split our dataset into 20% for model selection and 80% for (independent) training and testing (see Section 5).
We applied 5-fold cross validation.
The classifier performance using the two string kernels is shown in Figure 4a and b.
Calculation of the area under the receiver operating characteristic curve (auROC) showed that the WD kernel substantially outper-formed the baseline spectrum kernel (auROC 99.6, 93.5%).
We applied WD on varying window sizes around PAS clusters and found that the high performance largely resulted from fea-tures in the flanking region of (40,40).
Our model parameters indicate that most of these motifs are less than 8-mers long, and shifted within 12nt, which coincides with the findings of (Zhang et al., 2005a).
This suggests that motifs around constitutive polyA sites are highly conserved in both sequence and location, and that the WD kernel is powerful enough to capture this phe-nomenon with near perfect accuracy.
To illustrate the PAS se-quence landscape, we created sequence logos for the flanking regions, which visually showed that the conserved motifs were found in the region (30,30) nt, WebLogo (Crooks et al., 2004), Figure 4c.
The polyadenylation signal and DSE were clearly observed, and the cleavage site itself exhibited a strong BA dinucleotide bias (BC, G or T), in agreement with the previously reported CA dinucleotide.
2.4 Prediction of tissue-specific polyadenylation sites The presence of conserved motifs for constitutive polyA sites suggests the presence of other motifs that instruct the cell to start the polyadenylation process around APA sites in a condition-specific manner.
To investigate this, we first merged all individual-tissueregulated and the two-tissueoverlap PAS clusters and classified them against the positive constitutive data-set (Fig.5a).
The moderate but highly encouraging performance of the classifier on the individual-regulated and the overlap-regu-lated datasets (auROC 74.5 and 66.5%, respectively) support this hypothesis.
We then classified each of the individual tissue-specific PAS clusters against constitutive PASs (Fig.5b).
Brain-individual PAS clusters were highly distinguishable from consti-tutive PASs (auROC 81.5%), while kidney-individual and liver-individual regulated PASs were classified at lower but rea-sonable levels (auROC 72, 63.5%, respectively).
An inspection of the sequence logos of each group explained this performance (Fig.6).
We found an A-rich sequence just downstream of brain-individual regulated PAS clusters that is not present in the con-stitutive subset and other tissue-specific sets.
Moreover, while the canonical PAS signal is still found in liver-individual clusters, making them harder to be classified from constitutive clusters, it is completely absent in brain-individual regulated clusters.
Finally, we trained models to compare each of the individual-regulated clusters in one tissue against all regulated clusters in the other two tissue types (both individual and overlap, Fig.5c).
In agreement with the motifs found at brain-specific individual PAS clusters, classification of brain-specific individual regulated PASs showed the best performance (auROC 71%).
3 DISCUSSION APA is a regulatory process with major impact on the down-stream post-transcriptional fate of affected transcripts, yet it has been fairly sparsely investigated.
Recently, several studies have analyzed data resulting from new high-throughput sequencing protocols, and some studies reported on differential preferences for APA usage in some genes from one tissue to another (Shepard et al., 2011).
However, without a suitable methodology to specify the significance of these events, we may confuse alter-native with specifically regulated polyadenylation.
Using a high-throughput sequencing method particularly de-signed to probe the mRNA 30-end, PA-Seq, we were able to accurately identify polyA sites with high resolution.
PA-Seq data from brain, liver and kidney were collected and constitutive genes were separated from those having more than one APA isoform.
Given the large variability of tag counts across genes and coverage across libraries, simple tag number thresholds or ratios, or information theoretic metrics such as Shannon en-tropy, are not a well-suited methodology for deep sequencing data.
They drastically inflate the number of putative alternative sites, and cannot separate spurious events with little sequence evidence from truly significant ones.
We therefore designed a suitable statistical framework to iden-tify tissue-specific events such as APA sites across multiple deep sequencing libraries.
Using a fixed-effects linear model and per-mutation tests, we were able to assign significance levels to APA usage and identify tissue-specific regulated events.
Our stringent test left us with a highly specific and suitable dataset to investi-gate the regulation of alternative APA, but led to limited sample sizes.
For example, the GFER mRNA showed two polyA sites with the distal site being used in brain only, similar to the find-ings in (Shepard et al., 2011).
However, given its relatively low (a) (c) (b) Fig.4.
(a) ROC curve for the classification of WD kernel and Spectrum kernel on constitutive PAS clusters versus background.
WD outper-formed the Spectrum kernel (auROC 99.6, 93.5%).
(b) PRC curve.
(c) Sequence Logo for (30,30) region around PAS clusters for consti-tutive genes; PAS site at position 0 i112 D.Hafez et al.
read coverage, it did not meet our stringent specificity threshold.
Replicate datasets will enable the use of other statistical tests, which will likely detect a larger subset as significantly different, and may thus help to identify additional regulatory elements that are not covered in our examples.
This study is the first of its kind to analyze multiple APA sites for a transcript and across more than two conditions.
We sepa-rated constitutive genes from genes with multiple APA sites, and examined each group separately.
Our analysis demonstrated that the main cis-regulatory elements described to be responsible for polyadenylation, are a strongand in fact a highly inform-ativehallmark for constitutive sites only.
Studies have shown that 2030% of human genes do not have the canonical PAS signal and suggested that polyadenylation regulation is directed by non-canonical sequences (Tian et al., 2005; Zarudnaya et al., 2003).
Moreover, regulation by non-canonical sequences is more frequent in genes with APA (Nunes et al., 2010; Tian et al., 2005).
In specifically regulated subsets, in particular brain APA sites, we were able to define a highly enriched motif (AAAAAAAAAA) starting just downstream of the PAS cluster (Fig.6a; application of MEME to the brain-specific subset con-firmed its significance, resulting in an E-value of 1.8e-057).
The canonical polyA signal was not observed in brain-specific clus-ters, and was found at lower conservation in liver and kidney.
This agrees with an observation reported in (Nunes et al., 2010), where a polyA site did not possess the canonical polyA signal instead contained an A-rich element in its vicinity.
An analysis of a different recent polyA deep sequencing dataset also showed a roughly 2-fold enrichment of the A-rich motif at brain sites, compared with liver and kidney, despite being generated by a different protocol and processed by a different pipeline (Derti et al., 2012).
Given that the motif is specifically observed in only one tissue within multiple datasets, it is unlikely to be an experimental artifact resulting from internal priming, but we cau-tiously point out that it may reflect a property of brain mRNAs unrelated to polyadenylation.
Our methodology can be applied to data from additional libraries, such as the data generated from applying a high-throughput sequencing protocol on five mammals (Derti et al., 2012).
This will allow for the definition of specific subsets and aid in the identification of further candidates of regulatory sequence features.
Combined with knowledge of regulatory factors affecting polyadenylation and their expression patterns, this will enable the design of models that can build on the encoura-ging tissue-specific results we have reported here.
4 CONCLUSION In summary, we have combined high-quality genome-wide data with appropriate downstream analyses and computational mod-eling.
We have described a successful strategy to identify subsets of significant condition-specific polyA events, built sequence-based models to discriminate between them, and identified new candidates for post-transcriptional regulatory features.
5 METHODS 5.1 Paired-end sequencing and read mapping A new deep sequencing protocol, PA-seq, was used to identify polyade-nylation sites at genome-wide scale.
Briefly, total mRNA is randomly fragmented and reversed transcribed with a modified oligo(dT) primer (a) (b) (c) Fig.5.
Classification of (a) tissue-specific PAS individual and overlap against constitutive.
(b) individual tissue-specific regulated PAS clusters against constitutive.
(c) Each individual regulated PAS cluster in one tissue against all regulated in other tissue types Fig.6.
Sequence logo for tissue-specific individual PAS clusters in each tissue (a) brain-specific, (b) kidney-specific and (c) liver-specific.
PAS site at position 0 i113 Modelling of tissue-specific alternative polyadenylation that base pairs with the polyadenylation tail.
The modified oilgo(dT) primer has a dU in the fourth location in the 30-end to be later digested by USER digestive enzyme.
After that, the double-stranded cDNA frag-ments are captured by streptavidin-coupled magnetic beads, and sequenced using multiplexed paired end sequencing on Illumina (Fig.1).
Adult human normal kidney and liver samples were obtained from BioChain (Cat.
# R1234142-50 and R1234149-50), and brain sam-ples were obtained from Clontech (Cat.
# 636102).
Detailed description of the PA-seq protocol is available on the website.
Before mapping, we filtered out low-quality reads and tags that did not contain the adapter sequence TTT.
The Burrows-Wheeler Alignment Tool (Li and Durbin, 2009) was used to align the paired end reads inde-pendently to the human genome (hg19), allowing two mismatches and no gaps.
After that, we only considered 50 and 30 read pairs that mapped in the same orientation within 250 000nt on the same chromosome.
To investigate the genomic regions that our reads came from, we annotated the 50 aligned reads to their genomic regions using an in-house script (Ni et al., 2010).
We did not use 30 reads for annotation because they might fall beyond the end of annotated transcripts, indicat-ing novel polyA sites.
Locations were classified into six possible cate-gories: annotated 30-UTR, 5 1000nt downstream of 30-UTR, coding region, 50-UTR, intron and intergenic region.
Non-coding genes were ignored, as well as 50 reads that mapped to 50-UTR, intergenic regions, introns or upstream of 30-UTR, as the average insert distance between 50 and 30 paired end reads amounted to 180380bp.
After alignment, non-redundant mapped read pairs that had the same paired end tags were grouped for each unique 30 position, denoting a polyA site.
For each polyA site, the count of the non-redundant 50 pairs were used to indicate the relative usage of this site.
We then filtered out 30 tags that had exactly one 50 paired read (count 1).
Finally, 30 locations that mapped genomic regions with high A-content were filtered out (13 consecutive As in the 25nt downstream of the mapped 30 pos-ition), to exclude possible contamination by internal priming.
5.2 PolyA sites cluster identification To cluster our reads, we used an algorithm previously developed for the analysis of capped 50 mRNA tags (Ni et al., 2010).
Only clusters with tag numbers greater than or equal five were considered.
We then selected Narrow Peak clusters (NP), which span 525nt, with more than half of the reads falling within2nt of the mode.
A minority of 15% showed a broader distribution of tags and was not considered further.
The relative usage (count of the non-redundant 50 pairs) of all of the 30 tags in each PAS cluster was summed up and further used as a measure of the PAS usage, PAScount.
5.3 Identifying constitutive and alternative PAS sets To determine constitutive set, we first grouped all PAS clusters of the same gene from the three tissue types together if their regions overlapped and their modes were within 10nt from the median.
To get the median, we ordered clusters according to their start position, and referred to the PAS by the mode of the median cluster.
If the PAS appeared in two tissue types only, we used the mode of the second start position.
Finally, if the gene had one PAS cluster we called it constitutive; otherwise, it was considered alternative.
5.4 Linear model to identify tissue-specific PAS To determine tissue-specific contributions to PAS utilization, we imple-mented a linear fixed-effects model.
Let Ntg, p denote the PAScount for PAS cluster p for gene g in tissue t. Then logNtg, p  Tt Gg Tt Gg "tg, p 1 where is a general intercept term, Tt is a tissue-specific effect, Gg is a gene-specific effect, Tt Gg is a tissue by gene interaction term and "tg, p is the residual.
There was no need to incorporate random effect terms as we did not have variable replicates.
Because we controlled for different tissue depth, expression of each gene in each tissue, as well as any interaction between tissues and genes, a correlation of the residual for a particular PAS cluster with tissue suggests differences in PAS usages between tis-sues.
To quantify the differential usage of a PAS between tissues, we computed the differences in the residuals "tg, p. We accounted for the lack of usage of a PAS cluster in a certain tissue by adding pseudo-counts.
We applied this model on genes that are expressed in the three tissue types, and fitted the model using Maximum Likelihood approach as implemented in nlme R library (Pinheiro et al., 2011).
5.5 Permutation test to determine P-value Our dataset was composed of three tissue libraries, each with the same set of genes, but different PAS counts.
To preserve library depth and gene expression levels in each tissue, we first calculated the contribution per-centage of each PAS cluster on the gene level [cf.
Equation (1)].
Then, we used these percentages in our permutations, but noted the total expres-sion level of each gene in each library.
Our null hypothesis assumes that PAS clusters are non-tissuespecific regulated.
To model this assumption in our permutation test, in each round, we permuted tissue labels for each PAS cluster; then, for each gene, the percentages of the permuted PAS were used to represent a multinomial distribution, from which we drew a random sample, scaled by the total gene expression value in each tissue.
As the minimal evidence for each cluster was set to five reads, missing values, i.e.
PAS clusters not detected in some of the tissues, were repre-sented by a random number between 1 and 4.
5.6 Identifying individual and overlapping PASs To identify tissue-specific PASs that are highly used in one individual tissue, we used the difference between the highest and the median residual values for each PAS as test statistic.
For each PAS, we computed the test statistic ftigmi1, where m 5357 different PAS.
For each of the observed differences in our data, we obtained a P-value based on an empirical null distribution from 1000 permutations.
P-values were corrected for mul-tiple hypothesis testing using the Storey FDR calculation (Storey and Tibshirani, 2003).
We used a liberal FDR of 0.25, to allow for the dis-covery of significant events given the relatively small number of samples being analyzed.
The tissue-specificity threshold was set to 2.376 (in log space, corresponding to P50.01, FDR50.25); all PASs showing a dif-ference 42:376 were considered significant.
To characterize PAS clusters that are highly used in two tissue types simultaneously (overlapping), we computed the test statistics to be the difference between the mean of the highest two residual values and the lowest value.
PASs with residual difference between tissues 42:782 were considered significant to the two tissues with the highest values (corres-ponding to FDR50.25, P50.011).
5.7 Calculation of VI To explore differences in APA usage among tissues, we calculated a VI that compares the number of individual regulated PAS to overlap PAS.
The VI is defined as follows: VIx, y Ix Iy=Ox, y 2 where VIx, y is the VI between tissue x and tissue y, Ix and Iy are the number of individually regulated tissue-specific PAS clusters in tissue x and tissue y, respectively, and Ox, y is the number of overlapping regu-lated tissue-specific PAS clusters in tissues x and y simultaneously.
A low value of VI between a pair of tissues indicates a high degree of correlation in APA regulation, whereas a high value of VI indicates a weak i114 D.Hafez et al.
correlation.
The calculated indices for each pair are VILiverKidney 1.44, VIBrainLiver 2, VIBrainKidney 5.09.
5.8 Calculation of Shannon entropy We assessed tissue-specific APA in the three tissue types by calculating Shannon entropy on the count of each PAS cluster identified in each tissue, according to (Schug et al., 2005).
We only considered genes that were expressed in the three tissue types, i.e.
that had at least one PAS cluster annotated for each tissue.
We determined the relative expression of each PAS cluster of a gene as follows: wtg, p Ntg, p 1=Ntg xg, p 3 where Ntg, p the PAScount for PAS p for gene g in tissue t, Ntg is the summation of PAScount for all PASs of gene g in tissue t and xg, p is the number of different PAS clusters for gene g. Next, we computed the probability of observing a PAS cluster in each tissue by Pt j p wtg, p= X t wtg, p 4 Calculation of entropy values followed (Schug et al., 2005).
Entropy values close to zero represent the group of PAS clusters that are specific to a single tissue, and increase when the PAS cluster is more broadly used in different tissue types, or when the relative contribution of the tissue to the overall usage of the PAS decreases (Schug et al., 2005).
5.9 Dataset for constitutive classification against background Our dataset is best described as a set of sequences, each is composed of an array of characters A, C, G, T. The length of each sequence is 201 char-acters.
For the positive training data, the element at position 101 repre-sents the polyA site (median of the PAS cluster).
We chose a flanking region of 100nt upstream and downstream of the mode of the PAS clus-ter because previous studies have shown that most of the main features of polyA sites are located in this region (Cheng et al., 2006).
We refer to the 101th position as 0, upstream sequences as (100,1), and downstream sequences as (1,100).
We restricted our dataset to include PAS clusters for genes that are expressed in the three tissue types.
To choose a biologically motived background/negative dataset, for each true PAS mode in our PAS cluster positive dataset, we randomly selected 10 positions downstream of the stop codon, but did not include the 100nt just upstream of the mode of the PAS.
If the gene does not have an annotated stop codon, we select positions from the last 500nt but not including the last 100nt upstream of the mode of the PAS.
We then retrieved the sequence of the 100 nt upstream and downstream of these selected sites to compose our negative dataset.
5.10 String kernels and SVM Kernel functions measure the similarity between different data points in the feature space.
For our purposes, the similarity is between two seg-ments of DNA sequences with the same length.
As noted earlier, the main cis-regulatory elements responsible for polyadenylation are located in the flanking region (40,40) nt from polyA sites, while further downstream and/or upstream (100,100) of the polyA site lie some other G/U-rich segments of sequences, with varying length, location and exact sequence compositions.
The spectrum kernel considers the global similarities be-tween two given sequences, by counting the number of occurrences of k-mer motifs (referred to as order in Section 5.12) over the entire sequence.
The Weight degree kernel with shifts focuses on local similarities between the given sequences by counting the number of matching k-mers at the same positions, within a window around the matching position (referred to as shift).
We applied both string kernels on the region (100,100).
5.11 Handling unbalanced data Our negative dataset has 10 times more examples than the positive set.
This unbalanced dataset could be challenging for classifiers; because the data is unbalanced, the cost of misclassification is also unbalanced; thus, a false negative is more costly than a false positive (Ben-Hur et al., 2008).
Therefore, we assigned relative misclassification penalty, C, for each set according to its number of examples; for positive training data, C is 10 times larger than that of the negative training data (Provost, 2000).
5.12 Model selection To settle on the combination of parameters, which represent our models ability to accurately distinguish the surrounding sequence of polyA sites from other genomic loci, we applied model selection.
The four parameters to be optimized are (i) misclassification penalty or the SVM (C), (ii) length of the substrings compared (order), (iii) positional shifts/window around polyA site for WD kernel and (iv) length of the flanking region around the PAS.
We tried different values for each of these parameters, while fixing the rest.
To avoid over-fitting, first, we randomly split our data; 20% for model selection and 80% for training and testing.
These two sets were kept independent of each other.
In the model selection phase, we applied 2-fold cross validation, and selected parameters that gave the highest auROC.
The optimal values for each parameter is shown in Table 2.
We then used the selected parameters in the training and test phase by applying 5-fold cross validation.
Evaluation curves were drawn using ROCR package (Sing et al., 2005).
5.13 SVM on tissue-specific regulated PAS clusters In this experiment, our positive examples were the set of individual and overlap tissue-specific sites, and negative examples were constitutive sites, expressed in the three tissue types and with exactly one PAS cluster.
As the WD kernel clearly outperformed the spectrum kernel on the recogni-tion of constitutive sites, we only used the WD kernel for the rest of our analyses.
Funding: This project was funded by a grant from the National Science Foundation (MCB-0822033).
Conflict of Interest: none declared.
ABSTRACT Motivation: The number of completely sequenced genomes is continuously rising, allowing for comparative analyses of genomic variation.
Such analyses are often based on whole-genome alignments to elucidate structural differences arising from insertions, deletions or from rearrangement events.
Computational tools that can visualize genome alignments in a meaningful manner are needed to help researchers gain new insights into the underlying data.
Such visualizations typically are either realized in a linear fashion as in genome browsers or by using a circular approach, where relationships between genomic regions are indicated by arcs.
Both methods allow for the integration of additional information such as experimental data or annotations.
However, providing a visualization that still allows for a quick and comprehensive interpretation of all important genomic variations together with various supplemental data, which may be highly heterogeneous, remains a challenge.
Results: Here, we present two complementary approaches to tackle this problem.
First, we propose the SuperGenome concept for the computation of a common coordinate system for all genomes in a multiple alignment.
This coordinate system allows for the consistent placement of genome annotations in the presence of insertions, deletions and rearrangements.
Second, we present the GenomeRing visualization that, based on the SuperGenome, creates an interactive overview visualization of the multiple genome alignment in a circular layout.
We demonstrate our methods by applying them to an alignment of Campylobacter jejuni strains for the discovery of genomic islands as well as to an alignment of Helicobacter pylori, which we visualize in combination with gene expression data.
Availability: GenomeRing and example data is available atContact: kay.nieselt@uni-tuebingen.de 1 INTRODUCTION Advances in high-throughput sequencing technologies have dramatically increased the speed at which genomes are sequenced (Bennet, 2004; Droege and Hill, 2008; Eid et al., 2009; Porreca et al., 2006; Rothberg et al., 2011).
This led to the establishment of large-scale genome sequencing projects such as the 1000 genomes project (Durbin et al., 2010), the 1001 genomes project in Arabidopsis thaliana (Weigel and Mott, 2009), the 10K genomes project (Haussler et al., 2009), which aims at sequencing vertebrate genomes, the insect genomes initiative i5k (Robinson et al., 2011) as well as many projects sequencing prokaryotic species, often at the level of individual strains.
The genome sequencing projects are conducted with different long-term goals.
While the 10K and The authors wish it to be known that, in their opinion, the first three authors should be regarded as joint First Authors.
To whom correspondence should be addressed.
i5k initiatives aim at collecting genomes across a large part of the tree of vertebrates and insects, respectively, the 1000 and 1001 genome projects focus on genetic variation within one species.
Regarding the prokaryotic species, genome projects often focus on this latter aspect, comparing different strains of bacteria with the goal of understanding the genetic basis of pathogenicity and drug resistance, the adaptability to environments, the extent of horizontal gene transfer as well as to elucidate the architectural diversity of bacterial genomes.
Parallel to the increase in genomic data with the development of new sequencing technologies, powerful visualization tools have been developed and continue being developed.
An excellent review on methods as well as the challenges of visualizing genomes has recently been published by Nielsen et al.
(2010).
Generally, one can distinguish two approaches to the visualization of genomes: A single genome is visualized (often in comparison to a reference genome), or multiple genomes are compared.
In the first case, genome browsers are typically utilized, which represent the genome linearly and can display multiple variables in parallel tracks aligned to the genomic coordinates.
Such tracks can contain annotation, experimental, or statistical data.
For the comparison of multiple genomes, the same linear approach as applied by genome browsers can be used.
A typical example is the viewer integrated into Mauve (Darling et al., 2004).
However, large changes such as inversions can quickly lead to visual clutter, and it can be difficult to deduce similarities and differences between genomes from the visualization.
Some genome viewers employ a circular approach to visualize one genome with annotation and experimental data, or to present an alignment of several genomes.
Circos (Krzywinski et al., 2009) is one of the most often used circular genome visualization tools.
It displays genomic data as a circular plot, in which the relationships of genomic elements are displayed using arcs.
It is, therefore, particularly useful for visualizing variation within one genome, but it can also be applied to visualize the relationship of several genomes.
While Circos without doubt produces aesthetically very attractive figures, it has the major disadvantage of only presenting a static, non-interactive view of the data.
Further circular genome viewers are MEDEA (Broad Institute, 2009) and MizBee (Meyer et al., 2009).
Another circular approach is taken by the BLAST Ring Image Generator (BRIG) that visualizes multiple prokaryotic genomes (Alikhan et al., 2011).
Each genome is compared to a reference genome using BLAST.
The hits between each genome and the reference are then visualized as concentric rings using different colors for each genome.
Additional rings, representing meta information, such as GC content, can be added.
Its main focus is to accompany sequencing projects, in particular to handle and visualize assembly data.
Though a number of excellent visualization and analysis tools are already available for researchers working with multiple genome The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: SKG MANUSCRIPT CATEGORY: [18:10 29/5/2012 Bioinformatics-bts217.tex] Page: i8 i7i15 A.Herbig et al.
sequences [see Nielsen et al.
(2010) for a review], an important obstacle remains to be overcome.
While genomes in studies of strain diversity are usually highly similar, sharing long stretches of conserved sequence, they still show differences due to larger events such as inversions, translocations and insertions/deletions.
In the context of such differences, researchers are faced with the problem of mapping existing annotations as well as experimental data to each of the aligned genomes in a consistent manner.
The coordinate transformations necessary to visualize annotations in the context of alignments are implemented implicitly in programs that deal with the analysis and visualization of alignments, such as Mauve, tools in the VISTA suite (Frazer et al., 2004), or others.
However, as the basis for our proposed visualization, we need an approach that allows us to explicitly generate a joint coordinate system that can be used to consistently specify coordinates of annotations (and also of experimental data such as mapped sequencing reads or RNAseq expression graphs), which can be used independently from a specific analysis or visualization software.
In addition, many existing methods require the specification of one reference genome that is afforded a special status (e.g.
coordinates within insertions with regard to that reference can not be expressed), which we consider an artifact of the method rather than a choice based on biological facts.
Here, we present two complementary approaches to solve this problem.
First, we present the SuperGenome algorithm, which computes a common coordinate system for all genomes in a multiple alignment.
Using this coordinate system, genome annotations can be placed consistently in the presence of insertions, deletions and rearrangements between the different genomes.
Second, we present the GenomeRing visualization which, based on the SuperGenome coordinate system, visualizes the multiple genome alignment in a circular layout.
Its main advantages are a much more appealing and clearer visual presentation of deletion, insertion and rearrangement events compared to linear alignment viewers, as well as more interactivity than existing circular visualizations.
We designed GenomeRing to be a fast, interactive overview tool for alignments of several (ideally less than 10) genomes with high similarity (less than 25 genomic events for optimal visual clarity).
The general idea and proof-of-concept visualizations of our methods were submitted to the Illumina iDEA challenge 2011, where our submission was selected as the most creative algorithm.
We have now integrated the GenomeRing visualization with Mayday (Battke et al., 2010), our visual analysis platform for omics data.
As a result, GenomeRing can be linked with all other visualizations offered by Mayday, including a traditional, linear genome browser.
2 METHODS 2.1 SuperGenome construction The construction of the SuperGenome is based on whole-genome alignments.
In the case of genomic rearrangements, these can be viewed as a collection of local alignments, also called blocks.
In this context, we define our concept of a SuperGenome as a representation of the multiple sequence alignment with an additional common coordinate system, and mappings between this coordinate system and the aligned sequences.
To achieve this, we process the set of blocks as follows: for each block, the alignment information is used to calculate a bidirectional mapping between the coordinate system of the SuperGenome and the original coordinates of each input genome contained in the block.
The SuperGenome coordinate system is based on the alignment coordinates of all concatenated blocks, whose ordering is derived from the reference genome of the alignment.
Note that the chosen order of the blocks is not crucial for the functionality of the SuperGenome concept.
In comparison to the traditional alignment concept, which defines pairwise mappings between the coordinates of the involved sequences, the SuperGenome has the advantage that independent alignment blocks are combined into a global coordinate system.
This makes it also possible to assign coordinates to unaligned regions.
For the generation of whole-genome alignments for prokaryotic organisms, we decided to use the progressiveMauve algorithm (Darling et al., 2010) of the genome alignment software Mauve [Darling et al.
(2004); version 2.3.1], since besides insertions and deletions, Mauve is also able to discover genomic rearrangements, i.e.
translocations and inversions.
If such events occur, the alignment is provided as a set of blocks, where each block represents a region in two or more genomes that can be collinearly aligned.
In GenomeRing, however, we do not only want to visualize rearrangements but also large-scale insertions and deletions.
This requires further processing.
For this, each aligned sequence in a block is scanned for gaps that are longer than a user-defined threshold.
The start and end coordinates of these gaps are stored as break points.
Using the break points of all sequences, the block is split up into subblocks, which represent insertions or deletions in one or more of the aligned genomes.
Subblocks that are smaller than a user-defined threshold are discarded and neighboring subblocks are merged if their conservation pattern, i.e.
the set of contained genomes, is the same.
The set of all remaining subblocks is the basis for the GenomeRing visualization.
By adjusting the parameter for the minimal block length, the user can choose whether only large events will be displayed, which is especially useful for more diverse genomes, or whether smaller insertions and deletions should also be visualized.
2.2 Layout of GenomeRing To visualize a SuperGenome alignment, we created GenomeRing, an interactive circular visualization and integrated it into our visual analytics software Mayday (Battke et al., 2010).
The blocks computed by the SuperGenome algorithm give rise to circle segments sized according to their length and ordered as defined by the SuperGenome ordering.
Each block results in two segments of identical angular extent, one on the outer (forward) ring, one on the inner (backward) ring (Fig.1).
Assume the alignment contains n genomes.
We split each of the two rings into n lanes, each of which is assigned to exactly one genome.
With these preliminaries, we can visualize genomes as follows: Each aligned genome G is a concatenation of blocks.
This is visualized as a directed path connecting the SuperGenomes blocks in the order that they have within G. The path is drawn with a unique, distinct color assigned to G, based on the quantitative color scheme suggested by ColorBrewer (Harrower and Brewer, 2003).
Within each block, the path uses the lane assigned to G. Blocks that are not in G are not part of the path.
If a block appears in its native direction in the respective genome, the path includes the respective segment on the outer ring.
If the block is inverted in G, the segment on the inner ring is used.
The start and end of G are represented by small flags drawn inside of the inner ring, which also indicate where the path is heading (from the start) and where it is coming from (towards the end).
Several types of connections have to be visualized: Direct connections exist if two consecutive SuperGenome blocks are also consecutive in G and appear in the same direction.
The genomes path simply connects the two consecutive blocks, staying on the same radius.
Second, a deletion in G results in a jump connection.
This can either be an outer jump (both blocks are on the outer ring), or an inner jump (both blocks are on the inner ring), which are visualized by introducing a curved connection outside the outer ring, or inside the inner ring, respectively.
Third, inversions lead to interchange connections that link blocks on the outer circle with blocks on i8 Copyedited by: SKG MANUSCRIPT CATEGORY: [18:10 29/5/2012 Bioinformatics-bts217.tex] Page: i9 i7i15 GenomeRing Fig.1.
GenomeRing and its view elements.
The rings of the SuperGenome are laid out in two concentric circles representing the forward resp.
the reverse direction.
Each genome is represented by a colored path connecting the blocks according to their order in the genome.
Small flags indicate each genomes start and end position.
Deletions, inversions and translocations result in jumps, either outside of the outer or inside of the inner circle (representing deletions, translocations), or between circles (representing inversions, inverted translocations).
The synthetic example shown here comprises three genomes: The genome of species 1 contains the blocks D and B (inverted); Species 2 contains B, A (inverted), and C (inverted); Species 3 contains A, C and D A B Fig.2.
The influence of block sorting.
The alignment of (A) is shown sorted according to the green genome (B).
This optimal sort order reduces the number of jumps from four to two.
The long jumps over a total of three blocks which added up to over 300 in length have become unnecessary the inner circle, or vice versa.
Interchange connections can also jump over deleted blocks, as described above.
The visual clarity of the presentation is greatly influenced by two factors: The total number of jumps and interchange connections, and the number of jump edges that overlay each other.
First, jump and interchange connections should be avoided because these lead to visual clutter.
To minimize the number of indirect block connections, as well as to highlight different aspects of the alignment, users can reorder the SuperGenome blocks in a number of ways, either based on their native order in one of the aligned genomes, or reflecting the order chosen by the SuperGenome algorithm (see Section 2.1), or by using one of the block sorting algorithms (described in Section 2.3).
Finding an optimal block order is of high importance, as exemplified by Figure 2 which shows the same alignment as Figure 1, but requires only two short connections instead of three long arcs and one short connection.
Second, we use different rings to differentiate between blocks present in forward and backward direction, respectively, in each genome.
Thus the direction in which a genomes path traverses each one of the SuperGenome blocks can be freely chosen by the layout algorithm, instead of always traversing segments in a clockwise (or counter-clockwise) direction depending on their direction of incorporation in the genome.
This allows us to reduce the number of jump edges as well as their length (in degrees) by finding sets of consecutive blocks that have the same direction in a genome G and, for each such set, choosing to traverse all consecutive blocks in the direction which maximizes the number of direct connections between sets of consecutive blocks.
This leads to a much more appealing visualization as it allows limiting the maximal angular length of jump edges to <180.
Finally, our path layout algorithm minimizes the overlap between jump edges by placing each edge such that the length of overlap (in degrees) is minimal.
In addition to the SuperGenome blocks and the genomes paths, the view contains a scale indicating the number of bases displayed per degree, and a legend which maps each color to the respective genomes identifier.
Blocks in the SuperGenome are either labeled numerically by the SuperGenome algorithm, or with user-defined names [e.g.
with the name of a well-known pathogenicity island (Hacker et al., 1990)].
Interactivity is an important factor to allow users to understand the presented view, and to create figures for dissemination to collaborators as well as for publication.
The GenomeRing view allows for free rotation, zooming and panning using direct mouse interaction.
When zooming in, additional detail, such as annotated genes, can be presented (see Section 2.4).
Individual genomes can be hidden from view and the automatically assigned color can be changed.
The SuperGenome block labels can also be hidden.
Clicking on any position inside a lane of a block shows a tooltip window with a description of the position in terms of its coordinate (base pairs from the start), the size of the block, the index of the block and the offset in base pairs from the blocks start.
All of these numbers are given both for the SuperGenome coordinate system and for the coordinate system of the respective genome of the lane.
Three fundamental parameters of the view can be interactively adjusted: The radial spacing between jump connections, the size of the gaps between SuperGenome blocks and the width of the paths representing the genomes.
Adjusting these, users can create very different visualizations of the same alignment.
For example, reducing the inter-block gaps to zero presents a view focusing on the presence and absence of blocks in the particular genomes as well as which genomes share each block.
Increasing the inter-block gap until each block is drawn with zero angular extent, on the other hand, creates a view highlighting shared evolutionary events, such as inversions and translocations.
Note that all inter-block gaps are drawn with the same extent, irrespective of the actual number of bases that were removed during filtering (see Section 2.1).
To facilitate understanding of even very complex views with larger numbers of events, path animation can be added: A dash pattern is moved along each genomes path to visualize the direction of the path, traveling from the genomes start to its end position.
We propose that this can help users in understanding the displayed alignment.
2.3 Block sorting A clear visualization of the different genomes in GenomeRing strongly depends on the ordering of the blocks.
The default ordering as provided by the SuperGenome may not in every case be a good choice to visualize the multiple genome alignment since very long connecting arcs can be the result.
Therefore, we allow users to rearrange the blocks by three different approaches.
First, each of the aligned genomes can be chosen as a basis for an ordering.
This results in a consecutive ordering of the blocks necessary to display the chosen genome, while leaving the order of the other blocks with respect to each other unchanged.
This strategy is useful when one wants to focus on a single genome.
i9 Copyedited by: SKG MANUSCRIPT CATEGORY: [18:10 29/5/2012 Bioinformatics-bts217.tex] Page: i10 i7i15 A.Herbig et al.
The second approach is to find an ordering that optimizes an objective function.
This approach can find an ordering of the blocks such that a clear visualization of all genomes in GenomeRing is obtained.
Here, we present three criteria for minimization in order to find an optimal arrangement of the blocks in the SuperGenome.
These are: (1) Minimization of the number of jumps If two blocks A,B are consecutive in one genome G, but not consecutive in the SuperGenome, the result is a jump connection in Gs path.
By minimizing the total number of jumps found for all genomes, this method minimizes the number of non-consecutive blocks.
(2) Minimization of the number of skipped blocks A jump connection (as defined above) gives rise to one or more skipped blocks, as several blocks can lie between A and B.
This strategy minimizes the total number of skipped blocks regarding all genomes displayed in GenomeRing.
(3) Minimization of the total jump length This method is related to the previous strategy.
However, instead of using the number of skipped blocks for each jump, here a jump is weighted by the length of the resulting connecting arc.
The total jump length is the sum of the absolute magnitudes of the angles between each pair of connected blocks, computed for all genomes.
For each of these strategies an iterative process is applied in order to minimize the cost function f .
This cost function f determines the costs for visualizing the alignment with the currently defined block ordering given one of the abovementioned minimization criteria.
The minimization process then operates as follows: (1) The cost function f is applied to the initial arrangement to determine the cost c. (2) A genome G is chosen and the blocks in the SuperGenome are sorted according to G. This changes only the order of the blocks contained in G, leaving all other blocks in their original ordering with respect to each other.
If this new arrangement results in a smaller cost c, it is chosen as the new best arrangement and c is updated to c=c.
(3) New orderings are calculated by swapping pairs of blocks in GenomeRing.
This is done for all possible pairs.
The cost function f is applied to evaluate the new arrangement.
(4) New arrangements of the blocks are calculated by moving each block through the SuperGenome, i.e.
removing it from its original position and inserting it at another position.
This is done for each block and each possible insertion position.
Again the cost function f is applied.
(5) Steps 24 are performed for each genome in GenomeRing, always using the optimal ordering found in the previous rounds.
(6) To guarantee that a minimum is reached, the process is repeated until the cost function converges, and no smaller costs c <c can be found.
Our approach has a runtime in the order of O(n2 b2) for n genomes and b blocks in the SuperGenome, as b2 possibilities exist for swapping blocks, as well as for moving blocks.
A naive approach enumerating all possible block orderings would result in a runtime of O(b!
), which is clearly infeasible even for a moderate number of blocks.
For example, 15 blocks result in about 1012 different arrangements that have to be evaluated.
Finally, our third approach allows users to interactively change the arrangement of the blocks after visual inspection.
All three strategies can be used in combination.
2.4 Linked visualizations GenomeRing is integrated into our visual analytics platform Mayday (Battke et al., 2010) as a visualization which can display data from multiple per-species data sets.
Using Maydays facilities for data and meta-information management, we can for example add information about gene expression in the GenomeRing visualization.
For instance, genes that have been found to be coregulated (using statistical methods) can be mapped to the SuperGenome blocks to allow users to quickly identify genomic colocation.
Another interesting application is to map differentially expressed genes to the SuperGenome and then to find out whether some of these map to regions of interest, such as pathogenicity islands.
Within Mayday, all visualizations are implemented as linked views.
As a result, users can select genes of interest from any of the available visualizations and directly see them highlighted in the GenomeRing view.
Furthermore, double-clicking on any position within a genome displayed in GenomeRing will center a linked instance of Maydays genome browser (Symons et al., 2010) on the respective genomic position.
The genome browser can thus be used to investigate detailed information including gene annotations, expression data, mapped reads from RNA-seq experiments, sequence information and meta-data such as P-values from statistical tests.
All visualizations, including the GenomeRing view, can be exported as publication-quality figures in various bitmap (PNG, TIFF, JPG) and vector formats (SVG, PDF) in arbitrary resolution.
3 RESULTS 3.1 Discovery of genomic islands in Campylobacter jejuni strains One example for the application of our method is the discovery of large-scale deletions or insertions.
One reason for long insertions can be genomic islands.
These are regions in a genome which are usually acquired as a result of horizontal gene transfer.
They are of great interest because they often contain genes encoding proteins related to pathogenicity or drug resistance.
To demonstrate the ability of our concept to identify such regions, we applied it to an alignment of four Campylobacter jejuni strains (RM1221, NCTC11168, 81-176, 81116).
Campylobacter jejuni is a Gram-negative microaerophilic bacterium, which is one of the major causes of gastroenteritis (Snelling et al., 2005).
For the SuperGenome generation, the minimal block size was set to 10 kb.
This resulted in a SuperGenome consisting of 14 blocks, which we visualized with GenomeRing (Fig.3).
The majority of the blocks contain all four strains.
There are four large insertion blocks for C. jejuni RM1221, which are apparent at first glance.
They correspond to genomic islands, which are referred to as C. jejuni-integrated elements (CJIEs) (Fouts et al., 2005; Parker et al., 2006).
CJIE1, which is also known as CMLP1, is a Campylobacter Mu-like phage.
CJIE3 is a putative integrated plasmid while CJIE2 and CJIE4 also contain phage-related proteins.
This example nicely shows our visualization concept.
While a linear viewer defines blocks as a collinear alignment, we add blocks also for insertions and deletions so that organisms that contain or lack the respective regions in the alignment can be quickly visually identified.
3.2 Environment-specific gene expression in Helicobacter pylori The integration of GenomeRing with Mayday allows for linking the view to other visualizers.
This includes Maydays genome browser, where genomic annotations, such as the location of genes, as well as related expression values can be visualized.
In addition, genomic annotation loaded in the genome browser can be mapped into the lane of the respective genome in the GenomeRing visualization.
i10 Copyedited by: SKG MANUSCRIPT CATEGORY: [18:10 29/5/2012 Bioinformatics-bts217.tex] Page: i11 i7i15 GenomeRing Fig.3.
GenomeRing visualization of an alignment of four C. jejuni strains (RM1221, NCTC11168, 81-176, 81116).
The four genomic islands in RM1221 (CJIE14) appear as insertions in the view and the blocks are labeled accordingly.
For this view, the minimal block length was set to 10 kb.
None of the genomes contains an inversion, thus the reverse ring is empty We demonstrate this for an alignment of three Helicobacter pylori strains (26695, J99, P12) and gene expression data for H. pylori 26695.
Helicobacter pylori is a Gram-negative, microaerophilic bacterium that populates the human stomach causing gastritis and even gastric cancer (Cover and Blaser, 2009).
Because of its role as a major human pathogen, genetic factors responsible for its pathogenicity are of great interest.
Sharma et al.
(2010) published a comprehensive transcriptomic study of H. pylori strain 26695 under various experimental conditions.
The organism was grown to mid-logarithmic phase (ML), under acid stress (AS) as well as in contact with responsive gastric epithelial cells (AG) and non-responsive liver cells (HU).
In addition, the transcriptome was measured when the organism was grown in cell culture medium (PL).
When applying the SuperGenome construction to the alignment of the three H. pylori strains, we set the threshold for the minimal block size to 50 kb, which only preserves very large events.
This results in a SuperGenome consisting of eight blocks, of which two represent inversions between strain 26695 and J99/P12 (Fig.4).
We integrated the expression data of the study by Sharma et al.
(2010) into GenomeRing as follows: After loading the expression data into Mayday we performed a z-score normalization and a k-means clustering.
By this, we were able to identify groups of genes that are differentially regulated under specific experimental conditions.
We selected two large expression profile clusters of which one contains genes which are upregulated during acid stress (AS) and another one which contains genes upregulated when the organism is in contact with liver cells (HU).
Using different colors, Fig.4.
GenomeRing visualization of an alignment of three H. pylori strains (26695, J99, P12).
For this view, the minimal block length was set to 50 kb.
Two large-scale inversions between 26695 and J99/P12 are represented by blocks 5 and 6.
Gene expression data for 26695 has been mapped into the corresponding lane (red) of all blocks.
Genes upregulated in condition HU or AS are shown on the red lane, colored purple and green, respectively.
The region that is shown in more detail in Fig.5 is highlighted by a red rectangle (not part of GenomeRing visualization) both groups were mapped into the lane of H. pylori strain 26695 in the GenomeRing visualization.
By doing so, it is possible to get an instant overview about where in the genome the genes are located that specifically react to a certain condition.
It appears that genes reacting to the same condition tend to be organized in chromosomal clusters in many cases, which can be seen by stretches of visualized gene loci with the same color.
The investigation of chromosomal clusters of co-expressed genes is of interest because it allows researchers to generate hypotheses about the function of these genes as they are often involved in similar biological processes.
An example of such a locus is highlighted in Figure 4.
By double-clicking on that region in the GenomeRing visualization, the Mayday genome browser instance, which is linked to the view, jumps to that locus, thus allowing a more detailed inspection (Fig.5).
Here, we combined the locus information of the genes with a heatmap track showing the expression for all experimental conditions.
In addition, wiggle tracks show the expression level in single-nucleotide resolution for the two conditions (HU, AS), as calculated from the RNAseq data.
Two chromosomal clusters of coexpressed genes can be found in this region.
One larger cluster contains genes upregulated under the HU condition.
Another smaller cluster that consists of genes i11 Copyedited by: SKG MANUSCRIPT CATEGORY: [18:10 29/5/2012 Bioinformatics-bts217.tex] Page: i12 i7i15 A.Herbig et al.
Fig.5.
Visualization of the genomic region in H. pylori 26695 highlighted in Figure 4 using Maydays track-based genome browser.
The five tracks shown here from top to bottom are as follows: (A) genomic coordinates in the H. pylori 26695 genome; (B) locus-specific expression value heatmap of genes upregulated under HU or AS condition (forward strand: above the baseline, reverse strand: below the baseline).
The heatmap shows the expression for all five experimental conditions (from top to bottom: AG, AS, HU, ML, PL); (C) visualization of protein-coding genes located in that region.
The chromosomal gene clusters upregulated under the HU or AS condition are labeled by horizontal braces (not part of the genome browser visualization); (D) wiggle track for RNAseq data from the HU condition (reverse strand); (E) wiggle track for RNAseq data from the AS condition (forward strand) upregulated under acid stress (AS) is located further downstream.
An inspection of the functional annotation of these genes revealed that the larger cluster primarily consists of ribosomal proteins while the smaller cluster contains only four genes, two of which encode cation efflux system proteins (czcA), which have been shown to be required for growth at low pH (Bijlsma et al., 2000).
The other two genes are annotated as hypothetical.
However, these hypothetical proteins might be related to the same system as indicated by their coexpression.
This brief analysis demonstrates how our concept allows for the generation of hypotheses by an iterative visual inspection of data at several levels.
Starting with GenomeRing at a global overview level, which shows structural differences between genomes but which in addition can incorporate locus-specific data, users can step down to a level of analyzing single gene loci or even more fine-grained information such as RNAseq data in single nucleotide resolution, as also illustrated in Fig.5.
4 DISCUSSION In this work, we presented two complementary approaches to the multiple genome alignment problem.
Our SuperGenome method computes a consistent coordinate system for a multiple genome alignment.
As the SuperGenome mapping is performed on single nucleotide level, high-resolution expression height graphs resulting from RNAseq or tiling array experiments can also be investigated within a common coordinate system, which is especially useful for comparative analyses.
The comparison of gene expression, for example, is possible even if an ortholog mapping between the organisms is not available.
In addition, the SuperGenome allows for the inspection of conserved intergenic regions, e.g.
to discover yet unknown coding or non-coding transcripts.
Genomic annotations can also be mapped into the SuperGenome enabling users to compare the gene content of a region between several organisms independently from the location of that region in the respective genomes.
Even if translocations and inversions occur, the regions still map to the same coordinates in the SuperGenome.
The SuperGenome is complemented by GenomeRing to visualize completely aligned genomes within a common coordinate system to get a quick and broad overview of the structural differences between these genomes.
Thus for each region that appears in one of the aligned genomes, it is immediately apparent which of the other genomes also contain or lack that region.
The SuperGenome approach in general is not limited to visualization with GenomeRing.
Conventional genome browsers can be applied in parallel to obtain more detailed information on a specific locus in a chosen genome.
We have therefore linked GenomeRing with Maydays genome browser.
Furthermore, linear visualizations of the whole genome alignment based on the SuperGenome is of course feasible.
Linear representations of genome alignments certainly have their strong points, especially when larger number of genomes are compared.
However, the circular approach can be an effective means to minimize visual clutter (Nielsen et al., 2010), as it allows connecting edges to be, on average, much shorter than in a linear view (see Fig.6 for a comparison).
For example, if in one genome the last block of the SuperGenome is to be connected with the first block, the circular layout can represent this by a very short edge, while the linear layout requires an edge traversing the whole length of the alignment.
Furthermore, the circular layout results in two possible directions for each edge, clockwise or counter-clockwise, allowing us to limit the maximal i12 Copyedited by: SKG MANUSCRIPT CATEGORY: [18:10 29/5/2012 Bioinformatics-bts217.tex] Page: i13 i7i15 GenomeRing Fig.6.
Linear versus circular view.
An alignment of four genomes is shown.
(A) block-based display in the style of Mauve (Darling et al., 2004) using colors to distinguish blocks and edges to link identical blocks in the aligned genomes.
(B) linear representation of the SuperGenome for this example, using one color for each genome.
(C) circular GenomeRing visualization of the same example, built step-by-step from the alignment, starting with two genomes.
The inversion and deletion events accounting for the differences between the two genomes are very clearly visible.
The third genome gives rise to new blocks by its new start/end coordinates, by a deletion (new blocks A C), and by an insertion (new block F).
The addition of the fourth, orange, genome does not induce the formation of any more blocks, but simply adds a new path.
The increase in visual complexity is quite small when compared with the linear views.
(D) visualization of the same alignment using Circos.
Aligned blocks are connected length of each edge to <180 while at the same time offering the possibility to choose a longer edge route to avoid overlapping edges.
It is important to note that the GenomeRing approach to MSA visualization presents a different angle than traditional visualizations.
Most linear viewers, such as the one contained in Mauve, as well as the traditional Circos plot for genome comparison, show the order of the alignment blocks for each genome.
This allows users to quickly identify the composition of each genome, but results in a possibly large number of edges indicating block identities.
Our approach, on the other hand, focuses on the identification of differences and similarities between genomes: By using one circle A B Fig.7.
Visualization of multiple chromosomes in an alignment.
(A) concatenating chromosomal alignments.
(B) representing each chromosome by a circular view and adding inter-circle jumps to represent evolutionary events involving several chromosomes segment for each alignment block, and one color for each genomes path, users can immediately identify the composition of each block.
Determining, for example, which genomes contain a certain genomic island is straightforward in GenomeRing.
If blocks are ordered independently for each genome in order to achieve collinearity within each genome, identifying a genomic island requires the recognition of the corresponding block by a common attribute.
As colors are often used to identify blocks, the human visual system, which is only capable of distinguishing a small number of distinct colors (Ware, 2008), becomes the limiting factor, and users are forced to use mouse interaction to check whether their assumption about block identities are correct.
Based on these considerations, we present GenomeRing as an addition to the researchers toolbox, complementing existing (linear) viewers, and not as a replacement for existing tools.
The GenomeRing visualization can not only be used to display multiple alignments of whole genomes resulting from the SuperGenome algorithm.
Because of its flexible implementation, it can also be used to display other types of alignments, for example of a cluster of genes (each block representing one gene), to allow for the investigation of synteny.
Another application could be the analysis of splice variants, where each block represents one exon.
Currently, GenomeRing is designed to display the alignment of a single chromosome.
To display alignments spanning multiple chromosomes (or on bacterial genomes spanning several plasmids), we envision two strategies.
First, the alignment could be presented as the concatenation of several SuperGenomes, one computed for each chromosome (Fig.7 left).
This is already possible with the current GenomeRing version.
Second, several circular views (one per chromosome) could be displayed in a common visualization with an additional type of jump edge connecting blocks from different chromosomes to represent inter-chromosomal translocations (Fig.7 right).
An important feature of GenomeRing is the possibility to rearrange blocks in the SuperGenome in order to enhance visual clarity, as well as to highlight different aspects of the multiple genome alignment.
Clearly, our strategy of finding a good ordering for the visualization of the genomes in GenomeRing only results in a local optimum and thus does not guarantee to find an optimal solution for a specified optimization criterion.
However, a full evaluation of all possible arrangements of blocks in the SuperGenome is infeasible.
We approach this problem with a i13 Copyedited by: SKG MANUSCRIPT CATEGORY: [18:10 29/5/2012 Bioinformatics-bts217.tex] Page: i14 i7i15 A.Herbig et al.
quadratic-time heuristic which allows us to find a nearly optimal solution in acceptable time.
An optimal sorting of the blocks according to some objective function might on the other hand not be optimal for the user.
In the future, we, therefore, plan to develop further strategies for the rearrangement of blocks in the SuperGenome that incorporates information gained from a user study.
A user study for GenomeRing could provide highly valuable information on several aspects.
Regarding the block ordering methods, such a study could provide information on two different questions.
First, it could allow us to elucidate how the blocks of the SuperGenome have to be arranged such that individual blocks contained in a specific genome can easily be spotted by the user.
Second, one could gain information on the interpretability of the multiple genome alignment and how this correlates with different block arrangements.
Including such information in the design of a sorting algorithm could strongly improve the visualization.
In the course of such a user study, we could also explore the effectiveness of the provided interaction methods, and perhaps also include semi-automatic methods to highlight features of possible interest, or to zoom and pan to such features.
Furthermore, different strategies for the edge layout could be evaluated.
By allowing paths to traverse blocks in clockwise or counter-clockwise direction, we reduce visual clutter.
An important question to address would be, whether users experience difficulties interpreting the alignment, i.e.
whether minimizing the number or length of jump edges leads to a visualization which is not optimal from the users point of view.
Users might prefer a visualization using only a single direction of block traversal, at the expense of increased visual complexity.
Another central feature that we would like to investigate is whether users understand that the genome paths in GenomeRing only show the order of the blocks, and not the direction of the actual bases within each block.
Another route for further development lies in the summarization of events to display different views of the SuperGenome depending on the current zoom level.
As too many blocks lead to visual clutter due to the possible increase in the number of connecting edges, high-level views could summarize small blocks depending on some measure of similarity, for instance.
Clearly, every visual approach has its limitations due to the limited resolution and/or the limits of screen area, and also due to the limits of the human visual system.
We have conducted preliminary tests that show that visualization of more than 10 genomes is, in most cases, infeasible.
Likewise, large numbers of long-range events such as translocated inversions result in increasingly complex visualizations.
GenomeRing is not designed to visualize hundreds of genomes, blocks or events.
In our view, the challenge of adequately visualizing such very large alignments, or alignments with a large divergence between species lies not in finding a visualization that shows every detail at maximum resolution, but rather in appropriate summarization and a (semi-automatic) focusing on the important events.
5 CONCLUSION GenomeRing is a highly interactive tool for multiple alignment visualization based on SuperGenome coordinates.
The concept of the SuperGenome together with GenomeRing provides a quick and broad overview through the display of genomic events from completely aligned genomes and allows for a detailed analysis of specific genes through the linkage of GenomeRing to several other visualizations incorporated into Mayday, such as Maydays genome browser.
These aspects and the applicability of GenomeRing to other fields of research make it a highly usable visualization strategy complementing already existing visualization techniques.
In addition, the SuperGenome coordinate system can be widely applied beyond the field of visualization, as it provides a generic solution to the problem of consistently specifying coordinates in multiple genome alignments without attributing a special status to an arbitrarily chosen reference sequence.
ACKNOWLEDGEMENTS The authors wish to thank Dr Cynthia Sharma for kindly providing the H. pylori expression data.
Funding from the DFG Priority Program 1335 Scalable Visual Analytics is gratefully acknowledged.
Funding: DFG priority program 1335 Scalable Visual Analytics.
Conflict of Interest: none declared.
ABSTRACT Motivation: Analysis of relationships of drug structure to biological response is key to understanding off-target and unexpected drug ef-fects, and for developing hypotheses on how to tailor drug therapies.
New methods are required for integrated analyses of a large number of chemical features of drugs against the corresponding genome-wide responses of multiple cell models.
Results: In this article, we present the first comprehensive multi-set analysis on how the chemical structure of drugs impacts on genome-wide gene expression across several cancer cell lines [Connectivity Map (CMap) database].
The task is formulated as searching for drug response components across multiple cancers to reveal shared ef-fects of drugs and the chemical features that may be responsible.
The components can be computed with an extension of a recent ap-proach called Group Factor Analysis.
We identify 11 components that link the structural descriptors of drugs with specific gene expression responses observed in the three cell lines and identify structural groups that may be responsible for the responses.
Our method quan-titatively outperforms the limited earlier methods on CMap and iden-tifies both the previously reported associations and several interesting novel findings, by taking into account multiple cell lines and advanced 3D structural descriptors.
The novel observations include: previously unknown similarities in the effects induced by 15-delta prostaglandin J2 and HSP90 inhibitors, which are linked to the 3D descriptors of the drugs; and the induction by simvastatin of leukemia-specific response, resembling the effects of corticosteroids.
Availability and implementation: Source Code implementing the method is available at: http://research.ics.aalto.fi/mi/software/GFAsparse Contact: suleiman.khan@aalto.fi or samuel.kaski@aalto.fi Supplementary Information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Modeling and understanding the diverse spectrum of cellular responses to drugs is one of the biggest challenges in chemical systems biology.
Some of the responses can be predicted for targeted drugs, which have been designed to bind to a specific protein that triggers the biological response.
The binding of a drug to a target largely depends on the structural correspondence of the drug molecule and the binding cavity of the target mol-ecule, which can be modeled in principle, given ample computa-tional resources.
Off-target effects are harder to predict.
They are dependent on the cell types, individual genetic characteristics and cellular states making the spectrum of responses overwhelmingly diverse.
The less well-known the drugs mechanism of action and the characteristics of the disease, the harder the prediction from first principles becomes.
The most feasible way to approach this challenge in an unbiased way, which does not require prior knowledge of all on-and off-target interactions of drugs, is to collect systematic measurements across different drugs, cell types and diseases and search for response patterns correlating with the characteristics of the drugs.
The patterns found can be used as evidence for hypotheses on underlying action mechanisms or directly in predicting the responses.
The Connectivity Map (CMap; Lamb et al., 2006) described the basis for a data-driven study of drugeffect relationships at a genome-wide level.
CMap hosts the largest collection of high-dimensional gene expression profiles derived from treatment of three different human cancer cell lines with over one thousand drugs.
The CMap data have been used in a multitude of studies revealing new biological links between drugs and between drugs and diseases.
Genome-wide gene expression responses from the CMap have been used to discover clusters of drugs having simi-lar mechanisms of action, resulting in novel findings, such as effects of heat shock protein (HSP) inhibitors and identification of modulators of autophagy (Iorio et al., 2010).
The CMap data have also been successfully used in large-scale integrative studies including the analysis of regulation of drug targets (Iskar et al., 2010), hERG annotations to predict novel inhibitors (Babcock et al., 2013) and drugs interactions with protein networks (Laenen et al., 2013).
Quantitative structureactivity relationship analysis (QSAR; Cramer et al., 1988) is a widely adopted approach to studying drug responses.
Traditionally, univariate biological activities are predicted using a range of methods, including classical regres-sion, support vector machines and Random Forests.
The key challenge when moving from traditional QSAR to system-wide analysis of chemical effects is how to relate structural features to genome-wide cellular responses.
Integration of chemical structures with genome-wide responses has become a major research direction in chemical systems biol-ogy (Iskar et al., 2012; Xie et al., 2012).
Keiser et al.
(2009) studied structural similarities between ligand sets while Klabunde and Evers (2005) used proteinligand complexes to predict off-targets.
To infer potential indications for drugs, Gottlieb et al.
(2011) combined similarities from chemical structures, gene expression profiles, protein targets and several*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com well , , ,-large structure , systems-other datasets.
Atias and Sharan (2011) modeled linkage between structural descriptors of drugs and their side effects using canon-ical correlation analysis (CCA; Hotelling, 1936).
Structures have also been used with genomic datasets to predict toxicity and complex adverse drug reactions (Russom et al., 2013).
Recently, Menden et al.
(2013) combined structures of drugs and mutation information of cell lines to predict drug cytotox-icity in a series of cell lines.
Relationships between structural descriptors of drugs and their gene expression profiles have also been studied.
Cheng et al.
(2010) examined similarities between chemical structures and molecular targets of 37 drugs that were clustered based on their bioactivity profiles.
Low et al.
(2011) classified 127 rat liver samples to toxic versus non-toxic responses, based on com-bined drug-induced expression profiles and chemical descriptors, and identified chemical substructures and genes that were re-sponsible for liver toxicity.
In a broader setting, when the goal is to find dependencies between two data sources (chemical struc-tures and genomic responses), correlation-type approaches match the goal directly, and have the additional advantage that a predefined classification is not required.
Khan et al.
(2012) generalized structure response analysis to multivariate correlations with CCA on the CMap.
Because of the limitations of classical CCA, their study was restricted to a limited set of descriptors (76) and genomic summaries (1321 genesets), and did not attempt to take into account the data from the three separate cell lines.
In this article, we present the first probabilistic approach to the problem of integrated analysis of effects of chemical structures across genome-wide responses in multiple model systems.
We extend the earlier work in three major ways: (i) instead of using only two data sources (as in classical CCA), we used the recent Bayesian group factor analysis (GFA) method (Virtanen et al., 2012) that generalizes the analysis to multiple sources, here three cell lines and two sets of chemical descriptors.
(ii) Our Bayesian treatment with feature-level priors enabled us to cope better with the uncertainties in the high-dimensional data.
(iii) We included a more informative set of 3D chemical descriptors to complement the widely used 2D fingerprints, which are recognized to only explain limited aspects of drugs (Schneider, 2010).
Our goal was to uncover the big picture of relationships be-tween chemical structure parameters and genome-wide re-sponses, in a data-driven fashion (Fig.1).
The data came from CMap, 11 327 gene expression responses in three cell lines (HL60-Blood Cancer/Leukemia, MCF7-Breast Cancer and PC3-Prostate Cancer; Lamb et al., 2006) and from two sets of chemical descriptors: 780 3D Pentacle descriptors of drugs (Duran et al., 2008) and 2769 functionally relevant structural fragments (FCFP4; Glen et al., 2006) as 2D fingerprints of the drugs.
These five datasets consist of samples from the 682 drug treatments, coupled by the detailed drug identity.
We analyzed the statistical relationships between the datasets by decomposing them into a set of interpretable components.
Our method quan-titatively outperformed previous studies, thereby validating the approach.
We rediscovered findings reported earlier as well as identified novel drug associations and detailed structure response relationships.
2 METHODS 2.1 Gene expression datasets We used the CMap (Lamb et al., 2006) gene expression data as a measure of the biological response of the three cancer cell lines to drug treatments, forming the gene expression datasets.
The CMap hosts over 7100 gene expression profiles including technical replicates treated with 1309 drugs and is the largest available resource of its kind.
Responses from a subset of these drugs (682) were measured on all of the three cell lines, namely, HL60 (leukemia), MCF7 (breast cancer) and PC3 (prostate cancer cell line).
We obtained the raw gene expression profiles from the CMap and used the data from the most abundant microarray platform (HT-HG-U133A).
The data were preprocessed using the Robust Multiarray Averaging (RMA; Irizarry et al., 2003) and drug treatment versus control (log2) dif-ferential expression was calculated batchwise (Khan et al., 2012).
Technical replicates were merged by taking the mean of each gene.
This resulted in gene expression profiles for the 682 drugs having meas-urements over all three cell lines.
To reduce noise, we adapted the ap-proach of Iorio et al.
(2010) for our setting, by retaining the expression of top 2000 up-and 2000 downregulated genes for each sample, while con-sidering the rest as noise (set to zero).
The threshold was large to retain diverse effects and removed small values.
These profiles formed three biological response datasets (one for each cell line), each being a differ-ential gene expression matrix of 682 drugs times 11 327 genes.
2.2 Chemical descriptor datasets The chemical space of drugs was represented using two different types of chemical descriptors, namely, the 2D fingerprints FCFP4 and 3D de-scriptors Pentacle.
The FCFP4 (functional connectivity fingerprints of radius 4; Glen et al., 2006) are circular topological fingerprints designed specifically for structureactivity modeling and similarity searching.
They are rapidly computable and heavily used in a wide variety of applications (Rogers and Hahn, 2010).
Each dimension of the fingerprints represents a certain 2D fragment of the compounds, interpretable as presence of certain substructures, typically stereochemical information, and allows easy visual inspection of structures.
Therefore, FCFP4 can be used to identify the core 2D substructures that make compounds structurally similar and are responsible for biological activity.
The more complex 3D descriptors Pentacle (Duran et al., 2008) cap-ture the functional properties of the compounds using molecular inter-action fields.
They are able to group together compounds with dissimilar chemical structures and yet having the same type of molecular field prop-erties.
This is especially important in our study where the aim is to find small molecules that share biological functions despite structural dissimi-larity.
Most of the traditional fingerprints, like MACCS (Molecular Access System) and FCFP4, are superior to recognize 2D structural simi-larity but unfortunately unable to recognize structurally unrelated and yet biologically similar compounds binding into the same binding pocket.
The opposite is true with most (if not all) field-based similarity methods like Pentacle, which find more effective distant similarities; therefore, we decided to combine both approaches.
In the earlier work, Khan et al.
(2012) had used VolSurf descriptors to represent molecular properties.
Although VolSurf is an optimal method for physicochemical properties estimation, it is not able to describe pharmacophore features extensively, unlike the Pentacle descriptors, and thus is not an option in our study.
Pentacle field distance descriptors were computed using Pentacle v 1.0.4 (http://www.moldiscovery.com/soft_pentacle.php), by Molecular Discovery.
The descriptors were calculated for all the available 10 probe sets, namely, D2, O2, N2, T2, DO, DN, DT, ON, OT, NT, where D is dry probe to represent hydrophobic interactions, O is carbonyl oxygen probe to represent hydrogen bond donor feature of the molecules and N flat probe of Nitrogen is the hydrogen bond acceptor, while T is TIP probe representing shape of the molecule, in terms of steric hot spots.
i498 S.A.Khan et al.--Canonical Correlation Analysis ( ) Due to paper Instead , , ,-Expression Datasets Connectivity Map Connectivity Map Leukemia Breast Cancer Prostate Cancer Connectivity Map-.-very , 1 1 Descriptor Datasets-2-dimensional 3-dimensional very While For each probe set, 78 descriptors were obtained, representing the inter-action potentials of probes at different distances, resulting in 780 descrip-tors in total.
Distances in the Pentacle descriptors are true distances between putative interaction sites (hot spots) and are thus connected to the size of the compound and distances between potential pharmacopho-ric features.
This results in a 682 780 data matrix, with each row being a drug and the 780 columns representing the Pentacle descriptors.
This forms the first chemical dataset in our study.
The 2D FCFP4 represent the chemicals as structural fragments.
In FCFP, the fragments are not predefined, rather computed dynamically and thus can represent variation in novel structures.
The FCFP4 finger-prints were computed using Pipeline Pilot Student Edition software (http://accelrys.com/products/pipeline-pilot/), by Accelrys.
A total of 2769 unique structural fragments are found, and the fingerprints are rep-resented as a matrix of 682 compounds 2769 fragment descriptors.
This forms the second chemical dataset in our study.
2.3 Model: GFA We search for relationships between chemical descriptors and biological responses, as clues to the key underlying biological processes.
GFA is a model designed to capture such relationships (statistical dependencies) by explaining a collection of datasets (views) by a set of factors or compo-nents, which form a combined low-dimensional representation (Virtanen et al., 2012).
In themulti-view setting, each component is active in a subset of the datasets and is a simplified model of an underlying process visible in those sets.
The task solved by GFA is to separate the shared compo-nents that capture the structurebiology relationships from the rest of the data: the former are visible in all or a subset of the datasets, whereas components active in a single view describe variation specific to that particular view or noise.
Given a collection of M datasets X 1 2 RND1 .
.
.X m 2 RNDM , con-sisting of N co-occurring samples x m n , GFA finds a set of latent compo-nents (with upper limit K, see below).
Each dataset is assumed to have been generated as a linear combination of latent components Z 2 RNK, with weights of the combination given by a loadings matrix W m 2 RDmK: Assuming normal distributions for simplicity, the model is x m n Normal W m zn;S m  ; znNormal 0; I ; 1 where zn is the n th row of Z, and S m is a diagonal noise covariance matrix.
GFA is special in that the projections W are required to be group-wise sparse, i.e.
all the elements W m :;k are set to zero for the com-ponents k that are not active in the mth dataset.
The components with non-zero projections between two or more views capture dependencies between the views.
To increase the interpretability of the model, we extend GFA by intro-ducing element-wise sparsity in addition to the group sparsity for the projection matrices, matching the biological prior assumption that each process typically activates only a subset of genes.
We introduce element-wise automatic relevance determination (ARD; Neal, 1995) prior for the projection weight matrices, pushing irrelevant weight values W m d;k toward zero and making each component element-wise sparse.
For the group sparsity, we apply the group spike and slab prior where the binary vari-able H m k controls the activity of the k th component in the group m. The prior is W m d;k H m k Normal 0; m d;k 1  + 1H m k  0; H m  k Bernoulli k ; kBeta a; b ; m d;k Gamma a; b : 2 IfH m k becomes zero, all values inW m :;k will be set to zero.
To complete the model description, we set an uninformative before the diagonal elem-ents of the precision matrix S m 1.
Here we made two assumptions, (i) Fig.1.
Overview of the symmetric multi-structure to multi-response decomposition.
(A) The five datasets spanning the common 682 drugs are (B) decomposed into components by GFA.
Components of Type 1 represent shared patterns in both chemistry and biology, whereas Type 2 describes biology-only or chemistry-only variation (not as useful in our case).
(C) Each shared component identifies key structures and genes of an underlying biological process i499 Data-driven genome-wide structure-response links x functional connectivity fingerprints ( )Group group Factor factor Analysis analysis ( ) Group Factor Analysis ( ) `` '' ,-: ,-s prior to normal distributions for simplicity and (ii) sparsity.
Sparsity was imple-mented by combining the previously (Klami et al., 2013) separately used beta-Bernoulli formulation and the element-wise normal-gamma ARD.
We represent our (M=5) datasets as matrices of drugs versus fea-tures.
The rows represent the samples (drugs), and the columns are the features (genes or chemical descriptors).
Drugs pair all the views, i.e.
a row in all matrices corresponds to the same drug.
A total of N=682 drugs were used in the study.
The features of the chemical descriptors, Pentacle (m=1) and FCFP4 (m=2) are D1=780 Pentacle probe fields and D2=2769 fragments, respectively.
The biological responses of the three cell lines (m=3,4,5) are represented by differential expression of Dm=11327 genes each.
The hyperparameters are set to a; b=103 and a; b=1, to obtain uninformative priors.
We initialize the model by sampling the latent vari-ables from the prior.
The model parameters (W m :;k ,H m k , S m , m d;k , k, Z) are then learned from the data using Gibbs sampling.
The number of components is optimally learned from data by initializing K to be large enough, such that sparsity assumptions push some to be inactive.
Here for computational reasons, we set K=80, a value significantly larger than the actual number of shared components, and let the noise model represent the rest of the data.
For sampling, we ran 10 chains and selected for further analysis the one having its likelihood closest to the mean of non-outlier chains.
The first 5000 samples were discarded as the burn-in, and the chain was run for 1000 more iterations, with a thinning factor of 5.
The mean value of the samples was used as a representation of the model.
As a sanity check, we verified that our shared components had over 70% similarity in top genes and descriptors with the second (non-used) chain.
The models complexity is O(NDK2+K3) where D=sum(D1:M).
The current implementation ran for 5 days on a stand-ard desktop computer consuming 6 GB memory.
For interpretation, we represent each component by listing the high-valued latent scores Z and projection values W. For the latent scores, we performed a permutation test to detect the most significantly (q-value50.05) activated drugs, while for the projections we inspected the top 30 elements.
3 RESULTS Figure 2 gives an overview of the types of components discovered by the model.
For studying structureactivity relationships, the most important are the components shared by one or more chemical view and one or more of the cancer subtypes.
The com-ponents active in only the expression datasets represent drug responses not captured by the used chemical descriptors, and components only active in the chemical datasets represent bio-logically irrelevant structural variance.
Additionally, components active in only a single dataset may represent dataset-specific noise.
We found 11 shared components, which will be discussed below.
The detailed structureresponse relationships discovered from all the shared components are visualized in Supplementary Figure S1 and tabulated in a usable format in Supplementary Table S2.
3.1 Validation via chemical biology ontology We started by quantitatively evaluating how closely related the drugs in the shared components are in terms of known chemical biology relationships and compared our data with those of two previous studies (Iorio et al., 2010; Khan et al., 2012) that inves-tigated drug actions using the same CMap database version.
The established chemicalbiology relationships were obtained from the ontology Chemical Entities of Biological Interest (ChEBI; Degtyarenko et al., 2008), which is the largest such ontology of small compounds.
ChEBI links compounds with respect to chemical structure, biological roles they are known to play and their applications.
Examples of classifications are antibiotic, coenzyme and agonist (biological); donor, ligand, in-hibitor (chemical); and pesticide, antiasthmatic (applications).
ChEBI was downloaded as a graph and contained paths between 328 (of 682) of our compounds via 611 ontology terms (http://www.ebi.ac.uk/chebi).
The average similarity (inverse path distance) of drugs within the shared GFA components was consistently higher than the corresponding similarities of Khan et al., (2012) and Iorio et al., (2010) and random sets of compounds (Fig.3).
The largest path length (16) in ChEBI linked all drugs, whereas the smallest (2) linked only the most similar.
Interestingly, the difference in GFA and others on small path lengths was higher than that on larger ones, indicating that drugs closely connected in ChEBI were even better found by GFA.
Fig.3.
Quantitative validation of chemical biology similarity of drugs in shared GFA components.
Drugs in the same GFA component (blue squares) had a consistently higher mean average similarity (y-axis) in ChEBI than either of the earlier studies, and random sets of compounds, over the entire range of ChEBI path lengths (x-axis).
To access the rela-tive contribution of the 3D descriptors we additionally plotted results with components containing them (dashed line) and components contain-ing only 2D descriptors (dotted line), demonstrating that both descriptors are valuable.
Error bars (red) are one stdandard over 1000 randomly generated sets Fig.2.
Summary of the GFA components.
The plot demonstrates activ-ity (black is active) of each component (y-axis) over the five input datasets (x-axis).
Each component is active in some or all of the datasets.
Components shared by (active in) both chemical descriptor and expres-sion datasets capture structureresponse relationships i500 S.A.Khan et al.
Normal Beta .
, , ,-a total of-Chemical Biology Ontology chemical , chemical Chemical Biological , Applications-out while  3.2 Component interpretations We next analyzed the shared components in detail.
Each com-ponent connects a set of structural drug properties and gene ex-pression changes, forming a hypothesis of a structureactivity relationship.
A component can be characterized by the set of drugs that activate it the most, and by the set of genes that are expressed differentially when the component is active.
We first compared the findings with the two other studies that have investigated drug actions using the same CMap database (Iorio et al., 2010; Khan et al., 2012).
Of the 11 shared GFA components, the majority of the drugs in seven components were similar to the clusters found by Iorio et al.
(2010), while three components captured structurally driven cell-specific responses they had missed.
Compared with the other earlier study (Khan et al., 2012), the majority of the drugs in 6 of the 11 GFA com-ponents matched a corresponding structure-response subcompo-nent of Khan et al., (2012), again indicating conformance to known results.
Our components also revealed several novel drug actions because of cell-type specificity and advanced 3D descriptors that were missed by both of these earlier studies, and are presented below.
Detailed interpretation of all the 11 shared components is pre-sented in Supplementary Table S1.
The components are num-bered in the order of the amount of variation they captured; the cell line-specific components identified by the model are separ-ately ordered with the prefix SP.
One component (SP3) captured outlier response of a single drug and was omitted from further analysis.
The majority of the components captured effects shared among all the three cell lines, whereas five components had re-sponses that were cell line-specific (Components SP1, SP2, SP4), dominant in a specific cell line (Component 7) or revealed some cell line specificity indications for an interesting drug (Component 1).
The 2D structural features were active in most components, identifying similarities in structurally analogous drugs.
The pentacle descriptors captured similarities in five com-ponents, four of which indicated novel responses of drugs that have not been reported before.
We discuss these four novel com-ponents in detail below.
One of them had cell line-specific effects (SP2), whereas the remaining cell line-specific components (SP1 and SP4) are summarized in Table 1.
Component 1 was characterized by cardenolides.
The top seven drugs of the component, lanatoside C, digitoxigenin, digoxin, digoxigenin, ouabain, helveticoside and strophantidin belonged to this class.
The primary activity of the other drugs anisomycin, lycorine and cicloheximide is protein synthesis inhibition, and bisacodyl is used as a laxative through stimulation of secretion in the colon.
Cardenolides act onNa+/K+ pumps and are known for ion flux alterations.
Interestingly, the other compounds of Component 1 also appeared to affect membrane potassium ion flux.
Bisacodyl and anisomycin activate K+ flux, lycorine is known to reduce membrane potential (indicative of potassium efflux) and, indicative of affecting K+, emetine needs to be ad-ministered with potassium to reduce cardiotoxicity.
Interestingly, bisacodyl exhibited the response in MCF7 and PC3 cells only, suggesting that its target may be expressed selectively.
On the structural side, the top four FCFP4 fragments collect-ively represented the correct core 2D response triggering sub-structure in all the seven cardenolides, as detailed in Figure 4.
The other two key drugs, bisacodyl and anisomycin, were differ-ent from cardiac glycosides in terms of 2D structures, but the Pentacle descriptors indicated potential field similarities on ON, OT and NT probes.
These probes referred to existence of common structural pharmacophoric features: hydrogen bonding and shape-related features.
The 3D descriptors may therefore indicate that these drugs bind the same ion channels as the cardenolides.
Component 3 captured protein synthesis inhibition.
All drugs in the component are known to inhibit protein synthesis but each in a different way.
The only exception, alexidine, is a derivative of clorhexidine, which is used as an antibacterial mouth wash. Interestingly, it has been described to have anticancer cell activity through an unknown target (Yip et al., 2006).
The model identi-fied pentacle probe fields of D2, DO and DT (shape and lipophilicity-related probes) that relate alexidines protein synthe-sis inhibition response with the known protein synthesis inhibitors.
Component 5 was HSP90 inhibition response.
The component contains the three similar drugs geldanamycin, tanespimycin, alvespimycin, and on the 2D structure level dissimilar 15-delta prostaglandin J2 (PGJ2) and puromycin.
Geldanamycin and its two analogs tanespimycin and alvespimycin are HSP90 inhibi-tors, and the latter two have been explored in the clinic as anticancer drugs.
PGJ2 has also been described as having anticancer activity through an unknown mechanism, causing in-hibition of several cancer survival signals.
Puromycin is reported Table 1.
Shared components having cell line-specific response Drug description Biological interpretation Structural P. SP1 Antimetabolite (8-Azaguanine) used for antineo-plastic activity and anisomycin a protein synthesis inhibitor.
8-azaguanine has been used in leukemia (Colsky et al., 1955).
Protein synthesis inhibition in HL60 and PC3 cells only.
It could be interesting to explore 8-azaguanine as an anti-prostate cancer drug.
In a recent study, Wen et al.
(2013) also indicated 8-azaguanine for potential therapeutic efficacy in prostate cancer.
2D ring structures of 8-azaguanine SP4 Antiestrogen drugs Response visible in MCF7 (estrogen receptor) cell line only.
Pentacle ON/ OT fields.
Note: The components (rows) are summarized by their top drugs (Column 1), biological response (Column 2) and the structural properties (Column 3).
i501 Data-driven genome-wide structure-response links Interpretations structure to Out o 7 3 to out due toline , while which either-components component ,--component-while-7 4 7-do indeed Cardenolides-lipophilicity very very heat shock protein ( )--as an aminonucleoside antibiotic with a primary function of terminating ribosomal protein translation.
At the response level, this component appeared to be strongly inducing a heat shock response with many HSP and related genes being upregu-lated (see Fig.5, left).
The expression profile strongly indicated that PGJ2 and puromycin are also inhibiting HSP90.
PubChem drug-target data demonstrate that HSP90 targets have been re-ported as active in geldanamycin and its derivatives, while un-tested/unspecified for both puromycin and prostaglandin.
On the structural side, the 2D descriptors confirmed that puro-mycin and prostaglandin are dissimilar to the three geldanamy-cin analogs.
However, the Pentacle descriptors clearly indicated that N2, DN and NT fields shared a strong pattern across all the five drugs.
The patterns were only visible in features of smaller distances of these large molecules, indicating that only a small region of these compounds (polar atoms of all compounds) cre-ated the activity, whereas the rest of the structure is just needed to maintain the shape.
This fitted well with the observation that the drugs are overall structurally dissimilar.
At the smaller dis-tances, the structure responsible for biological response was char-acterized by N2: ligands hydrogen bonding capacity, DN: hydrogen bonding and lipophilicity and NT: hydrogen bond-ing/shape-based descriptors.
In geldanamycin and prostaglandin, this distance (see Fig.5 where N2 descriptor is plotted) was con-nected to polar ring atoms and more precisely corresponding hydrogen bonding positions.
These same positions, although in a different conformational arrangement (but with almost identi-cal distance), are critical in the binding of geldanamycin to HSP90.
Hence, while the expression data strongly argue for PGJ2 inhibiting HSP90 activity at some level, the structural in-formation suggests that this effect could be through a direct binding to HSP90 enzymes.
Component SP2 was characterized by responses to a set of corticoids, other steroids such as etynodiol, and surprisingly dif-ferent drugs simvastatin and repaglinide.
There appears to be a dual response: an HL60-specific metabolic regulatory response and an HL60 and PC3-selective anti-inflammatory response (Fig.6) with the MCF7 not exhibiting these responses at all, indicating that the relevant target or signal may be selectively expressed in HL60 and PC3 cells.
Both simvastatin (a choles-terol-lowering HMG-CoA reductase inhibitor drug) and repagli-nide (a diabetes drug) are highly dissimilar at the 2D level when compared with the corticosteriods, but both interestingly have been reported to have anti-inflammatory activities, likely because of targets other than the primary target(s).
Once again, Pentacle descriptors capture the underlying similarities between these drugs through NT and N2 fields, suggesting that the common gene expression patterns induced by the different drugs (cortico-steroids, simvastatin and repaglinide) is a result of binding the same targets.
4 CONCLUSIONS AND DISCUSSION We extended the drug response analysis paradigm from standard QSAR, of relating drug properties and univariate responses, to finding relationships between specific structural descriptors of drugs with the genome-wide responses they elicit in multiple cell lines.
The task was formalized as discovering dependencies be-tween multiple datasets and addressed using the state-of-the-art method GFA.
The approach identified structuregenomic re-sponse relationships as underlying components of the data and can be used as a tool for exploring such relationships from large-scale measurement datasets.
We quantitatively validated our structureresponse compo-nents over the established chemicalbiology relationships of ChEBI and found them to be better than earlier studies (Iorio et al., 2010; Khan et al., 2012) that did not account for separate cell lines and advanced 3D chemical descriptors.
Moreover, sev-eral drug groups we identified were consistent with earlier stu-dies, while several revealed interesting novel findings earlier studies had missed, demonstrating that our approach is viable for explorative multi-set structureactivity analysis.
These novel findings were clearly attributed to separate cell lines and advanced 3D descriptors in our formulation.
In a different set-ting, Yera et al., (2011) found 3D similarity to be more important Fig.4.
Structure identification in Component 1.
Left: the top four FCFP4 structural fragments identified by the model as strongly relating to the response of the drugs (right).
When combined, these fragments represent the core response triggering structure steroid backbone (shaded gray) in all the cardenolides i502 S.A.Khan et al.-very s very while very lipopholicity shape-H-s does to due to , Group Factor Analysis ( )-,---for off-target identification, and this was partially supported by our study as well.
The discovered components revealed interesting new findings of potential importance for revealing novel action mechanisms of drugs.
The 2D fingerprints highlighted important core structural groups primarily responsible for activity of similar drugs, such as the identification of the steroid backbone in cardiac glycosides and aromatic ring in HDAC (Histone deacetylases) inhibitors.
The joint analysis of data from multiple cell lines with advanced 3D Pentacle descriptors allowed us to identify relationships between drugs that were not known earlier.
If validated, this suggests an approach that could significantly help in medicinal chemistry and drug design.
For example, our data led to the identification of a previously unknown and novel shared mech-anism of 15-delta prostaglandin J2 (PGJ2) and HSP90 inhibitors.
Interestingly, PGJ2 and related prostaglandin analogs have re-peatedly been described in the literature for having anticancer activities, but their mechanism of action has not been clarified before (Fionda et al., 2007; Hegde et al., 2011; Zimmer et al., 2010).
Furthermore, our analysis revealed that simvastatin, a cholesterol-lowering drug, has a leukemia-specific response simi-lar to a range of corticosteroids.
This appears to be a significant finding as lovastatin, a close structural analog of simvastatin, was recently shown to selectively inhibit leukemic stem cells to-gether with several steroids (Hartwell et al., 2013).
Such systematic explorations raise the possibility for targeted interventions and will become a growing trend in the future as more large-scale datasets like the CMap will become available.
For drug designers, it opens up the opportunity to tailor drug molecules to match a desired gene expression fingerprint.
For medicinal chemists, it could help to increase understanding of actionmechanisms of existing drugs and revealing potential on-label and off-label applications for use in precision medicine.
ACKNOWLEDGEMENTS We thank Pekka Tiikkainen for generating the FCFP4 descriptors.
Funding: This work was supported by the Academy of Finland [140057 and Finnish Centre of Excellence in Computational Inference Research COIN 251170]; the Jane and Aatos Erkko Foundation; and the FICS doctoral program.
Confiict of Interest: none declared.
Fig.5.
Component 5 identified a novel HSP90 response of prostaglandin.
Left: gene expression response of the top seven drugs in the three cell lines (y-axis), over the top genes (x-axis) of the component, demonstrates HSP genes being strongly upregulated by the HSP90 inhibitors and by the strikingly different puromycin and prostaglandin.
Right: N2 descriptor in geldanamycin and prostaglandin connected to several polar ring atoms (red and blue).
The Pentacle feature (N2 distance range) found by GFA as related with HSP gene expression is represented with the yellow line Fig.6.
SP2: corticosteroids showing response specific to HL60 cells, while only minor regulation in PC3 and not at all in MCF7 i503 Data-driven genome-wide structure-response links h eat shock protein ( )-; Hegde etal., 2011 very ,
