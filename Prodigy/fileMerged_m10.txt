Summary: Intragenic duplications of genetic material have important biological roles because of their protein sequence and structural consequences.
We developed Swelfe to find internal repeats at three levels.
Swelfe quickly identifies statistically significant internal repeats in DNA and amino acid sequences and in 3D structures using dynamic programming.
1 INTRODUCTION Duplications play a major role in genome evolution by creating and modifying cellular functions (Marcotte et al., 1999).
Duplications can be large, up to the entire genome, or small, down to small parts of genes.
While genome and gene duplications have been extensively studied, few works have aimed at identifying and studying intragenic repeats.
These arise in DNA but are selected for their functional and structural consequences.
Therefore, the simultaneous study of repeats at DNA, protein sequence and protein structure levels is necessary to understand their biological role.
Currently, no tool allows for the integrated analysis of internal repeats at the three levels.
Several programs efficiently detect large very similar DNA repeats [e.g.Reputer (Kurtz and Schleiermacher, 1999), Repseek (Achaz et al., 2007)], or tandem repeats [e.g.Tandem Repeat Finder (Benson, 1999)].
But there is a lack of methods to identify small, closely spaced and divergent repeats using appropriate substitution matrices and statistical procedures.
Some programs detect structural similarities [Vast (Gibrat et al., 1996), CE (Shindyalov and Bourne, 1998), DALI (Holm and Sander, 1993)] but they are slow and not adapted to detect internal similarities.
Our tool, Swelfe, uses conceptually the same algorithm to detect internal similarities at these three levels allowing to analyze the evolution of DNA repeats at the light of their effects on protein sequence and structure.
This facilitates pinpointing sequence-structure associations and understanding the evolutionary forces acting upon the evolution of these elements.
To whom correspondence should be addressed.
2 ALGORITHM AND STATISTICS Swelfe identifies repeats by alignment of DNA sequences, amino acids sequences and three dimensional (3D) structures.
Preliminarily, 3D structures are encoded as linear sequences of angles ( angle is the dihedral angle between four consecutive C) (Usha and Murthy, 1986) (supplementary Fig.1).
Strings of angles have been shown to be very compact ways of representing protein backbones while conserving most of the structural features of the peptide skeleton (Carpentier et al., 2005).
In Supplementary Materials we show comparisons with DALI showing that Swelfe is capable of finding very distant similarities even in the absence of classical secondary structural elements.
Using this description we find repeats by dynamic programming with the Huang and Miller algorithm (Huang and Miller, 1991; Huang et al., 1990) on sequences and protein structures (Supplementary Fig.2).
The system of scores was adapted at each level (see Supplementary Table 1 for formulae and default parameters).
In sequences, Swelfe uses any BLOSUM or PAM matrix for proteins while it generates a similarity matrix explicitly accounting for the frequencies of each nucleotide in DNA (Achaz et al., 2007).
The structural score for two matching angles increases when the circular difference between them decreases and also accounts for the relative frequencies of-angles on the PDB (Supplementary Fig.3).
Thus very frequent angles, e.g.originating from-helices or-sheets, have a lower score.
As post-processing steps we check that the sequence repeats are statistically significant (see below).
Since a succession of nonperfectly matching-angles could theoretically lead to poor overall superposition of repeats we check that the relative root mean square deviation (RRMSD) (Betancourt and Skolnick, 2001) between the two copies of the repeat is low.
The default threshold (0.5) corresponds to a probability of 103 of finding such a low RRMSD in a 20 residues substructure.
The vast majority of significant repeats we find in the PDB structures has much lower values of RRMSD (see histograms of RRMSD and RMSD distributions in Supplementary Material).
Along with Swelfe we provide a python script that filters and simplifies the output of highly overlapping successive repeats (default: >50% overlap).
Most parameters of Swelfe can be tuned as described in the manual.
An example of protein exhibiting a repeat at the three levels is shown on Figure 1.
To assign a statistical significance for repeats in sequences we implemented the Waterman and Vingron method (Waterman and Vingron, 1994).
The P-value is computed using the distribution of scores in a large number of random sequences computed by shuffling codons or amino acids of the original sequence.
Full description 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Example of repeat found at the three levels in the Tata-box Binding Protein (TBP) of Sulfolobus acidocaldarius (1MP9).
(a) DNA (137 nt of repeat length), (b) amino acid sequence (82 aa), (c) 3D structure (83 aa).
Repeats are shown in light gray and non-repeated regions are shown in black.
Amino acid and 3D repeats are almost perfectly coincident, but the DNA repeat is smaller and within the region of the other repeats.
Among homologous elements, similarity decreases with divergence time at different rates.
It decreases quicker at the DNA and slower at the protein structural levels (Chothia and Lesk, 1986).
This frequently results in smaller repeats in DNA than at the other levels.
Edges of very degenerated repeats may also not precisely coincide at the different levels due to terminal mismatches at some but not at all levels.
This is a typical feature of methods aiming at optimizing local alignments.
can be found in Supplementary Material.
We observed that drawing 100 random sequences is enough in most cases to obtain the most significant repeats (see Supplementary Fig.4).
The same authors also proposed a faster declumping estimation method using fewer (e.g.20) random sequences.
We implemented it in Swelfe (see Supplementary Fig.5).
We find it to be 6 (DNA) to 10 (amino acids) times faster when calculating the same number of scores on random sequences, and we recommend it as a preliminary filter when scanning large databases.
On structural alignments there is no currently well-accepted method to assign statistical values to the alignment scores.
We thus chose a conservative default score based on the analysis of the resulting structural alignments (250 followed by the RRMSD filter described earlier).
This default value leads to finding approximately the same number of repeats at the level of amino acids and structures for the PDB proteins.
3 IMPLEMENTATION Swelfe was written in C language and we offer a number of precompiled binaries (Linux and Mac OS X) and the source code.
Swelfe is rather fast.
Using a Xeon MacPro we analyzed the 9537 proteins from the subset clusters50 of PDB (i.e.structures having <50% sequence identity with each other) for which we found DNA and amino acid sequences.
The program took less than a minute to find the 3D repeats or the amino acid repeats, 5 min for the DNA repeats.
Statistical evaluation slows the program because it needs generating and analyzing the random DNA and protein sequences.
Yet, when we made the same analysis including statistical evaluation for repeats using default parameters, the program took about 20 h for finding and classifying all DNA repeats and 30 min for the amino acid repeats.
It uses 16 MB RAM for the DNA bank.
The web server interface allows drawing relationships between the results at the three levels and visualization of the 3D structural results using Jmol (www.jmol.org).
We also built a databank linking explicitly PDB structures with their genes and amino acid sequences through extensive similarity searches.
This databank contains 85 845 entries, thus allowing extensive analyses at the three levels, and is available from the authors upon request.
ACKNOWLEDGEMENTS Swelfe is hosted by Ressource Parisienne en BioInformatique Structurale (RPBS).
Funding: Grants from Region Ile-de-France to ALA, ACI IMPBIO to EvolRep and ANR-06-CIS to project PROTEUS.
Conflict of Interest: none declared.
Summary: Detailed analyses of the population-genetic nature of copy number variations (CNVs) and the linkage disequilibrium between CNV and single nucleotide polymorphism (SNP) loci from high-throughput experimental data require a computational tool to accurately infer alleles of CNVs and haplotypes composed of both CNV alleles and SNP alleles.
Here we developed a new tool to infer population frequencies of such alleles and haplotypes from observed copy numbers and SNP genotypes, using the expectation maximization algorithm.
This tool can also handle copy numbers ambiguously determined, such as 2 or 3 copies, due to experimental noise.
Recent high-throughput experimental technologies have produced a vast amount of data on copy number variations (CNVs), which are variations of the number of DNA segments that are 1 k bases or larger, in human individuals, and their universality has been increasingly recognized (Redon et al., 2006).
Since DNA segments of this size often include entire genes and their regulatory regions, CNVs are likely to have a major influence on phenotypic traits such as disease susceptibility (Feuk et al., 2006).
To perform population-genetic analyses such as analyses of allele frequencies and linkage disequilibrium (LD), alleles or haplotypes have to be determined.
However, current high-throughput technologies cannot determine genotypes (pairs of alleles) of CNVs but instead measure only phenotypic copy numbers, which are the total numbers of allelic copies over two homologous chromosomes (Conrad and Hurles, 2007).
Moreover, because of experimental noise, such technologies often produce phenotypic copy numbers that are not uniquely determined as one number but rather are given an ambiguous value, such as 2 or 3 copies (Komura et al., 2006).
In the case of single nucleotide polymorphisms (SNPs), genotypes are experimentally determined, and then, using these genotypes, haplotypes can be computationally inferred (Niu, 2004).
Similarly, for trisomic chromosomes, as observed in Down syndrome, there To whom correspondence should be addressed.
is a method (Clark et al., 2004) which infers three haplotypes from data on three alleles at each SNP site.
Since the CNV data are different from such genotypic data, these methods cannot be applied to determine alleles of CNVs as well as haplotypes composed of CNV alleles and SNP alleles, which are necessary for calculating LD between CNV and SNP loci.
A recent CNV study (Redon et al., 2006) classified experimental measurements of signal intensities that correlate with copy numbers of individuals and regarded the three clusters as three genotypes; but it is unclear how to treat cases of more than three clusters and how many copies the alleles actually have.
For precise analyses of CNV data, it will be necessary to develop techniques that accurately infer CNV alleles and CNVSNP haplotypes.
In this study, we developed a new computational tool that infers population frequencies of allelic copy numbers as well as those of CNVSNP haplotypes from a mixture of the data of both phenotypic copy numbers at CNV loci and genotypes at SNP loci.
This tool can also handle the phenotypic copy numbers that are ambiguously determined.
We tested this tool using simulated datasets and showed a good accuracy of the inference.
We here introduce a tool called MOCSphaser (mixture-of-CNVSNP phaser), which is a commandline tool written in the Perl language.
2 ALGORITHM Let us call an allelic copy number the number of allelic copies at a CNV locus on a chromosome.
We denote an allelic copy number by its number.
Let us call a phenotypic copy number the total number of allelic copies over two homologous chromosomes.
Let us call an ambiguous (phenotypic) copy number a phenotypic copy number that is not uniquely determined as one number because of experimental noise or limitations.
Ambiguous copy numbers are classified into or-type and greater-type.
An or-type ambiguous copy number indicates that several candidate numbers are suggested.
We denote such an ambiguous number by concatenating these numbers by or.
For example, when a copy number is either 2 or 3, we denote this equivocal state by 2or3.
A greater-type ambiguous copy number indicates that copy numbers over a certain value are experimentally indistinguishable.
We denote this number using >.
For example, when copy numbers greater than 6 are impossible to discern, we denote this equivocal state by >6.
We denote SNP alleles by the letters a and b.
For example, [1, a/10, b] represents a diplotype composed of haplotype [1, a] and [10, b], in which the first haplotype contains allelic copy number 1 at a CNV locus and SNP allele a at the next SNP locus and the second haplotype contains alleles 10 and b at the same loci, respectively.
Suppose that we have a dataset that lists both phenotypic copy numbers at multiple CNV loci and SNP genotypes at multiple SNP loci for unrelated individuals (Fig.1).
From this dataset, we used the expectationmaximization (EM) algorithm to estimate haplotype frequencies under the assumption of HardyWeinberg equilibrium.
First, for a CNV locus, we list all possible pairs of allelic copy numbers whose total number is the same as the phenotypic copy number at the locus (Fig.1).
See Supplementary Material for the case of ambiguous copy numbers.
For a SNP locus, we list a genotype as experimentally determined.
Next, from the listed genotypes, we make up all possible diplotype configurations (Fig.1).
After enumerating all diplotype configurations, we iteratively update the frequencies of haplotypes contained in the diplotype configurations.
This procedure is essentially the same as in the EM algorithm of SNP haplotype frequency estimation (Excoffier and Slatkin, 1995); for details of this procedure, see Supplementary Material.
We examined the performance of our algorithm using simulation tests; see Supplementary Material for the results.
3 EXAMPLE We packaged the MOCSphaser program together with example datasets, which consisted of four simulated datasets and eight real datasets.
The simulated datasets were generated by random sampling from pre-defined, true haplotype frequencies under HardyWeinberg equilibrium.
We also packaged files containing the true frequencies and the frequencies estimated by MOCSphaser.
As shown in these files, all true frequencies in the true sets were close to the estimated frequencies, and the estimated frequencies of alleles or haplotypes that were present in the estimated sets but absent in the true sets were all negligibly low.
As example real datasets, we provided experimental CNV data (Hosono et al., 2008), which were measured by quantitative PCR, on CYP2D6 and MRGPRX1 genes for individuals of European descent from Utah, USA (CEU) and for individuals of the Yoruba from Nigeria (YRI) in the HapMap populations (The International HapMap Consortium, 2007).
We also provided real mixture data of these CNVs and neighboring SNPs that we arbitrarily selected as samples.
ACKNOWLEDGEMENTS M.K.
developed the algorithms and wrote the paper; T.T.
checked the algorithms and reviewed the paper; Y.N.
reviewed the paper.
We thank T. Kawaguchi for implementing the phasing algorithm into MOCSphaser and T. Morizono for coding the simulation algorithm.
We acknowledge N. Hosono and M. Kubo for information on the quantitative PCR data and S. Ishikawa and H. Aburatani for information on the Affymetrix GeneChip experiment data.
Funding: This work was partly supported by JSPS.KAKENHI (20790269).
An illustration of the enumeration procedure in our algorithm.
The symbol CP in the first table represents the count pattern, which is a unique series of counts along CNV and SNP loci.
For example, the count pattern 1 is 2 1 1 3.
The symbols #, and a and b represent the number of individuals with the count pattern, and the SNP alleles, respectively.
First, from the count pattern 1, the algorithm lists all possible genotypes consistent with the phenotypic copy number at each CNV locus and also lists the SNP genotype at each SNP locus.
Second, the algorithm takes all possible combinations of the listed genotypes along all CNV and SNP loci.
Third, it generates all possible haplotype pairs from each genotype combination.
Finally, it removes redundant haplotype pairs (diplotypes).
This procedure is performed for every count pattern.
Conflict of Interest: none declared.
Summary: The calibrated population resistance (CPR) tool is a web-accessible program for performing standardized genotypic estimation of transmitted HIV-1 drug resistance.
The program is linked to the Stanford HIV drug resistance database and can additionally perform viral genotyping and algorithmic estimation of resistance to specific antiretroviral drugs.
Antiretroviral (ARV) therapy has greatly advanced the management of human immunodeficiency virus type 1 (HIV-1) infection.
When used in combination, ARV drugs targeting the viral reverse transcriptase (RT) and protease (PR) activities can suppress HIV-1 replication to undetectable levels, leading to significant clinical benefit.
However, a number of factors can lead to the emergence of drug-resistant virus strains.
Once drug-resistant strains have emerged, they are archived in resting white blood cells and can rapidly re-emerge if therapeutic regimens using drugs to which they are resistant are restarted.
Expert panels recommend that, where possible, selection of ARV drug regimens should be guided by genotypic screening in which viral drug resistance mutations (DRMs) are identified by population sequencing of the dominant HIV-1 strain in plasma.
A number of software and web resources have been developed to support this procedure (Liu and Shafer, 2006).
Drug-resistant viruses selected by treatment can be transmitted, potentially compromising options for first line therapy in untreated individuals (Kuritzkes et al., 2008).
Surveillance of HIV-1 drug resistance (HIVDR) is therefore crucial to maintain the success of HIV-1 prevention efforts.
However, variations in the methodologies used for surveillance of transmitted HIVDR, such as the specific DRMs taken as indicating transmitted resistance, have so far limited the potential to draw general conclusions from these studies.
There is a widely recognized requirement for standardized protocols in this area, so that trends in HIVDR can be investigated through comparison between studies performed in distinct geographic regions and over time (Pillay, 2004; van de Vijver et al., 2007).
To whom correspondence should be addressed.
We recently published a list of standard surveillance DRMs (SDRMs), endorsed by the World Health Organization (WHO) for epidemiological surveillance of transmitted HIVDR (Bennett et al., 2008b; Shafer et al., 2007, 2008).
Here we describe an online program, the calibrated population resistance (CPR) tool, providing a standardized framework for estimating transmitted HIVDR from population-sampled HIV-1 PR and RT sequence sets.
2 FUNCTIONALITY The CPR program accepts FASTA-formatted HIV-1 PR and/or RT sequence data.
Options to carry out genotyping (subtyping) and to estimate genotypic resistance to specific ARV drugs are provided.
A profile alignment of the submitted sequence set is created by aligning each nucleotide sequence to a polypeptide reference sequence for the region of the HIV-1 genome encoding PR and RT (by default, a subtype B consensus sequence, available from http://hivdb.stanford.edu/).
Mutations, deletions and insertions (defined as changes relative to the reference sequence) are recorded for each submitted sequence.
The prevalence of individual mutations is calculated by dividing mutation frequency by the number of valid codons at the corresponding position in the alignment.
CPR implements a standard approach to handling contingencies such as missing data (i.e.incomplete sequences) and the nucleotide ambiguities common in HIV-1 sequence data obtained through population sequencing of viral RNA.
These procedures are described in the program release notes.
A list of DRMs (by default the most recent version of the SDRM list) is used to compute the prevalence of resistance to each of the three main classes of ARV drug: protease inhibitors (PIs), nucleoside RT inhibitors (NRTIs) and non-nucleoside RT inhibitors (NNRTIs).
The prevalence of transmitted HIVDR to each drug class is estimated as the number of sequences containing any DRM specific to that drug class relative to the number of times the target gene is represented in the alignment.
Analysis generates a report that summarizes the input dataset in terms of drug resistance, genetic diversity and sequence quality.
The CPR report includes a graphical overview of DRMs and resistance-associated mutations present in the input dataset, and a plot showing coverage across the target region (i.e.the PR and RT genes).
Once the report is generated the submitted sequences are deleted.
The HIV drug resistance database (HIVDB) is used to develop a list of PR and RT amino acid variants that have been observed at a prevalence of 0.1% in a database containing sequences from about 20 000 individuals (Rhee et al., 2003).
Atypical mutations that have been reported less frequently and that are not known drug-resistant variants are highlighted in the report.
APOBEC3Gmediated sequence editing [see, Holmes et al.(2007) for review] is detected using a subset of atypical mutations that typically occur in edited sequences (Gifford et al., 2008).
Other sequence quality indicators, such as stop codons and frameshifts are also identified and listed in a quality analysis section of the report.
A number of mutations have been described that are marginal with respect to their inclusion on the SDRM list, and an option is provided to highlight these borderline/suspicious mutations in the report in addition to SDRMs.
The CPR report shows the prevalence of individual mutations in the query dataset alongside their corresponding prevalence (stratified by subtype) in sequences from untreated patients in HIVDB.
This allows investigators to rapidly identify sequence polymorphisms that are disproportionally represented in query datasets, and to discriminate between subtypespecific polymorphisms, sequence quality problems and mutational markers of prior-drug selection pressure.
Mutation lists used within the program are standardized and version-tracked, as it is expected that changes may occur as new information about drug resistance and viral polymorphism becomes available.
The CPR tool is written in PERL and can readily be installed on computers running UNIX or LINUX operating systems.
Alignments are constructed using LAP (Huang and Zhang, 1996).
Viral subtypes are assigned using STAR (Myers et al., 2005).
Genotypic estimation of resistance is performed using the Stanford SIERRA web service.
3 DISCUSSION The CPR tool aims to promote consistency between epidemiological studies by providing investigators worldwide with ready access to a simple, standard protocol for genotypic estimation of transmitted HIVDR.
Because the CPR tool is closely linked to HIVDB, it allows investigators to leverage the power of large quantities of published HIV-1 sequence data within their analyses.
Additionally, by standardizing protocols for genotypic estimation of transmitted HIVDR, the CPR program can facilitate comparison between sequence datasets that cannot be shared due to legal or proprietary constraints.
These include datasets collated by some of the largest national and international surveillance programs (Little et al., 2002; SPREAD programme, 2008; UK Collaborative Group on HIV Drug Resistance, 2007; Yerly et al., 2007).
In regions of the world with minimal health infrastructure and large numbers of HIV-1 infected individuals, management of ART is necessarily based on simplified, standard treatment protocols (Bennett et al., 2008a; Gilks et al., 2006).
The WHO has developed a minimum-resource approach for surveillance of transmitted HIVDR to accompany the expansion of access to ART in these regions, based on routine genotypic screening in a representative subset of the HIV-infected, untreated population (Bennett et al., 2008b).
Due to resource constraints, the number of individuals surveyed is likely to be small (47), and surveillance is likely to rely partly on archived and convenience samples (Bertagnolio et al., 2007).
Since the threshold for implementing changes in ART policy is low [>5% prevalence of transmitted HIVDR (Bennett et al., 2008b)] it is crucial that protocols deal accurately and consistently with the contingencies of sequence analysis.
The quality control measures implemented in CPR will help investigators to identify artifacts in sequence datasets collected for surveillance purposes [e.g.spurious drug resistance mutations introduced by APOBECmediated sequence editing (Gifford et al., 2008)] so that expensive and unnecessary changes in health policy may be avoided.
Although designed specifically for surveillance of HIVDR, we propose that the framework implemented in the CPR program represents a prototype for other areas of molecular epidemiology in particular studies of microbial drug resistancein which the primary unit of analysis is a population-sampled set of sequences.
Funding: National Institute of Allergy and Infectious Diseases (AI068581).
Conflict of Interest: none declared.
Motivation: Identification of genes coding for ribosomal RNA (rRNA) is considered an important goal in the analysis of data from metagenomics projects.
Here, we report the development of a software program designed for the identification of rRNA genes from metagenomic fragments based on hidden Markov models (HMMs).
This program provides rRNA gene predictions with high sensitivity and specificity on artificially fragmented genomic DNAs.
Availability: Supplementary files, scripts and sample data are available at http://tools.camera.calit2.net/camera/meta_rna.
Contact: liwz@sdsc.edu Supplementary information: Supplementary Data are available at Bioinformatics online.
1 INTRODUCTION The emerging field of metagenomics promises a more comprehensive and complete understanding of the microbial world.
Many projects have been reported with metagenomic approaches to study microbes and microbial communities that live in many different environmental conditions (Tringe and Rubin, 2005).
Analyzing the sequence data generated by these projects is far from easy and requires accessible and user-friendly tools (Raes et al., 2007).
An essential step in any metagenomics project is the identification of genes encoding for ribosomal RNAs (rRNAs), which are widely used for phylogenetic analysis and quantification of microbial diversity.
Several methods haven been proposed for predicting non-coding RNA genes (Meyer, 2007), but a recent benchmark study by Freyhult et al.(2007) indicated that the most commonly used methods yield less than encouraging results.
Lagesen et al.(2007) proposed RNAmmer, a program based on hidden Markov models (HMMs) for annotation of rRNA genes.
Their algorithm predicts rRNAs in complete genomics sequences with high accuracy.
However, a major concern for their predictions is the inability to deal with fragments of rRNAs.
Compared with assembled genomic sequences from single species, the raw sequence reads from a typical metagenomic study often remain unassembled due to insufficient coverage.
For a typical metagenome dataset, the length of sequence read is 100450 bp using 454 pyrosequencing, or 700 bp long if using Sanger sequencing.
Meanwhile, the full lengths of most of 16S and 23S rRNAs are >1200 bp.
Therefore, most of rRNA To whom correspondence should be addressed.
genes in metagenomic sequencing reads are fragmentary, and will be overlooked by RNAmmer that focus on full length rRNAs.
To overcome this limitation, we used HMMs that can discover incomplete rRNA gene fragments for predictions.
In this article, we apply our algorithm on simulated sets of sequence reads of various lengths.
Our method provides rRNA predictions with high-sensitivities and specificities on the benchmark dataset.
As an important molecular machine in all living organisms, the ribosome can be broken down into two subunits, the small and the large subunit.
In prokaryotes, the large subunit of the ribosome contains 5S and 23S rRNAs, while the small subunit contains 16S rRNAs.
Therefore, we will try to build predictors for 5S, 16S and 23S rRNAs.
To obtain a reliable multiple sequence alignment (MSA) for HMM building, we retrieved MSAs of 5S rRNAs from the 5S Ribosomal Database (Szymanski et al., 2002), and MSAs of 16S and 23S rRNAs from the European rRNA database (Wuyts et al., 2004).
These databases provide high-quality alignment that combine sequence and structural information.
The MSAs were then divided into bacterial and archaeal domains.
All sequences with more than five ambiguous nucleotides in either end were removed from the alignment, and then sequences were further clustered at 98% identity threshold to reduce bias.
We then used software package HMMER (Eddy, 1998) version 2.3.2 to create HMMs from these alignments.
We used fs mode in HMMER package for HMM building instead of ls mode implemented in RNAmmer.
In HMMER package, ls mode is suitable for identification of a complete sequence domain, while fs mode is capable of finding domain fragments and maybe useful to detect incomplete rRNA genes.
In addition, domain information for sequences is not available in metagenomic projects, so HMMs from bacterial and archaeal rRNA alignments were both used to search input sequences.
Each sequence was classified to the domain that reported the most significant E-value, and results obtained from corresponding HMMs were used as final result.
3 EVALUATION Performance of our rRNA prediction algorithm was evaluated using artificial DNA fragments generated from fully sequenced archaea and bacteria genomes.
GenBank files for all fully sequenced genomes were retrieved from the ENTREZ Genome Project 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:17 8/4/2009 Bioinformatics-btp161.tex] Page: 1339 13381340 Identification of ribosomal RNA genes Table 1.
Sensitivities are represented in percentage (%).
(downloaded on September 30, 2008).
To reduce the impact of sequence redundancy, we removed species related to training set (see Supplementary Tables for remaining species used for evaluation).
To simulate the current sequencing techniques, fragments of the lengths 100800 bp (in intervals of 100 bp) were randomly sampled from each genome to 1 genome coverage for each length.
These fragments were used to investigate prediction performance of both our method and RNAmmer, they were also analyzed by BLASTN against 5S Ribosomal Database and SILVA database (Pruesse et al., 2007) to identify rRNA genes (with E-value of 105 or less).
In current analysis, sampling of fragments was done without considering the sequencing errors, therefore estimated performances are optimistic.
The annotation information of rRNA genes was also retrieved from GenBank files.
Sequence fragments that had an overlap (>40 nt) with a known rRNA gene in the same strand were considered as a positive sample.
The ratios of true-positives relative to all annotated fragments (sensitivity) and to all predicted fragments (specificity) were used as a performance measure.
Both exactly matching predictions and partially matching predictions with correct strand were counted as true-positives.
Tables 1 and 2 show the prediction sensitivities and specificities for all fragment lengths.
The result for RNAmmer is shown in Supplementary Table S5.
The sequence length of most 16S and 23S rRNA genes substantially exceeds 800 bp, therefore can not be detected by a full domain model like RNAmmer.
It can be shown that our algorithm can predict sequence reads with rRNAs with a high sensitivity and specificity (>90% in almost all configurations).
More important, the prediction performance does not vary much on different read lengths.
One commonly used method for predicting rRNAs in metagenomic projects is based on BLAST (Altschul et al., 1997, Frias-Lopez et al., 2008).
However, Lagesen et al.(2007) indicated that results based on BLAST can be problematic due to its inconsistency.
Compared with BLASTN, our algorithm achieves much better sensitivities (average 10.2% improvement) while the specificities are around 2.3% less for 5S RNA.
The performances for 23S rRNA are almost the same for our algorithm and BLASTN.
The biggest improvement comes from 16S rRNA prediction, it demonstrates that our algorithm improves the specificities significantly and keeps the sensitivities slightly better.
The average running time of our algorithm was 744 ms per 800 bp read, and 145 ms per 200 bp read for a single 2.33G Xeon CPU.
The running time for BLASTN was 239 ms per 800 bp read, and Table 2.
Specificities are represented in percentage (%).
Additional analyses were performed on Sargasso Sea metagenomic project (Venter et al., 2004) consisted of 811 372 entries totaling over 800 Mbp.
On this set the search speed was 1088 s per Mbp, and our algorithm identified 660 5S, 1337 16S and 2300 23S rRNA genes or fragments of genes.
4 CONCLUSION With the continued growth of metagenomic sequencing projects, identification of rRNA genes within sequence fragments from these projects continues to be a very important task.
Here, we reported a HMM based algorithm to detect rRNA genes in short metagenomic fragments with high accuracies.
Our algorithm is written in Python, and runs well on Linux/Unix and Windows XP systems with the installation of Python and HMMER package.
The scripts, sample dataset and usage instruction are available online at http://tools.camera.calit2.net/camera/meta_rna as a downloadable application.
Funding: Gordon and Betty Moore Foundation (CAMERA project,Conflict of Interest: none declared.
Motivation: Promoter prediction is an important task in genome annotation projects, and during the past years many new promoter prediction programs (PPPs) have emerged.
However, many of these programs are compared inadequately to other programs.
In most cases, only a small portion of the genome is used to evaluate the program, which is not a realistic setting for whole genome annotation projects.
In addition, a common evaluation design to properly compare PPPs is still lacking.
Results: We present a large-scale benchmarking study of 17 stateof-the-art PPPs.
A multi-faceted evaluation strategy is proposed that can be used as a gold standard for promoter prediction evaluation, allowing authors of promoter prediction software to compare their method to existing methods in a proper way.
This evaluation strategy is subsequently used to compare the chosen promoter predictors, and an in-depth analysis on predictive performance, promoter class specificity, overlap between predictors and positional bias of the predictions is conducted.
Availability: We provide the implementations of the four protocols, as well as the datasets required to perform the benchmarks to the academic community free of charge on request.
Contact: yves.vandepeer@psb.ugent.be Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Promoter prediction programs (PPPs) aim to identify promoter regions in a genome using computational models.
In early work, promoter prediction focused on identifying the promoter of (proteincoding) genes (Fickett and Hatzigeorgiou, 1997), but more recently it has become clear that transcription initiation does not always result in proteins, and that transcription occurs all over the genome (Carninci et al., 2006; Frith et al., 2008; Sandelin et al., 2007).
One important question is what the different PPPs are actually trying to predict.
Some programs aim to predict the exact location of the promoter region of known protein-coding genes, while others focus on finding the transcription start site (TSS).
Recent research has shown that there is often no single TSS, but rather a whole transcription start region (TSR) containing multiple TSSs that are used at different frequencies (Frith et al., 2008).
This article analyzes the performance of 17 programs on two tasks: (i) genomewide identification of the start of genes and (ii) genome-wide identification of TSRs.
Most PPPs that are published make use of a tailored evaluation protocol that almost always proclaims the new PPP outperforming To whom correspondence should be addressed.
Our aim is provide an objective benchmark that allows us to test and compare PPPs.
In the past few years, a number of papers have evaluated promoter prediction software.
The earliest work indicated that many of the early PPPs predicted too many false positives (FPs) (Fickett and Hatzigeorgiou, 1997).
A later genome-wide review included a completely new set of promoter predictors and introduced an evaluation protocol based on gene annotation (Bajic et al., 2004).
This protocol has later been used to validate promoter predictions for the ENCODE pilot project (Bajic et al., 2006).
Sonnenburg et al.(2006) proposed a more rigorous machine-learning-inspired validation method that uses experimentally determined promoters from DBTSS, a database of promoters.
The most recent large-scale validation of PPPs included more programs than any of the earlier studies and introduced for the first time an evaluation based on all experimentally determined TSSs in the human genome (Abeel et al., 2008a, b).
While many issues have been solved, there is still a large number of challenges that remain open for debate in evaluating the performance of promoter prediction software.
Generally, we can distinguish two main approaches in promoter prediction.
The first approach assigns scores to all single nucleotides to identify TSSs or TSRs.
Usually, the scoring is done with a classification algorithm that is typically validated using cross-validation.
This cross-validation provides a first insight in to the performance of the model and can be used to optimize the model parameters on a training set.
The scores obtained from these techniques can be used as input for a genome annotation pipeline, where they will be aggregated in gene models.
Because of their design, this type of promoter predictors will always work on a genome-wide scale.
Programs using this approach include ARTS (Sonnenburg et al., 2006), ProSOM (Abeel et al., 2008b) and EP3 (Abeel et al., 2008a).
The second approach identifies a promoter region without providing scores for all nucleotides.
Typically, this type of programs will output a start coordinate and a stop coordinate of the promoter, and a score that indicates the confidence in the prediction.
In rare cases, only one coordinate is given as TSS.
For two programs no score is provided (Wu-method and PromoterExplorer).
Within this approach, we can distinguish two subclasses of programs: the ones that work on a genomic scale and the ones that do not.
The latter are used to identify the promoter of a single gene.
In this work we will not consider these programs, because they are usually distributed as a website and are thus not suited for large-scale analyses.
PPPs can be applied to identify the promoter of known genes, or they can be used to identify the start of any transcription event, regardless of what the final fate of the transcribed sequence is.
For each application, we propose two evaluation protocols that can be used to assess the performance of a program for that particular application.
Each application has an associated reference dataset which the protocol will use to evaluate a PPP.
Several methods have been proposed to validate promoter predictions.
Cross-validation on a small set of promoter and nonpromoter sequences is sometimes used to validate a PPP (Xie et al., 2006), but the results are often an overestimation of the performance on a complete genome (Bajic et al., 2004).
Other methods make use of gene annotation to evaluate promoter predictions, based on the rationale that the start of a gene corresponds with a promoter (Bajic et al., 2004, 2006).
However, it is clear that not all promoters are associated with protein-coding genes and, furthermore, not all transcription events start at the beginning of a gene.
TSSs have been observed at the start of internal exons or at the 3 end of a gene (Carninci et al., 2006).
More recently, two large resources for promoter research in the human genome have been used to validate promoter predictions.
The first source is the DBTSS database, containing a large set of experimentally determined promoters (Wakaguri et al., 2008).
The second source is a genome-wide screening of the human genome using the CAGE technique (Shiraki et al., 2003), providing all TSSs in the genome.
The latter source is the most valuable as it is an exhaustive screening for all possible TSSs.
The remainder of this work proposes a set of protocols and datasets to use when validating promoter prediction software.
To illustrate our methods, we analyzed 17 PPPs with the proposed validation schemes.
While the methods are applicable to any genome, we focus in the current article on the human genome.
Finally, we highlight some challenges that arise in selecting the best PPP for a particular task.
2 MATERIALS AND METHODS 2.1 Datasets We used release hg18 of the human genome for all analyses.
For the validation protocols, we use the RefSeq genes downloaded from the UCSC table browser.
This set includes 23 799 unique gene models and is further referred to as the gene set.
We also use the CAGE tag dataset from Carninci et al.(2006).
The latter was preprocessed to aggregate all overlapping tags into clusters, resulting in 180 413 clusters containing a total of 4 874 272 CAGE tags.
A cluster is considered to be a TSR if it contains at least two tags.
Singleton clusters are removed as these could be transcriptional noise.
This dataset will be referred to as the CAGE dataset.
2.2 Promoter prediction software We used two criteria to select the PPPs to include in this analysis: (i) the program or predictions should be available without charge for academic use, and (ii) the program should be able to process the complete human genome or predictions should be available for the complete genome.
At least 17 programs (Table 1) fulfilled these criteria and have been included.
Details for settings and prediction extraction methods for each program are included in the Supplementary Material.
2.3 Evaluation protocols In this article, we propose four protocols to evaluate the quality of predictions made by PPPs.
The first two protocols are bin-based protocols, inspired by Sonnenburg et al.(2006).
The latter two are distance based, inspired by Abeel et al.(2008b).
Figure 1 shows a schematic overview of how each protocol determines the prediction performance.
For the explanation of each protocol we assume that we have a set of predictions.
Furthermore, we have a reference set (the gene set or the Table 1.
Overview of all the programs analyzed Name
Summary: Epigenetics, the study of heritable somatic phenotypic changes not related to DNA sequence, has emerged as a critical component of the landscape of gene regulation.
The epigenetic layers, such as DNA methylation, histone modifications and nuclear architecture are now being extensively studied in many cell types and disease settings.
Few software tools exist to summarize and interpret these datasets.
We have created a toolbox of procedures to interrogate and visualize epigenomic data (both array-and sequencing-based) and make available a software package for the cross-platform R language.
Availability: The package is freely available under LGPL from the R-Forge web site (http://repitools.r-forge.r-project.org/) Contact: mrobinson@wehi.edu.au Received on March 11, 2010; revised on April 14, 2010; accepted on May 5, 2010 1 INTRODUCTION Epigenetics is the study of the phenotypic changes unrelated to DNA sequence.
Epigenomics is the large-scale study of epigenetics, with various genome-wide assays having been introduced in the past few years and with many epigenome mapping projects on the horizon (Jones et al., 2008; Nature editorial, 2010).
DNA methylation is one of the best studied epigenetic marks and can be assayed genomewide using restriction enzyme, bisulphite or enrichment-based approaches (reviewed in Laird, 2010).
Another significant class of epigenetic regulators is histone modifications, typically studied using chromatin immunoprecipitation (ChIP) in combination with microarrays (ChIP-chip) or next-generation sequencing (ChIP-seq).
There are limited general tools available for the exploratory analysis and summarization of enrichment-based epigenomics data (see Table 3 of Laird, 2010).
We present Repitools, a software package for the R environment that is focused on the analysis of enrichment-based epigenomic data.
Examples are shown to illustrate the diversity of tools within the package; many further examples can be found in the comprehensive users guide.
The routines have been tested on Affymetrix and Nimblegen tiling microarrays and Illumina Genome Analyzer sequencing data; generic data types are used so that other platforms can be easily supported.
To whom correspondence should be addressed.
2 DATA SUMMARIZATION Various procedures for visualization are available within the package.
For example, enrichmentPlot displays the distribution of enrichment across the whole genome for sequencingbased experiments.
cpgBoxplots and cpgDensityPlot display microarray and sequencing results, respectively, for quality assessment of DNA methylation enrichment experiments.
20 Distribution of CpG Site Density Density of CpG Sites within reads F re qu en cy Methylated DNA enrichment Nonenriched Input DNA M A T S co re A B C Fig.1.
(A) In cpgDensityPlot, each line is a single experiments read distribution in terms of CpG density.
(B) For binPlots, the middle panel displays a heatmap of summarized signal according to 50 expression level bins (rows), organized into 100 bp locations (columns) within promoters.
The left panel gives the enrichment colour scale and the right panel displays the gene expression for each bin.
(C) For significancePlots, the purple and red lines illustrate the median signal for the gene sets of interest.
The blue line represents median signal of all remaining genes in the genome, while the blue shading illustrates a 95% confidence interval (example data taken from Coolen et al., 2010).
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:23 3/6/2010 Bioinformatics-btq247.tex] Page: 1663 16621663 Repitools CA, USA) where, as expected, the CpG density of the enriched DNA population is heavily skewed to the right compared to the input DNA control.
We have provided many ways to visualize and summarize promoter-level microarray or genome-wide epigenomic data.
For example, given a table of annotation, the binPlots function summarizes median signal across points of interest (e.g.transcription start sites).
We routinely use binPlots as a quality control step of new ChIP experiments where there is a previously known relationship between the interrogated chromatin mark and another metric, commonly gene expression.
For example, Figure 1B clearly illustrates the positive association between gene expression levels (Affymetrix Gene 1.0 ST data) and the occurrence of H3K9 acetylation in the proximity of the corresponding promoters (Affymetrix Promoter 1.0R data).
The routine handles tiling array or sequencing data as inputs, can accept alternative rankings for grouping and the display can be a plot with multiple lines, a heatmap or a 3D visualization.
Another useful strategy for summarizing sets of genes of interest is significancePlots.
As illustrated in Figure 1C, significancePlots shows the distinct methylated DNA enrichment changes associated with genes whose expression is up-or down-regulated >2-fold between two samples, and how the profiles differ between array and high-throughput sequencing readout.
For the comparison, a large number of random gene sets are taken to form the profile null distribution; median and confidence intervals are plotted.
These plots show evidence that there is a clear enrichment of sequencing reads and hence, DNA methylation surrounding many genes are down-regulated in this comparison.
Further data summaries are regularly added.
3 STATISTICAL PROCEDURES The visualization procedures detailed above aggregate signal over a large number of promoters or regions of the genome.
Often, it is of interest to focus on specific regions of the genome and summarize the signal observed at these regions (e.g.transcription start sites, exons, etc.).
For example, an experimenter may be interested in promoterlevel summaries of a particular epigenetic mark.
The general purpose blocksStats procedure focuses on data for the specified genomic regions of interest.
For microarray data, this involves the calculation of a probe-level score and applying a statistical test to the groups of probes within a specified distance from the region of interest.
For sequencing data, we calculate statistics on aggregated read counts around the features of interest.
Further details are available in the accompanying users guide.
We also have procedures for untargeted analysis of epigenomic tiling array data.
The regionStats function searches for a persistent change in signal in an untargeted fashion, similar in principle to model-based analysis of tiling arrays (Johnson et al., 2006), and therefore not relying upon annotation.
Analogous procedures for sequencing data are in development.
4 ACCESSORY TOOLS The package contains a number of useful tools in the spectrum of epigenomics.
For example, in the context of CpG methylation, microarray probes or sequence reads are often affected by the local CpG density of the regions being interrogated.
cpgDensityCalc is a procedure to calculate local CpG density according to a previous definition (Pelizzola et al., 2008).
annotationLookup provides a framework for relating annotation (e.g.transcription start sites) information to probe positions on a tiling array.
multiHeatmap is a general tool for creating adjacent heatmaps using separate colour scales.Additional included tools exist to access Nimblegen array quickly (e.g.readPairFile), access features of aroma.affymetrix objects (e.g.getProbePositionsDf) and aggregate sequencing reads according to proximity to annotation (e.g.annotationCounts).
We expect further tools to be added and encourage others in the epigenomic community to contribute generally useful procedures.
5 DISCUSSION There are relatively few tools currently available for the analysis of epigenomic data.
We have developed Repitools, a software package for the R environment; it contains many useful functions for quality assessment, visualization, summarization and statistical analysis of epigenomics experiments.
The package makes use of aroma.affymetrix and several Bioconductor packages for various preprocessing steps (Bengtsson et al., 2008; Gentleman et al., 2004) and may require an intermediate understanding of R for some features.
A comprehensive user manual is available and examples can be run using supplied data.
The analysis of large Affymetrix tiling array datasets is facilitated through the memory efficiency afforded by the aroma.affymetrix package (Bengtsson et al., 2008).
Summary: New software tools for graphical genotyping are required that can routinely handle the large data volumes generated by the high-throughput single-nucleotide polymorphism (SNP) platforms, genotyping-by-sequencing and other comparable genotyping technologies.
Flapjack has been developed to facilitate analysis of these data, providing real time rendering with rapid navigation and comparisons between lines, markers and chromosomes, with visualization, sorting and querying based on associated data, such as phenotypes, quantitative trait loci or other mappable features.
Availability: Flapjack is freely available for Microsoft Windows, Mac OS X, Linux and Solaris, and can be downloaded fromContact: flapjack@scri.ac.uk Received on September 14, 2010; revised on October 8, 2010; accepted on October 8, 2010 1 INTRODUCTION The concept of a graphical genotype to visualize haplotype diversity between chromosomes has been widely adopted since Young and Tanksley (1989) used it in the context of restriction fragment length polymorphism (RFLP) mapping populations.
Existing software tools to display graphical genotypes include GGT (van Berloo, 2008) and Geneflow (geneflowinc.com).
The advent of new highthroughput genotyping technologies have given a renewed stimulus to the concept of graphical genotyping, through a combination of dramatic reduction in cost per data point and vastly increased marker density and throughput.
The resultant high-density data underpin new genetic approaches such as genome-wide association analysis (Rostoks et al., 2006).
It also leads to the possibility of visually comparing many lines (e.g.samples or individuals) or sorting and selecting based on phenotype, identified groupings and genome features, such as quantitative trait loci (QTL) or gene models mapped to the genetic or physical genome.
However, the ability to generate datasets with many thousands of markers (McMullen et al., 2009) on many thousands of lines imposes a significant demand on both software tools and the underlying computer hardware.
Flapjack provides a high performance visual interface into graphical genotyping applications in genetics and plant breeding.
To whom correspondence should be addressed.
2 FEATURES Flapjacks main display (Fig.1) consists of a genotype rendering canvas that shows the data for a given chromosome.
The alleles are plotted as a grid, with lines/germplasm running horizontally across the screen and markers/loci running vertically.
The line names are shown in a list to the left, and across the top we provide a graphical view of the positions of the markers on the currently selected chromosome (from either physical or genetic maps).
Several alternative map displays are provided, including a global view that shows where the currently visible markers are located on the chromosome, and a local view, that scales and optimizes the map to concentrate only on the region containing the currently visible on-screen markers.
Hovering the mouse over an allele highlights not only the data under the point but also the name of the line in the lines list, the position on the chromosome display, and graphically displays the entire dataset for the line and marker at that position.
Flapjack provides several customizable colour schemes for data display, and will attempt to auto-select a suitable scheme based on the type of data loaded.
The schemes include a four-colour nucleotide model (homozygous genotypes get a single colour; heterozygous genotypes are split diagonally); similarity models, that use one colour for every allele of a reference line or marker, and a second colour for any data that differs from the reference; and a model that performs frequency-based colouring that can be used to highlight rare alleles and haplotypes on a per-marker basis.
Random colour schemes also exist that are applicable to datasets with a large number of possible values per allele position, such as SSR data.
Flapjacks subtle use of colours and gradients allows for pattern recognition and structure to still be seen, even at the highest levels of overview.
Once a project is in use, additional data types can be imported and visualized alongside the main display.
Information on phenotypic traitsboth numerical and categorical (per line)is displayed as a heat map running alongside the lines.
This can also be used to reorder the lines, for example, by yield or flowering date.
QTL aligned against chromosome map positions may be visualized at the top of the screen, using a novel method of packing and displaying the features across a custom number of tracks, with the number controlled by a slider.
A user interacts with Flapjack using one of three modes: navigation mode, marker mode or line mode; with the latter two options enabling support for object highlighting and selection.
This provides a graphical means of filtering the data, for example, to reorder the lines based upon their similarity across a specific subset The Author(s) 2010.
Published by Oxford University Press.
Flapjacks main interface, showing SNP genotypes, QTL and a trait-data heat map (additional screenshots can be seen online atof markers or to export sections of data into their own custom view.
Selections can be made either manually or by markers under a QTL.
Flapjack allows the user to create any number of these custom views, each containing its own set of lines, markers, ordering, colour schemes, bookmarked locations and so on.
The data for a given vieweither in graphical or in its underlying raw formatcan be exported back to disk.
Images can be produced and saved in PNG format for the current view, or the on-screen subsections of a view, and the user can select whether to include all components (allele data, chromosome maps, line names, traits, etc.)
or pick and choose only the ones of interest.
When exporting the underlying data, similar options are available to export the entire dataset or to only include data from specific chromosomes or the currently selected lines and markers.
The data are saved in tabdelimited plain text files identical in format to the files original imported into Flapjack.
Although completely standalone, with data imported via simple plain-text formats, integration with external data sources such as Germinate (bioinf.scri.ac.uk/germinate) is also possible.
This provides easy selection and export of data directly into Flapjack, along with web-links back to the line and marker data in the original database.
This feature has been designed to work with any external data source, by means of supplying Flapjack with a custom URL that can be queried with key/value pairs.
Flapjack projects are persistent, with all data, views, user selections and so on being saved to either an XML-based file or an experimental binary format more suited to very large datasets.
The XML and text formats are documented on our web site, and are also currently supported by iMAS (icrisat.org/bt-biomatrics-imas.htm), QU-GENE (Podlich and Cooper, 1998), Gramene (Liang et al., 2008), Genstat (vsni.co.uk/software/genstat) and The Hordeum Toolbox (hordeumtoolbox.org).
Projects can also be created using a command-line utility, which provides a convenient integration with custom analysis pipelines and databases.
3 IMPLEMENTATION Flapjack is written in Java and is compatible with any system running Java 1.6 or higher.
For convenience, we provide installable versions with everything required to run the application, including a suitable Java run-time.
These are available for Windows, Mac OS X, Linux and Solaris.
Flapjack regularly monitors our server for new versions and will prompt, download and update quickly and easily when a new release is available.
The code is internationalized and is distributed with translations in English (UK/US) and German.
The code can take advantage of multicore processors, a feature especially significant for the rendering code, whichamong its other optimizationsis capable of simultaneous rendering across all cores, greatly improving the end-user experience when navigating around large or complex datasets.
We have designed Flapjack to be very memory efficient, and are confident that it can comfortably handle datasets with hundreds of millions of alleles even on a machine with just 1 GB of main memory.
4 FUTURE WORK Future development with Flapjack will entail enhancing its visualizations to provide better support for very small datasets, primarily by enabling the display of all markers across the genome in a single view.
We want to extend support for rendering features beyond QTL to include more generic features, such as gene models for SNP data anchored to physical maps, and to provide a graph track to display summary information such as PIC values or test statistics.
We are also working with academic and breeding company partners to explore supporting additional data formats such as HapMap and PLINK, and on closer integration with Germinate, by allowing its databases to be automatically populated by the data imported into Flapjack.
ACKNOWLEDGEMENTS We would like to thank colleagues within the Genetics and BioSS Programmes at SCRI for their input to this project.
Funding: Scottish Government (RERAD, Programme 1); Scottish Funding Council; Scottish Enterprise through the Scottish Bioinformatics Research Network (SBRN) project.
Conflict of Interest: none declared.
Summary: A MapReduce-based implementation called MRMSPolygraph for parallelizing peptide identification from mass spectrometry data is presented.
The underlying serial method, MSPolygraph, uses a novel hybrid approach to match an experimental spectrum against a combination of a protein sequence database and a spectral library.
Our MapReduce implementation can run on any Hadoop cluster environment.
Experimental results demonstrate that, relative to the serial version, MR-MSPolygraph reduces the time to solution from weeks to hours, for processing tens of thousands of experimental spectra.
Speedup and other related performance studies are also reported on a 400-core Hadoop cluster using spectral datasets from environmental microbial communities as inputs.
Identifying the sequence composition of peptides is of fundamental importance to systems biology research.
High-throughput proteomic technologies using mass spectroscopy are capable of generating millions of peptide mass spectra in a matter of days.
One of the most effective ways to annotate these spectra is to compare the experimental spectra against a database of known protein sequences.
The main idea here is to generate candidate peptide sequences from the genome of the organism under study and then to use models of peptide fragmentation to generate model spectra that can be compared against each experimental spectrum.
However, as samples become richer in diversity (e.g.from environmental microbial communities), the number of candidate comparisons could increase by orders of magnitude (Supplementary Figure S1).
An increase in the number of candidates also increases the probability of finding high-scoring, random matches.
It is therefore essential to implement a peptide identification method that is both accurate and scalable to large sizes of spectral collections and sequence databases.
The prediction accuracy of peptide identification can be improved To whom correspondence should be addressed.
if experimental spectra are also compared against spectral libraries, although this would only exacerbate the computational demands.
Recently, Cannon et al.(2011) developed a novel hybrid statistical method within the MSPolygraph framework, which combines the use of highly accurate spectral libraries, when available, along with on-the-fly generation of model spectra when spectral libraries are not available.
This method demonstrated increases of 57147% in the number of confidently identified peptides at controlled false discovery rates.
This effort to enrich quality of prediction, however, comes at an increased computational cost.
While a parallel MPI version of the code exists, most users do not have access to large-scale parallel clusters.
Whereas, open-source science cloud installations and commercial vendors such as Amazon provide access to MapReduce clusters on an on-demand basis.
In this article, we present a MapReduce implementation of MSPolygraph called MR-MSPolygraph.
MapReduce (Dean and Ghemawat, 2008) is an emerging parallel paradigm for data intensive applications, and is becoming a de facto standard in cloud installations.
One of the popular open-source implementations for MapReduce is the Hadoop framework.
MR-MSPolygraph uses MapReduce to efficiently distribute the matching of a large spectral collection on a Hadoop cluster.
Previously, Halligan et al.(2009) ported peptide identifications tools that use the database search approach onto the Amazon EC2 cloud environment.
Our work incorporates the statistics of the hybrid search method in MSPolygraph to any cluster running the open-source Hadoop environment.
2 METHODS MR-MSPolygraph is designed to achieve parallelism across the number of experimental spectra to be matched.
The MapReduce framework requires developers to define two functions: mapper and reducer.
In our case, since the processing of each spectrum is independent of one another, we take advantage of the inherent data parallelism by splitting the input experimental spectra across map tasks.
More specifically, the user inputs: (i) (queries) a set of experimental spectra to be matched; (ii) (database) a fasta file containing known protein/peptide sequences; (iii) (spectral library) a set of peptides to be used as the spectral library (required only when the software is run in the hybrid mode); and (iv) a file with quality control and output parameters.
In addition, the user specifies a desired number of map tasks.
The algorithm executes as follows: first, the queries are automatically partitioned into roughly equal sized chunks and supplied as input to each map task.
The chunk size can be controlled either by altering the number of map tasks and/or the min.split.size parameter within Hadoop.
Each map task then runs a modified implementation of the serial MSPolygraph code, which matches the The Author(s) 2011.
Published by Oxford University Press.
Performance of MR-MSPolygraph: (a) Runtime as a function of the input number of spectra, keeping the number of map tasks fixed at 400; and (b) speedup of the hybrid version relative to 100 map tasks, for varying input sizes.
The number of map tasks is generally equal to the number of cores used, although that could slightly vary as determined by Hadoop at runtime.
local batch of queries against the entire database, and also against the spectral library if run on the hybrid mode.
The map tasks then output, in one file per task, a list of hits (sorted by statistical significance) for each of their queries.
The algorithm has a worst-case complexity of O(q(n1 +n2)/p), where q is the number of experimental spectra, p is the number of mappers and n1 and n2 are the sizes of the database and spectral library, respectively.
Since the mappers output cover different subsets of queries, the reducer functionality is not used.
However, if it is desired to have all the hits reported in one output file, then it can be achieved using a single reducer.
More usage details and parameter descriptions can be found at the software web site.
3 RESULTS MR-MSPolygraph was tested on the Magellan Hadoop cluster at National Energy Research Scientific Computing Center (NERSC).
The cluster has 75 nodes with a total of 600 cores dedicated for Hadoop, where each node has 2 quad cores Intel Nehalem 2.67 GHz processors and 24 GB DDR3 1333 MHz RAM.
These nodes run Clouderas distribution for Hadoop 0.20.2 + 228.
In our experiments, we used the following datasets: (i) a collection of 64 000 experimental spectra obtained from Synechococcus sp.
PCC 7002; (ii) a database containing 2.65 million microbial protein sequences downloaded from NCBI GenBank; and (iii) a spectral library containing a set of 1752 S.Oneidensis MR-1 spectra.
Figure 1a shows the runtime of MR-MSPolygraph as a function of input number of spectra (from 1K to 64K).
Both modes of the software, hybrid and database only, were tested.
As expected, the runtime grows linearly with the input number of spectra.
Furthermore, both the hybrid and database-only versions take almost identical times, indicating that the additional cost of matching against the spectral library is negligible for this input.
It can be expected that this cost grows gradually with the size of spectral library used.
We also studied the performance by measuring the parallel runtime as a function of the number of map tasks used.
Supplementary Table S1 shows the runtimes and Figure 1b shows the corresponding speedup up to 400 map tasks, calculated relative to the corresponding 100 mapper run.
As can be observed, the runtime roughly halves with doubling of the number of map tasks and the speedup becomes linear for larger inputs (e.g.398 on 400 map tasks for 64K spectra).
This can be expected as for smaller inputs; the overhead of loading the database and spectral library is likely to dominate in larger processor sizes.
Perhaps the merits of Hadoop parallelism become more evident upon comparing its performance against a serial implementation.
For instance, to match the entire collection of 64 000 spectra in hybrid mode, the MSPolygraphs serial implementation can be estimated to take >2000 CPU hours using a state-of-the-art desktop computer; whereas, our Hadoop implementation finishes this task in 6 h using 400 cores.
We also studied the effect of changing task granularity for each map task and the results are summarized under Supplementary Material.
ACKNOWLEDGEMENTS We thank Dr Ramakrishnan at NERSC for offering extensive help with the set up of Hadoop environment.
And, the National Energy Research Scientific Computing Center (NERSC) at Lawrence Berkeley National Laboratory.
Funding: This work was supported by the National Science Foundation (IIS 0916463 to A.K.
and W.R.C.)
and Department of Energys Office of Biological and Environmental Research and Office of Advanced Scientific Computing Research under contracts (57271 and 54976 to W.R.C.).
Conflict of Interest: none declared.
Summary: xQTL workbench is a scalable web platform for the mapping of quantitative trait loci (QTLs) at multiple levels: for example gene expression (eQTL), protein abundance (pQTL), metabolite abundance (mQTL) and phenotype (phQTL) data.
Popular QTL mapping methods for model organism and human populations are accessible via the web user interface.
Large calculations scale easily on to multi-core computers, clusters and Cloud.
All data involved can be uploaded and queried online: markers, genotypes, microarrays, NGS, LC-MS, GC-MS, NMR, etc.
When new data types come available, xQTL workbench is quickly customized using the Molgenis software generator.
Availability: xQTL workbench runs on all common platforms, including Linux, Mac OS X and Windows.
An online demo system, installation guide, tutorials, software and source code are available under the LGPL3 license from http://www.xqtl.org.
Contact: m.a.swertz@rug.nl Received on September 30, 2011; revised on December 19, 2011; accepted on January 20, 2012 1 INTRODUCTION Modern high-throughput technologies generate large amounts of genomic, transcriptomic, proteomic and metabolomic data.
However, existing open source web-based tools for QTL analysis, such as webQTL (Wang et al., 2003) and QTLNetwork (Yang et al., 2008), are not easily extendable to different settings and computationally scalable for whole genome analyses.
xQTL workbench makes it easy to analyse large and complex datasets using state-of-the-art QTL mapping tools and to apply these methods to millions of phenotypes using parallelized Big Data solutions (Trelles et al., 2011).
xQTL workbench also supports storing of raw, intermediate and final result data, and analysis protocols and history for reproducibility and data provenance.
Use of Molgenis (Swertz et al., 2010a) helps to customize the software.
All is conveniently accessible via standard Internet browsers on Windows, Linux or Mac (and using Java, R for the server).
To whom correspondence should be addressed.
The authors wish it to be known that, in their option, the first two authors should be regarded as joint First Authors.
2 FEATURES xQTL workbench provides visualization of QTL profiles, single and multiple QTL mapping methods, easy addition of new QTL analyses, scalable data management and analysis tracking.
2.1 Explore QTL profiles Through the web interface, users can explore mapped QTLs, and underlying information by viewing QTL plots in an interactive scrollable and zoomable window.
xQTL workbench has support for other common image formats, such as PNG, JPG, SVG and embedded postscript; useful for publishing scientific results online, and on paper.
From the output, main database identifiers, such as gene, protein and/or metabolite identifiers are automatically linkedout to matching external web pages of public database such as NCBI, KEGG and Wormbase.
2.2 Single and multiple QTL mapping xQTL workbench wraps R/qtl (Arends et al., 2010; Broman et al., 2003) in a web-based analysis framework offering all important QTL mapping routines, such as the EM algorithm, imputation, Haley-Knott regression, the extended Haley-Knott method, marker regression and Multiple QTL mapping.
In addition, xQTL workbench includes R/qtlbim, a library that provides a Bayesian model selection approach for mapping multiple interacting QTL (Yandell et al., 2007) and Plink, a library for association QTL mapping on single nucleotide polymorphisms (SNP) in natural populations (Purcell et al., 2007).
2.3 Add new analysis tools xQTL workbench supports flexible adding of more QTL analysis software: any R-based, or command-line tool, can be plugged in.
All analysis results are uploaded, stored and tracked in the xQTL workbench database through an R-API.
When new tools are added, they can build on the high-level multi-core computer, cluster and Cloud management functions, based on TORQUE/OpenPBS and BioNode (Prins et al., 2011).
xQTL workbench can be made part of a larger analysis pipeline using interfaces to R, Excel, REST and SOAP web services and Galaxy (Goecks et al., 2010).
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: ES MANUSCRIPT CATEGORY: APPLICATIONS NOTE [12:47 12/3/2012 Bioinformatics-bts049.tex] Page: 1043 10421044 xQTL workbench Fig.1.
Screenshot of xQTL workbench with all features enabled; (1) import phenotype, genotype and genetic map data, examples are given per import type; (2) search through the whole database, explore and browse your data using molgenis generated web-interfaces; (3) run R/qtl QTL mapping, the general plugin allows users to perform not only QTL mapping but also other analyze; (4) use default (or custom) plugins to explore results (e.g.Heatmaps, QTL profiles); (5) add new tools to the workbench (for Bio informaticians); (6) user management and access control of the system (Only for admins); (7) expert settings can be altered in the admin tab (Only for admins); (8) connect/share data using generated APIs to R statistics, REST/JSON, SOAP.
1043 Copyedited by: ES MANUSCRIPT CATEGORY: APPLICATIONS NOTE [12:47 12/3/2012 Bioinformatics-bts049.tex] Page: 1044 10421044 D.Arends et al.2.4 Track analysis and monitor performance When a new analysis protocol or R script is defined, this protocol can easily be applied to new data.
Also, xQTL workbench keeps track of history.
Re-use of analysis protocols can be done in an automated fashion.
Previous analyses can be rerun without resetting parameters.
xQTL workbench provides an online overview of past analyses e.g.which analyses were performed, by who, when and display settings applied.
2.5 Scalable data management xQTL workbench has a consistency checking database based on XGAP specification (Swertz et al., 2010b), user interfaces to manage and query genotype and phenotype datasets and support for various database back-ends including HSQL (standalone) and MySQL.
Phenotype, genotype and genetic map data can be imported as text (TXT), comma separated (CSV) and Excel files.
xQTL workbench handles and stores large data in a new and efficient binary edition of the XGAP format, named XGAPbin (extension.xbin), documented online.
Such binary formats are essential when handling, storing and transporting multi-Gigabyte datasets.
2.6 Customizable to research needs Additional modules for new data modalities can be added using Molgenis software generator (Swertz et al., 2010b).
The look and feelof xQTL workbench is adaptable to institute or consortium style by changing a simple template, which is described in the xQTL workbench documentation enabling seamless integration into an existing website or intranet site, such as recently for EU-PANACEA model organism project and LifeLines biobank.
3 IMPLEMENTATION We built xQTL workbench on top of Molgenis (Swertz et al., 2004), a Java-based software to generate tailored research infrastructure on-demand (Swertz and Jansen, 2007).
From a single blueprint describing the whole system, Molgenis auto-generates a full application including user interface, database infrastructure, application programming interfaces in R, REST and SOAP (APIs).
Molgenis flexibility and robustness is proven by the wide range of research projects, e.g.the Nordic GWAS Control database (Leu et al., 2010), EB mutation database (van den Akker et al., 2011) and the Animal observation database (Swertz et al., 2010a).
For data storage, the eXtensible Genotype and Phenotype (XGAP) data model was adopted (Swertz et al., 2010b) and extended for big data.
To support the increased demand for computational resources for included mapping routines, we added high-level cluster and cloud management functions for computation.
The scalable QTL mapping routines of xQTL workbench are written in R and C. The choice of R ties in with the general practice of using R for QTL mapping.
The user interface includes direct access to the R interpreter.
Both xQTL workbench and Molgenis are open-source software, and source code is transparently stored and tracked in online source control repositories.
4 CONCLUSION xQTL workbench provides a total solution for web-based analysis: major QTL mapping routines are integrated for use by experienced and inexperienced users.
Researchers can upload raw data, run analyses, explore mapped QTL and underlying information, and link-out to important databases.
New algorithms can be flexibly added, immediately available to all users.
Large analyses can be easily executed on a cluster or in the Cloud.
Future work include visualizations and search options to explore the results.
We also had an EU-SYSGENET workshop that envisioned further integration of xQTL with analysis tools like HAPPY, databases like GeneNetwork, and the workflow manager TIQS (Durrant et al., 2011).
ACKNOWLEDGEMENTS We thank Konrad Zych for Figure 1.
Funding: National Institutes of Health (GM074244 to KB); Netherlands Organisation for Scientific Research (NWO)/TTI Green Genetics (1CC029RP to P.P.
Motivation: Ancient DNA (aDNA) molecules in fossilized bones and teeth, coprolites, sediments, mummified specimens and museum collections represent fantastic sources of information for evolutionary biologists, revealing the agents of past epidemics and the dynamics of past populations.
However, the analysis of aDNA generally faces two major issues.
Firstly, sequences consist of a mixture of endogenous and various exogenous backgrounds, mostly microbial.
Secondly, high nucleotide misincorporation rates can be observed as a result of severe post-mortem DNA damage.
Such misincorporation patterns are instrumental to authenticate ancient sequences versus modern contaminants.
We recently developed the user-friendly mapDamage package that identifies such patterns from next-generation sequencing (NGS) sequence datasets.
The absence of formal statistical modeling of the DNA damage process, however, precluded rigorous quantitative comparisons across samples.
Results: Here, we describe mapDamage 2.0 that extends the original features of mapDamage by incorporating a statistical model of DNA damage.
Assuming that damage events depend only on sequencing position and post-mortem deamination, our Bayesian statistical framework provides estimates of four key features of aDNA molecules: the average length of overhangs (), nick frequency () and cytosine deamination rates in both double-stranded regions (d) and overhangs (s).
Our model enables rescaling base quality scores according to their probability of being damaged.
mapDamage 2.0 handles NGS datasets with ease and is compatible with a wide range of DNA library protocols.
Contact: jonsson.hakon@gmail.com Supplementary information: Supplementary data are available at Bioinformatics online.
Received on February 13, 2013; revised on April 16, 2013; accepted on April 18, 2013 1 INTRODUCTION DNA in historical samples is subject to a plethora of environmental conditions and degradation reactions (Sawyer et al., 2012).
Abasic sites, strand breaks, interstrand cross-links and a wide diversity of atypic nucleotidic bases are formed following oxidative and hydrolytic degradation (Lindahl, 1993; Paabo et al., 2004), even in the most favorable preservation conditions.
Post-mortem DNA damage limits our ability to access ancient DNA (aDNA) sequences and increases the risk of exogenous modern contamination, as undamaged DNA molecules are more prone to enzymatic manipulation.
Nucleotide misincorporation patterns, which are mostly driven by deaminated forms of cytosines (uracils), have been suggested as a powerful approach to authenticate aDNA sequences generated on nextgeneration sequencing (NGS) platforms (Briggs et al., 2007) and motivated the creation of the mapDamage package (Ginolhac et al., 2011).
Such patterns could vary according to the specific molecular approach used for constructing (Meyer et al., 2012) and/or amplifying (Ginolhac et al., 2011) second-generation DNA libraries.
For instance, for one of the most popular protocols (Meyer and Kircher, 2010), we observe inflated cytosine deamination rates at 50-overhangs, an increase in C!
T substitution rates toward sequencing starts and complementary increase in G!
A rates toward reads ends (Briggs et al., 2007).
Conversely, a novel procedure targeting single-stranded templates has shown elevated C!
T substitution rates at both ends (Meyer et al., 2012).
Statistical modeling of such patterns has been developed by Briggs et al., 2007 with strand break, overhangs and cytosine deamination as key factors.
Using read alignment to reference genomes and maximum likelihood optimization, this approach has delivered the first quantitative estimates of damage parameters.
However, the likelihood framework originally implemented scales poorly with the size of NGS datasets, and extensive running times have prevented common usage.
Here, we present an extension of mapDamage, which implements a fast approximation of the DNA damage model using a Bayesian framework.
mapDamage 2.0 opens the possibility of comparing DNA damage levels across temporal and environmental gradients.
Posterior distributions of damage parameters also enable penalizing the quality score of likely damaged bases, reducing noise in downstream single-nucleotide polymorphism (SNP) calling procedures.
2 APPROACH Here we build on the DNA damage model described in Briggs et al., 2007.
We make the simplifying assumption that mutations*To whom correspondence should be addressed.
For commercial re-use, please contact journals.permissions@oup.com and post-mortem DNA damage are independent within a fragment, with occurrences depending only on the relative position from the sequence ends.
3 METHODS The general idea is to mutate bases following an Hasegawa, Kishino and Yano (HKY) transition matrix (Hasegawa et al., 1985) and then independently add post-mortem damage on top of mutated bases.
A substitutions either originate from true biological differences or from damage driven misincorporations.
Posterior predictive intervals and empirical frequencies are in general agreement, as shown for the ancient plague dataset (Supplementary Table S2 and Supplementary Figs S4S9) (Schuenemann et al., 2011), demonstrating the adequacy of our method.
We observed a ratio of cytosine deamination rates for double-and singlestranded regions orders of magnitude greater than estimates based on in vitro experiments in aqueous solution (0.007 in Lindahl, 1993 versus 0.0260.070 for Schuenemann et al., 2011 in Supplementary Table S1).
This suggests that tissue-and sample-specific micro-environmental characteristics drive different DNA damage kinetics in situ.
We also found a significant rank correlation between the posterior mean for single-stranded cytosine deamination and sample age (Supplementary Table S3) in agreement with Sawyer et al., 2012.
However, remains of similar age and location showed diverse parameter estimates (Supplementary Table S2), suggesting a prominent role of micro-environmental characteristics over age in diagenesis.
We also applied our quality rescaling scheme to the sequence data of an Australian Aboriginal individual who died in 1920s (Rasmussen et al., 2011).
This increased the overlap of genotype calls to dbSNP v137, suggesting that lower false-positive SNP calls were achieved (Supplementary Table S4).
5 CONCLUSION We have developed a computational method for inferring aDNA damage parameters from NGS sequence datasets, with minimal changes to the DNA damage model presented by Briggs et al., 2007.
Our model is compatible with the specificities of different sequencing and library building protocols.
We believe that downscaling quality scores of likely damaged bases is the first from a long list of possible applications for damage parameter posterior distributions, limiting the impact of nucleotide misincorporations in downstream sequence analyses.
The knowledge of such distributions could also be instrumental for improving mapping procedures to reference genomes (Schubert et al., 2012).
ACKNOWLEDGEMENTS The authors are grateful to EUROTAST (Marie Curie FP7 Initial Training Network), a Marie Curie Career Integration Grant (293845), a Marie Curie Intra-European Fellowship (299176) and Danish Council for Independent Research; Natural Sciences (FNU) for funding.
Johannes Krause, Simon Rasmussen and Stefan Prost for sharing data/scripts.
A schematic view describing the DNA damage Markov chain, which extends the DNA substitution model.
The states Cend and Tend correspond to the final nucleotides in the sequences 1683 mapDamage2.0
