ABSTRACT Motivation: With the rapid development of high-throughput sequencing technologies, the genome-wide profiling of nucleosome positioning has become increasingly affordable.
Many future studies will investigate the dynamic behaviour of nucleosome positioning in cells that have different states or that are exposed to different conditions.
However, a robust method to effectively identify the regions of differential nucleosome positioning (RDNPs) has not been previously available.
Results: We describe a novel computational approach, DiNuP, that compares nucleosome profiles generated by high-throughput sequencing under various conditions.
DiNuP provides a statistical P-value for each identified RDNP based on the difference of read distributions.
DiNuP also empirically estimates the false discovery rate as a cutoff when two samples have different sequencing depths and differentiate reliable RDNPs from the background noise.
Evaluation of DiNuP showed it to be both sensitive and specific for the detection of changes in nucleosome location, occupancy and fuzziness.
RDNPs that were identified using publicly available datasets revealed that nucleosome positioning dynamics are closely related to the epigenetic regulation of transcription.
Availability and implementation: DiNuP is implemented in Python and is freely available at http://www.tongji.edu.cn/zhanglab/DiNuP.
Contact: yzhang@tongji.edu.cn Supplementary Information: Supplementary data are available at Bioinformatics online.
Received on October 5, 2012; revised on April 22, 2012; accepted on May 31, 2012 1 INTRODUCTION As a basic unit of the eukaryotic genome, the nucleosome is formed by an octamer of histones and the surrounding 147 bp of DNA (Kornberg and Lorch, 1999; Luger et al., 1997).
Nucleosomes play an important role in the epigenetic regulation of diverse cellular processes through covalent modifications of histone tails (Heintzman et al., 2007; Liu et al., 2005) and positioning of nucleosomes (Jiang and Pugh, 2009; Li et al., 2007).
Although previous studies have focused primarily on the former mechanism, the relative location of the DNA and the histone octamer, or the nucleosome positioning, is also a determining mechanism To whom correspondence should be addressed.
for epigenetic regulation through controlling the accessibility of transcription factor binding sites (Mellor, 2006; Workman and Kingston, 1998).
In addition, in the process of gene transcription, the frequency of nucleosome unwrapping and formation can reflect the rate of Pol II elongation (Luger, 2006; Schwabish and Struhl, 2004).
As a result, if genome-wide nucleosome profiles for cells exposed to different conditions are known, then researchers can better understand the dynamic behaviours of the transcriptional machinery.
With the rapid development of high-throughput sequencing technologies, genome-wide nucleosome profiles have been generated for several organisms at a single-nucleotide resolution (Kaplan et al., 2009; Mavrich et al., 2008; Schones et al., 2008; Shivaswamy et al., 2008; Valouev et al., 2008, 2011), which provides an opportunity to study nucleosome positioning dynamics in cells that are in different states or that are exposed to different conditions.
To identify regions of differential nucleosome positioning (RDNPs), Shivaswamy et al.
(2008) introduced the concept of a nucleosome score to indicate the stability of the nucleosome position and then compared scores for yeast samples before and after heat shock.
In addition, a fold change calculation can be an intuitive method for identifying regions with sequencing read number changes.
However, those approaches have several limitations.
First, to the best of our knowledge, none of the previous studies provided a statistical measurement, e.g.
a P-value, to evaluate the significance of the difference in the nucleosome positioning changes.
Second, both the fold change calculation and the nucleosome score comparison are sequencing-depth independent, which would affect the robustness of the results, especially when the sequencing depth is low.
To address these limitations, we present a novel method called differential nucleosome positioning (DiNuP) in this article.
DiNuP takes advantage of the single-nucleotide resolution of nucleosome profiles, and it directly compares the distributions of sequenced nucleosome-DNA centres along the genome between different samples to detect genomic regions with differential nucleosome positioning without introducing any intermediate concepts (such as the nucleosome score or the positioned nucleosome).
DiNuP also calculates P-values and empirically estimates the false discovery rate (FDR), to evaluate the statistical significance of the identified difference.
Moreover, DiNuP provides various parameters with which to characterize the physical properties of those identified regions.
When applied to publicly available nucleosome profiles for yeast (Kaplan et al., 2009), DiNuP reliably detected differences in nucleosome positioning in a sequencing-depth-dependent manner, The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [09:52 5/7/2012 Bioinformatics-bts329.tex] Page: 1966 19651971 K.Fu et al.
and the detection of nucleosome position differences in the functionally important regions implies a close relationship between nucleosome positioning and eukaryotic transcriptional regulation.
2 METHODS 2.1 Estimation of the FDR We first randomly choose 1% of all of the sliding windows to be the estimating region.
Next, the reads of each paired window were combined and then re-sampled based on the initial ratio of that windows number of reads.
Most of the re-sampled windows are expected to have no differences except for the differences in the sequencing depths.
The KolmogorovSmirnov (K S) test is then used to calculate the difference between these re-sampled windows, and the P-values are ranked from small to large.
Finally, a specific percentile of the ranked P-value is used as an FDR estimation.
The FDR that is obtained by this method accounts for both the sequencing depth and the background noise, giving a robust cutoff when identifying RDNPs.
2.2 Evaluation in simulated datasets To evaluate the reliability and accuracy of DiNuP, we simulated different types of differential nucleosome positioning and used DiNuP to detect the simulated regions.
First, to facilitate the computation, we chose a 20 kb region as the control and simulated the nucleosome profile region.
Data were also obtained from (Kaplan et al., 2009).
Second, regions with a length of 200 bp were randomly selected to simulate repositioned variation from 0 to 100 bp, an occupancy change percentage from 0 to 100% and a positioning degree change from 0 to 0.5.
The background noise was then added by applying a coordinate disturbance of 3 bp to the residual part of the 20 kb region.
Third, DiNuP was used to identify the region of differential nucleosome positioning between the simulated region and the original region, repeating every degree of the simulation 1000 times.
If the detected region was in accordance with the simulated region, then it was a true-positive hit; otherwise, it was a false-positive hit.
The FDR was calculated as the ratio of the number of true-positive hits to the number of all positive hits.
2.3 Evaluation in real datasets Three physical properties that we defined, the repositioned variation, the occupancy change and the positioning degree change, were used to characterize three major types of differential nucleosome positioning: changes in nucleosome location, changes in occupancy and changes in fuzziness.
We calculated those three properties in each 200 bp sliding window (10 bp as a step) separately.
For each property, windows with values that were larger than a specific cutoff were regarded as the positive condition.
Positive windows identified by DiNuP as differential windows were regarded as true positives; otherwise, they were regarded as false negatives.
Windows with no obvious change (i.e.
5 bp for repositioned variation, 20% for occupancy change and 0.05 for positioning degree change) were then regarded as the negative condition.
The negative windows identified by DiNuP as differential windows were then regarded as false positives; otherwise, they were regarded as true negatives.
Since the number of true negatives and false positives are constant values in our definition, we used the ratio between false positives and outcome positives as the FDR.
2.4 Physical properties Zhang et al.
(2009) defined the nucleosome positioning degree of a certain genomic site as the ratio of the number of reads for a 20 bp window to that of a 160 bp window centred around that location.
A positioning degree of 1.0 indicates that this site is a nucleosome that is perfectly positioned, whereas a positioning degree of 0.05 indicates that this site is a nucleosome that is poorly positioned.
The change in the positioning degree can be obtained by calculating the average difference in the positioning degree between samples.
First, each identified RDNP was divided into 160 bp regions with a step of 10 bp.
Second, the largest positioning degree of each short region was used to represent the degree of this region.
Third, the average degree of each 160 bp region was used to represent the positioning degree of the whole region.
Finally, the difference between the samples was defined as the change in the positioning degree.
2.5 Analysis of identified RDNPs Because the identified regions can overlap more than one gene, we assigned a summit (the candidate driver location) of each RDNP to its corresponding genomic feature.
Yeast promoters were defined as the region from-350 bp upstream from the transcription start site (TSS) to +50 bp downstream from the TSS.
Using this definition, 22% of the RDNPs should occur randomly within promoters.
The ratio of real hits to random hits was used to represent the enrichment.
The P-value significance was calculated using the binomial test.
Moreover, if one identified RDNP overlapped with one gene, we then put this gene into a gene list and used DAVID to perform GO analysis.
In addition, we used Transcription factor (TF) data with intermediate-confidence conservation and a binding criterion of 0.005.
The significance of TF enrichment was calculated using the binomial test.
Gene expression data for yeast grown in YPD medium and YPGal medium were obtained from (Komili et al., 2007; Verstrepen et al., 2008).
The significance of the overlapping between genes proximal to RDNPs and genes that were differentially expressed was calculated by a hypergeometric test.
2.6 Software implementation DiNuP is implemented in Python and is freely available.
It runs from a command line and inputs the following parameters:-t for the first file of nucleosome profiles;-c for the second file of nucleosome profiles;--name for the name of the run;-f for enabling DiNuP to calculate physical properties;--windowsize for the size of the sliding window (default 200 bp);--fdr for the FDR cutoff to detect RDNPs;--pvalue for setting a P-value cutoff;--region for the minimum length of the identified RDNPs (default 70 bp);--format for the format of the input file;--bias for the simulation of the experimental bias (default 3 bp);--times for the number of times to calculate the KS test and take the average P-value as the significance of the difference (Default 3);--wig for whether to save significant P-values into the wiggle file;-a for setting the average nucleosomal DNA length for Sample A and-b for setting the average nucleosomal DNA length for Sample B;--fold for additionally applying fold change method;--fcutoff for the cutoff of fold change method (Default 2).
3 RESULTS 3.1 Strategy to detect RDNP There are three major types of differential nucleosome positioning: changes in nucleosome location, occupancy and fuzziness (Fig.1A).
Our strategy was designed to capture all three types based solely on the changes in the distribution of nucleosome sequencing reads.
To date, all publicly available nucleosome profiles that were generated using high-throughput sequencing technology include sequences for only one end of nucleosome-DNA fragments.
To represent the whole nucleosome, each read was extended toward its 3 end by 147 bp, as described previously (Zhang et al., 2008, 2009).
We took the centre of the extended read (dyad) to represent the precise nucleosome location in the following steps (Fig.1A).
For two samples, we scanned the whole genome with a sliding window (200 bp as the default window size and 10 bp as the default step), and for each sample, the location coordinates (relative to the windows midpoint) of the derived dyads in each window were treated as a numeric list.
Since the two-sample KS test is a non-parametric approach for determining whether two numeric lists 1966 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [09:52 5/7/2012 Bioinformatics-bts329.tex] Page: 1967 19651971 Systematic approach to identify regions of differential nucleosome positioning Fig.1.
Approach for identifying RDNPs.
(A) Schematic of DiNuP.
(B) An example of reducing experimental bias by giving coordinate disturbances.
The read distribution is obtained from reads within the sliding window (yellow).
D is the largest distance between the cumulative distributions are derived from the same distribution, we applied this approach to test whether the nucleosome positions in each window were different between the two samples.
In this way, the centre of each window is assigned a P-value to indicate the significance of the difference.
To evaluate and eliminate the effect of sequencing depth and background noise, we used a sampling method to empirically estimate the FDR, which provided a robust cutoff for comparing the different pairs (see Section 2).
Consecutive regions with P-values smaller than the cutoff and having a certain length (5% as the default FDR and 70 bp as the default minimum length) are identified as candidate RDNPs.
The final length of each identified region will, thus, be the default minimum length plus the size of the sliding window.
To analyse the performance of this strategy, we checked the consistency between the regions of change in the profile and the identified candidate regions.
As expected, most of the candidate regions have obvious differences in the nucleosome profiles (Supplementary Fig.S1).
However, we also observed some candidate regions with similar profiles between samples.
An example region is shown in Figure 1B.
Nucleosome profiles for Sample A and Sample B are generated in the same way as described in a previous study (Zhang et al., 2008).
In the window chr5: 327, 340327, 540 (yellow), Sample A has 248 dyads at location 327, 443 (24.3% of the dyads) in the window, while Sample B has only 3 dyads at location 327, with 443 (0.7% of the dyads) in the window.
Although our strategy provides high sensitivity to detect even single-nucleotide changes between samples, such differences may not be biologically meaningful.
Potential artifacts that arise from MNase treatment, PCR amplification or sequencing biases (Zhang and Pugh, 2011) can cause such small changes.
To eliminate these artifacts, we introduced coordinate disturbances (3 bp as the default) by adding a random number to the location of the dyads for each sliding window within the candidate region, and then we re-calculated the differences.
This step filters out the majority of artifacts from the predicted RDNPs because the read distribution of the two samples has been transformed to be more similar (Fig.1B).
Finally, our strategy identifies a list of genomic regions with significantly different nucleosome profiles.
3.2 Method evaluation To systematically inspect the performance of DiNuP when identifying different types of nucleosome positioning dynamics, including changes in location, occupancy and fuzziness, we used computational simulations to evaluate the performance of our method by comparing simulated datasets with a real dataset that has an equivalent genomic coverage of 200 (see Section 2).
Since our approach has a very low false-positive rate, we use a true-positive rate and a FDR as measurements of performance.
In the simulation of the repositioning, DiNuP has a sensitivity of 96.6% and an FDR of 1.2% when the simulated variation is only 20 bp (Fig.2A and B).
This result indicates that our approach can identify even mild nucleosome repositioning.
In addition, DiNuP has a sensitivity of 83.4% and an FDR of 3.9% with a 2-fold occupancy change (Fig.2C and D), demonstrating that our approach can also detect this type of change.
We use the positioning degree, which has been described previously (Zhang et al., 2009), to represent the nucleosome fuzziness.
The results of the positioning degree simulation show that the DiNuP has a high sensitivity and a low FDR when detecting changes in the positioning degree.
For example, when the positioning degree change is 0.2, the sensitivity of DiNuP is 98.7% and the FDR is 1.9% (Fig.2E and F).
In general, the simulation results demonstrate that DiNuP performs well in identifying different types of differential nucleosome positioning.
In addition to characterizing the reliability of DiNuP, we also assessed the effect of the sequencing depth on the ability to identify RDNPs by read sampling (Fig.2AF).
The results indicate that the sequencing depth affects the performance of DiNuP to a large extent; for example, when the sequencing depth decreases from 200 to 10, the sensitivity for identifying repositioning by 30 bp decreases from 97.2 to 78.1%, the sensitivity for identifying a 3-fold occupancy change drops from 94.9 to 49.5%, and the sensitivity for identifying a positioning degree change of 0.1 decreases from 94.0 to 58.6%.
In other words, the ability of DiNuP to detect RDNPs improves with increasing sequencing depth.
We also evaluated the performance of DiNuP under different cutoffs in real datasets (see Section 2).
The evaluation results show that DiNuP has a high sensitivity for the identification of different types of differential nucleosome positioning (Fig.3A, C and E; Supplementary Fig.S3A, C and E).
In addition, the evaluated specificity of DiNuP is also very high (more than 98%).
We then compared DiNuP with the fold change method based on both the simulation and the real biological datasets.
Except for the occupancy change (where the standard was defined by the fold change), the 1967 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [09:52 5/7/2012 Bioinformatics-bts329.tex] Page: 1968 19651971 K.Fu et al.
Fig.2.
Sensitivity and FDR of DiNuP evaluated by the simulation method.
(A) Sensitivity for the detection of the repositioned variation.
(B) FDR for the detection of the repositioned variation.
(C) Sensitivity for the detection of the occupancy change.
(D) FDR for the detection of the occupancy change.
(E) Sensitivity for the detection of the positioning degree change.
(F) FDR for the detection of the positioning degree change fold change method performs much worse than DiNuP for both changes in nucleosome location and fuzziness (Fig.3B, D and Figs; Supplementary Figs.
S2 and S3B, D and F).
Furthermore, the FDR of DiNuP is also lower than the fold change method under different cutoffs (Supplementary Table S1).
This superiority of DiNuP arises from its natural characteristics, and an example region is shown in Supplementary Figure S4.
3.3 Physical properties of RDNPs A previous study has defined some physical properties of single nucleosomes, such as the positioning location, the occupancy and the fuzziness (Mavrich et al., 2008), all of which may change under different conditions.
To obtain a better understanding of RDNPs, we defined three parameters, the repositioned variation, the occupancy change and the change in the positioning degree, to characterize the identified regions (Table 1).
The mean location difference of nucleosomal dyads between samples within a certain RDNP was defined as the repositioned variation.
In addition, nucleosome occupancy changes or the change in the number of bound nucleosomes were measured by calculating the fold change Fig.3.
Sensitivity of DiNuP and the fold change method evaluated in Kaplans YPD-YPEtOH paired samples under different cutoffs.
(A) Sensitivity of DiNuP for the detection of repositioned variation.
(B) Sensitivity of the fold change method for the detection of repositioned variation.
(C) Sensitivity of DiNuP for the detection of occupancy change.
(D) Sensitivity of the fold change method for the detection of occupancy change.
(E) Sensitivity of DiNuP for the detection of the positioning degree change.
(F) Sensitivity of the fold change method for the detection of the positioning degree change in the number of reads between samples.
Finally, to represent the change from fuzzy nucleosomes to phased nucleosomes, a parameter for the change in the nucleosome positioning degree was introduced (see Section 2).
Assuming that there is a mechanistic driving force (e.g.
TF binding or Pol II elongation) behind each RDNP, we defined two additional parameters to characterize potential mechanisms.
To identify the location that might be related to the cause of the differential nucleosome positioning, the genomic site with the most significant P-value calculated by DiNuP was defined as the candidate driver location.
Based on the perspective that different drivers could cause changes with different ranges, we merged adjacent RDNPs and used the length of the merged region to represent the effective width (Table 1).
In addition to analysing the RDNPs in a quantitative way using the parameters provided above, we were also able to classify the regions into different groups based on one or several physical properties.
For example, the parameter for occupancy change can 1968 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [09:52 5/7/2012 Bioinformatics-bts329.tex] Page: 1969 19651971 Systematic approach to identify regions of differential nucleosome positioning Table 1.
Physical properties of the RDNPs Property General description Technical description Repositioned variation Change in the nucleosome location Mean location change for the nucleosomal dyads within an RDNP Occupancy change Change in the number of bound nucleosomes Fold change in the number of reads Positioning degree change A measure of the change in the delocalization of nucleosomes Difference in the positioning degree between samples Effective width Effectiveness of the differential nucleosome positioning Width of the RDNP Candidate driver location Locations that may drive differential nucleosome positioning Location with the most significant P-value within an RDNP classify regions into three groups, i.e.
groups with increased, equal or decreased occupancy.
This classification is useful when researchers are interested only in specific types of RDNPs.
3.4 Use of DiNuP to analyse public datasets To our knowledge, Kaplan et al.
(2009) generated nucleosome profiles with the greatest sequencing depth among the publicly available datasets for yeast grown in three culture media.
These media were YPD, YPGal and YPEtOH, and the sequencing had genomic coverage of 294, 152 and 187, respectively.
We applied DiNuP to compare the nucleosome profiles of pairs of these three datasets.
When comparing the YPD medium samples and the YPGal medium samples, 698 RDNPs were identified using an FDR cutoff of 5%, which is equivalent to 2.2% of the yeast genome.
After assigning each region to its relevant genomic feature, 228 regions are found to be within promoters (Fig.4A), with a fold enrichment of 1.54 and a P-value significance of 8.9 1011 relative to the background (see Section 2).
Based on the assumption that dynamic nucleosome positioning is related to genes that are responsive to different environmental signals, we also assigned each identified region to its neighbouring genes and performed gene ontology (GO) analysis using DAVID (Huang da et al., 2009).
As expected, the identified RDNPs are proximal to genes that are significantly enriched in GO terms, including oxidative phosphorylation, the galactose metabolic process and the generation of precursor metabolites and energy (Fig.4B).
Moreover, 9 out of 11 genes with the GO annotation of the galactose metabolic process were identified as having differential nucleosome positioning.
We next examined whether differential nucleosome positioning is closely related to the binding of transcription factors.
An enrichment score was calculated by comparing the number of functional cis-elements measured by ChIP-chip (MacIsaac et al., 2006) within RDNPs and the number that would be within the elements by chance.
We observed that several transcription factors with important roles in regulating the glucose and galactose metabolic processes were significantly enriched in the identified RDNPs (see Section 2) (Supplementary Table S2).
For example, among the 20 conserved binding sites of GAL4, 7 of them are within RDNPs, with a fold enrichment of 16.1.
Moreover, when the relationship between Fig.4.
Identified RDNPs are related to transcriptional regulation.
(A) Genomic distribution of the identified RDNPs.
(B) Enriched GO terms (biological process) among the genes that are proximal to the RDNPs.
The Benjamini adjusted P-value is listed above each bar.
(C) Percentage of genes surrounding the RDNPs with differential gene expression.
The group all includes the genes that are proximal to all of the identified RDNPs.
Group 1 includes the genes that are proximal to RDNPs that have an effective width shorter than 400 bp, Group 2 includes the genes proximal to RDNPs that have an effective width longer than 400 bp but shorter than 700 bp and Group 3 includes genes proximal to RDNPs that have an effective width longer than 700 bp.
(D) Venn diagram of the RDNPs obtained for the YPD_YPGal pair and RDNPs obtained for the YPD_YPEtOH pair RDNPs and differential gene expression was examined, 43% of the genes that were proximal to the RDNPs were differentially expressed (Fig.4C), with a significant P-value of 4.8 105 (see Section 2).
However, for more than half of the genes that were proximal to RDNPs, their expression levels in YPD medium and YPGal medium were almost the same.
This result suggests that nucleosome positioning dynamics has a close relationship with gene transcription but cannot determine the absolute expression level.
To assess the properties of the identified RDNPs, the parameters that are defined in Table 1 were used to classify these regions into groups.
We found that most of the regions were in the unrepositioned group (reposition variation smaller than 20 bp) and that six out of nine identified galactose response genes were accompanied by severe positioning degree changes.
In accordance with a previous report that nucleosome remodelling is always restricted to one or two individual nucleosomes (Shivaswamy et al., 2008), we observed that more than 80% of the identified RDNPs had effective widths that were shorter than 400 bp.
Interestingly, there was an increase in the percentage of differentially expressed genes among genes that were proximal to RDNPs when the effective width of the RDNPs was increased (Fig.4C).
For example, 25 out of 37 of the genes that were proximal to RDNPs with an effective width that was longer than 700 bp were differentially expressed, whereas only 264 out of the 610 genes that were proximal to RDNPs with an effective width shorter than 400 bp were differentially expressed.
This result suggests that extensive changes in nucleosome positioning might be more directly and closely associated with gene expression than slight nucleosome positioning changes.
We further compared the nucleosome profiles of yeast that was grown in YPD medium and YPEtOH medium.
The results 1969 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [09:52 5/7/2012 Bioinformatics-bts329.tex] Page: 1970 19651971 K.Fu et al.
show that the identified RDNPs are also slightly enriched in promoters (Supplementary Fig.S5A) and that the genes surrounding RDNPs are significantly enriched in GO terms, including the vacuolar protein catabolic process, oxidation reduction and the monosaccharide metabolic process (Supplementary Fig.S5B).
After obtaining two lists of RDNPs for different pairs, we then asked whether differential nucleosome positioning is cell-type specific.
By comparing the RDNPs of the YPD-YPGal pair and the YPD-YPEtOH pair, unique differential regions in each pair were obtained (Fig.4D).
Seven out of nine galactose response genes were found to be specifically proximal to RDNPs for the YPD-YPGal pair.
Genes that were proximal to unique RDNP in the YPD-YPGal pair also included a larger percentage of differentially expressed genes than the genes that were not proximal to these regions.
In summary, our analysis shows that the identified RDNPs are within functionally important regions and that differential nucleosome positioning is closely related to the epigenetic regulation of transcription.
The identified RDNPs for this dataset can be obtained in Supplementary Table S3.
It is reported that nucleosome profiles may vary between biological replicates in some genomic regions (Zhang and Pugh, 2011).
To systematically inspect the role of the variation between replicates, we applied DiNuP to all possible 15 pair-wise comparisons of the six biological replicates of YPD medium.
Even with a stringent cutoff (FDR 0.01), dozens of or even hundreds of RDNPs can still be identified.
After carefully checking the identified regions, we found that nucleosome profiles in these regions are largely different (Supplementary Fig.S6).
To check whether the variability between replicates shows any biological meaning, we picked genes that were proximal to the identified RDNPs and analysed the functional enrichment of those genes.
As a result, none of the gene lists obtained from the 15 pairs is enriched in any of the GO terms.
We further checked whether those identified RDNPs are randomly distributed by summarizing the number of shared RDNPs among 15 pairs (Supplementary Fig.S7).
Then, 75% of the identified RDNPs were in only one or two pairs, indicating that the variability between biological replicates is largely random.
This variation could be mainly caused by a bias in the MNase treatment, PCR amplification or sequencing, which cannot be corrected solely based on a computational method.
Considering the experimental bias, when comparing nucleosome profiles between samples, some nucleosome positioning changes might not be biologically meaningful.
However, if the compared samples were treated with the same experimental strategy of nucleosome profiling, then this variability would not largely affect the identification of RDNPs with real biological meaning.
4 DISCUSSION When comparing the nucleosome profiles of different samples, it is important to first make the samples comparable to each other.
DiNuP uses the KS test to calculate statistical P-values between sliding windows and estimates the FDR from the background P-value for use as the cutoff to achieve this goal.
After reducing the number of potential artifacts, genomic regions with P-values consecutively lower than the cutoff were then identified as RDNPs.
It is worth noting that, when using different FDRs or minimum length cutoffs, the number of identified RDNPs can differ greatly.
Therefore, the cutoff chosen depends on the features of the identified regions that are preferred by the user.
Because DiNuP is designed to capture the most significant read distribution changes, it should strike a balance between the detection of different types of nucleosome positioning changes at a certain FDR cutoff.
In the identification of short-range nucleosome occupancy change, although regions with different occupancy levels may have similar read distributions, the KS test can identify the differences in both boundaries around the occupancy change region.
After combining the differential signals on the boundary of sharp nucleosome occupancy changes, regions with differential occupancy levels can be identified.
However, in terms of long-range nucleosome occupancy change, as the differential signals on the boundaries can be too far away from each other to be combined as an RDNP, our method will have limitations in the accurate identification of regions with long-range occupancy changes.
As a result, we implemented the fold change method into the software package of DiNuP and provided an option to users who are especially interested in detecting occupancy changes.
Nevertheless, DiNuP is, in general, an effective and robust method for identifying RDNPs.
Another important factor that influences the detection of differential nucleosome positioning is sequencing depth.
To evaluate this effect, we sampled datasets with different levels of genomic coverage and used DiNuP to detect RDNPs.
Assuming that the results obtained from the datasets with the deepest sequencing depth were the most reliable results, we used these RDNPs as a standard to determine the level of consistency between the regions identified using datasets with lower sequencing depths and those identified using the standard dataset (Supplementary Fig.S8).
With an FDR cutoff of 0.01, the consistency percentage dropped to 60% with a genomic coverage of 50, indicating that the sequencing depth indeed affects the detection of RDNP to a large extent.
From this perspective, we argue that a minimum sequencing depth, i.e.
a genomic coverage of 20 or 30, is required for the accurate identification of differential nucleosome positioning; otherwise, genomic regions that are identified as different may not be truly different but instead may appear different as a result of random discrepancies or background noise.
ACKNOWLEDGEMENTS We thank Lin Liu, Yiqian Zhang, Chenfei Wang, Hanfei Sun, Meng Zhou, Qian Zhao, and Hongtao Sun for their kind help and insightful discussions.
We also thank the anonymous reviewers for their constructive suggestions.
Funding: The National Basic Research Program of China (973 Program; No.
2010CB944904 and 2011CB965104), the National Natural Science Foundation of China (31071114), the Shanghai Rising-Star Program (10QA1407300), the New Century Excellent Talents in the University of China (NCET-11-0389) and the Innovative Research Team Program Ministry of Education of China (IRT1168).
Conflict of Interest: none declared.
ABSTRACT Motivation: Comparative genomics aims to understand the structure and function of genomes by translating knowledge gained about some genomes to the object of study.
Early approaches used pairwise com-parisons, but today researchers are attempting to leverage the larger potential of multi-way comparisons.
Comparative genomics relies on the structuring of genomes into syntenic blocks: blocks of sequence that exhibit conserved features across the genomes.
Syntenic blocs are required for complex computations to scale to the billions of nu-cleotides present in many genomes; they enable comparisons across broad ranges of genomes because they filter out much of the individ-ual variability; they highlight candidate regions for in-depth studies; and they facilitate whole-genome comparisons through visualization tools.
However, the concept of syntenic block remains loosely defined.
Tools for the identification of syntenic blocks yield quite different re-sults, thereby preventing a systematic assessment of the next steps in an analysis.
Current tools do not include measurable quality objectives and thus cannot be benchmarked against themselves.
Comparisons among tools have also been neglectedwhat few results are given use superficial measures unrelated to quality or consistency.
Results: We present a theoretical model as well as an experimental basis for comparing syntenic blocks and thus also for improving or designing tools for the identification of syntenic blocks.
We illustrate the application of the model and the measures by applying them to syntenic blocks produced by three different contemporary tools (DRIMM-Synteny, i-ADHoRe and Cyntenator) on a dataset of eight yeast genomes.
Our findings highlight the need for a well founded, systematic approach to the decomposition of genomes into syntenic blocks.
Our experiments demonstrate widely divergent results among these tools, throwing into question the robustness of the basic ap-proach in comparative genomics.
We have taken the first step towards a formal approach to the construction of syntenic blocks by develop-ing a simple quality criterion based on sound evolutionary principles.
Contact: cristinagabriela.ghiurcuta@epfl.ch 1 BACKGROUND Comparative studies have long been the mainstay of knowledge discovery in biology.
With the advent of inexpensive sequencing tools, pairwise sequence comparison became a major research tool; programs such as BLAST (Altschul et al., 1990) are used to identify regions with similar sequences in order to study prob-lems in genetics and genomics by using knowledge from better characterized organisms.
Such comparisons have been carried out on relatively short sequence fragmentsusually up to the length of a protein transcript, i.e.
a few thousand nucleotides.
Such work continues at a great pace today, but the rapidly increasing availability of complete genome sequences has led to the desire to compare entire genomes at once, the better to understand the large-scale architectural features of genomes and the evolutionary events that have shaped these features, such as segmental and whole-genome duplication, horizontal transfer, recombinations of various types and rearrangements.
Comparing entire genomes is not new: almost a century ago, Thomas Morgan and his students used chromosomal banding to build genetic maps of various strains of Drosophila melanogaster.
What is new today is the possibility of comparing complete genome sequences to each other.
Comparing even just two gen-omes is a major computational challenge when the two genomes have several billion nucleotides and when most of the sequence (490% in humans) is poorly understood and so lacks a suitable evolutionary model.
Consequently, researchers have approached the problem by defining (or searching for) conserved sequence markers (mostly belonging to the better understood coding re-gions of the genome).
These markers are then used to form large-scale patterns that can be evaluated for similarity and conserva-tion.
Such large-scale patterns, when used systematically, can be viewed as alternative representations of the genomes.
The sim-plest such representation uses the concept of syntenic blocks (SBs), large blocks of sequence that are well conserved (as testi-fied by commonality of markers and similarity of high-level pat-terns) across the species (or within a genome).
Working with such blocks facilitates comparative studies: (i) it confers robust-ness against variability across individuals and against various sources of error; (ii) it reduces the dependence on an accepted model of sequence evolution for each region and is less likely to suffer from homoplasy; (iii) it reduces the complexity of the ana-lysis of the genomic structures; (iv) it provides high-level features for further evolutionary studies; and (v) it identifies specific regions of interest for detailed studies and possible bench experiments.
In this article, we provide a concise overview of the existing notions of synteny in the literature and propose a formal, prin-cipled definition of SBs based on homologies.
We discuss how the quality of SBs can be measured against this definition and illustrate our approach with a comparison of three current tools for the construction of SBsCyntenator (Roedelsperger and Dieterich, 2010), DRIMM-Synteny (Pham and Pevzner, 2010) (DRIMM) and i-ADHoRE 3.0 (Proost et al., 2012) (i-ADHoRe).
We investigate the underlying heuristics and evalu-ate the results on a dataset of eight full genomes of various spe-cies of yeasts from the Yeast Gene Order Browser (YGOB) (Byrne and Wolfe, 2005), pointing out the issues that arise when working with SBs.
1.1 Early notions of synteny Little has been done so far towards a formal definition of SBs and/or SB families, nor have developers of algorithms and*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com [1] , , .
over paper syntenic block syntenic block 3 syntenic block [23] [20] , [21] 8 [5] syntenic block syntenic block XPath error Undefined namespace prefix software for producing SBs given any quantifiable goals.
Instead, identifying SBs has been a matter of application-dependent heur-istics, lacking any serious attempt at evaluating the quality of the approachessomething that in any case would have proved dif-ficult in absence of quality criteria.
The first mention of synteny as it is understood today was in an article of Renwick (1971) on human chromosome mapping, where the term is introduced to denote collocation of markers on the same chromosome.
Nadeau and Taylor (1984) gave an informal definition of syntenic seg-ments, in a paper that has since been cited by most researchers concerned with synteny.
Nadeau and Taylor gave a list of fea-tures viewed as supporting inclusion of markers in an SB, a list that includes conserved orientation, conserved adjacency and conserved position of homologous markers associated with the corresponding mapped chromosomes, a collection of features that loosely defines what is more commonly called today collinearity.
The study of rearrangements led to the definition of common intervals (Bergeron et al., 2002; Jahn, 2011), conserved regions of a chromosome within which the same set of genes can be observed, albeit not necessarily in the same order.
The concept is formally and precisely defined and captures many of the prop-erties informally associated in the literature with SBs.
The definition is given in terms of families of non-duplicated genes (or other families of unique sequences) and their ordering.
It does not take into account precise locations on the genome, nor the actual nucleotide sequences of these genes.
Around the same time, the need to compare entire genomes of the newly sequenced model species led The Mouse Genome Sequencing Consortium (2002) to propose SBs as sets of adjacent syntenic fragments (possibly shuffled in order and orientation) belonging to the same chromosome, where a syntenic fragment consisted of markers arranged in a conserved order.
In this view, syntenic fragments obey collinearity, whereas SBs need not do so.
Calabrese et al.
(2003), authors of the FISH synteny tool, defined their model based on segmental homology, in which the ordering of features belonging to two homologous segments is roughly conserved, some variation being allowed.
Pevzner and Tesler (2003) and later Bourque et al.
(2004), both working on the GRIMM-Synteny tool, removed constraints on conserved segments, thereby implicitly defining an SB in terms of conserved segments that can be disrupted by internal rearrangementsre-arrangements that the authors found to be far more common than expected and that therefore had to be largely ignored in constructing SBs.
In contrast, Van de Peer and his group, au-thors of the ADHoRe tool (Vandepoele et al., 2002), chose to emphasize collinearity and to break larger blocks into smaller ones as necessary to maintain this property.
These and other early tools are briefly reviewed in (Deonier et al., 2005).
1.2 Markers, syntenic blocks and genomic alignment Identifying SBs and aligning whole genomes both rely on iden-tifying markers, i.e.
short sequences that are highly conserved across the genomes and long enough to make their conservation statistically significant.
SB construction uses subsets from the set of markers: if a sufficiently dense region is identified in most of the genomes, those regions can be viewed as SBs.
Genomic align-ment uses the markers as anchors, i.e.
fixed references in the alignment.
Most SB finders use genes as markers; a few use k-mers, for a fixed value of k, to define a de Bruijn graph on the k-mers.
[de Bruijn graphs are widely used for genome assem-blysee Compeau et al.
(2011) for an excellent introduction in this context.
In such a graph, every k-mer found in the input sequences is represented by an edge connecting two vertices that are the k 1 prefix and k 1 suffix of the k-mer.
Thus a path of j edges through such a graph corresponds to an assembled se-quence of length k+ j 1 formed by ordering j k-mers, with each consecutive pair presenting a perfect overlap of length k 1; in particular, an Eulerian path through the graph corresponds to an assembly of all k-mers into a single sequence.]
Genomic align-ment may use a richer pool of markers, such as scaffold data, maximum unique matches (perfectly conserved sequence frag-ments of maximal length), genes and even assembly contigs.
Those that use markers in the sense of highly conserved sequence fragments define markers through a variety of criteria, such as Bayesian statistics in Pecan (Paten et al., 2009) or sequence simi-larity iterated through a refinement pipeline in ProgressiveMauve (Darling et al., 2010).
Just as most work on defining SBs focuses on two genomes at a time, so is whole-genome alignment usually done pairwise.
Biologists have long known that multi-way comparisons provide more information than pairwise comparisons, especially multi-way comparisons within a phylogenetic context.
However, com-paring several genomes at once introduces problems: finding good markers that are present in all, or almost all, genomes; choosing or inferring a number of parameters related to attri-butes difficult to measure, such as the level of evolutionary divergence among the genomes or the quality of the genome sequences used; assigning one-to-one correspondences among similar blocks so as to minimize the number of evolutionary events needed to explain the architecture of the modern genomes; whether to insist on the transitivity of relationships such as hom-ology and orthology (among markers, among genes, among SBs, etc.
); and many others.
1.3 Work to date Nadeau and Taylor (1984) defined synteny in terms of two or more pairs of homologous genes occupying the same chromo-somal segment, where homologous loci are based on similarity of function of the products of the corresponding genes.
They carefully distinguished synteny, which they were basing on con-servation of function, from conserved segments, based on con-servation of sequence.
More recent work has typically used conservation of sequence rather than conservation of function, but has also made use of orthology, presumably because orthol-ogy is viewed as a stronger indicator of conserved function than homology.
Zeng et al.
(2008) based their Orthocluster tool strictly on gene orthology and used many parametric constraints, such as pos-ition, overall number of genes in a block, allowed number of genes per block without orthologs, etc.
Their tool handles large-scale genomic events such as translocation, transposition, indels and duplication.
The restriction to orthology, however, means that the applicability of the tool is limited to collections of closely related organisms.
i10 C.G.Ghiurcuta and B.M.E.Moret syntenic block syntenic block in 1971, syntenic block , [3,13] syntenic block syntenic block [24] syntenic block syntenic block syntenic block [25] [9] , syntenic block that is, Syntenic block syntenic block that is, syntenic block (------.)
, [17] [8] syntenic block syntenic block [16] [27]  Cassis (Baudet et al., 2010), also based on orthology relation-ships, prunes considerably the list of orthologous gene pairs pro-vided as input, eliminating those that disrupt collinearity.
The remaining pairs are used to form blocks based on a statistical evaluation of their match to the collinear model.
Modern tools all attempt to handle the loss of collinearity, in recognition of the fact that collinearity (absence of rearrange-ments) is unlikely to be observed in collections of genomes of any significant size or degree of divergence.
Equally important and still challenging is the ability to deal with varying marker (most often gene) content: given reasonably divergent genomes, markers will have been variously lost or acquired over time.
In the multiple alignment tool ProgressiveMauve, Darling et al.
(2010) focused on a very principled approach to define and then to use the markers for the alignment process.
Its strat-egy is to identify highly conserved, sufficiently long sequences (anchors) throughout a concatenated multi-chromosomal genome and then, for each interval between consecutive anchors that exceeds a certain length, to search recursively for additional, less perfectly conserved anchors.
This recursive refinement continues until the anchor coverage has reached a sufficient dens-ity or the heuristic cannot retrieve any additional anchors.
ProgressiveMauve was designed as an alignment tool, not a syn-teny tool, but it generates a list of homologous, locally collinear regions that can be used as a basis for defining SBs.
Cyntenator (Roedelsperger and Dieterich, 2010) uses genes as markers and is based on a progressive alignment of profiles of gene-order data.
It allows gene duplication and loss and thus, in order to distinguish between orthologs and paralogs, takes into account gene family information as part of its scoring scheme.
Pairwise alignments produced at each stage are refined before being used in the next stage.
As is the case for most such tools, the blocks identified by Cyntenator are not formally character-ized, but indirectly defined through the algorithm.
i-ADHoRe 3.0 (Proost et al., 2012) also uses genes as markers; it includes heuristics to deal with rearrangement and duplication.
Duplicated genes are mapped onto a representative of the gene family.
The tool produces profiles of collinear regions based on homology maps of pairs of genomic regions and uses heuristics based on network flow to resolve conflicting relations between pairs of genes.
The tool provides three constraint models for generating SBs: collinear (conserving both order and orienta-tion), cloud (conserving neither order nor orientation, but content) and a sequential mixture of the two.
DRIMM-Synteny (Pham and Pevzner, 2010), the multi-way successor of the pairwise GRIMM-Synteny, is, like most synteny tools, based on genes, but follows an entirely different approach, as it is based on de Bruijn graphs.
A somewhat different version of de Bruijn graphs, called A-Bruijn graphs, is used in order to take into account the different characteristics of the problem, such as the use of gene orders rather than overlaps.
Thus a gene adjacency becomes an edge of the graph and is weighted by the number of its occurrences across the genomes.
SBs correspond to paths through the graph.
Sibelia (Minkin et al., 2013) follows up on DRIMM, in that it is also based on de Bruijn graphs, but, being designed for bac-terial genomes, it works directly from sequence data and so builds standard de Bruijn graphs from sequence k-mers.
It also adds an iterative refinement procedure that provides a range of granularity for the blocks.
The pipeline is executed individually for increasing sizes of the k-mers, until the output block is the whole genome.
At each iteration, a different set of blocks is generated and is placed as a node into a tree structure, with the root of the tree corresponding to the whole genome.
Table 1 lists the main features of the synteny tools we used.
1.4 Syntenic blocks, homology and granularity That blocks generated from the same data by different tools may differ enormously is due mostly to the lack of a formal definition for SBs: with no verifiable constraints and no measurable opti-mality criterion, one cannot meaningfully compare two collec-tions of SBs for the same data.
In part, the lack of such constraints and criteria can be attributed to the very different uses to which SBs are put.
For instance, using SBs to pinpoint a region of interest in the genomes works best if the blocks are small and highly conserved, whereas using SBs to study the evo-lution of the architecture of genomes does better with larger blocks and can tolerate much larger divergence in any given block among the genomes.
(Indeed, the larger the evolutionary divergence, the larger and sparser the SBs should be, to account for the lower number of high-quality markers.)
When large-scale (segmental or whole-genome) duplications are present, multiple instances of the same SB will be found within the same genome, as well as throughout other gen-omesthat is, SBs, like genes, can be grouped into families of homologs.
Identifying orthologies among the markers or genes is thus intertwined with identifying SBsarguing for a simultan-eous construction, which can take into account positions, re-arrangements and duplications and losses of markers and of blocks all at once.
Thus homology is at the root of any principled definition of SBs: the process of construction of SBs is simply the process of extending homologies among markers to homologies among blocks under a suitable model of evolution.
In such a manner, partitioning the genomes into SBs defines the necessary higher-level homology relationships that relate such blocks within and across genomes.
Since all genomes share a common ancestor, every single genome is trivially an SB by itself, albeit with a very low degree of conservation across a collection of genomes.
At the other extreme, if we had available a detailed history of all Table 1.
Major features or constraints of five synteny tools: ProgressiveMauve (PM), OrthoCluster (OC), Cyntenator (Cy), i-ADHoRe (i-A) and DRIMM (DR); presence is denoted by +, absence by and options by o PM OC Cy i-A DR Collinearity o o Framed blocks +   Overlapping content + + + Selective content + + + Across chromosomes + + + o Duplicated regions + + + + i11 Evaluating synteny for improved comparative studies [2] [8] syntenic block [23] [21] syntenic block [20] Syntenic block [15] Syntenic block , syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block that is, syntenic block syntenic block , syntenic block syntenic block syntenic block syntenic block evolutionary events at the sequence level, we could construct SBs consisting of a single nucleotide position.
In a similar vein, two or more adjacent SBs can be viewed as single, larger SBs, pre-sumably at the cost of some loss in conservation.
In other words, granularity is an important attribute and one can construct a hierarchy of decompositions into SBs, taking the form of a rooted directed acyclic graph where the trivial decomposition into a single block sits at the root and the equally trivial decom-position into individual nucleotide positions sits at the single leaf.
Children of a node in this dag are associated with decompos-itions of finer granularity than that associated with the node itself.
Under some mild constraints, this dag is in fact a lattice (or partially ordered set).
It is important to note that the lattice is determined by con-straints resulting from the definition of an SB, but the selection of a particular node in the lattice (a particular decomposition into blocks) is driven by other criteria (such as granularity) and thus determined by the application.
(Of all the various tools reviewed here, only Sibelia makes explicit mention of a hierarchy of SBs.)
2 METHODS 2.1 Homology, orthology and synteny Any definition of synteny must use homology or orthology.
Most synteny tools today use bothhomology as a matter of principle and orthology as a result of practical constraints.
In evolutionary biology, two structures (character positions in a sequence, markers of various types, genes, SBs) are homologous if they are descended from a common ancestral structure (Fitch, 2000); if, in addition, the branching at the last common ancestor was a speciation, the structures are also orthologous.
Thus homology is an equivalence relationship and, as such, determines equivalence classes, the homologous families of structures.
Orthology, in contrast, depends on the speciation point and so is context-dependent; in particular, it is generally not transitive.
(For instance, two gene duplicates within the same genome cannot be orthologous, but these two duplicates and a homologous gene in another species are orthologous if the duplication followed the speci-ation.)
Instead, it must be specified through hierarchies structured through the phylogeny (see Gabaldon and Koonin, 2013).
Homology and orthology cannot be observed, but only inferred.
In practice, homology for markers and genes is determined on the basis of sequence similarity, using tools such as BLAST.
Orthology is also initially determined through sequence similarity, but often verified through phylo-genetic analysis or by ascertaining functional similarity.
However, only rarely is position along the genome taken into accountexceptions are the database OrthoDB (Waterhouse et al., 2011), which also provides a hierarchy of orthology relationships, and the orthology tool MSOAR (Fu et al., 2007).
In practice, therefore, identifying homologies is much easier than identifying orthologies.
Synteny is defined both through families of homologous markers and through placement within the genome.
Therefore identifying SBs, in add-ition to prior knowledge of homologies, requires taking into account rearrangements and duplications that disperses the members of a hom-ologous family throughout the genome.
(Conversely, of course, produ-cing SBs makes direct statements about the evolutionary history of the genomes by ruling out some of the possible scenarios.)
Therefore, in principle, the identification of SBs should proceed from homologies (which have little direct dependence on location) rather than from orthol-ogies inferred without regard to location.
Computing gene clusters, for instance, is best done based on families of homologous genes instead of relations derived from orthologous groups (Jahn, 2011).
Practice may dictate otherwise.
Inferred homologies are neither sym-metric nor transitive in practice, as they depend on similarity thresholds.
In addition, since orthology is the stronger relationship, it is often preferred, at least for pairwise synteny, as it may provide higher quality markers and because it simplifies the task.
(Some synteny finders simply transform orthologous relationships into bijections, in spite of the fact that orthology is a many-to-many relation.)
When moving from pairwise to multi-way syntenies, orthologies become problematic: the more diverse the group of genomes, the more difficult it becomes to identify orthologies.
In practice, therefore, synteny tools rely on both homology and orthology, viewed largely as different degrees of sequence similarity.
2.2 Towards a formal definition for syntenic blocks Here we propose a fundamental constraint on the makeup of SBs, based on an evolutionary perspective.
We first formalize that constraint for pairwise synteny, then extend it to multi-way synteny.
We also propose a second constraint, which provides added refinement for bacterial genomes and also helps narrow searches when looking for conserved regions of interest.
Our definitions are made in terms of markers and homology state-ments among them.
Thus we regard each genome as a multi-set of mar-kersa multi-set rather than a set, as the same marker may occur more than once in the same genome.
Associated with each marker is a set of homology statements relating that marker to its homologs in other gen-omes or in its own genome; a homology statement is just an unordered pair of markers.
Ideally, these homology statements define an equivalence relation on the set of markers; in practice, of course, these statements come from a variety of sources (databases, direct analysis of sequence similarity, etc.)
and are unlikely to obey all the requirements of an equiva-lence relation.
Viewed abstractly, identifying SBs is a clustering problem: how do we partition the multi-set of markers into smaller multi-sets, so as to maxi-mize the similarity (as attested by multiple homology statements) between some of the smaller multi-sets, while minimizing their similarity to others?
Because our definition rests on homologies rather than orthologies, we expect to find homology statements connecting related SBs as well as some connecting unrelated SBsby and large, the first are more likely to be orthologies, while the second are more likely to be paralogies.
Our main constraint, then, is that, in order for two blocks to be homologous SBs, they must be connected through homology statements and that neither includes markers that, while unconnected in this manner to any-thing in the other blocks, are connected to markers in unrelated SBs.
We now formalize our definition for the basic version of SBs: SBs for two genomes, in which we restrict each to be a contiguous range of pos-itions within a chromosome.
DEFINITION 1.
We are given two genomes, GA with a set A of nA markers and GB with a set B of nB markers; the markers of GA are ordered along the chromosomes, as are the markers of GB.
Let H be a set of pairs of distinct elements of A [ Bthe homology statements.
We assume that every marker in A and B is part of at least one homology statement.
Let SA be a set of contiguous markers on one chromosome of GA and SB a set of contiguous markers on one chromosome of GB.
We say that SA and SB are homologous SBs if and only if, for any marker x 2 SA, there exists a marker y 2 SB such that {x, y} is a homology statement, and, for any marker u 2 SB, there exists a marker v 2 SA such that {u, v} is a homology statement.
We can further require that the two end markers form a conserved frame, thereby setting defined boundaries on the range of positions form-ing an SB.
DEFINITION 2.
Let SA and SB be homologous SBs as per Definition 1.
If the first marker of SA is a homolog of one of the two endmarkers (the first or last marker) of SB and the last marker of SA is a homolog i12 C.G.Ghiurcuta and B.M.E.Moret syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block [10] , , [12] [26] [11] syntenic block syntenic block syntenic block [13]-syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block Definition syntenic block syntenic block Definition syntenic block .
of the other endmarker of SB, we say that SA and SB are (homologous) framed SBs.
Many of the existing tools require that the homology between markers respect the ordering of the markers along the blocksa property usually referred to as collinearity.
Because genomes are subject to rearrange-ments, we do not require collinearity, but we can define it as follows using our notation.
DEFINITION 3.
Let SA and SB be two homologous SBs as per Definition 1.
We say that SA and SB are collinear SBs if the following condition, stated in the direction from SA to SB, holds in both directions: for any markers x and y in SA with x appearing before y, there exist markers u and v in SB, with u appearing before v, such that both {x, u} and {y, v} are homology statements.
Our requirement that each block be fully contained with a chromo-some may require that some evolutionary events, such as translocation, fusion and fission, all of which can move genomic material between chromosomes, be treated as block-splitting events.
For instance, if prior to such an operation, we would have identified regions A and B as hom-ologous SBs, but the operation moved part of region A, call it At (tail) to another chromosome, leaving only Ah (head) in the original location, then after the operation we may be unable to associate either of Ah or At with B, but we may be able to associate Ah with a first subregion Bh of B and At with a second subregion Bt of B, thereby producing two pairs of smaller SBs.
We extend pairwise synteny to multi-way synteny by taking advantage of the transitive nature of true homology: we simply require transitive closure of pairwise relationships.
DEFINITION 4.
We say that blocks A1, A2, .
.
.
, Ak are homologous SBs if and only if, for any i and j; 1 i5j k; Ai and Aj are pairwise hom-ologous SBs.
This definition is unambiguous whenever our set of homology state-ments defines an equivalence relation, since this property ensures transi-tivity.
In practice, however, neither transitivity nor symmetry will hold: our set of homology statements will typically be incomplete as not all homologies among markers are detectable and homology defined through sequence similarity (the most common type in practice) need not be symmetric.
The output of a synteny tool is a collection of families of homologous SBs (henceforth SBFs), each family tied together with homology state-ments.
We illustrate our definitions with a few cartoons.
Figure 1 shows the building blocks for our cartoons and also demonstrates the additional structure present in framed SBs.
Figure 2 illustrates the main character-istics used in our definitions.
The first two cartoons in the figure show SBs defined through one-to-one (Fig.2A) and one-to-many (Fig.2B) hom-ology statements.
Homology statements may connect markers in non-homologous SBs, as long as other homology statements connect these markers to markers in homologous SBFs.
The third cartoon (Fig.2C) gives an example of invalid blocks: the red marker has a homolog in a non-homologous SB, but none in the putative homologous SBs.
3 RESULTS AND DISCUSSION Our goal is to enable evaluations and comparisons of decompos-itions into SBs.
Such evaluations and comparisons have mostly been missing and, when present, have typically been limited to aspects such as coverage of the genome or number of blocks, neither of which has much to do with quality.
Our first step was to propose formal constraints that any decomposition into SBs should satisfy.
These constraints are not likely to be met except in ideal cases, so our second step is to measure compliance with the constraints, which is to say, to measure quality.
We therefore assemble a dataset of whole genomes to use in testing various methods; devise specific measurements of compliance with our definitions; and provide other insights and measures regarding the various tools tested.
3.1 The data Because we chose to include DRIMM in our evaluation, but could not reproduce its authors results, we decided to use their results directly.
Of the datasets used in the DRIMM study, only the yeasts combined complete results from the authors and public availability of the genomic data.
We thus used the gene data from the Yeast Gene Order Browser (version of April 2009) (Byrne and Wolfe, 2005) for the following eight yeast genomes: Candida glabrata (c), Eremothecium gossypii (g), Kluyveromyces lactis (l), Lachancea thermotolerans (t), Saccharomyces cerevisiae (s), Zygosaccharomyces rouxii (r), Kluyveromyces waltii (w) and Saccharomyces kluyveri (k).
The _genome.tab files were used to retrieve the complete list of genes for each of the organisms and (a) (b) (c) Fig.2.
Cartoons illustrating SBF structures on three genomes.
Colors at marker level denote families of homologous units.
(a) Three SBFs; in the SBF on the left, three markers are in one-to-one homology.
(b) Three SBFs; in the SBF on the left, three markers are in one-to-many homol-ogy, including an additional homologous marker in another SBF.
(c) Three putative SBFs; as shown, the red marker violates our definition, since it has a homology statement, but that homology connects it to a marker in a different SBF, while there is no homology connecting it to any marker within its own putative SBF Fig.1.
A cartoon for SBFs among three genomes G1, G2 and G3.
The horizontal strips correspond to the genomes; small colored boxes denote markers; each SBF is framed by a dashed rectangular outline; and hom-ologous SBFs are aligned vertically and enclosed in a thin solid box.
Colored lines between horizontal strips connect markers and denote se-lected homology statements.
Shown are an SBF of three framed homolo-gous SBFs (on the left) and, using the same homology statements, an SBF of three ordinary homologous SBFs (on the right) i13 Evaluating synteny for improved comparative studies syntenic block Definition syntenic block .
syntenic block , syntenic block syntenic block Definition syntenic block syntenic block syntenic blocks ( ) syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block syntenic block devise [5] .
E. K. L. S. Z. K. , S. the associated NT.fsa file was processed in order to retrieve the sequences for these genes.
Table 2 summarizes the characteris-tics of the data.
All four tools require a list of homology statementsorthology statements for OrthoCluster.
We used Fasta36 (Pearson, 1998), with a cutoff of 105, to compile hom-ology statements for each gene, reflecting common practice.
We discarded any gene for which no homology statement was pro-duced and, because Cyntenator does not scale well with large gene-family sizes, we retained only the 10 best matches (homolog candidates) for each gene.
Computational constraints imposed by the tools meant that the number of markers could not be too large; moreover, a number of tools assume that the markers are genes; thus we used genes as markers.
3.2 The tools We used the results of the DRIMM study and ran OrthoCluster, Cyntenator and i-ADHoRe on the yeast dataset.
We had chosen DRIMM because it represented a very different approach to the problem (using de Bruijn graphs) and chose the other three be-cause all are of recent design and maintained, all support multi-way comparisons and all have clear statements about their design in the respective original publications.
Unfortunately, in spite of prompt support from the developers, OrthoCluster (Zeng et al., 2008) could not run within reasonable time on our dataset with-out removing so many genes and homology statements as to invalidate the exercise, so we had to exclude it from the study.
(We ran the tool for 2 weeks on a 48-core, 256 GB Dell Poweredge 815 without results.)
We ran Cyntenator with the parameter setting used by the authors in the original article: gap=0.3, mismatches=0.3, threshold=2 and filter=10 000.
The final output depends on, in effect, a guide tree (a phylogeny of the eight species), as it is obtained by running the tool on pairs of intermediate results the tool ran well on pairs, but not so well on triples, and almost never on larger subsets of genomes.
We eventually settled on the pattern described by the tree ((r, (w, (g, (k, (c, s))))), (l, t)).
We ran i-ADHoRe in collinear mode, with the following parameters: gap size=15, cluster gap=35, q value=0.9, prob-ability cutoff=0.001, anchor points=3, gg2 heuristic, no level 2 only and FDR as multiple hypothesis correction.
3.3 The output The output of all three tools is in the form of families of hom-ologous SBFs, where each family has at most eight blocks, each belonging to one of the eight genomes under comparison.
That we get no more than eight is due to the use of genes as markers: a large fraction of the genes are singletons (have no homolog within their own genome), thereby making it highly unlikely that a particular block structure would be found repeated within the genome.
A family has fewer than eight blocks when no homologous SB in that family can be identified in a particular genome.
Figure 3 gives an overall feel for the results of the study, showing how the blocks from one tool map onto those of an-other.
A very clear mapping pattern can be observed from both Cyntenator and DRIMM to a specific, small subset of the blocks generated by i-ADHoRe, as highlighted by the dark blue section on the ring of i-ADHoRe.
The number of blocks generated by i-ADHoRe is considerably higher than those generated by Cyntenator or DRIMM, so the blocks are smaller and the (blue) links thinner.
(This kind of mapping also illustrates the lattice concept discussed earlier: the thin links bind smaller blocks to a larger block made of these smaller blocks.)
3.4 Evaluation against our definitions Our main requirement is that markers within an SB have homo-logs within each of the other SBs in the family.
As we saw, this simple constraint is unlikely to be satisfied in practice, so we Fig.3.
SBFs defined by Cyntenator (purple), i-ADHoRe (blue) and DRIMM (green), mapped to each other in terms of gene content.
Each link bears the color of the tool, the output of which is mapped through the link onto the outputs of the other tools.
There are six pairwise com-parisons between the SBFs produced by the three tools.
The thickness of a link shows the level of similarity, measured by the overlap between the gene content of two SBFs relative to the SBF being mapped.
Each sector of the diagram is an ordering by size of all blocks generated by the cor-responding tool Table 2.
Characteristics of the data from YGOB Genomes Genes/genome Homolog pairs C.glabrata 5211 106 291 E.gossypii 4725 104 817 K.lactis 5086 113 075 L.thermotolerans 5111 94 262 S.cerevisiae 6600 140 851 Z.rouxii 5006 135 707 K.waltii 10825 194 234 S.kluyveri 5340 166 835 The genes for K.waltii are often contigs with various functions (ORFs, short com-plements with intron/exon annotation), which explains their abnormally high number.
i14 C.G.Ghiurcuta and B.M.E.Moret [18]--gene , 3 , [27] , 8 Q , 3 syntenic blocks ( ) 8 8 8 8 syntenic block syntenic block syntenic block relax the transitivity requirement and measure compliance with the resulting weakened constraint.
Our first measure relates to the families of SBFs: we compute the number of SBFs that include within one of their SBs a marker with no homolog within any block of the SBF.
This count is reported in the second column of Table 3.
Since this measure tolerates failures in transitivity, the number of SBFs not in perfect compliance with our definition may be much larger.
This first measure is an absolute count, although different tools produce different numbers of SBFs; moreover, it counts an SBF as a failure no matter how many markers in that SBF fail the test.
To address the first issue, we compute the percentage of failing markers in an SBFi.e.
markers that have homologs in other SBFs, but none in their own SBF.
We use two different base counts for normalization, to reflect fundamental differences between the tools with respect to selective use of markers: the first count is the total number of markers present in the SBF as generated by the tool, denoted E(X), while the second is the total number of markers present in the genome within the coordinates of the generated blocks, denoted E(X0).
Because DRIMM and i-ADHoRe eliminate markers from within SBs (within the co-ordinates of the block), something that Cyntenator does not do, the values of E(X) for DRIMM and i-ADHoRe may be signifi-cantly smaller than those of E(X0).
Figure 4 shows that i-ADHoRe generates more, and Cyntenator fewer, blocks with a very small fraction of markers lacking any homolog within their own SBF.
DEFINITION 5.
We define two scores, the first more forgiving than the second.
Relaxed Scoring uses a pairwise view of SBFs; for each block from an SBF, it counts the number of markers in that block that have at least one homolog within the SBF and normalizes it by the total number of markers present in the SBF.
Weighted Scoring attempts to quantify the deviation from our formal definition; for each block in an SBF, we count the number of markers in that block that have at least one homolog in each of the other blocks in the SBF and normalize this result by the number of blocks (minus 1) in the SBF and again by the total number of markers present in the SBF.
A perfect weighted score is 1, yet an SBF of n blocks with a weighted score of 1/(n 1) gets a perfect relaxed score.
These scores allow us to estimate the robustness of the homology state-ments, as they show how densely interconnected the SBs are through their homology statements.
A reduction from the first score to the second indicates that the tool has removed markers (to place them in other blocks) that fell within the blockso that the block produced is not contiguous.
Figure 5 gives histograms of the two measures for our experi-ments.
Since i-ADHoRe explicitly produces non-contiguous blocks, its two scores predictably differ significantly (by a third).
Like i-ADHoRe, DRIMM ignores many markers within a block, but in most cases it does not use them else-whereinstead, it eliminates them from the list of markers it uses.
As a result, its two base counts remain very close, but its two scores are very different.
Cyntenator and DRIMM yield similar distributions in both cases, but i-ADHoRE, which scores nearly perfectly under pair-wise scoring, scores poorly under weighted scoring.
i-ADHoRe does not place much emphasis on multi-way homologies: it keeps markers in its blocks even if these markers have just one hom-ology with one other block.
In contrast, Cyntenator progres-sively eliminates markers with few homology statements, therefore yielding blocks with strongly related markers.
DRIMM has much the same behavior under both scoring schemes, but its score drops by 80% when moving from pairwise to weighted scores, due to its dropping large numbers of markers from its working list.
That DRIMM scores poorly under both schemes, however, is due to a different set of goals: as stated by the authors, DRIMM aims at maximum genome coverage and simply ignores discordant homologies and other conditions that would cause Cyntenator or i-ADHoRe to break a block.
The yeast dataset contains several genes and ORFs that over-lap.
Such overlaps are discarded by DRIMM, but not by the other two tools; consequently, Cyntenator and i-ADHoRe occa-sionally output SBs with overlapping content (see Table 3).
Although we do not require collinearity, it remains desirable because it greatly simplifies the interpretation of the blocks.
Cyntenator makes this a formal constraint; in contrast, most of the blocks produced by DRIMM and i-ADHoRe are inter-rupted intervalsbetween the leftmost marker and the rightmost one, both tools pick and choose what to keep in the block.
The last column of Table 3 indicates the number of blocks affected by this selection.
The high proportion of blocks with selected con-tent explains in part the good scoring of i-ADHoRe.
In contrast, the very high proportion of such blocks, together with the 100% rate of homology violation, in DRIMM confirm the very Fig.4.
Histogram showing the percentage of markers from an SBF that do not have any homolog in that SBF.
The percentage is computed with respect to the total number of markers present in the SBF as generated by the tool and is supplemented by the E(X)/E(X0) ratio Table 3.
Characteristics of the SBFs generated by the tools SBFs w/o homologs in the SBF Content overlap Selective content DRIMM 509 509 0 455 Cyntenator 1106 583 39 0 i-ADHoRe 8088 278 2 7247 i15 Evaluating synteny for improved comparative studies syntenic blocks ( ) syntenic block `` '' that is, syntenic block Definition syntenic block--syntenic block &percnt; syntenic block `` '' different aim driving the tool.
A related issue is the handling of interchromosomal blocks: since genomic recombinations of vari-ous types can move parts of a conserved region to a different chromosome, one has to decide whether to split the conserved region into two SBs or to keep it as a single block.
Our definition requires a split, since it assumes that each block is contained within a chromosome; DRIMM and Cyntenator do the same, but i-ADHoRe allows blocks to span multiple chromosomes.
3.5 Quantifying the features of the blocks Comparing the blocks to each other is difficult, since explicit features of the blocks have not been defined a priori for any of the tools.
We chose to focus on three features: genome coverage in terms of used markers (the one measure commonly used in the original papers), overlap of blocks for each tool and agreement among blocks in terms of marker content.
We define marker coverage as the ratio of the total number of markers present in the blocks generated by a tool to the total number of markers present in the input within the generated block boundaries.
Figure 6 illustrates (qualitatively, not quantitatively) how the blocks generated by each tool cover a certain genomic area.
Figures 3 and 6 were generated using Circos (Krzywinski et al., 2009).
The three inner rings correspond to the three tools; each genome from our dataset corresponds to a cone in the figure, as indicated by the thin, labeled color indicator enclosing the diagram.
Block boundaries are drawn in thin black lines, so that dark areas represent short marker sets, thus small blocks and highly fragmented coverage.
Uncovered areas are white.
Our definition does not preclude using overlapping SBs, since it sets conditions on one SBF at a time.
In the lattice of decom-positions into SBFs, one may then choose to impose additional conditions to select good blocks.
DRIMM produces no overlap-ping blocks, because it does not reuse markers, whereas Cyntenator and (especially) i-ADHoRe do, which allows them to flag regions with ambiguous homologies or complex evolu-tionary histories.
Figure 7 illustrates the degree to which markers are reused by Cyntenator and i-ADHoRe.
While Cyntenator just reuses a few markers and not more than twice, i-ADHoRe reuses several of them up to 10 times, as depicted by the shape of the histograms.
We compute block similarity based on marker content: the markers of an SBF as generated by each tool are viewed as a single set and we compute the ratio between the overlap of two such sets relative to each of the sets, thereby yielding an asym-metric measure and six comparisons among the three tools.
Figure 8 shows that the distribution is skewed towards small valuesmost SBFs have a small overlap with other families.
Figure 8 also explains the types of links seen in Figure 3: most of the weight of the distribution is in the 1040% region, corres-ponding to overlaps with the many small blocks produced by i-ADHoRe and thus to the thin blue links of Figure 3, while the same small blocks are also responsible for the large spike at 100%, since many will completely overlap with the larger blocks.
4 DISCUSSION AND CONCLUSIONS We presented a review of the work to date on the definition and construction of SBs, pointing out the lack of a formal definition Fig.5.
Histograms of the two scores of Definition 5, illustrating the re-finement over the simple score used in Figure 4 Fig.6.
SBFs generated by DRIMM (inside ring), Cyntenator (middle ring) and i-ADHoRe (outside ring).
Each ring segment is a yeast genome.
Dark regions include many block boundariesthese SBFs have few markerswhile white regions have no identified SBFs.
Note the many contrasting outcomes from ring to ring: where one tool breaks a region into many small blocks, another produces a single block i16 C.G.Ghiurcuta and B.M.E.Moret syntenic block , [14] ) 3 3 l syntenic block ten 3 &percnt; syntenic block of SBs as well as the lack of clear objectives for the tools designed to construct these blocks.
The latter prevents us from evaluating each tool in terms of its own performance; the former prevents us from establishing a gold standard for evaluating the quality of SBs.
To remedy this situation, we proposed a simple set of homology-based criteria that SBs should satisfy.
These criteria do not identify unique solutionswe argued that a range of so-lutions should remain, since the specifics of the application should influence the selection of good blocks.
We based our definitions on homologies, because SBs are aimed at decompos-ing a genome into conserved regions (one of the few points on which all researchers agree) and conservation is embodied in homologies.
Since evaluating the quality of a decomposition into SBs is our main short-term goal, we defined new quality measures applic-able to all decompositions into SBs and applied them to the output of several synteny tools run on a dataset of eight yeast genomes.
This evaluation revealed very different behavior, as well as some reassuring commonalities, among the tools on the same dataset.
Almost all existing synteny tools use genes as markers.
Not only does such a choice restrict the usable range of granularity, but, at least in the case of most eukaryotic genomes, it discards most of the sequence data (close to 98% in the case of the human genome).
A sequence-based approach to the identification of markers, in the style of progressiveMauve or Sibelia, makes more sense in todays data environment.
Among choices that a user should be able to make are: (i) permissible degree of overlap of blocks; (ii) acceptable percentage of dropped markers; and (iii) granularity.
In addition, since the level of confidence in markers will vary, these choices should be further refined by taking into account the contribution of each shared, dropped or included marker.
Clearly, then, the next generation of tools needs a hier-archical organization of blocks, a measure of significance for blocks based on strong connections between markers in the same SBF, and user-defined (and application-motivated) con-straints and parameters.
Conflict of Interest: none declared.
Abstract Protein trafficking or protein sorting in eukaryotes is a complicated process and is carried out based on the information contained in the protein.
Many methods reported prediction of the subcellular location of proteins from sequence information.
However, most of these predic-tion methods use a flat structure or parallel architecture to perform prediction.
In this work, we introduce ensemble classifiers with features that are extracted directly from full length protein sequences to predict locations in the protein-sorting pathway hierarchically.
Sequence driven fea-tures, sequence mapped features and sequence autocorrelation features were tested with ensemble learners and their performances were compared.
When evaluated by independent data testing, ensemble based-bagging algorithms with sequence feature composition, transition and distribution (CTD) successfully classified two datasets with accuracies greater than 90%.
We compared our results with similar published methods, and our method equally performed with the others at two levels in the secreted pathway.
This study shows that the feature CTD extracted from protein sequences is effective in capturing biological features among compartments in secreted pathways.
Introduction Eukaryotic cells contain complex compartments called organ-elles enclosed within membranes.
Protein trafficking or protein sorting is a biological process where newly formed proteins get dan G).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A sorted and delivered to various organelles in the intracellular and secretory pathways [1].
Prediction of these protein locali-zation sites in the pathways from the full length amino acid sequence is a complex process, which has not been fully eluci-dated yet.
In 1982, Nishikawa et al.
[2] reported that amino acid composition correlates with localization sites and each localization site in a cell has a unique set of functions.
Hence protein localization prediction has implications both for the function of the protein and its possibility of interacting with other proteins in the same compartment [3,4].
Major protein sorting pathways can be divided hierarchi-cally into secretory and intracellular types [5,6].
In a secretory pathway, all non-secretory proteins are delivered to the endo-plasmic reticulum (ER) and then transported to other related cademy of Sciences and Genetics Society of China.
Production and hosting mailto:geetha@sctimst.ac.in386 Genomics Proteomics Bioinformatics 11 (2013) 385390 locations, which is controlled by ER signal sequences located in the N-termini.
On the other hand, in an intracellular path-way, proteins with organelle-specific signal sequences are im-ported into the nucleus or mitochondria, according to their signal sequence type.
The remaining proteins lacking sorting signals are located in the cytosol [7,8].
The success of computational prediction relies on the extraction of biological features from the sequence and the computational technique used [913].
A wide variety of meth-ods have been tried throughout the years in order to predict the subcellular localization of proteins from full length se-quence features.
Methods reported differ in terms of input data and the technique employed to make the prediction about subcellular location.
According to studies reported by Naka-shima and Nishawa [14], intracellular and secretory proteins differ significantly in their amino acid compositions and in res-idue pair frequencies.
Therefore, in this study simpler and less expensive methods that can extract features from full length protein sequence were given priority.
The main advantage of our feature extraction methods over existing techniques is that features are extracted from the full length protein sequence based on various coding schemes without referencing external databases.
For computation, we used hierarchical ensemble learning [1519] (Figure 1) by mimicking the protein trafficking phenomenon which is incorporated from the location descrip-tions provided by the Gene Ontology (GO) Consortium [20] with the sequence features as input.
Results and discussion Two basic ensemble based classifiers, bagging and AdaBoost M1 were trained to classify the location compartment of pro-teins in the intracellular and secretory pathways using the Wai-kato environment for knowledge analysis (WEKA) [21].
Two tests were carried out with two datasets for performance evalu-ation.
These include a 6-fold cross validation test, which means randomly partitioning the dataset into equally sized training and test sets, training on 5 sets and testing with the 6th set and averaging the results, and an independent data test, which means training on one set and testing with another set by divid-ing the dataset into two random groups.
The performance eval-uation parameters specificity (Sp), sensitivity (Sn), accuracy Level 0 Level 1 Level 2 Figure 1 Hierarchical structures of compartments in protein trafficking Adopted from [1519].
Level 0, root of hierarchy; Level 1, first division; Level 2, second division.
(Acc), Mathews correlation coefficient (MCC), positive predic-tive value (PPV), negative predictive value (NPV) and receiver operating characteristic (ROC) were calculated at all levels for comparing our results with the published results.
Tables S1 and S2 show the average of the classifier perfor-mance parameters obtained from the two datasets at various levels of the pathway hierarchy in 6-fold cross validation and independent data test.
These results were compared with the similar work of LOCtree [15] in Table S3.
Table S4 shows the comparison of our classifier performance parameters with the LocTree2 [16] dataset for 5-fold cross validation.
Comparison with existing methods Our method provides a hierarchical system for the prediction of protein subcellular localization with features generated exclu-sively from the full length sequence without using any server generated inputs.
Similar classification work was reported by LOCtree [15] and LocTree2 [16].
LOCtree used the amino acid composition (20 units), composition of the 50 N-terminal resi-dues (20 units) , amino acid composition from three secondary structure states and SignalP server [22] outputs as a feature vec-tor on a support vector machine, whereas LocTree2 used the profiles created by BLAST-ing [23].
Although the results reported by LOCtree [15] are not di-rectly comparable to ours in terms of features, selection of data, sizing of the data, and method of accuracy calculation, PPV, NPV and MCC reported by our method proved to be better at Level 0 and Level 1 of the hierarchy in the secreted pathway.
The overall accuracy mentioned in LOCtree [15] is the PPV re-sult based on the 6-fold cross validation experiments from a sin-gle dataset.
At Level 0, our independent data testing results based on AdaBoost M1 and bagging reported average accura-cies above 95% (Table S3) between the intracellular and secre-tory pathways with four of the sequence features.
Bagging reported accuracy above 91% for classifying proteins between the secretory and organelle pathways with independent data testing.
Because there is no result published for independent data tests by LOCtree [15], results obtained by this method can-not be compared.
For the 6-fold cross validation test (Table S3), our method reported accuracies above 92% at Level 0 for both bagging and AdaBoost M1 with an average MCC of 0.87, which was reported as 0.73 when using the LOCtree method.
At Level 1, AdaBoost M1 and bagging reported PPVs above 90% with MCC above 0.70 while LOCtree reported an MCC of 0.55.
Classifier bagging with sequence feature CTD performed bet-ter than LOCtree in differentiating the cytoplasm and mito-chondrial pathways at Level 2.
LocTree2 is developed using a different hierarchical path-way and hence we could do the testing only for two levels using a LocTree2 dataset under 5-fold cross validation.
Our method reported accuracies above 88% at Level 0 (Table S4) for all fea-tures under bagging while LocTree2 reported 90%.
For level 1, bagging with feature vector CTD reported an accuracy of 82%, which is also comparable to that reported by LocTree2, 83%.
Conclusion Previous protein localization prediction methods have been implemented using standard machine learning algorithms with Govindan G and Nair AS/ Hierarchical Prediction of Secreted Protein Trafficking 387 parallel architecture as a common practice in computer science [2426].
Here novel systems of ensemble learners using hierar-chical architecture from features extracted directly from full length protein sequences that can predict localization have been tested and the results have been compared.
Our testing results at the secretory pathway of hierarchy show that the prediction accuracy can be significantly im-proved by using the classifier bagging with feature vector CTD.
The system achieved an overall accuracy above 90% with this sequence signature using bagging on independent data tests, suggesting that the native protein localization for each compartment is imprinted onto the features extracted from protein sequence.
Feature generation methods described in this paper works independently and no server/external data reference is required for its extraction.
Methods are based on the composition of amino acid.
Additionally, this hierarchical structure has provided insights into the sorting process, such as the accurate distinction between the intracellular and secretory pathways.
However, we observed that, as one descends the hierarchical path, the prediction accuracy progressively de-creases as the classification task complexity increases.
The best scoring decisions reported are at the top, and the worst are at the bottom.
Thus, hierarchical model classification is unable to correct a prediction mistake made at the top node.
This study supports the hypothesis reported by Nakashima and Nishawa [14] that intracellular and secretory proteins dif-fer significantly in their amino acid compositions.
Both classi-fiers performed well using three sequence features at the top levels of hierarchy.
In the future, this classification method could be potentially extended to any level in the hierarchy using these sequence fea-tures and with the location descriptions provided by the Gene Ontology Consortium [20].
This method can predict the final localization of the protein as well as the mechanism underlying such localization.
Our result may aid the development of more accurate predictors of protein function.
G C A T G G T G C G A A A C T T T G G C T G Zero skip-c0TG= 4, c0GC=3, c0AT=1 G C A T G G T G C G A A A C T T T G G C T G One skip-c1TG =3, c1GC =1, c1AT =1 G C A T G G T G C G A A A C T T T G G C T G Two skips-c2TG=3, c2GC =1, c2AT =2 Figure 2 Amino acid di-peptide (GC, TG, AT) count with skips in a sample sequence c0 indicates count of dipeptides with zero skip, c1 indicates count of dipeptides with one skip and c2 indicates count of dipeptides with two skips.
Materials and methods Dataset construction Two datasets (Table S5) were compiled for this study, which are denoted as ASN_G 1756 (Human) and ASN_G 1008 (Eukaryote).
ASN_G (Human) is collected from a manually curated database for the subcellular localizations of proteins in human [27] and ASN_G (Eukaryote), which is from eSLDB [17], is a database for eukaryotic organisms.
These are the only two manually curated public databases with experimental annotations reported in www.psort.org [28] for eukaryotes.
ASN_G (Human) and ASN_G (Eukaryote) is maintained by the Rost lab of Columbia University Bioinformatics Centre and the Bologna Biocomputing Group, University of Bologna, respectively.
These experimentally annotated proteins were finalized by verifying with UniProt (www.uniprot.org, release 2011-02 SeptOct) and by selecting the sequences that had a determined single subcellular location.
Entries in the subcellu-lar location that were annotated as putative, potential, possible and by similarity were eliminated to remove se-quences with ambiguous and uncertain annotations.
We used the Cluster Database at High Identity with Toler-ance (CD-HIT-2D) [29] web server to eliminate sequences in both datasets that displayed a similarity greater than or equal to 30%.
The program (CD-HIT) takes a fasta format sequence database as input and produces a set of non-redundant repre-sentative sequences as output by removing the highly similar sequences.
For comparing our results with the LocTree2, we down-loaded 1682 sequences from the LocTree2 publication site [16] and generated a dataset with 1677 sequences (Table S7) after verifying the subcellular localizations with UniProt (March 2013).
Sequence feature formation The features extracted from protein full length sequence can be classified into three groups.
The first group consists of se-quence driven features, which are generated directly from se-quence through converting the protein sequence into a numeric sequence by replacing each amino acid with equiva-lent numeric values, counts, etc.
The second group consists of sequence mapped features, which are generated by mapping amino acids into sub groups and the third group contains se-quence autocorrelation features, which are obtained from cal-culations based on three types of spatial autocorrelation (Moreau-Broto, Moran and Geary).
Sequence driven features There are two composition features considered, which include amino acid dipeptide composition (dipeptide descriptors) and composition of physico-chemical properties (amino acid in-dex).
Properties of dipeptides are determined by the amino acids forming the dipeptide.
Dipeptide composition, which gives a fixed pattern length of 400 (20 20), encapsulates the global information about each protein sequence and the order it contains [30].
For example, in the sample protein sequence GCATGGTGCGAAACTTTGGCTG, 400 pairs of dipeptide occurrence frequency with no skips c0, are calculated by count-ing its presence in the sequence with no gaps.
In Figure 2, the count of c0GC is 3, one skip c1GC is 1 and two skips c2GC is 1.
The dipeptide count, cNxx, counts pairs with N skips between them.
The feature vector using the dipeptide occurrence fre-quency count for a protein sequence is represented as three separate numeric counts of its dipeptide c0, c1 and c2, each having 400 components.
The final feature vector of 1200 com-388 Genomics Proteomics Bioinformatics 11 (2013) 385390 ponents is formed by concatenating the corresponding vectors c0, c1 and c2.
The Amino Acid Index (AAindex-1,2,3) is a database of numerical indices representing various physico-chemical and biochemical properties of amino acids and pairs of amino acids [31].
Physico-chemical properties derived from the AAindex1 database having 544 indices are used to compute the features.
Feature vector having 544 components is represented as {f1 f2 f3 .
.
.
f544} where f1 is the physico-chemical property value for all residues of the sequence divided by the length of the sequence.
Sequence mapped features (CTD descriptors) Structural variation in the R groups of amino acids is con-sidered as the main factor for its difference in properties.
From side chains we can classify amino acids into four groups (1) non-polar and neutral, (2) polar and neutral, (3) acidic and polar, and (4) basic and polar.
The 20 amino acids forming the protein sequence can also be divided into several groups based on their other properties like (5) charge, (6) hydrophilicity or hydrophobicity, (7) size, and (8) functional groups.
Twenty amino acids can be mapped into 13 groups by replacing each amino acid code with its group code.
From the mapped sequence, features called composition, transition and distribution (CTD) can be calcu-lated.
Composition is determined as the number of amino acids of a particular property divided by total number of amino acids, whereas transition is determined as the number of transition from a particular property to different property divided by (total number of amino acids 1).
Distribution is the chain length within which the first, 25%, 50%, 75% and 100% of the amino acids of a particular property are located.
According to the property types, amino acids are divided into three groups and are marked as numeric indices 1, 2 and 3 (Table S6).
Properties whose attributes can be grouped perfectly into three sets like charge, hydrophobicity, normalized van der Waals volume, polarity, polarizability, secondary structure and solvent accessibility are used for this mapping [3235].
For example, according to secondary structure property grouping, the sample protein sequence HEAMRQLTIFVCYWNSPDDG is coded as 222222233 33333111111.
In this example with the property of second-ary structure, the total count of the coil is 6, the helix is 7 and the strand is 7.
Hence the composition is calculated as 6/20, 7/20 and 7/20, where 20 is the total length of the se-quence.
Three numbers of composition descriptors are formed from three groups.
The transition from class 1 to 2 is the percentage frequency with which class 1 is followed by class 2 or class 2 is followed by class 1 in the encoded sequence, likewise the transition from class 3 to class 1 or class 1 to class 3, etc.
For the sample se-quence, the sum of transition from 2 to 3 and 3 to 2 is 1.
Hence transition = 1/19.
The distribution descriptor describes the distribution of each property in the sequence.
Five distribution descriptors are formed for each group, including the position percentages in the sequence for the first residue, 25% of the residues, 50% of the residues, 75% of the residues and 100% of the residues.
Fifteen distribution descriptors are formed from three groups.
In total 21 CTD descriptors are formed from a sequence.
For this study, CTD calculation is performed for 7 proper-ties for each protein sequence after dividing each sequence into three equal segments.
In total, 21 3 attributes for a sequence and 441 attributes for 7 properties compose the final feature vector.
Sequence autocorrelation features (autocorrelation descriptors) Sequence autocorrelation-based features are based on the To-blers first law of geography everything is related to every-thing else but nearby things are more related than distant things [36].
Sequence autocorrelation-based features also as-sume that the disturbances in each area are systematically re-lated to those in adjacent areas [37].
Spatial autocorrelation is the correlation of the variable with itself through space.
Spatial autocorrelation measures the degree to which near and distant things are related, which is positive when nearby things are similar and negative when they are dissimilar.
This concept helps to analyze the dependency among the features of se-quences in each location.
Autocorrelation features are calculated based on the distri-bution of amino acid properties along the sequence.
Thirty nine amino acid indices related to hydrophobicity are used for calculation after replacing each amino acid with its equiv-alent normalized index as Pi.
Three autocorrelation descriptors are used as features, including normalized Moreau-Broto auto-correlation descriptors [38], Moran auto-correlation descrip-tors [39] and Geary autocorrelation descriptors [40].
The Moreau-Broto autocorrelation descriptor is defined as MBd XN-d i1 PiPid where d 1; 2; 3 upto Max:lag where d is the lag of the autocorrelation, N is the length of the sequence, and Pi and Pi+d are the amino acid index value of the selected property at position i and i + d, respectively.
Max.lag is the maximum value of the lags.
The normalized Moreau-Broto autocorrelation descriptors are defined as MB(d)/(N d).
The Moran autocorrelation descriptor is defined as Moran d 1 N-d PN-d i1Pi PPid P 1 N PN i1Pi P 2 d 1; 2; 3 .
.
.
; 30 P PN i1Pi N where Pi and Pi+d have the same meaning as above.
The Geary autocorrelation descriptor is defined as Geary d 1 2N-d PN-d i1Pi Pid 2 1 N1 PN i1Pi P 2 d 1; 2; 3 .
.
.
; 30: where P, Pi and Pi+d have the same meaning as above.
3510 attributes from 39 amino acid properties with 30 lags compose the sequence feature vector for autocorrelation.
Computational techniques used Among prediction algorithms, ensemble learning is a process by which multiple models such as classifiers are generated and combined to improve overall prediction accuracy [41].
Multiple learners (base learners) are trained to solve the same Govindan G and Nair AS/ Hierarchical Prediction of Secreted Protein Trafficking 389 problem by averaging over multiple classification models with different input feature vectors.
These ensemble techniques re-duce the small sample size problem which is critical in biolog-ical applications.
This method reduces the over fitting of data.
The three most popular classifiers based on the ensemble meth-od, are bagging [42], AdaBoost M1 [43] and Random Forest [44].
In this study, two methods bagging and AdaBoost were used to predict protein trafficking at all levels of protein sort-ing pathway.
Bagging is the name derived from bootstrap aggregation.
This method uses multiple versions of a training set on differ-ent models by using the bootstrap (sampling with replace-ment).
The outputs of the models are combined (average or vote) to create a single output.
AdaBoost M1 adopts an adap-tive sampling by using all instances of each iteration.
In bag-ging, each classifier has the vote of the same strength, whereas AdaBoost M1 assigns different voting strengths to classifiers based on their accuracy.
Performance evaluation parameters The classifier performance evaluation parameters specificity, sensitivity, accuracy, MCC [45], PPV [46], NPV [46] and ROC [47] were calculated at all levels as per the below equa-tions.
Specificity (Sp) is determined as (TN)/(TN + FP), where TN indicates true negative and FP means false positive.
Sensi-tivity is defined as (TP)/(TP + FN), where TP means true po-sitive and FN means false negative.
Accuracy is defined as (TP + TN)/(TP + TN + FP + FN).
PPV and NPV is calcu-lated as (TP)/(TP + FP) and (TN)/(TN + FN), respectively.
MCC is calculated as TPTNFPFN sqrtTPFNTPFPTNFNTNFP.
Authors contributions GG collected the dataset, conducted the data analysis, did ma-chine learning experiments and wrote the manuscript.
ASN conceived the original idea of using ensemble classifiers for the prediction of protein localization hierarchically.
Both authors read and approved the final manuscript.
Competing interests The authors declared that no competing interests exist.
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.2013.0 7.005.
Abstract Summary: Network pharmacology-based prediction of multi-targeted drug combinations is becoming a promising strategy to improve anticancer efficacy and safety.
We developed a logic-based network algorithm, called Target Inhibition Interaction using Maximization and Minimization Averaging (TIMMA), which predicts the effects of drug combinations based on their binary drug-target interactions and single-drug sensitivity profiles in a given cancer sample.
Here, we report the R implementation of the algorithm (TIMMA-R), which is much faster than the original MATLAB code.
The major extensions include modeling of multiclass drug-target profiles and network visual-ization.
We also show that the TIMMA-R predictions are robust to the intrinsic noise in the experi-mental data, thus making it a promising high-throughput tool to prioritize drug combinations in various cancer types for follow-up experimentation or clinical applications.
Availability and implementation: TIMMA-R source code is freely available at http://cran.r-project.
org/web/packages/timma/.
Contact: jing.tang@helsinki.fi Supplementary information: Supplementary data are available at Bioinformatics online.
1 Introduction Recently, a network pharmacology paradigm for anticancer drug discovery has been proposed, with an aim of developing multi-tar-geted drug combinations that consist of distinct chemical agents as a promising strategy to improve treatment efficacy and safety (Al-Lazikani et al., 2012).
Most mathematical models for predicting the effects of drug combinations relies on empirical cellular dynamic models, such as those based on signaling pathways or metabolic net-works.
These types of methods are hardly applicable for predicting cancer-selective drug combinations due to lack of accurate kinetic parameters for each cancer type (Sun et al., 2013).
Other methods use the transcriptional responses of drugs, i.e.
gene expression pro-files before and after drug treatments, to predict drug combinations (Zhao et al., 2014).
However, such drug-induced transcriptional phenotypes are not routinely profiled in a typical high-throughput drug screen, and thus provide limited translation potential in the clinical settings.
We have developed a logic-based network pharmacology ap-proach Target Inhibition Interaction using Maximization and Minimization Averaging (TIMMA) for predicting the effects of drug combinations in a given cancer cell sample (Tang et al., 2013).
TIMMA combines drug-target interaction networks with single-drug sensitivity profiles, derived either from individual cell line mod-els (Barrettina et al.
2012) or from patient-derived cell samples (Pemovska et al.
2013).
The network algorithm starts by searching a set of combinatorial targets that are most predictive of the single-drug sensitivities.
A drug combination is then treated as a combin-ation of target inhibitions, the effect of which can be estimated based on the set relationships with the target profiles of the drugs.
The outcome of the TIMMA model provides a list of predicted VC The Author 2015.
Published by Oxford University Press.
1866 This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Bioinformatics, 31(11), 2015, 18661868 doi: 10.1093/bioinformatics/btv067 Advance Access Publication Date: 31 January 2015 Applications Note TIMMA (synergy scores for drug combinations, from which a target inhib-ition network can be inferred.
To enable wider applications of the method, several major limi-tations need to be overcome.
First, the original TIMMA package was written in MATLAB, the accessibility of which in biomedical re-search community is rather limited compared with the open source R environment.
Second, for applying the target set comparison, the drug-target profiles must be binarized, encoding 1 for a true target and 0 for a non-target.
A typical drug-target profiling assay, how-ever, often reveals quantitative polypharmacological interactions more complex than what such binary data can capture.
Third, the top-ology of the target inhibition networks was derived manually from the model predictions.
The lack of efficient network reconstruction algorithms may become the bottleneck for more straightforward bio-logical interpretations when the network sizes increase.
These issues are now addressed in the newly developed R implementation.
2 Implementation The TIMMA-R workflow starts by preparing two types of input data (Fig.1).
To maximize the prediction power, the first input de-fines the drugs polypharmacological profiles by considering both strong and weak drug-target interactions, so that the effect of a drug combination can be modeled through its (multiple) target inter-actions.
The proteome-wide quantitative drug-target interaction data are available in PubChem, ChEMBL (Gaulton et al., 2012) or CanSAR (Halling-Brown et al., 2012) databases.
To minimize the false positive or negative interactions, bioactivity data integration from multiple studies is advised (Tang et al., 2014).
The second in-put data involves drug sensitivity profiles for a given cancer cell line, which can be obtained e.g.
from the Cancer Cell Line Encyclopedia (CCLE) (Barretina et al., 2012), the Genomics of Drug Sensitivity in Cancer (GDSC) (Yang et al., 2013) or the Library of Integrated Network-based Cellular Signatures (LINCS) (Vempati et al., 2014) databases.
However, the TIMMA-R is not limited to using cell line data only, but can also be applied to patient-derived drug sensitivity profiles (Pemovska et al., 2013).
In the following, we briefly report the major updates of the algorithm that will facilitate its usage for high-throughput drug combination discoveries.
2.1 Computational speed With the techniques of vectorization and multi-dimensional matrix computation, the TIMMA-R algorithm has been made more effi-cient and scalable for large-scale analyzes.
For a typical data, con-sisting of 50 drugs and 500 targets, the average running time on a desktop computer is <50 s, compared with >250 s when using the original MATLAB implementation.
Note that the current TIMMA-R aggregates the information from each drug sequentially.
A straightforward extension would be thus parallel computing using multiple processors, e.g.
one processor for one drug, which can fur-ther increase the computational speed.
2.2 Sensitivity analysis The traditional definition of a drugs target, originated from the magic bullet paradigm, often considers one or two primary targets that are thought to induce the therapeutic effects (Hopkins, 2008).
However, recent proteome-wide bioactivity studies have revealed much more low-affinity, multi-targeted drugs than previously thought (e.g.
Davis et al., 2011).
An important question for any pol-ypharmacological modeling method is therefore its robustness with respect to experimental uncertainties in drug-target interactions.
We performed a sensitivity analysis using simulated binary drug-target interaction data with 50 drugs and 100 targets, where experi-mental noise was modeled by flipping either from 0 to 1 (false posi-tive) or vice versa (false negative), for up to 30% of the drug-target interactions.
The prediction results between the selected target sets before and after the flipping were compared using the RV coefficient (Le et al., 2008).
The rate of positive interactions in the data were set at 0.3.
The simulation was repeated 100 times and summarized in Figure 2.
As expected, the RV coefficient decreased as the false interaction rates increase, yet being significantly higher than the ran-dom predictions (P<0.001, paired t-test).
These results indicate that TIMMA-R is applicable for those real case studies, where the drug-target interaction data has been either experimentally validated or manually curated.
2.3 Modeling of multi-class drug-target data Compared with the conventional drug-target binarization, a more realistic drug-target modeling classifies the targets of a given drug into several classes in terms of their binding affinities (Tyner et al., 2013).
TIMMA-R modeling of such multi-class data are straightfor-ward, as the set relationships still hold between two drugs, except that the number of possible scenarios for a target combination goes from 2n to pn; where n is the number of targets and p is the number of interactions classes.
Even though the target combination space in-creases, the prediction accuracy of TIMMA-R under the categorical setting stays at the same level as using the binary data (Supplementary Material).
On the other hand, we found that introducing more drug-target classes may not always lead to better prediction accuracy.
This may be due to the multi-classification scheme of Tyner et al.
(2013), where weak interactions, such as Kd or IC50 values close to 10 lM, were considered as one of the active classes.
Such a classification might be sub-optimal for characterizing the response of patient-derived samples given that the majority of the drugs in vivo or ex vivo efficacy is expected to be elicited via their targets with nanomolar potency.
Given that the drug-target data are already sparse in the binary case, we do not rec-ommend over-interpreting the drug-target interactions with more than three classes (see Supplementary Material for more detailed discussion).
Fig.1.
The TIMMA-R workflow Fig.2.
Robustness of TIMMA-R predictions using simulated drug-target inter-actions with a varying degree of false negative or false positives.
y-axis: aver-age RV coefficient, where error bars indicate standard error of the means; x-axis: false positive rate or false negative rate; Dotted trace, random prediction An R package for predicting synergistic multi-targeted drug combinations 1867 to ly-implementation ure l s less than  seconds to more than  seconds ., was p< TIMMAR to is2.4 Network reconstruction The TIMMA-R predictions can be formulated as a complete truth table, based on which the minimized Boolean expression is deter-mined by the enhanced Quine-McCluskey algorithm using the Qualitative Comparative Analysis (QCA) package in R (Dusa, 2010).
The minimized Boolean expression is a union of target com-bination scenarios leading to the same (maximal) response.
For net-work visualization, we utilized a two-terminal graph, with series and parallel components, to represent the predicted response of can-cer survival signaling pathways (see Supplementary Material).
With the network reconstruction algorithm, TIMMA-R is able to produce illustrative visualizations that facilitate the interpretation of the drug combination predictions.
The nested network format enables net-work visualization and analysis in Cytoscape.
5 Conclusion The TIMMA-R package will facilitate effective integration of drug-target and drug sensitivity profiles which are becoming increas-ingly available for the anticancer research.
The package does not only offer an efficient prioritization tool for selecting the most promising drug combinations for further experimental testing, but also produces data-driven hypotheses for their target interactions.
Such network-centric investigation of the underlying mechanisms of action in the given cellular context will greatly improve our knowledge about the functional cross-talks between parallel cancer signaling pathways to be utilized in anticancer treatment applications.
Funding Academy of Finland (Grants 272437, 269862, 279163); the Biocentrum Helsinki and the Cancer Society of Finland.
Conflict of Interest: none declared.
ABSTRACT Motivation: Next-generation DNA sequencing machines are generating an enormous amount of sequence data, placing unprecedented demands on traditional single-processor read-mapping algorithms.
CloudBurst is a new parallel read-mapping algorithm optimized for mapping next-generation sequence data to the human genome and other reference genomes, for use in a variety of biological analyses including SNP discovery, genotyping and personal genomics.
It is modeled after the short read-mapping program RMAP, and reports either all alignments or the unambiguous best alignment for each read with any number of mismatches or differences.
This level of sensitivity could be prohibitively time consuming, but CloudBurst uses the open-source Hadoop implementation of MapReduce to parallelize execution using multiple compute nodes.
Results: CloudBursts running time scales linearly with the number of reads mapped, and with near linear speedup as the number of processors increases.
In a 24-processor core configuration, CloudBurst is up to 30 times faster than RMAP executing on a single core, while computing an identical set of alignments.
Using a larger remote compute cloud with 96 cores, CloudBurst improved performance by >100-fold, reducing the running time from hours to mere minutes for typical jobs involving mapping of millions of short reads to the human genome.
Availability: CloudBurst is available open-source as a model for parallelizing algorithms with MapReduce at http://cloudburst-bio.sourceforge.net/.
Contact: mschatz@umiacs.umd.edu 1 INTRODUCTION Next-generation high-throughput DNA sequencing technologies from 454 Life Sciences, Illumina,Applied Biosystems and others are changing the scale and scope of genomics.
These machines sequence more DNA in a few days than a traditional Sanger sequencing machine could in an entire year, and at a significantly lower cost (Shaffer, 2007).
James Watsons genome was recently sequenced (Wheeler et al., 2008) using technology from 454 Life Sciences in just 2 months, whereas previous efforts to sequence the human genome required several years and hundreds of machines (Venter et al., 2001).
If this trend continues, an individual will be able to have their DNA sequenced in only a few days and perhaps for as little as $1000.
To whom correspondence should be addressed.
The data from the new machines consists of millions of short sequences of DNA (25250 bp) called reads, collected randomly from the target genome.
After sequencing, researchers often map the reads to a reference genome to find the locations where each read occurs, allowing for a small number of differences.
This information can be used to catalog differences in one persons genome relative to a reference human genome, or compare the genomes of closely related species.
For example, this approach was recently used to analyze the genomes of an African (Bentley et al., 2008) and an Asian (Wang et al., 2008) individual by mapping 4.0 and 3.3 billion 35 bp reads, respectively, to the reference human genome.
These comparisons are used for a wide variety of biological analyses including SNP discovery, genotyping, gene expression, comparative genomics and personal genomics.
Even a single base pair difference can have a significant biological impact, so researchers require highly sensitive mapping algorithms to analyze the reads.
As such, researchers are generating sequence data at an incredible rate and need highly scalable algorithms to analyze their data.
Many of the currently used read-mapping programs, including BLAST (Altschul et al., 1990), SOAP (Li et al., 2008b), MAQ (Li, et al., 2008a), RMAP (Smith et al., 2008) and ZOOM (Lin et al., 2008), use an algorithmic technique called seed-and-extend to accelerate the mapping process.
These programs first find sub-strings called seeds that exactly match in both the reads and the reference sequences, and then extend the shared seeds into longer, inexact alignments using a more sensitive algorithm that allows for mismatches or gaps.
These programs use a variety of methods for finding and extending the seeds, and have different features and performance.
However, each of these programs is designed for execution on a single computing node, and as such requires a long running time or limits the sensitivity of the alignments they find.
CloudBurst is a new highly sensitive parallel seed-and-extend read-mapping algorithm optimized for mapping single-end next generation sequence data to reference genomes.
It reports all alignments for each read with up to a user-specified number of differences including both mismatches and indels.
CloudBurst can optionally filter the alignments to report the single best non-ambiguous alignment for each read, and produce output identical to RMAPM (RMAP using mismatch scores).
As such CloudBurst can replace RMAP in a data analysis pipeline without changing the results, but provides much greater performance by using the open-source implementation of the distributed programming framework MapReduce called Hadoop (http://hadoop.apache.org).
The results presented below show that CloudBurst is highly scalable: the running times scale linearly as the number of reads increases, and with near linear speed improvements 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1364 13631369 M.C.Schatz Fig.1.
Schematic overview of MapReduce.
The input file(s) are automatically partitioned into chunks depending on their size and the desired number of mappers.
Each mapper (shown here as m1 and m2) executes a user-defined function on a chunk of the input and emits keyvalue pairs.
The shuffle phase creates a list of values associated with each key (shown here as k1, k2 and kn).
The reducers (shown here as r1 and r2) evaluate a user-defined function for their subset of the keys and associated list of values, to create the set of output files.
over a serial execution of RMAP for sensitive searches.
Furthermore, CloudBurst can scale to run on large remote compute clouds, and thus map virtually any number of reads with high sensitivity in relatively little time.
1.1 MapReduce and Hadoop MapReduce (Dean et al., 2008) is the software framework developed and used by Google to support parallel distributed execution of their data intensive applications.
Google uses this framework internally to execute thousands of MapReduce applications per day, processing petabytes of data, all on commodity hardware.
Unlike other parallel computing frameworks, which require application developers explicitly manage inter-process communication, computation in MapReduce is divided into two major phases called map and reduce, separated by an internal shuffle phase of the intermediate results (Fig.1), and the framework automatically executes those functions in parallel over any number of processors.
The map function computes keyvalue pairs from the input data, based on any relationship applicable to the problem, including computing multiple pairs from a single input.
For example, the map function of a program that counts the number of occurrences of all length k substrings (k-mers) in a set of DNA sequences could emit the keyvalue pair (k-mer, 1) for each k-mer.
If the input is large, many instances of the map function can execute in parallel on different portions of the input and divide the running time by the number of processors available.
Once the mappers are complete, MapReduce shuffles the pairs so all values with the same key are grouped together into a single list.
The grouping of keyvalue pairs effectively creates a large distributed hash table indexed by the key, with a list of values for each key.
In the k-mer counter example, the framework creates a list of 1s for each k-mer in the input, corresponding to each instance of that k-mer.
The reduce function evaluates a user-defined function on each keyvalue list.
The reduce function can be arbitrarily complex, but must be commutative, since the order of elements in the keyvalue list is unstable.
In the k-mer counting example, the reduce function is called once for each k-mer with its associated list of 1s, and simply adds the 1s together to compute the total number of occurrences for that k-mer.
Each instance of the reduce function executes independently, so there can be as many reduce functions executing in parallel as there are distinct keys, i.e.
k-mers in the input.
As an optimization, MapReduce allows reduce-like functions called combiners to execute in-memory immediately after the map function.
Combiners are not possible in every application because they evaluate on a subset of the values for a given key, but when possible, reduce the amount of data processed in the shuffle and reduce phases.
In the k-mer counting example, the combiner emits a partial sum from the subset of 1s it evaluates, and the reduce function sums over the list of partial sums.
Computations in MapReduce are independent, so the wall clock running time should scale linearly with the number of processor cores available, i.e.
a 10-core execution should take 1/10th the time of a 1-core execution creating a 10 speedup with complete parallel efficiency.
In practice, perfect linear speedup is difficult to achieve because serial overhead limits the maximum speedup possible as described by Amdahls law (Krishnaprasad, 2001).
For example, if an application has just 10% non-parallelizable overhead, then the maximum possible end-to-end speedup is only 10 regardless of the number of cores used.
High speedup also requires the computation is evenly divided over all processors to maximize the benefit of parallel computation.
Otherwise the wall clock running time will be limited to the time for the longest running task, and reduce overall efficiency.
MapReduce tries to balance the workload by assigning each reducer 1/N of the total key space, where N is the number of cores.
If certain keys require substantially more time than others, however, it may be necessary to rebalance the workload using a custom partition function or adjusting how keys are emitted.
MapReduce is designed for computations with extremely large datasets, far beyond what can be stored in RAM.
Instead it uses files for storing and transferring intermediate results, including the inter-machine communication between map and reduce functions.
This could become a severe bottleneck, so Google developed the robust distributed Google File System (GFS) (Ghemawat et al., 2003) to efficiently support MapReduce.
GFS is designed to provide very high-bandwidth for MapReduce by replicating and partitioning files across many physical disks.
Files in the GFS are automatically partitioned into large chunks (64 MB by default), which are replicated to several physical disks (three by default) attached to the compute nodes.
Therefore, aggregate I/O performance can greatly exceed the performance of an individual memory storage device (e.g.
a disk drive), and chunk redundancy ensures reliability even when used with commodity drives with relatively high-failure rates.
MapReduce is also data aware: it attempts to schedule computation at a compute node that has the required data instead of moving the data across the network.
Hadoop and the Hadoop Distributed File System (HDFS) are open source versions of MapReduce and the GFS implemented in Java and sponsored by Amazon, Yahoo, Google, IBM and other major vendors.
Like Googles proprietary MapReduce framework, applications developers need only write custom map and reduce functions, and the Hadoop framework automatically executes those functions in parallel.
Hadoop and HDFS are used to manage production clusters with 10 000 + nodes and petabytes of data, including computation supporting every Yahoo search result.
A Hadoop cluster of 910 commodity machines recently set a performance record by sorting 1 TB of data (10 billion 100 bytes records) in 209 s (http://www.hpl.hp.com/hosted/sortbenchmark/).
1364 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1365 13631369 CloudBurst In addition to in-house Hadoop usage, Hadoop is becoming a de facto standard for cloud computing where compute resources are accessed generically as a service, without regard for physical location or specific configuration.
The generic nature of cloud computing allows resources to be purchased on-demand, especially to augment local resources for specific large or time-critical tasks.
Several organizations offer cloud compute cycles that can be accessed via Hadoop.
Amazons Elastic Compute Cloud (EC2) (http://aws.amazon.com) contains tens of thousands of virtual machines, and supports Hadoop with minimal effort.
In EC2, there are five different classes of virtual machines available providing different levels of CPU, RAM and disk resources with price ranging from $0.10 to $0.80 per hour per virtual machine.
Amazon offers preconfigured disk images and launches scripts for initializing a Hadoop cluster, and once initialized, users copy data into the newly created HDFS and execute their jobs as if the cluster was dedicated for their use.
For very large datasets, the time required for the initial data transfer can be substantial, and will depend on the bandwidth of the cloud provider.
Once transferred into the cloud, though, the cloud nodes generally have very high-internode bandwidth.
Furthermore, Amazon has begun mirroring portions of Ensembl and GenBank for use within EC2 without additional storage costs, thereby minimizing the time and cost to run a large-scale analysis of these data.
1.2 Read mapping After sequencing DNA, researchers often map the reads to a reference genome to find the locations where each read occurs.
The read-mapping algorithm reports one or more alignments for each read within a scoring threshold, commonly expressed as the minimal acceptable significance of the alignment, or the maximum acceptable number of differences between the read and the reference genome.
The algorithms generally allow 110% of the read length to differ from the reference, although higher levels may be necessary when aligning to more distantly related genomes, or when aligning longer reads with higher error rates.
Read-mapping algorithms can allow mismatch (mutation) errors only, or they can allow insertion or deletion (indel) errors, for both true genetic variations and artificial sequencing errors.
The number of mismatches between a pair of sequences can be computed with a simple scan of the sequences, whereas computing the edit distance (allowing for indels) requires a more sophisticated algorithm such as the Smith Waterman sequence alignment algorithm (Smith et al., 1981), whose runtime is proportional to the product of the sequence lengths.
In either case, the computation for a single pair of short sequences is fast, but becomes costly as the number or size of sequences increases.
When aligning millions of reads generated from a next-generation sequencing machine, read-mapping algorithms often use a technique called seed-and-extend to accelerate the search for highly similar alignments.
This technique is based on the observation that there must be a significant exact match for an alignment to be within the scoring threshold.
For example, for a 30 bp read to map to a reference with only one difference, there must be at least 15 consecutive bases, called a seed, that match exactly regardless of where the difference occurs.
In general, a full-length end-to-end alignment of an m bp read with at most k differences must contain at least one exact alignment of m/(k+1) consecutive bases (Baeza-yates et al., 1992).
Similar arguments can be made when designing spaced seeds of non-consecutive bases to guarantee finding all alignments with up to a certain numbers of errors (Lin et al., 2008).
Spaced seeds have the advantage of allowing longer seeds at the same level of sensitivity, although multiple spaced seeds may be needed to reach full sensitivity.
In all seed-and-extend algorithms, regions that do not contain any matching seeds are filtered without further examination, since those regions are guaranteed to not contain any high-quality alignments.
For example, BLAST uses a hash table of all fixed length k-mers in the reference to find seeds, and a banded version of the Smith Waterman algorithm to compute high-scoring gapped alignments.
RMAP uses a hash table of non-overlapping k-mers of length m/(k+1) in the reads to find seeds, while SOAP, MAQ and ZOOM use spaced seeds.
In the extension phase, RMAP, MAQ, SOAP and ZOOM align the reads to allow up to a fixed number of mismatches, and SOAP can alternatively allow for one continuous gap.
Other approaches to mapping include using suffix trees (Kurtz et al., 2004; Schatz et al., 2007) to quickly find short exact alignments to seed longer inexact alignments, and Bowtie (Langmead et al., 2009) uses the BurrowsWheeler transform (BWT), to find exact matches coupled with a backtracking algorithm to allow for mismatches.
Some BWT-based aligners are reporting extremely fast runtimes, especially in configurations that restrict the sensitivity of the alignments or limit the number of alignments reported per read.
For example, in their default high-speed configuration, SOAP2 (http://soap.genomics.org.cn/), BWA (http://maq.sourceforge.net) and Bowtie allow at most two differences in the beginning of the read, and report a single alignment per read selected randomly from the set of acceptable alignments.
In more sensitive or verbose configurations, the programs can be considerably slower (http://bowtie-bio.sourceforge.net/manual.shtml).
After computing end-to-end alignments, some of these programs use the edit distance or read quality values to score the mappings.
In a systematic study allowing up to 10 mismatches, Smith et al.
(2008) determined allowing more than two mismatches is necessary for accurately mapping longer reads, and incorporating quality values also improves accuracy.
Several of these programs, including RMAPQ (RMAP with quality), MAQ, ZOOM and Bowtie, use quality values in their scoring algorithm, and all are more lenient of errors in the low-quality 3 ends of the reads by trimming the reads or discounting low-quality errors.
Consecutive or spaced seeds dramatically accelerate the computation by focusing computation to regions with potential to have a high-quality alignment.
However, to increase sensitivity the length of the seeds must decrease (consecutive seeds) or the number of seeds used must increase (spaced seeds).
In either case, increasing sensitivity increases the number of randomly matching seeds and increases the total execution time.
Decreasing the seed length can be especially problematic because a seed of length s is expected to occur L/4s times in a reference of length L, and each occurrence must be evaluated using the slower inexact alignment algorithm.
Therefore, many of the new short read mappers restrict the maximum number of differences allowed, or limit the number of alignments reported for each read.
2 ALGORITHM CloudBurst is a MapReduce-based read-mapping algorithm modeled after RMAP, but runs in parallel on multiple machines with Hadoop.
It is optimized for mapping many short reads 1365 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1366 13631369 M.C.Schatz Fig.2.
Overview of the CloudBurst algorithm.
The map phase emits k-mers as keys for every k-mer in the reference, and for all non-overlapping k-mers in the reads.
The shuffle phase groups together the k-mers shared between the reads and the reference.
The reduce phase extends the seeds into end-to-end alignments allowing for a fixed number of mismatches or indels.
Here, two grey reference seeds are compared with a single read creating one alignment with two errors and one alignment with zero errors, while the black shared seed is extended to an alignment with three errors.
from next-generation sequencing machines to a reference genome allowing for a user specified number of mismatches or differences.
Like RMAP, it is a seed-and-extend algorithm that indexes the non-overlapping k-mers in the reads as seeds.
The seed size s=m/(k+1) is computed from the minimum length of the reads (m) and the maximum number of differences or mismatches (k).
Like RMAP, it attempts to extend the exact seeds to count the number of mismatches in an end-to-end alignment using that seed, and reports alignments with at most k mismatches.
Alternatively, like BLAST, it can extend the exact seed matches into end-to-end gapped alignments using a dynamic programming algorithm.
For this step, CloudBurst uses a variation of the LandauVishkin k-difference alignment algorithm (Landau et al., 1986), a dynamic programming algorithm for aligning two strings with at most k differences in O(km) time where m is the minimum length of the two strings.
See Gusfields (1997) classical text on sequence alignment for more details.
As a MapReduce algorithm, CloudBurst is split into map, shuffle and reduce phases (Fig.2).
The map function emits k-mers of length s as seeds from the reads and reference sequences.
The shufffle phase groups together k-mers shared between the read and reference sequences.
Finally, the reduce function extends the shared seeds into end-to-end alignments allowing both mismatches and indels.
The input to the application is a multi-fasta file containing the reads and a multi-fasta file containing one or more reference sequences.
These files are first converted to binary Hadoop SequenceFiles and copied into the HDFS.
The DNAsequences are stored as the keyvalue pairs (id, SeqInfo), where SeqInfo is the tuple (sequence, start_offset) and sequence is the sequence of bases starting at the specified offset.
By default, the reference sequences are partitioned into chunks of 65 kb overlapping by 1 kb, but the overlap can be increased to support reads longer than 1 kb.
2.1 Map: extract K-mers The map function scans the input sequences and emits keyvalue pairs (seed, MerInfo) where seed is a sequence of length s, and MerInfo is the tuple (id, position, isRef, isRC, left_flank, right_flank).
If the input sequence is a reference sequence, then a pair is emitted for every k-mer in the sequence, with isRef = 1, isRC = 0, and position set as the offset of the k-mer in the original sequence.
If the given input sequence is a read, then isRef = 0, and a pair is emitted for the non-overlapping k-mers with appropriate position.
Seeds are also emitted for the non-overlapping k-mers of the reverse complement sequence with isRC = 1.
The flanking sequences [up to (m s + k) bp) are included in the fields left_flank and right_flank.
The seeds are represented with a 2 bit/bp encoding to represent the four DNA characters (ACGT), while the flanking sequences are represented with a 4 bit/bp encoding, which also allows for representing an unknown base (N), and a separator character (.).
CloudBurst parallelizes execution by seed, so each reducer evaluates all potential alignments for approximately 1/N of the 4s seeds, where N is the number of reducers.
Overall this balances the workload well, and each reducer is assigned approximately the same number of alignments and runs for approximately the same duration.
However, low-complexity seeds (defined as seeds composed of a single DNA character) occur a disproportionate number of times in the read and reference datasets, and the reducers assigned these high-frequency seeds require substantially more execution time than the others.
Therefore, CloudBurst can rebalance low-complexity seeds by emitting redundant copies of each occurrence in the reference and randomly assigning occurrences in the reads to one of the redundant copies.
For example, if the redundancy is set to 4, each instance of the seed AAAA in the reference will be redundantly emitted as seeds AAAA-0, AAAA-1, AAAA-2 and AAAA-3, and each instance of AAAA from the reads will be randomly assigned to seed AAAA-R with 0R3.
The total number of alignments considered will be the same as if there were no redundant copies, but different subsets of the alignments can be evaluated in parallel in different reducers, and thus improve the overall load balance.
2.2 Shuffle: collect shared seeds Once all mappers have completed, Hadoop shuffles the keyvalue pairs, and groups all values with the same key into a single list.
Since the key is a k-mer from either the read or reference sequences, this has the effect of cataloging seeds that are shared between the reads and the reference.
2.3 Reduce: extend seeds The reduce function extends the exact alignment seeds into longer inexact alignments.
For a given seed and MerInfo list, it first partitions the MerInfo tuples into the set R from the reference and set Q from the reads.
Then it attempts to extend each pair of tuples from the Cartesian product R Q using either a scan of the flanking bases to count mismatches, or the LandauVishkin k-difference algorithm for gapped alignments.
The evaluation proceeds block-wise across subsets of R and Q to maximize cache reuse, and using the bases flanking the shared seeds stored in the MerInfo tuples.
If an end-to-end alignment with at most k mismatches or k differences is found, it is then checked to determine if it is a duplicate alignment.
This is necessary because multiple exact seeds may be present within the same alignment.
For example, a perfectly matching end-to-end 1366 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1367 13631369 CloudBurst alignment has k + 1 exact seeds, and is computed k + 1 times.
If another exact seed with smaller offset exists in the read the alignment is filtered as a duplicate, otherwise the alignment is recorded.
The value for k is small, so only a small number of alignments are discarded.
The output from CloudBurst is a set of binary files containing every alignment of every read with at most k mismatches or differences.
These files can be converted into a standard tab-delimited text file of the alignments using the same format as RMAP or post-processed with the bundled tools.
2.4 Alignment filtration In some circumstances, only the unambiguous best alignment for each read is required, rather than the full catalog of all alignments.
If so, the alignments can be filtered to report the best alignment for each read, meaning the one with the fewest mismatches or differences.
If a read has multiple best alignments, then no alignments are reported exactly as implemented in RMAPM.
The filtering is implemented as a second MapReduce algorithm run immediately after the alignments are complete.
The map function reemits the end-to-end alignments as keyvalue pairs with the read identifier as the key and the alignment information as the value.
During the shuffle phase, all alignments for a given read are grouped together.
The reduce function scans the list of alignments for each read and records the best alignment if an unambiguous best alignment exists.
As an optimization, the reducers in the main alignment algorithm report the top two best alignments for each read.
Also, the filtration algorithm uses a combiner to filter alignments in memory and reports just the top two best alignments from its subset of alignments for a given read.
These optimizations improve performance without changing the results.
3 RESULTS CloudBurst was evaluated in a variety of configurations for the task of mapping random subsets of 7.06 million publicly available Illumina/Solexa sequencing reads from the 1000 Genomes Project (accession SRR001113) to portions of the human genome (NCBI Build 36) allowing up to four mismatches.
All reads were exactly 36 bp long.
The test cluster has 12 compute nodes, each with a 32 bit dual core 3.2 GHz Intel Xeon (24 cores total) and 250 GB of local disk space.
The compute nodes were running RedHat Linux AS Release 3 Update 4, and Hadoop 0.15.3 set to execute two tasks per node (24 simultaneous tasks total).
In the results below, the time to convert and load the data into the HDFS is excluded, since this time was the same for all tasks, and once loaded the data was reused for multiple analyses.
The first test explored how CloudBurst scales as the number of reads increases and as the sensitivity of the alignment increases.
In this test, sub-sets of the reads were mapped to the full human genome (2.87 Gbp), chromosome 1 (247.2 Mbp) or chromosome 22 (49.7 Mbp).
To improve load balance across the cores, the number of mappers was set to 240, the number of reducers was set to 48, and the redundancy for low-complexity seeds was set to 16.
The redundancy setting was used because the low-complexity seeds required substantially more running time than the other seeds (>1 h compared with <1 min), and the redundancy allows their alignments to be processed in parallel in different reducers.
Figure 3 shows the running time of these tasks averaged over three runs, and shows Fig.3.
Evaluation of CloudBurst running time while scaling the number of reads and sensitive for mapping to the (A) full human genome; (B) chromosomes 1; and (C) 22 on the local cluster with 24 cores.
Tinted lines indicate timings allowing 0 (fastest) through four (slowest) mismatches between a read and the reference.
As the number of reads increases, the running time increases linearly.
As the number of allowed mismatches increases, the running time increases superlinearly from the exponential increase in seed instances.
The four mismatch computation against the full human genome failed to complete due to lack of available disk space after reporting 25 billion end-to-end alignments.
that CloudBurst scales linearly in execution time as the number of reads increases, as expected.
Aligning all 7M reads to the full genome with four mismatches failed to complete after reporting 25 billion mappings due to lack of available disk space.
Even allowing zero mismatches created 771M end-to-end perfect matches from the full 7M read set, but most other tools would report just one match per read.
Allowing more mismatches increases the runtime superlinearly, because higher sensitivity requires shorter seeds with more chance occurrences.
The expected number of occurrences of a seed length s in a sequence of length L is (L s +1)/4s, so a random 18 bp sequence (k =1) is expected to occur 0.04, 0.003 and 0.001 times in the full genome and chromosomes 1 and 22, respectively, while a 7 bp sequence (k =4) is expected to occur >17 500, >15 000 and >3000 times, respectively.
Consequently, short seeds have drastically more chance occurrences 1367 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1368 13631369 M.C.Schatz Fig.4.
CloudBurst running time compared with RMAP for 7M reads, showing the speedup of CloudBurst running on 24 cores compared with RMAP running on 1 core.
As the number of allowed mismatches increases, the relative overhead decreases allowing CloudBurst to meet and exceed 24 linear speedup.
and correspondingly more running time even though most chance occurrences will fail to extend into end-to-end matches.
The second test compared the performance CloudBurst on 24 processor cores with a serial execution of RMAPM (version 0.41) on 1 core with the full read set to chromosomes 1 and 22.
RMAP requires a 64 bit operating system, so it was run on 1 core of a 64 bit dual core 2.4 GHz AMD Opteron 250 with 8 GB of RAM running RedHat Enterprise Linux AS Release 3 Update 9.
CloudBurst was configured as before, except with the alignment filtration option enabled so only a single alignment was reported for each read identical to those reported by RMAPM.
Figure 4 shows the results of the test, and plots the speedup of CloudBurst over RMAP for the different levels of sensitivity.
The expected speedup is 24, since CloudBurst runs in parallel on 24 cores, but CloudBursts speedup over RMAP varies between 2 and 33 depending on the level of sensitivity and reference sequence.
At low sensitivity (especially k =0), the overhead of shuffling and distributing the data over the network overwhelms the parallel computation compared with the in-memory lookup and evaluation in RMAP.
As the sensitivity increases, the overhead becomes proportionally less until the time spent evaluating alignments in the reduce phase dominates the running time.
The speedup beyond 24 for high-sensitivity mapping is due to implementation differences between RMAP and CloudBurst, and the additional compute resources available in the parallel environment (cache, disk IO, RAM, etc.).
The speedup when mapping to the full genome did not improve as the level of sensitivity increased because of the increased overhead from the increased data size.
This effect can be minimized by aligning more reads to the genome in a single batch, and thus better amortize the time spent emitting and shuffling all of the k-mers in the genome.
The next experiment compared CloudBurst with an ad hoc parallelization scheme for RMAP, in which the reads are split into multiple files, and then RMAP is executed on each file.
In the experiment, the full read set was split into 24 files, each containing 294k reads, and each file was separately mapped to chromosome 22.
The runtimes were just for executing RMAP, and do not consider any overhead of partitioning the files, remotely launching the program, or monitoring the progress, and thus the expected speedup should be a perfect 24.
However, the runtimes of the different files varied considerably depending on which reads were present, and the corresponding speedup is computed based on the runtime for the longest running file: between 18 and 41 s with a 12 speedup for zero mismatches, 2667 s with a 14 speedup for one mismatch, 3498 s with a 16 speedup for two mismatches, 132290 s with a 21 speedup for three mismatches and 13791770 s with a 29 speedup for four mismatches.
The superlinear speedup for four mismatches was because the total computation time after splitting the read set was less than the time for the full batch at once, presumably because of better cache performance for RMAP with fewer reads.
This experiment shows the ad hoc scheme works well with speedups similar to CloudBurst, but fails to reach perfect linear speedup in most cases because it makes no special considerations for load balance.
In addition, an ad hoc parallelization scheme is more fragile as it would not benefit from the inherent advantages of Hadoop: data-aware scheduling, monitoring and restart and the high-performance file system.
4 AMAZON CLOUD RESULTS CloudBurst was next evaluated on the Amazon EC2.
This environment provides unique opportunities for evaluating CloudBurst, because the performance and size of the cluster are configurable.
The first test compared two different EC2 virtual machine classes with the local dedicated 24-core Hadoop cluster described above.
In all three cases, the number of cores available was held constant at 24, and the task was mapping all 7M reads to human chromosome 22 with up to four mismatches, with runtimes averaged over three runs.
The first configuration had 24 Small Instance slaves running Hadoop 0.17.0, priced at $0.10 per hour per instance and provides one virtual core with approximately the performance of a 1.01.2 GHz 2007 Xeon processor.
The second configuration had 12 High-CPU Medium Instance slaves, also running Hadoop 0.17.0 and priced at $0.20 per hour per instance, but offers two virtual cores per machine and have been benchmarked to have a total performance approximately five times the small instance type.
The running time for the High-CPU Medium Instance class was 1667 s, and was substantially better per dollar than the Small Instance class at 3805 s, and even exceeds the performance of the local dedicated cluster at 1921 s. The final experiment evaluated CloudBurst as the size of the cluster increases for a fixed problem.
In this experiment, the number of High-CPU Medium Instance cores varied between 24, 48, 72 and 96 virtual cores for the task of mapping all 7M reads to human chromosome 22.
Figure 5 shows the running time with these clusters averaged over three runs.
The results show CloudBurst scales very well as the number of cores increases: the 96-core cluster was 3.5 times faster than the 24-core cluster and reduced the running time of the serial RMAP execution from >14 h to 8 min (>100 speedup).
The main limiting factor towards reaching perfect speedups in the large clusters was that the load imbalance caused a minority of the reducers running longer than the others.
This effect was partially solved by reconfiguring the parallelization settings: the number of reducers was increased to 60 and the redundancy of the low-complexity seeds was increased to 24 for the 48-core evaluation, 144 and 72 for the 72-core evaluation and 196 and 72 for the 96-core evaluation.
With these settings, the computation had better balance across the virtual machines and decreased the wall clock time of the execution.
1368 [11:50 14/5/2009 Bioinformatics-btp236.tex] Page: 1369 13631369 CloudBurst Fig.5.
Comparison of CloudBurst running time (in seconds) while scaling size of the cluster for mapping 7M reads to human chromosome 22 with at most four mismatches on the EC2 Cluster.
The 96-core cluster is 3.5 faster than the 24-core cluster.
5 DISCUSSION CloudBurst is a new parallel read-mapping algorithm optimized for next-generation sequence data.
It uses seed-and-extend alignment techniques modeled after RMAP to efficiently map reads with any number of mismatches or differences.
It uses the Hadoop implementation of MapReduce to efficiently execute in parallel on multiple compute nodes, thus making it feasible to perform highly sensitive alignments on large read sets.
The results described here show CloudBurst scales linearly as the number of reads increases, and with near linear parallel speedup as the size of the cluster increases.
This high level of performance enables computation of extremely large numbers of highly sensitive alignments in dramatically reduced time, and is complementary to new BWT-based aligners that excel at quickly reporting a small number of alignments per read.
CloudBursts superior performance is made possible by the efficiency and power of Hadoop.
This framework makes it straightforward to create highly scalable applications with many aspects of parallel computing automatically provided.
Hadoops ability to deliver high performance, even in the face of extremely large datasets, is a perfect match for many problems in computational biology.
Seed-and-extend style algorithms, in particular, are a natural fit for MapReduce, and any of the hash-table based seed-and-extend alignment algorithms including BLAST, SOAP, MAQ or ZOOM could be implemented with MapReduce.
Future work for CloudBurst is to incorporate quality values in the mapping and scoring algorithms and to enhance support for paired reads.
We are also exploring the possibility of integrating CloudBurst into RNA-seq analysis pipeline, which can also model gene splice sites.
Algorithms that do not use a hash table, such as the BWT based short-read aligners, can also use Hadoop to parallelize execution and the HDFS.
Implementing algorithms to run in parallel with Hadoop has many advantages, including scalability, redundancy, automatic monitoring and restart and high-performance distributed file access.
In addition, no single machine needs to have the entire index in memory, and the computation requires only a single scan of the reference and query files.
Consequently, Hadoop based implementations of other algorithms in computational biology might offer similar high levels of performance.
These massively parallel applications, running on large compute clouds with thousands of nodes, will drastically change the scale and scope of computational biology, and allow researchers to cheaply perform analyses that are otherwise impossible.
ACKNOWLEDGEMENTS I would like to thank Jimmy Lin for introducing me to Hadoop; Steven Salzberg for reviewing the manuscript; and Arthur Delcher, Cole Trapnell and Ben Langmead for their helpful discussions.
I would also like to thank the generous hardware support of IBM and Google via the Academic Cloud Computing Initiative used in the development of CloudBurst, and the Amazon Web Services Hadoop Testing Program for providing access to the EC2.
Funding: National Institutes of Health (grant R01 LM006845); Department of Homeland Security award NBCH207002.
Conflicts of Interest: none declared.
ABSTRACT Motivation: Due to different experimental setups and various interpretations of results, the data contained in online bioinformatics resources can be inconsistent, therefore, making it more difficult for users of these resources to assess the suitability and correctness of the answers to their queries.
This work investigates the role of argumentation systems to help users evaluate such answers.
More specifically, it looks closely at a gene expression case study, creating an appropriate representation of the underlying data and series of rules that are used by a third-party argumentation engine to reason over the query results provided by the mouse gene expression database EMAGE.
Results: A prototype using the ASPIC argumentation engine has been implemented and a preliminary evaluation carried out.
This evaluation suggested that argumentation can be used to deal with inconsistent data in biological resources.
Availability: The ASPIC argumentation engine is available from http://www.argumentation.org.
EMAGE gene expression data can be obtained from http://genex.hgu.mrc.ac.uk.
The argumentation rules for the gene expression example are available from the lead author upon request.
Contact: kcm1@hw.ac.uk 1 INTRODUCTION Biologists have access to an ever increasing number and range of online data resources (Bateman, 2007).
Many of these resources contain inconsistent data.
This is not surprising as biology is a complex science in which countless parameters affect the outcome of every experiment.
Added to this is the human element that causes two identical results to be evaluated differently by different people.
The consequence is that two seemingly identical experiments can produce contradictory outcomes.
These experiments may be stored in one or more of the online resources that service a particular field.
If both of these experiments are published by the same resource, it becomes inconsistent.
However, if each experiment is published by a different resource, then the inconsistency is between resources and becomes harder to detect.
Regardless of where it occurs, inconsistency confuses users, forcing them to research further in order to answer their query.
In McLeod and Burger (2007) it was suggested that argumentation could be one solution to this problem.
By using all the resources in a field, arguments could be created for and against potential answers to a query.
These arguments could be presented to the user, providing To whom correspondence should be addressed.
them with a powerful set of knowledge that could be used to identify the most likely solution to the query.
This case study created a prototype using a third-party argumentation engine from ASPIC, Argumentation Services Platform with Integrated Components (www.argumentation.org), to generate arguments for the data held in the EMAGE developmental mouse gene expression database (Davidson et al., 1997).
Future work will extend this to include data from a complementary developmental mouse gene expression database, GXD (Ringwald et al., 2001).
Section 2 starts with a discussion of argumentation.
It is followed in Section 3 by an examination of the gene expression resources EMAGE and GXD in order to explain the need for argumentation.
In Section 4, the argumentation engine is introduced, and Section 5 describes how the knowledge in the domain was interpreted for use with the ASPIC argumentation engine.
Subsequently, in Section 6, the creation of arguments by ASPIC is discussed.
The study continues with a preliminary evaluation of the prototype in Section 7 and a discussion of the work in Section 8 before the conclusions are presented in Section 9.
2 ARGUMENTATION An argument is a reason to believe that something is true.
Arguments can be used to support or attack statements.
Argumentation (Carbogim et al., 2000; Pollock, 2002) is the use of computers in the process of arguing, either for helping humans to argue or by actually using the computers to conduct the argument.
As an approach argumentation mimics a human process and appears intuitive to human users (Williams and Williamson, 2006).
The actual form of an argument will depend on the theory being implemented.
Commonly, an argument is viewed as being a series of inference rules that are chained together in a manner similar to logic programming: there are a number of statements (premises) that if true, imply that the conclusion is true.
A premise of a rule may be satisfied by the conclusion of another rule.
So in order for the first rule to be satisfied, all the premises of the second rule must also be satisfied.
Eventually, premises will be satisfied because they are known to be true: they appear in a knowledge base that holds all currently accepted knowledge for the domain.
As the knowledge changes, new arguments will be formed.
These new arguments may contradict existing arguments, thus creating conflict.
Conflict between arguments is usually represented in two ways.
The first is rebuttal, where two arguments have opposite conclusions: e.g.
it is raining outside versus it is not raining outside.
Undercut is the second form of conflict.
It is an attempt to show that another argument is not valid because the premises do not imply the conclusion.
For example, an argument that someone will get wet 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
http://www.argumentation.penalty 0org [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i305 i304i312 Towards the use of argumentation in bioinformatics because it is raining outside is undercut by the knowledge that the person has an umbrella.
Argumentation provides a means of resolving this conflict.
The arguments can be weighed and compared, with the strongest argument(s) winning.
Thus, the conclusion supported by the strongest argument(s) wins.
Argumentation has been used in areas such as medicine, law (Bench-Capon and Prakken, 2006) and practical reasoning (Rahwan and Amgoud, 2006).
Medical uses of argumentation vary from systems that provide advice on administering drugs (Hurt et al., 2003), to those that help clinicians plan the management of chronic illness through the provision of decision support (Glasspool et al., 2006).
Argumentation has also been used to generate explanations of diagnosis, produced by other computational means, for the benefit of patients (Williams and Williamson, 2006).
In contrast to medical-informatics, bioinformatics has produced little work on argumentation, although Jefferys et al.
(2006) used argumentation to successfully evaluate the output of a protein prediction tool.
This work showed clearly that argumentation could be applied to bioinformatics tools, but what about bioinformatics data resources?
3 ON THE NEED FOR ARGUMENTION IN BIOLOGY Bioinformaticians have access to an ever-increasing range of online resources (Bateman, 2007), many of which publish experimental results for a particular field.
For example, the results of in situ gene expression experiments for the developmental mouse are published in both EMAGE (Davidson et al., 1997) and GXD (Ringwald et al., 2001).
Genes are a set of instructions that tell the body what to build, e.g.
a particular set of genes results in the creation of a nose and a different set of genes produces whiskers.
In situ gene expression experiments are designed to identify the genes that are active in a particular anatomical structure.
For that structure, the experiment sets out to determine if the gene is active (expressed) or not active (not expressed).
EMAGE and GXD take their knowledge of embryonic anatomical structures from a common anatomy, EMAP (Baldock and Davidson, 2008), though GXD has additional structures for the adult mouse.
An in situ gene expression database, such as EMAGE or GXD, allows its users to find the conclusions of gene expression experiments.
These conclusions link a gene to a structure, with a level of expression (i.e.
expressed or not expressed).
The database also provides provenance data such as: who the research team was, details of where the experiment was published, the images showing the experimental result and details of the mouse experimented on.
When using such a resource, the user will start by asking for the genes (not) expressed in a particular structure, or for the list of structures where a specific gene is expressed.
The complex nature of biology means that it is possible for experiments to produce conclusions that seem to be contradictory, e.g.
one experiment may suggest the gene Hoxb1 is expressed in the Neural Ectoderm (EMAP:151) and a second report that it is not.
There are many reasons why this could be the case.
For example, the experiments though very similar may be slightly different, e.g.
using different probes may have produced different results, the results may have been analysed differently, e.g.
different interpretations of the original gene expression images generate different experimental conclusions, and there is always the possibility of a genuine error, e.g.
when entering the data into the database.
In addition to internal inconsistencies, resources covering the same field may contradict each other.
For example, although EMAGE and GXD have a high level of duplication (in terms of data), their contents are not identical.
To illustrate this, consider the gene Bmp4 and the structure Future Brain (EMAP:1199).
At the time of writing this article, GXD contains only one experiment for this combination, and it suggests that Bmp4 is not expressed.
EMAGE has this experiment, but in addition it contains another three experiments, all of which indicate that Bmp4 is expressed in the Future Brain.
With all the available evidence, the most likely conclusion is that the gene is expressed; however, if the user relies on a single resource, in this case GXD, a wrong conclusion may be drawn.
Because these resources are incomplete, it is vital that they are both used, in order to generate as many arguments as possible.
However, this highlights a number of issues, both practical and theoretical, which require consideration.
An example of a practical issue would be in identifying experimental results that are duplicated in the other resource.
If an argument is created from data in EMAGE, there is no point in creating an argument based on the same data in GXD.
Theoretical issues include determining whether or not the knowledge used to create arguments for EMAGE can be successfully applied to a similar resource such as GXD.
These issues are not considered in this study but will be the subject of future work.
Regardless of location or reason, contradictions are confusing for users, and require them to investigate the data more fully, often to the extent of re-reading the original paper in which the result was published.
It would be useful to conduct an investigation of the data automatically, presenting the findings to the user in a manner that they could analyse easily.
It is hoped that argumentation may provide a mechanism to achieve this.
4 ARGUMENTATION ENGINE Many different types of argumentation software exist.
Some are used for visualization and explanation of arguments, e.g.
Araucaria (Reed and Rowe, 2001), some for decision support (Fox, 2001) and some for collaborative decision support (Gordon and Karacapilidis, 1997).
However, an inference tool that generates and evaluates arguments is used in this study.
This case study intended to create a web-based tool that dynamically pulled data on-demand from EMAGE to conduct argumentation.
For this reason, it required a robust inference tool that could be integrated readily into applications.
Few tools are available that meet these requirements.
Many different theories for argumentation have been proposed, but few have a robust implementation that can be integrated freely into another application.
For example, Gordon (1993) produced an implementation of his theory, but it is not available publicly.
Oscar is an implementation from Pollock (2002) which is available for download.
Unfortunately, it is programmed in LISP making it difficult to integrate.
The original argumentation engine concept and theory was produced by Dung (1995), but it did not include an implementation.
ASPIC had the goal of standardizing argumentation theory in Artificial Intelligence and developing a suite of tools that could be used in standard application areas such as dialogue, decision-making i305 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i306 i304i312 K.McLeod and A.Burger and machine learning.
The foundation of this implementation is a JAVA tool that generates arguments using inference.
ASPICs argumentation engine is still in development.
Consequently, it is not as robust as might be desired, though it is reliable enough to work for more complex examples than those presented here.
Crucially, the engines design ensures that it can be integrated into other projects.
Although the code is not open source, the engine is available publicly.
The theory behind the engine is based on the work of Dung (1995).
Dungs system is abstract, in the sense that the notion of an argument was not defined.
ASPIC, however, defines an individual argument in the form of an inference tree: inference rules are chained together to form an argument that is organized in a tree structure (Fig.1).
The sole form of attack in the system is rebuttal.
ASPIC mimics an undercut by allowing the user to assign a name to a rule and then create a second rule that rebuts the name (Fig.2).
This succeeds because the name is automatically treated as a premise to its rule, and therefore the second rule rebuts a premise of the first rule.
Input to the engine is: a set of knowledge that models the domain being argued about, a set of rules used to infer new knowledge in the domain (when instantiated a rule forms an argument for the knowledge generated), a set of parameters that configure the behaviour of the engine, and a query that the user wishes the engine to argue about.
Once a query is submitted, the engine generates arguments that support and attack the query before evaluating them.
Output is the arguments, their status and the relationships between them.
In terms of status, the engine records whether or not an argument is true (w.r.t current knowledge) and for relationships the engine provides a list of which arguments attack which other arguments.
This information can be presented visually, in a graph, or in textual form.
ASPICs argumentation engine can be used via a supplied Graphical User Interface (GUI), or programmatically through a JAVA Application Programming Interface (API).
The engine has a fixed knowledge syntax, so an argument must conform to the Conclusion Premise 1 Premise 3 Rule: Premise 1 <-Premise 3 Premise 2 Rule 1: Conclusion <-Premise 1 & Premise 2 Fig.1.
Arguments in ASPIC are stored in a tree structure.
The earlier argument has the conclusion Outcome and three contributing subarguments.
Rule 1 provides the inference rule used to reach the conclusion.
This inference rule states that Outcome is true if both Premise 1 and Premise 2 are true.
Premise 2 is known to be true.
Premise 1 is the conclusion of another argument, and is only true when Premise 3 is true.
[ID_1] Conclusion <-Premise 1 & Premise 2 To undercut this rule: ID_1 <-Premise 3 & Premise 4 Fig.2.
Undercutting an argument in ASPIC.
The first rule states that Conclusion is true when Premise 1 and Premise 2 are both true.
This rule is assigned the name ID_1.
The second rule states that when Premise 3 and Premise 4 are both true, the rule called ID_1 cannot be applied.
specification created by the designers.
When using the GUI, input to the engine has the form of first-order logic.
The chosen logic is similar to PROLOG (Bratko, 2000) and features weak and strong negation.
The JAVA API is designed around the logic, with the methods reflecting the underlying language by using terminology such as Variable, Term, Consequent and Antecedent.
It is the API that the rest of this article deals with.
5 FORMALIZATION OF KNOWLEDGE Argumentation takes place in a particular domain.
That domain could be some everyday area such as planning how to travel to London, or it could be something more specialized such as in situ gene expression.
The argumentation engine is given two forms of knowledge from the domain of gene expression.
The first documents the current state of the domain, i.e.
what is believed to be true, which in this case is the results of gene expression experiments.
The second type, is the knowledge of how to interpret the first.
This knowledge came from the EMAGE curator, and was converted into inference rules that the engine uses to infer new arguments.
The domains state will change continually as new experiments are submitted to EMAGE daily.
However, the knowledge of how to interpret that experimental data changes far less often.
Therefore, it is safe to gather expert knowledge in advance and store it for use later.
Due to the high rate of change, the experimental knowledge must be obtained when it is to be used.
This on-demand creation of knowledge is achieved by pulling data through EMAGEs SOAP-based web service and subsequently converting it.
Knowledge can be strict or defeasible.
Strict knowledge is definitely true, e.g.
London is the capital of the UK.
Defeasible knowledge may be true, but an element of doubt remains, e.g.
it is raining, therefore I will get wet.
Associating knowledge with a real number between 0 and 1 indicates the users degree of belief in an item of knowledge.
If no degree of belief is specified, the piece of knowledge is assumed to be strict (have confidence equal to 1).
This confidence score is how ASPIC assigns a strength to an argument: the higher the score the stronger the argument.
In addition, each piece of knowledge can be assigned a description: a piece of natural language text that describes the knowledge.
The description can hold a simple explanation of a rule or fact.
It is also possible to assign a description to the conclusion of a rule.
Consequently, an argument can be viewed as a series of logic or natural language statements.
5.1 Expert knowledge to inference rules Inference rules are used by ASPIC to infer new arguments.
They model the inference processes of the domain being investigated.
Once captured the inference processes need to be converted into ASPICs chosen logic for use in the engine.
The example featured attempts to argue over the accuracy of data stored in EMAGE.
As such, new arguments are inferred from the contents of the database according to processes suggested by the EMAGE curation team.
This team is responsible for maintaining the quality of the resource by reviewing the experiments submitted for inclusion in EMAGE.
Expert knowledge was gathered in advance during a series of meetings.
These meetings started with informal discussions and i306 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i307 i304i312 Towards the use of argumentation in bioinformatics moved onto using concrete examples to illustrate how the curator processes information.
Although ASPICs engine uses a first-order logic, biologists tend to prefer natural language.
In order to provide a bridge between the two, the notion of an argument schema (Walton, 1996b) is used.
This allowed the experts reasoning to be captured in a semi-formal way using natural language.
A schema provides a natural language inference rule that documents an inference that can be assumed to be true unless shown otherwise (defeated by a counter-argument).
A schema also provides a collection of Critical Questions that highlight exceptions to, and extra conditions on, the use of the inference rule.
All the knowledge needed to create a formal logic inference rule is documented in a manner that can be easily understood by biologists.
For example, when an EMAGE curator evaluates an experiment, they record their confidence in the experiment as a score between 0 and 3, with 3 indicating a high level of confidence.
The curators confidence is made public because a high-quality experiment is more likely to produce a correct result than an experiment the curator has less confidence in.
Intuitively, it can be suggested that if the curator has high confidence in the experiment, the user can have high confidence in the result of the experiment.
This would lead to something like the following schema based on Waltons schema for an Expert (Walton, 1996a): EMAGE is a leading resource on mouse in-situ gene expression EMAGE has C confidence in experiment E suggesting gene G is expressed in structure S Therefore we may be C confident that G is expressed in S Assuming that anyone who uses the system automatically accepts the initial premise that EMAGE is an expert resource, it is possible to simplify the above schema and represent it in a PROLOG-like syntax (with capital letters indicating variables that unify and lower case letters indicating constants), so the basic rule is: expressed(Gene, Structure) <-experiment(Id, Gene, Structure, expressed), confidence(Id, Confidence).
The problem with this rule is that the confidence EMAGE has in the experiment is not passed to ASPIC.
There should be a direct link between EMAGEs confidence and the strength of the argument; therefore, it is necessary to add a degree of belief.
In the instance of EMAGE having high confidence the argument should be strong and thus have a high degree of belief, for example 0.8.
This can be set when passing the inference rule to ASPIC using its JAVA API.
A selection of further rules can be seen in Table 1.
These rules use notions such as Theiler Stages, Spatial Annotation and Textual Annotation.
The Theiler Stages are the 26 developmental phases of a mouse embryo.
Each experiment must be mapped to one of these stages.
The results of gene expression experiments (2D section images) can be described with respect to the EMAP anatomy ontology or spatially mapped into the 3D embryo models (one per Theiler Stage) of EMAP.
These are referred to as Textual Annotation and Spatial Annotation, respectively.
Rules 3, 4 and 5 from Table 1 are all variations of the schema discussed earlier in this section.
5.2 State of domain knowledge ASPIC refers to each item of knowledge (or belief) referring to the current state of the domain as a fact; like inference rules these can be strict or defeasible.
The EMAGE resource provides the setting for this case study, so the facts given to the argumentation engine correspond directly to the data held in EMAGE.
The contents of EMAGE can be abstracted to knowledge about an experiment and its conclusion.
The conclusion is literally that a gene was (not) expressed in a particular anatomical structure.
The experimental information states: who performed the experiment, Table 1.
Some of the rules defined by the EMAGE curator ID Description 1 If a gene G, is expressed in a structure S, in Theiler Stage (T 1) and also in Stage (T +1), then G is very likely to be expressed in S in Stage T .
2 If the user, after examining the image of the experimental result, is confident that the gene G, is expressed in the structure S, then G is very likely to be expressed in S. 3 If a spatial annotation SA, suggests a gene G is expressed in structure S, and the curator has high confidence in SA, then we may have high confidence that G is expressed in S. 4 If a textual annotation TA, suggests a gene G is expressed in structure S and the curator has high confidence in TA, then we may have high confidence that G is expressed in S. 5 If a textual annotation TA, suggests a gene G is expressed in structure S and the curator has medium confidence in TA, then we may have medium confidence that G is expressed in S. 6 If the user does not trust the research team that conducted experiment E, then all spatial and textual annotations based on that experiment should have a low level of confidence.
7 If a spatial annotation SA and a textual annotation TA disagree, then always trust TA.
8 If two experiments disagree on whether, or not, a gene G is expressed in structure S and the user believes the experiments are examining different parts of S, then G is likely to be expressed in part of S. 9 If two experiments disagree on whether, or not, a gene G is expressed in structure S and the user believes the experiments are examining different parts of S, then G is likely to be not expressed in part of S. Rules 17 are relatively straightforward.
However, Rules 8 and 9 may require further explanation.
They state that if two experiments are examining different parts of the same structure both results can be correct regardless of their conclusion.
For example, consider two experiments on the human hand.
The first experiment may find a particular gene expressed in the thumb, and the second conclude that the same gene is not expressed in the index finger.
These experiments show that the gene is both expressed and not expressed in the hand.
i307 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i308 i304i312 K.McLeod and A.Burger what type of experiment it was, how the gene was detected, what kind of mouse the experiment was performed on (its species, its age, whether it was normal or abnormal) and it provides photographs of the result taken by the researchers.
Consequently, the minimum requirement is to provide ASPIC with knowledge on genes, anatomical structures, the relationship between the two and some details of the experiment that established the relationship.
A fact is treated as a simplified inference rule, i.e.
a rule without premises.
One possible way of saying that an experiment in EMAGE, with the associated identifier EMAGE:772, reported the gene Hoxb1 was expressed in the Neural Ectoderm (EMAP:151) is: experiment( EMAGE:772, Hoxb1, EMAP:151, expressed).
Not all of the facts can be generated easily, due to the impossibility of automatically processing the experimental images.
These images are taken by the researchers at the end of the experiment, and are stored in EMAGE as part of an experiments provenance.
Image analysis is a vital part of evaluating the quality of the result: it is done manually by the EMAGE curator.
Consequently, in this study, the images are presented to the human user and they are asked specific questions such as: these images are from two experiments that examine the same structure, do they appear to investigate the same area?
These questions are straightforward for a regular user of EMAGE to answer, but are more challenging for someone with less experience.
6 GENERATING ARGUMENTS Arguments are generated from the contents of the knowledge base, in response to the user posing a query.
The results of the query are returned for the user to examine.
6.1 Query The query is the conclusion that the user wishes ASPIC to argue about.
It will take the form of a fact, and will conform to the earlier discussion in all but one respect: it will not have a degree of belief associated with it.
So in this case, an example would be: expressed(Hoxb1, EMAP:151).
Once the query has been created its status is determined by the argumentation engine.
6.2 Evaluating a query ASPIC uses a dialogue game to determine the status of a query (ASPIC, 2004).
The knowledge given in the query can be undefeated (true with respect to current knowledge), defeated (false with respect to current knowledge), or unknown.
The game features two computer players, the Proponent (PRO) who attempts to prove the query, and the Opponent (OPP) who tries to stop PRO.
The game starts with PRO creating an argument to support the query (an argument whose conclusion is identical to the query).
This process starts by searching for a rule with an appropriate conclusion.
Once found, rules with conclusions that are identical to the premises are sought.
If the premises cannot be satisfied in this manner, the facts are examined to determine if they satisfy the premises.
OPP now attempts to defeat PROs argument.
To succeed, OPPs argument must rebut part of PROs and have a higher degree of belief.
OPP starts by trying to construct arguments that rebut the conclusion of PROs argument.
If that cannot be done, OPP attempts to rebut the premises.
If OPP succeeds in defeating PROs argument, PRO will attempt to counter OPPs argument by defeating it.
This process of attack and counter-attack continues until one player (PRO or OPP) fails to defeat the others argument.
If PRO is stopped, they try a new line of defence by creating a new argument, to support the conclusion that OPP has defeated, if they fail OPP wins.
However, if OPP fails, they try to defeat one of PROs previous arguments, if they cannot do so PRO wins.
For an example we shall use a simplified set of data for Hoxb1 in EMAP:151, ignoring the distinction between textual and spatial annotations (see Section 5.1).
EMAGE has two relevant experiments.
The first suggests that Hoxb1 is expressed in EMAP:151 and the second that it is not.
The EMAGE curator has medium confidence in the first experiment, and a high level of confidence in the second.
In the game, PRO starts by using the first experiment to create the argument in Figure 3 (based on a variation of the schema in Section 5.1), which OPP defeats by creating the argument in Figure 4, based on the second experiment (and a different variation of the schema in Section 5.1).
The next argument of PRO depends on the information provided by the user.
Since it is impossible for the system to evaluate the images of experimental results the user is asked to help.
They are given a number of questions to answer, for example: are the two experiments dealing with the same part of the structure?
This question relates to Rules 8 and 9 from Table 1, and is asked because it is possible for a gene to be expressed in one part of an anatomical structure but not expressed in another part of it (e.g.
a gene may be expressed in the index finger but not the thumb, as the index finger and thumb are two separate parts of the hand, the gene is both expressed and not expressed in the hand).
If the user answers the question by suggesting that the experiments are examining different parts of EMAP:151, then it is possible that Hoxb1 is both Hoxb1 is expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is expressed The EMAGE curator has medium confidence in the experiment If the curator has medium confidence in the experiment, then we may   have medium confidence in the experiment and its result Degree of belief = 0.4 Fig.3.
Argument for Hoxb1 being expressed in EMAP:151. i308 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i309 i304i312 Towards the use of argumentation in bioinformatics Hoxb1 is not expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed The EMAGE curator has high confidence in the experiment If the curator has high confidence in the experiment, then we may have high confidence in the experiment and its result Degree of belief = 0.8 Fig.4.
A counter argument to the argument in Figure 3.
Because the EMAGE curator has more confidence in the experiment used in this argument than the experiment used in Figure 3, this argument has a higher degree of belief and so defeats the argument from Figure 3.
Hoxb1 is expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed EMAGE has an experiment suggesting Hoxb1 is expressed The experiments look at different parts of the same structure If the experiments look at different parts of the same structure, they can both be correct, so the gene is expressed.
Degree of belief = 0.9 Fig.5.
A second argument, based on Rule 8 from Table 1, showing Hoxb1 is expressed in EMAP:151.
Hoxb1 is not expressed in EMAP:151 EMAGE has an experiment suggesting Hoxb1 is not expressed EMAGE has an experiment suggesting Hoxb1 is expressed The experiments look at different parts of the same structure If the experiments look at different parts of the same structure, they can both be correct, so the gene is not expressed.
Degree of belief = 0.9 Fig.6.
A second argument, based on Rule 9 from Table 1, showing Hoxb1 is not expressed in EMAP:151. expressed and not expressed in EMAP:151.
This leads PRO to produce the argument in Figure 5 by using Rule 8 from Table 1.
As this argument defeats OPPs previous argument (based on the EMAGE experiment suggesting Hoxb1 was not expressed) PROs first argument is reinstated because it is no longer attacked.
OPP must counter PROs argument and does so with the same logic as PRO (Rule 9 from Table 1): the experiments are using different parts of EMAP:151, so Hoxb1 can be both expressed and not expressed, and therefore it is not expressed (Fig.6).
Currently there are two arguments of equal strength that contradict each other (Figs 5 and 6) .
The outcome of this conflict depends on which type of game semantics is used.
ASPIC provides two game semantics, skeptical and credulous, for the user to choose between.
When a skeptical game is played, if there is any doubt about the acceptability of an argument it is rejected.
In this case, there is doubt about the acceptability of both arguments, and so they are both rejected.
The credulous game is implemented in such a way that even if there is doubt about one of PROs arguments it is accepted, whilst OPPs argument is rejected if there is any doubt.
So here PROs argument that Hoxb1 is expressed is accepted, with OPPs counter argument being defeated.
It is left to the user to decide which game is most suitable for their situation.
Adopting credulous semantics, PROs last argument is accepted.
Because of this, both of OPPs arguments are defeated, leaving both of PROs arguments undefeated.
OPP must try to find another argument that defeats one of PROs two arguments.
However, there are no more arguments available, and so OPP fails.
This game has been won by PRO.
PRO starts a second game with the argument from Figure 5 (as the two experiments are looking at different parts of EMAP:151 Hoxb1 can be both expressed and not expressed, and therefore it is expressed).
The same arguments as before are constructed, once again PRO wins.
PRO can construct no more arguments to support the query so the game is over.
The results are given to the programmer to manipulate as they wish.
The results come in two separate parts.
The first is a series of yes and no.
Each one represents an argument that PRO has constructed to support the query.
As PRO won both games, the results from this example are yes and yes.
The second part of the results is called the proof.
Essentially it is all the arguments used in the game.
If calculated the status of the argument is also recorded.
In this example, the two arguments provided by PRO are undefeated with both of OPPs arguments being defeated.
The programmer can present the arguments to the user in any way they wish.
However, when communicating with biologists it makes more sense to use a natural language form similar to that used in this section (Fig.4) by using the descriptions attached to rules and facts.
i309 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i310 i304i312 K.McLeod and A.Burger Fig.7.
Screen shot from the prototype (top-left) with simplified presentation in a mock prototype (bottom-right).
Although this evaluation of a query may seem complicated, it is essentially the type of thought-process naturally deployed by a human user.
The apparent complexity relates primarily to the need to formalize this process for computational purposes.
Fortunately, the details of this formalization need not be communicated to the end user and our initial evaluation (see Section 7) adds weight to our view that the underlying argumentation reasoning is accessible and helpful to biologists.
7 EVALUATION Once the implementation of the above system was completed, a preliminary evaluation was undertaken.
This informal study involved demonstrating the system to the EMAGE curator, and recording the feedback given.
Overall the system was well received.
The tool was deemed easy to use, and the arguments were presented in far less time than the curator had expected.
The arguments made sense to the curator, and they covered the majority of the points the curator wished to see.
The curator felt that the arguments would be enough for most people to evaluate the data from EMAGE, and thus determine whether or not a gene was expressed in a particular structure.
As such the system was a success.
Although feedback from the curator was positive, four issues clearly require to be tackled.
The first is the presentation.
The examples discussed above are simplified in order to improve clarity.
However, the prototype displayed arguments in a rudimentary manner using a slightly amended version of a method ASPIC provided for the task (see top-left of Fig.7).
This resulted in a confusing output.
Much of this output was redundant as it restated what had already been given.
For example, in the first argument, the five lines quality_author(EMAGE:772) through to You have confidence in the research team said who the research team was twice, and that the user had confidence in this team twice.
Consequently, the test user was presented with a simplified version of these arguments in a mocked-up prototype (see bottom-right of Fig.7).
Feedback from the curator suggested simplifying further the presentation of the arguments.
For example, subarguments were indented to show that they were separate from the main argument but still contributed to it.
However, the curator did not understand the relationship.
Instead, he suggested the information should be presented in a simple paragraph comprising two or three sentences.
i310 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i311 i304i312 Towards the use of argumentation in bioinformatics The second key issue highlighted by the evaluation is trust.
When using EMAGE, a user must trust the researcher to have performed and evaluated the experiment correctly, in addition the user must trust that all mistakes were detected and corrected by the journal that published the experiment, and finally they must trust the curator of EMAGE (or GXD) to have mapped correctly the researchers findings to the EMAP ontology for inclusion in the database.
For example, if the research team suggested that Bmp4 was expressed in the presumptive infundibulum, then the curation team needed to map this structure to its equivalent in the EMAP anatomy (infundibular recess of third ventricle).
These trust issues should have been made clear to the user by explicitly asking them if they had confidence in each of the above groups.
The system did not do this.
The third issue related to the screen that asked the user for help in processing information.
As mentioned in Section 5.2 the user was asked to analyse some information when the system could not do so.
The curator felt that this screen presented the user with too many tasks to undertake.
One possible solution would be to use the image analysis already undertaken by the authors, journal and EMAGE curation team.
The final issue raised by the curator related to GXD.
The system worked with data from EMAGE.
In real life, the curator would advise anyone with doubts over data in EMAGE to examine GXD (and vice versa), he felt that extending the system to include arguments based on data in GXD was vital.
The goal of this work was to assess the usefulness of argumentation in bioinformatics.
Overall it was obvious that much work remained.
However, it was also evident that the current prototype system was the first step on the way to a useful and interesting tool.
8 DISCUSSION This work concentrates on two resources publishing in situ developmental mouse gene expression information.
However, other resources that perform this function exist, for example, the Mouse Atlas of Gene Expression, MAGE (http://www.mouseatlas.org).
Therefore, it would be beneficial to extend the system to include this and other related resources.
Unfortunately, this is not a simple task.
MAGE uses its own ontology to describe the mouse anatomical structures.
This ontology does not have a mapping to the EMAP ontology used by EMAGE and GXD.
One structure in EMAP may correspond to parts of several structures in the MAGE ontology, and vice versa.
Although work is progressing on a cross-linked mammalian ontology that will hopefully link EMAP to MAGE, currently there is no automatic mechanism to do this.
At present this makes it impossible to use these resources together in this system.
If MAGE had used the EMAP ontology, there would be no reason why it could not be included in the system.
Data from MAGE would need to be pulled and converted for use within ASPIC.
Likewise, there would be a need for an evaluation of the current inference rules to determine if they could be applied to MAGE.
It is probable that several extra inference rules would be required.
With the new rules in place ASPIC would be able to argue as before.
However, with futher knowledge at its disposal, it would be possible to create extra arguments and thus have a more complex argumentation process.
Although this would be unlikely to have a significant effect in the example discussed here, it is possible that the integration of a large number of resources (or resources with a larger number of expert generated inference rules) might cause the argumentation process to run too slowly to be useful.
In such a situation, it might be necessary to balance the inclusion of each resource against the usefulness of the information it provides for the arguments.
Alternatively, it might prove helpful to investigate the other argumentation engines that are beginning to appear e.g.
ArgKit (http://www.argkit.org).
Of course, in the context of the Internet, the argumentation workload can be distributed across more than one site.
We envision domain-specific argumentation engines, e.g.
one or more sites for in situ gene expression argumentation, that communicate with each other.
Efforts are already underway to develop an Argumentation Interchange Format (Chesevar et al., 2006) to facilitate such interactions.
In addition, we note that there is a potential issue with scalability in terms of formalizing enough relevant domain knowledge for the purposes of argumentation.
As with most semantics-based applications, it is unrealistic to expect that all relevant domain knowledge will be captured.
However, the experience with the Semantic Web so far shows that even a little semantics goes a long way (Wolstencroft et al., 2005), and we believe that this applies equally to argumentation.
Argumentation has been used within this work to resolve inconsistencies across biological data resources.
A variety of other mechanisms to integrate data and resolve inconsistency exist.
For example, data reconciliation (a.k.a.
data fusion) uses a function to turn multiple possible values into a single value, e.g.
computing the average of four numbers (Motro and Rakov, 1998).
A second possible mechanism would create multiple query plans for the resources, then select the best according to information quality criteria (Naumann, 1996).
Our work is not an attempt to replace these mechanisms.
We are not concerned with automatically resolving conflict, but instead wish to determine whether or not argumentation can enable biologists to resolve the differences themselves.
9 CONCLUSION This case study explored the usefulness of argumentation in helping biologists work around conflicting information presented by an online biological database, in this case a developmental mouse gene expression database called EMAGE.
By investigating the reasoning processes of an EMAGE curator, a series of rules for assessing the quality of an experiment were produced.
These rules were used by the ASPIC argumentation engine to generate arguments on the validity of the data provided by EMAGE.
This enabled arguments for and against each experimental result to be produced and presented to the user.
Following an implementation of the system, an evaluation was undertaken with the EMAGE curator.
The evaluation showed that the basic concept was correct: arguments could be used to highlight issues and help the user determine if data was valid.
However, it also stressed the importance of presenting the arguments in an appropriate manner, and here further work must be undertaken.
This is not the only work needed, in particular an effort must be made to extend the system so that it can create arguments based on the data held in another developmental mouse gene expression database, GXD.
Only then it will be possible to make an accurate assessment of the full worth of argumentation in a bioinformatics setting.
i311 [19:57 18/6/03 Bioinformatics-btn157.tex] Page: i312 i304i312 K.McLeod and A.Burger ACKNOWLEDGEMENT This work could not have been completed without the support of the EMAGE curation team.
Funding: Funding by the EU projects Sealife (FP6-2006-IST-027269) and REWERSE (FP6-2006-IST-506779) is acknowledged.
Conflict of Interest: none declared.
ABSTRACT Motivation: Transcription factors (TFs) are crucial during the lifetime of the cell.
Their functional roles are defined by the genes they regulate.
Uncovering these roles not only sheds light on the TF at hand but puts it into the context of the complete regulatory network.
Results: Here, we present an alignment-and threshold-free comparative genomics approach for assigning functional roles to DNA regulatory motifs.
We incorporate our approach into the Gomo algorithm, a computational tool for detecting associations between a user-specified DNA regulatory motif [expressed as a position weight matrix (PWM)] and Gene Ontology (GO) terms.
Incorporating multiple species into the analysis significantly improves Gomos ability to identify GO terms associated with the regulatory targets of TFs.
Including three comparative species in the process of predicting TF roles in Saccharomyces cerevisiae and Homo sapiens increases the number of significant predictions by 75 and 200%, respectively.
The predicted GO terms are also more specific, yielding deeper biological insight into the role of the TF.
Adjusting motif (binding) affinity scores for individual sequence composition proves to be essential for avoiding false positive associations.
We describe a novel DNA sequence-scoring algorithm that compensates a thermodynamic measure of DNA-binding affinity for individual sequence base composition.
Gomos prediction accuracy proves to be relatively insensitive to how promoters are defined.
Because Gomo uses a threshold-free form of gene set analysis, there are no free parameters to tune.
Biologists can investigate the potential roles of DNA regulatory motifs of interest using Gomo via the web (http://meme.nbcr.net).
Contact: t.bailey@uq.edu.au Supplementary information: Supplementary data are available at Bioinformatics online.
Received on October 23, 2009; revised on January 18, 2010; accepted on February 2, 2010 1 INTRODUCTION The regulation of gene expression is crucial in the development and functioning of cells.
DNA-binding proteins called transcription factors (TFs) are one cog in the regulatory machinery shared by all cellular and multi-cellular organisms.
The human genome, for instance, is estimated to contain up to 3000 such TFs, of which only about 1000 are annotated as such, and only 62 have experimentally To whom correspondence should be addressed.
verified in vivo DNA-binding and regulatory activity (Vaquerizas et al., 2009).
For the vast majority of TFs in higher organisms, the set of genes they regulate, as well as the biological functions they are involved in, is largely unknown.
However, for a growing number of TFs, models of their DNA-binding propensities are known.
The advent of protein binding microarrays (PBMs) in particular is rapidly making DNA-binding affinity data available for large numbers of TFs in many species (Berger and Bulyk, 2009).
Another source of DNA-binding affinity data for TFs is chromatin immunoprecipitation followed by deep sequencing (ChIP-seq; Barski and Zhao, 2009).
Both of these types of data can be used to construct a position weight matrix (PWM; Stormo, 2000) model of the DNA-binding affinity of a given TF.
Such models are herein referred to as motifs, and can also model the DNA-binding affinity of other molecules, including microRNAs.
Gene expression data can also be used to discover DNA-binding motifs utilizing ab initio motif discovery from the promoters of sets of co-expressed genes (Roven and Bussemaker, 2003).
In contrast to the PBM and ChIP-seq approaches, motif discovery in sets of co-expressed genes usually results in DNA-binding motifs for which the binding molecule (e.g.
TF or microRNA) is unknown.
One application of DNA-binding motifs is the in silico prediction of the regulatory targets of the TFs.
To predict the targets of a TF, its binding motif is used to score promoter regions of genes for their potential to bind the TF protein.
It is well-known that such predictions are not very specific, and many false positives must be tolerated if all regulatory targets of a TF are to be detected (Wasserman and Krivan, 2003).
However, as we have previously shown, even such noisy TF target predictions contain sufficient information to allow us to make useful predictions of the biological roles of the TF (Bodn and Bailey, 2008).
The focus of the current work is to improve the sensitivity of computational methods that make TF role predictions using DNA-binding motifs.
Our original method for predicting the roles of TFs starts with a PWM motif describing the DNA-binding affinity of the TF.
We use the PWM to score the promoter region of each gene in the genome for its likelihood to be bound by the TF.
We then use the resulting affinity scores to test each term in the Gene Ontology (GO; Ashburner et al., 2000) for association with high-scoring genes.
In contrast to other approaches (e.g.
Sinha et al., 2008) that use an user-specified affinity score threshold to separate TF target genes from non-targets, we use the MannWhitney U-test (Mann and Whitney, 1947), also known as Wilcoxon rank sum test, to determine if the genes associated with a particular GO term have significantly high scores.
This method was implemented in the original The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:11 17/3/2010 Bioinformatics-btq049.tex] Page: 861 860866 Assigning roles to DNA regulatory motifs Gomo algorithm (Bodn and Bailey, 2008), which reports GO terms with significant rank sum P-values, after adjusting for multiple tests.
One obvious place to look for improvement in TF role prediction is in the quality of the PWM-based function used by Gomo to score promoter regions for their potential to be targets of the TF.
In a similar application, other researchers (Sinha et al., 2008) used the likelihood function of a motif-based hidden Markov model (HMM) to score promoters.
They then reran the HMM 100 times on shuffled versions of the promoter in order to convert the likelihood to a P-value, which they used as their final target function.
This scoring function is computationally expensive compared with the average motif affinity (Ama) function used by Gomo (Fig.2 in Supplementary Material 1).
It does, however, reduce the significance of binding affinity scores for promoters whose GC content is similar to that of the motif, which has been suggested to be important when using DNA-binding scores in gene set enrichment analyses (Sinha et al., 2008).
In the current work, we examine the importance of GC content compensation for binding affinity scores, and develop a more computationally efficient scoring algorithm.
A second obvious approach to consider for improving TF role prediction algorithms is to use comparative genomics.
Such an approach assumes that functional TF binding sites are to some degree conserved in the promoters of orthologous genes from related species.
Based on this assumption, numerous methods for motif-based TF binding site and TF target gene prediction have been developed that utilize sets of orthologous sequences from multiple species.
The validity of the assumption is evident from the success of such methods, which includes phylogenetic footprinting and phylogenetic motif modeling (Hawkins and Bailey, 2008).
We could chose to use one of these methods to score genes for their likelihood as targets of a given TF, rather than one of the single-species binding affinity-based motif scores used by Gomo or Sinha et al.
(2008).
However, the above phylogenetic scoring methods require multiple alignments of orthologous genes from species being compared, and they suffer when the alignments contain inaccuracies or when the location or orientation of the TF binding sites has not been conserved (Moses et al., 2006).
Phylogenetic motif modeling has the additional problem of not scaling well to more than about five related species (Hawkins and Bailey, 2008).
In this work, we present a comparative genomics extension to Gomo (multiple-species Gomo) that does not require multiple alignments.
The approach requires sets of orthologous gene sequences, but does not require (nor use) alignments of the sequences.
Instead, our method estimates the association between the query TF and a GO term independently for each species, and then combines these single-species association scores into a single score for the TFGO term pair.
As our method requires only a single GO map, only one of the species need functional annotation.
We use multiple-species Gomo to assign functional roles to TFs in bacteria, fungi and mammals, and validate the accuracy of these predictions using known sets of regulatory targets for a number of TFs.
We further validate the predictions made by multiple-species Gomo by conducting a false discovery rate (FDR) analysis of the predicted TFGO term associations.
The enhanced version of Gomo is available for download and Gomos functionality has been fully integrated with the Meme motif discovery tool (Bailey et al., 2009; http://meme.nbcr.net), so that motifs discovered by Meme can be sent with a single mouse click to Gomo for analysis.
2 METHODS 2.1 Incorporating comparative genomics into TF role prediction Our alignment-free approach for improving the motif-based prediction of the roles of TFs is quite straightforward.
In a nutshell, it works as follows.
The input consists of a DNA-binding motif, a GO annotation map and the promoter sequences for n genomes.
Using the method described in Bodn and Bailey (2008) (and see below), we compute an association score between the (putative) targets of the input TF motif and each GO term in the GO map.
We do this separately n times, each time using the promoter sequences from a different genome.
We then combine the n scores for each TFGO term pair into a single score.
The final output of the method is, for each TFGO term pair, the q-value of the combined score.
The details of the algorithm are given below, and illustrated in Figure 5 in Supplementary Material 1.
The final score of our method combines the evidence for a TFGO term association from each of the species.
This is done by combining the single-species scores (P-values) for a single TFGO pair by taking their geometric mean, which is a simple way to combine evidence from multiple P-values.
Thus, if St,i is the association score for GO term t computed using genome i[1,...,n], then the overall association score for term t is defined as St = ( i St,i )1/n .
(1) The distribution of this score is not easy to estimate analytically due to the non-independence of the P-values being combined.
Therefore, we use a permutation test to assign statistical significance (q-values) to the multiple-species association scores, St , as we describe below.
The permutation test we use for computing the statistical significance of St , the association score for GO term t with respect to the current TF motif, is based on essentially the same null model as the rank sum test.
The rank sum test null model assumes that the order of the gene names, when sorted by motif affinity score, is random.
Therefore, our test permutes the assignment of gene names to scores.
Because the binding affinity scores for orthologous genes are highly correlated, we permute the gene names for each species in exactly the same way.
Failure to do this results in a null model that overestimates the significance of some TFGO term associations (data not shown).
As illustrated in Supplementary Material 1, Figure 4, after each permutation of the gene-name-to-score relationships, we compute null scores for all terms t. In this study, we repeat this process for 100 permutations, resulting in 100 null scores for every GO term, t. For a given TF, different GO terms have very similar null score distributions (data not shown).
So, in order to increase the statistical power of the permutation test, we treat all sampled scores for a single TF motif as samples from a single null distribution.
This gives us 100x null scores for estimating the significance of St , where x is the total number of GO terms.
We compute empirical P-values for each real St by counting the number of null scores that are smaller than St and then dividing by the total number of null samples.
These P-values are then adjusted for multiple tests by conversion to q-values using the method of Storey et al.
(2004).
Our previous implementation of Gomo uses the Ama score to rank promoters as (putative) targets of a TF.
However, because the base composition of promoters in higher eukaryotes is highly variable (Sinha et al., 2008), binding affinity scoring methods that ignore this variability might predict TFGO term associations that are not biologically meaningful.
For this study, we therefore developed a new version of the Ama algorithm (part of the MEME Suite of tools; Bailey et al., 2009) that analytically estimates P-values for Ama scores based on a zero-order Markov model of the particular promoter sequence being scored (described in Fig.1 in Supplementary Material 1).
For reasons of computational efficiency, this is implemented assuming that the sequence has equal G and C content on a given strand, i.e.
Pr(G) = Pr(C), and likewise for A and T. The method computes analytical Ama score distributions for a range of GC contents, and uses linear interpolation to estimate the P-value of the Ama score of a 861 [17:11 17/3/2010 Bioinformatics-btq049.tex] Page: 862 860866 F.A.Buske et al.
sequence, based on its actual GC content.
Ama can also compute P-values that are not compensated for the GC content of individual sequences, but are based on a single, zero-order Markov model of all the promoters in a genome.
Compared to the motif-based HMM (Hmm0) introduced by Sinha et al.
(2008), which calculates empirical P-values for each sequence, our GC-compensated version of Ama, which calculates analytical P-values, is almost an order of magnitude faster.
The P-values computed by the two methods have a median correlation coefficient of 0.92 for the yeast motifs (Fig.2 in Supplementary Material 1).
This speedup is important, because it makes a web-based version of Gomo feasible.
Unless noted, in this study all results are based on GC-corrected Ama scores.
2.2 Evaluation methods and datasets We study the ability of our prediction method (Gomo) to correctly identify associations between the target genes of a TF and GO functional categories, given only the DNA-binding motif of the TF.
We focus on three relatively well-studied species: Escherichia coli, Saccharomyces cerevisiae, Homo sapiens.
For each species, we utilize its GO annotation, the sequences of its promoters, the sequences of promoters of orthologous genes from three additional species and a species-specific set of TF binding motifs.
To evaluate prediction accuracy, we create two sets of reference TFGO associations based on the known targets of the TFs in E.coli and S.cerevisiae, respectively (Supplementary Material 2).
(We do not create a reference set of associations for H.sapiens due to the relatively small number of known gene targets for human TFs.)
We measure the accuracy of the associations predicted by Gomo with these reference associations in terms of the area under the ROC curve (AUC).
As a second measure of prediction reliability, we use FDR analysis.
2.2.1 Evaluation using known TFGO term associations To create our reference sets of TFGO associations for E.coli and S.cerevisiae, we apply the approach described in our previous study (Bodn and Bailey, 2008).
For each organism, we first obtain a set of known gene targets for TFs.
We obtain the known gene targets of TFs from RegulonDB v6.2 (Gama-Castro et al., 2008; http://regulondb.ccg.unam.mx/) for E.coli and from MacIsaac et al.
(2006) for S.cerevisiae.
We then perform gene set enrichment analysis by applying the Fishers exact test (Fisher, 1958) to the intersection of the set of known targets of a single TF and the set of genes annotated with a given GO term.
We include a TFGO term pair in our reference set for the given organism if, after adjusting for multiple tests, the enrichment is significant at 0.01 level.
Consistent with previous researchers (e.g.
Sinha et al., 2008), we do not include any TFGO pairs containing non-specific GO termsterms that are annotated to >20% of genes in the given genome.
Our E.coli reference set of TFGO associations contains 87 TFGO pairs, and our S.cerevisiae set has 503 pairs.
We utilize our TFGO association reference sets to measure the accuracy of predictions made by Gomo.
We treat the TFGO pairs in the reference set for a given species as positives, and all other TFGO pairs as negatives.
Our accuracy metric is AUC50, AUC up to the 50th false positive (Gribskov and Robinson, 1996), when all TFGO terms are sorted by increasing score St .
This metric is appropriate because it emphasizes differences in accuracy among prediction methods where it matters to biologistsin the short list of most confident predictions made by a prediction method.
We compute the AUC50 for each TF represented by at least one TFGO pair in the reference set for a given species.
The AUC50 value will be 1 if Gomo assigns lower St scores to all the GO terms associated with the TF according to reference set (positive GO terms) than it does to other GO terms (negative GO terms).
It will be zero if 50 (or more) negative GO terms have lower St scores than the GO terms associated with the TF according to reference set.
Our final accuracy measure for the species is the average of the AUC50 values of the TFs that have GO terms in the reference set.
2.2.2 Evaluation using FDR To further evaluate the reliability of TFGO term associations predicted made by Gomo, we also perform FDR analysis.
FDR analysis allows us to estimate the fraction of predicted associations that are statistically significant, but cannot guarantee the biological significance of predictions.
Nonetheless, FDR analysis has been widely used for estimating the accuracy of both TF role predictions (Sinha et al., 2008) and TF binding site predictions (Kheradpour et al., 2007) when only incomplete or noisy validation sets are available.
As discussed in Section 2.1, Gomo computes the q-values of all TFGO term associations from their empirical P-values.
The q-value of a TFGO pair represents the minimum FDR at which that association would be considered significant.
Therefore, we report the number of associations detected at a q-value of 0.05.
When computing q-values, we combine the P-values of all TFGO pairs for a single organism across all TFs used as queries in order to adjust for all of the multiple tests conducted.
As a further check on our FDR estimates, we verify that no significant predictions are reported when the input sequences are permuted.
2.2.3 Binding motifs We perform our study using position-specific probability representations of TF binding motifs taken from the following sources.
For E.coli, we use 85 of the 88 TF motifs from the Prodoric database release 8.9 (http://prodoric.tu-bs.de/; Mnch et al., 2003).
(We discard three TF motifsMX000203, MX000181 and MX000160because they are highly similar to other motifs.)
For S.cerevisiae, we use the 124 yeast TF binding motifs from MacIsaac et al.
(2006).
For H.sapiens, we use the 56 H.sapiens TF motifs contained in the JASPAR CORE database release 2008 (Sandelin et al., 2004).
We use all of the above motifs in the FDR analysis, and the subsets referenced in the TFGO association reference sets for E.coli and S.cerevisiae in theAUC50 accuracy analysis.
When the original source gives the motif in terms of observed counts, we convert them to position-specific probability PWMs by adding pseudocounts of 0.01 times the average base frequencies in the organisms promoter sequences, B, before normalizing to probabilities.
2.2.4 Promoter sequences We create sets of promoter sequences for each of our key species, E.coli, S.cerevisiae and H.sapiens.
Then, for each key species, we identify the orthologous genes in each of three related species and construct three additional sets of promoter sequences.
Critically, in the related-species promoter sets, we use the gene name from the orthologous gene in the key species as the gene name for a promoter.
This allows us to use the GO map for the key species when we compute the association scores for the related species.
Our related species for E.coli (K12) are E.coli (CTF073), Salmonella typhimurium and Shigella flexneri 2a.
Our S.cerevisiae related species are S.paradoxus, S.mikatae and S.bayanus.
For H.sapiens, our related species are Mus musculus, Canis familiaris and Equus caballus.
Our definition of what a promoter is depends on the key species.
For S.cerevisiae and H.sapiens, we define the promoter to be the upstream region [relative to the transcription start site (TSS) of a gene].
Because prokaryotes organize their genes into transcriptional units and operons that are transcribed together, for E.coli we define promoters to be the sequence upstream of operons, rather than of genes.
We take operon information for E.coli K12 from RegulonDB v6.2 (Gama-Castro et al., 2008).
To identify orthologous genes in species related to E.coli, we use the Enterobacter Genome Browser (http://engene.fli-leibniz.de/) to search for best pairwise Blast hits to E.coli K12 genes.
For simplicity, we assume that the operons are not altered across the species, i.e.
the genes and their order stay the same in an operon across closely related species.
To identify orthologous genes in S.cerevisiae relatives, we use the mappings from Kellis et al.
(2003).
To identify genes orthologous to H.sapiens genes in related species, we use one-to-one ortholog gene maps obtained from Biomart (Smedley et al., 2009).
To create the promoter sequence sets for E.coli and S.cerevisiae and related species, we use the RSAT sequence extraction tool (Thomas-Chollier et al., 2008).
We study varying the size of the upstream region, as well as allowing it to overlap upstream open reading frames (ORFs).
We refer to the truncated promoters as the intergenic set, and to the promoters that (may) overlap upstream ORFs as the full set.
For H.sapiens and related species, we define the promoter to be the 1000 bp upstream of the TSS, and extract them using Biomart (Smedley et al., 2009).
862 [17:11 17/3/2010 Bioinformatics-btq049.tex] Page: 863 860866 Assigning roles to DNA regulatory motifs (a) (b) Fig.1.
Single-species Gomo prediction accuracy using transferred GO maps.
Each point shows the average AUC50 of TFGO term associations predicted by Gomo using the E.coli (a) or S.cerevisiae (b) GO map and TFs, and promoter sequences from the single given species.
The AUC50 is computed using a single TF, then averaged over TFs.
The X-axis shows the maximum upstream extent of promoter sequences, which are truncated at the first ORF.
The inset shows the phylogenetic tree of the corresponding species.
Branch lengths denote average substitutions per site.
2.2.5 GO annotation To create GO maps for the three key species, we use the E.coli GO annotation file v1.5, S.cerevisiae v1.1411 and H.sapiens v1.12, respectively.
From each of these files, we create a GO map file that lists, for each GO term, the gene names annotated with it.
Note that for E.coli, our promoters are upstream of operons, not genes, so our E.coli GO map maps GO terms to operons.
To create this map, we first use the GO annotation file for E.coli to assign to each operon the union of all GO terms associated with any gene contained in the operon.
3 RESULTS 3.1 Successful transfer of GO annotation to related species Our method for incorporating comparative genomics into TF role prediction depends on GO annotation being reliable when mapped from a gene in the key species to its ortholog in another species.
The validity of this assumption is borne out for our choice of related species for E.coli and S.cerevisiae by the results shown in Figure 1, which is based on running Gomo with promoters from a single species.
For a wide range of size definitions of the promoter regions, the accuracy (mean AUC50) of TFGO term associations predicted by Gomo for the related species is similar to the accuracy using the key species.
Indeed, for E.coli, the measured accuracy is slightly higher using the promoters from its related species (Fig.1a).
The E.coli reference set contains only 87 TFGO term pairs, thus its accuracy measurements are based on a fairly small sample.
The S.cerevisiae reference set is much larger (503 TFGO term pairs), and the prediction accuracy using S.cerevisiae promoters is very similar to that using promoters from two of its related species (Fig.1b).
Somewhat surprisingly, using promoters from S.mikatae yields lower accuracy than using those from S.bayanus, even though S.mikatae appears evolutionarily closer to S.cerevisiae based on multiple alignments of all orthologous intergenic regions (Kellis et al., 2003).
The phylogenetic trees shown in Figure 1 are for reference onlyour method does not use them.
We created the phylogenetic tree for enterobacter from our 1500 bp promoter sequences using the topology from Elena et al.
(2005).
The tree for yeast is from Kellis et al.
(2003) and is based on intergenic sequences.
3.2 Appropriate size for promoters The (maximum) size of the upstream region defined to be the promoter of a gene affects the accuracy of predictions made by Gomo, as seen in Figure 1.
For both enterobacter and yeast, prediction accuracy drops sharply if promoters are limited to upstream regions <500 bp.
Increasing the maximum promoter size seems to confer little or no increase in the accuracy of Gomo predictions for enterobacter species.
However, for yeast species the optimal promoter size may be closer to 7501000 bp, which is in agreement with the observation made by Thomas-Chollier et al.
(2008) that 99% of known regulatory elements in promoters are found in regions within 800 bp upstream of the TSS.
3.3 Benefits of our comparative genomics approach We now assess the benefit of using the proposed method of incorporating comparative genomics into TF role prediction.
To do this, we assess the accuracy of predictions made by single-and multiple-species Gomo in two ways.
First, for E.coli and S.cerevisiae we utilize gold standard sets of TFGO term relationships for each of these organisms.
We realize that these reference TFGO term sets are extremely incomplete due to the current lack of knowledge about the functions of many TFs.
Consequently, although useful for comparing the accuracy of algorithms, these gold standards will label many true relationships as false positives.
Therefore, we also perform FDR analyses of predictions on E.coli, S.cerevisiae and H.sapiens, and compare the number of statistically significant TF role predictions made by Gomo when using a single species or when using multiple species.
As a further check, we also determine that Gomo makes no predictions judged by FDR to be statistically significant (q0.05) when given shuffled promoters as input.
Compared with using a single species, Gomo using multiple species gives a substantial increase in prediction accuracy (mean AUC50) for the yeast species, and a slight increase for the enterobacter species (Fig.2; e.g.
compare curves labeled single-species intergenic and multiple-species intergenic).
For yeast, the increase in accuracy is statistically significant using promoters defined as 500, 750 or 1000 bp upstream regions (P<0.05, two-tailed, paired t-test).
This is true both for promoter defined as upstream regions of the given length (full) and for 863 [17:11 17/3/2010 Bioinformatics-btq049.tex] Page: 864 860866 F.A.Buske et al.
(a) (b) Fig.2.
Multiple-species Gomo prediction accuracy.
Each point shows the average AUC50 of TFGO term association predictions made by Gomo in the key species E.coli (a) or S.cerevisiae ( b).
Points labeled multiple-species use promoter sequences from the key species and three related species; Monkey results use Monkey (Moses et al., 2004) minimum P-value scores instead of Ama scores (Supplementary Material 1).
Points labeled single-species use promoter sequences from the key species only, and are shown for comparison.
The AUC50 is computed using a single TF, then averaged over TFs.
The X-axis shows the upstream extent of promoter sequences (full), or the maximum upstream extent when they are truncated at the first ORF (intergenic).
For clarity, standard error bars are shown for the full promoter sequence set only; standard error bars for the intergenic promoter set are similar.
regions truncated upon reaching an upstream ORF (intergenic).
The improvement in accuracy is due to the use of multiple genomesthe mean AUC50 is 0.63 using the four yeast species compared with only 0.53 using the single species (19% improvement, 1000 bp, full promoters).
Although using the full yeast promoter regions yields slightly better accuracy than using the truncated regions, this difference is not statistically significant.
With enterobacter, the increase in accuracy using multiple species is smaller than with yeast, and the relatively small size of the set of known enterobacter TFGO term associations (only 87 compared with 503 pairs) causes the error bars to be large.
However, in our FDR analysis below, only using multiple species allows Gomo to discover any statistically significant TFGO term associations in E.coli at all.
The multiple-species prediction accuracy results shown in Figure 2a suggest that the optimal approach for identifying TFGO term associations in E.coli is to define promoters as full (non-truncated) upstream regions of length 1250 bp.
However, the reference set for E.coli contains only 87 TFGO pairs, so it is not possible to draw any strong conclusions about the optimal size for upstream regions to use as enterobacter promoters.
Nonetheless, the results for S.cerevisiae (Fig.2b), which are based on a much larger reference set, support a similar promoter definition.
Multiple-species S.cerevisiae predictions are most accurate using full promoters of length 1000 bp.
Longer regions appear to decrease the accuracy of both single-and multiple-species Gomo, as would be expected if regions farther than 1000 bp from the TSS were less likely to contain TF binding sites.
Since the Ama score averages the motif affinity along the entire defined promoter region, the signal-to-noise ratio decreases when the region is made too long.
As a further evaluation of the plausibility of TFGO term predictions made by Gomo, we perform a FDR analysis.
As the first step in our analysis, we follow Sinha et al.
(2008) and perform a test where we scramble all of the promoter sequences and input them to Gomo.
For all three key species, both single-and multiple-species versions of Gomo using GC-compensated Ama P-values report zero predictions with q0.05.
This provides a negative control on the reliability of the q-values reported by Gomo.
However, when we score the scrambled yeast species promoters using non-GC-compensated Ama P-values, we get 41 significant predictions using multiple-species Gomo.
For mammalian species, the number of significant predictions using scrambled promoters is 935 (H.sapiens) and 1403 (multiple-species Gomo).
This makes it clear that normalizing the TF binding affinity scores for the base content of the promoter sequences is important for accurate estimation of the FDR.
In what follows, we use GC-compensated Ama P-values as input to Gomo.
Having established that Gomo reports no significant (q0.05) TFGO term pairs when given scrambled promoters as input, we now count the number of significant pairs reported when using the real promoters.
Rather than counting all significant TFGO term pairs, we only count significant pairs for the most specific GO term.
In other words, if for a given TF, the GO term neuroblast division and its parent term neurogenesis are both deemed significant by Gomo, we only include the former term in our count.
Counting in this way is appropriate because GO is a hierarchy with the least specific terms at the base.
If a TF is associated with a GO term, it is implicitly associated with all the parents of that term.
Since a method that detects associations between a TF and highly specific GO terms is more useful than one that reports only general GO terms, we also measure the average depth of the most specific GO terms predicted by Gomo.
These results are summarized for both single-and multiple-species Gomo in Table 1.
Substantially, more significant TFGO term pairs for E.coli, S.cerevisiae and H.sapiens are predicted by multiple-species Gomo compared with single-species Gomo (Table 1, significant TFGO term pairs).
We observe increases of 75 and 200% in the number of significant pairs for S.cerevisiae and H.sapiens, respectively.
For E.coli, there are actually no significant pairs (q0.05) using the single-species approach, but 14 pairs are significant when we use all four enterobacter promoter sets as input.
The average number of significant GO terms predicted for TFs by Gomo increases accordingly (Table 1, GO terms per TF tested).
Using four yeast species, multiple-species Gomo predicts about six most-specific GO terms per TF; using the four mammal species, multiple-species Gomo predicts 20 most-specific GO terms per TF.
864 [17:11 17/3/2010 Bioinformatics-btq049.tex] Page: 865 860866 Assigning roles to DNA regulatory motifs Table 1.
Improvement in TF role prediction using comparative genomics Escherichia coli K12 Saccharomyces cerevisiae Homo sapiens Single Multiple Increase Single Multiple Increase Single Multiple Increase species species (%) species species (%) species species (%) Significant TFGO term pairs 0 14 NA 420 733 75 371 1112 200 GO terms per TF tested 0 0.16 NA 3.4 5.9 75 6.6 19.8 200 Covered TFs 0 9 NA 99 113 14 48 56 17 Term specificity 0 4.0 NA 4.5 4.6 2 3.8 4.2 11 TFs tested 85 124 56 The table shows FDR (q0.05) results for single-and multiple-species Gomo.
The results shown are the total number of most-specific significant pairs (significant TFGO term pairs), the average number of most-specific GO terms per TF tested (GO terms per TF tested), the number of TFs with at least one significant TFGO term pair (covered TFs), the average depth in the GO hierarchy of significant GO terms (term specificity), and the total number of TFs in each experiment (TFs tested).
All results are for GC-compensated Ama scores and full promoters of 500, 750 and 1000 bp for enterobacter, yeast and mammals, respectively.
NA, not applicable.
Very few predictions are made in E.coli when using the four enterobacter species (0.16 terms per TF tested), indicating that the sensitivity of multiple-species Gomo in enterobacter is very low.
Using multiple species also increases the chance that Gomo will predict that at least one GO term is significantly associated with a given TF.
No TFGO term associations are predicted by Gomo using E.coli promoters alone, whereas using the four enterobacter species, multiple-species Gomo predicts significant associations for nine (out of 85) TFs (Table 1, covered TFs).
The number of TFs with at least one significant GO term increases by 14% for S.cerevisiae and 17% for H.sapiens when we apply our multiple-species approach.
In S.cerevisiae and H.sapiens, single-species Gomo is more successful at finding at least one significant GO term for each TF than it is in E.coli, covering 99 (out of 124) TFs in S.cerevisiae, and 48 (out of 56) in H.sapiens.
However, using multiple species results in improvement in this regard as well for S.cerevisiae and H.sapiens.
Multiple-species Gomo identifies at least one significant GO term for almost all yeast TFs (113 out of 124), and for all 56 H.sapiens TFs tested.
Importantly, the specificity of predicted GO terms for a given TF increases when using our multiple species with Gomo (Table 1, term specificity).
The increase in specificity (as measured by the minimum distance from the predicted term to the root of the GO hierarchy) increases marginally using four species for S.cerevisiae (2%), and somewhat more (17%) when using four mammal species for H.sapiens, compared to using a single species.
When we compare the sets of predictions made on H.sapiens using the single-and multiple-species approach, respectively, we find that only 72 significant predictions from the most specific set in the single species have no or only less specific counterparts in the multiple-species results.
In contrast, 950 of the multiple-species results are more specific than corresponding single-species predictions, or are not captured by the single-species approach at all.
Given that the total numbers of significant predictions are 371 for single-species and 1112 for multiple-species (Table 1), we observe a substantial increase in both the number of significant predictions and the specificity of GO terms when using multiple-species Gomo.
Since more specific GO terms convey more detailed biological insight, the predictions made by multiple-species Gomo are more informative than those made using a single species.
Finally, Gomo can be used to create role-centric regulatory maps, where TF motifs are connected via the predicted GO terms considered significant (Supplementary Material 1, 3 and 4).
This representation of the data facilitates the identification of groups of TFs that are collectively involved in a particular biological process.
Role-centric maps can also shed light on when secondary binding motifs (Berger and Bulyk, 2009) are functionally distinct from the primary binding motif.
4 DISCUSSION We have presented a comparative genomics approach for assigning biological roles to sequence motifs using a form of gene set enrichment analysis.
The approach does not require multiple alignments because role predictions are made independently for each comparative genome and then combined across genomes.
This means that the method does not assume that the location and orientation of binding sites is conserved across species.
The method does require that orthologous genes be identified in the species being utilized, as would also be the case for a method employing multiple alignments.
The approach also assumes that the functions of orthologous genes, and the DNA-binding affinity of the regulatory molecule, have been conserved in the species being used.
Although the above assumptions are no doubt sometimes violated, our comparative genomics approach nonetheless substantially improves the sensitivity of TF role predictions.
Our principal result is that it is possible to substantially improve the prediction of the association of a DNA-binding motif with an annotation term by mapping the annotation from a key species to related species, and combining the association scores for a single motif and term across species.
The approach requires only that we are able to compute the P-value of the motif-term association in each species, and we combine across species by taking the geometric mean of the P-values.
A simple permutation test then assigns significance values to the combined motif-term score.
Since our method is alignment-free, it avoids problems that would be caused by imperfect alignments or motif drift (Moses et al., 2006).
865 [17:11 17/3/2010 Bioinformatics-btq049.tex] Page: 866 860866 F.A.Buske et al.
Our FDR analysis has shown the importance of compensating motif affinity scores for the base content of the sequences being scored.
We have demonstrated that our GC-compensatedAma scores pass a shuffled sequence test, yielding no spurious significant predictions using such random data.
We have also shown that our implementation can compute such GC-compensated scores fast enough to make a web-based service feasible.
It should be noted that Gomo is not limited toAma-derived gene scores but can work on any sequence scoring scheme.
For example, Hmm0 (Sinha et al., 2008) can compute motif affinity scores using more than one TF motif at a time, which enables Gomo to investigate the role of synergistic TFs.
For the particular case of predicting TFGO term associations, we have shown that our method is not particularly sensitive to the size of the upstream region designated as the promoter of a gene.
For species as diverse as enterobacter and mammals, using regions of 1000 bp upstream of the TSS will probably work about as well as any other reasonable definition.
Our results indicate that one should not truncate putative promoters at the nearest upstream ORF, but include overlapped ORF sequence up to the 1000 bp (or other) limit.
This seems to suggest that closely spaced genes may reciprocally harbor regulatory sequence elements.
We have shown that using a H.sapiens TF motif with our multiple-species version of Gomo almost always results in at least one significant prediction of an associated GO term, and in 20 significant terms on average.
Most yeast TF motifs also yield at least one significant prediction, but very few E.coli motifs do.
The relative failure of even our multiple-species approach on enterobacter might be due to the way we aggregate gene annotations for all genes in an operon, or it might be due to the relative sparsity of GO annotation for bacteria due to their simple cellular structure.
However, other factors than just the number of terms in the GO hierarchies for bacteria, fungi and mammals (1654, 4578 and 9409, respectively) may be at play here.
Although we have focused specifically on utilizing this approach with TF motifs and GO annotation via the Gomo software, the general method should be equally applicable to other types of sequence motifs (e.g.
microRNA-binding motifs) and other types of annotation (e.g.
metabolic pathways or gene sets from analyses of expression data).
Gene set enrichment analysis approaches similar to single-species Gomo have previously been shown to be useful with these and other types of motifs and annotation sources (e.g.
Sinha et al., 2008).
Since nothing in the implementation of Gomo is specific to TFs and GO annotation, multiple-species Gomo can perform analyses using any type of DNA-binding motif expressed as a PWM and any mapping of gene names to functional terms (e.g.
pathway names or tissue types).
In the future, we will apply multiple-species Gomo to annotate more extensive collections of motifs with additional (non-GO) types of functional annotation.
ACKNOWLEDGEMENTS The authors like to thank James Johnson for his skillful help during the implementation of multiple-species Gomo and for developing the web server under the supervision of T.L.B.
and F.A.B.
Funding: T.L.B and M.B.
are funded by NIH/NCRR award R01 RR021692 and by the ARC Centre of Excellence in Bioinformatics.
F.A.B and D.C.B are funded by UQ International Research Tuition Award.
Conflict of Interest: none declared.
ABSTRACT Summary: The advent of next-generation sequencing for functional genomics has given rise to quantities of sequence information that are often so large that they are difficult to handle.
Moreover, sequence reads from a specific individual can contain sufficient information to potentially identify and genetically characterize that person, raising privacy concerns.
In order to address these issues, we have developed the Mapped Read Format (MRF), a compact data summary format for both short and long read alignments that enables the anonymization of confidential sequence information, while allowing one to still carry out many functional genomics studies.
We have developed a suite of tools (RSEQtools) that use this format for the analysis of RNA-Seq experiments.
These tools consist of a set of modules that perform common tasks such as calculating gene expression values, generating signal tracks of mapped reads and segmenting that signal into actively transcribed regions.
Moreover, the tools can readily be used to build customizable RNA-Seq workflows.
In addition to the anonymization afforded by MRF, this format also facilitates the decoupling of the alignment of reads from downstream analyses.
Availability and implementation: RSEQtools is implemented in C and the source code is available at http://rseqtools.gersteinlab.org/.
Contact: lukas.habegger@yale.edu; mark.gerstein@yale.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on June 23, 2010; revised on November 5, 2010; accepted on November 8, 2010 1 INTRODUCTION The advent of next-generation sequencing technologies has revolutionized the study of genomes and transcriptomes.
In particular, the application of deep sequencing approaches to transcriptome profiling (RNA-Seq) is increasingly becoming the method of choice for studying the transcriptional landscape of cells (Hillier et al., 2009; Mortazavi et al., 2008; Wang et al., 2009).
Typically, the first step in this analysis is the alignment of the To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
sequence reads to a reference sequence set.
Recently, a number of different alignment tools have been developed to map short reads in an efficient manner (Trapnell and Salzberg, 2009).
While much progress has been made on this front, there is still a great need for a set of software tools that facilitate the downstream analysis of mapped RNA-Seq reads.
Further, two other issues remain to be addressed.
First, the immense file size of next-generation sequencing data poses many challenges in terms of data processing, storage and sharing.
Secondly, mechanisms to protect personal confidential genetic information need to be established.
With the birth of personal genomics, sequencing data stems fundamentally from individuals, and this type of data cannot be distributed as easily because significant privacy concerns arise with sharing all the sequence variations of a particular individual (Greenbaum et al., 2008; Lowrance and Collins, 2007).
One critical challenge for genomics, then, is to devise new data summaries that allow the sharing of large amounts of information from sequencing experiments without exposing the genotypic information of the underlying individual (Supplementary Material).
Although many data formats have been developed such as SAM (Li et al., 2009), there is no practical solution yet that addresses the privacy concerns when sharing large sequence alignment files.
Addressing this challenge is precisely what we have endeavored to do in putting together the Mapped Read Format (MRF), a format that allows data summaries to be exchanged, enabling many aspects of the RNA-Seq calculation to be performed such as expression measurements, but that also detaches the actual sequence variation in a person into separate files.
Further, it provides a very clear way of linking these two pieces of information so that the data summaries can be subsequently conjoined back to the original sequences for more in-depth analyses with potentially confidential data.
Here, we present an overview of a flexible suite of tools (RSEQtools) that are designed to facilitate easily customizable workflows and efficient pipeline building for the analysis of RNA-Seq experiments using this compact format (Fig.1).
Briefly, we first convert the aligned reads into MRF and thus decouple the alignment step from the downstream analyses.
RSEQtools implements several modules using this standardized format for performing common RNA-Seq analyses, such as expression quantification, discovery of transcribed regions, coverage computations annotation manipulation, etc.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:39 16/12/2010 Bioinformatics-btq643.tex] Page: 282 281283 L.Habegger et al.
Fig.1.
Schematic overview of RSEQtools.
Mapped reads are first converted into MRF from common alignment tool output formats, including SAM.
The resulting MRF files can be divided in two files: one with the alignment only and another with the corresponding sequence reads.
The read identifiers provide a mapping between the two files.
Then, several modules perform the downstream analyses independently from the mapping step, such as expression quantification, visualization of the mapped read and the calculation of annotation statistics, etc.
Other tools have been developed based on this framework to perform more sophisticated analyses such as transcript assembly, isoform quantification (IQSeq, http://rnaseq.gersteinlab.org/IQSeq), fusion transcript identification (FusionSeq, http://rnaseq.gersteinlab.org/fusionseq), as well as aggregation and correlation of signal tracks (ACT, http://act.gersteinlab.org).
2 FEATURES AND METHODS 2.1 MRF and converters MRF only stores a minimal set of information, i.e.
information that cannot be derived from the MRF data itself.
This has the advantage of keeping the format succinct, while still capturing the relevant information for most analyses.
MRF consists of three components: comment lines (optional) denoted by a leading # sign, a header line and the mapped reads.
The header line specifies the data type of each column: AlignmentBlocks, Sequences, QualityScores and QueryID.
The column type AlignmentBlocks is required and represents the mapped reads.
Each alignment block contains the coordinates with respect to the reference genome to which the read aligns as well as the read coordinates.
A read spanning multiple regions, e.g.
multiple exons, is denoted by multiple alignment blocks that are separated by a comma.
Paired-end reads can be represented by using a set of alignment blocks for each end, which are separated by the | symbol.
By using this format, it is straightforward to specify both gapped and paired-end alignments.
The RSEQtools package includes various utilities to convert the output of several mapping tools into MRF.
A converter for the commonly used SAM format is included as well.
The first example below represents two paired-end reads where one end is spliced, whereas the second example shows two un-spliced single-end reads with their associated QueryIDs: # Example 1 AlignmentBlocks chr2:+:601:630:1:30,chr2:+:921:940:31:50|chr2:+:1401:1450:1:50 chr9:+:451:460:1:10,chr9:+:831:870:11:50|chr9:+:945:994:1:50 # Example 2 AlignmentBlocks QueryID chr4:-:1221:1270:1:50 1 chr16:+:511:560:1:50 2 The optional types Sequences, QualityScores and QueryID provide additional information.
In particular, the confidentiality issues can be addressed by generating two files: one including the alignments and a second one containing the sequences such as a FASTQ file.
The former is useful for most analyses and can be publicly shared because it does not contain confidential information, whereas the latter can be subjected to a higher level of security and control.
The two files can be conjoined, if necessary, by using the common QueryID as shown in Figure 1.
2.2 RNA-Seq analysis with RSEQtools The RSEQtools suite contains a set of modules to perform a large variety of tasks including the quantification of expression values, manipulation of gene annotation sets, visualization of the mapped reads, generation of signal tracks, the identification of transcriptional active regions and several auxiliary utilities (Supplementary Table S1).
Genome annotation tools: to generate a splice junction library from any annotation set, we extract the genomic sequences of all the exons and synthetically create all splice junctions specified in the annotation set.
This splice junction library can be used in combination with the reference sequences.
A second tool is particularly useful when estimating expression 282 [13:39 16/12/2010 Bioinformatics-btq643.tex] Page: 283 281283 RSEQtools levels.
In order to capture the information of the various transcript isoforms, a gene model is required.
The module mergeTranscripts collapses the transcript isoforms into a single gene model by either taking the union or intersection of the exonic nucleotides.
Quantification of gene expression: one of the key features of RNA-Seq is the quantification of expression at different levels.
Hence, a key module calculates the gene expression values for a given annotation set and a collection of mapped reads in MRF format.
The annotation set specifies which elements will be quantified.
The program mrfQuantifier calculates RPKM (reads per kilobase per million mapped reads) values at the nucleotide level (Mortazavi et al., 2008).
Briefly, for a given entry in the annotation set (typically an exon or gene model), the number of nucleotides from all the reads that overlap with this annotation entry are added up and then this count is normalized by sequence length of the annotation entry (per killobase) and by the total number of mapped nucleotides (per million).
This calculation is not performed at the transcript level, which requires a more sophisticated analysis (Guttman et al., 2010; Trapnell et al., 2010).
Visualization of mapped reads: the RSEQtools package also contains various tools for visualizing the results in genome browsers, by means of wiggle (WIG) and bedGraph files, which are commonly used to represent a signal track of mapped reads.
Also, a GFF file can be generated from MRF files to visualize splice junction reads (example in Fig.1).
Identification of transcriptionally active regions (TARs): transcribed regions can be identified de novo by performing a maxGap/minRun segmentation (Kampa et al., 2004; Royce et al., 2005) from the signal files using the wigSegmenter program.
Briefly, the signal is first thresholded to identify transcribed elements.
Contiguous elements whose distance is less than maxGap are joined together and then filtered if the final size is less than minRun.
This type of analysis is particularly useful in discovering novel TARs such as small RNAs, etc.
MRF selection and auxiliary utilities: lastly, RSEQtools includes a set of utilities to easily manipulate MRF files and a collection of format conversion tools allowing for rapid pipeline development.
Implementation and run time: the modules of the RSEQtools suite were implemented in C and the code was optimized in order to efficiently handle large datasets.
The importance of code scalability cannot be overemphasized in a time where datasets become increasingly large and easily exceed several gigabytes.
For example, the conversion of an ELAND export file (uncompressed file size: 4 GB; total number of reads: 20 million; number of mapped reads: 12 million) to MRF takes 2 min and the resulting MRF file is significantly smaller (400 MB uncompressed, 130 MB compressed with gzip).
Converting the same ELAND export file to SAM generates a file of 3.1 GB (uncompressed) and the corresponding BAM file has a size of 1.2 GB.
The subsequent quantification of gene expression using mrfQuantifier requires 45 s to calculate estimates for about 20 000 genes.
In addition, the modularity of RSEQtools also enables the development of additional programs in any programming language and their seamless integration into this framework.
Finally, most modules use STDIN and STDOUT to process the data, making them suitable to be integrated into an automated pipeline.
3 CONCLUSIONS In summary, RSEQtools contains a number of useful and highly specific modules that can rapidly analyze RNA-Seq data.
The MRF format has two major features: it allows the decoupling of downstream analysis from the mapping strategy and addresses the issue of confidentiality that is intrinsic in any sequencing experiments involving human subjects.
By separating the actual sequencing reads from the alignments, MRF provides a mechanism to protect the private genotypic information of the underlying individual.
Although this approach removes the most obvious genotypic features, other distinctive attributes do remain.
First of all, the information in a MRF file is at least equivalent to that in traditional expression array, which can potentially identify the underlying individual.
Secondly, some information about structural variants may be contained in the MRF file of an RNA-Seq experiment.
However, it is not obvious how to extract genotypic information from a subset of structural variations just affecting genes.
In addition, inferring structural variations from RNA-Seq data as opposed to DNA sequencing would be more complicated due to the presence of alternative splicing.
Another advantage of storing the alignments without the underlying sequences is that it saves space, especially as reads become longer.
Moreover, a possible future extension is the development of a specific compression schema that could further reduce the size of the files.
In addition, this data format could be easily applied to sequence alignments obtained from other high-throughput functional genomic assays such as ChIP-Seq or chromosome conformation capture (3C).
ACKNOWLEDGEMENT We thank Raymond Auerbach for critical reading and editing as well as Wasay Hussain for testing of the software.
Funding: National Institutes of Health.
Conflict of Interest: none declared.
ABSTRACT Summary: We present a large-scale implementation of the Rankprop protein homology ranking algorithm in the form of an openly accessible web server.
We use the NRDB40 PSI-BLAST all-versus-all protein similarity network of 1.1 million proteins to construct the graph for the Rankprop algorithm, whereas previously, results were only reported for a database of 108 000 proteins.
We also describe two algorithmic improvements to the original algorithm, including propagation from multiple homologs of the query and better normalization of ranking scores, that lead to higher accuracy and to scores with a probabilistic interpretation.
Availability: The Rankprop web server and source code are available at http://rankprop.gs.washington.edu Contact: iain@nec-labs.com; noble@gs.washington.edu 1 INTRODUCTION Rankprop (Weston et al., 2004) is a network-based inference algorithm for identifying subtle protein sequence similarities, corresponding to remote homology relationships or to structural similarities.
The algorithm operates on a protein similarity network, a graph in which each node is a protein and each weighted edge connecting two proteins indicates their similarity.
Such a network can be built using existing tools, such as PSI-BLAST (Altschul et al., 1997).
The key idea of the Rankprop algorithm is to extract global information from a protein similarity network by propagating outward from a user-specified query protein.
Effectively, the algorithm sums over all possible paths from the query to each target protein.
Thus, after propagation, the resulting activation score for each node includes global information about that proteins relationship to the query.
Ranking proteins by these scores is analogous to performing a database search using a tool such as PSI-BLAST, except that the ranking induced by Rankprop reflects the global topology of the protein similarity network.
In Weston et al.
(2004), PSI-BLAST is used to measure sequence similarity, and the unnormalized weight for the edge from node i to node j is Wij =exp(Sj(i)/ ), where Sj(i) is the PSI-BLAST E-value assigned to protein i given query j, and the parameter is a positive constant.
Edges are only included in the network for To whom correspondence should be addressed.
E-values smaller than a fixed threshold.
We obtain a stochastic connectivity matrix M for the protein similarity network by row-normalizing edge weights Wij to obtain transition probabilities: Mij =Wij/ j Wij .
Given such a network and a query sequence q, the Rankprop algorithm is simple to describe.
First, all nodes are assigned initial activation scores that reflect each target proteins similarity to q.
Like the edge weights, these scores are computed from PSI-BLAST E-values using the same equation.
At each iteration of the algorithm, the activation score at a given node is replaced by the weighted sum of the scores from all of its incoming edges.
The update rule includes a diffusion constant that controls the rate of diffusion through the network.
Formally, we define the initial activation scores as P0i =exp(Sq(i)/ ).
Viewing Pt as the column vector of activation levels at iteration t, the algorithm is given by Pt+1i =MPti +P0i if Pi =q and Pt+1i =1 otherwise, where (0,1).
One can show that this iterative procedure converges to a fixed point, which in practice happens in a small number of iterations.
The output of the Rankprop algorithm is a ranking of the nodes in the network according to their final activation values.
Proteins that receive a high activation score are linked to the query via many strongly weighted paths and vice versa.
A multidomain query protein will produce strong matches to any target protein that contains one or more of the query domains.
A single domain query A may connect through a multidomain protein AB to infer a false relationship with B.
However, previous work (Weston et al., 2004) has found that as long as the query sequence is connected to many other proteins, then the true homologs will be mutually reinforcing and receive a higher rank.
In this work, we extend the original Rankprop algorithm in two ways: (1) improving accuracy by propagating simultaneously from proteins that are very closely related to the query, and (2) improving the interpretability of the scores produced by Rankprop by empirically mapping them to probabilities.
The mapped score can be interpreted as the probability that the target protein is a member of the same SCOP superfamily as the query.
We also announce the availability of a free web server that allows individual queries against a protein similarity network derived from the NRDB40, comprising 1.1 million targets.
2 METHODS The Rankprop server uses the PSI-BLAST all-versus-all similarity matrix for NRDB40 provided by the PairsDB website (Heger et al., 2008).
NRDB40 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
I.Melvin et al.
is a subset of the non-redundant sequence database, filtered so that no pairs exhibit >40% sequence identity.
We generalize the Rankprop algorithm to accept a set Q of query proteins, rather than a single query protein.
To use this extra information we perform propagation as usual, but we constrain the activation scores for all the query points such that they are highly ranked.
In particular, we choose the set Q to be all the proteins that have a match with the initial query q with a PSI-BLAST E-value <0.001.
We then constrain our algorithm to have Pj = 1Sq(j),jQ.
This modification is useful because, instead of propagating from a single query source node in the graph, we can propagate from several source nodes that all belong to the same family or superfamily that we are searching for.
The original Rankprop algorithm outputs scalar values that are not directly interpretable.
In the new version of the algorithm, we map each Rankprop score to an estimate of the probability that the corresponding query and target proteins belong to the same structural superfamily.
We employ the SCOP database (Murzin et al., 1995) to compute a histogram of empirical frequencies of the activation levels Pi for each protein i.
More specifically, we choose bin centers vk and compute the following quantities: nk , the number of times Pi falls into bin vk , and sn, the number of times that the latter occurs and i is in the same superfamily as the query.
We are interested in the value sk/nk , which can be interpreted as the probability for each activation value bin of the target being in the same superfamily as the query.
We choose the bin centers v= (0,0.01,0.02, ...,0.2,0.3,...,1), and we enforce monotonicity in the final output by setting pi/ni =pi1/ni1 if pi/ni < pi1/ni1.
3 RESULTS Table 1 compares our large-scale Rankprop results with PSI-BLAST (using NRDB40 and the same blastpgp parameters as PairsDB:-j 10-e 1-h 0.001-b 10000-v 10000 ) and the previously published version of Rankprop (using the SWISSPROT database, 108k proteins).
Rankprop NRDB40 is a straightforward scaling up of the previous Rankprop algorithm to NRDB40.
In addition, Rankprop+homologs NRDB40 employs the extensions described in Section 2.
Accuracy is measured following the methodology given in Weston et al.
(2004): SCOP version 1.59 is split into train and test portions, and hyperparameters are chosen by using the training set.
Then, each test protein is treated as a query, and the quality of a methods protein ranking is measured by using the area under the receiver operating characteristic (ROC) curve, up to the first (ROC1) or 50th (ROC50) false positive.
We report results as average ROC1 and ROC50 scores across all 3083 test proteins.
Using a larger network yields improvements across all four performance metrics, and propagating from multiple queries improves the performance still further.
A Wilcoxon signed rank test, corrected for multiple tests, shows that all differences in Table 1 are significant at 0.01, except for the three pairs of methods marked with asterisks.
We also evaluate the performance of Rankprop using a combined ROC curve across all the queries in our test set, following the protocol of Altschul et al.
(1997).
Figure 1 shows the combined ROC curves for Rankprop NRDB40 (ranked by activation value), Rankprop+homologs NRDB40 (ranked by probability) and PSI-BLAST (ranked by E-value).
Compared with average per-query ROC scores, the combined ROC curve requires that scores are well Table 1.
Ranking accuracy Family Family S-Fam S-Fam Method ROC1 ROC50 ROC1 ROC50 PSI-BLAST 0.833 0.851 0.609 0.628 RankProp SWISSPROT 0.816 0.906 0.592 0.725 RankProp NRDB40 0.872 0.923 0.696 0.779 RankProp+homologs NRDB40 0.884 0.928 0.710 0.775 *Indicate pairs of values that are not different at P < 0.01 (Wilcoxon signed rank).
0 0.2 0.4 0.6 0.8 1 0 500 1000 1500 2000 2500 F ra ct io n of P os iti ve s False positives PSI-BLAST Rankprop SWISSPROT sigma 100 homologs e<1e-3 sigma 100 Fig.1.
Combined ROC curve across multiple queries.
For each method, search results from 3083 queries were sorted into a single list.
The figure plots, for varying thresholds in the ranked list, the fraction of all known homologs (SCOP superfamily members) that fall above the threshold, as a function of the number of non-superfamily members above the threshold.
calibrated from one query to the next.
The figure shows that the mapping of Rankprop scores to probabilities significantly improves the calibration, yielding better performance than PSI-BLAST for all but the first few false positives (across 3083 queries).
The Rankprop web server first looks for an exact match of the query sequence against the sequences in NRDB40.
If such a match is found, the server will retrieve the precomputed PSI-BLAST results from the database and then apply the Rankprop algorithm.
In this case the server takes around 90 s to process a query.
If the sequence is not found in the database, then the server will run PSI-BLAST first, which on average takes an additional 15 min.
Funding: National Institutes of Health award (R01 GM074257).
Conflict of Interest: none declared.
Abstract Neurological disorders comprise a variety of complex diseases in the central nervous system, which can be roughly classified as neurodegenerative diseases and psychiatric disorders.
The basic and translational research of neurological disorders has been hindered by the difficulty in accessing the pathological center (i.e., the brain) in live patients.
The rapid advancement of sequencing and array technologies has made it possible to investigate the disease mechanism and biomarkers from a systems perspective.
In this review, recent progresses in the discovery of novel risk genes, treatment targets and peripheral biomarkers employing genomic technologies will be dis-cussed.
Our major focus will be on two of the most heavily investigated neurological disorders, namely Alzheimers disease and autism spectrum disorder.
Introduction Neurological disorders include a wide spectrum of diseases in the central nervous system (CNS).
Up till now, hundreds of neurological disorders have been classified, with symptoms varying from cognitive dysfunction to manic behavior or depression [1].
Due to the complex nature of this group of diseases, it is difficult to identify the mechanisms using conven-tional methodologies, where only small pathways around specific target genes are investigated.
The advent of systems biology approaches has made it possible to study these complex problems from the whole-genome perspective.
In the recent years, genomic technologies have been increasingly applied to the investigation of neurological disorders [2].
Excit-ing discoveries have thus emerged including novel risk genes, peripheral biomarkers and treatment targets.
For the conve-nience of the limited space, we will mainly focus on two of the most studied neurological disorders, Alzheimers disease (AD) and autism spectrum disorder (ASD).
AD is a major form of neurodegenerative diseases [3].
AD starts from memory loss and cognitive deficit in the early stage and gradually evolves into severe dementia in the late stage.
The pathological hallmarks of AD include extracellular deposit of amyloid plaques and intra-neuronal neurofibrillary tangles (NFT).
Although the disease-causing mutations have been identified for the familial early-onset AD (FEOAD), the genetic landscape has been perplexing nces and Han G et al/ Genomics in Neurological Disorders 157 for the late-onset AD (LOAD) that constitutes 95% of all AD patients [4].
The prevailing hypothesis for the disease mechanism of AD has been primarily based on the studies of FEOAD, which advocates the central role of amyloid-b (Ab) in the chain of events leading to neuronal death and cognitive and behavioral symptoms.
However, Ab-based interventions have not been successful in the clinical trials so far [5].
Due to the lack of effective treatment for curing or slowing down AD, it becomes imperative to search for novel risk genes and drug targets, as well as biomarkers for early diagnosis.
Autism spectrum disorder is a neurodevelopmental disorder characterized by social and communication deficit as well as stereotyped and repetitive behaviors [6].
According to a recent survey, 1 in 68 US children has ASD.
In contrast to AD, the disease onset for ASD starts from 3 years of age to early child-hood.
The gender ratio is approximately 4:1 disfavoring boys.
Like other psychiatric disorders, there are no clear pathologi-cal hallmarks for ASD [1].
It is believed that brain wiring is altered in ASD children, although the exact interplay between gene and environment has not been clarified.
In terms of the genetic factors, some types of ASD may be caused by rare mutations, while others may be due to the combination of common variations [7].
The genetic alterations in ASD are also more complex than those in AD, which include copy number variation, insertion, deletion and single nucleotide polymor-phism (SNP).
In addition to the genetic and environmental factors, prenatal and perinatal factors may also contribute to the development of ASD.
Genomic studies of neurological disorders involve the investigation of the genome, transcriptome and epigenome (Figure 1).
There are two types of technologies available for genomic studies, including sequencing and various array plat-forms.
For the investigation of genomic variation, the samples Figure 1 Application of genomic technologies to the investigation of n High quality brain tissues can be used in transcriptome study, in add genotyping and whole exome/genome sequencing generally come from can also be performed on peripheral blood and cerebrospinal fluid reprogrammed or trans-differentiated into neurons for comprehensive generally come from peripheral blood, although saliva has also been used.
For the investigation of transcriptome, brain tissue is the most studied since it is more relevant to the disease mechanism.
The peripheral blood and cerebrospinal fluid (CSF) have also been investigated, mostly for the discovery of novel biomarkers.
These three tissues have also been utilized in the investigation of epigenomic alteration.
In addition, skin fibroblast has been increasingly used in induced pluripotent stem cell (iPSC) technologies.
Recent advances in these fields will be summarized in the following sections.
Brain transcriptome studies Since the disease mechanism for most of the neurological disorders is still under debate, it is necessary to conduct inves-tigation from a systems perspective.
In brain transcriptome studies, information regarding gene expression at the whole genome level can be extracted, and the dysregulation of gene expression in a disease condition can be revealed by comparing the gene expression with that from the matched healthy con-trols.
Microarray platforms have been the main workhorse for brain transcriptome studies due to the mature technology and low cost.
Sequencing technology has been increasingly used since 2008, but generally limited to small sample size due to the high cost.
Although it is extremely challenging to collect relevant brain tissues for transcriptome studies consid-ering the stringent requirement of short post mortem delay, dozens of brain transcriptome studies have already been per-formed and much of the original data have been released to the public [8,9].
Aberrations in the control of gene expression might contribute to the initiation and progression of AD [10] and other neurological disorders.
In a recent work, Zhang et al.
eurological disorders ition to the examination of pathological hallmarks.
Samples for peripheral blood or saliva.
Transcriptome and epigenome profiling in addition to other biomarker studies.
Skin fibroblasts can be analysis of the dysfunctional network in patients.
158 Genomics Proteomics Bioinformatics 12 (2014) 156163 conducted a brain transcriptome study employing hundreds of brain samples covering three distinct brain regions, namely prefrontal cortex, primary visual cortex and cerebellum [11].
Based on the comprehensive analysis of the gene co-expression network, functional modules were identified, especially those with gain-of-function or loss-of-function in AD, compared to the control group.
TYROBP was identified as one of the most important causal factors.
Most importantly, this study pro-vided to the public a rare resource of genome-scale measure-ment including both genome and transcriptome on a large cohort of AD patients and elderly controls.
In addition, several small-scale studies on AD have been conducted in recent years.
A transcriptome study of astrocytes in the aging brain provided information regarding the correla-tion between APOE genotype and AD pathology [12].
In another study, distinct regions of AD brain were examined by RNA-seq, and the alternative splicing and promoter usage of APOE was found to be correlated with AD progression [10].
A network model integrating the AD proteome and transcrip-tome revealed key proteins, protein interactions, and the regu-lation between genes and their protein products in the disease pathology [13].
Compared to AD transcriptome studies, much fewer ASD transcriptome studies have been conducted due to inaccessibil-ity of brain samples from young ASD patients.
In a study by Geschwind and coworkers, the dysregulation of gene expres-sion in the autistic brains was investigated [14].
The difference in gene expression between the frontal cortex and temporal cortex in normal brains diminished in autistic children.
Critical modules were revealed by gene co-expression analysis, includ-ing a neuronal module and an immune and glia module.
Based on comparison with risk genes from genome-wide association studies (GWAS), the neuronal module was highly enriched in risk genes, whereas the immune and glia module was not.
These data suggest that genes in the neuronal module are likely causal factors while those in the immune and glia module merely reflect the secondary response during disease development.
GWAS studies Brain transcriptome is generally considered as a consequence of the disease progression, because most of the brain samples come from the late stage of the disease.
Discovery of the causal genetic factors requires the examination of genetic variations in patients.
Since the human genome became available a dec-ade ago, genome-wide association study gradually became the predominant method for the discovery of risk genes/varia-tions, as compared to the traditional target gene/loci approach.
Several SNP array platforms have been developed and constantly improved through the past decade.
The general analytical procedure consists of genotyping, quality control, imputation, association test and meta-analysis.
In addition to APOE, recent large-scale GWAS studies on LOAD have identified nine other genes/loci, including CR1, BIN1, CLU, PICALM, MS4A4/MS4A6E, CD2AP, CD33, EPHA1 and ABCA7 [15].
For example, in the work by Lambert and his colleagues [16], two loci showed evidence of association with AD in two independent sample pools [16]: one within CLU (also called APOJ) on chromosome 8 (rs11136000, P = 7.5 109) and the other within CR1 on chromosome 1 (rs6656401, P = 3.7 109).
In another work, Harold et al.
undertook a two-stage GWAS study involving over 16,000 individuals including AD patients and controls [17].
In the first stage, two loci with significant P value included CLU (rs11136000, P = 1.4 109) and PICALM (rs3851179, P = 1.9 108).
These associations were recapitulated in stage 2, producing compelling evidence for association with AD in the combined dataset.
A three-stage design was adopted in a recent work [18] and several novel risk loci were discovered.
These include MS4A4A (rs4938933; P = 8.2 1012), CD2AP (rs9349407; P= 8.6 109), EPHA1 (rs11767557; P = 6.0 1010) and CD33 (rs3865444; P = 1.6 109).
The same group also replicated previous associations at CR1, CLU, BIN1 and PICALM.
Similarly, Seshadri et al.
also used a three-stage design [19] and found two loci with significant association, including rs744373 near BIN1 and rs597668 near EXOC3L2/BLOC1S3/MARK4.
These two loci, together with the previously identified loci in CLU and PICALM, were fur-ther confirmed in the Spanish sample.
In the most recent large-scale GWAS study of AD, 74,046 individuals were included in the meta-analysis where a two-stage design was adopted [20].
In addition to APOE, 19 loci were found to be significantly associated with AD, among which 11 were newly discovered in this study.
The new risk loci included INPP5D (rs35349669), MEF2C (rs190982), NME8 (rs2718058), ZCWPW1 (rs1476679), CELF1 (rs10838725), FERMT2 (rs17125944), CASS4 (rs7274581), SORL1 (rs11218343), PTK2B (rs28834970), HLA-DRB5/DRB1 (rs9271192) and SLC24A4/RIN3 (rs10498633).
This work clearly demonstrated the power of large sample size in GWAS studies.
Due to the high heritability of ASD, GWAS studies of ASD usually involve families rather than case-control design adopted in most GWAS studies of LOAD.
In an earlier study, a SNP between CDH10 and CDH9 (rs4307059) was found to be significantly associated with ASD in both discovery and replication stages [21].
In another study, a significant associa-tion of SEMA5A with ASD was found, which was further sup-ported by the reduced expression of SEMA5A in autistic brains [22].
In a later study, MACROD2 (rs4141463) was found to be significant in the discovery stage and was then reasonably replicated [23].
In a study of rare copy number vari-ants (CNVs), it was found that the ASD group carried a higher burden of rare CNVs compared to the control group [24].
The implicated genes included SHANK2, SYNGAP1, DLGAP2 and DDX53/PTCHD1.
Whole exome/genome sequencing studies The heritability of LOAD has been estimated to be 58%79% [25].
The classical SNP association studies have identified dozens of genetic variants associated with AD [20].
But the genetic alterations of AD found by GWAS studies cannot fully explain the estimated heritability [26].
The potential sources of the missing heritability include large number of small effect variants, rare genetic variants, structural genetic variations and possible genegene interactions.
To explore the missing heritability, rare variants associated with AD have been extensively examined with whole genome/exome sequenc-ing technology.
TREM2 is a gene located on chromosome 6, which encodes a membrane protein that plays a role in immune response.
Han G et al/ Genomics in Neurological Disorders 159 TREM2 on microglial is also important for the clearance of neuronal debris produced by the impaired CNS [27,28].
Jons-son et al.
reported that a missense mutation (rs75932628, R47H) within TREM2 is strongly associated with AD [29].
Similar results were obtained in another study with a smaller discovery dataset [30].
In this study, the functional relevance of TREM2 was demonstrated by the higher mRNA expression level in an AD mouse model compared to the wild type con-trol.
Later, the prevalence of R47H mutation was examined in a Spanish population comprising 180 EOAD, 324 LOAD patients and 550 controls [31] and R47H mutation was found in 1.4% of the AD patients but not in the controls.
Significant risk conferred by rs75932628 was also revealed in another EOAD related study examining 726 patients and 783 controls [32].
Besides association studies based on large sample size of case-control design, family-based studies have also been car-ried out for AD.
Guerreiro et al.
reported a mutation of NOTCH3 (R1231C) in a Turkish family [33], whereas muta-tions in SORL1 were identified as associated with FEOAD [34].
In this study, 7 samples in a total of 29 FEOAD cases car-ried SORL1 mutations, while none of 1500 controls carried those mutations.
SORL1 gene is located on chromosome 11, and the protein product is involved in the Ab production.
It is of note that common SNPs of SORL1 were also reported to be associated with AD [35].
PLD3, which encodes a membrane protein catalyzing the hydrolysis of membrane phospholipids, was also reported to be associated with AD [36].
It was found that PLD3 mutation V232M was segregated with disease status in two independent families.
Furthermore, overexpression of PLD3 led to a significant decrease in intra/ extra-cellular APP related species, whereas knockdown of PLD3 gene led to an increase in extracellular Ab level.
Besides novel genes reported to be associated with AD, novel mutations in classical AD genes, such as APP and MAPT have also been reported.
A missense mutation in the APP gene, which leads to an alanine to threonine substitution at position 673 (A673T), was significantly associated with AD (0.62% in control vs. 0.13% in AD, odds ratio (OR) = 5.29, P = 4.78 107) in a study with 1795 samples [37].
The asso-ciation was further confirmed with 3661 additional samples in the same study.
This is one of the very few rare mutations found to be protective against AD.
In another work, a rare mutation residing in MAPT (A152T) was found to increase the risk of AD (OR = 2.3, P = 0.004) [38].
However, further replication would be needed to validate this association due to the less significant P value.
In the first trio-based (parents plus child) whole exome sequencing (WES) study for ASD, 21 de novo mutations were found [39].
Among them, four mutations were predicted to be causative, including mutations in FOXP1, GRIN2B, SCN1A and LAMC3.
In a large-scale WES study of ASD with 928 individuals [40], 279 de novo coding mutations were discovered, with the strongest evidence showing SCN2A to be causative.
A WES study of 16 families [41] identified candidate recessive mutations in four genes including UBE3B, CLTCL1, NCKAP5L and ZNF18.
Genes disrupting de novo mutations were twice as frequent in probands compared to unaffected siblings and many of these genes were targets of FMRP gene, as revealed by a WES study of 343 families [42].
Moreover, rare complete gene knockout was found to contribute to a small portion of ASD in a large WES case-control study [43], whereas partial loss-of-function in several genes was found to contribute to ASD in another WES study that com-pared consanguineous to non-consanguineous families [44].
Compared to WES, whole genome sequencing (WGS) can provide variations in the non-coding regions of the genome, which may also have major contribution to the disease devel-opment.
In the first large-scale WGS study of ASD, it was found that risk genes tended to reside in hypermutability regions [45].
Later on, seven candidate genes were found with the recessive model and 59 candidate variants were found with the model free approach in a WGS study of a large pedigree with two probands [46].
In another WGS study with 32 families [47], deleterious de novo mutations were found in six families and X-linked autosomal inherited alterations were found in ten families.
With the richer information from WGS and fast dropping sequencing cost, WGS may soon replace WES as the main deep-sequencing technology for genetic studies.
Epigenomic studies Neurological conditions are not only reflected on genomic mutations and transcriptomic dysregulations, but also on the change of epigenome.
Among the various types of epige-nomic modifications, DNA methylation and histone modifi-cations have attracted high attention and been the most widely studied.
Additionally, the expression level of microR-NA (miRNA) adds another layer of complexity in the epige-nomic landscape.
During human brain development, widespread methylome reconfiguration occurs and the highly conserved non-CG methylation accumulates in the neuronal genome [48].
Dynamic alterations in the epigenome play a critical role in regulating cellular phenotype during differen-tiation, and distinct tissue-specific patterns of DNA methyla-tion have been identified across multiple human brain regions [49].
Several studies have been conducted on the change of DNA methylation and other types of modifications in AD.
In a recent investigation of four types of cytosine modifications [50], it was found that the abundance of 5hmC is lower in the entorhinal cortex and cerebellum of AD patients compared to healthy elderly controls.
In an earlier study on hippocampus [51], both 5mC and 5hmC were found to be lower in AD patients.
However, in another study focusing on 5mC and 5hmC, a global hypermethylation was observed in the middle frontal gyrus and the middle temporal gyrus [52].
Nevertheless, due to the small sample size in most of the epigenomic studies, some of the results should be interpreted with caution.
The dysregulation of miRNA has been investigated in AD brains, blood and CSF.
A pioneering miRNA study on AD was done by Cogswell and his colleagues [53].
They examined miRNA expression in both brain and CSF and showed that expression of let-7i was significantly altered in both comparing AD patients with controls.
Further pathway enrichment anal-ysis suggested that deregulated miRNAs might be related to amyloid processing, neurogenesis, insulin resistance and immunity.
miRNA profiling has also been performed in the hippocampus and prefrontal cortex and miR-132a-3p was down regulated in both brain regions [54].
As for the function of critical miRNAs in the AD brains, it has been reported that miR-124 may have a role in regulating the APP alternative 160 Genomics Proteomics Bioinformatics 12 (2014) 156163 splicing [55], and miR-15a was reported to be associated with neuritic plaque score [56].
Other works on miRNAs are focused on the discovery of biomarkers.
In Villas work, an inverse relationship between SP1 mRNA and miR-29b levels in PBMCs was observed [57].
This was of particular interest because SP1 is a dysregu-lated transcription factor in AD brains [58].
In a recent study on serum, six miRNAs were identified as peripheral biomark-ers for AD [59], whereas a 7-miRNA signature displayed >95% accuracy in discriminating AD from controls in another peripheral biomarker study [60].
In yet another study, a panel of 12 miRNAs was identified from blood samples, which showed 93% accuracy in differentiating AD from controls and 74%78% in discriminating AD from other neu-rological diseases, such as Parkinsons disease [61].
In a study on CSF, the level of hsa-miR-27a-3p was found to be lower in AD patients [62].
Compared to AD, few epigenomic studies have been con-ducted on ASD.
In a study of DNA methylation in autistic brains, four differentially methylated regions were discovered, with three of them independently validated [63].
In addition, several differentially methylated regions were identified in a recent study on monozygotic twins discordant for ASD [64], among which some were strongly correlated with quantitative behavioral traits.
Different DNA methylation patterns were also observed in monozygotic twins with Rett syndrome (a subtype of ASD) when no distinct difference could be found in SNP mutations and number of indels and CNVs [65].
iPSC technology in neurological disorders The iPSC technology is an attractive approach to model a live neuron in neurological disorders.
iPSCs provide researchers with a source of patient-specific stem cells and potential applications including cellular modeling, drug discovery and cell-based therapy [66].
Direct reprogramming of fibroblasts to neurons could be a promising approach in neurological disease modeling [67].
iPSC as an experimental research tool is now widely used in the neurological disorders including AD [6872].
Fibroblasts reprogrammed to neuron cells could offer clues to the mecha-nisms related to Ab production and processing [68].
By increasing gene regulation of GSK-3b, phosphorylated tau protein was found in the neurons derived from familial and sporadic AD (fAD and sAD) patients.
Levels of early endo-some and synapse were also found to be different in patient derived neurons.
The development of AD pathology could also be studied in iPSC using the Down syndrome induced neurons with much shorter duration of pathology development compared to in vivo [70].
Despite all the promising progress, we shall note that several pitfalls exist in the application of iPSC technology to neurological disease research.
For exam-ple, neurons might not fully mature and different phenotypes may exist between in vivo and cultured neurons [73].
Database resources for AD and ASD To facilitate the discovery of novel risk genes, the research communities of AD and ASD have formulated policies for genetic data sharing.
As a pioneer, the National Institute on Aging (NIA) has set up the NIA Genetics of Alzheimers Dis-ease Data Storage site (NIAGADS), a national genetic data repository which provides access to genotypic data for the study of the genetics of LOAD upon approval by the data access committee (Table 1).
Currently, NIAGADS has col-lected about 20 AD GWAS datasets.
Compared to the SNP-array technology, next-generation sequencing technology can provide a deeper insight into the missing heritability of AD.
Currently, there are two ongoing sequencing projects for AD: the Alzheimers Disease Neuroim-aging Initiative (ADNI) and Alzheimers Disease Sequencing Project (ADSP) [74].
The ADNI project has made publicly available the whole genome sequencing data of about 818 par-ticipants including AD, mild cognitive impairment (MCI) and control.
The ADSP project plans to complete whole genome sequencing of about 111 AD families and whole exome sequencing of about additional 11,000 subjects, where the first batch of data has been released to the public.
Autism Genetic Database (AGD) is a comprehensive data-base for autism susceptibility [75].
It currently contains the full list of autism susceptibility genes as well as CNVs related to autism.
Additionally, all non-coding RNA molecules, such as small nucleolar RNA (snoRNA), miRNA and Piwi-interacting RNA (piRNA), and chemically induced fragile sites are archived as well.
The information in this database can be accessed through a human genome browser focusing specifi-cally on the chromosomal features associated with autism or a tabular format broken down by chromosome.
Autism Database (AutDB) supports an integrated resource for the autism research community [76].
The main focus of this database is to provide an up to date, annotated list of ASD candidate genes in the form of reference datasets for interro-gating molecular mechanisms underlying the disorder.
The information in AutDB is extracted from the published scien-tific literature on molecular genetics and biology of ASD and organized to optimize its use through experts.
The database provides users four modules for free, including information on human genes, animal models, protein interactions and copy number variants.
The National Database for Autism Research (NDAR) is an NIH-funded research data repository that aims to accelerate progress in ASD research through data sharing, and reporting of research results.
NDAR also serves as a scientific community platform and portal to multiple other research repositories, allowing for integration and secondary analysis of data.
Comparing with the two above-described databases, this database is mainly aimed at supporting an open access platform that allows researchers to obtain and integrate the public data, so as to facilitate novel discoveries for the preven-tion and cure of the disease.
Concluding remarks The human brain remains to be one of the biggest mysteries in biological sciences.
Preventing and curing neurological disor-ders require better understanding of the brain.
The ongoing big brain initiatives in Europe, USA and China will take this grand challenge, where the main focus is future computing, brain wiring and brain disorders, respectively.
The genomic approaches described in this review together with the T a b le 1 G en o m ic d a ta re so u rc es fo r A lz h ei m er s d is ea se a n d a u ti sm sp ec tr u m d is o rd er A b b re vi a ti o n F u ll n a m e D es cr ip ti o n W eb li n k N IA G A D S N a ti o n a l In st it u te o n A g in g G en et ic s o f A lz h ei m er s D is ea se D a ta S to ra g e S it e A n a ti o n a l g en et ic s d a ta re p o si to ry (U S A ) th a t fa ci li ta te s a cc es s o f g en o ty p ic d a ta to q u a li fi ed in v es ti g a to rs fo r th e st u d y o f th e g en et ic s o f la te-o n se t A lz h ei m er s d is ea se h tt p s://w w w .n ia g a d s. o rg/ A D S P A lz h ei m er s D is ea se S eq u en ci n g P ro je ct A im in g to d ev el o p a n d ex ec u te a la rg e-sc a le se q u en ci n g p ro je ct to a n a ly ze th e g en o m es o f a la rg e n u m b er o f w el l-ch a ra ct er iz ed in d iv id u a ls in o rd er to id en ti fy a b ro a d ra n g e o f A lz h ei m er s d is ea se ri sk a n d p ro te ct iv e g en e v a ri a n ts , w it h th e u lt im a te g o a l o f fa ci li ta ti n g th e id en ti fi ca ti o n o f n ew p a th w a y s fo r th er a p eu ti c a p p ro a ch es a n d p re v en ti o n h tt p s://w w w .n ia g a d s. o rg/a d sp/ A D N I A lz h ei m er s D is ea se N eu ro im a g in g In it ia ti v e W o rk in g to d efi n e th e p ro g re ss io n o f A lz h ei m er s d is ea se a n d to co ll ec t, v a li d a te a n d u ti li ze d a ta su ch a s M R I a n d P E T im a g es , g en et ic s, co g n it iv e te st s, C S F a n d b lo o d b io m a rk er s a s p re d ic to rs fo r th e d is ea se h tt p ://a d n i. lo n i. u sc .e d u/ A G D A u ti sm G en et ic D a ta b a se A co m p re h en si v e d a ta b a se fo r a u ti sm su sc ep ti b il it y g en es a n d C N V s, in te g ra te d w it h k n o w n n o n co d in g R N A s a n d fr a g il e si te s h tt p ://w re n .b cf .k u .e d u/ A u tD B A u ti sm D a ta b a se A n in te g ra te d re so u rc e fo r th e a u ti sm re se a rc h co m m u n it y , w h ic h is b u il t o n in fo rm a ti o n ex tr a ct ed fr o m th e st u d ie s o n m o le cu la r g en et ic s a n d b io lo g y o f a u ti sm sp ec tr u m d is o rd er h tt p ://a u ti sm .m in d sp ec .o rg/ a u td b/ N D A R N a ti o n a l D a ta b a se fo r A u ti sm R es ea rc h A n N IH (U S A )-fu n d ed re se a rc h d a ta re p o si to ry th a t a im s to a cc el er a te p ro g re ss in a u ti sm sp ec tr u m d is o rd er re se a rc h th ro u g h d a ta sh a ri n g , d a ta h a rm o n iz a ti o n a n d re p o rt in g o f re se a rc h re su lt s h tt p ://n d a r. n ih .g o v/ N o te : M R I, m a g n et ic re so n a n ce im a g in g ; P E T , p o si tr o n em is si o n to m o g ra p h y ; C S F , ce re b ro sp in a l fl u id ; C N V , co p y n u m b er v a ri a ti o n .
Han G et al/ Genomics in Neurological Disorders 161 state-of-the-art brain imaging technologies will be the main workhorses in the foreseeable future for the investigation of neurological disorders.
Mapping out the 100 billion neurons and 100 trillion connections in the human brain is certainly not a trivial task.
Efficient integration with genomic technolo-gies will undoubtedly lead to breakthroughs.
The exciting new progress in this field will be closely followed.
In addition, stem cell technology may be a viable option in the future for treating neurodegenerative disorders especially for patients at the mid-dle or late stage of the disease.
Nevertheless, much of the focus in this field is to detect the disease early and intervene before massive neurodegeneration occurs.
As for the childhood disor-ders such as autism, non-invasive diagnosis of mutations and genome correction are the technologies to watch.
Competing interests The authors declare that there are no conflicts of interest.
Acknowledgements This work was supported by the grant from the National Basic Research Program of China (973 Program, Grant No.
2014CB964901) awarded to HL from the Ministry of Science and Technology of China.
ABSTRACT Motivation: Understanding key biological processes (bioprocesses) and their relationships with constituent biological entities and pharmaceutical agents is crucial for drug design and discovery.
One way to harvest such information is searching the literature.
However, bioprocesses are difficult to capture because they may occur in text in a variety of textual expressions.
Moreover, a bioprocess is often composed of a series of bioevents, where a bioevent denotes changes to one or a group of cells involved in the bioprocess.
Such bioevents are often used to refer to bioprocesses in text, which current techniques, relying solely on specialized lexicons, struggle to find.
Results: This article presents a range of methods for finding bioprocess terms and events.
To facilitate the study, we built a gold standard corpus in which terms and events related to angiogenesis, a key biological process of the growth of new blood vessels, were annotated.
Statistics of the annotated corpus revealed that over 36% of the text expressions that referred to angiogenesis appeared as events.
The proposed methods respectively employed domain-specific vocabularies, a manually annotated corpus and unstructured domain-specific documents.
Evaluation results showed that, while a supervised machine-learning model yielded the best precision, recall and F1 scores, the other methods achieved reasonable performance and less cost to develop.
Availability: The angiogenesis vocabularies, gold standard corpus, annotation guidelines and software described in this article are available at http://text0.mib.man.ac.uk/mbassxw2/angiogenesis/ Contact: xinglong.wang@gmail.com Received on May 4, 2011; revised on July 5, 2011; accepted on July 31, 2011 1 INTRODUCTION 1.1 Background and motivation Biological processes (i.e.
bioprocesses) occur in living organisms and the regulation of them is crucial to control and maintain the life cycles of the organisms.
A bioprocess may consist of any number of chemical reactions or other types of biological events that may result in maintenance, changes or transformations of the organism.
In drug discovery, it is important to understand bioprocesses and how they are regulated under normal conditions and dysregulated in disease.
Regulation of bioprocesses may involve modulating their frequency, rate or extent, through the control of gene expression, To whom correspondence should be addressed.
protein modification or interaction with a protein, substrate molecule or larger structures.
Scientists often need to gather facts that come from clear scientific evidence of how, when modulated, an existing or potential drug target affects critical pathophysiological processes leading to either the disease cure, prevention or amelioration of symptoms in the clinical setting.
Typically, a bank of preclinical evidence is developed using cell lines, model organisms and clinical samples associating a target with key bioprocesses (and so disease phenotype).
However, this process is very expensive and time consuming (Kola and Landis, 2004).
To avoid unnecessary duplication of research, scientists must first review external activity in their area of interest in order to determine what questions remain unanswered, and to derive information to support or contest hypotheses.
Laboratory resources may then be more efficiently directed to explore those questions.
One important source of this information is published biomedical articles.
However, given the vastness of the literature and an accelerating publication rate, manual techniques and conventional information retrieval techniques are unable to deliver timely, reliable, exhaustive and specific results.
In addition, the scientific and publication process is not static in nature, but instead a continuous one.
Text mining technology has been increasingly popular to support knowledge discovery, hypothesis generation and to manage the mass of biological literature (Ananiadou et al., 2010; Hunter and Cohen, 2006), and text mining has shown promises for finding key biological entities (e.g.
Smith et al., 2008), relationships among proteins (e.g.
Krallinger et al., 2008a) and for establishing functional annotations (e.g.
Alex et al., 2008).
As far as we know, there has been limited work in text mining to extract bioprocesses.
One reason is due to its complexity.
A bioprocess often involves a series of bioevents, where an event expresses a change of state of a cell or tissue, and such events are often used to refer to the bioprocess they participate in.
In this article, we systematically investigate the extraction of bioprocess-related terms and events, including the definition of the task, the construction of a gold standard corpus for learning and evaluation, and the proposal of a number of approaches to identifying bioprocesses.
We then compare the methods in terms of their performance results, as well as the amount of manual supervision required, as both of the factors are main considerations when deploying a new text mining system in practice.
Our work focuses on an exemplar biological process, angiogenesis.
Angiogenesis is a key physiological process involving the growth of new blood vessels from pre-existing vessels, and it is vital in growth and development of tissues and organs.
It is also a crucial step in the transition of tumours from a dormant state The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:25 8/9/2011 Bioinformatics-btr460.tex] Page: 2731 27302737 Automatic extraction of angiogenesis bioprocess from text to a malignant one.
Therefore, the identification of gene products involved in regulating angiogenesis, and pharmacological agents that have angiogenesis inhibitory effects, has been one of the main lines of research for treatment of solid tumours, and hence mining facts related to angiogenesis is a crucial step towards this goal.
1.2 Related Work The recognition of specific biological processes in unstructured text has received relatively less attention in the biomedical text mining community.
However, researchers have attempted to mine general bioprocess information from other knowledge sources such as ontology and biological data.
For example, Hvidsten et al.
(2003) proposed a systematic supervised learning model to predicting bioprocess by analysing microarray data.
Their method benefited from the functional annotation of genes in the Gene Ontology (GO) (http://www.geneontology.org/GO.doc.shtml#biological_process).
The method was evaluated on genes coding for proteins known to be involved in bioprocesses using cross-validation.
Koike et al.
(2004) reported work on finding relations between biological functions and genes and gene products.
Their method first used a named entity recognition programme to annotate genes, gene products and biological function terms as defined in GO, and then extracted relations between the entities and biofunctions by analysing syntactic structures of the sentences.
As noted by Koike et al., the terms for bioprocesses in GO were insufficient for automatic extraction in terms of recall.
They experimented with a number of techniques to augment the functional terms, including mining-related terms using high co-occurrence counts, retrieving similar terms having similar collocations with GO terms, etc.
However, neither of the approaches described above was able to find more complex bioprocess expressions such as events.
Recently, research has been conducted on the extraction of biomolecular events.
In particular, the BioNLP 2009 shared task (Kim et al., 2009) attracted much attention and interesting solutions.
The shared task provided annotated data for several types of gene-related bioevents, such as gene expression, transcription and regulation, and participants were asked to identify the event type and the word that triggers the event (i.e.
trigger).
The results of the shared task showed that event extraction was challenging: the best performing system achieved an overall F1-score of 51.95% (Bjrne, et al., 2009).
After the shared tasks, researchers have proposed methods that further improved the state of the art.
For example, Miwa et al.
(2010) proposed a method that detects, in sequence, the event trigger and the edges linking the participants and the trigger, and then finds the best combination of the edges to form a complex event.
The BioNLP 2009 shared task did not include cellular or tissue bioevents, which play a central role in bioprocesses and are the focus of our work.
The GENIA corpus (Kim et al., 2008), however, contains annotated examples of cellular physiological process, which is similar to our event definition.
However, this type of event refers to a broad category and is not linked to any specific bioprocess, whereas we would like to extract events that closely relate to a given bioprocess such as angiogenesis.
Section 2.1 provides more discussion on the similarity and difference between GENIA and our event definitions.
Also, GENIA style event annotation required both biological and linguistic expertise and a lengthy annotation process (Kim et al., 2008).
Despite the fact that the GENIA annotation has been proven highly useful, people interested in mining bioevents in a new domain may hesitate to follow GENIAs strategy due to its high cost factor.
This contributed to our effort in exploring methods taking into account not only performance scores, but also development cost.
2 METHODS 2.1 Task definition Generally speaking, angiogenesis descriptions appear in text in one of the following two forms: angiogenesis terms and angiogenesis events, where a term refers to the name, synonym and other lexical variants of angiogenesis, and an event refers to a cellular or tissue bioevent or reaction, taking place as part of the bioprocess.
For example, angiogenesis and angiogenic are angiogenesis terms, and phrase capillary endothelial cell proliferation, indicating a change of state of capillary endothelial cell, is an angiogenesis event.
In order to examine how often angiogenesis bioprocess is expressed in the form of events, we created a corpus of 262 medline abstracts (http://www.nlm.nih.gov/databases/databases_medline.html), where angio-genesis terms and events, as well as several types of bioentities that were considered to closely relate to angiogenesis bioprocess, were manually annotated.
Table 1 shows the annotation markables and their counts of occurrence in this corpus: 36.5% (479 out of 1313) of the angiogenesis mentions are events, which highlights the importance of event recognition for bioprocess extraction.
The corpus was also used to develop supervised machine learning systems and for evaluation.
Section 2.5 gives detail on the annotation and the learning systems.
The definition of angiogenesis event largely follows that of cellular physiological process, a type of event defined in GO and also in the GENIA ontology (Kim et al., 2006).
However, our events are restricted to the domain of angiogenesis, and do not contain explicit links to their trigger words and participants.
In GENIA definition (Kim et al., 2008), a participant is the bioentity that is involved in an event, and each event must attach to a trigger word denoting the action that caused the changes.
For example, in GENIA annotation, phrase capillary endothelial cell proliferation would have proliferation marked as the trigger word and capillary endothelial cell as the participant.
In contrast, our annotation would regard the entire phrase as an angiogenesis event, where trigger words and participants would not be explicitly linked to the event.
Section 2.5 elaborates the difference between our event annotation and the GENIA project, and Table 3 shows some illustrative examples.
This decision significantly simplified the annotation process, and hence reduced the annotation cost.
In summary, we define an angiogenesis bioevent as either: (1) the change of state of a cell, cell component or tissue that is specific to angiogenesis.
(e.g.
vascular sprouting) or (2) the change of state of some property of the entities mentioned above (e.g.
increase in vascular density).
As mentioned in the above definition, the key entities taking part in angiogenesis are cell, cell components and tissues.
For simplicity, they are referred to as tissues, tissue terms or tissue entities in the rest of the article.
Table 1.
Annotated markables and their counts in the angiogenesis corpus Markable Counts Angiogenesis term 834 Angiogenesis event 479 Gene or gene product 2901 Tissue 2190 Cell 1065 2731 [17:25 8/9/2011 Bioinformatics-btr460.tex] Page: 2732 27302737 X.Wang et al.
The term bioentity, on the other hand, is used to refer to genes, gene products and the tissue entities as defined above.
Also, a trigger or trigger word is the word which denotes the action that caused the changes of state in an event.
In our definition, the tissue terms are not restricted to nouns.
In fact, our corpus contains many tissues composed of words of other parts of speech (POS), which would not be annotated as entities in other annotation projects, such as GENIA (Kim et al., 2008).
This decision was made due to the observation that words of syntactic classes other than nouns (e.g.
adjectives) were often as informative as nouns in indicating the presence of a bioentity.
For example, vascular, arterial and microvascular are adjective tissue entities that frequently occur in our corpus denoting something related to, affecting, or consisting of blood vessels.
We treated angiogenesis terms the same way.
The following sections propose methods for identifying angiogenesis terms and events, and evaluation results.
2.2 Dictionary-based method One solution to the extraction of angiogenesis terms and events is using vocabularies.
There are, however, limited lexical resources available for biological process terms.
While GO and MeSH (http://www.nlm.nih.gov/mesh/) contain branches for biological processes, the information provided for each specific process is very limited.
For example, the angiogenesis term in GO (GO:0001525) is annotated with only one synonym: blood vessel formation from pre-existing blood vesselsand its descendent terms also look like definitions, e.g.
angiogenesis involved in wound healing (GO:0060978).
Such terms and synonyms are insufficient to help computer programmes mine angiogenesis terms and events, which highlighted the lack of ontological support for extracting bioprocesses from text.
Therefore, we manually built three vocabularies: angiogenesis terms, tissues and triggers, where the first vocabulary contains 10 variants of the names and synonyms of angiogenesis, the second consists of 27 cell and tissue entities related to angiogenesis and the third contains 39 derivation forms of verbs that are good indicators of angiogenesis events.
For example, angiogenesis and angiogenic are terms in the first vocabulary, vascular and endothelial cell appear in the second and development and proliferation are examples from the third.
The vocabularies are flat lists and do not contain any hierarchical information, and a domain expert spent 40 h developing the vocabularies.
We then designed patterns to extract angiogenesis terms and events using the terms in the vocabularies.
The patterns are shown in Figure 1, where NP denotes a noun phrase, Prep is a preposition, Phrase is a container enclosing any other query components and ws denotes the maximum distance (i.e.
number of words) allowed between the components in a phrase.
We applied pattern (A) to find angiogenesis terms, and based on the definition described in Section 2.1, patterns (B) and (C) were used for recognizing angiogenesis events.
In more detail, pattern (B) finds noun phrases in which a tissue modifies a trigger next to it (i.e.
ws = 0); and pattern (C) recognizes phrases where a tissue modifies a trigger as a preposition phrase and all components should occur adjacent to each other.
We chose these two particular syntactic structures to model angiogenesis events based on domain experts knowledge and observations.
For example, according to pattern C, phrase development of endothelial cell will be tagged as an angiogenesis event, because development is a trigger word, of a preposition and endothelial cell is a tissue.
The patterns were applied at sentence level, and before applying the patterns, the documents were pre-processed using the following natural language processing (NLP) steps: sentence splitting, tokenization, POS tagging and chunking, as described in Alex et al.
(2008).
Fig.1.
Patterns for finding angiogenesis terms and events.
2.3 Pattern matching with syntactic relation The patterns B and C defined in Section 2.2 are rather strict in that a trigger and a tissue must follow the designated word order to form an angiogenesis event.
An angiogenesis event, however, can be expressed in many ways in text and the tissue and trigger may not follow a specific word order.
For example, as shown in Figure 2, endothelial cells that migrate to can be an angiogenesis event, where endothelial cell is a tissue entity and migrate a trigger.
However, neither pattern B nor C would identify the event, because the trigger migrate appears in a relative clause.
One solution is to generalize patterns (B) and (C) in Figure 1 so that a syntactically related pair of tissue and trigger can be considered to be an angiogenesis event.
We can employ a natural language parser to find syntactic relations between words.
In our experiments, we used the ENJU HPSG parser (Miyao and Tsujii, 2008), which has been shown to yield good results on finding proteinprotein interactions (Miyao et al., 2009) and species disambiguation (Wang et al., 2010), among other biomedical information extraction tasks.
ENJU analyzes sentences and generates predicateargument structures (PASs), each of which consists of a predicate, an argument and a relation between them.
Figure 2 shows a phrase parsed by ENJU, where each arrowed line and the words it connects denote a PAS, and the direction of the line is from the predicate to the argument.
For example, predicate migrate, argument cells and relation verb_arg1 form a PASs.
A sentence parsed by ENJU can be represented as a graph, in which each node maps to a word and each edge to a PAS relation between the words.
As shown in Figure 3, we define that, if the first node on a syntactic path is a tissue term, and the last node a trigger, then the sequence of words on this path is tagged as an event, and vice versa.
Using pattern (B) in Figure 3, the example shown in Figure 2 will be recognized as an event, because trigger word migrate and tissue endothelial cells are connected via the syntactic path migrate, cells and endothelial.
When constructing events with paths, the direction of PAS was not taken into account, and we also set the maximum number of edges allowed on a path to 5.
We also expanded the vocabularies by including the derivation forms of the tissues and triggers, because they appear in text not only as one POS, but also others with the same lexical root.
For example, both vascular development and develops vasculature are angiogenesis events, where vascular is an adjective derivation of vasculature and development a noun derivation of develop.
We used the Lexical Variants Generation Tool from NIH (http://lexsrv3.nlm.nih.gov/LexSysGroup/Projects/lvg/current/ web/) for generating the derivations.
2.4 Automatic vocabulary construction using domain-specific documents In addition to the syntactic patterns, the tissue and trigger vocabularies play an important role in the dictionary-based approach.
The vocabularies used in Sections 2.2 and 2.3 were manually developed, where the choice of terms can be subjective and highly dependent on the curators domain knowledge.
In an attempt to alleviate this problem, we adopted a method Fig.2.
A parsed phrase in ENJUs predicateargument representation.
Fig.3.
Patterns for finding angiogenesis events with PAS relations.
2732 [17:25 8/9/2011 Bioinformatics-btr460.tex] Page: 2733 27302737 Automatic extraction of angiogenesis bioprocess from text to automatically populate the vocabularies using domain-specific texts.
This approach requires little human input, because domain-specific documents are relatively easy to obtain.
For example, the review articles in the Nature special issue on angiogenesis (DeWitt, 2005) and the Wikipedia page on this subject (http://en.wikipedia.org/wiki/Angiogenesis) are readily available as angiogenesis-related texts.
Compared with other knowledge sources, domain-specific texts have received less attention for their applications to biomedical text mining.
One application is keyphrase extraction, which can be cast as a classification task (Frank et al.
1999; Turney, 1999): each phrase in a document is either a keyphrase or not, and the problem is to correctly classify a phrase into one of the two categories, for which off-the-shelf supervised machine learning tools can be used.
However, this method requires a set of training documents, where the keyphrases in each document must be manually identified.
Alternatively, previous work tackled keyphrase extraction using statistical measures (e.g.
Frantzi et al., 2000) using a single corpus.
In contrast, our method extracts salient angiogenesis-related predicateargument pairs by comparing the statistical language models built respectively on the PAS generated from two ENJU-parsed corpora: a domain-specific corpus (i.e.
foreground corpus) and a general one (i.e.
background corpus).
Intuitively, angiogenesis-related tissues and trigger words occur more frequently in the foreground corpus than in the background one, and therefore it is possible to extract these terms from the key predicateargument pairs.
Using the patterns defined in Section 2.3, the automatically generated tissue and trigger vocabularies can then be used to construct angiogenesis events.
2.4.1 Capturing domain-specific keyphrases by comparing language models A statistical language model assigns a probability to a sequence of n words P(w1,...,wn) by means of a probability distribution.
In NLP, a simplifying assumption is often made such that the probability of a word given all the previous words can be approximated by the probability of the word given a number of previous words.
For example, a bigram model approximates P(wi|wi11 ) by the conditional probability of the preceding word P(wi|wi1), and similarly, a trigram model is the conditional probability of the preceding two words, i.e.
P(wi|wi1wi2).
Tomokiyo and Hurst (2003) approached keyphrase extraction by comparing the KL divergence between the language model of a foreground corpus, and that of a background one, where KL divergence is a non-symmetric metric of the inefficiency of assuming that the distribution is q when the true distribution is p (Cover and Thomas, 1991).
Let p(x) and q(x) be two probability mass functions, the KL divergence between p and q is defined in Equation (1).
D(p||q)= x p(x)log p(x) q(x) (1) They also defined the term inside the summation of Equation (1) as point-wise KL divergence, as shown in Equation (2).
Intuitively, point-wise KL divergence quantifies the contribution of the phrase w to the expected loss of the entire distribution.
w(p||q)=p(w)log p(w) q(w) (2) Tomokiyo and Hurst went on to score each n-gram in text according to its phraseness and informativeness, where the former computes how much information would be lost if assuming the independence of each word by applying the unigram model, instead of the n-gram one, and the latter is how much we lose information by assuming w is drawn from the background model instead of the foreground one.
More formally, phraseness and informativeness are respectively defined in Equations (3) and (4): w(LM n fg||LM1fg) (3) w(LM n fg||LMnbg) (4) where, LM= i={1,...,n} j={1,...,n} p(wi|wj) (5) and LMnfg is the n-gram model constructed from the foreground corpus, while LMnbg from the background one.
The linear addition of phraseness and informativeness was used to score each n-gram, and the higher the score, the more salient the n-gram is in the foreground corpus.
This strategy was shown to outperform some other keyphrase extraction methods, such as the likelihood ratios (Damerau, 1993).
This method could be used to find angiogenesis-related n-grams, given an angiogenesis-specific corpus and a general one.
However, tissues and triggers are likely to contain one to several words, and calculating point-wise KL divergence for every n-gram, where, for example, n[1,4], is computationally expensive.
More importantly, by definition, n-gram models only take into account strings of adjacent words, whereas as argued in Section 2.3, statistics of co-occurrences of words that are physically distant but syntactically close can be very useful to capture related concepts.
2.4.2 Comparing language models based on PAS relations We also extract keyphrases by comparing language models computed on different corpora.
However, instead of using n-gram language models, we adopted a language model based on pair-wise predicateargument relations produced by the ENJU parser.
There has been some research in incorporating syntactic and semantic relations produced by a natural language parser in language modelling.
For example, Pad and Lapata (2007) built semantic vector space models using word syntactic relations, instead of word co-occurrence counts, and the models were shown to be comparable or superior to the state of the art, on single word priming, synonym detection and word sense disambiguation.
When using the PAS-based language models, phraseness became irrelevant because the parser would have determined the relation between the predicate and argument.
Consequently, we only needed to calculate the informativeness of each argument and predicate pair (i.e.
n=2), using Equation (4), where wi and wj , respectively, are an argument and a predicate, which have a direct PAS relation, as opposed to two neighbouring words as in a bi-gram language model.
In our experiments, the background corpus contained 1000 documents that were randomly selected from the collection of medline abstracts published between year 2000 and 2010.
As to the foreground corpus, we experimented with three different document sets, in order to study how domain-specific a foreground corpus needed to be.
The first foreground corpus (i.e.
RandomMedline) consisted of 250 randomly selected medline documents (52k tokens) that contain the keyword angiogenesis.
The second (i.e.
AngioCorpus) was the 262 abstracts in our angiogenesis corpus (58k tokens), retrieved using the patterns defined in Section 2.2.
Note that in this experiment, we only used the raw text but not the annotation (Section 2.5).
Finally, the third corpus (i.e.
ReviewArticles) contained the six full-text review articles from the angiogenesis special issue of Nature (DeWitt, 2005) and the Wikipedia page on angiogenesis (53k tokens).
The background and foreground corpora were processed by ENJU to generate the PAS relations, and then the PAS-based language models were computed for each corpus, where Katz smoothing (Katz, 1987), reportedly to perform well on NLP tasks (Chen and Goodman, 1996), was applied to alleviate the data sparseness problem, and functional words such as prepositions and determiners, as well as words consisting of only digits and punctuation, were removed.
We then coupled each foreground corpus with the background one, and extracted a list of salient predicateargument pairs using Equation (4), and the pairs were ranked according to their point-wise KL divergence scores.
To assess the quality of the results, a domain expert manually reviewed the highest ranked 50 predicateargument pairs, extracted using ReviewArticles as the foreground corpus.
In this exercise, an argument predicate pair was judged as relevant if it was either an angiogenesis-related entity (i.e.
cell, tissue, gene or gene product) or an angiogenesis term or event.
The result was promising: 32 phrases were judged as relevant and 12 possibly relevant.
In other words, 88% of the automatically extracted argumentpredicate pairs were relevant or possibly relevant.
In addition, all phrases in the top 20 were considered relevant (14 out of 20) or possibly 2733 [17:25 8/9/2011 Bioinformatics-btr460.tex] Page: 2734 27302737 X.Wang et al.
Table 2.
Top 10 angiogenesis-related phrases from ReviewArticles Rank Predicate Argument Rank Predicate Argument 1 Vascular Development 6 Vessel Growth 2 Retinal Vessels 7 Growth Factor 3 Endothelial Cells 8 Growing Vessel 4 Dorsal Aorta 9 Retinal Angiogenesis 5 Retinal Vascularization 10 Retinal Development relevant (6 out of 20).
Table 2 shows the top 10 angiogenesis-related predicateargument pairs extracted in this experiment.
2.4.3 Event extraction The automatically extracted list contains angiogenesis-related predicateargument pairs, but in order to construct patterns for mining events as described in Section 2.3, we needed to acquire the vocabularies for tissue terms and trigger words.
We consider a term as a tissue if it satisfies the following two conditions: (i) it belongs to a tissue dictionary; and (ii) it occurs in the predicateargument list.
Essentially, the conditions state that we want tissues (i.e.
condition 1) that are relevant to the angiogenesis bioprocess (i.e.
condition 2).
We constructed the tissue dictionary using a subset of the UMLS ontology (http://http://www.nlm.nih.gov/research/umls/), which contains the following branches: body, body parts, organ, tissue, cell and cell components.
UMLS was chosen because it is relatively comprehensive and integrated a number of biomedical ontology and vocabularies, such as MeSH (http://www.nlm.nih.gov/mesh/) and NCI thesaurus (http://ncit.nci.nih.gov/).
We flattened the dictionary so that it did not contain any hierarchy, and the final tissue dictionary contained 188 069 unique entries.
Meanwhile, according to condition 2, a term must also match an argument or a predicate, which ranks among the top (=500 in our experiments) in the predicate-argument list.
Note that a tissue entity can appear as either a predicate or an argument.
For example, both vascular and vasculature are angiogenesis tissue entities, but the former is more likely to be a predicate as an adjective, whereas the latter, a noun, is more likely to be an argument.
Similarly, a trigger word is a verb that appears in the salient predicate argument list.
We needed a verb dictionary and for that purpose we examined lexical resources for the biological domain such as BioLexicon (Sasaki et al., 2008a).
The coverage of the verb list in BioLexicon is small and does not include angiogenesis trigger words such as proliferateand migrate.
On the other hand, verbs in English dictionaries, such as WordNet, are too general.
We then decided to build a verb dictionary by computing point-wise KL divergence scores for verb unigrams using Equation (4), on the foreground and background corpora, where n was set to 1 and w must be a verb as identified by a POS tagger (Alex et al., 2008).
The verb list was then expanded to include the derivation forms using the NIH Lexical Variants Generation Tool.
Finally, the top-ranked terms ( =150 in our experiments) in the intersection of the verb list and the predicateargument list were selected as angiogenesis trigger words.
We then tagged angiogenesis events in text using the patterns defined in Figure 3.
The only difference is that Tissue and Trigger were taken from the automatically constructed tissue and trigger lists, respectively, instead of the manually created ones.
See the additional material to this article (http://text0.mib.man.ac.uk/mbassxw2/angiogenesis/additional.html) for more discussion on how the parameters and affect the event extraction performance.
2.5 Learning from manual annotation A supervised learning approach infers a model over training examples, where each example consists of a set of predefined features (e.g.
word form and contextual information) and an output value.
The trained model is then Table 3.
Comparison of the annotations of GENIAs cellular physiological process and the angiogenesis event Angiogenesis corpus GENIA corpus 1 MEK5 signaling modulates endothelial cell migration and focal contact turnover.
MEK5 signaling modulates endothelial cell migration and focal contact turnover.
2 resulting in increased vascular proliferation but defective maturation.
resulting in increased vascular proliferation but defective maturation.
Events are highlighted in bold font, entities are italicized and trigger words are underlined.
used to classify new instances.
Supervised systems consistently excel as demonstrated in a range of evaluation challenges, such as BioCreative I (Hirschman et al., 2005) and II (Krallinger et al.
2008b) and the BioNLP shared tasks (e.g.
Kim et al., 2009).
Such methods do not rely on dictionaries.
However, the availability of a training corpus is essential and therefore we hand-built a gold standard corpus for the identification of angiogenesis terms and events.
In addition to training machine learning models, the corpus enabled systematic evaluation and comparison of the techniques proposed in this article.
2.5.1 Selecting documents for manual annotation We first retrieved an initial pool of documents from the collection of medline abstracts that were published on and before October 2009.
The patterns defined in Figure 1 were submitted as queries.
More specifically, if an abstract contains a sentence that matches pattern (A), (B) or (C) (Fig.1), then it will be retrieved and stored in the pool.
We did not only use angiogenesis terms (e.g.
angiogenesis) as queries, because documents that contain angiogenesis events were also of interest.
We then randomly selected abstracts in several batches for annotation and in total the final annotated corpus contained 262 abstracts.
Note that this retrieval procedure gave some advantage to the dictionary-based approach described in Section 2.2 in evaluation, because each document was guaranteed to contain at least an angiogenesis term or event that the dictionary-based method would be able to identify.
2.5.2 Manual annotation Table 1 summarizes the annotation markables.
The guidelines for annotating entities (i.e.
gene or gene product, tissue and cell) and angiogenesis terms were relatively straightforward: every mention of the above entities should be annotated, and a mention of an entity can be either its full-name or abbreviation and acronym forms.
As for angiogenesis events, we followed the definitions set in Section 2.1 and annotated the phrases that indicated the change of state of angiogenesis-related cells, cell components and tissues.
Similar to GENIAs annotation (Kim et al., 2009), each event should contain a tissue term as a participant, and a trigger word indicating the action that changes the state of the participant or its biological property.
Although this rule was not enforced due to limitation of the annotation tool, we found the majority of events contain a participating tissue entity (463 out of 479, or 97%), and those that do not have participants were mostly annotation errors.
Different from the GENIA guidelines, a participating entity may consist of words of any POS, and triggers were not explicitly marked, in order to reduce annotation time.
Table 3 shows two examples that illustrate the difference in our annotation and GENIAs.
For the first example, we annotated phrase endothelial cell migration as an angiogenesis event and endothelial cell a cell entity, whereas according to the GENIA guidelines, endothelial cell migration would be marked as a cellular physiological process event, endothelial cell as a cell entity and migration a trigger word.
As to the second example, GENIA annotators would not add any annotation, while we would annotate vascular as a tissue entity, vascular proliferation as one, and vascular maturation as the other event.
2734 [17:25 8/9/2011 Bioinformatics-btr460.tex] Page: 2735 27302737 Automatic extraction of angiogenesis bioprocess from text Table 4.
Angiogenesis term results (precision/recall/F1-score, in %) Method Angiogenesis term IAA 82.35/67.47/74.14 DictionaryBased 71.94/59.52/65.15 CRF 94.68/91.00/92.80 Table 5.
Evaluation results (precision/recall/F1-score, in %) Method Exact match Boundary relaxed (2) IAA 35.00/58.33/43.75 45.00/75.00/56.25 PatternBaseline 33.43/21.91/26.47 49.43/32.40/39.14 PatternExtended 68.16/22.85/34.22 87.71/29.40/44.04 CRF 67.75/33.97/45.22 83.19/41.77/55.62 CRF-entity 71.06/40.93/51.94 88.64/51.05/64.79 RandomMedline 10.40/6.74/8.18 27.17/17.60/21.36 AngioCorpus 43.05/25.26/31.71 52.47/30.90/38.73 ReviewArticles 43.93/31.84/36.92 56.07/40.64/47.12 Our annotation guidelines also stated that the annotation was concerned with identifying what the author(s) intend to communicate in the text, and the annotators should not make any judgment as to the validity of the authors claims.
During the annotation, the annotators may seek help from external resources and search engines, such as PubMed and Google.
Also, when marking the entities and events, the annotators were permitted to nest them, but neither entities nor events were allowed to cross.
Discontinuous co-ordinations such as A and B cells were annotated as two nested entities A and B cells and B cells.
Three domain experts went through the abstracts and annotated angiogenesis terms, events and related bioentities.
The annotation was carried out using the Callisto tool (http://callisto.mitre.org/) for its relative simplicity to use.
In total, 150 h was spent on annotation.
2.5.3 Quality control To ensure the quality of annotation, we first went through a pilot study, in which 20 documents were doubly annotated.
Inter-annotator agreement (IAA) for every type of markable was then calculated.
The IAA results of angiogenesis terms were good (Table 4), indicating the annotators consistently agreed with each other.
However, the IAA for angiogenesis events was not satisfactory (Table 5), which demonstrated the complexity and diversity in how angiogenesis events appear in text, and the fact that the task can be a challenge even for human annotators.
Nevertheless, we endeavored to improve the annotation consistency.
During the pilot study, we reconciled the doubly annotated documents, found the causes of the discrepancies and re-emphasized the aforementioned annotation guidelines.
Then the three annotators started two rounds of annotation, where 100 documents were annotated in the first round and 150 in the second.
Double annotation was not performed in this phase due to resource constraints.
However, to ensure annotation quality, for each document, a second annotator carried out validation after each round of annotation, and if in doubt, the two annotators were asked to agree on a gold standard through reconciliation.
At the end of the annotation, two annotators revisited the 20 doubly annotated documents used for pilot study and updated the annotation.
Then the reconciled 20 documents were added to the gold standard dataset.
In total, the annotation project generated 270 unique abstracts.
However, eight of which were discarded, because they were considered irrelevant to the domain, and consequently the final corpus contains 262 abstracts.
2.5.4 Supervised learning We tackled both the entity and event recognition tasks with logistic regression models, which have been shown to be effective in handling large-scale classification problems (Andrew and Gao, 2007).
In addition, an attractive feature of logistic regression models is that they produce probabilistic output that allows the information on the confidence of the decision to be used by subsequent components in the text processing pipeline.
When the random variable to predict is a sequence, the logistic regression model is called linear chain Conditional Random Fields (CRFs) (Lafferty, 2001), which has demonstrated good results on a number of NLP tasks, ranging from POS tagging to chunking (e.g.
Tsuruoka et al., 2009).
In our experiments, we used CRFSuite (http://www.chokkan.org/software/crfsuite/), a fast implementation of CRF, for tagging the angiogenesis terms and events.
In more detail, we converted the data to IOB2 representation (Ramhsaw and Marcus, 1995), where words that were not entities or events of interests received the tag O.
For the words that formed an entity or event of semantic class x (e.g.
angiogenesis event), the first word was tagged with B-x, and the remaining ones with I x.
For tagging angiogenesis terms, we used the following features: unigram, bigram and trigram to the left and right of the current word, whether the current word was the head word of the current noun phrase, whether the term was seen in a noun phrase in the document title.
For extracting angiogenesis events, we tested two feature settings: the first was the same as described above, and the second exploited the gold standard annotation of genes, cells and tissues, in addition to the first feature set.
These two settings respectively correspond to the CRF and CRF-entity systems in Table 5.
We used the gold standard entities to estimate whether entity information was helpful for event identification.
In more detail, both the semantic type and the text string of the gold standard entities were incorporated as features for classification.
In practice, automatic systems (e.g.
Hanisch et al., 2005; Sasaki et al., 2008b; Wilbur et al., 2007) can be used to generate the named entities.
This way, the overall performance of event extraction may decrease.
However, in order to focus on examining the performance of complex text mining tasks such as relation and event extraction, in experiments, it is a common practice to assume previous components produce gold standard annotations (e.g.
Alex et al., 2008; Kim et al., 2009).
3 EVALUATION AND RESULTS The performance of the systems was measured by precision, recall and F1-score (i.e.
balanced precision and recall).
To be considered correct, a system prediction must match not only the type of the entity or event, but also both boundaries.
For angiogenesis events, sometimes boundaries are not crucial.
For example, if the gold standard is vascular endothelial cell proliferation, then endothelial cell proliferation is perhaps a good prediction, even if its left boundary does not match the gold standard.
Therefore, we used an additional measure called approximate boundary matching, which allows the spans of the predicted events to slightly differ from the gold standard.
A similar measure was also adopted in the BioNLP event evaluation tasks (Kim et al., 2009).
Table 4 compares three sets of results for tagging angiogenesis terms, where the CRF results were obtained by 5-fold cross-validation on the manually created gold standard data.
The CRF model (Section 2.5.4) clearly outperformed DictionaryBased, which uses the manually compiled dictionary of angiogenesis terms (Section 2.2).
Note that the IAA was calculated on a small set of 20 documents that were doubly annotated in the pilot annotation, and therefore it was possible that system performance exceeded IAA (Section 2.5.3).
Table 5 shows the results for angiogenesis event identification.
Both PatternBaseline and PatternExtended exploited the manually 2735 [17:25 8/9/2011 Bioinformatics-btr460.tex] Page: 2736 27302737 X.Wang et al.
compiled tissue and trigger vocabularies, but the former performed matches following simple patterns (Fig.1), whereas the latter applied patterns incorporating ENJUs predicateargument relations (Fig.2).
PatternExtended was a clear winner over PatternBaseline, which demonstrated that syntactic relations were useful.
CRF and CRF-entity were supervised methods, and they were trained and tested by 5-fold cross-validation on the manually created corpus.
As mentioned, the difference between the two systems is that CRF used only contextual word and n-gram as features, while CRF-entity also exploited the gold standard entity annotation.
CRF-entity obtained the best results as measured by every metric, and the performance of CRF was also promising.
Nevertheless, the two methods were the most expensive to develop, as they required high-quality training data, which were laborious and time consuming to produce, even though we significantly simplified the annotation guidelines as compared with other annotation projects such as GENIA.
The bottom three rows in Table 5 present the results of the method that automatically constructs tissue and trigger vocabularies by comparing PAS language models between a domain-specific corpus and a general one (Section 2.4).
We experimented with three different domain-specific foreground corpora, and the distinct performance indicates that this method is sensitive to the choice of foreground corpus.
The empirical results show that employing the collection of the angiogenesis review articles and Wikipedia page obtained the best results, which correlates with the fact that this foreground corpus contained more concentrated information regarding angiogenesis than the others.
While using the manually constructed vocabularies achieved good precision (87.71%), it suffered from a poor recall (29.40%).
On the other hand, the automatic method using ReviewArticles, yielded better recall and F1 scores, indicating its ability to discover a wider range of terms from the domain-specific documents.
4 CONCLUSIONS This article presented solutions to a text mining task of automatically extracting terms and events describing specific bioprocesses.
We examined angiogenesis, a bioprocess of blood vessel growth, and manually created two types of resources to assist the study: angiogenesis-related vocabularies and a gold standard corpus.
In particular, the gold standard corpus consists of 262 medline abstracts, where angiogenesis terms and events, as well as genes, gene products, cells and tissues, were manually annotated.
The statistics of the corpus shows that 36.5% of mentions of angiogenesis appear in text as an event, which previous bioprocess extracting techniques struggle to find.
We developed and compared a range of methods using the manually built vocabularies and gold standard corpus, and the experimental results showed that a CRF model outperformed the others: on detecting angiogenesis terms, it achieved an F1-score of 92.8%; on event recognition, the model yielded an F1 of 64.79% and a precision of 88.64%, when the restriction on event boundaries was relaxed.
Nevertheless, the CRF model relied on the manually created gold standard, which domain experts spent 150 person/h to create.
In contrast, the angiogenesis-specific vocabularies were less time consuming to develop (40 person/h), but the pattern matching approaches using the vocabularies obtained lower performance results.
We also proposed a new method that automatically discovers angiogenesis-related tissue terms and trigger words, by comparing the language models built on the predicateargument relations of two ENJU-parsed corpora: one contained angiogenesis-specific documents and the other general biomedical texts.
This method required very little human supervision, and the pattern-based systems achieved better results when using the automatically built angiogenesis vocabularies than the manual ones.
Overall, the relative low development cost of this new method indicates that it has better domain adaptability than the others, while achieving reasonable performance results.
ACKNOWLEDGEMENTS We would like to thank Catriona Tate who helped on the annotation.
The Oncology group at AstraZeneca provided the domain knowledge.
Funding: This work was funded by UK Biotechnology and Biological Sciences Research Council (BBSRC) under project Automated Biological Event Extraction from the Literature for Drug Discovery (reference number: BB/G013160/1).
The UK National Centre for Text Mining is funded by UK Joint Information Systems Committee (JISC).
Conflict of Interest: none declared.
ABSTRACT Motivation: The classification of biological entities in terms of species and taxa is an important endeavor in biology.
Although a large amount of statements encoded in current biomedical ontologies is taxon-dependent there is no obvious or standard way for introducing taxon information into an integrative ontology architecture, supposedly because of ongoing controversies about the ontological nature of species and taxa.
Results: In this article, we discuss different approaches on how to represent biological taxa using existing standards for biomedical ontologies such as the description logic OWL DL and the Open Biomedical Ontologies Relation Ontology.
We demonstrate how hidden ambiguities of the species concept can be dealt with and existing controversies can be overcome.
A novel approach is to envisage taxon information as qualities that inhere in biological organisms, organism parts and populations.
Availability: The presented methodology has been implemented in the domain top-level ontology BioTop, openly accessible at http://purl.org/biotop.
BioTop may help to improve the logical and ontological rigor of biomedical ontologies and further provides a clear architectural principle to deal with biological taxa information.
Contact: stschulz@uni-freiburg.de 1 INTRODUCTION The classification of biological entities according to their morphological, genetic, evolutionary and functional characteristics is a fundamental organizing principle since Carolus Linnaeus established conventions for naming living organisms (Ereshefsky, 2001).
One century later, the distinction of species received its theoretical underpinning with Charles Darwins theory of evolution (1859) and was finally demystified by the spectacular advances of molecular biology in the late 20th century.
Although these changes have drastically challenged the basic assumptions of Linnaeus biological theory and have given rise to an ongoing debate about the concept of biological species and taxa Hey (2006), his main organizing principle remains the same.
All biology is, in some way, related to the concept of biological taxa.
Taxa are hierarchically structured labels or categories used for biological classification, such as species, family, class, etc.
All organisms, populations, tissues, cells, cell components and biological macromolecules that are under scrutiny of experimental or descriptive biologists are related to some hierarchy of taxa and most biological discoveries have their scope related to one To whom correspondence should be addressed.
species or taxon.
Table 1 gives an exemplary overview of the hierarchical order of taxa.
The basic taxon is the species.
Several species are grouped together by a genus.
Several genera constitute a family, several families an order, several orders a class and then several classes a phylum or division.
Finally, the top-most level, the kingdom distinguishes between animals and plants.
Similar to the several criteria that are discussed to delineate the concept of species, no clear principles exist that govern the division of superordinate taxa.
For instance, orders can be further split into superorders and suborders.
Even more, the number of taxonomic divisions is variable, and there are also divisions without rank name.
The importance of species and biological taxa is evidenced by many sources.
Biological taxa constitute 3497 out 24 766 descriptors of MeSH1, the indexing vocabulary of Medline.
In the Open Biomedical Ontologies (OBO) collection2 (Smith et al., 2007), 30 out of 66 ontologies are taxon specific, with taxa ranging from species such as Homo Sapiens or Caenorhabditis elegans, genera such as Plasmodium over families such as Poaceae to classes such as Mammalia.
Due to the sheer number of taxa there is no universal authoritative source, but every important subfield within biology has been independently maintained by curators, so-called systematists, and for a long time the field of biological systematics has been considered an important research discipline.
A converging effort in unifying taxon information for whole biology is the Catalogue of Life3 targeted for complete coverage of all 1.75 million known species by 2011.
In the mentioned OBO collection, nearly half a million taxon entries of medical interest is available in computer-processable form via the rapidly growing NCBI Taxonomy (Wheeler et al., 2008).
To sum up, biological taxa constitute an overarching and systematic ordering principle that is relevant in practically all biological subject areas.
In this article, we will show how the realm of biological systematics can be embedded into an ontological framework.
It is structured as follows: We start with a summary introduction of domain ontologies in general, as well as in the context of the biology, addressing the OBO ontologies and the BioTop biomedical top-domain ontology.
Then we provide a formal account of different aspects of the conceptualization of biological taxa and demonstrate how this is implemented in BioTop.
Finally, we briefly describe our tentative implementation supporting our claim that an overarching ontological framework for biology must have a conclusive and practical account of biological taxa.
1Medical Subject Headings, http://www.nlm.nih.gov/mesh 2Open Biomedical Ontologies, http://www.obofoundry.org 3Catalogue of Life, http://www.catalogueoflife.org 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[19:58 18/6/03 Bioinformatics-btn158.tex] Page: i314 i313i321 S.Schulz et al.
Table 1.
Biological taxa with examples Taxon (rank) Asian elephant Chimpanzee Drosophila Species Elephas maximus Simia troglodytes Drosophila melanogaster Genus Elephas Pan Drosophila Subfamily Drosophilinae Family Elephantidae Hominides Drosophilidae Superfamily Elephantoidea Order Proboscidea Primates Diptera Class Mammalia Mammalia Insecta Subphylum Vertebrata Vertebrata Phylum Chordata Chordata Arthropoda Kingdom Animalia Animalia Animalia 2 BIOMEDICAL ONTOLOGIES 2.1 The foundations of biomedical ontology It is mainly the information explosion in biology and the necessity to process huge amounts of research data that have stimulated the proliferation of biomedical ontologies.
Rubin et al.
(2008) give an overview of the broad range of biomedical information services that can be supported by domain ontologies, with the Gene OntologyAshburner et al.
(2000) and the OBO collection as the most prominent examples.
Whereas this tenet used to be addressed in the past mainly by what had been termed biomedical terminologies (with the UMLS4 as prototypical example), more recently we have seen a steady growth in the usage of the term ontology.
Due to the lack of a clear notion of what an ontology actually constitutes (Kusnierczyk, 2006) there is a tendency for either insupportable expectations or general rejection of this term.
In this article, we detach the concept of terminology from the one of ontology subscribing to the following definitions: According to ISO (2000), a terminology is defined as a set of terms representing the system of concepts of a particular subject field.
Terminologies relate the senses or meanings of linguistic entities.
In contrast, according to Quine (1948), Ontology (in singular and upper case) is the study of what there is.
In our understanding, ontologies (plural and lowercase) are formal theories that attempt to give precise formulations of the types of entities in reality, of their properties, and of the relations between them (Guarino, 1998).
In contradistinction to terminology, formal ontologies strive for describing (as much as possible) what the consensus in a given scientific domain is, independently of human language.
Their constituent nodes are referred to as types, kinds or universals.
As they are well suited to hierarchically order and classify particular entities (e.g.
a given piece of tissue, a cell under a microscope, an amount of biological substance, an animal, a particular population of bacteria, etc.
), they are also referred to as classes, a parlance we will use in the following, in accordance with the more recent language use in current biomedical ontology engineering and research.5 Although the question whether certain entities really exist are subject to major philosophical disputes, we contend that at any 4Unified Medical Language System (UMLS): http://umlsinfo.nlm.nih.gov 5We follow a general trend and restrict the use of the word concept to the realm of terminologies, where it denotes artifacts that represent meanings of linguistic expressions.
We avoid it in relation to formal ontologies.
given stage in the development of science, there is a consensus core of scientific understanding of reality, and in our view, it is this which should serve as starting point for developing science-based ontologies.
Examples of statements belonging to this consensus core are that: primates are vertebrates, cells contain cytoplasm, aspirin tablets contain a derivative of salicylic acid, ADP is phosphorylated in mitochondria or that certain biochemical compounds have a clearly delineated composition.
2.2 Top-level ontologies It is widely recognized that the construction of formal ontologies should obey principled criteria.
To this end, several top-level ontologies have been devised, such as DOLCE (Gangemi et al., 2002), BFO (Smith et al., 2005), or GOL (Heller and Herre, 2004).
These ontologies mainly coincide in their fundamental division between continuants (also called endurants, e.g.
material objects) and occurrents (also called perdurants, e.g.
events, processes).
Orthogonal to this distinction, there is also a coincidence in clearly separating concrete entities or particulars (e.g.
the chimpanzee named Washoe, the elephant named Clyde, or the 3rd author of this article) from the classes they instantiate (e.g.
Chimpanzee, Asian Elephant, Human).
To this end, we introduce the irreflexive, anti-transitive and asymmetric instantiation relation instance_of which relates particulars to classes.
In addition, we need a formal relation for subsumption between classes.
Here we follow the OBO standard and introduce, for this purpose, the taxonomic subsumption relation Is_a by means of instance_of6 just as proposed by Smith et al.
(2005): Is_a (A,B)=def x : (instance_of (x,A) instance_of (x,B)) In the following discussion, we are proposing several possible alternative solutions for an ontological account of species.
2.3 Domain top-level ontologies Whereas top-level ontologies contain only a restricted set of highly general classes, such as the aforementioned Continuant, Occurrent, Function or Object, which are not tied to any particular domain of interest, a domain top-level ontology contains all the classes that are essentially needed to describe a certain domain, like Organism, Tissue, Cell and also Species in the case of biology.
Those more specific classes are in turn a specialization of the top-level classes as expressed in the formula Is_a (Cell, Object).
2.4 BioTopa domain top-level ontology Recently, two separate implementations to encode the top-level of the biomedical domain into ontologies have been created, namely, BioTop7 (Stenzhorn et al., 2007) and the Simple Top Bio (Rector et al., 2007).
At the moment, efforts set forth by the authors are ongoing to converge these two implementations.
The goal of BioTop is to provide classes and classificatory criteria to categorize the foundational kinds of biology, without any restriction to granularity, species, developmental stages or states 6Throughout this article, we use capitalized initial letters for the names of relations between universals, as well as for the names of universals.
Particulars are highlighted by lower case or by quoted names, bold face is used for relations between particulars.
7Available at http://purl.org/biotop i314 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i315 i313i321 Ontology of biological taxa of structural well-or ill-formedness (Schulz and Hahn, 2007).
The initial impetus for creating the BioTop ontology was the idea of redesigning and expanding the GENIA ontology (Ohta et al., 2002) in a comprehensive and formally sound way, i.e.
to adhere to the fundamental principles of formal rigor, explicitness and precision of ontological axioms.
In BioTops initial development, no definitive commitment existed towards any existing upper ontology, except for the distinction between continuants and occurrents (cf.
Section 2.2).
The primary focus at this stage was set on representing continuants from the area of interest.
In the continued development, however, the focus was broadened to include the representation of biological processes, functions and qualities.
Additionally, BioTop was aligned with the BFO upper level ontology.
BioTop is implemented in OWL DL,8 an official Semantic Web standard published by the World Wide Web Consortium (W3C).
By using this language, our ontology can benefit from a large amount of support tools for editing, automatic classification, etc.
OWL DL is also one of the languages accepted by the OBO consortium.
The significance of this lies in the fact that, in our view, the high-level BioTop classes can serve as a bridge to link and interface the domain-specific ontology classes in the OBO collection.
Using such interfacing facility can both potentially reveal overlaps or design errors in OBO ontologies and also create synergetic effects.
2.5 The difficult concept of species Before we embark on a more general ontological account of biological taxa, we first turn to the most basic taxon, namely, species.
Both biologists and philosophers disagree on the proper definition of the term species and its ontological status (Ereshefsky, 2001).
It had been principally the criterion of similarity between organisms and organism groups that guided Linnaeus classificatory efforts.
Although there are rarely any two individuals with exactly identical characteristics, we made the following observations in regard to the similarity of organisms.
From a diachronic point of view, there are generally significant but relatively minor differences between an organism and its offspring due to sexual or asexual reproduction and spontaneous mutations.
However, the distance increases with the number of generations and so todays organisms have little in common with their ancestors.
The genetic and phenotypic modifications can be assumed to lie on a mainly continuous scale, and the boundary of the emergence of a new species cannot be drawn by unambiguous criteria, a phenomenon that is ubiquitous in biology (Schulz and Johansson, 2007).
No obvious distinguishing feature exists that is apt to clearly divide the species Homo sapiens from Homo erectus and nothing indicates any sort of qualitative leap.
As a corollary of this, the parallel evolution of independent lines of organisms increases their genetic and phenotypic distance.
Under a synchronic viewpoint, this manifests itself as groups of organisms with clear criteria of species identity.
In contrast to the diachronic view, the distinguishing features do not lie on a continuous scale but they are clearly discrete.
For instance, the boundary between the species Homo sapiens and Simia troglodytes (chimpanzee) can be clearly drawn, as there are no organisms existing in the middle.
Even under the diachronic perspective, the distinction between groups of organisms with diverging characteristics may be blurred, 8Web Ontology Language (OWL): http://www.w3.org/TR/owl-features e.g.
by the distinction of subgroups of the same species.And different species may even form hybrids and merge to a new species.
All these peculiarities claim for a non-arbitrary conceptualization of what constitutes exactly a species.
There are different types of species concepts, from which the concept of biological species as a group of organisms that can interbreed and produce fertile offspring (Mayr, 1969), has found the widest acceptance.
Nevertheless, this definition provides only necessary but not sufficient criteria.
A defined population of organisms (e.g.
the Asian elephants living in Thailand) certainly fulfills this criterion although they do not form a species of their own since they can mate and produce fertile offspring with elephants from Cambodia, for instance.
Abbreviating the ability of producing fertile offspring by , according to the biological species concept, the pertinence of biological organisms to the same species is expressed by the predicate : ( o1,o2 )= def (t :(o1,o2,t ))(o,t1,t2 : ( ( o1,o,t1 )(o2,o,t2 ))) The shortcomings of Mayrs definition are well known (Grene and Depew, 2004, ch.
10): first, it only allows the comparison of organisms living at the same time.
Second, the definition depends on the dispositional criterion , the verification of which remains speculative in many cases.
Third, the definition fails with infertile individuals, as well as with species in extinction of which only female or male individuals remain.
Fourth, it fails in the numerous cases of asexual reproduction such as bacteria.
It is therefore neither easily applicable, nor generally valid, in spite of its theoretical soundness (Hull, 1997).
So it is not surprising that other species concepts compete with Mayrs one.
The 22 different conceptualizations of species identified and discussed by Mayden (1997) bear witness on the intensive discussions and disagreements among theoretical biologists and philosophers.
For our practical purpose of biomedical ontologies the formalization of species ormore generallyof biological taxa that we propose, is intended to be neutral to the different and conflicting species conceptualizations.
It departs from the principle that biological taxa are something that regardless of its existence in nature or its (fiat) attribution by biologists has a highly ranked importance in biology and therefore requires to be accounted for in biomedical ontologies.9 In the following, we will analyze the ontological status of biological taxa and propose and critically assess alternative solutions.
3 CONCURRENT ACCOUNTS OF BIOLOGICAL TAXA 3.1 Biological taxa as meta-properties The above restriction to a two-leveled ontological framework (i.e.
dividing the world exhaustively into particulars and universals) has often been challenged.
(Gangemi et al., 2001) contend that there is a fundamental difference between instances in an ontology on the one hand and domain entities (particulars, cf.
Section 2.1) on 9The approach should be flexible enough to support even classification schemes that contradict classic taxonomic principles such as carnivore and herbivore.
The authors are aware of the fact that this may challenge some of the philosophical foundations underlying Basic Formal Ontology (BFO).
i315 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i316 i313i321 S.Schulz et al.
the other hand.
They argue that we can extend a Theory A (which follows the two-level assumption) by a meta-Theory B.
Whereas Theory A describes domain entities (particulars) that instantiate universals (classes), B takes As universals as instances of so-called meta-properties.
Indexing the instantiation relation by theory level (using subscripts in the formulae) we may state in Theory A that instance_ofA(x,y) and then place this in the context of Theory B with instance_ofB (y,z) To give a concrete example: instance_ofA(Clyde, Elephas maximus) instance_ofB (Elephas maximus, Species) Due to the algebraic property of antitransitivity (as claimed by (Gangemi et al., 2001), we can then coherently reject the hypothesis that our elephant Clyde is an instance of Species.
There are several arguments against this solution.
Let us consider the second-level predications instance_ofB (Elephas maximus, Species) on the one hand and instance_ofB (Elephas maximus, Genus Elephas) on the other hand.
Whereas the first one asserts that the class Elephas maximus is an instance of a Species, the second one states that the species class Elephas maximus as a member of the genus Elephas.
In the same right as we have stated instance_ofB (Elephas maximus, Species) we could then assert in a third-level predication (instance_ofC) instance_ofC (Genus Elephas, Genus) Clyde would then be a second-level instance of Species and a second-level instance of Genus Elephas, as well as, in virtue of the latter, a third-level instance of Genus.
Given instance_ofC (Species, Taxon) and instance_ofC (Genus, Taxon), Clyde would finally act simultaneously both as third and fourth-level instance of Taxon.
Together with the argument that Clyde might also directly instantiate Genus Elephas and the fact that some taxonomic levels (such as subfamilies) are sometimes skipped, it is very obvious that this solution leads to an obscure and inconsistent picture.
Another shortcoming of this approach lies in the fact that it lacks a transitive hierarchical relation between taxa of different levels that would be able to express in simple terms (e.g.
that all Indian elephants are vertebrates).
From a computational viewpoint, there is also an important performance argument.
For example, efficient reasoning algorithms which have been developed for description logics (Baader et al., 2003) and are coherent with the Semantic Web standard OWL DL do not provide support for reasoning capabilities about instances of instances.
3.2 Biological taxa as hierarchies of classes We could simplify the above approach (and render it well-suited for description logics-based reasoning) by conflating the level of classes with the one of the meta-level classes.
Given the definitions above and a division of all entities in either particulars or classes, it may appear straightforward to use the Is_a relation for expressing that Chimpanzees, Indian Elephants, Humans, etc.
are species, or that Genus Pan, Genus Elephas and Genus Homo are genera: Is_a (Elephas maximus, Species) Is_a (Simia troglodytes, Species) Is_a (Genus Elephas, Genus) Is_a (Genus Pan, Genus), just as Is_a (Elephas maximus, Genus Elephas) Is_a (Simia troglodytes, Genus Pan) The weakness of this solution, however, immediately derives from the above definition of the Is_a relation.
So given that instance_of (Clyde, Elephas maximus) instance_of (Washoe, Simia troglodytes) we can infer that instance_of (Clyde, Genus Elephas) instance_of (Washoe, Genus Pan) as well as that instance_of (Clyde, Species) instance_of (Washoe, Species) instance_of (Clyde, Genus) instance_of (Washoe, Genus) We finally end up with all taxa in a specialization hierarchy, having individual organisms as instances.
This neither captures the nature of a biological organism, nor the intended meaning of Species or Genus, since neither Clyde nor Washoe or any other individual animal is an instance of the class Species.
Nevertheless, we could consistently do this excluding the terms species, genus, etc.
This would reduce the instances of taxa (Elephant, Elephantidae, Vertebrates) to classes of organisms and we would no longer be able to account for the meaning of terms like Genus or Species in a description logic-based framework.
However, the resulting assertions such as Clyde is an instance of Mammalia (on par with Clyde is an instance of Elephant) would collide with the plural meaning of the taxon terms.
3.3 Biological taxa as populations Several authors have argued in favor of the inclusion of collectives into an ontological framework (Bittner et al., 2004; Rector et al., 2006; Schulz et al., 2006a) .
BioTop has embraced these aspects by introducing the relation has_granular_part, an irreflexive and intransitive subrelation of the OBO Relation Ontology relation has_part (Schulz et al., 2006b).
This allows us to relate a collective entity to each of its constituent elements, without, however, resorting to set theory.
For instance, has_granular_part (PopulationofThaiElephants,Clyde) asserts that there is a collective entity Population of Thai Elephants that is constituted by granular parts like our elephant Clyde and a number of other individuals similar to Clyde.
It permits to define i316 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i317 i313i321 Ontology of biological taxa Is_A W ashoe Genus Pan Quality Family Elephantidae Quality Order Primates Quality Family Hominides Quality Class Mammalia Quality Subphylum Vertebrata Quality Genus Elephas Quality Simia troglodytes Quality Elephas maximus Quality Order Proboscidea Quality Phylum ChordataQuality Kingdom Animaliaa Quality Is_A Is_A Is_A Is_A Is_A Is_A Is_AIs_A Is_A Is_A Is_A Clydes taxon quality Washoes taxon quality instance_ofinstance_of inheres_in Clyde Washoe P ar tic ul ar s U ni ve rs al s (C la ss es ) inheres_in Fig.1.
Taxon qualities inhering in individual organisms.
collectives in terms of granular parts such as x : instance_of (x, ElephantPopulation) y1,y2,...yn : instance_of (y1,y2,...,yn, Elephant) has_granular_part (x,y1,y2,...,yn) z : (instance_of (z, Elephant) has_granular_part (x,z)) Note that Population of Thai Elephants is a particular collective and an instance of the universal collective ElephantPopulation.
The union of all possible instances of ElephantPopulation, namely, Total ElephantPopulation would then be the maximal population of elephants every individual elephant is a granular part of.
x : instance_of (x, Elephant) has_granular_part (Total ElephantPopulation,x) Yet, TotalElephantPopulation is a particular entity.
Our proposal here is to consider it as an instance of Species.
In the same way, we could introduce other populations in different degrees of abstraction such as TotalVertebratePopulationwhich would then be an instance of Phylum.
It may be practical for many purposes to equate biological taxa with biological populations although the meaning of Elephantidae or Vertebratae, in practice, goes further.
Especially in molecular biology, species information is not only attributed to whole organisms, but also to organism parts, their constituting cells and derived cell lines.
As an example, individual cells from the HELA cell line are considered human cells, but their existence is not dependent on any human population.
The interpretation of biological taxa as populations is therefore not adequate for such cases.
We can use the OBO relation derives_from in order to express that a HELA cell is a human cell: x : instance_of (x, HELA Cell) y : instance_of (y, Human) derives_from (x,y) has_granular_part (TotalHumanPopulation,y) 3.4 Biological taxa as qualities Most top-level ontologies coincide in granting qualities a prominent status.
For instance, BFO describes the class Quality as A dependent continuant that is exhibited if it inheres in an entity or categorical property.
Examples: the color of a tomato, the ambient temperature of air, the circumference shape of a nose, the mass of a piece of gold, the weight of a chimpanzee.10 DOLCE introduces qualities as the basic entities we can perceive or measure: shapes, colors, sizes, sounds, smells, as well as weights, lengths, electric charges (Masolo, 2003) and also makes reference to the relationship of inherence.
The position of the class Quality in BFO makes clear that qualities are dependent entities, i.e.
they can only exist in dependence on the entities they inhere in.
Our proposal here is to interpret the relation of a biological object to a given taxon as the ascription of a quality.
For example, the quality of belonging to the species Homo sapiens is a quality that inheres in any human organism, tissue or cell.
The quality of belonging to the phylum Chordata is a quality that inheres in any biological object that is part of or derived from an organism the species of which belongs to the phylum Chordata.
Figure 1 depicts a segment of our proposed subclass hierarchy of taxon qualities.
The hierarchy exhibits two organizational principles: generalization versus specialization on one side, and the relevance to an organizational level on the other.
Every instance of a material biological object has one inherent taxon quality.
Since, e.g.
every human is a hominid, every inhering instance of the class Homo sapiens Quality is also an instance of Family Hominides Quality, etc.
The introduction of qualities is helpful for 10SNAP Continuant Definitions: http://www.ifomis.org/bfo/manual/snap.pdf i317 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i318 i313i321 S.Schulz et al.
Fig.2.
Taxon qualities inhering in individual organism and their location in Taxon Regions consistent with the DOLCE upper level ontology.
ontological definitions such as x : instance_of (x, Human) instance_of (x, Organism) y : instance_of (y, HomosapiensQuality) inheres_in (y, x) x : instance_of (x, Vertebrate) instance_of (x, Organism) y : instance_of (y, VertebrateQuality) inheres_in (y, x) Based on a hierarchy of qualities, such definitions permit inferences such as that every human is a vertebrate or that every human population is part of some vertebrate population.
In addition, it allows for linking organism parts with qualities such as x,: instance_of (x,VertebrateHeart) instance_of (x, Heart) y : instance_of (y,VertebrateQuality) inheres_in (y, x) If the import of the taxon concept should be extended from biological organisms to their parts, as argued in Section 3.3 (e.g.
human leukocyte), the attribution of qualities to organism parts or derivatives can easily be axiomatized by the so-called right identity rules (with being the relation concatenation symbol): part_of (x, y) inheres_in (z, y) inheres_in (z, x) derives_from (x, y) inheres_in (z, y) inheres_in (z, x) 3.5 Biological taxa as Qualia An alternative approach to a subclass hierarchy based on the DOLCE upper ontology (Masolo, 2003) is represented in Figure 2.
Since DOLCE is inspired by trope theory (Goodman, 1951), which distinguishes between qualities and their values (i.e.
Qualia) this proposal introduces another layer of abstraction.
Each quality type has an associated quality space (i.e.
Region) in which it is located.
As in BFO, qualities are dependent entities which are inherent in their respective particulars.
Compared to the representation depicted in Figure 1 only few taxon qualitiesone for every taxonare organized in a flat hierarchy and are related to corresponding value regions.
The subsumption hierarchy of taxon qualities of the former approach is represented as a partonomic hierarchy of the Taxon Regions in the latter, e.g.
the Species Region is part of the Class Region which is itself part of the Kingdom Region.
The variety of features is represented as subclasses of the basic Taxon Regions, e.g.
Mammalia Class Region Is_a Class Region.
The main advantages of this approach are a clearer separation of hierarchies and the possibility to make explicit assertions on the specialized Taxon Regions without uncontrolled inheritance of restrictions.
Its disadvantage lies in a higher complexity.
3.6 Synthesizing different taxon accounts We have proposed four mutually dependent kind of ontologically relevant entities that describe different aspects of what is meant i318 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i319 i313i321 Ontology of biological taxa by biological taxa on the one hand, and that are expressible in a description logics-based framework on the other.
The totality of organisms belonging to one taxon (e.g.
all Gram-positive bacteria, all primates or all humans).
This entity is a particular one that instantiates the class Maximal Biological Population.
For each taxon there is one such instance.
Population classes, the instances of which are defined as parts of some instance of Maximal Biological Population.
For example, Elephant Population in Thailand is an instance of the class Elephas Maximus Population, the latter being a subclass of Elephas Population and so on.
For each taxon there is one such population class.
Taxon quality classes that are instantiated by each and every particular object to which a taxon can be ascribed.
There is one such taxon quality class for each taxon.
Because taxon classes are arranged in an Is_a hierarchy, the quality of a subordinate taxon is also the quality of a superordinate taxon.
For example, an instance tqClyde of Elephas Maximus Quality can be ascribed to the elephant Clyde.
tqClyde is equally an instance of Genus Elephas Quality, of Family Elephantidae Quality, and so on.
Taxon quality regions that are represented by a mereological inclusion hierarchy.
In contrast to the third approach, every taxon-relevant entity has an inherent quality instance from each taxonomic level.
4 IMPLEMENTATION We extended BioTop by the notion of biological taxa following the quality approach discussed in Section 3.4. bfo:Entity  bfo:Continuant bfo:DependentContinuant  bfo:SpecificallyDependentContinuant  bfo:Quality   biotop:ContinuantQuality   biotop:TaxonQuality The class biotop:TaxonQuality has the following restrictions11: biotop:TaxonQuality implies inheres_in.
(has_part.biotop:NucleicAcid)AND inheres_in.
(has_part.biotop:NucleicAcid) So we claim the existence of genetic information as a limiting and necessary condition for those entities biological taxa can be ascribed to.
In the inverse direction, we claim the inherence of taxon qualities to the classes biotop:Cell, biotop:Organism, biotop:Tissue,biotop:OrganismPart, biotop:NucleicAcid, e.g.
biotop:Cell implies inv_inheres_in.biotop:TaxonQuality The class biotop:TaxonQuality is then the interface to a specialized ontology such as the NCBI taxon ontology.
For demonstration purposes we created taxdemo, a small example ontology.12 11For the Description Logics notation cf.
(Baader et al., 2003), or12Available at http://purl.org/biotop taxdemo:TaxonQuality  biotop:TaxonQuality  taxdemo:KingdomAnimaliaQuality  taxdemo:PhylumChordataQuality   taxdemo:ClassMammaliaQuality   taxdemo:OrderPrimatesQuality    taxdemo:FamilyHominidaeQuality    taxdemo:GenusHomoQuality     taxdemo:HomoSapiensQuality In parallel, the taxonomic ranks (TaxonQuality, KingdomQuality, etc.)
are indirectly represented as a second hierarchy.
taxdemo:TaxonQuality biotop:TaxonQuality  taxdemo:KingdomQuality  taxdemo:KingdomAnimaliaQuality taxdemo:KingdomBacteriaQuality taxdemo:KingdomVirusesQuality  taxdemo:PhylumQuality  taxdemo:PhylumChordataQuality  taxdemo:ClassQuality  taxdemo:ClassMammaliaQuality  taxdemo:OrderQuality  taxdemo:OrderPrimatesQuality  taxdemo:OrderProboscideaQuality  taxdemo:FamilyQuality  taxdemo:FamilyHominidesQuality  taxdemo:FamilyElephantidaeQuality  taxdemo:GenusQuality  taxdemo:GenusHomoQuality taxdemo:GenusPanQuality taxdemo:GenusElephasQuality  taxdemo:SpeciesQuality  taxdemo:HomoSapiensQuality  taxdemo:ElephasMaximusQuality This allows us to define population as a plurality of organism of the same species as follows: taxdemo:Population IMPLIES has_granular_part.biotop:OrganismAND =1 inv_inheres_in.taxdemo:SpeciesQuality These criteria are not met by mixed groups of individuals, e.g.
a group of different primates which coincide only at the level of taxdemo:OrderQuality The flexibility of our approach becomes obvious when we use taxon information for parts of the organisms.
For instance, the class HumanLeukocyte can be defined as taxdemo:HumanLeukocyte EQUIVALENT TO taxdemo:Leukocyte AND inv_inheres_in.taxdemo:HomoSapiensQuality If we define taxdemo:AnimalCell EQUIVALENT TO taxdemo:Cell AND inv_inheres_in.taxdemo:KingdomAnimaliaQuality i319 [19:58 18/6/03 Bioinformatics-btn158.tex] Page: i320 i313i321 S.Schulz et al.
then taxdemo:HumanLeukocyte can be classified as taxdemo:AnimalCell, provided that the ontology supports: taxdemo:HomoSapiensQuality Is_a taxdemo:KingdomAnimaliaQuality together with taxdemo:Leukocyte Is_abiotop:Cell It is obvious that this kind of reasoning can be of great advantage for biological fact retrieval from databases or for semantically enriched information extraction from texts.
From a computational perspective, however, we acknowledge that there still is a bottleneck with regard to the use of inverses (such as inheres_in versus inv_inheres_in) and qualified number restrictions (such as =1) in description logics reasoners.13 We admit that the meaning of the taxonomic rank classes SpeciesQuality, GenusQuality, KingdomQuality, etc.
is somewhat counterintuitive, since every instance of SpeciesQuality is also an instance of GenusQuality and so on.14 They are, therefore, not suited to comprehensively represent the meaning of Species as disjoint from Genus, Kingdom, etc.
Such a reading would require the meta-class representation as discussed in Section 3.1, discarded due to computational reasons.
In our framework, the only way to have an instantiable Species (Genus, Kingdom) class would be to collect all maximal populations (cf.
Section 3.3) with identical species-(genus-, kingdom-) level qualities as instances of Species (Genus, Kingdom) which, again, would only partially match the meaning of Species (Genus, Kingdom).
We refrained from implementing the solution discussed in Section 3.5, because its more differentiated approach to the representation of qualities is not supported by the BFO upper ontology, currently in use for BioTop.
5 RELATED WORK Literature on the ontology of taxa roughly falls into two categories: the conceptualization of the nature of species on the one hand, and the ontological status of taxa on the other.
In both cases, the focus lies mainly on species whereas higher taxa are seldom addressed.
The first line of scientific discussion is characterized by numerous publications that started with the seminal book of Mayr (1942), who compared several approaches to delineate the nature of species15 and propagated the popular concept of species as a group of organisms that interbreed and produce fertile offspring.
Hull (1997) casts doubt on the monistic assumption that there is one single and ideal way to define species and hypothesizes a trade-off between theoretical significance and practical applicability of species concepts.
He classifies the existing species concepts into three categories, namely, (i) similarity-based (which, of course, hinges on some unambiguous notion of phenic or genetic resemblance), (ii) biological and evolutionary (which includes Mayrs and other proposals such as Hennig, 1966) centering around the behavior (i.e.
mating, reproduction) of biological organisms and (iii) phylogenetic, focusing the historic development of species.
Mayden (1997) 13See frequently updated list at http://www.cs.man.ac.uk/sattler/ reasoners.html 14An instance of HomoSapiensQuality would be an instance of KingdomQuality, too.
15For an overview of earlier approaches see Hey (2006).
performed an extensive literature review and identified 22 distinct species concepts.
In contradistinction to Hull, he propagates the cladistics-based evolutionary significant unit (Evolutionary Species Concept, Simpson, 1961), rooted in the philosophical principle of identity: An evolutionary species is an entity composed of organisms that maintains its identity from other such entities through time and over space and that has its own independent evolutionary fate and historical tendencies.
According to (Goodman, 1951) this concept of species is the most acceptable and most compatible with other species concepts that are rather criterion-based detection protocols than theoretically underpinned concepts.
He argues that no criterion that presumes to delineate natural boundaries can overcome the generic vagueness (Hull, 1965) of species concepts.
Our approach advocates neutrality towards the conceptualization of species and is apt to coexist with both monistic and pluralistic approaches.
We are aware of the fact that in the latter case species qualities with multiple parents may be taken into account, due to different categorizations according to conflicting species concepts.
The second line of discussion is on more abstract grounds, and scrutinizes the ontological nature of species, regardless of the species concepts subtleties as exposed above.
A fundamental question in here is whether speciesseen as single evolving lineage that act as units of evolutionare classes or individuals, the latter being advocated by Ghiselin (1974) and Hull (1978), with the consequence that every single organism is a spatiotemporal part of its species.
This theory comes close to our view of species as the totality of organisms belonging to one specific species, which can be generalized from species to taxa.
We prefer this mereological approach over the set-theoretical one (also pointed out by (Ereshefsky, 2007), because the view of a group of organisms as mathematical sets (that are not localized in space and time) is rather counterintuitive.
The conceptualization of species as universals or natural kind conflicts with the fact that there are relatively few essential properties that are shared by all individuals of a species (including developmental stages and malformations).
Boyds (1999) Homeostatic Property Cluster Theory tries to overcome this, but is still too much committed to similarity-based criteria according to (Ereshefsky, 2007).
The approach pursued in this article, namely, introducing theory-neutral species qualities that are extensible to general taxon qualitiesseems to be rather novel.
6 CONCLUSION We have proposed an ontological approach to biological taxa in the context of the domain top-level ontology BioTop.16 It is essentially based upon the assumption that every biological organism, population or biological matter has some inherent taxon quality.
Since it does not raise further reaching ontological claims, our approach largely bypasses the ongoing dispute on species concepts.
This enables us to delineate biological populations in terms of shared taxon qualities and to formulate taxon-specific axioms in the framework of description logics.
Our proposal is fully embedded into the standards of Open Biological Ontology and is in line with a major top-level ontology, BFO.
Our account of taxon qualities (i.e.
the preference of the 16BioTop, together with a tentative taxon-specific extension is available at[19:58 18/6/03 Bioinformatics-btn158.tex] Page: i321 i313i321 Ontology of biological taxa simpler approach described in Section 3.4 over the more complex solution found in Section 3.5) also demonstrates how fundamental ontology design decisions depend on the choice of the underlying top-level model.
As our approach represents taxon qualities as a simple is_a hierarchy, the import of subsets of existing taxonomy databases such as the NCBI taxonomy is straightforward and scalable.
These data can automatically be transformed into an OWL subtype hierarchy and linked to the BioTop node TaxonQuality.
ACKNOWLEDGEMENTS The authors would like to thank Alan Rector (Manchester), Elena Beiwanger (Jena), Udo Hahn (Jena), Eric van Mulligen (Rotterdam) and Lszl van den Hoek (Rotterdam), as well as Olivier Bodenreider (Bethesda), for fruitful discussions.
Funding: This work was supported by the EC STREP project BOOTStrep (FP6 028099).
Conflict of Interest: none declared.
ABSTRACT Motivation: Analyses and algorithmic predictions based on high-throughput data are essential for the success of systems biology in academic and industrial settings.
Organizations, such as companies and academic consortia, conduct large multi-year scientific studies that entail the collection and analysis of thousands of individual experiments, often over many physical sites and with internal and outsourced components.
To extract maximum value, the interested parties need to verify the accuracy and reproducibility of data and methods before the initiation of such large multi-year studies.
However, systematic and well-established verification procedures do not exist for automated collection and analysis workflows in systems biology which could lead to inaccurate conclusions.
Results: We present here, a review of the current state of systems biology verification and a detailed methodology to address its shortcomings.
This methodology named Industrial Methodology for Process Verification in Research or IMPROVER, consists on evaluating a research program by dividing a workflow into smaller building blocks that are individually verified.
The verification of each building block can be done internally by members of the research program or externally by crowd-sourcing to an interested community.
www.sbvimprover.com Implementation: This methodology could become the preferred choice to verify systems biology research workflows that are becoming increasingly complex and sophisticated in industrial and academic settings.
Contact: gustavo@us.ibm.com Received on November 16, 2011; revised on February 8, 2012; accepted on March 5, 2012 1 BACKGROUND AND PHILOSOPHY OF SYSTEMS BIOLOGY VERIFICATION 1.1 What is verification?
In the past two decades molecular biology has experienced an increase in the amount and diversity of data that are produced to answer key scientific questions.
Systems biology has emerged as a new paradigm for the integration of experimental and computational To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first three authors should be regarded as joint First Authors.
efforts.
This uses algorithmic analyses to interpret the data and mathematical models are built to predict yet unmeasured states of the biological system.
However, algorithms and models are not unique and the determination of the right algorithm and model leading to the true interpretation of the natural phenomena under study becomes a fundamental question that falls within the realm of the philosophy of science.
Popper postulated (Popper, 1959) that a hypothesis, proposition, theory or in the case of systems biology a model, is scientific only if it is falsifiable.
In Poppers thesis, a theory can be proven wrong by producing evidence that is inconsistent with the theory.
In contrast, a theory cannot be proven correct by evidence because other evidence, yet to be discovered, may exist that will falsify the theory.
Conversely, according to the verificationist school, a scientific statement is significant only if it is a statement of logic (such as a mathematical statement deduced from axioms) or if the statement can be verified by experience (Ayer, 1936).
Statements that do not meet these criteria of being either analytic or empirically verifiable are judged to be non-sensical.
The McGraw-Hill Concise Dictionary of Modern Medicine (2002) defines verification as: The process of evaluating a system, component or other product at the end of its development cycle to determine whether it meets projected performance goals (http://medical-dictionary.thefreedictionary.com/verification).
For systems biology, a fundamental question to address is how to verify the correctness of a model that integrates vast amounts of data into a representation of reality.
These data are not only high-dimensional but noisy given the biological variability, sample preparation inconsistencies and measurement noise inherent to the sensor instrumentation.
While the concept of verification may be applied to different contexts with slightly different meanings, here we always use verification as checking for the truth or correctness of either data (i.e.
whether the data represents what we wish to measure) or the correctness of a theorys predictions.
1.2 Crisis in peer-review/slow and low throughput The quality of a scientific prediction or the accuracy of a scientific model is the subject of rigorous scrutiny, usually by the researchers themselves or by colleagues in the peer-review process that is at the heart of scientific publishing (Spier, 2002).
As stated by the editors of the journal Science (Alberts et al., 2008), The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1194 11931201 P.Meyer et al.
peer review is under increasing stress [] The growth of scientific publishing is placing a burden on the entire scientific enterprise.
Papers today are more interdisciplinary, use more techniques, and have more authors.
Many have large volumes of data and supplementary material.
The coming of age of systems biology and its computational methods such as data-interpreting algorithms are challenging the peer-review process as large numbers of simultaneous predictions are generated, but only a small minority is tested.
In the best cases, a very small sampling of predictions are verified using sound experimental assays and methods and then are presented as representative confirmation of the soundness of the entire set of predictions.
Typically, this verification method lacks sufficient rigor, objectivity and a clear characterization of the relative strengths and weaknesses of the algorithms (Dougherty, 2010; Jelizarow et al., 2010; Mehta et al., 2004).
The same lack of rigor in verification of model predictions can be found in many areas of science where complex systems are measured, analyzed and modeled.
For example, in systems biology, high-throughput data are collected and analyzed together with insufficient verification.
Specifically, false positive and, equally important, false negative rates, are rarely considered a requisite for verification of the analysis for publication.
Consider that the first experimentally-generated, genome-wide interactomes in yeast (Gavin et al., 2006; Ito et al., 2001; Uetz and Hughes, 2000) showed minimal overlap, generating some concerns within the scientific community that the data and methodologies were unreliable.
Later work showed that high quality interactome maps could be generated by including controls and quality standards in data collection, careful verification of all interacting pairs and validation tests using independent, orthogonal assays (Dreze et al., 2010).
Similarly, Genome-Wide Association Studies (GWAS) generate a high rate of false positives as correlations are found for single nucleotide polymorphisms with no direct effect on the phenotype.
The community responded by defining a quality-control process and software package for analysis (Purcell et al., 2007).
Similar problems are found in other fields including protein structure prediction (Moult et al., 1995), prediction of docking between proteins (Wodak and Mendez, 2004), text mining from scientific literature (Hirschman et al., 2005) and biological network inference (Stolovitzky et al., 2007).
In these cases the response has been to set up community-based efforts, as discussed below.
1.3 Proposed community approaches for science verification The difficulties in verifying complex science with traditional methods is driving changes in the methods of evaluation.
Advances in web technology (called web 2.0) have allowed communities to stay tightly in touch to develop their interests, even when they are geographically dispersed.
The journal Nature developed in 2006 an experiment allowing an online public review of manuscripts that in parallel were undergoing peer-review (http://www.nature.com/nature/peerreview/).
Faculty of 1000 is an annotation service that allows researchers to locate outstanding or influential papers from the whole body available that can completely overwhelm the individual.
Faculty of 1000 has domain experts cull, rate and summarize both the importance of the papers findings and context within the field and hence is a good example of new practices in research evaluation that go far beyond simple indexing and content annotation (as in PubMed, for example).
The journal PLoS ONE and now even mainstream sites like Twitter have become places where manuscripts are publicly criticized (Mandavilli, 2011).
We think that these changes in research evaluation, while valuable, will not have sufficient rigor and consistency for the needs of research workflows verification.
2 COMMUNITY APPROACHES FOR SCIENCE VERIFICATION 2.1 Community consensus as criteria of science done right A natural evolution of allowing community feedback has been the development of crowd-sourcing, a modality of distributed problem-solving.
Challenges are broadcasted to potential interested stakeholders (solvers) in the form of an open call for participation.
Participants submit solutions for the challenges, and the best solutions are typically chosen by the crowd-sourcer (the entity that broadcasted the challenge).
The top performing participants are sometimes rewarded either with monetary awards, prizes, certificates or with recognition.
We think that such directed community approaches could complement and enhance the peer-review process.
Most importantly, we think that these could serve as a tool to verify the scientific results and fulfill the ultimate goal of scientific research that is to advance our understanding of the natural world (Meyer et al., 2011).
Community-based approaches to verify scientific research can be considered a more focused attempt to tap the consensus building that historically occurs in scientific progress.
Kuhn understood progress in science as an eminently social process, in which the scientific worldview is dominated by the paradigm embraced by the scientific community at any given time (Kuhn, 1962).
When the number of anomalies accumulated under the current paradigm generates distrust, the community may adopt a new paradigm that now guides how research is conducted.
In this view, the scientific community, and not just nature itself, needs to be taken into account when considering what is accepted as verified science.
For our purposes, we abbreviate the typical definition of verification given in the first paragraph to: science done right, where the right refers to the accepted best practices of the scientific community or similar criteria.
Accepted best practices means that there is a consensus in the community as to the proper collection and analysis of a data modality.
Obviously, a modality must already be accessible to a wide community for the consensus to form.
For newly developed modalities, crowd-sourcing provides a means to a rapid consensus as to the best collection and analysis methodologies.
2.2 Summary of community approaches for verification in other fields Recent practices involving a new form of research quality control have become well-established during the last decade and a half.
These efforts have merged the need of scientific verification of methods used in research, with the widespread practice of crowd-sourcing, to create a sort of collaboration-by-competition communities.
The practice of this idea has been sufficiently well-established to become the business model of for-profit companies.
In this section, we summarize three relevant community-based 1194 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1195 11931201 IMPROVER Table 1.
Additional information for the eight community-based efforts described in the paper.
The last row describes other efforts not discussed in the main text Name Domain and Regularity Website KDD Cup Knowledge discovery and machine learning in various domains.
http://www.sigkdd.org Knowledge Discovery and Data Mining.
Every year since launch in 1997.
InnoCentive The name mixes Innovation and Incentive.
http://www.innocentive.com/ Crowd-sourcing for problems of commercial interest.
Founded in 2001.
New challenges are released on a rolling schedule.
Netflix Prize The name comes from the sponsoring company, Netflix.
http://www.netflixprize.com//index Prediction of user ratings for films, based on previous ratings.
Only challenge so far, released in 2006, lasted 3 years to complete.
CASP Critical Assessment of Techniques for Protein Structure Prediction.
http://predictioncenter.org/ Protein 3D structure prediction assessment.
Every 2 years since 1994.
CAPRI Critical Assessment of PRedicted Interactions.
Assessment of predictions of http://www.ebi.ac.uk/msd-srv/capri proteinprotein docking or protein-DNA interaction from 3D structure.
Goes by Round 22 since 2001.
Starts whenever an experimentalist offers an adequate target.
Predicted structures are submitted 68 weeks later.
DREAM Dialogue for Reverse Engineering Assessments and Methods.
http://www.the-dream-project.org/ Assessment of quantitative modeling in systems biology.
Every year since 2006.
BioCreAtIve Assessment of Information Extraction Systems in Biology.
Evaluating text mining http://www.biocreative.org and information extraction systems applied to the biological literature.
http://biocreative.sourceforge.net Every 2 years beginning in 2004.
FlowCAP Flow Cytometry Critical Assessment of Population Id Methods.
http://flowcap.flowsite.org/ Evaluation of automated analysis of flow cytometry data.
http://groups.google.com/group/flowcap Only one iteration on 2010, second one on planning phase.
Others efforts TunedIT: http://tunedit.org/, RGASP-RNAseq Genome Annotation Assessment Project: www.sanger.ac.uk/PostGenomics/encode/RGASP.html Pittsburgh brain competition: http://pbc.lrdc.pitt.edu/ CAMDA Critical Assessment of Microarray Data Analysis: http://camda.bioinfo.cipf.es/camda2011/ Genome Access Workshop evaluation of statistical genetics approaches: http://www.gaworkshop.
verification approaches with overlapping objectives but different focus areas.
Some relevant details of these efforts are listed in Table 1.
Knowledge Discovery and Data Mining Cup (KDD Cup) is an annual competition organized by the Association for Computing Machinery (ACM) Special Interest Group on Knowledge Discovery and Data Mining, the leading professional organization of data miners (Fayyad, 1996).
KDD goals are to achieve a better understanding and analysis of data in many knowledge domains, such as medical informatics, consumer recommendations, diagnostics from imaging data and Internet user search query categorization.
InnoCentive, a spin-off of Eli Lilly, was founded in 2001 to match problems in need of solutions with problem solvers.
The main entry point of InnoCentive is a web portal where solutions to scientific and business problems are solicited on behalf of organizations seeking innovations.
An example of a recent challenge is Solutions to Respond to Oil Spill in the Gulf of Mexico.
InnoCentive works with seekers to design the challenge, score/judge solutions and manage the intellectual property transfer.
There is usually a cash award to the winning solver.
Netflix Prize was a competition to produce a better algorithm to substantially improve the accuracy of predictions about how much a customer is going to enjoy a movie based on their past movie preferences.
The results were measured against the predictions proposed by Cinematch, the algorithm then used by Netflix for customer preference prediction.
In 2009, the $1M Grand Prize was awarded, and the description of the best performing algorithm (if not the source code) was made publicly available.
2.3 Summary of community approaches for verification in the bio-sciences In this section, we summarize five different verification approaches in the bio-sciences, with overlapping objectives but different scientific focus.
A summary of these efforts is listed in Table 1.
CASP (Critical Assessment of protein Structure Prediction) is used to objectively test structure prediction methods against experimentally found structures in a worldwide-community context (Moult et al., 1995; Moult, 1996; Shortle, 1995).
Even though the primary goal of CASP is to advance the methods of predicting protein 3D structure from its amino acid sequence, the pioneering efforts started by CASP have inspired other similar collaboration-by-competition challenges, such as those listed below.
CAPRI (Critical Assessment of PRediction of Interactions) is a community-wide experiment designed on the model of CASP (Wodak and Mendez, 2004).
Both CASP and CAPRI are blind prediction experiments that rely on the willingness of structural biologists to provide unpublished experimental 1195 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1196 11931201 P.Meyer et al.
structures as targets.
CAPRI is a blind test of the ability of proteinprotein docking algorithms to predict the mode of association of two proteins based on their 3D structure.
DREAM (the Dialogue for Reverse Engineering Assessment and Methods) is a community-based effort whose goal is to help improve the state of the art in the experimental design, application and assessment of systems biology models.
DREAM organizers do this through annual reverse-engineering and modeling challenges and conferences (Prill et al., 2010; Stolovitzky et al., 2007; Stolovitzky et al., 2009).
The challenges, based on either new or pre-existing but obfuscated datasets, test participants in biological network inference and model predictions.
Overall, a handful of best-performer teams are identified in each challenge, while some teams make predictions equivalent to random.
As observed in many DREAM challenges, the aggregation of the predictions of all the teams improves the predictive power beyond that of any single method (G.Stolovitzky, personal communication), providing a sort of community wisdom that truly gives meaning to the notion of collaboration by competition.
BioCreAtIve is the Critical Assessment of Information Extraction systems in Biology.
Patterned on CASP, BioCreAtIve is a community-wide project for assessing the application of information retrieval, information extraction and text mining to the biomedical literature.
An example of a BioCreAtIve task is the recognition of gene names in sentences.
Tasks are released biannually, with associated workshops for dissemination of the methods applied to the tasks by the participating researchers.
Results and level of participation in BioCreAtIve I and II are detailed in (Hirschman et al., 2005; Morgan, Lu et al., 2008), where the lessons learned and the remaining opportunities in this important area of systems biology are also discussed.
FlowCAP is a community-based effort to develop new methods for flow cytometry applications.
The motivation for the project comes from the rapid expansion of flow cytometry applications that have outpaced the functionality of traditional analysis tools used to interpret flow cytometry data.
Hence, scientists are faced with the daunting prospect of manually identifying interesting cell populations in 20 dimensional data from a collection of millions of cells.
For this reason a reliable automated approach to flow cytometry analysis is becoming essential.
FlowCAP is a community-based project to assess the interpreting flow cytometry data and automated gating of single-cell multi-variate data compared with gold standards based on manual gating.
2.4 Lessons from community approaches for verification in the biosciences The discussion in the previous section supports the notion that different communities have embraced crowd-sourcing and collaborative-competition as an aid toward science verification and problem solving.
The value of these efforts is well-demonstrated by the level of acceptance by their respective communities.
The main goals of approaches such as CASP or DREAM are, within their respective areas of focus, to determine the state of the art in predictive models, to identify progress over time, to reveal bottlenecks that stymie progress and to show where effort may best be focused.
For all these efforts, clear gold standards and metrics are necessary to quantify and score the entries of the participants.
Three kinds of gold standards are commonly used.
In one case, evoking the classical machine learning paradigm, some of the data is released as a training set whereas the remainder of the data is withheld as a gold standard test set.
The second case consists of using an established method, a technology or a database accepted by the community as a reference.
The third case consists of combining numerous datasets, algorithms or techniques, to get a closer estimate of the ground truth.
A complication is that gold standard datasets are typically hard to obtain, and in many cases, are presently unobtainable in biology.
For example, in protein structure prediction or macromolecular interactions, unpublished experimental structures can be hard to obtain, depending on the willingness of structural biologists to share their pre-publication data.
On the other hand, the complete connectivity of a signaling network in a cell may be unobtainable with todays technology.
Therefore, gold standards for signaling networks are lacking.
There are solutions to this, however, such as requesting participants to train their network models to be consistent with measured levels of phospho-proteins provided in a training set, while testing the resulting models on their ability to predict levels of phospho-proteins under previously unseen perturbations provided in the test set (Prill et al., 2011).
Establishing a performance metric for scoring a challenge is another far-from-trivial task, which is central to challenge design.
There is no unique or perfect scoring metric.
The three main steps involved in evaluation are: (i) identification of a suitable metric (such as the area under the ROC and root mean square between prediction and measurement); (ii) simulation of a null distribution for the chosen metric by evaluation of randomly sampled predictions; and (iii) assignment of a P-value for a prediction with respect to the null distribution for the metric.
The choice of a useful scoring metric involves complexities that may not be as straightforward as ones intuition might suggest.
Consider the case of CASP in which participants predictions are compared with measured 3D structures.
Early experience with matching only-carbon position rather than side chains led to artifacts and over-fitting that were later addressed by more complex metrics than in averaged structure similarities over multiple spatial scales (Ben-David et al., 2009).
The invariance of the metric under different transformations of the data is another issue to take into account when scoring.
For example, when testing a model prediction that spans a large dynamic range (such is the case in phosphoproteomics and gene expression measurements), a root mean square of the differences between predicted and measured variables may depend on the scale of interest.
For example, the sum of differences squared in linear scale could overemphasize the difference over the large scales, whereas the sum of differences squared after log transforming the data amplifies the differences at the smaller values of the predictions.
The results of such different measures of proximity could yield different best performers.
Thus, aggregation of metrics plays an important role to balance the different biases imposed when choosing a metric.
Even in the simple case of binary classification, metrics such as area under the ROC curve, may be misleading if the positive and negative sets are very unbalanced, and it may need to be 1196 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1197 11931201 IMPROVER complemented with the area under the precision recall curve (Davis and Goadrich, 2006; Stolovitzky et al., 2007).
Other typical performance metrics involve the correlation between the predicted values and the gold standard values.
Potential correlation methods include rank correlation, linear correlation, correlation of the log-values and mutual information.
Combined community predictions can yield meta-predictions that are robust and often more accurate than any of the individual predictions.
In CASP, Meta-servers that poll the results of automatic servers are among the best performers.
Similar observations have been made for some of the DREAM challenges (Marbach et al., 2010; Prill et al., 2010).
Lessons from DREAM suggest that in the absence of first principle understanding, algorithms should be simple to avoid over-fitting to a particular dataset.
In general, there is no one-size-fits-all algorithm, as the DREAM results have shown that the best algorithm depends on the subtleties of the data or on the system studied.
For example, gene network reconstruction algorithms that may work very well in prokaryotes do not translate to eukaryotes, and data based on gene deletions have to be treated differently than data based on gene overexpression in network inference tasks.
The community-wide acceptance of these crowd-sourcing methodologies can be thought of in the context of the discussions between verificationists and falsificationists on when a theory is correct or not.
Instead of choosing between validation and refutation the option is finding a practical solution that is accepted by the community.
Of course, this acceptance is not arbitrary as the scientific community is the guardian of rigor and good science.
The community acceptance of the efforts described here gives credibility to the use of the same techniques and challenges to check theories, hypothesis and models.
How we can use this credibility to implement a methodology to verify systems biology results will be discussed next.
3 PROCESS OF VERIFICATION IN INDUSTRIAL RESEARCH 3.1 IMPROVER methodology: research workflow and building blocks Among the lessons that we extracted from the community approaches described in the previous section, the notion that challenges can be used for science verification is paramount.
In this section, we embrace that concept and present a methodology for process verification that can be used in industrial research workflows and other settings.
We call this methodology IMPROVER, for Industrial Methodology for Process Verification in Research.
IMPROVER evaluates the robustness of a research workflow by dividing it into building blocks that are relatively small and verifiable (Meyer et al., 2011).
A building block is the small functional unit of a research pipeline that has a defined input (data, samples or materials), resulting in a defined output (data analyses, samples, high-throughput data or materials).
Functionally, a building block is a discrete research operation at the small end of the scale that is amenable to verification.
Similar divide and conquer approaches are employed in other fields.
Typically, however, building blocks are developed around rigidly defined criteria in which the output is a known function of the input.
In contrast, IMPROVER building blocks need to accommodate a priori A B Fig.1.
Organization of a research workflow by decomposition into building blocks amenable to verification.
(A) Research pipelines are indicated by the gray arrows, whereas the orange blocks are the more specific building blocks necessary to execute the pipeline.Aconcatenation of research pipelines forms a research workflow.
Each of the building blocks in this diagram can be verified by the challenges indicated by the black arrows emerging from the orange blocks.
(B) Example of a research pipeline including the challenges discussed in Section 3.
For the internal challenge example, levels of RNA extracted from tissue or cells are measured with 2 different technologies, one of which is used as reference.
For the external challenge example, gene expression data from patients and control subjects are used to test whether a disease signature can be extracted and verified.
unknown inputoutput functions.
The development of appropriate scoring metrics is a key element to the verification methodology that helps identify the strength or weakness of a building block when a precise knowledge of an inputoutput relationship is not possible.
The verification can be done internally by members of a research group, or externally by crowd-sourcing to an interested community.
IMPROVER is, therefore, a mix of internal/non-public as well as external/public assessment tests or challenges.
The concepts of research workflow and building blocks are clarified in Figure 1.
The chain resulting from linking together the building blocks is a research pipeline (Fig.1A).
The integration of several pipelines forms a research workflow.
Note that there is no unique way of parceling a research pipeline into modules and building blocks.
In general, however, any decomposition will ultimately have some interdependence on natural functional boundaries and the ability to isolate and verify the building block.
In order to be verified, a research building block has to be recast into a challenge (similar to the challenges of the crowd-sourcing efforts discussed in the previous section), that may be assessed internally or broadcasted externally to stakeholders in the interested community.
In both cases, the challenge construction has critical features such as producing the gold standard datasets that will be used as an anchor against which to compare the predictions of a challenge 1197 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1198 11931201 P.Meyer et al.
output, and the scoring methodology to assess the performance of the predictions.
Although IMPROVER has some commonalities with other crowd-sourcing methods, fundamental differences exist.
Here we briefly highlight the differences between DREAM and IMPROVER.
DREAM is a forum to organize the academic systems biology research community around challenges.
These challenges are chosen by the DREAM organizers in collaboration with the community and are mostly structured to tackle independent problems in systems biology, with no specific link between challenges.
DREAM challenges are widely advertised to the community, and its results are publicly announced.
Conversely, IMPROVER challenges are designed following the interests of a research organization.
These challenges, in turn, are designed to verify building blocks that work synergistically in a research workflow.
Challenges performed to verify these building blocks can help the organization determine a way forward with respect to a previously laid plan: if the task that a building block was supposed to perform at a given level of accuracy is not verified, then the building block has to be modified.
If a building block is verified, then its outcomes can be trusted with a higher degree of confidence.
Examples of building block tasks and possible challenges to verify them are shown in Figure 1B.
IMPROVER can pose its challenges internally, that is within the organization, or externally, to a wide community.
3.2 Internal challenges An organization will use internal assessment challenges to verify in-house data generation, analysis and interpretation, either because of proprietary concerns or because the scope does not require a community effort.
An IMPROVER challenge internal to an organization could help researchers identify building blocks that need either improvement or replacement with a new technology.
As it will be described for external challenges, internal assessment challenges should be scored by an objective third party, who will not participate in the challenge but that could be from another group within the same company or institution.
An internal challenge could be designed to evaluate the quality of data used for an external challenge.
While data production can be ensured by Good Laboratory Practices (OECD 1998), the robustness of the technology used to collect the data may evolve in time, and therefore the quality of the data collection process itself may need to be verified (exemplified by the Noise Level in Gene Expression Data challenge in Fig.1B).
Consider that an organization must decide if the output data from the Gene Titan System for gene expression profiling fromAffymetrix is of sufficient quality to consider its adoption.
This technology allows researchers to process hundreds of samples in one experiment with minimal hands-on time, thus considerably increasing gene expression profiling throughput.
An internal challenge is then constructed to compare the Gene Titan platform with the more established standard using Affymetrix single cartridge technology.
A first verification challenge could consist of profiling a gold standard mRNA references sample, containing known quantities of spiked RNA.
These reference samples, when hybridized on both technology arrays, would allow for the comparison of the sensitivities and error levels of both technologies.
What is essential here is that the assessment be done by an objective third party who knows the composition of the reference sample that is unknown to the experimenter.
In general, the IMPROVER internal challenge contribution to a research workflow will result in an understanding of the limitations of the methodology used in a pipeline.
This understanding could be used to improve the results expected from a building block, thus increasing the robustness and value for the larger research pipeline.
3.3 External challenges/the first IMPROVER challenges An external challenge can be designed to achieve multiple goals when aimed at verifying a building block within a pipeline.
First, a public challenge invites novel approaches to a problem, not considered by the internal researchers.
Second, a blended prediction aggregated from the entire community of predictions is often more accurate than any individual prediction (G.Stolovitzky, personal communication).
Third, the public discourse centered on a challenge, including conference presentations and papers on the best-performing methods, can rapidly build a consensus in the community as to which approaches are the most fruitful for a given task.
Fourth, if despite wide participation, no single team manages to achieve a good performance at solving the challenge, then the building block can be considered as non-verified, increasing the risk of failure of that building-blocks pipeline.
Wide participation by the community is particularly important.
While financial incentives are only one approach to increase participation, other incentives could be just as attractive, including the opportunity to verify the algorithm predictions against newly collected experiments, bragging rights for the best algorithm, the ability to publish and to drive the field for purely academic interests.
We illustrate the concept of an IMPROVER external challenge using as an example the search for robust signatures to perform diagnosis of diseases based on commonly available transcriptomics data.
There are examples of gene expressions signature in use today, such as Oncotype DX and MammaPrint, two FDA approved tests that provide prognostic value and can guide treatment in subsets of breast cancer patients (Paik et al., 2004; van de Vijver et al., 2002).
While diagnostic signatures exist in limited cases, the wide availability of high-throughput transcriptomics data makes plausible the discovery of diagnostic signatures for a multitude of diseases.
The community has recognized the need for robust genomic and gene expression signatures as important enablers for personalized medicine, as patients could directly benefit from treatments tailored to the individual (Subramanian and Simon, 2010).
While there has been a clear need for diagnostic signatures, efforts to discover such signatures in commonly available transcriptomics data have generally fallen short of expectation.
There are many reports in the literature in which the lists of differentially expressed genes purported to distinguish between two biological conditions showed little overlap when the data were taken from different cohorts or when experiments were performed in different laboratories with different platforms (Ioannidis, 2005).
Hence, the discovered signatures do not generalize and perform poorly when classifying datasets other than the ones used to develop the methods.
Even with good control over data collection and patient selection, signature discovery can be inhibited by inherent variability in gene expression.
One proposed method to discover robust classifiers in spite of inherent variability is to separate driver genes from the passenger genes (Lim et al., 2009).
The driver genes (sometimes 1198 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1199 11931201 IMPROVER referred to as master regulators) are upstream controllers that are proposed to be better indicators of disease state than the downstream regulated genes that can show more inherent variability.
The first set of IMPROVER challenges, termed the Diagnostics Signature Challenge, addresses the problem of diagnostics from transcriptional data in a biomedical context.
(This challenge is being organized at the time of this writing.)
The need to find biomarkers that stratify a population into segments characterized by a given phenotype is felt not just in biomedicine but also in other contexts such as the pharmaceutical industry, where a similar IMPROVER challenge could be deployed.
We consider four prevalent diseases: multiple sclerosis (MS), psoriasis, lung cancer and chronic obstructive pulmonary disease.
The building block that this challenge is designed to verify is Find Gene Expression Signature (Fig.1B).
In other words, what needs to be verified is the hypothesis that transcriptomics data contains enough information for the determination of these human disease states.
In a context such as the pharmaceutical industry, a test of validity of the notion of transcriptomics-based signatures would be a pre-requisite to attain the research pipeline goal of finding a product (such as a drug) tailored for each individual (Fig.1B).
We will now describe the operational steps for the Diagnostic Signature Challenge taking out of the four diseases, MS as an example.
MS is an inflammatory disease, believed to be an autoimmune disease that affects the central nervous system.
The trigger of the autoimmune process in MS is unknown, but it is believed that MS occurs as a result of some combination of genetic, environmental and infectious factors (Compston and Coles, 2008), and possibly other factors such as vascular problems (Minagar et al., 2006).
The symptoms of the disease result from inflammation, swelling and lesions on the myelin and in 85% of patients start with a relapse-remitting stage of MS (RRMS).
Finding a robust genetic signature would be of great importance, as diagnosis by a neurologist usually involves ruling out other nervous system disorders with invasive and expensive tests (NINDS Multiple Sclerosis Information Page, http://www.ninds.nih.gov/disorders/ multiple_sclerosis/multiple_sclerosis.htm) and recently drugs can delay the progression of MS when RRMS, is diagnosed early on (Rudick et al., 2006).
IMPROVER organizers will procure from the public literature, a training set of gene expression data from peripheral blood mononuclear cells (PBMCs) corresponding to MS and healthy patients (Fig.2).
In this challenge, the test set corresponds to an unpublished cohort of 129 samples whose labels will be hidden from the participants.
This set of samples obtained from patients that were determined as healthy or RRMS by a physician will constitute the gold standard.
A wealth of additional useful gene expression data is also available through databases such as the Gene Expression Omnibus or ArrayExpress.
Participants can use the training set, open literature information and any other publicly available data.
With this data at hand, participants will generate the transcriptomics-based molecular signature that can differentiate between healthy and RRMS patients.
Participants will be asked to submit for each sample a confidence of the prediction to belong to the RRMS class.
The confidence of the classification should have a value between 1 and 0, 1 being the most confident and 0 the least confident.
After predictions from participants are collected via website submissions, the results will be scored using metrics such as the Area Under the Precision versus Recall (AUPR) curve.
Precision Fig.2.
Schematic diagram of MS Disease signature challenge organization.
A dataset with both gene expression and corresponding clinical diagnoses or prognosis forms the basis of the challenge.
The test data contains the gene expression data generated only and is transmitted to the participants via a web portal.
There are three participants shown, the actual challenges could involve many more.
The participants generate predictions-based gene signatures that are submitted back via the website.
A trusted party will blindly score and rank the prediction by comparing to the gold standard dataset that contains both the gene expression data and actual clinical outcomes.
is defined as the fraction of correct positive set predictions, and recall is the proportion of correct positive set predictions out of all patients in the positive set.
Other metrics for binary classification assessment will also be evaluated.
Teams will be ranked according to their overall performance based on those metrics.
Figure 2 illustrates how the MS disease signature challenge will be organized in order to verify through the IMPROVER methodology whether a robust MS gene signature can be found.
A diagnostic signature for those phenotypes can be accepted as existing, and the building block Find a Transcriptomics-based signature for control versus RRMS verified, only if there is at least one participating team who classified in the correct class a statistically significant number of subjects.
A subsequent verification of the molecular signature discovered by the best performer could be further tested by evaluating its performance in a similar, but biologically independent dataset.
Finally, if no team managed to distinguish the RRMS patients from healthy donors from PBMC transcriptomics data, then we can assert that the building block failed verification, and an alternative way of classification should be explored.
If the building block was verified, an obvious by-product of the challenge is the identification of the best diagnostic signature and the corresponding discovery algorithm for each of the diseases.
Other expected advantageous outcome of the IMPROVER challenge is that it enables a fair comparison of competing methods, as the IMPROVER format requires blind prediction by the participants and blind scoring of the submissions (Fig.2).
This approach will alleviate many of the problems that produce overestimation of results when the authors of an algorithm compare their own method with other existing methods (Norel et al., 2011).
For example, over-fitting and information leakage between training and test datasets are two common pitfalls that can be avoided.
A final advantage of the methodology is that it allows for an assessment of the performance of submissions across both participants and diseases.
This will provide an unparalleled opportunity to assess whether the diagnostic signature discovery approaches can be applied across 1199 Copyedited by: TRJ MANUSCRIPT CATEGORY: REVIEW [12:41 9/4/2012 Bioinformatics-bts116.tex] Page: 1200 11931201 P.Meyer et al.
different diseases.
Such a controlled assessment is harder to reach with traditional scientific approaches, as it requires a wide variety of participants using different methodologies on the same data and scored under the same metrics.
3.4 Gold standard and metrics of performance A foremost concern in designing a challenge for IMPROVER is to obtain a gold standard dataset against which a set of predictions can be scored in order to verify a building block.
While designing a challenge to verify a building block, the possibility exists that a gold standard cannot be defined or is considered suboptimal as an adequate database, unpublished good quality data or an accessible expert in the field is unavailable.
In this case, the rationale behind the challenge has to be altered and the challenge must be redesigned before the building block can be verified.
Redesigning a challenge can be laborious as it might imply obtaining data for a new gold standard and change assumptions that simplified the underlying biology and favored a good challenge formulation.
A building block can be considered as verified if the predictions made within the challenge are close enough to the known gold standard.
For each challenge, a quantitative metric of performance must be defined.
Like the gold standard, the performance metric is central and should be an integral part of the challenge formulation.
This performance metric can also be used to assign a risk that the verification was a fluke (e.g.
computing a P-value).
It is also possible that a challenge results in lack of verification: none of the participating teams could find an acceptable solution to the problem.
There is generally no a priori reason why one metric should be better than the others.
As a rule of thumb, aggregating the several metrics into one overall metric may have advantages and provide less arbitrary performance metric.
In other cases, however, the nature of the problem guides the choice of metric.
For example, the large dynamic range of gene expression data suggest a performance metric in which the values are represented in logarithmic scale.
4 CONCLUSION AND FUTURE DIRECTIONS The great opportunities made possible by the emergence of high-throughput data in all realms of science and technology have also resulted in the problem of extracting knowledge from these massive datasets.
The proliferation of algorithms to analyze this data creates the conundrum of choosing the best algorithms among the multiple existing ones.
Crowd-sourcing efforts that take advantage of new trends in social networking have flourished.
These initiatives, summarized in Section 2, match discipline-specific problems with problem solvers, who are motivated by different incentives to compete and show that their solution is the best.
In this way, the best method available to solve a given problem can be found in an unbiased context.
Interestingly, these crowd-sourcing methodologies also have an epistemological value, shedding light to the question of when a theory is correct or not.
Instead of tasking a researcher to self-assess (a process suspect of biases) the truth of a model or methodology, the alternative is finding how it fares in an unbiased and rigorous test.
The community acceptance of the efforts described in the first part of this article gives some credibility to the use of similar approaches to verify the sometime elusive results attained in systems biology research.
Extrapolating the idea of using challenges for verification of scientific results, we propose the IMPROVER methodology to assess the performance of a research workflow in contexts such as industrial research.
A main concept in IMPROVER is the formalization of a process to determine a go or no-go decision for the research pipeline in an industrial context (internal and external challenges), as well as better methods inspired by the community participation (external challenges).
If the results are positive, that is, if the pipeline passes all the challenges and there is active community participation, then the credibility of the data, analysis and of the subsequent results would be enhanced in the eyes of the scientific community and regulatory agencies.
The challenge-based approach creates a metric for comparison between possible solutions to a challenge designed to verify a building block.
Superior performance by one methodology could promote acceptance by the community of the best performer methodology as a reference standard.
IMPROVER could offer a complement and enhancement to the peer-review process in which the results of a submitted paper are measured against benchmarks in a double-blind challenge, a process that can well be called challenge-assisted peer-review.
The IMPROVER approach could be applied to a variety of fields where the outputs of a research project are fed into the input of other projects, such as is the case in industrial research and development, and where the verification of the individual projects or building blocks is elusive, as is the case in systems biology.
ACKNOWLEDGEMENTS We thank Robert J. Prill and Alberto de la Fuente for useful discussions and Hugh Browne and Jennifer Galitz McTighe for a careful reading of the manuscript.
Funding: IBM and PMI authors performed this work under a joint research collaboration funded by PMI.
Conflict of Interest: none declared.
ABSTRACT Motivation: Copy number variations (CNVs) are a major source of genomic variability and are especially significant in cancer.
Until re-cently microarray technologies have been used to characterize CNVs in genomes.
However, advances in next-generation sequencing tech-nology offer significant opportunities to deduce copy number directly from genome sequencing data.
Unfortunately cancer genomes differ from normal genomes in several aspects that make them far less amenable to copy number detection.
For example, cancer genomes are often aneuploid and an admixture of diploid/non-tumor cell frac-tions.
Also patient-derived xenograft models can be laden with mouse contamination that strongly affects accurate assignment of copy number.
Hence, there is a need to develop analytical tools that can take into account cancer-specific parameters for detecting CNVs directly from genome sequencing data.
Results: We have developed WaveCNV, a software package to iden-tify copy number alterations by detecting breakpoints of CNVs using translation-invariant discrete wavelet transforms and assign digitized copy numbers to each event using next-generation sequencing data.
We also assign alleles specifying the chromosomal ratio following duplication/loss.
We verified copy number calls using both microarray (correlation coefficient 0.97) and quantitative polymerase chain reaction (correlation coefficient 0.94) and found them to be highly concordant.
We demonstrate its utility in pancreatic primary and xenograft sequencing data.
Availability and implementation: Source code and executables are available at https://github.com/WaveCNV.
The segmentation algorithm is implemented in MATLAB, and copy number assignment is implemented Perl.
Contact: lakshmi.muthuswamy@gmail.com Supplementary information: Supplementary data are available at Bioinformatics online.
Received on June 24, 2013; revised on October 1, 2013; accepted on October 21, 2013 1 INTRODUCTION DNA copy number variations (CNVs) are associated with a wide range of diseases including cancer where detection of copy number alterations has led to guided-therapeutic interventions.
For example, amplification of the ERBB2 locus is used to iden-tify patients for trastuzumab treatment.
Although Comparative Genome Hybridization (CGH), microarrays have an intrinsic kilobase (kb) resolution for CNV detection, the advent of high-throughput next-generation sequencing (NGS) technologies offers us the potential to probe genomic structural variation at base-pair level.
However, with the increase in signal resolution comes a substantially increased noise signature and the problem of how to remove false positives.
Recent efforts by various groups (Abyzov et al., 2011; Ivakhno et al., 2010; Kim et al., 2010; Klambauer et al., 2012; Magi et al., 2011; Medvedev et al., 2009; Miller et al., 2011; Waszak et al., 2010; Xie and Tammi, 2009; Yoon et al., 2009) have attempted to mitigate the noise by carrying out a smoothing (binning) of the sequencing read depth on scales of tens to hundreds of base pairs and examining this smoothed read depth.
The smoothing process is performed on a set, arbitrary, scale, which can smooth-out physically interesting features of a signal.
This is of significant concern for cancer genomes, which are known to have unstable genomes that con-stantly evolve.
Smoothing methods also assume that the noise signature of the signal is overwhelmingly concentrated on a single base-pair genomic scale (high frequency) and ignores the possibility of strong long-range (low-frequency), systemic, corre-lated noise that may increase the false-positive rate of any detec-tion algorithm.
Another recent effort, Varbin (Baslan et al., 2012) uses a variable binning approach to take into account an uneven distribution of mappable reads.
Although this method is suitable for low or sparse coverage as illustrated in single cell sequencing (Navin et al., 2011), it does not fully harness the available basepair-scale genomic resolution.
Assignment of digitized copy number to genomic segments in tumors is further complicated in cancer genomes due to a number of sample-specific confounding factors.
For example, primary tumor tissues may contain low tumor cellularity due to an admixture of diploid/non-tumor cell fraction in patient samples, including pancreatic cancer where tumor cellularity can vary from 5 to 80%, thus making the detection of cancer driver mutations difficult (Biankin et al., 2012).
In addition to primary tumors, patient-derived samples grown in mouse *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
zPresent address: Icahn Institute for Genomics and Multiscale Biology, Graduate School of Biological Sciences, Icahn School of Medicine at Mount Sinai, New York, NY 10029-6574.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com https://github.com/WaveCNV mailto:lakshmi.muthuswamy@gmail.comxenograft (PDX) models are being increasingly used in pre-clin-ical settings to understand tumor biology and therapy response (Huynh et al., 2011; Morton and Houghton, 2007).
Assignment of digitized copy number to CNVs in these models becomes in-creasingly difficult due to mouse contamination of the tumor samples that introduces noise in the sequencing coverage as well as allele frequencies for SNVs (both of which are integral to CNV calling methods).
Although algorithms such as qpure (Song et al., 2012), genoCN (Sun et al., 2009), ASCAT (Van Loo et al., 2010) and ABSOLUTE (Carter et al., 2012) model for stromal contamination and ploidy estimation on SNP array data, they do not function for genome sequencing data.
Also these methods cannot correct for the additional effects of xeno-graft mouse contamination.
To fill this need, we have developed WaveCNV, a tool that uses DNA sequencing data to model for complex cancer genomes.
The algorithm estimates ploidy, tumor cellularity in primary tumors, mouse content in xenograft models and assigns digitized copy numbers and alleles to indicate which parental chromosome pair was affected by each copy number event.
Also to overcome limitations associated with binning-based approaches, we use the well-established theory of wavelets to take full advantage of the genomic resolution available in sequencing data.
Figure 1 illustrates the overall flowchart of data generation and copy number modeling.
2 METHODS 2.1 Segmentation algorithm Wavelet theory is used both for denoising of the depth of coverage in NGS data (which is inherently multiscale and carries non-uniform cover-age signal) and to identify rapid transitions corresponding to CNV break-points.
The wavelet transform (Mallat, 2008) breaks a given signal into different frequency components with a resolution matched to its intrinsic scale and can thus claim fundamental advantages over traditional Fourier methods in detecting sharp localized discontinuities as observed in copy number alterations.
This specific property of wavelet transform is crucial in analyzing signals, specifically NGS coverage data, where size of copy number alterations can vary from base pair to length of a chromosomal arm.
We give a brief description here, while the mathematical details are provided in Supplementary Materials S.1 and S.2.
We first select the wavelet basis function by using the inherent nature of copy number alterations that a genomic region with a read depth is likely to make digitized step transitions and hence choose the simplest of all wavelets, a step function or the Haar wavelet.
Given our choice of the Haar basis, we use a translation-invariant discrete wavelet transformation on the normalized read depth (Coifman and Donoho, 1995) to obtain detailed signal frequency and scale informationencapsulated by the approximation and detail coefficients.
The approximation coefficients will contain both the low-frequency component (feature sizes of the order of a few kilobases) and a high-frequency component unique to sequencing data (feature sizes less than a kilobase).
The detail coefficients will contain an exclusively high-frequency component, which is more likely to have significant noise but also possibly important small-scale insertions and deletions.
We scan across scales of interest by successively iterating the decomposition of signal , with successive approximation coefficients being decomposed in turn.
This results in the signal being broken down into many lower genomic-resolution components starting from a small scale.
We then use de-noised approximation coefficients to define boundaries where there is a transition from one copy number state to another.
Detection of breakpoints is achieved by asking when the coefficients of the maximal scale intersect those of the finest scale as given in Equation (1).
For reasons of economy and because the CNV distribution is largely unknown, we examine the intersections between the approximation coef-ficients at entropy scale (aL*) and the partial autocorrelation scale (aP) (Supplementary Material S.2).
a0Lp a0L sgn 1 2Lp  ap 1 2L  aL  1 The main point in the approach is that by examining the zero crossings of this special function in Equation (1), we should have an extremely low false-negative rate owing to the inherent sensitivity of the Haar wavelets to abrupt changes in the signal at this wide range of scales.
One can show that this procedure is equivalent to searching for local maxima of the squared modulus of the dominant wavelet coefficients in the signal (Legarreta et al., 2005).
Figure 2 illustrates clearly that the major features of the signal discontinuities are captured by the wavelet transformed and de-noised signal, but with subtle differences in that the detail coeffi-cients are extremely sensitive to steep-gradient features and miss gradual read depth changes that are instead captured by the approximation coefficients.
2.2 Allele-specific copy number estimation After the breakpoints are detected using our segmentation algorithm, we assign digital copy numbers to each segment.
Our basic method is similar to copy number models applied to microarray data (Sun et al., 2009; Van Loo et al., 2010; Wang et al., 2007) with additional layers of complexity added to the model due to tumor-specific confounding factors (Supplementary Materials S.5S.11 and methods below).
We use sequen-cing coverage modeled as a Poisson distribution and minor allele frequency (MAF) modeled as a binomial distribution to assign digitized copy numbers to each CNV event.
We also assign alleles to each copy number event describing the parental chromosome ratio following each duplication or loss.
For example, a three-copy region might have an allele of 1:2 (one copy from the first parental chromosome and two copies from the other parental chromosome), whereas allele 0:3 would also be possible (three copies of one parental chromosome and complete loss of the Fig.1.
Flow chart showing the analysis procedure 769 WaveCNV While , , tilize methods next-generation sequencing-, next-generation sequencingas well as very tilizeas well as M A F ile other).
Alleles are assigned based on MAF distribution within the CNV event, which will be specific to chromosomal balance (e.g.
a 1:2 allele would produce MAF distribution peaks at 0.33 for SNVs on one chromosome and 0.66 for SNVs on the other chromosome).
Allelic assignment is possible in cancer because somatic duplication/loss events are recent, so linkage among SNVs is not expected to break down as it does in germline CNVs.
The allele assignments in WaveCNV can be used to associate CNVs with SNVs/indels that appear to be preferentially gained or lost.
In addition to modeling for basic coverage and MAF, we also model for aneuploidy, normal/diploid contamination of primary tumor samples, mouse contamination of human tumors grown in xenograft and we per-form auto-correction of systematic sequencing biases using matched normal/control samples.
For validation purposes, we used WaveCNV to identify CNV events in human pancreatic cancer samples.
Sequencing data were aligned using Novoalign (Novovcraft, Inc.) and processed using Genome Analysis Tool Kit to identify SNVs and minor allele frequencies (see Fig.1 and Supplementary Material S.3 for data generation and S.4 for data pre-processing).
2.3 Estimation of minimum detectable CNV length Given that coverage is modeled as a Poisson distribution, the variance for the median coverage can be approximated after adapting Raikovs the-orem using the equation: V ce=n0 2 where ce is the expected segment median coverage and n is the number of independent data points in the region (See Supplementary Material S.5).
Variance is thus a function of both coverage and segment length, and a relationship can be derived to identify the minimum segment length required to identify a copy number event to a specified confidence thresh-old (See Supplementary Materials S.5 and S.7).
The length of all segments must then satisfy the following relationship to be detectable: n04 ci 2 d 0:5 2 3 where ci is the average expected median coverage on the region of interest, d is the difference in coverage from the neighboring segment and is a selected threshold factor (3.890592 for 0.01%).
This relationship specifies that events become detectable with either deeper coverage or longer segments, and low copy events are more easily distinguished than high copy events.
Such information is invaluable because it allows us to de-termine the minimum sequencing coverage required before even begin-ning an experiment.
This can be especially useful when sequencing tumor samples with diploid/normal fraction contamination that dilutes apparent separation between copy number levels.
For example, the smallest events that could be identified in a primary tumor sample sequenced with 101 base pair reads and having a cellularity of 0.20 would be 7kb in length at 30 coverage and 2kb at 100 coverage.
We also use this relation-ship to simplify our calling algorithm and improve run times by merging short segments before calculating fits to each copy number model.
2.4 Estimation of mouse contamination in xenograft models Human derived tumors are commonly grown as xenografts in mice to facilitate continued study of the tumors biology or increase total tumor content of low cellularity tumor types.
When using these xenografted samples with NGS, mouse DNA contamination of the human-derived tumors can introduce confounding factors into both coverage and MAF, which can falsely alter the apparent copy number.
The overall effect of this contamination becomes more extreme as the mouse content of the sample increases.
One approach to removing mouse contamination used by tools like Xenome (Conway et al., 2012) is to try and directly identify non-human sequencing reads and remove them upstream of any data processing.
However, there are many conserved regions of high sequence identity between human and mouse for which sequencing reads cannot be separated in this way.
Unfortunately these regions of conservation are primarily concentrated in gene coding regions (which are of main interest in cancer analysis).
We thus take another approach that could be used in complement with tools like Xenome.
We adjust expected coverage higher and shift expected MAF values based on the estimated amount of mouse contamination in the region (The mouse is assumed to come from an inbred line, so it will be diploid and homozygous for most mouse-specific SNVs).
Figure 3A and B show a two copy 30-megabase region observed in chromosome 1 of a human pancreatic cancer cell line that is expected to be free of mouse DNA contamination.
The observed MAF distribution for the cell line (Fig.3B red line) is centered around the MAF value of 0.5 and closely matches the calculated expected MAF distribution (Fig.3B blue line).
For the exact same two copy 30-megabase region from the same human tumor sample grown in xenograft, the observed distribution Fig.2.
Detection of signal discontinuities using wavelet transformed and de-noised signal over a 16kb region.
Top panel shows the raw read depth (gray) and the denoised signal (red).
Bottom panel illustrates copy number break points where the coefficient of the maximal scale intersects those of the finest scale.
The y-axis is the squared approximation wavelet coefficient, and x-axis is the genomic position in megabases 770 C.Holt et al.
for example, , as (GATK) M Skilobases x kilobases x next-generation sequencing m 3 which of the MAF (Fig.3D red line) is centered at 0.47, below the expected value of 0.5 (Fig.3D blue line).
There is also an observable band of data introduced by the mouse contamination around 0.16 (Fig.3C and D blue arrows).
Owing to the multi-modality of the MAF distribution, the ob-servation deviates significantly from the expected distribution curve.
In our CNV calling algorithm, we adjust the expected MAF frequen-cies to take confounding factors caused by the aligning mouse reads into account by adding an independent distribution peak for mouse-derived SNVs as well as modeling for the degree that MAF peaks will be shifted by mouse reads (mouse-derived SNVs will be two copy homozygous for inbred lines).
Our improved expected distribution seen in Figure 3D (green line), clearly matches the observed distribution (red line) better than the standard expect (blue line).
We also alter expected coverage for the segments by estimating the quantity of mouse reads that will align (these values are fixed into WaveCNV, but can also be supplied as a BAM file if mouse was sequenced independently).
Based on kernel density estimation of mouse-expected coverage, the average mouse contamination of this particular xenograft was 21% of the total DNA content of the sample.
We validated the estimated mouse contamination using qPCR.
Two target loci were chosen such that one of them maps uniquely to human and another to the mouse genome.
The values from TaqMan qPCR analysis were used to calcu-late the relative absolute quantity between human and mouse probes, which demonstrated a 27% mouse contamination in the pancreatic xeno-graft compared with the tumor cell line derived from the same tumor.
Thus these two alternate approaches ascertain the estimation of mouse contamination using our model within an acceptable margin of error.
2.5 Estimation of cellularity in primary tumors Normal/diploid cell contamination of primary tumors complicates CNV calling by diluting signal from the tumor cells and reducing the amount of observed coverage separating copy number levels as well as altering the expected minor allele frequencies at each copy number level.
Corrections for shift in coverage andMAF can be obtained if you know the cellularity of a sample.
Previously qpure (Song et al., 2012) has attempted to esti-mate cellularity using a relationship for the shift in MAF in the single outermost peak of loss of heterozygosity (LOH) events.
Notably, how-ever, they found that the relationship they use does not hold linear for values520% cellularity.
We followed an approach similar to theirs by using the shift in MAF for LOH events to estimate cellularity; however, we make use of LOH events at multiple copy number levels and derived a relationship that does hold linear even at low cellularity: 1 MLOH T 1 T  N 2 4 where T is the tumor cellularity, N is the copy number of the region, and MLOH is the left-most central MAF peak for the region at copy number N. The slope of the relationship is therefore a function of the cellularity T. Also because the y intercept of the relationship is always fixed at 2 (reciprocal of MAF 0.5), we can fit N to the proper copy number for complex aneuploidy events.
Supplementary Figure S4, panel A clearly shows the outer most MAF peaks for copy numbers 13 of a patient-derived pancreatic cancer pri-mary tumor sample.
As shown in Supplementary Figure S4, panel B, when the MAF values from LOH peaks are used with Equation (4), the slope allows us to derive the cellularity of the sample.
The resulting slope 0.611 (R2 0.99876) corresponds to a cellularity of 0.38 for this tumor sample.
We further validated our model using a dilution series of pancreatic tumor cells derived from a primary tumor cell line mixed with increasing quantities of diploid cells derived from matched normal.
Table 1 shows a convincing validation of tumor content estimation for these samples ran-ging from 5 to 100% cellularity.
Estimates match well with expected values even for low cellularities, demonstrating the effectiveness of our method.
Identifying genomic mutational landscape has been difficult in tumor genomes where tumor content is520%.
However, using sequencing data, it may now be possible to use low cellularity tumors to detect mutational landscape if coverage is sufficient [overall coverage determines the min-imum length of detectable copy number events according to Equation (3) earlier mentioned in the text].
2.6 Estimation of ploidy One of the most difficult aspects of assigning digital copy number values to a sample is in determining what the expected coverage or copy neutral coverage would be.
Many algorithms assume that the majority of a sample is diploid and any gains and losses are determined based on normalizing the coverage of each chromosome using this assumption.
Fig.3.
MAF distribution of SNVs in a 30Mb region of chr1.
(A) MAF density in a pancreatic cancer cell line; (B) observed (red) and normal fitted expect (blue) distribution curves of MAF for pancreatic cancer cell line; (C) MAF density in a pancreatic xenograft model; (D) observed (red), normal fitted expect (blue) and expect with mouse contamination (green) for pancreatic xenograft model Table 1.
Experimental validation of cellularity estimates Mixed tumor fraction WaveCNV estimate 0.05 0.043 0.10 0.088 0.15 0.155 0.20 0.236 0.40 0.403 0.60 0.602 1.00 1.00 Note: The table shows WaveCNV-derived cellularity estimates for a dilution series of diploid/normal contamination mixed into a pancreatic cancer cell line model.
771 WaveCNV minor allele frequency 3 Due to i below sincevery &percnt; very lower than tilize ( above) .
This becomes problematic especially for tumor samples where the major-ity of the genome is often not expected to be diploid.
We have developed a procedure for identifying the base coverage cor-responding to a one-copy shift that can be used to determine the ploidy of a given sample.
Because multiples of the optimal value for the base cover-age should correlate with the observed coverage for all segments of the genome, we perform an iterative search for a value that generates a genome-wide maximum coverage likelihood while simultaneously gener-ating the best fit to an MAF as measured by residual sum of squares (rss).
This conveniently happens at the point of maximum separation between normalized curves of coverage likelihood and rss.
The overall procedure for selecting a base coverage is further detailed in the Supplementary Material S.10.
We validated our procedure using a triploid pancreatic tumor sample and its diploid matched control/normal.
The expected coverage median for the diploid matched normal genome was determined to be 38 using Gaussian kernel density estimation (Fig.4A).
A search through the base coverage candidate space (Fig.4B) using normalized coverage likelihood (red line) and normalized rss fit for MAF (blue line) reveals that max-imum separation (yellow line) occurs at coverage 19.73.
Given that the kernel-derived genome median coverage is 38, a base coverage of 19.73 would give a correct ploidy estimate of two for the genome.
When the same procedure is applied to the triploid tumor sample (Fig.4C and D) the base coverage is calculated to be 9.84 and the expected median cover-age is 28, giving a correct ploidy estimate of three for the sample.
2.7 Matched normal corrects for coverage bias and germline events Because WaveCNV is a somatic CNV caller, as CNVs are assigned to the tumor sample it can simultaneously assign copy numbers to the same segment in the diploid matched control (sequenced together with the tumor).
This allows WaveCNV to determine if given losses, gains and LOH events are in fact somatic or germline events.
Furthermore, the matched normal also allows WaveCNV to correct for anomalies present in the reference sequence including systematic variance in coverage, high repeat regions, unsequencable regions with consistently missing coverage and so forth.
Supplementary Figure S5 clearly demonstrates the decrease in variance for genomic coverage when a matched control based correc-tion is applied (blue line) as opposed to the standard coverage distribu-tion (red line).
Final somatic calls are highlighted in the output report to distinguish them from other copy number calls, thus allowing researchers to immediately focus on the events most likely to be important to tumor progression.
Further details for matched normal/control-based correction are found in the Supplementary Materials S.11.
3 RESULTS We identified 764 somatic copy number aberrations in pancreatic cancer genome sequencing data using WaveCNV.
The size of CNV events varied from 284bp to 33Mb.
Supplementary Table S4 lists these events along with their verification status using alternate platforms, and Supplementary Figure S7 illus-trates the size distribution of those events.
3.1 Ascertainment of somatic CNVs using microarray and qPCR technologies CNVs identified using WaveCNV were verified using three alter-nate technologies: Nimblegen, 2.1 million CGH tiling array, Illumina 1 million Omni-quad SNP array and verification of 80 CNV loci with copy number varying from 0 to 30 using qPCR.
We find a high correlation between CN estimated from qPCR method and WaveCNV as shown in Figure 5A.
We fit linear regression and found that regression coefficient (0.94) with P52e-16.
With the regression coefficient close to 1, it confirms that our CN model used in WaveCNV algorithm is able to pre-dict accurately a wide range of copy numbers.
We also compared CNVs from the whole genome with two different array-based platforms, Illumina Omni 1M quad and Nimblegen 2.1M array CGH.
Invariably, most of the array platforms have lower dynamic range compared with sequencing that results in approximate digitized CN.
Hence we compared CN from sequencing to the median intensity signal of the probes covering the region from array platforms as shown in Figure 5B and C. We find a high concordance between array platforms and WaveCNV.
The weighted Pearson correlation coefficients are calculated to be 0.86 for Illumina array and 0.97 for Nimblegen array with weights proportional to the length of the segment.
Fig.4.
Modeling for aneuploidy.
(A) The expected segment median coverage for a diploid genome is estimated using kernel density estima-tion.
This value then serves to define a range for estimating the sample base coverage (coverage of copy number 1).
(B) The normalized likeli-hood of the observed coverage (red line) as well as the normalized residual sum of squares value (rss) for all MAF distribution fits (blue line) are calculated for each candidate base coverage (assuming ploidy range 14).
The base coverage that produces the maximum separation between likelihood and rss (yellow line) is then selected.
(C and D) show the expected segment median coverage and the base coverage selected for a triploid genome 772 C.Holt et al.
3.2 Algorithm performance comparison We additionally compared our results to the sequencing-based CNV calling algorithms CNVnator (Abyzov et al., 2011) and OncoSNP-SEQ (Yau, 2013).
We used base pair level congruency between algorithm calls to compare matches.
We define congru-ency to be the average of sensitivity (the fraction of a reference feature predicted) and specificity (the fraction of a prediction overlapping a reference feature).
In all cases the reference is the algorithm we are comparing with.
Table 2 shows the comparative statistics between the three algorithms.
Comparing copy number events observed in WaveCNV with CNVnator, we observed an overall congruency of 93% (95% in gains and 92% in losses).
When comparing WaveCNV to OncoSNP-SEQ (a cancer-specific CNV caller), we see an overall congruency of 80% (87% for amplifications and 80% for deletions).
The lower match for OncoSNP-SEQ is primarily due to our sample coverage being lower than that rec-ommended for accurate OncoSNP-SEQ performance (our sample was sequenced to 30, whereas OncoSNP-SEQ requires a minimum of 60).
Our concordance with CNVnator is one of the highest reported so far between any two programs for sequencing data thereby supporting the validity of our algorithm.
There are key additional features that are unique to our algorithm, which are critical for cancer genomes.
WaveCNV successfully combines the read depth distribution, MAF and reference-based normalization of tumor with matched normal to estimate ploidy of the genome and corrects for mouse contamination with the additional bene-fits of copy number allele assignments and LOH detection.
We have a well-defined mechanism to control for detectable event sizes at different levels of sequencing coverage and tumor sample cellularity.
Also although WaveCNV can assign copy numbers to any segment within the genome, the primary focus of cancer research is on the somatic changes and somatic CNVs are identified in our output by integrating matched normal/ controls into the copy number analysis.
4 DISCUSSION We have developed a computational algorithm to detect CNV boundaries from whole-genome sequencing data and assigned digitized copy number by modeling for sample-specific con-founding factor such as aneuploidy, normal/diploid contamin-ation of primary tumors and mouse contamination in xenograft models.
The segmentation algorithm based on wavelet transform provides a unique opportunity to probe the genome in any Fig.5.
Validation of copy number calls using three methods.
(A) Verification of 80 CNV loci by qPCR on a pancreatic cancer genome.
Copy numbers from qPCR were estimated based on threshold cycle (Ct) values.
The Pearson correlation coefficient is 0.94.
(B) Verification of 473 somatic CNVs on the whole-genome using Illumina Human Omni 1Million microarray.
Shown here is the concordance be-tween intensity ratios in microarray to WaveCNV CN.
The Pearson cor-relation coefficient is 0.86.
(C) Verification of 468 somatic CNVs on the whole genome using Nimblegen 2.1 Million aCGH microarray.
Shown here is the concordance between aCGH intensities ratio in microarray to WaveCNV CN.
The Pearson correlation coefficient is 0.97 Table 2.
WaveCNV comparison to other algorithms Algorithm Events Gains Losses Total basepair gains Total basepair losses Congruency gains Congruency losses Congruency all WaveCNV 764 359 405 312922 439 567442 194  CNVnator 3658 829 2829 319703 400 622106 100 0.95 0.92 0.93 OncoSNP 1423 567 856 260783 488 912819 293 0.87 0.80 0.80 Note: This table shows the base pair level congruency in copy number alterations called by WaveCNV compared with CNVnator and OncoSNP-SEQ.
773 WaveCNV , to to x ile x minor allele frequency , , loss-of-heterozygosity while , , spatial genomic scale.
Although the first part of the algorithm identifies all discontinuities, the second part of WaveCNV pro-vides a statistical framework to assign CN and merge neighbor-ing events carrying the same copy number.
This corrects for false shearing of copy number events that may arise due to poor qual-ity of sequencing data.
A key component of WaveCNV is the matchednormal-based copy number correction.
Being aware of the diploid control ensures that any systemic artifacts that may appear in both tumor and normal genomes, including platform-specific biases, unsequenceable regions and so forth, are effectively removed or corrected for.
This resulted in a high concordance between somatic CN calls from our algorithm in sequencing data to both microarray data and qPCR.
Xenograft models for many types of primary tumors have increasingly become useful tools to understand cancer biology and to test therapeutic targets.
Our model estimates mouse contamination and the reported allele and copy number reflects the correction for mouse contamination.
The mouse contamin-ation estimate matches well with our mouse-specific qPCR data.
On the same note, most directly sequenced primary tumor samples contain stromal contamination, and our algorithm can quantify and model for the presence of contaminating diploid cells in that sequencing data.
5 CONCLUSION Our segmentation algorithm is unique from its methodology per-spective, and can potentially improve the boundary assignments on the smaller CNV events found via whole-genome sequencing.
In addition, the assignment of specific alleles to copy number losses/gains can give researchers the ability to explore relation-ships between selected sequence mutations and structural vari-ation.
For example, in pancreatic cancer a KRAS activating point mutation is often coupled with duplication events, thus amplifying the effect of this oncogene.
Being able to identify similar correlations based on reports from our algorithm could prove useful in prioritizing-specific genes for further study.
ACKNOWLEDGEMENTS The authors acknowledge support from the Ontario Institute for Cancer Research, Pancreatic Cancer Genome initiative.
They thank the OICR genome production sequencing group.
They thank Dr Lincoln Stein (OICR) for valuable comments on the manuscript and Dr Christopher Yau (University of Oxford) for discussions on running OncoSNP-SEQ.
Funding: New Investigator award, Ontario Institute for Cancer Research, supported by Ontario Ministry of Research and innovation.
Conflict of interest: none declared.
ABSTRACT Motivation: Most microbial species can not be cultured in the laboratory.
Metagenomic sequencing may still yield a complete genome if the sequenced community is enriched and the sequencing coverage is high.
However, the complexity in a natural population may cause the enrichment culture to contain multiple related strains.
This diversity can confound existing strict assembly programs and lead to a fragmented assembly, which is unnecessary if we have a related reference genome available that can function as a scaffold.
Results: Here, we map short metagenomic sequencing reads from a population of strains to a related reference genome, and compose a genome that captures the consensus of the populations sequences.
We show that by iteration of the mapping and assembly procedure, the coverage increases while the similarity with the reference genome decreases.
This indicates that the assembly becomes less dependent on the reference genome and approaches the consensus genome of the multi-strain population.
Contact: dutilh@cmbi.ru.nl Supplementary Information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION DNA sequencing is cheaper than ever before.
Use of a 454 Pyrosequencer and/or Illumina Genome Analyser can produce a nearly complete bacterial genome at a cost of <E10 000 (for a review of next generation sequencing see Mardis, 2008).
However, most microbes can not be readily obtained in pure culture, apparently because their phenotype is not compatible with growth on solid media.
A promising solution to this problem is to perform selective enrichment in continuous culture, where the conditions favorable for the species growth can be approximated more closely.
Further, in enrichment culture, interdependency on other species (e.g.
the exchange of cofactors) is not problematic.
Metagenomic sequencing could yield a near-complete genome if the resulting population is sufficiently enriched and if the sequencing coverage is high (degree of enrichment times sequencing coverage >20).
A culture that is To whom correspondence should be addressed.
inoculated with a natural sample can yield a population that is highly enriched for a single species within a few months (e.g.
Ettwig et al., 2008).
Currently, high sequencing coverage is achieved most cost-effectively with massive parallel sequencing methods that produce short reads [SOLiD sequencing (http://solid.appliedbiosystems.
com) and Illumina/Solexa sequencing (http://www.illumina.com/ pages.ilmn?ID=203; Bentley, 2006)].
Such reads are usually processed with mapping algorithms such as Eland (Bentley et al., 2008) or Maq (Li et al., 2008) if a reference genome from a closely related species is available.
Truly de novo assembly directly from short reads (e.g.
Velvet; Zerbino and Birney, 2008) remains difficult, although innovative techniques that use e.g.
conservation at the gene level are promising (Salzberg et al., 2008).
The mapping algorithms are generally highly conservative: they permit no more than one or two mismatches per read, and do not allow the presence of gaps in the alignment.
This means that any read derived from a region with a lower conservation than 30/32 94% identity will not be mapped, and it restricts the use of a reference genome to highly similar species.
Therefore, mapping reads to a reference genome has two limitations.
First, it depends on an available reference genome of a closely related organism (>94% identity).
Second, an enriched microbial community culture often contains multiple related strains with similar fitness (quasispecies), and the sequence diversity between such strains can be quite high (Venter et al., 2004).
Such a polymorphic population can be expected to confound the highly conservative mapping and/or assembly programs leading to unnecessary fragmentation of the assembly, as well as a large fraction of the reads not being used.
Here, we set out to decipher the consensus genome of parallel populations of a quasispecies sequenced with short-read Solexa sequencing.
Solexa instruments can now generate >50 nt reads, but we used an earlier version of the instrument that generates 32 nt reads.
We use a related genome as a scaffold, and first map the reads to their best possible position on this reference.
Then, we ask per reference position which nucleotide is the most highly represented in the population of strains.
Because the resulting assembly is already a better approximation of the sequences in the strain population than the external reference, we iterate the mapping and assembly procedure to increase the coverage.
The final consensus assembly The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:40 14/10/2009 Bioinformatics-btp377.tex] Page: 2879 28782881 Iterative mapping and assembly Fig.1.
(a) Distribution of coverage scores per nucleotide in the first and 10th iterations; (b) average coverage score in each iteration; (c) percentage identity of non-zero coverage regions of the assemblies with the reference genome and with the previous assembly (i.e.
the reference for that iteration); and (d) percentage of reads in the assembly for each E-value (cumulative).
captures the majority vote of the genomes in the multi-strain population.
2 METHODS 2.1 Data We performed one single-end Illumina sequencing run of an enriched metapopulation (Ettwig et al., 2009), yielding 6 667 153 32 nt reads [details about these data and the reference genome will be published elsewhere (K.F.
Ettwig et al., manuscript in preparation)].
2.2 Mapping reads to their optimal position The list of reads were mapped against the reference genome (2 752 854 nt) with each of three programs: BlastN v2.2.20 (Altschul et al., 1997), MegaBlast v2.2.20 (Zhang et al., 2000) and Maq 0.7.1 (Li et al., 2008).
For Maq, we assigned the highest sequencing quality score () to every nucleotide, and then ran Maq with default parameters.
For BlastN and MegaBlast, the reads were made non-redundant and given a unique identifier containing the number of instances and the sequence (e.g.
7xACGT).
We used relaxed search parameters, to make sure many reads were mapped, even if they were quite divergent.
We used a short word length of 8 (other word lengths were tested as well, see Supplementary Material), turned low-complexity filtering off and used a high E-value threshold of 100, although all reads included in the assembly were mapped with much lower E-values (Fig.1d) due to the alignment length cutoff described below.
To account for the high-coding density in bacterial genomes, we performed ungapped searches (gap open and extend penalties 1000).
The output for each read was immediately parsed, removing all hits with a sub-optimal score.
Not only will we require none other than the highest scoring location(s) on the reference genome for each read, but this filtering step also frees disk space.
2.3 Assembly Next, we assembled the mapped reads to form a consensus genome.
For Maq, we used the consensus sequence provided by the program, while for the BlastN and MegaBlast results, we wrote a custom Perl script (available from the authors on request), taking the following into account.
For each position on the reference genome, we assessed which of the reads covered it with an aligned region of at least 20 nt.
The nucleotide with the highest occurrence in the community was called to align to that reference position.
Draws were replaced by their IUPAC nucleotide code (Cornish-Bowden, 1985).
The coverage at each position equals the number of reads contributing to the consensus.
Positions with no aligned reads (zero coverage) were replaced with Ns.
2.4 Iteration After assembly, the whole procedure was iterated.
Positions with zero coverage in the assembly were replaced with the nucleotide in the reference genome, and all Solexa reads were re-queried against this new reference (as above).
We carried out at least 10 iterations with each read mapping algorithm.
3 RESULTS 3.1 Similarity search algorithm Here we combine the short 32 nt Solexa sequencing reads from a metapopulation of strains to form a consensus genome describing the majority of the population.
The first step in the process is to map as many of the sequencing reads as possible to their optimal position on the reference.
The conservative mapping algorithm Maq (Li et al., 2008) mapped 602 120 reads, leading to an average coverage of 10.8 in the assembled regions, but 35.0% of the reference genome still has zero coverage (Supplementary Material).
The large gaps remaining with this conservative mapping algorithm already shows that the reference is distant enough from the community to require a more relaxed sequence similarity search.
We used BlastN and MegaBlast as examples of less restrictive read mapping algorithms.
We used very relaxed search parameters (see Section 2), allowing even quite distant reads to be mapped to their optimal position in the reference.
However, this approach does require that we employ a filter for spurious short hits, so we selected only those reads that were aligned to the reference over at least 20 nt.
In this preliminary search, BlastN mapped 1 598 549 reads and MegaBlast mapped 1 595 338 reads, both leading to an average coverage in the assembled regions of 18.5, while 14% of the reference nucleotides have zero coverage (Supplementary Material).
3.2 Coverage increases by iteration Any available mapping algorithm will suffice to map highly identical reads to a reference.
Our aim was also to map the more divergent reads to obtain a higher coverage of the polymorphic community on the divergent reference.
The initial coverage of the Blast-based assemblies are already higher than the conservative Maq assembly, but many nucleotides still have a low coverage <10 (Fig.1a).
However, since this first assembly is composed of the metagenomic reads themselves, we expected that using it iteratively as a new mapping scaffold would yield a higher sequencing coverage.
Indeed, the average coverage clearly increases after a second round of querying and assembling the reads to the consensus genome.
Additional iterations gradually increase the number of reads that could be mapped for all algorithms (Fig.1b).
The statistics for the BlastN-and MegaBlast-based approaches are almost identical (Supplementary Material).
These results show that more reads can be mapped as the reference is adjusted to the reads, indicating that the assembly becomes more similar to the consensus genome of the community.
2879 [12:40 14/10/2009 Bioinformatics-btp377.tex] Page: 2880 28782881 B.E.Dutilh et al.
Fig.2.
A region of the assembled sequence showing some of the changes that occur with the iterations.
Gaps in the assembly are filled and single nucleotides are settled.
The coverage per position in every iteration is shown in the bottom panel.
3.3 Consensus genome Observing this clear increase in coverage with the iterations, including a 15% drop of the zero-coverage nucleotides (e.g.
from 387 421 to 329 788 in the BlastN-based approach), we decided to take a look in detail at how the consensus sequence changes with the iterations.
Figure 2 shows a small part of the genome, illustrating some of the changes that occur as the iterations progress.
For example, position 2 314 927 in the alignment (indicated with an arrow), a cytosine in the reference, changes into a Y (i.e.
cytosine or thymine; Cornish-Bowden, 1985) after the second round of read mapping.
Subsequently, in iteration 2, this trend is confirmed and the consensus nucleotide present in the population of reads is settled as a thymine from then on.
Another example is the region with zero coverage (stretches of Ns) in the first and second iteration assemblies that are filled in the subsequent iterations.
It should be noted that we map the entire list of reads against the reference or previous assembly in every iteration, and there is no source of new reads.
It is possible that reads are re-mapped to a different region (e.g.
to the zero-coverage region in Fig.2) if (i) the new region has altered and gained similarity with reads that were not mapped before or that were mapped to another part of the reference; or (ii) the region where these reads were mapped before has altered and lost similarity with the reads so that they now map to this new position instead.
However, as we see that the reads generally gain similarity with the evolving genome (Fig.1c and d), explanation (i) seems the most frequent.
In general, we observe that the assembly slowly drifts away from the reference genome, as measured by the percentage identity of the mapped regions (i.e.
regions with non-zero coverage) to the original reference (Fig.1c, drawn lines).
At the same time, the assembly becomes more coherent, as measured by the percentage identity of the mapped regions to the assembly from the previous iteration (Fig.1c, dashed lines).
Moreover, a larger fraction of the reads is mapped with a lower E-value (Fig.1d).
This indicates that the consensus genome of the population of strains is gradually approached.
The optimum in the curves is reached around iteration four, and the reads do not obtain a better mapping than this if we include more iterations (Supplementary Material).
4 DISCUSSION Here we show how a consensus genome can be composed by mapping metagenomic sequencing reads from a community of strains to a reference.
Furthermore, this consensus genome better represents the community if we iterate the mapping and assembly at least once.
This increase is independent of the read mapping algorithm.
A strict mapping and assembly program such as Maq initially maps 602 120 reads, but this number is increased to 835 328 reads in iteration 10.
A less strict mapping algorithm like BlastN maps 1 598 549 and 2 051 404 reads in the first and 10th iteration, respectively (Supplementary Material).
Note that there is no (artificial) evolution in this method, and no optimality criteria used.
The higher coverage solely results from the fact that the assembly better accommodates the reads.
Thus, we profit from the best of both worlds: we use a reference to scaffold the reads, yet the iterated assembly allows the sequence to drift away from the scaffold and approach the consensus genome of the population.
Iterative read mapping and assembly has previously been applied in the reconstruction of a bacterial genome from environmental sequence data (Pelletier et al., 2008), but the sequencing reads in that experiment had a much longer mean size of 633 nt, and the idea was not systematically analyzed.
We show that our approach can be used with very short 32 nt reads, and the results can only be expected to improve with longer read length.
The sequence we create can be interpreted as the consensus genome of the metapopulation of strains.
As always when mapping short sequencing reads, the structure of the genome is scaffolded onto the reference and therefore does not necessarily reflect the genome structure of any particular strain in the sequenced community.
Thus, this approach is suited to construct the consensus genome of the most abundant lineages in the sample.
Moreover, the DNA sequence at any site within the genome is not even necessarily an existing sequence, but rather the consensus of the most abundant sequences.
However, we note that generally, this is also the case for the genome sequencing projects of species that can not be amplified clonally for sequencing, like animals.
For example, the first human genome was composed of the combined DNA of several individuals (Lander et al., 2001).
Therefore, we expect that the consensus genome we obtain using our iterated assembly method can still provide meaningful information about the encoded proteins and other genomic features.
The in-depth analysis thereof will be the topic of a subsequent paper (K.F.
Ettwig et al., manuscript in preparation).
Funding: Dutch Science Foundation (NWO) Horizon Project 050-71-058.
Conflict of Interest: none declared.
ABSTRACT We present the Stochastic Simulator Compiler (SSC), a tool for exact stochastic simulations of well-mixed and spatially heterogeneous systems.
SSC is the first tool to allow a readable high-level description with spatially heterogeneous simulation algorithms and complex geometries; this permits large systems to be expressed concisely.
Meanwhile, direct native-code compilation allows SSC to generate very fast simulations.
Availability: SSC currently runs on Linux and Mac OS X, and is freely available at http://web.mit.edu/irc/ssc/.
Contact: mieszko@csail.mit.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 BACKGROUND Cells interact with their environment via receptors that bind to extracellular molecules; these events are then translated into functions by biochemical signaling networks.
Non-linearities arising from the complex topology of such networks often make it difficult to intuit qualitative behavior of signaling modules.
Moreover, recent imaging experiments have revealed that signaling components are organized into spatial patterns that modulate signaling (Grakoui et al., 1999; Lee et al., 2003).
Finally, extrinsic and intrinsic stochastic effects, which make each cells response unique, can be important when small numbers of signaling molecules are involved (Artyomov et al., 2007).
As computational studies are increasingly becoming necessary complements to genetic, biochemical and imaging experiments in unraveling this non-intuitive behavior of cell signaling networks, efficient and easy to use tools that can carry out stochastic simulations of biochemical networks, both in well-mixed and spatially inhomogeneous approximations, have become key technologies.
Since the original stochastic simulation algorithm (Gillespie, 1977), basic computer science techniques have reduced the rate at which the per-step computation time grows with the number of possible reactions to logarithmic growth (Gibson and Bruck, 2000; Li and Petzold, 2006; Wylie et al., 2006), or optimized performance by noting that a few reactions account for most events (Cao et al., 2004; McCollum et al., 2006); more recently, Slepoy et al.
(2008) To whom correspondence should be addressed.
have reduced per-step computation to expected constant time via an elegant compositionrejection algorithm.
Similar techniques have been applied to reduce spatially heterogeneous simulation time to logarithmic (Elf and Ehrenberg, 2004).
The combinatorial growth of the instantiated reaction network size, another limiting factor for complex systems, has been addressed either by generating species and reactions on the fly (Faeder et al., 2005; Lok and Brent, 2005) during a Gillespie-based simulation, by representing each molecule separately (Morton-Firth and Bray, 1998), or ingeniously do away with explicit counts altogether by adjusting the sampling distribution (Danos et al., 2007; Yang et al., 2008).
Efficient formulation of such simulations in a general programming language like C or FORTRAN, however, is not a trivial task: while simulating a few reactions is fast even with a simple implementation, a system with thousands of reactions and subvolumes demands more complex algorithms which are much more tricky to code.
The programming burden has been reduced by libraries (e.g.
Li et al., 2008) as well as by simulators for well-mixed (e.g.
Gillespie et al., 2006; Mauch, 2009) and spatially inhomogeneous (e.g.
Hattne et al., 2005; Meier-Schellersheim et al., 2006) models.
File formats like SBML (Hucka et al., 2008), developed to express biochemical models, can be read by several simulators.
The modeling task is further complicated by the explosion in combinatorial complexity which arises when modeling post-translational modification or reactions local to one molecule in a complex (Hlavacek et al., 2006): in SBML (and, indeed, in most simulators) all possible species and each combination of every possible reacting complex must be written out as a separate reaction, which renders expressing even modestly complex reaction networks impractical.
To mitigate these limitations, BioNetGen (Faeder et al., 2009) and (Danos and Laneve, 2004) have proposed higher level specifications where the reactants in each reaction are written as patterns covering many possible species; such descriptions not only naturally correspond to the intuitive concept of a biochemical reaction, but are significantly smaller and therefore more readable as well as much less error-prone.
The main contribution of the Stochastic Simulation Compiler (SSC) that we present here lies in combining a higher level specification required for modeling larger systems with the ability to model spatially heterogeneous systems.
It differs from BioNetGen and because their syntax and expansion algorithms offer no 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:38 4/8/2009 Bioinformatics-btp387.tex] Page: 2290 22892291 M.Lis et al.
support for spatially inhomogeneous containers, while SSC supports multiple regions with arbitrarily complex shapes specified using Constructive Solid Geometry (CSG); meanwhile, while MesoRD allows such regions and geometries, it suffers from the combinatorial complexity limitations described above.
In addition, SSC produces fast simulations (cf.
Supplementary Material) by directly generating machine code tailored to a specific architecture.
2 IMPLEMENTATION 2.1 Tool flow The tool flow resembles a programming language compiler.
The user writes a high-level description of the reaction system (see Supplementary Material for examples), using patterns to select and change specific parts of compounds (similar to how a cell biologist would describe a known or hypothesized cell signaling network).
Regions are specified using CSG, a technique that employs simple operations (e.g.
union, intersection, difference and scale) on basic shapes (such as spheres, cubes, cylinders, etc.)
to describe arbitrarily complex geometries and widely used in solid modeling and computer graphics (see, e.g.
Requicha and Voelcker, 1977).
Any reaction can be restricted to a subset of regions, and diffusions within and among the regions are written using the same high-level pattern syntax as reactions.
The compiler then expands the model starting with initial species and reaction patterns, creating the necessary instances with specific properties and connections as well as specific reactions operating on each of those compounds in each region; meanwhile, the regions are discretized into cubic subvolumes.
This intermediate representation is used to produce a simulator executable, which, in turn, simulates the model signaling pathway.
2.2 Reaction expansion Most biologically relevant signaling reactions are conceptually local, that is, they see only a part of a larger molecule or complex (say, a single phosphorylation site).
Therefore, we write reactions and diffusions locally, using pattern matching to recognize and modify parts of complexes, and rely on the compiler to derive all the possible cases in all regions.
Similarly, only initially present compounds are specified; the compiler generates the rest from the initial set and the reactions.
Formally, the reactions and diffusion form a graph term-rewriting system, which is fully evaluated to generate the simulator.
Briefly, each expansion step considers a rule in the system, finding all combinations of substrates in the relevant region that match the rule.
The rule is then applied to each match, possibly resulting in new compounds, and a compound-specific reaction is created for the specific substrate combination.
Any new compounds not excluded by predefined limits (used to prevent infinite expansion) are added to the region where the reaction took place and any regions reachable by following the given diffusion patterns; the cycle then repeats until no more new compounds have been created.
(See Supplementary Material for details of the expansion process).
2.3 Direct code generation We obtain the efficiency of hand-optimized code by directly generating assembly code from the fully expanded set of reachable species and reactions.
This allows us to avoid the interpretive overhead of consulting dependency graphs to determine which copy counts and propensities must be recomputed.
The generated code is also tailored for model complexity and processor architecture.
For most sizes, the compiler creates a separate, straight-line segment of code for each possible reaction in a region; each segment is parameterized only on the subvolume (or, in the case of diffusion, two subvolumes), and directly updates and propagates the affected propensities (see Section 2.4).
This avoids pipeline stalls and cache flushes caused by mispredicted branches, and reduces the number of data memory reads and writes (which are the performance bottleneck) to the absolute minimum.
(See Supplementary Material for a detailed description of the code generation method).
2.4 Reactiondiffusion simulation algorithm The simulation algorithm is similar to the logarithmic-time versions of the direct stochastic simulation algorithm (Li and Petzold, 2006; Wylie et al., 2006).
The simulation-time representation details may be found in the Supplementary Material; briefly, the reactions in each subvolume (or on each boundary between subvolumes) are arranged in an n-ary heap with the leaves corresponding to individual reaction propensities and each node carrying the combined propensity of the reactions underneaththe topmost node for each subvolume is, then, the propensity of any reaction taking place within.
The subvolume and boundary reaction propensities are, in turn, themselves arranged in a heap where each leaf is either a subvolume or a boundary propensity; the topmost node is the propensity of any reaction in the system taking place (and, hence, the range from which the random number should be selected).
Simulation proceeds as follows: a random number r is selected from range [0,R) where R is the propensity of any reaction taking place; then the subvolume and reaction corresponding to r is selected by n-ary search in the heap.
Next, the reaction is executed, that is, the copy numbers of the affected species are adjusted as the reaction dictates.
Finally, the propensity of each reaction whose substrate copy counts were altered is recomputed, and the partial propensities are propagated up the propensity heap until the new R is recomputed and the cycle can be repeated.
Since the propensity heap in each subvolume (or boundary) has height logarithmic in the number of reactions within, and the heap above is logarithmic in the number of subvolumes and boundaries, the total tree depth scales roughly logarithmically in the number of reactions in the system.
Both the reaction selection/search and copy number/propensity update step, therefore, run in time logarithmic in the number of reactions.
3 PERFORMANCE We compared spatially homogeneous SSC against BioNetGen 2.0.46 (Faeder et al., 2009) (since, like SSC, it builds reaction networks from pattern-matching rules), and against simulators built with the StochKit library (Li et al., 2008); because of the complexity of the larger models, we had SSC automatically generate the required StochKit C++ configurations.
To test real-world performance, we selected two toy systems and two more realistic systems with various reaction counts: a dimer decay model (Gillespie, 2001) with four reactions, a simplified EGFR signaling model (Blinov et al., 2006) with 64 reactions, a model for the earliest events in 2290 [12:38 4/8/2009 Bioinformatics-btp387.tex] Page: 2291 22892291 Efficient stochastic simulation of reactiondiffusion processes T-cell signaling (Wylie et al., 2006) with 1120 reactions, and an enhanced version of the same with 2422 reactions.
To test spatially heterogeneous models, we compared with the latest development revision of MesoRD (Hattne et al., 2005), SVN r559; we used the T-cell signaling model above where single molecules (but not compounds) were permitted to diffuse around a membrane interface, which was divided into 100, 10 000, and 50 000 subvolumes.
All simulations produced the same results (modulo random seed variation and precision loss during floating point arithmetic).
To focus on measuring only the simulation time, we disabled all output except the final species counts, and repeated each experiment 5-fold to account for initial random seed variation and possible effects of other processes executing on the system.
We found that SSC consistently outperformed the faster of the two spatially homogeneous simulators we tested by 2 to 6, with the advantage growing with the size of the model (see Supplementary Fig.3).
For spatially heterogeneous simulation, we found that SSC was 50 faster than MesoRD, although both scaled very well with the number of subvolumes (see Supplementary Fig.4).
4 CONCLUSIONS We have described the SSC, a new tool for exact stochastic simulations of biochemical reaction networks.
SSC is, to our knowledge, the first tool to combine a succinct high-level description (which avoids combinatorial complexity explosion) with spatially resolved simulation where species and reactions may be restricted to specific regions of arbitrarily complex shapes, and unique in employing direct native machine code generation to produce fast simulators.
ACKNOWLEDGEMENTS This research was funded by NIH Grant #1PO1/AI071195/01.
Conflict of Interest: none declared.
ABSTRACT Motivation: We have to cope with both a deluge of new genome sequences and a huge amount of data produced by high-throughput approaches used to exploit these genomic features.
Crossing and comparing such heterogeneous and disparate data will help improving functional annotation of genomes.
This requires designing elaborate integration systems such as warehouses for storing and querying these data.
Results: We have designed a relational genomic warehouse with an original multi-layer architecture made of a databases layer and an entities layer.
We describe a new querying module, GenoQuery, which is based on this architecture.
We use the entities layer to define mixed queries.
These mixed queries allow searching for instances of biological entities and their properties in the different databases, without specifying in which database they should be found.
Accordingly, we further introduce the central notion of alternative queries.
Such queries have the same meaning as the original mixed queries, while exploiting complementarities yielded by the various integrated databases of the warehouse.
We explain how GenoQuery computes all the alternative queries of a given mixed query.
We illustrate how useful this querying module is by means of a thorough example.
Availability: http://www.lri.fr/lemoine/GenoQuery/ Contact: chris@lri.fr, lemoine@lri.fr 1 INTRODUCTION With the entry in the genomics era, the advances of genome sequencing (700 published microbial genomes in April 2008) and the increasingly massive use of high-throughput approaches have produced a huge amount of data.
We urgently need management systems in order to store and query biological information.
In particular, this is critical in functional annotation of genomes (Ouzounis and Karp, 2002).
Indeed, it is now easy to get the complete sequence of a prokaryotic genome and to detect all its genes (structural annotation).
On the contrary, the functional annotation of its putative coding sequences is increasingly difficult, especially for organisms never studied by experimental biology.
For instance, it is a challenging goal to reconstruct the complete metabolism of a species using uniquely its genomic sequence (Karp et al., 2005).
However, such a reconstruction is crucial to disclose potential drug targets in the case of pathogenic microbes or to exploit novel pathways in the case of species that are potentially useful for bioremediation or bioenergy needs.
Such a To whom correspondence should be addressed.
functional annotation step requires combining various pieces of knowledge and correctly handling heterogeneous data stored in various databases that are either local sources or distributed sources on the web.
Several approaches have been proposed in the field of databases integration.
Portals like SRS (Etzold et al., 1996) and EXPASY (Gasteiger et al., 2003) allow users to query easily multiple sources by means of a single website.
Path-based systems such as Biomediator (Cadag et al., 2007), Bionavigation and BioGuide (Cohen-Boulakia et al., 2006) are based on cross-references between data sources, in order to navigate from one source to another, and provide the user with ranked paths selected according to her/his preferences.
Such systems allow to query easily data sources by means of a more or less expressive language.
Mediator systems such as Tambis (Stevens et al., 2000) or K2/Kleisli (Davidson et al., 2001) are designed to query the distant sources through a virtual mediated schema.
The query formulated on the mediated schema is further translated into queries over the schemata of the sources and the answers are processed locally.
In these non-materialized integration systems, data are not stored locally, but remain in the distant sources.
Thus, mediator systems provide up-to-date data, but do not permit complex computations.
In particular, data-mining techniques are very difficult to apply on such systems.
In peer-to-peer systems like Piazza (Halevy et al., 2003) or Orchestra (Green et al., 2007), data are stored in multiple distant peers, which communicate with a few other peers and queries can be formulated over each peer.
Finally, in fully materialized systems such as GUS (Davidson et al., 2001), BioWarehouse (Lee et al., 2006), Biozon (Birkland and Yona, 2006), GEDAW (Gurin et al., 2005), Biomart (Durinck et al., 2005) or Columba (Trissl et al., 2005), data are integrated within a warehouse based on a locally constructed schema.
While updating warehouses is a challenging task, performing complex computations is easier.
We are expecting a huge amount of prokaryotic genomic sequences (e.g.
more than 2.106 proteins for the 700 genomes presently available) and many derived heterogeneous data, that must be integrated and on which we need to perform data-mining techniques and other complex computations.
Therefore, we decided to gather these data within the Microbiogenomics data warehouse, with an ad hoc architecture.
In this way, the integrated databases may provide complementary, different or even divergent points of view on the same data, by adding supplementary useful information as discussed subsequently.
Accordingly, we are developing Microbiogenomics to fulfill two main objectives: improving functional annotation or reannotation of microbial genomes and studying molecular evolution of genes and genomes of micro-organisms.
To perform these tasks, Microbiogenomics contains a large variety of primary 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:02 18/6/03 Bioinformatics-btn159.tex] Page: i323 i322i329 Querying a genomic warehouse (sequence-based) and secondary (homology-based) data.
Primary data (genomes, protein sequences, metabolic pathways, enzymes and bibliographic data) come from different and complementary public sources.
We have generated secondary data using various pipelines (Bryson et al., 2006; Le Bouder-Langevin et al., 2002; Lemoine et al., 2007) that have been designed to find out homology relationships between proteins as the result of sequence alignment programs such as Blast (Altschul et al., 1990) and Darwin AllAll (Gonnet et al., 2000).
This article describes GenoQuery, an innovative approach to query data warehouses such as Microbiogenomics.
This querying method is based on a concerted use of a few of the concepts and tools developed by the already described approaches.
Indeed, GenoQuery uses the notion of view from mediator and peer-to-peer systems applied to a local integration of data sources as in warehouses approaches, and uses the notion of path and graph of entities as in browsing approaches.
Moreover, our approach offers a new notion of alternative queries permitting to exploit both matches and conflicts between data sources.
GenoQuery is based on a dedicated two-layers architecture that is outlined on Figure 1.
The first layer is made of the different relational databases present in the Microbiogenomics warehouse.
The second layer is made of entities organized in two levels.
Since biological entities can be considered at two degrees of abstraction, we distinguish the abstract entities level and the concrete entities level.
Abstract entities are the biological entities present in the content of the databases, that we consider as relevant, whereas concrete entities are the manifestations of abstract entities in the sources.
Both levels are represented by a specific elementary graph, as detailed subsequently, and the graphs of abstract and concrete entities are further combined in the so-called general graph of entities.
This general graph of entities is further used to build the mixed query graphs.
Finally, we propose a first prototype with a friendly user interface allowing complex queries that lead to alternative answers rich in biological information.
A prototype of GenoQuery is available at http://www.lri.fr/lemoine/ GenoQuery/.
2 DATABASES LAYER 2.1 Biological data and databases The biological databases we integrated into the Microbiogenomics warehouse are major public data sources: RefSeq (Pruitt et al., 2007), Genome Reviews (Sterk et al., 2006), Prose (http://genome.jouy.inra.fr/prose) a relational version of UniProt (Bairoch et al., 2005) and Pareo (http://genome.jouy.inra.fr/pareo/) a relational version of KEGG (Kanehisa et al., 2006).
We have added local sources developed by the different labs participating in the Microbiogenomics project: elements of AGMIAL (Bryson et al., 2006), Genopage (Cohen-Boulakia et al., 2002), Syntebase (Lemoine F. et al., manuscript in preparation), Orenza (Lespinet and Labedan, 2006).
These primary and secondary data were then used to generate tertiary data such as conservation of gene order in genomes, orthologous relationships between proteins, phylogenetic relationships using a pipeline recently described (Lemoine et al., 2007).
2.2 Warehouse schema based on independent databases Microbiogenomics is built in relational format and is implemented under PostgreSQL.1 We chose the relational format because it is well known and allows expressive queries.
Each database remains independent from the others in terms of schema and data.
Thus, the data are not fully reconciled, allowing to consider the different points of view given by the various databases.
The advantages of such a global schema are the following: (1) each database can be updated independently from each other, (2) addition of new sources is possible at any time, allowing to personalize the warehouse according to specific and/or new needs by adding a locally developed new data source and (3) the complementarities and divergences about some biological data are kept because they potentially add supplementary useful information.
Microbiogenomics is focusing mainly on protein-related data.
However, public databases use different identifiers to describe the same protein.
To achieve semantic integration of the data, we built associations between the relational tables of the sources by means of an independent links table that is updated whenever a database is updated or a new database is added.
3 ENTITIES LAYER 3.1 The abstract entities level The abstract entities level is a conceptual model that we have built using a bottom-up approach.
We used the concepts present in the sources and the biological associations linking these concepts.
This level is represented by a directed graph, the graph of abstract entities (Figs 1 and 2).
Vertices are divided into two parts: abstract entities themselves such as Protein or MetabolicPathway (circles in the top panel of Fig.2), and properties of these abstract entities (data not shown), such as name, sequence for the entity Protein.
Edges describe three types of links: (1) biological links between two abstract entities, such as Transcript-CodesFor-Protein (black lines between abstract entities in Fig.2), (2) links between the abstract entities and their properties such as Protein-has For Property-sequence, (3) hierarchical relations between abstract entities, the isa links represented as black arrows in Figure 2.
For instance, Figure 1 shows an isa link between C3 and C4 (gray arrow), and Figure 2 shows an isa link between Enzyme and Protein.
3.2 The concrete entities level Below the conceptual model, we built the level made of the concrete entities present in each database, in a way similar to Cohen-Boulakia et al.
(2007).
Each concrete entity is a view of some abstract entity in a given database, which provides instances of it.
For example, GenopageProtein is the concrete entity mapped with the abstract entity Protein, for which instances can be found in the database Genopage.
We define the properties of a concrete entity as the set of properties of its corresponding abstract entity, and possibly other properties specific to the entity in its underlying database.
With each concrete entity is associated a query in its underlying database, which expresses how to retrieve instances of the concrete entity in the database (e.g.
v1 for C11 in Fig.1).
1http://www.postgresql.org i323 [20:02 18/6/03 Bioinformatics-btn159.tex] Page: i324 i322i329 F.Lemoine et al.
Entities layer Databases layer Source 1 Source 2 Source 3 General graph of entities Graph of abstract entities Graph of concrete entities C33C23C13C32C22 C1 C2 C3 C4isa1 C21 C11 v1 v2 v3 v4 v5 v6 v7 Fig.1.
The multi-layer warehouse architecture.
Fig.2.
Part of the general graph of entities.
Arrows link concrete entities to their corresponding abstract entity.
The dotted line between GenopageProtein and MolecularFunction is an example of an inherited link.
The concrete entities level is represented by a directed graph, the graph of concrete entities (Figs 1 and 2).
Its vertices are of three kinds: (1) concrete entities such as ProseProtein (circles in the bottom panel of Fig.2), (2) properties of these entities such as length or amino acid sequence (data not shown) and (3) link entities, such as ProteinLink, which represent the links tables.
Edges linking the vertices are of different kinds.
The biological links between two concrete entities (black lines in the bottom panel of Fig.2) are mapped to a path (possibly of length 1) in the abstract entities level and a join (either a foreign key or links table) in the databases layer.
For example, the link GenopageCDS-CodesFor-GenopageProtein is mapped to the path of length 2 CDS-Transcribed To-Transcript-CodesFor-Protein in the graph of abstract entities.Asecond kind of link is the association between the concrete entities and their properties.
Finally, there are links between a concrete entity and a link entity.
This last kind of link corresponds to the entities that are present in different sources and for which the links table is involved (lines between squares and circles in Fig.2).
For instance, i324 [20:02 18/6/03 Bioinformatics-btn159.tex] Page: i325 i322i329 Querying a genomic warehouse the links table represented by the link entity ProteinLink (square in Fig.2) associates GenopageProtein, ProseProtein and PareoProtein.
Such links have no mapping in the abstract entities level, but match with the links table in the databases layer (Fig.1).
3.3 The general graph of entities The general graph of entities that is the basis of our querying module GenoQuery (see subsequently), is a directed graph, where the vertices are the union of the vertices of the graph of abstract entities and the graph of concrete entities (Fig.2).
Its edges are the edges of both graphs, the isa links between concrete entities and their corresponding abstract entities, and some additional links between them that are needed for constructing mixed queries (Fig.2).
If some abstract entity is linked to another abstract entity by a biological link, then all the corresponding concrete entities will also be linked to the other abstract entity with the same kind of link.
These links are called inherited biological links.
For example, the association Protein-EndowedWith-Function creates the inherited biological link ProseProtein-EndowedWith-Function.
4 THE QUERYING MODULE GENOQUERY We designed a dedicated querying module that uses mechanisms of query reformulation and exploits the architecture of the data warehouse.
Presently, GenoQuery queries databases that are mirrored locally.
First, mixed queries are expressed in terms of abstract as well as concrete entities.
Second, the querying module translates these mixed queries into equivalent queries, only expressed in terms of concrete entities (alternative queries).
4.1 Defining mixed and alternative queries 4.1.1 Mixed queries We call mixed query a query that searches for both abstract and concrete entities, properties of these entities and relationships between them.
The possibility to formulate queries that involve abstract entities, concrete entities or both confers a good expressivity power on the query language of our system.
On the one hand, one can feel free from having to specify the data sources of the warehouse that are relevant to a query, using only abstract entities.
We call such a query a high-level (or transparent) query.
On the other hand, using concrete entities allows to specify for some given entities the source in which their instances must be searched for.
If the query has only concrete entities, we will refer to such a query as a low-level query.
Amixed query graph is a directed graph where vertices are abstract or concrete entities chosen in the general graph, and edges are associations between these entities, taken from the general graph of entities.
It is worth noticing that a given entity can have more than one occurrence in a mixed query.
4.1.2 Alternative queries Every abstract entity in a mixed query has to be first translated into concrete entities to be answered.
As the user does not have to specify in which source instances of this abstract entity has to be searched for, the querying module will search for all the sources of the warehouse that can provide the corresponding concrete entities using the isa links between concrete and abstract entities (Fig.1).
Moreover, we will exploit a nice feature of the global schema we have designed for the warehouse.
Since the reconciliation of the data is not mandatory, it is possible to examine the different points of view of the sources, without forcing the reconciliation of all the data.
Indeed, alternative queries could give complementary or divergent data that are potentially interesting for improving functional annotation.
We will show subsequently that links between concrete entities in some databases can correspond to more elaborate paths between concrete entities in other sources, and thus involve other entities not present in the original mixed query.
One mixed query can therefore lead to several alternative queries, which are low-level queries, each one corresponding to the same high-level query as the initial mixed query.
This mapping is established using the general graph of entities.
The reader is referred to Technical Report (submitted for publication) for a formal definition in terms of graphs.
Let us give an example of mixed query that we will work with in the following (Fig.3): What are the pairs of proteins that are encoded by neighboring genes in Genopage and which participate in the same metabolic pathway in Escherichia coli from Genopage.
Figure 3 shows that the corresponding graph is complex and necessitates multiple sources.
GenopageOrganism is the concrete entity for E.coli from Genopage, MetabolicPathway is the abstract entity for metabolic pathway, GenopageCDS are the concrete entities for neighboring genes and GenopageProtein is the concrete entity for the proteins encoded by the genes.
Note that the last two entities have two occurrences in the mixed query graph.
Moreover, the gray entities correspond to the ones returned by the query.
Additional entities are present in the mixed query graph (MolecularFunction,BiochemicalReaction and GenopageGeneticElement) although they are not explicitly mentioned in the natural language query.
They come from the general graph and are necessary to associate the entities explicitly searched for.
Fig.3.
Graphical representation of the mixed query: What are the pairs of proteins encoded by neighboring genes in Genopage which participate in the same metabolic pathway in Escherichia coli from Genopage.
The following properties are represented: name for metabolic pathway, name for GenopageProtein and name for GenopageOrganism.
i325 [20:02 18/6/03 Bioinformatics-btn159.tex] Page: i326 i322i329 F.Lemoine et al.
4.2 Building alternative queries The mixed query graph representing the initial users query is processed through four steps in order to get the low-level query graphs leading to the alternative queries.
4.2.1 Splitting up the mixed query graph into elementary paths In the mixed query one entity searched for by the user can be linked to more than two entities, so that the mixed query graph can become rather complex.
From the initial query we build elementary queries where each entity is only linked to one or two other entities.
Such elementary queries are represented by elementary paths in the mixed query graph, where each vertex has at the most two adjacent vertices.
To delineate elementary paths, we distinguish in the initial query some entities that play a special role.
A vertex representing an entity in the graph is considered as a breakpoint if it verifies at least one of the following conditions: (1) it has less (or more) than two links with other entities nodes (they are start points or end points of the query, or they are central in the query being linked to many other entities), (2) it is linked to at least one property.
We then compute all the linear paths beginning and ending with breakpoints in the initial query.
In Figure 3, the five breakpoints are in bold: GenopageCDS because it is linked to more than two other entities, and the shaded entities that are linked to a property.
We extract five elementary paths (EPs) from the initial query graph: EP 1 (right part of the graph): Genopage Protein Endowed WithMolecular FunctionCatalyzesBiochemical Reac-tion InvolvedInMetabolic Pathway EP 2 (left part of the graph): Genopage ProteinEndowed WithMolecular FunctionCatalyzesBiochemical Reac-tionInvolvedInMetabolic Pathway EP 3: Genopage CDSNextToGenopageCDSCodes ForGenopage Protein EP 4: Genopage CDSCodesForGenopage Protein EP 5: Genopage OrganismHas For GEGenopage Genetic ElementContainsGenopageCDS (The biological associations are in italic, and the entities in normal font).
4.2.2 Computing intermediate queries Alternative paths are high-level paths (containing only abstract entities), that take into account the mapping between the biological links in the concrete entities level and the paths in the abstract entities level defined in Section 3.2.
They are defined as alternative ways of linking abstract entities, while keeping the meaning of the initial query.
Alternative paths can be computed for each elementary path in two phases (Fig.4): (1) abstraction of the elementary paths and (2) computation of all the corresponding alternative paths.
Since the mapping gives a semantic correspondence between an edge in the concrete entities level and a path in the abstract entities level, a path in the general graph has equivalent reformulations.
In theory, this step can give rise to a very large number of alternative paths.
In practice, however, in the context of Microbiogenomics, the entities graph contains a few entities (about 50).
Therefore, the number of different alternative paths generated is low.
For instance, in the elementary path EP4, GenopageCDS is mapped with the abstract entity CDS and GenopageProtein with Protein.
The abstract path corresponding to EP4 is: CDS Transcribed ToTranscriptCodesForProtein.
From this Fig.4.
Splitting up an initial query into a set of elementary paths.
For each elementary path we compute all the semantically equivalent alternative paths, using the mappings.
The ovals represent the properties, attached to the entities represented by circles.
The shaded circles represent the breakpoints.
The lines represent biological links in the query, and the dotted lines represent the shortcuts defined in the mapping.
resulting abstract path, we compute all the alternative paths.
Here is an example of a resulting alternative path: CDSTranscribed To a transcript which Codes ForProtein.
We can see that there is a new link in this alternative path that is a shortcut standing for the first two links in the abstract path.
The set of intermediate queries is made of all the queries that are combinations of the alternative paths computed above.
The queries we collected never involve many entities (at the most 15).
Therefore, whereas the number of intermediate queries could be very high in theory, this is not the case in practice.
4.2.3 Building querying graphs For each intermediate query, we build a querying graph where each abstract entity is linked to all its corresponding concrete entities (Fig.5).
Note that these links are not shown in Figure 5, but the entities linked together belong to the same box.
For instance, MolecularFunction is linked to PareoMolecularFunction and ProseFunction.
We attach to each concrete entity of the querying graph the properties of the abstract entity corresponding in the intermediate query (data not shown in the figure).
Figure 5 further shows how we add three kinds of associations to this querying graph: (1) Edges between abstract entities of the intermediate query (e.g.
the thick black line in Figure 5 linking the boxes Protein and CDS) (2) Edges between the corresponding concrete entities (e.g.
the dotted line linking ProseCDS and ProseProtein) (3) Links tables that associate concrete entities of different sources (black circle in the box Protein linking ProseProtein, GenopageProtein and PareoProtein).
Intuitively, a querying graph is an undirected graph which regroups all the entities (abstract and concrete) and the links (between concrete or abstract entities) necessary to compute all the alternative queries from an intermediate query.
A querying graph is valid (i.e.
there is at least one alternative query corresponding to the intermediate query) if (1) it is connected, (2) for each abstract entity there exists a concrete entity and (3) each association of the intermediate query corresponds to at least one association between two concrete entities.
i326 [20:02 18/6/03 Bioinformatics-btn159.tex] Page: i327 i322i329 Querying a genomic warehouse Fig.5.
Example of a querying graph constructed from an intermediate query.
The thick black lines represent biological associations between abstract entities of the intermediate query.
The dotted lines represent biological associations between concrete entities.
Finally, the black circles represent the links tables that associate proteins of different sources (thin black lines).
For the sake of readability neither the entities properties nor the association names are represented.
4.2.4 Getting alternative queries The last step is the computation of the set of alternative queries.
After traversing the querying graph, all the low-level queries are extracted.
First, we perform a depth first search of the querying graph, and for each edge linking two abstract entities in the querying graph (thick black lines in Fig.5), we calculate all the ways of instantiating it by associations between concrete entities (dotted lines in Fig.5) together with their properties.
Then, we add associations between concrete entities that are not in the same source, (thin black lines in Fig.5) in order to link the concrete entities between the sources (e.g.
Proseprotein and GenopageProtein).
Eventually, we obtain a set of graphs that involve only concrete entities, and constitute the alternative queries.
The shaded concrete entities together with the dotted lines linking them in Figure 5 constitute the graph of such an alternative query for the running mixed query.
This alternative query interrogates the databases Genopage for entities Protein, CDS, Organism and Genetic Element, and Pareo for entities MolecularFunction, Biochemicalreaction and MetabolicPathway, respectively.
5 IMPLEMENTATION 5.1 General graph of entities The Resource Description Framework, RDF (Klyne, 2004), is a simple and expressive language for representing information in the web, and is especially well suited to graph representation.
We have thus implemented the general graph of entities in RDF (Klyne, 2004), with the use of RDFs, a RDF Vocabulary Description Language that allows to build concepts and relationships between them.
RDFs is able to model isa relationships between entities, as well as associations between two entities, or between an entity and its properties.
To each abstract or concrete entity corresponds one RDFs class and to each property corresponds a literal.
To each isa relationship corresponds a RDF statement which indicates that the first class is a subclass of the second one; to each edge associating an entity (abstract or concrete) with its property corresponds an RDFs property that has a literal as range; finally to each other edge corresponds a RDFs property that makes the link between two classes.
The mappings between the graph of concrete entities and the graph of abstract entities are expressed in XML, with a specific XML schema describing how the document must be formed.
5.2 Mixed query graphs There are several ways of expressing mixed queries.
First, one can use the graphical interface prototype developed in the Java language (Fig.6).
This interface allows to construct a mixed query graph, guided by the general graph of entities.
A second possibility is to write the query using a subset of the SparQL query language (Prudhommeaux and Seaborne, 2007) over the general graph of entities itself represented in RDFs.
Then, the SparQL query can be imported in the interface (click on File/import SparQL query) to be processed by the GenoQuery module.
Using a formal query language allows to save and compare queries, which can be interesting for repeated uses of the warehouse.
We chose SparQL because it is well adapted for the definition of queries on RDFs documents, its syntax is relatively simple to learn, and some frameworks currently exist for parsing and handling SparQL queries such as Jena.2 5.3 Alternative query graphs Once the initial query is constructed, one can compute the alternative queries, by clicking on Query/Compute alternative queries menu.
The alternative queries are displayed as a graph in the right panel.
The user can navigate through the alternative queries by clicking on the black arrows.
By clicking on the execute button, the current alternative query is converted to a SQL query that can be run on the relational warehouse.
6 USING THE QUERYING MODULE We illustrate the usefulness of GenoQuery by means of the following mixed query.
Assume that we are interested in the genes of the bacterium Mycobacterium tuberculosis that have been annotated as encoding an enzyme registered in Pareo (KEGG) with a known activity described by an EC number.
The abstract entities are CDS (linked to the property name) and Organism (linked to the property speciesname), whereas the concrete entities are PareoProtein and PareoBiochemicalReaction (linked to the property EC_number).
Using our querying module we recover data from the last updated relational versions of UniProt (Prose) and KEGG (Pareo).
We mention two alternative queries.
The first one is What are the genes of M.tuberculosis in Pareo that have been annotated as encoding an enzyme in Prose (UniProt) and that display an 2http://jena.sourceforge.net/ i327 [20:02 18/6/03 Bioinformatics-btn159.tex] Page: i328 i322i329 F.Lemoine et al.
Fig.6.
Snapshot of the prototype developed in the Java language.
EC number in Prose?
The second one is What are the genes of M.tuberculosis in Pareo that have been annotated as encoding an enzyme in Pareo and that display an EC number in Pareo?
Results of both alternative queries are presented to the user in a tabular form, with two columns.
The first one displays the name of the gene, and the second one the EC number.
We found 1298 gene products associated with an EC number in UniProt (Prose) and only 1089 in KEGG (Pareo).
Among them, 102 EC numbers associated with the same gene are different, as detailed subsequently.
We note increasing kinds of dissimilarities about the molecular function.
For 57 enzymes, UniProt and KEGG disagree about the last digit, i.e.
the precise definition of the biochemical reaction.
For instance, the gene Rv1086 (O53434, ZFPP_MYCTU) is annotated as encoding a short-chain Z-isoprenyl diphosphate synthetase (EC 2.5.1.68) corresponding to the first step of decaprenyl diphosphate biosynthesis in UniProt.
In KEGG, Rv1086 is viewed as encoding a di-trans, poly-cis-decaprenylcistransferase (EC 2.5.1.31), one of the step of terpenoid biosynthesis.
The 45 remaining dissimilar EC numbers differ at least at the level of their first digit, i.e.
they correspond to very different molecular functions.
Among them, we get eight fully annotated EC numbers that differ in their four digits.
For instance, Rv1248 (O50463, KGD_MYCTU) is encoding a 2-oxoglutarate decarboxylase (EC 4.1.1.71) according to UniProt.
In KEGG, Rv1248 is viewed as encoding an-ketoglutarate decarboxylase (EC 1.2.4.2).
Thus, GenoQuery allows finding easily major conflicts or minor discrepancies between main public databases.
Ongoing work includes automating the last steps of the results, the analysis of which is currently done manually.
This is especially important because we are expecting to see an amplification of these inconsistencies as both automatic and manual annotation processes are becoming increasingly complex.
7 DISCUSSION We have designed GenoQuery, a module for querying a relational genomic warehouse.
GenoQuery is based on an original multi-layer architecture of the warehouse, made of two layers, the entities layer and the databases layer.
We have further distinguished two levels in the entities layer: abstract entities that are extracted from the databases and concrete entities that are views of theses abstract entities in the databases.
These two levels are linked with isa relationships that map concrete entities to abstract entities, and inherited biological links.
Each concrete entity is associated with a query in its corresponding database, which allows retrieving its instances in the database.
The global schema of the data warehouse is flexible enough to allow easy addition of a new source or update existing ones, as the various data coming from the different databases have been merely gathered in the warehouse, and since we did not remove redundancies or potential conflicts.
This kind of integrating schema is close to the approach of Trissl et al.
(2005), where the data from different sources are never mixed into a single table and each data source is considered as an essentially independent dimension around the central fact (protein structure).
Likewise, the content of our warehouse is centered on the fundamental entity Protein.
In our approach, semantic integration is achieved thanks to a links table that connects the ids of the same proteins on the basis of their amino acid sequence identity and their gene position.
This is crucial to take into account the possible polymorphisms carried by the different sources.
Accordingly, one can take advantage of complementary points of view on the same data.
The maintenance of this multi-layer architecture depends on two kinds of modifications.
In the case of addition (or deletion) of a source, we have to add/remove concrete entities and links in the concrete entities layer.
Similarly, a revision of the relational schema of a source requires the adjustment of the concrete entities layer.
Note that updates of the sources in terms of instances do not affect the diverse layers.
It is worth noting that this maintenance additional cost is mandatory in order to keep the different points of view given by the sources.
Indeed, discussions are opened (Pennisi, 2008) to propose systems that permit to confront points of views.
We have used this architecture to define mixed queries in terms of both abstract and concrete entities, allowing to search for instances of biological entities and their properties in the databases, without requiring specifying in which database they have to be found.
In order to exploit the complementarities of the points of view displayed on the warehouse, we have introduced the central notion of this article, alternative queries.
Indeed, GenoQuery yields as alternative queries, concrete queries that have the same meaning as the original mixed query, but that may consider the entities in other databases than those specified in the mixed query (if any).
We have further shown how to calculate all the alternative queries of a given mixed query and have described the prototype available on the web.
The usefulness of the approach has been demonstrated through a thorough example.
GenoQuery helps to settle varying interpretations by pinpointing the dissensions between sources.
i328 [20:02 18/6/03 Bioinformatics-btn159.tex] Page: i329 i322i329 Querying a genomic warehouse One original aspect of GenoQuery is the capacity to reformulate an initial query in alternative queries.
The concept of query reformulation has been already used in other works but with a different meaning.
For example, in the approach of Lowden and Robinson (2004), reformulation of an initial query leads to minimal cost that provide exactly the same set of answers and is merely a semantic query optimization process.
In the work of Necib and Freytag (2004), query reformulation exploits the knowledge of an ontology to build more meaningful query answers.
In these works, reformulation is performed in the context of a single database.
On the contrary, our approach fully exploits the fact that the data come from various sources that give their own point of view in the same warehouse.
Accordingly, our reformulation process keeps unchanged the meaning of the initial query while proposing all the ways of consulting the different databases for retrieving the instances of the entities searched for.
Our notion of reformulation can be applied to other integration systems.
Actually, GenoQuery is generic enough to be used straightforwardly in other relational genomic data warehouses where reconciliation of data is not fully achieved (redundant and divergent pieces of knowledge are allowed).
More precisely, the warehouse architecture should make available several points of view on the data, by keeping all the data from the different databases and mentioning in the warehouse their provenance (from which database they come from), as done in Biowarehouse (Lee et al., 2006), Columba (Trissl et al., 2005) and Ensmart (Kasprzyk et al., 2004).
To plug our querying module on such warehouses, it would be necessary to design the entities layer on the top of the warehouse architecture, together with the mapping to the databases layer.
Then, our module offers a user-friendly interface for defining mixed queries and allows calculating alternative queries and their translation into SQL queries on the databases.
The notion of alternative queries we have introduced seems to be especially promising to discover new unexpected knowledge from data warehouses such as Microbiogenomics.
ACKNOWLEDGEMENTS We would like to thank all the participants of the Microbiogenomics project, in particular J.F.
Gibrat and his group at MIG, for their help in the design of the global schema of the warehouse.
We are also grateful to Anne Morgat (SIB), Eric Coissac (LEC) and Sarah Cohen Boulakia (LRI) for fruitful discussions.
Funding: This work was funded by the CNRS (UMR 8621, UMR 8623), the PPF Bioinformatique et BioMathematique of the Universit Paris-Sud and the Agence Nationale de la Recherche (ANR-05-MMSA-0009 MDMS_NV_10).
F.L.
is a PhD student supported by the French Ministry of Research.
Conflict of Interest: none declared.
ABSTRACT Interactions of the ESCRT complexes are critical for endosomal trafficking.
We identify two domains with potential significance for this process.
The MABP domain present in metazoan ESCRT-I/MVB12 subunits, Crag, a regulator of protein sorting, and bacterial pore-forming proteins might mediate novel membrane interactions in trafficking.
The UBAP1-MVB12-associated UMA domain found in MVB12 and UBAP1 defines a novel adaptor that might recruit diverse targets to ESCRT-I.
Contact: aravind@ncbi.nlm.nih.gov Supplementary information: Supplementary data are available at ftp://ftp.ncbi.nih.gov/pub/aravind/UMA/MVB12.html.
Received on March 9, 2010; revised on April 16, 2010; accepted on April 21, 2010 1 INTRODUCTION A key aspect of eukaryotic intracellular trafficking is the sorting of cell-surface proteins into multi-vesicular endosomes or bodies (MVBs), which eventually fuse with the lysosome, where they are degraded by lipases and peptidases.
This is the primary mechanism for downregulation of signaling via transmembrane receptors and removal of misfolded or defective membrane proteins (Raiborg and Stenmark, 2009).
This process is also utilized by several viruses (e.g.
HIV-1) to facilitate budding of their virions from the cell membrane (Morita et al., 2007).
Studies in animals and fungi have shown that it depends on an intricate series of interactions, which is initiated via ubiquitination (typically one or more mono-ubiquitinations) of the cytoplasmic tails of membrane proteins by specific E3 ligases (dAzzo et al., 2005).
Ubiquitinated membrane proteins are then captured into endosomes by the ESCRT system and prevented from being recycled back to the plasma membrane via the retrograde trafficking system.
The ESCRT system also folds the endosomal membranes into invaginations that are concentrated in these ubiquitinated targets and catalyzes their abscission into intraluminal vesicles inside the endosome.
This largely seals the fate of these membrane proteins as targets for lysosomal degradation.
The ESCRT system is comprised of four major protein complexes, ESCRT-0 to ESCRT-III, which are successively involved in the above-described steps (Raiborg and Stenmark, 2009).
ESCRT-0, containing proteins with multiple Ub-binding modules, is the primary sensor for ubiquitinated membrane proteins.
Both ESCRT-I and ESCRT-II have proteins with a single Ub-binding domain and are subsequent successive recipients of the ubiquitinated To whom correspondence should be addressed.
cargo.
ESCRT-II proteins also contain lipid-binding modules and are likely to initiate invagination of the endosomal membrane.
ESCRT-III, which includes the conserved AAA+ ATPase VPS4 as a component, mediates the final abscission of the invaginated membrane to form the intraluminal vesicle.
In this relay, ESCRT-I is the critical bridge between the sensor of ubiquitinated targets and the membrane-binding ESCRT-II.
ESCRT-I contains three subunits that are conserved between yeast and animals, namely the inactive E2-ligase protein TSG101/VPS23, VPS28 and VPS37 (Raiborg and Stenmark, 2009).
Additionally, both yeast and metazoan ESCRT-I contain a fourth subunit termed MVB12 [multivesicular body sorting factor of 12 kD (Chu et al., 2006)]; however, the MVB12 subunits from the two lineages do not show significant sequence similarity (Audhya et al., 2007; Chu et al., 2006; Konishi et al., 2006; Morita et al., 2007).
Metazoan MVB12 was shown to be critical for receptor endocytosis and also virus release (Morita et al., 2007).
Given its key role in receptor downregulation, we were interested in understanding if the lack of detectable similarity with yeast MVB12 might reflect emergence of novel adaptations in animals.
Accordingly, we analyzed the animal MVB12 proteins using sensitive sequence and structure analysis methods and identified two novel conserved domains in them.
Identification of these domains allowed us to detect several putative, uncharacterized ESCRT-I subunits in animals.
Characterization of these domains also provides new insights into recognition of cargo by endosomal sorting regulators.
2 METHODS Profile searches were conducted using the PSI-BLAST program (Altschul et al., 1997) with a default profile inclusion expectation (E)-value threshold of 0.01.
Profileprofile comparisons were performed using the HHpred program (Soding et al., 2005).
Hidden Markov model searches were conducted using JACKHMMER from the HMMER3 package (Eddy, 2008).
Multiple alignments were constructed using Kalign (Lassmann and Sonnhammer, 2005) followed by manual adjustments based on PSI-BLAST results.
Protein secondary structure was predicted using a multiple alignment as the input for the JPRED program (Cuff et al., 1998).
The 3D structures were rendered using the PYMOL program (http://www.pymol.org/).
3 RESULTS AND DISCUSSION 3.1 Identification of the UMA and MABP domains To investigate the relationships of the animal MVB12, we used the closely related human paralogs MVB12A (FAM125A; gi: 24308440) and MVB12B (FAM125B; gi: 58761488) as seeds for sequence profile searches with the PSI-BLAST program and Published by Oxford University Press on behalf of the US Government 2010.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
ftp://ftp.ncbi.nih.gov/pub/aravind/UMA/MVB12.html[16:26 12/5/2010 Bioinformatics-btq235.tex] Page: 1478 14771480 R.F.de Souza and L.Aravind Fig.1.
Multiple sequence alignment of the UMA (A) and MAPB (B) domains.
Residues are colored according to the 85% consensus.
Conserved MAPB positions are highlighted as listed in the lower box.
iterative hidden Markov model searches with the JACKHMMER program.
The N-terminal region (human MVB12A, region 1-150) and the C-terminal region (MVB12A, region 210264) recovered distinct sets of proteins.
The N-terminal region of the MVB12A/B proteins hit several proteins from eukaryotes and bacteria.
These included proteins typified by DENND4A/B/C from vertebrates (iteration 2, 105 in a PSI-BLAST search), the membrane-trafficking regulator Crag from Drosophila (iteration 3, 1019), bacterial proteins typified by the MAC/Perforin (MACPF)-like protein plu1415 (PDB: 2QP2; iteration 4, 1010) from Photorhabdus luminescens and uncharacterized proteins from choanoflagellates and stramenopiles (Figs 1 and 2).
In contrast, the C-terminal region produced significant hits only to metazoan proteins.
These included the human ubiquitin-associated protein-1 (UBAP1; e = 103, iteration 3 in PSI-BLAST), which is implicated in nasopharyngeal carcinoma risk and fronto-temporal lobar degeneration (Rollinson et al., 2009; Wu et al., 2009).
Also recovered were several other poorly characterized proteins, including at least one orthologous group of proteins conserved in vertebrates prototyped by the human protein LOC390595 (iteration 3, 105 in PSI-BLAST searches) and another group conserved across Metazoa typified by human tcag7.903 (e = 104, iteration 1478 [16:26 12/5/2010 Bioinformatics-btq235.tex] Page: 1479 14771480 UMA and MABP domains Fig.2.
(A) Domain architectures of UMA and MAPB containing proteins.
(B) Structure of the MAPB domain from P.luminescens plu1415 (PDB: 2QP2).
Conserved residues P and Y of the second strands signature (PXGY, see Fig.1) are represented as spheres.
Only known domains are represented above, with unknown or uncharacterized regions omitted for simplicity.
See text for domain name abbreviations.
6).
These findings indicated that the metazoan MVB12 proteins contain two distinct conserved domains that occur independently in various proteins (Figs 1 and 2; the MVB12 orthologs are currently grouped as a single-domain model DUF2464 in the PFAM database, which does not detect the other homologous proteins identified in the current study).
Furthermore, searches with the N-terminal domain of MVB12A/B and the equivalent domain in the DENND4A/B/C and Crag indicated that it has an internal repeat structure of three homologous segments.
Consistent with this, the structurally characterized representative, Photorhabdus plu1415, showed that this region precisely corresponds to a type-I-prism domain with an internal 3-fold symmetry (Rosado et al., 2007).
Each of the three subdomains of the-prism structure is a distinctive three-stranded-sheet (Fig.2B) that was congruent to the repeat units detected in the sequence searches (Fig.1B).
This domain shares a triradial symmetry with-sheets parallel to the prism axis as in the type-I-prism domains observed in the vitelline membrane outer layer protein I (VMO-I) and the Bacillus thuringiensis-endotoxin (Shimizu and Morikawa, 1996).
However, the topology of the strands in the-sheet of the individual subdomains of the Photorhabdus plu1415-prism is entirely different (Fig.2B).
We named this novel domain the MVB12-Associated-prism (MABP).
A multiple alignment of the MABP domain showed that majority of the eukaryotic versions contains a conserved cysteine in the first and third subdomain of the-prism (Fig.1B).
We named the C-terminal domain of MVB12A/B domain, which is shared with UBAP1 as the UBAP1-MVB12-associated (UMA) domain.
A multiple alignment of the UMA domain showed a conserved proline followed by a hydrophobic residue in the N-terminus and a nearly absolutely conserved glutamate at the C-terminus (Fig.1A).
Secondary structure prediction using JPRED suggested that it adopts an +-fold (Fig.1A).
3.2 Domain architectures and functional interactions of MABP and UMA domain proteins To understand the functional significance of the MABP and UMA domains, we systematically determined domain architectures of the proteins which contain them (Fig.2A).
In addition to co-occurring with the UMA domain in MVB12 proteins found in all metazoans, the MABP domain is found independently of it but fused to several other domains: (i) In a group of related proteins typified by Crag and DENND4A/B/C found in metazoans and ciliates, it is present N-terminal to the triad of domains known as uDENN, DENN and dDENN (Levivier et al., 2001).
Additionally, C-terminal to the DENN triad, these proteins have a pentatricopeptide repeat (PPR), a novel Zn-ribbon (ZnR) and an uncharacterized-helical domain.
(ii) Two MABP domains are inserted into the choanoflagellate VPS13 ortholog, which also contains APG2-C and Dysferlin (DysF) domains.
(iii) Stand-alone MABP domains are found in certain fungi.
(iv) In stramenopiles, several architectures are observed including fusions to peptide-N-glycanase-type transglutaminase and PUG domains (Phytophthora infestans PITG_02329), to 8 EF-HANDs (EFh) and two Ub-binding ZnR domains (zUb in Fig.2A, P.infestans PITG_06630) and to a Sec7 domain (Phaeodactylum PHATRDRAFT_49198).
Two MABP domains are also found inserted into a deubiquitinating peptidase (DUB) domain in another P.infestans protein (PITG_02561; it also contains six N-terminal zUb Ub-binding ZnRs).
(v) In bacteria, the MABP domain occurs as a solo (e.g.
Frankia FRAAL0413), fused to the C-terminus of a MACPF domain (e.g.
plu1415) or at the N-terminus of a protein with two types of-helix repeats (bH1/2) and a novel cysteine-containing domain (CCD) that are typical of cell-wall proteins (e.g.
Clostridium CLOL250_02048; Fig.2A and Supplementary Material).
In eukaryotes, several of the fused domains have been implicated in trafficking machinery: the DENN domain is a Rab GEF that is required for Rab35-mediated recycling of endosomal proteins and trafficking of surface proteins to the apical membrane (Allaire et al., 2010).
VPS13 and APG2-C domains have been implicated in protein cycling through the trans-Golgi network and formation of vesicles targeted for autophagy (Rampoldi et al., 2001).
The other fusions are to DUBs and deglycanases that are also involved in the sorting of cargo in the trafficking process (Raiborg and Stenmark, 2009; Yoshida and Tanaka, 2010).
In particular, the Ub-binding ZnRs associated with the MABP domain in at least two proteins have been found to bind monoubiquitin, a key trafficking signal (Raiborg and Stenmark, 2009).
MABP domain-containing Drosophila Crag protein localizes to endosomal vesicle and plasma membranes (Denef et al., 2008).
Likewise, bacterial proteins with MABP-MACPF domains have been suggested to target membranes (Rosado et al., 2007).
Vertebrate MACPF proteins contain a fusion to the lipid-binding C2 in place of the MABP domain.
These contextual connections suggest that the MABP domain has a membrane-associated function, perhaps even specific interactions with membrane components.
The structure of the MABP domain in plu1415 reveals several exposed hydrophobic residues that are 1479 [16:26 12/5/2010 Bioinformatics-btq235.tex] Page: 1480 14771480 R.F.de Souza and L.Aravind consistent with such an interaction.
Hence, it is plausible that the eukaryotic MABP domains are adaptors that help linking other associated domains found in the same polypeptide to vesicular membranes.
In MVB12, the region including the UMA domain, but not the MABP domain, has been shown to interact with the N-terminal part of VPS37 and the C-terminal part of TSG101, both ESCRT-I components (Morita et al., 2007).
This suggests that the UMA domain probably specifically recruits MVB12 to the ESCRT-I complex to form a quaternary complex.
In UBAP1 and LOC390595, the UMA domain is fused to three C-terminal UBA domains, which are known to bind ubiquitin (Raiborg and Stenmark, 2009).
Hence, they could interact via the UBA domains with ubiquitinated tails of membrane proteins, while their UMA domains recruit them to the core ESCRT-I complex.
The remaining UMA domain proteins (e.g.
tcag7.903 group; Fig.2A) have their own conserved N-terminal extensions that could potentially interact with specific protein partners.
Based on these observations, we propose that the different UMA domain proteins might function as alternative MVB12-like subunits that recruit different targets via their specific interaction modules (such as MABP or UBA or the specific extensions) to the ESCRT-I complex.
Thus, different types of UMA domains are likely to be required for downregulation of different sets of receptors in animals.
4 GENERAL CONCLUSIONS Identification of the MABP and UMA domains throws light on two vital aspects of vesicular trafficking.
First, the MABP domain could be a common denominator in the recognition of specific membrane-associated features by a functionally diverse set of trafficking proteins in eukaryotes and bacterial proteins involved in pore formation and cell-wall interaction.
The prediction that the diverse metazoan UMA domain proteins are alternative MVB12-like proteins implies that the recruitment of ESCRT-I to endosomal structures could occur via diverse mechanisms, including the possible direct recognition of membranes by the MABP domain, interaction with ubiquitinated peptides or other protein protein interactions.
This could have been a response to the vast expansion of diverse signaling receptors such as receptor tyrosine kinases, ion channels and 7TM receptors in the metazoan lineage.
Intriguingly, we found that plants (e.g.
Arabidopsis AT5G53330) have a conserved protein that has a series of C-terminal UBA domains closely related to those found in UBAP1.
While we failed to find statistically significant similarity between the N-terminal region of these plant proteins and the UMA domain, they share a few tantalizing sequence patterns.
It cannot be ruled out that these plant proteins contain a region remotely related to the UMA domain and perform a comparable function in relation with the ESCRT system.
While certain core components of this system (e.g.
VPS4 and MIT domains of ESCRT-III) have been traced to archaea (Hobel et al., 2008), the MABP domain is not currently found in any archaea.
Instead it is found in diverse bacteria, suggesting that the eukaryotes could have acquired it early in their evolution from a bacterial precursor.
Thus, the eukaryotic vesicular trafficking system appears to have been pieced together from different components acquired from both archaeal and bacterial precursors.
Funding: Intramural funds of the National Library of Medicine, National Institutes of Health, USA.
Conflict of Interest: none declared.
ABSTRACT Motivation: Discovering the transcriptional regulatory architecture of the metabolism has been an important topic to understand the impli-cations of transcriptional fluctuations on metabolism.
The reporter algorithm (RA) was proposed to determine the hot spots in metabolic networks, around which transcriptional regulation is focused owing to a disease or a genetic perturbation.
Using a z-score-based scoring scheme, RA calculates the average statistical change in the expres-sion levels of genes that are neighbors to a target metabolite in the metabolic network.
The RA approach has been used in numerous studies to analyze cellular responses to the downstream genetic changes.
In this article, we propose a mutual information-based multi-variate reporter algorithm (MIRA) with the goal of eliminating the fol-lowing problems in detecting reporter metabolites: (i) conventional statistical methods suffer from small sample sizes, (ii) as z-score ranges from minus to plus infinity, calculating average scores can lead to canceling out opposite effects and (iii) analyzing genes one by one, then aggregating results can lead to information loss.
MIRA is a multivariate and combinatorial algorithm that calculates the aggre-gate transcriptional response around a metabolite using mutual infor-mation.
We show that MIRAs results are biologically sound, empirically significant and more reliable than RA.
Results: We apply MIRA to gene expression analysis of six knockout strains of Escherichia coli and show that MIRA captures the underlying metabolic dynamics of the switch from aerobic to anaerobic respir-ation.
We also apply MIRA to an Autism Spectrum Disorder gene ex-pression dataset.
Results indicate that MIRA reports metabolites that highly overlap with recently found metabolic biomarkers in the autism literature.
Overall, MIRA is a promising algorithm for detecting meta-bolic drug targets and understanding the relation between gene expression and metabolic activity.
Availability and implementation: The code is implemented in C# language using .NET framework.
Project is available upon request.
Contact: cicek@cs.cmu.edu Supplementary information: Supplementary data are available at Bioinformatics online 1 INTRODUCTION Changes with respect to environmental or genetic modifications lead to complex cellular responses.
Standing on the top of the omics hierarchy, metabolomics reflects changes taking place in the transcriptome and in the genome.
These responses are ana-lyzed by researchers to discover regulatory mechanisms and dy-namics of cells.
Transcriptional responses of the cells, along with the corresponding metabolic alterations, have been investigated in various contexts.
Some examples are plant research (Brosche et al., 2005; Carari et al., 2006), diabetes (Ferrara et al., 2008; Zelezniak et al., 2010), insulin resistance (Jans et al., 2011) and cancer (Schramm et al., 2010).
Functional class-based (Gerstein and Jansen, 2000; Hughes et al., 2000; Karp et al., 2002; Pavlidis et al., 2002; Seshasayee et al., 2009) and proteinprotein inter-action network-based analysis of gene expression data (Chowdhury et al., 2010; Chuang et al., 2007; Goh et al., 2007; Ideker et al., 2002; Rhodes and Chinnaiyan, 2005) have been well established.
Metabolic network-and metabolic pathway-driven analysis of transcriptional data have been receiving attention lately.
Efforts have centered on discovering transcriptional regu-lation architecture of metabolic networks of organisms using genome-wide association studies (David et al., 2006; Ihmels et al., 2004; Kharchenko et al., 2005; Tanay et al., 2004).
Various methods with different goals have been developed to use transcriptomic and metabolic data together in the context of a metabolic network such as (i) mining for new metabolite-gene/transcription factor relationships (Ideker et al., 2001; Yeang et al., 2006), (ii) flux balance analysis and constraint-based mod-eling of organisms (Covert and Palsson, 2002a, b; Shlomi et al., 2008) and (iii) using metabolic network topology to identify sig-nificant changes in related groups of genes (Cakir et al., 2006; Deo et al., 2010; Dinu et al., 2007; Hancock et al., 2012; Nam et al., 2009; Oliveira et al., 2008; Patil and Nielsen, 2005; Subramanian et al., 2005; Ulitsky and Shamir, 2009).
Network topology-based analysis of biological data is a broad research area (Maayan, 2008).
In the context of metabolic net-works and transcriptomics, the literature so far can be divided into two subcategories.
The first type of analysis uses predefined metabolic pathways as targets for transcriptional regulation and analyzes the changes in the pathways.
Gene Set Enrichment Analysis (GSEA) is the first and most established analysis in this subcategory (Subramanian et al., 2005).
Improvements have been proposed in the literature to eliminate the shortcom-ings of the GSEA approach (Dinu et al., 2007; Draghici et al., 2007; Hancock et al., 2012).
The second type of analysis con-siders the metabolic network as a whole, and aims to find signa-tures or hot spots in the metabolic network that are subject to transcriptional regulation (Cakir et al., 2006; Nam et al., 2009; Patil and Nielsen, 2005; Schramm et al., 2010).
The most estab-lished method in this group is the reporter algorithm (RA; Patil and Nielsen, 2005).
Using the z-score-based method introduced before (Ideker et al., 2002), the algorithm aims to find metabol-ites around which transcriptional regulation is centered and link the complex transcriptional motives to metabolome.
Various*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com mailto:cicek@cs.cmu.edu; Ideker etal., 2002; Chuang etal., 2007 ; David etal., 2006 tiliz 1 2 ; Covert and Palsson, 2002b; 3 ; Subramanian etal., 2005; Dinu etal., 2007; Hancock etal., 2012; Patil and Nielsen, 2005; &Ccedil;akir etal., 2006; Deo etal., 2010; Nam etal., 2009; Oliveira etal., 2008 , ; Draghici etal., 2007 ) XPath error Undefined namespace prefix XPath error Undefined namespace prefix extensions and modifications to the algorithm have been pub-lished in the literature.
For instance, same idea has been used to discover reporter reactions (Cakir et al., 2006).
A similar z-score-based approach has also been used analyze the rate-limiting steps of pathways (Nam et al., 2009).
In this article, we focus on the original RA (Patil and Nielsen, 2005) and its scoring mechanism (Ideker et al., 2002).
Our observations, discussed next, also apply to the extensions of the RA method.
RA first maps the differential data onto the enzymes (reac-tions) in the metabolic network, and then calculates the P-values for each gene, using students t-test.
P-values are then converted to z-scores using the inverse normal cumulative distribution (1).
Equation (1) shows how each pi is converted to the cor-responding z-score zi.
Given all samples, z-score measures how many standard deviations away a P-value is.
It ranges from negative to positive infinity, where negative infinity corresponds to no significance at all.
zi= 11 pi 1 The z-score zm for a metabolitem is the aggregation of z-scores of the k enzymes that are neighbors of m in the metabolic net-work (through metabolic reactions), and calculated as shown in Equation (2).
The null hypothesis is that the genes adjacent to a metabolite display their normalized average response by chance.
Should the significance of a metabolite need to be determined, z-score is normalized and converted back into the corresponding P-value.
zm= 1ffiffiffi k p P zi such that enzyme i is a neighbor of metabolite m 2 Although RA has been shown to be effective in several different contexts such as analysis of Type 2 Diabetes (Zelezniak et al., 2010), genome scale analysis of organisms (David et al., 2008; Usaite et al., 2009), genome scale analysis of cell lines (Agren et al., 2012), gene knockout analysis (Holm et al., 2010; Cimini et al., 2009), targeted pathway analysis (Vongsangnak et al., 2009), there are some shortcomings that may affect the accuracy of the algorithm.
First, the z-score method uses students t-test to measure the amount of change between two variables.
However, the resulting P-value is highly dependent on the degrees of free-dom.
It has been shown that, owing to the small number of samples in cohorts, P-values may not work as intended (Cicek et al., 2013).
Second, RA uses a univariate approach.
That is, it determines the changes per gene, and does not take dependencies among genes into account.
For a reaction associated with mul-tiple genes, the gene with the highest z-score is used, and the rest are discarded (Zelezniak et al., 2010).
In Figure 1A, we illustrate the problem via an example.
The figure shows an example from the application of the RA algo-rithm to compare "arcA"fnr and wild-type (WT) strains of Escherichia coli (aerobic) in the gene expression dataset (Covert et al., 2004) (please see Section 2 for details).
Genes (pentagons) are assigned z-scores first, and then the maximum z-score is se-lected and assigned as the z-score of the reaction (rectangles).
For the reaction copragon transport via ton system, the max-imum z-score belongs to the gene fhuE (0.88).
This scoring ignores the contribution of the gene tonB, as any z-score value for tonB in the range [Infinity, 0.88) would yield the same z-score for the reaction.
Finally, the method is additive in aggregating z-scores of each neighboring reaction to determine the z-score of ametabolite [as shown in Equation (2)].
However, z-score ranges from nega-tive to positive infinity, negative infinity representing the most insignificant case.
Therefore, averaging individual results intro-duces the problem of opposite signs cancelling each other out.
In Figure 1A, copragon is assigned a z-score: 1ffiffi 2 p 0:1+0:88=0:54.
Negative z-score (0.1) assigned to the reaction copragon trans-port via ABC system partially cancels out the z-score of the reac-tion copragon transport via ton system (0.88).
The problem would have been resolved if the scoring mechanism used, assigned zero to the most insignificant case (P=1).
In this article, we present a new algorithm, called Mutual Information-based Reporter Algorithm (MIRA) that addresses the shortcomings of the original RA.
MIRA is a multivariate and combinatorial algorithm that calculates the aggregate tran-scriptional response around a metabolite using mutual A B Fig.1.
Application of RA in comparison of "arcA"fnr and WT strains of E.coli (aerobic).
Rectangles represent reactions, pentagons represent genes and the circle represents the metabolite, copragon.
(A) Reactions transfer copragon from extracellular space to periplasm, and then to cytoplasm.
The maximum change (z-score) for the genes of copragon transport via ABC system is 0.1, and the genes of copragon transport via ton system is 0.88.
Aggregate z-score for the metabolite coprogen is assigned as 0.54.
(B) When genes of copragon transport via ABC system are considered together they return MI of 0.792, and MI for the genes of copragon transport via ton system is 0.808.
Average turns out to be 0.8, which is a relatively high mutual information value.
Result is different than the prediction made by RA i176 A.E.Cicek et al.
employed paper reporter algorithm &Unicode_x1D703; <inlinemediaobject><imageobject><imagedata fileref= : <inlinemediaobject><imageobject><imagedata fileref= IG(IG)-due employs .
Methods-( IG(IG)-paper MIRA , reporter algorithm  information.
Mutual Information is an information theoretic method that measures how much knowing one variable reduces the uncertainty about the other.
In gene expression analysis, it has been used frequently (Butte et al., 2000; Chowdhury et al., 2010; Gupta et al., 2010; Steuer et al., 2002; Zhang et al., 2010).
Chowdhury et al.
show that combinatorially dysregulated subnet-works found through mutual information is predictive for cancer.
In metabolomics analysis, using the combinations of metabolites and their levels, ADEMA(the algorithm for determining expected metabolite alterations) predicts expected changes of metabolite levels in a given metabolic network using mutual information (Cicek et al., 2013).
In the context of MIRA, the two variables are (i) the genotype (e.g., case and control) and (ii) the combin-ations of the genes associated with a reaction and their expression levels.
More specifically, MIRA performs the following tasks: (i) discretize gene expression levels using B-spline functions, (ii) cal-culate the mutual information between the genotype, as well as combinations of genes associated in a reaction and their discre-tized expression levels, and (iii) calculate the average mutual in-formation for each metabolite using the mutual information of each neighboring reaction.
Figure 2 shows the overall flow of the algorithm.
The advantages of MIRA over RA are as follows: (1) Combinatorial mutual information works well when the sample sizes are small, and performs better than univariate significance testing.
(2) Unlike the RA, MIRA uses a multivariate method, ana-lyzing multiple genes at a time.
Therefore, it does not dis-card less insignificant changes unlike RA.
MIRA uses measurements of individual samples instead of comparing sample means and is able to capture linear and non-linear dependencies among variables.
(3) Mutual information is bounded by zero and the minimum of the entropies of the two random variables.
The most insignificant case is assigned the score zero; therefore, in-significant changes do not cancel out significant changes.
(4) MIRA has no bias toward highly connected metabolites, as it normalizes the sum of changes around a metabolite using the number of reactions instead of the square root of number of reactions as RA does [see Equations (2) and (8)].
Figure 1B shows the results obtained by MIRA for the intro-ductory example shown in Figure 1A.
Unlike the RA, which assigns a low score to the reaction copragon transport via ABC system indicating that there is no significant change on this enzyme, MIRA predicts relatively high mutual information indi-cating that when considered together genes fhuB, fhuC and fhuD are expressed differently.
MIRA predicts similar results for both reactions and the average is found as 0.8 (max 0.98 in this test), whereas the RA assigns 0.54 to copragon (max 13 in this test).
The difference between the two algorithms stems from the fact that (i) MIRA performs a multivariate analysis compared with Fig.2.
Algorithm Flow.
Rectangles represent reactions, pentagons represent genes and circles represent metabolites (darker red represents higher average mutual information).
Algorithm starts by generating gene-reaction and reaction-metabolite associations out of the SBML file of the recon-structed metabolic network.
Next, for each metabolite, gene sets are constructed based on their association with the neighboring reactions.
As the third step, gene expression levels are discretized using B-splines.
Fourth step consists of calculating the mutual information between the class variable and the discretized expression levels of groups of genes.
After each metabolite is assigned average mutual information, based on the calculations done in Step 5, metabolites are ranked based on their average mutual information, and reporter metabolites are determined (darker red means it is a reporter metabolite) i177 MIRA: mutual information-based reporter algorithm for metabolic networks employed ; Gupta etal., 2010; Steuer etal., 2002; Butte etal., 2000 .
1 , 2 1 2 and 3 .
reporter algorithm employs tiliz , s ( reporter algorithm quite reporter algorithm , 1 to  the univariate analysis done by RA, (ii) given that there are only seven samples (3 "arcA"fnr and 4 WT bacteria), mutual information is able to capture the change better than z-scores and (iii) being non-negative, mutual information does not cancel out significant effects.
To evaluate MIRA, we have analyzed six strains of E.coli with knockouts of transcriptional regulators in the oxygen response ("arcA, "appY, "fnr, "oxyR, "soxS and "arcA"fnr) in aerobic and anaerobic conditions (Covert et al., 2004).
We have used the reconstructed metabolic network of E.coli (iAF1260; Feist et al., 2007).
We have also analyzed the autism gene expression dataset (Voineagu et al., 2011) using the Recon 1 genome scale metabolic network for humans (Duarte et al., 2007).
For the E.coli dataset, we focused on the "fnr knockout, which affects the switch be-tween aerobic and anaerobic respiration.
MIRA was able to suc-cessfully capture metabolites that were closely related to this enzyme and anaerobic respiration mechanism.
For the autism dataset, MIRA predicted metabolites that have been recently discovered in the autism literature.
We have also shown that MIRA has no bias toward hub metabolites unlike RA, and scor-ing scheme for MIRA is empirically significant.
2 MATERIALS AND METHODS This section describes subcomponents and techniques MIRA uses to detect the hot spots in the metabolic network.
Supplementary Table S1 describes the abbreviations and notations used throughout the section.
Please see Supplementary Appendix A, Table S1, for a list of terms and variables and their explanations.
2.1 Constructing the network In the context of our algorithm, the network is a hyper-graph G(V,E) where the vertex set V is the union of three entity types: metabolites, genes and reactions.
The edge set E contains two types of edges: (i) edges that connect reactions to the associated genes whose expression lead to the corresponding enzyme and (ii) edges that connect metabolites to asso-ciated reactions based on producer/consumer relationships.
Network in-formation is obtained through the genome-scale reconstructed metabolic network of the organism to be analyzed using an SBML parser.
2.2 Discretizing gene expression data To calculate mutual information between the genotype and the gene ex-pression observations, one needs to calculate the probability of observing that profile.
This is a well-studied problem in the literature (Silverman, 1986).
There are three techniques that do not assume that the values come from a known distribution (which is the case for gene expression data).
Kernel Density Estimation aims to measure the density of observations falling into a predetermined window using a kernel (Moon et al., 1995), though it suffers from the high computational cost, and the results are dependent on the kernel length.
Second technique is the histogram-based classification, which determines thresholds by dividing the domain of the variable into equal-sized chunks and classifying observations based on these thresholds.
Despite the low computational cost, observations that are close to the thresholds are likely to be misclassified, as analytical methods associate an error term with each observation (Cakmak et al., 2012; Cicek and Ozsoyoglu, 2012).
To fix this shortcoming, B-spline functions have been used to associate probabilities with each bin deter-mined by the histogram-based approach (Cicek et al., 2013; Faith et al., 2007).
That is, each observation is associated with a probability to be in a bin, instead of making a binary decision to determine if it is in a given bin or not.
B-spline functions are defined by parameters M and k where M is the number of bins (chunks) that the domain is going to be divided into, and k, 1 kM, is the number of bins an observation can be assigned to (e.g.
k=1 is equivalent to histogram-based binning).
Based on M and k, each B-spline curve i, 1 iM, is assigned a basis vector, the so-called knot vector ti, defined as in Equation (3), and then the curve i is defined as a function of the neighboring curves, as shown in Equations (4) and (5).
ti= 0; i5k i k+1; k i M M k+2; M5i 8>>< >>: 3 Bi;1gn= 1; ti gn5ti+1 0; otherwise ( 4 Bi;kgn=Bi;k1gn gn ti ti+k1 ti  +Bi+1;kgn tik gn ti+k ti+1  5 Each measurement (e.g.
expression level for gene gn for person x) is as-signed to a bin i with probability Bi,k((gn)), where (gn) is a function that normalizes the value of the measurement for gn using the maximum and the minimum values observed for gn in the dataset.
2.3 Calculation of mutual information After expression values are calculated, mutual information is calculated.
Given two random variables X and Y, mutual information I(X;Y)meas-ures how much knowing one reduces the uncertainty about the other.
In the context of MIRA, the first variable is the binary class variable C [e.g.
control versus variable) and the second variable B(G(r)) is the binned measurements of a group of genes G(r) that are associated with a reaction r. For instance, for the example shown in Figure 2B, genes fhuE and tonB are grouped based on their association with the reaction coprogen transport via ton system (ctts).
Assuming we use 2 bins (up and down) then, C= {WT, "arcA"fnr} and B(G(ctts))= {fhuE up & tonB up, fhuE up & tonB down, fhuE down & tonB up, fhuE down & tonB down}.
I(C;B(G(ctts))) is found to be 0.808.
The goal of calculating I(C;B(G(r))) is to learn how much knowing discretized gene expression levels of related genes reduces the uncertainty on the genotype.
In other words, we find out whether a reaction r and its corresponding genes are predictive on the genotype.
If the expression levels of these genes (when considered together) are different with respect to the class variable, then we obtain a high mutual information value.
Equation (6) specifies the mutual information formula.
IC;BGr= X bg2BGr X c2Cpbg; c lg pbg; c pbgpc  6 p(bg) is calculated as shown in Equation (7) and k is a constant input to the algorithm.
That is, given a dataset D, probability of observing g (e.g.
fhuE up and tonB up) is the multiplication of spline values for each gene (e.g.
fhuE and tonB) to be in the corresponding bins (e.g.
up and up in this case), summed and averaged over all individuals in D. This calculation assumes that gene expression values are independent of each other.
p(bg,c) is calculated similarly and p(c) is constant in the dataset.
pbg= X D Y gn2GrBbinofgnbg;kgn jDj 7 We define the aggregate transcriptional regulation around a metabolite m as the average mutual information of the consumer and producer reac-tions of m. Given that R(m) is the set of neighboring reactions, then average mutual information Im for metabolite m is defined as in Equation (8).
i178 A.E.Cicek et al.
2 ( ) , 3 In order )-s ;In order In order ; Cicek etal., 2013 <inlinemediaobject><imageobject><imagedata fileref= , &Unicode_x1D456; &Unicode_x1D440; , ( ) .
if  or not is , &amp; , Im= 1 jRmj X r2RmIC;BGr 8 MIRA fits a beta distribution to Im values given the interval.
Then, ImBeta(, ) where and are learned from the sample.
Finally, all metabolites with P50.05 are picked as reporter metabolites.
2.4 Datasets and experimental design To test the performance of MIRA and compare it with RA, we con-sidered two resources.
First, we used mRNA expression profiles of six E.coli strains with knockouts of transcriptional regulators of the oxygen response ("arcA, "appY, "fnr, "oxyR, "soxS and "arcA"fnr) pub-lished and released in Covert et al.
(2004).
For each strain they obtained measurements in aerobic and anaerobic conditions.
Consequently, we used 12 datasets for the knockouts and 12 for the WT.
We compared the expression profiles of selected knockouts against the control, to obtain reporter metabolites using each respective method.
We did not perform cross-condition comparisons.
For instance, we compared knock-out measurements under aerobic conditions with WT measurements under aerobic conditions only.
Second, we ran tests on the autism spec-trum disorder (ASD) brain gene expression dataset (Voineagu et al., 2011).
The dataset contains gene expression levels for 8858 genes for 58 human cortex samples (29 ASD and 29 controls).
We implemented MIRA in C# language using ET Framework 4.0.
Tests were run on a Dell PowerEdge R710 Server with two Intel Xeon quad processors and 48 GB main memory.
The server runs on Windows Server 2008 operating system.
For E.coli knockout tests, we used the genome scale-reconstructed metabolic network model of E.coli, iAF1260 (Feist et al., 2007).
The model contains 1972 metabolites, 2382 reactions, 1261 genes and 3 com-partments.
We mapped the measured genes to the corresponding reac-tions in the metabolic network, as annotated in the model.
After the mapping, we obtained 1643 metabolites, to which there was at least one reaction associated with a gene measured in the dataset.
Only the obtained 1643 metabolites were considered in the tests.
For the Autism dataset, we used the Recon1 genome-scale metabolic network for humans (Duarte et al., 2007).
The model consists of 3188 metabolites, 3742 reac-tions, 1499 genes and 8 compartments.
Mapping the genes measured to the network resulted in a metabolite set of size 2331 for further consid-eration.
For both datasets, we used M=6 and k=4 to discretize meas-urements using B-splines.
Please see Supplementary Appendix B for time requirements.
3 RESULTS 3.1 Comparing reporter metabolite sets of MIRA and RA To compare the performances of MIRA and RA, we obtained reporter metabolites using both algorithms.
First, we investigated how similar two sets are using the Jaccard distance (JD).
JD is complementary to the Jaccard index, which is the ratio of shared items between the two sets, A and B, and the union of the items in two sets.
Jaccard index is denoted as J(A,B).
JD, J(A,B), is equivalent to 1 J(A,B).
Two algorithms yield different sets of reporter metabolites for E.coli knockouts.
For the knockouts "appY (anaerobic) and "OxyR (aerobic), the sets of reporter metabolites are totally distinct (JD=1) and the smallest JD obtained in these tests is 0.85.
For the Autism data-set, JD is 0.9.
3.2 Robustness against hub metabolites Metabolic networks are known to be scale free (Albert, 2005), that is, their degree distribution follows the power law.
Our re-sults show that the reporter metabolites found by RA are usually highly connected metabolites known as hubs or common metab-olites, which are rare in scale-free networks.
For instance, in the test for "fnr (aerobic), RA marks H+, H2O, ATP, ADP, Phosphate, Diphosphate and CO2 as the top 7 reporter metab-olites, which participate in many reactions and are associated with many genes in the metabolic network.
To test if RA has a bias toward hub metabolites, we calculated the average gene connectivity and reaction connectivity of the reporter metabolites for each algorithm.
We also formed a random set of metabolites where the size of the random set is randomly chosen as a number between the sizes of reporter sets produced by MIRA and RA.
We repeated the random set selection 10 000 times and found the average connectivity for the random set.
Figure 3A shows the average gene connectivity, and Figure 3B shows the average re-action connectivity of the reporter metabolites for MIRA, RA and the random set for E.coli knockout tests.
Results show that RA favors highly connected metabolites, whereas reporter me-tabolites found by MIRA have a closer degree distribution to the random set, and does not have such a bias.
This also applies to reporter metabolites found in the Autism dataset by RA (Supplementary Table S5).
The intuitive reason for this differ-ence is that MIRA averages the mutual information found for each reaction, whereas RA divides the sum by the square root of the number of reactions around a metabolite.
3.3 Interpreting reporter metabolites for "fnr (anaerobic) dataset When E.coli has no O2 as the final electron acceptor, it switches to anaerobic respiration and uses electron donating dehydrogen-ases and accepting reductases on the membrane.
Fumarate Nitrate Reductase (fnr) is a transcriptional regulator that regu-lates 100+ genes and nitrate/fumarate reduction in response to the switch from aerobic to anaerobic respiration in E.coli.
Fnr has an iron-sulfur cluster [4Fe4S] that senses the presence of oxygen and becomes inactivated when oxidized in the presence of oxygen.
It can also be converted into a disulfide form by glutathione or thioredoxin when inactive (Daruwala and Meganathan, 1991).
Figure 4 shows a simplified depiction of the dynamics in anaerobic respiration with respect to fnr (Keseler et al., 2013; MetaCyc; Unden and Bongaerts, 1997; UniProt).
Although there are many types of dehydrogenases and reductases, we draw them in two groups for the sake of simplicity: (i) hydrophilic side toward periplasm and (ii) hydrophilic side toward cytoplasm.
Electron donors like G3P, formate, lactate, NADH, H2 are oxi-dized by the hydrogenases, and electrons are transported using menaquinone to the reductases.
Focusing on nitrate reductase, this electron is transferred to protoheme, then to Fe-S cluster and, finally, to molybdenum (Mo).
It is used to reduce nitrate to nitrite.
Fnr stimulates the expression of this enzyme.
In the presence of O2, fnr is oxidized and inactivated.
As stated above, glutathione and thioredoxin act as electron donors to activate fnr.
In Supplementary Appendix C, Table S2 lists the reporter me-tabolites found by MIRA and Table S3 lists reporter metabolites i179 MIRA: mutual information-based reporter algorithm for metabolic networks In order to wild-type wild-type .
, , , , , , , , ,accard distance , Jaccard distance ( )-very-In order--,--please see--, , 1 s 2 s ,in Appendix C  found by RA based on "fnr knockout under anaerobic condi-tion.
Previous description is based on the data obtained from UniProt Database, which summarizes the key concepts and me-tabolites related to fnr.
The metabolites reported by MIRA highly overlap with this definition.
The second reporter metab-olite protoheme (heme b) is an important metabolite in the heme-biosynthesis pathway, and is the first electron acceptor in nitrate reductase complex (Metacyc).
As mentioned above, glutathione and thioredoxin are key agents to convert the enzyme, and MIRA detects them as reporter metabolites [glutaredoxin (reduced/oxidized) and thioredoxin (reduced/oxidized)].
Nitrite is one of the direct products of nitrate reductase, and it is also reported as a reporter metabolite.
Aside from the metabolites in UniProts definition, MIRA found dimethyl sulfide/sulfoxide (DMSO) and trimethylamine/ trimethylamine n-oxide (TMAO) as reporter metabolites.
As Figure 4 shows, E.coli uses N-and S-oxides as the terminal electron acceptors (Daruwala and Meganathan, 1991).
When RA is considered, the top metabolite picked by MIRA is not a reporter metabolite in RAs list.
Tungstate is known as a direct inhibitor of nitrate reductase as well as TMAO and DMSO reductases, which are also reporter metabolites (Prins et al., 1980).
Similarly, tripeptide murein units [short name for two linked disacharide tripeptide murein units (uncrosslinked middle of chain)] is also not picked by RA and might seem un-related at first.
Murein units constitute bacterial cell walls.
Membrane-bound lytic murein transglycosylase is the enzyme that degrades mureins, and the gene responsible to transcribe this enzyme is dniR (mltD).
A mutant of this enzyme is known to be defective in producing nitrite reductase.
Nitrate and nitrite also stimulate the expression of dniR (Kajie et al., 1991).
Most striking difference in the reporter metabolites found by RA is that the first seven metabolites are H+, H2O, ADP, P, ATP, NAD, NADH (NADP, NADPH, O2, CDP, UDP, dADP, dCDP, GDP, GTP, CoA are also listed).
These are highly connected metabolites, which take place in many biochemical activities in the cell, and therefore, it is hard to link them to the effect of the knockout in this test.
MIRA puts these metab-olites within (695 964] in the ranking of 1643 metabolites con-sidered.
Also RA lists many metabolites from the central metabolism: Glycerophosphoglycerol, pyruvate, glycerol, F6P, succinate d-glucose and acetyl-CoA.
Similarly, these metabolites can be considered as general owing to the diverse functionality of the pathway they are in.
Having that said, RA detects some metabolites that are not picked by MIRA and are relevant.
For instance, sulfate and molybdate are incorporated into fnr and nitrate reductase (Tavares et al., 2006).
Menaquinone 8, menaquinol 8 and 2-demethylmenaquinol 8, link dehydrogenases and reductases/oxidases in the electron transport chains, and hence are directly related to fnr enzyme as shown in Figure 4 (Unden and Bongaerts, 1997).
Although ubiquinone-8/ubiqui-nol-8 play an important role in aerobic respiration, they are listed higher than menaquinone 8/menaquinol 8.
In conclusion, the results suggest that (i) MIRA has no bias toward hub metabolites, and successfully downplays their im-portance, and (ii) MIRA yields reporter metabolites, which are in close proximity to the enzyme and are relevant with respect to the literature.
3.4 Interpreting reporter metabolites for the autism dataset ASD is a developmental genetic disorder that causes social inter-action abnormalities, communication deficiencies and repetitive behavior.
Although the disease has a genetic origin, metabolic implications have been studied widely in the literature (Boccuto et al., 2013; Emond et al., 2013; Yap et al., 2010).
Running MIRA on the gene expression dataset provided by Voineagu et al., 2011, has resulted in 52 reporter metabolites as shown in Supplementary Appendix C, Table S4.
Reporter metabolites found by RA is listed in Supplementary Appendix C, Table S5.
A B Fig.3.
Application of Average gene and reaction connectivity of the reporter metabolites.
Panel (A) shows average number of genes associated with the reporter metabolites found by RA and MIRA in E.coli knockout tests.
Random set reports the average number of genes connected randomly chosen metabolites of the same size as the original sets (repeated 10 000 times and averaged).
Panel (B) shows the same results for the average number of reactions associated with metabolites i180 A.E.Cicek et al.
( ) (DMS) (TMA) ( ) , , , due a s b Autism spectrum disorder ( ) ; Boccuto etal., 2013 Table S5 inThe first and the eighth metabolites located as reporter by MIRA are protein-linked serine/threonine residue and protein-linked asparagine residue at glycosylation sites.
Glycosylation is a process that attaches glycans to proteins as a post-trans-lational modification in the secretion pathway.
Secretion path-way is known as an important factor in brain development, and DIA1R mutation leads to ASD and mental retardation (Aziz et al., 2011).
More specifically, it is reported in the literature that 7 glycosylation-related genes are affected by copy number variations (CNVs) in autism patients (van der Zwaag et al., 2009; Pinto et al., 2010).
We also observe glycans such as glycophosphatidylinositol later in the reporter list.
Glucuronidation is an important process for detoxification of most xenobiotics by making such components more water-soluble and less toxic by attaching glucuronate to the substrates (Stein et al., 2011).
The second metabolite d-glucorate and the third metabolite d-glucurono-6,3-lactone (d-glucurone), located by MIRA, are direct precursors of glucuronate.
Stein et al.
(2011) reports evidence on lower glucuronidation levels in chil-dren ASD, which may explain d-glucurono-6,3-lactone being a stress point in the metabolic network.
L-Arabinose is also in this pathway and is located as a reporter by MIRA.
Histone n6-methyl-l-lysine, protein n6,n6-dimethyl-l-lysine, peptdyl-l-lysine and protein n6,n6,n6-trimethyl-l-lysine, all located by MIRA, belong to lysine degradation pathway.
These metabolites are centered on the path that consumes lysine and produces carnitine.
Celestino-Soper et al.
have revealed that dysregulation of carnitine metabolism may be im-portant in non-dysmorphic autism.
The results show that a de-letion in TMLHE gene on this pathway has a significant correlation with ASD (Celestino-Soper et al., 2012).
In a recent work, Frye et al.
(2013) links the abnormalities in acyl-carnitine levels and autism.
Lipoylprotein, lipoamide, dihydrolipolprotein and dihydroli-poamide, all located by MIRA, are four metabolites that are in Fig.4.
Anaerobic respiration of E.coli with respect to fnr and reporter metabolites found by MIRA.
Anaerobic respiration of E.coli couples electron donors to electron acceptors via dehydrogenases and reductases on the inner membrane.
There are many types of dehydrogenases and reductases; however, only two types are shown: hydrophilic side toward cytosol and toward periplasm.
Sizes and shapes of the proteins are not drawn to scale.
Only nitrate reductase is shown in more detail and separately.
Formate, lactate, NADH, H2 G3P are electron donors and lead to reduction of acceptors like nitrate, nitrite, DMSO, TMSO and fumarate.
Menanquinone acts as a mediator between dehydrogenases and reductases.
In the case of nitrate reductase, electron is transferred through protoheme, Fe-S cluster and Molybdenum to reduce nitrate to nitrate.
Fnr activates this enzyme, and tungstate is a well-known inhibitor.
Fnr is inactivated by oxygen and can be reactivated by agents like glutaredoxin and thioredoxin.
Murein units constitute the cell wall, and the enzyme that degrades murein units is transcribed by dniR gene.
It is known that dniR regulates nitrite reductase and it is stimulated but nitrite and nitrite i181 MIRA: mutual information-based reporter algorithm for metabolic networks (Stein etal., 2011) s very  (Frye etal., 2013) , the glycine cleavage pathway.
MIRA reports four metabolites out of six in this pathway and points to an alteration in this process.
A recent work by Yu et al.
(2013) confirms that a mutation in AMT gene is also associated with ASD.
This muta-tion leads to a deficiency in glycine cleavage system.
In addition to this, the metabolic profiling done by Yap et al.
(2010) shows significant differences in glycine levels in the urine of autistic children.
Starting with stearidonyl coenzyme A, MIRA detects 17 inter-mediates of fatty acid synthesis pathway as reporter metabolites.
Fatty acid metabolism and autism have been associated in the literature.
Richardson and Ross point to the growing evidence on the relation between neurodegenerative diseases and fatty acid abnormalities (Richardson and Ross, 2000).
Among more recent works, Tamiji and Crawford state that children with autism show higher rates of lipid metabolism than controls (Tamiji and Crawford, 2010).
El-Ansary et al.
(2011) report increase in most of the saturated fatty acids in a cohort of 52 autism patients.
In comparison, none of the aforementioned reporter metabol-ites are located by RA, but common metabolites are highly ranked as in E.coli tests.
In summary, reporter metabolites picked by MIRA for autism are backed by literature.
Results show that, although some pre-dictions (e.g.
glycine cleavage deficiency) are not obvious targets, the MIRA method was able to predict them.
The literature on the relation between autism and (i) glucuronation (Stein et al., 2011), (ii) lysine degradation and carnitine metabolism (Celestino-Soper et al., 2012; Frye et al., 2013) and (iii) glycine cleavage (Yu et al., 2013) did not exist at the time of the gene expression dataset was published.
Hence, MIRA shows promis-ing prospect for discovering new metabolic targets.
3.5 Empirically testing the significance of scoring schemes used by MIRA and RA To assess the significance of the scores calculated by both algo-rithms and assess the reliability of the rankings, we used an empirical significance testing using the following method.
We (i) shuffled the labels of the individuals in the autism dataset 100 times to obtain 100 random datasets, (ii) ran MIRA and RA on these datasets, as well as on the original data, and (iii) sorted and plotted the scores in descending order for all 101 instances.
Figure 5A shows the scores for RA, and Figure 5B shows the results for MIRA.
Big red circles represent the results found on the original dataset.
Figures 5C and 5D show a close-up for the first 50 metabolites in the ranking.
MIRAs results for the original dataset dominate the random curves and, hence, suggest an empirically significant result.
On the other hand, RAs output for original data follows a similar pattern with the random datasets.
4 CONCLUSION Metabolic networks have received significant attention in the past decade.
This advancement has led to the investigation of various genetic diseases and their metabolism with the use of the reconstructed genome scale metabolic networks.
One application is to find the regulatory architecture of the metabolic network using the underlying transcriptome.
The RA finds the metabolic hot spots around which transcriptional regulation is centered.
In this article, we developed a novel method, called MIRA, that uses a combinatorial approach and mutual information to find reporter metabolites.
Our approach addresses the shortcomings of the existing RA algorithm.
More specifically, it is robust against small sample sizes, uses a multivariate approach instead of a univariate one and does not cancel out significant changes in expression levels with non-significant ones.
Our results show that (i) MIRA has no bias on picking hub metabolites as reporter metabolites, (ii) reporter metabolites found by MIRA are bio-logically sound and are supported by literature even for a com-plex disease like Autism, (iii) MIRA captures the effects of a knockout of the Fnr gene in E.coli successfully and (iv) MIRA provides empirically significant results, which supports the fact that it captures the underlying biological phenomenon.
Fig.5.
Empirical testing of the significance of the scoring schemes.
Panel (A) shows series of z-scores obtained for each random set, and for the original dataset (shown as the red line with large circles) by RA.
Scores are sorted in descending order.
Panel (B) shows the Ims for the same data obtained by MIRA.
Panels (C) and (D) show close-ups for the first 50 metabolites for Panels (A) and (B), respectively i182 A.E.Cicek et al.
4 6 , (Yu etal., 2013) (Yap etal., 2010) s s (El-Ansary etal., 2011) bove-, 1 2 , 3 In order employed 1 2 , 3 s , s l reporter algorithm paper employs , 1 2 3 , 4 Funding: This research has been supported by the National Science Foundation grants DBI 0743705, DBI 0849956, CRI 0551603 and by the National Institute of Health grant GM088823.
A. Ercument Cicek has also been supported by Ray and Stephanie Lane Fellowship.
Conflict of Interest: none declared.
ABSTRACT Motivation: Programs that evaluate the quality of a protein structural model are important both for validating the structure determination procedure and for guiding the model-building process.
Such programs are based on properties of native structures that are generally not expected for faulty models.
One such property, which is rarely used for automatic structure quality assessment, is the tendency for conserved residues to be located at the structural core and for variable residues to be located at the surface.
Results: We present ConQuass, a novel quality assessment program based on the consistency between the model structure and the proteins conservation pattern.
We show that it can identify problematic structural models, and that the scores it assigns to the server models in CASP8 correlate with the similarity of the models to the native structure.
We also show that when the conservation information is reliable, the methods performance is comparable and complementary to that of the other single-structure quality assessment methods that participated in CASP8 and that do not use additional structural information from homologs.
Availability: A perl implementation of the method, as well as the various perl and R scripts used for the analysis are available atContact: nirb@tauex.tau.ac.il Supplementary information: Supplementary data are available at Bioinformatics online.
Received on December 1, 2009; revised on March 9, 2010; accepted on March 13, 2010 1 INTRODUCTION The function of a protein is largely determined by its 3D structure.
Therefore, the determination of a proteins structure is an important step in understanding how the protein achieves its function, and it can also aid in predicting protein function or designing experiments.
However, experimental structure determination can be a long and difficult procedure, and naturally errors may occur (Kleywegt, 2009).
This was recently demonstrated when several protein structures published in the Protein Data Bank (PDB) were discovered to be erroneous (Chang et al., 2006).
Even when the structure determination process is correct, the determined structure may adopt a non-physiological fold, for example, due to non-physiological constraints imposed by the crystal in the case of X-ray crystallography.
Such errors can cause confusion and To whom correspondence should be addressed.
mislead further research, so it is important to be able to spot them before the structures are published.
Errors are even more frequent in computationally derived structures, which are built either by extrapolating from a homologous protein whose structure is already solved (Fiser and Sali, 2003; Ginalski, 2006) or by computer simulation (Das and Baker, 2008; Zhang and Skolnick, 2004).
In the latter case, many alternative conformations might be generated during the simulation, and differentiating between erroneous conformations and structures that are more likely to be correct could help guide the simulation and limit the search space.
Programs that try to numerically assess the correctness of a given structural model for a protein are called Model Quality Assessment Programs (MQAPs).
The need for such programs is widely recognized by the structural biology community, as evidenced by the inclusion of a category for assessing MQAP performance in the biennial Critical Assessment of Techniques for Protein Structure Prediction (CASP) experiment, starting from its seventh round (CASP7; Cozzetto et al., 2007).
The two pioneering MQAPs, still widely used today, are Verify3D (Eisenberg et al., 1997) and ProSa (Wiederstein and Sippl, 2007).
Both methods check the compatibility between the proteins structure and its sequence.
Verify3D, for example, classifies each residue in the protein into one of the 18 classes according to the residues structural environment in the input model.
The propensity of each amino acid to exist in each such structural environment class is calculated according to statistics collected from structures in the PDB, and the final score given to the protein structure is the sum of propensities of the individual residues.
Newer MQAPs were recently assessed in the blind experiments of CASP7 (Cozzetto et al., 2007) and CASP8 (Cozzetto et al., 2009).
The models given as input to the MQAPs were the server models, which are generated by the various servers participating in CASP shortly after the round starts, and long before the native structures are published.
Many of the participating MQAPs, including QMEAN (Benkert et al., 2009) and MULTICOM-REFINE (Cheng et al., 2009), functioned similarly to Verify3D and ProSa, receiving only one structure as input and assigning it a quality score based on the compatibility of various features computed from the sequence with the predicted 3D structure.
However, the most successful MQAPs in CASP8 were the consensus-based methods, such as Pcons (Larsson et al., 2009) and ModFOLDclust (McGuffin, 2009), which used as input the entire decoy set instead of just coordinates of a given model and took a consensus approach to rate each model according to how similar it was to the other structures in the set.
This approach, while clearly advantageous in the setting of the CASP experiment, is not applicable in many scenarios in which The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1300 12991307 M.Kalman and N.Ben-Tal few structures (possibly only one) are available, or when the decoy set is not likely to contain many correct models (Cozzetto et al., 2007; Wallner and Elofsson, 2008).
The single-structure MQAPs that performed best in CASP8 were LEE and LEE-server.
The group produced their own structural model for each target and ranked the decoys according to how similar they were to their model (Cozzetto et al., 2007).
The models produced by the LEE group were homology based, so at least part of the success of the method could be attributed to the fact that it used additional constraints from structural models of homologous proteins.
Two other methods from CASP8, SAM-T08-MQAU and SAM-T08-MQAO (Archie et al., 2009), also used such constraints taken from structural homologs, and they also performed significantly better than the rest of the single-structure methods.
While homology-based approaches have proven very promising, they are only usable when reliable structural homologs exist.
Therefore, there is still a need for devising methods that do not use additional structural information, neither from structural homologs nor from the other decoys.
We term such methods pure single-structure MQAPs.
An alternative strategy to that of most single-structure MQAPs is to check the compatibility of the suggested 3D structure with the evolutionary conservation pattern of the sequence.
There are various ways to calculate the conservation level (or evolutionary rate) of an amino acid position (Glaser et al., 2003; Mihalek et al., 2004).
A residue that is conserved throughout evolution has undergone strong purifying selection; this suggests that the conserved residue is important for the proteins normal function (Brndn and Tooze, 1999).
This observation has been used in many applications, such as identifying the active site of a protein, which is usually composed of a patch of clustered residues on the proteins surface (Nimrod et al., 2008).
An interesting observation is that for most proteins, the structural core is composed mainly of such conserved residues (see for example, Fig.1A).
These residues are usually not involved directly in the mechanism of the proteins function.
Rather, they are conserved because a mutation in such a buried residue would tend to perturb the architecture.
The protein surface, in contrast, is mostly variable.
If the association between residue accessibility and conservation level is strong enough, it might be used to differentiate between correct and incorrect model structures, as incorrect structures are unlikely to feature this pattern by chance.
This conservation pattern has been used for computational modeling of proteins, both manually for checking the validity of a built model (Landau et al., 2007) and automatically for generating a C model of transmembrane proteins starting from a low-resolution cryo-EM map (Fleishman et al., 2004a, b, 2006).
Conservation information has also been used for quality assessment in several studies.
The first conservation-based approach is to use the observation that conserved residues tend to be clustered in the native structure (Mihalek et al., 2003; Muppirala and Li, 2006; Schueler-Furman and Baker, 2003).
This clustering is expected both for structurally conserved residues, as they form the structural core, and for functionally conserved residues, which are usually localized on the surface, at the functional site of the protein.
Mihalek et al.
(2003) used the evolutionary trace method to collect a set of conserved residues and quantified the sets tendency to cluster using a measure they termed the selection clustering weight (SCW).
They applied this method to the Decoys RUs decoy set (Samudrala and Levitt, 2000) Fig.1.
Tendency of conserved residues to be buried in correct structures.
(A and C) The native structure for the CASP7 target T0289 (Aspartoacylase, PDB 2gu2A) in (A), and a poor model for the same target (model FPSOLVER-SERVER_TS1, GDT-TS = 7.9) in (C), colored by the ConSurf color map using the ConSurf-DB database.
ConSurf colors 17 are semitransparent to show the cluster of conserved residues buried at the structural core in the native structure.
(B and D) Distribution of relative residue accessibilities as calculated by Naccess for variable residues (cyan; ConSurf classes 1, 2, 3) and for conserved residues (purple; ConSurf classes 8, 9).
The distributions are shown for the native structure in (B) and for the poor model in (D).
(E) Distribution of relative residue accessibilities for all residues of all structures in the dataset, classified by their ConSurf conservation grades.
The different grades are colored according to the ConSurf color scheme.
There is a consistent shift to the right, with the most variable residues (ConSurf class 1) being most accessible.
Molecular graphic images were generated using UCSF Chimera (Pettersen et al., 2004).
and showed that indeed 78.1% of the decoys in the set were assigned a lower (less favorable) SCW score compared with the native structure.
However, the assessment of a method by its ability to rank a native structure higher than decoys has been shown to be problematic (Handl et al., 2009).
Schuler-Furman and Baker (Schueler-Furman and Baker, 2003) took a similar approach, adopting a simpler strategy for selecting the set of conserved residues based on entropy, as well as a different measure for quantifying the clustering.
However, they validated their method in a more relevant scenario, showing that when the method is used to select decoys generated by ROSETTA (Das and Baker, 2008), there is a statistically significant enrichment in correct models.
The second approach to exploit conservation data is to use it initially to make contact predictions and subsequently use the predictions for quality assessment.
Olmea et al.
(1999) provided a set of contact predictions, using the observation that in pairs of 1300 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1301 12991307 Assessment of protein model structures conserved residues, as well in pairs of residues whose mutations are correlated, the members of the pair tend to be spatially close to each other in the 3D structure.
They have shown that such predictions are usually more precise for native structures than for deliberately misfolded ones.
They also used this information as a post-processing step in a threading method and showed that it improved the methods results.
More recently, Miller and Eisenberg (Miller and Eisenberg, 2008) built an MQAP based on the agreement between such contact prediction information and the set of contacts in the proposed model.
They checked their method on several of the CASP7 targets and proved that it performed significantly better than random.
While previous studies have suggested that evolutionary information can be used for quality assessment, the performance of these methods was never compared with that of other MQAPs.
Furthermore, these methods only used the tendency of conserved residues to be spatially close to one other, which captures only partially the information that is present in the conservation accessibility relation.
In this study, we present a new very simple MQAP called ConQuass (conservation-based quality assessment), which is based on the correlation between each residues degree of evolutionary conservation and its accessibility in the structure.
We check the performance of ConQuass on the CASP8 dataset, and show our method to be comparable to the other pure single-structure MQAPs that participated in CASP8.
We also show that ConQuass is complementary to existing methods and could potentially be integrated with them to improve their performance.
2 METHODS 2.1 Collecting a training set of known structures The PISCES server (Wang and Dunbrack, 2003) was used to collect a non-redundant set of X-ray, full-atom protein structures from the PDB that have resolution better than 3.0 , R-factor better than 0.3 and sequence identity <25%.
This resulted in a set of 6132 protein chains.
Of those, we used only the 5648 proteins for which evolutionary conservation information was available in the ConSurf-DB database (Goldenberg et al., 2009).
We generated three structures for each protein chain.
The first contained only the given chain in isolation, without the other chains in the PDB structure.
In addition, two versions of the chain in the context of its biological unit were generated using either the protein quaternary structure (PQS) server (Henrick and Thornton, 1998) or the progressive iterative signature algorithm (PISA) server (Krissinel and Henrick, 2007).
Non-protein chains were removed.
Finally, we removed each protein whose complex contained >26 protein chains and eliminated protein structures that were too big to run in Naccess (Hubbard and Thornton, 1993), leaving a total of 5543 proteins in the final set.
2.2 Features collected for each structure 2.2.1 Conservation We collected the conservation level of each residue from the ConSurf-DB database (Goldenberg et al., 2009), which provides precalculated conservation profiles for every structure in the PDB.
These profiles assign each residue to one of nine conservation levels, with 9 being the most conserved and 1 being the most variable.
For some residues, the information in the multiple sequence alignment is not enough to compute the conservation level (for example, if that position consists mostly of gaps).
In these cases, ConSurf-DB assigns the residue value of insufficient data.
2.2.2 Accessibility We used the program Naccess to calculate the total relative accessibility for each residue (Hubbard and Thornton, 1993).
We further normalized these accessibility values by transforming them into quantiles, so that the most buried residue in a given protein would get the value 0 and the most exposed would get the value 1.
This normalization was done in light of the observation that some protein structures might overall be more accessible than others owing to their geometric properties, but within a single structure conserved residues still tend to be more buried compared with other residues in the same structure.
Each residue was then classified into one of ten evenly distributed accessibility classes.
2.2.3 Structure quality features For each structure, we extracted the resolution and the R-and free R-factors from the PDB as measures of the general structure quality.
This was done in order to validate our assumption that the correlation between the level of burial and evolutionary conservation of the amino acids would increase with the structure quality (see Section 3.1.1).
2.2.4 Alignment quality features We collected the following measures for each structure: (i) Nseq, the number of homologs in ConSurf-DBs alignment; (ii) Nseq20, the number of homologs in the alignment whose identity is >20% [the level of identity for each homolog is extracted from the PSI-BLAST output (Altschul et al., 1997), taken from ConSurf-DB]; (iii) Resnum, the number of residues with significant conservation information; (iv) %insig, the fraction of the protein residues whose conservation level is assigned the value insufficient data.
These features were chosen to reflect the general quality of the alignment and evolutionary rates generated by ConSurf-DB for each protein.
2.2.5 Finding the optimal filtering cutoffs The four measures of alignment quality that we collected could each help predict in advance whether a given protein would be well-suited for use with our method.
To find the optimal way to integrate these features, we solved the following optimization problem: given a ratio X (called the filtering degree), find the optimal quadruple of cutoffs such that when filtering the dataset according to these cutoffs, X of the proteins in the dataset pass the filter, and their average ConQuass score (as defined in Section 2.3) is maximal.
This problem was solved for each X in 0.01, 0.02, ,1 using an exhaustive enumeration, enumerating for each cutoff over 50 discrete values distributed evenly across the dataset.
In what follows we refer to proteins that passed the filter corresponding to a given filtering degree X as having a high-quality alignment, according to the X-filter, where a higher filtering degree corresponds to a more stringent requirement.
2.3 The ConQuass score Similarly to Verify3D (Bowie et al., 1991), we built a 109 propensity matrix, where each cell gives the compatibility score for assigning a residue with conservation class c an accessibility class a, as given by the information value (Fano, 1961): score(c, a)= ln ( P(c|a) P(c) ) where P(c|a) is the probability of finding a residue of conservation class c in the accessibility class a, and P(c) is the overall probability of finding a residue in conservation class c. These probabilities are estimated using the conservation and accessibility levels of the residues in the dataset of known protein structures.
The accessibilities were calculated using the biological unit given by PQS.
We also tried using the PISA biological unit or the isolated chain, but the propensity matrices generated were very similar (data not shown).
The final propensity matrix is shown in Supplementary Table S1.
ConQuass assigns each structure the average score of its residues: score(C, A)= 1 L score(Ci,Ai) where C and A are vectors of the same length L (number of amino acids in the protein), giving, respectively, the conservation and accessibility classes of the residues.
1301 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1302 12991307 M.Kalman and N.Ben-Tal 2.4 Assessment on the CASP dataset In the model quality assessment category of CASP, the participating groups were asked to rank the models built by the participating automatic servers.
We downloaded these server models, as well as the predictions of the participating MQAPs, from the CASP web site (http://predictioncenter.org).
We also downloaded for each server model its global distance test total score (GDT-TS) (Zemla, 2003), which is in the range (0, 100] and is the standard quality evaluation score given by CASP.
For each CASP target, we downloaded conservation information, if available, from the ConSurf-DB database entry for the native structure.
The same conservation information was aligned to all full-atom models of the target, as there is sometimes a shift between the residue sequence numbers in the native structures and in the CASP models.
To this end, we ranked each such alignment by giving each column a score of +1 if the residue identity in the native matched that of the model, 2 if the residues did not match, 1 for an insertion/deletion and 0 if the residue was missing in one of the structures.
The optimal alignment was then found using the SmithWaterman algorithm (Smith and Waterman, 1981).
For each model, we computed the accessibility class of each residue, as was done for the structures in the training set (see Section 2.1).
The conservation levels and accessibilities were used to calculate the MQAP score for each model (see Section 2.3).
We did not score targets whose native structure had no ConSurf-DB information.
When comparing ConQuass to the MQAPs that participated in CASP7, we considered only the 16 MQAPs that had ranked at least 15 000 models.
For MQAPs that participated in CASP8, we considered only the 22 pure single-structure methods that had ranked at least 20 000 models.
We restricted each analysis to models that had been ranked by all considered methods (including ConQuass), and from this set we eliminated targets for which fewer than 100 ranked models were available.
For each MQAP and each target, we calculated the Pearson correlation between the quality scores given by the MQAP and the GDT-TS scores downloaded from the CASP web site.
2.5 Integration of ConQuass with other methods To demonstrate that the conservation information used in ConQuass is complementary to that used by other methods, we built three new MQAPs, integrating the score given by ConQuass (Section 2.3) with the scores given by Circle-QA (Terashi et al., 2007), QMEANfamily (Benkert et al., 2009) and MULTICOM-REFINE (Cheng et al., 2009), respectively.
We chose Circle-QAbecause it was the leading pure single-structure method in CASP7, and we chose QMEANfamily and MULTICOM-REFINE because they were the leading pure single-structure methods in CASP8.
For each integrated MQAP, the score we assigned to each model was a simple linear combination of the ConQuass score and the score produced by the other method (the two scores were each assigned a weight of 0.5).
The analysis described in Section 2.4 was repeated for these three MQAPs.
We compared the first MQAP (integration with Circle-QA) to MQAPs that had participated in CASP7 and compared the other two (integration with QMEANfamily or MULTICOM-REFINE) to MQAPs that had participated in CASP8.
3 RESULTS AND DISCUSSION 3.1 Experimentally determined structures match their conservation pattern 3.1.1 Examining a dataset of high-quality structures It is widely recognized that residues buried in the protein core tend to be evolutionarily conserved, whereas residues on the surface are usually variable (Brndn and Tooze, 1999; Lichtarge et al., 1996).
This implies that the accessibilities of variable residues should be shifted toward higher values in comparison with those of conserved residues, as indeed seems to be the case for many experimentally solved protein structures we examined (e.g.
Fig.1A and B).
This characteristic is expected for true protein structures, and we would Fig.2.
ConQuass scores assigned to experimental structures from the PDB and to a few erroneous models.
The scatter plot shows the propensity score of the protein versus the number of residues with ConSurf information for all the structures in the dataset (in gray).
Only structures that have ConSurf information for at least 40 residues were included.
Also shown are pairs of incorrect (triangle) and correct (circle) structures for EmrE (black, 2f2m and 3b5d), Connexin (gray, 1txh and 2zw3) and MsbA (white, 1jsq and 3b5w).
For each of these structures, the ConQuass score was calculated for the residues of all the chains in the biological unit.
For models containing only the C-trace, the full-atom structures were rebuilt using MaxSprout and SCWRL4.
The correct structure of MsbA (3b5w) was truncated to contain the same set of residues as the erroneous structure (1jsq).
generally not expect to see it in incorrect models.
Figure 1C and D show the evolutionary profile of an extremely poor model structure (analysis of an intermediate quality model of the same protein is provided in Supplementary Fig.S1).
We first set out to measure the magnitude of this trend in real protein structures.
For that purpose, we collected a comprehensive dataset of high-quality experimentally determined structures, which we can reasonably assume to contain mostly correct structures (Section 2.1).
For this dataset, it is obvious that the more variable residues are consistently more accessible than the conserved residues (Fig.1E).
The information in this dataset was used to calculate a propensity matrix, giving the compatibility of each conservation class with each accessibility class (Section 2.3, and Supplementary Table S1).
The matrix confirmed our intuitive expectations, giving high propensity scores to accessible-variable residues and to buried conserved residues.
Consequently, the matrix was used to calculate each protein structures ConQuass score, which was the average of the propensity scores of the proteins residues.
A score was calculated for each structure in the dataset (Fig.2), using the biological unit complexes as given by PQS (Henrick and Thornton, 1998).
Only 7.9% of the structures received a negative score, meaning that for most structures the residues conservation levels tended to be compatible with their accessibility levels.
However, when we determined scores for the individual chains in the dataset without the context of the biological unit, more structures were assigned a negative score (12.5%).
This was due to monomers exposing conserved interface residues that are 1302 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1303 12991307 Assessment of protein model structures Fig.3.
Compatibility of the structure with the evolutionary profile of the protein is higher for higher-quality structures or higher-quality multiple sequence alignments, as described by different quality measures.
(A) The mean ConQuass score of the proteins in the dataset when filtering only for the top X proteins (x-axis), as measured by several crystallographic structure quality measures: the R-factor, free R-factor and the resolution.
(B) As in (A), but when filtering by non-structural measures: the number of residues (red), the number of homologous sequences in the alignment (black), the ratio of residues with insignificant conservation information as measured by ConSurf (green) and the number of homologous sequences in the alignment with at least 20% identity with the query (blue).
Also shown is the optimal ratio achieved by integrating these four measures (gray).
actually buried in the physiological complex.
We also tried to determine scores for the biological unit complexes given by PISA (Krissinel and Henrick, 2007) and the results were very similar to those obtained for the PQS complexes (data not shown).
The ConQuass scores also seemed to become progressively higher for structures of higher quality, as measured by various structure quality measures such as resolution, R-factor and free R-factor (Fig.3A).
The conservation data was calculated according to the multiple sequence alignment generated automatically by ConSurf-DB, and it is possible that a high-quality structure would be assigned a low ConQuass score if an inadequate alignment was used.
To discern these cases, we collected four measures that are indicative of the alignment quality or that could otherwise predict an incorrect ConQuass score for a protein model (see Section 2.2.4).
As can be seen in Figure 3B, the ConQuass score becomes progressively higher as the dataset is filtered to leave only structures whose alignment is of higher quality according to any one of the four measures.
Obviously, a better indicator for how suitable a protein is for ranking with ConQuass can be achieved by integrating the different alignment quality measures.
We used an exhaustive enumeration to find the optimal way to integrate these measures, each time filtering the database to leave only X% of the proteins such that the mean ConQuass score of the remaining proteins is maximal (Section 2.2.5).
This procedure assumes that after filtering, a higher mean ConQuass score is achieved because the remaining proteins have a higher quality alignment.
The integration achieves a much higher mean ConQuass score than does filtering by each measure separately (Fig.3B; gray).
The optimal cutoffs found for some selected filtering degrees are shown in Supplementary Table S2.
3.1.2 The conservation profile may reveal incorrect structures To test whether the ConQuass score is capable of discriminating incorrect structures, we collected three examples of structural models that had been deposited in the PDB but were later found to be incorrect.
All these structures also have corrected versions available, which we also scored using ConQuass (Fig.2).
The first two examples are EmrE (Fig.2; black) and MsbA (Fig.2; white).
Both structures were determined by Chang and coworkers using a faulty piece of in-house software, which caused the group to misinterpret the crystallization data and eventually yielded false models.
Following the detection of the error in the software, the structures were retracted (Chang et al., 2006), and corrected versions have since been published (Chen et al., 2007; Ward et al., 2007).
Calculating the ConQuass score for these structures is not straightforward, as they are all C-only models, with the exception of the erroneous EmrE structure.
However, we were able to apply ConQuass after reconstructing the full-atom models using MaxSprout (Holm and Sander, 1991) and SCWRL4 (Krivov et al., 2009).
Clearly, the correct structures are much more compatible with their conservation pattern than are the incorrect ones (Fig.2).
The third example is the gap junction connexin channel (Fig.2; gray), which was previously modeled by our group using low-resolution electron cryo-microscopy data (Fleishman et al., 2004b).
The helix assignment of the model recently turned out to be wrong when an experimentally determined high-resolution structure of a homologous protein was reported (Maeda et al., 2009).
For the purpose of comparing the two structures, we truncated the non-membrane residues from the experimental structure and also removed all non-C atoms.
This procedure left us with two C-only models composed of the same set of residues.
We then rebuilt the two full-atom models as above and scored them using ConQuass.
While both the truncation of non-membrane residues and the full-atom reconstruction lowered the score for the crystallographic model (data not shown), it was still assigned a much higher score than the erroneous model (Fig.2).
There are some cases in which a correct model seems not to match its conservation pattern, as denoted by a negative ConQuass score.
However, a closer examination can usually provide an explanation for the low score.
Some representative examples are discussed in Supplementary Section S1.1.
3.2 Ranking decoys in CASP ConQuass may also assess how distant a given model is from the native structure.
To show this, we checked how ConQuass scores models of varying quality for the same protein.
A good source for such models is the biennial CASP experiment (Moult et al., 2009), where each round consists of several targets, corresponding to proteins whose structure have recently been solved (but not yet published), and each participant submits computational models in an attempt to predict the structure of each target.
At the end of the round, the experimental structures are revealed, and the quality of each submitted model is measured by the similarity measure GDT-TS (Zemla, 2003), which is based on the superposition between the model and the native structure.
The seventh and eighth CASP rounds included a quality assessment category (Cozzetto et al., 2007, 2009), in which different MQAPs participated and were consequently evaluated according to their performance.
The models scored by the MQAPs were server models that were generated by the structure prediction servers participating in CASP and published shortly after the round began.
The MQAPs were evaluated according to the correlation between the scores they gave the different models and the quality of those models as measured by GDT-TS.
The scores given by the participating MQAPs are available for download from 1303 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1304 12991307 M.Kalman and N.Ben-Tal Fig.4.
The ability of the ConQuass score to rank decoys in the CASP8 dataset.
(A) Demonstration for target T0449.
For each decoy, the GDT-TS (similar to the native structure) is plotted versus the ConQuass score.
The vertical line is the ConQuass score assigned to the native structure.
The Pearson correlation for this target was 0.827.
(B) Box plots of the correlation values for the 22 MQAPs that ranked at least 20 000 models.
The number signifying each MQAP (x-axis) is the number assigned in the original CASP8 experiment (see http://predictioncenter.org/casp8).
Also shown is the box plot for the correlation values of ConQuass (999, gray).
The correlations were calculated only for models ranked by all 23 MQAPs.
The box plots were sorted by the mean correlation, indicated by the black dots.
The figure is cut to show only the correlation range [0.5, 1] in order to make the differences between the methods more apparent (the uncut version is shown in Supplementary Fig.S3).
(C) Same as (B), when looking only at targets with the highest quality alignment, using the 20% filter.
Although ConQuass is ranked first here, the specific ordering of the top ranking methods is irrelevant, as the correlation values achieved by ConQuass are not significantly higher than those achieved by MULTICOM-REFINE (013).
the CASP web site (http://predictioncenter.org), which allowed us to compare them with ConQuass.
The results for the CASP8 set are presented below.
The analysis of the CASP7 set showed a similar performance, and it is presented in Supplementary Section S1.2.
To be able to best compare ConQuass with the other pure single-structure methods, we have excluded from our analysis methods that use structural data from the other decoys or from homologs (a comparison of ConQuass with the latter methods is shown in Supplementary Fig.S2).
For brevity, we will use the term MQAP in this section to refer only to pure single-structure methods.
3.2.1 Example of the performance on one CASP8 target As an illustrative example, Figure 4A shows the GDT-TS values of the server models of CASP8 target T0449, plotted as a function of the assigned ConQuass scores.
There is a striking correlation between the score and the structure quality, and the set of highly scored models was enriched with high-quality structures.
The Pearson correlation in this case was 0.827, and the score of the native structure (Fig.4A; vertical line) was higher than the scores of all the decoys except three.
3.2.2 Overall performance on all CASP8 targets In our calculation on the CASP datasets (see Section 2.4), we used the conservation data recorded in the ConSurf-DB dataset.
There are cases in which the alignment could have been manually improved in order to achieve a better performance, but we deliberately refrained from doing this to avoid biasing our results.
Four CASP8 targets could not be ranked, because their native structures did not have any ConSurf-DB information.
This usually happens when ConSurf-DB cannot find enough homologs to construct a meaningful alignment.
Ten additional targets were cancelled by CASP8 or had no corresponding native structure listed in the CASP8 web site.
A ConQuass score was given to each of the full-atom models of the remaining 114 targets.
CASP allowed each participating MQAP to choose to rank any subset of models, for any subset of targets.
Indeed, many MQAPs are not applicable for all models.
This makes performance comparison problematic.
For example, it might be easier to assess the quality of full-atom models, and if so an MQAP (such as ConQuass) that ranks only such models would have an advantage over methods that also rank C models.
To avoid this problem, we carried out all calculations on the set of 11 686 models and 75 targets that were ranked by all participating MQAPs.
To avoid excluding too many models, only the 22 participating MQAPs that scored at least 20 000 models were used.
To evaluate the performance of each MQAP, we calculated for each CASP8 target the Pearson correlation between the scores determined by the MQAP and the GDT-TS values of all the models for that target.
The sets of correlation values for each MQAP are plotted in Figure 4B.
Our ranking of the methods is slightly different from the ranking published in the CASP8 proceedings (Cozzetto et al., 2009) due to differences in the ranking protocol (see a detailed explanation of the differences in Supplementary Section S1.3).
However, as in the CASP8 results, the MQAPs that performed best according to our assessment were the different variants of QMEAN (Benkert et al., 2009) and MULTICOM (Cheng et al., 2009).
The variants with the highest mean correlation were QMEANfamily (082) with a mean correlation of 0.778 and MULTICOM-REFINE (013) with a mean correlation of 0.768.
Following the different variants of QMEAN and MULTICOM, the method with the next 1304 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1305 12991307 Assessment of protein model structures Fig.5.
The relation between the ConQuass score assigned to the model and the models quality.
(A) Plot of the GDT-TS (similar to the native structure) versus the ConQuass score for the following models: all models in the CASP8 dataset (black, Pearson correlation 0.678); the models with the highest quality alignment by the 50% filter (blue, Pearson correlation 0.780); models that passed the less permissive 20% filter (red, Pearson correlation 0.843).
(B) Box plots of the GDT-TS values for models in each ConQuass score quartile.
For example, the median GDT-TS for the models scoring very low (below 0.017) is 22.8, and 50% of these models have GDT-TS values between 14.8 and 40.6.
For the models scoring very high (>0.12), the median GDT-TS is 73.1, and 50% of these models have GDT-TS values between 65.0 and 81.7. highest ranking, with a mean correlation of 0.722, was CIRCLE (396), which was the best performing single-structure MQAP in CASP7 (Cozzetto et al., 2007).
ConQuass (999, Fig.4B; gray) ranked next, with a mean correlation of 0.715.
As shown in Section 3.1.1, some structures were assigned a low ConQuass score because of a low-quality alignment rather than a low-quality model.
Indeed, for some CASP8 targets, the native structure itself scored very low by ConQuass.
Such targets were clearly not suitable for use with our method.
Many of these cases could be discerned in advance by using the alignment quality measures presented in Section 2.2.4.
To check how our method performs on more appropriate targets, we used the 20% filtering to select only the 17 targets with the highest quality alignment.
The performance of the different methods for this subset of targets is shown in Figure 4C.
With these targets, ConQuass performs significantly better, and it is ranked first with a mean correlation of 0.844.
It is important to stress that the set of targets that are more suitable for use with ConQuass can be selected a priori, as all the features used for the filtering are based on the multiple sequence alignment alone.
The Pearson correlations we evaluated were calculated for each target independently, so scores produced by an MQAP that achieves a high correlation value can be used to select among alternative structural models for the same protein.
However, in many scenarios one wants to evaluate the absolute quality of a single uncertain structural model without comparing it to other decoys.
For these cases, it is informative to know the relation between the MQAP score and the structure quality, as measured by the GDT-TS.
This relation, for all models of all CASP8 targets, is shown in Figure 5A.
The overall correlation between the ConQuass score and GDT-TS is good (0.678), especially when filtering for targets with a high-quality alignment (0.843 if using the 20% filtering, Fig.5A; red).
The overall correlation was also compared with that of the other participating MQAPs (Supplementary Table S3).
Figure 5B presents these results in a way that is more intuitive for interpreting the score given to a model by ConQuass.
If a model is assigned a very low ConQuass score (below-0.017), it is expected to be of rather low-quality (median GDT-TS 22.8, most GDT-TS values in the range [14.8, 40.6]).
However, if a model is assigned a very high ConQuass score (>0.12), it will very rarely be a low-quality structure (median GDT-TS 73.1, most GDT-TS values in the range [65.0, 81.7]).
3.2.3 Complementarity to other MQAPs ConQuass uses the evolutionary conservation properties of the protein structure, a feature that is not directly used by any other contemporary MQAP.
It therefore seems reasonable to suggest that ConQuass is complementary to the other prevalent methods.
To support this claim, we scored the CASP8 models using two new MQAPs that were trivial integrations of ConQuass with, respectively, MULTICOM-REFINE and QMEANfamily, the two best performing single-structure MQAPs in CASP8 (see Section 2.5).
The performance of these two integration methods was analyzed using the same procedure described above.
The integration with ConQuass significantly improved the correlations achieved by both MULTICOM-REFINE (P-value 4.2e-14) and QMEANfamily (P-value 1.1e-08); see Supplementary Section S1.4.
4 CONCLUSION Here we have presented ConQuass, a very simple MQAP based directly on the compatibility between the conservation and accessibility patterns of a given structural model.
We studied the scores that ConQuass assigns to experimental structures, demonstrated its ability to discern erroneous models and checked the relation between the ConQuass scores given to different models and the models resemblance to the native structure.
We have also shown that ConQuasss performance is comparable to that of other pure single-structure MQAPs, despite being much simpler than most.
Our approach is different from previous MQAPs that used evolutionary conservation, which were based on the spatial clustering of the conserved residues.
We feel our approach is more direct, since this clustering is mostly an effect of the conserved residues tendency to be buried in the structural core (for a direct comparison with the method developed by Mihalek and coworkers; see Supplementary Section S1.5).
ConQuass is also the first conservation-based approach to be rigorously compared with contemporary MQAPs.
In addition, our score is based on summation of information that is local in the structure (the propensity of the conservation class of each residue for its accessibility class), so it should be adaptable to provide a local quality score for each residue of the structure, as is done by local quality assessment tools (Fasnacht et al., 2007).
Preliminary tests for a local MQAP based on summing the propensities over a fixed-width window on the sequence have yielded promising results (data not shown).
In this study, we have clearly shown that evolutionary conservation is a powerful property for use in model quality assessment, so it would be advantageous for new MQAPs to integrate this property with other more commonly used properties.
Evolutionary conservation is currently not used directly by any MQAP, although some methods, like QMEAN and MULTICOM, use it indirectly by comparing model surface accessibilities with the predicted accessibilities, which are associated, in part, with the evolutionary conservation.
However, we have demonstrated that these methods do not use the conservation information to its full extent, as their results improve when their scores are integrated with 1305 [20:37 22/4/2010 Bioinformatics-btq114.tex] Page: 1306 12991307 M.Kalman and N.Ben-Tal those of ConQuass.
In this work, we have followed a very nave approach for such an integration, using a simple linear combination.
Much better results would doubtless be obtained by a more intricate approach, for example by using the residue conservation as one of the features in a machine learning-based tool.
In any case, such integration would have to take into account the quality of the alignment, as the evolutionary conservation property is more indicative for high-quality alignments.
The same approach could also be used to integrate conservation in many other practices, such as finding the physiological complex of a crystal structure and scoring docking results.
In addition, as the ConQuass score reflects the consistency between the alignment and the structure, its functionality could be reversed to check the quality of an alignment based on a high-quality structure.
While ConQuass is not the best performing of the examined MQAPs, many of which use a mixture of complex features including geometric and energetic properties of the structure, it has the advantage of being straightforward and easy to interpret.
The conservation pattern of the protein is not used by most modeling and structural determination programs, so ConQuass gives independent support for a structural model, whether experimental or computational.
If the model is assigned a low score, it is easy to visualize the discrepancy of the model with the conservation pattern by projecting it on the structure using ConSurf (Glaser et al., 2003), as we have done in the examples in Figure 1 and Supplementary Figure S4.
This can either yield relevant insights regarding the mechanism associated with the structure (for example, hint that it may bind to another molecule; see Supplementary Section S1.1), lead to a rejection of the model (see Fig.1C) or perhaps in some cases guide further refinements of the model.
ACKNOWLEDGEMENTS The authors thank Gilad Wainreb and Maya Schushan for helpful discussions.
Funding: Israel Science Foundation (grant 611/07); Edmond J. Safra Bioinformatics program at Tel-Aviv University (to M.K.).
Conflicts of Interest: none declared.
Abstract As a class of cis-regulatory elements, enhancers were first identified as the genomic regions that are able to markedly increase the transcription of genes nearly 30 years ago.
Enhancers can regulate gene expression in a cell-type specific and developmental stage specific manner.
Although experimental technologies have been developed to identify enhancers genome-wide, the design principle of the regulatory elements and the way they rewire the transcriptional regulatory network tempo-spatially are far from clear.
At present, developing predictive methods for enhanc-ers, particularly for the cell-type specific activity of enhancers, is central to computational biology.
In this review, we survey the current computational approaches for active enhancer prediction and discuss future directions.
Introduction Gene transcription is regulated by a series of accurately orches-trated interactions between transcription factors (TFs) and cis-regulatory DNA elements, e.g., promoters and enhancers [1].
Enhancers are often found in non-coding regions of a genome and generally distal to their target promoters.
The first charac-terized enhancer was a DNA segment that markedly increased the transcription of the b-globin gene in a transgenic assay in ang Z).
eijing Institute of Genomics, tics Society of China.
g by Elsevier jing Institute of Genomics, Chinese A the SV40 tumor virus genome about 30 years ago [2].
Nonethe-less, global identification of enhancers and their activities re-mains challenging, since enhancers can activate transcription regardless of their location or orientation [3].
The development of computational enhancer recognition approaches has been greatly facilitated by the massive amount of genomic data available owing to the rapid advances in sequencing technolo-gies in recent years.
Early algorithms were developed largely based on evolutionary constraints with the assumption that highly conserved non-coding regions should have functional potential [4].
However, conservation by itself is not sufficient to confer cell-type specific enhancer activities, suggesting that additional (e.g., epigenetic) information is required for accurate prediction.
Genome-wide maps of chromatin marks have been used to show that active enhancers are likely to be associated with certain characteristic chromatin signatures, e.g., monomethylation of histone H3 at lysine residue 4 (H3K4me1) [5].
But, Bonn et al.
reported that H3K4me1 is cademy of Sciences and Genetics Society of China.
Production and hosting mailto:zhangzhihua@big.ac.cnWang C et al/ Computational Methods for Enhancer Prediction 143 distributed similarly between mesodermally active and inactive enhancers, indicating that the placement of H3K4me1 is not cell type specific during embryonic development [6].
Hitherto, to the best of our knowledge, there is no evidence that active enhancers should necessarily exhibit the same single or a com-bination of epigenetic marks across all the cell types [7].
There-fore, it is necessary to select optimal combinations of epigenetic marks to predict when and where an enhancer is ac-tive [811].
In this review, we first survey the most commonly adopted strategies in enhancer recognition and then discuss potential future directions.
The principle of enhancer recognition Enhancers may be characterized by quantitative measures, termed features, associated with the underlying DNA se-quences.
In principle, an enhancer recognition algorithm uti-lizes informative and discriminative features as input to discriminate enhancers from non-enhancers, ideally from other non-enhancer cis-regulatory elements.
Algorithms and features are both important.
We therefore will discuss them separately.
Features can be briefly classified into three categories, namely comparative genomic features, TF binding related ge-netic features and epigenetic features (Figure 1).
Comparative genomic features mainly refer to the conservation scores calcu-lated by comparing the genome sequences of different species.
The predictive power of comparative genomic features stems from the fact that functional genome regions (e.g., enhancers) are subjected to negative selection [12,13].
TF binding related genetic features use quantitative scores presumably represent-ing the TF binding affinity at the DNA sequence of interest.
Figure 1 Features used in enhancer prediction algorithms The comparative genomic features are usually generated from compari features result from two sources, one from known TF binding motifs measured by various technologies.
See the main text for more details.
The DNA binding sites of a given TF are usually determined by the DNA nucleotide sequence and the binding affinity be-tween the TF and the DNA sequence [1416].
It is believed that TFs are the actual operators for enhancer regulatory activities [17], which may explain why TF binding related ge-netic features are predictive.
Direct measurement of the bind-ing affinity between a TF and DNA sequence is not easy.
However, the binding affinity can be inferred indirectly, either by experimentally measuring frequency of TF binding events, such as chromatin immunoprecipitation (ChIP) [18], or by cal-culating the similarity of the DNA sequences with a known TF binding motif [19,20].
The epigenetic feature mainly includes the level of histone modifications and of DNA methylation.
Recent experimental evidence supports the association of sev-eral histone modifications with enhancer activity.
The histone modification levels thus have served as features to predict ac-tive enhancers in humans [21,22].
Researchers also attempt to seek optimum combinations of these features for whole-gen-ome prediction of active enhancers [5,911] (Table 1).
Obvi-ously, not all the aforementioned features are equally important for active enhancer prediction.
The level of some dominant features showed strong correlation with enhancer activity [5,23], although the nature of the relationship between the features and enhancer states is poorly understood.
Further development of superior predictive methods can not only help us to reveal such structure, but also help to improve sensitivity and specificity of the predictions.
Algorithms for enhancer recognition can be roughly di-vided into two groups.
One group comprises probabilistic graphical models which describe the generative process of spe-cific signals, such as Bayesian networks (BNs) [24] and hidden Markov models (HMMs) [25].
The other group employs son between DNA sequences in closely-related species.
TF binding and the other from ChIP experiments.
Epigenetic features can be Table 1 Features of computational methods for enhancer prediction Feature Method Ref Comparative genomic features Aparicios method [4] Visels method (2008) [30] Chens method [8] Yips method [50] Sequence-based TF binding related features Narlikars method [65] Chens method [8] Lees method [44] Yips method [50] Experiment-based TF binding related features Visels method (2009) [46] Zinzens method [67] Mays method [48] Chens method [8] Epigenetic features Heintzmans method [5] Wons method [11] Firpis method [10] SEGWAY [69] Kharchenkos method [60] Hes method [23] Ernsts method [61] ChromaGenSVM [9] Yips method [50] Chens method [8] Bonns method [6] Note:More than one type of features were employed to build enhancer recognition model in some studies.
For example, Chen et al.
used all four types of features to develop active enhancer recognition model [8].
144 Genomics Proteomics Bioinformatics 11 (2013) 142150 discriminative filters and includes thresholds or classification boundaries in the features.
This group mainly includes support vector machines (SVMs) [26] and artificial neutral networks (ANNs) [27].
The features used in enhancer recognition Comparative genomic features Comparative genomic features comprise conservation scores calculated from multi-species genome sequence alignment.
With the completion of more vertebrate genome sequencing projects, many methods have been developed to discern slowly evolving genome regions.
For example, by comparing point substitutions, insertions and deletions between humans, mice and rats, Cooper et al.
comprehensively annotated slowly evolving regions in the human genome [28].
Phastcon score, another example representing the evolutionary conservation of genomic regions [29], has been employed to predict putative enhancer location [8].
In early systematic recognition of poten-tial enhancers in fugu [4], a pair-wise identity score of Hoxb-4 between mouse and fugu was used to detect conserved sequence blocks, followed by transgenic mouse assays to measure their enhancer activities.
Likewise, ultraconserved non-coding elements between humans, mice and rats were also found to be highly enriched in enhancer regions [30].
However, conservation per se is not sufficient to deduce enhancer activity in any given cell type.
Moreover, several enhancers with little conservation were found carrying identical regulatory patterns in different species [3133].
Therefore, additional information is required to predict enhancer activity in a given cell type.
Transcriptional factor binding related genetic features Transcriptional factor binding related genetic features can be roughly classified into two groups.
One group includes quanti-tative scores of similarity to a known TF binding motif, repre-senting the TF binding affinity to the DNA segments (sequence-based TF binding related genetic features).
The other group includes experimental measurements of TF bind-ing frequency, which also presumably represents TF binding affinity (experiment-based TF binding related genetic features).
The sequence-based TF binding related genetic features comprise individual TF binding and the enrichment of modu-lar combinations of TF binding.
Measuring TF binding affin-ities is not an easy task experimentally; however, it can be approached from the nucleotide preferences at each sequence position [20], e.g., position weight matrix (PWM).
PWM de-scribes the probability of observing the respective nucleotides A, C, G, and T in each position of a sequence motif.
It has been found that there is a strong correlation between PWM scores and the TF binding affinity [15,16,20].
PWMs for known TFs have been cataloged in databases [34,35].
These matrices enable people to assign a quantitative score to any se-quence to evaluate the binding affinity of the specific TF at that sequence (Figure 1).
In vertebrates, functional TF binding sites are usually clustered into a modular structure, which motivates researchers to seek cis-regulatory modules (CRMs) as the advanced predictive features for cis-regulatory element recognition [36,37].
The CRM features are often calculated as the likelihood of the CRM in a given genome context [38].
For example, MSCAN value measures the statistical sig-nificance of the appearance of potential combinatorial TF binding sites [39].
All the TF binding sites are represented by PWM scores and MSCAN returns the significance of the CRM.
A similar strategy is adopted in MCAST [40].
To further improve the performance, additional phyloge-netic footprinting is employed to align interested orthologous DNA sequences to define a conserved region and then the signif-icance of the CRM is calculated in the regions.
For example, EEL approach was used to scan a given pair of orthologous se-quences to identify conserved TF binding sites, and, then EEL scores were calculated by considering both distances and differ-ences in the angles between adjacent binding sites [41].
Another example, MorphMS, implemented a pair-HMM statistical alignment between two species [42].
A first order Markov net-workwith three states (match, deletion and insertion)was imple-mented and emits two strings, one for each species.
The string emitted in the match state was chosen by another probabilistic process, which models the arrangement of binding sites and non-binding (background) sites by PWM.
Then, two log like-lihood ratio (LLR) scores were reported.
The two scores (LLR1 and LLR2) compare the likelihood of a sequence under the MORPH model to the likelihood of the sequences under null models.
The null model used in LLR1 only considers back-ground PWM, while the null model for LLR2 assumes that the two orthologous sequences were generated independently.
Besides the similar strategy used in MorphMS, another algorithm EMMA incorporates gains and losses at binding Wang C et al/ Computational Methods for Enhancer Prediction 145 site, a process that is believed to be an important part of CRM evolution [43].
However, the computational cost increases exponentially with the number of TFs considered.
One alter-nate choice for this type of sequence features is k-mer profile, which is the frequency of all possible k-mer (putative motifs with length of k) in a given sequence region [44].
The profile measures how likely the k-mers in one enhancer would be found in another independently-generated sequence.
Using such k-mer features, Leung and Eisen developed a profile similarity between pairs of sequences to detect novel enhancers [45].
However, the search space is growing exponentially with k. The sequence-based TF binding related genetic features alone are not sufficient for active enhancer recognition.
First, most of the features are conserved TF binding sites, while many enhancer elements are not conserved.
For example, in Drosophila, the cone-specific Pax2 enhancer carries barely-conserved TF binding sites, which have been shown to possess similar enhancer functions in transgenic assays [31].
Similarly, a large proportion of a 40 kb region in the Phox2b locus showed regulatory activity by transgenic assay in zebrafish, while only 2961% identified regulatory sequences were con-served [32].
Second, in any given tissue, only a subset of enhancers is active.
This tissue-specific activity may result from a tissue-specific combination of binding TFs or from regula-tion at the epigenetic level.
TF binding in given tissues or cell types can be experimen-tally measured, which gives experiment-based TF binding re-lated genetic features.
For example, data from chromatin immunoprecipitation followed by massively parallel DNA sequencing (ChIP-seq) technology precisely provide binding loci for the TFs under the given conditions [18].
Visel et al.
mapped the genome-wide occupancy of p300 in three cell lines by ChIP-seq.
Using transgenic mouse assay, they show that p300 binding sites are predictive for enhancer activity in the cell types examined [46].
Similarly, CREBBP-bound enhancers also show environment-dependent activity in neurons [47], or in transgenic mouse enhancer assays [48].
Recently, ENCODE project has generated high-throughput sequencing (ChIP-seq or ChIP-chip) data sets for 119 distinct transcription factors over five main cell lines [49].
These experimental results have been used for enhancer recognition [50].
Epigenetic features Epigenetic features consist of chromatin structure, histone modifications, DNA-methylation levels and non-coding RNAs.
In this review, we mainly focus on the first two types of epigenetic features, since other features have been reviewed elsewhere (such as [51]).
Chromatin structure controls DNA accessibility of TFs to enhancer or other regulatory elements.
DNA accessibility can be inferred as DNase I hypersensitivity [52,53] or by Formaldehyde-Assisted Isolation of Regulatory Elements (FAIRE) technology [54].
The regions detected by DNase I or FAIRE are associated with all known classes of ac-tive DNA regulatory elements, including enhancers [55].
For example, Wiench et al.
found that CpG methylation at gluco-corticoid receptor (GR)-associated DNase I hypersensitive sites was a cell type-specific event and suggested that these sites could be a unique class of active enhancers [56].
Comparing DNase I-seq and FAIRE-seq data in seven human cell types indicated that data from these two assays were not fully over-lapping [57].
DNase I tended to find the regions around tran-scriptional start sites, while FAIRE was more sensitive in detecting distal regulatory elements.
Notably, neither DNase I nor FAIRE hypersensitive sites detected in one cell type are sufficient to demonstrate their enhancer state, as many other regulatory element sites, such as repressors or insulators, are also DNase I or FAIRE hypersensitive [57].
Therefore, DNase I or FAIRE hypersensitivity data should be regarded as a necessary but not sufficient input for active enhancer prediction.
In addition to DNA accessibility, the presence of characteristic histone modifications is another sign for the activity of enhancers, e.g., elevated H3K4 monome-thylation (H3K4me1) levels and depleted H3K4 trimethylation (H3K4me3) levels have been correlated with enhancer activity [5].
Further experiments showed that active enhancers marked by H3K4me1 in ES cells are also flanked by H3K27 acetyla-tion (H3K27ac), while regions marked by H3K27 trimethyla-tion (H3K27me3) are associated with early developmental genes which are poised in ES cells [58,59].
In another study, however, Bonn et al.
found that H3K4me1 was distributed similarly between mesodermally active and inactive enhancers, indicating that the placement of H3K4me1 is not completely cell type specific during embryonic development [6].
Instead, they found a conditional link between the presence of H3K79me3, H3K27ac marks and enhancer activity.
Although the histone modification patterns mentioned above showed promising potential for enhancer activity pre-diction in certain cell types, the general pattern of histone mod-ifications for prediction still remains elusive.
In human CD4+ T cells, 39 histone modification types have been mapped and several histone mark combinations showed correlation with enhancers, yet no single mark is associated with more than 40% of enhancers [7].
Integrating more epigenetic marks may render a more reliable, robust and precise model to cap-ture active enhancers.
Several attempts have been made [5,9 11].
One such attempt employed 10-fold cross-validation for all possible combinations of six histone modification marks to predict p300 binding sites, and found that enrichment of H3K4me1 and depletion of H3K4me3 is the most predictive combination for p300 binding [5].
Many more sophisticated computational technologies have also been applied to search for optimal combinations for active enhancers.
For example, Won et al.
coupled HMM with simulated annealing to search for the most informative combination of histone modification marks [11].
In Drosophila, Kharchenko and coworkers found that active enhancers lack H3K4me3 and are enriched for H3K4me1, H3K27ac and H3K18ac [60].
Similarly, Chrom-HMM labeled active enhancers with the H3K4me1, H3K4me2 and H3K27ac signature [61].
In a vast collection of epigenetic marks (20 histone methylations and 18 histone acetylations), genetic algorithms indicated that the most pre-dictive histone modification signals within enhancers are H3K4me1 and H3K4me3 [9].
A similar pattern was also ex-tracted from nearly 40 ENCODE histone modifications by using fisher discriminate analysis [10].
The features we discussed above can also be roughly classi-fied into two classes, based on the prediction power for enhan-cer activity.
One class of features represents the potential of a locus to be an enhancer, e.g., comparative genomic features or sequence-based TF binding related genetic features, because Figure 2 Flow scheme of model building To improve model interpretability and reduce overfitting, sophisticated computational strategies implement feature selection algorithm to select a subset of relevant features for model building.
Then, appropriate classification model is employed to differentiate active enhancers from non-enhancers.
Generally, there are two major classification models.
The first is the discriminative models which find the optimal classification border in the feature space (lower left panel).
The other one is the probabilistic graphical models that try to model the joint distribution of states and associated features with graph (lower right panel).
ANN, artificial neutral network; BN, Bayesian network; HMM, hidden Markov model; SVM, support vector machine.
146 Genomics Proteomics Bioinformatics 11 (2013) 142150 the features describe the static DNA sequence characteristics which are shared by almost all cells in an organism.
The other class of features, e.g., experiment-based TF binding related ge-netic features or epigenetic features, further indicates enhancer activity of the loci in a given tissue or cell type.
These features are the actual measurement of cellular or molecular activities that had already been associated with enhancer activity in liv-ing cells.
For example, when Visel and colleagues compared the evolutionary conservation score and p300 binding sites, they found that only 47% (246 out of 528) of conserved enhan-cer candidates were active in a transgenic mouse assay, whereas 87.7% of p300 binding sites were reproducibly active in the same transgenic assay [46].
Another study employed chromatin signatures of H3K4me1, H3K4me3 and H3K27ac to recognize active enhancers in 19 mouse cell lines.
By com-paring predicted enhancers with 726 experimentally validated enhancers, they found that 82% of predicted enhancers were correctly identified [62].
Androgen receptor binds primarily to active enhancers in human prostate cancer cells [63].
Inter-estingly, He et al.
found that the H3K4me2 signal was detected in the known androgen receptor binding sites [23].
At present, although some features showed strong preference in the putative enhancer regions, and some other features showed association with enhancer activity, the relationship between features and enhancer activity is complicated, and sophistic models are still essential to achieve sensitive and specific active enhancer prediction.
Model building The general process of enhancer prediction is summarized in Figure 2, and the commonly used methods are listed in Table 2.
The simplest method to differentiate active enhancers from background is to look for the presence of characteristic fea-tures.
For example, p300 ChIP-seq data were used to deter-mine p300-enriched regions, which were considered as putative active enhancers.
Of the 122 tested p300 binding ele-ments in mouse, 107 (87.7%) showed reproducible enhancer activity [46].
Heintzman et al.
exhaustively searched all combi-nations of six different histone modification marks, and iden-tified the optimal combinations of H3K4me1 and H3K4me3 [5].
Despite the fine performance of this simple model, the best predictive power in one dataset does not guarantee its perfor-mance in another.
Moreover, an ever increasing number of fea-tures would challenge these simple methods.
This is not only because of the inter-correlations between the features, but also because of the difficulties in interpreting the relative impor-tance of each feature.
A class of computational technology, named feature selection, has been applied to solve such prob-lems [64].
For example, Narlikar et al.
built a linear regression model to identify active enhancers in heart based on 727 se-quence features including 721 TF binding related genetic fea-tures [65].
The LASSO linear regression method was then applied to find features relevant to enhancer activity and 45 Table 2 Model building strategies and performance of enhancer prediction methods Category Method Operational model Positive predictive value (%) N te Ref Discriminative model Heintzmans method Thresholds of histone modification profiles 39.5 M pped to distal p300 binding sites in HeLa cells [5] Visels method (2009) Thresholds of p300 binding profiles 87.7 W th reproducible enhancer activity in transgenic m use [46] Narlikars method Linear regression 62 W th reproducible enhancer activity in vivo in m use and zebrafish [65] Zinzens method Support vector machine 71.4 W th reproducible enhancer activity in transgenic D osophila [67] Firpis method Time-delay neural network 66.3 O erlapped with p300 binding sites, Dnase I h persensitivity sites or TRAP220 binding sites in H La cells [10] Lees method Support vector machine 74.5 O erlapped with Dnase I hypersensitive enhancers i embryonic mouse whole brain cells [44] ChromaGenSVM Support vector machine 57 O erlapped with p300 binding sites, Dnase I h persensitivity sites or TRAP220 binding sites in H La cells [9] Probabilistic graphical model Wons method Hidden Markov model 54.8 O erlapped with p300 binding sites, Dnase I h persensitivity sites or TRAP220 binding sites in H La cells [11] Bonns method Bayesian network 78 O erlapped with previously identified TF binding s s in Drosophila [6] Other Chens method Multinomial logistic 83 O erlapped with at least one TF peak from 7 m use embryonic stem cell ChIP-seq datasets [8] Yips method Random forest 67 W th enhancer activity in vivo in mouse and m daka fish (28/42) [50] Note: The performance shown here is the reported performance compared to experimental results.
The positive predictive value (percentage as calculated as follows: positive predictive value = true positive/(true positive + false positive).
W a n g C et a l/ C o m p u ta tio n a l M eth o d s fo r E n h a n cer P red ictio n 1 4 7 o a i o i o i r v y e v n v y e v y e v ite v o i e ) w 148 Genomics Proteomics Bioinformatics 11 (2013) 142150 features were assigned nonzero weights.
The accuracy of 92% was achieved in distinguishing heart enhancers from a large pool of random noncoding sequences.
Recently, more sophisticated methods have been imple-mented to find the optimal classification border in the feature space.
Typical methods include ANNs and SVMs.
A neural network is a parallel system, capable of resolving paradigms that linear computing cannot [27].
A case concerning enhancer recognition is a time-delay neural network (TDNN) which combines 39 histone modifications [10].
In an independent test, 66.3% of the putative regions identified by this model over-lapped with experimentally supported enhancers [10].
A SVM performs classification by seeking a hyperplane in high dimensional labeled feature space that optimally separates the data into two categories regarding the classification labels [66].
A SVM model has been applied to ChIP-seq data of five different TFs and 77% of all known muscle-specific enhancers in Drosophila have been correctly predicted [67].
In addition, using ChromaGenSVM, which was based on five histone mod-ification marks, 57.0% of identified potential enhancers over-lapped with experimentally supported enhancers in the pilot ENCODE region in HeLa cells [9].
In fact, SVM models are closely related to ANNs.
SVMs are alternative training meth-ods for multi-layer perception classifiers, in which the weight of the network is found by solving a quadratic programming problem with linear constrains, rather than by solving an unconstrained minimization problem in ANNs [26].
A com-parison in HeLa cells between ChromaGenSVM and TDNN showed that ChromaGenSVM recovered 70.2% of the p300-bound putative active enhancers, while TDNN achieved a pre-cision of 84.0% [9].
However, due to different feature sets used by these two models, these data do not necessarily indicate that SVM is more effective than ANN for active enhancer recognition.
Another type of approaches try to model the joint distribu-tion of states and associated features with graph, generally termed as probabilistic graphical models.
The nave Bayes clas-sifier (NBc) is the simplest one of this type [68].
For enhancer recognition, NBc learns the conditional probability of each feature related to enhancer activity from a training data.
For example, a NBc on 6-mer features has been trained to detect active enhancers in the mouse genome [44].
However, com-pared with a SVM model with the same feature set [area under receiver operating characteristic curve (AUC) > 0.9], the NBc preformed significantly less accurately in discriminating active enhancers from random sequences (AUC < 0.79).
HMM is another example in probabilistic graphical models.
The current model of the genome is a linear combination of stated DNA sequences, e.g., promoter, enhancer or coding region.
By assuming that the state of any locus is only dependent on its nearest neighbor, HMM provides a natural solution for the task of segmenting the stated DNA sequences [25].
For exam-ple, Kharchenko and coworkers used a HMM to identify the prevalent combinatorial pattern of 18 histone modifications and captured the overall complexity of chromatin profiles ob-served in Drosophila S2 and BG3 cells with 9 states [60].
They found that enhancer regions are always enriched with H3K4me1, H3K27ac and H3K18ac.
A similar strategy was implemented in ChromHMM, which mapped 15 chromatin states in nine human cell lines [61].
BN represents another probabilistic graphical model that allows effective representa-tion of the joint probability distribution over feature set [24].
BN provides a powerful framework for modeling the compli-cated hidden relationships that explain the observed chromatin patterns in a genome.
For instance, SEGWAY used BN tech-niques to simultaneously segment and cluster 1% of the hu-man genome with 31 ENCODE signal tracks including histone modifications, TF binding and open chromatin, and revealed active enhancer associated patterns at nucleosomal resolution [69].
BN has also been applied to predict active enhancers in Drosophila [6], and the trained BN identified a conditional link between the H3K79me3 and H3K27ac marks and enhancer activity.
This BN model achieved better perfor-mance (AUC = 0.82), compared to the aforementioned NBc model.
Conclusion and outlook Enhancers are regulatory DNA elements that can activate transcription largely independent of their location or orienta-tion.
Often, enhancers regulate gene expression in a tissue-spe-cific manner and play important roles in cell differentiation [17].
In this review, we have described the general computa-tional strategies for enhancer prediction.
It has been suggested that H3K4me1 and p300 binding signatures are the most pre-dictive features for active enhancer recognition [5,46], how-ever, this notion may be disputed by new data.
For example, a recent study found that H3K79me3 and H3K27ac, instead of H3K4me1, are predictive for cell type specific enhancer activity during embryonic development [6].
Recently, a more complicated picture, which involves nuclear organization, chromatin structure and non-coding RNAs, is emerging for enhancer activation.
Accumulating data suggested that the insulators are critical in the regulation of enhancerpromoter interaction which is believed to be accomplished by long-range inter-or intra-chromosomal chromatin interactions [70].
From the perspective of computational biology, the field of enhancer research is now moving toward the modeling of 3D chromatin structure in nuclei, to reveal the principle of enhan-cer-promoter interactions.
Polymer models are valuable tools in 3D chromatin structure study, e.g., the dynamic random loop model [71] and the fractal globular model [72].
To under-stand enhancers in the context of gene regulatory networks, it is necessary to integrate data from ultra-heterogeneous data sources in this big data era.
For example, enhancer tran-scribed RNAs (eRNAs) were recently found prevalent at en-hancer loci [47].
Some of such non-coding RNAs even act like enhancers [73].
Therefore, the integration of RNA-seq data is essential for a model which aims to understand eRNA associated enhancer activity.
Competing interests The authors have declared that no competing interests exist.
Acknowledgements We apologize to many authors whose important works could not be cited owing to space limitations or our ignorance.
This work was supported by grants from the National Natural Sci-ence Foundation of China (NSFC, Grant No.
31271398 and Wang C et al/ Computational Methods for Enhancer Prediction 149 91131012) and 100 Talents Project to ZZ, NSFC (Grant No.
91019016) and National Basic Research Program of China (NBRPC, Grant No.
2012CB316503) to MQZ.
ABSTRACT Summary: Data fusion methods are powerful tools for evaluating experiments designed to discover measurable features of directly unobservable systems.
We describe an interactive software platform, Visual Integration for Bayesian Evaluation, that ingests or creates Bayesian posterior probability matrices, performs data fusion and allows the user to interactively evaluate the classification power of fusing various combinations of data sources, such as transcriptomic, proteomics, metabolomics, biochemistry and function.
Availability: http://omics.pnl.gov/software/VIBE.php Contact: bj@pnl.gov Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION The goal of data fusion is to integrate different types of data about a system to create models that are more complete and accurate than those derived from any individual data source, i.e.
the whole is greater than the sum of its parts.
The improvement of statistical classification, and direct applications such as prediction of protein function, is often the end goal of data fusion; however, heterogeneity of the data (varying dynamic range and specificity) presents a major challenge.
Methods that transform the data into a common form, such as kernel matrices or Bayesian posterior probabilities, are often used (Hwang et al., 2005; Jarman et al., 2008; Lanckriet et al., 2004; Troyanskaya et al., 2003; Webb-Robertson et al., 2009), since after applying such methods, fusion is simply a matter of merging matrices in a statistical manner.
Fusion methods look to take advantage of orthogonal information captured by multiple analytical platforms that, when taken together, increase the classification power over that of a single measurement platform, thus allowing more accurate and complete predictions of the phenotype of interest.
However, in many cases improvement is only achieved with a subset of the available data sources (Lu et al., 2005), and therefore it is important to provide an intuitive interpretation of these results in an interactive form that can be used to evaluate the impact of each individual data stream in the context of the overall integrated analysis.
Visual Integration for Bayesian Evaluation (VIBE) 2.0 offers a simple and flexible approach to fuse To whom correspondence should be addressed.
complementary datasets and dynamically evaluate the contribution of each dataset.
VIBE 2.0 is a stand-alone software tool that allows a user to explore the effects of including or excluding specific data sources in a Bayesian fusion analysis.
VIBE works by integrating probability models from multiple data streams.
The software can either ingest precomputed probability models or create them from the raw data.
The statistical methods used to derive the probability models and the data that is included in the fusion can be modified on the fly to analyze the system dynamically.
2 ANALYSIS CAPABILITIES VIBE 2.0 takes as input either raw datasets or precomputed probability models for each data type.
The probability model is a matrix where each value (i,j) is the probability of observing a specific known experimental group (j) given a sample (i) associated with one or more datasets.
To create these probability models VIBE uses statistical learning algorithms, including nave Bayes classification (Mitchell, 1997), degree of association (Jarman et al., 2000), k-nearest neighbors (Ativa, 2005) and multinomial logistic regression (McCullagh and Nelder, 1990).
These statistical learning algorithms compute the probability of observing the specific data associated with a sample given a particular experimental group.
Bayesian statistics are used to generate the posterior probabilities that are represented in the probability matrices used by VIBE (Webb-Robertson et al., 2009).
For each data source, VIBE 2.0 then calculates the classification accuracy (the fraction of samples assigned to the correct experimental group) providing the user a baseline that shows quantitatively the effectiveness of each individual analysis platform.
A class assignment table is also graphically displayed, depicting the experimental groups into which the true samples from an experimental group are classified.
The visualization allows the user to gain insight into the efficacy of the individual platforms, for example, showing that a particular data type is unable to distinguish between two of the experimental groups.
The user then selects a subset (or the full set) of the data sources to be included in the integrated analysis and VIBE 2.0 performs a Bayesian fusion and gives the classification accuracy based on the integrated probability model (Webb-Robertson et al., 2009).
As the fusion calculation is almost instantaneous, the user can experiment with multiple combinations of the input data sets to evaluate the impact of including each data set in the fused analysis.
The Author(s) 2009.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:31 7/1/2010 Bioinformatics-btp639.tex] Page: 281 280282 VIBE 2.0 3 IMPLEMENTATION VIBE 2.0 is built in MATLAB 2009b from The Mathworks, Inc. and is packaged, using Version 4.11 MATLAB Compiler, as a stand-alone executable for the Windows platform.
The application consists of three graphical user interface screens.
The first two screens are associated with the user input where the data sources are specified, as well as the statistical methods that will be used to analyze each dataset.
The third or analysis screen then provides visualization of the data integration analysis.
3.1 Input screen On the input screen (Fig.1A), the user specifies the source data files containing the class matrix (defining the true experimental group of each sample in the experiment) and the raw data or probability matrices for each individual data source to be used in the integration.
The data handling screen (Fig.1B) is used to select the type of data for upload and the statistical method to be used to create the probability model.
VIBE 2.0 does not perform any data quality checks beyond assuring dataset sample sizes match and the data have appropriate values for the statistical method to be employed.
The assumption is that the data are of adequate quality and has been properly normalized prior to analysis.
VIBE 2.0 does offer auto-scaling of the data, which will normalize all variables to have a common mean of zero and unity variance.
The uploaded files can be MATLAB (.mat), Microsoft Excel (.xls or .xlsx) or flat text (.txt) files.
There are also fields in the input screen where the user may also enter a name, an abbreviation and a brief description for each dataset, as well as names for each experimental group if they are not specified in the class file.
Once all information is entered, the user presses the Continue button to launch the analysis screen.
3.2 Analysis screen The analysis screen (Fig.1C) has two visualization sections, one displaying the analysis results of each individual data source and a second displaying the results of the data fusion.
Although the example shown has three data sources, up to six are viewable simultaneously.
Upon launch, the classification accuracy and the class assignment table for each individual data source are calculated and displayed.
The class assignment table is displayed as a plot with true class along the left axis and predicted class along the top axis, where the color at each location represents the fraction of samples classified into the associated class.
Thus, a diagonal line of red boxes running top left to bottom right represents perfect classification.
The user selects the subset of datasets to use in the integrated analysis via the Use in Integration buttons adjacent to each data source (default is all selected).
The Integrate button calculates the integrated probability model and displays the classification accuracy and the class assignment table for the fused analysis.
Multiple combinations can be explored interactively as the calculation is nearly instantaneous.
Additional features are available to facilitate the use of the integrated results in further analysis.
Optional annotations can be added and the Save Screen button saves a jpeg image of the current state of the analysis screen.
The Output File button exports a (.xls) file containing results from the integrated analysis giving, for each sample in the experiment, the true class, the predicted class and probability of being assigned to the predicted class.
A B C Fig.1.
(A) The input screen is shown as it appears with data loaded for fusion.
(B) Shows the data handling screen for the third dataset (MALDI) for leave-one-out cross-validation.
(C) Output analysis screen showing the results of integrating all three datasets.
4 CASE STUDY A previously described experiment to detect early response in mice to Francisella novicida (FTN) is shown in Figure 1 (Webb-Robertson et al., 2009).
The experiment shows seven classes where the mice are exposed to one of three microbes at both 4 and 24 h; FTN, Pseudomonas aeruginosa (PA), or an avirulent strain of FTN that contains a mutation to the transcriptional regulator mglA (MGLA).
Bronchial alveolar lavage fluid was collected from each animal and analyzed using three instrument platforms: nuclear magnetic resonance spectroscopy (NMR), matrix assisted laser desorption/ionization mass spectrometry (MALDI) and accurate mass and time mass spectrometry (Orbitrap).
Features were extracted and a probability model was constructed for each instrument using either nave Bayes classification (Mitchell, 1997) 281 [10:31 7/1/2010 Bioinformatics-btp639.tex] Page: 282 280282 N.Beagley et al.
or degree of association (Jarman et al., 2000).
The probability matrices as input to VIBE can either be the result of independent test data or the result of cross-validation, as is the case for this example.
Details of this analysis can be found in the user manual available through the software.
VIBE 2.0 was used to explore the metabonomics and proteomics results using different combinations of the three instruments in an integrated analysis.
As demonstrated in Figure 1, a higher level of classification accuracy is achieved by using all three datasets than can be achieved from any one individual dataset.
This example also demonstrates that incorporating data from additional instruments does not always improve results.
The probability models were developed using leave-one-out cross-validation, which is equivalent to the number of separations as samples in the data (Fig.1B).
The classification accuracy of using NMR and MALDI is 61% compared with 78% using MALDI alone (data not shown).
Similarly, classification accuracy is 81% with MALDI and Orbitrap compared with 83% with Orbitrap alone (data not shown), suggesting that MALDI analysis does not complement the NMR and Orbitrap datasets as might have been expected.
However, the integration of only NMR and Obitrap attains an accuracy of 86%, which is the same as integrating all three datasets (Fig.1C).
ACKNOWLEDGEMENTS PNNL is a multiprogram national laboratory operated by Battelle for the U.S. Department of Energy under Contract DE-AC06-76RL01830.
Funding: U.S. Department of Energy through the Environmental Biomarkers Initiative at Pacific Northwest National Laboratory; National Institutes of Health (grants U54 016015 and U54 AI057141).
Conflict of Interest: none declared.
ABSTRACT Motivation: Analysis of relationships of drug structure to biological response is key to understanding off-target and unexpected drug ef-fects, and for developing hypotheses on how to tailor drug therapies.
New methods are required for integrated analyses of a large number of chemical features of drugs against the corresponding genome-wide responses of multiple cell models.
Results: In this article, we present the first comprehensive multi-set analysis on how the chemical structure of drugs impacts on genome-wide gene expression across several cancer cell lines [Connectivity Map (CMap) database].
The task is formulated as searching for drug response components across multiple cancers to reveal shared ef-fects of drugs and the chemical features that may be responsible.
The components can be computed with an extension of a recent ap-proach called Group Factor Analysis.
We identify 11 components that link the structural descriptors of drugs with specific gene expression responses observed in the three cell lines and identify structural groups that may be responsible for the responses.
Our method quan-titatively outperforms the limited earlier methods on CMap and iden-tifies both the previously reported associations and several interesting novel findings, by taking into account multiple cell lines and advanced 3D structural descriptors.
The novel observations include: previously unknown similarities in the effects induced by 15-delta prostaglandin J2 and HSP90 inhibitors, which are linked to the 3D descriptors of the drugs; and the induction by simvastatin of leukemia-specific response, resembling the effects of corticosteroids.
Availability and implementation: Source Code implementing the method is available at: http://research.ics.aalto.fi/mi/software/GFAsparse Contact: suleiman.khan@aalto.fi or samuel.kaski@aalto.fi Supplementary Information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Modeling and understanding the diverse spectrum of cellular responses to drugs is one of the biggest challenges in chemical systems biology.
Some of the responses can be predicted for targeted drugs, which have been designed to bind to a specific protein that triggers the biological response.
The binding of a drug to a target largely depends on the structural correspondence of the drug molecule and the binding cavity of the target mol-ecule, which can be modeled in principle, given ample computa-tional resources.
Off-target effects are harder to predict.
They are dependent on the cell types, individual genetic characteristics and cellular states making the spectrum of responses overwhelmingly diverse.
The less well-known the drugs mechanism of action and the characteristics of the disease, the harder the prediction from first principles becomes.
The most feasible way to approach this challenge in an unbiased way, which does not require prior knowledge of all on-and off-target interactions of drugs, is to collect systematic measurements across different drugs, cell types and diseases and search for response patterns correlating with the characteristics of the drugs.
The patterns found can be used as evidence for hypotheses on underlying action mechanisms or directly in predicting the responses.
The Connectivity Map (CMap; Lamb et al., 2006) described the basis for a data-driven study of drugeffect relationships at a genome-wide level.
CMap hosts the largest collection of high-dimensional gene expression profiles derived from treatment of three different human cancer cell lines with over one thousand drugs.
The CMap data have been used in a multitude of studies revealing new biological links between drugs and between drugs and diseases.
Genome-wide gene expression responses from the CMap have been used to discover clusters of drugs having simi-lar mechanisms of action, resulting in novel findings, such as effects of heat shock protein (HSP) inhibitors and identification of modulators of autophagy (Iorio et al., 2010).
The CMap data have also been successfully used in large-scale integrative studies including the analysis of regulation of drug targets (Iskar et al., 2010), hERG annotations to predict novel inhibitors (Babcock et al., 2013) and drugs interactions with protein networks (Laenen et al., 2013).
Quantitative structureactivity relationship analysis (QSAR; Cramer et al., 1988) is a widely adopted approach to studying drug responses.
Traditionally, univariate biological activities are predicted using a range of methods, including classical regres-sion, support vector machines and Random Forests.
The key challenge when moving from traditional QSAR to system-wide analysis of chemical effects is how to relate structural features to genome-wide cellular responses.
Integration of chemical structures with genome-wide responses has become a major research direction in chemical systems biol-ogy (Iskar et al., 2012; Xie et al., 2012).
Keiser et al.
(2009) studied structural similarities between ligand sets while Klabunde and Evers (2005) used proteinligand complexes to predict off-targets.
To infer potential indications for drugs, Gottlieb et al.
(2011) combined similarities from chemical structures, gene expression profiles, protein targets and several*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com well , , ,-large structure , systems-other datasets.
Atias and Sharan (2011) modeled linkage between structural descriptors of drugs and their side effects using canon-ical correlation analysis (CCA; Hotelling, 1936).
Structures have also been used with genomic datasets to predict toxicity and complex adverse drug reactions (Russom et al., 2013).
Recently, Menden et al.
(2013) combined structures of drugs and mutation information of cell lines to predict drug cytotox-icity in a series of cell lines.
Relationships between structural descriptors of drugs and their gene expression profiles have also been studied.
Cheng et al.
(2010) examined similarities between chemical structures and molecular targets of 37 drugs that were clustered based on their bioactivity profiles.
Low et al.
(2011) classified 127 rat liver samples to toxic versus non-toxic responses, based on com-bined drug-induced expression profiles and chemical descriptors, and identified chemical substructures and genes that were re-sponsible for liver toxicity.
In a broader setting, when the goal is to find dependencies between two data sources (chemical struc-tures and genomic responses), correlation-type approaches match the goal directly, and have the additional advantage that a predefined classification is not required.
Khan et al.
(2012) generalized structure response analysis to multivariate correlations with CCA on the CMap.
Because of the limitations of classical CCA, their study was restricted to a limited set of descriptors (76) and genomic summaries (1321 genesets), and did not attempt to take into account the data from the three separate cell lines.
In this article, we present the first probabilistic approach to the problem of integrated analysis of effects of chemical structures across genome-wide responses in multiple model systems.
We extend the earlier work in three major ways: (i) instead of using only two data sources (as in classical CCA), we used the recent Bayesian group factor analysis (GFA) method (Virtanen et al., 2012) that generalizes the analysis to multiple sources, here three cell lines and two sets of chemical descriptors.
(ii) Our Bayesian treatment with feature-level priors enabled us to cope better with the uncertainties in the high-dimensional data.
(iii) We included a more informative set of 3D chemical descriptors to complement the widely used 2D fingerprints, which are recognized to only explain limited aspects of drugs (Schneider, 2010).
Our goal was to uncover the big picture of relationships be-tween chemical structure parameters and genome-wide re-sponses, in a data-driven fashion (Fig.1).
The data came from CMap, 11 327 gene expression responses in three cell lines (HL60-Blood Cancer/Leukemia, MCF7-Breast Cancer and PC3-Prostate Cancer; Lamb et al., 2006) and from two sets of chemical descriptors: 780 3D Pentacle descriptors of drugs (Duran et al., 2008) and 2769 functionally relevant structural fragments (FCFP4; Glen et al., 2006) as 2D fingerprints of the drugs.
These five datasets consist of samples from the 682 drug treatments, coupled by the detailed drug identity.
We analyzed the statistical relationships between the datasets by decomposing them into a set of interpretable components.
Our method quan-titatively outperformed previous studies, thereby validating the approach.
We rediscovered findings reported earlier as well as identified novel drug associations and detailed structure response relationships.
2 METHODS 2.1 Gene expression datasets We used the CMap (Lamb et al., 2006) gene expression data as a measure of the biological response of the three cancer cell lines to drug treatments, forming the gene expression datasets.
The CMap hosts over 7100 gene expression profiles including technical replicates treated with 1309 drugs and is the largest available resource of its kind.
Responses from a subset of these drugs (682) were measured on all of the three cell lines, namely, HL60 (leukemia), MCF7 (breast cancer) and PC3 (prostate cancer cell line).
We obtained the raw gene expression profiles from the CMap and used the data from the most abundant microarray platform (HT-HG-U133A).
The data were preprocessed using the Robust Multiarray Averaging (RMA; Irizarry et al., 2003) and drug treatment versus control (log2) dif-ferential expression was calculated batchwise (Khan et al., 2012).
Technical replicates were merged by taking the mean of each gene.
This resulted in gene expression profiles for the 682 drugs having meas-urements over all three cell lines.
To reduce noise, we adapted the ap-proach of Iorio et al.
(2010) for our setting, by retaining the expression of top 2000 up-and 2000 downregulated genes for each sample, while con-sidering the rest as noise (set to zero).
The threshold was large to retain diverse effects and removed small values.
These profiles formed three biological response datasets (one for each cell line), each being a differ-ential gene expression matrix of 682 drugs times 11 327 genes.
2.2 Chemical descriptor datasets The chemical space of drugs was represented using two different types of chemical descriptors, namely, the 2D fingerprints FCFP4 and 3D de-scriptors Pentacle.
The FCFP4 (functional connectivity fingerprints of radius 4; Glen et al., 2006) are circular topological fingerprints designed specifically for structureactivity modeling and similarity searching.
They are rapidly computable and heavily used in a wide variety of applications (Rogers and Hahn, 2010).
Each dimension of the fingerprints represents a certain 2D fragment of the compounds, interpretable as presence of certain substructures, typically stereochemical information, and allows easy visual inspection of structures.
Therefore, FCFP4 can be used to identify the core 2D substructures that make compounds structurally similar and are responsible for biological activity.
The more complex 3D descriptors Pentacle (Duran et al., 2008) cap-ture the functional properties of the compounds using molecular inter-action fields.
They are able to group together compounds with dissimilar chemical structures and yet having the same type of molecular field prop-erties.
This is especially important in our study where the aim is to find small molecules that share biological functions despite structural dissimi-larity.
Most of the traditional fingerprints, like MACCS (Molecular Access System) and FCFP4, are superior to recognize 2D structural simi-larity but unfortunately unable to recognize structurally unrelated and yet biologically similar compounds binding into the same binding pocket.
The opposite is true with most (if not all) field-based similarity methods like Pentacle, which find more effective distant similarities; therefore, we decided to combine both approaches.
In the earlier work, Khan et al.
(2012) had used VolSurf descriptors to represent molecular properties.
Although VolSurf is an optimal method for physicochemical properties estimation, it is not able to describe pharmacophore features extensively, unlike the Pentacle descriptors, and thus is not an option in our study.
Pentacle field distance descriptors were computed using Pentacle v 1.0.4 (http://www.moldiscovery.com/soft_pentacle.php), by Molecular Discovery.
The descriptors were calculated for all the available 10 probe sets, namely, D2, O2, N2, T2, DO, DN, DT, ON, OT, NT, where D is dry probe to represent hydrophobic interactions, O is carbonyl oxygen probe to represent hydrogen bond donor feature of the molecules and N flat probe of Nitrogen is the hydrogen bond acceptor, while T is TIP probe representing shape of the molecule, in terms of steric hot spots.
i498 S.A.Khan et al.--Canonical Correlation Analysis ( ) Due to paper Instead , , ,-Expression Datasets Connectivity Map Connectivity Map Leukemia Breast Cancer Prostate Cancer Connectivity Map-.-very , 1 1 Descriptor Datasets-2-dimensional 3-dimensional very While For each probe set, 78 descriptors were obtained, representing the inter-action potentials of probes at different distances, resulting in 780 descrip-tors in total.
Distances in the Pentacle descriptors are true distances between putative interaction sites (hot spots) and are thus connected to the size of the compound and distances between potential pharmacopho-ric features.
This results in a 682 780 data matrix, with each row being a drug and the 780 columns representing the Pentacle descriptors.
This forms the first chemical dataset in our study.
The 2D FCFP4 represent the chemicals as structural fragments.
In FCFP, the fragments are not predefined, rather computed dynamically and thus can represent variation in novel structures.
The FCFP4 finger-prints were computed using Pipeline Pilot Student Edition software (http://accelrys.com/products/pipeline-pilot/), by Accelrys.
A total of 2769 unique structural fragments are found, and the fingerprints are rep-resented as a matrix of 682 compounds 2769 fragment descriptors.
This forms the second chemical dataset in our study.
2.3 Model: GFA We search for relationships between chemical descriptors and biological responses, as clues to the key underlying biological processes.
GFA is a model designed to capture such relationships (statistical dependencies) by explaining a collection of datasets (views) by a set of factors or compo-nents, which form a combined low-dimensional representation (Virtanen et al., 2012).
In themulti-view setting, each component is active in a subset of the datasets and is a simplified model of an underlying process visible in those sets.
The task solved by GFA is to separate the shared compo-nents that capture the structurebiology relationships from the rest of the data: the former are visible in all or a subset of the datasets, whereas components active in a single view describe variation specific to that particular view or noise.
Given a collection of M datasets X 1 2 RND1 .
.
.X m 2 RNDM , con-sisting of N co-occurring samples x m n , GFA finds a set of latent compo-nents (with upper limit K, see below).
Each dataset is assumed to have been generated as a linear combination of latent components Z 2 RNK, with weights of the combination given by a loadings matrix W m 2 RDmK: Assuming normal distributions for simplicity, the model is x m n Normal W m zn;S m  ; znNormal 0; I ; 1 where zn is the n th row of Z, and S m is a diagonal noise covariance matrix.
GFA is special in that the projections W are required to be group-wise sparse, i.e.
all the elements W m :;k are set to zero for the com-ponents k that are not active in the mth dataset.
The components with non-zero projections between two or more views capture dependencies between the views.
To increase the interpretability of the model, we extend GFA by intro-ducing element-wise sparsity in addition to the group sparsity for the projection matrices, matching the biological prior assumption that each process typically activates only a subset of genes.
We introduce element-wise automatic relevance determination (ARD; Neal, 1995) prior for the projection weight matrices, pushing irrelevant weight values W m d;k toward zero and making each component element-wise sparse.
For the group sparsity, we apply the group spike and slab prior where the binary vari-able H m k controls the activity of the k th component in the group m. The prior is W m d;k H m k Normal 0; m d;k 1  + 1H m k  0; H m  k Bernoulli k ; kBeta a; b ; m d;k Gamma a; b : 2 IfH m k becomes zero, all values inW m :;k will be set to zero.
To complete the model description, we set an uninformative before the diagonal elem-ents of the precision matrix S m 1.
Here we made two assumptions, (i) Fig.1.
Overview of the symmetric multi-structure to multi-response decomposition.
(A) The five datasets spanning the common 682 drugs are (B) decomposed into components by GFA.
Components of Type 1 represent shared patterns in both chemistry and biology, whereas Type 2 describes biology-only or chemistry-only variation (not as useful in our case).
(C) Each shared component identifies key structures and genes of an underlying biological process i499 Data-driven genome-wide structure-response links x functional connectivity fingerprints ( )Group group Factor factor Analysis analysis ( ) Group Factor Analysis ( ) `` '' ,-: ,-s prior to normal distributions for simplicity and (ii) sparsity.
Sparsity was imple-mented by combining the previously (Klami et al., 2013) separately used beta-Bernoulli formulation and the element-wise normal-gamma ARD.
We represent our (M=5) datasets as matrices of drugs versus fea-tures.
The rows represent the samples (drugs), and the columns are the features (genes or chemical descriptors).
Drugs pair all the views, i.e.
a row in all matrices corresponds to the same drug.
A total of N=682 drugs were used in the study.
The features of the chemical descriptors, Pentacle (m=1) and FCFP4 (m=2) are D1=780 Pentacle probe fields and D2=2769 fragments, respectively.
The biological responses of the three cell lines (m=3,4,5) are represented by differential expression of Dm=11327 genes each.
The hyperparameters are set to a; b=103 and a; b=1, to obtain uninformative priors.
We initialize the model by sampling the latent vari-ables from the prior.
The model parameters (W m :;k ,H m k , S m , m d;k , k, Z) are then learned from the data using Gibbs sampling.
The number of components is optimally learned from data by initializing K to be large enough, such that sparsity assumptions push some to be inactive.
Here for computational reasons, we set K=80, a value significantly larger than the actual number of shared components, and let the noise model represent the rest of the data.
For sampling, we ran 10 chains and selected for further analysis the one having its likelihood closest to the mean of non-outlier chains.
The first 5000 samples were discarded as the burn-in, and the chain was run for 1000 more iterations, with a thinning factor of 5.
The mean value of the samples was used as a representation of the model.
As a sanity check, we verified that our shared components had over 70% similarity in top genes and descriptors with the second (non-used) chain.
The models complexity is O(NDK2+K3) where D=sum(D1:M).
The current implementation ran for 5 days on a stand-ard desktop computer consuming 6 GB memory.
For interpretation, we represent each component by listing the high-valued latent scores Z and projection values W. For the latent scores, we performed a permutation test to detect the most significantly (q-value50.05) activated drugs, while for the projections we inspected the top 30 elements.
3 RESULTS Figure 2 gives an overview of the types of components discovered by the model.
For studying structureactivity relationships, the most important are the components shared by one or more chemical view and one or more of the cancer subtypes.
The com-ponents active in only the expression datasets represent drug responses not captured by the used chemical descriptors, and components only active in the chemical datasets represent bio-logically irrelevant structural variance.
Additionally, components active in only a single dataset may represent dataset-specific noise.
We found 11 shared components, which will be discussed below.
The detailed structureresponse relationships discovered from all the shared components are visualized in Supplementary Figure S1 and tabulated in a usable format in Supplementary Table S2.
3.1 Validation via chemical biology ontology We started by quantitatively evaluating how closely related the drugs in the shared components are in terms of known chemical biology relationships and compared our data with those of two previous studies (Iorio et al., 2010; Khan et al., 2012) that inves-tigated drug actions using the same CMap database version.
The established chemicalbiology relationships were obtained from the ontology Chemical Entities of Biological Interest (ChEBI; Degtyarenko et al., 2008), which is the largest such ontology of small compounds.
ChEBI links compounds with respect to chemical structure, biological roles they are known to play and their applications.
Examples of classifications are antibiotic, coenzyme and agonist (biological); donor, ligand, in-hibitor (chemical); and pesticide, antiasthmatic (applications).
ChEBI was downloaded as a graph and contained paths between 328 (of 682) of our compounds via 611 ontology terms (http://www.ebi.ac.uk/chebi).
The average similarity (inverse path distance) of drugs within the shared GFA components was consistently higher than the corresponding similarities of Khan et al., (2012) and Iorio et al., (2010) and random sets of compounds (Fig.3).
The largest path length (16) in ChEBI linked all drugs, whereas the smallest (2) linked only the most similar.
Interestingly, the difference in GFA and others on small path lengths was higher than that on larger ones, indicating that drugs closely connected in ChEBI were even better found by GFA.
Fig.3.
Quantitative validation of chemical biology similarity of drugs in shared GFA components.
Drugs in the same GFA component (blue squares) had a consistently higher mean average similarity (y-axis) in ChEBI than either of the earlier studies, and random sets of compounds, over the entire range of ChEBI path lengths (x-axis).
To access the rela-tive contribution of the 3D descriptors we additionally plotted results with components containing them (dashed line) and components contain-ing only 2D descriptors (dotted line), demonstrating that both descriptors are valuable.
Error bars (red) are one stdandard over 1000 randomly generated sets Fig.2.
Summary of the GFA components.
The plot demonstrates activ-ity (black is active) of each component (y-axis) over the five input datasets (x-axis).
Each component is active in some or all of the datasets.
Components shared by (active in) both chemical descriptor and expres-sion datasets capture structureresponse relationships i500 S.A.Khan et al.
Normal Beta .
, , ,-a total of-Chemical Biology Ontology chemical , chemical Chemical Biological , Applications-out while  3.2 Component interpretations We next analyzed the shared components in detail.
Each com-ponent connects a set of structural drug properties and gene ex-pression changes, forming a hypothesis of a structureactivity relationship.
A component can be characterized by the set of drugs that activate it the most, and by the set of genes that are expressed differentially when the component is active.
We first compared the findings with the two other studies that have investigated drug actions using the same CMap database (Iorio et al., 2010; Khan et al., 2012).
Of the 11 shared GFA components, the majority of the drugs in seven components were similar to the clusters found by Iorio et al.
(2010), while three components captured structurally driven cell-specific responses they had missed.
Compared with the other earlier study (Khan et al., 2012), the majority of the drugs in 6 of the 11 GFA com-ponents matched a corresponding structure-response subcompo-nent of Khan et al., (2012), again indicating conformance to known results.
Our components also revealed several novel drug actions because of cell-type specificity and advanced 3D descriptors that were missed by both of these earlier studies, and are presented below.
Detailed interpretation of all the 11 shared components is pre-sented in Supplementary Table S1.
The components are num-bered in the order of the amount of variation they captured; the cell line-specific components identified by the model are separ-ately ordered with the prefix SP.
One component (SP3) captured outlier response of a single drug and was omitted from further analysis.
The majority of the components captured effects shared among all the three cell lines, whereas five components had re-sponses that were cell line-specific (Components SP1, SP2, SP4), dominant in a specific cell line (Component 7) or revealed some cell line specificity indications for an interesting drug (Component 1).
The 2D structural features were active in most components, identifying similarities in structurally analogous drugs.
The pentacle descriptors captured similarities in five com-ponents, four of which indicated novel responses of drugs that have not been reported before.
We discuss these four novel com-ponents in detail below.
One of them had cell line-specific effects (SP2), whereas the remaining cell line-specific components (SP1 and SP4) are summarized in Table 1.
Component 1 was characterized by cardenolides.
The top seven drugs of the component, lanatoside C, digitoxigenin, digoxin, digoxigenin, ouabain, helveticoside and strophantidin belonged to this class.
The primary activity of the other drugs anisomycin, lycorine and cicloheximide is protein synthesis inhibition, and bisacodyl is used as a laxative through stimulation of secretion in the colon.
Cardenolides act onNa+/K+ pumps and are known for ion flux alterations.
Interestingly, the other compounds of Component 1 also appeared to affect membrane potassium ion flux.
Bisacodyl and anisomycin activate K+ flux, lycorine is known to reduce membrane potential (indicative of potassium efflux) and, indicative of affecting K+, emetine needs to be ad-ministered with potassium to reduce cardiotoxicity.
Interestingly, bisacodyl exhibited the response in MCF7 and PC3 cells only, suggesting that its target may be expressed selectively.
On the structural side, the top four FCFP4 fragments collect-ively represented the correct core 2D response triggering sub-structure in all the seven cardenolides, as detailed in Figure 4.
The other two key drugs, bisacodyl and anisomycin, were differ-ent from cardiac glycosides in terms of 2D structures, but the Pentacle descriptors indicated potential field similarities on ON, OT and NT probes.
These probes referred to existence of common structural pharmacophoric features: hydrogen bonding and shape-related features.
The 3D descriptors may therefore indicate that these drugs bind the same ion channels as the cardenolides.
Component 3 captured protein synthesis inhibition.
All drugs in the component are known to inhibit protein synthesis but each in a different way.
The only exception, alexidine, is a derivative of clorhexidine, which is used as an antibacterial mouth wash. Interestingly, it has been described to have anticancer cell activity through an unknown target (Yip et al., 2006).
The model identi-fied pentacle probe fields of D2, DO and DT (shape and lipophilicity-related probes) that relate alexidines protein synthe-sis inhibition response with the known protein synthesis inhibitors.
Component 5 was HSP90 inhibition response.
The component contains the three similar drugs geldanamycin, tanespimycin, alvespimycin, and on the 2D structure level dissimilar 15-delta prostaglandin J2 (PGJ2) and puromycin.
Geldanamycin and its two analogs tanespimycin and alvespimycin are HSP90 inhibi-tors, and the latter two have been explored in the clinic as anticancer drugs.
PGJ2 has also been described as having anticancer activity through an unknown mechanism, causing in-hibition of several cancer survival signals.
Puromycin is reported Table 1.
Shared components having cell line-specific response Drug description Biological interpretation Structural P. SP1 Antimetabolite (8-Azaguanine) used for antineo-plastic activity and anisomycin a protein synthesis inhibitor.
8-azaguanine has been used in leukemia (Colsky et al., 1955).
Protein synthesis inhibition in HL60 and PC3 cells only.
It could be interesting to explore 8-azaguanine as an anti-prostate cancer drug.
In a recent study, Wen et al.
(2013) also indicated 8-azaguanine for potential therapeutic efficacy in prostate cancer.
2D ring structures of 8-azaguanine SP4 Antiestrogen drugs Response visible in MCF7 (estrogen receptor) cell line only.
Pentacle ON/ OT fields.
Note: The components (rows) are summarized by their top drugs (Column 1), biological response (Column 2) and the structural properties (Column 3).
i501 Data-driven genome-wide structure-response links Interpretations structure to Out o 7 3 to out due toline , while which either-components component ,--component-while-7 4 7-do indeed Cardenolides-lipophilicity very very heat shock protein ( )--as an aminonucleoside antibiotic with a primary function of terminating ribosomal protein translation.
At the response level, this component appeared to be strongly inducing a heat shock response with many HSP and related genes being upregu-lated (see Fig.5, left).
The expression profile strongly indicated that PGJ2 and puromycin are also inhibiting HSP90.
PubChem drug-target data demonstrate that HSP90 targets have been re-ported as active in geldanamycin and its derivatives, while un-tested/unspecified for both puromycin and prostaglandin.
On the structural side, the 2D descriptors confirmed that puro-mycin and prostaglandin are dissimilar to the three geldanamy-cin analogs.
However, the Pentacle descriptors clearly indicated that N2, DN and NT fields shared a strong pattern across all the five drugs.
The patterns were only visible in features of smaller distances of these large molecules, indicating that only a small region of these compounds (polar atoms of all compounds) cre-ated the activity, whereas the rest of the structure is just needed to maintain the shape.
This fitted well with the observation that the drugs are overall structurally dissimilar.
At the smaller dis-tances, the structure responsible for biological response was char-acterized by N2: ligands hydrogen bonding capacity, DN: hydrogen bonding and lipophilicity and NT: hydrogen bond-ing/shape-based descriptors.
In geldanamycin and prostaglandin, this distance (see Fig.5 where N2 descriptor is plotted) was con-nected to polar ring atoms and more precisely corresponding hydrogen bonding positions.
These same positions, although in a different conformational arrangement (but with almost identi-cal distance), are critical in the binding of geldanamycin to HSP90.
Hence, while the expression data strongly argue for PGJ2 inhibiting HSP90 activity at some level, the structural in-formation suggests that this effect could be through a direct binding to HSP90 enzymes.
Component SP2 was characterized by responses to a set of corticoids, other steroids such as etynodiol, and surprisingly dif-ferent drugs simvastatin and repaglinide.
There appears to be a dual response: an HL60-specific metabolic regulatory response and an HL60 and PC3-selective anti-inflammatory response (Fig.6) with the MCF7 not exhibiting these responses at all, indicating that the relevant target or signal may be selectively expressed in HL60 and PC3 cells.
Both simvastatin (a choles-terol-lowering HMG-CoA reductase inhibitor drug) and repagli-nide (a diabetes drug) are highly dissimilar at the 2D level when compared with the corticosteriods, but both interestingly have been reported to have anti-inflammatory activities, likely because of targets other than the primary target(s).
Once again, Pentacle descriptors capture the underlying similarities between these drugs through NT and N2 fields, suggesting that the common gene expression patterns induced by the different drugs (cortico-steroids, simvastatin and repaglinide) is a result of binding the same targets.
4 CONCLUSIONS AND DISCUSSION We extended the drug response analysis paradigm from standard QSAR, of relating drug properties and univariate responses, to finding relationships between specific structural descriptors of drugs with the genome-wide responses they elicit in multiple cell lines.
The task was formalized as discovering dependencies be-tween multiple datasets and addressed using the state-of-the-art method GFA.
The approach identified structuregenomic re-sponse relationships as underlying components of the data and can be used as a tool for exploring such relationships from large-scale measurement datasets.
We quantitatively validated our structureresponse compo-nents over the established chemicalbiology relationships of ChEBI and found them to be better than earlier studies (Iorio et al., 2010; Khan et al., 2012) that did not account for separate cell lines and advanced 3D chemical descriptors.
Moreover, sev-eral drug groups we identified were consistent with earlier stu-dies, while several revealed interesting novel findings earlier studies had missed, demonstrating that our approach is viable for explorative multi-set structureactivity analysis.
These novel findings were clearly attributed to separate cell lines and advanced 3D descriptors in our formulation.
In a different set-ting, Yera et al., (2011) found 3D similarity to be more important Fig.4.
Structure identification in Component 1.
Left: the top four FCFP4 structural fragments identified by the model as strongly relating to the response of the drugs (right).
When combined, these fragments represent the core response triggering structure steroid backbone (shaded gray) in all the cardenolides i502 S.A.Khan et al.-very s very while very lipopholicity shape-H-s does to due to , Group Factor Analysis ( )-,---for off-target identification, and this was partially supported by our study as well.
The discovered components revealed interesting new findings of potential importance for revealing novel action mechanisms of drugs.
The 2D fingerprints highlighted important core structural groups primarily responsible for activity of similar drugs, such as the identification of the steroid backbone in cardiac glycosides and aromatic ring in HDAC (Histone deacetylases) inhibitors.
The joint analysis of data from multiple cell lines with advanced 3D Pentacle descriptors allowed us to identify relationships between drugs that were not known earlier.
If validated, this suggests an approach that could significantly help in medicinal chemistry and drug design.
For example, our data led to the identification of a previously unknown and novel shared mech-anism of 15-delta prostaglandin J2 (PGJ2) and HSP90 inhibitors.
Interestingly, PGJ2 and related prostaglandin analogs have re-peatedly been described in the literature for having anticancer activities, but their mechanism of action has not been clarified before (Fionda et al., 2007; Hegde et al., 2011; Zimmer et al., 2010).
Furthermore, our analysis revealed that simvastatin, a cholesterol-lowering drug, has a leukemia-specific response simi-lar to a range of corticosteroids.
This appears to be a significant finding as lovastatin, a close structural analog of simvastatin, was recently shown to selectively inhibit leukemic stem cells to-gether with several steroids (Hartwell et al., 2013).
Such systematic explorations raise the possibility for targeted interventions and will become a growing trend in the future as more large-scale datasets like the CMap will become available.
For drug designers, it opens up the opportunity to tailor drug molecules to match a desired gene expression fingerprint.
For medicinal chemists, it could help to increase understanding of actionmechanisms of existing drugs and revealing potential on-label and off-label applications for use in precision medicine.
ACKNOWLEDGEMENTS We thank Pekka Tiikkainen for generating the FCFP4 descriptors.
Funding: This work was supported by the Academy of Finland [140057 and Finnish Centre of Excellence in Computational Inference Research COIN 251170]; the Jane and Aatos Erkko Foundation; and the FICS doctoral program.
Confiict of Interest: none declared.
Fig.5.
Component 5 identified a novel HSP90 response of prostaglandin.
Left: gene expression response of the top seven drugs in the three cell lines (y-axis), over the top genes (x-axis) of the component, demonstrates HSP genes being strongly upregulated by the HSP90 inhibitors and by the strikingly different puromycin and prostaglandin.
Right: N2 descriptor in geldanamycin and prostaglandin connected to several polar ring atoms (red and blue).
The Pentacle feature (N2 distance range) found by GFA as related with HSP gene expression is represented with the yellow line Fig.6.
SP2: corticosteroids showing response specific to HL60 cells, while only minor regulation in PC3 and not at all in MCF7 i503 Data-driven genome-wide structure-response links h eat shock protein ( )-; Hegde etal., 2011 very ,
ABSTRACT Motivation: Biological data generation has accelerated to the point where hundreds or thousands of whole-genome datasets of various types are available for many model organisms.
This wealth of data can lead to valuable biological insights when analyzed in an integrated manner, but the computational challenge of managing such large data collections is substantial.
In order to mine these data efficiently, it is necessary to develop methods that use storage, memory and processing resources carefully.
Results: The Sleipnir C++ library implements a variety of machine learning and data manipulation algorithms with a focus on heterogeneous data integration and efficiency for very large biological data collections.
Sleipnir allows microarray processing, functional ontology mining, clustering, Bayesian learning and inference and support vector machine tasks to be performed for heterogeneous data on scales not previously practical.
In addition to the library, which can easily be integrated into new computational systems, prebuilt tools are provided to perform a variety of common tasks.
Many tools are multithreaded for parallelization in desktop or high-throughput computing environments, and most tasks can be performed in minutes for hundreds of datasets using a standard personal computer.
Availability: Source code (C++) and documentation are available at http://function.princeton.edu/sleipnir and compiled binaries are available from the authors on request.
Contact: ogt@princeton.edu 1 INTRODUCTION Whole-genome assays have now become pervasive, and the resulting wealth of data represents a new opportunity for biological discovery.
A single genome-scale dataset can capture a snapshot of cellular function; integrative analysis of hundreds or thousands of genome-scale datasets can provide even more extensive systems-level insights regarding gene interactions under diverse conditions (Troyanskaya, 2005).
Integrated approaches have already resulted in important biological discoveries (Hong et al., 2008; Myers and Troyanskaya, 2007), and the breadth and depth of possible analyses will only increase as additional experimental data is collected.
As the amount of data to be analyzed continues to increase, computational efficiency becomes a greater concern.
Specialized resources exist to enable very high-throughput computing for To whom correspondence should be addressed.
specific applications (Pekurovsky et al., 2004; Swindells et al., 2002), but few computational options exist allowing researchers to quickly mine large collections of genome-scale datasets.
To address this need, we have created the Sleipnir library for computational functional genomics.
The library contains algorithms and data types for efficiently manipulating and mining very large biological data collections.
The core C++ library can be integrated into computational systems to provide rapid analysis of functional genomic data.
Additionally, a variety of tools are provided that use the library to perform common tasks: microarray processing, Bayesian and support vector machine (SVM) learning and so forth.
Even when analyzing individual datasets, Sleipnir often outperforms existing utilities in processing time, memory usage or both (Table 1).
Tools provided with Sleipnir address common data manipulation requirements, in many cases processing hundreds of datasets on a standard desktop computer.
Additionally, the core Sleipnir library can be easily employed to efficiently apply new algorithms to complex biological data.
2 METHODS The Sleipnir library contains a wide variety of tools for consuming standard biological data formats, manipulating and normalizing data and performing machine learning and prediction.
These are discussed extensively in the user and developer documentation included with the library (http://function.princeton.edu/sleipnir) and are presented here in summary.
Sleipnir provides C++ classes to parse pairwise interaction data and standard microarray file formats.
Microarray data can be converted into pairwise similarity/distance scores using a variety of measures, discretized, normalized, randomized for bootstrapping or synthetic data production, split or merged, imputed or clustered.
To facilitate functional enrichment analysis, gene function prediction and gold standard generation from known gene functions and relationships, Sleipnir provides a uniform interface to several organism-independent function annotation catalogs.
Information from organism-specific annotations can be merged with these functional annotations.
Sleipnir also includes collections of data structures for dealing with common biological entities: gene identifiers, sets of genes, groups of related files, etc.
Other utility classes include resources for multithreading, a ready-made network client/server class and a variety of mathematical and statistical tools.
Sleipnir provides several tools for rapid machine learning and data mining.
The SMILE Bayesian network library (Druzdzel, 1999) and the SVM Light (Joachims, 1999) library are used to learn and evaluate Bayesian or SVM models from very large collections of biological data.
Arbitrary Bayesian structures are allowed, with parameters learned either discriminatively or generatively (Greiner and Zhou, 2005) from data in a context-specific 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[23:34 18/6/03 Bioinformatics-btn237.tex] Page: 1560 15591561 C.Huttenhower et al.
Table 1.
Sleipnir efficiency on integration and single dataset tasks Implementation Peak RAM (KB) Time (s) Bayesian learning (500 genes, 15 datasets) Sleipnir 1376 <1 GeNIe 6832 4 BNT 593 180 15 Bayesian inference (500 genes, 15 datasets) Sleipnir 1216 1 BNT 273 992 >600 Missing value estimation (10% missing, k =10) Sleipnir 27 232 195 knnimpute 115 708 368 Hierarchical clustering Sleipnir 83 188 156 Cluster 3.0 176 836 154 MeV 198 292 361 K-means clustering (k =100) Sleipnir 8780 114 Cluster 3.0 28 544 102 MeV 198 292 361 Memory usage and runtimes for Sleipnir and a number of other common tools for Bayesian analysis and biological data manipulation (de Hoon et al., 2004; Druzdzel, 1999; Murphy, 2001; Saeed et al., 2003; Troyanskaya et al., 2001).
All microarray operations were performedon the 300 conditions and 6153 genes of (Hughes et al., 2000) using Euclidean distance.
Bayesian operations were performed on simulated data using a binary gold standard and five randomly distributed values per dataset.
Tests were run in a single thread on a 2 GHz Intel Core 2 Duo.
In every case, Sleipnir demonstrates a substantial advantage in speed, memory usage or both.
manner (Huttenhower et al., 2006); extremely fast-customized learning and evaluation implementations are used for naive structures.
3 RESULTS While Sleipnirs efficiency in integrating and mining biological datasets is most critical for very large data collections, it is also practical for single dataset tasks and smaller analyses (Table 1).
When compared to several common tools for microarray manipulation or Bayesian learning, Sleipnir consistently demonstrates a substantial advantage in runtime, memory usage or both.
These improvements arise from a variety of optimizations but are broadly attributable to the flexibility allowed by C++ in manipulating large quantities of individual data (microarray values, interaction pairs, etc.)
What Sleipnir trades off in generality (e.g.
with respect to BNT) or robustness to malformed input (e.g.
with respect to MeV), it gains in speed, memory management and overall scalability, allowing it to efficiently manipulate large data collections.
The Sleipnir library is particularly useful for large integration tasks involving hundreds of diverse biological datasets; example applications of Sleipnir in such settings include Huttenhower et al.
(2006) and Myers and Troyanskaya (2007).
A schematic of such a task is shown in Figure 1, where Sleipnir was used to learn 200 context-specific Bayesian classifiers each integrating 186 Saccharomyces cerevisiae datasets.
Conditional probability tables were learned for each dataset within each context, entailing 75 000 probability distributions.
The resulting Bayesian classifiers were used to infer context-specific functional relationship networks, each consuming 90 MB of disk space and calculated in 16.3 min.
Sleipnir also supports an online mode for functional relationship inference Fig.1.
Sample application of the Sleipnir library to integrate 186 heterogeneous genomic datasets in S.cerevisiae within 200 biological contexts.
White boxes indicate externally generated data, grey boxes data generated by Sleipnir, arrows processing performed by Sleipnir, and black bubbles highlight time-consuming tasks.
Times were generated on a 2 GHz Intel Xeon CPU; peak RAM usage was 200 MB.
Sleipnir is extensively parallelizable, and running these tasks on four cores reduces processing time by an optimal 4-fold to 13 h each for Bayesian learning and inference.
in which no additional disk space is consumed and individual context-specific functional relationships can be produced in as little as 100 ns.
Parallelization on four processor cores reduces the total learning and evaluation time by an optimal 4-fold speedup (13 h each for Bayesian learning and inference).
Every stage of this complex data integration and machine-learning task was performed using Sleipnir and its associated tools.
4 DISCUSSION The Sleipnir library for computational functional genomics provides a wide range of data processing and machine learning algorithms optimized for integrating very large collections of heterogeneous biological data.
These include algorithms for data integration, machine learning by Bayesian networks or SVMs, and data types for manipulating microarrays, gene identifiers, functional annotations and other common biological entities.
Several tools are provided with the core library to perform common tasks, and most algorithms are multithreaded or parallelizable for distributed computing.
The Sleipnir library enables computational biologists to efficiently integrate thousands of genomic datasets and to rapidly mine them for biological knowledge.
ACKNOWLEDGEMENTS We thank Matthew Hibbs and Chad Myers for helpful discussions and code reviews.
Funding: This research is supported by NSF CAREER DBI-0546275, NIH R01 GM071966, NIH T32 HG003284 and NIGMS Center of Excellence P50 GM071508.
O.G.T.
is an Alfred P. Sloan Research Fellow.
Conflict of Interest: none declared.
Abstract Glioblastoma multiforme (GBM) is the most common adult primary tumor of the cen-tral nervous system.
The current standard of care for glioblastoma patients involves a combination of surgery, radiotherapy and chemotherapy with the alkylating agent temozolomide.
Several mech-anisms underlying the inherent and acquired temozolomide resistance have been identified and con-tribute to treatment failure.
Early identification of temozolomide-resistant GBM patients and improvement of the therapeutic strategies available to treat this malignancy are of uttermost impor-tance.
This review initially looks at the molecular pathways underlying GBM formation and devel-opment with a particular emphasis placed on recent therapeutic advances made in the field.
Our focus will next be directed toward the molecular mechanisms modulating temozolomide resistance in GBM patients and the strategies envisioned to circumvent this resistance.
Finally, we highlight the diagnostic and prognostic value of metabolomics in cancers and assess its potential usefulness in improving the current standard of care for GBM patients.
Introduction Glioblastoma multiforme (GBM) is the most prevalent and aggressive primary brain tumor [1].
It accounts for approxi-orin PJ).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A mately 60% of all primary brain gliomas diagnosed yearly in the United States [2].
Although early symptoms associated with GBMs depend on location, size and rate of growth of the tumor, 3060% of patients experience headaches and sei-zures [3].
Despite recent progresses in the molecular character-ization of GBMs, median survival time of patients suffering from GBMs remains between 12 and 15 months [4,5].
Current standard of care to treat GBM patients consists of surgical resection followed by a regimen that includes radiotherapy plus concurrent and adjuvant chemotherapeutic treatment with temozolomide (TMZ) [6,7].
TMZ is a DNA alkylating agent of the imidazotetrazine class that can effectively cross the bloodbrain barrier [8].
Researchers have started to cademy of Sciences and Genetics Society of China.
Production and hosting mailto:pier.morin@umoncton.ca200 Genomics Proteomics Bioinformatics 11 (2013) 199206 uncover mechanisms that underlie TMZ resistance to GBMs.
These include the enzyme O6-methylguanine-DNA methyl-transferase (MGMT) that removes methyl groups from DNA, as well as the DNA mismatch repair cascades capable of repairing mispaired DNA bases [9,10].
Yet, the complete molecular picture associated with TMZ resistance in GBMs re-mains elusive and the development of novel approaches to characterize the metabolic footprint of GBMs is of great inter-est.
In this review, we present the deregulated pathways in-volved in GBM formation and progression, focus on the mechanisms underlying TMZ resistance in GBMs and discuss metabolomics-based approaches that could be leveraged in the quest to improve the current therapeutic outcomes in GBMs.
Glioblastomas: a molecular overview GBMs are grade IV gliomas and arise either de novo as pri-mary GBMs or through progressive development from lower grade astrocytomas, which ultimately leads to secondary GBMs [11].
Molecular profiling has revealed deregulated core signaling pathways that confer GBM formation and progres-sion [12].
In addition, a study of gene signatures expressed in GBMs highlighted four distinct subtypes: proneural, neural, classical and mesenchymal [13].
A well-characterized molecu-lar event is amplification of the gene encoding epidermal growth factor receptor (EGFR), which occurs in approxi-mately 50% of primary GBMs [14].
Moreover, 2030% of GBM patients express a shortened and constitutively active version of EGFR, EGFRvIII, which no longer requires inter-action with its ligand EGF to activate downstream signaling cascades such as the PI3K/Akt pathway [15,16].
Genes coding for other receptor tyrosine kinases, including the platelet-de-rived growth factor receptor (PDGFRA) and the proto-onco-gene (MET), are amplified to various degrees in GBMs, resulting in the modulation of proliferative and survival path-ways [17,18].
Increased levels of the MDM2 and CDK4 onco-genes via chromosome 12q13-15 amplification are also of note in GBMs [19].
On the other hand, mutations of PTEN and p53, genes coding for two proteins with tumor suppressive capabilities, are frequent occurrences in GBMs [20].
Modifica-tions in PTEN and p53 expression contribute to sustained acti-vation of the PI3K/Akt signaling axis and to evasion of programmed cell death, respectively [21,22].
Furthermore, epi-genetic silencing of tumor suppressor genes, including CDKN2A, CDKN2B, PTEN, RB1 and p53, via hypermethyla-tion is common in GBMs [23].
Aside from the gene modulation and epigenetic regulation, the potential implications of microRNAs (miRNAs) in GBMs should not be overlooked [24,25].
MiRNAs are small (1824 nucleotides) noncoding RNAs that act as post-tran-scriptional modulators of gene expression and thus play cru-cial roles in regulating different cellular processes [26].
Several miRNAs with oncogenic potential, or oncomiRs, emerge as underlying drivers of various malignancies including GBMs.
miR-26a, which targets PTEN and RB1 tumor suppressors, is frequently coamplified with CDK4 in GBMs [27].
Expres-sion of miR-21, another known regulator of PTEN protein expression, is frequently up-regulated in human GBM sam-ples [28], while antiproliferative effect of miR-21 silencing has been reported in GBM cells [29].
Down-regulation of tu-mor suppressor miRNAs has also been identified in GBMs.
For example, expression of miR-7, which inhibits EGFR and Akt pathway activities by interacting with key transcript targets within these cascades, was frequently down-regulated in GBMs [30].
Expression of miR-34a, a transcriptional tar-get of p53, is frequently down-regulated in cancer including GBMs.
More importantly, expression levels of miR-34a were inversely correlated with protein levels of MET and Notch in gliomas [31].
Overall, either through amplification of selected receptor tyrosine kinases, loss of molecules with tumor suppressive properties or modulation of a family of oncogenic miRNAs, numerous signaling cascades are driving GBMs.
The current standard of care for GBM treatment combines surgical resection, radiotherapy and adjuvant TMZ treatment, leading to increased median survival time [6].
However, the 5-year survival rate remains considerably low either for TMZ treatment combined with radiotherapy or for radiotherapy alone (9.8% vs 1.9%) after surgery [7].
The mechanism of ac-tion and challenges associated with this chemotherapeutic agent will be discussed in the next section.
Other FDA-ap-proved therapeutic approaches for GBMs include 1,3-bis (2-chloroethyl)-1-nitrosourea (BCNU) wafers.
This method is based on controlled release delivery of carmustine from biode-gradable polymer wafers deposited in the tumor cavity upon tumor removal [32].
A meta-analysis of phase III trials re-vealed that BCNU wafers increased overall survival of primary malignant glioma patients by 2.2 months (13.1 vs 10.9 months) [33].
NovoTTF-100A, a noninvasive electrode system that gen-erates pulsating electric fields and induces apoptosis [34], has been recently approved by FDA.
While NovoTTF-100A pro-vides benefits over TMZ treatment including negligible side-ef-fects, a recent study comparing the two approaches indicated that the method was at best comparable to TMZ in terms of survival rate [35].
The monoclonal antibody bevacizumab has also garnered interest as a therapeutic alternative to treat GBMs since its approval in 2009.
Bevacizumab, a recombinant anti-VEGF antibody, notably increased progression-free sur-vival and reduced tumor vascularization in GBMs [36].
None-theless, bevacizumab does not seem to impact overall survival in these patients either and further trials to evaluate this treat-ment option are required [36].
Besides the aforementioned FDA-approved therapies, sev-eral therapeutic strategies to treat GBM patients are currently being investigated in clinical trials.
Such strategies have nota-bly been directed toward differentially expressed or hyperacti-vated kinases identified in GBMs, such as EGFR [37].
Unfortunately, the effectiveness of small molecule inhibitors of EGFR such as gefitinib and erlotinib has been proved to be highly dependent on PI3K and PTEN status and yielded modest results [16].
PI3K pathway inhibition is an attractive axis in the development of targeted treatments in GBMs.
While preclinical studies using PI3K inhibitors have lead to promising results [38], clinical trials evaluating enzastaurin, a PKC/PI3K/AKT inhibitor, did not positively impact progres-sion-free survival in GBM patients and were therefore halted [39].
Inhibition of MET, a frequently overexpressed receptor in GBMs, is also currently under evaluation in clinical trials [40].
Unfortunately, a phase II trial using an anti-MET anti-body, AMG102, demonstrated no significant antitumor activ-ity in patients with recurrent GBMs [41].
Overall, these therapeutic approaches have yielded at best marginally posi-tive results and TMZ, the hallmark chemotherapeutic agent St-Coeur P-D et al/ Metabolomics and Temozolomide Resistance in Glioblastomas 201 routinely used to treat GBMs, remains the primary therapeutic alternative.
Several hurdles need to be overcome to improve the current standard of care offered to GBM patients.
An inherent prob-lem associated with GBM treatment remains the bloodbrain barrier, which restricts tumor site access for many therapeutic agents [42].
Novel strategies to deliver therapeutic agents to the tumor site are being explored including convection-enhanced delivery, a positive-pressure infusion-based method that can be used to administer chemotherapeutics directly into peritu-moral brain [43].
Unfortunately, using small molecular weight inhibitors directed against one molecular target has often lead to activation of compensatory signaling pathways leading to treatment failure [44].
Nevertheless, the drawbacks of single-agent therapies provide crucial insight into the ongoing devel-opment of combination treatments in GBMs.
Simultaneous targeting of key molecular nodes including EGFR, VEGFR, PI3K, CDKs and the JAK/STAT signaling axis has generated promising results in rodent models of GBMs [4547].
A recent study using a PI3K/mTOR dual inhibitor in human GBM xenografts showed increased survival, when compared to TMZ treatment alone [48].
In addition, clinical trials targeting these molecular nodes are underway to identify sensitizing agents to be used in combination treatments for TMZ-resistant GBM patients [2].
Overall, further investigations are required to address these challenges and improve outcomes for GBM patients.
Temozolomide resistance in glioblastomas TMZ, the chemotherapeutic agent given as part of the primary standard of care to treat GBMs, is an alkylating agent that adds a methyl (m) group to purine bases of DNA, producing O6-guanine (G) (6%), N7-G (70%) and N3-adenine (A) (9%) [49,50].
TMZ treatment leads to cell death primarily through O6-G methylation [51].
This modification leads to G pairing with thymine (T) during DNA replication and promotes dou-ble-stranded DNA crosslinking lesions that are difficult to repair by the DNA mismatch repair system, ultimately con-tributing to cell death [52,53].
As a lipophilic molecule, TMZ is administered orally and can penetrate the bloodbrain bar-rier with relative ease and has a high bioavailability (>99%) [8,54,55].
TMZ toxicities are typical of an alkylating agent and include hematological side effects such as lymphopenia, thrombocytopenia and leucopenia [6].
Unfortunately, inherent and acquired TMZ resistance is a common occurrence in GBM patients.
Such resistance in glio-mas is strongly correlated with the presence and activity status of the DNA-repair enzyme O6-MGMT, an enzyme capable of removing methyl groups from the O6 position of G residues and counteracting the cytotoxic effects of TMZ [9,56].
MGMT protein expression in GBM tumors can significantly increase their ability to resist TMZ treatment [57,58].
MGMT expres-sion is reduced by hypermethylation of its promoter, resulting in increased TMZ sensitivity [59].
MGMT hypermethylation is notably detected in 4570% of high grade gliomas [60].
Other factors, besides MGMT expression, can contribute to TMZ resistance in GBMs.
A functional DNA mismatch repair system is required for TMZ sensitivity.
This pathway recog-nizes the O6-mG-T mispair and recruits proteins that excise specifically the erroneous T thus recycling the original O6-mG. O6-mG is subsequently mispaired with another T and the adduct is repaired creating a cycle that ultimately leads to persistent DNA breaks, cell cycle arrest and apoptosis [61,62].
A deficient DNA mismatch repair system can thus con-tribute to TMZ resistance.
Expression of mismatch repair pro-tein MSH6 is down-regulated in GBM patients treated with TMZ, which could play an influential role in acquired resis-tance to the drug [63].
While TMZ cytotoxic effects are primar-ily attributable to the O6-mG lesion, the N7-mG and N3-mA modifications cannot be overlooked.
Components of the base excision repair pathway rapidly remove and repair the modi-fied bases and contribute to TMZ resistance.
AP endonuclease (APE-1), a key enzyme in this pathway, is linked to TMZ resis-tance due to its up-regulated expression in human gliomas [64].
Interestingly, inhibition of base excision repair pathway sensitized cells to TMZ in ovarian cancer via increased cyto-toxic effects of N3-mA and N7-mG [65].
Looking ahead, improving TMZ sensitivity in GBM pa-tients is conceivable and might require undertaking multi-tar-geted therapeutic approaches directed at the aforementioned repair mechanisms or modifying the TMZ molecule itself.
Inhibition of the base excision repair pathway can sensitize GBM cells to TMZ [66].
Pharmacological inhibition of APE-1 with small molecule inhibitors in preclinical models potenti-ated the cytotoxicity of alkylating agents [67].
Inhibition of poly(ADP-ribose)polymerase (PARP), an enzyme involved in the DNA repair pathway, also improved TMZ sensitivity in various models in vitro and in vivo [68].
Structurally, TMZ is an imidazotetrazine that can deliver methyl groups to selected DNA bases.
Synthesis of imidazotetrazine analogues capable of adding a chemical group unrecognizable by MGMT could potentially circumvent the basic repair mechanisms underlying TMZ resistance.
A series of such analogues were recently tested for their cytotoxic effect in TMZ-resistant GBM cells and two lead molecules with anticancer properties irrespective of MGMT and DNA mismatch repair pathway status were identified [69].
Unfortunately, the lead compounds identified in this study also demonstrated significant plasma instability in a mouse model thus raising doubts on their in vivo useful-ness.
While multi-targeted approaches to sensitize GBM cells to TMZ and improvement of the drug itself are of interest, a better characterization of the molecular footprint associated with TMZ resistance is needed for such strategies to succeed.
Metabolomics as a tool for cancer research Metabolomics and metabonomics, provide the quantitative measurement of metabolic composition as well as metabolic changes that occur in living systems as a result of a pathophys-iological stimuli or genetic modification.
Metabolomics (in this text used to represent both metabolomics and metabonomics approaches) can provide a snapshot of the biochemical path-ways modulated under different conditions [70].
It measures the collection of all small molecule metabolites or chemicals that can be found in a cell, organ or organism [71].
This met-abolic profile, the metabolome, can be leveraged for different purposes.
This section focuses on the usefulness of metabolo-mics in cancer diagnosis, prognosis and therapeutic response assessment with a special emphasis on GBMs.
Various metabolic changes are at work in cancer cells ini-tially due to the functions of oncogenes and oncosuppressors Table 1 Differentially expressed metabolites with diagnostic significance in selected brain tumors Metabolites Sample type Tumor samples Sample size Method Ref Choline, flNAA In situ Gliomas vs. non-neoplastic lesions 28 H-MRS [88] Choline, flNAA, flcreatine, In situ Gliomas vs. non-neoplastic lesions 164 H-MRS [87] Alanine, valine, flproline, flglutamate, flglutamine, flGABA, flNAA Primary tissue samples High-grade vs. low-grade oligodendrogliomas 34 HR-MAS [93] Taurine, GPC, P-choline, choline, flNAA, myo-inositol Intact tissue samples Medulloblastomas vs. ependymomas and pilocytic astrocytomas (all pediatric) 20 HR-MAS [85] Fatty acids, isoleucine, leucine, valine, NAA Intact tissue samples Pilocytic astrocytomas vs. ependymomas and medulloblastomas (all pediatric) 20 HR-MAS [85] flMyo-inositol In situ GBMs vs. low-grade astrocytomas 39 H-MRS [91] Lactate In situ High-grade vs. low-grade gliomas (WHO grades 2 and 3) 213 H-MRS [86] Note: GABA, c-aminobutyric acid; GBM, glioblastoma multiforme; GPC, glycerophosphocholine; NAA, N-acetyl-aspartic acid; P-choline, phosphocholine; H-MRS, proton magnetic resonance spectroscopy; HR-MAS, high-resolution proton magnetic angle spinning spectroscopy; WHO, World Health Organization.
202 Genomics Proteomics Bioinformatics 11 (2013) 199206 and are subsequently promoted by changing cellular environ-ment [72,73].
The highly proliferative status of cancer cells translates into elevated energy and biomaterial requirements and leads to increased consumption of some metabolites such as glucose and glutamine, altered energy generation and changes in biomaterial generation routes [74,75].
Increased gly-colytic capacity and elevated phospholipid levels have been re-ported in several cancer models [76].
In contrast, metabolites such as amino acids and nucleotides have different signatures, depending on the cancer type assessed [77].
Metabolomics, as a cancer diagnosis tool, can help in characterizing differentially expressed metabolites between normal and cancer cells or be-tween cancer subtypes or stages.
Metabolomics analysis of tis-sue ex vivo as well as in vivo can provide clear distinction between tumor and healthy cells and can be used in diagnosis of many tumor types.
A study comparing the metabolome of breast cancer samples and normal specimens identified the malignant samples with considerable sensitivity and specificity [78].
Using a similar approach, elevated levels of taurine, lac-tate and choline were also detected in colorectal cancer tissue specimens [79].
While unlocking the metabolome of a primary tumor can yield interesting insights into the differentially reg-ulated pathways underlying the malignancy, assessing circulat-ing metabolites in cancer patients also holds tremendous diagnostic potential albeit with still outstanding issues regard-ing confounding factors.
A metabolomics-based approach was employed to characterize the profile of circulating metabolites in epithelial ovarian cancer patients and was able to discrimi-nate between cancer patients and healthy premenopausal sub-jects [80].
A recent study identified 22 differentially expressed metabolites in the urine of epithelial ovarian cancer patients versus healthy individuals [81].
The metabolic signatures of ur-ine samples collected from esophageal cancer patients demon-strated a distinctive footprint that allowed discrimination between esophageal carcinoma and healthy controls as well [82].
Similarly, several metabolomics-based approaches have been undertaken in gliomas [83].
Choline, lactate and gluta-mine were able to differentiate between GBM cell lines [84].
In primary tumors of pediatric origin, phosphocholine was identified as a potential differentiator between medulloblasto-mas, ependymomas and pilocytic astrocytomas [85].
More-over, levels of lactate and lipid could assist in differentiating low-grade from high-grade primary gliomas [86].
Similarly, an-other research group demonstrated that gliomas of higher grade exhibited significantly elevated choline levels and in-creased lipid synthesis [87].
In biopsies obtained from different brain mass lesions, increased choline levels and decreased N-acetyl-aspartic acid (NAA) levels were indicators of tumori-genic samples [88].
Choline is an intermediate of phospholipid metabolism and serves as an important building block for syn-thesis of selected lipids required for cell membrane structure and function [89].
As a result, elevated choline levels are needed in conditions of increased cell-membrane turnover such as in proliferating cells [90].
Lower myo-inositol levels were re-ported in GBMs, when compared to low-grade astrocytomas [91].
This finding is aligned with a previous report that de-scribed myo-inositol as a molecule primarily located in astro-cytes [92].
In addition, alanine and valine were capable of assisting with the grading of oligodendrogliomas [93].
Interest-ingly, this study demonstrated increased levels in high-grade oligodendrogliomas of these two amino acids, which were linked to anaerobic metabolism, along with a concurrent reduction in molecules related to the Krebs pathway, such as proline, glutamate, glutamine and NAA.
It was hypothesized that this metabolic shift toward fermentative metabolism was indicative of tumor hypoxia in high-grade oligodendrogliomas.
Metabolites isolated from cerebrospinal fluid (CSF) of glioma patients revealed a distinctive metabolic signature, when com-pared with samples of healthy controls [94].
A summary of key findings on differentially expressed metabolites in gliomas is presented in Table 1.
Besides its diagnostic utility, metabolomics also holds tre-mendous potential as a tool to monitor treatment response in cancer patients.
Several studies highlighting this application have notably been conducted in breast cancer patients.
Analy-sis of the metabolic profiles of serum collected from metastatic breast cancer patients assisted in identifying a subset of pa-tients that were more likely to respond to combination therapy [95].
Four metabolites including threonine, isoleucine, gluta-mine and linolenic acid were also found to serve as good pre-Figure 1 Metabolomics analysis of hydrophilic metabolites extracted from GBM cell lines NMR profiles of hydrophilic metabolites extracted from TMZ-sensitive (U373) (A) and TMZ-resistant (LN229) (B) GBM cell lines were obtained for TMZ-treated and control lines.
C. Principal component analysis (PCA) of metabolic profiles indicates that PC1 summarizes 86% of variances in the data and shows changes in different sample groups.
Further analysis of specific metabolic differences is possible from NMR data, possibly leading to markers for TMZ response and treatment follow-up.
St-Coeur P-D et al/ Metabolomics and Temozolomide Resistance in Glioblastomas 203 dictors of neoadjuvant chemotherapeutic response in breast cancer patients [96].
The potential importance of choline-con-taining compounds as a biomarker for therapeutic response in cancer has also been proposed.
Down-regulation of such com-pounds was associated with a positive therapeutic response in breast, prostate and brain cancer [97].
Studies using metabolo-mics to monitor and predict therapeutic response in GBM pa-tients are sparse, yet this application is of great interest.
Metabolic assessment of extracellular fluid collected in GBM patients that undergo conventional radiotherapy highlighted the potential of detecting metabolic markers for the prediction of early treatment response [98].
With the inherent challenges that exist with the current therapies available to treat GBMs, a noninvasive tool for early prediction of TMZ response would hold great promise.
Primary analysis of metabolic profiles by nuclear magnetic resonance (NMR) spectroscopy in a TMZ-resistant and a TMZ-sensitive GBM cell line shows clear dif-ferences (Figure 1).
NMR spectroscopy provides highly reli-able measurements of metabolic profiles in any biological system.
Spectral data can be used directly for the analysis of metabolic differences between cell lines using principal compo-nent analysis (PCA).
PCA evaluation of U373 and LN229 cells treated with 250 lM TMZ or vehicle clearly depicted differ-ences in metabolic profiles in two cell types as well as changes in metabolic profiles following TMZ treatment (St-Coeur et al., unpublished data).
This indicates potential for utilizing metabolomics for prediction of tumor response to TMZ.
Nonetheless, much remains to be done, such as comparing the quantitative metabolic profiles of various TMZ-resistant GBM cell lines with similar samples collected from TMZ-sen-sitive cell models as well as analysis of metabolic response to TMZ treatment in these distinct cell types.
This process will subsequently need to be validated in clinically relevant samples such as primary GBM tumors or serum collected from GBM patients, thus striving toward the identification of metabolic markers for TMZ treatment planning.
Outlook GBMs are aggressive brain tumors for which therapeutic alter-natives are limited.
In addition, the chemotherapeutic agent used as part of the current standard of care is linked to inher-ent and acquired resistance, which often leads to treatment failure.
Looking ahead, rational design of modified alkylating agents using TMZ as scaffold and combinatorial therapeutic approaches are envisioned to improve the current prognosis for GBM patients.
It is expected that identification of meta-bolic markers via metabolomics-based tools, whether to dis-criminate between specific tumor subtypes or to assist in predicting treatment response, will be of great help in the man-agement of GBMs.
Competing interests The authors declare no conflict of interests.
204 Genomics Proteomics Bioinformatics 11 (2013) 199206 Acknowledgements PJM would like to thank the Beatrice Hunter Cancer Research Institute and the Brain Tumour Foundation of Canada for funding.
ABSTRACT Motivation: LibSBGN is a software library for reading, writing and manipulating Systems Biology Graphical Notation (SBGN) maps stored using the recently developed SBGN-ML file format.
The library (available in C++ and Java) makes it easy for developers to add SBGN support to their tools, whereas the file format facilitates the exchange of maps between compatible software applications.
The library also supports validation of maps, which simplifies the task of ensuring compliance with the detailed SBGN specifications.
With this effort we hope to increase the adoption of SBGN in bioinformatics tools, ultimately enabling more researchers to visualize biological knowledge in a precise and unambiguous manner.
Availability and implementation: Milestone 2 was released in December 2011.
Source code, example files and binaries are freely available under the terms of either the LGPL v2.1+ or Apache v2.0 open source licenses from http://libsbgn.sourceforge.net.
Contact: sbgn-libsbgn@lists.sourceforge.net Received on December 13, 2011; revised on April 24, 2012; accepted on May 1, 2012 1 INTRODUCTION The Systems Biology Graphical Notation (SBGN, Le Novre et al., 2009) facilitates the representation and exchange of complex biological knowledge in a concise and unambiguous manner: as standardized pathway maps.
It has been developed and supported by a vibrant community of biologists, biochemists, software developers, bioinformaticians and pathway databases experts.
To whom correspondence should be addressed.
SBGN is described in detail in the online specifications (see http://sbgn.org/Documents/Specifications).
Here we summarize its concepts only briefly.
SBGN defines three orthogonal visual languages: Process Description (PD), Entity Relationship (ER) and Activity Flow (AF).
SBGN maps must follow the visual vocabulary, syntax and layout rules of one of these languages.
The choice of language depends on the type of pathway or process being depicted and the amount of available information.
The PD language, which originates from Kitanos Process Diagrams (Kitano et al., 2005) and the related CellDesigner tool (Funahashi et al., 2008), is equivalent to a bipartite graph (with a few exceptions) with one type of nodes representing pools of biological entities, and a second type of nodes representing biological processes such as biochemical reactions, transport, binding and degradation.
Arcs represent consumption, production or control, and can only connect nodes of differing types.
The PD language is very suitable for metabolic pathways, but struggles to concisely depict the combinatorial complexity of certain proteins with many phosphorylation states.
The ER language, on the other hand, is inspired by Kohns Molecular Interaction Maps (Kohn et al., 2006), and describes relations between biomolecules.
In ER, two entities can be linked with an interaction arc.
The outcome of an interaction (for example, a protein complex), is considered an entity in itself, represented by a black dot, which can engage in further interactions.
Thus ER represents dependencies between interactions, or putting it differently, it can represent which interaction is necessary for another one to take place.
Interactions are possible between two or more entities, which make ER maps roughly equivalent to a hypergraph in which an arc can connect more than two nodes.
ER is more concise than PD when it comes to representing protein modifications and protein interactions, although it is less capable when it comes to presenting biochemical reactions.
Finally, the third language in the SBGN family is AF, which The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [12:06 20/7/2012 Bioinformatics-bts270.tex] Page: 2017 20162021 Software support for SBGN maps represents the activities of biomolecules at a higher conceptual level.
AF is suitable to represent the flow of causality between biomolecules even when detailed knowledge on biological processes is missing.
Efficient integration of the SBGN standard into the research cycle requires adoption by visualization and modeling software.
Encouragingly, a growing number of pathway tools (see http://sbgn.org/SBGN_Software) offer some form of SBGN compatibility.
However, current software implementations of SBGN are often incomplete and sometimes incorrect.
This is not surprising: as SBGN covers a broad spectrum of biological phenomena, complete and accurate implementation of the full SBGN specifications represents a complex, error-prone and time-consuming task for individual tool developers.
This development step could be simplified, and redundant implementation efforts avoided, by accurately translating the full SBGN specifications into a single software library, available freely for any tool developer to reuse in their own project.
Moreover, the maps produced by any given tool usually cannot be reused in another tool, because SBGN only defines how biological information should be visualized, but not how the maps should be stored electronically.
Related community standards for exchanging pathway knowledge, namely BioPAX (Demir et al., 2010) and SBML (Hucka et al., 2003), have proved insufficient for this role (more on this topic in Section 4).
Therefore, we observed a second need, for a dedicated, standardized SBGN file format.
Following these observations, we started a community effort with two goals: to encourage the adoption of SBGN by facilitating its implementation in pathway tools, and to increase interoperability between SBGN-compatible software.
This has resulted in a file format called SBGN-ML and a software library called LibSBGN.
Each of these two components will be explained separately in the next sections.
2 THE SBGN-ML FILE FORMAT SBGN-ML is a dedicated lightweight XML-based file format describing the overall geometry of SBGN maps, while also preserving their underlying biological meaning.
SBGN-ML is designed to fulfill two basic requirements: (1) easy to draw (as a machine) and read (as a human) and (2) easy to interpret (as a machine).
The first set of requirement deals with the graphical aspect of SBGN.
It means it should be easy to render a SBGN-ML file to the screen.
Therefore, the format stores all necessary information, such as coordinates, to draw the map faithfully, so that rendering tools do not have to perform any complex calculations.
Incidentally, this implies the layout of the whole SBGN map has to be expressed explicitly: the size and position of each graphical object and the path of each arc.
Various efforts have shown that generating a layout for heterogeneous biological pathways is a computationally hard problem, so a good layout is always worth preserving, if only from a computational perspective.
Besides, the choice of a specific layout by the author of a map is often driven by concerns related to aesthetics, readability or to reinforce ideas of chronology or proximity.
This information might be lost with automated layouts.
Layout conventions predate SBGN, and are not part of any standard, Fig.1.
An example PD map (right) with the corresponding SBGN-ML code (left).
This example shows the import of glucose followed by the first step of glycolysis.
The colors used have no special meaning in SBGN, here they merely indicate the relation between each SBGN glyph and its SBGN-ML representation; a process node in orange, a simple chemical (ATP) in green, a production arc in cyan, a catalysis arc in purple, a compartment in yellow and a state variable in blue but they nonetheless play a large role in making it easier for other human beings to understand the biological system being described.
The second requirement encompasses two perpendicular characteristics of SBGN as a language: semantics and syntax.
Beyond the picture itself, the format should capture the biological meaning of an SBGN map.
Therefore, SBGN-ML specifies the nature of graphical elements (glyphs), following the SBGN terminology (e.g., macromolecule, process, etc.).
For example, we can distinguish between a logic arc and a consumption arc even though they have the same visual appearance.
Supporting tools refer to this terminology and draw the glyph according to the SBGN specifications.
In terms of syntax, SBGN-ML encodes information on relationships between the various SBGN objects: the glyphs at both ends of an arc, the components of a complex, the members of a compartment and the decorations (such as unit of information and state variable) belonging to specific glyphs and arcs.
This semantic and syntactic information is essential to a number of automated tasks, such as map validation, or network analysis (as the topology of the underlying biological network can be inferred from the various relationships encoded by the format).
To explain the syntax of SBGN-ML in more detail, consider the example in Figure 1.
This figure shows a PD map describing the import of glucose by GLUT4, followed by the first step of the glycolysis.
The root element is named sbgn (line 1).
Below that, there is a map element with an attribute indicating that the PD language is used.
Below the map element, one finds a series of glyph and arc elements.
Each glyph carries a class attribute to denote the meaning in SBGN terms.
In this example, there is a 2017 Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [12:06 20/7/2012 Bioinformatics-bts270.tex] Page: 2018 20162021 M.P.van Iersel et al.
glyph with class process (lines 1418, in orange).
Each glyph also carries an id attribute that can be referred from elsewhere in the document, thus storing the network topology (in this case merely the letter f for the sake of brevity).
Each glyph must define a bbox or bounding box, which allows the glyph to be placed at the correct position.
Its coordinates denote the smallest rectangle that completely encompasses the glyph.
Consumption and production arcs connect to process nodes at a so-called port just outside the glyph.
Port elements are part of the network topology, so they carry identifiers as well (lines 16 and 17).
Another glyph in this example represents the active form of hexokinase (lines 2431).
It carries a label element, which should be positioned in the center of the parent glyph, unless otherwise defined.
Hexokinase also contains a sub-glyph for a state variable (lines 2730, in blue) to indicate that it is the allosterically active form of the enzyme.
ATP (lines 19 23, in green) is a simple chemical, and uses a circle as its shape, as opposed to macromolecules that use a rounded rectangle shape.
Small molecules often occur multiple times in a map, in which case they must carry a clone marker, a black bottom half.
In SBGN-ML this is represented by the clone element (line 21).
Cellular compartments are represented by glyphs as well (lines 3235, in yellow).
Entities refer to their surrounding compartment using a compartmentRef attribute.
Just like glyphs, arcs must define a class attribute and an id attribute.
See for example the production arc (lines 8487, in cyan).
Each arc must have a source attribute, referring to the identifier of a glyph that the arc points from, as well as a target attribute, referring to the identifier of the glyph that the arc points to.
Source and target may refer to identifiers of either glyphs or ports.
Arcs must also define start and end coordinates.
Arcs can optionally include waypoints for path routing as with the catalysis arc (lines 8892, in purple).
It is not possible to deduce the start and end coordinates from the source and target glyphs, as there may be some white space between the end of the arc and the border of the glyph.
Each element can be freely annotated with notes encoded with valid XHTML elements (lines 35).
Each SBGN-ML can also be extended with elements in proprietary namespaces to add additional features (not shown in this example).
3 THE LIBSBGN LIBRARY A software library called LibSBGN complements the file format.
It consists of two parallel implementations in Java and C++.
The libraries share the same object model, so that algorithms operating on it can be easily translated to different programming languages.
The primary goal of LibSBGN is to simplify the work for developers of existing pathway tools.
To reach this goal we followed three design principles.
First, we avoided tool-specific implementation details.
Implementation artifacts that are specific for one bioinformatics tool would impose difficulties for adoption by others.
We sought input from several tool developers into the LibSBGN effort early on.
Second, we do not want to force the use of a single rendering implementation (meaning the software routine that translates from memory objects to screen or graphic format).
Early in the development of LibSBGN, it became clear that for most pathway drawing tools, the rendering engine is an integral part that is not easily replaced by a common library.
The typical usage scenario is therefore to let LibSBGN handle input and output, but to translate Fig.2.
Example of reading a file using the Java version of LibSBGN.
Here an SBGN-ML file named adh.sbgn (included in the LibSBGN source distribution) is read, and some basic information about each glyph in that file is printed to standard output.
The complete program can be found as ReadExample.java in the LibSBGN source distribution to the applications own object model, and display using the applications own rendering engine.
Enforcing a common rendering library would hamper adoption of LibSBGN.
We instead opted to build a render comparison pipeline to ensure consistency between various renderers (this pipeline is described in more detail in Section 3.2).
Third, we wish to provide optimal libraries for each development environment.
For both the C++ and Java versions, code is automatically generated based on the XML Schema definition (XSD).
The method of generating code from XSD has reduced the effort needed to keep the Java and C++ versions synchronized during development.
The generated Java code plus helper classes form a pure Java library.
The alternative possibility, to create a single C++ library and a Java wrapper around that, is not preferable because it complicates multi-platform installation and testing.
Our experience with a related project, LibSBML (Bornstein et al., 2008), is that the community has a need for a pure Java library in spite of existing Java bindings for C++, which has led to the development of the pure Java JSBML (Drger et al., 2011) as an alternative.
Although both LibSBML and JSBML are successful projects, the maintenance of two similar projects in different languages is costly in terms of developer time.
By generating native libraries for both environments automatically, we hope to avoid that extra cost.
3.1 Code sample See Figure 2 for an example of usage of LibSBGN in practice.
The Java library contains convenient helper functions for reading, writing and validation.
In the case of this example the function readFromFile from the SbgnUtil class is used.
The source package contains example programs for common operations, and the LibSBGN wiki includes a developer tutorial (see http://sourceforge.net/apps/mediawiki/libsbgn/index.php?title= 2018 Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [12:06 20/7/2012 Bioinformatics-bts270.tex] Page: 2019 20162021 Software support for SBGN maps Fig.3.
Rendering comparison.
A series of test-cases is rendered by all supported tools in an automated render comparison pipeline.
The rendering results are compared with the reference map (top-left), in this case an ER map.
A couple of significant differences have been highlighted with red circles.
In the PathVisio case (top-right), arrowheads are drawn where none is expected.
In the SBML Layout example (bottom-right), the wrong arrowheads are drawn for absolute inhibition and stimulation arcs.
Note that these are historical images for illustration purposes, and the highlighted issues have already been fixed Developer_tutorial) aimed at developers who want to include LibSBGN into an existing bioinformatics application.
3.2 Rendering comparison We created dozens of test-cases for each of the three languages of SBGN, covering all aspects of the syntax.
Each test-case consists of a reference diagram in PNG format and a corresponding SBGN-ML file.
To test our software, all SBGN-ML files are automatically rendered by the participating programs, currently SBGN-ED (Czauderna et al., 2010), PathVisio (van Iersel et al., 2008) and SBML Layout (Deckard et al., 2006).
The resulting images are viewable side-by-side with the reference map.
An example of this can be found in Figure 3.
This pipeline was of tremendous value during development.
Typically, an observed difference between a given rendering and the reference diagram could lead to several possible outcomes.
Most commonly, the difference indicated a mistake in the participating renderer, which had to be fixed by the author of that software.
A second possibility is that the mistake is due to an ambiguity in the interpretation of SBGN-ML.
This could lead to a correction in the specification or a clarification in the documentation, so that all involved are in agreement.
In several instances, the source of ambiguity was derived not from SBGN-ML but from the SBGN specification.
This way, LibSBGN has led to feedback on SBGN itself.
A final possibility is that the difference was deemed insignificant.
Certain differences in use of color, background shading and line thickness are not meaningful in terms of biological interpretation of the SBGN map.
An exception here is differences in layout.
As mentioned before, we consider layout valuable to preserve even though it is not semantically significant.
This pipeline is now fully automated, and runs automatically, whenever new test-cases are added to the source repository.
It can be viewed online at http://libsbgn.sourceforge.net/render_comparison/.
We encourage developers of software to contact us to add their tool to the gallery.
3.3 Validation For syntactic validation of SBGN-ML documents, we created an XML Schema definition (XSD).
Unfortunately, XSD is not sufficient to validate the many semantic rules defined in the SBGN specification.
To solve this we also developed higher level, semantic validation using the Schematron (http://www.schematron.com) language.
To give a few examples: in PD, a production arc should point from a process towards an entity pool node.
It is not allowed to draw the arc in the other direction, or to connect two entity pools directly without an intermediate process (see Figure 4).
In ER, outcome glyphs may be drawn on interaction arcs but not on influence arcs.
If such a rule were violated, the meaning of the map would be ambiguous or contradictory.
LibSBGN provides functionality for users and developers to validate diagrams against these rules.
This validation capability is built using Schematron language which has been previously used for Molecular Interaction Map diagram validation (Luna et al., 2011).
Schematron rules are assertion tests written using XPath syntax.
Each rule possesses a role to denote the severity of failure, a human-readable message and diagnostic elements to identify the source of the error or warning.
Rules in Schematron can be grouped in phases; this feature can be used to denote subsets of rules to be activated during validation.
Schematron makes use of XML stylesheet transformations (XSLT) and the validation process occurs in two steps.
The first step is the transformation of the rule sets written in the Schematron language to an XSLT stylesheet, and the second step is the transformation of an SBGN-ML file using the XSLT stylesheet from the first step.
The product of this second transformation is a validation report that uses the Schematron Validation Report Language (SVRL).
The usage of Schematron rule sets allows for validation to be flexibly incorporated into various environments and using any programming language with an XSLT processor.
Command-line validation can be done using XSLT processors such as Saxon (http://saxon.sourceforge.net/) by performing the two transformation steps mentioned above.
Alternatively, validation can also be incorporated into automated pipelines using the Ant task for Schematron (http://code.google.com/p/schematron/); an example of this is provided in the distributed files.
Lastly, validation can be incorporated into projects by using provided utility Java classes found in the LibSBGN API.
The PathVisio-Validator plugin (Chandan et al., 2011) is an example of diagram validation using LibSBGN and Schematron.
There are three rule sets for SBGN-ML, one for each of the SBGN languages.
These rule sets validate syntactic correctness of SBGN maps.
An example validation is shown in Figure 4, where a stimulation arc is incorrectly drawn by pointing to an entity pool node, rather than a process node.
Unfortunately software can have bugs, and if the validation routine does not report any validity errors, this could indicate that either the diagram is indeed correct (true negative), or that there is a bug in the software encoding the rules (false negative).
To ensure correctness of the validation rules themselves, we have created 2019 Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [12:06 20/7/2012 Bioinformatics-bts270.tex] Page: 2020 20162021 M.P.van Iersel et al.
Fig.4.
Typical validator benchmark.
This particular example tests the software for rule pd10110: in PD maps, catalysis arcs must point to a process node (not to an entity pool node).
In the negative test-case on the left, the enzyme GPI appears to catalyze a molecule rather than a reaction.
This is a logical impossibility.
The positive test-case on the right shows correctly how the enzyme GPI catalyzes the reaction from glucose-6P to fructose-6P.
Taken together, these test-cases help to prevent bugs in the validation software Fig.5.
Screenshots of a number of tools that use LibSBGN.
Clockwise, from the top: CellDesigner, SBGN-ED, VISIBIOweb and PathVisio.
These tools are able to use SBGN-ML for import, export or both.
At the time of writing, for some of these tools a version with SBGN support has not been officially released, but is expected soon benchmarks for each of them.
For each rule there is a positive test-case, for which the rule should pass, and a negative one, for which the rule should fail, similar to the example given in Figure 4.
3.4 Supporting tools As mentioned earlier, we seek support from a wide community of tool developers.
The following tools are already using LibSBGN: PathVisio (van Iersel et al., 2008), SBGN-ED (Czauderna et al., 2010), SBML Layout (Deckard et al., 2006) and VISIBIOweb (Dilek et al., 2010).
We are aware of two other tools with LibSBGN support in development: Arcadia (Villger et al., 2010) and CellDesigner (Funahashi et al., 2008).
Desktop applications using LibSBGN are shown in Figure 5.
4 DISCUSSION We have set out to fulfill the dual goals of simplifying SBGN support as well as standardizing electronic exchange of SBGN.
The first goal has been addressed with an open-source software library, which can be used to read, write, manipulate and validate SBGN.
The second goal has been addressed with a file format named SBGN-ML.
SBGN-ML fills a pragmatic need for a format that can be mapped directly to concepts from the SBGN specification.
We see the rapid adoption of SBGN-ML by a number of tools as proof of the pragmatic need for it.
A potential criticism of SBGN-ML is the addition of yet another file format to the repertoire of file formats in systems biology.
Different approaches have been explored for electronically representing SBGN: from graphical file formats such as SVG, or graph representation stored as GraphML files, to additional information on top of an existing model, such as the Systems Biology Markup Language (SBML) layout extension (Gauges et al., 2006).
All these approaches have limitations, as they have been developed independently of SBGN.
A new format was needed to support all characteristics of SBGN maps (graphics, relationships and semantics).
The other formats could be extended to cover these concepts, but at the expense of brevity and clarity.
So we created a new format for the following reasons.
First, SBGN-ML focuses on the domain of visualization of SBGN concepts.
This sets it apart from existing exchange formats for pathways.
BioPAX is a pathway exchange format that occupies the domain of knowledge management, and has close relations to the semantic web.
SBML occupies the domain of computational modeling of systems biology.
The latter two could be extended to accommodate SBGN concepts, but there is not a straight one-to-one mapping.
For example, there is no good equivalent for the AND/OR gates which can be drawn in SBGN.
Furthermore, omitted/uncertain processes can be drawn in SBGN but have no direct equivalent in BioPAX.
Second, SBGN-ML is easier to validate against the SBGN specification.
As mentioned before, the complexity of SBGN makes software support for validation a must.
Rules describing validation of SBGN-ML are simpler and more concise than they would be if they were encoded on top of an existing format.
Third, the rendering comparison pipeline has ensured that conversion of SBGN-ML to graphical formats is straightforward.
On the other hand, conversion from a graphical format such as SVG to SBGN-ML requires inferring the meaning of lines, glyphs and symbols, which is bound to lead to loss of information.
Fourth, by making SBGN independent, it is not tied to either the SBML, BioPAX or any other research community.
We observe that currently LibSBGN is being used by both BioPAX-oriented tools such as ChIBE and PaxTools as well as SBML-oriented tools such as CellDesigner or GraphML-oriented tools such as SBGN-ED.
SBGN-ML is officially endorsed by the SBGN scientific committee as a reference implementation and the best way to exchange diagrams between applications.
It is orthogonal to specific formats used to represent pathways and models such as BioPAX (Demir et al., 2010) and SBML (Hucka et al., 2003), and thus follows the vision of the COMBINE initiative (http://co.mbine.org/about).
In the field of bioinformatics, it occurs all too often that the lack of a feature in an existing piece of software is used to justify the development of a complete new bioinformatics tool, which will in its turn lack features in another area.
The end result is the current state of affairs: a balkanization of bioinformatics tools, or in other words, many fragmented tools that integrate poorly.
One of the goals of LibSBGN is to improve existing software.
LibSBGN could serve 2020 Copyedited by: PSB MANUSCRIPT CATEGORY: ORIGINAL PAPER [12:06 20/7/2012 Bioinformatics-bts270.tex] Page: 2021 20162021 Software support for SBGN maps as a model to counter the balkanization trend.
We prefer to see the development of software libraries instead of incomplete tools.
Libraries, especially if they are open source, can be shared, re-used and adopted by developers.
5 CONCLUSION The SBGN-ML file format and LibSBGN library provide open-source software support for SBGN maps.
They have been adopted by several tools already, and development is ongoing.
It is expected that the availability of a community-supported API will significantly expedite SBGNs adoption.
We use the word Milestone for versioning purposesthe latest release is Milestone 2, which was released in December 2011.
LibSBGN is primarily focused on exchanging between SBGN software.
Other functionalities, such as conversion to other formats, or generating suitable layout, are not currently supported.
It is certainly likely that some or all of these functionalities will be added in the future as optional modules.
SBGN-ML will likely see the addition of fine-grained graphics specification, support for linking between files, and improved usage of ontologies.
Additionally, LibSBGN will see expansion to other programming languages beyond Java and C++, such as for example Javascript.
The SBGN-ML file format is represented as an XML schema (SBGN.XSD).
Examples are available as test files (XML, PNG).
The accompanying documentation reflects the content of the schema, and clarifies a number of additional rules and conventions (e.g., coordinate system).
This set of resources constitutes the SBGN-ML specifications.
The LibSBGN library (in C++ and Java) and the file format have been released on Sourceforge, under a dual license: the Lesser General Public Licence (LGPL) version 2.1 or later, and Apache version 2.0.
The development process is an active community effort, organized around: regular online meetings, discussions on the mailing list, and development tools on Sourceforge (bug tracker, SVN repository and documentation wiki).
New developers are very welcome.
ACKNOWLEDGEMENTS The authors thank their individual sources of funding.
Authors are grateful for useful feedback from the Path2Models project.
Funding: This work was in part supported by the Biotechnology and Biological Sciences Research Council (BBSRC); the Netherlands Consortium for Systems Biology (NCSB), which is part of the Netherlands Genomics Initiative/Netherlands Organisation for Scientific Research; BioPreDyn which is a grant within the Seventh Framework Programme of the EU, the Intramural Research Program of the NIH, National Cancer Institute, Center for Cancer Research; and the German Ministry of Education and Research (BMBF).
Conflict of Interest: none declared.
ABSTRACT Motivation: Genetic interactions between genes reflect functional relationships caused by a wide range of molecular mechanisms.
Large-scale genetic interaction assays lead to a wealth of information about the functional relations between genes.
However, the vast number of observed interactions, along with experimental noise, makes the interpretation of such assays a major challenge.
Results: Here, we introduce a computational approach to organize genetic interactions and show that the bulk of observed interactions can be organized in a hierarchy of modules.
Revealing this orga-nization enables insights into the function of cellular machineries and highlights global properties of interaction maps.
To gain further insight into the nature of these interactions, we integrated data from genetic screens under a wide range of conditions to reveal that more than a third of observed aggravating (i.e.
synthetic sick/lethal) interactions are unidirectional, where one gene can buffer the effects of perturbing another gene but not vice versa.
Furthermore, most modules of genes that have multiple aggravating interactions were found to be involved in such unidirectional interactions.
We demonstrate that the identification of external stimuli that mimic the effect of specific gene knockouts provides insights into the role of individual modules in maintaining cellular integrity.
Availability: We designed a freely accessible web tool that includes all our findings, and is specifically intended to allow effective browsing of our results (http://compbio.cs.huji.ac.il/GIAnalysis).
Contact: maya.schuldiner@weizmann.ac.il; hanahm@ekmd.huji.ac.il; nir@cs.huji.ac.il Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION A major goal in biology is to understand how thousands of genes act together to create a functional cellular environment.
An emerging powerful strategy for investigating functional relations between genes involves high-throughput genetic interaction maps (Butland et al., 2008; Byrne et al., 2007; Collins et al., 2007a; Fiedler et al., 2009; Makhnevych et al., 2009; Pan et al., 2006; Roguev et al., 2008; Schuldiner et al., 2005; Segr et al., 2005; Tong et al., 2001; Wilmes et al., 2008), which measure the extent by which a mutation in one gene modifies the phenotype of a mutation in another.
The interactions in these maps can be divided to alleviating interactions, where the defect of the double mutant is less than expected from To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First authors.
two independent effects, and aggravating interactions, where the defect of the double mutant is greater than expected from the single-gene perturbations.
Such systematic mapping typically uncovers a large number of observed genetic interactions, which confounds straightforward interpretation.
Despite the large number of published maps, a systematic methodology for extracting biological insights remains a major challenge.
Previous analyses of genetic interaction data have primarily focused on hierarchical clustering, resulting in many new discoveries in key cellular processes (Collins et al., 2007a; Pan et al., 2006; Schuldiner et al., 2005).
Nonetheless, hierarchical clustering has two major drawbacks: first, the similarity score between genes is based on their entire interaction profile (with all other genes) allowing large fraction of background interactions to dominate the similarity.
Second, it does not directly extract meaningful groups of genes or interactions between such groups, preventing a system-level view of the interaction map.
Both challenges were addressed by several methods.
For example, the PRISM algorithm (Segr et al., 2005) uses monochromatic interactions (i.e.
solely aggravating or solely alleviating) within and between groups of genes to define pathways (Fig.1A).
However, this algorithm, which was evaluated on simulated interaction maps, fails on actual data from large-scale maps due to the added complexity in real cellular systems and assay noise (data not shown).
Biclustering is another approach that was suggested as an alternative to hierarchical clustering, aiming to identify local signatures of functional modules in the genetic interaction maps (Pu et al., 2008).
While this approach identifies many modules of genes, it does not eliminate their overlap, hampering the generation of one coherent network structure describing both the intra-and inter-modular interactions.
One possible way to overcome these drawbacks is by adding different types of data or additional constraints.
For example, methods that combine physical proteinprotein interactions in the analysis of genetic interaction data identify functional modules with high precision (Bandyopadhyay et al., 2008; Kelley and Ideker, 2005; Ulitsky et al., 2008).
However, the requirement for physical interaction data limits such approaches to protein sets and organisms where such data exist, and may miss many functional pathways that are not mediated by protein complexes (e.g.
metabolic pathways).
Here, we introduce an automated approach that builds a concise representation of large-scale genetic interaction maps.
Toward this goal, we relied on previous observations that complexes and pathways induce signatures in the form of monochromatic cliques and bi-cliques (Fig.1A; Beyer et al., 2007; Boone et al., 2007; Segr et al., 2005).
Our method seeks to find an organization that is globally coherent, in the sense that genes are organized into a hierarchy of modules.
Moreover, our method requires that the The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:39 12/5/2010 Bioinformatics-btq197.tex] Page: i229 i228i236 Modularity and directionality in genetic interaction maps Fig.1.
Modularity of genetic interactions.
(A) Pathway architecture (left) leads to expected patterns of genetic interactions between genes (right).
Each row/column represents the genetic interactions of a specific gene with all other genes.
Among these there are subsets of interactions that can be represented as monochromatic cliques and bicliques.
(B) Monochromatic interactions can be captured by edges within and between modules (grey boxes) organized in a hierarchical structure.
interactions between these modules will account for a large portion of the data.
We show how the resulting representation facilitates better understanding of the underlying cellular phenomena.
In turn, we use these insights to shed light on the function of concrete cellular pathways and also to provide information on the overall organization of the network.
We demonstrate how integration of data from genetic screens for reduced fitness under various conditions results in automatic creation of biological insights into the functional role of gene modules.
2 HIERARCHY OF INTERACTING MODULES Our basic premise is that a good hierarchical organization is defined by a trade-off between succinct description of the network on one hand, and capturing as much of the interactions in the map on the other hand.
To capture this quality, we devised a score based on the minimum description length (MDL) principle (Rissanen, 1983) and devised an iterative procedure that optimizes this score.
2.1 Hierarchical representation The hierarchical representation consists of two parts.
The first is a hierarchy of modules.
Briefly, a hierarchy is a set M of modules, such that each module m is associated with a subset of genes Genes(m) and a parent module Parent(m)M{}, where represents a null module (i.e.
the module is a root).
We say that a module m is an ancestor of m if m =Parentk(m) for some k 1.
The hierarchy is legal if for every m,m M such that m =Parent(m), we have that Genes(m)Genes(m), and moreover Genes(m)Genes(m) = if and only if m is an ancestor of m or vice versa.
In the hierarchy of Figure 1B, we have four modules, so that Genes(m1)={G1,G2,G3}, Genes(m2)= {G5,G6}, Genes(m3)={G4,G5,G6}, and Genes(m4)={G7,G8}.
In this example, Parent(m1)=Parent(m3)=Parent(m4)=, and Parent(m2)=m3.
The second component of the hierarchy describes a set E of edges between modules.
An edge can be of two types, alleviating (denoted in yellow in our figures) or aggravating (denoted in blue).
Each edge represents a type of genetic interactions that is common for the members of the modules linked by the edge.
Formally, an edge m1 m2, represent the set Int(m1 m2)=Genes(m1)Genes(m2) of genetic interactions.
Edges in the hierarchy can be self-edges, in which case they induce a clique of interactions, or between two different modules in which case they induce a bi-clique of interactions.
In the example of Figure 1B, we have the alleviating edges m1 m1, m3 m3, m4 m4, and the aggravating edges m1 m3 and m2 m4.
These edges represent the interactions described in the interaction matrix of Figure 1A.
2.2 Minimal description length score We use the MDL principle (Rissanen, 1983) to score the quality of module hierarchy as a guide for lossless encoding of the genetic interaction map.
Conceptually, imagine that we need to transmit the genetic interaction map over a channel and search for the encoding that would require the fewest bits.
Under this principle, the length of the transmission is a proxy for the quality of the representation, with a shorter encoding denoting a better representation.
The application of this principle involves deciding how we encode the interactions in the map.
When we do not have any organization of the map, we use the same codebook for each interaction.
Since weak interactions are much more abundant than strong ones, their code words will be shorter (Cover and Thomas, 2001).
Thus, we will incur a penalty for strong interactions.
When we have a module hierarchy, we can use a different codebook for each edge in the hierarchy and an additional codebook for background interactions.
This allows us to exploit a group of monochromatic interactions for efficient encoding by a codebook that assigns strong interactions of the appropriate short codewords.
The benefit from covering a large portion of the map with coherent edges is offset by the cost of transmitting the codebooks themselves, which involves coding the hierarchical organization and the edges with their signs.
Thus, when evaluating a possible organization of the genetic interaction map there is a trade-off between the coverage of interactions and the number of modules and edges.
Formally, if we denote the genetic interaction map by D and the hierarchical organization by (M,E) then the MDL score consists of two main terms: S(D;M,E)=DL(M,E)+DL(D|M,E) where DL(M,E) is the description length of the hierarchical organization and DL(D|M,E) is the description length of the interactions, given that we already encoded the hierarchy.
We start with the first term, DL(M,E).
Here, we need to encode the module hierarchy (which module is the parent of each module), the assignment of genes to modules and the list of edges.
This is a relatively straightforward encoding using standard MDL practices.
The second term represents how to describe the genetic interaction map once we know the modular organization.
Standard results in information theory (Cover and Thomas, 2001) show that if the frequency of each word is p(w), then the optimal codebook is one where encoding a word w is of length log2p(w).
Thus, in each codebook we use the distribution of the strengths of interactions covered by an edge to build an efficient codebook.
We assume that the different values are distributed according to a Gaussian distribution.
Thus, the encoding length is the minus log-probability (or likelihood) of the data given the parameters of each Gaussian codebook (i.e.
the closer the distribution is to its parametric description, the score is higher).
To this length, we add the number of bits needed to encode the parameters of each distribution.
To calculate the encoding length, for each edge eE we estimate the i229 [11:39 12/5/2010 Bioinformatics-btq197.tex] Page: i230 i228i236 A.Jaimovich et al.
maximum likelihood parameters, (e,e).
In addition, we estimate the background distribution (b,b).
We then define DL(D|M,E) =  eE (i,j)Int(e) log2p(Ii,j|e,e)  (i,j)Bg log2p(Ii,j|b,b) + eE log2 |Int(e)|+log2 |Bg| where p(Ii,j|,) is the likelihood of the genetic interaction score Ii,j according to the Gaussian N(,2), Bg is the set of interactions that do not belong to any edge in E , and log2(|Int(e)|) is the encoding length of the parameters for the edge.
Thus, we score interactions in their specific context (either inside an edge or in the background).
For practical concerns, we restrict the network to include only coherent edges.
Thus, we require that an edge satisfies |e|e >, where is a strictness parameter (which we set to 1 in the results below).
If this is not the case, the network receives a large penalty which effectively excludes it from consideration.
2.3 Constructing module hierarchy Given a genetic interaction map D, we want to find the module hierarchy that minimizes the MDL score.
This problem is non-trivial as the search space is huge.
To address this we combine two ideas.
First, we use hierarchical clustering to get a good initial guess for our hierarchical organization.
Second, once we have a reasonable initial candidate, a heuristic search procedure can perform local improvements to find a much better one.
Our procedure implements these ideas by performing the following steps.
Clustering: we cluster the genetic interaction map using hierarchical clustering with uncentered Pearson correlation (Eisen et al., 1998).
This results in a dendrogram, which in our terminology is a detailed hierarchy, where each internal node defines a group of genes that correspond to the leaves in its sub-tree and each pair of such internal nodes defines a rectangle in the clustered matrix (Fig.2a).
Identifying edges: treating the dendogram as an initial hierarchy of modules, the procedure traverses overall pairs of internal nodes in the dendrogram and in a greedy fashion adds modules and edges as long as they increase the MDL score.
At this stage, we have a very large number of modules and some number of edges.
We then prune modules that do not participate in edges (while maintaining the ancestral relationships between the remaining modules).
This results in a hierarchy that summarizes the initial clustering (Fig.2b).
Greedy improvements: to re-evaluate and refine the modular structure, the procedure performs a heuristic search by evaluating local changes to the modular organization.
These local changes include: addition/removal of a gene to/from an existing module, merging a module with its parent, transferring an edge from a module to its parent (or vice-versa) and addition/removal of an edge.
Each of these local changes is evaluated and based on their score the procedure decide which one to apply.
We use a best-first-search heuristic combined with a TABU list (Glover et al., 1993) to avoid revisiting explored networks and thus escape local maxima.
This search leads to a refined model (Fig.2c).
Fig.2.
Outline of our iterative algorithm.
After clustering the interactions (left) our procedure identifies modules of genes in the clustering hierarchy that define monochromatic on-diagonal squares (e1) and off-diagonal rectangles (e2), resulting in a hierarchical organization of genes into modules (middle).
Next, the module graph is refined by a series of local changes (e.g.
moving one gene from m2 to m1; right).
At the end of each iteration (bottom arrow), we re-cluster the genetic interaction matrix while maintaining the identified modules.
These steps are iterated until convergence.
Reiterations: to find structures that might elude local search steps, the procedure iterates by returning to the first step.
In each re-iteration, we re-cluster the genetic interaction map while conserving the module hierarchy from the previous step.
That is, we allow only agglomerative steps that do not break existing modules into separate subunits.
This constraint forces the resulting clustering to maintain the found structure, but it can identify new sub-modules as well as new modules of genes that are not assigned to a module.
These iterations are repeated until convergence (in score) (Fig.2d).
2.4 Application to genetic interaction maps in Saccharomyces cerevisiae We applied our methodology to two large-scale genetic interaction maps in the budding yeast S. cerevisiae.
The first contains genes localized to the Early Secretory Pathway (ESP; Schuldiner et al., 2005) and the other comprises genes involved in Chromosome Biology (CB; Collins et al., 2007b).
This procedure automatically constructed a hierarchical organization of modules in both: in the ESP map it identified 113 modules covering 264 genes (out of 424) and in the CB map it identified 242 modules covering 487 genes (out of 743).
Most of these modules represent functionally coherent groups of genes (ESP: 76/113, CB: 193/242; Appendix A in the Supplementary website), such as physical complexes (e.g.
Mediator subunits, HIR complex, SAS complex) and functional pathways (e.g.N-linked glycosylation, chromatid cohesion).
Inter-and intra-module interactions correspond to a large fraction of the interactions in the original maps, particularly the high confidence ones (Fig.3A and B).
In addition, the edges we capture are also coherent in the sense that most interactions covered by alleviating edges have positive interaction scores and most interactions covered by aggravating edges have negative scores (Fig.3C and D).
Thus, the modular organization of the genetic interactions faithfully captures a large portion of these maps.
The hierarchical nature of the network allows the definition of large modules with more general functions that contain sub-modules with more specific functions, which are distinguished by sets of unique interactions.
For example, module ESP-98 comprises eight i230 [11:39 12/5/2010 Bioinformatics-btq197.tex] Page: i231 i228i236 Modularity and directionality in genetic interaction maps Fig.3.
Edges capture most interactions.
(A) Coverage of aggravating interactions by our network (y-axis) as a function of threshold for EMAP score (x-axis).
Magenta solid lines and green dashed lines show results for CB and ESP networks, respectively.
(B) Coverage of alleviating interactions.
(C) Coherence of aggravating and alleviating edges in the CB network.
Shown is a histogram (y-axis) of EMAP scores (x-axis) for interactions covered by aggravating and alleviating edges in blue and yellow, respectively.
Histogram for the entire data is show in grey.
(D) Coherence of edges in our ESP network.
genes that take part in the maturation of glycoproteins within the ER lumen (Fig.4).
Specifically, these genes encode the sequential enzymes adding on sugar moieties to a synthesized polysaccharide chain.
Our analysis identified two sub-modules that correspond to two distinct stages in this process: one module (ESP-97) involves genes encoding proteins that transfer mannose residues to the nascent chain, and the second module (ESP-96) involves genes that subsequently transfer glucose residues to the nascent chain (Helenius and Aebi, 2004).
This division was obtained automatically, based on interactions that are specific to each of these sub-modules (Fig.4).
Notably, the protein products of genes in these two modules do not form physical complexes, and thus could not be identified by methods that use proteinprotein interactions to define the modules.
In addition, this subdivision was not obtained by solely applying hierarchical clustering methods (Schuldiner et al., 2005).
2.5 Comparison to other methods Comparing our method to previous methods for analysing genetic interaction maps is difficult due to the different focus of the various methods.
A common theme to most methods is the determination of gene modules.
Although this is only one aspect of our analysis, we compared our module list to modules found by other studies of the CB map (Bandyopadhyay et al., 2008; Pu et al., 2008; Ulitsky et al., 2008).
Comparing to the methods of Bandyopadhyay et al.
(2008) and Ulitsky et al.
(2008, Fig.5A and B), we find many more modules (242 modules compared with 91 and 62, respectively), covering more genes (487 genes compared with 374 and 313, respectively).1 In addition, many of these modules are 1When comparing to Bandyopadhyay et al.
(2008) we considered only modules with more than one gene.
Fig.4.
Hierarchical organization of modules represents functional hierarchy.
(A) Modules are denoted by grey boxes (red labels denote functional assignment based on annotations; black labels denote the name of each module and in parentheses the number of genes included in it).
Blue edges between modules indicate that these modules create aggravating bicliques.
Module ESP-98 contains eight genes related to N-linked glycosylation.
It is further divided into two sub-modules (ESP-96 and ESP-97), each identified by different interactions, which have more specific functions.
(B) Schematic view of the N-linked glycosylation pathway (adapted from Helenius and Aebi, 2004).
Inside the ER lumen, four mannose residues (green circles) are added to Man5GlcNAc2 by Alg3, Alg9 and Alg12 (comprising module ESP-97).
In turn, three glucose residues (red triangles) are added by Alg5, Alg6, Alg8 and Alg10 (comprising module ESP-96).
Fig.5.
Comparison to other methods: bar charts showing how many genes (A) and interactions (B) are covered by each method.
(C) Bar chart showing how many of the protein pairs that are in the same module share a GO function annotation, or physically interact with each other.
GO annotations are divided into categories according to the number of genes in the annotation.
i231 [11:39 12/5/2010 Bioinformatics-btq197.tex] Page: i232 i228i236 A.Jaimovich et al.
not enriched with physical proteinprotein interactions, yet have a coherent function.
Furthermore, our approach is also applicable to other systems, in which the proteinprotein interaction data is very sparse (such as in the ESP dataset) or in organisms in which it does not exist.
When comparing our results to those of Pu et al.
(2008) who finds 298 overlapping modules covering 181 genes, we see that we find similar numbers of modules organized in a global hierarchy and covering more genes.
However, these advantages come at the price of lower precision (Fig.5C).
Yet, as the larger modules at the top of the hierarchy might correspond to more global functions, their enrichment in more general GO terms is reasonable.
We conclude that each of the methods strikes a different trade off between precision, sensitivity and global coherence.
3 UNCOVERING UNIDIRECTIONAL COMPENSATION Strikingly, a relatively large number of the gene pairs exhibit genetic interactions, especially aggravating ones.
We find that aggravating interactions play a major role in the definition of many modules (e.g.
150 of the 242 modules in the CB network are defined solely based on aggravating interactions).
Aggravating interactions are commonly interpreted as an indication of bidirectional compensation, where each gene can compensate for the absence of the other by performing a similar function.
However, in many cases this explanation cannot account for the observed patterns of aggravating interactions and the large number of such interactions between genes with distantly related functions.
An alternative explanation (Boone et al., 2007; Pan et al., 2006) is that one gene is crucial for functions that compensate for the abnormal cellular state resulting from the loss of the other gene.
In this scenario, termed unidirectional compensation, the relationship between the genes is asymmetric in the sense that one gene can compensate for the loss of the other but not vice versa.
We refer to the gene whose knockout causes the perturbation as the upstream gene and to the compensating gene as the downstream gene.
While examples for this type of interpretation have been shown on existing data (Pan et al., 2006), no systematic test was carried out to identify the aggravating interactions that can be explained by such unidirectional interpretation and to assess their fraction within the observed aggravating interactions.
3.1 Identifying unidirectional compensation Our premise is that we can identify unidirectional compensation by comparing the perturbation of a putative upstream gene with perturbations caused by external stimuli.
We say that an external stimulus (e.g.
a drug or an environmental insult) phenocopies a gene deletion if the genes required for coping with the stimulus are the same ones required to compensate for the perturbation of the upstream gene.
Stated in terms of available data, this definition implies a significant overlap between the genes whose knockout lead to sensitivity to the stimulus and these that have aggravating interactions with the upstream gene.
Moreover, genes in this overlap are downstream to the specific upstream gene.
By establishing such phenocopy relations, we implicate unidirectional interactions from the upstream genes and their matching downstream genes.
For example, deletion of the CHL1 gene leads to abnormal chromosome segregation similar to the damage caused by external Fig.6.
Identifying unidirectional interactions.
(A) An example of aggravating interactions (middle) that might be due to different mechanisms.
Both CHL1 and CTF19 genes (red ellipses) have functions related to sister chromatid pairing during the S-phase.
Thus, their aggravating interaction (denoted by a blue line) might be a result of their overlapping functions (left).
However, the aggravating interactions of CHL1 and BUB3, which is part of the spindle assembly checkpoint, is more likely the result of a different mechanistic reason (denoted by a directed red arrow; right), where the lack of a gene (i.e.
chl1) induces abnormal chromosome segregation, that requires the activation of the spindle assembly checkpoint including BUB3.
(B) Yeast cells exposed to benomyl (denoted by a green diamond) show the same sensitivity to BUB3 perturbation as the chl1 strain, suggesting that chl1 background causes a stress similar to exposure to benomyl.
microtubule depolymerizing agents (e.g.
benomyl).
In turn, the deletion strain of bub3 shows growth retardation under benomyl.
Thus, we interpret the aggravating interaction between CHL1 and BUB3 as resulting from unidirectional compensation, where CHL1 is the upstream gene and BUB3 is the downstream gene (Fig.6).
Indeed, this interpretation is conceivable, as Chl1 is involved in sister chromatid pairing during the S phase, and Bub3 is part of the spindle assembly checkpoint, in charge of delaying anaphase in cases of abnormal spindle assembly.
When elaborating this reasoning we have to be careful not to confuse unidirectional compensation with dosage effect: if a gene phenocopies a stimulus, we might expect to see that its deletion amplifies the effect of this stimulus, showing higher sensitivity to its application (loosely stated, higher dosage of the stimulus).
In such cases, we might mistakenly implicate an upstream gene to be downstream to another gene that also phenocopies the same stimulus.
However, in such situations we will, by definition, identify bidirectional interactions where one gene is both upstream and downstream to another gene.
Thus, we can detect these situations, and distinguish them from a proper unidirectional compensations.2 The reasoning we outline here (and apply below) detects, up to usual concerns about experimental or statistical noise, asymmetries of aggravating interactions with respect to phenotypes of external stimuli.
This is a well-defined and clear criterion.
A more ambitious step is to deduce from this asymmetry directionality in the underlying biological mechanisms.
In our example of CHL1 and BUB3, we have strong intuitions about the causal direction (as sister chromatid pairing precedes spindle assembly).
In other cases, the underlying causality is much murkier.
Moreover, we can imagine external perturbations that will lead to opposite asymmetry.
For example, if a certain drug targets in a specific manner the spindle assembly checkpoint, we would detect asymmetric behavior of CHL1 and BUB3 to it, but in the opposite direction.
This thought exercise implies that we need to be careful about deducing 2We estimate that up to five percent of unidirectional interactions are actually caused by dosage effect but were not identifed as such since not all the genes were tested in all the screens (data not shown).
i232 [11:39 12/5/2010 Bioinformatics-btq197.tex] Page: i233 i228i236 Modularity and directionality in genetic interaction maps directionality in the underlying biology.
However, we believe it is reasonable to assume that in most cases external perturbations are ones that causes cellular imbalances or stress conditions rather than disable mechanisms that cope with such situations.
3.2 Application to genetic interaction maps in S.cerevisiae To systematically detect unidirectional compensation, we collected data from genetic screens that measured growth of yeast deletion strains under various external conditions and insults compared to YPD conditions (Bennett et al., 2001; Dudley et al., 2005; Giaever et al., 2002; Hillenmeyer et al., 2008; Parsons et al., 2004, 2006).
We considered deletion strains from both homozygote diploid and haploid deletions.
We converted all measurements into a binary score, by defining genes with growth defects as those that passed the threshold defined by the authors of each study (for a detailed description of how we handled each dataset see Appendix B in the Supplementary website).
This process resulted in listing for each external stimulus the repertoire of deletion strains that display a growth defect in its presence.
In a similar manner, each gene deletion defines a list of genes that are sensitive to its deletion, i.e.
display aggravating interactions with it (using the same threshold, 2.5, as Collins et al., 2007a; Schuldiner et al., 2005).
We then define a unidirectional compensation between genes X and Y (associated with external perturbation P) if (i) there exists an aggravating interaction between X and Y ; (ii) the perturbation of Y leads to sensitivity to the external perturbation P; (iii) X has aggravating interactions with a significant number of genes whose perturbations cause sensitivity to the perturbation P (using hyper-geometric test with FDR of 0.1); and (iv) at least one of the conditions 2 or 3 do not hold on the opposite direction (when switching the roles of X and Y ).3 We applied this procedure to the CB and ESP genetic interaction maps and found 348 gene deletions that are phenocopied by at least one external stimulus.
These stimuli include a wide range of external perturbations that match the nature of the specific data set analyzed.
For example, many external stimuli corresponding to gene deletions in the CB map include agents causing DNA damage and microtubule depolymerization, while the stimuli related to the ESP map mostly include agents causing protein synthesis and glycosylation inhibition (see Supplementary website).
To our surprise, more than one-third of the aggravating genetic interactions (CB: 4659/11539; ESP: 1036/2718) could be explained by unidirectional compensation.
4 ELUCIDATING THE FUNCTION OF CELLULAR PATHWAYS We next asked whether unidirectional compensation can also be assigned within the modular hierarchy in terms of upstream and downstream modules.
Toward this end, we incorporated these unidirectional interactions into our hierarchical organization of interacting modules.
We annotated an aggravating edge between two modules as caused by unidirectional compensation if the majority 3To measure the statistical significance of the interactions we found, we created a random permutation of the names of the genes in the genetic interaction screen, and repeated the procedure described above.
In 10 repeats, no significant overlaps between genes and external stimuli were found, thus no unidirectional interactions were identified.
Fig.7.
Inter-module unidirectional interactions.
(A) Systematic identification of unidirectional interactions: a systematic search discovers cases of statistically significant overlap between patterns of gene sensitivities under specific external stimuli (green lines) and the aggravating partners of specific genes (blue lines).
We annotate these aggravating interactions as unidirectional, and denote them by red arrows directed from the upstream gene (whose deletion causes the cell perturbation) to the downstream genes (which deal with the particular cell perturbation).
(B) All inter-module aggravating edges were scanned and searched for potential unidirectional edges.
If the majority of the interactions involved in an inter-module aggravating edge are marked as consistent unidirectional interactions (corresponding to the same external stimulus and in the same direction), this edge was annotated as a unidirectional edge with respect to the specific external stimulus (green diamond).
of interactions between these modules are unidirectional and share the same context (i.e.
have the same directionality and are related to the same external stimulus; Fig.7A; Supplementary website).
By requiring consistent unidirectional interactions between modules, this incorporation also removes potential errors in the annotation of unidirectional interactions (Supplementary website).
We find that this designation elucidates the cellular role of modules and their interactions.
Coming back to our previous example, we find that perturbations of modules CB-119 and CB-187 lead to stress conditions similar to those caused by microtubule de-polymerizing agent benomyl (Fig.7B).
Our analysis identified module CB-183 as downstream to benomyl-like stress caused by mutations of genes in CB-119 and CB-187.
Indeed, the protein products of the genes in CB-119 and CB-187 are components of the machinery responsible for the correct distribution of chromosomes during cell division (Hanna et al., 2001; Measday et al., 2002).
By de-polymerizing microtubules that create the spindle fibres, benomyl attacks a crucial component of this process.
Finally, the genes in module CB-183 participate in the spindle assembly checkpoint that delays the onset of anaphase in cells with defects in mitotic spindle assembly (Nasmyth, 2005).
This example demonstrates the power of our approach in automatically providing biological insights into the function of the genes in various modules.
The concise representation of the observed genetic interactions as edges within and between modules, in combination with the specific interpretation of many aggravating edges as caused by unidirectional compensations, pinpoints novel functions of modules that are not readily apparent from clustering of genetic interactions alone.
The results of our automatic search provide an elaborate network of such inter-and intra-module edges, thus, we constructed a web-tool providing a user-friendly interface to browse our results in an effective manner (Supplementary website).
For example, examining unidirectional edges related to DNA damage agents, such as hydroxyurea and camptothecin, we find multiple upstream and downstream modules (Fig.8A).
A notable downstream module (CB-137) comprises three sub-modules; of i233 [11:39 12/5/2010 Bioinformatics-btq197.tex] Page: i234 i228i236 A.Jaimovich et al.
Fig.8.
Unidirectional interactions enable inference of functional hypotheses.
Unidirectional edges between modules (grey boxes) are annotated by red arrows.
Aggravating and alleviating interactions between modules are annotated by blue and yellow lines, respectively.
(A) Unidirectional edges involving stimuli similar to hydroxyurea and camptothecin, two DNA damage-inducing drugs.
Some edges were omitted from the graphical view for clarity.
(B) Unidirectional edges involving the deacetylation inhibitor Trichostatin-A (TSA).
Some edges were omitted from the graphical view for clarity.
these, both CB-136, that contains the Holiday junction complex, and CB-134 that comprises genes of the Rad51 pathway and MRX complex are established mechanisms of DNA damage repair.
The third sub-module (CB-132) comprises five genes whose protein products were recently characterized as involved in the acetylation of histone H3 lysine 56 (H3K56Ac) pathway (Collins et al., 2007a).
In addition, we find an alleviating interaction between the H3K56Ac module and S-phase-related module (CB-194), suggesting that the function of H3K56Ac pathway is S-phase-related.
This example illustrates the power of the combination between the hierarchical structure of modules and the annotation of unidirectional edges.
Our method identifies one parent module with a general DNA repair annotation that contains three sub-modules with different interactions that imply different specific functions.
For example, the alleviating interaction of CB-132 with CB-194 suggests that the H3K56Ac pathway is involved in relieving DNA damage in the S-phase.
Indeed, loss of H3K56 acetylation results in higher sensitivity to exposure to DNA damaging agents during S-phase (Masumoto et al., 2005) and this pathway was proposed as a DNA integrity check point following replication (Collins et al., 2007a).
Another example regards the unidirectional edges related to TSA, a histone deacetylation inhibitor that affects class I and II histone deacetylases (Furumai et al., 2001; Fig.8B).
We find two modules whose perturbation is phenocopied by TSA: Set3 complex (CB-82) and Thp1Sac3 complex (CB-92).
Set3 complex is a histone deacetylation complex, and thus it is plausible that TSA phenocopies its perturbation.
However, the relation of the Thp1Sac3 complex, comprising mRNA export factors associated with the nuclear pore, to deacetylation is less obvious.
Clues to this puzzle can be found when examining the downstream modules with respect to this external stimulus.
Most of these downstream modules are related to chromosome segregation (CB-121 and CB-183) and the Swr1 complex (CB-218), a chromatin modifier with genome integrity phenotype (van Attikum et al., 2007).
This suggests that TSA damages chromosome integrity, and that perturbations of Thp1 Sac3 complex and Set3 complex lead to similar damage.
Indeed, previous studies showed that Thp1Sac3 complex has a role in transcription elongation, and that its perturbation affects genome stability (Gonzlez-Aguilera et al., 2008).
Previous works suggested that histone deacetylation by Set3 is also associated with active transcription (Kim and Buratowski, 2009; Wang et al., 2002), leading us to hypothesize that perturbations of these complexes interfere with transcription elongation, resulting in chromosome instability.
Interestingly, we observe a directed interaction from Set3 to the Rpd3 complex (CB-40), also a histone deacetylase.
This asymmetry is consistent with the wider range of functions of Rpd3 (Suka et al., 2001) in contrast to the specificity of Set3 targets (Wang et al., 2002), explaining why Rpd3 can (partially) compensate for defects in Set3 and not vice versa.
5 DISCUSSION From maps to networks: our methodology takes a step forward towards automating the extraction of biological knowledge from large-scale genetic interaction maps.
A crucial step in dealing with the large quantities of interaction data is summarizing the observations in a representation that identifies patterns in the data.
Previous works mainly used local signatures to capture interactions between pairs of modules (Bandyopadhyay et al., 2008; Pu et al., 2008) or learn a network of disjoint modules that are coherent in terms of physical and genetic interactions (Ulitsky et al., 2008).
Here, we focus on finding a global representation that captures the bulk of the genetic interactions, without requiring additional information, and employ a module hierarchy to capture functional specialization of different sub-modules.
Our representation facilitates inspection of the large-scale results, by presenting each module along with all its interacting partners as well as its hierarchical context.
This representation defines the modules within their biological context, minimizing the requirements for expert knowledge for inference of testable biological hypotheses from genetic interaction data.
Our empirical results on two very different genetic interaction maps show that this representation captures much of the patterns of interactions in the data.
Although our representation captures many interactions, it does not include all the interactions.
Some of the missed interactions may be false positives, and thus at this front our analysis would serve to purge such data from the genetic interaction maps.
There are, however, various reasons for missing i234 [11:39 12/5/2010 Bioinformatics-btq197.tex] Page: i235 i228i236 Modularity and directionality in genetic interaction maps true interactions.
For example, some interactions are excluded since we restrict the module size to at least two genes, so that noisy measurements for a specific deletion will not dominate the results.
This implies that our procedure may miss a consistent set of interactions between a single gene and a module.
Also, the constraint of a strict hierarchy may lead to situations where a gene with multiple functions has to choose which module to belong to and thus to miss some of its interactions (Pu et al., 2008).
A natural extension of our method, which can partially resolve this issue, is to allow an extended hierarchy, where a module can be the child of more than one parent.
As demonstrated by the success of GO ontology in capturing functional annotations (Ashburner et al., 2000), such hierarchical graphs are natural in the context of functional gene organization.
Striving for mechanisms: one goal of the analysis of genetic interaction maps is to decipher the causal explanation underlying the observed interactions.
Automating this aspect of the analysis provides a significant advance toward interpretation of genetic interaction maps.
Earlier studies mostly focused on interpretations that involve complexes and pathways (alleviating interactions among members of the complex/pathway, and a similar spectrum of interactions with other genes) and redundant functions of such complexes/pathways (parallel pathways may have aggravating interactions between genes involved in these pathways).
Although other explanations were acknowledged (Boone et al., 2007; Pan et al., 2006) and implicitly used in interpreting the results, these were not reflected in automated analyses.
Here, we introduce a novel automated analysis to systematically detect unidirectional interactions where a downstream gene buffers or compensates for the effect of the perturbation of an upstream gene.
Using our automated analysis, we find that a large portion of the observed aggravating genetic interactions (at least a third) can be attributed to such unidirectional interactions.
This finding partially accounts for the large number of aggravating interactions between genes of distantly related functions.
Moreover, the analysis annotates interactions by the type of damage caused by the perturbation of the upstream genes, providing informative clues for interpreting the results.
Finally, we combine this analysis with the modular hierarchy representation to understand the relations between modules.
When looking at the types of external stimuli phenocopied by gene deletions in our analysis, we find that many of them can cause major stress conditions in the cell such as DNA damage (e.g.
by UV, hydroxyurea, camptothecin and MMS) and translation inhibition (e.g.
cycloheximide and hygromycin B).
In this case, we can interpret unidirectional compensations as connecting between a module whose perturbation causes stress and a module that has a part in relieving this stress.
Indeed, many of the downstream modules associated with such stress conditions are known to be central players in the cellular response to various stress conditions, for example the DNA damage repair module (CB-137) and spindle assembly checkpoint (CB-183).
Global examination of the resulting network shows that many highly connected modules have a high percentage of their aggravating partners related through unidirectional edges related with major stress conditions (Fig.9).
Moreover, highly connected modules tend to be either upstream (i.e.
their removal causes stress conditions) or downstream (i.e.
stress relieving), but not both (Supplementary website).
These observations suggest that Fig.9.
Many hubs of genetic interactions are related to unidirectional compensation.
A histogram of the fraction of unidirectional edges (y-axis) for modules with different degree of aggravating edges (x-axis).
Each bar shows the portion of unidirectional edges out of all edges that are connected to modules with this degree.
unidirectional compensation plays a pivotal role in forming interaction hubs in genetic interaction maps.
Furthermore, they suggest that responses of cellular integrity mechanisms to genetic perturbations are a major factor in shaping genetic interaction maps.
Toward organizational principles of genetic interaction maps: the methodology we present here puts forward two major contributions toward understanding the organization of genetic interaction maps.
First, the hierarchy of modules is automatically built independent of additional data sources, allowing its application to various existing genetic interaction maps and also to less studied organisms.
Moreover, the creation of a visual platform to study these results should boost the usability of these datasets, many of which are currently only used to find single interactions between genes of interest.
Second, we elucidate some of the mechanisms underlying the interactions between modules.
By integrating an additional data source we enabled the distinction between uni-and bi-directional aggravating interactions, and provided more functionally coherent interpretations to the genetic interaction maps.
Our results demonstrate that searching for a causal explanation for the genetic interactions highlights specific insights into the cellular roles of genes and pathways as well as elucidates global features of the genetic interaction map.
With the increasing availability of genetic interaction maps in yeast and as they become available for a large number of organisms, many of them with sparser annotation (Butland et al., 2008; Byrne et al., 2007; Roguev et al., 2008), we believe that these methods can be generalized and will prove valuable in the automated highlighting of both the functional structure of the network as well as specific biological phenomena.
This should allow us to make the first steps necessary to turn high-throughput maps into a true understanding of cellular complexity by interpreting how such maps relate to the underlying landscape of interacting cellular pathways.
ACKNOWLEDGEMENTS We thank N. Barkai, S. Gasser, Z. Itzhaki, T. Kaplan, P.D.
Kaufman, O.J.
Rando, A. Regev, M. Yassour, E. Yeger-Lotem, I. Wapinski, and J.S.
Weissman for discussions and useful comments on the article.
i235 [11:39 12/5/2010 Bioinformatics-btq197.tex] Page: i236 i228i236 A.Jaimovich et al.
We also thank S. Collins and N. Krogan for making data available prior to publication.
Funding: Eshkol fellowship from the Israeli Ministry of Science (to A.J.
); Rudin Foundation (to R.R.
); Human Frontiers Science Program Career Development Award (to M.S.
); European Union grant 3D-Repertoire, contract number LSHG-CT-2005-512028 (to H.M.); National Institutes of Health grant 1R01CA119176-01 (to N.F.).
Conflict of Interest: none declared.
ABSTRACT Motivation: The NanoString nCounter Platform is a new and promising technology for measuring nucleic acid abundances.
It has several advantages over PCR-based techniques, including avoidance of amplification, direct sequence interrogation and digital detection for absolute quantification.
These features minimize aspects of experimental error and hold promise for dealing with challenging experimental conditions such as archival formalin-fixed paraffin-embedded samples.
However, systematic inter-sample technical artifacts caused by variability in sample preservation, bio-molecular extraction and platform fluctuations must be removed to ensure robust data.
Results: To facilitate this process and to address these issues for NanoString datasets, we have written a pre-processing package called NanoStringNorm in the R statistical language.
Key features include an extensible environment for method comparison and new algorithm development, integrated gene and sample diagnostics, and facilitated downstream statistical analysis.
The package is open-source, is available through the CRAN package repository, includes unit-tests to ensure numerical accuracy, and provides visual and numeric diagnostics.
Availability: http://cran.r-project.org/web/packages/NanoStringNorm Contact: paul.boutros@oicr.on.ca Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 16, 2011; revised on March 24, 2012; accepted on April 11, 2012 1 INTRODUCTION The NanoString nCounter Platform is an emerging medium-throughput technology for measuring mRNA and miRNA abundances and for assessing copy number variants (Geiss et al., 2008).
NanoString technology has several potential benefits relative to microarray-and PCR-based technologies.
First, its parallelized nature and small number of manual manipulations generate data faster than many PCR-based methods.
Second, the hybridization method used directly interrogates target sequences, avoiding the need for bias-prone amplification steps, even for low-abundance To whom correspondence should be addressed.
transcripts.
Third, measurement is achieved using digital detection of uniquely bar-coded probes, providing absolute quantification.
This combination of advantages is thought to provide favorable conditions for testing formalin-fixed paraffin-embedded (FFPE) samples.
FFPE preparation is a standard protocol for the long-term storage of human clinical specimens.
Despite some positive reports (Hui et al., 2009), the degradation of RNA in FFPE samples challenges existing mRNA quantitation assays.
The ability to reliably process FFPE samples would greatly facilitate retrospective studies.
Accordingly there has been a dramatic increase in the uptake of NanoString technology in our research facility.
In some cases, it is used as a validation procedure; in others as a discovery tool using candidate genes.
A common rationale is the desire to exploit large, clinically well-annotated FFPE collections.
Since NanoString is a new technology, many details of its analysis remain unexplored: optimal methods of data pre-processing are unknown, although this is well-known to impact biological conclusions (Shippy et al., 2006).
NanoString currently recommends pre-processing using Microsoft Excel spreadsheet functions.
This has significant limitations.
First, manual analysis is both hard to reproduce and prone to errors (McCullough and Heiser, 2006): gene-and sample-name auto-formatting are well-known issues (Zeeberg et al., 2004).
Second, macros and worksheets cannot easily be adapted to changing experimental designs or pre-processing methods: spreadsheet software lacks the sophisticated statistical tools common in bioinformatics (e.g.
survival or mixed models).
Third, it is highly desirable to integrate workflows in a single environment, and downstream analyses are typically performed in statistical environments.
Fourth, the quality of data pre-processing requires careful assessment with data visualizations that are difficult to automate using spreadsheets.
For these reasons we have developed an open-source R package, called NanoStringNorm, to implement a reference set of pre-processing techniques.
We chose the R statistical environment to allow integration with BioConductor libraries (Gentleman et al., 2004), and to exploit its data-visualization (Chen and Boutros, 2011) and statistical tools.
NanoStringNorm outlines a pipeline of pre-processing steps with multiple options at each stage and provides a framework for method development and comparison.
By standardizing code and automating error capture, NanoStringNorm will enable more reproducible and robust analysis of NanoString datasets.
The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: APPLICATIONS NOTE [16:46 7/5/2012 Bioinformatics-bts188.tex] Page: 1547 15461548 NanoStringNorm 2 IMPLEMENTATION 2.1 Pre-processing pipeline NanoString provides analysis guidelines for miRNA and mRNA assays online, which provide detailed instructions on options available for pre-processing of the output of an nCounter.
Pre-processing involves five independent, but ordered, steps.
Complete equations for each algorithm are given in Supplementary Material.
2.2 Probe-level background correction The first step is not discussed in the NanoString guidelines, but it is mentioned in nCounter documentation: during some runs, probes are flagged for correction.
Adjustment values are annotated in text files which can be acquired from core facilities.
Correction factors are multiplied by the maximum positive control value, then subtracted from the raw counts, per sample.
2.3 Code-count normalization The second step accounts for lane-by-lane variation of the nCounter platform.
A series of exogenous probes of known concentration are used, allowing observed variation to be attributed specifically to the assay.
Normalization occurs by summarizing (i.e.
mean, median, sum or geometric mean) the positive control counts and adjusting samples by a factor relative to other samples.
2.4 Background correction Even though data are presented as absolute counts, detectable levels of non-specific binding remain, as measured by negative controls.
The background level is calculated as an aggregate summary of six to eight control probes with no target transcript.
The calculated background level for each sample is then subtracted from the adjusted probe value.
Negative values are scaled to zero as a baseline for no expression.
The mean of negative controls is a non-stringent correction that maximizes sensitivity in exchange for increased false positives.
Alternatively, the maximum or mean plus two SDs can be used to prevent spurious results.
2.5 Sample content normalization Sample RNA input variation is caused by laboratory and pipetting anomalies independent of NanoString assay variation.
Using housekeeping genes is one common method of removing this bias.
For miRNAs and FFPE samples, it is often unclear how such genes should be selected.
Alternative approaches include summarizing all genes, or focusing on a set of highly expressed genes.
Sample content normalization factors are calculated similar to code count ones: a group of normalization genes is selected, summarized (e.g.
geometric mean, etc.)
and a sample adjustment factor is calculated.
2.6 Concentration prediction A convenience function has been added to transform counts to concentrations using positive controls: fitting of positive control concentrations is used to predict endogenous probe concentrations.
2.7 Best normalization To evaluate the best pre-processing method for specific code sets and study designs, a function was created to rank methods based on the Fig.1.
NanoStringNorm volcano plot showing differential expression caused by batch-effects between experiments on two days.
Point-size reflects mean counts per probe.
Naive co-normalization introduces bias coefficient of variation of control probes, the intra-class correlation of replicate groups and the fold change accuracy for primary traits of interest.
This function is suited for studies with technical replication, such as titrations.
2.8 Software engineering decisions The package is highly modular, to maximize extensibility.
Gene and sample summary plots and diagnostics (Fig.1) are auto-generated and flag experimental artifacts.
Unit-tests are continuously added to diagnose and prevent numerical errors.
2.9 Visual and numeric diagnostics We developed extensive diagnostics to enhance interpretation, with a particular focus on experimental quality and reliability.
NanoStringNorm flags potential systematic batch-effects, identifies background artifacts and assesses negative/positive-controls.
Each flag is associated with plots (Supplementary Fig.S3) and all results, diagnostics and differential expression can be output in interactive Google motion charts (Supplementary Fig.S4).
3 RESULTS AND CONCLUSIONS For a variety of reasons, NanoString usage is rising; making standardized approaches to data analysis increasingly critical.
We have therefore developed an extensible, open-source R package to automate pre-processing.
Our package enables a wide range of pre-processing approaches to be evaluated, and will be updated as improved techniques are developed.
For example, NanoString data can be processed and statistically analyzed based on the negative binomial distribution (Brumbaugh et al., 2011).
Similar modeling, integrated into the NanoStringNorm API, will make NanoString data broadly useful for clinical applications.
Future work will account for 1547 Copyedited by: TRJ MANUSCRIPT CATEGORY: APPLICATIONS NOTE [16:46 7/5/2012 Bioinformatics-bts188.tex] Page: 1548 15461548 D.Waggott et al.
correlation among experimental design features such as replicates, tissue subgroups and inter-experimental batch-effects.
ACKNOWLEDGEMENTS We thank the Boutros lab for support and software testing.
Funding: Ontario Institute for Cancer Research (to P.C.B.
and B.G.W.
); Ontario Ministry of Health and Long Term Care (OMOHLTC; to B.G.W.
and F.-F.L.
); Dr Mariano Elia Chair in Head & Neck Cancer Research; philanthropic support from the Wharton family, Joes Team and Gordon Tozer.
Views expressed do not necessarily reflect those of the OMOHLTC.
Conflict of Interest: none declared.
ABSTRACT Summary: The Unstructured Information Management Architecture (UIMA) framework and web services are emerging as useful tools for integrating biomedical text mining tools.
This note describes our work, which wraps the National Center for Biomedical Ontology (NCBO) Annotatoran ontology-based annotation serviceto make it available as a component in UIMA workflows.
Availability: This wrapper is freely available on the web at http://bionlp-uima.sourceforge.net/ as part of the UIMA tools distribution from the Center for Computational Pharmacology (CCP) at the University of Colorado School of Medicine.
It has been implemented in Java for support on Mac OS X, Linux and MS Windows.
Contact: chris.roeder@ucdenver.edu Received on February 1, 2010; revised on April 12, 2010; accepted on May 9, 2010 Integration and ease of installation are increasingly important concerns as the number of biomedical text mining tools grows in size and complexity.
Many tools are deployed as web services to avoid complex installation.
The National Center for Biomedical Ontology (NCBO)s Annotator (Jonquet, 2009) is one such tool.
It integrates many biomedical ontologies into a concept annotation service available on the web.
Instead of installing the tool locally, users outside of the NCBO can simply embed a web service client in their applications.
However, challenges for a bioinformatician extend beyond using a single service.
Pipelines of tools are often assembled to accomplish a greater goal like identifying and annotating protein protein interaction events.
These tools work with different data formats for input and output, making the creation of a processing pipeline cumbersome.
The Unstructured Information Management Architecture (UIMA) (Ferruci, 2006) is a framework for integrating such tools into a common data representation and interface.
It provides a mechanism for running the tools in unison and extensions for scaling to larger processing loads.
UIMA is commonly used in the biomedical natural language processing (NLP) community, and recognition of ontology concepts is an important component of many text mining applications.
Hence, adaptation of the NCBO Annotator to UIMA will likely result in broad adoption of the Annotator in biomedical text mining research.
Once a tool has been adapted to UIMA, it can be used in many different assemblies or pipelines with other tools also adapted To whom correspondence should be addressed.
to UIMA.
The Center for Computational Pharmacology (CCP) at the University of Colorado School of Medicine has a collection of such adaptations or wrappers (Baumgartner, 2008), and has successfully used UIMA in the development of systems for participating in the BioCreative and BioNLP shared tasks.
The CCP has now also adapted the NCBO Annotator to UIMA, making it available to UIMA projects.
The NCBO Annotator automatically processes a piece of raw text to annotate (or tag) it with relevant ontology concepts and return the annotations (Jonquet, 2009).
The Annotator accesses over 200 biomedical ontologies from the Unified Medical Language System (UMLS) Metathesaurus (http://www.nlm.nih.gov/research/umls/) and the NCBO BioPortal (http://bioportal.bioontology.org/).
The biomedical ontologies used by the Annotator can be thought of as enriched term lists that include relationships and synonyms.
One of the ontologies available is the Gene Ontology (GO), (http://www.geneontology.org/) which can be used to find references to cell components, biological processes and molecular functions.
The Annotator identifies ontology terms in submitted text and returns formally described annotations in the form of a Uniform Resource Identifier.
For example, if mitochondrion appeared in the submitted text, the Annotator would return the start and end character indexes of the word, the GO ontology id, 39917, and an id for the concept within GO, GO:0005739.
Such direct matches are found using the MGREP tool (Xuan, 2007).
The Annotator enables the use of the hierarchical structure of the ontologies as well to provide more functionality: given a particular parameter setting, it can climb the ontologys is-a hierarchy and return more general concepts that relate to a particular term.
For instance, it could return the ancestors of mitochondrion, i.e.
intracellular membrane-enclosed organelle, intracellular organelle, organelle and cell component.
Such matches are called is-a matches.
They are considered indirect matches because MGREP does not match mitochondrion with them, rather they are found through relationships in the ontology.
The UMLS and BioPortal also allow the Annotator to search between ontologies using mappings to produce a broader range of results including those from different forms of the term such as plurals (mitochondria would match mitochondrion for example).
The UIMA wrapper and the Annotator are actively being used in ongoing research that performs concept matching in text.
The ultimate utility of this wrapper is to use the Annotator in more elaborate pipelines where the generated semantic annotations are used as input to downstream processing, for instance to help identify complex relationships between biological entities.
For example, the CCPs work in BioNLP09 (Cohen, 2009) used similar methods to The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:00 16/6/2010 Bioinformatics-btq250.tex] Page: 1801 18001801 UIMA wrapper for the NCBO annotator create annotations that were then referenced in OpenDMAP (Hunter, 2008) patterns for identifying protein interactions.
To make the functionality of the Annotator easily available to other projects, it is deployed as a web service.
A web service is a software component that makes functionality available over the web in order to be used by computer programs.
In this case, the Annotator is accessible to both humans and computers, respectively through the BioPortal user interface and the BioPortal web service application programming interface.
The ubiquity of the web and the simple protocols involved make it easy to access.
All that is required is an HTTP library and an XML parser.
Other output formats are available that are even easier to parse, though do not include as much detail.
The Annotator allows for control of term matching by exposing parameters available in the web service.
For example, climbing the inheritance hierarchy can be limited with the levelMax parameter.
A full description of parameters is available online (http://www.bioontology.org/wiki/index.php/Annotator_Web_service).
All options described there are supported by the UIMA wrapper with the exception of outputFormat which is set to XML so that the results can be parsed internally to the wrapper itself.
Integrating the Annotator with UIMA requires a type system specification that defines the Java classes used to store annotations.
This wrapper uses the CCPs type system, making it compatible with any of the other wrappers available in the CCPs collection, BioNLP-UIMA (Baumgartner, 2008), freely available online (http://bionlp-uima.sourceforge.net).
Using this wrapper in combination with other UIMA tools will involve some adaptation between type systems.
Since no standard type system has emerged (Hahn, 2008), different UIMA adaptations are likely to use different type systems.
The Julie Labs JCORE (Hahn, 2008) and the Tsujii labs U-Compare system (Kano, 2009), for example, use different type systems.
Each has used different methods for adapting between type systems that apply here as well.
JCORE makes use of a pair of type system adapters that convert from the local type system to the foreign one and back.
Such a converter can be re-used whenever another tool from the same foreign type system is used.
U-Compare makes use of both a shared type system where comparable annotations share a base class (Kano, 2008), and adapter pairs.
Included with U-Compare is a pair of adapters for using CCP wrappers.
The topic of type system design is explored more fully in Verspoor (2009).
Annotations are retrieved from the web service by the wrapper using the HttpComponents (http://hc.apache.org/) from Apache.
They are unmarshalled from the returned XML by JAXB (https://jaxb.dev.java.net/).
Then they are represented in the primary UIMA data structure, the Common Analysis Structure or CAS, using the structures available in the CCP type system (Verspoor, 2009).
A CCPTextAnnotation delineates the span of the matched concept and holds references to other objects describing the annotation such as a CCPAnnotator that shows it came from the NCBO Annotator.ACCPClassMention object, named after the ontology and the term id, GO:000557, for example, contains references to CCPSlotMention objects that contain the attribute values.
The attributes are ontology, concept and type.
The type attribute tells what kind of a match the Annotator used to find the concept.
Possible values are DIRECT, MAPPING and IS-A.
DIRECT is for direct matches from the text to an ontology term found with MGREP.
MAPPING is a match found in a second ontology based on a correspondence to a term in an initial ontology.
More general concepts found through an ontologys hierarchy are marked with a type of IS-A.
IS-A matches have a fourth attribute that tells how far up the hierarchy the term is, named LEVEL.
The wrapper limits the size of the requests it makes on the web service by using sentence annotations created by an analysis engine upstream in the processing pipeline (we currently use the LingPipe toolkit for sentence analysis), and sending one sentence at a time.
This keeps requests below the 300 word limit.
The NCBO reports that with 10 simulated users, requests of 280 words take <5 s. Our tests have been limited to five concurrent requests to leave some bandwidth for others.
On the rare occasions the Annotator is unavailable, the wrapper waits for a short timeout and throws an exception that stops the pipeline.
The wrapper is available as part of the CCPs BioNLP-UIMA distribution.
It requires Java 1.6 and includes build scripts that run on Macintosh and Linux variants.
Windows support is forthcoming.
Scripts download required third party jars and run the build.
A sample pipeline is included ready to run after installation.
Users outside our lab have installed the software quickly and easily.
Further pipelines can be built using a UIMA-supplied GUI.
The NCBO Annotator provides annotations from a wealth of ontologies.
Packaged as a web service, it is readily available to NLP researchers.
With a UIMA adaptation provided by the CCP, it is now also available to the world of UIMA tools and pipelines.
ACKNOWLEDGEMENTS The authors would like to thank the anonymous reviewers and Kevin Livingston for helpful discussion and review.
Funding: National Institutes of Health (5R01-GM083649-02, 2R01-LM008111-04A1, 2R01-LM009254-04 to L.H.
); National Center for Biomedical Ontology (U54 HG004028).
Conflict of Interest: none declared.
ABSTRACT Motivation: We propose an efficient method to infer combinatorial association logic networks from multiple genome-wide measurements from the same sample.
We demonstrate our method on a genetical genomics dataset, in which we search for Boolean combinations of multiple genetic loci that associate with transcript levels.
Results: Our method provably finds the global solution and is very efficient with runtimes of up to four orders of magnitude faster than the exhaustive search.
This enables permutation procedures for determining accurate false positive rates and allows selection of the most parsimonious model.
When applied to transcript levels measured in myeloid cells from 24 genotyped recombinant inbred mouse strains, we discovered that nine gene clusters are putatively modulated by a logical combination of trait loci rather than a single locus.
A literature survey supports and further elucidates one of these findings.
Due to our approach, optimal solutions for multi-locus logic models and accurate estimates of the associated false discovery rates become feasible.
Our algorithm, therefore, offers a valuable alternative to approaches employing complex, albeit suboptimal optimization strategies to identify complex models.
Availability: The MATLAB code of the prototype implementation is available on: http://bioinformatics.tudelft.nl/ or http://bioinformatics.
nki.nl/ Contact: m.j.t.reinders@tudelft.nl; l.wessels@nki.nl 1 INTRODUCTION To explain complex biological phenomena it is of vital importance to measurein the same sampleall relevant (complementary) biological variables, and to measure these at a genome-wide scale.
For this reason, many multimodal screens have been performed that have complemented transcriptional profiling with, among others, copy number variation measurements, transcription factor binding assays, methylation status profiling or genotype calls (Bystrykh, 2005; Pollack et al., 2002; Shames et al., 2006; Visel et al., 2009).
A common aim in analyzing these multimodal datasets is to find associations between the biological variables measured to infer their regulatory role.
Consider, for instance, a study in which expression profiles and genome-wide genotype data were obtained in hematopoietic cells from a panel of fully homozygous recombinant inbred mouse strains (Fig.1A).
This genetical genomics approach To whom correspondence should be addressed.
enables the determination of expression quantitative trait loci (eQTLs) characterized by strong associations between the genotype and the observed expression levels (Jansen and Nap, 2001; Schadt et al., 2003).
In the absence of a strong direct association between the genotype and gene expression, real multi-locus interactions may still be present, due to epistatic interaction (Frankel and Schork, 1996; Michaelson et al., 2009).
Such interactions may not be detectable as (marginal) direct associations between the genotype and gene expression (Fig.1B).
To alleviate this, approaches which evaluate the joint association of multiple loci and a phenotype of interest are required.
Several approaches have been proposed to attack this problem.
These approaches differ mostly regarding the way the associations are modeled and the strategy employed to solve the combinatorial optimization problem.
Some approaches (Manichaikul et al., 2009; Wongseree et al., 2009) follow what could be loosely termed a two-stage approach, where all two-locus models are first evaluated, which, in stage two, are used in a greedy search to yield multi-locus models.
Approaches employing more advanced strategies to traverse the space of possible models are represented by a genetic programming approach (Nunkesser et al., 2007) and Markov Chain Monte Carlo (MCMC) approaches associated with Bayesian analyses (Mukherjee et al., 2009; Zhang and Liu, 2007).
Since two-stage approaches have been demonstrated to be suboptimal (Evans et al., 2006) and advanced search strategies such as MCMC are very sensitive to their implementation and parameter settings, and are not guaranteed to be optimal, an approach that finds a provably global solution to a selected model within reasonable time is highly desirable.
Of particular interest is the method of Ljungberg et al.
(2004) which is used for the pair-scan analysis that is available on the GeneNetwork on http://genenetwork.org.
Ljungberg et al.
(2004) stress the importance of performing a global search rather than relying on greedy searches by (pre)selecting markers based on their marginal effects.
To deal with the computational complexity associated with such an optimization problem, the authors present a method to find global optima of a linear regression problem for up to three predictors that is fast enough to be employed in permutation procedures.
In contrast to the class of additive models employed by Ljungberg et al.
(2004) (and many other approaches), we follow others (Kooperberg and Ruczinski, 2004; Mukherjee et al., 2009; Nunkesser et al., 2007) and employ Boolean combinatorial logic to explicitly incorporate interactions in the eQTL inference.
To this end, we infer combinatorial association logic (CAL) networks that combine the observed genotypes through and (), or () and xor The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i150 i149i157 J.de Ridder et al.
A B C Fig.1.
Schematic overview of data and association inference.
(A) A panel of BXD mice that is densely genotyped and expression profiled.
The genotype data can be considered as binary vectors by choosing a binary encoding of the alleles (in the figure D=true and B=false) and putting thresholds that divide the genome into loci such that each locus differs in at least one element from its neighbors.
The cartoon shows that good association is obtained between Locus 5 and Gene 7 because elevated expression is consistently observed in conjunction with the D allele of Locus 5.
(B) Interaction among genetic features may destroy direct associations between individual loci and genes.
The cartoon shows that configurations exist in which the gene expression can only be predicted by considering two loci simultaneously (using Boolean xor logic).
(C) By inferring CAL networks, interaction among genetic features is taken into account in the association inference.
Inferring CAL networks is achieved by selecting the input loci with the selection function S and combining these with the appropriate Boolean function B, such that the association (as measured by a scoring function f ) between the network output and the gene of interest is maximized.
() functions by searching for associations between the result of the Boolean operation and the gene expression.
The Boolean and function can be used if altered expression is consistently observed in combination with a particular combination of two alleles (which do not necessarily have to be equal), but remains unchanged in all other genotype configurations.
An example of a situation in which this may be observed is the case of two parallel pathways that only promote transcription of their downstream target when the genes in these pathways have specific alleles.
Conversely, we may also consistently observe differential transcription in the strains for which either one of two loci is of a certain genotype.
This may, for instance, be observed in case of a cascaded signaling pathway: a silencing mutation in one of the alleles can repress the entire pathway, regardless of which gene in the cascade contained this mutation.
Boolean or () and xor () are capable of capturing this behavior (Fig.1B).
Like the search for optimal predictors in the additive model, inferring optimal predictors of a Boolean function is a challenging computational problem, especially considering that more complex combinations of these functions are also possible.
Moreover, we noted that the objective function that needs to be optimized is highly discontinuous and nonlinear so that standard optimization techniques, such as genetic algorithms, simulated annealing and MCMC do not provide an optimal solution.
Nevertheless, an efficient andmost importantlyglobal solution is highly desirable, since this allows permutation procedures with which significance estimates of the discovered associations can be realized (Ljungberg et al., 2004).
In the following, we will mathematically prove that, under reasonable conditions, CAL network inference provides an efficient way to obtain globally optimal multi-locus models that associate multiple genomic loci with the expression of target genes.
We illustrate our approach on the genetical genomics dataset from Gerrits et al.
2009, and using these data show that 100% accuracy is achieved at runtimes that are a fraction of those required for exhaustive search.
Furthermore, we observe that using this approach complex associations are revealed that otherwise would have gone unnoticed.
As such, our approach offers a useful alternative to the commonly used additive models and suboptimal search strategies.
2 METHODS 2.1 CAL network search The construction of a CAL network that predicts the expression profile from a set of binary predictors can be formulated as an optimization problem.
Interesting logic networks are those for which maximal association between the network output and the gene expression is obtained.
Let g be the (T 1) vector, with T the number of samples, containing the expression values of a gene and L the (T L) matrix of binary predictors, e.g.
the genotypes, where L is the number of predictors.
A CAL network L is defined in terms of S(L;n) :BL BN , a selection function that selects N columns from L and B(I) :BN B, a Boolean logic function that specifies the network topology.
In the latter, (T N) matrix I is a concatenation of the columns selected by S, i.e.
I= (in(1),...,in(N)), where n is a (N 1) vector containing the indices of the selected columns.
Consequently, CAL network L maps the genotype matrix L to a (T 1) output vector y as follows: y=L(L;B,n)=B(S (L;n)).
(1) The association between g and y is quantified with an association measure f (g,y) f (g,y)=  |x0x1| (n01)s20+(n11)s21 n0+n12 ( 1 n0 + 1n1 ) if (n0 >)(n1 >) 0 otherwise.
(2) For notational convenience, we used x0 ={g() :y()=0, (1,...,T )} and x1 ={g() :y()=1, (1,...,T )}, i.e.
vector g is split into x0 and x1 according to the Boolean values in y.
Furthermore, x0 (x1), s20 (s 2 1) and n0 (n1) are defined as the sample mean, the sample variance and the number of elements in x0 (x1), respectively.
Note that Equation (2) is equal to the absolute value of the t-statistic, except when n0 or n1 becomes too small, which ensures high f-values are only obtained in Case x0 and x1 have at least elements.
The inference of CAL networks is a computationally challenging problem.
Primarily, because the feature selection problem, i.e.
finding the optimal vector n, critically depends on the number of features that are considered.
i150 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i151 i149i157 Combinatorial association logic networks In the case of genetic markers, this easily runs in the several hundreds to thousands.
Moreover, the optimal subset of markers is heavily dependent on how these markers are combined, i.e.
dependent on the optimal Boolean function B.All together, one frequently has to rely on greedy search strategies that easily get stuck in local optima or near exhaustive searches that are computationally too expensive, especially when employed in permutation procedures required to assess statistical significance.
Our solution to this problem hinges upon two observations.
First, in most practical datasets the sample size is relatively small, especially when compared to the number of features.
This means that we can limit ourselves to considering only small CAL networks with few inputs, since larger networks are prone to overfitting, which makes them less informative.
For this reason, and because most networks have many equivalent topologies that do not need to be evaluated due to symmetry, the set containing all unique and meaningful network topologies {Bj : j=1,2,} is relatively small (in the order of 10100, depending on the desired topology).
Consequently, the set of optimal input vectors {nj : j=1,2,}, associated with each Bj , can be found by fixing Bj and maximizing for each Bj separately nj =argmax n { f ( g,Bj(S(L;n)) )} .
(3) Second, we observe that Equation (3) still represents a complex optimization problem that can be significantly simplified by employing an approximation to the association measure, denoted by f .
In the following, we show that maximizing f is equivalent to maximizing f , but the maximization of the former can be very efficiently realized by using a branch and bound search.
Before defining f , we define the Boolean vector yopt as the solution for which f reaches a global maximum independent of the network topology, i.e.
yopt =argmaxy f (g,y).
Note that yopt can be easily determined by sorting the gene expression vector g and evaluating all positions for a threshold t that splits g into x0 and x1 (Fig.2A).
For f , we use the weighted Hamming similarity between yopt and the network output y f (yopt,y)= w()I(yopt()=y()) (4) A B C Fig.2.
Association versus approximated association.
(A) Example gene expression vector (circles) split in x0 and x1 according to yopt.
The magenta line denotes the association measure f , defined in Equation (2), as a function of a threshold t that splits the expression vector in x0 and x1.
The blue triangles indicate the error weights w() that result after optimizing them.
(B and C) 500 random samples that are generated by introducing up to seven bit-flips in yopt to show the relation between f and f .
The red dot indicates f and f values for yopt.
(B) shows the samples in case the weights are assumed equal.
Although the trend of the data is monotonically increasing, a large spread around this trend is observed.
(C) shows the same samples in case the weights are optimized, resulting in a near one-to-one relation between f and f .
where w()>0 denotes the weight for sample , and I() the indicator function, evaluating to 1 if the-th element of vectors yopt and y are equal.
For an example gene expression vector, Figure 2B shows 500 random samples of (f ,f ) pairs, in case all weights are equal to one.
Although the trend of this distribution is monotonically increasing, the spread around the trend is substantial.
This is undesirable because a maximum in f is only guaranteed to correspond to a maximum in f in case there is a direct one-to-one relation between them.
Clearly, this is not the case in Figure 2B, since each value of f corresponds to many values of f .
However, by optimizing the weights such that the difference between f and f is minimal, a near one-to-one relation can be obtained, as exemplified by Figure 2C.
With the proper adjustments, detailed below, it is thus ensured that maximizing f is equivalent to maximizing f .
The major advantage of maximizing f instead of f is that in the former each sample has an independent contribution to the association measure.
This can be readily exploited using a branch and bound search, so that it is possible to avoid the expensive evaluation of the association measure.
2.2 Optimizing Equation (3) Here, we show that optimizing Equation (3) can be achieved by first determining f =maxn(f ), where f was defined in Equation (4).
After this the search for f =maxn f is readily solved by searching in the neighborhood of f .
For a single sample , let V () be the set of input combinations such that y()=yopt()nV (), where y=L(L;B,n).1 Figure 3AC shows how V () can be inferred from L and the truth table of B.
For a set of samples C, the input combinations nV (C) for which all C reach the optimal output yopt are found by taking the intersection of all the individual sets of input combinations, i.e.
V (C) =C V ().
Note that, under the assumption that each sample has at least one non-zero locus, V () 	=.
In other words, for individual samples there always exists a combination of inputs for which the network can reach the desired optimal output yopt.
However, for an arbitrary combination of samples this is clearly not the case.
If we observe that V (C) =, this means that for the collection of samples in C there does not exist a valid combination of inputs.
Moreover, if V (C) =, all supersets of C will also result in the empty set.
Finally we note that, by choosing a convenient binary encoding, V () and V (C) can be computed very efficiently by means of bitwise xnor and and operations, respectively (see Fig.3D and the Supplementary Fig.S1 for details).
With these definitions in mind, we propose the following lemma: Lemma 1. f =max C C w() subject to: V (C) 	= (5) Proof.
Let C =argmaxC C w(), i.e.
C is the set of solutions for which f is obtained.
Since it is required that V (C) 	=, there must be at least one solution n such that yopt()=y() C. Since for C the optimum in f is obtained, it must also hold that yopt() 	=y()/C.
This means that Equation (4) can be rewritten as follows:  w()I(yopt()=y())= C w(), proving the statement in this lemma.
As argued by Lemma 1, Equation 4 is thus maximized by having as many samples in C as possible, while taking into account their respective weights w().
Before we will show that Equation (5) fits a branch and bound framework, we first make the observation that for the relation between f and f the following holds: (f (yopt,y1)< f (yopt,y2)) (f (g,y1)< f (g,y2)), (6) 1Since we optimize Equation 3 for each Bj separately, we omit its subscript if its meaning is inconsequential.
i151 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i152 i149i157 J.de Ridder et al.
A B C D Fig.3.
Computation of solution sets for each sample.
(A) Example data from Figure 1A.
(B) The topology and the truth table of the Boolean function B under investigation.
(C) Explanation by example of the calculation of V (), the set of all possible input combinations to B such that yopt()=y().
This panel shows how V (1) is determined.
Since yopt(1)=1, the rows from the truth table for which y=1 are applicable, i.e, r ={2,4,6,7}.
According to r =2, the desired output for =1 is obtained by selecting any of the loci that are 0 for inputs i1 and i2, and loci that are 1 for input i3.
Accordingly, for i1 we may select from the set: {l1,l2,l4}.
This can be efficiently calculated by taking the xnor (evaluates to 1 when both inputs are equal) between row =1 from the data matrix and the row r =2 from the truth table, as shown in (C).
Observe that the result is an efficient encoding of all the possible input combinations that satisfy yopt(1) while using r =2 from the truth table.
In general, we denote this set by V ()r , and its binary encoding by V ()r .
To determine the complete set of valid input combinations for =1, rows 4, 6 and 7 need to be considered in a similar fashion.
V (1) is now determined by taking the union of the subsets, i.e.
V (1) =V (1)2 V (1)4 V (1)6 V (1)7 , which, in binary form, may be represented by a concatenation of V (1)2 , V (1)4 , V (1)6 and V (1)7 .
(D) This panel shows the valid input combinations for =1 and =3 in binary representation (i.e.
V (1) and V (3)).
For any set of samples C the input combinations for which the output equals yopt can be obtained by taking the intersection of the individual sets.
In binary representation, this is equivalent to taking the row-wise cartesian product (row-wise product of all combinations of rows), as is shown in the panel.
where y1 and y2 are two Boolean vectors.
Note that, for =0, Equation (6) reduces to the requirement for strict monotonicity, and that for larger >0 this requirement is increasingly relaxed.
Even though this seems trivial, the value of this relation becomes clear by considering that if there exists a strong positive correlation between f and f , there may in fact exist a small for which Equation (6) is true.
Based on Lemma 1 and Equation (6), we observe that solutions that are suboptimal in terms of f may still be optimal in terms of f , since can be non-zero.
In the following, let {yi : i=1,2,} and {Ci : i=1,2,} be all the network outputs and the sample sets for the solutions for which holds that f  f (yopt,yi) f *, respectively.
Finally, let be chosen such that Equation (6) holds.
Our main theorem can now be formulated as follows: Theorem 2. n  Ci V (Ci) (7) Proof.
First, assume that Equation (6) holds for =0, and thus f (yopt,yi)= f i.
Furthermore, from Equation (6) it follows that in this case there exists a direct one-to-one relation between f and f .
Consequently, a maximum in f is guaranteed to correspond to a maximum in f and V (Ci) must contain n. This is true because from Lemma 1 it follows that V (Ci) 	=.
For non-zero values of , the one-to-one relation does not hold.
However, from Equation (6), it follows that all values of f for which the corresponding f lies outside the interval [f , f ] are strictly smaller than the value of f corresponding to f .
Thus, it must be the case that the maximum of f is constrained to solutions for which f lies in the interval [f , f ].
Therefore, the union of the sets of solutions that lie in this interval will contain n. From Theorem 2 it naturally follows that: Corollary 3. n =argmax n f (g,L(L,B,n))nV (Q), (8) where V (Q) =Ci V (Ci).
Notably, if there exists a small for which Equation (6) holds, the number of solutions in V (Q) is limited, and hence n is easily determined by an exhaustive search over all possible solutions in V (Q).
In the following, we show that in practice the set V (Q) is small by choosing w such that is small.
2.2.1 Estimating the weights Ideally, vector w is chosen such that is minimal.
For practical purposes, it is sufficient to choose w so that is small, which can be realized by minimizing the difference between f and f .
For this purpose, we sample the (f ,f ) relation by generating N random instances yn by introducing up to m random bit-flips in yopt (shown in Fig.2B and C).
The N corresponding association measures fn and Hamming similarities are collected in vector f =[f (g,y1),f (g,y2),]T and matrix F=[(yopty1)T ,(yopt y2)T ,]T , respectively.
In the latter, denotes the xnor operation, which evaluates to 1 in case its arguments are equal.
Notably, m (the number of bit-flips) should be chosen such that the region of interest of the distribution of f is sampled.
Since we are interested only in network outputs that associate well with the gene expression, we can choose m rather small to focus only on the right tail for which a good fit between f and f is obtained.
We found that smaller residuals were obtained by converting log-transformed f-values to z-scores, i.e.
f =z(lnf).
Furthermore, to deal with the intercept, the matrix F is mean centered, denoted by F. Using the vector f and matrix F we can find the weights w by constraint linear least squares minimization w=argmin w ||f Fw||2, subject to: w()w (9) where w > 0 is a small scalar that ensures each sample receives a non-zero weight.
Figure 2 illustrates a typical example showing that the trend of the relation is monotonically increasing, and the spread around the trend is marginal, indicating that Equation (6) indeed holds for a small .
2.2.2 Estimating The parameter can be estimated by randomly resampling the (f ,f ) relation using the obtained weights and measuring the spread around the trend in the data in the f direction (Fig.2C illustrates this schematically).
To this end, lowess smoothing was performed to obtain the the trend in the data (Cleveland, 1979).
Subsequently, the spread around i152 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i153 i149i157 Combinatorial association logic networks this trend was obtained by applying a sliding window in the f direction and defining as the maximum spread across all window positions.
2.2.3 Branch and bound search tree Equation (5) naturally fits a branch and bound framework with a backtracking search tree, in which each node corresponds to a particular set of samples C (shown in Supplementary Fig.S2).
Although this tree exhaustively represents all possible sample sets C, the search is very efficient since most nodes can be pruned from the search tree.
First of all, if V (C) becomes equal to the empty set, all child nodes of node C can be discarded because these will also result in the empty set.
Second, as a result of the search tree topology, for each node C we can define an upper bound f (C)up and lower bound f (C) low.
The upper bound f (C) up is defined as the value of f that would be obtained assuming all its subnodes do not result in the empty set (best case scenario) f (C)up = C w()+ Csub w(), (10) where Csub denotes the collection of all samples in the subnodes of C. The lower bound f (C)low is defined as the value of f that would be obtained assuming all subnodes will result in the empty set (worst-case scenario) f (C)low = C w().
(11) A vast reduction of the search space is realized by considering the following branch and bound principle: any node C can be pruned if there exists a node C , for which the following is true: f (C)up < f (C) low , under the condition: V (C) 	= (12) Thus, if we encountered a branch whose worst-case error is better than the best-case error of another branch, we can safely discard the latter.
After the complete search tree is traversed, the set V (Q) is determined by the union of all the nodes that resulted in a non-empty V (C).
In Equation (12), the parameter is included to ensure that set V (Q) includes n (Theorem 2).
An optimal leaf ordering is obtained when the samples are sorted based on their weight w().
This ensures that f (C)up decreases as quickly as possible, in effect pruning the tree early in the search.
Also, note that most V (C) will contain many duplicates when symmetries in the topology of B are considered.
By filtering these from V (C) before evaluating the succeeding node results in an additional search speedup.
2.2.4 Tolerance level A final, yet influential, search-space reduction is achieved by only considering solutions for which a certain minimum level of association is achieved.
This is realized by enforcing that f low can never be below a user defined tolerance level.
In other words, for this bounded f low, we can write: f low =max(f tol, f low).
As a result, branches for which f low f tol can be pruned even before the search is started.
The search procedure is explained by example in Supplementary Figure S2.
2.2.5 Estimating the false discovery rate Because our primary interest lies with the interpretation of the selected genotype markers and combinatorial logic, it is of critical importance to assess frequency of false positives among the networks called significant.
Due to the efficiency of the proposed method, it is possible to employ a permutation procedure to obtain a null-distribution for each Bj .
From this distribution, it is possible to estimate the false discovery rate (FDR) and the associated q-values by using the method proposed in Storey and Tibshirani (2003).
Not surprisingly, in many cases, multiple network topologies yield significant associations with the same gene.
The q-values, available for each of the solutions, provide a convenient way of performing selection of the most parsimonious model by accepting only the topology for which the q-value is minimal.
3 RESULTS 3.1 Genetical genomics dataset The genetical genomics dataset used to demonstrate our method contains genome-wide RNA transcript measurements performed on four related hematopoietic cell populations (Gerrits et al., 2009).
These were isolated from the bone marrow of 25 BXD recombinant inbred mouse strains that were derived by crossing C57BL/6J (B6) and DBA/2J (D2) (Peirce et al., 2004).
A typical analysis of these data includes determining eQTLs, i.e.
regions in the genome for which the genotype across strains associates well with RNA transcript levels.
We inferred associations only for the myeloid cell population, as for this cell type data for the largest number (T =24) of unique BXD strains were available.
The expression data were preprocessed as described in the Supplementary Methods.
Because the CAL networks inferred for highly correlated genes are equivalent, rather than starting the optimization for each gene separately, we constructed gene clusters and searched for CAL networks for the centroids of each gene cluster.
To ensure only tightly correlated probes were clustered, we employed a stringent cutoff (correlation distance cutoff 0.2).
This resulted in 6139 clusters that were used to determine eQTLs.
Genotype information for the strains was retrieved from The GeneNetwork (http://www.genenetwork.org/dbdoc/BXDGeno .html).
Genotype markers that were highly similar across strains and on the same chromosome were also grouped into clusters to prevent the algorithm from finding many combinations of genotype markers that are equivalent (such as the markers in linkage disequilibrium).
This resulted in 453 marker clusters (L=453).
The cluster centroids were defined as the majority vote of the individual markers in the cluster and were used as putative inputs to the network (see also the Supplementary Methods and Supplementary Figs.
S3 and S4).
For setting the tolerance level ftol no straightforward method exists.
Preferably, the tolerance level is set close to the final significance threshold to minimize the effort spent on finding optima for gene clusters that can never be significant.
We settled for a tolerance level equal to the 75th percentile of the f opt distribution (ftol =7.6), obtained by computing the f-values associated with each yopt.
Gene clusters for which the maximum f-score is below this tolerance level (i.e.
in case f opt < ftol) were not included in the CAL network search, to result in a set of 1525 high-potential gene clusters.
3.2 Algorithm performance From the methods section it follows that, under the condition that an appropriate value for is found, our algorithm produces an optimal solution.
We empirically validate this claim by comparing solutions of the proposed algorithm with the global optimum obtained with an exhaustive search.
To ensure realistic conditions, we do this using the real data described above.
For each gene expression vector, we performed our CAL network search as described with seven network topologies containing and, or and xor logic as well as a more complex combination of these Boolean functions.
A rather low tolerance level (ftol =4) was used, which turned out to capture most of the solution-space (>80% for all topologies).
The solutions obtained were compared with the optimal solutions determined by means of an exhaustive search for the same seven Boolean logic functions using Grid computing i153 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i154 i149i157 J.de Ridder et al.
A B C D Fig.4.
Algorithm performance in terms of accuracy and runtime under various conditions.
(A) Bargraph displaying accuracy for different network topologies and different values of the f-score.
For each of the network topologies the 75th percentile of the solution distribution is also given, showing that for solutions in the tail 100% accuracy is obtained.
For the two missing bars in the 4-6 and 5-6 bins no solutions were found.
(B) and C) Runtimes for different network topologies and dataset sizes.
The horizontal lines reflect runtimes for exhaustive search.
From bottom to top these represent the runtimes for: a single input network, two input network and three input network with one, two and four times the number of predictors, respectively.
facilities.
The accuracy is expressed as the percentage of times that the algorithm finds the same solution as the exhaustive search.
Figure 4A shows the resulting accuracy.
We observe that for solutions with f-scores between 5 and 6 already >95% accuracy is achieved, while for solutions with f-scores of 6, virtually 100% accuracy is achieved for each topology.
For comparison, Figure 4A also gives the 75th percentiles of the solution distributions for each topology.
Because solutions of interest (putatively significant solutions) are required to have f-scores substantially higher than the 75th percentile, we can conclude that our method achieves 100% accuracy for a reasonable operating range (solutions with f-scores between 4 and 5where the accuracy is below 95%are well the 75th percentile for all networks).
While comparing our method to the method presented in Mukherjee et al.
(2009), using simulated gene expression vectors and a predetermined random network (ground truth), we found that our method reaches higher true positive rates (see Supplementary Material).
These results illustrate the benefit of searching for solutions for each of the network topologies separately, and employing a significance estimate to enforce parsimony.
Obtaining the same accuracy as an exhaustive search is only useful if this is achieved for runtimes that are substantially lower.
To asses this, we randomly selected 200 gene expression vectors from the 1525 gene clusters and measured runtimes for both our CAL network search as well as the exhaustive search.
Figure 4BD shows these runtimes for a range of conditions.
The boxplots represent the results obtained with the CAL network search and the horizontal lines the runtimes for the exhaustive search.
Figure 4B compares runtimes for different network topologies.
Clearly, the branch and bound algorithm significantly outperforms the exhaustive search under all experimental conditions with differences in runtime of up to four orders of magnitude.
For the three input networks in particular, the runtime required for exhaustive search (>5 h per gene per network) prohibits any further permutation procedures.
The CAL network search, on the other hand, is able to find the solution in a matter of seconds, thereby enabling the large number of permutations required to obtain reliable significance estimates.
Compared to the variance in runtime of the exhaustive search, which was negligible, the variance of the CAL network search is quite high.
This is expected as our CAL network search finishes rapidly when a good solution presents itself early in the search, while more time is needed to conclude that no acceptable solution is present.
For a similar reason, the more complex networks, those containing xor logic, have higher median runtimes.
On no occasion, however, does this increase runtimes >100 s for any of the networks.
To evaluate performance as a function for dataset size we artificially increased the number of predictors and the number of samples (Fig.4C).
In addition, runtimes for different tolerance levels were examined (Fig.4D).
The number of predictors was increased by horizontally concatenating the original matrix L with copies of L containing 10% random bit-flips.
The sample size was increased by vertically concatenating matrix L as well as all gene expression vectors g with copies of L and g, respectively.
In case of the latter, normally distributed noise was added to the copies with noise =0.1g.
We observe that for both the exhaustive search as well as the CAL network search runtimes increase substantially as the number of predictors increase.
In case of the CAL network search, this is explained by the fact that many very good solutions are present due to the increased imbalance between the number of predictors and the sample size.
It is expected, yet not quantitatively established, that better performance is observed when this balance is restored.
The increase in runtime as a result of an increased number of samples is moderate, with a median runtime considerably lower than an exhaustive search for only two input networks.
Likewise, increasing the tolerance level only moderately speeds up the CAL network search, demonstrating that runtime is robust for the setting of this parameter.
3.3 Combinatorial eQTLs We performed the CAL network search for the set of 1525 high-potential gene clusters.
The complete search (e.g.
for all gene clusters and all topologies) was repeated 100 times using a permuted version of the gene-expression vectors.
For each topology, this resulted in a null-distribution containing 152 500 values, which was used to estimate q-values for each of the resulting solutions.
We considered network topologies with a maximum of three inputs listed in Supplementary Figure S5.
Notably, we included two single-input networks to account for direct positive and negative association, respectively, which is equivalent to positive association with the i154 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i155 i149i157 Combinatorial association logic networks A B C Fig.5.
(A) Bargraph with an overview of the number of gene clusters for which a significant (10% FDR) solution is found.
Network topologies are sorted according to the 10% FDR level (blue line).
(B) CAL networks significant at 10% FDR.
The color and shape of the symbols correspond to the symbols used in (C).
Small circles at the inputs of the networks denote negation, i.e.
for these inputs the mapping from allele to binary representation is switched.
We also indicate whether the best single marker coincides, for that gene cluster, with one of the inputs of the CAL network.
(C) Marker/probe-plot for the top CAL networks showing both the eQTLs (blue crosses) and ceQTLs (sets of colored symbols of various shapes).
The colors and shapes of the markers refer to the network topologies listed in (B).
Horizontal gray lines connect the inputs and the output of the CAL network.
Because probes were clustered, it occurs that the ceQTLs map to multiple probes in case these probes were part of the same cluster.
The numeric labels near the the colored symbols correspond to the input of the network.
Notably, some probes seem to be predicted by more ceQTLs than there are inputs to the CAL network reported.
This occurs when there are multiple combinations of markers that show the same association with the gene expression level of the network output, and can be explained by similarity among markers.
The cis-band (diagonal) is clearly visible, and in one occasion contains a ceQTL.
Overlap among ceQTLs from different networks is marked by red dashed lines, overlap between ceQTLs and eQTLs by black dashed lines.
D2 and B6 allele, respectively.
This ensures that the algorithm has the option of choosing the least complex model in case an eQTL is capable of explaining a significant portion of the variance in the expression of the gene cluster.
Figure 5A gives an overview of the number of gene clusters for which the output of a CAL network significantly (at the 10% FDR level) associated with its expression (red bars).
To obtain additional confidence in the significance threshold, we calculated q-values for 10 additional permutations of the whole dataset.
For none of the network topologies did the mean number of significant gene clusters across the 10 permutations exceed 0.6, indicating that the expected number of false discoveries is conservatively kept under control.
The yellow bars indicate the number of significant gene clusters after model size selection based on the q-value as detailed in Section 2.
It appears that most of the gene clusters for which association is observed can be explained by one of the single input networks.
For nine gene clusters (corresponding to 17 genes), however, a CAL network was capable of explaining significantly more of the variance than one of the single input networks or any one of the other CAL networks.
The network topologies, q-values and association scores of the significant CAL networks are given in Figure 5B.
Not surprisingly, for all gene clusters at the output of these networks, the combination of loci is vastly superior in explaining the variance in expression over any of the markers in isolation.
Interestingly, many of these genomic regions would have been missed, as in seven of the networks the best markers do not coincide with one of the inputs of the CAL network.
The sets of markers that were found as the optimal inputs for the seven topologies were mapped onto the genome.
Combinatorial eQTLs (ceQTL) were then defined as stretches of consecutive markers.
A genome map of the (c)eQTLs is given in Figure 5C, showing the eQTLs (red and blue crosses for positive and negative association, respectively) and ceQTLs (colored symbols) on the x-axis versus the genomic positions of the probes measuring expression on the y-axis.
The numbers near the ceQTL symbols correspond to the inputs of the CAL networks depicted in Figure 5B.
Before we zoom in on one of the CAL networks in more detail, some general observations can be made.
In particular, we note that in some cases overlap exists among the markers selected at the i155 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i156 i149i157 J.de Ridder et al.
Fig.6.
Input regions of the CAL network for Lilrb4 The line graphs give the f-score for association between the output gene and the individual markers (blue) and the network output (red).
The latter was computed by taking the maximum f-score of the network using the marker under evaluation for one input and any of the other markers for the second input of the network.
Where possible the IDs of the genetic markers are given, but some were omitted for readability.
The dot plots gives the expression values separated by network output (right) and the best markers in the inputs (left).
Finally, for one particular combination of markers the genotype for all strains is depicted as a Boolean heat map.
In these diagrams, the not gates were already incorporated.
inputs of the CAL networks and between other network inputs and eQTLs.
In seven instances, the identified ceQTLs coincide with eQTLs (connected by black dashed lines in the figure).
Some of these eQTLs are located in cis.
The finding of CAL networks that share one of their inputs (ceQTLs) with an eQTL suggests that the local genotype associated with the eQTL is involved in the regulation of a local gene (cis-regulation), but in addition collaborates with the other CAL input locus/loci to regulate the CAL network output gene(s).
Furthermore, two of the CAL networks (ranked sixth and ninth) share a ceQTL between the inputs (connected by red dashed lines).
It is not inconceivable that a gene present in this ceQTL is indeed involved in the regulation of the target genes of both networks, but that the interaction partners through which this regulation is established differs for both target genes.
Among the list of output genes of the nine most significant CAL networks is Lilrb4 (ranked third).
Lilrb4 encodes a leukocyte immunoglobulin-like receptor which is expressed on the surface of mast cells, neutrophils, and macrophages.
It plays a key role in counter-regulating the inflammatory response to prevent pathologic excessive inflammation (reviewed in Katz, 2007).
Figure 6 shows small regions around the ceQTLs that were selected as inputs for the CAL network of Lilrb4.
For each region, the association was measured between the expression of Lilrb4 and the individual markers (blue lines).
The red lines, on the other hand, give the association score for the network output.
Clearly, the association between the logical combination of inputs and the expression of Lilrb4 is markedly higher than considering any of the markers in isolation.
The regions for which the red curves reach their maximum correspond to the ceQTLs.
The Boolean heat map, displayed at the bottom of Figure 6, outlines the genotype of one particular combination of genetic markers in the ceQTLs across the BXD mouse strains.
The bottom two rows of this heat map give the optimal network output and predicted output, respectively.
For the Lilrb4 network the optimal network output is exactly recapitulated by the CAL network.
For Lilrb4 elevated expression is exclusively observed in case of B6 alleles in both the ceQTL regions of Chromosomes 7 and 19.
To focus our attention to the most interesting genes in the ceQTLs we performed a literature search using Ingenuity pathway analysis (IngenuitySystems, www.ingenuity.com).
Interestingly, we found a substantial number of interactions between genes localized in the ceQTLs and Lilrb4.
For example, the literature search revealed a link between Apba1 (located in the ceQTL region on Chromosome 19) and Lilrb4.
Both protein products have been described to bind ITGB3 (Calderwood et al., 2003; Castells et al., 2001).
In addition, the search revealed a link between Psenen (Chromosome 7 ceQTL) and Apba1 (Chromosome 19 ceQTL).
Both protein products have been described to bind PSEN1 and PSEN2 (Biederer et al., 2002; Steiner et al., 2002).
While literature is able to link the genes in the ceQTLs to Lilrb4 and thereby gives the first clues as to how the expression of Lilrb4 may be regulated, we do not exclude that other interactions (not yet represented in literature) exist.
In any case, the result of our method should provide a set of testable hypotheses that can be validated in the laboratory.
4 DISCUSSION Unravelling (transcriptional) regulatory networks by inferring complex associations, for instance, between genotype and gene expression, necessitates algorithms that take into account possible (allele-specific) interactions.
For this purpose, we have proposed a method to efficiently infer CAL networks, i.e.
small logic networks in which allele-specific interactions are modeled by Boolean functions.
To find the best possible fit of the model given the data, a computationally challenging optimization problem had to be solved.
i156 [11:18 12/5/2010 Bioinformatics-btq211.tex] Page: i157 i149i157 Combinatorial association logic networks This was achieved by rewriting the optimization such that it could be effectively solved by a customized branch and bound algorithm.
Proof and empirical evidence for optimality of the solution, under appropriate conditions, was given.
At the same time, differences in runtimes of up to four orders of magnitude were observed when compared to exhaustive search.
Because the CAL network search is able to find the optimal solution in a matter of seconds a permutation procedure becomes feasible, which can be employed to obtain estimates of the FDR.
This is a major advantage as the resulting q-values allow selection of the most parsimonious model and enable ranking the network topologies in terms of their complexity.
We demonstrated our algorithm on a genetical genomics dataset, and found that, from the 1525 gene clusters (2913 genes) that resulted after selection of high potential genes, 9 gene clusters (17 genes) were significantly associated (at 10% FDR level) through a logical combination of genomic loci rather than a single eQTL.
Notably, without incorporating the complex interactions, these associations would have gone unnoticed.
Many of the discovered input regions were found to overlap eQTLs or were shared inputs of CAL networks explaining the expression of other genes, suggesting that these regions, indeed, are involved in transcriptional regulation.
ACKNOWLEDGMENTS The authors thank Daoud Sie and Leonid V. Bystrykh for their valuable input.
Funding: This work was part of the BioRange programme of the Netherlands Bioinformatics Centre (NBIC), which is supported by a BSIK grant through the Netherlands Genomics Initiative (NGI); Dutch Life Science Grid initiative of NBIC and the Dutch e-Science Grid BiG Grid, SARA-High Performance Computing and Visualisation; Netherlands Genomics Initiative (Horizon, 050-71-055); Dutch Cancer Society (RUG2007-3729); Netherlands Organization for Scientific Research (NWO) (VICI to G.d.H., 918-76-601); European Community (EuroSystem, 200720).
Conflict of Interest: none declared.
ABSTRACT Motivation: The automatic analysis of scientific literature can support authors in writing their manuscripts.
Implementation: PaperMaker is a novel IT solution that receives a scientific manuscript via a Web interface, automatically analyses the publication, evaluates consistency parameters and interactively delivers feedback to the author.
It analyses the proper use of acronyms and their definitions, and the use of specialized terminology.
It provides Gene Ontology (GO) and Medline Subject Headings (MeSH) categorization of text passages, the retrieval of relevant publications from public scientific literature repositories, and the identification of missing or unused references.
Result: The author receives a summary of findings, the manuscript in its corrected form and a digital abstract containing the GO and MeSH annotations in the NLM/PubMed format.
Availability: http://www.ebi.ac.uk/Rebholz-srv/PaperMaker Contact: rebholz@ebi.ac.uk Received on October 5, 2009; revised on January 25, 2010; accepted on February 9, 2010 1 INTRODUCTION The primary purpose of scientific publications is to report on new scientific findings and to embed them in prior research work.
The author receives best acceptance if a large audience accurately perceives what he had in mind, and manuscripts of good quality have a higher likelihood to pass the review process.
Scientists write their manuscripts in loosely structured natural language but have to comply with standards concerning the document format, the use of language and the citation of prior work.
The availability of electronic data resources such as ontologies, reference databases and electronic literature in the biomedical scientific community exposes the scientists to new requirements: the use of domain-specific terminology has to follow standards from scientific databases and the author has to support submission of data to the reference database as part of the manuscript submission process (Leitner and Valencia, 2008).
Furthermore, the author has to avoid duplication of the existing work.
Reliable automatic feedback on any of these parameters will improve the speed and efficiency of the manuscript preparation phase and the review phase for authors, publisher and reviewers.
Existing solutions for the analysis of the scientific literature focus on improving the search of relevant To whom correspondence should be addressed.
information (Kim and Rebholz-Schuhmann, 2008) but cannot be applied during the manuscript preparation phase.
2 METHODS 2.1 Text analysis (Whatizit) and evaluation PaperMaker uses the modular infrastructure of Whatizit (Kirsch et al., 2006; Rebholz-Schuhmann et al., 2008).
Modules are described in the order as they are applied (Fig.1).
Candidates for acronyms are all tokens with 211 characters in length having an initial uppercase letter.
Tokens at the beginning of a sentence were ignored as well as stop words including highly frequent English function words (e.g.
In, If).
The co-occurrence of the acronym with the long form was identified with syntactic patterns (Schwartz and Hearst, 2003).
Terminologies for the named entity recognition and normalization comply with public resources [e.g.
UniProtKb/Swiss-Prot, Gene Ontology (GO), DrugBank, Medline Subject Headings (MeSH), see below].
Complete Medline has been analyzed to generate a terminological resource of all known biomedical terms.
The annotation of the gene and protein named entities is based on the BioLexicon (prec/rec: 94/63; Rebholz-Schuhmann et al., 2009).
ChEBI was the reference data resource for the identification of chemical entities.
For the annotation of medical entities we used the terms for diseases and syndromes from UMLS (Jimeno et al., 2008) in combination with drug names (DrugBank).
The annotation of text passages with GO concepts uses the solution by (Gaudan et al., 2008): F-measure close to 40% for the terms at Rank 1.
The categorization of manuscripts with MeSH terms relies on k-nearest neighbor clustering to attribute text passages to MeSH categories similar to the ones delivered with Medline abstracts (Trieschnigg et al., 2009).
The references in the document were identified based on syntactic patterns and compared with the list of citations in the reference section.
The syntactical patterns are defined according to the citation standard in Nucleic Acids Research (NAR).
Missing publications in the reference section as well as unused citations are recognized by the automatic analysis.
The identification of related work is achieved by translating a given passage into a Lucene query, which is then run against the Medline and Pubmed Central repositories.
PaperMakers analysis was evaluated on 50 randomly selected publications from NAR.
Fig.1.
The diagram gives an overview of the different processing steps for the manuscript.
The results from each step are presented to the author.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:15 5/3/2010 Bioinformatics-btq060.tex] Page: 983 982984 PaperMaker: validation of biomedical scientific publications 2.2 Processing of proprietary document formats The Web application PaperMaker uses JODConverter to convert files into the OpenDocument Format (ODF), an XML format.
It integrates OpenOffice.org/Writer functionality that enables import and export of OpenDocument and Microsoft documents (DOC and RTF).
Formatting details in the document are preserved through special XML tags in the document structure.
Once the text has been analyzed and prepared to be returned to the author, PaperMaker converts it back to the original XML format, which can be used to generate the original and alternative file formats (JODConverter, OpenOffice.org).
3 RESULTS 3.1 Use of PaperMaker The user interface enables the upload of manuscripts in different file formats (see Section 2.2).
None of the data will be stored on site.
The interface provides an overview on the availability of required services (top right corner) and a button for help.
The uploaded manuscript will be processed step by step by the modules M1M8 (Fig.1).
Each module is embedded in its own Web page with its own Help page and the GoTo-butting indicates the index of the next module.
The last page summarizes the results of the analysis.
3.2 Identification of unknown terms In the first step (module M1), we determine the number of unknown, and thus potentially non-standard or misspelled terms in the scientific publications.
A potentially unknown term is one which is neither mentioned in the British National Corpus (BNC, general English) nor in any Medline abstract.
Both resources combined contain 3 015 437 distinct single-word token terms in total: 89% from Medline only, 5% from both resources and 6% from BNC only.
All 50 documents contained unknown terms: 29.5 terms per document and 1474 in total.
Most unknown terms turned out to be identifiers (64%, 935 terms): nomenclature identifier, public database identifier and other identifier, and author names.
The remaining 36% of unknown terms (539) are either novel hyphenations of known terms (57%, in 47 publications) or truly novel terms.
Removing the hyphenation leads either to composite terms or to single words (e.g.
fig leaf, pigeonhole) that conform to dictionary resources (Shorter Oxford English Dictionary).
Amongst the truly novel terms, we identified algorithms, methods, brand names and gene names.
A few terms are neologisms derived from other English words (e.g.
organismally).
All remaining 22 novel terms (4%) have been misspelled variants of known terms.
3.3 Categorization of known terms 3.3.1 Acronyms resolution (M2) PaperMaker identified 1445 occurrences of a known acronym without a definition in all 50 documents (Gaudan et al., 2005).
Of the occurrences, 49.6% (717) were correct.
False positive results included short sequences of nucleotides, general English words and Roman numerals (e.g.
AC, AS and II).
Three hundred and sixteen acronym mentions are novel and would require a definition: 11% (45) of the mentions represent gene/protein named entities and were resolved at a later stage through the biological entity tagger.
In contrast, the majority of acronyms were correctly defined in the manuscript.
However, a small subset uses a less frequent definition.
The acronym resolution filter also identified acronyms that have been defined at least twice (14) and that have been used before its proper definition.
3.3.2 Identification of named entities (M3) The average document contained 3877 words (reference section not included), 1242 unique words, 29 unique acronyms, 6 unique gene/protein mentions, 18.5 chemical terms and 7 medical terms.
Medical terms were identified at 78.7% precision, protein/gene names at 71.2% and chemical entities at 61.7% according to our manual analysis.
The low precision of the last module results from the fact that this set is very heterogeneous (DNA is a chemical entity as well as glutamate).
Preferred names proposed by the reference data resources were used at low frequency: 19.3% for medical terms, 33% for protein/gene names and 46.3% for chemical entities.
3.3.3 GO and MeSH recognition (M4, M5) PaperMaker assigns first GO terms and then MeSH taking the whole publication into consideration.
Forty percent (391 of 973) of the MeSH concepts matched the manually attributed MeSH terms and 20% of manually assigned MeSH headings were not identified.
3.3.4 Reference check module (M6) A total of 4738 publications from PubMed Central were analyzed.
Five hundred and seventy-one publications had inconsistencies in the reference section.
A random selection of 54 publications was manually inspected to find 12 (22%) true positive results where the authors were mentioned in the bibliographic section but not referenced in the main text.
False positive results were due to authors being mentioned in tables or pictures but not in the main text.
3.3.5 Related work and summary (M7, M8) In the last two steps, the author receives an overview on related work and the summary of all findings.
4 CONCLUSIONS The integration of scientific publications into the biomedical data resources is ongoing work (Rebholz-Schuhmann et al., 2005).
PaperMaker supports authors in this integration work without putting efforts to curation teams (Leitner and Valencia, 2008).
The final result is the production of structured abstracts in the NLM/PubMed format.
Furthermore, the manuscript generation process is well embedded into the literature search and the referencing of relevant prior research.
ACKNOWLEDGEMENTS The scientific full-text documents were made available by the journal Nucleic Acids Research, published by Oxford Journals, Oxford University Press.
Funding: EC Strep project BOOTStrep (FP6-028099).
Conflict of Interest: none declared.
Abstract Proteinpeptide interactions, where one partner is a globular protein (domain) and the other is a flexible linear peptide, are key components of cellular processes predominantly in signal-ing and regulatory networks, hence are prime targets for drug design.
To derive the details of the proteinpeptide interaction mechanism is often a cumbersome task, though it can be made easier with the availability of specific databases and tools.
The Peptide Binding Protein Database (Pep-Bind) is a curated and searchable repository of the structures, sequences and experimental observa-tions of 3100 proteinpeptide complexes.
The web interface contains a computational tool, protein inter-chain interaction (PICI), for computing several types of weak or strong interactions at the pro-teinpeptide interaction interface and visualizing the identified interactions between residues in Jmol viewer.
This initial database release focuses on providing proteinpeptide interface informa-tion along with structure and sequence information for proteinpeptide complexes deposited in the Protein Data Bank (PDB).
Structures in PepBind are classified based on their cellular activity.
More than 40% of the structures in the database are found to be involved in different regulatory pathways and nearly 20% in the immune system.
These data indicate the importance of protein peptide complexes in the regulation of cellular processes.
PepBind is freely accessible at http://pepbind.bicpu.edu.in/.
hur PP).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A Introduction Functional analyses of proteins involve the exploration of their interactions with other molecules, which plays vital roles in dif-ferent pathways.
Nearly 60% of the interaction pathways such as signal transduction, apoptotic, immune system and other pathways contain domains with bound peptides [1].
These inter-actions are prevalent in Src homology 2 (SH2) domain, major histocompatibility complex (MHC), antibodies, proteases, cal-modulin, PapD chaperone and OppA (oligopeptide permease cademy of Sciences and Genetics Society of China.
Production and hosting 242 Genomics Proteomics Bioinformatics 11 (2013) 241246 A) structures, with variable sequence specificity and binding affinity [2].
Proteinpeptide interactions require only a small interface and can occur in many interaction networks.
Hence, these are attractive drug targets both for small molecules and inhibitory peptides [35].
This implies that synthetic peptides can be designed to alter specific interactions in disease or other pathways [1,6,7].
Out of the structures deposited in the Protein Data Bank (PDB) [8], every month around 20 new entries are shown to exhibit interactions with small peptides.
As the num-ber of new and interesting proteinpeptide complex structures continue to expand, our understanding of these proteinpeptide recognition events should improve.
To understand and analyze the proteinpeptide interactionmechanisms, a reliable database of proteinpeptide complexes is necessary.
A number of se-quence-based proteinpeptide interaction databases are avail-able, such as ELM [9], PhosphoELM [10], DOMINO [11], SCANSITE [12], PepBank [13], APD [14], ASPD [15] and BIO-PEP [16].
Structural data are also available on proteinpeptide complex structures in peptiDB [17] and PepX [18].
While pep-tiDB is a set of 103 curated PDB files for non-redundant proteinpeptide complexes, PepX contains 1431 non-redundant X-ray structures clustered based on their binding interfaces and backbone variations.
Previous studies report heterogeneity of domains or proteins to bind multiple peptides (e.g., at least 13 different types of peptides have been reported to bind to SH3do-mains [19]).
For detailed analysis of interactions of similar pro-teins with different peptides, an enormous amount of data concerning proteinpeptide complex structures are needed.
To address this problem, we have created the Peptide Binding Pro-tein Database (PepBind), which contains 3100 available protein Figure 1 Interactions between the complex of Apopain with the tetrap Hydrogen bonds (A), hydrophobic interaction (B) and ionic interact colored in brown.
structures from the PDB, irrespective of the structure determi-nation methods and similarity in their protein backbone.
Different kinds of interactions have been noted in the stabil-ization of proteinpeptide binding.
Analyses of various interact-ing interfaces between linear peptide and protein domains help us in distinguishing transient and permanent complexes [20 22].
It has been demonstrated that protein-peptide interfaces contain more hydrogen bonds per 100 A2 solvent accessible sur-face area (ASA) (i.e., 50% more than proteinprotein interac-tions and 100% more than intrinsically-unstructured regions to protein interactions) [17].
The importance of other interac-tions such as interactions between nonpolar hydrophobic amino acid residues and ionic interactions in the structure and function of proteins is also well known [23,24].
Knowing the importance of proteinpeptide interface hydrogen bonds and other kinds of interactions, we developed and integrated a web-based interac-tion tool, protein inter-chain interaction (PICI), which calcu-lates all the interface hydrogen bonds along with other interactions (such as disulfide bonds, hydrophobic interactions and ionic interactions) in tertiary structures of proteinpeptide complexes and can be visualized with an integrated Jmol [25] viewer.
Although a similar tool, Protein Interaction Calculator (PIC) [26], has been available, this tool calculates interface inter-actions specific for the peptide chain of a proteinpeptide com-plex structure and visualizes them in a single web page along with highlighted interacting residues on sequences.We have also developed a binding prediction server built in PepBind (http://pepbind.bicpu.edu.in/PepBind_prediction_beta.php) to predict the possible protein domains in the PepBind database that may bind the user-defined peptide sequence.
eptide inhibitor ACE-DVA-ASK (PDB ID: 1CP3) ions (C) were identified by PICI server.
Interacting residues are Das AA et al/ PepBind: Proteinpeptide Database with PICI Tool 243 Results The PepBind database provides researchers with residue and atomic-level information about sequences and structures of proteinpeptide complexes and their interfaces, helping in the analysis of proteinpeptide interactions by computing various interface interactions and by providing structural information both interactively on screen and in a text format (Figure 1).
The PepBind database also maintains a repository of structure coordinate files, PDBML [27] data files and proteinpeptide interaction files generated by PICI tool.
The database is up-dated on a regular basis to serve as a resource for structural, functional and proteinpeptide interaction studies of peptide-binding proteins.
Researchers can also submit proteinpeptide complexes to the database, which will be uploaded to PepBind after manual verification.
Database statistics As shown in Table 1, current version of PepBind contains structural information for a total of 3100 proteinpeptide com-plexes.
Based on cellular activity, 1745 complexes of all the 3100 proteins (56.3%) are involved in regulatory pathways, along with inhibitory complexes.
Our study shows 1278 struc-tures (41.2%) in the database play major roles in hormonal activity, gene regulation, transcription and signal transduction pathways along with transferases.
Furthermore, 600 structures (19.3%) in the database are found to function in the immune system.
It has been found that 252 proteins (8.1%) are struc-tural, contractile and membrane proteins involved mainly in transport (5.2%) and cell adhesion (1.9%).
In addition, 953 (30.7%) structures have protease or other hydrolase activities, Table 1 Contents of the PepBind database Cellular activity No.
of complexes (%) Cell cycle 90 (2.9) Structural proteins 126 (4.0) Cell adhesion 59 (1.9) Transporta 163 (5.2) Calmodulin (CaM) 42 (1.3) Apoptosis 125 (4.0) Signaling 626 (20.2) Hormones 84 (2.7) Transferasesb 415 (12.7) Transcription 268 (8.6) Gene regulation 38 (1.2) Inhibitory complex 663 (21.4) MHC 340 (10.9) Immunoglobulin (Ig) 250 (8.0) Antibiotics 15 (0.5) Other immune system proteins 98 (3.1) Proteases 687 (22.1) Other hydrolases 266 (8.5) Others 326 (10.5) Note: There are totally 3100 proteinpeptide complexes in PepBind.
Since categories.
a Transporters, channels and pumps; b Transferases along with while 10.5% structures in the database are associated with pro-teins involved in other cellular activities.
Web interface The user interface has been developed for browsing through all the contents of the database as a list or by different categories (Figure 2).
For the ease of users to search and access data, we have integrated many search tools (Figure 2A) into the web interface.
Using the simple search function, users can retrieve information about proteinpeptide complexes using their PDB ID or protein name.
Our keyword search tool scans all the fields of all the tables in PepBind for the matched word and re-turns a list of all protein structures related to the query.
Using the advanced search function, users can filter search based on peptide length, cellular activity of proteins, structure determi-nation methods (e.g., X-ray diffraction, nuclear magnetic res-onance and electron microscopy) and authors contributing to solving protein structure.
All these search options with their parameters are joined by AND operator for an intensive search.
Additionally, to find any protein sequences homolo-gous to the sequence submitted, we provide BLAST searching [28] against PepBind/PDB/SwissProt.
The web interface for the output result has been designed to show all the chains present in the protein structure (Figure 2B).
Each chain is linked to the PICI web tool for analyzing its interactions with other chains of the protein.
This tool shows the interaction details by highlighting the corresponding inter-acting residues in the displayed sequence along with the Jmol visualization tool for the identified interactions between the residues (Figure 2C).
Different tab viewers have been designed for various types of interactions.
The protein detail page shows information about protein complex on a single web page under Functional category No.
of complexes (%) Structural, contractile and membrane proteins 252 (8.1) Regulatory proteins 1278 (41.2) Inhibitory complexes 663 (21.4) Immune system 600 (19.3) Proteases and other hydrolases 953 (30.7) Others 326 (10.5) some proteins are multi-functional, there are overlaps among different kinase, phosphomutase, transaldolase and transketolase.
Figure 2 Snapshots of PepBind output A.
Search page with search parameters.
B.
Result summary page showing all the chains with their sequence.
C. Jmol showing protein peptide interface and sequence viewer showing protein chains with identified residues highlighted.
D. Detailed result page displaying summary of the protein and other tab options.
244 Genomics Proteomics Bioinformatics 11 (2013) 241246 different tabs (Figure 2D), such as summary, sequence and source, gene ontology, methodology, Ramachandran plot, citation and external links.
While the sequence and source tab displays amino acid sequence in different colors as per their biochemical properties along with source organism data, the Ramachandran plot tab shows the Ramachandran plot im-age developed by the MolProbity [29] server, and the Gene Ontology tab shows GO functional annotation [30].
For a structure similarity search, we take advantage of the web ser-vice of PDB, which employs the FATCAT algorithm [31] to recognize homologous domains available at PepBind, SCOP [32] and PDP [33].
Discussion Proteinpeptide interactions are the key components of cellu-lar processes such as signal transduction, protein trafficking, defense mechanisms and enzyme regulation.
Various databases are available on protein interactions.
They can be grouped as protein-small molecule, protein-nucleic acid and protein protein interaction databases.
However, the retrieval of struc-tural and functional information of proteinpeptide interac-tions in biological processes is tedious due to the lack of specific databases to provide such details.
The establishment of the PepX database has resolved the difficulty of unavailabil-ity of a proteinpeptide interaction database, whereby authors have classified the proteins based on backbone variations and binding interfaces.
While in PepX, grouping is based solely on 3D similarity, PepBind complements PepX by providing inter-face information for both the peptide and protein chains of the complexes along with their cellular functions and options for sequence and structure similarity searches.
PepBind is inte-grated with the Jmol viewer to visualize the interface residues along with the interaction files generated by the PICI tool.
Furthermore, PepBind provides BLAST search and structure similarity search for protein chains.
It also provides a predic-tion service for binding of user-given peptides to possible pro-tein domains present in the PepBind database.
Links to other related databases and servers for the queried protein are provided for further analysis of the structures.
These resources include PDB [8], PDBsum [34], Pfam [35], Das AA et al/ PepBind: Proteinpeptide Database with PICI Tool 245 CASTp [36], OCA Browser (http://bip.weizmann.ac.il/oca/), PSI/KB (http://sbkb.org/kb/), SRS [37], MMDB [38], PQS [39], SCOP [32], CATH [40], Proteopedia [41], Jena Library [42] and UniProt [43].
Currently our interaction tool PICI is capable of analyzing inter-chain interactions like hydrogen bond, disulfide bridge, hydrophobic interaction and ionic interaction.
Keeping in view the importance of other weak interactions in stabilizing the protein structure, we plan to improve our tool to study inter-actions such as aromatic-aromatic interactions [44], cation-pi interactions [45] and aromatic-sulfur interactions [46].
In addi-tion, the current interaction tool capabilities will be extended to user-submitted structures, allowing for examination of interfaces in complexes currently not present in the PepBind.
Methods Data collection and curation Files for atomic coordinate (pdb files version 3.30), sequences (fasta files) and other data (pdbml files version 4.0) of 3100 proteinpeptide complexes in the PDB were downloaded fol-lowing a thorough manual screening of all the available struc-tures in the PDB.
Because PepBind intends to be a comprehensive collection of proteinpeptide complexes from the PDB, the database contains all the available proteinpep-tide complexes, irrespective of their sequence or structure redundancy.
Classification of all the collected structure data was done in three steps: (I) an automated program to scan the amino acid sequences and classify them based on length of the bound peptide, (II) manual curation for the cellular activity of the complexes through study of the literature and (III) an automated program to read the data file and group the complexes as per their structure determination methods.
Functionality has been analyzed through literature studies and classified as proteins involved in different cellular activities and grouped in 19 categories.
Database schema and implementation The PepBind database consists of a series of server-side scripts written in the PHP programming language with HTML and JavaScript for user interface functions, which runs on the Apache 2.2 web server, using MySQL 5.1 as a database back-end.
Atomic coordinate information from the PDB and other related information from other remote databases and web servers were mined through an automated program and stored in a file repository for further processing.
We developed sets of PHP scripts for operating with the available data and process them for easy integration in the database and front-end user interface.
The first set of scripts reads the PDBML files [27], extracts the data, and inserts them into the database tables; the second set sorts these data with respect to each attri-bute and the third set generates web pages with specific infor-mation about individual complexes.
Utilities and tools The PICI tool for depicting potential hydrogen bonds and other interactions between the short peptide and core protein was developed and integrated into PepBind.
This tool parses the structure coordinate files, removes the hetero atoms and water molecules, and predicts the interaction based on coordi-nate distance between atoms of amino acid residues of small peptide and the protein.
For structures determined by NMR, the first model in the file is taken for calculation by PICI tool.
For the two atoms A(x1, y1, z1) and B(x2, y2, z2), linear distance D is calculated as per the Euclidean distance equation D(A, B) = p {(x1 x2)2 + (y1 y2)2 + (z1 z2)2}.
Various potential interactions are calculated based on stan-dard and published criteria.
The hydrogen bond is detected if the distance between oxygen or nitrogen atoms of the peptide and the protein domain is 63.5 A [47].
Interactions between hydrophobic residues (such as alanine, valine, leucine, isoleu-cine, methionine, phenylalanine, tryptophan, proline and tyro-sine) [48] have been predicted if they fall within 5 A range.
Apart from these interactions, ionic residue (arginine, lysine, histidine, aspartic acid and glutamic acid) pairs falling within 6 A contribute to ionic interactions.
The tool with integrated Jmol viewer shows various interactions between the peptide and the amino acid residues of the interacting protein chains.
Moreover, it highlights the positions of interacting amino acid residues on the displayed sequence (Figure 2D).
This tool also generates an interaction file for each type of interactions.
A sequence modification tool has been developed and incorporated into the result page, which can read the protein sequence file and color the amino acid sequence (using single letter code) of protein according to their biochemical proper-ties (such as green for non-polar hydrophobic amino acids, yel-low for uncharged polar amino acids, blue for positively charged amino acids, red for negatively charged amino acids and black for non standard amino acids).
A web-based predic-tion server has been provided to find the protein domains pres-ent in the database that likely bind to the user-given peptide.
The sequence search tool present in the web interface allows users to BLAST search the queried sequence in the database using various parameters.
All data related to structure, sequence and interface interac-tions currently in the PepBind database have been made avail-able for further analysis.
These files along with the complete list of the PepBind dataset can be downloaded freely from our database.
A reporting tool has been integrated to generate the result in a printer-friendly PDF file.
Authors contributions PPM, RK and MSK conceived and designed the project.
AAD collected the data, developed the database, developed the tools and designed the website.
OPS developed the BLAST search script.
AAD and OPS wrote the manuscript.
All authors read and approved the final manuscript.
Competing interests The authors have no competing interests to declare.
Acknowledgements This work is supported by the Department of Biotechnology (Grant No.
BT/BI/03/015/2002) and Department of 246 Genomics Proteomics Bioinformatics 11 (2013) 241246 Information Technology (Grant No.
DIT/R&D/15 (9)2007), Government of India.
ABSTRACT Motivation: RNA-Seq technique has been demonstrated as a revolu-tionary means for exploring transcriptome because it provides deep coverage and base pair-level resolution.
RNA-Seq quantification is proven to be an efficient alternative to Microarray technique in gene expression study, and it is a critical component in RNA-Seq differential expression analysis.
Most existing RNA-Seq quantification tools require the alignments of fragments to either a genome or a tran-scriptome, entailing a time-consuming and intricate alignment step.
To improve the performance of RNA-Seq quantification, an align-ment-free method, Sailfish, has been recently proposed to quantify transcript abundances using all k-mers in the transcriptome, demon-strating the feasibility of designing an efficient alignment-free method for transcriptome quantification.
Even though Sailfish is substantially faster than alternative alignment-dependent methods such as Cufflinks, using all k-mers in the transcriptome quantification impedes the scalability of the method.
Results: We propose a novel RNA-Seq quantification method, RNA-Skim, which partitions the transcriptome into disjoint transcript clus-ters based on sequence similarity, and introduces the notion of sig-mers, which are a special type of k-mers uniquely associated with each cluster.
We demonstrate that the sig-mer counts within a cluster are sufficient for estimating transcript abundances with ac-curacy comparable with any state-of-the-art method.
This enables RNA-Skim to perform transcript quantification on each cluster inde-pendently, reducing a complex optimization problem into smaller op-timization tasks that can be run in parallel.
As a result, RNA-Skim uses 54% of the k-mers and510% of the CPU time required by Sailfish.
It is able to finish transcriptome quantification in510 min per sample by using just a single thread on a commodity computer, which represents 4100 speedup over the state-of-the-art alignment-based methods, while delivering comparable or higher accuracy.
Availability and implementation: The software is available at http://www.csbio.unc.edu/rs.
Contact: weiwang@cs.ucla.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION RNA-Seq technique has been demonstrated as a revolutionary means for examining transcriptome because it provides incom-parable deep coverage and base pair-level resolution (Ozsolak and Milos, 2010).
Though RNA-Seq sequencing exhibits itself as an efficient alternative to Microarray techniques in gene ex-pression study (Wang et al., 2009), it also brings unprecedented challenges, including (but not limited to) how to rapidly and effectively process the massive data produced by the proliferation of RNA-Seq high-throughput sequencing, how to build statis-tical model for accurate quantification of transcript abundances for transcriptome, etc.
Most of current RNA-Seq tools for RNA-Seq quantification contain two main steps: an alignment step and a quantification step.
Various aligners [TopHat (Trapnell et al., 2009), SpliceMap (Au et al., 2010), MapSplice (Wang et al., 2010)] are devised to infer the origin of each RNA-Seq fragment in the genome.
The alignment step is usually time-consuming, requiring substantial computational resources and demanding hours to align even one individuals RNA-Seq data.
Because there are multiple variations of RNA-Seq sequencing techniques, e.g.
single-end sequencing and paired-end sequencing, to facilitate the discussion in this article, we simply refer to the read or the pair of reads from a RNA-Seq fragment as a fragment.
More importantly, a signifi-cant percentage of the fragments cannot be aligned without am-biguity, which yields a complicated problem in the quantification step: how to assign the ambiguous fragments to compatible tran-scripts and to accurately estimate the transcript abundances.
To tackle the fragment multiple-assignment problem, an expect-ation-maximization (EM) algorithm (Pachter, 2011) is often used to probabilistically resolve the ambiguity of fragment as-signments: at each iteration, it assigns fragments to their com-patible transcripts with a probability proportional to the transcript abundances, and then updates the transcript abun-dances to be the total weights contributed from the assigned fragments, until a convergence is reached.
The EM algorithms simplicity in its formulation and implementation makes it a popular choice in several RNA-Seq quantification methods [Cufflinks (Trapnell et al., 2010), Scripture (Guttman et al., 2010), RSEM (Li and Dewey, 2011), eXpress (Roberts and Pachter, 2013)].
Because all fragments and all transcripts are quantified at the same time in the EM algorithm, it usually re-quires considerable running time.
Some studies [IsoEM (Nicolae et al., 2011), MMSEQ (Turro et al., 2011)] reduced the scale of the problem by collapsing reads if they can be aligned to the same set of transcripts.
It is also worth mentioning that RNA-Seq quantification is an important first step for differential ana-lysis on the transcript abundances among different samples (Trapnell et al., 2012).
The alignment step is a vital step in the RNA-Seq assembly study (Trapnell et al., 2010) and has become the computational bottleneck for RNA-Seq quantification tasks.
If users are only interested in RNA-Seq quantification of an annotated transcrip-tome, aligning RNA-Seq fragments to the genome seems cum-bersome: not only do the RNA-Seq aligners take a long time to align fragments to the genome by exhaustively searching all*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Since employed very ( ) Since ( ) XPath error Undefined namespace prefix possible splice junctions in the fragments, they may also generate misaligned results owing to repetitive regions in the genome or sequencing errors, introducing errors in the quantification results (Zhang et al., 2013).
From another perspective, the annotation databases of tran-scriptome, e.g.
RefSeq (Pruitt et al., 2007) and Ensembl (Flicek et al., 2011), play an increasingly important role in RNA-Seq quantification.
For example, TopHat/Cufflinks supports a mode that allows users to specify the transcriptome by supplying an annotation database (a GTF file).
RSEM (Li and Dewey, 2011) uses bowtie (Langmead et al., 2009)a DNA sequence alignerto align fragments directly to the transcriptome.
Aligning RNA-Seq fragments to transcriptome avoids the com-putation to detect novel splice junctions in fragments and elim-inates the non-transcriptome regions in the genome from further examination, and thus reduces the total running time of the quantification method and the number of erroneous alignments in the results.
To further improve the performance, the utility of k-mers was recently proposed.
The concept of k-mersshort and consecu-tive sequences containing k nucleic acidshas been widely used in bioinformatics, including genome and transcriptome assembly (Fu et al., 2014; Grabherr et al., 2011), error correction in sequence reads (Le et al., 2013), etc.
Because the number of k-mers in the genome or transcriptome is enormous when k is large (e.g.
k 25), the need to store all k-mers impedes their counting.
Most of existing methods save memory usage during the computation by using sophisticated algorithms and advanced data structures [bloom filter (Melsted and Pritchard, 2011), lock-free memory-efficient hash table (Marcais and Kingsford, 2011), suffix array (Kurtz et al., 2008)] or relying on disk space to com-pensate memory space (Rizk et al., 2013).
Thanks to the recent advances in both annotated transcrip-tome and algorithms to rapidly count k-mers, the transcriptome-based alignment-free method, Sailfish (Patro et al., 2013), requires 20 times less running time and generates comparable results with alignment-dependent quantification methods.
Sailfish is a lightweight method: it first builds a unique index of all k-mers that appear at least once in the transcriptome, counts the occurrences of the k-mers in the RNA-Seq fragments and quantifies the transcripts by the number of occurrences of the k-mers through an EM algorithm.
Regardless of being alignment-dependent or alignment-free, all methods need to recover the fragment depththe number of fragments that cover a specific locationacross the whole transcriptome as one of the initial steps.
However, none of the existing methods exploit the strong redundancy of the fragment depth in RNA-Seq data.
More specifically, Fig.1 shows a strong correlation between the fragment depth of any two locations that are a certain distance apart on the transcriptome, varying the distance from 1 to 100bp.
Even when the two locations are 20 bp away from each other, the Pearson correlation score is still as high as 0.985.
In other words, if an RNA-Seq quantifica-tion method that is able to recover the fragment depths for every 20 bp and quantify the abundance levels based on such informa-tion, there should be no significant accuracy loss in the result.
Recently, Uziela and Honkela (2013) developed a method that simply counts the number of alignments that covers the locations of hybridization probes used in the gene expression studies.
Though these probes only represent a sparse sampling on every transcript in the transcriptome, the method still provides reason-ably accurate results.
The observation and the method inspire us to ask the following question: what is the minimum information we need to achieve comparable accuracy in RNA-Seq quantifi-cation to the state-of-the-art methods?
More specifically, does there exist a subset of k-mers that can provide accurate transcrip-tome quantification?
And if so, how do we identify and use them to quantify transcriptome efficiently?
To answer these questions, we introduced a special type of k-mers called sig-mers, which only appear in a (small) subset of transcripts in the transcriptome.
Based on these sig-mers, we developed a method, RNA-Skim, which is much faster than Sailfish and also maintains the same level of accuracy in the results.
RNA-Skim includes two stages, preparation and quanti-fication.
In the preparation stage, RNA-Skim first partitions transcripts into clusters and uses bloom filters to discover all sig-mers for each transcript cluster, from which a small yet in-formative subset of sig-mers is selected to be used in the quanti-fication stage.
In the quantification stage, a rolling hash method (Karp and Rabin, 1987) is developed to rapidly count the occur-rences of the selected sig-mers, and an EM algorithm is used to properly estimate the transcript abundance levels using the sig-mer counts.
Because no sig-mer is shared by two transcript clus-ters, the task can be easily divided into many small quantification problems, which significantly reduces the scale of each EM pro-cess and also makes it trivial to be parallelized.
While RNA-Skim provides similar results to those of alternative methods, it only consumes 10% of the computational resources required by Sailfish.
In this article, we first describe the RNA-Skim method, then discuss how we compared RNA-Skim with other methods, fol-lowed by the experimental results using both simulated and real data.
2 METHOD In this section, we introduced the notion of sig-mers, which is a special type of k-mers that may serve as signatures of a cluster of transcripts, distinguishing them from transcripts in other clusters in the transcriptome that do not contain these k-mers.
Fig.1.
This figure shows the correlations of the fragment depth of any pair of locations as a function of the distance between the two locations from 1 to 100bp.
This figure is generated based on the alignments re-ported by TopHat on a real RNA-Seq data i284 Z.Zhang and W.Wang due Since of ( ) to very , base-pair ase-airs ase-air ase-airs in order , are employed Since  2.1 sig-mer In this article, an annotated transcriptome consists of a set of T tran-scripts: =ft1; :::; tTg.
A transcript t is an RNA sequence composed of a string of four bases A, U, C and G. In this article, we use the corresponding four DNA nucleotide bases A, T, C, G to represent.
The length of a transcript sequence may vary from 100 to 10 000bp.
A partition of a given transcriptome groups all transcripts into P dis-joint non-empty subsets or clusters, denoted by =f1; :::; Pg.
For example, one commonly adopted partition of transcriptome is to group transcripts into genes based on their locations on the genome.
For any transcript t, we use t to denote the cluster to which t belongs.
A substring of length k from a transcript sequence, its reverse se-quence, its complimentary sequence or its reverse and complimentary sequence is called a k-mer of the transcript.
We define a function k-mer() to represent the set of all k-mers from a single transcript or a cluster of transcripts, denoted as k-mer(t) or k-merp, respectively.
For simplicity, if a string s is a k-mer of transcript t, we say s 2 k-mert.
In this case, s 2 k-mert is also true.
DEFINITION.
Given a length k, a transcriptome and its partition , if a k-mer s only exists in one cluster p and never appears in other clusters n p, we call it a sig-mer of cluster p. And for any given cluster p, we denote all of its sig-mers as p. That is, p=fsjs 2 k-merp;8q 2 n p; s 2 k-merqg: Sig-mers characterize the uniqueness of each cluster.
It is obvious that the number of sig-mers heavily depends on the transcriptome partition.
If transcripts with similar sequences are assigned to different clusters, k-mers from these transcripts may not qualify as sig-mers.
Consequently, fewer sig-mers may be identified, and in the worst case, some cluster may not have any sig-mers.
2.2 Workflow of RNA-Skim Because sig-mers are unique to only one cluster of transcripts, if a sig-mer occurs in some RNA-Seq reads, it indicates the sig-mers corresponding transcripts may be expressed.
Therefore, its occurrence in the RNA-Seq data may serve as an accurate and reliable indicator of the abundance levels of these transcripts.
We proposed a method, RNA-Skim, for quan-tifying the transcripts using the sig-mer counts in RNA-Seq data.
Because no sig-mer is shared between transcript clusters, the problem reduces to quantifying transcript abundances using sig-mer counts within each clus-ter, which can be solved much more efficiently and can be easily paralle-lized.
This is different from Sailfish that uses all k-mers in the transcriptome.
In fact, RNA-Skim can be considered as a generalization of Sailfish: if the whole transcriptome is treated as a single cluster that includes all transcripts, all k-mers become sig-mers, and RNA-Skim de-generates to the exact formulation of Sailfish.
The workflow of RNA-Skim includes two stages: preparation and quantification.
In preparation, RNA-Skim clusters the transcripts based on their sequence similarities, finds all sig-mers for each transcript cluster and selects a subset of sig-mers to be used in the quantification stage.
In quantification, RNA-Skim quickly counts the occurrences of sig-mers and quantifies transcripts within each cluster.
The preparation stage of RNA-Skim does not require RNA-Seq read data and thus can be com-puted once as an offline process and be repeatedly used in the quantifi-cation stage.
2.3 Preparation stage In the preparation stage, RNA-Skim only requires users to supply a transcriptome (including all transcript sequences and gene annotations) and specifies a desired sig-mer length to be used in RNA-Skim.
Transcriptome Partitioning A straightforward way to partition tran-scripts is based on their genome locations from an annotation database.
However, the result of this partitioning approach may not be optimal because some transcripts of different genes may have similar sequences or share common subsequences.
To minimize the number of common k-mers shared between clusters, RNA-Skim uses a sequence similarity-based algorithm to generate a partition of transcriptome, instead of using any existing partition.
We first define the k-mer-based similarity, which is used as the sequence similarity in the algorithm.
DEFINITION.
The k-mer-based similarity of two sets of sequences i and j is defined as the higher of the two ratios: the number of common k-mers divided by the total number of k-mers in i, and the number of common k-mers divided by the total number of k-mers in j: k-mer-Similarityi; j= 1 max jk-meri \ k-merjj jk-merij ; jk-meri \ k-merjj jk-merjj !
: 2 Transcripts from the same gene are likely to be similar to each other.
To avoid unnecessary computation, RNA-Skim operates at the level of genes rather than transcripts.
However, calculating the exact similarity between a pair of genes requires generating all k-mers appearing in each gene and taking the intersection of the two sets.
This is computationally expensive.
To expedite the computation, RNA-Skim uses the data struc-turebloom filter (Bloom, 1970)coupled with a sampling-based ap-proach to approximate the similarity between two genes.
The bloom filter is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set, without the need of storing the set explicitly.
A bloom filter includes a vector of bits and several inde-pendent hash functions.
Initially, all bits are set to 0.
When an element is added to the bloom filter, the bits based on the hash values of the element are set to 1.
The bloom filter reports an element is in the bloom filter if its corresponding bits are all set to 1.
A bloom filter may yield a small number of false positives, but no false negatives.
The false-positive rate is bounded if the number of elements in the set is known.
It can be maintained efficiently when new members are added to the set.
RNA-Skim first builds a bloom filter for the set of k-mers of each gene.
Then, it randomly samples two subsets of k-mersnoted as Xi and Xjfrom the pair of genes, and the k-mer-Similarityi; j is approximated by maxjXi \ k-merjjjXij ; jk-meri \ Xjj jXjj (our experiments show that we only need a small number (e.g.
10) of k-mers from each gene to achieve approximation with high accuracy).
After we calculate the approximated similarities for every pair of genes, an undirected graph is built with each node representing a gene.
There is an edge between two nodes if the similarity of the two corresponding genes exceeds a user-specified threshold .
Each connected component of this graph naturally forms a cluster of nodes; each cluster of nodes forms a cluster of genes and transcripts of the genes.
Sig-mers discovery By definition, the sig-mers are essentially the k-mers occurring in only one cluster of transcripts.
A brute force way to find all sig-mers is, for every k-mer in the transcriptome, to determine whether the k-mer that appears in one cluster also appears in some other cluster.
Because the number of possible k-mers is in the order of billions, without any sophisticated data structure and data compression algorithms, stor-ing the k-mers alone will easily take at least tens of gigabytes of memory space, which is way beyond the capacity of any commodity computer.
RNA-Skim again uses bloom filters to reduce memory usage.
Three types of bloom filters are used: a bloom filter BF:ALL for checking whether a given k-mer has been examined, a bloom filter BF:DUP for checking whether a given k-mer appears in more than one cluster and a bloom filter BF:Sp for each cluster p for checking whether a given k-mer is in k-merp.
i285 RNA-Skim paper , ase-airs , Since Since , y In order very es employs 1 tiliz employ or not or not, First, for each cluster p, all distinct k-mers in it are enumerated: RNA-Skim enumerates all k-mers for every transcript in the cluster, and adds them to BF:Sp; if a k-mer is already in BF:Sp, it will be ignored.
Second, every distinct k-mer in p is added into BF:ALL, and if it is already in BF:ALL (that is, it was added when RNA-Skim examined other clusters), it is added into BF:DUP.
Therefore, if a k-mer occurs in multiple clusters, it is added in BF:DUP.
Last, every k-mer of the tran-scriptome is enumerated again, and if the k-mer is not in BF:DUP, it is reported as a sig-mer, as it only occurs in one cluster.
Because bloom filters may have false-positive reports, but never have false-negative reports, through this approach, some genuine sig-mer strings may be missed, but a non-sig-mer will never be labeled as a sig-mer.
Figure 2 shows the pseudocode of our algorithm.
Sig-mers selection RNA-Skim does not use all sig-mers because they are still numerous.
Instead, RNA-Skim selects a subset of sig-mers for the quantification stage.
We used a simple approach to select sig-mers from all sig-mers found by the previous step: for every transcript, sig-mers are evenly chosen based on the sig-mer locations such that any two sig-mers are at least 50 base pair away from each other in the given transcript.
Because some sig-mers may appear in multiple transcripts in the same cluster, for every selected sig-mer, all transcripts are re-examined, and the ones that contain the sig-mer are also recorded.
Through this approach, we can guarantee that every transcript is associated with some sig-mers (as long as there exist some sig-mers).
A good sig-mer coverage is crucial for accurate quantification of transcript abundance.
The final output of the preparation step includes the partition of the transcriptome, selected sig-mers and their associating clusters and transcripts.
2.4 Quantification stage The quantification stage requires users to provide RNA-Seq data (e.g.
FASTQ/FASTA files) and the selected sig-mers associated with tran-scripts containing them from the preparation stage.
Sig-mer counting Because the number of sig-mers used in RNA-Skim is much smaller than the number of k-mers typically used by other k-mer-based approaches, all sig-mers can be stored in a hash table in memory.
The number of occurrences of all sig-mers can be counted by enumerating all k-mers in the RNA-Seq reads and looking up the k-mers in the hash table to update the corresponding counters.
RNA-Skim basically follows this scheme with a tweak on the hash function to further speed up the computation.
In a straightforward implementation of the previously described algo-rithm, every k-mer incurs an O(k) operation to calculate its hash value, and this hash operation can be further reduced to O(1) by the Robin Karp pattern matching algorithm (Karp and Rabin, 1987).
The Robin Karp pattern matching algorithm requires a special hash function rolling hashthat only uses multiplications, additions and subtractions.
In rolling hash, the hash value Hr of the first k-mer in the RNA-Seq read r is calculated by Hr0; :::; k 1=r0 hk1+r1 hk2+:::+rk 1 h0; where h is the base of the hash function, ri is the ith character in s and the character hash function maps a character to an integer value.
One way to calculate the hash value for the (sequentially ordered) second k-mer r1; :::; k is Hr1; :::; k=r1 hk1+r2 hk2+:::+rk h0: But thanks to the structure of the rolling hash function, Hr1; :::; k can be calculated in a much faster way: Hr1; :::; k=Hr0; :::; k 1 r0 hk1 h+rk h0; which only requires one subtraction, three multiplications and one add-ition.
We can look up the hash value in the hash table, and if it is in the hash table, its associated counter is incremented accordingly.
Because RNA-Skim uses this specially designed hash function, we implemented our own hash table in RNA-Skim using open addressing with linear probing.
The base h is arbitrarily set to be a prime number 37, and the function maps every character to its actual ASCII value.
Quantification Because every cluster of transcripts has a unique set of sig-mers, which are the k-mers that never appear in other transcript clus-ters, every cluster can be independently quantified byRNA-Skim, resulting in a set of smaller independent quantification problems, instead of one huge whole transcriptome quantification problem in other approaches.
Formally, if p is a cluster of transcripts, the set of sig-mers of P is denoted by Sp, a sig-mer is denoted by s (s 2 Sp), the set of all occurrences of sig-mers is denoted by Op, an occurrence of a sig-mer in the RNA-Seq dataset is denoted by o (o 2 Op) and the sig-mer of the occurrence is denoted by zo.
From the previous steps, we obtained cs (the number of occurrences of the sig-mer s in the RNA-Seq data), ys;t (binary variables indicating whether the sig-mer s is contained in the sequence of transcript t) and bt (the number of sig-mers that are contained by transcript t).
C is the number of occurrences of all sig-mers (C= X s cs).
Same as in the previous study (Pachter, 2011), we define )=ftgt2p where t is the proportion of all selected sig-mers that are included by the reads from transcript t, and X t=1.
For an occurrence o; po 2 t represents the probability that o is chosen from transcript t, in a genera-tive model, po 2 t=yzo;t t bt 3 Therefore, the likelihood of observing all occurrences of the sig-mers as a function of the parameter ) is L= Y o2Op X t2p po 2 t= Y o2Op X t2p yzo;t t bt 4 = Y s2Sp X t2p ys;t t bt cs : 5 Fig.2.
The pseudocode to find all sig-mers i286 Z.Zhang and W.Wang s since Since-Since , Since--, Since Since , or not , This is in spirit similar to the likelihood function used in other studies, except that this is the likelihood of observing sig-mers rather than frag-ments (Li and Dewey, 2011) or k-mers (Patro et al., 2013).
Thus, we also used an EM algorithm to find that maximizes the likelihood: it alter-nates between allocating the fraction of counts of observed sig-mers to transcripts according to the proportions and updating given the amount of sig-mers assigned to transcripts.
RNA-Skim also applies the same technique used in Patro et al.
(2013), Nicolae et al.
(2011) and Turro et al.
(2011) to collapse sig-mers if they are contained by the same set of transcripts.
(See the Supplementary Material) RNA-Skim reports both Reads Per Kilobase per Million mapped reads (RPKM) and Transcripts Per Million as the relative abundance estimations of the transcripts, and both metrics are calculated by the way used in Sailfish (Patro et al., 2013).
So far, we have described both preparation and quantification stages in RNA-Skim.
In the last, a toy example is provided to illustrate how each stage works in RNA-Skim in Figure 3.
3 SOFTWARE FOR COMPARISON RNA-Skim is implemented in C++ with heavy usage of the open-source libraries bloomd (Dadgar, 2013), protobuf (Google, 2013) and an open-source class StringPiece (Hsieh, 2013).
The parameter settings will be discussed in the Section 5.
We compared RNA-Skim with four different quantification methods: Sailfish (0.6.2), Cufflinks (2.1.1), RSEM (1.2.8) and eXpress (1.5.1) in both simulated and real datasets.
TopHat (2.0.10) and Bowtie (1.0.0) are used as the aligners when needed.
For Sailfish, we set k-mer size to be 31 because this value gives the highest accuracy in the simulation study, among all k-mer sizes supported by Sailfish (k 31).
For other software, we followed the experiments in Patro et al.
(2013) to set the param-eters.
Input to Cufflinks was generated by TopHat, which used Bowtie (bowtie1), allowing up to three mismatches per read (-N 3 and read-edit-dist 3).
Both TopHat and Cufflinks were pro-vided with a reference transcriptome.
RSEM and eXpress dir-ectly used Bowtie to align the reads to the transcriptome, with the argument (-N 3) to allow up to three mismatches per read.
The eXpress was executed in the streaming mode, to save the total quantification running time.
For simulation study, we used the estimations without bias correction for Sailfish, Cufflinks and eXpress.
For real datasets, the estimations with bias correction are used for these three methods.
For RSEM, since it does not provide an option to control the bias correction, we did not differentiate its usage in the simulation and real data studies.
Other parameters were set to default values for these methods.
All methods were run on a shared cluster managed by the Load Sharing Facility (LSF) system.
The running time and CPU time of these methods are measured by LSF.
Each cluster node is equipped with Intel(R) Xeon(R) 12-core 2.93GHz CPU and at least 48GB memory.
All files were served by the Lustre file system.
4 MATERIALS All materials including both simulated and real data are based on the mouse population and consist of paired-end reads with 100bp length per read.
We used C57BL/6J downloaded from Ensembl (Build 70) as the reference genome in all experiments.
All methods studied in this article were provided with 74215 protein-coding annotated transcripts from the Ensembl database.
The simulation Fig.3.
An illustration of how RNA-Skim works on a toy transcriptome of five transcripts i287 RNA-Skim , (TPM) Results , LSF ( ase-airs datasets, including 100 mouse samples with the number of reads varying from 20 millions to 100 millions, were generated by the flux-simulator (Griebel et al., 2012) with its default error model enabled.
For real datasets, we used the RNA-Seq data from 18 inbred samples and 58 F1 samples derived from three inbred mouse strains CAST/EiJ, PWK/PhJ and WSB/EiJ.
The RNA-Seq data was sequenced frommRNA extracted from brain tissues of both sexes and from all six possible crosses (including the reciprocal).
5 RESULTS In this section, we first compared alternative partition algorithms and how they impact sig-mer selections in RNA-Skim and then furnish a comparison with four alternative methods on both simulated and real data.
At last, we demonstrated that RNA-Skim is the fastest method among all considered methods.
5.1 Similarity-based partition algorithm We compared the result of our similarity-based partition algo-rithm with those from two alternative ways to partition tran-scripts: transcript-based partition (every cluster contains a transcript) and gene-based partition (every cluster contains the transcripts from an annotated gene).
The similarity threshold in our partition algorithm was set to be 0.2 (more details are provided later on the parameter choice).
Table 1 compares these partitions on the same transcriptome.
The number of clus-ters generated by our similarity-based partition is 20% fewer than the number of genes.
The average number of transcripts per cluster is20% more than the average number of transcripts per gene.
Most clusters only contain transcripts from a single gene, though the largest cluster contains 6107 transcripts.
These transcripts in the largest cluster share a substantial number of k-mers (e.g.
from paralogous genes), which need to be examined altogether to accurately estimate their abundance levels.
Failing to consider them together (e.g.
by using transcript-based or gene-based partitions) will compromise the number of sig-mers that help distinguish transcripts and hence impair the accuracy of transcriptome quantification.
Even though this clus-ter contains many transcripts, it represents510% of the total number of transcripts.
We used these three types of partitions as the input to the sig-mer discovery method.
To evaluate the goodness of a partition, we measured the proportion of each transcript that is covered by sig-mers and plot the cumulative distribution of all transcripts sorted in ascending order of their sig-mer coverage in Figure 4, with varying k-mer sizes.
For any transcript, the higher the sig-mer coverage is, the more accurate the abundance estimation will be.
Our similarity-based partition is the best: almost all tran-scripts have at least 80% sig-mer coverage, which pushes the curves to the upper left corner of the plot regardless of the k-mer size.
The gene-based partition is slightly worse: 95% of transcripts have at least 80% sig-mer coverage.
The gene-based partition tends to result in low sig-mer coverage for genes sharing similar sequences.
The transcript-based partition is the worst for an obvious reason: transcripts from the same genes may share exons and thus the number of sig-mers that can distinguish a transcript may be very small.
We also observed that using longer k-mer improves the sig-mer coverage.
In the end, RNA-Skim selects 2 586388 sig-mers to be used in the quantification stage, and these sig-mers count for53.5% of 74 651 849 distinguished k-mers used by Sailfish.
Because RNA-Skim uses a much smaller set of sig-mers, it is able to use the rolling hash methoda very fast but memory-inefficient meth-odto count sig-mers in RNA-Seq reads.
5.2 Simulation study Figure 5 compares the performance of the five methods on the simulated data using four metrics: Pearsons correlation coeffi-cient, Spearmans rank correlation coefficient, significant false-positive rate (SFPR) and significant false-negative rate (SFNR).
For brevity, we use Pearson (Truth), Spearman (Truth), SFPR and SFNR to denote these metrics, respectively.
The Pearsons correlation coefficient is calculated in a logarithmic scale, using all transcripts whose true and estimated abundance values are 40.01 RPKM.
This calculation is the same as that used by Sailfish (Patro et al., 2013).
The Spearmans rank correlation is calculated on the set of transcripts whose true abundance values are40.01 RPKM.
The SFPR and SFNR are calculated to assess the estimation distributions on the set of transcripts excluded by Fig.4.
The distribution sig-mer coverages across all transcripts an as-cending order of the sig-mer coverage.
The upper the curve is, the better the corresponding partition is Table 1.
This table compares three different partitions Type Number of clusters Average number of transcripts per cluster Size of the largest cluster Transcript 74 215 1 1 Gene 22 584 3.29 39 RNA-Skim 18269 4.06 6107 Sailfish 1 74 215 74215 Note: If the partition contains only one cluster of all transcripts, RNA-Skim degen-erates to Sailfish.
We thus listed it in the table for comparison.
i288 Z.Zhang and W.Wang , 6 about , , in order , less than about a total of , , less than , , Since larger than larger than significant false positive rate significant false negative rate the previous metrics: if a transcripts estimation is40.1 RPKM, but its true abundance value is50.01 RPKM (a 10-fold suppres-sion), we call it a significant false positive; similarity, if a tran-scripts estimation is50.01 RPKM, but its true abundance value is40.1 RPKM (a 10-fold amplification), we call it a significant false negative.
There are two reasons that we chose SFPR and SFNR instead of the regular false-positive rate and false-negative rate: first, we prefer the transcripts with relatively large abun-dance values (40.1 RPKM) because they are accountable for 99% the RNA-Seq data; second, owing to the noisy nature of RNA-Seq, for the transcripts with small abundance values (50.01 RPKM), it is difficult to calculate accurately, e.g.
both RSEM and Sailfish set the default minimal abundance value to be 0.01 RPKM.
For RNA-Skim, we varied the sig-mer length from 20 to 95 bp.
Other methods are presented as horizontal lines for comparisons.
Despite the small differences by individual metrics, Figure 5 shows that these five methods exhibit comparable performance: no method outperforms the remaining methods across all metrics and the maximal difference by any metric is within 0.05.
Figure 5(a) and 5(b) show two concave curves of Pearson (Truth) and Spearman (Truth) for RNA-Skim by varying its sig-mer length.
There are two factors explaining the concave curves.
First, when the sig-mer length increases, sig-mers become more distinct and the sig-mer coverage increases, which improves the correlations between the truth and estima-tion.
Second, for any fixed read length, when we increased the sig-mer length, the probability that a sig-mer is contained by a single read drops, causing the decrease in the number of sig-mers observed in the RNA-Seq data, which may exacerbate the cor-relations.
In summary, there is a clear trade-off on the sig-mer length.
Empirically, the best sig-mer length is between 55 to 60, and we thus used 60 in other experiments.
For the same reason, in Figure 5(c) and 5(d), we found that the increase in the sig-mer length affects positively on SFPR, but negatively on SFNR.
When the sig-mer length equals 30, it has similar SFPR with Sailfish, but worse SFNR score than Sailfish, indicating that the complete set of k-mers still has its advantage than a small set of sig-mers.
However, RNA-Skim is able to use much longer k-mers that Sailfish does not support, so RNA-Skim using longer k-mers can have a better SFPR than Sailfish.
Other methods also follow the same inverse correlation: while Sailfish and eXpress are the worst in SFPR among these five methods, they are the best two in SFNR.
Figure 6 shows the Pearson (Truth), Spearman (Truth), SFPR and SFNR as a function of the number of sig-mers used in RNA-Skim.
In Figure 6(a), 6(b) and 6(d), when the number of sig-mers increases, the three metrics improve substantially, though at different paces.
Figure 6(c) shows no significant change in SFPR for different numbers of sig-mers.
This obser-vation suggests that we should use as many sig-mers as possible given available memory space.
To ensure RNA-Skim to have similar memory usage to that of other methods, RNA-Skim uses 2.58 million sig-mers.
This is also the default setting in other experiments in this article.
Table 2 shows that the metrics do not vary much when using different similarity thresholds.
In the simulation study, we varied the similarity threshold from 0.06 to 0.28 and observed at most 0.005 change across all metrics.
Owing to limited space, the de-tailed results for the thresholds between 0.06 and 0.28 are omitted.
Figure 7 shows a strong and clear linear correlation between the estimated RPKM scores by RNA-Skim and the true RPKM scores on one simulated sample.
In simulation study, we note that the accuracy of RNA-Skim depends on the sig-mer length and the number of sig-mers, but is insensitive to the threshold .
When these parameters are chosen properly, RNA-Skim produces similar results to those by other methods.
5.3 Study using real RNA-Seq data Because the flux simulator cannot simulate RNA-Seq data with bias effects, and there might also be other unknown factors in the real RNA-Seq data that the simulator fails to capture, we also compared RNA-Skim with other methods on real data.
Because we do not know the ground truth on real data, we computed the Pearson correlation and Spearman correlation between the re-sults produced by RNA-Skim and one other method, referred to as Pearson (methods) and Spearman (methods) to distinguish from the previous computed correlations between RNA-Skim result and the ground truth.
Figure 8 shows that the distributions of the Pearson (methods) and Spearman (methods) are not significantly different between real data and simulated data.
For example, the differences be-tween the mean values of the correlations on both simulated and real data are no more than 0.02 across all methods.
This (a) (b) (c) (d) Fig.5.
(a), (b), (c) and (d) plot Pearson (Truth), Spearman (Truth), SFPR and SFNR of RNA-Skim as a function of sig-mer length, respectively.
For comparison, we also plotted that of the other four methods as the horizontal lines.
The reported values are the average across 100 simulated samples.
The red crosses indicate the sig-mer length (i.e.
60 bp) used in other experiments in this article i289 RNA-Skim larger than less than smaller than larger than larger than due less than very , ase-airs ( ) Fig.5( ) , ( ) ( ) , ( )( )( ) Due Since Since  consistency suggests that the result from RNA-Skim may have similar correlations with the unobserved truth.
The slightly wider distribution of the correlations in real data (than that in simu-lated data) suggests the real data may exhibit more diversity than simulated data.
(For the comparison with gene expression data, please see the Supplementary Material).
5.4 Running time For the preparation stage (including transcriptome partitioning and sig-mer selection), RNA-Skim takes 3h to finish on the mouse transcriptome by using a single thread.
Most time is spent on calculating the k-mer-based similarities between different pairs of genes.
It takes 10min to finish sig-mer discovery and selection.
It is worth noting that these steps only need to be run once for one population beforehand, and after sig-mers are se-lected and their connections with transcripts are established, the result can be repeatedly used on quantifying the transcriptome of many samples.
Therefore, the running time for the preparation stage is less critical than the running time of the quantification stage, and the one-time investment of 3 h is acceptable.
For the quantification stage, we compared both the running time and the CPU time of these five methods on a real sample with 44 millions of paired-end reads.
The running time is the (a) (b) (c) (d) Fig.6.
(a), (b), (c) and (d) plot Pearson (Truth), Spearman (Truth), SFPR and SFNR as a function of the number of sig-mers used in RNA-Skim, respectively.
For comparison, we also showed that of the other four methods as horizontal lines.
The reported values are the average across 100 simulated samples.
The red crosses indicate the number of sig-mers (i.e.
2.58 million sig-mers) used in other experiments in this article (a) (b) (c) (d) Fig.8.
(a), (b), (c) and (d) show the distributions of the Pearson (methods) and Spearman (methods) correlations between the results from RNA-Skim and the results from each of the remaining methods on both simulated and real data Fig.7.
The scatterplot of the estimated RPKM scores by RNA-Skim versus the true RPKM scores.
Both axes are in a logarithmic scale, and all transcripts whose true RPKM or estimated RPKM is 50.01 are omitted Table 2.
This table shows that the four metrics do not change much for different similarity threshold  Pearson Spearman SFP SFN 0.06 0.9438 0.9242 0.0692 0.0233 0.28 0.9440 0.9237 0.0698 0.0235 i290 Z.Zhang and W.Wang about ours about utes ours elapsed time between the start and end of a method, and the CPU time is the total time a method uses on each core of the CPU.
For a single thread method, the running time is exactly the same as the CPU time.
And for a multi-threading method run-ning on a multi-core CPU, the running time is typically shorter than the CPU time.
RNA-Skim is submitted as a single thread method.
Sailfish, Cufflinks with TopHat as the aligner and RSEM with Bowtie as the aligner are submitted with multi-threading enabled and requiring eight threads.
eXpress is an online algorithm, and it can quantify a streaming input of align-ments generated by Bowtie in real time.
Bowtie and eXpress use six and two threads for alignment and quantification, respectively.
Table 3 summarizes the running time of all five methods.
RNA-Skim is the fastest, 11 faster than the second best method, Sailfish, on the CPU time.
Even when Sailfish uses eight threads, RNA-Skim is 1.6 faster on the running time by just using one thread.
Because the aligner usually consumes lots of computation time, RNA-Skim has4100 times speedup on the CPU time compared with Cufflinks, RSEM and eXpress.
Overall, these results demonstrate that RNA-Skim provides comparable accuracy with other methods on both simulated and real data, using a much shorter running time.
6 DISCUSSION AND CONCLUSION We introduced RNA-Skim, a lightweight method that can rap-idly and efficiently estimate the transcript abundance levels in RNA-Seq data.
RNA-Skim exploits the property of sig-mers, significantly reducing the number of k-mers used by the method and the scale of the optimization problem solved by the EM algorithm.
Based on our benchmark, it is at least 10 faster than any alternative methods.
To the best of our know-ledge, the design principle of almost all existing methods is to use as much data as possible for RNA-Seq quantification.
Our re-sults are encouraging, in the sense that they demonstrate a dif-ferent, yet promising, direction of building a much faster method by discovering and using only informative and reliable features the counts of sig-mers in RNA-Seq data.
Currently, the annotation databases are incomplete and still under development.
Aligners and alignment-dependent RNA-Seq methods are commonly used to allow unknown transcript discovery, which will further improve the completeness and ac-curacy of the annotation databases.
The performance of tools like Sailfish and RNA-Skim depends on the quality of the annotation database.
Their accuracy is likely to improve when annotation databases become complete or nearly complete in the future.
They will become better choices when we have a better understanding of transcriptome and transcript discovery task be-comes less important.
Because RNA-Skim is still under development, there are sev-eral directions to further improve its performance.
(i) RNA-Skim uses a simple hash table implementation without any optimiza-tion on the memory usage.
We will investigate advanced data structures enabling better memory utilization.
(ii) Currently, the sig-mer selection algorithm in RNA-Skim only ensures uniform coverage.
In the future, we will explore variable selection tech-niques to select fewer but more informative sig-mers.
(iii) The current version of RNA-Skim does not have built-in bias correc-tion capability, even though it already produces results compar-able with the state-of-the-art methods with bias correction on real data.
We plan to incorporate bias correction in the next version of RNA-Skim, which is likely to improve the perform-ance further.
In addition, we also plan to support multi-thread implementation and deploy RNA-Skim in differential expression analysis.
We are optimistic that, when we add the multi-thread-ing capability to RNA-Skim, the running time will be further improved.
ACKNOWLEDGEMENTS We would like to thank those center members who prepared and processed samples as well as those who commented on and encouraged the development of RNA-Skim, in particular, Leonard McMillan, Vladimir Jojic, William Valdar and Yunjung Kim.
We also would like to thank three anonymous reviewers for their thoughtful comments.
Funding: This work was funded by NIH R01HG006703, NIH P50 GM076468-08 and NSF IIS-1313606.
Conflict of Interest: none declared.
ABSTRACT Motivation: Pre-mRNA cleavage and polyadenylation are essential steps for 30-end maturation and subsequent stability and degradation of mRNAs.
This process is highly controlled by cis-regulatory elements surrounding the cleavage/polyadenylation sites (polyA sites), which are frequently constrained by sequence content and position.
More than 50% of human transcripts have multiple functional polyA sites, and the specific use of alternative polyA sites (APA) results in isoforms with variable 30-untranslated regions, thus potentially affecting gene regulation.
Elucidating the regulatory mechanisms underlying differen-tial polyA preferences in multiple cell types has been hindered both by the lack of suitable data on the precise location of cleavage sites, as well as of appropriate tests for determining APAs with significant dif-ferences across multiple libraries.
Results: We applied a tailored paired-end RNA-seq protocol to spe-cifically probe the position of polyA sites in three human adult tissue types.
We specified a linear-effects regression model to identify tissue-specific biases indicating regulated APA; the significance of differences between tissue types was assessed by an appropriately designed permutation test.
This combination allowed to identify highly specific subsets of APA events in the individual tissue types.
Predictive models successfully classified constitutive polyA sites from a biologic-ally relevant background (auROC99.6%), as well as tissue-specific regulated sets from each other.
We found that the main cis-regulatory elements described for polyadenylation are a strong, and highly in-formative, hallmark for constitutive sites only.
Tissue-specific regu-lated sites were found to contain other regulatory motifs, with the canonical polyadenylation signal being nearly absent at brain-specific polyA sites.
Together, our results contribute to the understanding of the diversity of post-transcriptional gene regulation.
Availability: Raw data are deposited on SRA, accession numbers: brain SRX208132, kidney SRX208087 and liver SRX208134.
Pro-cessed datasets as well as model code are published on our website: Contact: uwe.ohler@duke.edu 1 INTRODUCTION Almost all eukaryotic mRNAs undergo a post-transcriptional processing step called polyadenylation, in which they acquire a polyA tail at their 30-end.
After transcription, the 30-most seg-ment of the newly made RNA is cleaved off at specific sites (polyA sites) by a set of RNA regulatory proteins, which is fol-lowed by the synthesis of the polyA tail by the addition of ad-enine (A) residues in a non-templated fashion (Andreassi and Riccio, 2009).
Around 90 protein factors regulate this process, with CPSF (cleavage and polyadenylation specificity factor), CstF (cleavage simulator factor), CFI (cleavage factor I), CFII (cleavage factor II), PAP (polyA polymerase) and PABII (polyA binding protein) playing a crucial role (Beaudoing et al., 2000; Ji and Tian, 2009; Shi et al., 2009; Tian et al., 2005).
PolyA sites are essential for 30-end maturation, stability and degradation of mRNAs.
Furthermore, polyadenylation defines the extent of the 30-untranslated region (30-UTR) of mRNAs, which spans from the stop codon up to the polyA tail and con-tains many post-transcriptional regulatory sequence elements such as microRNA (miRNA) target sites.
In addition, alternative polyadenylation (APA) events arise from the presence of more than one particular functional cleavage/polyadenylation (polyA) site.
The specific use of different polyadenylation sites can play a direct role in gene regulation.
For instance, eliminating large parts of a 30-UTR by using the more proximal polyA site enables a transcript to escape from miRNA regulation of its longer iso-form.
In proliferating cells, proximal polyA sites are therefore favored over distal ones, resulting in the production of mRNAs with shorter 30-UTR and fewer miRNA-binding motifs (Ji and Tian, 2009; Sandberg et al., 2008).
APA can influence mRNA nuclear export, cytoplasmic localization and non-miRNA mediated changes in mRNA stability and translational efficiency (Majoros and Ohler, 2007; Mayr and Bartel, 2009; Moore, 2005).
As such, it is important to identify not just alternative but specifically regulated alternative events, such as tissue-specific APA.
Based on earlier analysis of expressed sequence tags (ESTs), over 50% of the human and more than 30% of mouse genes were observed to have multiple polyadenylation sites, which results in mRNA isoforms different in their 30-UTR and/or coding sequences (Tian et al., 2005).
Initial studies on ESTs and tiling microarrays also indicated a bias in the regula-tion of polyA sites in certain human tissues (David et al., 2006; Tian et al., 2005; Zhang et al., 2005a).
The introduction of high-throughput sequencing technology has vastly expanded the opportunities to explore APA.
Recent deep sequencing of mRNA populations from multiple tissue types has shown that 86% of human genes exhibit variants due to APA sites (Wang et al., 2008).
In addition, several protocols relevant for studying polyadenylation have been developed.
*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com These protocols are designed to capture the 30-end of mRNAs using specific primers, then sequence these fragments using second-and third-generation sequencing technologies (Jan et al., 2010; Mangone et al., 2010; Ozsolak et al., 2010; Shepard et al., 2011).
However, with one exception (Derti et al., 2012), these approaches have been applied on small samples or non-mammalian genomes, leaving human normal tissues unexplored.
A thorough analysis of the polyadenylation process in adult tissue types, showing differential gene expression, would help us understand tissue-specific APA regulation.
Although genome-wide APA profiling enables us to discover genes with multiple polyA isoforms at a genome-wide scale, it introduces major chal-lenges.
Without adequate methodology to specify the significance of APA biases in different tissues, we may confuse the mere presence of multiple APA with their specific up-or downregula-tion across conditions.
A clean definition of truly specific sets is necessary to investigate which features allow for successful dis-crimination via computational models, and to suggest candidate regulatory features for future studies.
In this article, we address several of these challenges by using data from a new RNA-seq protocol applied to sequence the 30-UTR end of mRNAs from different adult normal tissue types.
Using a linear model, we distinguish between constitutive, alternative and alternatively regulated polyadenylation sites.
Our linear regression model takes into account different library depth, expression of each gene in each tissue, as well as inter-actions between tissues and genes.
As is still the case with many deep sequencing datasets, we do not have multiple replicates at our disposition that can be used to identify significantly differing APAs across tissues.
Instead, significance of differences between samples from different tissue types is assessed by an appropri-ately designed permutation test.
We then use the flanking se-quence region around polyA sites to build predictive models both for the discrimination of constitutive polyA sites from gen-omic background, as well as to distinguish between regulated APA sets from different tissues.
2 RESULTS 2.1 A paired-end sequencing strategy for identifying polyadenylation sites To precisely map polyA sites at genome-wide scale, we made use of several new libraries generated by a tailored sequencing ap-proach, PA-seq.
This protocol yields paired-end tags, with one tag located directly at the cleavage site, and its pair mapping to a more upstream location, typically in the 30-UTR of the same transcript, Figure 1 (see Section 5).
We used PA-seq to monitor the differential usage of polyade-nylation sites in three different human adult tissue types: brain, liver and kidney.
Each tissue was sequenced at varying depth.
We obtained 2.8 million raw paired reads from liver, 8 million from kidney and 3.5 million from brain.
Of those paired reads, 85% mapped to the human genome (hg19).
Non-redundant read pairs, i.e.
those that showed differences in at least one of the paired end tags, were grouped for each unique 30 position, denot-ing a polyA site.
These sites were filtered to exclude 30 locations that mapped to genomic regions with high A content, to exclude possible contaminations by internal priming.
PA-seq reads were then clustered into clusters (PAS clusters) analogous to an algo-rithm previously developed for the analysis of capped 50 mRNA tags (Ni et al., 2010).
We used the total sum of the non-redun-dant read pairs of all of the 30 tags in each PAS cluster as a measure of the PAS usage, and considered PAS clusters covering narrow genomic regions and with five or more reads for all fur-ther analyses (see Section 5).
Table 1 summarizes the data for all libraries.
To differentiate between PAS clusters that are constitutively used versus those with more than one polyA site, we grouped all overlapping PASs of the same transcript from the three tissue types together.
Each PAS cluster was referred to by the mode of its median (see Section 5).
If the gene has one PAS cluster, we refer to it as a constitutive gene; if it has more than one PAS cluster, we refer to it as an alternatively polyadenylated gene.
Overall, we identified PAS clusters for 11 454 genes: around 7278 are constitutive and 4176 are alternative polyadenylated.
From genes that are expressed in the three tissue types, 2171 are constitutive genes and 1965 are alternative polyadenylated genes.
Alternative-polyadenylated genes had 5357 different PAS clusters; this is the set included in our analysis.
2.2 Characterization of tissue-specific regulated polyadenylation sites Previous research on APA has shown that most of human genes have multiple polyadenylation sites, with many of them being tissue-specific.
Testing the statistical significance of differential preferences for APA usage for a gene between tissues has been previously investigated by applying Fishers exact tests, chi-square tests or linear trend test (Beaudoing and Gautheret, 2001; Fu et al., 2011; Zhang et al., 2005a).
Applied on a gene with multiple PAS, measured across multiple conditions, Fishers test will detect a significant difference of the pattern from the null assumption, but further tests are needed to pinpoint exactly which PAS, in which tissue, deviates from constitutive expres-sion.
A popular approach for identifying specific events across multiple tissues/sites has therefore been introduced based on Fig.1.
Summary of PA-Seq Protocol: Total mRNA is randomly frag-mented and reversed transcribed with a modified oligo(dT) primer, which synthesizes with the polyA tail.
cDNA fragments are then captured and sequenced using multiplexed paired-end sequencing on Illumina i109 Modelling of tissue-specific alternative polyadenylation Shannon entropy (Schug et al., 2005).
Entropy values close to zero represent events specific to a single tissue; values increase as the relative usage spreads more across tissue types, or when the relative contribution of the tissue to the overall usage decreases.
However, entropy does not directly reflect significance, as sam-ples with vastly different levels of evidence (e.g.
read coverage) may lead to similar entropy values.
To avoid these shortcomings, we specified a linear effects re-gression model for the read counts of each PAS cluster in each tissue type, motivated by previous applications to detect signifi-cant changes in gene expression (Marioni et al., 2008) and alter-native splicing patterns (Blekhman et al., 2010).
We controlled for fixed effects including different tissue depth, expression of each gene in each tissue, as well as any interaction between tis-sues and genes.
The resulting residual for a given PAS cluster in a given tissue reflects evidence that this PAS cluster is specific, and highly used, in the tissue.
We then needed to quantify whether for a given PAS cluster, an observed difference in read counts in a specific tissue is sig-nificant, i.e.
more pronounced than what would be expected owing to random variation.
Given that the libraries were sequenced without experimental replicates, we applied permuta-tion tests on the read counts of PASs for each gene in our libraries, to determine a tissue-specificity threshold (see Section 5).
With three libraries at our disposal, we separated tissue-specific PAS clusters into two groups: clusters that are highly used in one individual tissue (individual), and clusters that are highly used in two tissue types simultaneously (overlap-ping).
Figure 2a shows the test statistics for assessing the over-lapping tissue-specificity applied to both original and permuted data.
By applying our linear model on alternative-polyadenylated genes, our strict selection led to 234 tissue-specific individual PAS clusters, and 214 tissue-specific clusters overlapping in two tissue types [at P50.01; false-discovery rate (FDR)50.25] (Fig.2b).
To study the biased usage of APA in different tissues, we calculated a variability index (VI) between each pair of tis-sues.
A low VI between two tissues indicates strong concordance in their usage of PAS clusters (see Section 5).
Confirming expect-ations, liver and kidney showed the highest correlation, while brain and kidney were the lowest.
To illustrate the difference of our model compared with pre-vious approaches, we calculated the Shannon entropy for the subset of PAS clusters that showed significant tissue specificity for both the individual and overlap PAS, Figure 3a.
While the Shannon entropy is less than one for about 480 PAS clusters, our model identified only half of these as significantly tissue specific, with few additional PASs that had higher entropy.
This is mainly because Shannon entropy does not take the abundance of evi-dence into account.
For example, in Figure 3b, while the residual for the most proximal PAS site of the gene HDLBP (on negative strand) in brain indicates its outlier character, it is based on 17 tags (510% of the total) and thus not large enough to be sig-nificantly brain specific.
Additional data would be needed to confirm the specific trend.
In turn, our model characterized spe-cific PAS clusters that would have been characterized as non-specific due to higher entropy values.
As an example, Figure 3c shows two PAS clusters for the gene BDH1 (on negative strand).
The distal cluster is used in the three tissue types, while the prox-imal is used in kidney and brain only.
Using the linear model, the distal cluster was detected as significant in liver, given that the other cluster, the proximal one, shows higher usage in the other two tissues (more than 2-folds).
2.3 Modeling constitutive polyadenylation sites Because the PAPs responsible for synthesizing the polyA tail lack substrate specificity, it necessitates the presence of specific signals in the sequences around polyA sites that control mRNA poly-adenylation (reviewed by Tian and Graber, 2012).
One of the known main cis-regulatory elements is a conserved hexamer with consensus AWUAAA, located 1035nt upstream of the polyA Table 1.
Summary of PA-seq generated data, filtering steps and clustering in each tissue library PA-seq reads and clustering Liver Kidney Brain Raw read pairs with identifiable linker sequences 2 851 978 8044 879 3 533 285 Read pairs mapped 2 449 567 7198 135 2 711 473 Non-redundant read pairs no priming 649 410 1353 072 1 320 265 Non-redundant read pairs 2 distinct 50 tags 545 708 1190 344 1 001 479 Different polyA sites 57 396 99 482 132 616 PAS clusters 8537 12 477 15 727 PAS clusters with NPa 7439 10 291 13 205 aNPNarrow Peak.
(a) (b) Fig.2.
(a) Test statistic for the residual of the original (red) and permuted data (blue) for calculating overlap-significant PAS sites.
(b) Number of tissue-specific PAS clusters found in each tissue: total individual sites: 234 (90 68 76), total overlap sites: 214 (31 100 83) i110 D.Hafez et al.
site, referred to as polyadenylation signal (Beaudoing et al., 2000).
The sequence composition at the cleavage site itself is not well characterized, but a dinucleotide preference CA was found in vitro (Chen et al., 1995).
The sequence around polyA sites are usually G/U-rich with a remarkable downstream elem-ent (DSE), located within 30 nt downstream of the cleavage site.
Upstream of polyA sites are upstream elements (USE) that are usually also U-rich, while some G-rich sequences have been re-ported as well.
These elements are largely located in the region (100,100) nt around polyA sites (Tian and Graber, 2012).
Most early attempts for the computational prediction of polyA sites considered only samples containing the canonical PAS signal.
Position weight matrices (PWM) for DSE and USE along with the PAS signal were used as input features for hidden Markov model (HMM) or support vector machines (SVM) (Hajarnavis et al., 2004; Legendre and Gautheret, 2003; Liu et al., 2003; Salamov and Solovyev, 1997; Tabaska and Zhang, 1999).
After the characterization of 15 putative regula-tory elements surrounding PAS signals (Hu et al., 2005), pos-ition-specific scoring matrices for the identified motifs and structural patterns of mRNA, have been later used as input fea-tures (Ahmed et al., 2009; Akhtar et al., 2010; Chang et al., 2011; Cheng et al., 2006; Shao et al., 2009).
Most recently, the appli-cation of artificial neural network and random forests techniques have been proposed (Kalkatawi et al., 2012).
These models were largely trained on low-abundance pooled EST data from varying human tissues; none of them examined tissues independently.
While the use of curated quality controlled data from collections such as PolyA_DB (Zhang et al., 2005b) made it possible to design models with high accuracy, studies typically restricted their dataset to only include transcripts with PAS signals and results were sometimes hard to interpret owing to negative data not matched to the problem faced by the RNA processing machinery (such as using random genomic locations).
Modeling constitutive and/or APA sites specifically has so far rarely been investigated.
The exact motifs responsible for APA are frequently still unknown, especially when it comes to tissue-regulated APA.
Calculating PWM scores as features of classi-fiers, as in the case of constitutive sites with known motifs, will likely not reflect all of the regulatory elements.
It is thus more applicable to use a sparse sequence-based classifier that uses a broad definition of the feature space.
String kernels transform the input sequences into a higher-dimensional feature space, ef-fectively looking for similarities among substrings, and have been proven to be successful in the prediction of alternative splicing and transcription start sites (Sonnenburg et al., 2006, 2007).
Here, we build an SVM, using all of the information available in the sequences flanking the polyA sites, by applying two string kernels, the spectrum kernel (Leslie et al., 2002) and the weighted degree kernel with shifts (WD) (shogun toolbox; version 2.0.0) (Ratsch et al., 2005).
While the spectrum kernel highlights the global similarities between sequences as it counts the number of occurrences of similar motifs, the WD kernel counts the number of matching substrings of similar lengths at the same position but allowed to be shifted within a specified window size around that position.
To investigate whether local sequence features around polyA sites are sufficient to explain polyadenylation, we first examined whether PAS clusters for constitutive genes could be classified from non-polyA sites.
We focused on (100,100) nt around polyA sites, given that the known constitutive elements are located in this region, and that it has additionally been shown to exhibit a biased nucleotide composition (Legendre and Gautheret, 2003; Tian et al., 2005).
As the polyadenylation ma-chinery scans transcribed sequences for cleavage locations, it is not appropriate to use random genomic locations as negative set.
Within transcripts, the highly distinct higher order nucleotide composition in coding sequences renders them inappropriate.
Fig.3.
(a) Histogram of Shannon entropy Q-values for all PAS clusters (range from 0 to 9).
Red bars represent entropy values for individual-significant PAS detected by our model, blue bars represent overlap-significant PAS.
Individual sites cluster at 02 entropy, and overlap sites cluster at the upper end of the range.
(b) Example showing that Shannon entropy does not take the abundance of the evidence into account for calling sites significant.
The usage (count) of each PAS cluster is marked in each tissue, followed by the entropy values then the test statistics resulted from our linear model.
Tissue specificity is determined by low entropy values but high test statistics above a certain threshold.
The proximal site in brain (17 tags; 510% of total) is classified as specific (entropy 0.46).
However, this relatively low tag number compared with the overall expression of the gene and the total library depth is not enough to call this site brain-specific.
Entropy values for this proximal site in liver and kidney represent pseudo-counts (not shown in figure).
(c) Example of a specific PAS cluster detected by our linear model and not by Shannon entropy (BDH1 gene on negative strand).
The distal site (65 tags) is the only PAS site for this gene used in liver.
Given the relatively low expression level of the gene and the liver-low library depth compared with brain and kidney, this site is classified as significant (test statistic of our model is marked by red circle).
Shannon entropy values do not reflect this relative usage i111 Modelling of tissue-specific alternative polyadenylation Instead, we built a biologically motivated and challenging nega-tive dataset: for each PAS, we randomly selected 10 positions in the 30-UTR sequence between the transcript stop codon and the PAS, but not including the last 100 nucleotides.
We retrieved the flanking (100,100) regions around these positions to create our negative dataset.
In total, we extracted 2171 positive ex-amples, and 21 710 negative examples.
Because we needed to set multiple hyper-parameters for the SVM and kernels, like order, shift and the classification penalty, we randomly split our dataset into 20% for model selection and 80% for (independent) training and testing (see Section 5).
We applied 5-fold cross validation.
The classifier performance using the two string kernels is shown in Figure 4a and b.
Calculation of the area under the receiver operating characteristic curve (auROC) showed that the WD kernel substantially outper-formed the baseline spectrum kernel (auROC 99.6, 93.5%).
We applied WD on varying window sizes around PAS clusters and found that the high performance largely resulted from fea-tures in the flanking region of (40,40).
Our model parameters indicate that most of these motifs are less than 8-mers long, and shifted within 12nt, which coincides with the findings of (Zhang et al., 2005a).
This suggests that motifs around constitutive polyA sites are highly conserved in both sequence and location, and that the WD kernel is powerful enough to capture this phe-nomenon with near perfect accuracy.
To illustrate the PAS se-quence landscape, we created sequence logos for the flanking regions, which visually showed that the conserved motifs were found in the region (30,30) nt, WebLogo (Crooks et al., 2004), Figure 4c.
The polyadenylation signal and DSE were clearly observed, and the cleavage site itself exhibited a strong BA dinucleotide bias (BC, G or T), in agreement with the previously reported CA dinucleotide.
2.4 Prediction of tissue-specific polyadenylation sites The presence of conserved motifs for constitutive polyA sites suggests the presence of other motifs that instruct the cell to start the polyadenylation process around APA sites in a condition-specific manner.
To investigate this, we first merged all individual-tissueregulated and the two-tissueoverlap PAS clusters and classified them against the positive constitutive data-set (Fig.5a).
The moderate but highly encouraging performance of the classifier on the individual-regulated and the overlap-regu-lated datasets (auROC 74.5 and 66.5%, respectively) support this hypothesis.
We then classified each of the individual tissue-specific PAS clusters against constitutive PASs (Fig.5b).
Brain-individual PAS clusters were highly distinguishable from consti-tutive PASs (auROC 81.5%), while kidney-individual and liver-individual regulated PASs were classified at lower but rea-sonable levels (auROC 72, 63.5%, respectively).
An inspection of the sequence logos of each group explained this performance (Fig.6).
We found an A-rich sequence just downstream of brain-individual regulated PAS clusters that is not present in the con-stitutive subset and other tissue-specific sets.
Moreover, while the canonical PAS signal is still found in liver-individual clusters, making them harder to be classified from constitutive clusters, it is completely absent in brain-individual regulated clusters.
Finally, we trained models to compare each of the individual-regulated clusters in one tissue against all regulated clusters in the other two tissue types (both individual and overlap, Fig.5c).
In agreement with the motifs found at brain-specific individual PAS clusters, classification of brain-specific individual regulated PASs showed the best performance (auROC 71%).
3 DISCUSSION APA is a regulatory process with major impact on the down-stream post-transcriptional fate of affected transcripts, yet it has been fairly sparsely investigated.
Recently, several studies have analyzed data resulting from new high-throughput sequencing protocols, and some studies reported on differential preferences for APA usage in some genes from one tissue to another (Shepard et al., 2011).
However, without a suitable methodology to specify the significance of these events, we may confuse alter-native with specifically regulated polyadenylation.
Using a high-throughput sequencing method particularly de-signed to probe the mRNA 30-end, PA-Seq, we were able to accurately identify polyA sites with high resolution.
PA-Seq data from brain, liver and kidney were collected and constitutive genes were separated from those having more than one APA isoform.
Given the large variability of tag counts across genes and coverage across libraries, simple tag number thresholds or ratios, or information theoretic metrics such as Shannon en-tropy, are not a well-suited methodology for deep sequencing data.
They drastically inflate the number of putative alternative sites, and cannot separate spurious events with little sequence evidence from truly significant ones.
We therefore designed a suitable statistical framework to iden-tify tissue-specific events such as APA sites across multiple deep sequencing libraries.
Using a fixed-effects linear model and per-mutation tests, we were able to assign significance levels to APA usage and identify tissue-specific regulated events.
Our stringent test left us with a highly specific and suitable dataset to investi-gate the regulation of alternative APA, but led to limited sample sizes.
For example, the GFER mRNA showed two polyA sites with the distal site being used in brain only, similar to the find-ings in (Shepard et al., 2011).
However, given its relatively low (a) (c) (b) Fig.4.
(a) ROC curve for the classification of WD kernel and Spectrum kernel on constitutive PAS clusters versus background.
WD outper-formed the Spectrum kernel (auROC 99.6, 93.5%).
(b) PRC curve.
(c) Sequence Logo for (30,30) region around PAS clusters for consti-tutive genes; PAS site at position 0 i112 D.Hafez et al.
read coverage, it did not meet our stringent specificity threshold.
Replicate datasets will enable the use of other statistical tests, which will likely detect a larger subset as significantly different, and may thus help to identify additional regulatory elements that are not covered in our examples.
This study is the first of its kind to analyze multiple APA sites for a transcript and across more than two conditions.
We sepa-rated constitutive genes from genes with multiple APA sites, and examined each group separately.
Our analysis demonstrated that the main cis-regulatory elements described to be responsible for polyadenylation, are a strongand in fact a highly inform-ativehallmark for constitutive sites only.
Studies have shown that 2030% of human genes do not have the canonical PAS signal and suggested that polyadenylation regulation is directed by non-canonical sequences (Tian et al., 2005; Zarudnaya et al., 2003).
Moreover, regulation by non-canonical sequences is more frequent in genes with APA (Nunes et al., 2010; Tian et al., 2005).
In specifically regulated subsets, in particular brain APA sites, we were able to define a highly enriched motif (AAAAAAAAAA) starting just downstream of the PAS cluster (Fig.6a; application of MEME to the brain-specific subset con-firmed its significance, resulting in an E-value of 1.8e-057).
The canonical polyA signal was not observed in brain-specific clus-ters, and was found at lower conservation in liver and kidney.
This agrees with an observation reported in (Nunes et al., 2010), where a polyA site did not possess the canonical polyA signal instead contained an A-rich element in its vicinity.
An analysis of a different recent polyA deep sequencing dataset also showed a roughly 2-fold enrichment of the A-rich motif at brain sites, compared with liver and kidney, despite being generated by a different protocol and processed by a different pipeline (Derti et al., 2012).
Given that the motif is specifically observed in only one tissue within multiple datasets, it is unlikely to be an experimental artifact resulting from internal priming, but we cau-tiously point out that it may reflect a property of brain mRNAs unrelated to polyadenylation.
Our methodology can be applied to data from additional libraries, such as the data generated from applying a high-throughput sequencing protocol on five mammals (Derti et al., 2012).
This will allow for the definition of specific subsets and aid in the identification of further candidates of regulatory sequence features.
Combined with knowledge of regulatory factors affecting polyadenylation and their expression patterns, this will enable the design of models that can build on the encoura-ging tissue-specific results we have reported here.
4 CONCLUSION In summary, we have combined high-quality genome-wide data with appropriate downstream analyses and computational mod-eling.
We have described a successful strategy to identify subsets of significant condition-specific polyA events, built sequence-based models to discriminate between them, and identified new candidates for post-transcriptional regulatory features.
5 METHODS 5.1 Paired-end sequencing and read mapping A new deep sequencing protocol, PA-seq, was used to identify polyade-nylation sites at genome-wide scale.
Briefly, total mRNA is randomly fragmented and reversed transcribed with a modified oligo(dT) primer (a) (b) (c) Fig.5.
Classification of (a) tissue-specific PAS individual and overlap against constitutive.
(b) individual tissue-specific regulated PAS clusters against constitutive.
(c) Each individual regulated PAS cluster in one tissue against all regulated in other tissue types Fig.6.
Sequence logo for tissue-specific individual PAS clusters in each tissue (a) brain-specific, (b) kidney-specific and (c) liver-specific.
PAS site at position 0 i113 Modelling of tissue-specific alternative polyadenylation that base pairs with the polyadenylation tail.
The modified oilgo(dT) primer has a dU in the fourth location in the 30-end to be later digested by USER digestive enzyme.
After that, the double-stranded cDNA frag-ments are captured by streptavidin-coupled magnetic beads, and sequenced using multiplexed paired end sequencing on Illumina (Fig.1).
Adult human normal kidney and liver samples were obtained from BioChain (Cat.
# R1234142-50 and R1234149-50), and brain sam-ples were obtained from Clontech (Cat.
# 636102).
Detailed description of the PA-seq protocol is available on the website.
Before mapping, we filtered out low-quality reads and tags that did not contain the adapter sequence TTT.
The Burrows-Wheeler Alignment Tool (Li and Durbin, 2009) was used to align the paired end reads inde-pendently to the human genome (hg19), allowing two mismatches and no gaps.
After that, we only considered 50 and 30 read pairs that mapped in the same orientation within 250 000nt on the same chromosome.
To investigate the genomic regions that our reads came from, we annotated the 50 aligned reads to their genomic regions using an in-house script (Ni et al., 2010).
We did not use 30 reads for annotation because they might fall beyond the end of annotated transcripts, indicat-ing novel polyA sites.
Locations were classified into six possible cate-gories: annotated 30-UTR, 5 1000nt downstream of 30-UTR, coding region, 50-UTR, intron and intergenic region.
Non-coding genes were ignored, as well as 50 reads that mapped to 50-UTR, intergenic regions, introns or upstream of 30-UTR, as the average insert distance between 50 and 30 paired end reads amounted to 180380bp.
After alignment, non-redundant mapped read pairs that had the same paired end tags were grouped for each unique 30 position, denoting a polyA site.
For each polyA site, the count of the non-redundant 50 pairs were used to indicate the relative usage of this site.
We then filtered out 30 tags that had exactly one 50 paired read (count 1).
Finally, 30 locations that mapped genomic regions with high A-content were filtered out (13 consecutive As in the 25nt downstream of the mapped 30 pos-ition), to exclude possible contamination by internal priming.
5.2 PolyA sites cluster identification To cluster our reads, we used an algorithm previously developed for the analysis of capped 50 mRNA tags (Ni et al., 2010).
Only clusters with tag numbers greater than or equal five were considered.
We then selected Narrow Peak clusters (NP), which span 525nt, with more than half of the reads falling within2nt of the mode.
A minority of 15% showed a broader distribution of tags and was not considered further.
The relative usage (count of the non-redundant 50 pairs) of all of the 30 tags in each PAS cluster was summed up and further used as a measure of the PAS usage, PAScount.
5.3 Identifying constitutive and alternative PAS sets To determine constitutive set, we first grouped all PAS clusters of the same gene from the three tissue types together if their regions overlapped and their modes were within 10nt from the median.
To get the median, we ordered clusters according to their start position, and referred to the PAS by the mode of the median cluster.
If the PAS appeared in two tissue types only, we used the mode of the second start position.
Finally, if the gene had one PAS cluster we called it constitutive; otherwise, it was considered alternative.
5.4 Linear model to identify tissue-specific PAS To determine tissue-specific contributions to PAS utilization, we imple-mented a linear fixed-effects model.
Let Ntg, p denote the PAScount for PAS cluster p for gene g in tissue t. Then logNtg, p  Tt Gg Tt Gg "tg, p 1 where is a general intercept term, Tt is a tissue-specific effect, Gg is a gene-specific effect, Tt Gg is a tissue by gene interaction term and "tg, p is the residual.
There was no need to incorporate random effect terms as we did not have variable replicates.
Because we controlled for different tissue depth, expression of each gene in each tissue, as well as any interaction between tissues and genes, a correlation of the residual for a particular PAS cluster with tissue suggests differences in PAS usages between tis-sues.
To quantify the differential usage of a PAS between tissues, we computed the differences in the residuals "tg, p. We accounted for the lack of usage of a PAS cluster in a certain tissue by adding pseudo-counts.
We applied this model on genes that are expressed in the three tissue types, and fitted the model using Maximum Likelihood approach as implemented in nlme R library (Pinheiro et al., 2011).
5.5 Permutation test to determine P-value Our dataset was composed of three tissue libraries, each with the same set of genes, but different PAS counts.
To preserve library depth and gene expression levels in each tissue, we first calculated the contribution per-centage of each PAS cluster on the gene level [cf.
Equation (1)].
Then, we used these percentages in our permutations, but noted the total expres-sion level of each gene in each library.
Our null hypothesis assumes that PAS clusters are non-tissuespecific regulated.
To model this assumption in our permutation test, in each round, we permuted tissue labels for each PAS cluster; then, for each gene, the percentages of the permuted PAS were used to represent a multinomial distribution, from which we drew a random sample, scaled by the total gene expression value in each tissue.
As the minimal evidence for each cluster was set to five reads, missing values, i.e.
PAS clusters not detected in some of the tissues, were repre-sented by a random number between 1 and 4.
5.6 Identifying individual and overlapping PASs To identify tissue-specific PASs that are highly used in one individual tissue, we used the difference between the highest and the median residual values for each PAS as test statistic.
For each PAS, we computed the test statistic ftigmi1, where m 5357 different PAS.
For each of the observed differences in our data, we obtained a P-value based on an empirical null distribution from 1000 permutations.
P-values were corrected for mul-tiple hypothesis testing using the Storey FDR calculation (Storey and Tibshirani, 2003).
We used a liberal FDR of 0.25, to allow for the dis-covery of significant events given the relatively small number of samples being analyzed.
The tissue-specificity threshold was set to 2.376 (in log space, corresponding to P50.01, FDR50.25); all PASs showing a dif-ference 42:376 were considered significant.
To characterize PAS clusters that are highly used in two tissue types simultaneously (overlapping), we computed the test statistics to be the difference between the mean of the highest two residual values and the lowest value.
PASs with residual difference between tissues 42:782 were considered significant to the two tissues with the highest values (corres-ponding to FDR50.25, P50.011).
5.7 Calculation of VI To explore differences in APA usage among tissues, we calculated a VI that compares the number of individual regulated PAS to overlap PAS.
The VI is defined as follows: VIx, y Ix Iy=Ox, y 2 where VIx, y is the VI between tissue x and tissue y, Ix and Iy are the number of individually regulated tissue-specific PAS clusters in tissue x and tissue y, respectively, and Ox, y is the number of overlapping regu-lated tissue-specific PAS clusters in tissues x and y simultaneously.
A low value of VI between a pair of tissues indicates a high degree of correlation in APA regulation, whereas a high value of VI indicates a weak i114 D.Hafez et al.
correlation.
The calculated indices for each pair are VILiverKidney 1.44, VIBrainLiver 2, VIBrainKidney 5.09.
5.8 Calculation of Shannon entropy We assessed tissue-specific APA in the three tissue types by calculating Shannon entropy on the count of each PAS cluster identified in each tissue, according to (Schug et al., 2005).
We only considered genes that were expressed in the three tissue types, i.e.
that had at least one PAS cluster annotated for each tissue.
We determined the relative expression of each PAS cluster of a gene as follows: wtg, p Ntg, p 1=Ntg xg, p 3 where Ntg, p the PAScount for PAS p for gene g in tissue t, Ntg is the summation of PAScount for all PASs of gene g in tissue t and xg, p is the number of different PAS clusters for gene g. Next, we computed the probability of observing a PAS cluster in each tissue by Pt j p wtg, p= X t wtg, p 4 Calculation of entropy values followed (Schug et al., 2005).
Entropy values close to zero represent the group of PAS clusters that are specific to a single tissue, and increase when the PAS cluster is more broadly used in different tissue types, or when the relative contribution of the tissue to the overall usage of the PAS decreases (Schug et al., 2005).
5.9 Dataset for constitutive classification against background Our dataset is best described as a set of sequences, each is composed of an array of characters A, C, G, T. The length of each sequence is 201 char-acters.
For the positive training data, the element at position 101 repre-sents the polyA site (median of the PAS cluster).
We chose a flanking region of 100nt upstream and downstream of the mode of the PAS clus-ter because previous studies have shown that most of the main features of polyA sites are located in this region (Cheng et al., 2006).
We refer to the 101th position as 0, upstream sequences as (100,1), and downstream sequences as (1,100).
We restricted our dataset to include PAS clusters for genes that are expressed in the three tissue types.
To choose a biologically motived background/negative dataset, for each true PAS mode in our PAS cluster positive dataset, we randomly selected 10 positions downstream of the stop codon, but did not include the 100nt just upstream of the mode of the PAS.
If the gene does not have an annotated stop codon, we select positions from the last 500nt but not including the last 100nt upstream of the mode of the PAS.
We then retrieved the sequence of the 100 nt upstream and downstream of these selected sites to compose our negative dataset.
5.10 String kernels and SVM Kernel functions measure the similarity between different data points in the feature space.
For our purposes, the similarity is between two seg-ments of DNA sequences with the same length.
As noted earlier, the main cis-regulatory elements responsible for polyadenylation are located in the flanking region (40,40) nt from polyA sites, while further downstream and/or upstream (100,100) of the polyA site lie some other G/U-rich segments of sequences, with varying length, location and exact sequence compositions.
The spectrum kernel considers the global similarities be-tween two given sequences, by counting the number of occurrences of k-mer motifs (referred to as order in Section 5.12) over the entire sequence.
The Weight degree kernel with shifts focuses on local similarities between the given sequences by counting the number of matching k-mers at the same positions, within a window around the matching position (referred to as shift).
We applied both string kernels on the region (100,100).
5.11 Handling unbalanced data Our negative dataset has 10 times more examples than the positive set.
This unbalanced dataset could be challenging for classifiers; because the data is unbalanced, the cost of misclassification is also unbalanced; thus, a false negative is more costly than a false positive (Ben-Hur et al., 2008).
Therefore, we assigned relative misclassification penalty, C, for each set according to its number of examples; for positive training data, C is 10 times larger than that of the negative training data (Provost, 2000).
5.12 Model selection To settle on the combination of parameters, which represent our models ability to accurately distinguish the surrounding sequence of polyA sites from other genomic loci, we applied model selection.
The four parameters to be optimized are (i) misclassification penalty or the SVM (C), (ii) length of the substrings compared (order), (iii) positional shifts/window around polyA site for WD kernel and (iv) length of the flanking region around the PAS.
We tried different values for each of these parameters, while fixing the rest.
To avoid over-fitting, first, we randomly split our data; 20% for model selection and 80% for training and testing.
These two sets were kept independent of each other.
In the model selection phase, we applied 2-fold cross validation, and selected parameters that gave the highest auROC.
The optimal values for each parameter is shown in Table 2.
We then used the selected parameters in the training and test phase by applying 5-fold cross validation.
Evaluation curves were drawn using ROCR package (Sing et al., 2005).
5.13 SVM on tissue-specific regulated PAS clusters In this experiment, our positive examples were the set of individual and overlap tissue-specific sites, and negative examples were constitutive sites, expressed in the three tissue types and with exactly one PAS cluster.
As the WD kernel clearly outperformed the spectrum kernel on the recogni-tion of constitutive sites, we only used the WD kernel for the rest of our analyses.
Funding: This project was funded by a grant from the National Science Foundation (MCB-0822033).
Conflict of Interest: none declared.
ABSTRACT Motivation: As the use of microarrays in human studies continues to increase, stringent quality assurance is necessary to ensure accurate experimental interpretation.
We present a formal approach for microarray quality assessment that is based on dimension reduction of established measures of signal and noise components of expression followed by parametric multivariate outlier testing.
Results: We applied our approach to several data resources.
First, as a negative control, we found that the Affymetrix and Illumina contributions to MAQC data were free from outliers at a nominal outlier flagging rate of = 0.01.
Second, we created a tunable framework for artificially corrupting intensity data from the Affymetrix Latin Square spike-in experiment to allow investigation of sensitivity and specificity of quality assurance (QA) criteria.
Third, we applied the procedure to 507 Affymetrix microarray GeneChips processed with RNA from human peripheral blood samples.
We show that exclusion of arrays by this approach substantially increases inferential power, or the ability to detect differential expression, in large clinical studies.
Availability: http://bioconductor.org/packages/2.3/bioc/html/array Mvout.html and http://bioconductor.org/packages/2.3/bioc/html/ affyContam.html affyContam (credentials: readonly/readonly) Contact: aasare@immunetolerance.org; stvjc@channing.harvard.edu 1 INTRODUCTION Recent successes with microarrays for the identification of gene patterns that correlate with disease states has resulted in their increased use in human studies.As this approach moves from smaller scale efforts into biomarker discovery efforts in clinical trials, rapid and reliable quality assurance approaches are necessary, both from the perspective of ensuring accurate data for inclusion as clinical trial secondary endpoints, and as a measure to contain costs (Group, 2004).
As a large clinical trial consortium, we have processed over 1500 Affymetrix Gene Chips from eight different clinical trials, with these numbers growing yearly.
Thus, we required a streamlined and accurate method for defining which arrays are of high quality To whom correspondence should be addressed.
The authors wish to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
and to determine the overall success rate of arrays processed in our central laboratory.
Many of the currently used microarray quality assessments are manufacturer-based recommendations that rely on a limited number of parameters with imprecise specifications for acceptable results.
In this report, we review definitions of quality assessment criteria for Affymetrix and Illumina expression arrays, and describe algorithms for formally identifying aberrant arrays with a specified false positive rate.
Our procedure involves parametric multivariate outlier testing using a multivariate Gaussian model, which we apply to principal components of the quality measure matrix.
2 METHODS 2.1 Definitions of quality metrics For Affymetrix Gene Chips, quality criteria include the actin (HSACO7) and GAPDH 3/5 ratios, the percent present calls according to the MAS5 algorithm, the array-specific scale factor and average background [see Hubbell et al.
(2002) and the Affymetrix Statistical Algorithm Reference Guide].
The actin and GAPDH ratios indirectly reflect the efficiency of reverse transcription of the total RNA template, as high 3 to 5 ratios indicate poor transcription from the 3-end, resulting in small fragments of cDNA/cRNA available for hybridization.
The other indicators reflect hybridization efficiency as they indicate the number of probes hybridized from the total (percent present call) and the amount of background noise.
Bolstad et al.
(2003) define probe-level modeling (PLM) for Affymetrix arrays as a flexible generalization of the established robust multiarray preprocessing procedure (RMA) due to Irizarry et al.
(2003).
PLM computes two quantities of particular interest for quality assessment.
The first, relative log expression (RLE), examines point estimates of expression values at the probe set level.
Suppose, here and in the sequel, that there are N arrays and G probe sets.
PLM computes eij , i = 1, ...,G, j=1, ...,N , on the basis of a robust regression model allowing general probe and sample effects.
The quantity ei, is the median log expression of the i-th probe set.
RLEij = eij ei,, computed for all probes, all arrays.
Denote by RLEj the G-vector of RLE measures for the j-th array.
Features of the distribution of RLEj are informative on array quality: medians should be close to zero for all arrays, and variances should be similar across arrays.
Departures from these conditions are usually associated with quality defects.
Another quantity derived from PLM is the normalized unscaled standard error (NUSE).
Here, the focus is on variability of estimation of expression.
PLM computes standard errors of all expression measures, and these are standardized, gene by gene, across arrays, so that the median standard error across arrays is 1.
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Parametric array outlier testing The G-vector of NUSE for each array should have median 1 and common variance across arrays.
Another indicator provided by the affy package of Bioconductor/R (Gautier et al., 2004) is the RNA degradation slope, which assesses the 3/5 ratio for all genes on the array.
Probes are ordered within probe sets from 5-to 3-most and the intensity gradient along this sequence is qualitatively estimated pointwise for each array.
Arrays with quality problems can possess aberrant gradients, typically assessed visually.
For Illumina arrays, the Bioconductor lumi package (Du et al., 2008) includes the lumiQ function which computes a four-dimensional quality feature vector for each array in a lumiBatch.
The quality measures are average intensity and intensity SD, average detection rate as reported by the BeadStudio preprocessor and average Euclidean distance of probe-specific intensities to their means over all samples.
2.2 Array quality feature-vector and dimension reduction For Affymetrix arrays, the metrics described above lead to nine quantitative measures of array quality computed on each array: ABG (average background), SF (scale factor), PP (percent present), AR (actin 3/5 ratio), GR (GAPDH 3/5 ratio), median NUSE, median RLE, RLE-IQR (interquartile range of IQR per array, to measure variability in RLE) and RNAS (slope of RNA degradation measure).
This sequence of numbers is regarded as the array-specific quality feature vector.
A principal components transformation is conducted using R function prcomp to obtain linear combinations of the original features that are mutually orthogonal and that capture substantial fractions of variation among samples.
The use of linear combinations also aids in procuring a multivariate representation of the quality information that may, under a null hypothesis of equivalent quality, be reasonably approximated by a multivariate Gaussian probability model.
We denote by Q the N 9 feature matrix (with arrays defining rows and quality measures defining qualities).
The matrix consisting of the user-selected m>1 initial principal components of Q as columns is denoted as P. For Illumina arrays, we denote by Q the N 4 feature matrix of lumi-based quality measures, to which principal components transformations may be applied.
2.3 Multivariate outlier detection algorithms If there are r > N/2 non-outlying rows of the N m matrix, P are regarded as a collection of r samples from the m-dimensional Gaussian model Nm(,) with m-dimensional mean and mm covariance matrix , an algorithm of Caroni and Prescott (1992) can be used to identify outlying observations among the N data rows with a fixed small probability of incorrectly labeling a non-outlying observation as an outlier.
Let S denote the m m sample covariance matrix of the m first principal components of Q [taking the m columns of P as variables and the N rows of P as observations (denoted Pi, i = 1, ...,N)].
Wilks scatter ratios are the quantities Wl =|S(l)|/|S|, l=1,...,N, where S(l) denotes the sample covariance matrix of P computed with row l removed.
Wilks scatter ratio is the likelihood ratio test statistic of H0 :Pi Nm(,), i=1,...,N against H1 :Pi Nm(,), i = j, and Pj Nm(+aj,) where the outlier index j, the m-dimensional slippage parameter aj and the variance parameter are all unknown.
The scatter ratio may be re-expressed as a function of the Mahalanobis distance: Wl =1 N N 1 (Pi ) tS1(Pi ).
Caroni and Prescott implement an inward peeling followed by outward testing procedure following a univariate procedure due to Rosner (1983).
Define D1 =minj(Wj), and D2,...,DNr as the sequence of scatter ratios formed from successive eliminations of rows of P posessing the smallest scatter ratios at each stage.
Caroni and Prescott observe that the distribution of Wj for given j is Beta([np1]/2, p/2) and derive Bonferroni bounds using Rosners arguments to obtain approximations to the distributions of Ds, s=1,...,N r. Critical values are obtained using these approximations and the quantities DNr ,...,D1 are each tested at level .
If DNq is smaller than the associated critical value, then the associated sample and all samples associated with minimal scatter ratios computed at stages earlier than N q are declared outlying.
This procedure mislabels non-outlying samples as outliers with overall error rate whether or not any outliers are actually present in P, and is implemented in R source code in the arrayMvout package.
In tables below, we refer to the results of using this procedure as PMVO (for Parametric MultiVariate Outlier labeling).
An important recent contribution to array outlier assessment is the methodology of Cohen Freue et al.
(2007) in which robust Mahalanobis distances are used to identify aberrant arrays.
The standard Mahalanobis distance can be robustified by substituting for the sample covariance a covariance estimator based on the minimum covariance determinant, minimum volume ellipsoid or a specific S-estimator.
Parametric or simulation-based critical values for outlier labeling are available.
In comparative tabulations below this algorithm is denoted MDQC.
2.4 Implementation The arrayMvout package includes a function ArrayOutliers that accepts an AffyBatch (imported intensity data structure derived from CEL files) or lumiBatch instance, specification of and a vector indicating which principal components are to be used to define P.ArrayOutliers returns an instance of the arrOutStruct class, for which a simple reporting method is defined, showing the indices and feature values of outlying arrays; a plot method returns the principal components biplot of Q.
The object quietly maintains results of all quality metric computations, such as the fitPLM result, facilitating more detailed diagnosis should such be required.
2.5 Power estimation PowerAtlas, a power and sample size estimation tool for microarray study, was adopted to investigate impact of aberrant arrays on statistical power for identification of differentially expressed genes (Page et al., 2006).
The PowerAtlas tool is developed based on a mixture model approach for estimation of power and sample size of high-dimensional data (Gadbury et al., 2004).
A list of per-gene P-values, generated from a pair-wise comparison using LIMMA package, was used as input for PowerAtlas for power estimation of the comparison.
3 RESULTS 3.1 Negative control: application to MAQC arrays The Affymetrix contribution to the MAQC expression study consists of 120 Affymetrix hgu133plus2 arrays collected from three MAQC labs with two replicates on each of four sample types.
These arrays were produced under a strict protocol for the MAQC cross-platform comparison (Shi et al., 2006) and so are expected to be of very high quality.
Our outlier algorithm was applied with m=3 and =0.01,0.05 and 0.10 with two approaches to feature representation.
As requested by two referees, we applied outlier detection to the raw quality features without principal components re-expression.
The table entry under PMVO-raw shows that relatively large numbers of outliers are flagged with this approach.
When principal components re-expression is applied, no outliers are detected with PMVO at = 0.01,0.05 or 0.10.
49 A.L.Asare et al.
Table 1.
Application of multivariate outlier detection to negative and positive controls derived from MAQC and Affymetrix spike-in series, the latter with digital contamination Negative controls Source No.
of chips No.
of chips flagged PMVO-raw PVMO-PC MDQC-PC Affy.
MAQC 120 (34,34,23) (0,0,0) (9,3,1) Illu.
MAQC 19 (0,0,0) (0,0,0) (3,1,0) Digitally contaminated arrays Source No.
of chips Contaminated Chips flagged PMVO-PC MDQC Affy.
spike-in 12 none 2,8,10 1 1 1,8 1,2 1,2,8 1,2 1,2,11 1,2,8,11 8,10 For negative controls, table cells give number of arrays flagged at =0.10,0.05,0.01.
For positive controls, cell entries give indices of arrays contaminated or identified by various algorithms.
Method labels are: PMVO-raw, for parametric multivariate outlier detection applied to raw QA features; PMVO-PC, for PMVO applied with dimension reduction to first three principal components; MDQC, for Mahalanobis distance-based algorithm of Cohen Freue et al.
(2007) with the MCD estimator of covariance, applied to raw QC features; and MDQC-PC, for MDQC with the S-estimator of covariance applied on PC1PC3 of QC features.
We obtained the raw MAQC data contributions from Illumina, Inc. (Le.
Shi, personal communication), and created raw reads of 19 arrays using the lumiR procedure of the lumi package of Bioconductor (Du et al., 2008), and then computed the quality measures via the lumiQ procedure.
Table 1 shows that PMVO finds no outliers, while MDQC finds a small number as long as 0.05.
We conclude that PMVO has reasonable specificity when used in conjunction with principal components re-expression for arrays produced in good quality conditions.
3.2 Sensitivity to specific contamination events Figure 1 displays a digitally contaminated CEL file from the Affymetrix spike-in experiment archive.
The readily visible artifacts are created by altering intensity levels in specific regions of the chip either by fixing them to constant value or by rescaling them to achieve altered intensity variance.
For conciseness, the figure shows a chip on which four contamination events occur simultaneously; in our data analyses these were applied separately to different subseries of chips as shown in Table 1.
The base series is the first 12 chips in the U133A spike-in subset.
Each of four types of artifact were introduced to chip 1, then chips 1 and 2, and then chips 1, 2 and 11, to understand the effects of artifact type and number on sensitivity of outlier detection algorithms.
We see that when there are no artifacts, PMVO does not declare any array to be outlying, but MDQC (run with default settings) declares three arrays to be outliers.
Results for PMVO and MDQC were invariant to the type of artifact when only 1 or 2 chips were contaminated, so contamination type is not recorded in Table 1.
When three chips were contaminated (1, 2 and 11), Fig.1.
Composite of four types of digital contamination applied to raw Affymetrix intensity datathe three circular subregions are, counterclockwise from upper left, low constant, variable and high constant blobs, and the rectangular region on the right has inflated variance.
PMVO always flagged chips 1, 2, 8 and 11, regardless of the type of contamination.
MDQC was sensitive to type; in the table we show that it flagged chips 8 and 10, as was true for low constant and increased variability blobs; with the high constant blob MDQC flagged 10 and 11, and for rectangle of increased variability, MDQC flagged 1 and 8.
Further study of differential sensitivity to artifact type will be warranted.
3.3 Large-scale applications We applied this method to identify aberrant arrays from a pool of 507 microarrays from multiple clinical studies conducted by the Immune Tolerance Network (ITN).
This dataset was generated from clinical human studies using peripheral blood samples, representing a very different experimental setting from the MAQC study.
There is higher variability and little to no ability to perform technical replicates due to cost and sample limitations.
In the human peripheral blood RNA microarray sample set, 18 microarrays, or 3.5%, were detected as outliers at =0.01 (Fig.2).
We confirmed this approach to outlier detection by plotting the distribution of arrays by individual QA indicator (Fig.1) with the outlier arrays highlighted in red.
As shown, blue traces correspond to arrays of high quality that fall within acceptable QA ranges as previously described (GeneChip Expression Analysis Technical Manual, www.affymetrix.com).
Arrays that the approach flagged as outliers (Fig.2) do not fall within the acceptable range for at least one if not more of the QA parameters.
To demonstrate that our outlier detection method improves the ability to determine meaningful changes in gene expression (differential gene expression), we assessed the impact of our outliers on gene expression estimates (absolute measures that are averaged across all probes for a particular gene).
Our assessment focused on two trials where we expected differential expression based on clinical phenotype or treatment regimen.
In the first case, we used a set of 204 arrays from a ragweed allergy clinical trial.
Our method identified five outliers (Fig.3A, shown in red and designated as AE) that are coincident with five points outside the major grouping 50 Parametric array outlier testing Fig.2.
Parallel coordinate plots are a common way of visualizing multi-variate data with different scales to facilitate detection of outliers.
Applying our QA approach, 18 of the 507 microarrays were flagged as aberrant (highlighted in red).
As shown, our approach to QA has selected samples as problematic where one or more indicators appear as an outlier based on reduction of the dimensionality of the data via PCA and applying a sequential Wilkss multivariate outlier test at an = 0.01.
Our approach provides greater consistency in designating problematic arrays through a statistical framework that does not rely on arbitrary cutoffs for any individual indicator.
of samples on plots of PCA gene expression estimates.
For outliers ABC, this was due to poor (high) NUSE values, with only array A having poor (low) percent present calls.
Outliers D and E had normal percent present calls and NUSE values, but poor GAPDH , actin (HSACO7) and RNA slope values.
Our second case study used a set of 42 microarrays from a kidney transplant trial, in which three patient cohorts were compared.
Here, we detected three outliers using our method.
These outliers also correspond to arrays which fall outside the primary cluster on PCA gene expression estimate plots (Fig.3B Points FH).
In this case, all three points falling out of the cluster had poor GAPDH, actin (HSACO7) ratio, RNA slope, and NUSE values.
Array F had a poor percent present call, while G and H did not.
While the outlier arrays detected by our method correspond to arrays that are outside the cluster of gene expression estimate PCA plots, our approach provides improved discriminatory power in that it permits discrimination of arrays that are outliers due to poor array quality compared with those that are likely to reflect actual biological variance.
3.4 Improvement of statistical power The benefit of excluding poor quality arrays in differential expression analyses was further demonstrated through statistical power calculations for these two clinical trials.
Statistical power is a major limiting factor in clinical microarray studies due to limited sample size and lack of technical replicates.
Power calculations were performed, using PowerAtlas (Gadbury et al., 2004), both with and without the two array outliers (Fig.3A, Points B and C) identified by our method.
These two microarrays correspond to data points that were collected to assess treatment effect.
The other three outliers in Fig.3A addressed seasonal effects of the study.
Gadburys procedure fits a two-component 110 83 56 29 2 25 51 78 105 132 160 670 579 489 399 309 220 129 39 50 140 230 PC #1 21.9% P C # 3 4.
34 % A A B B C D E 210 172 134 96 58 20 17 55 93 131 170 690 608 527 446 365 285 203 122 41 39 120 PC #1 P C # 2 11 .5 % F G H Fig.3.
PCA was applied to gene expression estimates for all genes in two clinical trials.
(A) The outlier detection approach described was applied to 204 arrays from a ragweed allergy study and identified five samples.
These microarrays are highlighted in red in the PCA 1 versus PCA 3 plot for gene expression to show the relationship of outlier samples detected by the system to actual gene expression estimates per array.
Points A, B and C have problematic NUSE values.
Points D and E have abnormally high GAPDH and HSAC07 ratios.
The location of these arrays based on gene expression PCA suggests that QA problems may contribute to deterioration of overall expression.
(B) A kidney transplant trial with 42 arrays where three were detected as outliers.
The three arrays are highlighted in red in the PCA 1 versus PCA 2 gene expression plot.
Points F, G and H have abnormal NUSE, GAPDH and HSAC07 ratios.
Again, the samples flagged by the QA approach appear to have gene expression estimates that differ from the majority of other arrays.
The arrayMvout package includes a map fig3map from records in the ITN QA metrics matrix to samples labeled AH in these figures.
mixture of Beta variates to the distribution of P-values of gene-specific differential expression tests.
The two components of the Beta model for P-values correspond to (i) the distribution of P for non-differentially expressed genes (which will be uniform), and 51 A.L.Asare et al.
A B Fig.4.
Statistical power calculations.
(A) Ragweed allergy study showing improved EDR upon removal of arrays flagged as Points B and C in Figure 3A comparing two time points of interest.
(B) Kidney transplant study showing removal of two arrays flagged as F and G in Figure 3B in a differential expression comparison of two treatment cohorts.
(ii) the distribution of P for differentially expressed genes (which will have mass predominantly near zero).
Parametric bootstrapping is then used in conjunction with the mixture model fit to relate the sample size of an experiment to the operating characteristics of tests for differential expression.
Gadbury defines the expected discovery rate (EDR) as the expected proportion of genes that are truly differentially expressed that will be declared to be differentially expressed under a given design.
If differential expression is present, but tests of differential expression are impaired by the presence of poor quality arrays, the P-values obtained will not be readily resolved into two components, and power will be diminished.
With the arrays of low quality included, Gadburys estimate of the EDR [with false discovery rate (FDR) = 0.05] was 24.2% at a sample size of 10, whereas the EDR improved to 73.3% for the same sample size, same FDR, through exclusion of the poor quality arrays (Fig.4A).
Similarly, in our kidney transplantation case study, we compared two of the three cohorts; each of these two cohorts had one outlier (Fig.3B, Points F and G).
Power calculations with outlier arrays both included and excluded showed that the EDR reached 14.1% and 74.2%, respectively for a sample size of 30 (Fig.4B).
4 CONCLUSION While microarray post-hybridization quality indicators are readily generated via standard output from analysis software packages, cutoffs used to identify problematic arrays have typically been subjective and arbitrary in nature.
These indicators, by themselves, do not always give the level of discrimination needed to distinguish microarrays that are poor in quality.
We have proposed a three-step procedure for decision-making about array quality.
First, choose a collection of quantitative quality metrics.
For Affymetrix expression arrays, we have identified nine metrics that appear to have reasonable utility, and for Illumina expression arrays we use four metrics that are routinely computed by open source software.
These metrics can be supplemented or restricted as desired by users.
Second, compute the principal components re-expression of the metrics and reduce the quality data to a modest number of components.
This step pursues parsimonious integration of the various metrics and yields a multivariate quality representation that should be reasonably approximated by the multivariate Gaussian model.
Third, apply calibrated parametric multivariate outlier detection to a subset of the resulting quality principal components.
We propose Caroni and Prescotts generalization of Rosners GESD procedure, show that it has reasonable specificity and sensitivity in several contexts, and indicate that its use leads to increased inferential power in an important clinical application.
Despite the attractive features identified above, prospective users of statistical outlier labeling in microarray contexts need to be cautious in their application of these methods.
It is a commonplace that the outliers are often the most interesting records in any given database.
Because we are studying outlyingness with respect to average quality, the outlying arrays may be informative about important events or discrepancies in the overall processing workflow.
It is also inevitable that the procedure described here has several components conferring flexibility, and, consequently, manipulability.
There is no basis at present for objective choice of base quality metrics, for the choice of number of principal components to use in reduction, or for the choice of null outlier labeling rate that will lead to greatest confidence that arrays labeled as outliers are truly aberrant and that unlabeled arrays are of adequate quality.
Thus, it is possible that two users may obtain different decisions on identical data.
We have designed the ArrayOutliers tools so that they are reasonably self-documenting, so that all applications can be audited.
There are various avenues along which the work described here should be extended.
First, by working with larger numbers of arrays that have been independently classified into acceptable and unacceptable states, it should be possible to analyze the contributions of different quality metrics to probabilities of class membership.
Second, when large numbers of arrays that are known to be of good quality are available, the outlier detection process can be supplemented by a reference database.
Specifically, parametric outlier testing can be conducted on the basis of a fixed null mean and covariance for quality features derived from a family of arrays known to be of high quality.
The arrayMvout package includes matrices of quality measures for the Affymetrix and Illumina MAQC contributions, which can support exploration of this reference-based testing concept.
52 Parametric array outlier testing ACKNOWLEDGEMENTS We acknowledge the constructive comments of referees and an associate editor.
Funding: Subcontract from the Immune Tolerance Network; a project of the National Institute of Allergy and Infectious Diseases; The National Institute for Diabetes, Digestive and Kidney Diseases; Juvenile Diabetes Research Foundation.
National Institute of Health (P41 HG004059-01 to V.J.C.).
Conflict of Interest: none declared.
ABSTRACT Motivation: The nucleosome is the basic repeating unit of chromatin.
It contains two copies each of the four core histones H2A, H2B, H3 and H4 and about 147 bp of DNA.
The residues of the histone proteins are subject to numerous post-translational modifications, such as methylation or acetylation.
Chromatin immunoprecipitiation followed by sequencing (ChIP-seq) is a technique that provides genome-wide occupancy data of these modified histone proteins, and it requires appropriate computational methods.
Results: We present NucHunter, an algorithm that uses the data from ChIP-seq experiments directed against many histone modifications to infer positioned nucleosomes.
NucHunter annotates each of these nucleosomes with the intensities of the histone modifications.
We demonstrate that these annotations can be used to infer nucleosomal states with distinct correlations to underlying genomic features and chromatin-related processes, such as transcriptional start sites, enhancers, elongation by RNA polymerase II and chromatin-mediated repression.
Thus, NucHunter is a versatile tool that can be used to predict positioned nucleosomes from a panel of histone modification ChIP-seq experiments and infer distinct histone modification patterns associated to different chromatin states.
Availability: The software is available at http://epigen.molgen.mpg.de/ nuchunter/.
Contact: chung@molgen.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 20, 2013; revised on June 24, 2013; accepted on August 1, 2013 1 INTRODUCTION The genome of eukaryotes is packaged into a macromolecular structure called chromatin.
The basic repeating unit of chromatin is the nucleosome, which contains two copies each of the four core histones H2A, H2B, H3 and H4, around which a 147-bp stretch of DNA is wrapped in a flat left-handed superhelix (Luger and Richmond, 1998).
Nucleosomes form approximately every 200 bp along the genome to package the underlying DNA.
Apart from the packaging function, nucleosomes may serve as a signaling module (Turner, 2012) that is integrated into biological processes acting with and on chromatin.
This signaling function depends on post-translational modifications of the histone proteins, such as acetylation and methylation of lysine residues.
These histone modifications may serve as a binding platform for non-histone proteins, whose activities change chromatin struc-ture and function.
When nucleosomes tend to form at the same or nearby gen-omic positions in different cells, they are called (well) positioned.
Positioned nucleosomes are important for the hypothesis that nucleosomes constitute a signaling module, because gross move-ments of modified nucleosomes along the chromatin fibers may lead to a loss of coherence between the modifications and the genomic features and/or functions.
The binding locations of modified histone proteins can be determined by a technique called chromatin immunoprecipitation followed by sequencing [ChIP-seq; Johnson et al.
(2007)].
The immunoprecipitation step enriches for chromatin fragments con-taining a histone modification of interest, whereas the sequencing step is used to quantify the abundance of the underlying DNA.
Because the core histone proteins are part of a stable protein DNA complex, it is natural to assume that the localization of modified histone proteins corresponds to the position of the nu-cleosomes.
This suggests that histone modification ChIP-seq data can be used to infer nucleosome positions.
However, this is far from being a trivial task for a number of reasons: (i) histone binding does not seem to be as sequence-specific as for many transcription factors; (ii) nucleosome positions can change con-siderably with time and across cells; and (iii) the data are affected by sparse sampling and high noise.
Nucleosome calling algorithms, such as the one presented here, aim at detecting positioned nucleosomes.
To obtain a com-prehensive and reliable set of predictions, one should combine the information contained in as many ChIP-seq experiments as possible and allow for some plasticity in the shape of the signal.
However, modified histones tend to be mixed-source factors (Landt et al., 2012), which means that the degree of positioning can vary considerably across the genome.
In regions where nu-cleosomes occupy different positions in different cells (e.g.
within the body of actively transcribed genes), nucleosome calling algo-rithms are less suitable than segmentation approaches [Song and Smith (2011); Zang et al.
(2009), to mention a few], which aim at detecting domains of high nucleosome abundance.
A number of tools for the inference of nucleosome positions have already been developed.
Most of them apply signal process-ing techniques, such as Fourier transforms (Flores and Orozco, 2011), wavelet decomposition (Zhang et al., 2008a) and ad hoc filters (Albert et al., 2007; Weiner et al., 2010), to smooth the enrichment profile, followed by the detection of local maxima.
Others are based on Bayesian modeling of the nucleosome*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com bas e pair s base pairs ( ) ile Since very-is for example, ) in order  enrichment pattern (Zhang et al., 2012).
These methods do not allow one to control for systematic biases by comparing the nu-cleosome calls with data from control experiments.
Furthermore, they cannot integrate data from multiple histone marks in a straightforward manner.
Finally, because of the large size of the problem, e.g.
the human genome, and the potentially high number of histone modifications, the runtime and memory con-sumption of these tools may limit their applicability.
Our tool can use information from a control sample to correct for systematic biases inherent in this high-throughput technol-ogy.
It is designed to integrate multiple histone marks to broaden the range of nucleosome positions that can be detected.
It anno-tates each identified nucleosome with the contributing histone modifications.
We will demonstrate that these annotations can be used to cluster nucleosomes by their histone modification patterns.
This clustering yields patterns of modifications that can be correlated to the function of the chromatin, such as tran-scriptional start sites and enhancers, or to the underlying process, such as transcriptional elongation by RNA polymerase II.
These results support the assumption that nucleosomes serve as signal modules for biological process and that the corresponding his-tone modification patterns are a reflection of the signaling taking place on these modules.
2 METHODS The algorithm performs three major steps: (i) a preprocessing step, where each file containing the chromosomal positions of mapped reads is turned into a numerical signal, (ii) a shape detection step, where candidate pos-itions for nucleosome formation sites are detected and (iii) a filtering step, where these candidates are filtered and scored accounting for a number of possible sources of bias.
In the following, we will refer to the enrichment profile on the positive or negative strand as the signal that counts for each location the number of positive or negative reads whose 50 end maps there, and they will be denoted P(p) and N(p), respectively, where p is the chromosomal position.
2.1 Preprocessing A well-positioned nucleosome typically exhibits the enrichment profile shown in Figure 1: a peak of positive strand reads upstream of the nucleo-some location, and one of negative strand reads downstream.
To obtain a consensus signal, which will be called the input signal I, the enrichment profile on the positive strand P is shifted to the right, the one on the negative strandN is shifted to the left and the sum of the two is considered.
Denoting with F the average length of a fragment in the DNA library, the amount of this shift is about F=2, which yields the input signal: Ip Pp F=2 Np F=2: In case of single-end sequencing data, usually the average fragment length needs to be estimated from the data itself.
This estimation can be carried out by several available tools [such as Zhang et al.
(2008b)].
However, because of the mixed source nature of the data, we found the available methods unsatisfactory when applied to histone marks, and therefore, as part of NucHunter, we also provide a method for estimating the average fragment length (described in Section 2.4).
2.2 Peak detection In the peak detection step (see Fig.2), a suitable filter is applied to the input signal, followed by the detection of local maxima in the filtered signal and the analysis of the statistical significance of these maxima.
A filter (more precisely a linear time-invariant filter) is characterized by a discrete signal K(p) called impulse response.
Given an input signal I(p), the filter output O(p) is the result of the following operation, called convolution: Op I Kp X j Ip j K j , where the index j ranges over positions where K(j) is not 0.
The impulse response in our approach has been chosen according to the following two criteria: first, it must separate sharp peaks from more spread out read distributions or non-enriched regions; second, it must have good smoothing properties, so that the convoluted signal contains a limited number of local maxima (Rice, 1944) and, therefore, the algo-rithm returns fewer false positives.
We chose as impulse response the second derivative of a Gaussian density function, also known as the Mexican hat wavelet (see Fig.3): Ki 1 i 2 2 e i2 22 : TheMexican hat wavelet removes from the Fourier spectrum of the input signal both high-and low-frequency components (band-pass filter), which is appropriate if we interpret high frequencies as random oscillations Fig.1.
Preprocessing: from mapped reads to consensus signal.
Positive and negative reads generate a strand specific enrichment profile which counts at each position the amount of reads whose 50 end maps there.
The consensus signal is obtained by shifting the strand specific enrichment profiles F/2 bases downstream, where F is the average fragment size, and summing them up Fig.2.
Peak detection from the consensus input signal.
The input signal is smoothed using a filter with a certain impulse response, then the maxima of the resulting signal are detected and non-significant local maxima are filtered out 2548 A.Mammana et al.
due to which P , ' , ( ) due to , zero , ly because of noise or insufficient coverage, and low frequencies as broad ambiguous peaks coming from a mixture of nucleosome positions, or as local biases such as GC content or open chromatin.
The wavelet is para-metrized by the scale parameter .
In our studies, we chose a default value of 50 for because, in general, it corresponds to a good compromise between calling too many peaks and merging closely spaced ones.
The parameter can also be fitted to the dataset under consideration using the method outlined in Section 2.4.
Obtaining the convoluted signal for large genomes poses computa-tional problems.
In fact, a long signal as impulse response results in a slow convolution operation.
In NucHunter the convolution has been implemented using recursive filters, an efficient signal-processing technique (Hale, 2006).
Once local maxima are extracted from the filter output, their statistical significance is assessed.
To this end, we model the noise by assuming that values of the input signal within a certain region are independent identi-cally distributed random variables (rvs).
Using this assumption, we derive the mean and standard deviation of the convoluted signal, and we assign a z-score to each local maximum.
If I(p) denotes the input signal, K(p) the impulse response and O(p) the convoluted signal at position p, let m(p) and std(p) denote, respectively, the mean and standard deviation of the input signal in a large region R that contains position p, then the z-score is given by: mp X k2R Ik jRj , stdp ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiX k2R Ik mp2 jRj 1 vuut , z-scorep Op mp P i2N Ki stdp ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiP i2N Ki2 q : The detected peaks are all those local maxima with a z-score above a certain threshold.
This z-score represents the strength of a peak, and a user-defined threshold, whose default value is 3, specifies how many standard deviations above average the peaks strength must be.
Additionally, the peaks are assigned a fuzziness score that represents the degree of uncertainty about the peak position, given by the formula: fuzziness p ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 2 Op O0 0p s , where O0 0p denotes the second discrete derivative of the filter output.
2.3 Filtering and scoring After a set of putative peaks has been derived, additional filtering steps are carried out when a control sample is available.
They are all based on the enrichment level of a peak, defined as the total number of reads that contribute to the input signal in a window of a certain size (by default 147bp) centered around the peak.
Peaks are filtered in a similar manner as in Zhang et al.
(2008b): the enrichment level is modeled as a Poisson rv whose parameter is estimated from both a global and a local average of the control sample, which is rescaled so that the number of sequenced reads in the two samples matches.
From this model a P-value is obtained, and peaks can be filtered based on a user-defined threshold (which defaults to 105).
In more detail, let G denote the genome length.
The noise level in the control sample is rescaled according to the total coverage ratio  P k2N IkP k2N Ck , so that the local and global noise estimates W and tot can be expressed, respectively, as PW kW Cpk 2W1 and P k2N Ck G , and the final noise estimate is chosen as the maximum between the two (W defaults to 1000).
Finally, the null model for the read counts in a window of a fixed radius R is given by: XF kF Ip k Poiss2F 1: The next filtering step consists in controlling the relative amount of posi-tive and negative reads in the enrichment level, as highly unbalanced contributions from the two strands are likely to arise from mapping biases.
Following the approach of Zhang et al.
(2008a), we filter out peaks where the ratio between the two contributions is not contained in the interval r, 1=r, where r defaults to 4.
A final step takes place when the sample is obtained from multiple ChIP-seq experiments.
In this case, the consensus input signals from the different samples are added together, and the above steps are carried out as if the signal came from a single experiment.
After that, however, the enrichment level at each peak is decomposed into the contributions of the different experiments, and each of them is tested independently to assess whether a certain histone modification is present or not (see Fig.4).
The tests are carried out using the same noise model and formulas shown above, where now I corresponds to the consensus signal derived from the single histone modifications.
Finally, for each nucleosome call the algorithm provides, along with the genomic coordinates, the following statistics: (i) the z-score (peak strength), (ii) the input signal enrichment level (sum of the raw read Fig.4.
Integration of multiple histone modification experiments.
First, peak detection is performed on the sum of the input signals, then the signal is decomposed into the contributions of the single histone modifi-cations and then a statistical test is performed for each of them to asses whether their contribution is significant or not Fig.3.
The Mexican hat wavelet for  50 (on the right) and its fre-quency spectrum (on the left) 2549 NucHunter due to , , that s base pairs random variable p since counts in a window of 147bp around the peak), (iii) the control signal enrichment level (sum of the smoothed read counts in the same window), (iv) a P-value derived from the comparison between input and control enrichment levels (significance of the enrichment) and (v) the fuzziness score for the peak position.
In case multiple samples are simultaneously analyzed, it is also provided, for each input sample and each nucleosome call, the contribution to the total enrichment level in terms of raw reads and the result of the statistical test as an on/off flag.
2.4 Inferring the average fragment length The average fragment length F is typically inferred based on the strand cross-correlation function, defined as: CCk X p PpNp k, where the index p spans all genomic positions.
For point-source factors and low noise levels the cross-correlation function usually has a peak at position F (the fragment peak), as shown in Figure 5, which yields a straightforward method for the estimation of F. However, for many his-tone marks the cross-correlation plot is harder to interpret because of the presence of a so-called phantom-peak (Landt et al., 2012) and other sys-tematic biases, which can sometimes completely obscure the fragment peak (see Fig.5b).
To account for these biases, we introduce a modified cross-correlation function that we call peak cross-correlation (pcc):dCCk X p PpNp k: The signals N and P are a dense representation of the peaks obtained applying a peak detection algorithm to the strand-specific signals N and P. More specifically, N and P are binary signals whose only non-zero entries are ones occurring at the peaks locations.
The peak detection technique presented in Section 2.2 applied to the consensus signal I is applied to the signals N and P. The pcc function, which is used to infer F, can also indicate how ap-propriate the choice of is, where is the parameter used for peak detection (see Section 2.2).
If N and P are assumed to be two replicates of the same signal with a systematic shift of F base pairs in the nucleo-some peaks, a good choice of should result in a strong peak in the pcc function around position F, whereas a bad choice should lead to almost independent peaks in the two strands and a flat pcc function.
After the pcc function is computed, a clustering technique is applied to interpret it, which yields an estimate for F and a quality score for .
We assume that the plot is generated by sampling from a mixture of three rvs: a uniform rv to model the background noise, a Gaussian rv to model the phantom peak and another Gaussian rv to model the fragment peak, as shown in Figure 5c.
The parameters of these distributions are inferred using an expectation-maximization algorithm, and the mean of the Gaussian rv corresponding to the fragment peak is used as an estimate for the average fragment length.
The quality score, which is derived from the likelihood of the inferred model, can be computed for different values of and yields a score curve (see Section 1 in Supplementary Material for more details).
3 RESULTS 3.1 Comparison to other available tools We have developed NucHunter to identify nucleosome positions using histone modification ChIP-seq data.
To test the predictive power of our algorithm and to compare it with other available tools, we ran NucHunter and two other tools [Nucleosome Positioning from Sequencing (NPS) from Zhang et al.
(2008a); Template Filter from Weiner et al.
(2010)] on a H3K9ac dataset from yeast (Weinberger et al., 2012).
Some tools had to be (a) (b) (c) Fig.5.
Strand cross-correlation analysis for some ChIP-seq experiments in human K562 cells.
(a) Histone modification H3K4me3, a point-source or mixed-source factor.
The phantom peak and the fragment peak are clear.
(b) Histone modification H3K9me3, a broad-source factor.
The fragment peak is almost not visible, in contrast with the phantom peak.
(c) The pcc for the histone modification H3K9me3.
Now also the frag-ment peak is visible, and it is possible to infer the average fragment length with an EM algorithm 2550 A.Mammana et al.
s p , `` '' due to ` `` ' '' ( ) that random variables ( ) g g ( ) g In order t to ( ) excluded from the comparison either because they were not able to deal with the large amount of data or because the results obtained using default parameters were unsatisfactory.
We chose yeast because we wanted to compare the predictions to a base pair resolution map of nucleosome positions in yeast (Brogaard et al., 2012).
This map has been obtained with a tech-nique that, even if it has not been tested widely yet, is independ-ent from ChIP-seq, and it is claimed to be more accurate.
In line with previous studies (Chung and Vingron, 2009), to compare the nucleosome predictions with the nucleosome map, we used the (normalized) area under the cumulative error curve (AUC) as a performance measure.
The AUCwas obtained apply-ing the following procedure (see also Supplementary Material): (1) we consider the set of all distances between nucleosome predictions and nucleosomes in the map573bp, (2) we obtain a cumulative error curve.
In such a curve, a point x, y means that a fraction y of the distances is less than x base pairs (see Supplementary Fig.S5), (3) we compute the AUC and we normalize it, so that a set of perfect predictions has an AUC of 1 and a random set of genomic positions has an expected AUC of 0.5.
Along with the AUC, we also computed the sensitivity and the specificity, defined, respectively, as the fraction of nucleosomes in the map that are closer than 20bp to a nucleosome prediction and the fraction of nucleosome predictions that are closer than 20bp to a nucleosome in the map.
Moreover, to account for the great variability in the number of predictions returned by each tool, we repeated the performance measurements for different score thresholds.
The results from Figure 6 show that NucHunter makes more accurate predictions compared with the other tools.
Considering the default score thresholds, NucHunter andNPS return a similar number of predictions but the former has an higher AUC than the second, whereas Template Filter returns many more predictions and of lower quality.
When the score threshold is increased, the AUC difference between NucHunter and NPS becomes much more pronounced.
This suggests that the nucleosome predictions with highest score from NucHunter are, in general, much more precise compared with those from the other tools.
All the tools suffer from low sensitivity in this dataset, in particularNucHunter and NPS when the default score thresholds are used.
The reasons for unidentified nucleosomes or incorrect predic-tions can be many.
In the first place, the experimental procedures used for the ChIP-seq experiment and that used for the nucleo-some map are different.
Roughly 5.6% of the nucleosomes in the map, for instance, are located in low-mappability regions and are not covered by any read.
Moreover, the ChIP-seq experiment targeted only acetylated nucleosomes, as opposed to the nucleosome map.
A more general problem is the identifi-cation of fuzzily positioned nucleosomes.
If the nucleosome positioning varies extensively from cell to cell, the assumptions made by the algorithms are violated and nucleosomes are hard to identify.
Lastly, both specificity and sensitivity are affected from high noise levels, insufficient sequencing coverage and sequencing biases.
In addition to the yeast dataset, we also tested the algorithms on a simulated dataset and on different histone modification ChIP-seq files in human K562 cells.
In the simulated dataset, the nucleosome map is randomly generated and the reads are generated accordingly.
Because there is no nucleosome map for the human dataset, we used pairs of replicate experiments and pairs of different histone modifications as gold standard-pre-dictions pairs.
The details of the simulation and the performance evaluations are reported in Supplementary Material in Sections 3 and 4.
In general, the results are in agreement with those shown previously.
In the Supplementary Material, it is also shown that NucHunter runs faster and requires less memory than the other two algorithms (see Supplementary Material Section 5).
Fig.6.
Accuracy assessment of different tools on the yeast dataset.
The performance measures (AUC, sensitivity and specificity) are computed for every possible score threshold, which results in an AUC number of calls curve (left) and a specificitysensitivity curve (right).
The circles indicate the performance of the algorithms using the default thresholds 2551 NucHunter to less than s base pairs in Supplementary Material area under the cumulative error curve s base pairs s base pairs to to very for the human dataset as well as above 3.2 Clustering of nucleosomes based on histone marks We ran NucHunter on a composite dataset from a human leu-kemia cell line [K562, Myers et al.
(2011)] consisting of a control experiment and 12 ChIP-seq experiments for different histone modifications.
For each detected nucleosome and for each ex-periment the algorithm returned, along with other statistics, the raw read count within a window of a specified width (which defaults to 147) around the inferred nucleosome location (see Methods).
We used these read counts for an exploratory analysis of the chromatin landscape.
After a normalization procedure that cor-rects the read counts taking into account the control sample, the different sequencing depths of the datasets and the nucleosome abundance at each locus, we obtained a joint histone modifica-tion level distribution and we applied the k-means clustering al-gorithm on it (see Supplementary Material for more details).
Given a parameter k, this unsupervised learning method aims at partitioning the data points into k different families (clusters) such that elements in the same cluster are as similar to each other as possible.
Each cluster is characterized by its centroid, which is, in our case, a prototypical histone modification pattern.
We found that with k equals 6 the results are robust, whereas for higher values of the parameter the clusters tend to change depending on the initialization (see Supplementary Material).
Moreover and most importantly, we found that such a partition-ing, derived solely from the histone modification patterns, can also capture biologically meaningful positional features of the nucleosomes.
We assigned labels to each cluster based on the histone modification pattern and genomic localization.
The labeled centroids are shown in Figure 7.
We studied the genomic localization of nucleosomes from the different clusters using the RefSeq annotation dataset as well as publicly available data from cap analysis of gene expres-sion (CAGE) and DNase I hypersensitivity sequencing experi-ments (Myers et al., 2011).
We performed the following analyses (further discussed in Supplementary Material): (i) we derived a consensus nucleosome profile along genes by considering a large set of annotated genes, by rescaling their nucleosome profiles to the same length and by adding them up (Fig.8a); (ii) we analyzed the nucleosome positioning around promoters of active genes by considering the distribution of distances between CAGE tags and nucleosomes (Fig.8b); and (iii) we obtained the average DNase I hypersensitivity profile around nucleosomes for each class (Fig.8c).
Overall these data give a clear picture of the nucleosome land-scape and recapitulate previous knowledge (see Fig.7).
The nu-cleosomes in the first family are characterized by a strong enrichment of H3K4me2/3 and H3K9ac, and they tend to reside in the 50 portion of a gene near the transcriptional start site (TSS; Fig.8a).
Thus, we labeled them promoter nucleo-somes.
In proximity of promoters of active genes, these nucleo-somes exhibit a strikingly regular pattern (Fig.8b), whose main features are a nucleosome-depleted region right upstream the TSS and a well-positioned nucleosome 170bp downstream (the 1 nucleosome).
The second and third clusters show an enrich-ment of H3K4me1 and H2AZ as well as a general enrichment of active marks, whereas TSS-associated histone marks, such as H3K4me2/3 and H3K9ac, are less enriched compared with the promoter cluster.
These features, together with the high levels of DNase I hypersensitivity that we observe (Fig.8c), suggest that these nucleosomes may flank enhancer sequences.
Thus, we labeled them as enhancer 2 and enhancer 1 nucleosomes.
The fourth centroid is enriched in H3K79me2 and H4K20me1, whereas the fifth centroid is enriched in H3K36me3 and H3K9me1, which are all histone marks related to elongation of RNA polymerase II (Vavouri and Lehner, 2012).
The localiza-tion of these two classes of elongation nucleosomes along the gene body, shown in Figure 8a, suggests that the 5th centroid is enriched toward the 30 end of a gene, whereas the 4th centroid is enriched more to the 50 end.
Thus, we termed them elongation early and elongation late nucleosomes, respectively.
The last centroid is characterized by an enrichment of H3K9me3 and H3K27me3, suggesting that it represents chromatin-repressed genomic regions (Margueron and Reinberg, 2011).
Thus, we termed it repressed.
Lastly, we explored the relation between the different clusters that we obtained and a previously published study (Ernst and Kellis, 2010) that aimed at classifying the chromatin landscape into discrete states.
Even though the last method uses a more complex model, different data sources and positional relations between histone modification patterns, we found that the overall results are comparable (see Supplementary Material).
We believe that the joint analysis of histone modification patterns and nucleosome positioning that NucHunter allows for provides complementary information and offers a greater potential than histone modification studies based on arbitrary binning schemes of the genome.
Fig.7.
Using the k-means algorithm, 422547 nucleosomes called by NucHunter were clustered into six clusters: promoter (20.4%), enhancer 1 (19.8%), enhancer 2 (14.4%), elongation early (16.4%), elongation late (14.7%) and repressed (14.3%).
The rows of the heatmap represent the centroids of the clusters and the columns represent the histone modifica-tions.
The labels have been assigned based on prior biological knowledge 2552 A.Mammana et al.
( ) ile l ( ) ( ) ( ) is s s very ' ( ) l `` '' ( ) very s ile to ( ) l `` '' `` '' ile ( ) s ' ile ' `` '' `` '' `` '' which 4 DISCUSSION The fast-paced development of chromatin immunoprecipitation-based techniques is heading toward an increased spatial reso-lution for DNAprotein interactions.
In line with this trend, we developed NucHunter, a software for base pair resolution nucleosome identification in ChIP-seq experiments.
The innova-tive aspects of this tool reside in a more accurate and efficient signal processing, an improved statistical analysis of the peaks, the possibility of integrating data from a control sample and to consider multiple histone modifications at once.
We put forward a nucleosome-centric view, because if we view the modifications (either sequentially or in a combinatorial pat-tern) as a reflection of a signaling activity then nucleosomes can be viewed as signaling modules (Turner, 2012).
In agreement with this idea, we found that nucleosomes can be clustered into distinct subgroups.
These subgroups either mark certain func-tional regions of the genome, such as promoters and enhancers, or are related to biological processes, such as elongation or chro-matin-mediated repression.
Although this is not a new finding [see Ernst and Kellis (2010)], we think that our approach has the benefit of assigning the data to a physical entity that carries the information: the nucleosome.
Thus, separation of different histone modification patterns into distinct subgroups becomes much more meaningful than by arbitrarily binning the genome into non-overlapping windows (Ernst and Kellis, 2010), where two nucleosomes with different modification patterns could be present.
(a) (b) (b) 5000 0 5000 10000 15000 20000 25000 0 5 10 15 20 25 30 distance from TSS (bp) co un t TSS TES elongation early elongation late enhancer 1 enhancer 2 promoter repressed Fig.8.
Genomic localization of the different nucleosome classes in human K562 cells.
(a) Occupancy of nucleosomes from the different classes along the gene body.
The nucleosome occupancy profiles from a subset of genes in RefSeq have been rescaled to the same length and summed up.
(b) Nucleosome distribution at promoters of active genes.
The profile has been obtained by computing the distribution of distances between CAGE tags and nucleo-somes.
(c) DNase I hypersensitivity levels in relation to nucleosomes.
The profile for each nucleosome class is the average DNase I hypersensitivity profile of all nucleosomes from that class 2553 NucHunter-`` '' ( ) In summary, we developed a new tool called NucHunter that is able to identify positioned nucleosomes along the genome using ChIP-seq data of histone modifications and annotates each nu-cleosome with (i) a flag indicating presence or absence of a certain histone modification and (ii) the number of contributing reads (if one is interested in a more quantitative view).
We demonstrated that NucHunter performs better than currently available tools and has some features not present in any of them.
By focusing on the nucleosome as information carrier, charting the epigenome will become much more meaningful and will in the long run allow for unraveling novel chromatin-mediated mechanisms.
ACKNOWLEDGEMENTS A special thank goes to Johannes Helmuth and Matthew Huska for critical reading of the manuscript.
Funding: This work was supported by the International Max Planck Research School for Computational Biology and Scientific Computing; and the Bundesministerium fur Bildung und Forschung for the Deutsches Epigenom Programm (DEEP) [01KU1216C].
Conflict of Interest: none declared.
Abstract Since the proposal for pangenomic study, there have been a dozen software tools actively in use for pangenomic analysis.
By the end of 2014, Panseq and the pan-genomes analysis pipeline (PGAP) ranked as the top two most popular packages according to cumulative citations of peer-reviewed scientific publications.
The functions of the software packages and tools, albeit variable among them, include categorizing orthologous genes, calculating pangenomic profiles, integrating gene annotations, and constructing phylogenies.
As epigenomic elements are being gradually revealed in prokaryotes, it is expected that pangenomic databases and toolkits have to be extended to handle information of detailed functional annotations for genes and non-protein-coding sequences including non-coding RNAs, insertion elements, and conserved structural elements.
To develop better bioinformatic tools, user feedback and integration of novel features are both of essence.
Introduction In the past decade or so, the remarkable advancement of DNA sequencing technology and application has led to an astro-nomical accumulation of genomic data.
This is especially true for the prokaryotic genomes as individual of them is only a few megabases in size.
It is expected that in the next decade or two, there will be more data collected than what we can actually handle.
Therefore, database construction, improvement, and consolidation, as well as new tool development, are especially welcome.
In this way, the sibling fields of genomics, such as pangenomics and metagenomics, can all be ready for curating, sharing, and mining floods of the incoming genomic big data.
Coming back to the reality and focusing on pangenomics, there were, as of December 2014, more than 40 bacterial spe-cies that have over 20 fully-assembled genomes from different strains and isolates, allowing for comprehensive pangenomic studies.
The concept of pangenome was first proposed in 2005 by Tettelin et al.
[1,2], which is defined as the entire geno-mic repertoire of a given species or phylogenetic clade when multiple species are defined by systematics.
According to the definition, gene profile (content) of a pangenome is divided into three groups: core (shared by all genomes), dispensable, and strain-(or isolate-) specific genes.
A series of pangenomic studies have been performed in genomic dynamics [36], pathogenesis and drug resistance [79], bacterial toxins [10], nces and 74 Genomics Proteomics Bioinformatics 13 (2015) 7376 and species evolution [11].
The concept has also been extended to viral [12], plant [1315], and fungal genome studies [16].
A review on ten-year history and field achievement of pange-nomics has just been published at the beginning of 2014 [2], which detailed major projects as well as methodology and tech-nology advancements.
Here, we provide a brief review on the pangenomic software packages and tools, including their basic function, general utility, and popularity based on their cumulative citation by peer-reviewed scientific publications.
Although such a single-criterion evaluation may never be adequate and thorough, we hope that it provides a field guide for students and young scien-tists to make the right choice for their preferred applications.
Highlights of the software packages and tools Since 2010, we have seen a dozen or so software packages and tools being put forward, which are capable of clustering orthologous genes, identifying single nucleotide polymor-phisms (SNPs), constructing phylogenies, and profiling core/ shared/isolate-specific genes.
Although they may share similar functions, each has its own characteristics and limitations, leaving rooms for further improvement.
Among the early-developed packages, Panseq [17] and PanCGHweb [18] were published in 2010, followed by CAMBer [19] and the Prokaryotic-genome Analysis Tool (PGAT) [20] in 2011.
PanCGHweb is a web tool for pange-nomic microarray analysis based on PanCGH algorithm [21].
It enables users to group genes into orthologs and to construct gene-based phylogenies of related strains and isolates.
However, this package is rather specific for handling microar-ray data but not RNA-seq data.
Panseq, another online pange-nomic tool, is able to determine core and accessory regions of genome assemblies based on MUMmer and BLASTn, as well as to identify SNPs among the core genomic regions.
In addi-tion, Panseq also has a locus selector module that selects the most discriminatory loci among the accessory loci or core gene SNPs [17].
Panseq, however, is not able to provide pangenomic profile and functional enrichment analysis that is important for the biologists to filter out functional relevance of the pange-nomic elements.
The later released CAMBer is designed to identify multi-gene families from multiple bacterial strains and isolates.
These multi-gene families can be used for sequencing error detection, mutation identification, and pangenomic profile computation [19].
CAMBer is supreme in refining gene function prediction according to multi-gene fam-ily information, but it does not provide tools for comparative or evolutionary analysis among strains and isolates.
As a web-based database, PGAT integrates several useful functions, such as plotting the presence and absence of genes among members of a pangenome, identifying SNPs among orthologs and syntenic regions, comparing gene orders among different strains and isolates, providing KEGG pathway analysis tools, and searching for genes through different annotations such as the Cluster of Orthologous Groups of proteins (COG), PSORT, SignalP, the Tied Mixture Hidden Markov Model (TMHMM), and Pfam.
However, PGAT is just a database with a limited number of species curated and it cannot perform analysis for new sequencing data from users.
PGAP is a stand-alone program developed by Zhao et al.
in 2012, which contains five functional models [22].
Based on functional gene clustering and analysis, PGAP presents pange-nomic profile (partitions of pangenomic elements or gene categories), genetic variation, species evolution, and function enrichment of different strains and isolates of a given pangen-ome.
In addition, all analyses are performed with a single com-mand, and such integration is rather user-friendly and efficient.
Nonetheless, PGAP has its limitation as well.
For instance, all its output files of the five models are text files, which lacks of intuitiveness.
Contreras-Moreira et al.
subsequently proposed a program called GET_HOMOLOGUES in 2013, which is also a versatile software package for pangenomics [23].
This soft-ware package integrated data download, sequence feature extraction, homologous gene identification, pangenome profil-ing, graphical display, and phylogenetic tree construction into one powerful toolkit.
Several other tools were also available in 2013, such as PanCake [24] and PANNOTATOR [25].
PanCake was developed for identifying singletons and core regions in arbitrary sequence sets, while PANNOTATOR, a web-based automated pipeline, was designed for the annotation of closely-related genomes for pangenomic analysis.
However, these two tools only focus on simple functions, such as cluster-ing homologous genes and gene curation.
In 2014, a powerful and flexible toolkit, the Integrated Toolkit for Exploration of microbial Pan-genomes (ITEP), was published by Benedict and colleagues [26].
ITEP integrates plenty of existing bioinfor-matics tools for pangenomic analysis, including protein family prediction, ortholog detection, functional domain analysis, pangenomic profiling, and metabolic network integration.
Moreover, ITEP also integrates some visualization scripts that assist biologists in phylogenetic tree construction, annotation curation, and specific query for conserved protein domain iden-tification.
In 2014, another rapid core-genome alignment and visualization pangenomic software package, Harvest, was pro-posed by Treangen et al.
Harvest contains tools, such as Parsnp and Gingr, for core gene alignment, variant calling, recombina-tion detection, and phylogenetic trees construction [27].
To analyze pangenomic profile in a larger scale, a software package PanGP was developed with a graphic interface by Zhao et al.
in 2014 [28].
Spine and AGEnt were also developed in 2014, which are capable of profiling pangenomes based on both finished and draft genomic sequences [29].
We summarized all the software packages and tools in Table 1, highlighting their platforms and major features.
We went one step further and ranked them according to their citations by peer-reviewed scientific publications (Figure 1), which were collected from ISI Web of Science-Science Citation Index Expanded.
Our summary indicates that Panseq and PGAP have been the most popular packages up to the end of 2014.
A wish list for improving the current software Although single-tool solution could not usually satisfy the need for understanding the whole picture, a wish list from the users is always helpful for prioritizing goals for the package developers, hence providing directions for improving each package.
First, the performance of pangenomic analysis strongly depends on the accuracy of genome assembly and annotation.
Therefore, an adequate number of complete sequence assem-blies are a prerequisite.
Currently, most of the existing bacter-ial genome sequences are actually incomplete (in most of the Table 1 Software tools for pangenomic studies Name Link Platform Main features Ref.
Panseq https://lfz.corefacility.ca/panseq/ Online Windows Linux a, b [17] PGAT http://nwrce.org/pgat Online a, b, e [20] PanCGHweb http://bamics2.cmbi.ru.nl/websoftware/pancgh/pancgh_start.php Online a, d [18] PGAP http://pgap.sourceforge.net/ Linux a, b, c, d, e [22] ITEP https://price.systemsbiology.net/itep Linux a, b, d, e, f, g [26] CAMBer http://bioputer.mimuw.edu.pl/camber/index.html Windows Linux a, c, f [19] Harvest https://github.com/marbl/harvest Mac OSX Linux a, b, d, g [27] GET_HOMOLOGUES http://www.eead.csic.es/compbio/soft/gethoms.php Mac OSX Linux a, c, d, f, g [23] PanCake https://bitbucket.org/CorinnaErnst/pancake/wiki/Home Windows Linux a [24] PanGP http://PanGP.big.ac.cn Windows Linux c, g [28] PANNOTATOR http://bnet.egr.vcu.edu/pannotator/index.html Online a, f [25] Spine and AGEnt http://vfsmspineagent.fsm.northwestern.edu/index_age.html Online Mac OSX Linux a [29] Note: Only letters are used in main features column, their corresponding feature descriptions are listed as below: (a) Clustering homologous genes, assigning their presence/absence or analyzing core/accessory genomes; (b) Identifying SNPs; (c) Plotting pangenomic profiles; (d) Building phylogenetic relationships of orthologous genes/families of strains/isolates; (e) Function-based searching or analysis; (f) Annotation and/or curation; and (g) Visualization.
0 10 20 30 40 50 60 70 80 90 100 2010 2011 2012 2013 2014 R el at iv e an nu al c ita tio n (% ) Year of citation PANNOTATOR PanGP GET_HOMOLOGUES CAMBer PGAP PanCGHweb PGAT Panseq Figure 1 Relative citation of the pangenomic software tools from peer-reviewed scientific publications Xiao J et al/ Software Tools for Pangenomics 75 cases, contigs are not joined together into single chromo-somes), and some only have high-quality and high-coverage raw data available.
The inclusion of incomplete genome assem-blies for pangenomic analysis may need scaffold building that requires reformatting of the contig data files.
Despite the devel-opment of the third-generation sequencing technology, which would certainly help the assembly and finishing of prokaryotic genomes [30], incomplete prokaryotic genomes are expected to be deposited into public databases in mass.
It would be a waste if such data are left unused.
Second, orthologous gene identification is a key step in pangenomic analysis.
At present, the existing software for ortholog detection is mainly based on sequence similarity, phylogenetic relationship, or other annotation information such as functional information.
The development of novel and more efficient ortholog identification method for multiple closely-related strains and isolates can greatly improve the accuracy of pangenomic analysis.
One possibility is to inte-grate gene gain-and-loss information for phylogeny building among strains and isolates.
Third, sampling is also important for pangenomics in a couple of counts.
One is how many strains or isolates to choose for a pangenomic analysis.
The other is how to implement a filter that differentiates more diverse strains or isolates from the less diverse for pangenomic analysis.
For instance, if we choose all genomes of a species for an analysis, which include one or a few divergent genomes, the core genome will be much shorter or reduced.
Obviously, individual genomes should be selected and regrouped for better representation of average nucleotide identity (ANI).
ANI is one of the most useful mea-surements for species delineation [31].
Therefore, for a better pangenomic analysis, detailed information for the available samples is of essence, which should include their genotypes, phenotypes, and habitats.
Fourth, the current tools have not incorporated some recent advancements in prokaryotic genomics, such as the so-called genome-organization frameworks (GOFs), which are not only unique to each species but also provide guidance for sequence assembly and finishing [32].
Other annotation information, such as that of non-coding RNAs, pseudogenes, and epigenetic elements, remains to be implemented into the relevant software packages.
Finally, a never-ending improvement of pangenomic tools is visualization that provides not only better displays but also quality graphics for publication.
Concluding remarks We provide an overview on the existing pangenomic analysis tools and hope to see improvements of the software tools from their original developers.
We certainly express our enthusiasm for new tools to join the competition, and after all, for a piece of bioinformatic work, a database or a toolkit, the survival or winning game is in its long-term maintenance and constant improvement.
Competing interests The authors declared that there are no competing interests.
Acknowledgements This study was supported by the National High-tech R&D Program (863 Program; Grant No.
2012AA020409) from the https://lfz.corefacility.ca/panseq/76 Genomics Proteomics Bioinformatics 13 (2015) 7376 Ministry of Science and Technology of China, the Key Program of the Chinese Academy of Sciences (Grant No.
KSZD-EW-TZ-009-02), and the National Natural Science Foundation of China (Grant Nos.
31471248 and 31271386).
ABSTRACT We have implemented aggregation and correlation toolbox (ACT), an efficient, multifaceted toolbox for analyzing continuous signal and discrete region tracks from high-throughput genomic experiments, such as RNA-seq or ChIP-chip signal profiles from the ENCODE and modENCODE projects, or lists of single nucleotide polymorphisms from the 1000 genomes project.
It is able to generate aggregate profiles of a given track around a set of specified anchor points, such as transcription start sites.
It is also able to correlate related tracks and analyze them for saturationi.e.
how much of a certain feature is covered with each new succeeding experiment.
The ACT site contains downloadable code in a variety of formats, interactive web servers (for use on small quantities of data), example datasets, documentation and a gallery of outputs.
Here, we explain the components of the toolbox in more detail and apply them in various contexts.
Availability: ACT is available at http://act.gersteinlab.org Contact: pi@gersteinlab.org Received on July 24, 2010; revised on November 2, 2010; accepted on November 23, 2010 1 INTRODUCTION There is now an abundance of genome-sized data from high-throughput genomic experiments.
For instance, there are ChIP-chip, ChIP-seq and RNA-seq experiments from the ENCODE (ENCODE Project Consortium, 2007) and modENCODE (modENCODE consortium, 2009) projects.
There are also genome sequence data that can be used to generate tracks measuring sequence content, such as the densities of single nucleotide polymorphisms (SNPs) from dbSNP (Sharry et al., 2001) and the 1000 genomes project.
In most cases, the representations of these data take the form of either signal tracks that describe a genomic landscape or distinct region tracks that tag portions of the genome as active.
The aggregation and correlation toolbox (ACT) provides a powerful set of programs that To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first three authors should be regarded as joint First Authors.
can be applied to any experiments producing data in these formats.
The ability to analyze multiple genomic datasets is important, as demonstrated by tools like Galaxy (Giardine et al., 2005).
ACT provides a unique set of functionality that complements existing methods of analysis.
2 THE ACT TOOLBOX: OVERVIEW ACT facilitates three main types of analysis: Aggregation: in many scenarios, it is useful to determine the distribution of signals in a signal track relative to certain genomic anchors (Fig.1, aggregation).
For example, it has recently been reported that the contribution of each transcription factor binding site to tissue-specific gene expression depends on its position relative to the transcription start site (TSS) (MacIssac et al., 2010).
It is thus useful to aggregate binding signals of transcription factors at a certain distance from the TSSs of all genes (the anchors).
In general, this type of aggregation analyses helps identify proximity correlations and functional relationships between the signals and anchors.
In the ENCODE pilot study (ENCODE Project Consortium, 2007), it has been used to demonstrate positional relationships between chromatin features and TSSs.
Correlation: it is also useful to consider how multiple-related signal tracks are correlated with each other.
For example, a previous study (Zhang et al., 2007) demonstrated, using whole-track correlation methods, that there was a consistent relationship among transcription factors as judged by their signal profiles across several ChIP-chip experiments.
By providing a means of correlating signal tracks with each other,ACT allows for initial comparison of different experiments to see which are more similar or related than others (Fig.1, correlation).
Saturation: another important type of analysis is determining the number of experimental conditions required to achieve a high genomic coverage of the biological phenomenon under study.
For example, using ChIP-chip or ChIP-seq experiments, one could identify a set of transcription factor binding sites from a human cell line.
When the experiment is repeated using another cell line, some additional binding sites could be identified.
How many cell lines need to be considered in order to reach the point of saturation, so The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:43 21/3/2011 Bioinformatics-btr092.tex] Page: 1153 11521154 ACT Fig.1.
Uses of ACT using signal tracks from various sources.
Signal around all TSSs is aggregated to give an average signal profile, for example of Baf155 binding around TSSs (Encode Project) (aggregation).
Figure made in Excel (correlation).
Multiple signal tracks are correlated to show which tracks are more or less related to each other.
In the selected example, a heatmap of the SNP track correlation between four individuals (dbSNP) leads to a dendogram of their phylogenetic relationship.
Figure made using Web ACT.
Each additional signal track increases the number of base pairs covered (saturation).
When the addition of signal tracks is considered in all possible combinations, the average increase in coverage, with error bars, can be visualized by a saturation plot.
In the example, data are taken from individuals from dbSNP [with additional genomes from Ahn et al.
(2009), Bentley et al.
(2008), Drmanac et al.
(2010), Kim et al.
(2009)].
In each box plot, the top and bottom pink bars correspond to the maximum and minimum normal values, the top edge, middle line and bottom edge of the box correspond to the top 25 percentile, median and bottom 25 percentile, the black dot is the mean, and red circles are outliers.
Figure made using ACT downloadable saturation program.
that few new binding sites would be identified by extra experiments?
ACT produces plots that help answer this type of question.
3 DETAILS AND USE CASES ACT is available as a suite of downloadable scripts corresponding to the aggregation, correlation and saturation components of the toolbox.
The tool is intended for Linux/Unix users with Java and Python.
In addition, it is useful to have R for output visualization for the aggregation and correlation tools.
There is also a compendium of other versions of the tool components written in different languages and with varied functionality.
For some types of analysis, there are web components for demonstration purposes on small datasets with built-in visualization features.
However, because most whole-genome signal tracks are too large to upload via standard Internet connections, users are recommended to download the toolbox and run it locally.
As performing these calculations on whole-genome data can be especially time intensive, the version of the tools presented here has been designed to run efficiently on large datasets.
Aggregation: the aggregation component is designed to take a signal track (.sgr or .wig) and an annotation track (.bed) as input, and compute the average signal over a certain number of base pairs upstream and downstream of (i.e.
a fixed radius around) the annotations.
In other words, signal values are taken from the region surrounding each annotation, and averaged over the number of annotation anchors provided.
The base pair resolution of the aggregation can be specified by the number of bins (narrower bins give more data points and therefore finer granularity).
Results of such calculation can be plotted as in Figure 1 (aggregation).
ACT also provides features such as computing the standard deviation, median and quartiles that can be viewed as a boxplot, as well as scaling aggregation over regions such as areas between transcription start and end sites or within exons so that all of the aggregate signals within those regions fall into a fixed number of bins.
In this case, bin size is dynamically computed for each region so that the same number of bins cover regions of different sizes.
Correlation: the correlation analysis takes a set of active genomic regions (.bed) such as a SNP track or a genomic signal track (.wig).
It then divides genomic coordinates into bins and gives each bin a value corresponding to the mean or maximum signal values which fall within the bin, or assigns value based on the number of active regions which fall within the bin.
A final correlation matrix is created based on either the Spearmans, Pearsons or normal score correlation between each pair of binned datasets.
The results can be visualized as a heatmap or as a phylogenetic tree using programs such as PHYLIP (Felsenstein, 1996).
One version of the correlation tool uses parallelization to decrease the pro-grams overall running time.
This component was written largely in Java.
Examples of correlation output based on SNP tracks and ChIP-chip data are shown in Figure 1 (correlation).
Saturation: we provide an efficient implementation of saturation plot generator.
Each input file corresponds to one dataset (e.g.
one new individual, in .bed format), and each line in a file specifies a genomic location that has the biological phenomenon under study (e.g.
tagged SNPs).
The saturation plot shows, with each new dataset (x-axis), what percentage of genomic base pairs are covered (y-axis).
The program considers the various combinations in which tracks can be added so that the increase in base pair coverage is a range of values based on all the files in the input.
The resulting plot is output in PDF format (Fig.1, saturation), in which a series of boxplots depicts increasing base pair coverage, where the boxplot at each position m on the x-axis shows the coverage values of all combinations of m conditions.
Boxplots that approach a horizontal asymptote indicate that the coverage has reached saturation.
Our implementation makes use of special data structures to avoid redundant counting.
It normally takes less than a minute to generate the plot for up to 30 input files each with a few thousand lines.
To handle more files and files with more lines, the tool also provides an option to compute the coverage of a random sample of the input file combinations.
4 DISCUSSION There are number of additional analyses that can be done to fine-tune the output of ACT.
For instance, it is possible to use the online genomic signal aggregator (GSA), which assigns each genomic position to the nearest anchor in order to reduce the artifacts caused 1153 [11:43 21/3/2011 Bioinformatics-btr092.tex] Page: 1154 11521154 J.Jee et al.
by the subsets of anchors clustering together, to handle tightly clustered anchors.
Also, aggregation can be used in conjunction with genome structure correction to determine if the enrichments of a given signal with respect to anchor points are significantly relative to the non-random positioning of the anchors (ENCODE Project Consortium, 2007).
This correction takes into account the fact that a random distribution of anchors on the genome arises from a distinctly non-uniform distribution.
Practically, this could be carried out through ACT by comparing the aggregation over anchors (e.g.
TSSs) to that from randomized anchors, where the latter is generated by shifting anchor coordinates along the chromosome or transferring anchor coordinates from a second chromosome to the one of interest.
Finally, ACT can be used as a starting point for other downstream analyses.
In the instance of RNA-seq data tracks, further analysis can be conducted with RseqTools (Habegger et al., 2011) to, for example, determine additional similarities between two or more highly correlated tracks.
The results of correlation analysis, for instance, can also be fed into downstream principal component analysis, allowing for grouping of coregulating factors with their coregulated sites.
This would simply involve diagonalization of the output correlation matrix from ACT.
Saturation analysis can also be used to inform future experimental design.
Funding: National Institute of Health; A.L.
Williams Professorship funds.
Conflict of Interest: none declared.
ABSTRACT Motivation: Cells receive a wide variety of environmental signals, which are often processed combinatorially to generate specific genetic responses.
Changes in transcript levels, as observed across different environmental conditions, can, to a large extent, be attributed to changes in the activity of transcription factors (TFs).
However, in unraveling these transcription regulation networks, the actual environmental signals are often not incorporated into the model, simply because they have not been measured.
The unquantified heterogeneity of the environmental parameters across microarray experiments frustrates regulatory network inference.
Results: We propose an inference algorithm that models the influence of environmental parameters on gene expression.
The approach is based on a yeast microarray compendium of chemostat steady-state experiments.
Chemostat cultivation enables the accurate control and measurement of many of the key cultivation parameters, such as nutrient concentrations, growth rate and temperature.
The observed transcript levels are explained by inferring the activity of TFs in response to combinations of cultivation parameters.
The interplay between activated enhancers and repressors that bind a gene promoter determine the possible up-or downregulation of the gene.
The model is translated into a linear integer optimization problem.
The resulting regulatory network identifies the combinatorial effects of environmental parameters on TF activity and gene expression.
Availability: The Matlab code is available from the authors upon request.
Contact: t.a.knijnenburg@tudelft.nl Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Transcription factors (TFs) mediate the activation or repression of gene expression by binding specific regulatory sequences (motifs) in gene promoters.
The combinatorial interactions of multiple TFs play an essential role in transcriptional regulation.
A classical example is Escherichia colis lactose system, where the lac operon is expressed only if the concentration of TF CRP is high and that of TF LacI is low.
Presently, many studies have revealed an important role for combinatorial interactions between different TFs in establishing the To whom correspondence should be addressed.
complex patterns of gene expression (Balaji et al., 2006).
The advent of high-throughput genomic measurement techniques enabled the application of genome-wide computational approaches aimed at inferring these regulatory relations.
Sequence data, microarray gene expression data and ChIPchip TF binding data have been integrated in many different ways to derive regulatory networks.
Several approaches fit expression data using linear regression models, where the predictors are the TFs, i.e.
their binding potential or number of motifs in a gene promoter (Bussemaker et al., 2001; Gao et al., 2004; Nguyen and Dhaeseleer, 2006).
The effect of multiple TFs on gene expression is modeled as the weighted sum of the contribution of individual TFs.
Combinatorial regulation by TFs, i.e.
synergistic or antagonistic effects of multiple TFs on gene expression, are not incorporated into these models.
Most methods that do include combinatorial effects limit the scope to TF pairs, e.g.
(Bonneau et al., 2006; Chang et al., 2006; Das et al., 2004; Yu et al., 2006).
Bonneau et al.
employ continuous versions of logic functions (OR, AND and XOR) of the activities of TF pairs as additional predictors in the regression model.
Although, in principle, these methods can be extended to model the combinatorial effects of more than two TFs, the model will be too complex to reliably estimate its parameters given the currently available data.
Segal et al.
(2003) and Yeang and Jaakkola (2006) present quite different approaches to the problem of combinatorial regulation in transcription networks.
Segal et al.
constructed regulatory networks by building decision trees.
Genes are grouped into regulatory modules, which are defined by a hierarchical decision tree, where the decisions at the nodes of the tree are based on the expression levels of TFs.
In Yeang and Jaakkola, a TF is characterized as an enhancer or a repressor, being either necessary or sufficient to cause up-or downregulation of a gene.
The combinatorial function of all TFs that can bind a gene promoter is modeled as the consensus prediction of the individual TFs.
It should be noted that these two approaches, as well as many of the abovementioned ones, rely on the often incorrect assumption that the activity of a TF can be derived from the expression of the gene that codes for the TF.
So far, regulatory networks have been presented as graph structures showing the (combinatorial) regulatory effect of TFs on individual genes, modules of similarly expressed or otherwise related genes or on other TFs.
The extracellular signals that trigger the activation or deactivation of TFs are usually not part of the generated network.
Yet they could provide more direct and trustworthy evidence to infer TF activity than other signals, such as the gene expression of a TF.
Three main reasons for 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
file:t.a.knijnenburg@tudelft.nl [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i173 i172i181 Combinatorial influence of environmental parameters on TF activity their exclusion can be identified.
First, many studies on yeast are based on shake-flask cultures, where parameters like growth rate and nutrient availability are continuously changing and cannot be controlled or accurately measured.
Consequently, conditions can not be accurately defined.
Second, very often research questions are approached from a single perspective, i.e.
a condition of interest is compared to a reference condition.
Differential gene expression is then attributed to the difference between the condition of interest and the reference condition.
These approaches ignore combinatorial effects of growth parameters, the presence of which have been established by various studies, e.g.
Castrillo et al.
(2007); Knijnenburg et al.
(2007); Regenberg et al.
(2006).
That is, if the measurements were repeated using a different medium composition or temperature, chances are that a different set of differentially expressed genes would be identified.
Thus, these approaches only model the differences between growth conditions, and not the growth conditions themselves.
Note that this strategy is implicitly incorporated into two-channel microarray measurements, which output the gene expression ratio between the condition of interest and the reference condition.
Third, when combining different microarray experiments, differences in mRNA extraction protocols, microarray platform and possibly normalization and summarization algorithms, add to the already large amount of unquantified heterogeneity amongst experimental conditions (Bammler et al., 2005; Tan et al., 2003).
The context dependency of regulatory networks has been identified and acknowledged in many studies.
For example, in Bar-Joseph et al.
(2003) annotation data are employed to identify the biological context in which the inferred regulatory interactions are assumed to take place.
In Luscombe et al.
(2004) condition-specific regulatory networks were derived.
In this case, condition-specific refers to one of five phenomena (cell cycle, sporulation, DNA damage, stress response or diauxic shift), which were investigated with five different microarray datasets.
Myers and Troyanskaya (2007) propose a Bayesian approach for context-sensitive integration of diverse genomic data.
Note however, that in these approaches, the precise environmental conditions under which the microarray measurements were taken are not included in the model.
In this work we do incorporate the actual cultivation parameters in the computational framework and use this information to infer combinatorial regulation by TFs.
The work is based on a yeast transcriptome compendium, comprised of 170 microarray measurements (Knijnenburg et al.
manuscript in preparation).
These measurements encompass 55 unique growth conditions with a variable number of independent biological replicates per condition.
All cultivations were performed in chemostat fermentors under steady-state conditions.
In a chemostat, culture broth (including biomass) is continuously replaced by fresh medium at a fixed and accurately determined dilution rate.
When the dilution rate is lower than max, the maximal specific growth rate of the micro-organism, a steady-state situation will be established in which the specific growth rate equals the dilution rate.
In such a steady-state chemostat culture, is controlled by the (low) residual concentration of a single growth-limiting nutrient.
Across the 55 different conditions, there are nine varying cultivation parameter types, including limiting element, growth rate, carbon source, aeration and temperature.
Each type can assume a unique set of values.
For example, in a given experiment, the employed limiting element is either carbon, nitrogen, sulfur, phosphorus, zinc or iron.
Thus, each condition is characterized by a configuration of settings of these nine cultivation parameter types (Fig.1).
In order to model the effects of the cultivation parameters on gene expression while explicitly incorporating TFs, we follow a two-step procedure.
An overview of this procedure is presented in Figure 2.
First, we apply a forward stepwise regression strategy to quantify the (combinatorial) effect of these environmental parameters on gene expression.
The regression is performed for each gene individually.
Figure 1 depicts the results of the regression analysis for one particular gene.
The influence of a cultivation parameter on the expression of a gene is represented by its regression weight.
These weights are discretized by mapping non-zero elements to 1 or 1, depending on the sign of the weight.
Given that changes in gene expression levels as observed across different environmental conditions can be attributed to changes in the activity of TFs, we aim to infer the activity of TFs as a function of the cultivation parameters.
This forms the second step of our approach.
The goal is to estimate M, such that R is the optimal approximation of the discretized regression coefficients in R. The elements of M are 1, 0 or 1 and indicate whether a TF is activated as an enhancer (1) or a repressor (1) under a (combinatorial) cultivation parameter.
Additionally, each TF has a particular generic enhancer strength and a repressor strength.
In the procedure we employ auxiliary matrix T, which is derived from ChIPchip experiments and literature and indicates whether a TF can bind a gene promoter.
To decide whether a gene is upregulated, downregulated or not affected by a particular cultivation parameter, indicated by a 1, 1 and 0 in R, respectively, we use the following rules concerning transcriptional regulation: if there is at least one active enhancer in a gene promoter, then the gene can be upregulated.
If there are only active enhancers in a gene promoter, then the gene is upregulated.
Similar rules apply to the repressors.
If there are both active enhancers and repressors in a gene promoter, we compare total enhancer strength, which is the sum of the strengths of the activated enhancers, with its repressor counterpart.
When the enhancer strength is larger than the repressor strength, the gene is upregulated.
The gene is downregulated when the repressor strength exceeds the enhancer strength.
Figure 2c visualizes the active TFs that bind the gene promoters of genes g1, g2 and g3 under cultivation parameter A.
From M we deduce that three TFs are activated; and are enhancers, is a repressor.
From T we deduce that binds all three promoters, binds the g2 and g3 promoters and only binds the promoter of g3.
Gene g1 and g2 are upregulated, since only active enhancers bind the promoters.
For gene g3, the repressor strength of TF exceeds that of the sum of the two enhancers, thereby downregulating the gene.
The concept of TF strength enables the inference of hierarchical or combinatorial effects amongst TFs that bind a gene promoter.
The inference algorithm is translated into a linear mixed integer optimization problem and solved accordingly.
Both the elements of M as well as the TF strengths are estimated, such that the predicted gene regulation in R maximally corresponds with the discretized regression coefficients in R. The abovementioned rules become constraints in the optimization problem.
See the Methods section for details.
The resulting model identifies the combinatorial influence of cultivation parameters on TF activity and gene expression.
Furthermore, it infers the combinatorial regulatory code of multiple TFs in gene promoters.
i173 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i174 i172i181 T.A.Knijnenburg et al.
Aerobic Anaerobic AceEth EthGal Glucose GlucoseMal Chlo ChlSulfate (Ammonium) Sulfate (A Sulf Sulfate (Ammonium) Sulfate (Amm Sul SulAspLeu LeuMet Met MethPhen Pro Met MetSulate Sulate Sulate Carbon CarbonIro Nitrogen NitrogenPhos PhoSulf SulZin Zin .03 .03.05.1 .1 .1 .1 .1.2 .2 12 1230 30 30 3.55 5 56.5 Ace BenzCO2 CO2 CO2EthEthFor ProSorTwe Twenone none none none none 7 7.5 8 8.5 9 9.5 10 10.5 11 11.5 12 E xp re ss io n le ve l Aeration type Csource(s) Nsource Ssource Limiting element Growth rate Temperature (C) pH Extra compound Fig.1.
Expression levels of a gene (COX5A) across the 55 cultivation conditions.
The colored matrix is a schematic representation of the settings of the nine cultivation parameter types across the 55 conditions.
The colored lanes indicate the cultivation parameter types that are employed to order the experiments, in this case, aeration type and limiting element.
The regression model which models the gene expression as a function of the cultivation parameters, selected one single effect, i.e.
aeration type, and one combinatorial effect, i.e.
aeration type anaerobic together with limiting element carbon.
The reconstructed expression pattern based on these two effects is indicated by the shaded area.
2 METHODS 2.1 Microarray data The Saccharomyces cerevisiae laboratory reference strain CEN.PK 113-7D (MATa) was grown in chemostat fermentors under 55 different conditions.
For each condition, a variable number of independent biological replicates was performed, although mostly three, summing up to 170 microarray measurements.
Across the 55 conditions, nine different cultivation parameter types can be identified.
A cultivation parameter type, e.g.
the carbon source, is described as a categorical variable and contains two or more categories, e.g.
the used carbon source can be either maltose, glucose or ethanol.
Each condition is characterized by a specific combination of these categories across the nine cultivation parameter types.
Figure 1 presents an overview of the relevant categories assumed by the parameter types per condition.
Sampling of the chemostat cultures, probe preparation and hybridization to single-channel Affymetrix GeneChip YG S98 microarrays was performed as previously described (Piper et al., 2002).
Chip quality control, condensing probe intensities to gene expression levels and normalization was performed using GeneData Refiner Array.
The RMA algorithm was used to derive the log2 scale measure of the expression levels (Irizarry et al., 2003).
Quantile normalization was applied to normalize between arrays (Bolstad et al., 2003).
2.2 Inferring the influence of cultivation parameters on gene expression A design matrix was created, containing both main (or single) effects and interaction (or combinatorial) effects: each category within each cultivation parameter type is represented by a binary indicator column with 170 entries.
These columns represent the main effects, which indicate, for each array, under which category of a particular cultivation parameter type, the yeast was grown.
Interaction effect columns were obtained by applying the logic AND function to all possible pair-wise combinations of main effect columns.
Redundant columns and columns containing only zeros were removed, resulting in 112 columns, of which 37 represent main effects and 75 represent interaction effects.
These data are stored in the binary [AC] design matrix D. Here, A equals 170 and is the number of arrays.
C equals 112 and is the number of (combinatorial) cultivation parameters.
A forward stepwise ordinary least squares regression strategy was applied to each gene individually: y=X + (1) Here, yi denotes the measured gene expression level of a particular gene for array i, with i=1, ...,A; X is the predictor matrix, represents the regression coefficients and the error, which is assumed to be independent zero-mean normally distributed.
Initially, X contains only the intercept, i.e.
a column of A ones.
In an iterative fashion, columns from D are added to X.
For this we apply a leave-one-out cross validation (LOOCV) scheme, where a single sample is used for testing, while the remaining (A1) samples are used for training the regression model.
This is repeated such that each sample is used once as test data.
The column from D, with the smallest root-mean-squared (RMS) LOOCV error and absolute regression coefficient larger than one, is selected and added.
The iterative process of adding columns is discontinued when the P-value, as output by a t-test that determines whether the regression coefficient significantly differs from zero, exceeds 0.05/C.
To prevent the inclusion of spurious combinatorial effects, the following strategy is applied: when a combinatorial effect column is selected, we check whether the addition in explained variance is larger than the addition in explained variance when adding the two main effect columns that constitute the combinatorial effect.
Only in the cases where this is true, we add the combinatorial effect column.
Otherwise the two main effect columns are added, provided that they satisfy the P-value threshold and their absolute regression coefficients are larger than one.
Note that only coefficients larger than 1 or smaller than 1 are allowed.
In terms of the absolute expression measure, this means we only take into i174 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i175 i172i181 Combinatorial influence of environmental parameters on TF activity (a) (b) (c) Fig.2.
Schematic overview of the approach.
The goal is to build R, the optimal approximation of the discretized regression coefficients in R. (a) The coefficients in R are derived from a regression analysis, which assesses the influence of cultivation parameters on gene expression by employing these parameters as predictors in the regression model.
The discretization procedure maps non-zero regression weights to 1 or 1, depending on their sign.
(The schematic representation of R is given for five genes and three cultivation parameters.)
(b) The elements of R are determined by T and M. T is fixed and indicates binary TF binding potential to gene promoters.
The elements of M are estimated and indicate the activity of TFs as enhancers or repressors under the different (combinatorial) cultivation parameters.
A logic circuit derived from M is graphically depicted above the representation of M. (c) Visualization of the active TFs on the gene promoters of genes g1, g2 and g3 under cultivation parameter A. Enhancers are depicted as red boxes; repressors are depicted as green boxes.
(TF can bind the promoter of g1, but is not active under A.)
The height of a box indicates the enhancer or repressor strength.
The strength of a particular enhancer or repressor is the same for all genes.
A gene is upregulated when its activator strength, i.e.
the sum of the heights of the red boxes, is larger than the repressor strength, which equals the sum of the heights of the green boxes.
Downregulation is inferred in the opposite situation.
See text for details.
account expression differences of 1-fold change or more.
(The expression data are on log2 scale.)
So, we focus on the cases where a cultivation parameter has a large influence on expression.
Finally, the regression coefficients for all yeast genes are discretized and put in R ([GC]Z[1,0,1]), where G is the number of yeast genes.
The discretization procedure maps positive coefficients to 1 and negative coefficients to 1.
R is quite sparse since for most of the genes only two or three columns from D were selected as significant predictors.
2.3 TF binding data For 111 TFs we extracted their known regulatory sites from TRANSFAC (Wingender et al., 2000) and ChIPchip data (Harbison et al., 2004; MacIsaac et al., 2006) (no conservation, binding P-value cutoff 0.001).
These geneTF pairs were put in the binary [GF] TF-binding matrix T, where 1 indicates that a TF can bind a gene promoter.
F equals 111 and is the number of TFs.
2.4 Inferring TF activity and TF strengths The goal of our optimization problem is to infer the activity of TFs as a function of cultivation parameters, such that we can optimally explain the regression coefficients, which were distilled from the observed gene expression data.
These TF activities form tertiary matrix M ([F C] Z[1,0,1]).
A non-zero element in M indicates that a TF is activated under a cultivation parameter and either acts as an enhancer (1) or a repressor (1).
Other data used in the optimization problem are: TF binding matrix T ([GF]Z[0,1]), discretized regression coefficient matrix R ([GC] Z[1,0,1]) and its reconstructed version R ([GC]Z[1,0,1]).
First, from the tertiary matrix R two binary matrices with the same dimensions, R+ and R, are derived.
R+ has non-zero entries, where R contains 1s, and thus indicates the elements, where genes are upregulated under influence of a particular cultivation parameter.
R has non-zero entries, where R contains 1s, and thus indicates the downregulated elements.
A similar procedure is undertaken for tertiary matrix M, thereby obtaining M+, which contains the active enhancers and M, which contains the active repressors.
Now, all i175 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i176 i172i181 T.A.Knijnenburg et al.
variables consist of binary integers (and are restricted to remain binary integers).
The objective function for the optimization problem is as follows: minimize g,cI+ [Rgc R+gc]+ g,cI [Rgc Rgc]+ + F f =1 C c=1 [M+f ,c +Mf ,c] (2) where I+ is the set of index pairs referring to the elements where R is 1, and similarly, I refers to the negative elements of R. Thus, we only try to explain the non-zero elements of R, which represent the large expression changes due to the influence of the cultivation parameters.
The zero elements of R do not only contain cases where there is no change in expression, but they contain the whole spectrum of no change in expression up to moderately large changes in gene expression.
Therefore, we do not want to enforce TFs to be deactivated because of these zero elements.
The last term of Equation (2) restricts the model complexity by penalizing the number of activated TFs.
Parameter can be interpreted as the number of non-zero elements in R that a TF needs to help explain in order for it to be activated.
Below, the constraints of the optimization problem are stated.
These constraints are linear in M+, M, R+ and R, which are the variables in the system.
In the appendix a detailed explanation for constraints c5, c8 and c12 is given.
The first two constraints are straightforward.
Constraint c1 states that a TF cannot be an active repressor and an active enhancer at the same time.
Constraint c2 states that a gene cannot be upregulated and downregulated at the same time.
c1: M+fc +Mfc 1 f ,c c2: R+gc +Rgc 1 g,c Constraint c3 states that if there is at least one active enhancer in a gene promoter, i.e.
the inner product is positive then the gene can be upregulated, i.e.
the regression coefficient can be 1.
Constraint c4 is the analogue constraint for the case of active repressors.
Constraint c5 forces a gene to be either upregulated or downregulated, when there is at least one active enhancer or one active repressor in the gene promoter.
c3: Tg,M+c R+gc g,c c4: Tg,Mc Rgc g,c c5: Tg,M+c + Tg,Mc F (Rgc +R+gc) g,c To decide upon upregulation or downregulation when multiple active enhancers and repressors bind a promoter, four continuous variables are introduced: S+ and S; both ([F C]R[0,F]) and S+ and S; both ([F 1]R[1,F]).
S+fc , represents the strength of TF f as an enhancer under cultivation parameter c. S+fc is zero when M + fc is zero, i.e.
when f is not activated as an enhancer under c. This rule is stated in constraint c6.
S+fc equals the generic TF strength for f , S+f , when M + fc is one.
Thus, the strength of a TF f is the same for all genes under the cultivation parameters, where the gene is activated (and zero otherwise).
This rule is stated in constraints c7 and c8.
Analogue rules apply for S and S. The corresponding constraints c9, c10 and c11 are omitted for brevity.
c6: S+fc F M+fc f ,c c7: S+fc S+f f ,c c8: S+fc S+f F (M+fc 1) f ,c Constraint c12 states that when the sum of the strengths of active enhancers that bind a gene promoter is larger than its repressing counterpart, the gene is upregulated.
Constraint c13 encloses the reverse scenario.
Note that if an identical set of enhancers and repressors is active on a promoter, this will lead to the same reconstructed regression coefficient for any gene and under any cultivation parameter.
c12: Tg,S+c Tg,Sc (F2 +F2) R+gc F2 g,c c13: Tg,Sc Tg,S+c (F2 +F2) Rgc F2 g,c The optimization problem is implemented within the MATLAB environment and executed using the MOSEK optimization toolbox with standard settings for mixed integer optimization.
Given constraints c1 to c13, MOSEK estimates variables M+, M, R+, R, S+, S, S+ and S such that the optimization function in Equation (2) is minimized.
3 RESULTS 3.1 TF activity in response to changes in oxygen and carbon presence The regulatory network inference algorithm is run on a subset of the data.
In particular, we focus on oxygen and carbon; two environmental factors, which have a large and well studied effect on the transcriptional program of S.cerevisiae.
Four cultivation parameters are selected, i.e.
aeration type, carbon-limitation and the combinatorial cultivation parameters, carbon-limited aerobic growth and carbon-limited anaerobic growth.
Note that aeration type is actually a cultivation parameter type that assumes two values, i.e.
aerobic growth and anaerarobic growth.
Since these are mutually redundant, only aerobic growth was included in the regression model and subsequent optimization algorithm.
(Downregulation under aerobic growth and upregulation under anaerobic growth are interchangeable.)
There are 40 genes, which are influenced by at least two of these four cultivation parameters, i.e.
there are 40 rows in R with at least two non-zero elements in the four columns of interest.
These 40 genes are bound by 46 different TFs.
In this experiment is set to two.
The algorithm correctly inferred the regression coefficients of 58 of the 84 (70%) non-zero elements in R. A particularly large concentration of incorrectly predicted values appears toward the bottom of R, where zeros are predicted while the true expression coefficients are non-zero.
See Figure 3d.
This stems from the fact that the promoters of these genes have almost no motifs for the activated TFs, in which case the model cannot explain the up-or downregulation.
3.1.1 Inferred TF activity In total, nine different TFs were activated across the four cultivation parameters, some under more than one cultivation parameter.
Three of these TFs, HAP1, HAP2/3/4 and ROX1, have a significantly larger strength, when compared to the others.
See Figure 3a, b.
The large strength indicates their dominating effect on transcriptional regulation.
If one of these TFs is active and binds the promoter, it will determine the direction of transcriptional regulation.
For example under aerobic conditions (Aer) the promoter of gene PAU3 (the tenth gene from the bottom in Fig.3c) is bound by one active enhancer, i.e.
YAP7, and one active repressor, i.e.
ROX1.
Since the repressor strength of ROX1 is (much) larger than the enhancer strength of YAP7, the gene is (correctly) predicted to be downregulated.
Interestingly, in the resulting network for this data, the TF strength of ROX1 equals 45.9995, which is very close to the maximum value of 46, the number of TFs F. However, this number is slightly smaller than i176 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i177 i172i181 Combinatorial influence of environmental parameters on TF activity (a) (c) (b) Fig.3.
Overview of the results obtained for the oxygen and carbon limitation data.
(a) Inferred influence of cultivation parameters aerobic growth (Aer), anaerobic growth (Ana) and carbon limitation (Clim) on TF activity.
Only the three dominating TFs are reported.
(b) Representation of S, indicating the strength of the activated TFs under each of the four cultivation parameters.
Enhancers are depicted in red; repressors are depicted in green.
(c) Representation of T, indicating which gene promoters can be bound by the activated TFs.
The enhancer or repressor strengths for the four cultivation parameters are visualized by the colored blocks inside the rectangle that represents a binding site.
(d) Representation of R, indicating the inferred regression coefficients.
Upregulation is indicated by red; downregulation is indicated by green.
Incorrectly inferred elements are marked with a gray cross.
White boxes without a cross are the zero elements of R. These elements are not part of the optimization scheme.
the strength of HAP2/3/4 which has the maximal strength of 46.
This difference can be attributed to gene PET9 (the ninth gene from the top in Fig.3c).
Both HAP2/3/4 and ROX1 can bind the PET9 promoter.
To ensure that this gene is upregulated when grown aerobically, as was deduced from the regression analysis, the active enhancers should have a larger strength than the active repressors.
Therefore, the strength of ROX1 is set a bit smaller than the strength of HAP2/3/4, however, still large enough to dominate other active enhancers.
3.1.2 Regulation of gene expression by oxygen The role of the three dominant TFs in the regulation of gene expression by oxygen is widely reported in the literature.
Both HAP1 and the HAP2/3/4 complex activate genes in response to heme, which is synthesized only in the presence of oxygen (Zitomer and Lowry, 1992).
TF ROX1 is needed for the repression of hypoxic or heme-repressed genes under aerobic conditions (Lowry and Zitomer, 1988).
Also, the relation between carbon source and the HAP2/3/4 complex has been investigated.
The HAP2 and HAP3 proteins enable DNA binding of the complex, whereas HAP4 contains the transcriptional activation domain.
The synthesis of the activator subunit HAP4 is regulated by the carbon source.
More specifically, the expression of HAP4 is repressed by glucose, S.cerevisiaes preferred carbon source (Forsburg and Guarente, 1989).
Tai et al.
(2005) reports that HAP4 mRNA is present in carbon-limited cultivations even under anaerobic conditions, where HAP4 has no obvious role.
We can corroborate and even further substantiate these findings with the observation that the HAP4 protein is an activator under carbon-limited anaerobic conditions.
Note that all genes, which are upregulated under carbon-limited anaerobic growth are also upregulated under aerobic growth.
See the top 13 genes in Figure 3c.
The expression profile of one of these genes, COX5A, across all conditions is depicted in Figure 1.
This expression profile is typical for all the 13 members of this group.
It shows that these genes are most highly expressed when grown aerobically.
Yet, in the anaerobic case, where the expression is in general lower, these genes show different expression behavior in carbon-limited growth compared to other nutrient limitations.
That is, these genes have a higher expression level in carbon-limited cultivations, where there is hardly any glucose, compared to the situation, where glucose is abundant.
Also, for the other TFs, which are activated according to the inference algorithm, evidence is found in literature.
For example REB1, which acts as an enhancer under three cultivation parameters, is a RNA polymerase I enhancer binding protein as well as an activator for many genes transcribed by RNA polymerase II (Ju et al., 1990).
STE12 is known to activate genes associated with pseudohyphal (low oxygen) growth (Norman et al., 1999).
SUT1 is reported to encode a glucose transporter (Weierstall et al., 1999), however SUT1 also has a putative role in the regulation of some hypoxic genes (Regnacq et al., 2001).
In general, the precise regulatory role of these TFs in (an)aerobiosis and response to the carbon source is not known.
The results of this analysis provide hints for elucidating the regulatory mechanisms of these factors.
3.1.3 Setting Parameter , which restricts the model complexity by penalizing the number of activated TFs, is chosen using a 5-fold CV scheme.
The genes are divided into five parts, where consecutively four parts are used for training and one part is i177 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i178 i172i181 T.A.Knijnenburg et al.
Fig.4.
CV errors for different values of .
used for testing.
The M and S matrices, which are computed on the training set, are applied to the test set to obtain the reconstructed regression coefficients for the test set, Rtest .
The error on the test set is defined as: Err = 1 J g,cI Rtestgc Rtestgc (3) where I is the set of index pairs referring to the non-zero elements of Rtest and J the number of these non-zero elements.
The CV scheme is repeated 10 times.
Figure 4 depicts the average error over all CV runs.
For small values of , many TFs are activated in order to approximate the regression coefficients.
Clearly, this strategy is prone to overfitting, which is also illustrated by the large CV error.
For large values of , activating a TF is severely penalized, such that only a few TFs will be activated.
(For =20, no TF is activated and every element of Rtest is zero).
The high CV error in this case, indicates that a lot of true regulation is missed.
The optimal will be found between these extremes.
In this experiment, =2 led to the smallest CV error and was therefore selected.
3.2 Transcriptional regulation of nitrogen metabolism Across the conditions of the compendium, yeast was grown on six different nitrogen sources.
This inspired the second experiment, where we analyzed the transcriptional regulation of the genes that comprise the nitrogen compound metabolism category of GO biological processes (Ashburner et al., 2000).
A total of 119 of these genes are influenced by at least one cultivation parameter and bound by one of 78 different TFs.
In total, there are 68 cultivation parameters that cause up-or downregulation of at least one of these 119 genes.
The resulting transcription regulation network (with opt =2) revealed the activation of 14 different TFs under 28 different cultivation parameters, of which 11 are combinatorial.
Figure 5 depicts the network for the cultivation parameters, which are most straightforwardly related to nitrogen metabolism, i.e.
the different nitrogen sources, nitrogen as growth limiting element and combinatorial effects involving these cultivation parameters.
The six different nitrogen sources can be dichotomized into preferred and non-preferred nitrogen sources.
The preferred nitrogen sources are asparagine (Asn) and ammonium [in ammonium sulfate (AS)].
Proline (Pro), phenylalanine (Phe), methionine (Met) and leucine (Leu) are non-preferred (or poor) nitrogen sources (Boer et al., 2007; Magasanik and Kaiser, 2002).
In S.cerevisiae, the use of nitrogen sources is controlled by a transcriptional regulation mechanism known as nitrogen catabolite repression (NCR).
When a good nitrogen source is present, NCR shuts down the pathways for the use of poor nitrogen sources.
NCR is mediated by a four-member family of GATA-binding TFs: GLN3, GAT1, DAL80 and GZF3 (Hofman-Bang, 1999).
In the absence of a good nitrogen source, GLN3 is activated and in turn activates the transcription of NCR-sensitive genes.
Indeed, for three of the four non-preferred Fig.5.
Inferred TF activity derived from genes, which are involved in nitrogen metabolism.
Preferred nitrogen sources are printed in bold; non-preferred nitrogen sources are printed in italic style.
Abbreviations for the nitrogen and sulfur sources are explained in the text.
nitrogen sources, GLN3 acts as an enhancer.
When methionine is the nitrogen source, the MET31/32 complex is activated.
This complex controls the biosynthesis of sulfur containing amino acids (Blaiseau et al., 1997).
(Methionine is also used as a sulfur source.)
In the case of leucine, two additional TFs are activated; LEU3 and GCN4, the two key regulators in the regulation of branched-chain amino acid metabolism (Boer et al., 2005).
The inferred role of GCN4 as an activator in the presence of a poor nitrogen source and as a repressor in the presence of good nitrogen sources corroborates the work of Sosa et al.
(2003).
It further supports the fact that NCR is not solely achieved through the action of the abovementioned family of GATA factors, but conceivably also through GCN4.
3.2.1 Missing and dubious TF activity Remarkably, the other three GATA factors, GAT1, DAL80 and GZF3, are not part of generated network.
Inspection of the TF binding data in the promoters of the 119 nitrogen metabolism genes revealed that GAT1, DAL80 and GZF3 bind only 3, 4 and 0 genes, respectively.
This could indicate that their targets are not transcriptionally regulated under the influence of the cultivation parameters.
However, this observation should also be related to the ChIPchip data.
From TRANSFAC, we extracted many TFgene pairs, which are not present in the ChIPchip data.
This indicates that not all TF targets are detected by the ChIPchip experiments.
Furthermore, Gao et al.
(2004) estimate that 40% of the ChIPchip TF targets are non-functional.
Obviously, this complicates regulatory network inference.
Another dubious result was identified when analyzing the cases in which two or more TFs were active on a promoter.
In this experiment, there are 72 such cases, of which 10 are unique.
Amongst the most frequent cases, we found the combinatorial regulation of TFs, which have already been reported in literature, e.g.
the interplay between LEU3 and GCN4 (Boer et al., 2005) and that of CBF1 and GCN4 (OConnell et al., 1995).
Also, GLN3 and GCN4 were found activated together in a set of nine gene promoters.
These nine genes were upregulated under two cultivation parameters, i.e.
sulfur limitation and zinc limitation, where both GLN3 and GCN4 are enhancers.
However, under another cultivation parameter, i.e.
where leucine is used as a nitrogen source, the same genes were downregulated, where now GLN3 acts as a repressor (which is stronger than enhancer GCN4).
These results seem i178 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i179 i172i181 Combinatorial influence of environmental parameters on TF activity Fig.6.
Representation of S for the regulatory program inferred using the compendium.
Color coding is identical to Figure 3b.
implausible and imply that this regulation pattern should involve another TF, which might not be present in the employed TF binding data set.
Preliminary experiments with artificial datasets have demonstrated that especially missing TFs (simulated by removing colums from T) can have a large negative effect on the ability to reconstruct the correct regulatory network (results not shown).
3.3 Compendium analysis The algorithm was also run on the complete compendium for all genes that are up-or downregulated under at least two cultivation parameters and for all cultivation parameters that influence the expression of at least 10 genes (G=766,C =67,F =101,opt =5).
In the resulting regulatory network, 41 (61%) cultivation parameters activated at least one of the TFs, resulting in 29 (29%) different activated TFs in total.
See Figure 6.
Network inference on the complete dataset allows for a more rigorous and unbiased estimation of the regulatory program.
It reveals confounding factors, with respect to the previously discussed programs, which were based on a subset of the data.
For example, the regulatory program of GATA factor GLN3, as discussed before, is also depending on other (combinatorial) cultivation parameters, e.g.
zinc limitation and nitrogen limitation at low temperature.
These results offer interesting leads, however the combinatorial regulation of TFs, as inferred by this analysis, becomes complicated.
There are up to four active TFs on gene promoters.
This calls for an automated procedure that uses these inferred TF activities and accompanying strengths to derive logic rules, in which the influence of multiple TFs on transcriptional regulation is formalized.
Note that the inference algorithm was run on a selection of genes and cultivation parameters.
The number of variables and constraints in optimization problem is 4FC +2GC and 7FC +6GC, respectively, which becomes quite large for the complete dataset.
It is yet unclear (due to computation time) if converge is reached for the dataset with all genes and cultivation parameters.
4 DISCUSSION The transcriptional program of a cell is largely determined by its extracellular environment.
The accurate measurement of environmental parameters, e.g.
with chemostat cultures, have inspired several approaches that analyze the (combinatorial) effect of environmental parameters on gene expression.
In this study, we have, for the first time demonstrated how environmental parameters can be employed to derive transcriptional regulation networks.
In these networks, the cultivation parameters form the signals that trigger the activation or deactivation of TFs.
Since many TFs are regulated post-transcriptionally, this approach seems more natural than the often employed strategy of deducing the TF activity from the mRNA expression of TFs.
The inference algorithm was translated into a linear optimization problem, solvable without having to rely on greedy and/or heuristic search strategies.
The combinatorial regulatory code of multiple TFs that are able to bind a promoter, is modeled using the linearly weighted sum of inferred enhancers and repressor strengths.
Previous approaches have also modeled gene expression as a linearly weighted sum of TF contributions, e.g.
Gao et al.
(2004).
The main improvement of our method is the fact that the activity of TFs can be explicitly turned on or off, and that the inference algorithm optimizes this choice with respect to the direction of regulation, i.e whether a gene is up-or downregulated.
This strategy enables the inference of combinatorial effects between TFs.
For example, a repressor, which interacts directly with the TATA binding protein, thereby completely blocking transcription independent of the possible enhancers that bind the promoter, would acquire a strength that is larger than the sum of the strengths of all enhancers that can bind the promoter.
Thus, the repressor, when active, will cause downregulation of the gene, thereby nullifying the influence of the enhancers.
This is in contrast with the linear regression strategies, where these enhancers would still have influence on the gene expression level.
Additional validation experiments indicate that more pairs of TFs, which are simultaneously active according to our approach, are found to co-occur in PubMed abstracts when compared to TF pairs uncovered with Gao et al.
(2004).
This difference can be attributed to the fact that we decompose the expression in terms of cultivation parameters, and analyze these cultivation parameters separately.
When using only the expression data itself, some cultivation parameters (such as aeration type) can have a much larger influence than others, thereby dominating the expression pattern and thus controlling which TFs are found to be the most significant, leading to less diversity in activated TFs (and thus fewer TF pairs).
An overview of this comparison is published as Supplementary Material.
A future challenge lies in the integral interpretation of the inferred regulatory networks, which must be accompanied by a computational approach that derives logic rules, which are able to describe the interplay of multiple TFs on gene promoters.
ACKNOWLEDGEMENTS Funding: T.A.K.
and M.J.T.R.
are part of the Kluyver Centre for Genomics of Industrial Fermentation, which is supported by the Netherlands Genomics Initiative (NGI).
L.F.A.W.
is part of the Cancer Genomics Centre, which is supported by the Netherlands Genomics Initiative (NGI).
i179 [19:36 18/6/03 Bioinformatics-btn155.tex] Page: i180 i172i181 T.A.Knijnenburg et al.
Conflict of Interest: none declared.
Abstract The completion of the Human Genome Project lays a foundation for systematically studying the human genome from evolutionary history to precision medicine against diseases.
With the explosive growth of biological data, there is an increasing number of biological databases that have been developed in aid of human-related research.
Here we present a collection of human-related biological databases and provide a mini-review by classifying them into different categories according to their data types.
As human-related databases continue to grow not only in count but also in volume, challenges are ahead in big data storage, processing, exchange and curation.
Introduction As biological data accumulate at larger scales and increase at exponential paces, thanks principally to higher-throughput and lower-cost DNA sequencing technologies, the number of biological databases that have been developed to manage such data deluge is growing at ever-faster rates.
The major objec-tives of biological databases are not only to store, organize and share data in a structured and searchable manner with the aim to facilitate data retrieval and visualization for humans, but also to provide web application programming interfaces (APIs) for computers to exchange and integrate data from various database resources in an automated manner.
Therefore, developing databases to deal with gigantic volumes of biological data is a fundamentally essential task in bioinfor-matics.
To be short, biological databases integrate enormous amounts of omics data, serving as crucially important resources and becoming increasingly indispensable for scien-tists from wet-lab biologists to in silico bioinformaticians.
According to a report of 2014 Molecular Biology Database Collection in the journal Nucleic Acids Research, there are a sum of 1552 databases that are publicly accessible online [1].
It should be noted, however, that such count of publicly accessible databases is conservative.
In fact, there are some databases providing online services without publication in peer-reviewed journal (e.g., The RNA Modification Database at http://mods.rna.albany.edu) or being developed by commer-cial companies (e.g., Ingenuity Pathway Analysis at http://www.ingenuity.com/products/ipa), making them under-represented in the scientific community.
Considering the con-tinuously proliferating number of biological databases, it becomes increasingly daunting and time-consuming to navi-gate in the huge volume of databases of interest.
The nces and 56 Genomics Proteomics Bioinformatics 13 (2015) 5563 completion of the Human Genome Project in 2003 holds sig-nificant benefits for many fields from human evolution to per-sonalized healthcare and precision medicine.
In this report, we present a collection of biological databases relevant to human research and provide a mini-review by classifying them into different categories.
Database classification Biological databases are developed for diverse purposes, encompass various types of data at heterogeneous coverage and are curated at different levels with different methods, so that there are accordingly several different criteria applicable to database classification.
Scope of data coverage According to the scope of data coverage, biological databases can be classified as comprehensive and specialized databases.
Comprehensive databases cover different types of data from numerous species and typical examples are GenBank [2], European Molecular Biology Laboratory (EMBL) [3], and DNA Data Bank of Japan (DDBJ) [4].
These three databases were established as the International Nucleotide Sequence Database Collaboration in 1988 to collect and disseminate DNA and RNA sequences.
On the other hand, specialized databases contain specific types of data or data from specific organisms.
For example, WormBase [5] is for nematode biol-ogy and genomics and RiceWiki [6] is for community curation of rice genes.
Level of biocuration According to level of data curation, biological databases can roughly fall into primary and secondary or derivative data-bases.
Primary databases contain raw data as archival reposi-tory such as the NCBI Sequence Read Archive (SRA) [7], whereas secondary or derivative databases contain curated information as added value, e.g., NCBI RefSeq [8].
Method of biocuration As a consequence of the explosive growth of data, curation increasingly requires collective intelligence for collaborative data integration and annotation.
Therefore, biological data-bases can also be classified as (1) expert-curated databases, e.g., RefSeq [8] and TAIR, [9] and (2) community-curated databases, which are curated in a collective and collaborative manner by a number of researchers, e.g., LncRNAWiki [10] and GeneWiki [11].
Type of data managed According to the types of data managed in different databases, biological databases can roughly fall into the following cate-gories: (1) DNA, (2) RNA, (3) protein, (4) expression, (5) path-way, (6) disease, (7) nomenclature, (8) literature, and (9) standard and ontology.
Human-related databases Decoding the human genome bears great significance in, from a theoretical view, unveiling human evolutionary history, and from an application view, exploring personalized medicine against diverse diseases.
Considering the heterogeneity in data type, scope and curation, biological databases can be classified into multiple categories under different criteria as presented above, making it easier for people to effectively characterize databases and identify the database(s) of interest.
However, some databases are inaccessible over time or poorly main-tained/updated or even never used [12].
In this study, therefore, we assemble a collection of human-related databases that are widely used and currently accessible via the Internet (Table 1).
As database classification based on data type is informative and straightforward, we assign onemajor category to each data-base, albeit one databasemay correspond tomultiple categories.
In what follows, we focus on databases categorized in DNA, RNA, protein, expression, pathway and disease, respectively.
DNA databases A DNA database centers on managing DNA data from many or some specific species.
The primary function of human DNA databases includes establishment of the reference genome (e.g., NCBI RefSeq [8]), profiling of human genetic variation (e.g., dbSNP [13]), association of genotype with phenotype (e.g., EGA [14]), and identification of human microbiome metagen-omes (e.g., IMG/HMP [15]).
A representative example of DNA database is GenBank [2], a collection of all publicly-available DNA sequences (http://www.ncbi.nlm.nih.gov/gen-bank).
Since its inception in 1982, GenBank grows at an extra-ordinary pace and as of December 2014, contains over 184 billion nucleotide bases in more than 179 million sequences (http://www.ncbi.nlm.nih.gov/genbank/statistics).
RNA databases It is well acknowledged that only a tiny proportion of the human genome is transcribed into mRNAs, whereas the vast majority of the genome is transcribed into dark matter non-coding RNAs (ncRNAs) that do not encode proteins [16], including microRNAs (miRNAs), small nucleolar RNAs (snoRNAs), piwiRNAs (piRNAs), and long non-coding RNA (lncRNA).
Therefore, an increasing number of human RNA databases have been built for deciphering ncRNAs (e.g., GENCODE [17]), in particular lncRNAs that attract the rising interest (e.g., LncRNAWiki [10]), and characterizing their functions and interactions (e.g., RNAcentral [18]).
A representative example of RNA database is RNAcentral [18].
It provides unified access to the ncRNA sequence data supplied by multiple databases including Rfam [19], lncRNAdb [20], and miRBase [21] (http://rnacentral.org).
Protein databases The purpose of constructing protein databases includes collec-tion of universal proteins (e.g., UniProt [22]), identification of Table 1 Human-related biological databases* Name Link Brief description Refs.
Category# 1000 Genomes http://www.1000genomes.org A deep catalog of human genetic variation [17] DNA AFND http://www.allelefrequencies.net Allele Frequency Net Database [37] dbSNP http://www.ncbi.nlm.nih.gov/snp Database of single nucleotide polymorphisms [13] DEG http://www.essentialgene.org Database of Essential Genes [38] EGA http://www.ebi.ac.uk/ega European Genomephenome Archive [14] Ensembl http://www.ensembl.org Ensembl genome browser [39] euGenes http://eugenes.org Genomic information for eukaryotic organisms [40] GeneCards http://www.genecards.org Integrated database of human genes [41] IMG/HMP https://img.jgi.doe.gov/imgm_hmp Human Microbiome MetaGenomes [15] JASPAR http://jaspar.genereg.net Transcription factor binding profile database [42] JGA http://trace.ddbj.nig.ac.jp/jga Japanese Genotypephenotype Archive [43] KEGG http://www.genome.jp/kegg Kyoto Encyclopedia of Genes and Genomes [44] MITOMAP http://www.mitomap.org Human mitochondrial genome database [45] NCBI RefSeq http://www.ncbi.nlm.nih.gov/refseq NCBI Reference Sequence Database [8] PolymiRTS http://compbio.uthsc.edu/miRSNP Polymorphism in miRNAs and their Target Sites [46] UCSC Genome Browser http://genome.ucsc.edu UCSC Genome Browser database [47] ChIPBase http://deepbase.sysu.edu.cn/chipbase Database of transcriptional regulation of lncRNA and miRNA genes [48] RNA DARNED http://darned.ucc.ie DAtabase of RNa EDiting in humans [49] DIANA-LncBase http://diana.imis.athena-innovation.gr/ DianaTools/index.php?r=lncBase/index miRNA targets on lncRNAs [50] GENCODE http://www.gencodegenes.org Encyclopedia of genes and gene variants [17] H-DBAS http://www.h-invitational.jp/h-dbas Human-transcriptome DataBase for Alternative Splicing [51] HEXEvent http://hexevent.mmg.uci.edu Database of Human EXon splicing Events [52] LNCipedia http://www.lncipedia.org Annotated human lncRNA sequences [53] LncRNA2Target http://www.lncrna2target.org Database of differentially-expressed genes after lncRNA knockdown or overexpression [54] lncRNAdb http://www.lncrnadb.org lncRNA Database [20] lncRNASNP http://bioinfo.life.hust.edu.cn/ lncRNASNP Database of SNPs in lncRNAs [55] LncRNAWiki http://lncrna.big.ac.cn Human lncRNA Wiki [10] miRBase http://www.mirbase.org miRNA Database [21] miRTarBase http://mirtarbase.mbc.nctu.edu.tw Experimentally-validated miRNAtarget interactions [56] miRWalk http://mirwalk.uni-hd.de Database of miRNAtarget interactions [57] NONCODE http://www.noncode.org Database of ncRNA genes [58] NPInter http://www.bioinfo.org/NPInter Database of ncRNA interactions [59] RADAR http://RNAedit.com Rigorously Annotated Database of A-to-I RNA editing [60] piRNABank http://pirnabank.ibab.ac.in Database of piwi-interacting RNAs [61] RBPDB http://rbpdb.ccbr.utoronto.ca Database of RNA-binding specificities [62] RDB http://ndbserver.rutgers.edu The nucleic acid database [63] Rfam http://rfam.xfam.org Database of ncRNA families [19] RNAcentral http://rnacentral.org International database of ncRNA sequences [18] snoRNABase https://www-snorna.biotoul.fr Database of human H/ACA and C/D box snoRNAs [64] starBase http://starbase.sysu.edu.cn Database of ncRNA interaction networks [65] TarBase http://diana.imis.athena-innovation.gr/ DianaTools/index.php?r=tarbase/index Experimentally-validated miRNA:gene interactions [66] TargetScan http://www.targetscan.org Predicted miRNA targets in mammals [67] CATH http://cath.biochem.ucl.ac.uk Protein structure classification [68] Protein CPLM http://cplm.biocuckoo.org Compendium of Protein Lysine Modifications [69] DIP http://dip.doe-mbi.ucla.edu Database of Interacting Proteins [70] EKPD http://ekpd.biocuckoo.org Eukaryotic Kinase and Phosphatase Database [71] HPRD http://www.hprd.org Human Protein Reference Database [72] hUbiquitome http://bioinfo.bjmu.edu.cn/hubi/ Ubiquitination sites and cascades [73] InterPro http://www.ebi.ac.uk/interpro Protein sequence analysis and classification [74] MEROPS http://merops.sanger.ac.uk Database of proteolytic enzymes, their substrates, and inhibitors [75] MINT http://mint.bio.uniroma2.it/mint Molecular INTeraction Database [76] (continued) Zou D et al/ Human-related Biological Databases 57 Table 1 (continued) Name Link Brief description Refs.
Category# ModBase http://salilab.org/modbase Database of comparative protein structure models [77] mUbiSiDa http://reprod.njmu.edu.cn/mUbiSiDa Mammalian Ubiquitination Site Database [78] PANTHER http://www.pantherdb.org Protein ANalysis THrough Evolutionary Relationships [79] PDB http://www.rcsb.org/pdb Protein Data Bank for 3D structures of biological macromolecules [25] PDBe http://www.ebi.ac.uk/pdbe Protein Data Bank in Europe [80] Pfam http://pfam.xfam.org Database of conserved protein families and domains [23] PhosSNP http://phossnp.biocuckoo.org Genetic polymorphisms that influence protein phosphorylation [81] PIR http://pir.georgetown.edu Protein Information Resource [82] PROSITE http://www.expasy.org/prosite Database of protein domains, families and functional sites [83] SysPTM http://lifecenter.sgst.cn/SysPTM Post-translational modifications [84] TreeFam http://www.treefam.org Database of phylogenetic trees of animal species [24] UniPROBE http://thebrain.bwh.harvard.edu/ uniprobe Universal PBM Resource for Oligonucleotide Binding Evaluation [85] UniProt http://www.uniprot.org Universal protein resource [22] UUCD http://uucd.biocuckoo.org Ubiquitin and Ubiquitin-like Conjugation Database [86] ArrayExpress http://www.ebi.ac.uk/arrayexpress Database of functional genomics experiments [87] Expression BioGPS http://biogps.org Portal for querying and organizing gene annotation resources [88] Expression Atlas http://www.ebi.ac.uk/gxa Differential and baseline expression [27] Human Protein Atlas http://www.proteinatlas.org Tissue-based map of the human proteome [29] MOPED https://www.proteinspire.org Multi-Omics Profiling Expression Database [89] NCBI GEO http://www.ncbi.nlm.nih.gov/geo Gene Expression Omnibus [26] NRED http://nred.matticklab.com Database of lncRNA expression [90] ONCOMINE https://www.oncomine.org Cancer microarray database [91] PrimerBank http://pga.mgh.harvard.edu/primerbank Public resource for PCR primers [92] PRIDE http://www.ebi.ac.uk/pride PRoteomics IDEntifications [93] TiGER http://bioinfo.wilmer.jhu.edu/tiger Tissue-specific Gene Expression and Regulation [28] WikiCell http://www.wikicell.org Unified resource for Human transcriptomics research [94] CPDB http://consensuspathdb.org Database of human interaction networks [95] Pathway HMDB http://www.hmdb.ca Human Metabolome Database [96] KEGG PATHWAY KEGG pathway maps [30] MetaCyc http://metacyc.org Metabolic pathway database [97] Pathway Commons http://www.pathwaycommons.org Pathway commons [98] PID http://pid.nci.nih.gov Pathway Interaction Database [99] Reactome http://www.reactome.org Curated and peer-reviewed pathway database [100] UniPathway http://www.grenoble.prabi.fr/ obiwarehouse/unipathway Universal Pathway [101] AlzBase http://alz.big.ac.cn/alzBase Database for gene dysregulation in Alzheimers disease [102] Disease CADgene http://www.bioguo.org/CADgene Coronary Artery Disease gene database [103] COSMIC http://cancer.sanger.ac.uk Catalog Of Somatic Mutations In Cancer [104] DiseaseMeth http://bioinfo.hrbmu.edu.cn/diseasemeth Human disease methylation database [105] DisGeNET http://www.disgenet.org/web/ DisGeNET/v2.1 Genedisease associations [106] GOBO http://co.bmc.lu.se/gobo Gene expression-based Outcome for Breast cancer Online [107] GWAS Central http://www.gwascentral.org A comprehensive resource for the comparison and interrogation of genome-wide association studies [108] GWASdb http://jjwanglab.org/gwasdb Human genetic variants identified by genome-wide association studies [109] HbVar http://globin.cse.psu.edu/hbvar Hemoglobin variants and thalassemias [110] HGMD http://www.hgmd.org Human Gene Mutation Database [111] 58 Genomics Proteomics Bioinformatics 13 (2015) 5563 Table 1 (continued) Name Link Brief description Refs.
Category# ICGC http://icgc.org International Cancer Genome Consortium [33] IDbases http://structure.bmc.lu.se/idbase Immunodeficiency-causing variations [112] LncRNADisease http://cmbi.bjmu.edu.cn/lncrnadisease lncRNA and disease database [113] LOVD http://www.lovd.nl Leiden open (source) Variation Database [114] MalaCards http://www.malacards.org Human maladies and their annotations [115] MethHC http://methhc.mbc.nctu.edu.tw Database of DNA methylation and gene expression in human cancer [116] MethyCancer http://methycancer.psych.ac.cn Database of human DNAMethylation and cancer [117] miR2Disease http://www.miR2Disease.org Database for miRNA deregulation in human disease [118] MITOMAP http://www.mitomap.org/MITOMAP Polymorphisms and mutations in human mitochondrial DNA [119] NHGRI GWAS Catalog http://www.genome.gov/gwastudies Curated resource of SNP-trait associations [120] OMIM http://omim.org Online Mendelian Inheritance in Man [121] T2D@ZJU http://tcm.zju.edu.cn/t2d Connections associated with type 2 diabetes [122] TCGA http://cancergenome.nih.gov The Cancer Genome Atlas [32] Universal Mutation Database http://www.umd.be/ Locus-specific database [123] ViRBase http://www.rna-society.org/virbase Virushost ncRNA associated interactions [124] GO http://geneontology.org Gene ontology [125] Standard and ontology HGNC http://www.genenames.org Database of human gene names [126] Europe PMC http://europepmc.org Literature database in Europe [127] Literature PubMed http://www.ncbi.nlm.nih.gov/pubmed Database of biomedical literature from MEDLINE [128] PubMed Central http://www.ncbi.nlm.nih.gov/pmc Free full-text literature archive [129] Note: *This collection, however, by no means pictures the whole range of human-related databases that are currently available.
Primary databases (DDBJ/EMBL/GenBank) are not shown, as they contain raw data.
#A database may correspond to multiple categories and only one major category is shown here.
Zou D et al/ Human-related Biological Databases 59 protein families and domains (e.g., Pfam [23]), reconstruction of phylogenetic trees (e.g., TreeFam [24]), and profiling of pro-tein structures (e.g., PDB [25]).
A representative example of protein database is PDB, the main primary database for 3D structures of biological macromolecules determined by X-ray crystallography and NMR.
Established in 1971, PDB contains 105,465 biological macromolecular structures as of 30 December 2014, in which 27,393 entries belong to human (http://www.rcsb.org/pdb).
Another example is the Universal Protein Resource (UniProt).
As a collaborative project between EMBL-EBI, Swiss Institute of Bioinformatics (SIB), and Protein Information Resource (PIR), UniProt provides a comprehensive, high-quality, and freely-accessible resource of protein sequence and functional information.
Currently, UniProt includes three member databases: UniProt Knowledgebase (UniProtKB), UniProt Reference Clusters (UniRef), and UniProt Archive (UniParc).
In addition, UniProtKB consists of two sections: Swiss-Prot (containing a collection of 547,357 manually-annotated and-reviewed proteins as of January 2015) and TrEMBL (containing a collection of 89,451,166 un-reviewed proteins as of January 2015) (http://www.uniprot.org).
Expression databases Expression databases can be used for various purposes, includ-ing archiving expression data (e.g., GEO [26]), detecting dif-ferential and baseline expression (e.g., Expression Atlas [27]), exploring tissue-specific gene expression and regulation (e.g., TiGER [28]), and profiling expression information based on both RNA and protein data (e.g., Human Protein Atlas [29]).
A representative case of expression database is Human Protein Atlas.
As of 30 December 2014, it encompasses expres-sion profiles for a large majority of human protein-coding genes based on both RNA (transcriptome analysis based on 213 tis-sue and cell line samples) and protein data (proteome analysis based on 24,028 antibodies) (http://www.proteinatlas.org).
Pathway databases Pathway databases contain biological pathways for metabolic, signaling, and regulatory pathway analysis.
A representative example is KEGG PATHWAY [30], a curated biological path-way resource on the molecular interaction and reaction net-works.
As the core of KEGG, KEGG PATHWAY integrates many entities that are stored inKEGGsibling databases, includ-ing genes, proteins, RNAs, chemical compounds, and chemical reactions (http://www.genome.jp/kegg/pathway.html).
Disease databases There are at least 200 forms of cancer in the world, causing 14.6% of all human deaths (http://en.wikipedia.org/wiki/ Cancer).
Thus, obtaining complete cancer genomes and identifying molecular mutations and abnormal genes can pro-vide new insights for cancer prevention, detection, and 60 Genomics Proteomics Bioinformatics 13 (2015) 5563 eventually, personalized treatment [31].
Toward this end, there are two well-known cancer projects, viz., The Cancer Genome Atlas (TCGA) [32] and International Cancer Genome Consortium (ICGC) [33].
TCGA, founded in 2006 by the National Cancer Institute and National Human Genome Research Institute at the National Institutes of Health, aims to collect a wide diversity of omics data (including exome, SNP, mRNA, miRNA, and methylation) for more than 20 dif-ferent types of human cancer (http://cancergenome.nih.gov).
Unlike TCGA, ICGC is a voluntary collaborative organization initiated in 2008 and open to all cancer and genomic researchers in the world.
It aims to obtain a comprehensive description of genomic, transcriptomic, and epigenomic changes in 50 differ-ent tumor types and/or subtypes, which are of clinical and soci-etal importance across the globe (http://icgc.org).
Perspectives Here we summarize a collection of biological databases rele-vant to human research.
This collection, however, by no means pictures the whole range of human-related databases that are currently available.
As primary databases store raw data, data-bases in this collection are most derivative databases, which are built from primary databases and contain curated informa-tion for different data types, and thus would be of great useful-ness for studying the human genome.
In the era of big data, human-related biological databases continue to grow not only in count but also in volume, posing unprecedented challenges in data storage, processing, exchange, and curation.
From this point, it would be necessary to establish a cloud computing platform to store and process such big data and facilitate con-struction/update of a secondary or derivative database [34].
As biological databases are physically distributed and heteroge-neous in data type and format, it is additionally required to build web open APIs to ease data exchange and sharing among different resources [35].
The last but not the least is curation, which becomes an indispensable part in biological databases, principally because curation involves added value by standard-ization and quality control and accordingly enhances data interoperability and consistency [36].
Taken together, biologi-cal databases hold great utilities for human research and can be regarded as an indicator of our potential to translate big data into big discovery.
Considering the current situation in China when compared to other countries, it is our hope that this report may raise the general awareness, albeit better improved nowadays, of the significant role of human-related biological databases not only for academic studies but also for clinical applications.
Competing interests The authors declared that there are no competing interests.
Acknowledgements This work was supported by the 100-Talent Program of Chinese Academy of Sciences, the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No.
XDB13040500), and the National High-tech R&D Program (863 Program; Grant No.
2012AA020409) by the Ministry of Science and Technology of China awarded to ZZ.
ABSTRACT Motivation: Large-scale phenotyping projects such as the Sanger Mouse Genetics project are ongoing efforts to help identify the influ-ences of genes and their modification on phenotypes.
Genepheno-type relations are crucial to the improvement of our understanding of human heritable diseases as well as the development of drugs.
However, given that there are20 000 genes in higher vertebrate gen-omes and the experimental verification of genephenotype relations requires a lot of resources, methods are needed that determine good candidates for testing.
Results: In this study, we applied an association rule mining approach to the identification of promising secondary phenotype candidates.
The predictions rely on a large genephenotype annotation set that is used to find occurrence patterns of phenotypes.
Applying an association rule mining approach, we could identify 1967 secondary phenotype hypotheses that cover 244 genes and 136 phenotypes.
Using two automated and one manual evaluation strategies, we demonstrate that the secondary phenotype candidates possess biological relevance to the genes they are predicted for.
From the results we conclude that the predicted secondary phenotypes constitute good candidates to be experimentally tested and confirmed.
Availability: The secondary phenotype candidates can be browsed through at http://www.sanger.ac.uk/resources/databases/phenodigm/ gene/secondaryphenotype/list.
Contact: ao5@sanger.ac.uk or ds5@sanger.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION A causative gene has not yet been identified for almost half of the existing human heritable diseases (Schofield et al., 2012).
Without the knowledge of the molecular basis of a disease, treat-ment possibilities are limited to treating symptoms instead of curing the underlying defects.
In order to be able to find cures and prevention mechanisms for human genetic disorders, we need to comprehensively understand how each disease originates and progresses over time.
A collection of human diseases to-gether with confirmed and speculative causes is available from resources such as the Online Mendelian Inheritance in Man (OMIM) (Amberger et al., 2011) or Orphanet (Ayme, 2003) database.
In the quest for identifying causative genes for human genetic disorders, model organisms have gained increasing importance due to the opportunities arising from targeted gene modifications.
For example, the mouse shares 99% of genes with humans, and gene modifications leading to phenotypes characteristic for a disease may offer clues to the origins of this disease (Rosenthal and Brown, 2007).
Experimental results of mutagenesis experiments are stored in species-specific Model Organism Database (MOD)s (Leonelli and Ankeny, 2012), e.g.
the Sanger Mouse Genetics Project (Sanger-MGP) (White et al., 2013), WormBase (Yook et al., 2012), the Mouse Genome Database (MGD) (Bult et al., 2012) or FlyBase (Drysdale and FlyBase Consortium, 2008).
The Sanger-MGP is part of the International Mouse Phenotyping Consortium (IMPC) project that aims to identify the phenotypic implications of 20 000 genes by 2021 (Brown and Moore, 2012).
In the framework of this project genetically mod-ified mouse models are assessed according to 20 pre-defined standard operating procedures (SOPs) that are linked to meas-urable physical parameters to ascertain the implications of gen-etic mutations on phenotypes (Mallon et al., 2012).
An example of a SOP is the assessment of the grip strength of mice at the age of 9 weeks to assess their neuromuscular function as muscle strength (https://www.mousephenotype.org/impress/protocol/ 83/7).
Mammalian Phenotype Ontology (MP) (Smith and Eppig, 2009) annotations are assigned using a reference range method followed by an expert review.
Later studies explore the application of other statistical methods to assign MP phenotype annotations based on the obtained parameter readings (Beck et al., 2009; Karp et al., 2012a), however, these methods only cover a subset of the phenotypes covered by the 20 SOPs.
The process of assessing physical measurements in accordance with the 20 pre-defined SOPs is referred to as primary phenotyp-ing (Justice, 2008).
According to the European Mouse Disease Clinic (EUMODIC) web page (http://www.eumodic.org/) A distributed network of centres with in depth expertise in a number of phenotyping domains will undertake more complex, sec-ondary phenotyping screens and apply them to a subset of the mice which have shown interesting phenotypes in the primary screen..
However, with the increasing amount of genes being assessed in the primary phenotyping screen, a manual investigation for inter-esting results from the primary screens becomes impossible.
In addition, the manual assessment of experimental results is time consuming, expensive and requires trained biologists.
Therefore, automated methods enabling the search for promising secondary phenotypes are needed to complement the results obtained from the primary screens.
Existing automated solutions include the prediction of pheno-types based on orthologous genes (Groth et al., 2007; McGary et al., 2010) as well as functional annotations of genes*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com standard operating procedure nine https://www.mousephenotype.org/impress/protocol/83/7 https://www.mousephenotype.org/impress/protocol/83/7 1XPath error Undefined namespace prefix XPath error Undefined namespace prefix (King et al., 2003).
However, orthologous genes do not necessar-ily exhibit the same function or expression patterns across differ-ent species and therefore, do not always provide reliable answers.
A solution relying only on existing phenotype annotations could overcome the problem in differing gene function and resulting phenotypes across different species.
To the best of our know-ledge, the prediction of secondary phenotypes from primary screen annotations in combination with literature-curated pheno-types in mouse has not been addressed before.
Here, we present an association rule mining approach that enables the identification of potential secondary phenotype screens in mouse using data from MGD, and complementing the primary phenotype screens in Sanger-MGP.
Applying asso-ciation rule mining, we were able to discover 188 rules, covering 242 phenotypes and leading to 1967 predictions for secondary phenotypes for 244 genes contained in the Sanger-MGP data-base.
These 1967 suggested genephenotype associations include 136 unique phenotypes for which new assays can be defined for.
The predicted associations are neither contained in MGD nor Sanger-MGP.
We automatically as well as manually evaluated the secondary phenotype predictions and can demonstrate that our results show viable candidates.
In conclusion, we believe that novel biological hypotheses and secondary phenotype screens can be formulated from the predicted secondary phenotypes.
2 METHODS Figure 1 illustrates the overall workflow of this study.
The following subsections describe the prediction approach and the utilized datasets in detail.
2.1 Prediction of secondary phenotype candidates for mouse genes To determine candidates for secondary phenotyping, we first analysed MGDs phenotype annotations for mouse mutants.
We hypothesized that phenotypes that significantly co-occur with each other more often than expected by chance, given the overall amount of phenotype anno-tations, constitute good candidates for the secondary phenotype experiments.
For example, it is known that body weight correlates with bone density or grip strength and changes in body weight often lead to changes in the correlated phenotypes (Karp et al., 2012a, b; Valdar et al., 2006).
Using a large dataset of phenotype annotations, we can determine pairs of phenotypes that may be biologically linked.
Association rule mining was originally used to find patterns of items that are frequently purchased together in one transaction in a supermar-ket.
Each association rule assigns a probability to an implication based on the dataset, e.g.
how likely is it that someone who bought bread and milk also purchased butter.
In the Bioinformatics domain, association rule mining has been previously successfully applied to large annotation sets with the aim to find relationships between gene functions described with Gene Ontology (GO) (Botstein et al., 2000; Kumar et al., 2004; Manda et al., 2012).
As the goal of determining significantly co-occurring con-cepts to define relationships is the same here, association rule mining can also be applied.
We used the apriori (http://www.borgelt.net/doc/apriori/ apriori.html) software implementation (Agrawal et al., 1996; Borgelt, 2003) with the following parameter settings:-tr-s-6-m2-n2-c90-ep-v %e with-tr to enforce the output of association rules instead of item sets,-s-6 to obtain only rules that are supported by at least six item sets,-m2 to include only rules with a minimum of two items,-n2 to include only rules with a maximum of two items,-c90 to only allow rules with a confidence of 90%,-ep to provide P-values for each rule and-v %e to add the P-value separated by space to each of the rules.
The input to the apriori software was the set of literature-curated phenotype annotations of mouse genes and the output of rules of the type phenotype_1 !
pheno-type_2.
As a starting point, we limited the output to rules including only two phenotypes to avoid complex dependencies between the annotations.
However, in future work we aim to extend the approach to address more complex dependencies between phenotypes.
The two parameters that are used to narrow down the associations rules to obtain meaningful, biologically related phenotypes, are support and confidence.
We set the support for association rules to six which means that a minimum of six genes have to be annotated with both phenotype_1 and phenotype_2.
The confidence corresponds to the ratio of genes being annotated with phenotype_1 as well as phenotype_2 over the genes that are only annotated with phenotype_1.
In our case, at least 90% of the genes annotated with phenotype_1 must have also pheno-type_2 as annotation in order for this rule to be reported.
Changing either parameter may lead to the report of different association rules in the output.
We considered this to be conservative settings for an initial study, and the determination of the ideal settings for both parameters is subject to future work.
Rules are returned together with their corresponding P-value to enable potential further filtering and user confidence, e.g.
MP:00047255-MP:00094480 MP:00056065-MP:00094483.9905e-212 MP:00056065-MP:00095574.22726e-182 MP:00002455-MP:00111712.64518e-125 All extracted rules are then sorted according to the phenotypes includ-ing them, i.e.
one phenotype may potentially be associated with more than one secondary phenotype candidate.
As shown by the rules given before as an example, a decreased platelet ATP level phenotype (MP:0009448) would be associated with an increased bleeding time phenotypes (MP:0005606) and a decreased platelet serotonin level (MP:0004725).
Following this procedure will lead to a list of mapped phenotypes including the phenotypes from the high-throughput assess-ment in the primary phenotype screening defined in the SOPs.
Therefore, the mapped phenotypes are then filtered to exclude the phenotypes cov-ered by the Sanger-MGP SOPs.
Because of this filtering, we obtain only Fig.1.
Overall workflow of the study.
After determining-related pheno-types, the primary phenotype annotations assigned to genes in Sanger-MGP are enriched with potentially related phenotypes.
The additional, predicted secondary phenotypes are evaluated in several steps i53 Automatically generating secondary phenotyping hypotheses , , s ;Karp etal.,2012apredictions for secondary phenotypes that have not been included in the primary screens.
For all the genes contained in Sanger-MGP, we then generated a list of secondary phenotype annotation predictions by going through all the existing phenotype annotations for a gene and adding those phenotypes that have been mapped based on co-occurrence.
Then we removed all genephenotype associations that have been identified already and are contained in either MGD or Sanger-MGP to only generate potentially novel links between genes and phenotypes.
We refer to the remaining phenotype annotations as predicted secondary phenotypes.
We chose the MGD phenotype annotations for gene knockouts as basis for our predictions and downloaded the report file on July 20, 2013.
The downloaded file comprised 126522 MP annotations for 9447 genes, covering 7393 unique MP concepts.
The Sanger-MGP covers 20 SOPs that correspond to 367 MP annotations.
Deducting the 367 MP that are covered by the SOPs from the unique number of MP concepts in MGD, provides the target phenotype annotation space.
This means that 7027 unique phenotype concepts can be potentially associated with any of the 725 genes that had been assessed by the primary screens in the Sanger-MGP at the time this study was conducted.
All the phenotype annotation datasets were applied without conducting a taxonomic closure on the annotations.
However, once the secondary phenotypes have been pre-dicted, corresponding assays would have to be determined to test the generated phenotype hypotheses.
2.2 Evaluation of secondary phenotype predictions We evaluated the secondary phenotype predictions automatically as well as manually.
The automated evaluation was realized by applying the secondary phenotype candidates in two use cases for phenotype annota-tions: the clustering of genes according to phenotypes leading to clusters of gene function, and the prediction of disease gene candidates by com-paring disease phenotypes with phenotypes that have been determined to be affected by a gene mutation.
In both use cases, we applied first pheno-type annotations determined during the primary screens, and after that a combination of the primary screen annotations together with the predic-tions for secondary phenotypes.
We assume that if the performance im-proves when adding the secondary phenotype predictions, the secondary phenotypes possess biological validity.
In addition, we manually investi-gated five diseases further where the predictability of at least one known causative gene improved.
More information about the evaluation of the secondary phenotypes is provided in the following subsections.
2.2.1 Automated evaluation based on gene function In previous studies, it has been demonstrated that phenotype annotations can be used to determine biologically meaningful clusters with respect to gene function and protein interactions (van Driel et al., 2006).
Oti et al.
ex-tended the method to validate the content of three human phenotype databases with respect to consistency and completeness.
We assume that the secondary phenotype predictions once added to the annotations assigned in the primary screens improve consistency and completeness of the phenotype data.
Therefore, we applied the method introduced by Oti et al.
relying on the biological coherence of gene clusters built on pheno-type similarity.
The biological coherence is calculated based on the over-lap of GO annotations among all the genes falling into one cluster.
To assess the biological coherence without and with the predicted sec-ondary phenotypes, we generated gene clusters based on the primary phenotypes solely, as well as clusters based on the primary and secondary phenotype data in conjunction.
Before the actual clustering step, we per-formed a taxonomic closure based on MP, which means that all super-classes for each assigned phenotype annotation were added to a genes phenotype annotation set.
Clusters were formed based on the phenotype similarities, and the similarity between pairs of genes based on their phenotype annotations using a Jaccard coefficient (the ratio of shared phenotypes over the unique set of phenotypes assigned to both the genes).
Genes were clustered with respect to their phenotype similarity using average linkage clustering.
Clusters were determined by applying the Dynamic Treecut package in R (Langfelder et al., 2008) to the obtained dendrogram.
We set the parameters of the Dynamic Treecut package to require a minimum of two genes falling into one cluster.
For each of the determined cluster, the biological coherence was calculated with Cc= Xn i;j Ci; j=n; 1 where C(i,j) is the term overlap between gene i and gene j, and n is the number of genes in this cluster.
The overall biological coherence score for all clusters is obtained by averaging the individual scores for the clusters: Ct= Xm i=1 Cc=m; 2 where m is the number of clusters formed for a particular dataset.
In compliance with the method described in (Oti et al., 2009), the datasets are not directly compared, instead they are compared to rando-mized datasets to correct for gene annotation biases.
For this purpose, we randomized each of the two phenotype annotation setsprimary and secondarymaintaining the number and uniqueness of phenotype anno-tations per gene.
We randomized the original set of annotations 1000 times leading to 2002 phenotype annotation sets in total.
We increased the number of randomizations from 30 to 1000 to compensate for the fact that Sanger-MGP contains a number of genes that are poorly described in terms of gene function and may generate a high variation of coherence score ratios otherwise.
For each phenotype annotation set, the ratio of the overall biological coherence Ct of the respective original phenotype annotation set (either primary or secondary) over the rando-mized data is calculated.
If this ratio is41, the biological coherence of the original dataset is greater than the randomization data; vice versa for scores in the range [0,1], the biological coherence for the randomized data exceeds the coherence of the respective original dataset.
The ratio scores are then summarized in box plots and the difference between all the ratios of both datasets is calculated with a non-parametric, two-sided Wilcoxon rank-sum test implemented in R. Figure 2 illustrates this evalu-ation step.
We assessed gene cluster coherence based on functional annotations of mouse genes.
For this purpose, we downloaded the GO annotations of Fig.2.
Illustration of the calculation of biological coherence scores to evaluate secondary phenotype predictions.
Boxes that possess the same background colour are based on the same analysis scripts, only the input data differ (either randomized or original data).
Black boxes symbolize the ratio of the biological coherence original versus randomized data which are used as input for the box plots depicted in Figure 3 i54 A.Oellrich et al.
20 , , , Sanger Mouse Genetics Project , s s s----, , , s s s s mouse genes from the MGD database on July 12, 2013 (ftp://ftp.inform atics.jax.org/pub/reports/gene_association.mgi).
The dataset comprised annotations for 25 499 MGD marker accession identifiers with 13 551 unique GO concepts, with an average of 11.64 GO annotations per gene.
Using the original Sanger-MGP dataset with primary annotations only, we obtain 33 clusters for the 480 genes investigated that are then assessed for the biological coherence based on their gene function annotations.
2.2.2 Automated evaluation based on disease gene candidate predictions Among other tools for disease gene candidate prediction, PhenoDigm uses phenotype annotations to predict gene candidates underlying a disease (Smedley et al., 2013).
Disease gene candidates are predicted based on the primary phenotype annotations assigned to mouse and zebrafish models and their phenotypic similarity to human genetic disorders described in OMIM (http://www.human-phenotype-ontology.
org/contao/index.php/downloads.html).
The better the overlap of pheno-types between a model and a disease, the higher the corresponding knock-out gene is ranked for this disease.
To assess the performance of a ranking algorithm, commonly Receiver Operating Characteristic (ROC) curves are used that are calculated based on a benchmark dataset.
In our case, we used known genedisease asso-ciations to assess the value of the predictions.
If the secondary phenotypes add value to the predictions, then a performance increase should be vis-ible from the Area Under Curve (AUC) of the two ROC curves (one for the predictions based on primary phenotype data, and one for the pre-dictions based on primary and secondary phenotype data).
To test whether the increase in the AUC is significant, we used a two-sided test for ROC curves available online (http://vassarstats.net/roc_comp.html; Hanley and McNeil, 1982).
Using PhenoDigm as an automated evaluation algorithm of the sec-ondary phenotype predictions required a benchmark set of known gene disease associations.
If the secondary phenotype annotations improve the phenotypic overlap of genes and diseases, the ROC curves used for the evaluation should show an improvement.
To generate the ROC curves, we used the genedisease associations contained in OMIMs MorbidMap file (http://omim.org/downloads), which was downloaded on July 20, 2013.
This dataset comprised 3781 genedisease associations, including 2530 genes and 3158 diseases.
2.2.3 Manual evaluation To evaluate some of the secondary pheno-type predictions, we manually investigated some of the cases were the predictability of known disease genes improved when adding the pre-dicted secondary phenotypes.
We chose five genedisease associations where the gene improved with respect to its rank for the disease and looked based on which annotations the match between disease and gene could be made.
The information concerning the matched phenotype annotations of a disease and a gene is contained in Supplementary Material S1.
2.3 Implementation of PhenoDigm extension to provide secondary phenotype predictions online To enable access to the predicted secondary phenotypes, we imple-mented an extension to our online tool PhenoDigm (Smedley et al., 2013) that predicts causative genes for human heritable disorders.
The extension is, as well as the original tool, implemented using the Play!
Framework (http://www.playframework.com/) (version 1.2.5), jQuery (http://jquery.com/) (version 1.6.4) and jQuery UI (http://jquer yui.com/) (version 1.9.1).
The secondary phenotype predictions were imported into PhenoDigms underlying MySQL database (http://www.mysql.com/) by extending the database schema.
However, secondary phenotype predictions are not incorporated into PhenoDigms disease gene candidate predictions available from the web page, unless experimentally confirmed and integrated into one of the phenotype an-notation databases.
3 RESULTS Applying association rule mining, we were able to identify 188 rules (provided in Supplementary Material S1) that lead to sec-ondary phenotype hypotheses for 244 Sanger-MGP genes.
In total, we could predict 1967 novel genephenotype associations containing 136 unique phenotypes that are not contained in MGD.
Out of these 1967 genephenotype relationships, 47 were covered by the taxonomy of the ontology, i.e.
the predicted phenotypes were ancestor concepts of annotations already used for one particular gene.
The 136 unique phenotype concepts span 23 of MGDs 30 top level phenotypes, such as tumorigenesis (MP:0002006), nervous system phenotype (MP:0003631) or muscle phenotype (MP:0005369), showing the diversity of pheno-types that could be added to the annotation of genes.
The 136 phenotypes also cover different hierarchy levels in the ontology spanning from the second to the 11th level, with the highest group falling into level 6 (all measured as shortest distance from the root node of the MP ontology).
For example, adeno-hypophysis hypoplasia (MP:0008365) as well as abnormal cranium size (MP:0010031) are suggested as secondary phenotypes.
In general, the deeper an ontology term is the more specific is the concept it is representing.
This means that the predictions not only span a variety of different high level phenotypes but also add detailed information to the genes they are associated with which allows for a better characterization of individual genes.
Supplementary Material S1 provides all the predicted secondary phenotype annotations together with additional information such as the high level phenotype, term name and frequency of occurrence in the prediction dataset.
3.1 Predicted secondary phenotypes significantly improve the biological coherence of gene clusters In recent studies, phenotypes have successfully been applied to determine disease gene candidates and gene function (Smedley et al., 2013; van Driel et al., 2006).
In order to asses the validity and quality of the predicted secondary phenotype annotations, we assessed the biological coherence of gene clus-ters, built based on phenotype similarity between genes.
Applying the method described by Oti et al.
first to the primary phenotype annotations only, and then to both primary and predicted secondary phenotype annotations, shows that the bio-logical coherence of clusters increases when adding the predicted secondary phenotype annotations.
The obtained results are de-picted in Figure 3.
In addition to calculating the biological coherence for both datasets, we determined the significance of the fold-increase of the coherence of the clusters.
Using a two-sided Wilcoxon signed-rank test (as implemented in R, =0.05), we obtained a P-value of 2.2 1016, indicating a significant improvement when adding secondary phenotype annotations to the previously confirmed in the primary phenotype scans.
These results suggest that the predicted secondary phenotypes possess biological val-idity but will have to be experimentally verified in secondary phenotype screens.
i55 Automatically generating secondary phenotyping hypotheses 12 ftp://ftp.informatics.jax.org/pub/reports/gene_association.mgi ftp://ftp.informatics.jax.org/pub/reports/gene_association.mgi 4 , ,20 , , ,3.2 Secondary phenotypes significantly improve the predictability of disease gene candidates In addition to assessing the biological coherence of gene clusters, we also verified the predicted secondary phenotype annotations by applying them in a second application use case: the prediction of disease gene candidates.
One tool that already uses the pri-mary phenotype data of genes to predict disease gene candidates is PhenoDigm (Smedley et al., 2013).
To assess whether the sec-ondary phenotypes possess biological validity, we first used only the primary annotations to predict disease gene candidates and then added the secondary phenotype annotations.
For both pre-dictions, we calculated the ROC curves, using genedisease as-sociations contained in MGD as benchmark dataset.
Both the obtained ROC curves are depicted in Figure 4.
Applying a one-tailed Students t-test (=0.05) to the ROC curves, we obtain a P-value of P=0.02.
3.3 The Coq9 mouse improves as a model for primary coenzyme Q10 deficiency 5 To further assess the value added by the predicted secondary phenotypes, we manually assessed diseases that show rank im-provements for known causative genes.
We determined the number and particular phenotypes that could be matched be-tween models and diseases, where causative genes improved in the ranking as disease candidates.
In the best case, the gene Coq9 (MGI:1915164) that has been recognized as a being disrupted in cases of Primary coenzyme Q10 defiency 5 (COQ10D5; MIM:#614654) improves from rank 181 to rank 2 based on the secondary phenotype annotations.
Using the annotations as-signed in the primary screens, only one pair of matching phenotypes can be determined: Hyperreflexia (HP:0001347) and hyperactivity (MP:0001399).
Applying in addition the pre-dicted secondary phenotypes, other signs and symptoms of this disease, such as Postnatal microcephaly (HP:0005484) and Left ventricular hypertrophy (HP:0001712), are detected.
Another example for gene rank improvement is that the Cfh (MGI:88385) gene was ranked in second place after adding the predicted secondary phenotypes (rank 43 when using only pri-mary phenotypes) for Complement factor H deficiency (MIM:#609814).
Including the predicted secondary phenotypes allows for a coverage of the following additional phenotypes: Thickening of the glomerular basement membrane (HP:0004722), Progressive renal insufficiency (HP:0000106) and Hematuria (HP:0000790).
Interestingly, the Cfh gene is not only associated with Complement factor H deficiency but also with Atypical hemolytic uremic syndrome 1 (MIM:#235400), and adding the secondary phenotype information, the gene also obtained an im-proved rank for this disease (rank 177 with primary phenotypes only and rank 41 with inserting secondary phenotype informa-tion).
From the improvement in both cases, we conjecture that the secondary phenotypes cover correct functional aspects of the gene that could not have been identified with the primary pheno-type screens.
This information together with additional information for an-other three diseases and their respective genes is provided in Supplementary Material S1.
3.4 Browsing the secondary phenotype predictions online To provide access to the secondary phenotype predictions, we implemented an extension to our disease gene candidate prediction tool PhenoDigm (Smedley et al., 2013).
The results can be browsed at http://www.sanger.ac.uk/resources/databases/ phenodigm/.
The web interface provides all the genes that pos-sess secondary phenotype candidates as a list and the user can Fig.3.
Adding the predicted secondary phenotype annotation to the Sanger-MGP genes with reference range annotations and using these to create gene clusters based on phenotype similarity, improves the biolo-gical coherence of the obtained gene clusters Fig.4.
Accumulating the predicted secondary phenotypes together with reference range annotations for Sanger-MGP genes improves the predict-ability of causative disease genes using PhenoDigm i56 A.Oellrich et al.
s twoselect individual genes for further investigation.
Upon selecting a gene, the user is provided with all the details available for this gene, i.e.
diseases the gene has been confirmed to be a cause for, phenotype annotations from the primary screens, the literature-curated annotations from MGD and the suggested phenotypes for secondary screens.
Providing this information together, a biologist or clinician could easily assess whether the secondary phenotype candidates are worthwhile to be tested in a biological experiment.
Figure 5 provides a snapshot of the available gene-centric information through PhenoDigms web interface.
4 DISCUSSION In this study, we applied an association rule mining approach to mine secondary phenotypes and computationally verify their bio-logical validity using two automated and a limited manual evalu-ation.
We used the manually assigned phenotype annotations contained in MGD for learning phenotype co-occurrence pat-terns and merged the identified patterns with phenotypes that were experimentally verified in primary screens and are available from the Sanger-MGP (White et al., 2013).
Using data from particular resources creates dependencies to-wards those data resources.
For example, annotation guidelines such as those employed by MGD ensure a consistency of human annotators but can also create artefacts in the predictions gener-ated from the data.
In our particular case, we may find pheno-type co-occurrence patterns that are intrinsic to annotation guidelines and not purely due to their co-occurrence.
This may occur when the annotation guidelines cover rules that enforce a set of phenotypes to be annotated in particular circumstances instead of only one.
However, as these guidelines exist to ensure biological correctness of the data, we expect that those cases still constitute biologically interesting, though known con-nections between individual phenotypes.
The implementation of the secondary phenotype prediction pipeline relies solely on an association rule mining approach.
Using the pipeline in conjunction with 7027 unique phenotypes (see Section 2.1), the obtained result of 188 new rules seems comparatively small.
As the number of rules is directly related to the settings for the apriori software, the number of potential hypotheses may be increased by changing these parameters.
However, and as with any prediction tool, we applied conserva-tive measures that would reduce the likelihood of creating false hypotheses.
In addition, starting with a small subset of rules enables better verification possibilities and selection mechanisms for biological experiment design.
Potential areas of extension are the incorporation of additional pattern recognition methods that could then be used to form a support system and provide prov-enance for identified patterns, e.g.
only predictions that are made by a number of systems are more likely to be secondary phenotypes.
Using phenotype patterns to generate secondary phenotype predictions implies that phenotypes that co-occur often with each other, are likely to always co-occur.
For some biological phenomena this assumption has been validated, e.g.
the correl-ation of body weight with bone density or blood calcium levels (Karp et al., 2012b).
Given that the secondary phenotypes per-form well in the evaluation, we feel that the assumption can still be used for forming secondary phenotype hypotheses.
However, in future work we envisage a more complex filtering strategy for assigning phenotype annotations using not only primary pheno-types, but also gene function annotations and disease Fig.5.
An extension of PhenoDigms web interface holds the secondary phenotype predictions i57 Automatically generating secondary phenotyping hypotheses i , involvement to reduce the number of falsely associated genes and phenotypes in addition to voting from different prediction algorithms.
4.1 Predicted secondary phenotypes improve the biological coherence of the clusters Using automated evaluation procedures that apply predictions in biological use cases may mask-specific problems that can only be spotted by human curators, e.g.
if it is known that one particular gene does not cause a particular phenotype in particular circum-stances.
However, the methods can provide a summarized judge-ment over all the results instead of providing details for all cases.
As demonstrated by van Driel et al., clustering genes according to phenotypes leads to clusters consistent with gene function and protein interaction networks.
The same evaluation mechanism that has been applied here, had been successfully applied to assess the quality of existing human phenome databases (Oti et al., 2009).
Using gene function annotation to determine biological coherence is directly influenced by the number of annotations available.
4.2 Secondary phenotypes improve the predictability of causative disease genes In addition to the use case of gene characterization, phenotype annotations are applied to identify the underlying mechanisms of human heritable diseases (Hoehndorf et al., 2011; Robinson et al., 2013; Smedley et al., 2013; Washington et al., 2009).
As the primary screens only cover 20 SOPs, the outcome of pheno-type annotations is limited by the screens.
Using the predicted secondary phenotype annotations may highlight phenotypes of genes that are currently limiting the predictability of certain dis-eases or groups of diseases.
Our results show that adding the secondary phenotype annotations improve the predictability of disease genes and the characterization of genes on a phenotype level can be improved with the suggested phenotypes.
However, experimental verification is necessary and assays would have to be incorporated to test for the 137 identified phenotypes.
4.3 Secondary phenotypes further characterize genes assessed with primary phenotyping As illustrated with a small subset of selected diseases, adding the predicted secondary phenotypes leads to an increase of matched phenotypes between diseases and models.
As discussed before, these results indicate that the predictions indeed possess biolo-gical validity and constitute good biological hypotheses to guide the design of experimental setups for secondary screens.
The se-lection of diseases, however, was limited to the cases where an improvement for the causative gene happened.
In future work, we will also have to extend our analysis to genes where no im-provement or rank decrease was experienced.
However, we note here that the rank changes of known causative disease genes are only indicators for performance changes and need manual inves-tigation.
Some of the OMIM diseases may possess multiple causative genes, some of which have not yet been discovered or listed in OMIM.
As a consequence, those genes will be recog-nized as false positives during the evaluation.
If one of the causa-tive genes that have not been listed in OMIM improves tremendously over those genes that are listed, we could obtain a rank decrease for genes that are listed in OMIM.
4.4 Future development of the PhenoDigm extension The data have been made available through a web interface that provides a gene-centric view on the data.
All the predictions can be assessed and are provided in the context of diseases and in-formation about the primary phenotype screens for easy verifi-cation and hypothesis derivation.
As more data become available, e.g.
through additional automated, statistical screens such as suggested by Karp et al., further information can be included such as effect size and additional phenotype annotations.
Furthermore, even though the annotations have been vali-dated using the predicted secondary phenotype annotations in PhenoDigms disease prediction algorithm, the disease gene can-didate predictions based on the secondary phenotype data are not yet available.
A possible extension of the web interface in future work could be the inclusion of these predictions.
If the predictions are included, possible new emerging disease groups relevant for a gene could be easily spotted from the list and guide new experiments.
5 CONCLUSION Here, we presented a method to predict secondary phenotype candidates based on existing large-scale phenotype annotation sources and primary screens for genes.
We verified the secondary phenotype candidates by applying it in two use cases and could demonstrate that the predictions add value in either use case and, therefore, seem biologically relevant to the genes they are pre-dicted for.
We could show that the phenotype candidates not only increase the biological coherence of gene clusters, but also improve the candidate prediction of genes for human heritable diseases.
In conclusion, we provide a set of genephenotype as-sociations that can be further assessed in biological experiments and guide the experimental design to further investigate specific genes or gene groups.
All the data are freely available online from http://www.sanger.ac.uk/resources/databases/phenodigm/.
In future work, we aim to further improve the method by determining the best parameter settings for the association rule learning, but also investigate other phenotype co-occurrence pat-tern recognition methods.
One possibility is the application of a hypergeometric distribution and find support for patterns that have been identified with the association rule mining approach.
We further intend to provide update results through the web interface and improve the integration with other existing resources.
ACKNOWLEDGEMENTS A.O.E.
designed the experimental setup, implemented the sec-ondary phenotype prediction pipeline, and executed part of the automated and manual evaluation.
D.S.
conducted the evalu-ation using PhenoDigm and IP contributed and verified the manual assessment of the prediction results.
J.J. implemented the web pages required to provide the data online.
All authors contributed and approved the final manuscript.
i58 A.Oellrich et al.
s s s s-s s s is is Funding: This work was supported by the Wellcome Trust grant [098051] and National Institutes of Health grant (NIH) [1 U54 HG006370-01].
Conflict of Interest: none declared.
ABSTRACT Summary: We present the Sample-based Laboratory Information Management System (SLIMS), a powerful and user-friendly open source web application that provides all members of a laboratory with an interface to view, edit and create sample information.
SLIMS aims to simplify common laboratory tasks with tools such as a user-friendly shopping cart for subjects, samples and containers that easily generates reports, shareable lists and plate designs for genotyping.
Further key features include customizable data views, database change-logging and dynamically filled pre-formatted reports.
Along with being feature-rich, SLIMS power comes from being able to handle longitudinal data from multiple time-points and biological sources.
This type of data is increasingly common from studies searching for susceptibility genes for common complex diseases that collect thousands of samples generating millions of genotypes and overwhelming amounts of data.
LIMSs provide an efficient way to deal with this data while increasing accessibility and reducing laboratory errors; however, professional LIMS are often too costly to be practical.
SLIMS gives labs a feasible alternative that is easily accessible, user-centrically designed and feature-rich.
To facilitate system customization, and utilization for other groups, manuals have been written for users and developers.
Availability: Documentation, source code and manuals are available at http://genapha.icapture.ubc.ca/SLIMS/index.jsp.
SLIMS was developed using Java 1.6.0, JSPs, Hibernate 3.3.1.GA, DB2 and mySQL, Apache Tomcat 6.0.18, NetBeans IDE 6.5, Jasper Reports 3.5.1 and JasperSofts iReport 3.5.1.
Contact: denise.daley@hli.ubc.ca Received on January 29, 2010; revised on May 10, 2010; accepted on May 20, 2010 1 INTRODUCTION Genetic studies of complex diseases that interrogate the human genome are known as genome-wide association (GWA) studies.
They require both high-throughput genotyping platforms and large samples sizeswith thousands of subjects from multiple sources and time pointsto clearly define the contributions of common alleles associated with common complex diseases.
To obtain the large sample sizes needed, GWA studies have led to international consortia formed from smaller studies, with differences in study designs and quality control procedures.
If not dealt To whom correspondence should be addressed.
with appropriately, these situations can lead to decreased data accessibility, increased laboratory errors and an overall reduction in operational efficiency.
Laboratory information management systems (LIMSs) provide solutions for these situations; however, the perception that learning to use new software will be time-consuming and difficult can be a significant barrier to the adoption of a LIMS (Anderson et al., 2007).
This situation is worsened by the high turnover rate of research staffresulting in recurrent training costsand the lack of user-centric designs in open source LIMSresulting in steep learning curves.
In searching for a solution to the challenges we faced managing our biological samples, we failed to find any open-source systems that would provide adequate functionality for our needs.
We developed Sample-based LIMS (SLIMS) to make available a feature-rich system capable of handling longitudinal data and to increase the presence of user-centric design in LIMSs.
SLIMS is based on a previously published LIMS: PASSIM (Viksna et al., 2007).
2 FUNCTIONALITY SLIMS manages information for subjects, the biological samples they have provided and the containers (plates or boxes of tubes) used to store and ship them.
Each level of abstraction provides the user with more power; they can choose to deal with wells individually, all the wells of a plate, all the samples from one subject, all the plates containing a sample from one subject, etc.
At each level, users can view, edit and create new entries.
Data integrity is protected by validating input and controlling which users accounts have privileges to edit and create different fields.
Accounts also specify their usersroles in the lab.
Because statisticians and database administrators (so-called dry lab staff) may be interested in very different information than laboratory (or wet lab) staff, SLIMS has tailored default view schemes that are fully customizable.
This cuts down on the confusion caused by information overload.
A users preferred view scheme is remembered every time they log in, so that a user can make the system their own and maximize its efficiency.
SLIMS features several new tools that expedite experiment operations while ensuring complete record keeping and facilitating intra-lab communications.
The most broadly successful of these is a shopping cart that allows users to create, store and share lists of subjects, samples and containers.
Lists can be created by anyone in the lab and populated by selecting elements singly, adding the results of a search or uploading a file.
Being able to share these lists directly through SLIMS can be useful in labs spread over The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:04 16/6/2010 Bioinformatics-btq271.tex] Page: 1809 18081810 SLIMS several rooms, buildings or disciplines because it imposes structured veins of communication with set vocabularies.
It also reduces the use of personal spreadsheets, which can become out of sync or indecipherable if the creator leaves.
Lists allow wet lab staff to perform batch operations such as volume updates or plate creations.
When a new plate is designed in SLIMS, record keeping is automatically linked with the production of documents that aid in plate creation (a map of a plates samples and a recipe of sources detailing how much of which samples from where are to be used).
Lists of plates can be used to design plates, serving cases such as the mechanical combination of four 96-well plates into one 384-well plate for TaqMan genotyping (Heid et al., 1996).
SLIMS integrates record-keeping and lab operations for this process by giving the user a tool to design their new 384-well plate, which in one stroke makes a record in the database and creates an import file necessary for machine operation.
New plates can also be created without the use of lists, simply from pre-existing plates.
A plate cloning tool allows the user to make a new plate and sample records, adjust the volumes of the source well records and print off documents for plate creation, all in one step.
To facilitate preparation for genotyping, SLIMS features a customizable, criteria-based sample selector and plate layout design tool.
Given a list of subjects in a study, the sample selector allows a user to define a hierarchical tree of criteria that SLIMS uses to retrieve the most appropriate sample for each subject.
Once the initial selection process has been performed, the user can then manually curate the results: if they reject a particular sample, the next-bestwill be presented and so on.
When the final set of samples is ready, it is stored in a shopping cart list.
This list can then be used with the plating tool, which designs plate layouts, makes plating documents for the wet lab and updates records of the new plate and the source samples.
All of the data updates that occur while using these toolsor any other SLIMS functionare logged in the supporting database.
This automatic, detailed and reversible record keeping encourages lab members to use the system even if they are as yet unfamiliar with it by providing them with a safety net.
Higher level data tracking is made possible by the availability of dynamically filled, preformatted reports created using Jasper Reports technology (JasperSoft, 2009).
Further, any search result or shopping cart list can be exported to a delimited text file.
Providing users with a powerful, feature-rich systemsuch as SLIMSempowers them to increase their efficiency and productivity.
However, if a system appears complex and feels overwhelming it can have the exact opposite effect.
To ensure a high level of user-acceptance, all interfaces were designed with formal user testing and a detailed user manual describing all views, tools and features was produced.
3 DISCUSSION The consequences of incomplete user acceptance of a LIMS are very serious.
Even the best professional systems lose value and can even become detrimentalif they fall out of date because of inconsistent use.
In a busy laboratory, instructing staff to use a new system can only have so much effect; successfully achieving consistent usage requires incentives in addition to instruction.
To this end, SLIMS aims to combine record-keeping with features that save time and simplify daily Table 1.
Summary of SLIMSs features, this table summarizes the general features adapted from PASSIM as well as those newly developed now being presented General features Manages subjects, samples and containers Data browsing and simple or complex searching Manual creation and editing of entries Input validation and user-based editing privileges New features Widely applicable for sample-based laboratories Shopping cart for subjects, samples and plates Database change-logging Dynamic, pre-formatted reports Tailored and fully customizable views Plate cloning tool Dynamic to do lists for lab procedures Sample upload tool for wet lab Manuals for users and developers Targeted for genotyping laboratories Criteria-based sample selector and plate creation tool Plate combiner for 384-well plate TaqMan genotyping laboratory tasks.
We have taken a three-pronged approach to achieve this goal.
First, facilitation of bench-top procedures is integrated with record keeping in SLIMS by the automatic production of documents required for procedure completion.
Second, we have ensured that the users experience is as easy and efficient as possible by formally involving a broad spectrum of users in the development process from requirements analysis to interface design and testing.
This approach allowed us to map different user-groups mental models of a LIMS system and incorporate them in our design, making the system more intuitive for users.
Though focusing development on users instead of programmers reduces a systems learning curve, it cannot be erased.
Our third prong focused on easing and accelerating the learning process with user manuals and instructional text throughout the system.
Though SLIMS applicable scope may be limited by a setup targeting sample-based laboratories and tools targeting genotyping workflows, the approaches to LIMS development described above incorporate widely applicable, well-established techniques (Debbie Stone et al., 2005) that are lacking in existing open-source LIMS.
Documentation, source code, and manuals are available atWe would like to thank Loubna Akhabir, Scott Brown, and Andrew Sandford for participating in usability testing and for their valuable comments and suggestions.
D.D.
holds a Tier II Canadian Research Chair.
Funding: Canadian Institutes for Health Research (CIHR); AllerGen NCE Inc. (the Allergy, Genes and Environment Network), a member of the Networks of Centres of Excellence Canada program.
Career Scholar Award from the Michael Smith Foundation for Health Research (to D.D.).
Conflict of Interest: none declared.
1809 [11:04 16/6/2010 Bioinformatics-btq271.tex] Page: 1810 18081810 T.Van Rossum et al.
ABSTRACT Motivation: Similarity searching and clustering of chemical compounds by structural similarities are important computational approaches for identifying drug-like small molecules.
Most algorithms available for these tasks are limited by their speed and scalability, and cannot handle todays large compound databases with several million entries.
Results: In this article, we introduce a new algorithm for accelerated similarity searching and clustering of very large compound sets using embedding and indexing (EI) techniques.
First, we present EI-Search as a general purpose similarity search method for finding objects with similar features in large databases and apply it here to searching and clustering of large compound sets.
The method embeds the compounds in a high-dimensional Euclidean space and searches this space using an efficient index-aware nearest neighbor search method based on locality sensitive hashing (LSH).
Second, to cluster large compound sets, we introduce the EI-Clustering algorithm that combines the EI-Search method with JarvisPatrick clustering.
Both methods were tested on three large datasets with sizes ranging from about 260 000 to over 19 million compounds.
In comparison to sequential search methods, the EI-Search method was 40200 times faster, while maintaining comparable recall rates.
The EI-Clustering method allowed us to significantly reduce the CPU time required to cluster these large compound libraries from several months to only a few days.
Availability: Software implementations and online services have been developed based on the methods introduced in this study.
The online services provide access to the generated clustering results and ultra-fast similarity searching of the PubChem Compound database with subsecond response time.
Contact: thomas.girke@ucr.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on August 26, 2009; revised on November 19, 2009; accepted on February 16, 2010 1 INTRODUCTION Software tools for mining the available small molecule space play an important role in many bioscience and biomedical areas.
To whom correspondence should be addressed.
They are ranging from drug discovery, chemical biology and chemical genomics to medicinal chemistry (Haggarty, 2005; Oprea, 2002; Oprea et al., 2007; Savchuk et al., 2004; Strausberg and Schreiber, 2003).
Currently, the structures of over 20 million distinct small molecules are available in open access databases, like PubChem (Austin et al., 2004), ChemBank (Seiler et al., 2008), NCI (Ihlenfeldt et al., 2002), ChemMine (Girke et al., 2005), ChemDB (Chen et al., 2007) and ZINC (Irwin and Shoichet, 2005).
To analyze these important data resources, efficient structure similarity search tools are essential for retrieving chemical and bioactivity information from databases (Chen and Reynolds, 2002; Sheridan and Kearsley, 2002).
In addition, they are often useful for predicting bioactive small molecules (Cao et al., 2008; Cheng et al., 2007).
A variety of structure similarity search methods are available (reviewed by Willett et al., 1998).
Unfortunately, they are often not fast enough for systematic analyses of very large compound collections with millions of compounds.
This is because most of these methods sequentially compare a query structure against all entries in the database and then rank the results by a chosen scoring system, and thus the cost to perform similarity searches grows linearly with the size of the compound database.
Therefore, more efficient and sophisticated search methods need to be developed to utilize the available chemical space efficiently.
Clustering of compound sets is essential on practically all stages of the discovery process of bioactive compounds (reviewed by Downs and Barnard, 2002).
Commonly, structure similarity-based clustering utilizes the pairwise similarity measures generated by the above compound search methods to partition the data into discrete groups of similar compounds.
An example is JarvisPatrick clustering, which is among the most widely used clustering methods in cheminformatics (Willett, 1987).
Alternatively, they can be used to build hierarchical trees that represent the similarity relationships among all items in a compound dataset.
One of the main challenges in this area is the difficulty to cluster the millions of compound structures that are currently available in the public domain.
This is because many cluster analysis approaches multiply the complexity of a chosen similarity search method by the number of compounds in the dataset.
They often require the calculation of all-against-all similarities for the compounds under investigation and the computational cost grows quadratically with the size of the dataset.
Therefore, novel clustering methods need to be developed for exploring this vast chemical space efficiently.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 954 953959 Y.Cao et al.
A few methods have been proposed to accelerate the nearest neighbor searching in compound databases.
Agrafiotis and Lobanov (1999) reported an algorithm based on k-dimensional (or k-d) trees for diversity analyses.
This problem is relevant to clustering (Bentley, 1975) and the method can possibly be adopted to nearest neighbor searching.
It works by representing each chemical compound as a multidimensional vector and organizing the compound dataset in a k-d tree structure to speedup diversity analyses.
However, the vector representation used is very limited and the k-d tree cannot handle high-dimensional vector data.
Later the authors proposed a new data structure called-tree to perform guided nearest neighbor search of compounds in general metric space (Xu and Agrafiotis, 2003).
A-tree organizes the database in recursively partitioned Voronoi regions and represents these partitions as a tree.
The method greatly reduces the number of similarity comparisons required for the nearest neighbor search by applying an efficient branch and bound strategy.
As a distance-based indexing method, the-tree approach does not require the compounds to be represented as multidimensional vectors.
However, it requires the similarity measure used to be a metric.
Swamidass and Baldi (2007) used upper bounds on similarity measures for structural fingerprints to reduce the number of molecules that need to be considered in the nearest neighbor searches.
Later, Baldi et al.
(2008) employed tighter bounds to achieve further time savings.
These latter two methods have been designed to be used only with fingerprint-based similarity measures.
A variety of data structures and algorithms have been proposed to accelerate the nearest neighbor search in multidimensional Euclidean space (reviewed by Bohm et al., 2001).
These methods, often referred to as multidimensional access methods (MAMs), have not been widely used in similarity searching and clustering of chemical compounds.
The reason for this may be the popularity of non-Euclidean similarity coefficients in chemical structure similarity searching, which are not immediately compatible with MAMs.
Moreover, Fu et al.
(2000) reported that the embedding of the generic metric space into multidimensional space can introduce considerable inaccuracy in the nearest neighbor search applications.
In this article, we introduce the embedding and indexing (EI)-Search and EI-Clustering methods for ultra-fast similarity searching and clustering of very large datasets using an EI strategy.
First, we introduce an efficient embedding algorithm.
Subsequently, we describe the design of EI-Search and EI-Clustering, and test their efficiency and accuracy.
Finally, experimental test results for three large compound datasets are presented and discussed.
2 METHOD 2.1 Embedding compounds in Euclidean space The foundation of our algorithms is the usage of embedding techniques to build multidimensional vector representations of compound structures, which can be used to approximate compound dissimilarities by the inter-vector distances.
Embedding objects in Euclidean space offers many benefits, such as the possibility of accelerating the nearest neighbor search.
In addition, they are useful for all-pair query approaches used in data visualization, clustering and data mining (Faloutsos and Lin, 1995).
The problem of geometric embedding has previously been studied and applied to the nearest neighbor search in metric space.
Three of the most widely used methods in this area are multidimensional scaling (MDS; Kruskal and Wish, 1978), stochastic proximity embedding (SPE; Agrafiotis, 2003; Agrafiotis and Xu, 2002; Smellie et al., 2006) and FastMap (Faloutsos and Lin, 1995).
MDS is used to discover structures in datasets by representing the relationships among its objects as spacial distances in a low-dimensional display plane.
It is computationally expensive because it depends on the availability of all pairwise dissimilarities among the objects in a dataset.
As a result, it becomes quickly infeasible for compound databases with more than a few thousand entries.
Many variances of MDS have been proposed to solve the problem for large datasets.
Chang and Lee (1973) selected from the whole dataset a smaller number of representative objects, called pivots, and applied classic MDS to this subset.
The remaining objects were then embedded to the same Euclidean space based on their distances to the pivots.
SPE is an alternative to MDS.
As a self-organizing algorithm, SPE starts with an initial assignment of data points to coordinates and carries out iterative pairwise refinement steps by adjusting randomly selected pairs of coordinates to better approximate the corresponding dissimilarity values.
SPE is very efficient and is reported to scale linearly with the size of the dataset.
FastMap is another fast alternative of MDS with linear-time complexity.
It gains its computational efficiency by directly calculating the induced vectors rather than iterative improvement steps used by most MDS implementations.
However, the proper choice of the set of pivot objects can be complicated by the presence of large numbers of outlier compounds that are not similar to any other compounds in a compound library.
This study presents an alternative embedding method that is accurate and robust enough to process very large compound datasets.
The initial steps of the embedding procedure are similar to the method from Chang and Lee (1973), but it differs significantly in its final optimization steps.
The method starts by dividing all compounds into two sets: a small reference compound set and a much larger target compound set.
The reference compound set is a user-definable parameter that can be generated by maximum diversity or random selection methods of compounds in a given library.
Traditional MDS is applied to obtain the coordinates of the induced reference vectors for the reference compounds.
Subsequently, an induced target vector is obtained for each target compound by computing the vector coordinates that can best preserve its dissimilarity to all reference compounds.
More specifically, for the i-th target compound oi, the following stress function is minimized: stress= |R| j=1 ( d(oi,rj) d(xi,rj) )2 .
(1) In this equation, d(oi,rj) is the dissimilarity value between target compound oi and reference compound rj .
The variable rj is the coordinate of the j-th induced reference vector obtained by applying MDS to the reference compounds.
The unknown coordinate of the i-th target vector is xi, and d(xi,rj) gives the Euclidean distance between xi and rj .
By minimizing the stress function with a global optimization algorithm, the coordinate xi can be computed so that it best preserves the dissimilarities from target compound oi to all reference compounds.
Two very important parameters in our modified version of the MDS algorithm are the number of dimensions D and the reference compound set.
Large D values will not increase the minimum value of the stress function, and thus will never negatively impact the embedding quality.
To maximize the embedding quality, our method can be used with conservatively large D values of over 100, but at the expense of longer computation times for both the embedding and the downstream similarity searches.
Often D values >100 may not be necessary for many types of molecular descriptors (e.g.
physicochemical), due to their frequently high redundancy and correlation among each other (Agrafiotis and Xu, 2002).
It might also be possible to take advantage of low intrinsic dimensionality of some similarity measures, using methods such as ISOMAP (Tenenbaum et al., 2000) and locally linear embedding (LLE; Roweis and Saul, 2000).
However, our tests using ISOMAP with atom pair descriptor also showed that D values >100 may still be necessary for some molecular descriptors to achieve satisfactory accuracy.
954 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 955 953959 EI-Search and EI-Clustering The reference compound set is the second important parameter for our approach.
The size of the set, R, directly controls the complexity of the stress function.
Therefore, the larger the reference compound set, the longer the embedding time.
Furthermore, both the size and the composition of the reference set have an influence on the embedding quality.
This is because the induced reference vectors serve as similarity landmarks to position each target compound in the embedding space according to its dissimilarity profile to all reference compounds.
To minimize ambiguous placements, the reference set should ideally be chosen as diversely as possible and cover the entire chemical space of the target compounds.
However, our benchmark tests for structure similarity searching and clustering, have shown that randomly selecting reference compounds is sufficient to obtain robust results (Section 3.5).
This is partly because R is usually much larger than the number of D. Due to this redundancy, choosing one or several suboptimal reference compounds will not have a great impact on the embedding quality.
Moreover, it is important to identify the smallest values for R and D, which can achieve satisfactory accuracy.
Increasing either value will result in longer processing time.
This does not only apply to the embedding of the compound database, but also to every query structure used in similarity searches, because both components have to go through the embedding process.
This slightly offsets the time saving gained by the other parts of the algorithm.
Agrafiotis et al.
(2001) presented a neural network-based method that could reduce the embedding time of our method significantly.
However, in our tests we were not able to achieve embedding qualities with this method that were sufficient for similarity search and clustering applications (Supplementary Table S-1).
Compared to the quadratic time complexity of traditional MDS methods, our algorithm offers large time savings.
Given similar distributions of pairwise dissimilarities and a fixed number of reference compounds, the running time of the above embedding algorithm is roughly linear to the number of compounds.
Because each target compound is processed independently, our method can be easily parallelized on computers with multiple CPU cores or compute clusters.
This is a very desirable feature when processing very large compound databases.
The related SPE method is in its current form less effective for processing very large datasets because of the challenges involved in implementing a parallelized version of this algorithm.
Our tests with a publicly available implementation of the SPE algorithm show that its unparallelized compute times on very large datasets with millions of compounds are too long to obtain embeddings of high enough qualities to perform accurate nearest neighbor searches with acceptable recall rates (Supplementary Table S-2).
Nevertheless, SPE is a powerful algorithm that can be easily utilized as an alternative embedding method by our EI-Search and EI-Clustering tools.
2.2 EI-Search After the compounds are embedded into the D-dimensional Euclidean space, structure similarity searches can be performed by a nearest neighbor search in the embedding space.
For this, the query compound is embedded into the same D-dimensional Euclidean space.
Subsequently, the corresponding induced vector is used to perform a nearest neighbor search in the embedding space.
This search method offers great time savings and flexibility for similarity searches because of two major reasons.
First, compared to other similarity measures used for comparing compound structures, Euclidean distances can be calculated very efficiently.
Second, many MAMs can be employed in the nearest neighbor search to further improve its time efficiency.
Although many MAMs have been proposed to speedup the nearest neighbor search problem, most of them are affected by the dimensionality problem, often referred to as the curse of dimensionality.
This is the phenomenon that for various geometric search problems, including the nearest neighbor search, the best algorithms known have performance trends that degrade exponentially with an increase in dimensionality (Weber et al., 1998).
Our tests also confirmed that the SR-tree method (Katayama, 1997) becomes slower than sequential scans of the dataset as soon as D is set to values >100.
However, our tests also indicated that the dissimilarities between compound structures can only be preserved reliably with large D values of at least 100.
Thus, it is important for our method to combine high-dimensional embedding and an indexing method that are both insensitive to the dimensionality problem.
It has become increasingly popular to use the approximate nearest neighbor search in high-dimensional space to avoid the dimensionality problem.
One of these approximation approaches utilizes a spatial index using locality sensitivity hashing (LSH) to perform the fast nearest neighbor search in Euclidean space.
LSH uses a family of hashing functions to map database objects into buckets.
It is designed to join related items based on a given similarity measure with high probability.
Accordingly, many hashing functions vary with respect to their similarity measures.
For example, Gionis et al.
(1999) introduced in the original LSH paper a bit sampling-based LSH approach that uses the Hamming distance measure.
Datar et al.
(2004) proposed an LSH scheme for p-stable distributions along with Euclidean distances.
However, so far no hashing functions have been developed for the similarity measures that are commonly used for comparing compound structures.
This limitation can be addressed by embedding compounds in Euclidean space and building for them a spatial index using induced vectors in embedding space.
Taking advantage of the above embedding and the LSH-based spatial indexing approaches, we designed an efficient approximate compound structure similarity search algorithm.
This algorithm is named EI-Search after its two key components: embedding and indexing.
The algorithm first preprocesses the compound dataset by embedding it into a high-dimensional Euclidean space and generating a spatial index using LSH.
When searching for k compounds that are most similar to a given query compound, a two-step approach is employed to reduce the error introduced in the embedding process and the approximate nearest neighbor search.
First, the query compound is embedded in the same Euclidean space.
The resulting induced vector is then used in an index-assisted nearest neighbor search of the embedding space to retrieve a candidate set consisting of k vectors that are most similar to it.
The relaxation ratio is a user-defined parameter that controls the trade-off between processing time and search accuracy.
Larger values for result in larger candidate sets and possibly higher accuracy, but at the cost of longer search times.
Second, a refinement step applies exact structure similarity searches to the candidate set obtained in the first step.
This allows the selection of the final k compounds that are most similar to the query structure.
Compared to commonly used structure similarity search methods, EI-Search has several advantages.
The most important ones are its time efficiency and compatibility with a wide range of similarity measurements.
This makes the method potentially useful for accelerating similarity searches of a variety of data objects that are of relevance to many life science and non-life science areas.
2.3 EI-Clustering In addition to similarity searching, Euclidean space representations can be used for clustering large datasets very efficiently.
For example, spatial join can be used to perform single linkage clustering by finding all vector pairs, which are separated by a distance below a given threshold (Brinkhoff et al., 1993).
The nearest neighbor information required for JarvisPatrick clustering can also be obtained very efficiently in Euclidean space by using an efficient algorithm for the all-nearest-neighbors problem (Vaidya, 1989).
In this article, we introduce a new clustering method, EI-Clustering, that takes advantage of the accelerated search speed provided by EI-Search to cluster very large sets of chemical compounds under the JarvisPatrick clustering framework.
JarvisPatrick clustering requires a nearest neighbor table, which consists of p nearest neighbors for each compound in the dataset.
This information is then used to join compounds into clusters that share at least m nearest neighbors.
The values for p and m are user-defined parameters.
In case of EI-Clustering, the EI-Search method generates the nearest neighbor information for each compound in the dataset.
The resulting nearest neighbor table is then used as direct input for JarvisPatrick 955 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 956 953959 Y.Cao et al.
clustering.
When clustering very large datasets, EI-Clustering is particularly efficient due to its fast computation of the nearest neighbor table.
3 EVALUATION 3.1 Implementation We implemented the EI-Search and EI-Clustering algorithms as C++ programs.
Internally, they use the L-BFGS-B library (Zhu et al., 1997) for the global optimization step in the embedding procedure.
For the LSH-based spatial indexing, we utilized the lshkit library that implements the MPLSH algorithm, a variant of LSH that requires fewer hash tables and offers large space savings as well as shorter query time (Lv et al., 2007).
In addition, we wrote a reusable C++ library for calculating atom pair descriptors.
We were particularly interested in testing atom pair descriptors because of their superior performance in structure similarity searching (Chen and Reynolds, 2002).
To also include in our tests one of the most widely used fingerprint methods, we employed the fingerprint descriptors used by PubChem and utilized a fingerprint implementation provided by the NIH Chemical Genomics Center (2009).
JarvisPatrick clustering was performed with a custom program implemented in C++.
3.2 Datasets and testing platform To test the performance of the proposed methods, benchmark comparisons for similarity searching and clustering were performed using the publicly available compound structures from NCI and PubChem.
The NCI dataset consisted of 260 071 compounds.
After removing entries that did not generate any usable atom pair descriptors, 260 027 compound structures were used from this collection.
From PubChem, we used the structures from the PubChem Compound dataset.
From this collection, we selected two sets: one subset consisting of 2.3 million compounds with at least five non-hydrogen bonds (PubChem Subset) as well as the entire PubChem Compound library with over all 19 million compounds (PubChem Compound).
The tests were performed on a Linux computer equipped with Xeon CPUs clocked at 2.4 GHz and 16 GB of RAM.
For parallelized computations, a computer cluster was used with the exact same hardware configuration per compute node.
Compute times are given as total CPU hours throughout the text.
3.3 Time efficiency of embedding First, we evaluated the running time of our modified MDS algorithm for different parameters.
For this, the compounds from all three datasets were embedded into a high-dimensional Euclidean space.
3.3.1 Time efficiency with respect to the size of the dataset Our results show that the time required for embedding increases almost linearly with the number of compounds in the library (Table S-3 in Supplementary Materials).
The average time to embed one compound varies only slightly across the three datasets and is below 0.3 s using a practical parameter set.
Because the embedding algorithm processes each compound independently after applying MDS to the reference compound set, it is trivial to further reduce its computation time by using a compute cluster.
In our experiments, it took around 10 min to process the 260 027 compounds of the NCI dataset using 87 CPUs.
Similarly, the 19 million compounds of the PubChem Compound library could be processed in less than a day using 80 CPUs.
3.3.2 Time efficiency with respect to the embedding parameters As discussed above, the number of dimensions D and the number of reference compounds R are the two main factors that will affect the embedding time.
To estimate their impact, we randomly selected from the NCI library seven reference compound sets with sizes ranging from 240 to 800 compounds.
These reference sets were used in independent test runs of our embedding algorithm where D was set to a fixed value to study the influence of R on the total CPU time.
Similarly, to model the impact of D, the value of R was fixed to three times the value of D and the total CPU time was collected using D values ranging from 120 to 260.
These ranges were chosen to ensure high-quality embedding, but also to keep the computation within manageable time limits.
Our test results (Supplementary Fig.S-1) indicate that the total CPU time of our method grows linearly with R, and exponentially with D. Based on this time behavior, the values for R and D should be chosen as small as possible, but large enough to maintain an embedding quality that is sufficient for the downstream similarity search and clustering steps (see below).
3.3.3 Time efficiency and global optimization parameters Solving the global optimization problem accounts for most of the embedding time.
Therefore, the parameter choice of this step has a great impact on the time efficiency of the embedding step of our method.
Using less stringent termination conditions for the L-BFGS-B algorithm, will typically reduce the embedding time, but at a cost of embedding quality.
For example, the average time to embed one compound could be easily cut into half with a small loss in accuracy (Supplementary Table S-3).
Although the preprocessing of new datasets is time consuming, our method is relatively flexible with respect to adding new entries to an already preprocessed library.
Because all entries are embedded independently, one can easily add new ones without repeating this process for the entire dataset.
This meets the work flow requirements of many large compound databases, where minor updates occur frequently, but major revisions are rare.
3.4 Quality of embedding As pointed out by previous studies, embedding metric repre-sentations of objects in Euclidean space may reduce the accuracy of the nearest neighbor searches (Fu et al., 2000).
However, when the parameters for our embedding method are chosen properly, the method is accurate enough to be used in the prescreening step of EI-Search (as shown in Section 3.5).
As a performance test of the embedding step, we randomly selected 10 million compound pairs from the NCI dataset, computed their atom pair-based Tanimoto similarity coefficients and compared them with the corresponding distance values obtained from the induced vectors.
The two datasets were highly correlated, as indicated by a Pearsons correlation coefficient of 0.79.
Additionally, the agreement among the datasets was evaluated by grouping the Tanimoto coefficients into 10 similarity intervals from 0 to 1 using increments of 0.1.
Subsequently, the distributions of the vector distances for each interval were plotted in the form of box plots.
In this representation, a low degree of overlap among the boxes from adjacent intervals indicates a strong agreement between the two 956 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 957 953959 EI-Search and EI-Clustering methods.
The box plots obtained from our tests show only a moderate overlap among adjacent intervals, and only minor to no overlap among more distant intervals (Supplementary Fig.S-2).
This indicates a strong agreement among the two methods for the majority of the compounds with some inconsistencies at the interval boundaries, but for a much smaller number of cases.
For example, for high Tanimoto similarities ranging from 0.8 to 1, the similarities of the corresponding vectors pairs (1distance) fall all into a very narrow range 0.751.0.
The two methods also agree very well in the low similarity range.
This is indicated by the fact that 98.17% of all compound pairs with a Tanimoto coefficient <0.4 are placed into a very similar range (<0.38) in vector space.
Based on these results, our embedding method appears to preserve well-separated ranges of Tanimoto coefficients in a robust manner.
3.5 Accuracy of EI-Search To further examine the accuracy of the EI-Search method for compound similarity searching, we implemented an EI-Search program and tested it with different parameters.
The accuracy of EI-Search is measured by the recall rate.
When retrieving the k most similar compounds to a query structure, the recall rate is defined as the percentage of compounds obtained with EI-Search that are also returned by sequential search methods.
To evaluate the impact of the different parameters used by EI-Search, the program was run using different values of D, R, k and .
In each run, the recall rates for 1000 random queries from the NCI dataset were calculated.
First, we compared the recall rates when searching for the k most similar compounds with different R and values while the values of k and D were fixed.
(Supplementary Fig.S-3a shows the results for constant k and D values of 100 and 140, respectively.)
The results indicate that increasing from 1 to 50 results in a significant improvement of the recall performance, while this effect starts to plateau off at values of >20.
With respect to the number of reference compounds R, the recall rate increases from values 240 to 420.
Based on these results, a good empirical choice for R is between two and three times the value of D. Another observation is the fact that the effect of R diminishes for values >20.
In other words, large enough values can compensate a suboptimal choice of R. For example, when was set to 20, and k to 100 and D to 140, the best and worst recall rates were 97.87% and 97.35%, respectively.
This corresponds to a difference of only 0.52%.
Similarly, we investigated the correlation between the recall rates and the values of D and .
According to the results from the previous tests, the R values were always set to three times the values of D. When searching for the k most similar compounds the recall rates were collected for different D and values while the value of k was fixed.
Supplementary Figure S-3b shows the results for a k value of 100.
These results indicate that the recall rates consistently improve with the number of dimensions.
This effect is much stronger for smaller values.
For example, when k was set to 100 and to 1, then the recall rate could be improved from 58.35% to 65.57% for D values of 120 and 260, respectively.
For large values above 20 this effect is again much less pronounced.
While larger values will result in an increase in processing time, their impact is less severe than increasing the value of D. Accordingly, we chose in the subsequent experiments the D values as small as possible and the Table 1.
Performance tests of EI-Search Dataset NCI PubChem Subset PubChem Compound Descriptor type Atom pair Atom pair Atom pair Fingerprint Average search time (s) Sequential search 0.800 11.570 93.121 19.658 EI-Search 0.067 0.170 0.427 0.499 Recall of EI-Search (%) Mean 99.95 99.60 97.38 96.32 SD 0.44 1.82 5.61 11.54 Search times and recall rates are listed for searching three large compound sets with EI-Search and the sequential search methods.
The same descriptor type was used for each comparison pair.
The experiments were performed on the same hardware using the same embedding and relaxation parameters (R = 300, D = 120 and = 30).
The LSH parameters were supplied by lshkit.
values as large as necessary to maintain both high accuracy and time efficiency of the method.
3.6 Time efficiency of EI-Search While maintaining high recall rates, EI-Search was able to greatly reduce the time for performing structure similarity searches in large compound databases.
To examine the time efficiency of EI-Search, 1000 random queries were performed on each of the three datasets using first the EI-Search program and then exhaustive sequential searches with the atom pair and fingerprint similarity search programs.
For each comparison we used for EI-Search the same descriptor type as for the sequential search methods.
The query compounds were randomly selected from the dataset.
To obtain realistic search results, the 100 most similar compounds with a minimum similarity of 0.5 were retrieved for each query.
Atom pair descriptors combined with Tanimoto coefficients were used as similarity measure.
For the PubChem Compound library, we also included in the tests the Tanimoto similarities of PubChems fingerprints.
According to the above parameter optimization results, we used in all tests the following settings: R = 300, D = 120 and = 30.
The obtained search times and recall rates are listed in Table 1.
Several conclusions can be drawn from Table 1.
First, in comparison to the highly accurate atom pair method, EI-Search achieves in the atom pair descriptor tests very high recall rates ranging from 97.38% to 99.95%.
In comparison to the fingerprint method, the recall rate of the corresponding EI-Search is slightly lower with 96.32%.
The reason for this reduction may be the fact that fingerprints provide less accurate similarity measures than atom pairs (Chen and Reynolds, 2002).
This could result in less robust rankings of the nearest neighbor search results, and therefore a slightly lower recall rate is reasonable.
Second, EI-Search provides significant time savings for the nearest neighbor searches.
For example, the average time required to search >19 million compounds of the PubChem Compound dataset is reduced for the atom pair approach from >93 to <0.5 s, and for the fingerprint method from >19 to <0.5 s. This corresponds to accelerations by our EI-Search method of over 200 and 40 folds, respectively.
Third, while the search time for the exhaustive sequential search methods increases linearly with the size of the databases, this increase is less than linear for the EI-Search method.
For instance, searching 957 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 958 953959 Y.Cao et al.
Table 2.
Performance tests for EI-Clustering Dataset NCI PubChem Subset PubChem Compound Similarity measure Atom pair Atom pair Atom pair Fingerprint Total clustering time (h) JarvisPatrick 72.9 7355.6 N/A N/A EI-Clustering 3.5 92.2 1517.2 2869.71 Jaccard coefficient 0.9913 0.9887 N/A N/A The table compares the time and accuracy performance of EI-Clustering with Jarvis Patrick clustering when using exhaustive search methods for generating the required nearest neighbor information.
The compute time is given in hours of total CPU time.
The agreement among the clustering results is given in the last row in form of Jaccard partition coefficients.
Clustering of the PubChem Compound dataset was not possible with the exhaustive search methods due to their insufficient performance on this large dataset.
the PubChem Subset dataset with EI-Search takes on average only 2.5 times longer than searching the NCI dataset, that has one-ninth of the size of PubChem Subset.
Finally, another outstanding feature of EI-Search is the fact that its search speed is much less impacted by the complexity of the similarity measure used for database searching than this is the case for the exhaustive methods.
For instance, the switch from the fingerprint similarity measure to the more computationally expensive atom pair similarity measure results only in a minor increase of EI-Searchs query time, while it is a >4-fold increase for the exhaustive sequential search methods.
3.7 Accuracy and time efficiency of EI-Clustering To test the performance of our EI-Clustering method for partitioning large compound sets, we clustered all three compound sets with the JarvisPatrick algorithm.
The required nearest neighbor tables were generated with EI-Search and the exhaustive sequential search methods.
EI-Search was run with the same embedding and searching parameters (R = 300, D = 120 and = 30) as in the previous section.
The LSH parameters were slightly changed to achieve higher accuracy.
The process of generating the nearest neighbor tables was parallelized on a computer cluster.
For each clustering result, the total search time was calculated for all utilized CPUs.
To measure the agreement among the clustering results, we computed the Jaccard partition coefficient for each pair of clustering results (Table 2).
Jaccard coefficients close to zero indicate low similarities and values close to one indicate high similarities among the evaluated cluster sets.
The results in Table 2 show that the EI-Clustering method can dramatically reduce the processing time of the Jarvis Patrick clustering approach while maintaining a high level of agreement with the results obtained by JarvisPatrick clustering with exhaustive nearest neighbor search methods (Jaccard coefficients >0.98).
The total CPU time to process over 2.3 million compounds from the PubChem Subset could be reduced from over 306 days to just 4 days.
This is a major improvement, because it makes it feasible to cluster millions of compounds in a few days on a regular workstation or in a few hours when a small computer cluster is available.
By running EI-Search on 80 CPUs on a computer cluster, we were able to cluster the entire PubChem Compound library in only a day.
Because EI-Clustering spends most of the time running EI-Search, which runs in time sublinear in the size of the dataset, the compute time of EI-Clustering is subquadratic to the size of the dataset.
Therefore, EI-Clustering scales much more efficiently to larger datasets than traditional methods.
The superior speed of EI-Clustering comes at the cost of a larger memory footprint compared to the other methods.
Most of the memory is consumed by its LSH index.
For example, when clustering the 19 million PubChem Compound library, the LSH index requires around 13 GB of memory.
Considering the performance of todays research workstations, this memory requirement appears to be manageable.
3.8 Availability of the programs and data The EI-Search and EI-Clustering programs can be downloaded from http://chemmine.ucr.edu/ei/.
The same web site features an online service using EI-Search for ultra-fast similarity searching of the PubChem Compound library with subsecond response time.
In addition, the site provides access to the EI-Clustering results of the entire PubChem Compound library that are based on atom pair and PubChem fingerprint descriptors.
4 CONCLUSIONS AND FUTURE WORK In this study, we have presented EI-Search and EI-Clustering as efficient methods for accelerating structure similarity searches and clustering of very large compound datasets.
The acceleration is achieved by applying embedding and indexing techniques to represent chemical compounds in a high-dimensional Euclidean space and to employ ultra-fast prescreening of the compound dataset using the LSH-assisted nearest neighbor search in the embedding space.
Our tests show that the method can dramatically reduce the search time of large databases, by a factor of 40200 folds when searching the 100 closest compounds to a query.
Recently published acceleration methods achieved only a 5.5-fold reduction in search time when using a Tanimoto threshold of 0.8 and up to 20-fold with a relatively restrictive threshold of 0.9 (Baldi et al., 2008; Swamidass and Baldi, 2007).
Another limitation of these methods is their narrow utility spectrum that is currently restricted to fingerprint-based searches.
In contrast to this, the EI-Search framework is designed to be useful for a wide spectrum of similarity measures.
After embedding, EI-Search will run in most cases with comparable time efficiencies independent of the complexity of the similarity measure.
This can be particularly useful for accelerating searches that use much more accurate, but computationally very expensive similarity measures, such as maximum common substructures or 3D approaches (Cao et al., 2008; Raymond et al., 2003; Willett, 2005).
By taking advantage of the fast similarity search speed of EI-Search, we developed EI-Clustering into an effective clustering method for very large datasets.
The method accelerated the clustering of the three test datasets used in this study by 2080 folds.
Most importantly, the EI-Clustering made it feasible to cluster datasets of almost 20 million entries within acceptable time limits.
Due to its subquadratic running time, the EI-Clustering method should scale well enough to cluster even larger datasets with tens or even hundreds of millions of objects.
In the future, we will expand the performance and utility spectrum of the EI-Search and EI-Clustering methods on several levels.
First, several statistical methods will be employed to further improve the embedding algorithm by dynamically optimizing its parameters and the selection of the most effective reference compounds.
Second, 958 [10:54 5/3/2010 Bioinformatics-btq067.tex] Page: 959 953959 EI-Search and EI-Clustering the EI-Search method will be tested and optimized for the usage of a variety of more complex similarity measures that are available for compound structures.
Finally, additional clustering algorithms, besides JarvisPatrick clustering, will be incorporated into the EI-Clustering method.
ACKNOWLEDGEMENTS We acknowledge the support from the Bioinformatics Core Facility, the Center for Plant Cell Biology (CEPCEB) and the Institute for Integrative Genome Biology (IIGB) at UC Riverside.
Funding: National Science Foundation (grants IOB-0420033, IOB-0420152, IGERT-0504249 and IIS-0711129).
Conflict of Interest: none declared.
ABSTRACT Motivation: Most functions within the cell emerge thanks to proteinprotein interactions (PPIs), yet experimental determination of PPIs is both expensive and time-consuming.
PPI networks present significant levels of noise and incompleteness.
Predicting inter-actions using only PPI-network topology (topological prediction) is difficult but essential when prior biological knowledge is absent or unreliable.
Methods: Network embedding emphasizes the relations between net-work proteins embedded in a low-dimensional space, in which protein pairs that are closer to each other represent good candidate inter-actions.
To achieve network denoising, which boosts prediction per-formance, we first applied minimum curvilinear embedding (MCE), and then adopted shortest path (SP) in the reduced space to assign like-lihood scores to candidate interactions.
Furthermore, we introduce (i) a new valid variation of MCE, named non-centred MCE (ncMCE); (ii) two automatic strategies for selecting the appropriate embedding dimen-sion; and (iii) two new randomized procedures for evaluating predictions.
Results: We compared our method against several unsupervised and supervisedly tuned embedding approaches and node neighbourhood techniques.
Despite its computational simplicity, ncMCE-SP was the overall leader, outperforming the current methods in topological link prediction.
Conclusion: Minimum curvilinearity is a valuable non-linear framework that we successfully applied to the embedding of protein networks for the unsupervised prediction of novel PPIs.
The rationale for our ap-proach is that biological and evolutionary information is imprinted in the non-linear patterns hidden behind the protein network topology, and can be exploited for predicting new protein links.
The predicted PPIs represent good candidates for testing in high-throughput experi-ments or for exploitation in systems biology tools such as those used for network-based inference and prediction of disease-related func-tional modules.
Availability: https://sites.google.com/site/carlovittoriocannistraci/home Contact: kalokagathos.agon@gmail.com or timothy.ravasi@kaust.
edu.sa Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Detection of new interactions between proteins is central to modern biology.
Its application in protein function prediction, drug delivery control and disease diagnosis has developed along-side a deeper understanding of the processes that occur within the cell.
One key task in systems biology is the experimental detection of new proteinprotein interactions (PPIs).
However, such experiments are time consuming and expensive.
Because of this, researchers have developed computational approaches for predicting novel interactions (You et al., 2010), intended also to guide wet lab experiments.
The topological prediction of new interactions is a novel and useful option based exclusively on the structural information provided by the PPI network (PPIN) topology.
This option for prediction is particularly con-venient when the available biological information on the proteins being tested for interaction (seed proteins) is incomplete or un-reliable.
One of the most efficient approaches is the Functional Similarity Weight (FSW) (Chua et al., 2006).
Such method be-longs to the large and well-established family of predictors that are referred to as node neighbourhood techniques (Cannistraci et al., 2013a), because to assign a likelihood score to any candi-date interaction (i.e.
a pair of non-connected proteins in the observed PPIN), they rely on the topological properties of the seed proteins neighbours.
The set of candidate interactions is then ranked.
The main problem with these techniques is that their performance is poor when applied to sparse and noisy net-works (You et al., 2010).
In 2009, Kuchaiev et al.
(2009) proposed a method for geo-metric denoising of PPINs.
The algorithm is based on the use of multidimensional scaling (MDS) to preserve the shortest paths (SP) between nodes in a low dimensional space.
The predicted interactions are scored according to their Euclidean distance (ED) in the low dimensional space, following the principle that the closer two proteins are, the higher the likelihood that they interact (Kuchaiev et al., 2009).
Although it is not explicitly men-tioned in the article, the embedding method adopted by Kuchaiev et al.
is equivalent to Isomap (Tenenbaum et al., 2000).
In an independent study, You et al.
(2010) proposed a hybrid strategy based on network embedding to assign predic-tion scores to candidate interactions.
They exploited the notion that a PPINor theoretically, any networklies on a low di-mensional manifold shaped in a high-dimensional space.
The shape of the manifold and the associated topology are deter-mined by the constraints imposed on the protein interactions through biological evolution.
You et al.
used a renowned mani-fold-embedding algorithm, Isomap (Tenenbaum et al., 2000), to *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com https://sites.google.com/site/carlovittoriocannistraci/home mailto:kalokagathos.agon@gmail.com mailto:timothy.ravasi@kaust.edu.sa mailto:timothy.ravasi@kaust.edu.saembed the PPIN in a space of reduced dimensionality.
Then, they applied FSW to the embedded network (pruned according to a cut-off on the ED) to assign likelihood scores to the candi-date interactions.
In general, the embedding strategy offers two advantages: (i) the topological prediction performance is im-proved even when networks are sparse and noisy; and (ii) the computational time is reduced because the time required for the network embedding is much lower than that required by node neighbourhood techniques for computing the topological prop-erties of each candidate interaction.
A disadvantage is that if the network is not a unique connected component, only the largest connected component can be considered for embedding (Kuchaiev et al., 2009).
Here, we introduce several variations of these approaches that all together offer a new solution for topological link prediction by network embedding.
The first variation uses minimum curvi-linear embedding (MCE) (Cannistraci et al., 2010) and its non-centred variant, ncMCE (which is introduced for the first time), to project the network on the reduced dimensionality space.
MCE is a parameter-free algorithm designed for the unsuper-vised exploration of high-dimensional datasets by non-linear di-mension reduction (Cannistraci et al., 2010).
Recently, MCE ranked first among 12 different approaches (evaluated on 10 diverse datasets) in a study on the stage prediction of embryonic stem cell differentiation from genome-wide expression data (Zagar et al., 2011).
This proof of power and robustness moti-vated us to test its performance in the context of PPI prediction by network embedding.
In the second variation, we use the SP distance (instead of the ED, as in Kuchaiev et al.
and You et al.)
over the network embedded in the reduced space to assign the likelihood scores to the candidate interactions.
The method pro-posed here undoubtedly presents a novel combination of steps.
We prove that the combination of ncMCE/MCE and SP achieves excellent results, boosting the separation between good and bad candidate links.
2 DATA AND ALGORITHMS 2.1 Network datasets The main datasets analysed in this work comprise four yeast PPINs.
Yeast networks are the preferred benchmark for testing topological algorithms to predict links because of the large amount of information available for yeast, in terms of both de-tected interactions and Gene Ontology (GO) associations (You et al., 2010).
The PPIs in these datasets are mainly physical inter-actions, but also include literature-curated and functional links.
Details on the characteristics of the networks are provided in Supplementary Section I and Table S1.
2.2 Network embedding algorithms As this work focuses on link prediction based on network top-ology, each of the abovementioned datasets can be represented as an undirected unweighted graph G V,E with a set of Vj j nodes and a set of Ej j edges, which is a set of two-element subsets of V. Network embedding consists of finding a mapping (embed-ding),M : V!
X, where X is a set of points x1, x2, .
.
.
, x Vj j  with xi 2 Rd: i.e.
each node of G is assigned a coordinate in a space of d dimensions, such that some original topological properties of the network are preserved in this low-dimensional space.
As explained in the Introduction, manifold embedding algorithms can be easily adopted for network embedding, al-though not all algorithms that learn manifolds are applicable for this task: only those able to embed a topology starting from a distance or adjacency matrix can be used.
We chose to compare MCE, ncMCE and Isomap (and IsomapFSW) against well-established unsupervised and supervisedly tuned manifold embedding algorithms that accept a distance or adja-cency matrix as input.
The unsupervised embedding techniques considered are Sammon mapping (a type of non-linear MDS) (Sammon, 1969), and two force-based embedding techniques: stochastic neighbourhood embedding (SNE) (Hinton and Roweis, 2003) and tSNE (a variant of SNE) (van der Maaten and Hinton, 2008).
The supervisedly tuned techniques are local MDS (Venna and Kaski, 2006) and neighbour retrieval visualiser (NeRV) (Venna et al., 2010).
These methods are also force based, but instead of using forces based on kernels (like SNE or tSNE), they use forces based on neighbourhood graphs (Shieh et al., 2011).
Both local MDS and NeRV require a parameter to be tuned between 0 and 1.
In this work, we assessed the perform-ance of these two techniques using values of from 0 to 1 in steps of 0.1, and took the low-dimensional coordinates that yielded the best prediction result (see Supplementary Sections II.2 and II.3).
2.3 MCE followed by shortest-path distance MCE is a parameter free and time-efficient unsupervised algo-rithm for non-linear dimensionality reduction (Cannistraci et al., 2010), which was presented as a new form of non-linear MDS (see Algorithm 1 and Supplementary Section II.1 for details on the original version of MCE, the innovations proposed in this article and details on MCEs time complexity).
Here, we propose the use of MCE for embedding a network into a space of reduced dimensionality.
MCE performs the embedding of the network connectivity distances measured over the minimum spanning tree (MST) of the original network.
This novel MST-derived measure of connectivity was more generally formalized in a pre-vious study as a non-linear measure that we refer to as minimum curvilinearity (MC), and the pairwise MC distances between nodes of the MST give rise to the MC matrix (Cannistraci et al., 2010).
The fact that the MST is a neighbourhood graph that can well approximate the main network informationoffering a general summary of the network topologyhas been extensively shown in different applications (Cannistraci et al., 2010; Shaw and Jebara, 2009; Shieh et al., 2011), and in our case, this can be particularly useful for denoising the information present in PPINs.
In fact, the false-positive (FP) rate of currently widely used experimental technologies is significantly high, sometimes exceeding 60% (Kuchaiev et al., 2009).
MC tends to stress local topological distances and dilate large connectivity distances (Cannistraci et al., 2010).
A consequence is that the use of MCE for embedding causes a sort of network deformation when the network structure is compressed in a reduced space of just a few dimensions.
The deformation aug-ments the separation between nodes far apart in the network topology and maintains or reduces the distances between nearby nodes (Cannistraci et al., 2010).
This might be a point i200 C.V.Cannistraci et al.
of weakness for network visualization because it stretches the network shape in the reduced space.
However, it is a point of strength for link prediction because it generates a non-linear soft-threshold effecta type of gradual denoising (Cannistraci et al., 2009) based on a non-linear transformationon the network connectivity distances measured in the reduced space.
The soft-threshold discriminates between candidate links of protein pairs far apart in the original network topology (which earn large score values because they are now connected by enlarged path distances in the embedded space), and candidate links connecting nearby proteins in the original network topology (which earn small score values because they maintain or reinforce their topo-logical proximity in the embedded space).
To maximally exploit the effect of such soft-thresholding, we propose the use of the SP distance in the low-dimensional space.
As the network topologymapped to a reduced spaceshould now be sufficiently denoised by means of the MCE device, the use of the SP (instead of the ED) appears to be a more appro-priate way to assign distances between nodes because it obeys the denoised network topology.
This is even more reasonable considering that each interaction is remapped in the reduced space with a positive and definite numerical weight (see Supplementary Section II.1 and Fig.S2).
In conclusion, we expect the SP to be a congruous measure for converting the topological discrimination obtained by the MCE soft-threshold effect into a value.
This computational engagement between MCE as a technique for embedding (useful for denoising net-works affected by FP interactions) and SP for determining net-work-connectivity distance (effective when the networks are pure or denoised, and present few FP interactions), gives rise to a synergy that can boost the separation between good and bad candidate links in the ranking.
2.4 Non-centred MCE The expression crowding problem means that after dimension reduction, data clusters collapse on top of each other in the reduced embedding space (van der Maaten and Hinton, 2008).
This problem has particular relevance in network embedding because we want to avoid diverse network components collap-sing in the same region of the reduced space, as this can cause incorrect link predictions.
For this reason, we decided to intro-duce the ncMCE and test its performance in solving the crowd-ing problem.
Here, we propose a new version of MCE as an inedited form of non-linear-kernel principal component analysis.
In this new version, the MC matrix is interpreted as a non-linear and par-ameter-free kernel, and MC is a non-linear and parameter-free measure that produces a distance transformation stored in the MC kernel (previously referred to as the MC matrix).
The first step of the new algorithm remains unaltered, while the embed-ding of the MC kernel is now executed by singular value decom-position (SVD) according to the following procedure: (i) centring of the MC kernel; and (ii) SVD decomposition of the centred MC kernel, followed by the embedding in an arbitrary dimen-sion.
An advantage of the new algorithm is that the centring of the MC kernel can be omitted (see Algorithm 1 and Supplementary Fig.S1).
In practice, this generates a different dimension-reduction device, which we refer to as non-centred MCE (ncMCE).
There is no universal rule for when centring transformation should be used in the analysis.
Nevertheless, non-centring has been shown to offer several advantages (Basnet, 1993; Jolliffe, 2002).
This is particularly evident in visu-alization tasks, when the set of points that form each cluster is distributed around the centre of the mass in the high-dimensional space.
If we perform embedding in two dimensions after the centring transformation, the points tend to overlap around the origin of the first two dimensions, which is a typical example of the crowding problem.
However, in most cases, executing the embedding without centring can significantly reduce this issue.
In addition, omission of the MC kernel centring means that ncMCE has a time complexity of O Vj j2  , and thus is more ef-ficient than the other considered embedding techniques, such as MCE and Isomap, that have a time complexity of O Vj j3  .
For this reason, ncMCE also offers a significant computational ad-vantage for handling very large networks (see Supplementary Section II.1.2 and Table S2).
The new algorithm for ncMCE and MCE is available on the website indicated in the abstract.
Algorithm 1 Minimum Curvilinear Embedding (MCE) Input: A, n n adjacency matrix representation of a PPIN (nnumber of nodes in the network); d, the embedding dimension; c, a Boolean specifying whether the MC kernel will be centred or not; Output: X, n d matrix whose rows are the points with coordinates in a d-dimensional reduced space; Description: Extract the minimum spanning tree T out of A; Compute the distances between all node pairs over T to obtain the MC kernel D; If c TRUE: Centre kernel D, i.e.
D  12 JD2J with J I 1n 11T; Else: Continue; Perform economy size singular value decomposition of D UddVTd ; Return X ffiffiffiffiffiffi d p Vd T ; * MT indicates matrix transpose, I is the n n identity matrix and 1 is a column vector of ones.
2.5 Node neighbourhood methods To assess interaction reliability, several node neighbourhood techniques for link prediction have been proposed to exploit the topology of a PPIN, such as Interaction Generality (IG1) (Saito et al., 2002), IG2 (Saito et al., 2003) and IRAP (Chen et al., 2005), or to predict protein function, such as the Czekanowski-Dice Dissimilarity (CDD) (Brun et al., 2003) and FSW (Chua et al., 2006).
These techniques have also been used to predict PPIs based on the topological properties of the neigh-bours of candidate protein pairs (You et al., 2010).
As shown in Chen et al.
(2006) and Chua et al.
(2006) and mentioned in You et al.
(2010), FSW and CDD outperform IG1, IG2 and IRAP.
Because of this evidence, and considering that IG2 and IRAP are very computationally expensive (Chen et al., 2005), we decided to use only IG1 (as a baseline), CDD and FSW (details and i201 Minimum curvilinearity for PPI prediction associated formulae for these approaches appear in Supplementary Section III).
3 METHODS 3.1 Testing the proposed innovations 3.1.1 Fraction of FPs visited by MC and SP We generated 1000 random geometric graphs (for details see section 3.1.4) with small-world and scale-free topology, which are properties common to real biological and PPI networks.
Each network had 1000 nodes and was modelled with levels of noise similar to those of real PPINs: around 40% false negatives (FNs) and 60% FPs (Kuchaiev et al., 2009).
We counted the fraction of unique FPs visited out of the total number of FPs present when comput-ing SPs between all node pairs over the entire network (first step of the Isomap algorithm) and over the MST (first step of the MCE algorithm).
3.1.2 Solving the crowding problem Although it is not a biological dataset, the radar signal dataset is a point of reference in machine learning and is an important benchmark for testing the ability of embedding tech-niques to solve the crowding problem (Shieh et al., 2011).
Instead of creating an artificial dataset, we decided to use a real one to test whether ncMCE is able to solve the crowding problem and, as a result, better embed networks into low dimensions.
The radar signal dataset is highly non-linear and has 351 samples characterized by two classes: good radar signals that are highly similar, and bad radar signals that are highly dis-similar (Shieh et al., 2011).
3.1.3 Discrimination between good and bad candidate links Following Kuchaiev et al.
(2009) and You et al.
(2010), for each of the four considered yeast networks, we fitted a non-paramet-ric estimate to the distribution of low-dimensional distances between connected nodes in the network p distancejoriginal and another one to the distribution of distances between non-adjacent nodes p distancejcandidate .
We used the MannWhitney non-parametric test to determine whether there was a statistically significant difference be-tween p distancejoriginal and p distancejcandidate over the different di-mensions of embedding.
3.1.4 Evaluation on random geometric graphs Random geometric graphs (RGGs) are important because there is indication that they can be good models for networks such as PPINs (Przulj et al., 2004).
We gen-erated RGGs by accommodating 1000 points uniformly at random in the 100-dimensional unitary cube and then connected them if and only if the dot product (similarity) between the vectors with tails in the origin and heads over these points was above a connectivity threshold r. We set the threshold by ensuring that properties common to real biological networks (small-world and scale-free topologies) and connectivity were present.
The advantage of using RGGs to test our innovations is that the sets of true and spurious interactions are clearly defined: true interactions are those that fulfil the threshold and spurious links are those that do not.
Based on this, we generated noise in the structure of 1000 different RGGs (noisy networks) in amounts typical of PPINs (40% FNs and 60% FPs) and performed a sparsification experiment in which the embedding pre-dictors (a detailed explanation of how embedding prediction works is given in section 3.2 and Fig.1) were adopted to rediscover the removed true interactions present in the generated RGG.
During this test, the networks were embedded into dimensions 1 to 10, which is the recom-mended range for testing the performance of Isomap (You et al., 2010).
Next, we repeated the experiment to assess the performance of the embed-ding predictors on a sparsification experiment over 1000 different RGGs without noise (clean networks).
3.2 General prediction and GO-based evaluation framework The flow diagram in Figure 1 depicts the required steps for link prediction and GO-based performance evaluation in PPINs.
In the prediction phase, the original network lying in the high-dimensional space is represented as an adjacency matrix A with entries Ai, j 1 if nodes i and j interact and Ai, j 0 otherwise (each of these non-adjacent pairs of nodes is con-sidered a candidate interaction).
Next, the network is embedded into a reduced space (initially of dimension 1) where both the original network links and the candidate edges are scored by means of either ED (as in Kuchaiev et al.
and You et al.)
or SP (our proposed variation, see section 2.3).
Both sets of links are then ranked (see table of scored interactions in Fig.1).
A criterion (based on the ranked list) is used to automatically determine an appropriate dimension into which the network should be embedded (see sections 3.3 and 4).
If the criterion is not fulfilled, the above procedure is repeated for a higher dimension, otherwise it stops and a list is output using only the ranked candidate interactions (see table of scored candidate interactions in Fig.1).
The code that takes A as input and provides the scored list of candidates as the output is available on the website provided in the abstract.
The evaluation phase is specific to PPINs and follows the same gene ontology (GO) strategy adopted in past studies (Chen et al., 2005, 2006; Saito et al., 2002, 2003; You et al., 2010).
The proteins involved in the interactions from the candidate list are annotated via GO terms (molecu-lar function or MF, biological process or BP, and cellular compartment or CC).
If the terms associated with a protein pair have a high Wangs GO semantic similarity (see Supplementary Section IV.2), the PPI is Fig.1.
Link prediction and performance evaluation in PPINs.
The com-ponents in red correspond to the novel features proposed in this study i202 C.V.Cannistraci et al.
considered to be biologically relevant (marked with a Yes in the table in Fig.1) and is used to quantify the precision of the predictors.
GO is used to assess how precisely the prediction techniques place candidate interactions that are likely to be real at the top of the ranking list (You et al., 2010).
A recursive procedure is applied to create a precision curve.
Each time, an increasing fraction of candidate PPIs (the first 100, the first 200 and so on) is taken from the top of the list of ranked candidate interactions for con-sideration.
The fraction of candidate interactions that are relevant to GO generates a point on a precision curve.
Conventionally, a number of top-ranked candidate links, equivalent to 10% of the links in the original net-work, is used to compute the entire precision curve (You et al., 2010).
We also examine the curve generated for a number of candidates equal to 100% of the original network links (see Supplementary Section IV).
The area under the precision curve (AUP)normalized with respect to the x-axis so that it ranges from 0 to 1summarizes the performance of the prediction technique for a given network.
Precision and AUP are the pre-ferred statistics (Chen et al., 2005, 2006; Saito et al., 2002, 2003; You et al., 2010) for evaluating the predicted links in biological networks, owing to their noisy nature.
Having a non-adjacent pair of proteins in PPINs does not mean they cannot interact at all, and we cannot label the missing interaction as a TN.
It is quite possible that these proteins have not yet been tested for interaction, or that it is experimentally difficult to do so (see Supplementary Section IV).
As a result, performance statistics that do not rely on the number of TNs, such as Precision, are more suitable in this context than others, such as the AUC.
We also propose an innovative strategy for evaluating the performance of a link prediction technique at different levels of random sparsification of the original PPIN.
Given a network G V,E, we generated an initial set of 50 sparsified networks by removing a fixed portion of links e 0:1 Ej j(10% of links) uniformly at random from the original topology (sparsification process).
Then, we generated a second set of 50 sparsified networks by removing the same fixed amount of links, e, uniformly at random from the networks sparsified in the previous step (a total of 20% of links removed).
This process was repeated several times up to the point where network connectivity was lost.
The AUP of each prediction tech-nique was computed for each percentage (proportion of links removed) for the 50 networks, and the average AUP is reported as a sparsification curve.
In addition, the area under this sparsification curve is useful for quantifying the robustness of a technique as a function of the network sparsity, which is one of the main issues for current link predictors (You et al., 2010).
Some GO annotations may be subject to experimental bias or come from not very reliable sources (Rhee et al., 2008).
To address this issue, and as an additional verification of the candidate interactions proposed by the best techniques, we performed what we call an in-silico validation.
We took the top 100 candidate interactions proposed by the best techniques and intersected them with the entire STRINGDatabase (Szklarczyk et al., 2011) in March 2013.
STRING is the most complete compendium of PPIs found in the literature, experiments, coexpression, etc.
Given a list of pro-teins, it finds the interactions between them along with an assigned confi-dence value based on the available evidence that they exist.
The output of this validation was used to compute (i) the number of protein pairs vali-dated for each network out of the top 100; (ii) the average STRING con-fidence along with its standard deviation; and (iii) the average GO confidence along with its standard deviation.
Note that this validation was carried out for candidate interactions only, and the PPIs of the used main network datasets were not considered in this analysis.
Therefore, the overlap between the studied networks and STRING is unlikely to influence the results (see Supplementary Section I).
3.3 The AUC criterion for dimension determination The AUC criterion is designed to work in combination with any network embedding algorithm adopted for link prediction: it automatically deter-mines the dimension into which the network should be embedded (see Supplementary Section II.4.1 for details).
For a certain dimension of embedding, the prediction procedure (Fig.1) assigns a likelihood score to each interaction (low scores correspond to interactions that are likely to occur and high scores to interactions that are not).
The scores are computed for both the original interactions in the network (O) and the candidate interactions (C), which are all those protein pairs that were not linked in the input network.
The scored O and C interactions generate two distance distributions (see Supplementary Fig.S7 for an example).
As suggested by Kuchaiev, You and their teams, we can vary a cut-off ", from 0 up to the maximum distance of the two distributions, so that all protein pairs with scores below " are considered positives and all protein pairs with scores above " are considered negatives (Supplementary Fig.S7).
Kuchaiev et al.
and You et al.
suggest that by taking the original network PPIs as our positive set, we can compute the number of TPs, FNs, FPs and TNs at each " cut.
This will yield a pair (1-Specificity, Sensitivity) that, measured for the entire " range, generates a Receiver Operating Characteristic curve (ROC) and an Area Under the ROC Curve (AUC) that characterizes the performance for the current dimension (Supplementary Fig.S7).
Both research groups showed that the AUCs for different dimensions were very similar and the increase in the AUC value tended to vanish for higher dimensions; thus, they considered a fixed dimension of 5 and 10 respectively for their experiments (You et al., 2010).
We took advantage of this finding (Supplementary Fig.S7) by computing the AUC for each dimension, starting with dimension 1 and continuing until the difference between the AUC of one dimension and the next was less than 1E-3.
In several tests, we found that 1E-3 represents such a small difference be-tween AUCs that we can consider it not significant; thus, the last AUC is considered appropriate to identify the dimension for embedding.
We then took the scored candidate interactions given by this dimension for the final evaluation of the method used.
3.4 The resolution criterion for dimension determination One of the motivations for proposing a second criterion was that the AUC criterion considers the original network as a sort of gold standard, when in reality it includes several false interactions (You et al., 2010).
The new criterion for dimension determination is based on the idea that the greater the difference between likelihood score values, the better they discriminate between good candidates and bad candidates in the ranking.
Thus, we need to define a measure of the resolution of the score values provided by each dimension, such that the higher themeasure, the higher the resolution and the more we should consider this dimension as correct for embedding.
The measure we use for dimension determination is as follows: ResolAll unique scores Dim 1 This formula takes all of the unique score values of the candidate interactions in dimension Dim, computes its standard deviation and divides it by Dim.
The unique score values give us an indication of the Dims resolution.
We then determine the quality of that resolution by computing , which quantifies the variation between the unique score values.
Finally, the division by Dim penalizes the higher dimensions, which have been shown not to provide any relevant increase in perform-ance (You et al., 2010).
We specifically designed this criterion to fit with the quality of MCE, which provides more of a soft-threshold effect in low dimensions.
This is why we only tested the resolution criterion in com-bination with MCE.
We also applied a variation of Equation (1) to check whether dimension determination using only the ranked interactions be-tween 0 and 100 would generate a better AUP.
The difference between Equation (1) and (2) is that in (2) we compute on the unique scores from the top 100 candidate protein pairs: Resol100 unique scores1 to 100 Dim 2 i203 Minimum curvilinearity for PPI prediction 4 RESULTS AND DISCUSSION 4.1 Fraction of FPs visited by MC and SP The results presented in Supplementary Figure S3 for the artifi-cial networks (RGGs) suggests that the estimate of non-linear connectivity measure using the MST (i.e.
MC) takes into account only a small proportion of FPs, offering a denoised estimate of the network connectivity.
In contrast, using the SP over the entire noisy network counts all FPs at least once, which intro-duces a lot of noise into the link prediction process.
The same investigation was conducted on the four yeast networks, in which the FP links were identified using the same GO-based strategy mentioned in section 3.2.
The outcome of this second analysis (Supplementary Fig.S3) converged to the same result obtained for the artificial networks.
These findings support the hypothesis that MCE should be a powerful tool for link prediction in noisy networks.
In fact, as noisier networks present more FP inter-actions, the use of ncMCE/MCE in such cases should produce an even greater increase in performance over the use of Isomap.
As current PPINs are sparse and noisy (You et al., 2010), the use of ncMCE/MCE instead of Isomap should offer clear advan-tages in network denoising and link prediction in the reduced space.
A proof of this is provided in the computational experi-ment on RGGs discussed in section 4.4.
4.2 Solving the crowding problem The ncMCE (Fig.2A) offered the best embedding of the radar signal dataset and attained high linearization (Fig.2E) in both the first (AUC 0.95) and the second dimensions (AUC 0.96).
The ROC curve is used to evaluate the discrimination power along a dimension of projection: if the dimension offers a linear discrimination between the good and bad signals, the re-spective AUC will be 1.
We also tested the performance of Isomap, which is a reference algorithm for non-linear dimension reduction, but its embedding was highly crowded (Fig.2C).
In contrast, Tree Preserving Embedding (TPE) (Shieh et al., 2011)a recent parameter-free algorithm for non-linear dimen-sion reductionproduced non-linear discrimination (good sig-nals in the centre and bad signals on the periphery) of the clusters around the origin of the axis (Fig.2D).
This demon-strates that TPE can address the crowding problem but cannot solve the non-linearity of the dataset.
MCE solved the non-lin-earity in the second dimension (Fig.2E), but only partially ad-dressed the crowding problem (Fig.2B).
The only algorithm that was able to simultaneously solve both the non-linearity and the crowding problem in this dataset was ncMCE (Fig.2A and E).
Interestingly, on the basis of the embedding offered by ncMCE and MCE, one might speculate that the high dissimilarity be-tween the bad radar signals pointed out in previous studies (Shieh et al., 2011) could be interpreted as the presence of at least two different kinds of bad radar signal clusters that are difficult to embed due to their high non-linearity (elongated and/or irregular high-dimensional structure).
The possible dif-ferent bad-signal clusters are indicated in grey and black in Figure 2.
Finally, whereas only a few seconds were needed to run ncMCE, MCE and Isomap, TPE took several hours to embed this small dataset, and its current implementation can be prohibitively slow for large datasets.
As TPE is inefficient for embedding networks composed of thousands of nodes, we could not evaluate its performance in the present study.
4.3 Discrimination between good and bad candidate links Given the embedding of any PPIN, if the hypothesis that nodes closer to each other in the reduced space are more likely to inter-act is true, the network-link distribution p distancejoriginal should have higher peakedness (kurtosis) than the candidate-link distribution p distancejcandidate ; in addition, p distancej original) should be shifted towards zero.
The results in Figure 3 and Supplementary Figure S4 show that in all networks, ncMCE-SP had the highest kurtosis and shift towards zero.
Moreover, links from the original network topology that are distant from the origin are likely to represent false positives, while non-adjacent nodes whose distance is close to zero are good candidates for interaction.
Furthermore, Supplementary Figure S4 shows that in the four considered networks, there was a statistically significant differ-ence between p distancejoriginal and p distancejcandidate .
This significant difference was conserved across the different dimen-sions, and was much larger when the SP scoring technique was used in the reduced space.
This indicates that SP should work better than ED for scoring proximity distances between network nodes (proteins) in the reduced space.
Fig.2.
Embedding of the radar signal dataset.
The red spots indicate good radar signals.
The grey and black spots indicate bad radar signals, which might be interpreted as two diverse sub-categories of bad signals.
(A) ncMCE.
(B) MCE.
(C) Isomap.
(D) TPE.
(E) ROC and respective AUC computed for evaluating the linear discrimination performance of the first (Dim1) and second (Dim2) dimensions.
The evaluation was re-peated for each of the four techniques on the two dimensions of embed-ding.
To facilitate the visualization, we do not report the ROC for TPE due to its poor performance i204 C.V.Cannistraci et al.
4.4 Evaluation on random geometric graphs Figure 4AC shows that the two variations of MCE (especially ncMCE-SP using dimension one) were the strongest approaches for re-predicting true interactions in 1000 RGGs with similar levels of noise to those in real protein networks.
Next, Figure 4DF shows that when we repeated this experiment in 1000 RGGs without noise, ncMCE-SP still had the best perform-ance (using dimension 1), but Isomap-SP came significantly closer.
However, as our RGGs are sparse, the number of candi-date links is very large compared with the number of links deleted during sparsification.
In such conditions, link prediction is generally a difficult task, and this justifies the low precision values in Figure 4 (see Supplementary Section II.1.1 for more details).
Altogether, these results indicate that (i) the use of ncMCE presents a clear advantage over MCE; (ii) the lower dimensions (especially dimension 1 for ncMCE) are very effective when using ncMCE/MCE-based algorithms; (iii) the gap be-tween ncMCE-SP and Isomap-SP increases in the presence of noise, which especially encourages the use of ncMCE-based al-gorithms in noisy networks such as PPINs; and (iv) the use of SP (to assess the scoring in the reduced space) generally offers a clear advantage over the use of ED.
RGGs are crucial for designing a ground-truth evaluation that allows us to directly observe the effect of introducing noise (false interactions) in the re-prediction of the real/original network topology.
Because GO-free evalu-ations are essential for demonstrating the performance of link predictors in the absence and presence of network noise, the findings here are our first important results.
4.5 Evaluation using gene ontology The novel approach we propose is based on the intuition that network embedding by ncMCE/MCE combined with the SP connectivity distance in the reduced space can boost the perform-ance in topological prediction of candidate PPIs.
In Figure 5, we provide experimental confirmation of our intuition (see Supplementary Figs S11, S12 and S13, where we show that even when the Molecular Function GO category is excluded, when a wider candidate list of interactions is included in the evaluation or when proteins involved in large complexes are removed from the analysis, in general, our proposed approaches outperform the others).
MCE and ncMCE combined with SP outperformed both Isomap and pure SP (computed on the ori-ginal network without embedding) in all networks.
Isomap Fig.4.
Sparsification and reprediction of random geometric graphs.
Mean re-prediction precision of true-positive interactions for different sparsification levels of noisy networks with 60% false-positive interactions in their original topology: embedding dimensions 1 (A) and 4 (B) are displayed.
The standard error bar is reported for each point.
Analogous plots for clean networks (which do not present false-positive interactions in their original topology) are reported again for dimensions 1 (D) and 4 (E).
The Area under the Mean Precision Curve is reported for each dimension of embedding, considering the re-prediction of true-positive interactions in noisy (C) and clean (F) networks.
The arrow indicates the overall best performance (given by ncMCE-SP in dimension one).
The percentage improvement in respect to the best Isomap (ISO-SP) performance is reported Fig.3.
Discrimination between original network and candidate PPIs.
Distribution of shortest-path scores in the reduced space (dimension 3 displayed) for Ben-Hur and Noble 2005 dataset.
Network links p(dis-tancejoriginal) (solid line) and candidate links p(distancejcandidate) (dashed line) after (A) ncMCE and (B) Isomap network embedding.
The insets show the distribution of Euclidean distance scores i205 Minimum curvilinearity for PPI prediction performed even worse than pure SP in the first network.
Besides, the simulation in Figure 5 suggests that in general ncMCE-SP slightly outperforms MCE-SP; and Supplementary Figure S6 shows that ncMCE-SP even outperfomed IsomapFSW.
In addition to the above results, there is evidence (Fig.6) that although we used several advanced techniques for dimensionality reduction (both unsupervised and supervisedly tuned), ncMCE-SP remained the overall leader, which represents our second im-portant finding.
Surprisingly, we discovered that even though local MDS and NeRV were supervisedly tuned to achieve their best performance, they could not equal ncMCE-SP.
This result suggests that force-based methods for embedding are not appro-priate in this context, at least when combined with ED or SP in the reduced space.
The reason for their poor performance is that these algorithms perform an embedding that finely preserves the network topology, thus also preserving the noise.
In contrast, ncMCE provides a soft-threshold effect (discussed in Section 2.3), which boosts the separation between good and bad candi-date links in the ranking.
For completeness, we compared ncMCE-SP with FSW and CDD, two of the most efficient node neighbourhood techniques (You et al., 2010).
ncMCE-SP ranked first, with a notable im-provement, in the first two networks (Ben-Hur and Noble 2005; Chen et al.
2006; Supplementary Fig.S9), and second in the third network (You et al.
2010 sparse, Supplementary Fig.S9).
In the fourth network, all of the techniques produced similar perform-ances (You et al.
2010 dense, Supplementary Fig.S9).
According to the minimum precision curve attained in the four different networks, ncMCE-SP was also the most robust technique (Robustness comparison, Supplementary Fig.S9).
FSW ranked first in the third network, while in the first two networks its performance was similar to that of CDD.
Given these results, we can conclude that ncMCE-SP offers a general improvement, particularly in robustness, compared with the other techniques.
4.6 Testing the criteria for dimension determination Another important variation we introduce here is the use of two diverse criteria for automatically selecting the congruous dimen-sion into which the network should be embedded.
So far, in the simulation showed in Figures 5 and 6, we used the AUC criter-ion, which was designed to work with any algorithm for embed-ding.
Unlike the AUC criterion, the resolution criterion was designed to fit better with MCE, which provides more of a soft-threshold effect (thus stronger denoising) in the lowest dimensions.
This is experimentally confirmed in Supplementary Figure S8A, where the peaks of the resolution criteria (both ResAll and Res100) are always in one of the first two reduced dimen-sions.
From Supplementary Figure S8B, we gather that the AUC criterion and the ResAll criterion selected the same dimensions, and thus show equal precisions.
However, in terms of robustness (Supplementary Fig.S8C), the Res100 criterion slightly outper-formed the others.
These results corroborate our intuition to invent a new and radically different criterion based on the reso-lution of the unique score values, which is an easy and time-efficient strategy.
4.7 Network sparsification evaluation To present a more refined vision of the potential offered by topological link-prediction techniques, we introduce a new evalu-ation strategy called network sparsification experiment (see sec-tion 3.2 for details).
This approach was used to generate the results shown in Figure 7, which compares the main embedding techniques (ncMCE, MCE and Isomap) and the reference node neighbourhood techniques (FWS, CDD, IG1 and SP).
All of the embedding methods were tested in combination with the same distance (SP) to measure candidate-link likelihood in the reduced space.
Figure 7A and B display the sparsification curves of the first two networks for ncMCE-R (R indicates the use of the resolution criterion) and FSW that were the highest ranked methods overall in their respective categories.
Although ncMCE-A (A indicates the use of the AUC criterion) attained the same result as ncMCE-R in each network, for the sake of clarity, we display only the curve of the latter.
The methods were ranked considering the area under the sparsification curve (AUS).
To evaluate the general performance of the methods, we considered the minimum AUS performance of each method for all networks (Fig.7C).
A special variation of this experiment was performed on each network (Supplementary Fig.S5) to investigate whether the ex-traction of different MSTs from the networks resulted in Fig.5.
Performance comparison between ncMCE, MCE, Isomap and pure SP computed in the high-dimensional space.
The x-axis indicates how many interactions are taken from the top of the candidate inter-action list (sorted decreasingly by score), and the y-axis indicates the precision of the technique for that portion of protein pairs.
Solid lines indicate the use of the SP in the reduced space to assign scores and dashed lines the use of the ED i206 C.V.Cannistraci et al.
important changes in ncMCE performance.
This is a possibility because all of the network links have a weighting value of 1.
For this test, only ncMCE was used because it generally outper-formed MCE, as shown in Figures 5 and 6.
Here, for each per-centage of link deletions, 100 different MSTs were extracted (by random initialization).
The AUPs attained by the different ncMCEs (each of which uses a different MST) were averaged and their standard error bars included in the sparsification curve.
The standard error bars for the ncMCEs sparsification curves (Supplementary Fig.S5) show that the difference in the perform-ance of ncMCE when using different MSTs was negligible.
In general, MCE-based embedding techniques (red bins in the histogram, Fig.7C) outperformed the node neighbourhood tech-niques (green bins in the histogram, Fig.7C), and ncMCE was again the best method.
Taken together, our experiments suggest that ncMCE-SP might represent a new benchmark for robust-ness in the topological prediction of PPIs, and this is the third main result of our study.
As a further investigation, starting with the final set of sparsified networks generated in the previous experiment, we re-densified their topologies by random addition of links and applied two approaches (ncMCE-R and FSW) at each percent-age of densification.
As we can see in Figure 7A and B, this process was unable to re-create a meaningful topology that might have been shaped by evolutionary features in the history of the protein interactome.
If a topology analogous to the ori-ginal had been recovered, the prediction techniques would have been able to achieve a performance comparable with that reached before network sparsification.
This finding emphasises the presence of preferential bio-infor-mation in the PPIN topology that cannot be modelled by uni-form random sampling of new interactions.
Therefore, the simple unweighted topology can be highly informative for different pur-poses, one of them being the prediction of new interactions or alternatively, as recently shown, the structural controllability of any complex network (Liu et al., 2011).
4.8 In silico validation As mentioned in the Introduction, the experimental detection of PPIs can be very expensive in terms of both time and money.
The computational approaches we investigated to predict novel inter-actions are meant to guide wet-lab experiments rather than to complete the interactome of the organism under study.
Currently, the Y2H validation of 100 protein pairs can represent Fig.6.
ncMCE-SP against advanced unsupervised and supervisedly tuned embedding techniques.
The x-axis indicates how many interactions are taken from the top of the candidate interaction list (sorted by decreas-ing score), and the y-axis indicates the precision of the technique for that portion of protein pairs.
Solid lines represent the performance of tech-niques that use SP in the reduced space to assign scores and dashed lines represent techniques that use ED.
Although ncMCE-SP (red solid line) is an unsupervised approach, it appears on both sides for reference Fig.7.
Network sparsification and redensification.
(A and B) Sparsification curves (solid line) and redensification curves (dashed line).
The arrows indicate the direction of the simulation (right for spar-sification and left for redensification) as a function of the average node degree.
Each point on the curves is obtained as the average AUP on 50 random sparsified or redensified networks, and the standard error bar is reported.
(C) Sparsification robustness is useful for quantifying the ro-bustness of a technique as a function of the network sparsity.
It is com-puted as the minimum area under the sparsification curve (AUS) among all networks.
The red bins indicate MCE-based methods; the green bins indicate neighbourhood-based methods; the blue bins indicate Isomap-based methods; and the violet bin indicates the pure SP directly applied on the network i207 Minimum curvilinearity for PPI prediction a challenging upper limit to simulate a real scenario for the budget of many labs.
We decided to suggest different sets of candidate interactions to test in wet-lab experiments, and we report the evaluations for different thresholds: 20, 40, 60, 80 and 100.
We executed an in-silico validation to verify the quality of the candidate interactions proposed by the best techniques.
The top 100 ranked interactions for ncMCE-SP-Res100 and FSW were tested on the STRING database, which is the most com-plete PPI database.
The results for the different thresholds are reported in Supplementary Figure S10E.
ncMCE-SP-Res100 at-tained promising results in this last test, surpassing FSW for GO precision (Supplementary Fig.S10A and D), GO robustness (Supplementary Fig.S10B) and STRING confidence robustness (Supplementary Fig.S10C and D).
The list of the top 100 candidate interactions ranked by ncMCE-SP-Res100 is reported for each of the analysed networks in Supplementary Table S1 and the respective list for FSW in Supplementary Table S2.
GO semantic similarities and STRING confidence values are also included.
To search for the biological information related to the interactions predicted by ncMCE-SP-Res100 and validated in STRING, for each network we per-formed a pathway enrichment analysis using DAVID Bioinformatics Resources 6.7 (Huang da et al., 2009a, b).
For each network, the list of proteins involved in the predicted and validated interactions was tested against all network proteins as background.
This kind of background choice was motivated by the fact that it tends to produce more conservative P-values and, in fact, a general guideline for the enrichment analysis is to use a narrowed-down list of genes instead of all genes in the genome (Huang da et al., 2009a, b).
In addition, the Benjamini correction for multiple hypotheses test was applied.
The results of the ana-lysis (reported in Supplementary Table S3) emphasize that the lists of predicted and STRING-validated protein interactions have significant biological meaning in at least one pathway for each of the investigated networks.
Interestingly, the predicted proteins were involved in cellular processes (e.g.
cell cycle), nu-cleotide metabolism (e.g.
pyrimidine and purine metabolism) and genetic information processes (e.g.
RNA polymerase and RNA degradation).
This evidence suggests that the proposed method predicted interactions in different network modules that are related to significant and heterogeneous pathways in yeast.
5 CONCLUSIONS AND PERSPECTIVE Considering the difficulty of dealing with sparse and noisy pro-tein networks (You et al., 2010), our results represent a promis-ing achievement and encouragement to further the investigation of network embedding techniques for topological prediction of candidate protein interactions.
In our tests, the ncMCE showed enhanced performance in network embedding-based link predic-tion compared with the other dimension-reduction algorithms.
In addition, ncMCE has a time complexity of only O Vj j2  which is lower than the complexity of the other considered ma-chine learning techniquesand is a valid candidate for handling very large networks.
Finally, our experiments revealed that the shortest path works significantly better than the Euclidean dis-tance for scoring proximity distances between network nodes (proteins) embedded in the reduced space.
We envision that net-work-embedding techniques for predicting novel PPIs might play an important role in the development of systems biology tools, such as those used for network-based inference of disease-related functional modules and pathways (Cannistraci et al., 2013b).
The real biological interactions could be complemented with the in-silico predicted ones to boost the inference of the functional modules.
In the near future, this last point will become increas-ingly important for patient classification, diagnosis of disease progression and planning of therapeutic approaches in persona-lized medicine (Ammirati et al., 2012).
ACKNOWLEDGEMENTS The authors thank Prof. Vladimir Bajic, Dr Taewo Ryu and Loqmane Seridi for inspiring the sparsification experiments.
Ewa Aurelia Miendlarzewska, Maria Contreras, Emily Giles and Anthony Hoang for proofreading the article; and Prof. Samuel Kaski and Prof. Edo Airoldi for providing the code to run NeRV and TPE, respectively.
C.V.C.
thanks Prof. Trey Ideker for introducing him to the PPI prediction problem, and Dr Alberto Roda for his support in finalizing the article.
Funding: This work was supported by King Abdullah University of Science and Technology.
Conflict of Interest: none declared.
ABSTRACT Summary: We present a tool suited for searching for many short nucleotide sequences in large databases, allowing for a predefined number of gaps and mismatches.
The commandline-driven program implements a non-deterministic automata matching algorithm on a keyword tree of the search strings.
Both queries with and without ambiguity codes can be searched.
Search time is short for perfect matches, and retrieval time rises exponentially with the number of edits allowed.
Availability: The C++ source code for PatMaN is distributed under the GNU General Public License and has been tested on the GNU/Linux operating system.
It is available from http://bioinf.eva.mpg.de/patman.
Contact: pruefer@eva.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION There is an increasing need to rapidly and accurately align short sequences to genomic or other biological sequences.
Short sequence motifs, including restriction enzyme sites, microarray probe sequences, transcription factor binding motifs and miRNA sequences, are abundant in many areas of molecular biology.
Identifying these short sequences is a crucial step in designing experiments and analyzing newly available genomic sequence data.
The most widely used approach for aligning sequences to large databases is the BLAST algorithm (Altschul et al., 1990).
Further optimized versions have been presented to speed searches for large numbers of sequences.
The BLAST family of algorithms search for good alignments only where short, perfect seed matches between the query and target sequence exist.
This heuristic vastly improves the overall speed by restricting the expensive alignment process to regions containing these short exact matches.
There is a tradeoff between an extensive search and the speed performance of the algorithm.
A search with longer seeds may miss some good alignments that contain mismatches or gaps, while shorter seeds will prolong alignment time.
This tradeoff is especially severe for short query sequences because these may not contain a seed match to trigger full alignment, thereby missing good hits.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
A well-known algorithm for searching multiple strings was introduced by Aho and Corasick in 1975.
Although this approach has previously been implemented to search for restriction enzyme sites (Mount and Conrad, 1986; Smith, 1988), a comprehensive implementation for searches with mismatches and gaps is not available to our knowledge.
We developed PatMaN (Pattern Matching in Nucleotide databases), a tool for performing exhaustive searches to identify all occurences of a large number of short sequences within a genome-sized database.
The program reads sequences in FastA format and reports all hits within the given edit-distance cutoff (i.e.
total number of gaps and mismatches).
We demonstrate the programs functionality by aligning Affymetrix HGU95-A microarray probes to the chimpanzee genome.
2 METHODS 2.1 Usage The program accepts several parameters to specify a search.
The user can specify both the maximum number of gaps and the total number of edits (gaps+mismatches) allowed in any reported match.
Additionally the interpretation of ambiguity codes can be modified.
When the ambiguity flag is set, any ambiguous character in the query sequences will be counted as a match if the aligning base is one of the nucleotides represented by the ambiguity code.
When the flag is omitted, only the ambiguity code N is allowed in the query sequences, and a base aligning to this character will be counted as a mismatch.
Both the query and target sequences must be in FastA format.
The output is given in a tab-separated format containing the target and query sequence identifier, the start and end position of the alignment in the target sequence, the strand and the number of edits per match.
2.2 Algorithm When initiated, the program begins constructing a single keyword tree of all the query sequences (Fig.1).
All bases along a query sequence are added as a path from the root of the tree to a leaf, with the edges representing the bases added, and the leaf node containing the query sequence identifier.
If the user sets the ambiguity flag, all possible bases at ambiguous positions are added to the tree.
If the user does not trigger the ambiguity flag, each base is added only once to the tree.
The search for occurrences on forward and reverse strands is facilitated by also adding the reverse complement of all query sequences to the same tree.
If an outgoing edge is not yet occupied after storing the query sequences, an additional suffix link is set to the longest existing suffix for the sequence represented by the path from the root to the node under consideration.
The resulting graph will consist of internal nodes with outgoing edges for all four possible bases and for the ambiguity base N. This procedure corresponds to the initial processing steps in the 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[23:15 18/6/03 Bioinformatics-btn223.tex] Page: 1531 15301532 PatMan CCC GA GT A C C C G T GC G A,T,N A,T,N A,T ,N G N Fig.1.
Keyword tree with suffix links after adding the sequences CCC, GA and GT.
The keyword tree (represented as bold lines) encodes the probe sequence as a path leading from the root node on the left side to the leaves on the right side.
Suffix links are shown as arrows, but have been omitted at leaf nodes for brevity.
AhoCorasick algorithm [for a complete discussion see Navarro and Raffinot (2002)].
Figure 1 depicts the resulting data structure for a small input example.
Once the tree is constructed, each sequence in the target database is evaluated base by base and compared to a list of partial matches.
Each partial match consists of a node together with the number of mismatches and gaps accumulated.
The list is initialized with one element containing the root node of the tree and an edit count of zero.
In each iteration of the algorithm, all partial matches are advanced along a perfectly matching outgoing edge.
Additional elements are stored for following mismatched edges and for producing all possible gaps, as long as the number of edits remains below the threshold given.
If the outgoing edge is a suffix link, the resulting partial match is only included if no mismatch or gap occured in the part before the suffix.
The number of edits needed to align the suffix is stored in the partial match when following a suffix link.
Matches are reported when a partial match reaches a leaf node before exceeding the predefined number of allowed edits.
The sequence identifier, match coordinates and number of edits are printed.
2.3 Complexity When ambiguity codes are not interpreted and the query sequences contain no N character, the keyword tree can be constructed in O(L) time and requires O(L) space, where L represents the total length of all query sequences (Navarro and Raffinot, 2002).
When ambiguity is enabled, both time and space requirements increase exponentially in the number of ambiguity codes used in the patterns.
The time efficiency of the search algorithm is linear in the size of the target database, but depends heavily on the maximum edit distance as well as the average length and number of query sequences.
For each additional edit operation, an exponentially increasing number of partial matches must be considered, since neighboring mismatched nodes and all possible gapped alignments are searched along with the perfect matching path through the tree.
However, if only perfect matches are searched, the algorithm acts like the AhoCorasick algorithm, and search time depends solely on the length of the target sequence.
Time constraints therefore mean that PatMaN is only suitable for searching short sequences with a limited number of edit operations.
3 RESULTS We used PatMaN to match 201 807 Affymetrix HGU95-A micro-array 25mer probes to the chimpanzee genome (panTro2).
The parameters chosen for this evaluation allowed up to one mismatch, Table 1.
HGU95-A probes and Bonobo Reads against Chromosome 22 Dataset Edits Gaps Run time Hits HGU95-A probesa 0 0 0m13.31s 93 225 HGU95-A probesa 1 0 1m51.87s 327 028 HGU95-A probesa 1 1 3m36.92s 496 296 HGU95-A probesa 2 1 1h21m59s 1 843 008 Bonobo Solexa GAII datab 2 2 12h58m50s 14.3 109 aBenchmarking was performed on a 2.2 GHz workstation.
Independently of the chosen parameters 260 MB RAM were used.
bBenchmarking was performed on a 1.8 GHz workstation and 8.6 GB of RAM was used during execution.
The dataset contains 2.8 million reads of 38 bp length of genomic sequence from a Bonobo individual sequenced on the Solexa GAII platform.
but no gaps.
The program spent 2.5 h searching through all chimpanzee chromosomes and found 15.9 million hits (including 14.4 million hits to ALU repeat sequences).
A table containing all unique hits to the chimpanzee genome is available on our website.
Table 1 summarizes the time measured for conducting searches with different edit distance parameters using the same microarray probes and reads from one lane of the Solexa platform for chimpanzee chromosome 22.
The measurement shows the exponential increase in runtime with the maximum allowed edit distance.
4 CONCLUSION We present a new tool for mapping short sequences to large nucleotide databases.
The program does not require target or query database preprocessing and runs rapidly when a search is restricted to small edit distances.
While we demonstrate the programs utility by aligning microarray probes, we anticipate further applications in the near future.
In particular, mapping tags generated using next generation resequencing technology will require fast approximate matching to genomes to facilitate large-scale analysis of gene expression.
ACKNOWLEDGEMENTS We would like to thank Christine Green for critically reading the article.
Funding: Funding has been provided by the the Max-Planck Society.
Conflict of Interest: none declared.
ABSTRACT Summary: Computational methods designed to discover transcription factor binding sites in DNA sequences often have a tendency to make a lot of false predictions.
One way to improve accuracy in motif discovery is to rely on positional priors to focus the search to parts of a sequence that are considered more likely to contain functional binding sites.
We present here a program called PriorsEditor that can be used to create such positional priors tracks based on a combination of several features, including phylogenetic conservation, nucleosome occupancy, histone modifications, physical properties of the DNA helix and many more.
Availability: PriorsEditor is available as a web start application and downloadable archive from http://tare.medisin.ntnu.no/priorseditor (requires Java 1.6).
The web site also provides tutorials, screenshots and example protocol scripts.
Contact: kjetil.klepper@ntnu.no Received on April 21, 2010; revised on June 17, 2010; accepted on June 30, 2010 1 INTRODUCTION Computational discovery of transcription factor binding sites in DNA sequences is a challenging problem that has attracted a lot of research in the bioinformatics community.
So far more than a hundred methods have been proposed to target this problem (Sandve and Drabls, 2006) and the number of publications on the topic is steadily increasing.
There are two general approaches for discovering potential transcription factor binding sites with computational tools.
One is to examine regulatory regions associated with a group of genes that are believed to be regulated by the same factors and search for patterns that occur in all or most of these sequences.
This approach, often referred to as de novo motif discovery, can be used when we have no prior expectations as to what the binding motifs might look like.
One concern with this approach, however, is that it might be necessary to consider rather long sequence regions to ensure that the target sites are indeed covered.
Since binding motifs for transcription factors are usually short and often allow for some degeneracy, the resulting signal-to-noise ratio can be quite low, making it difficult to properly discriminate motifs from background.
Another problematic issue is that DNA sequences inherently contain a lot of repeating patterns, such as tandem repeats and transposable elements, which To whom correspondence should be addressed.
can draw focus away from the target binding motifs when searching for similarities between sequences.
The other general motif discovery approach, called motif scanning, searches for sequence matches to previously defined models of binding motifs, for instance in the form of position weight matrices (PWMs; Stormo, 2000).
The main drawback with motif scanning is that it tends to result in an overwhelming number of false positive predictions.
According to the futility theorem put forward by Wasserman and Sandelin (2004), a genome-wide scan with a typical PWM could incur in the order of 1000 false hits per functional binding site, which would make such an approach practically infeasible for accurate determination of binding sites.
The problem here lies not so much in the predicted binding patterns themselves, since many of these would readily be bound by transcription factors in vitro.
In vivo, however, most such binding sites would be non-functional, perhaps because the chromatin conformation around the sites precludes access to the DNA (Segal et al., 2006) or because the target factors require the cooperative binding of additional factors nearby to properly exert their regulatory function (Ravasi et al., 2010).
One way to improve accuracy in motif discovery is to try to narrow down the sequence search space as much as possible beforehand, for instance, by masking out portions of the sequences that resemble known repeats or considering only sequence regions that are conserved between related species (Duret and Bucher, 1997).
Kolbe et al.
(2004) introduced a measure they called Regulatory Potential which combines phylogenetic conservation with distinctive hexamer frequency profiles to identify possible regulatory regions.
This measure calculates a score for each position along the sequence, and regions receiving higher scores are deemed more likely to have a regulatory role.
Regulatory Potential can be considered as an example of a positional prior since each position is associated with an a priori probability of possessing some specific property.
Positional priors can be used as an aid in motif discovery by assigning high prior values to regions that we consider more likely to contain functional binding sites and then focus the search on these regions.
Besides conservation and oligonucleotide frequencies, other features that can be relevant for assigning prior values include: localized physical properties of the DNA double helix, distance from transcription start site or other binding sites, ChIP-chip and ChIP-seq data, and potentially tissue-specific epigenetic factors such as the presence of nucleosomes and associated histone modifications.
Many of the aforementioned features have previously been applied and shown to improve the performance of motif discovery by themselves (see e.g.
Bellora The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:32 30/7/2010 Bioinformatics-btq357.tex] Page: 2196 21952197 K.Klepper and F.Drabls Fig.1.
The top left panel in this screenshot shows examples of some of the features that can be used as a basis to create positional priors.
These features are visualized as data tracks in the main panel for a selected set of sequences.
The bottom-most track contains predicted matches to TRANSFAC and JASPAR motifs in regions with non-zero RegulatoryPotential7X scores.
et al., 2007; Segal et al., 2006; Whitington et al., 2009), and it has also been demonstrated that further gain can be achieved by integrating information about multiple features (see e.g.
Ernst et al., 2010; Lhdesmki et al., 2008).
We present here a program called PriorsEditor, which allows users to easily construct positional priors tracks by combining various types of information and utilize these priors to potentially improve the motif discovery process (Fig.1).
2 SOFTWARE DESCRIPTION The first step in constructing a priors track with PriorsEditor is to specify the genomic coordinates for a set of sequences one wishes to analyze.
Next, data for various features can be imported to annotate these genomic segments.
PriorsEditor supports three types of feature data.
The first type, numeric data, associates a numeric value with each position in the sequence and can be used to represent features such as phylogenetic conservation scores, DNA melting temperatures and nucleosome-positioning preferences.
Numeric data tracks are also used to hold the final positional priors.
The second feature type, region data, can be used to refer to continuous stretches of the DNA sequence that share some unifying properties which distinguish them from the surrounding sequence.
Different regions are allowed to overlap, and regions can also be assigned values for various attributes, including type designations, score values and strand orientations.
Features best represented as regions include genes, exons, repeat regions, CpG-islands and transcription factor binding sites.
The last feature type, DNA sequence data, represents the DNA sequence itself in single-letter code.
DNA sequence data can be passed on to motif discovery programs for further analysis, and it can also be used to estimate various physical properties of the DNA double helix, such as GC content, bendability and duplex-free energy.
Additional feature data can be obtained from web servers such as the UCSC Genome Browser (Rhead et al., 2010) or be loaded from local files.
Once the data for the desired features have been loaded, the data tracks can be manipulated, compared and combined to create a priors track using a selection of available operations.
These include operations to extend regions by a number of bases upstream and/or downstream, merge overlapping regions or regions within close proximity, filter out regions, normalize data tracks, smooth numeric data with sliding window functions, interpolate sparsely sampled 2196 [15:32 30/7/2010 Bioinformatics-btq357.tex] Page: 2197 21952197 PriorsEditor data, weight numeric data tracks by a constant value or position-wise by another track, combine several numeric tracks into one using either the sum or the minimum or maximum value of all the tracks at each position and several more.
It is also possible to specify conditions for the operations so that they are only applied to positions or regions that satisfy the condition.
For example, to design a priors track that will focus the search toward conserved regions within close proximity of other binding sites, one could start off with a phylogenetic conservation track, then load a track containing previously verified binding sites from the ORegAnno database (Griffith et al., 2008), extend these sites by a number of bases on either side and lower the prior values outside these extended sites.
After a priors track has been constructed, there are several ways to make use of this new data.
The most straightforward way is to provide it as input to a motif discovery program that supports such additional information, for instance, PRIORITY (Narlikar et al., 2006) or MEME version 4.2+ (Bailey et al., 2010).
Unfortunately, not many motif discovery programs are able to incorporate priors directly, so an alternative is to mask sequence regions that have low priors by replacing the original base letters with Xs or Ns since most motif discovery tools will simply ignore positions containing unknown bases when searching for motifs.
Apart from being used to narrow down the sequence search space, priors information can also be applied to post-process results after motif discovery has been carried out, for instance, by filtering out predicted binding sites that lie in areas with low priors or adjusting the prediction scores of these sites based on the priors they overlap.
Positional priors tracks and masked sequences can be exported for use with external tools, but it is also possible to perform motif discovery from within PriorsEditor itself by using operations to launch locally installed programs.
To facilitate motif scanning, PWM collections from TRANSFAC Public (Matys et al., 2006) and JASPAR (Portales-Casamar et al., 2010) have been included, and users can also import their own PWMs or define new collections based on subsets of the available PWMs.
Constructing priors tracks and performing motif discovery analyses can be tedious, especially when it involves many datasets and requires several steps to complete.
If a user discovers a good combination of features to use for priors, it may be desirable to repeat the same procedure to analyze other sequence sets as well.
PriorsEditor allows such repetitive tasks to be automatized through the use of protocol scripts.
Protocol scripts describe a list of operations to be performed along with any specific parameter settings that apply for these operations.
They can be programmed manually in a simple command language or be constructed using a macro recording function which logs all operations the user carries out while in recording mode.
With protocol scripts these same series of operations can be automatically applied to new sequence sets simply by the click of a button.
These scripts can also be set up so that users can provide values for certain settings during the course of an execution, enabling users to select for instance a different background model or PWM threshold value to use in the new analysis.
By providing a protocol script describing the operations to be performed along with a file specifying the target sequences, it is possible to run PriorsEditor from a command-line interface instead of starting up the normal graphical interface.
This allows the construction and use of positional priors to be incorporated into a batch-processing pipeline.
Funding: The National Programme for Research in Functional Genomics in Norway (FUGE) in The Research Council of Norway.
Conflict of Interest: none declared.
ABSTRACT Summary: Here, we present riboPicker, a robust framework for the rapid, automated identification and removal of ribosomal RNA sequences from metatranscriptomic datasets.
The results can be exported for subsequent analysis, and the databases used for the web-based version are updated on a regular basis.
riboPicker categorizes rRNA-like sequences and provides graphical visualizations and tabular outputs of ribosomal coverage, alignment results and taxonomic classifications.
Availability and implementation: This open-source application was implemented in Perl and can be used as stand-alone version or accessed online through a user-friendly web interface.
The source code, user help and additional information is available atContact: rschmied@sciences.sdsu.edu; redwards@cs.sdsu.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 5, 2011; revised on November 28, 2011; accepted on November 29, 2011 1 INTRODUCTION Metatranscriptomic approaches are drastically improving our understanding of metabolism and gene expression in microbial communities.
By investigating all functional mRNA transcripts isolated from an environmental sample, metatranscriptomic analyses provide insights into the metabolic pathways important for a community at the time of sampling.
Although metatranscriptomes are used to investigate metabolic activities, the majority of RNA recovered in metatranscriptomic studies is ribosomal RNA (rRNA), often exceeding 90% of the total reads (Stewart et al., 2010).
Even after various treatments prior to sequencing, the observed rRNA content decreases only slightly (He et al., 2010) and metatranscriptomes still contain significant amounts of rRNA.
Although rRNA-like sequences are occasionally removed from metatranscriptomes, the removal is performed only with a subset of the publicly available rRNA sequences.
Failure to remove all rRNA sequences can lead to misclassifications and erroneous conclusions during the downstream analysis.
It is estimated that misannotations of rRNA as proteins may cause up to 90% false positive matches of rRNA-like sequences in metatranscriptomic studies (Tripp et al., 2011).
The potential for false positives arrises To whom correspondence should be addressed.
from a failure to completely remove all rRNA prior to translating the putative rRNA and querying a protein database.
The rRNA operons in Bacteria and Archaea are not known to contain expressed protein coding regions that at the same time code for rRNA and therefore, annotations of proteins in rRNA coding regions should be presumed to be misannotations (Aziz et al., 2008).
Metagenomic sequence data generated to asses the metabolic potential of a community will also be affected by false positive matches of rRNA sequences when querying a protein database.
Therefore, transcript analysis should only proceed after it has been verified that all rRNA-like sequences have been found and removed from the dataset to allow accurate identification of the transcribed functional content.
The high-throughput nature of community sequencing efforts necessitates better tools for the automated preprocessing of sequence datasets.
Here, we describe an application able to provide graphical guidance and to perform identification, classification and removal of rRNA-like sequences on metatranscriptomic data.
The application incorporates a modified version of the BWA-SW program (http://bioinformatics.oxfordjournals.org/citmgr?gca =bioinfo;26/5/589), and is publicly available through a user-friendly web interface and as stand-alone version.
The web interface allows online analysis using rRNA sequences from public databases and provides data export for subsequent analysis.
2 METHODS 2.1 Implementation and computational platform The riboPicker application was implemented as stand-alone and web-based version in Perl.
The web application is currently running on a web server with Ubuntu Linux using an Apache HTTP server to support the web services.
The alignments are computed on a connected computing cluster with 10 working nodes (each with 8 CPUs and 16 GB RAM) running the Oracle Grid Engine version 6.2.
All graphics are generated using the Cairo graphics library (http://cairographics.org/).
2.2 Identification of rRNA-like sequences The identification of rRNA-like sequences is based on sequence alignments using a modified version of the BWA-SW program.
The modifications do not change the default behavior of the algorithm and include parameter forced changes in the alignment of ambiguous bases and the generation of an alternative output.
The documentation provides a detailed list of changes and is available on the program website.
riboPicker uses query sequence coverage, alignment identity and minimum alignment length thresholds to determine if an input sequence The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:50 31/12/2011 Bioinformatics-btr669.tex] Page: 434 433435 R.Schmieder et al.
is an rRNA-like sequence or not.
This approach is based on the idea that looking for similar regions consists of grouping sequences that share some minimum sequence similarity over a specified minimum length.
Threshold percentage values are rounded toward the lower integer and should not be set to 100% if errors are expected in the input sequences.
The results for multiple databases are automatically joined before generating any outputs.
Using simulated datasets, we evaluated the classification of rRNA-like sequences and showed that riboPicker performed with high accuracy comparable to the latest version of meta_rna (Huang et al., 2009) and BLASTn (Supplementary Material).
A comparison on real metatranscriptomic data showed that riboPicker processes data more than twice as fast as Hidden Markov Model (HMM)-based programs and >100 times faster than BLASTn (Supplementary Material).
2.3 Reference databases The web-based version offers preprocessed databases for 5S/5.8S,16S/18S and 23S/28S rRNAsequences from a variety of resources, currently including SILVA (Pruesse et al., 2007), RDP (Cole et al., 2009), Greengenes (DeSantis et al., 2006), Rfam (Gardner et al., 2011), NCBI (Sayers et al., 2011) and HMP DACC (The NIH HMP Working Group et al., 2009).
To reduce the number of possibly misannotated entries, sequences were filtered by length to remove very short and long sequences and by genomic location to remove overlapping rRNA misannotations.
The remaining sequences were then converted into DNA sequences (if required) and filtered for read duplicates to reduce redundancy in the sequence data.
Detailed information for each reference database is provided on the website.
Taxonomic information was either retrieved with the sequence data from the resources or was added based on the NCBI Taxonomy.
The databases are automatically updated on a regular basis and can be requested from the authors for offline analysis.
A non-redundant database is made available for the stand-alone version on the program website.
3 WEB-INTERFACE 3.1 Inputs The web interface allows the submission of compressed FASTA or FASTQ files to reduce the time of data upload.
Uploaded data can be shared or accessed at a later point using unique data identifiers.
It should be noted at this point that the input datasets should only contain quality-controlled, preprocessed sequences to ensure accurate results (Schmieder and Edwards, 2011).
In addition to the sequence data, the rRNA reference databases have to be selected from the list of available databases.
Unlike the stand-alone version, the web-based program allows the user to define threshold parameters based on the results after the data are processed.
This does not require an a priori knowledge of the best parameters for a given dataset and the parameter choice can be guided by the graphical visualizations.
3.2 Outputs Users can download the results in FASTA or FASTQ (if provided as input) format or its compressed version.
Results will be stored for the time selected by the user (either 1 day or 1 week), if not otherwise requested, on the web server using a unique identifier displayed during data processing and on the result page.
This identifier additionally allows users to share the result with other researchers.
The current implementation offers several graphical and tabular outputs in addition to the processed sequence data.
The Coverage versus Identity plot shows the number of matching reads for different coverage and identity threshold values.
The coverage plots show where the metatranscriptomic sequences aligned to the rRNA reference sequences and provide an easy way to check for possible bias in the alignment or the rRNA-removal prior to sequencing.
The coverage data for each database sequence is available for download.
The taxonomic classifications of rRNA-like sequences are presented as bar charts for each selected database.
The summary report includes information about the input data, selected databases and thresholds, and rRNA-like sequence classifications by database, domain and phyla.
4 BRIEF SURVEY OF ALTERNATIVE PROGRAMS There are different applications that can identify rRNA-like sequences in metatranscriptomic datasets.
The command line program meta_rna (Huang et al., 2009) is written in Python and identifies rRNA sequences based on HMMs using the HMMER package (Eddy, 2009).
Another program based on HMMER is rRNASelector (Lee et al., 2011), which is written in Java and can only be used through its graphical interface.
The web-based MG-RAST (Meyer et al., 2008) uses the BLASTn program, identifying rRNA-like sequences based on sequence similarity.
The HMM-based programs currently allow identification of bacterial and archaeal rRNAs.
The sequence similarity-based programs make it easy to assign sequences to taxonomic groups.
5 CONCLUSION riboPicker allows scientists to efficiently remove rRNA-like sequences from their metatranscriptomic datasets prior to downstream analysis.
The web interface is simple and user-friendly, and the stand-alone version allows offline analysis and integration into existing data processing pipelines.
The tool provides a computational resource able to handle the amount of data that next-generation sequencers are capable of generating and can place the process more within reach of the average research lab.
ACKNOWLEDGEMENT We thank Matthew Haynes and Ramy Aziz for comments and suggestions.
We thank the HMP DACC for making reference genomes data from the NIH Human Microbiome Project publicly available.
Funding: National Science Foundation Advances in Bioinformatics grant (DBI 0850356 to R.E.).
Conflict of Interest: none declared.
ABSTRACT Summary: Genomes undergo large structural changes that alter their organization.
The chromosomal regions affected by these rearrangements are called breakpoints, while those which have not been rearranged are called synteny blocks.
Lemaitre et al.
presented a new method to precisely delimit rearrangement breakpoints in a genome by comparison with the genome of a related species.
Receiving as input a list of one2one orthologous genes found in the genomes of two species, the method builds a set of reliable and non-overlapping synteny blocks and refines the regions that are not contained into them.
Through the alignment of each breakpoint sequence against its specific orthologous sequences in the other species, we can look for weak similarities inside the breakpoint, thus extending the synteny blocks and narrowing the breakpoints.
The identification of the narrowed breakpoints relies on a segmentation algorithm and is statistically assessed.
Here, we present the package Cassis that implements this method of precise detection of genomic rearrangement breakpoints.
Availability: Perl and R scripts are freely available for download at http://pbil.univ-lyon1.fr/software/Cassis/.
Documentation with methodological background, technical aspects, download and setup instructions, as well as examples of applications are available together with the package.
The package was tested on Linux and Mac OS environments and is distributed under the GNU GPL License.
Contact: Marie-France.Sagot@inria.fr Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 3, 2010; revised on May 10, 2010; accepted on June 3, 2010 1 INTRODUCTION Large scale modifications of the genome, such as inversions or transpositions of DNA segments, translocations between non-homologous chromosomes, fusions or fissions of chromosomes and deletions or duplications of small or large portions are called rearrangements.
They are further involved in evolution, speciation and also in cancer.
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First authors.
One crucial step before analysing the rearrangements and their possible relation with other genomic features is to locate these events on a genome.
In the case of two genomes, it is possible to identify conserved regions, also known as synteny blocks, by comparing the order and orientation of orthologous markers along their chromosome sequences.
A region located between two consecutive synteny blocks on one genome, whose orthologous blocks are rearranged in the other genome (not consecutive or not in the same relative orientations), is called breakpoint.
As far as we know, current methods for detecting breakpoints [Grimm-synteny (Pevzner and Tesler, 2003) Mauve (Darling et al., 2004), for example] are in fact strategies for detecting synteny blocks: they provide the coordinates of the breakpoint regions only as a byproduct, simply by returning regions that are not found in a conserved synteny.
Lemaitre et al.
(2008) developed a formal method that aims to go one step further and to extend the synteny blocks by focusing on the breakpoints themselves.
This method was shown to improve significantly the precision of breakpoint locations on mammalian genomes and enables to better characterize breakpoint sequences and distributions (Lemaitre et al., 2008, 2009) (see also datasets and comparisons available together with the package).
The first step of the method is to process a list of orthologous genes to identify synteny blocks between the genomes of two related species (a reference genome Gr and a second genome Go).
This step outputs a list of ordered and non-intersecting synteny blocks that are used to identify the breakpoints.
For each breakpoint on the genome Gr , we can define three sequences: the breakpoint sequence Sr , and its two orthologous sequences on the second genome Go, SoA and SoB (Fig.1).
In a second step, the method aligns the breakpoint sequence Sr against SoA and SoB and the information provided by the hits of the alignments is coded along Sr as a sequence of discrete values.
A segmentation algorithm calculates the best segmentation of this sequence of discrete values into at most three segments: a segment related with SoA, a segment related with SoB and a central segment which will represent the refined breakpoint.
2 CASSIS Cassis is a package which contains the implementation in Perl and R of the methods developed by Lemaitre et al.
(2008).
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[13:23 16/7/2010 Bioinformatics-btq301.tex] Page: 1898 18971898 C.Baudet et al.
Gr BrAr sequence SoA CoAo Go extended sequence SoA sequence Sr extended sequence Sr sequence SoB BoDo Go extended sequence SoB Fig.1.
Sequence Sr is defined by the boundaries of two consecutive synteny blocks Ar and Br on the genome Gr .
SoA (SoB) is defined by the boundaries of the orthologous block Ao (Bo) and of the previous/next synteny block (according to the orientation of the blocks) in the genome Go.
To perform the segmentation, the package considers the extended version of the sequences Sr , SoA and, SoB which includes the first/last genes of the synteny blocks.
The package receives as input data a list of pairs of one2one orthologous genes which can be found in the genomes Gr and Go.
First, all pairs of intersecting genes which have same order and direction in both genomes are merged.
Overlapping genes that do not respect this criterium are discarded.
After that, the list of genes is used to create synteny blocks according to the algorithm described by Lemaitre et al.
using k =2.
Basically, the parameter k controls for the flexibility degree of the method.
With k =2, the algorithm enables individual isolated genes to be out of order without disrupting a synteny block, and all synteny blocks must contain at least two genes.
For each breakpoint on the genome Gr , we define the boundaries of the sequences Sr , SoA and, SoB according to the synteny blocks.
We perform the alignment with LASTZ (Harris, 2007) of the sequences Sr against SoA and Sr against SoB.
LASTZ was chosen because it was shown to be more sensitive in the alignment of intergenic sequences.
To obtain better results in the segmentation step, we align the extended version of the sequences Sr , SoA and, SoB.
This includes the genes that are on the boundaries of the blocks that define the sequence (Fig.1).
If at least one of the alignments (Sr against SoA or Sr against SoB) leads to a hit, the breakpoint sequence can be refined.
The segmentation algorithm is applied to the breakpoint and the refined coordinates can thus be obtained.
During this step, we perform a statistical test that verifies if the breakpoint region is actually structured into three segments to validate the obtained results.
The package Cassis also works with lists of orthologous synteny blocks.
In this case, the steps of overlapping identification and synteny blocks definition are not executed and the input data is directly submitted to the breakpoint identification step.
As we do not have information about the genes that are inside of the synteny blocks that are given by the user, to build the extended sequences we add on each side of the sequence a fragment of length L. If the resulting extended sequence has length smaller than Lmin, it means that we have a considerable overlap between consecutive blocks.
Thus, we cannot properly define the sequence and the corresponding breakpoint is not refined.
The default values of the parameters L and Lmin are 50 kbp.
This was chosen because it is close to the average size of a gene.
The package contains a main script which controls the whole process of breakpoint identification and refinement.
The script is very simple to use and receives the following parameters: Input table: tab separated values file that contains the orthology information.
It can be a list of pairs of one2one ortologous genes or a list of pairs of orthologous synteny blocks, which can be found on the genomes Gr and Go; Input type: flag that indicates the type of the given input table: G for genes and B for synteny blocks; Directory Gr (Go): directory where the script can find the sequences of the chromosomes of the genome Gr (Go); Output directory: directory which will receive the results; and Other optional parameters including a stringency level for the LASTZ alignments and the values for sequences extensions (L and Lmin).
The script generates a table that contains, for each breakpoint, the chromosome of the genome Gr where the breakpoint is located, the coordinates of the breakpoint before and after the segmentation process and a flag that can have the following values: 1, 0 and 1.
The value 1 denotes that it was impossible to execute the segmentation because the alignments output no hit.
The values zero/one denote, respectively, that the segmentation failed/passed on the statistical test.
The package also produces, for each breakpoint, a plot with the graphical representation of the segmentation.
We recommend the use of chromosome sequences whose repeats have been masked.
The alignment of masked sequences results in more relevant hits and, consequently, on better segmentation results.
The package contains a main script which controls the execution of a set of scripts that performs atomic tasks.
The modularization of the implementation answers to the needs of advanced users who may desire to create their own pipelines of breakpoint refinement.
Funding: Coordenao de Aperfeioamento de Pessoal de Nvel Superior (4676/08-4 to C.B.
); Conselho Nacional de Desenvolvi-mento Cientfico e Tecnolgico (472504/2007-0, 479207/2007-0 and 483177/2009-1 to Z.D., partial); French project ANR (MIRI BLAN08-1335497); French-UK project ANR-BBSRC (MetNet4SysBio ANR-07-BSYS 003 02); Project ERC Advanced Grant Sisyphe.
Conflict of Interest: none declared.
Abstract Summary: Genome-wide proximity ligation assays, e.g.
Hi-C and its variant TCC, have recently be-come important tools to study spatial genome organization.
Removing biases from chromatin con-tact matrices generated by such techniques is a critical preprocessing step of subsequent analyses.
The continuing decline of sequencing costs has led to an ever-improving resolution of the Hi-C data, resulting in very large matrices of chromatin contacts.
Such large-size matrices, however, pose a great challenge on the memory usage and speed of its normalization.
Therefore, there is an urgent need for fast and memory-efficient methods for normalization of Hi-C data.
We developed Hi-Corrector, an easy-to-use, open source implementation of the Hi-C data normalization algorithm.
Its salient features are (i) scalabilitythe software is capable of normalizing Hi-C data of any size in reasonable times; (ii) memory efficiencythe sequential version can run on any single computer with very limited memory, no matter how little; (iii) fast speedthe parallel version can run very fast on multiple computing nodes with limited local memory.
Availability and implementation: The sequential version is implemented in ANSI C and can be easily compiled on any system; the parallel version is implemented in ANSI C with the MPI library (a standardized and portable parallel environment designed for solving large-scale scientific problems).
The package is freely available at http://zhoulab.usc.edu/Hi-Corrector/.
Contact: alber@usc.edu or xjzhou@usc.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 Introduction The recent development of genome-wide proximity ligation assays such as Hi-C (Lieberman-Aiden et al., 2009) and its variant TCC (Kalhor et al., 2012) has significantly facilitated the study of spatial genome organization.
The raw chromatin interaction data obtained by Hi-C methods can have both technical and biological biases (Imakaev et al., 2012).
Therefore, correcting biases in the Hi-C data is an important preprocessing step.
Among several recently de-veloped methods (Hu et al., 2012; Imakaev et al., 2012; Yaffe and Tanay, 2011), the iterative correction (abbreviated as IC) algorithm (Imakaev et al., 2012) has been used most widely by recent studies (Ay et al., 2014; Le et al., 2013; Naumova et al., 2013; Varoquaux et al., 2014) due to its conceptual simplicity, parameter-free algo-rithm and ability to account for unknown biases, although its as-sumption of the equal visibility across all loci may require further exploration.
Mathematically, the IC algorithm is a matrix scaling or balancing method that transforms a symmetric matrix into one that is doubly stochastic, meaning that the row and column sums of the matrix are equal to 1.
However, the Hi-C chromatin interaction ma-trix is of the massive size O(N2), where N is the number of genomic VC The Author 2014.
Published by Oxford University Press.
960 This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Bioinformatics, 31(6), 2015, 960962 doi: 10.1093/bioinformatics/btu747 Advance Access Publication Date: 12 November 2014 Applications Note regions.
Thus, it requires expensive computing resources such as large memory and long computation time.
This is especially prob-lematic for high-resolution data at the kilobase level or beyond (Jin et al., 2013; Le et al., 2013).
For example, at the resolution of 10K base pairs per region, the human genome has 303 640 regions and the matrix of the Hi-C data occupies about 343 GB of memory, which cannot be loaded into any common desktop computer even in the compressed format.
Most scaling algorithms in the matrix com-putation field (Knight, 2008; Knight and Ruiz, 2012) suffer from this scalability issue, because their main focus is improving the con-vergence rate and numerical stability.
Therefore, there is high de-mand for a fast and scalable IC algorithm that can work with massive Hi-C data matrices on common computing resources.
Here we propose a set of scalable algorithms (adapted from the original IC algorithm) to meet this need.
Both sequential and paral-lel versions were implemented in the standard and efficient C lan-guage, which allows for precise memory control.
The sequential implementation is memory efficient and can run on any single com-puter with limited memory, even for Hi-C datasets of large size.
It is designed to overcome the memory limit by loading a portion of data into the memory at each time, so requires some extra time for file reading.
The parallel implementation is both memory efficient and fast.
It can run on one of the most popular parallel computing re-sources: a computer cluster (i.e.
a distributed-memory computing environment).
In this environment, a set of general-purpose proces-sors or computers can be interconnected to share resources, and each computer retains its local and limited memory.
The parallel al-gorithm is designed with very low communication overhead among computing nodes, so that it runs faster on clusters with more com-puters.
Although the Hi-C analysis pipeline, ICE (Imakaev et al., 2012), implements the IC algorithm, it works only on a single com-puter and cannot utilize as many computing resources as possible to speed up the computation.
Very few parallel matrix scaling or balancing algorithms have been developed prior to this work (Amestoy et al., 2008; Zenios and Iu, 1990).
However, none of them are suitable for the bias correc-tion task of Hi-C data.
Zenios and Iu (1990) parallelized the matrix balancing algorithm in 1990 for a shared-memory computer, which cannot address the memory shortage problem.
Amestoy et al.
(2008) designed a complicated data distribution strategy based on the parti-tions of non-zero elements.
Their method is not applicable to the raw Hi-C contact map, which contains a high proportion of non-zero elements.
We performed experiments on high-performance computing resources and clusters with different numbers of nodes and memory capacities.
The results showed that this package could meet the strong demand for normalizing massive Hi-C data given limited computing resources.
2 Algorithms and implementation Given an observed chromosome contact frequency matrix O OijNN over N genomic regions, the IC method eliminates biases so that all genomic regions have equal visibility (Imakaev et al., 2012).
To make this algorithm memory-efficient, we designed a strategy of breaking the matrix O into K equal partitions of com-plete rows and loading only one partition into memory at any given time.
Therefore, the memory requirement can be very low when K is large.
This strategy adapts the IC algorithm by adding two steps: (i) loading the kth matrix partition Ok into memory and (ii) updating this partition with the last updated bias vector b.
The new Memory Efficient Sequential algorithm (called IC-MES) works even for the extreme case of KN, where only one row is loaded each time.
IC-MES is memory efficient, but it is still a sequential algorithm that runs on a single computing machine.
Therefore, it may be too slow when the machine has small memory.
To normalize large Hi-C matrices in a short time, we also designed a fast, scalable and Memory-Efficient Parallel algorithm (called IC-MEP) that can max-imally exploit the parallelism of the normalization problem and make use of many commonly available computing resources.
In es-sence, the normalization problem is a data divisible task: a series of operations that can independently work on separate partitions of the data.
This problem is perfectly suited to the data-parallel model in a distributed-memory computing environment such as a computer cluster, which consists of K independent processors (or nodes) that are loosely or tightly connected in high-speed networks and have limited local memory.
We employed the managerworker parallel programming paradigm.
The manager task partitions the data into K blocks, then initiates K worker tasks in different nodes; each worker task processes a single data block.
The manager coordinates all workers and synchronizes their calculations with updated bias vectors.
The IC-MEP algorithm has very little network messaging overhead, because no communication exists between workers.
Therefore, it is computationally efficient.
Furthermore, in order for each worker to run its task on limited memory, we also used the memory-saving strategy of the IC-MES algorithm.
That is, each worker further partitions its assigned data block into a set of sub-blocks and loads only one sub-block into memory at any given time.
Theoretically, the IC-MEP algorithm can work on any number of processors with any local memory capacity.
Details of these three al-gorithms and their flowchart figures are provided in the Supplementary materials.
We used ANSI C to implement the two se-quential algorithms IC and IC-MES, because of its maximum con-trol and memory efficiency.
We implemented the parallel algorithm IC-MEP using the popular message passing interface, which is a highly standardized and portable environment designed for solving large-scale scientific problems on distributed memory systems.
3 Results We compared three algorithms (IC, IC-MES and IC-MEP) on the TCC/Hi-C data of two human cell types: GM12878 and hESC (Dixon et al., 2012; Kalhor et al., 2012).
The whole genome is parti-tioned into the equal-size regions (or bins); the bin size is the main Table 1.
Running time of three algorithms on 10K and 20K bp reso-lution Hi-C data Algorithm IC IC-MES IC-MEP 20K bp data (151 825 bins) #Processor 1 1 16 48 Memory 86 GB 4 GB 1 GB 1 GB Time (gm12878) 0:36:50 3:58:14 0:19:50 0:6:38 Time (hESC) 0:35:01 3:49:18 0:19:48 0:6:47 10K bp data (303 640 bins) #Processor 1 1 16 48 Memory 343 GB 32 GB 2 GB 2 GB Time (gm12878) NA 47:27:32 4:50:03 0:26:02 Time (hESC) NA 37:26:15 4:49:27 0:26:09 All algorithms were terminated after 10 iterations for the purpose of perform-ance comparison, since each iteration has almost the same running time.
Memory includes only the memory allocated for computation in each pro-cessor, not system overhead.
The elapsed time format is hours : minutes : seconds.
Fast, scalable and memory-efficient normalization for Hi-C data 961 gigabytes--, ; Amestoy etal., 2008--(MPI) , ; Dixon etal., 2012 indicator of Hi-C data resolution.
The results are listed in Table 1.
In the experiment with 20K bp resolution data, the basic IC algo-rithm requires a minimum memory of 86 GB.
The algorithm IC-MES can run with just 4 GB memory (a common memory configur-ation in office computers) and complete the same work in reason-able time (within 4 h).
IC-MEP can dramatically speed up the computation using more processors (about 6 min with 48 proces-sors), while using only 1 GB of memory in each processor.
For the 10K bp data, none of HPC computer nodes (with 128 GB memory limit) can load the full matrix (about 343 GB) for the basic IC algo-rithm.
But IC-MES and IC-MEP can use 2 GB memory to quickly get the results (even in half hour using 48 processors).
Details are provided in the Supplemental materials.
4 Conclusion With the rapidly increasing resolution of Hi-C datasets, the size of the chromatin contact map will soon exceed the memory capacity of general computers.
We developed Hi-Corrector, a scalable and memory-efficient package for bias removal in HiC data.
Hi-Corrector can run on any single computer or a computer cluster with limited memory size to complete the task.
We performed ex-periments on high-resolution HiC data from two human cell types to show that the package can process very large data sets in reason-able time using the single processor, and in very short time with mul-tiple processors.
The experiments further demonstrate the scalability of our package with the observation shown in Table 1 that the more processors used, the faster it is.
Therefore, Hi-Corrector is a timely resource addressing the challenge of normalizing high-resolution Hi-C data.
Funding National Science Foundation Grant CAREER 1150287 and the Arnold and Mabel Beckman foundation (BYI program) (to F.A.
); and National Institutes of Health Grant (NHLBI MAPGEN U01HL108634 to X.J.Z.
); and F.A.
is a Pew Scholar in Biomedical Sciences, supported by the Pew Charitable Trusts.
Conflict of Interest: none declared.
Abstract Exosomes are 40100 nm nano-sized vesicles that are released from many cell types into the extracellular space.
Such vesicles are widely distributed in various body fluids.
Recently, mRNAs and microRNAs (miRNAs) have been identified in exosomes, which can be taken up by neighboring or distant cells and subsequently modulate recipient cells.
This suggests an active sort-ing mechanism of exosomal miRNAs, since the miRNA profiles of exosomes may differ from those of the parent cells.
Exosomal miRNAs play an important role in disease progression, and can stimu-late angiogenesis and facilitate metastasis in cancers.
In this review, we will introduce the origin and the trafficking of exosomes between cells, display current research on the sorting mechanism of exo-somal miRNAs, and briefly describe how exosomes and their miRNAs function in recipient cells.
Finally, we will discuss the potential applications of these miRNA-containing vesicles in clinical settings.
Introduction Exosomes, membrane-bound vesicles of 40100 nm in diame-ter, are present in almost all biological fluids [13].
They are released from most cell types into the extracellular space after fusion with the plasma membrane [13].
Lipids and proteins are the main components of exosome membranes, which are enriched with lipid rafts [13].
In addition to the proteins, vari-ous nucleic acids have recently been identified in the exosomal nces and 18 Genomics Proteomics Bioinformatics 13 (2015) 1724 lumen, including mRNAs, microRNAs (miRNAs), and other non-coding RNAs (ncRNAs) [4].
These exosomal RNAs can be taken up by neighboring cells or distant cells when exo-somes circulate, and they subsequently modulate recipient cells.
The discovery of their function in genetic exchange between cells has brought increasing attention to exosomes.
MicroRNAs are a class of 1724 nt small, noncoding RNAs, which mediate post-transcriptional gene silencing by binding to the 30-untranslated region (UTR) or open reading frame (ORF) region of target mRNAs [5].
The involvement of miRNAs in many biological activities has been well docu-mented, including cell proliferation, cell differentiation, cell migration, disease initiation, and disease progression [610].
Accumulating evidence has shown that miRNAs can stably exist in body fluids, including saliva [11,12], urine [13], breast milk [14], and blood [11,15,16].
In addition to being packed into exosomes or microvesicles, extracellular miRNAs can be loaded into high-density lipoprotein (HDL) [17,18], or bound by AGO2 protein outside of vesicles [16].
All these three modes of action protect miRNAs from degradation and guarantee their stability.
Given the transportability of vesicles, the role of miRNAs in exosomes is gaining increasing attention.
Conveying information via circulating vesicles is deemed to be the third way of intercellular communication that is as essential as the cell-to-cell contact-dependent signaling and sig-naling via transfer of soluble molecules [19,20].
Formation and secretion of exosomes require enzymes [21,22] and ATP [23], and the miRNA and mRNA profiles of exosomes differ from those of the parent cells [24].
Therefore, cells may possess an active selecting mechanism for exosomes and their cargos.
Besides, functions of the trans-ferred exosomal molecular constituents in the recipient cells are under investigation.
Hereby, this review will concisely introduce the origin and trafficking of exosomes and discuss the sorting mechanism and function of exosomal miRNAs.
Formation and secretion of exosomes Exosomes were first discovered by Pan and Johnstone in 1983 [25].
They reported that the release of transferrin receptors into the extracellular space during the maturation of sheep retic-ulocytes was associated with a type of small vesicle [23,25].
In 1989, Johnstone defined such functional vesicles as exo-somes [26].
To date, a series of extracellular vesicles have been described [27].
However, in the last three decades, no unified terminology for extracellular vesicles has been presented.
The definition for such extracellular vesicles named as microvesi-cles, exosomes, and microparticles remains confusing among different reports [2830].
Now, according to the way of vesicu-lar secretion from cells, extracellular vesicles can be grouped into two general classes.
One of these classes is known as microvesicles, which are directly shed from the cell membrane.
The other is known as exosomes, which are released by exo-cytosis when multivesicular bodies (MVBs) fuse with the plasma membrane [31].
Here, we mainly focus on the second group of vesicles, i.e., exosomes.
Exosomes can be revealed using transmission microscopy, possessing a cup-shaped morphology after negative staining [13].
These vesicles can be concentrated in the 1.101.21 g/ml section of a sucrose density gradient [13].
They can also be identified by the presence of proteins common to most exosomes, such as the tetraspanin proteins CD63, CD9, and CD81 [13].
As mentioned above, exosomes are originally formed by endocytosis.
First, the cell membrane is internalized to produce endosomes.
Subsequently, many small vesicles are formed inside the endosome by invaginating parts of the endosome membranes.
Such endosomes are called MVBs.
Finally, the MVBs fuse with the cell membrane and release the intralumi-nal endosomal vesicles into the extracellular space to become exosomes [32].
The regulatory molecules involved in the release of exo-somes were identified by Ostrowski and colleagues, who observed that Rab27a and Rab27b were associated with exo-some secretion.
Knockdown of Rab27 or their effectors, SYTL4 and EXPH5, could inhibit secretion of exosomes in HeLa cells [33].
Moreover, Yu et al.
discovered that both the tumor repressor protein p53 and its downstream effector TSAP6 could enhance exosome production [34].
Baietti et al.
found that syndecan-syntenin interacted directly with ALIX protein via Leu-Tyr-Pro-X(n)-Leu motif to support the intraluminal budding of endosomal membranes, which is an important step in exosome formation [35].
All of these studies indicate that a set of molecules act as a regulatory network and are responsible for the formation and secretion of exosomes in parent cells.
The trafficking of exosomes Exosomes present in body fluids play an important role in exchanging information between cells.
In general, there are three mechanisms of interaction between exosomes and their recipient cells.
First, the transmembrane proteins of exosomes directly interact with the signaling receptors of target cells [36].
Second, the exosomes fuse with the plasma membrane of recipient cells and deliver their content into the cytosol [37].
Third, the exosomes are internalized into the recipient cells and have two fates.
In one case, some engulfed exosomes may merge into endosomes and undergo transcytosis, which will move exosomes across the recipient cells and release them into neighboring cells.
In the other case, endosomes fused from engulfed exosomes will mature into lysosomes and undergo degradation [37,38].
Some recent studies have reported the fac-tors influencing internalization of exosomes in recipient cells.
Koumangoye et al.
observed that disruption of exosomal lipid rafts resulted in the inhibition of internalization of exosomes and that annexins, which are related to cell adhesion and growth, were essential for the uptake of exosomes in the breast carcinoma cell line BT-549 [39].
Escrevente et al.
described a decrease in exosome uptake after the ovarian carcinoma cell line SKOV3 and its derived exosomes were treated with pro-tease K, which indicated that the proteins mediating exosome internalization are presented on the surface of both the cells and the exosomes [40].
However, the detailed mechanism of exosome internalization is still not well understood.
The function of exosomes Exosomes can be released from many cell types, such as blood cells, endothelial cells, immunocytes, platelets, and smooth muscle cells [4143].
It is believed that exosomes can regulate Zhang J et al/ Trafficking, Sorting, and Function of Exosomes/miRNAs 19 the bioactivities of recipient cells by the transportation of lipids, proteins, and nucleic acids while circulating in the extra-cellular space.
Several reports have shown that exosomes play important roles in immune response, tumor progression, and neurodegenerative disorders.
Esther et al.
reported that acti-vated T cells could recruit dendritic cell (DC)-derived exo-somes that contain major histocompatibility complex (MHC) class II to down-regulate the immune response during interac-tion of T cells and DCs [44].
Exosomes derived from platelets that were treated with thrombin and collagen stimulated pro-liferation and increased chemoinvasion in the lung adenocarci-noma cell line A549 [45].
Exosomes derived from SGC7901 promoted the proliferation of SGC7901 and another gastric cancer cell line, BGC823 [46].
In addition, CD147-positive exo-somes derived from epithelial ovarian cancer cells promoted angiogenesis in endothelial cells in vitro [47].
Interestingly, Webber et al.
incubated exosomes derived from a mesothe-lioma cell line, a prostate cancer cell line, a bladder cancer cell line, a colorectal cancer cell line, and a breast cancer cell line with primary fibroblasts in vitro, and found that fibroblasts could be transformed into myofibroblasts [48].
A similar phe-nomenon was also observed by Cho et al., who described that tumor-derived exosomes converted mesenchymal stem cells within the stroma of the tumor tissue into cancer-associated myofibroblasts [49].
Although the function of exosomes has been documented in the aforementioned studies, it remains an open question which specific class of molecules contained in exosomes influences the recipient cells.
The sorting mechanism for exosomal miRNAs As described above, a wide variety of molecules are contained in exosomes, including proteins, lipids, DNAs, mRNAs, and miRNAs, which are recorded in the ExoCarta database [50].
Among these molecules, miRNAs have attracted most atten-tion, due to their regulatory roles in gene expression.
Goldie et al.
demonstrated that, among small RNAs, the proportion of miRNA is higher in exosomes than in their parent cells [51].
As some profiling studies have shown, miRNAs are not randomly incorporated into exosomes.
Guduric-Fuchs et al.
analyzed miRNA expression levels in a variety of cell lines and their respective derived exosomes, and found that a subset of miRNAs (e.g., miR-150, miR-142-3p, and miR-451) preferentially enter exosomes [52].
Similarly, Ohshima et al.
compared the expression levels of let-7 miRNA family mem-bers in exosomes derived from the gastric cancer cell line AZ-P7a with those from other cancer cell lines, including the lung cancer cell line SBC-3/DMS-35/NCI-H69, the colorectal cancer cell line SW480/SW620, and the stomach cancer cell line AZ-521.
As a result, they found that members of the let-7 miRNA family are abundant in exosomes derived from AZ-P7a, but are less abundant in exosomes derived from other cancer cells [53].
Moreover, some reports have shown that exo-somal miRNA expression levels are altered under different physiological conditions.
The level of miR-21 was lower in exosomes from the serum of healthy donors than those glioblastoma patients [29].
Levels of let-7f, miR-20b, and miR-30e-3p were lower in vesicles from the plasma of non-small-cell lung carcinoma patients than normal controls [30].
Different levels of eight exosomal miRNAs, including miR-21 and miR141, were also found between benign tumors and ovarian cancers [54].
All these studies show that parent cells possess a sorting mechanism that guides specific intracellular miRNAs to enter exosomes.
According to previous studies, there exists a class of miRNAs that are preferentially sorted into exosomes, such as miR-320 and miR-150.
Members of the miR-320 family are widely distributed in exosomes derived from normal tissue and tumors [29,41,52,55,56].
miR-150 is highly expressed in exosomes derived from the HEK293T cell line, peripheral blood of tumor patients, colony-stimulating factor 1 (CSF-1)-induced bone marrow-derived macrophages, and the serum of colon cancer patients [52,54,55,57,58].
In addition, some miRNAs, miR-451 for example, are highly expressed in exo-somes derived from normal cells, such as the HMC-1 cell line, the HEK293T cell line, primary T lymphocytes, and Epstein Barr virus-transformed lymphoblastoid B-cells [52,5961].
Other miRNAs, such as miR-214 and miR-155, are enriched in exosomes derived from tumor cell lines or peripheral blood from cancer patients [54,58,62].
Based on current research, there are four potential modes for sorting of miRNAs into exosomes, although the underlying mechanisms remain largely unclear.
These include: 1) The neu-ral sphingomyelinase 2 (nSMase2)-dependent pathway.
nSMase2 is the first molecule reported to be related to miRNA secretion into exosomes.
Kosaka et al.
found that overexpression of nSMase2 increased the number of exosomal miRNAs, and conversely inhibition of nSMase2 expression reduced the number of exosomal miRNAs [22].
2) The miRNA motif and sumoylated heterogeneous nuclear ribonu-cleoproteins (hnRNPs)-dependent pathway.
Villarroya-Beltri et al.
discovered that sumoylated hnRNPA2B1 could recog-nize the GGAG motif in the 30 portion of miRNA sequences and cause specific miRNAs to be packed into exosomes [59].
Similarly, another two hnRNP family proteins, hnRNPA1 and hnRNPC, can also bind to exosomal miRNAs, suggesting that they might be candidates for miRNA sorting as well.
However, no binding motifs have been identified yet [59].
3) The 30-end of the miRNA sequence-dependent pathway.
Koppers-Lalic et al.
discovered that the 30 ends of uridylated endogenous miRNAs were mainly presented in exosomes derived from B cells or urine, whereas the 30 ends of adenylated endogenous miRNAs were mainly presented in B cells [60].
The above two selection modes commonly indicate that the 30 portion or the 30 end of the miRNA sequence contains a critical sorting signal.
4) The miRNA induced silencing com-plex (miRISC)-related pathway.
It is well known that mature miRNAs can interact with assembly proteins to form a com-plex called miRISC.
The main components of miRISC include miRNA, miRNA-repressible mRNA, GW182, and AGO2.
The AGO2 protein in humans, which prefers to bind to U or A at the 50 end of miRNAs, plays an important role in mediat-ing mRNA:miRNA formation and the consequent transla-tional repression or degradation of the mRNA molecule [63].
Recent studies recognized a possible correlation between AGO2 and exosomal miRNA sorting.
In exosomal protein analyses, AGO2 has sometimes been identified by using mass spectrometry (MS) or Western blotting [51,64].
Guduric-Fuchs et al.
discovered that knockout of AGO2 could decrease the types or abundance of the preferentially-exported miRNAs, such as miR-451, miR-150, and miR-142-3p, in HEK293T-derived exosomes [52].
Other evidence has also sup-ported a relationship between miRISC and exosomal miRNA 20 Genomics Proteomics Bioinformatics 13 (2015) 1724 sorting.
First, the main components of miRISC were found to be co-localized with MVBs [65].
Second, blockage of the turn-over of MVBs into lysosomes could lead to the over-accumu-lation of miRISCs, whereas blockage of the formation of MVBs resulted in the loss of miRISCs [66].
Third, the changes in miRNA-repressible targets levels that occur in response to cell activation may cause miRNA sorting to exosomes, par-tially by differentially engaging them at the sites of miRNA activity (miRISCs) and exosome biogenesis (MVBs) [55].
In summary, specific sequences present in certain miRNAs may guide their incorporation into exosomes, whereas some enzymes or other proteins may control sorting of exosomal miRNAs as well, in a miRNA sequence-independent fashion (Figure 1).
The function of exosomal miRNAs Since Valadi et al.
[24] described that miRNAs could be trans-ferred between cells via exosomes, more similar observations AAA 7m G DNA AA pri-miRNA pre-miRNA pre-miRNA Drosha complex exp ort in5 Dicer complex Multivesicular body Nucleus nSMase2 Donor cell ?
(U > A) 2 3 1 4  hnRNP ?
Figure 1 The sorting mechanism for exosomal microRNAs In animals, microRNA (miRNA) genes are transcribed into primary m form precursor miRNAs (pre-miRNAs), which are exported into the digestion by the Dicer complex to become mature miRNAs.
Mature nSMase2-dependent pathway; (2) miRNA motif and sumoylated hnR recognizes the GGAG motif in the 30 portion of the miRNA sequenc miRNA sequence-dependent pathway; miRNAs that are preferentiall end.
(4) The miRISC-related pathway.
miRISCs co-localize with th components, such as AGO2 protein and miRNA-targeted mRNA, ar have been reported [6769].
The miRNAs in cell-released exo-somes can circulate with the associated vehicles to reach neigh-boring cells and distant cells.
After being delivered into acceptor cells, exosomal miRNAs play functional roles.
Although it is difficult to completely exclude the effects of other exosomal cargos on recipient cells, miRNAs are consid-ered the key functional elements.
The functions of exosomal miRNAs can be generally classified into two types.
One is the conventional function, i.e., miRNAs perform negative reg-ulation and confer characteristic changes in the expression levels of target genes.
For example, exosomal miR-105 released from the breast cancer cell lines MCF-10A and MDA-MB-231 reduced ZO-1 gene expression in endothelial cells and pro-moted metastases to the lung and brain [70].
Exosomal miR-214, derived from the human microvascular endothelial cell line HMEC-1, stimulated migration and angiogenesis in neigh-boring HMEC-1 cells [71].
Exosomal miR-92a, derived from K562 cells, significantly reduced the expression of integrin a5 in the human umbilical vein endothelial (HUVEC) cells and enhanced endothelial cell migration and tube formation [72].
A Ago2 GW182 GW182 Ago2 Recipient cell ?
?
iRNAs (pri-miRNAs), and processed by the Drosha complex to cytoplasm by the exportin5 complex.
The pre-miRNAs undergo miRNAs are sorted into exosomes via four potential modes: (1) NPs-dependent pathway; The sumoylated hnRNP family protein e and guides specific miRNAs to be packed into exosomes.
(3) 30 y sorted into exosomes have more poly(U) than poly(A) at the 30 e sites of exosome biogenesis (multivesicular bodies) and their e correlated with sorting of miRNAs into exosomes.
Zhang J et al/ Trafficking, Sorting, and Function of Exosomes/miRNAs 21 The other one is a novel function that has been identified in some miRNAs when they are studied as exosomal miRNAs rather than intracellular miRNAs.
Exosomal miR-21 and miR-29a, in addition to the classic role of targeting mRNA, were first discovered to have the capacity to act as ligands that bind to toll-like receptors (TLRs) and activate immune cells [73].
This study uncovered an entirely new function of miRNAs.
To further understand this novel function of miRNAs, more investigations are worthwhile.
Notably, current functional studies of exosomal miRNAs have some limitations.
First, diverse methods are used for exo-some isolation.
Exosomes can be enriched from cell culture media by ultracentrifugation, density gradient separation, immunoaffinity capture, size exclusion chromatography, and ExoQuick Precipitation (System Biosciences, USA).
Use of different exosome purification strategies could slightly affect exosomal contents, including proteins and miRNAs [7476].
Second, the large number of variable miRNAs carried by exo-somes may regulate many different signaling pathways, and will generate integral effects on recipient cells.
Therefore, it is difficult to gain a thorough understanding of the functions of exosomal miRNAs.
According to studies of miRNA sorting mechanisms, certain miRNAs may be classified by portions of their sequences, and the functions of each group may be eluci-dated separately.
Third, it is difficult to identify exosomal miRNAs in a single exosome or to measure the amount of a given miRNA carried by an exosome when it is present in low abundance.
Chevillet et al.
quantified the number of exo-somes by a NanoSight instrument (Malvern, UK) and the Table 1 Exosomal microRNAs capable of distinguishing different path Sample description Isolation strategy Tumor cells from glioblastoma patients at passage 115; serum from glioblastoma patients and controls Ultracentrifugation Plasma from NSCLC patients (n= 28 for test, n= 78 for validation); plasma from controls (n= 20 for test, n= 48 for validation) Immunobead (EpCAM) Serum from malignant tumor patients (n= 50); serum from benign tumor patients (n= 10); serum from controls (n= 10).
Immunobead (EpCAM) and ultracentrifugation Plasma from lung adenocarcinoma (n= 27); plasma from control (n= 9).
Size exclusion chromatography and immunobead (EpCAM) Note: NSCLC, non-small-cell lung carcinoma; EpCAM, epithelial cell adh number of miRNA molecules in an exosome collection using a real-time PCR-based absolute quantification method.
They found that, on average, most exosomes did not harbor many copies of miRNA molecule [77].
According to this study, accu-mulation of exosomal miRNAs in recipient cells is necessary for miRNA-based communication.
More sophisticated tech-niques and methods need to be developed to enrich the sub-population of miRNA-rich exosomes, and functionally sufficient quantities of exosomal miRNAs need to be determined.
Applications of exosomes and exosomal miRNAs Exosomal miRNAs can stably exist in the blood, urine, and other body fluids of patients, and exosomes can reflect their tis-sue or cell of origin by the presence of specific surface proteins [13].
Furthermore, the amount and composition of exosomal miRNAs differ between patients with disease and healthy individuals.
Thus, exosomal miRNAs show potential for use as noninvasive biomarkers to indicate disease states.
Several previous studies have profiled exosomal miRNAs in different samples.
It is of note that some exosomal miRNAs can be used to aid in clinical diagnosis (Table 1) [29,30,54,62].
For exam-ple, a set of exosomal miRNAs, including let-7a, miR-1229, miR-1246, miR-150, miR-21, miR-223, and miR-23a, can be used as the diagnostic biomarker of colorectal cancer [57].
Another set, miR-1290 and miR-375, can be used as the prog-nostic marker in castration-resistant prostate cancer [56].
ological conditions in patients Quantification method Findings Ref.
Quantitative PCR 11 miRNAs (miR-15b, miR-16, miR-196, miR-21, miR-26a, miR-27a, miR-92, miR-93, miR-320, miR-20, and let-7a) were known to be abundant in gliomas, able to be detected in their derived microvesicles; the level of exosomal miR-21 was elevated in serum microvesicles compared with controls [29] Quantitative PCR The levels of exosomal let-7f and/or miR-30e-3p in NSCLC patients can distinguish patients with resectable tumors from those with non-resectable tumors [30] Microarray The levels of 8 exosomal miRNAs (miR-21, miR141, miR-200a, miR-200b, miR-200c, miR-203, miR-205, and miR-214) from malignant tumor are significantly distinct from those observed in benign tumor; exosomal miRNAs could not be detected in normal controls [54] Microarray The levels of 12 exosomal miRNAs (miR-17-3p, miR-21, miR-106a, miR-146, miR155, miR-191, miR-192, miR-203, miR-205, miR-210, miR-212, and miR-214) are significantly different between patients and controls [62] esion molecule.
22 Genomics Proteomics Bioinformatics 13 (2015) 1724 Besides the endogenous miRNAs, exogenous miRNAs can also be sorted into exosomes, which has been experimentally confirmed by Pegtel et al.
[78] and Meckes et al.
[79], who observed that human tumor viruses can exploit exosomes as delivery vectors to transfer their exogenous miRNAs to other non-infected cells [78,79].
Hence, exogenous small RNAs have also been transferred by exosomes by mimicking the molecular mechanism of endogenous miRNAs transportation.
RNA interference (RNAi) has been applied to gene therapy [80,81].
The findings on the employment of exosomes by the exogenous miRNAs suggest that combination of exosomes with RNAi technology is a promising method for gene therapy and this idea has been supported by several lines of evidence.
For instance, Wahlgren et al.
used plasma exosomes as gene delivery platforms to transfer exogenous siRNAs to monocytes and lymphocytes, which resulted in the silencing of the target MAPK gene [82].
In addition, Shtam et al.
introduced exoge-nous siRNA into exosomes derived from HeLa cells, and used these transfected exosomes to knock down the target gene RAD51 in the recipient cells [83].
Moreover, the effect of exo-some-siRNA gene silencing has also been validated in a mouse model [84].
It is therefore possible to use exosomes to modulate target genes for therapeutic purposes, but a great deal of addi-tional research will be required to develop these therapies for clinical use.
Perspective Although exosomes were first identified in the 1980s, studies on exosomes have been increasing remarkably during the last five years, especially following the discovery of functional mRNAs and miRNAs in exosomes.
Exosomes play a key role in the process of cell-to-cell communication and influence the phenotype of recipient cells.
However, exosome study is still in its infancy.
People may want to know whether other non-coding RNAs such as long non coding RNAs could be present in exosomes, and whether they get involved in target gene reg-ulation in recipient cells.
With the discovery that exosomal miRNAs can function as ligands, a new field in exosome study has been opened up.
It remains controversial whether the mechanism for packing of bioactive molecules into exosomes and secreting them into the extracellular space is an active or a passive process.
More investigations on this matter will be warranted.
Nonetheless, the most exciting but challenging application will be to utilize exosomes and their cargo as a clinical tool to diagnose and monitor disease, perhaps even for gene therapy, but much work remains to achieve this goal.
Competing interests The authors declared that there are no competing interests.
Acknowledgments This work was supported by the Projects of International Cooperation and Exchanges from the National Natural Science Foundation of China (Grant No.
31161120358), the National Basic Research Program from the Ministry of Science and Technology of China (973 program; Grant Nos.
20111CB510106 and 2015CB910603), the Open Project of State Key Laboratory of Biomembrane and Membrane Biotechnology, and the Scientific Research Foundation for Returned Scholars from the Ministry of Education of China.
ML was supported by National Natural Science Foundation of China (Grant No.
31400741).
ABSTRACT Motivation: In the analysis of differential peptide peak intensities (i.e.
abundance measures), LC-MS analyses with poor quality peptide abundance data can bias downstream statistical analyses and hence the biological interpretation for an otherwise high-quality dataset.
Although considerable effort has been placed on assuring the quality of the peptide identification with respect to spectral processing, to date quality assessment of the subsequent peptide abundance data matrix has been limited to a subjective visual inspection of run-by-run correlation or individual peptide components.
Identifying statistical outliers is a critical step in the processing of proteomics data as many of the downstream statistical analyses [e.g.
analysis of variance (ANOVA)] rely upon accurate estimates of sample variance, and their results are influenced by extreme values.
Results: We describe a novel multivariate statistical strategy for the identification of LC-MS runs with extreme peptide abundance distributions.
Comparison with current method (run-by-run correlation) demonstrates a significantly better rate of identification of outlier runs by the multivariate strategy.
Simulation studies also suggest that this strategy significantly outperforms correlation alone in the identification of statistically extreme liquid chromatography-mass spectrometry (LC-MS) runs.
Availability: https://www.biopilot.org/docs/Software/RMD.php Contact: bj@pnl.gov Supplementary information: Supplementary material is available at Bioinformatics online.
Received on March 7, 2011; revised on June 27, 2011; accepted on July 29, 2011 1 INTRODUCTION The majority of statistical strategies to assess peptide/protein differential abundances from liquid chromatography-mass spectrometry (LC-MS) proteomic experiments are based on analysis of variance (ANOVA) methodologies applied to peak intensities (i.e.
abundance measures) of proteolytic peptides (Bukhman et al., 2008; Daly et al., 2008; Karpievitch et al., 2009; Oberg and Vitek, 2009; Oberg et al., 2008).
However, To whom correspondence should be addressed.
the ANOVA approach relies upon accurate estimates of sample variance, and proteomics studies not only have inherent variability associated with the biological samples, but potentially diverse process-based sources of variability.
That is, the accurate estimate of sample variances is often difficult to obtain.
For example, sample preparation protocols and instrument variations associated with the LC column (particularly important for multi-column platforms) and mass spectrometer can cause variations in peak intensities, as well as peptides identified across MS analyses within an experiment.
Data quality is especially important when the number of biological samples is small, often the case in proteomics experiments, and extreme values can negatively influence all subsequent data analysis outcomes.
Identification of statistical outliers in univariate data is an established but highly debated statistical topic (Barnett and Lewis, 1994; Hawkins, 1980).
There are many consecutive outlier procedures, focusing on one suspect value at a time, that have been proposed and implemented across many fields of application, such as Grubbs test and Dixons Q-test (Dixon, 1950; Grubbs, 1950).
Because these methods iteratively remove outlier points, the false positive rate (i.e.
Type 1 error) is inflated (Jain, 2010).
In contrast, recursive outlier detection procedures detect the presence of any number of outliers and control Type 1 errors.
For example, Jain (2010) presents a recursive version of Grubbs test and Caroni and Prescott (1992) derived a sequential application of Wilkss multivariate outlier test.
There are downfalls to these recursive procedures: (i) they are designed for univariate data, and if applied to multivariate data, will likely fail to detect statistically influential extreme values, and (ii) they are negatively affected by masking (i.e.
the inability to detect an outlier in the presence of another outlier) and swamping (i.e.
identify non-outliers as outliers) effects.
The identification of statistical outliers in multivariate data, such as microarray and proteomic data, is non-trivial.
The multiple dimensions of the data often subject outliers to masking (Filzmoser et al., 2008).
The microarray community, however, has made considerable progress in applying statistical metrics to assess the quality of microarray data (Kauffmann et al., 2009; Kemmeren et al., 2005; Lee et al., 2006; Wilson and Miller, 2005).
Of particular applicability to proteomics data are the ideas presented by Kauffmann et al.
They note that a poor quality array will impede the statistical and biological significance of the analysis due to the added noise.
This is also true for proteomics data.
That is, poor quality peptide abundance data will hinder downstream statistical analysis, including normalization, and subsequent biological interpretations.
The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:27 22/9/2011 Bioinformatics-btr479.tex] Page: 2867 28662872 Outlier discovery in peptide-centric proteomics data For proteomics data, a routine but non-probabilistic approach used for the identification of outlier LC-MS analyses (i.e.
runs) during data preprocessing is through a correlation matrix plot (Metz et al., 2008).
The sample correlation coefficient is calculated among technical replicates and biological replicates.
Those runs with a relatively low correlation are removed from the dataset.
The determination of low correlation is subjective, and varies across analysts, experiments and time.
Correlation may be examined via a heat map in which a color palette represents the numeric value, the color palette choice as well as the range of correlation values it covers can be highly subjective and extremely influential on the selection of which runs should be removed from the dataset.
In addition, the sample correlation coefficient can only be computed across peptides with common identifications between runs (i.e.
it does not account for missing data), it does not account for the multivariate nature of LC-MS runs, nor is there any statistical certainty associated with the exclusion of a run.
Advanced statistical approaches to outlier detection in proteomics data have focused either on the identification of outlier spectra maps (Rudnick et al., 2010; Schulz-Trieglaff et al., 2009) or on peptide/protein abundances independent of LC-MS run behavior (Cho et al., 2008; MacCoss et al., 2003; Xia et al., 2006).
Rudnick et al.
(2010) described a large set of metrics for the quantitative assessment of system performance and evaluation of technical variability among inter-and intra-laboratory LC-MS/MS proteomics experiments.
However, the use of these metrics to assess the quality of an individual LC-MS/MS run is not addressed.
Schulz-Trieglaff et al.
(2009) applied a multivariate method to perform a quality assessment of raw LC-MS maps using 20 quality descriptors.
The goal of their approach was to identify and remove outlier runs using unprocessed spectra before noise filtering, peak detection or centroiding was performed.
Cho et al.
(2008) presented a peptide outlier detection method using quantile regression to account for the heterogeneity of variance between replicate LC-MS/MS runs.
Peptide intensity ratios were plotted on an MA plot, where M is the difference in peptide abundance values and A is the average peptide intensity value.
MacCoss et al.
(2003) developed a correlation algorithm to detect outlier peptides using fractional changes between sample and reference intensities.
Xia et al.
(2006) proposed a two-stage method, combining Dixons Q-test and a median absolute deviation (MAD) modified z-score test, for outlier detection of peptide ratios.
These latter methods focus on assessing individual peptides for extreme behavior rather than the distribution of peptide abundance values for an entire LC-MS run.
Our goal is to statistically identify runs that exhibit extreme peptide abundance distribution properties, and thus will likely impact downstream statistical analyses.
Consequently, we are not focused on outliers specific to the spectral properties.
We describe a statistical strategy to identify and remove extreme LC-MS runs with a high level of statistical certainty, thus removing subjectivity from the filtering process.
The approach, based on a robust Mahalanobis distance (rMd), assesses the reproducibility of the distribution of peptide abundance values across replicate runs of the same biological sample as well across related biological samples.
Statistical methods, which limit the influence of extreme observations, are applied to obviate assumptions about underlying probabilistic models (Hoaglin et al., 2000).
We demonstrate the approach by applying it to simulated and real LC-MS datasets.
2 METHODS Our approach to detect and ascertain if an individual LC-MS run within an experiment, is a statistical outlier with a four-step process.
The algorithm was implemented in MATLAB (version 7.10.0.499, R2010a, The MathWorks Inc.: Natwick, MA, USA).
2.1 Summarize each LC-MS run as five metrics Five statistical metrics were chosen to describe the distribution of observed peptide abundance values in a single LC-MS run.
These metrics described below capture selected aspects of the peptide abundance distribution such as shape and scatter.
The location of each distribution is not directly considered since it could potentially be a false indicator of outlingness.
In addition, location can easily be corrected by a simple overall normalization factor.
The metrics are vectorized for each run, represented as x ; initially reducing the dimension of each run from p peptides to q metrics with the resulting dataset dimensionality of (nq) where n is the number of LC-MS runs.
2.1.1 Metric 1: correlation coefficient The sample correlation coefficient, rij , is calculated for peptide abundance values between all LC-MS runs (i=1,...,n; j=1,...,n) resulting in an nn matrix.
The correlation coefficient metric for the i-th run, Ri, which is used for the robust principal component analysis, is the average correlation within a common grouping (e.g.
treatment group, G), and has dimension (n1).
For the i-th run this is computed as, Ri = 1 NG(i) jG(i) rij (1) where NG(i) is the total number of runs in the group associated with run i.
The average correlation among biological replicates, rather than among technical replicates, is used due the small number of technical replicates, if any at all, observed in a typical LC-MS experiment.
2.1.2 Metric 2: fraction of missing peptide abundance data The fraction of missing abundance data in the i-th (1,...,n) LC-MS run is defined as, Fmi = p j=1 aij p (2) where aij =1 if the j-th peptide abundance is absent for the i-th run; otherwise, aij =0.
2.1.3 Metric 3: median absolute deviation of peptides within a LC-MS run The MAD (Hoaglin et al., 2000) is a robust measure of the spread of the data, and is used as an estimate of the sample standard deviation if scaled by a factor of 1.483.
The MAD of the i-th LC-MS run is defined as, MADi =med xj med(X)i (3) That is, within a run, each abundance value for peptide j is compared with the median peptide abundance values of the run i.
2.1.4 Metric 4: skew The asymmetry of a distribution is described by skew.
In our application to the i-th (1,...,n) LC-MS run, p is the number of peptides observed in the i-th run, xis the average peptide abundance value of all peptides observed in the i-th run and S is the sample standard deviation of the i-th run.
Skewi = 1 n n i=1 [ xi x S ]3 (4) 2.1.5 Metric 5: kurtosis The peakedness, or heavy-tailedness, of a distribution is described by kurtosis.
The same parameters are used as skew.
Kurtosis is calculated as, Kurtosisi = 1 n n i=1 [ xi x S ]4 3 (5) 2867 [15:27 22/9/2011 Bioinformatics-btr479.tex] Page: 2868 28662872 M.M.Matzke et al.
2.2 Obtain a robust estimate of the covariance matrix The purpose of robust principal component analysis (rPCA) in our method is to obtain the eigenvalues and eigenvectors to calculate a robust covariance matrix, which will be used in the calculation of the rMds.
We employ a rPCA algorithm developed by Croux et al.
that is based on the projection-pursuit approach to estimate the eigenvalues, and subsequent scores obtained from the projections of the metrics on the eigenvectors (Croux and Ruiz-Gazen, 2005; Li and Chen, 1985).
The robust covariance estimate is defined as, CSn = p k=1 Sn,kSn,k t Sn,k (6) for which Sn is the robust scale estimator used by the projection-pursuit index Sn,k is the k-th eigenvalue and Sn,k is the k-th eigenvector (Croux and Ruiz-Gazen, 2005).
The rPCA algorithm uses the L1-median value to center the data, and (MAD*1.483) as the robust scale estimate.
2.3 Identify outlier LC-MS run(s) using the rMd A widely accepted measure of distance in multivariate data is the Mahalanobis distance because it accounts for not only the average value, but also the covariance structure of the measured variables (Mahalanobis, 1936).
The distance of an individual LC-MS run from the center of the data is measured by a rMd.
For a q-dimensional multivariate vector x i for i=1,...,n, the rMd is defined as, DM (x)= (x i m)T C1Sn (x i m) (7) where CSn, a robust estimate of the covariance matrix, is obtained from the robust principal component analysis of the nq quality matrix, and mi is a vector of medians of the five metrics.
2.4 Statistical assessment of the rMds The rMd squared values associated with the peptide abundances vector (rMd-PAV) is the score used to assess whether an individual LC-MS run is an outlier.
The rMd-PAV scores are approximately chi-square distributed with q degrees of freedom (2q).
Therefore, outlier LC-MS runs are defined by a large rMd-PAV score such that the calculated squared distance exceeds a critical value of the 2q distribution specified a priori.
2.5 Proteomics data processing We present two independent real datasets to demonstrate the application of this outlier discovery strategy to LC-MS proteomics data.
Human cell culture samples were analyzed with an Exactive mass spectrometer (Thermo Electron Corp.), and mouse plasma samples were analyzed with an LTQ-Orbitrap mass spectrometer (Thermo Electron Corp.).
Nanoelectrospray ionization was used in the analysis of all samples.
Spectra were collected at 4002000 m/z with a resolution of 100 k and analyzed using the accurate mass and elution time (AMT) tag approach (Smith et al., 2002).
The mass de-isotoping process was performed using Decon2LS (Jaitly et al., 2009), and the matching process was performed using VIPER (Monroe et al., 2007).
Features from the LC-MS analyses were matched to AMT tags to identify peptides, using an initial tolerance of 3 p.p.m.
for mass and 2.5% for the LC normalized elution time (NET).
The human cell culture peptide datasets were further processed to remove peptides identified with low confidence, using the uniqueness filter Statistical Likelihood Confidence (SLiC) (Anderson et al., 2006) score of 0.35 and a DelSLiC of 0.2.
In circumstances where a peptide was identified in some LC-MS analyses, but not others, the missing data were coded as NaN.
All peptide abundance values were transformed to the log10 scale.
Minimum occurrence data filters were used to identify those peptides for which the amount of data present was not adequate for differential abundance analysis (Webb-Robertson et al., 2010).
The sample complexity of the sham controls (SCs) in each of the designed experiments is the same with respect to original biological material.
3 RESULTS Simulations of size 500 based on the p-variate standard normal distribution Np(0,I), and an empirically influenced p-variate normal distribution Np(,) were performed to examine a range of outlier configurations.
In addition, we assessed the performance of the multi-dimensional outlier detection method against the conventional method of using a Pearsons correlation coefficient [previously described in Section 2.1 as metric 1Equation (1)] to ascertain whether a LC-MS run is an outlier.
Simulation is useful to investigate the properties of rMd-PAV, however; since simulation of expected distribution parameters in real proteomics data is not well understood, these results are presented in Supplementary Material (Rocke et al., 2009).
The results of the multi-dimensional outlier detection analysis are displayed in a simple yet effective graphic in which rMd-PAV scores are plotted for each LC-MS run and compared with a reference line representing the 2 critical value.
For improved visualization, the rMd-PAV scores and the 2 critical value are transformed to the log 2 scale.
The red horizontal line represents the log2( 2 0.9999,5) critical value.
That is, at a significance level of 0.0001, a LC-MS run may be classified as a statistical outlier if the calculated test statistic 20.9999,5 critical value, or equivalently, the 2 P0.0001.
LC-MS runs with log2(rMd-PAV) scores above the red horizontal line are suspect and should be removed from the dataset.
3.1 Real data benchmarkexpert identified outlier runs Calu-3 cells, a human lung adenocarcinoma cell line, were infected with the severe acute respiratory syndrome coronona virus (SARS-CoV) at a multiplicity of infection of 5.
Cell monolayers were inoculated with SARS for 40 min at 37C, and sham-infected controls were inoculated with medium only.
Following inoculation, monolayers were rinsed and incubated for times 0, 3, 7, 12, 24, 30, 36 and 48 h. At the indicated times post-infection, wells were washed three times with ice cold 150 mM ammonium bicarbonate buffer and cells lyzed for 5 min in ice cold 8 M urea.
Samples were frozen at 80C until assayed.
Samples were analyzed in triplicate, except where noted in Supplementary Table S2, and the minimum occurrence filter returned a total of 26 776 peptides (Webb-Robertson et al., 2010).
This study included three biological replicates per time point as well as a large number of LC-MS runs (n=141), thus the removal of runs with poor quality abundance data is essential to maintain statistical power in downstream analyses.
An LC-MS expert at Pacific Northwest National Laboratory upon reviewing the chromatography maps for this study was able to designate 28 out of 141 (20%) LC-MS analyses as suspect due to various reasons (e.g.
electrospray instability, elution time, sample prep/collection problem).
We performed the rMd-PAV analysis, and compared its performance with t correlation alone to identify statistical outliers (runs at the peptide abundance level) via a receiver operating characteristic (ROC) curve analysis.
The rMd-PAV approach identified 12 out of the 28 expert-designated suspect runs as statistical outliers at the 0.0001 significance level (Fig.1a).
Electrospray issues represent almost half (13/28) of the expert identified runs, while the statistical algorithm identified three of these runs.
It is the most likely technical issue to occur and the most difficult to detect.
One reason 2868 [15:27 22/9/2011 Bioinformatics-btr479.tex] Page: 2869 28662872 Outlier discovery in peptide-centric proteomics data (a) (b) Fig.1.
Calu-3 cell-line experiment.
(a) The rMd-PAV plot of the LC-MS runs.
Runs identified as outliers (blue downward triangles) sit above the red horizontal line which represents the log2 ( 20.9999,5 ) critical value (i.e.
P=0.0001).
The empty upward triangles below the red horizontal line represent runs identified as suspect by the MS expert that were not identified as statistical extreme.
(b) The correlation plot of the LC-MS runs.
could be that the electrospray issue does not translate to a poor peptide abundance distribution, and thus an outlier.
The other 15 runs identified by the MS expert are due to elution time (5/28; 4/5 identified by algorithm), chromatography (3/28; 1/3 identified by algorithm) and sample prep/collection (7/28; 4/7 identified by algorithm).
LC-MS runs that were expert designated as suspect, but did not exhibit different peptide abundance distributions from those runs that were not designated as suspect are identified in Figure 2a as unfilled triangles.
Although the MS expert identified these runs as suspicious, the peptide abundance distributions are indistinguishable from those runs that were not designated as suspect.
In addition, we reviewed the sample correlation coefficient between all the study runs (Fig.1b).
Based on a subjective visual inspection of this graph, 6 out of the 28 expert-designated suspect LC-MS runs (#6, 25, 67, 78, 131 and 132) would have been dropped from the dataset.
The rMd-PAV scores identified six additional runs as statistical outliers.
This method did not identify any of the extreme runs due to electrospray issues; it did identify 3/5 runs labeled as suspect due to elution time, 1/3 suspect runs due to chromatography and 2/7 runs due to sample prep/collection issues.
A ROC analysis was completed to compare all levels of sensitivity and specificity.
A comparison of the ROC curves for the rMd-PAV scores and the correlation metric alone by a Wilcoxon signed Fig.2.
The ROC curves from the rMd-PAV and correlation alone outlier analyses of the calu-3 cell-line experiment.
2869 [15:27 22/9/2011 Bioinformatics-btr479.tex] Page: 2870 28662872 M.M.Matzke et al.
(a) (b) (c) Fig.3.
Cigarette smoke exposure experiment.
(a) Box plots of peptide abundance values observed in LC-MS runs (n=98) for the mouse plasma dataset.
The color indicates experimental group membership.
(b) The rMd-PAV plot of the LC-MS runs.
Those runs identified as outliers sit above the red horizontal line which represents the log2 ( 20.9999,5 ) critical value (i.e.
P0.0001).
The downward triangles represent outlier runsred represents all technical replicates from a biological sample, and blue represents individual technical replicates within a sample.
(c) The run-by-run (rij) correlation plot of the LC-MS runs.
rank test results in statistically significant differences between the curves in favor of rMd-PAV (P<0.0001, Fig.2).
Therefore, for this benchmark dataset we observe that rMd-PAV scores are superior to correlation alone for the identification of statistical outlier runs in LC-MS peptide abundance data.
3.2 Case studycigarette smoke exposure data Groups (N =8 biological replicates) of regular weight (RW) and diet-induced obese (OB) C57BL/6 mice (15 weeks old) were exposed to either filtered air (SCs), mainstream (MS) or side stream (SS) cigarette smoke by nose-only inhalation exposure for 5 h/day for 8 days.
Target cigarette smoke exposure concentrations were 250 g wet-weight total particulate matter (WTPM)/L of air for the MS exposures and 85 g WTPM/L for the SS exposures.
RW mice are defined as those mice fed a regular diet (PMI 5002 Rodent Diet, Richmond, IN, USA; 5kal% fat) throughout the study.
DIO mice were fed a high-calorie/high-fat diet (D12492 Rodent Diet, Research Diets Inc., New Brunswick, NJ, USA; 60kal% fat) starting at 6 weeks of age and continued throughout the study.
Immediately following the last exposure, each animal was removed from the exposure unit and anesthetized.
Blood was collected into tubes containing potassium ethylenediaminetetraacetic acid (EDTA) (Tyco Healthcare Group LP, Mansfield, MA, USA) and centrifuged to obtain plasma for analysis by LC-MS/MS.
Samples were analyzed in duplicate, except where noted in Supplementary Table S3 and a minimum occurrence filter returned a total of 3655 peptides (Webb-Robertson et al., 2010).
As in any data analysis problem, visual inspection of complex data before statistical analysis is vital.
Box plots are a simple and statistically robust techniques that are informative concerning distributional properties (e.g.
skew and kurtosis), and provide visual guidance when interpreting analysis results.
Peptide abundance data for each example has been displayed versus a LC-MS run order 2870 [15:27 22/9/2011 Bioinformatics-btr479.tex] Page: 2871 28662872 Outlier discovery in peptide-centric proteomics data Fig.4.
Cigarette smoke exposure experiment.
The score plot of the first two latent variables resulting from the rPCA of the data.
It suggests the runs labeled on the plot are outliers due to the fraction of missing peptide abundance values, and the skewness and kurtosis of the peptide abundance distribution within a run.
identification number (not true LC run order) using a box plot.
The box plot of the mouse plasma data (Fig.3a) shows a fair amount of variability from run to run making visual determination of statistical outlier runs difficult.
The rMd-PAV approach identified 6 out of the 98 LC-MS runs as statistical outliers at the 0.0001 confidence level (Fig.3b).
Singleton technical replicates were removed (run id #23 and 96), in addition to two complete biological samples (run id #11 and 12obese SC sample; run id #27 and 28obese MS inhalation sample).
Of the six runs identified as statistical outliers, it is unlikely any would have been removed using run-by-run correlation coefficient, rij , as the median correlation of all runs is 0.86 (Fig.3c), and ranging from 0.72 to 0.87 across the pool of identified outlier runs.
Using a more reflective score of correlation, Ri, which for the i-th run is the average correlation among the biological replicates within a group, the rMd-PAV identified runs would not have been removed from the dataset as the median correlation is 0.88 ranging from 0.73 to 0.85 across the identified outlier runs.
An additional benefit of the rPCA is the ability to explore the behavior of the metrics (e.g.
skew, kurtosis, fraction missing, etc.)
used to describe the peptide abundance distributions for the LC-MS runs within an experiment.
Explaining high-dimensional data in two or three latent variables (i.e.
principal components) is highly desirable.
With only a few latent variables, data can be graphically displayed and the key contributing attributes to the total explained variation is easily interpreted.
The relationship among the five metrics for peptide abundance data can be understood by examining the score plots of the latent variables.
In addition, the behavior of the outlier runs can be understood relative to the non-outlier runs (i.e.
average).
The most dominant manner in which these runs deviate is Kurtosis, Skew and Fraction Missing Data, as observed in the score plot associated with the rPCA (Fig.4).
The score plot is unique to an experiment, and thus is an excellent tool to further understand statistical differences in the peptides distributions among the LC-MS runs.
The first score plot to consider is a comparison of the first two rPCA components (i.e.
latent variables).
In combination they account for >88% of the total variation in the data, and suggest differences among kurtosis, skew and fraction of missing abundance data explain most of the variation in the data.
The plot shows the rMd-PAV identified runs located at the extreme ends of the observed data with respect to the first and second latent variables.
Using the angle between vectors as a visual guide, for this data, it can be deduced the Fraction Missing Data and Skew of the peptide abundance distribution are correlated.
In total, the first three components account for 95% of the variation observed in the data.
While a two-dimensional view of the data is helpful in understanding relationships among variables, outliers and non-outliers, it is the relationship among the data under the full dimensionality that is the basis for the evidence of outlier runs.
4 DISCUSSION Outlier detection in multivariate data is a non-trivial statistical task often subject to the masking effect (Filzmoser et al., 2008; Rocke and Woodruff, 1996).
Caution should always be taken when removing data from any dataset, large or small, and data should not be removed solely on the grounds of a statistical outlier test.
Rather, the results of any statistical outlier algorithm used should always be reviewed in the context of the research goal and the experiment.
Often the extreme data values are of interest and may explain technical difficulties in the process (e.g.
sample preparation issues, technical difficulties with instrumentation and a mislabeling of samples).
However, as with any statistical analysis and especially those dealing with small sample sizes, reviewing the outcome of the analysis is imperative.
Specifically, graphical methods allow the analyst to review the analysis in a stepwise manner.
For example, as our first step, we first plot the peptide abundances observed in the experiment for each LC-MS run using a box plot.
Then to understand how the abundance distributions vary across the LC-MS runs we examine the scores plot resulting from the robust PCA.
5 CONCLUSION We have presented a novel approach to the identification of statistical outliers in LC-MS proteomics peptide abundance data.
The value of the multivariate outlier discovery strategy utilizing rMd-PAV scores is the use of an objective probabilistic model to assess statistical certainty of the exclusion of runs within an experiment in the context of the complete dataset.
Proteomics has placed considerable effort on assuring the quality of the peptide identification with respect to spectral processing (Piening et al., 2006; Rudnick et al., 2010; Schulz-Trieglaff et al., 2009; Stead et al., 2008); however, quality assessment of the subsequent data matrix has focused on subjective visual inspection of run-by-run correlation, or individual peptide components.
The quality of the LC-MS peptide abundance data matrix is essential to the identification of robust biomarkers.
Moreover, statistical evaluation of the data relies upon tools often based on linear models, such as ANOVA which require accurate estimates of variance (Bukhman et al., 2008; Daly et al., 2008; Karpievitch et al., 2009; Oberg et al., 2008).
Without proper identification of statistical outlier runs the estimates of variance will be inflated, which may have a considerable effect on the identification of significant peptides and proteins.
2871 [15:27 22/9/2011 Bioinformatics-btr479.tex] Page: 2872 28662872 M.M.Matzke et al.
ACKNOWLEDGEMENTS Portions of this work were performed in the Environmental Molecular Science Laboratory, Office of Biological and Environmental Research, National Scientific User Facility at Pacific Northwest National Laboratory (PNNL) in Richland, WA, a U.S. Department of Energy (DOE).
Funding: This work was supported by the National Institutes of Health, the National Institute of General Medical Sciences (R01 GM084892 to B.-J.M.W.-R.); the National Institute of Environmental Health Sciences (U54 ES 016015 to J.G.P.
); and the National Institute of Allergy and Infectious Disease (HHSN272200800060C to K.M.W.).
PNNL is operated by Battelle Memorial Institute for the DOE under contract number DE-AC05-76RLO1830.
Conflicts of Interest: none declared.
Abstract Pluripotency-associated factors and their rivals, lineage specifiers, have long been consid-ered the determining factors for the identity of pluripotent and differentiated cells, respectively.
Therefore, factors that are employed for cellular reprogramming in order to induce pluripotency have been identified mainly from embryonic stem cell (ESC)-enriched and pluripotency-associated factors.
Recently, lineage specifiers have been identified to play important roles in orchestrating the process of restoring pluripotency.
In this review, we summarize the latest discoveries regarding cell fate conversion using pluripotency-associated factors and lineage specifiers.
We highlight the value of the seesaw model in defining cellular identity, opening up a novel scenario to consider pluri-potency and lineage specification.
Introduction Understanding how cellular identity is established is a major goal for modern biology.
The programming and reprogram-ming of cellular identity elicit tremendous scientific and public interest.
The groundbreaking work of Takahashi and Yama-naka established a precedent with the generation of induced pluripotent stem cells (iPSCs) by the forced expression of only (Deng H).
eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese A four transcription factors Oct4, Sox2, Klf4 and c-Myc [1].
Similar to embryonic stem cells (ESCs), iPSCs can proliferate and self-renew indefinitely under appropriate conditions and give rise to all types of cells in the body, which bestows these cells with many potential uses in regenerative medicine.
Pa-tients with degenerative diseases such as diabetes and cancer, along with aging individuals could all benefit from iPSC-based therapies [2].
The discovery of iPSCs Somatic cells can be reprogrammed by nuclear transfer [3] or by fusion with ESCs [4], suggesting that oocytes and ESCs contain factors that can reprogram somatic cells into stem cells.
Inspired by this discovery, Yamanaka and his colleagues selected 24 genes that are specifically expressed in ESCs, which cademy of Sciences and Genetics Society of China.
Production and hosting mailto:hongkui_deng@pku.edu.cn260 Genomics Proteomics Bioinformatics 11 (2013) 259263 also play important roles in the maintenance of ESC identity, as candidate factors to induce pluripotency in mouse somatic cells.
By transducing all 24 candidate genes together, G418-resistant colonies were generated by using Fbx15bgeo/bgeo as a selection marker for pluripotency.
These cells were further identified to possess ESC properties.
To determine which of the 24 candidates were essential, Yamanaka and his colleagues tested the effects of the withdrawal of individual factors from the 24-candidate gene pool on the generation of G418-resistant colonies.
Ultimately Oct4, Sox2, Klf4 and c-Myc were identi-fied to be pivotal for the induction of pluripotency in mouse somatic cells [1].
Furthermore, these factors were proven to be able to induce pluripotency in human somatic cells as well [5].
Other laboratories have also been working on inducing plu-ripotency in somatic cells.
By using Lin28 and Nanog together with Oct4 and Sox2, the Thomson laboratory also indepen-dently discovered a set of four reprogramming factors that are highly enriched in ESCs [6].
These first proof-of-principle studies opened the realms of iPSC research to a future in regenerative medicine.
Discovering novel pluripotency regulators for the induction of pluripotency It is coherent to test whether there are novel ESC-associated factors that can regulate iPSC induction.
Several reports have suggested factors that are important for the maintenance of ESC identity can also facilitate the induction of pluripotency.
For example, Nr5a2, an orphan nuclear receptor that is en-riched in ESCs, can replace Oct4 in the induction of pluripo-tency [7].
Esrrb, another orphan nuclear receptor that plays a pivotal role in the maintenance of pluripotency, can replace Klf4 and c-Myc [8].
PRDM14 and NFRKB were identified as novel determinants of human ESC identity and can substitute for Klf4 in reprogramming [9].
Recently, by a single-cell anal-ysis of the reprogramming process, Lin28, Sall4, Esrrb and Dppa2 were identified as a completely novel set of reprogram-ming factors [10], which are different from the factors initially identified by Yamanaka et al.
These factors are all important for the maintenance of ESC identity [10].
In addition to the highly expressed factors in ESCs, the maternal factor Glis1 in oocytes was reported to be a novel facilitator of reprogram-ming [11].
Direct reprogramming into iPSCs by lineage specifiers For years, it was generally believed that ESCs are maintained by a shield of pluripotency factors.
These factors function in concert with each other to prevent ESCs from differentiating into any lineage, thus preserving the ESCs at an undifferenti-ated state [12,13].
A more challenging perspective has been put forward recently.
Pluripotency factors might as well func-tion as classical lineage specifiers that direct ESCs to differen-tiate into a specific lineage and inhibit their commitment to mutually exclusive lineages [14].
Consistent with the notion, in ESCs, Oct4 promotes the differentiation of mesendoderm (ME) and primitive endoderm, while suppressing differentiation of the ectoderm (ECT) [1517]; Sox2 inhibits ME differentiation but promotes neural ECT differentiation [16,17].
Shu et al.
provided the first proof-of-principle report showing that modulating lineage-specifying forces can restore the pluripotency of mouse somatic cells [18].
When screening for factors that may substitute for Oct4 in the induction of pluripotency, Shu et al.
found that GATA3, which is known to regulate ME commitment and specification, can substitute for Oct4.
Subsequent analysis of other lineage specifiers that mainly function in ME differentiation and early embryonic patterning, which are generally not enriched in ESCs, found that GATA6, SOX7 and PAX1, among others, were also able to substitute for Oct4 to induce pluripotency, whereas ectodermal specifiers could not.
All Oct4 substitutes were also able to attenuate the upregulated expression of ECT-associated genes that is triggered by the expression of Sox2, Klf4 and c-Myc (SKM), whereas knockdown of the key ectodermal marker Dlx3 promoted SKM-only reprogram-ming [18].
These findings suggest that a novel function of Oct4/ GATA3 is to suppress ECT differentiation during reprogramming.
Accordingly, the ECT lineage specifiers SOX1, SOX3, RCOR2 and GMNN can replace Sox2 during reprogramming.
Similarly, Sox2 and its substitutes attenuate the expression of ME-specific markers induced by expression of Oct4, Klf4 and c-Myc (OKM) [1820].
Strikingly, co-expression of GATA6 and GMNN can substitute for Oct4 and Sox2 to reprogram mouse fibroblasts into iPSCs in the presence of Klf4 and c-Myc [18].
More recently, Montserrat et al.
showed that lineage speci-fiers can also be used to reprogram human fibroblasts into iPS-Cs.
The authors found that GATA3 can replace OCT4 and the ECT specifier, ZNF521, can replace SOX2.
Lastly, they showed that GATA3, together with ZNF521, OTX2 and PAX6, can substitute for both OCT4 and SOX2 for human iPSC induction in the presence of KLF4 and c-MYC [21].
A seesaw model for cell fate conversion A binodal model for cell fate determination, such as GATA1 and PU.1, RUNX2 and PPARc, has been examined in various instances of pluripotent stem or progenitor cells that assume a binary cell fate decision [22].
Such circuit hints at the concept of a balanced pluripotent state.
Inspired by these insights, Shu et al.
proposed a new model, termed the seesaw model, in which the pluripotent state has a precarious balancing equi-librium that results from continuous mutual competition be-tween rival lineage specification forces (Figure 1).
This model comprises two coupled modules the canonical pluripotency module and the lineage-antagonism module.
The former mod-ule is represented by the mutual activation of Oct4 and Sox2, whereas mutual inhibition of the ME and ECT genes repre-sents the latter module [18].
Both the canonical pluripotency module and the lineage-antagonism module are incorporated leading into the integrated seesaw model.
The novelty of the seesawmodel is the proper combination of the two modules.
This model led to unexpected insights and scenarios of cell fate conversion.
The activation of the cross-activating pluripotency module is important for the reestablishment of the pluripotency network to achieve successful reprogramming.
The self-activating pluripotency module gets activated when all of the lineage-specifying forces are Pluripotent stateLineage A inducer (e.g., ME inducer) Pluripotent state Pluripotent state Lineage A inducer (e.g., ME inducer) Lineage B inducer (e.g., ECT inducer) Lineage A state Lineage B state Pluripotent state Pluripotent state Pluripotent state Lineage B inducer (e.g., ECT inducer) Pluripotent state Lineage B inhibitor (e.g., ECT inhibitor) Lineage A inhibitor (e.g., ME inhibitor) Lineage A inhibitor (e.g., ME inhibitor) Lineage B inhibitor (e.g., ECT inhibitor) Figure 1 A seesaw model for cell fate conversion A modified diagram of the seesaw model [18].
Blue clouds indicate the regions that the cell states are likely to sample with noise.
The pluripotent state (red ball) is located near the balance region.
When the seesaw is balanced between the two differentiation potentials, the cell has a higher probability of entering the pluripotent state.
ME stands for mesendoderm and ECT stands for ectoderm.
Shu J and Deng H/ Lineage Specifiers in the Induction of Pluripotency 261 counteracted at the dynamic balance point of the seesaw.
In other words, no particular lineage-specifying activity is dominant in inhibiting the pluripotency module.
In this case, the pluripo-tent state becomes achievable, eliciting the Oct4 and Sox2 self-activating module to coordinate with other pluripotency factors, thus collaboratively restoring the pluripotency network.
Once the cross-activating pluripotency module is activated, the ME and ECT lineage fates are blocked by Sox2 and Oct4, respec-tively.
As a result, the pluripotent state is maintained [18].
This innovative model can illustrate the aforementioned points and predicts novel strategies for cell fate conversion, including strategies for the direct conversion of somatic cells into iPSCs by pluripotency factors or lineage specifiers along with the direct conversion of somatic cells into specific lineages by lineage specifiers or pluripotency factors.
Direct reprogramming into other cell types by lineage specifiers or pluripotency factors The direct reprogramming strategy for cell fate conversion has beenwidely adapted for some other cell types in addition to iPSCs.
The direct conversion of fibroblasts into myoblasts by over-expressing MyoD was reported in 1987 by Davis and colleagues [23].
Recently, increasing numbers of different cell types have been obtained by direct conversion, termed transdifferentiation.
For example, a combination of three neuronal lineage-specific tran-scription factors, Ascl1, Brn2 and Mytl1, is sufficient to induce neurons from fibroblasts [24].
The cardiac-specific transcription factors Gata4, Tbx5 and Mef2c can induce fibroblast transdiffer-entiation into cardiomyocytes [25].
It is therefore assumed that the more specific and closer the endogenous regulatory network is to the factors, the more efficient the conversion will be [26].
The seesaw model also predicts that inhibiting the mu-tual antagonistic lineage-specifying forces could convert one cell type into another.
Furthermore, consistent with the seesaw model, repro-gramming factors have been reported to directly produce line-age-committed cells.
Two pivotal pluripotency factors, Oct4 and Sox2, were reported to regulate ESC differentiation into different germ layers and to induce direct conversions between different cell types beyond iPSCs.
Previous studies have shown that a twofold increase in Oct4 expression induces ESCs toward mesendodermal specification [15], whereas high levels of Sox2 trigger the neuroectodermal commitment of ESCs.
Recently, it was reported that Oct4 and Sox2 also orchestrate a germ-layer fate selection.
Oct4 inhibits neuroectodermal differentiation and promotes mesen-dodermal differentiation, whereas Sox2 promotes neuroecto-dermal differentiation and inhibits mesendodermal differentiation [16,17].
More recently, overexpression of Oct4 in fibroblasts was shown to lead to transdifferentiation into hematopoietic cells of a mesendodermal lineage [27], whereas overexpression of Sox2 directly converted fibroblasts into neural stem cells.
Additionally, decreased expression of Oct4 among the four Yamanaka factors can result in the direct conversion of fibro-blasts into neural stem cells [2830].
These discoveries suggest that pluripotency factors, such as Oct4 and Sox2, can regulate not only pluripotency but also lineage specification.
Conclusion and outlook After the discovery of the famous Yamanaka factors, a set of transcription factors consisting of Oct4, Sox2, Klf4 and 262 Genomics Proteomics Bioinformatics 11 (2013) 259263 c-Myc, regenerative biology has stepped into a new era.
Increasing numbers of pluripotency-related factors have been identified either to replace the Yamanaka factors or to boost the process.
Meanwhile, direct transdifferentiation has been successfully demonstrated through a similar strategy, by using lineage-specific transcription factors for specifying each lineage fate.
Recently, discoveries and notions related to the seesaw model for cell fate conversion have introduced a novel scenario for cell fate conversion causing us to re-evaluate the characteristics of pluripotency factors and lineage specifiers, which are two rivals in the conventional conception of the development of cellular identity.
Increasing evidence suggests that pluripotency factors are also lineage specifiers.
For example, Oct4 specifies ME differentiation while inhibiting ECT differentiation, and induces hematopoi-etic transdifferentiation from fibroblasts.
Sox2 directs ECT differentiation while prohibiting ME commitment, and in-duces direct transdifferentiation from fibroblasts into neural stem cells.
Overexpression of Nanog, Esrrb, or Tbx3 pro-motes mesendodermal determination [14].
Lastly, the lineage specifiers depicted as pluripotency rivals, such as GATA3 and PAX6, have been identified to be able to restore pluripo-tency in somatic cells.
Based on these discoveries, we should reconsider the defini-tions of pluripotency and lineage specification and present a novel perspective for understanding the determinants of cellu-lar identity, which is one of the most important topics in mod-ern biology.
Competing interests The author has declared that no competing interests exist.
Acknowledgements This work was supported by the National Basic Research Pro-gram of China (973 Program, Grant No.
2012CB966401), the Key New Drug Creation and Manufacturing Program (Grant No.
2011ZX09102-010-03), the National Science and Technol-ogy Major Project (Grant No.
2013ZX10001003), the Ministry of Science and Technology (Grant No.
2011DFA30730 and 2013DFG30680) and 111 Project.
ABSTRACT Motivation: Advanced technologies are producing large-scale proteinprotein interaction data at an ever increasing pace.
A fundamental challenge in analyzing these data is the inference of protein machineries.
Previous methods for detecting protein complexes have been mainly based on analyzing binary protein protein interaction data, ignoring the more involved co-complex relations obtained from co-immunoprecipitation experiments.
Results: Here, we devise a novel framework for protein complex detection from co-immunoprecipitation data.
The framework aims at identifying sets of preys that significantly co-associate with the same set of baits.
In application to an array of datasets from yeast, our method identifies thousands of protein complexes.
Comparing these complexes to manually curated ones, we show that our method attains very high specificity and sensitivity levels (80%), outperforming current approaches for protein complex inference.
Availability: Supplementary information and the program are available at http://www.cs.tau.ac.il/roded/CODEC/main.html.
Contact: roded@post.tau.ac.il Supplementary information: Supplementary data are available at Bioinformatics online.
Received on December 17, 2008; revised on February 25, 2009; accepted on March 16, 2009 1 INTRODUCTION Procedures such as yeast two hybrid and co-immunoprecipitation (CoIP) (Mann et al., 2001) are routinely employed nowadays to detect new proteinprotein interactions, producing large-scale protein interaction networks for various species.
The networks provide a step stone for finding protein complexesthe fundamental units of macromolecular organization (Alberts, 1998).
The discovery of protein complexes based on yeast two hybrid data is a challenging task, since a protein complex may share common members with other complexes, and not all members of a certain protein complex directly interact with one another.
CoIP data, however, can be used for finding complexes by itself since co-immunoprecipitation experiments directly test complex co-membership: a bait protein is tagged and a purification of its complex co-members (prey proteins) is made followed by mass spectrometry.
To whom correspondence should be addressed.
Surprisingly, most methods for detecting protein complexes are based on treating protein interaction data as binary, i.e.
interactions are between pairs of proteins only.
This is commonly done by translating non-binary CoIP associations, of a bait to the set of preys obtained by tagging it, into binary interactions using the spoke model (Bader and Hogue, 2002), where a purification is translated into a set of pairwise interactions between the bait and each of the precipitated preys.
One of the most commonly used algorithms for this task is the Molecular Complex Detection (MCODE) algorithm by Bader and Hogue (2003).
MCODE detects densely connected components of the protein network.
It is based on weighing vertices by the density of their local neighborhoods.
Starting from a high weight vertex, a local expansion is done in a greedy fashion.
Another common clustering algorithm is the Markov clustering algorithm (MCL) (Enright et al., 2002).
MCL simulates random walks on the protein interactions network.
Random walks are performed iteratively; after sufficiently many iterations, the probability that a walk that starts in a dense area of the graph will end in the same dense area is high.
In order to magnify this effect, MCL applies, after each walk, an inflation step that separates high probability connections from low probability ones.
Eventually, the process converges and a cluster structure of the graph is formed.
MCL was shown to outperform other clustering algorithms for protein complex detection (Brohee and van Helden, 2006).
Recently, Rungsarityotin et al.
(2007) presented a new clustering method based on Markov random fields (MRF).
MRF applies a statistical model that assumes that the membership of each protein in a given cluster is only dependent on the membership status of its neighbors.
Finally, Friedel et al.
(2009) presented an unsupervised approach to find protein complexes that uses a bootstrapping mechanism to derive reliability scores for interactions between proteins.
The resulting weighted network is then clustered using MCL.
The only unsupervised approach we are aware of that uses CoIP data directly is that of Scholtens et al.
(2005).
This approach is called Local Modeling and is probabilistic in nature.
It relies on building a directed network of baitprey relationships and searching for subnetworks in which all protein pairs that were tested for a bait prey relation are connected.
Such fully connected subnetworks are shown to correspond well to protein complexes.
Supervised methods for identifying protein complexes have also been developed.
Gavin et al.
(2006) defined a socio-affinityscoring system that measures the log ratio of the number of times two proteins are seen together in CoIP purifications, relative to what would be expected from their frequency in the dataset.
These scores are used for clustering the proteins employing various The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 112 111117 G.Geva and R.Sharan clustering algorithms and parameters.
Result sets that exhibit poor correspondence to manually curated complexes are discarded.
Complex cores are identified as those stable parts of complexes that are not affected by the clustering algorithms/parameters.
Collins et al.
(2007) devised another scoring system for protein pairs, which combines the evidence in each purification for baitprey and preyprey relationships.
They applied hierarchical clustering to these scores to produce putative complexes.
Pu et al.
(2007) used the (Collins et al., 2007) scoring system together with the MCL algorithm to produce complex predictions.
Another scoring system was used by Hart et al.
(2007) in combination with the MCL algorithm to derive protein complexes.
Another scoring scheme was developed by Zhang et al.
(2008), who used a maximum clique finding algorithm to derive complex predictions.
In this article, we propose a novel method for inferring protein complexes from CoIP data, which we call CODEC (COmplex DEtection from Coimmunoprecipitation data).
We represent the data using a bipartite graph, where one set of vertices corresponds to the prey proteins, and the other one corresponds to the bait proteins.
Edges connect a bait to its associated preys.
Ideally, protein complexes should be manifested as fully connected bipartite subgraphs of this graph, as also argued in Scholtens et al.
(2005).
In practice, experimental noise results in false positive and false negative associations in the CoIP data.
In addition, for proteins that occur both as baits and preys in the data, we expect that if the bait (prey) instance is included in a complex, also its corresponding prey (bait) instance will be part of the complex.
Thus, a complex is expected to appear as a dense bipartite subgraph such that every participating protein has both its bait and prey instances present.
To identify those dense balanced bipartite subgraphs of the bait prey graph, we adapted the SAMBA biclustering algorithm (Tanay et al., 2002).
We applied CODEC to three datasets from three large-scale experiments in yeast (Gavin, 2002; Gavin et al., 2006; Krogan et al., 2006), identifying thousands of protein complexes.
We evaluated CODEC and compared it to extant approaches by using a collection of manually curated complexes from the MIPS (Mewes, 2002) and GO (Cherry et al., 1998) databases.
First, we compared CODEC to the three clustering approaches: MCODE, MCL and MRF.
We show that CODEC outperforms these approaches on two large-scale datasets, attaining higher values of specificity and sensitivity.
We did not include a comparison to the bootstrap method of Friedel et al.
(2009) as the software was not readily available.
Second, we show that CODEC can be useful even when supervised approaches are applicable, comparing it to two representative supervised approaches: those of Gavin et al.
(2006) and Collins et al.
(2007).
Remarkably, CODEC outperforms these approaches as well, even though they use curated information in the protein complex identification process.
Finally, we show that CODEC compares favorably to the Local Modeling approach (Scholtens et al., 2005), and at the same time it is much more scalable, allowing the analysis of much larger datasets.
2 METHODS 2.1 Data acquisition We downloaded CoIP data for three datasets: (i) Gavin et al.
(2006), which contains 1993 bait proteins, 2670 prey proteins and 19 277 baitprey relationships; (ii) Krogan et al.
(2006), which contains 2233 bait proteins, 5219 prey proteins [94 prey proteins were omitted from the raw data, since they were suspected as non-specific contaminants (Krogan et al., 2006)] and 40 623 baitprey relations; and (iii) Gavin (2002), which contains 455 bait proteins, 1364 prey proteins and 3413 baitprey relations.
MIPS complexes were obtained from the MIPS database (Mewes, 2002) (February 2007 download).
Only manually annotated complexes were used (category 550 was excluded).
From the 243 manually annotated MIPS complexes, we considered only complexes at level 3 or lower.
Higher level complexes were collapsed to level 3.
Overall, the data contained 229 complexes.
Gene ontology (GO) complexes were obtained from the Saccharomyces Genome Database (Cherry et al., 1998) (March 2007 download).
The GO dataset contained 193 complexes.
2.2 Graph construction and statistical data modeling We represent the CoIP data using a bipartite graph G= (U,V ,E), where vertices on one side (U) represent purifications with specific baits, and vertices on the other side (V ) represent the union of the set of preys detected in all the purifications and the set of baits.
For convenience, we name the vertices according to the proteins they represent.
Edges connect baits to their associated preys.
In addition, every purification with a bait u is connected to u on the prey side.
A candidate protein complex corresponds to a connected subgraph H = (U ,V ,E) of this graph, where V V is the set of member proteins in the complex, and U U is a set of purifications.
We use a likelihood ratio score to evaluate a candidate protein complex.
The score measures the fit of a subgraph to a protein complex model versus the chance that the subgraph arises at random.
The protein complex model assumes that each edge in the subgraph occurs with high probability pc, independently of all other vertex pairs.
This assumption ignores possible dependencies between baitprey associations, but allows computing candidate complex scores in an efficient manner.
The null model assumes that each edge (u,v) occurs with probability pu,v, independently of all other vertex pairs, where pu,v is the probability of observing an edge between u and v in a random bipartite graph with the same vertex degrees as G. In practice, we use pc =0.9 as recommended in Tanay et al.
(2002).
pu,v is approximated by d(u)d(v)|E| (Itzkovitz et al., 2003), where d(v) denotes the degree of a vertex v. Thus, the score of H is: L(H )= (u,v)E log pc pu,v + (u,v)E log 1pc 1pu,v By setting the weight of each edge (u,v) to be log pcpu,v >0 and the weight of each non-edge (u,v) to be log 1pc1pu,v <0, we have that the score of a subgraph is the sum of weights of its vertex pairs.
There are two exceptions to setting the edge weights: (i) an edge of the form (v,v) is assigned zero weight.
(ii) We call a vertex whose corresponding protein serves as a bait in some purification, but never detected as a prey, artificial.
For such a vertex, we consider two weighting schemes.
The first, which we call w0, sets all weights involving artificial vertices to 0, based on the assumption that these cases represent proteins that cannot be detected as preys due to experimental limitations.
The second scheme, which we call w1, treats such vertices the same as all other vertices, resulting in all the weights involving artificial vertices being non-positive.
2.3 The algorithm Our algorithm for protein complex identification employs a greedy search heuristic, which starts from high weight seeds and expands them using local search.
We describe these phases below.
2.3.1 Seed definition.
Recall that we seek heavy subgraphs of the bait prey graph with the additional requirement that these subgraphs are consistent, namely that a bait instance of a protein is included if and only if the prey instance of the same protein is included.
As seeds, we use complete bipartite subgraphs (bicliques) of the baitprey graph, augmented by additional vertices so that the consistency requirement is satisfied.
For a 112 [16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 113 111117 Identification of protein complexes prey vV , denote its corresponding bait (if such exists) by m(v).
Similarly, for a bait uU, denote its corresponding prey (which might be artificial) by m(u).
Then a prey subset S V with a set of common (bait) neighbors N(S) induces the following consistent seed: C(S)=SN(S){m(v) :vSN(S)} 2.3.2 Seed identification.
We start by identifying a high weight seed around each protein.
To find consistent seeds, we adapt the algorithm in Tanay et al.
(2002).
Basically, as shown in Tanay et al.
(2002), the heaviest biclique in a bipartite graph can be identified by an iterative algorithm.
At each iteration, the neighborhood of a vertex uU is scanned, and each subset of its neighbors is credited by the weight from u to the vertices of this subset.
After scanning all vertices in U, the subset that attained the highest weight induces the heaviest biclique.
In our case, we have a further consistency requirement.
Hence, we have to augment each of the possible seeds by appropriate vertices.
To this end, we add a post-processing step to the algorithm above which updates the weight of every subset according to the consistent seed it induces.
For computational efficiency, we limit the size of the scanned subsets to 24.
We only scan subsets that contain the prey vertex that corresponds to u.
Each candidate seed is scored by its log-likelihood ratio.
We retain the 500 000 highest scoring candidates and store them in a heap to prevent duplicates.
2.3.3 Greedy expansion.
This phase iteratively applies modifications to the seed so as to expand it and increase its weight.
Seeds are sorted by their log likelihood in a descending order.
The greedy expansion is applied to the seeds by that order.
At each iteration, all possible vertex additions to the seed and vertex deletions from the seed are considered, where baits are coupled to their corresponding preys to maintain consistency under these modifications.
The modification that improves the score the most is accepted.
This process continues until the score of the subgraph cannot be further improved.
For efficiency reasons, this phase is applied only to seeds that were not contained in previous expanded subgraphs.
2.3.4 Filtering the results.
We focus on clusters with at least three preys.
We evaluate the significance of a cluster by comparing the score of its corresponding subgraph to those obtained on randomized instances.
Specifically, we create random graphs with the same vertex degrees as G by using the MaslovSneppen procedure (Maslov and Sneppen, 2002).
The procedure switches a pair of edges (u,v) and (u,v) with (u,v) and (u,v), provided that the latter did not exist in the first place.
The switches are done 100m times, where m is the number of edges in the original graph (Milo et al., 2003).
Our algorithm is applied to these randomized instances to compute a null distribution of subgraph scores.
We use this distribution to compute a P-value for each of the clusters and retain only clusters whose P-value is smaller than a threshold.
To avoid redundant solutions, we filter putative protein complexes with high similarity to one another.
The similarity is measured based on the intersection of the prey sets of the compared clusters.
Specifically, for two putative complexes V1 and V2 we measure their similarity as |V1 V2|/min{|V1|,|V2|}.
If the similarity exceeds a predefined threshold, then the subgraph with the higher P-value is discarded.
We used 80% as the similarity filtering threshold [as in (Sharan et al., 2005)]; a lower value of 50% yielded a similar performance (see Supplementary Table S2).
2.3.5 Implementation and running time We implemented CODEC using the microsoft .net framework 2.0 and the C# programming language.
CODEC was applied to three datasets, as detailed above, on a Intel core 2 duo 1.86 GHz processor with 1 GB memory.
The running time ranged from minutes to hours, depending on the size of the dataset.
The running time of CODEC on the smallest (Gavin, 2002) dataset was 5 min; the application to the medium (Gavin et al., 2006) dataset took 3 h; finally, the run on the largest (Krogan et al., 2006) dataset lasted 30 h. 2.4 Quality assessment We assess the quality of the produced complexes by measuring their specificity and sensitivity with respect to a set of gold standard (known) complexes.
To this end, for each output cluster we find a known complex with which its intersection is the most significant according to a hypergeometric score.
The hypergeometric score is compared with those obtained for 10 000 random sets of proteins of the same size, and an empirical P-value is derived.
These P-values are further corrected for multiple hypothesis testing using the false discovery rate procedure (Benjamini and Hochberg, 1995).
We say that a cluster is a significant match to a complex if it has a corrected P-value lower than 0.05.
Let C be the group of clusters from the examined result set, excluding clusters that do not overlap any of the true complexes.
Let C C be the subset of clusters that significantly overlap a known complex.
The specificity of the result set is defined as |C|/|C |.
Let T be the set of true complexes, excluding complexes whose overlap with the examined dataset is less than 3 proteins and ensuring a maximum inter-complex overlap of 80%.
Let T T be the subset of true complexes with a significant match by a cluster.
The sensitivity of the result set is defined as |T|/|T |.
The F-measure is a measure combining the specificity and sensitivity measures.
It is defined as the harmonic average of these two measures: 2 specificitysensitivity specificity+sensitivity In addition, we also used the Accuracy measure suggested by Brohee and van Helden (2006).
This measure also evaluates the quality of complex predictions against a gold standard set.
The accuracy measure is the geometric mean of two other measures: positive predictive value (PPV) and sensitivity.
PPV measures how well a given cluster predicts its best matching complex.
Let Ti,j be the size of the intersection between the i-th annotated complex and the j-th complex prediction.
Denote PPVi,j = Ti,jn i=1 Ti,j = Ti,j Tj where n is the number of annotated complexes, and Tj is the sum of the sizes of all of cluster j intersection sizes.
The PPV of a single cluster j is defined as PPVj = nmax i=1 PPVi,j The general PPV of the complex prediction set is defined by PPV= m j=1 TjPPVjm j=1 Tj where m is the number of complex predictions.
The sensitivity measure used by Brohee et al.
(which is different from the one defined above) represents the coverage of a complex by its best-matching cluster.
Denote Sni,j = Ti,j Ni where Ni is the number of proteins in the annotated complex i. Complex-wise sensitivity is defined as Sni = mmax j=1 Sni,j The sensitivity of a complex set is defined as Sn= n i=1 NiSnin i=1 Ni The Accuracy measure can be influenced by small and insignificant intersections of a predicted complex and an annotated one.
For example, if a predicted complex intersects only one annotated complex, and the size of the intersection is 1, the PPV of that predicted complex will be 1.0.
Thus, we used a threshold to limit the effect of such small intersections, and evaluated the different solutions under varying thresholds ranging from 0 to 10.
For each such threshold t, all intersections of size at most t were not included in the Accuracy computation.
113 [16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 114 111117 G.Geva and R.Sharan 2.5 Parameter tuning The input for the MCL and MCODE clustering algorithms was the set of interactions resulting from connecting a bait protein to its preys [the spoke model (Bader and Hogue, 2002)] for each of the datasets.
For setting the parameters of the algorithms, we used the values recommended by Brohee and van Helden (2006).
Specifically, we used the inflation parameter 1.8 for MCL.
For MCODE, we used the parameters depth = 100, node score percentage = 0, Haircut = TRUE, Fluff = FALSE and percentage for complex fluffing = 0.2.
MRF was applied using the spoke model, using the parameters suggested by Rungsarityotin et al.
(2007), i.e.
K =698 and =3.5.
We obtained the Local Modeling implementation from the bioconductor http://www.bioconductor.org.
The parameters used to run Local Modeling are the default parameters mentioned in Scholtens et al.
(2005).
When creating complex estimates from Collins et al.
(2007), we used MCL with the same parameters as described above, and used the PE values as the input to the MCL algorithm.
3 RESULTS AND DISCUSSION 3.1 CODEC overview CODEC is based on reformulating the protein complex identification problem as that of finding significantly dense subgraphs in a bipartite graph.
We construct a bipartite graph whose vertices on one side represent prey proteins, and vertices on the other side represent bait proteins.
Edges connect a bait protein to its associated preys.
Ideally, a complex should appear as a fully connected bipartite subgraph (biclique) in this graph.
In practice, due to experimental noise, a complex will appear as a dense bipartite subgraph.
We note that further experimentation using methods such as cross-linking and sequential CoIP can improve the detection process, but is far more costly.
In addition, we impose a consistency requirement: some proteins occur in the data both as baits and as preys.
For such proteins, we require that if a certain prey (bait) vertex is included in the subgraph, so must be the corresponding bait (prey).
These definitions are exemplified in Figure 1.
The example dataset contains 10 proteins marked as P1-P10 (Fig.1a).
Four purifications are made.
The proteins used as baits are P3, P4, P5 and P7.
There are two sets of preys that are supported by more than one bait: {P2,P3,P4,P5} and {P5, P6, P7, P8}.
It can be hypothesized that these sets correspond to two protein complexes, shown in Figure 1b.
In both cases, the consistency requirement is satisfied.
The missing edge between P5 and P2 is a likely false negative, since both P3 and P4 interact with P2.
There may be additional complexes in this toy example, but there is only weak evidence for their existence since they are detected as preys by a single bait protein.
We adapted the SAMBA algorithm (Tanay et al., 2002) to find putative complexes, henceforth called clusters.
As further detailed in the Methods, the algorithm relies on a scoring component and a search heuristic to identify high scoring subgraphs.
The scoring of a subgraph is based on a likelihood ratio score, which measures the density of the subgraph versus the chance that its connections arise at random.
We experimented with two scoring variants: a permissive one, w0, and a stricter one, w1 (see Section 2).
In all the applications below, we report on the results of both variants.
The search heuristic starts from small bicliques and expands them using greedy search.
Unlike SAMBA, the search procedure also ensures that the consistency requirement is met by coupling together the prey and bait instances of a protein.
Fig.1.
An example data set.
(a) An input baitprey graph.
Baits are colored in blue and preys are colored in red.
(b) Two possible protein complexes and their corresponding subgraphs.
The significance of the identified clusters is evaluated by comparing their scores to those obtained on randomized instances, where the edges of the bipartite graph are shuffled while maintaining node degrees.
We retain only significant clusters and further eliminate redundant clusters with high overlap among them.
3.2 Application and evaluation As a first test of CODEC, we applied it to two recently published large-scale CoIP datasets in yeast.
The first dataset from Gavin et al.
(2006) contains 1993 bait proteins and 2670 prey proteins, and its edge density in the bipartite graph model is 0.006.
The second dataset from Krogan et al.
(2006) contains 2233 bait proteins and 5219 prey proteins, and its edge density in the bipartite graph model is 0.003.
This dataset has a much lower bait to prey ratio than the former one and, thus, serves as a different test case for our method.
CODEC was applied to the two original datasets; no proteins were filtered.
The application of CODEC to the first dataset using the w0 weighting scheme yielded clusters with 12 baits and 22 preys on average The average edge density within an output cluster was very high (0.65).
When using the stricter w1 scheme, a similar number of clusters was obtained, but the clusters were much smaller (4.5 baits and 13.5 preys on average).
The application of CODEC to the second dataset using the w0 weighing scheme produced clusters with 4 baits and 16 preys on average.
The average interaction density within the output clusters was high (0.54).
When using the w1 scheme, the number of clusters dropped by 3-fold although their sizes remained similar to the w0 case.
The size distributions of the obtained protein clusters in each of two applications are provided in Supplementary Table S1.
To assess the quality of our results, we measured their specificity and sensitivity with respect to a collection of manually curated complexes taken from the MIPS (Mewes, 2002) database (see Section 2).
Specificity is defined as the fraction of clusters that significantly overlap a known complex; sensitivity is defined as the fraction of known complexes that significantly overlap an identified cluster.
We computed receiver operating characteristic (ROC) curves for the two datasets, which plot the sensitivity and (1specificity) values over a range of P-value cutoffs for the output clusters (Figures 2 and 3).
In each plot, we chose the point that maximizes 114 [16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 115 111117 Identification of protein complexes Fig.2.
A comparison of protein complex identification approaches on the data of Gavin et al.
(2006).
For each method shown is the sensitivity of the output solution as a function of one minus its specificity.
For CODEC shown are two receiver operating characteristic (ROC) curves, corresponding to different weighting strategies (w0 and w1).
The evaluation is based on a comparison to known protein complexes from the MIPS database (Mewes, 2002).
The CODEC plots were smoothed using a cubic spline.
Fig.3.
A comparison of protein complex identification approaches on the data of Krogan et al.
(2006).
See legend of Figure 2 for details.
the sum of sensitivity and specificity (Coffin and Sukhatme, 1997) as the P-value cutoff for the output clusters.
The results attained are summarized in Table 1.
We compared CODEC to three clustering algorithms: MCODE, MCL and MRF (Table 1 and Figures 2 and 3).
On both datasets, CODEC outperformed MCODE and MCL, yielding significantly higher sensitivity values.
The cluster set provided by Rungsarityotin et al.
(2007) was computed by applying MRF using the spoke model to the Gavin et al.
(2006) dataset (the MRF results with the matrix model were inferior and, hence, were not used in the comparison).
CODEC and MRF achieved similar sensitivity scores, but at the same time CODEC attained significantly higher specificity.
Qualitatively similar results were obtained when evaluating the collections of protein complexes based on known complexes from the GO (Cherry et al., 1998) database (see Supplementary Figures S1 and S2).
When using an alternative evaluation measure the Accuracy measure suggested by Brohee and van Helden (2006)CODEC was again shown to outperform MCL and MCODE, while providing results that were only slightly better than those of MRF (see Supplementary Figures S3 and S4).
Notably, all the tested methods perform worse on the data of Krogan et al.
because of its low bait to prey ratio.
3.3 Comparison to extant CoIP-based approaches The results above demonstrate the utility of using CoIP data for protein complex identification.
Next, we compared CODEC to extant protein complex inference methods that use such data.
As a first test, we compared CODEC to two other methods that use CoIP data for scoring pairs of putatively interacting proteins.
The first (Gavin et al., 2006) computes cores of complexes based on socio-affinity scoring system that measures the log ratio of the number of times two proteins are seen together in CoIP purifications, relative to what would be expected from their frequency in the dataset.
The second (Collins et al., 2007) scores pairs of proteins using a purification enrichment (PE) score, which combines the evidence in each purification for baitprey and preyprey relationships.
We used these PE scores as input to the MCL algorithm [as suggested in Brohee and van Helden (2006)].
Importantly, both methods use manually curated information (known protein complexes from MIPS) to tune their parameters.
We conducted the comparison on the the Gavin et al.
(2006) dataset, for which we had the complex cores from Gavin et al.
(2006).
The results are summarized in Table 2 and depicted in Figure 2.
Notably, even though the methods of Gavin et al.
(2006) and Collins et al.
(2007) use prior biological information in the inference process, CODEC outperforms both, attaining higher sensitivity and specificity values.
The most pronounced difference is with respect to the specificity of Gavins cores (78% versus 51%).
Our final comparison was to the Local Modeling method (Scholtens et al., 2005).
The available implementation of the method could not run on the datasets of Gavin et al.
(2006) and Krogan et al.
(2006) due to their relatively large size.
Hence, we used a smaller data set as a test case (Gavin, 2002), containing 455 bait proteins and 1364 prey proteins.
The protein complexes inferred by Local Modeling are partitioned into three categories: complexes that are supported by multiple baits (marked as MBME), complexes that are supported by a single bait (marked as SMBH) and complexes that contain two baits where only one of the baits identifies the other bait as its prey.
We focused on the 272 MBME complexes, which represent the highest confidence predictions.
As can be seen in Table 3 and Figure 4, CODEC outperforms local modeling, attaining higher specificity and sensitivity.
When including in the Local Modeling solution also the SMBH complexes (336 in total) the sensitivity increased to 93%, at the price of a decrease in specificity 115 [16:42 16/12/2010 Bioinformatics-btq652.tex] Page: 116 111117 G.Geva and R.Sharan Table 1.
Comparison to MCODE, MCL and MRF Gavin et al.
(2006) Krogan et al.
(2006) Number of Specificity Sensitivity F-measure Number of Specificity Sensitivity F-measure Complexes (%) (%) (%) Complexes (%) (%) (%) CODEC using w0 1082 77.5 77 77 8348 30 76.2 43 CODEC using w1 1005 78.5 79 78.5 2973 46.5 72 56.5 MCODE 73 73.5 32 44.5 130 25 14 18 MCL 411 49.5 44.5 47 818 19.5 46 27.5 MRF 698 79.7 46.7 59   A comparison of CODEC, MCODE, MCL and MRF on the datasets Gavin et al.
(2006) and Krogan et al.
(2006).
The best result in each column appears in bold.
Table 2.
Comparison to Collins et al.
and Gavin et al.
2006 Number of Specificity Sensitivity F-measure Complexes (%) (%) (%) CODEC using w0 1082 77.5 77 77 CODEC using w1 1005 78.5 79 78.5 Gavin et al.
2006 480 51.5 70.5 59.5 Collins et al.
258 70 69.5 69.5 A comparison of CODEC and the methods of Collins et al.
and Gavin et al.
on the dataset of Gavin et al.
(2006).
The best result in each column appears in bold.
Table 3.
Comparison to Local Modeling Number of Specificity Sensitivity F-measure Complexes (%) (%) (%) CODEC using w0 185 80 85 82.5 CODEC using w1 180 79.5 81 80 Local Modeling 272 73 67 70 A comparison of CODEC to the Local Modeling approach on the dataset of Gavin (2002).
The best result in each column appears in bold.
(to 69%).
Overall, these results are comparable to those of CODEC, although providing a slightly worse F-measure (79% compared with CODECs 82.5%).
4 CONCLUSION We have provided a novel algorithm for identifying protein complexes from co-immunoprecipitation data, which is based on reformulating the problem as that of finding heavy subgraphs in a bipartite graph.
We have shown that our approach, which uses non-binary co-complex information, is superior to clustering methods that dissect binary proteinprotein interaction data.
Our algorithm was also shown to outperform existing approaches for inferring protein complexes from CoIP data.
All complex predictions made by CODEC can be found at http://www.cs.tau.ac.il/roded/CODEC/main.html.
An interesting open challenge is to combine yeast two-hybrid data into the inference process.
Such a combined approach is expected to become increasingly important as proteinprotein interaction databases continue to grow in size and species coverage.
Fig.4.
A comparison of protein complex identification approaches on the data of Gavin (2002).
See legend of Figure 2 for details.
Funding: Israel Science Foundation (grant no.
385/06) to R.S.
Conflict of Interest: none declared.
ABSTRACT Motivation: Most of the previous data mining studies based on the NCI-60 dataset, due to its intrinsic cell-based nature, can hardly provide insights into the molecular targets for screened compounds.
On the other hand, the abundant information of the compound target associations in PubChem can offer extensive experimental evidence of molecular targets for tested compounds.
Therefore, by taking advantages of the data from both public repositories, one may investigate the correlations between the bioactivity profiles of small molecules from the NCI-60 dataset (cellular level) and their patterns of interactions with relevant protein targets from PubChem (molecular level) simultaneously.
Results: We investigated a set of 37 small molecules by providing links among their bioactivity profiles, protein targets and chemical structures.
Hierarchical clustering of compounds was carried out based on their bioactivity profiles.
We found that compounds were clustered into groups with similar mode of actions, which strongly correlated with chemical structures.
Furthermore, we observed that compounds similar in bioactivity profiles also shared similar patterns of interactions with relevant protein targets, especially when chemical structures were related.
The current work presents a new strategy for combining and data mining the NCI-60 dataset and PubChem.
This analysis shows that bioactivity profile comparison can provide insights into the mode of actions at the molecular level, thus will facilitate the knowledge-based discovery of novel compounds with desired pharmacological properties.
Availability: The bioactivity profiling data and the target annotation information are publicly available in the PubChem BioAssay database (ftp://ftp.ncbi.nlm.nih.gov/pubchem/Bioassay/).
Contact: ywang@ncbi.nlm.nih.gov; bryant@ncbi.nlm.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.
Received on June 17, 2010; revised on September 2, 2010; accepted on September 22, 2010 1 INTRODUCTION Understanding the mechanism of interaction of small molecules with their macromolecular targets is critical for drug and chemical probe development.
The innovation of drug has long been recognized as time-consuming and labor-intensive, costing on To whom correspondence should be addressed.
average about $800 million as well as 1012 years to bring a new drug to market (DiMasi et al., 2003).
Apart from the challenges in optimizing pharmacokinetic properties and minimizing toxicities of lead compounds, the lack of publicly available/accessible biomedical assay data may represent another barrier for the success of drug discovery.
Fortunately, this is changing since more public resources are emerging, offering new opportunities to chemical biology researchers for drug development.
Open-access, information-rich resources include the Protein Data Bank (PDB; Berman et al., 2000), DrugBank (Wishart et al., 2006, 2008) and KEGG (Kanehisa et al., 2004), to name only a few.
Without a doubt, existing public resources, as well as new ones, will evolve in future with both speed and capacity.
PubChem is a public repository for the chemical structures of small molecules and information of their biological properties (Wang et al., 2009, 2010).
It was launched as a component of the NIH Molecular Libraries Roadmap Initiative (Zerhouni, 2003), with the aim to discover chemical probes via high-throughput screening (HTS) of small molecules.
It also receives biological property contributions from many other organizations.
As of March 17, 2010, PubChem contains more than 26 million unique compounds, among which over 870 000 have biological assay data for more than 3000 molecular targets, including proteins and genes.
The public accessibility to such assay data is particularly valuable to the community, since this kind of critical information needed by drug research is typically held by pharmaceutical companies.
The public availability and information-rich features altogether make PubChem an extremely valuable resource for biomedical research, as well as data mining studies (Chen and Wild, 2010; Han et al., 2008; Li et al., 2009; Rohrer and Baumann, 2009; Weis et al., 2008; Xie and Chen, 2008).
Launched by the National Cancer Institute (NCI), the Developmental Therapeutics Program (DTP) provides in vitro screening for new anticancer drugs that tested in 60 human tumor cancer cell lines (often known as the NCI-60 dataset; Shoemaker, 2006).
This well-curated, publicly available dataset has been recognized as a rich resource for studying the mechanism of growth inhibition for tumor cells (Shoemaker, 2006; Weinstein et al., 1997).
It has also inspired interests for developing and validating data mining tools (Paull et al., 1989; Zaharevitz et al., 2002).
Bioactivity profiles derived from the NCI-60 cell lines can provide insights into the mode of actions for tested compounds (Rabow et al., 2002; Shi et al., 1998a, b, 1999; Wallqvist et al., 2003; Weinstein, et al., 1992).
Structure-activity relationships (SARs) studies have also been reported for predicting or characterizing the cytotoxicity of the The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
ftp://ftp.ncbi.nlm.nih.gov/pubchem/Bioassay/[13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2882 28812888 T.Cheng et al.
screened compounds in the NCI-60 dataset (Guha, 2008; Lee et al., 2008; Wang et al., 2007).
However, due to the intrinsic cell-based nature of the NCI-60 dataset, most of the studies described above can hardly provide insights into the molecular targets for screened compounds.
On the other hand, PubChem has more than 1200 publicly available HTS bioassays with over 690 defined protein targets (as of March 17, 2010).
In addition, the screening laboratories under the NIH Molecular Libraries Program (MLP) share a common compound library, i.e.
the Molecular Libraries Small Molecule Repository (MLSMR), which is required to be tested for each assay project if possible.
As a result, the compounds in the MLSMR library are often tested in hundreds of bioassays with many of them having associated protein targets.
It thus represents a rich resource for constructing the compoundtarget interaction network, deriving target profiles and evaluating polypharmacological properties for a large library of compounds (Chen et al., 2009).
Moreover, there is a significant overlap between the MLSMR compound library and those screened in the NCI-60 cell lines.
Therefore, the bioassay data in PubChem can provide experimental evidence for the interactions between the compounds in the NCI-60 dataset and their targets.
In this work, we proposed a new strategy for combining and data mining the NCI-60 dataset and PubChem HTS assays, and investigated the correlations among the bioactivity profiles, compoundtarget interaction network and chemical structures of small molecules.
Bioactivity profiles were derived from the screening results contained in the NCI-60 dataset.
Compounds were hierarchically clustered based on their bioactivity profiles.
Compoundtarget interaction networks were constructed using the annotated bioassay data in PubChem.
Strong correlations were suggested between bioactivity profiles and target networks, especially when chemical structures were related.
2 METHODS 2.1 NCI-60 dataset The NCI-60 dataset is also available in the PubChem BioAssay database as 73 bioassays with the name of NCI human tumor cell line growth inhibition assay under the NCI/DTP data source.
In this study, 13 bioassays were eliminated considering their relatively small number of tested compounds (less than 16 000).
The screening data for the remaining 60 bioassays (will be referred to hereafter as the NCI-60) was downloaded from the PubChem FTP site (accessed on March 17, 2010).
In total, 5083 unique compounds were compiled and further filtered by the following rules: (1) Compounds must have been tested in all of the 60 NCI cell lines with a complete spectrum of log (GI50) values, where GI50 is the compound concentration required for 50% inhibition of tumor cell growth.
That is, any compound with missing log (GI50) value in one or more of the NCI-60 cell lines was discarded.
4452 compounds met with this criterion.
(2) Compounds must demonstrate activity in at least one PubChem bioassay which has a defined protein target.
This resulted in an initial set of 257 compounds with both complete bioactivity profiles and known protein targets.
(3) Compounds must have log (GI50) values below 6 for at least 15 out of the 60 NCI cell lines.
A final set of 37 compounds matched all of the above three criteria, and were analyzed in this study.
2.2 Clustering analysis based on bioactivity profiles End-point activity data from a single cell line may give only limited information on a compounds biological response.
However, the tested activities in a broad panel of 60 cell lines (i.e.
bioactivity profile) can be used to characterize the mechanism of drug action, resistance and modulation (van Osdol et al., 1994; Weinstein et al., 1992).
In this study, bioactivity profiles were subjected to hierarchical clustering by using the Hierarchical Clustering Explorer (HCE, version 3.5; Seo and Shneiderman, 2002), with the complete-linkage algorithm and Euclidean distance: dAB = 60 i=1 (Ai Bi)2 (1) where Ai and Bi are the log (GI50) values in the i-th NCI-60 cell line for the compound A and B, respectively.
2.3 Compoundtarget interaction network A compoundtarget interaction network can offer a direct view of the interactions between compounds and their protein targets.
The first step to construct such a network is to identify the protein targets for the compounds of interest.
The detailed target annotations in the PubChem BioAssay database (assay identifier: AID) made this step very straightforward.
For each of the 37 compounds in this study, the PubChem bioassays in which the compound was tested active (see each assay description for the definition of bioactivity outcome) were identified.
If the bioassay was specified with a protein target, then the target was assigned to the compound and included for network construction.
Note that a compound may be found active in several bioassays, so it is possible for a compound to have multiple targets associated with it.
In our network, the compound and target were denoted as two different nodes, respectively.
An edge was drawn to link a compound node (labeled by the PubChem compound identifier: CID) and a target node (labeled by the NCBI protein identifier: GI) if the compound is active against the target.
We applied the E-Utilities tool (http://eutils.ncbi.
nlm.nih.gov/entrez/query/static/eutils_help.html) to get the target GI for a respective bioassay.
For example, the following URL: http://eutils.ncbi.
nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pcassay&db=protein&cmd= neighbor&linkname=pcassay_protein_target&id=915 will return an XML file containing the GI of the protein target for the bioassay with AID 915.
An in-house script was used to extract GI from the resultant XML file.
To avoid the ambiguity in target specification, several PubChem bioassays that associate with multiple GIs were excluded from analysis.
The PubChem bioassays as well as the target information used for network construction are listed in Supplementary Table S1 based on the assay data in the PubChem BioAssay database as of March 17, 2010.
The compoundtarget interaction network was visualized by using the Cytoscape (version 2.3.6; Shannon et al., 2003).
3 RESULTS AND DISCUSSION 3.1 Hierarchical clustering analysis based on bioactivity profiles Hierarchical clustering was first carried out for the initial set of 257 compounds, which was obtained prior to the application of the third filter.
The dendrogram graph of the clustering result is given in Supplementary Figure S1.
The log (GI50) value of 6 was adopted as the bipartite cutoff to determine whether a compound is active (6) or inactive (>6) in a respective NCI-60 cell line.
This criterion for discriminating active compounds from inactive ones is consistent with that specified in the PubChem BioAssay database by the original NCI/DTP depositors, and as well as in other studies (Lee et al., 2008).
2882 [13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2883 28812888 Investigating the correlations among the chemical structures, bioactivity profiles and molecular targets of small molecules Fig.1.
Hierarchical clustering of the 37 compounds in the final set based on their bioactivity profiles in the NCI-60 cell lines.
The bioactivity profile of each compound is shown in spectrum (horizontal view).
A minimum similarity threshold of 0.88 (red solid line) is employed in HCE.
Six clusters that contain more than one compound are marked as A through F from top to bottom.
Relevant compounds (24 in total) are labeled with PubChem compound identifiers (CID).
As shown in Supplementary Figure S1, compounds that demonstrate similar bioactivity spectra were clustered.
However, a majority of compounds were clustered in proximity simply because they were inactive (shown in blue) in most of the NCI-60 cell lines.
Though inactive information is also important, it is less relevant to our study, which is to investigate the correlations among the bioactivity profiles, molecular targets and chemical structures of bioactive compounds.
Furthermore, previous studies have shown that the log (GI50) values in the NCI-60 dataset are skewed toward certain thresholds (Lee et al., 2008).
In our case, nearly 25% of the log(GI50) values for the initial set of 257 compounds were 4.
The reason is that the highest tested concentration in the NCI-60 cell lines is generally 4 (in log units), and if a compound is not sufficiently active to show 50% cell growth inhibition at this highest concentration, a upper bound of log(GI50) value of 4 is typically reported (Shi et al., 1998b).
Therefore, the bioactivity profiles that contain primarily skewed inactivity data may provide biased information.
To avoid such bias to certain extent, a third filter is needed to require every compound in the initial set to be active, i.e.
log(GI50)6, in at least 15 out of the 60 NCI cell lines.
This criterion can ensure, at least to a partial extent, that the derived similarity in bioactivity profiles result from biological activity rather than inactivity.
As a result, the initial compound set (257) was narrowed down to contain only 37 compounds.
With a much reduced dataset, we were able to investigate the SAR and compoundtarget interaction network for those compounds in greater details.
The hierarchical clustering for the 37 bioactive compounds was carried out by using exactly the same algorithm as applied to the initial compound set.
The results are shown in Figure 1.
The dendrogram graph indicates that compounds with biologically similar bioactivity were grouped together.
By setting a relatively tight cutoff of the minimum similarity of 0.88 in the HCE, clusters containing more than one compound were obtained and labeled as A through F from top to bottom.
The minimum similarity cutoff of 0.88 was chosen empirically based on visual exploration.
Interesting results obtained on these clusters are given below.
3.2 Highly similar structures with highly similar bioactivity profiles (Cluster B) Five compounds (CID: 24360, 97226, 72402, 354677 and 60699) were identified from cluster B.
Their 2D chemical structures are depicted in Figure 2A.
Compared to the compounds in other clusters, they gave the highest structural similarity as calculated by using the Tanimoto metric and PubChem fingerprint (ftp://ftp.
ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.
txt).
The average inter-compound structural similarity for these compounds was 0.887.
Therefore, it may not be surprising to see that they also exhibited similar biological responses in the NCI-60 cell lines (Fig.2B).
Indeed, similar bioactivity profiles were observed for these compounds, indicating strong and consistent inhibitory activity for a considerable number of cell lines.
The results from Figure 2A and B suggest that this group of compounds demonstrated strong SAR.
Further analysis on chemical structures shows that these compounds are the analogs of camptothecin, a selective inhibitor of the topoisomerases I (TOP1).
Among the five compounds, two (CID: 24360 and 60699) are well-known inhibitors of TOP1 (Pizzolato and Saltz, 2003; Wethington et al., 2008).
The former is camptothecin itself, while the latter has recently been approved by the FDA (trade name Hycamtin) in 2007 for oral use to treat ovarian cancer (http://en.wikipedia.org/wiki/Hycamtin, accessed on April 12, 2010), which is consistent with its activity in the ovarian cell lines (Fig.2B).
Considering the significant similarity in both chemical structures and bioactivity profiles, we proposed that the other three compounds (CID: 97226, 72402 and 354677) might be novel candidates of TOP1 inhibitors.
Nevertheless, one must always keep in mind that this may only be confirmed if the binding mechanism is understood.
In this case, the binding modes of the two known inhibitors (CID: 24360 and 60699) have already been previously clarified (Staker et al., 2002, 2005).
The X-ray crystal structures of the enzymeinhibitor complexes indicate that the oxygen atoms connected to the positions 10, 17, 20, 21 and 22 (Fig.2A) are critical for the binding process by forming several hydrogen bonding interactions directly or indirectly (through water salt bridges) with relevant residues on TOP1.
These key interacting sites are basically preserved in other three compounds (CID: 97 226, 72 402 and 354 677).
Therefore, it further suggests that these compounds might be true TOP1 inhibitors, as supported by previous studies (Ping et al., 2006; Rapisarda et al., 2002; Wethington et al., 2008).
As mentioned in the Section 1, PubChem can provide rich information of the compoundtarget associations for a number of tested compounds in the NCI-60 dataset.
By combining such data from both repositories, it is possible for us to characterize the bioactivity of tested compounds at cellular level and molecular level simultaneously.
The compoundtarget interaction network drawn from the available PubChem HTS bioassays for the five compounds in cluster B is shown in Figure 2C.
As one can see, these five compounds were closely packed by sharing some common or relevant protein targets.
Among the three compounds (CID: 24360, 354677 and 72402), the first compound shared four common protein targets (GI: 119579178, 134304838, 2883 ftp://ftp[13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2884 28812888 T.Cheng et al.
Fig.2.
The five camptothecin analogs identified from cluster B.
(A) 2D chemical structures, (B) bioactivity profiles in the NCI-60 cell lines on nine different organs and (C) compoundtarget interaction network (see Fig.4 for general description).
124263658 and 5174617) with the second compound.
In addition, it also shared three common protein targets (GI: 11545912, 25952111 and 5174617) with the third compound.
Moreover, these three compounds shared a common protein target (GI: 5174617).
This observation again demonstrates the effectiveness of the similarity principle.
While it remains to be further evaluated when sufficient data is available, we propose that compounds sharing significant similarity in both chemical structures and bioactivity profiles may have a higher chance for sharing similar patterns of interactions in the compoundtarget interaction network, comparing to those with only structural similarity.
The remaing two compounds (CID: 97226 and 60699) appear to be apart from the above three compounds in the common (or shared) compoundtarget interaction network.
This is mainly because they had not been tested on the same targets, against which the previous three compounds were tested, and thus gave insufficient information on their interactions with those relevant protein targets.
However, as indicated in Figure 2C, they had demonstrated similar activity to the compound CID: 24360 against several common protein targets in a pairwise manner, which may provide links to the other two compounds (CID: 354677 and 72402).
It should be mentioned that the abundant information of compoundtarget association in PubChem bioassays may also contain experimental noises such as promiscuous results.
While we cannot rule out the possibilities of the promiscuous effects or other artifacts in our analysis, we found that some of the compoundtarget associations were supported by previous studies.
For example, for the two compounds (CID: 24360 and 60699) in cluster B, they both exhibited activity against two common targets with one of them, hypoxia-inducible factor 1 (HIF-1, GI: 32879895), having been reported as the biological target for these two compounds (Klausmeyer et al., 2007; Rapisarda et al., 2002).
These investigations support that our findings result from experimental signals rather than noises.
It is noticeable that the compound CID: 354677 is also a known HIF-1 inhibitor (Rapisarda et al., 2002), though it had not been included in the compound target network due to insufficient data in PubChem (Fig.2C).
This again demonstrates that similarity in bioactivity profiles and patterns of interactions with relevant targets can be used to identify novel compounds for a certain target.
Nevertheless, further experiments will be needed to validate some of the potentially novel compounds identified by the MLP project.
3.3 Moderately similar structures with highly similar bioactivity profiles (Cluster F) Three compounds (CID: 2723601, 3246652 and 5351879) bearing partially structural similarity were identified from cluster F. Their 2D chemical structures and bioactivity profiles in the NCI-60 cell lines are given in Figure 3A and B, respectively.
These three compounds stood out from the rest because they exhibited the maximal intra-cluster similarity in their bioactivity profiles (Fig.3B).
In fact, cluster F was the first merged sub-tree during the hierarchical clustering process (Fig.1).
Compared to cluster B, the observation in cluster F may be even more interesting as the inter-compound structural similarities were significantly lower than those of cluster B.
For example, the two compounds (CID: 2723601 and 3246652), which produced the highest structural similarity (0.462) among this cluster, indicates only a moderate level of similarity in their chemical structures.
Another interesting observation seen from Figure 3B is that these three compounds show effective but still selective activity in the six leukemia cell lines, suggesting their potential 2884 [13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2885 28812888 Investigating the correlations among the chemical structures, bioactivity profiles and molecular targets of small molecules Fig.3.
The three compounds identified from cluster F. (A) 2D chemical structures, (B) bioactivity profiles in the NCI-60 cell lines on nine different organs and (C) compoundtarget interaction network (see Fig.4 for general description).
treatment to leukemia.
Indeed, one compound (CID: 2723601) is an approved small-molecule drug used in the therapy of several forms of leukemia (http://www.drugbank.ca/drugs/DB00352, last accessed date October 7, 2010).
The compoundtarget interaction network for the three compounds in cluster F is given in Figure 3C.
A single, common protein target (-globin, GI: 4504349) was shared by all three compounds, demonstrating again a strong correlation between the similarity in bioactivity profiles and that in the patterns of interactions with relevant protein targets.
According to the PubChem BioAssay database, the compound CID: 2723601 was tested active in the bioassay (AID: 910), while the other two compounds (CID: 3246652 and 5351879) were tested active in another bioassay (AID: 925).
Despite two separate bioassays, they were actually part of a series of assays in an attempt to seek for the modulators of hemoglobin-splicing (Supplementary Table S1).
A further analysis shows that though the overall structural similarity was relatively low, these three compounds possessed a common fragment of thioguanine, which may play a key role for the compounds to exhibit activity in modulating the hemoglobin splicing and some other biological processes.
This example suggests that similarity in bioactivity profiles derived from a broad panel of assays, together with the common features in chemical structures, can also indicate similarity in the mode of action for respective compounds, and can be used as a basis to determine information such as molecular targets or biological pathways for uncharacterized compounds.
3.4 Results for clusters A, C, D and E Unlike the compounds in clusters B and F, where strong correlations among chemical structures, bioactivity profiles and patterns of interactions with relevant protein targets can be observed, the compounds in the other four clusters did not fall in the same category for various reasons.
The three compounds in cluster A did not show significant similarity in either chemical structures or bioactivity profiles based on the data available (Supplementary Fig.S2), yet the results are still interesting despite the lack of overall coherence in the compoundtarget interaction network.
For example, two compounds (CID: 107985 and 253602), regardless of the difference in chemical scaffolds, were identified as showing inhibitory activities for the heat shock factor 1 (HSF1, GI: 62740231), which is in agreement with previous findings that both compounds are involved in the heat shock response pathway (Park and Liu, 2001; Westerheide et al., 2006).
Likewise, the four compounds in cluster E generally show low similarity in their bioactivity profiles (Supplementary Figure S3).
Moreover, structural similarity is also missing among compounds, making the current bioactivity profiles less useful in discovering novel compounds for the given targets.
Nevertheless, it remains interested to investigate the relationships among the compounds and their target networks in future when more bioassay data become available in PubChem.
The chemical structures and bioactivity profiles for the three compounds identified from cluster C are listed in Supplementary Figure S4.
These three compounds exhibited certain structural similarity by sharing a common fragment of di-ketone (Supplementary Figure S4A), which may be responsible for the notable similarity in their bioactivity profiles (Supplementary Figure S4B).
This observation resembled the results of cluster F, where compounds moderately similar in chemical structures with common fragment demonstrated significant similarity in bioactivity profiles.
As for the compoundtarget interaction network (Supplementary Figure S4C), however, only one compound (CID: 4212) in cluster C had been extensively assayed on multiple protein targets, against which the other two compounds were not tested.
Therefore, target networks cannot be compared directly due to lack of experimental support.
Nevertheless, for the compound CID: 548171, considering its notably high similarities to the compound 2885 [13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2886 28812888 T.Cheng et al.
Fig.4.
The complete diagram of the compoundtarget interaction network for the 24 compounds identified from the six clusters (i.e.
A to F) obtained by hierarchical clustering.
Compounds are denoted as ellipses, which are labeled with PubChem compound identifier (CID) and colored according to the clusters they belong to.
Targets are denoted as rectangles, which are labeled with NCBI protein identifier (GI) and colored with dark or light red if the corresponding assay is a confirmatory or primary bioassay in PubChem, respectively.
The edge linking an ellipse and a rectangle indicates that there is an interaction if the current compound is found active against the target of interest.
No edge is allowed between either two ellipses or two rectangles.
For simplicity, target nodes that have only single connecting compound node are not shown.
CID: 4212 in both chemical structures and bioactivity profiles (Supplementary Figure S4A and B), it may also interact with certain protein targets shared by the compound CID: 4212.
This hypothesis for predicting partially characterized compound according to well-characterized ones remains highly interested to be verified by future experiments.
As for the six compounds identified from cluster D, though there was no obvious similarity either in chemical structures or bioactivity profiles, they seemed to considerably show several common patterns of interactions with relevant protein targets (Supplementary Figure S5).
For example, four (CID: 262093, 5614, 221363 and 252101) out of the six compounds in cluster D were found active against several protein targets belonging to various cytochrome p450 families and/or subfamilies, suggesting that they may be effective in the p450-regulated pathways.
This observation suggests that the compoundtarget interaction network derived from PubChem bioassays may be useful to identify a set of related compounds involving in the same/similar biological pathway.
3.5 Overview of compoundtarget interaction network The compoundtarget interaction networks analyzed in the above sections were drawn for the compounds within the same hierarchical cluster of bioactivity profiles.
It remained highly interested to investigate how compounds can be organized solely by their patterns of interactions with relevant protein targets, and how that can be compared to the hierarchical clusters derived from the bioactivity profiles analysis.
To this end, a complete diagram of compound target interaction network, as shown in Figure 4, was built for all the compounds identified from the six clusters (i.e.
A to F) using the abundant information of compoundtarget association in PubChem bioassays.
In general, the network was rather complex and presented a great challenge for data analysis as a considerable number of compounds had demonstrated interactions against multiple protein targets.
Though these interactions remained to be further evaluated by identifying and excluding noises in the current assay data, the multitude of compoundtarget associations may reveal the promiscuous properties for certain compounds at the first glance and may facilitate the investigation of the polypharmacological properties of small molecules.
Despite the observed complexity, the compounds shown in Figure 4 can still be roughly grouped by their patterns of interactions with relevant protein targets.
For instance, the six compounds obtained from the bioactivity profile cluster D (colored in green) tended to pack into a group, which was well supported by the fact that there were so many common interacting targets shared by two or more compounds (Supplementary Figure S5).
Similarly, the three compounds from the bioactivity profile cluster F (colored in cyan) can also be identified as a group due to a commonly shared protein 2886 [13:01 19/10/2010 Bioinformatics-btq550.tex] Page: 2887 28812888 Investigating the correlations among the chemical structures, bioactivity profiles and molecular targets of small molecules target (GI: 4504349).
Therefore, it is interesting to observe that the groups of compounds identified from the target network were, to certain extent, consistent with those obtained by the clustering analysis based on bioactivity profiles.
This observation indicates that there could be strong correlations between a compounds bioactivity profile (cellular level) and its pattern of interactions with relevant protein targets (molecular level).
The compounds in the above two clusters exhibited much larger variances (i.e.
higher specificity) in their bioactivity profiles, which may contribute to their relatively converged patterns of interactions with relevant protein targets.
In contrast, some compounds presented in bioactivity profile cluster A (colored in yellow) showed generalized toxicity with low selectivity and specificity (Supplementary Figure S2B), making them difficult to be identified as a group from Figure 4.
This analysis was done using a binary bioactivity outcome when considering the compound target association.
Further analysis may be performed in future work by incorporating the quantitative potency data (e.g.
IC50) of each compound to provide more insights.
4 CONCLUSIONS By taking advantages of the publicly available data from both PubChem HTS bioassays and NCI-60 human tumor cancer cell line screens, we have investigated the correlations among the bioactivity profiles, molecular targets and chemical structures of small molecules.
Hierarchical clustering of tested compounds was carried out based on their bioactivity profiles derived from the NCI-60 cell line screens, and several interesting clusters were identified.
First, the correlation between bioactivity profiles and chemical structures was analyzed and strong SAR was suggested.
For example, the compounds in cluster B, which were highly similar in chemical structures, also demonstrated notable similarity in their bioactivity profiles.
Even more interesting observations were given by cluster F, where compounds were only moderately similar in chemical structures and produced extremely significant similarity in bioactivity profiles.
Second, analysis on the compoundtarget interaction network was performed and showed clear correlations between the bioactivity profiles of compounds and their patterns of interactions with relevant protein targets, especially when chemical structures were related.
Furthermore, a complete compoundtarget interaction network, which was drawn for all the compounds identified from the six clusters, produced roughly the same groups of compounds as that obtained by hierarchical clustering analysis based on bioactivity profiles.
This study shows that strong correlations can be observed between similarity in bioactivity profiles (cellular level) and that from the patterns of interactions with relevant protein targets (molecular level), and suggests that novel compound candidates with desired pharmacological properties can be identified by comparing their bioactivity profiles and/or compoundtarget interaction network to well-characterized compounds.
ACKNOWLEDGEMENTS We thank the National Institutes of Health Fellows Editorial Board (FEB) for article revision.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine.
Conflict of Interest: none declared.
ABSTRACT Motivation: For many years, the Unified Medical Language System (UMLS) semantic network (SN) has been used as an upper-level semantic framework for the categorization of terms from terminological resources in biomedicine.
BioTop has recently been developed as an upper-level ontology for the biomedical domain.
In contrast to the SN, it is founded upon strict ontological principles, using OWL DL as a formal representation language, which has become standard in the semantic Web.
In order to make logic-based reasoning available for the resources annotated or categorized with the SN, a mapping ontology was developed aligning the SN with BioTop.
Methods: The theoretical foundations and the practical realization of the alignment are being described, with a focus on the design decisions taken, the problems encountered and the adaptations of BioTop that became necessary.
For evaluation purposes, UMLS concept pairs obtained from MEDLINE abstracts by a named entity recognition system were tested for possible semantic relationships.
Furthermore, all semantic-type combinations that occur in the UMLS Metathesaurus were checked for satisfiability.
Results: The effort-intensive alignment process required major design changes and enhancements of BioTop and brought up several design errors that could be fixed.
A comparison between a human curator and the ontology yielded only a low agreement.
Ontology reasoning was also used to successfully identify 133 inconsistent semantic-type combinations.
Availability: BioTop, the OWL DL representation of the UMLS SN, and the mapping ontology are available atContact: stschulz@uni-freiburg.de 1 INTRODUCTION As high-throughput experimental methods and advanced information technology have impressively increased the amount of data, the resulting information congestion has well-known consequences such as fragmentation of data and knowledge and duplication of research efforts (Stevens, 2000).
Factual information about proteins, genes, diseases and other relevant biomedical entities are increasingly available in structured databases but their dissemination by unstructured, texts i.e.
research articles, still prevails.
It is estimated that as much as 80% of new scientific facts are communicated only in their original journal To whom correspondence should be addressed.
publication (Jelier, 2005), the authors relying on a limited group of curators to manually extract, annotate and transfer these facts into the appropriate databases.
Although the pooling of such facts in databases like UniProt (Mulder, 2008) offers clear advantages over the traditional publication process, it would be of great benefit to concentrate all this information in a structured manner in one centralized repository: ongoing research information, peer-reviewed articles, external, authoritative knowledge bases, together with formalizations of the basic kinds of entities and their interrelations in formal ontologies.
Several projects [e.g.
WikiProteins (Mons, 2008)] try to achieve this goal.
Although resource annotation can rely on huge terminological sources as they have evolved in the last decades, automatic reasoning services for tasks including hypothesis generation and knowledge discovery require sound ontologies, whereas they may produce suboptimal results when based on traditional terminological systems.
For this reason, we set out to examine how a formal domain ontology covering the basic kinds of entities in the biomedical domain can replace an informal legacy system.
More precisely, we created a mapping between the UMLS SN (McCray, 2003) and BioTop (Beisswanger, 2008), and assessed through this mapping how each resource contributes to the interpretation of the relation between pairs of co-occurring concepts.
The article is organized as follows: after giving an overview of basic concepts like terminology and ontology (Section 2) we describe the resources used, the mapping approach and the evaluation methodology (Section 3).
Eventually we present our results and discuss them in the context of related work (Sections 4 and 5).
2 BACKGROUND We here introduce the basic concepts underlying our work, viz.
terminology, ontology and description logics.
2.1 Terminology Both text mining and manual annotation require some kind of semantic standard.
Originally, this issue was supposed to be addressed by controlled vocabularies and terminology systems (DeKeizer, 2000a, b; ISO, 2000), a heterogeneous group of mostly language-oriented artefacts that relate the various senses or meanings of linguistic entities to one another (e.g.
by assessing the synonymy between Nephroblastoma and Wilms Tumor).
Sets of (quasi-) synonymous terms are commonly referred to 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i70 i69i76 S.Schulz et al.
as concepts, and in many terminology systems concepts are furthermore related by informal semantic relationships often following vague natural language predicates (narrower than, associated with, etc.).
Terminology systems are generally built to serve a well-defined purpose such as document retrieval, resource annotation, the recording of mortality and morbidity statistics or billing.
In the medical field, the largest terminological system is the Unified Medical Language System (UMLS) (Bodenreider, 2004; UMLS, 2009) in which synonymous terms from different source vocabularies are clustered into concepts, each of which is categorized using a system of semantic types (STs) (McCray, 1995).
Today, the UMLS comprises 1.9 million concepts and almost 7 million terms from close to 150 sources.
2.2 Ontology In reaction to the language-and purpose-oriented and informal approaches to representing a given domain, there has been a growing interest in using formal methods for precisely describing the invariant and language-independent properties of the entities in a domain.
In biomedicine, the Gene Ontology (GO) (Ashburner, 2000) was the pioneer of moving from a purpose-oriented annotation vocabulary to a more principled resource.
Similarly, collaborative initiatives have emerged such as the Open Biomedical Ontologies (OBO) Foundry (Smith, 2007), the continuing development of SNOMED CT (SNOMED, 2009), which is increasingly challenged and guided by ontological principles, as well as increasing mutual awareness between the Semantic Web and Life Sciences communities (Ruttenberg, 2007; Sagotsky, 2008).
The term ontology stems from analytical philosophy, concerned with the question of what exists?
(Quine, 1948).
It became popular by information sciences, and despite quite contradictory definitions (Kusnierczyk, 2006) it has increasingly been used to refer to domain representation of various kinds.
In order to emphasize the use of a formal language in domain representations, we here subscribe to the concept of formal ontologies (Guarino, 1998) as theories that attempt to give precise representations of the types of entities in reality, of their properties and of the relations among them, using axioms and definitions that support algorithmic reasoning.
2.3 Upper-level ontologies The purpose of upper domain ontologies is to define the foundational kinds and relations relevant to the entire domain.
In the life sciences, this includes classes like gene, protein, cell, tissue, nucleotide, population, organism, diagnostic procedure and biological function, among others.
Upper domain ontologies can either be used alone as a source of basic categories (e.g.
for the coarse annotation of resources) or as a common reference for more specialized domain ontologies.
In contrast to domain-specific ontologies such as the GO, upper ontologies propose to trade detail for scope by introducing general categories that are the same across all domains.
Whether or not this is achievable and desirable has been subject of debate.
Nevertheless, several upper-level ontologies have been developed and are being maintained such as BFO1 (Smith, 2007a), DOLCE2 (Gangemi, 1Basic Formal Ontology.
2Descriptive Ontology for Linguistic and Cognitive Engineering.
2002; Masolo, 2003), SUMO3 (Pease, 2008) or GFO4 (Heller, 2004).
More recently, development of application-oriented domain ontologies such as the OBO5 ontologies have led to the proposal of a kind of intermediate-level ontologies, also called top-domain ontologies, such as the Simple Bio Upper Ontology (Rector, 2006), GFO-Bio (Hoehndorf, 2008) or BioTop (Beisswanger, 2008).
In contrast to these recent and more theory-laden resources, the pragmatic UMLS SN6 , developed 15 years ago, can be regarded as the archetype of a biomedical domain upper ontology (McCray, 2003).
Moreover, the SN has already proved its usefulness in providing a consistent categorization of all concepts represented in the UMLS Metathesaurus.
From an upper-level ontology viewpoint, domain upper ontologies play the role of domain ontologies, but from a domain perspective they act as upper ontologies.
For example, the placement of BioTop under BFO or DOLCE could be seen as a domain ontology placed under an upper ontology.
Conversely, BioTop itself may also play the role of an upper ontology when linked to the Cell Ontology (CO) or the GO.
Different upper-level ontologies not only use different formalisms for their representation but also represent the domain in slightly different ways.
As a consequence, the constraints they impose on domain-specific ontologies affect the result of reasoning services based on these upper-level ontologies.
2.4 Description logics Since the 1980s, the application of formal reasoning on ontology structures has led to various formalisms.
Later on, the vision of the Semantic Web (Berners-Lee, 2001) has resulted in a significant standardization of representation languages, formats and reasoning engines.
One of the most noteworthy standards of the Semantic Web was the development of the Web ontology language OWL (Horrocks, 2003) and especially its expressive but still computable subset, OWL description logic (DL).
DLs constitute a family of decidable fragments of first-order logic which have a clean and intuitive syntax (Baader, 2007).
They come in various flavours, ranging from lightweight to highly expressive ones.
The trade-off between expressivity of the logic and computability (and thus, scalability) of its reasoning has to be made in order to properly address the ontology application.
Whereas overly inexpressive DL may lead to underspecifications that imply unintended models of the ontology, highly expensive reasoning makes it infeasible from practical viewpoints.
OWL DL constitutes a compromise between expressiveness and decidability and is supported by DL classifiers like RACER, Fact++ and Pellet (Haarslev, 2003; Tsarkov, 2006; Sirin, 2007).
Description logics are built around the notions of class and relationship and follow model-theoretic semantics.
Classes such as Heart are interpreted as sets of all instances belonging to that class, i.e.
here all particular hearts in the domain.
Relationships then are sets of pairs of class instances like hasPart, which extends to all pairs of objects in the domain that are related in terms of parts and wholes.
So are all pairs of heart instances with their 3Suggested Upper-Merged Ontology.
4General Formal Ontology.
5Open Biomedical Ontologies.
6Unified Medical Language System.
i70 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i71 i69i76 Alignment of the UMLS semantic network with BioTop respective mitral valve instances in the extension of hasPart.
We will illustrate DL syntax and semantics through a set of increasingly complex examples, starting with the class Liver, which in our domain extends to all individual livers of all organisms.
Analogously, the class BodilyOrgan then extends to all individual bodily organs.
When those two statements are put together, we can introduce the key concept of taxonomic subsumption: the class BodilyOrgan forms a superclass of the class Liver, i.e.
the former subsumes the latter if and only if all particular livers are also instances of the class BodilyOrgan.
In DL notation, this taxonomic subsumption is expressed by the operator, e.g.
Liver BodilyOrgan, and is also known as subtype, subclass or is-a relationship.
It is important to stress that this kind of relationship always relates two classes.
In contradistinction to this, the instantiation relationship relates an individual entity to some class, e.g.
the particular liver of the first author of this article to the class Liver.
Such simple class statements can then be combined by different operators and quantifiers, e.g.
the (and) operator and the existential quantifier (exists).
For example, InflammatoryDisease hasLocation.Liver denotes all instances that belong to the class InflammatoryDisease and are further related through the relationship hasLocation to some instance of the class Liver.
This example actually gives both necessary and sufficient conditions in order to fully define the class Hepatitis: Hepatitis InflammatoryDisease hasLocation.Liver.
The equivalence operator indicates that every instance of hepatitis is necessarily an inflammatory disease that is located in some liver.
But through the equivalence operator, one can go in the other direction as well and say that any inflammatory disease that is located in some liver can be classified as hepatitis.
In practice, the term on the left and the expression on the right are equivalent.
The constructors introduced so far allow for automated classification and the computation of equivalence, but not for satisfiability checking.
This is, however, important, wherever the validity of an assertion is to be assured.
For instance, the assertion Immaterial Object  hasPart.ImmaterialObject restricts the value of the role hasPart by using the universal quantifier (only).
It should therefore reject any assertion that states that an immaterial object (e.g.
a space) has a material object as part.
However, a nave use of this construct tends to fail.
The reason of this is the so-called open world assumption: unless otherwise stated, everything is possible.
The following class Strange Object Immaterial Object hasPart.MaterialObject would remain consistent as long as we do not explicitly state that there is nothing that can be both a material and an immaterial object: Immaterial Object MaterialObject (with being the negation operator not).
This means that nothing can be equally an instance of either object, i.e.
the two classes are disjoint.
3 MATERIALS AND METHODS 3.1 UMLS SN The provision of an overarching conceptual umbrella over the biomedical domain was the rationale for the development of the UMLS SN (McCray, 2003).
A tree of 135 STs forms the backbone of the SN.
It is partitioned into the branches entity and event, in which nodes are linked by subclass relations.
In addition, the SN contains a hierarchy of 53 associative relationships (e.g.
location_of, treats).
These relationships are used to form 612 assertions (e.g.
Tissue, location_of, Diagnostic Procedure) from which 6 252 additional assertions can be inferred.
For each semantic relationship, domain and range are specified in terms of one or more STs.
Each concept from the UMLS Metathesaurus is categorized by at least one ST from the SN.
The UMLS SN is a widely used resource in biology and medicine.
However, it suffers from some well-known shortcomings (class descriptions that are ambiguous or vague, relatively low granularity, arbitrary divisions) (Schulze, 2004).
In view of that we wanted to assess these limitations by making them explicit in an OWL DL representation and to explore alternative upper domain ontologies.
3.2 BioTop BioTop (Beisswanger, 2008; Schulz, 2006) originated from a redesign and enrichment of the GENIA ontology.
Like the UMLS SN, its backbone is constituted by a taxonomic tree, consisting of 334 classes.
Its relation hierarchy is populated with 60 relations with domain and range constraints.
The main difference from the UMLS SN is given by its use of OWL DL (see Section 3.1).
BioTop contains 636 logical axioms among which there are subclass, disjointness and equivalence axioms.
The latter (61) enable the computation of additional taxonomic links using DL reasoners.
BioTop exhibits links to the upper-level ontologies DOLCE (Gangemi, 2002; Masolo, 2003), BFO (2007a) and the OBO relation ontology (Smith, 2005).
Furthermore, it provides mappings to OBO Foundry ontologies (e.g.
GO, CO, FMA, ChEBI).
3.3 Mapping Our main objective of bridging between the UMLS SN and BioTop was to capitalize on the categorization of the UMLS Metathesaurus with SN types on the one hand, and to benefit from the ontologically sound and computationally more sophisticated architecture of BioTop on the other.
The aim was to represent the totality of the SN knowledge using BioTop, encompassing the SN types and hierarchical organization as well as the semantic relations with their domain and range restrictions.
In order to meet this requirement, an analysis of the UMLS SN semantics in the light of description logics and its transformation into the formalism used by BioTop had to be performed.
Technically, the plan was to use a central mapping file, which imported both UMLS SN and BioTop, and served as a store for class and relation equivalences and restrictions.
In order to provide mappings for each UMLS SN type, we adjusted the coverage of BioTop wherever justified.
3.4 Assessment methodology 3.4.1 Formative evaluation of BioTop: We used the logic-driven knowledge reengineering described by Schulz (2001), which employs an iterative approach.
Each major ontology redesign (including mapping) step is checked by a description logics reasoner, the results of which are then analysed and corrected under two perspectives: first, the classes tagged as inconsistent are identified and the causes are investigated and repaired; second, every time the ontology has reached a consistent state, the logical entailments are analysed for adequacy.
Whenever inadequate entailments are encountered, the causes are investigated and fixed.
3.4.2 Consistency of SN-type combinations: As numerous UMLS Metathesaurus concepts are categorized by more than one ST, their consistency against BioTop should be checked, based on the SN-BioTop map.
On the basis of the assumption that combinations of STs linked to Metathesaurus concepts constitute conjunctions, all occurring combinations are identified and then attached to the ontology.
3.4.3 Named entity co-occurrence: Named entity recognition (NER) is a widely used text mining technique (Park, 2006).
A well-known problem in NER is when the word or phrase to be recognized is ambiguous, i.e.
it denotes different things.
The implementation of the UMLS SN in BioTop offers the possibility to check ambiguous named entities for whether the i71 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i72 i69i76 S.Schulz et al.
competing referent concepts are compatible with respect to the SN relations allowed for UMLS STs.
We obtained 100 million unique pairs from 15 million PubMed abstracts that had been mined with the state-of-art named entity (NE) recognizer Peregrine (Schuemie, 2007) to recognize UMLS concepts and Uniprot identifiers referred to within the same sentence.
We here consider only the UMLS concept pairs.
The task was to manually assess a sample of 300 UMLS concept pairs.
The curator assessed the plausibility of the linkage between the two concepts in the sentence context.
Each co-occurring pair was first checked against the SRSTRE1 table from the SN and alternatively against the mapping ontology, based on the OWL DL implementation of the BioTop/UMLS SN integration.
4 RESULTS 4.1 Mapping of UMLS STs DL-based ontologies are hierarchies of types (classes) that can be instantiated by particular entities only.
According to (McCray, 2002) we can consider the SN as a hierarchy of upper-level classes (regardless of the naming of some of the types that suggest a meta-level interpretation, e.g.
the type Functional Concept).
The categorization relation (that attaches UMLS Metathesaurus concepts to SN types) can therefore be mostly interpreted as a taxonomic subsumption relation (is-a).
Exceptions include geographical locations and a few other true instances, e.g.
laws and persons.
In these cases the categorization relation is to be interpreted as an instance-of relation.
The mapping was done as follows.
First of all, the taxonomic tree of the UMLS SN types was remodelled in OWL (SN.OWL) by expressing the taxonomic subsumption (is-a) as OWL subclasses.
No further assumptions were made.
Especially, no partitions were introduced, as the source and its documentation do not make any statements as to whether STs are mutually exclusive.
On the basis of the textual (SN, BioTop) and the formal (BioTop) definitions available we then attempted to map each ST to BioTop.
Lexical mapping criteria were not used.
In cases of doubt, domain experts were consulted.
The mapping was performed in close collaboration among the authors.
At several occasions, problems encountered when accommodating STs in BioTop were discussed in face to face meetings, conference calls and e-mail discussions.
In controversial cases other existing ontologies, e.g.
OBI, were consulted.
For the mapping a new OWL-bridging file was created that referenced both resources with owl:imports statements using the Protg 4 ontology editor.7 This allowed us to bring together two resources that were out of our direct control and to introduce new assertions linking them.
Mapping the STs of the SN to BioTop the following cases could be distinguished.
4.1.1 Direct match: The ST is equivalent to a class in BioTop, or the difference is small enough that creating a separate new class alongside an existing one would not be justified; e.g.
Animal in BioTop has the exact same meaning as in the SN.
4.1.2 Restriction: No BioTop class is a straight match for the ST, but it can be defined by restricting an existing BioTop class, e.g.
AnatomicalAbnormality is mapped to the expression: OrganismPart bearerOf.PathologicalCondition, where OrganismPart and PathologicalCondition are existing BioTop classes and bearerOf is an existing BioTop relation.
7http://www.protege.stanford.edu/.
4.1.3 Union: If the ST cannot be defined by a single class, it corresponds to the union of several classes.
Any combination of the previously described types can participate in the union.
For example, the SN type Gene or Genome was mapped to the disjunction biotop:Gene 	 biotop:Genome.
4.1.4 Out of scope: The ST cannot be expressed using any of the options above; the immediate solution was to create a new class inside the mapping file itself, defined as the subclass of an existing BioTop class and map the ST to this new class.
In the incremental mapping/BioTop redesign process, all ST leaf nodes (but two) introduced this way were recreated in BioTop.
The non-matching STs (e.g.
daily or recreational activity) were mapped to a more general BioTop class.
4.1.5 No match: The ST is regarded meaningless for BioTop in one of the following cases: its definition does not sufficiently differentiate it from its parent, it is too abstract, or it is only included in the SN as a housekeeping node in order to group more meaningful child nodes.
For example, Chemical Viewed Functionally has a meta-class meaning (it groups UMLS concepts, but is useless as a distinguishing criterion for their individuals) which cannot be represented by BioTop.
Leaving the class undefined allows for the existing subsumption hierarchy of the SN to reason up to the nearest parent that does have a mapping, in this case Substance.
Most STs on an upper level have imprecise definitions and do not coincide with any BioTop class, e.g.
Idea or concept (An abstract concept, such as a social, religious or philosophical concept.
), the definition of which seems not plausible to its subtypes, e.g.
Geographic Area.
The names, textual definitions and the hierarchical context of SN types created mapping difficulties in many cases.
For instance, the ontologically crisp distinction between function and process is mixed up in the SN.
So does the type Phenomenon or Process subsume Pathologic Function, which is a parent of, e.g.
Neoplastic Process.
As a result, some upper-level classes were mapped not to a single class in BioTop but to the union of several classes.
An example is Spatial Concept, defined by the union of Body Location or Region, Body Space or Junction, Geographic Area and Molecular Sequence.
Others were mapped to quite complex expressions including disjunctions, value restrictions and exclusions.
4.2 Interpretation and mapping of UMLS semantic relations The treatment of UMLS SN semantic relations turned out to be more complicated thus requiring a two-step approach; they first have to be semantically interpreted and properly built into an OWL DL model before they can be mapped to BioTop.
Their simple interpretation as description logics relations (object properties) is semantically problematic as SN relations range over STs (i.e.
instantiable classes) whereas object properties range over individual entities.
Such an interpretation of concept to concept relations in the light of formal logic has been repeatedly discussed in the recent years (Smith, 2005).
For example, five different possible interpretations of SN triples are discussed in Kashyap (2003).
For most UMLS semantic relations there is a quite complex arrangement of domain and range restrictions, in which certain range restrictions are only valid with certain domain restrictions.
For instance, the UMLS SN restricts the domain of the treats relation to i72 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i73 i69i76 Alignment of the UMLS semantic network with BioTop drugs and physicians, and its range to patients and diseases (among others).
However, it does not allow the combination of drug and patient, or health professional and disease.8         Range   Domain Drug Physician Disease allowed disallowed Person disallowed allowed We could, of course, ignore this and take simply the union of the extension of the UMLS concepts as the restriction of new BioTop relations that have to be included into the ontology.
Thus we would have to accept unintended models, e.g.
that a drug treats a person.
We discussed and implemented different solutions how to adequately represent these constraints using OWL DL.
As a first solution, we introduced subrelations, in the following style (again simplified): treatsMED treats (domain: Drug, range: Disease) treatsPHY treats (domain: Physician, range: Person).
In this first step, we obtained a total number of 210 relations (OWL object properties).
However, we have to acknowledge that this is a rather cosmetic solution, because such a model is only able to reject unwanted assertions if the specialized relations but not the general ones are used.
Furthermore, by lack of disjointness statements in the class hierarchy it cannot even be rejected that, e.g., something is both a drug and a physician.
This is, however, not a fault of the representation language but an underspecification of the UMLS SN.
As a second solution we discussed the following, as it achieves the desired result without the creation of subrelations.
Drug treats.Disease Physician treats.Person Together with: treats.Disease Drug treats.Person Physician The drawback is here that this solution uses general concept inclusions (GCIs).
Although they are part of the OWL DL specifications, they were not supported by our tools.
Both approaches, however, face a severe problem when it comes to the mapping to BioTop, as the latter includes only a relatively low number of relations.
Enhancing BioTop by the whole array of SN relations would conflict with its design principle to keep the set of relations small but semantically precise, restricting them to those that are needed for BioTop class definitions.
This is not the case with most SN relations: treats, interacts, diagnoses, etc.
Instead, BioTop contains, in its Processual Entity branch, already classes such as Treating, Interacting, etc.
which convey the same meaning and can be regarded as reifications.
TreatingPerson Action  has_agent.
Physician  has_patient.
Person  has_agent.
Physician  has_patient.
Person TreatingDisease Action  has_agent.
Drug has_patient.
Disease  has_agent.
Drug has_patient.
Disease Treating TreatingPerson 	 TreatingDisease 8For the sake of understandability the example is simplified and does not use the lengthy UMLS SN names.
We therefore decided to mapas an alternative approachthe SN relational constraintsexpressed as triplessuch as D1 REL R1, D2 REL R2, D3 REL R3, , Dn REL Rn (Di referring to domain and Ri to range) to an equally uncomplicated DL formula.
As a consequence, we do not need to create new DL relations (which would contradict the DL design principles), but simplify the above formula: REL1 has_domain.
D1  has_range.
R1 REL2 has_domain.
D2  has_range.
R2 REL3 has_domain.
D3  has_range.
R3 ... RELn has_domain.
Dn  has_range.
Rn REL REL1 	 REL2 	 REL3 	 	 RELn has_domain and has_range are then mapped to biotop: has_agent and biotop:has_patient.
Of course, the agent/patient reading does not make sense with many spatial or temporal relations.
In these cases we extended the map by additional value restrictions.
Finally, there are SN relations that cannot be expressed as relations between particulars because they simply do not relate anything at the level of particulars.
The prototypical example is prevent, such as in the statement contraceptive drugs prevent pregnancy.
On a UMLS concept level it is, without doubt, sensible to express this in a relational form, such as prevents (contraceptive drugs; pregnancy).
Such a close-to-human-language assertion on prevention carries several implicit assumptions that must be made clear before expressing it via an ontology; preventing pregnancy does not exclude the possibility of becoming pregnant but it brings about a strong risk reduction.
Furthermore, there is both a temporal and a dose association between the drug and the risk.
We can therefore rephrase Contraceptive drugs prevent pregnancy as follows: The administration of contraceptive drugs of an adequate dose and regularity to a woman reduces her pregnancy risk within a defined timeframe or more simply: The administration of contraceptive drugs to a woman reduces her pregnancy risk within a defined timeframe.
We could express this as follows: PregnancyRiskReductionBySubstanceIntake Action has_agent.Substance has_agent.Substance  has_patient.
(Risk ( inheres_in.
Organism  inheres_in.
Organism) risk_of.Pregnancy) This digression illustrates the difficulty if not impossibility of an ontologically precise formal reconstruction of seemingly simple close-to-language predicates.
For the semantic relationship mapping we proceeded the following way: all relationships were reified (i.e.
expressed as classes) and added as OWL classes using value restrictions on the roles has_agent and has_patient.
Those relationships which had a direct correlate in BioTop (i.e.
the SN spatiotemporal relationships) were additionally mapped directly to BioTop relationships (object properties).
In both cases the domain and range-specific subrelations were accounted for by additional subclasses/subrelations (in analogy to the Treating example above).
The reification classes were furthermore provided with so-called covering axioms that assure the enforcement of one of the child classes with their restrictions.
Again, no mappings were performed for some upper-level relationships i73 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i74 i69i76 S.Schulz et al.
(and, accordingly, to upper-level reification classes), for the same reasons as explained for the type hierarchy.
The final result of the mapping of each ST to BioTop yielded 132 equivalence and 19 subclass axioms in the mapping ontology.
The OWL reconstruction of the UMLS SN comprised 626 classes and 1530 axioms, and BioTop grew from 200 to 334 classes, 30 to 40 object properties and from 470 to 636 axioms.
4.3 Assessment results The whole mapping exercise constituted an ideal testbed for the ongoing quality assurance and formative evaluation of BioTop.
Because of the constant need of inconsistency checking and resolving, many hidden errors in BioTop were detected, especially faulty disjointness axioms (e.g.
Organic Chemical was disjoint from Carbohydrate), unrecognized ambiguities (e.g.
Sequence as information entity versus molecular structure) as well as granularity mismatches (e.g.
Chromosome as molecule).
The maintenance work was, however, very time consuming, totalling at least one person year, divided among five modellers.
A significant advance for inconsistency checking and resolution was achieved by the use of a new Protg add-in that presents precise explanations of entailments in OWL ontologies (Horridge, 2008).
Runtime performance, however, proved to be a major drawback.
The more axioms are being added (especially negations, disjointness axioms, and inverse properties) the more the performance decreases so that classification time now constitutes a major obstacle in the whole ontology construction and maintenance process.
Nevertheless, it was possible to use the ontology in order to validate an important feature in UMLS, viz.
multiple ST categorization.
In the 2008 Metathesaurus (totalling more than 1.80 million concepts) release there are 397 different combinations of two to four STs, linked by about 220 000 UMLS concepts.
On the basis of the assumption that STY combinations should be interpreted as conjunction, we checked each occurring combination for consistency.
The DL classifier recognized 133 combinations as inconsistent, affecting a total of 6116 UMLS concepts.
The most frequently occurring unsatisfiable type combination was Manufactured Object with Health Care Related Organization (e.g.
Hospital as building versus organization).
The preliminary results of the named entity experiment are, however, less encouraging (Table 1).
Because of so many ambiguities, the curator had made a clear assessment of semantic relatedness in only half of the cases.
The comparison of the manual classification to the automated one (into true and false) clearly demonstrates the dilemma.
The checking against the UMLS SN table STSTR1 shows a certain correlation with the curators judgment but still produces many false negatives and false positives.
BioTop via the SN and the mapping ontologyrejects extremely few associations.
In order to correctly interpret these results, we emphasize that the question of whether two UMLS concepts are related is not the same as to ask whether their STs exhibit some allowed relationship.
For instance, the expert rating for the association between Superoxide reductase (ST: Enzyme) and Aldehyde (ST: Organic Chemical) was negative.
Of course, this does not mean that any kind of association between Enzyme and Organic Chemical should de disallowed.
On the contrary, these two STs are closely associated, which is not Table 1.
Named entity co-occurrence results Expert judgement: concepts related Expert judgement: concepts unrelated SN: related 31 22 SN: unrelated 21 71 BioTop: related 52 90 BioTop: unrelated 0 3 changed by the fact that most random combinations of some enzyme with some chemical are irrelevant.
The low rate of rejections by BioTop demonstrates the problem of the so-called open-world semantics (Baader, 2007), i.e.
all models are accepted unless they are explicitly falsified.
In the case a description logics ontology is used for this kind of consistency check, the modellers have to be very meticulous in filling the holes.
On the other hand, it must be acknowledged that the OWL reconstruction of the idiosyncratic categorization in SN required many disjunctive statements which resulted in a relaxation of the domain and value restrictions.
In any way, it is known to be difficult to keep an OWL model water-proof in this aspect, and OWL has recently been criticized that it is generally ill-suited for tasks like schema validation (Rajsky, 2008).
However, we argue that this is not an inherent but rather a tooling problem, at least for those description logics dialects that support some kind of negation.
As a consequence, we performed a thorough fault analysis and could identify and fix several underspecifications that gave rise to unintended models.
5 RELATED WORK There are many reports in the literature about the conversion of thesauri, frame knowledge bases and ontologies from various representational formats into description logics.
Examples are Pisanelli (1998) and Schulz (2001) for the UMLS; Beck (2003), Dameron (2005) and Golbreich (2006) for the Foundational Model of Anatomy; Wroe (2003) and Egana (2008) for the GO and Heja (2007) for ICD-10.
What most of these approaches have in common is (i) that the mapping is not straightforward, (ii) it relies on several ontological basic assumptions that are not explicitly stated in the sources, e.g.
on disjointness axioms, on the intended meaning and the algebraic properties of relationships and (iii) that not all knowledge conveyed by the sources is expressible in description logics, due to the language constraints.
The UMLS SN was targeted by Kashyap (2003) who concluded that the logical interpretation of the semantic relations in the SN should depend on the application in which the ontology is to be used.
More specifically, ontological aspects of the UMLS SN were discussed by Schulze-Kremer (2004).
The latter authors acknowledge the importance of the SN for the semantic integration of terminology but spot a number of weaknesses future revisions should address.
A major point of criticism is the mixture of concrete with abstract entities, real entities with bauplan entities, objects with their roles, functions and processes.
This mainly coincides with our mapping experiences as described in Sections 4.1 and 4.2. i74 [09:53 15/5/2009 Bioinformatics-btp194.tex] Page: i75 i69i76 Alignment of the UMLS semantic network with BioTop 6 CONCLUSION We have described the ongoing development and improvement of a semantic resource, the life science ontology BioTop in the light of the mapping to the legacy UMLS SN.
The purpose of this effort is to bring together the large amount of data categorized by the latter with the formal foundation of the former, using emerging standards and tools developed by the Semantic Web community.
Semantic and terminological support is especially important for facilitating an opening of the curation process towards a broader community.
The alignment of a formal ontology with a relatively informal system of hierarchically ordered categories like the UMLS SN challenges the ontology engineer to formally re-interpret the latter and to overcome its ontological shortcomings.
The logical machinery of description logics, implemented in reasoning engines, was an indispensable part of the mapping process, which, ultimately, not only provided a consistent mapping ontology but contributed, by large, to error detection and improvement of BioTop.
We described two assessment experiments.
One of them, aiming at satisfiability checking of SN-type combinations yielded good results that revealed hidden ambiguities of UMLS concepts.
The other, however, generated rather poor results.
It attempted to use the ontology for determining which UMLS concept pairs were closely related to each other.
As a result, the mapping ontology rejected very few models, thus supporting the recent critique on the suitability of OWL for schema verification.
However, this result also challenged the evaluation scenario: judgements on the relatedness of very specific instances can not be necessarily carried over to judgements at the level of STs.
Nevertheless, it was disappointing because the modellers had spent a great effort in partitioning the BioTop ontology in order to antagonize the unwarranted effects of the open-world assumption.
This is an issue where more sophisticated tool support for OWL ontology construction and validation is desperately needed, in order to grant formal ontologies and logic-based reasoning a central place in future high-throughput and high-impact life sciences knowledge management technologies.
ACKNOWLEDGEMENTS The authors thank Martin Boeker (Freiburg) and Holger Stenzhorn (Freiburg) for their BioTop maintenance efforts, as well as Robert Hoehndorf (Leipzig) and Alan Rector (Manchester) for fruitful discussions.
Funding: EC STREP project BOOTStrep (FP6 028099); Intramural Research Program of the National Institutes of Health; National Library of Medicine.
Conflict of Interest: none declared.
ABSTRACT Motivation: High throughput sequencing technologies generate large amounts of short reads.
Mapping these to a reference sequence consumes large amounts of processing time and memory, and read mapping errors can lead to noisy or incorrect alignments.
SNP-o-matic is a fast, memory-efficient and stringent read mapping tool offering a variety of analytical output functions, with an emphasis on genotyping.
Availability: http://snpomatic.sourceforge.net Contact: mm6@sanger.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Analysis of genome variation has been revolutionized by the advent of next-generation sequencing technologies (Bentley et al., 2008; Li et al., 2008b; Shendure and Ji, 2008).
The short length of sequence reads, e.g.
50 base pairs, can pose considerable challenges in achieving accurate genome alignment, particularly if the genome sequence is highly polymorphic.
Discovery of single nucleotide polymorphisms (SNPs) and other variants depends on the alignment algorithm allowing some mismatches to the reference sequence, but allowing too many mismatches may lead to incorrect alignments.
Thus the process of discovering novel variants amounts to a complex statistical problem, particularly if sequencing errors and other sources of noise are taken into account.
Various discovery algorithms have been developed and this is an area of much research interest (for example MAQ, Li et al., 2008a; and bowtie, Langmead et al., 2009).
Here we focus on the problem of describing the genotype of an individual using short-read sequencing data.
In principle, this can be incorporated into the same algorithms used for discovering novel variants, an approach that appears to work well for the human genome (Bentley et al., 2008).
However, there are circumstances in which it may be useful to separate SNP discovery from SNP genotyping.
For example, SNP discovery in Plasmodium falciparum is particularly complicated due to 80% AT content, many repeat sequences, regions of extreme polymorphism and the multiclonality of natural isolates.
Thus different SNP discovery algorithms return widely different results.
One way of addressing this problem is to begin by annotating the reference genome with all the putative SNPs generated by different discovery algorithms.
Then individual SNPs may be genotyped by performing a stringent alignment of the sequencing reads against the reference genome, allowing for all the putative variable positions.
To whom correspondence should be addressed.
To support this sort of genotyping analysis, we developed SNP-o-matic as a fast way of mapping short sequence reads to a reference genome with a list of putative variable positions that are specified at the outset.
The default settings are highly stringent, returning only those sequence reads that align perfectly with the reference genome after allowing for the putative variable positions.
An important feature of SNP-o-matic, which allows the rapid processing of large volumes of sequencing data, is that the reference genome sequence is first indexed (on the fly or by using a pre-computed index from disk), and then each sequence read or read pair is examined one at a time.
This avoids having to build and store an index of the reads saving both compute cycles and memory.
Indexing of the reference genome is done in memory on the fly from a generic FASTA file.
A list of putative SNPs, if supplied, is integrated into the reference before indexing, and all permutations of this SNP-containing sequence are indexed.
Indexing the 25 Mb P. falciparum genome (without SNPs) takes about 30 s on a single CPU core and occupies 1 GB of memory.
A memory-saving option can reduce both memory and indexing time significantly at the expense of a longer mapping phase.
The index can be stored in a file for future use, further reducing the time required for this step, or to facilitate the analysis for larger genomes (Supplementary Material).
Reads are supplied in either FASTA or FASTQ (http://maq.sourceforge.net/fastq.shtml) format; read pairs can be in either single or split files.
In performance tests, mapping 10-million 37 base paired reads against the P. falciparum genome takes 70 s on a single CPU core, not counting the indexing.
No additional memory is required for the mapping.
Additional time and memory may be required for some of the output functions.
For genotyping, a variable length indexed kmer (default 26 bases) is compared to the same length kmer for each read (or both reads in a read pair).
Matches in these bases thus have to be perfect, with respect to the putative SNPs.
The remaining bases of the read are then compared base-by-base to the reference.
By default, these matches have to be perfect as well, but a limited number of mismatches can be allowed.
This stringency will avoid false SNP calls in genotyping mode that would otherwise be caused by aligning reads containing sequencing errors.
Thus, SNP-o-matic will generally map less reads than other algorithms, but the mapping will have much higher accuracy.
When allowing mismatches, the kmer length can be varied to increase mapping tolerance (Supplementary Material).
Both parts of a read pair have to map on the same chromosome for valid mapping; a fragment range can be used to limit their mapping distance to conform to the expected size distribution for the library.
An optional mode can increase stringency by ensuring that at least one read of a read pair maps uniquely within reference the genome.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:46 14/8/2009 Bioinformatics-btp403.tex] Page: 2435 24342435 SNP-o-matic By default, SNP-o-matic will find and use all valid mappings for a read or read pair within the reference.
When using read pairs, the stringent mapping algorithm can sometimes map one of the reads in the pair, but not the other.
SNP-o-matic can output various data about such read pairs.
From the mapping position, orientation, and fragment size, a likely position can be estimated for the non-mapping read.
Based on this information, reads can be grouped by position and assembled to discover variation.
Additionally, the estimated area can be searched for mappings with some mismatches, resulting in potential new SNP calls.
This output is the primary method used by SNP-o-matic to discover new SNPs and small-scale variation, both of which require further downstream analysis (Supplementary Material).
Scripts for such analysis are under development and will eventually be incorporated into the SNP-o-matic package to augment its core function.
Similarly, both reads of the pair may map to the reference, but not on the same chromosome.
This information can be used to detect misassemblies.
When using (super)contigs as reference sequence, read pairs can thus be used to link contigs together, determine their order, and estimate the size of the gap between two contigs.
An output type of SNP-o-matic is a read bin, a file containing reads grouped by mapping behavior.
Bins are a quick and easy way to filter a read set, for example to remove DNA contamination and noise from non-uniquely mapping reads, or to gather non-mapping reads for further study or assembly.
Available bins are single mapping reads (uniquely mapped in the genome), multiple-mapping reads, non-mapping reads, and reads containing IUPAC bases (e.g.
N); the later are ignored by SNP-o-matic for mapping.
Mapping/alignment output is supplied for pileup, coverage (base count per position), CIGAR format (http://biowiki.org/CigarFormat), gff format (http://biowiki.org/GffFormat), SAM format (http://samtools.sourceforge.net/) and sqlite database (http://www.sqlite.org/).
For accurate SNP genotyping, it is advantageous to take account of sequence quality scores, especially in regions with low coverage.
SNP-o-matic can generate an output file showing each instance where a mapped read covers a putative SNP.
Each output line contains the read name, allele position on the reference, reference and observed allele, quality score of the allele base, average and minimum quality of both the entire read as well as the five bases on either side of the allele-calling base, and auxiliary data.
This data can be further quality filtered, and used to generate a list of non-reference majority alleles.
Other outputs include observed fragment size distribution, insertion/deletion predictions and inversion detection.
These can also be determined by alternative algorithms from the afore-mentioned mapping/alignment outputs.
SNP-o-matic is written in C++/ C (for performance optimization).
Compilation with the Intel icpc compiler has shown significant runtime improvements over g++.
We carried out a number of performance tests which are described in the Supplementary Material and briefly summarized below.
The initial tests were based on an artificial dataset consisting of a 1mbp reference genome whose AT content (80%) is similar to the P. falciparum genome, and a duplicate genome with randomly introduced SNPs and indels.
Solexa read pairs (2 37 bases) with random errors (one in five reads) were generated from the altered genome.
SNP-o-matic correctly genotyped the SNPs when they were given as a putative SNP list.
As expected, coverage dropped substantially when a SNP list was not supplied, unless the mapping stringency was reduced.
We have not attempted to conduct a comprehensive comparison of the performance of SNP-o-matic with SNP discovery algorithms such as MAQ as it is designed primarily as a tool to be used after the stage of SNP discovery.
However, as an illustration of where SNP-o-matic may be useful, we found that, when analyzing clusters of six SNPs in the simulated dataset, MAQ only called two of the SNPs, whereas SNP-o-matic called all six correctly when they were supplied in a putative SNP list.
The current version of SNP-o-matic does not directly detect indels, but can be adapted to do so by using an optional wobble function to identify read pairs where one read maps perfectly but the other does not, and then using an algorithm such as velvet (Zerbino and Birney, 2008) to assemble the non-mapping reads into a contig which is then mapped to the region covering the deletion site using an algorithm such as blat (Kent, 2002).
Using this approach, we found that it was possible to detect a five-base deletion that was introduced into the simulated dataset described above.
Finally, in the Supplementary Material, we provide data on the performance of SNP-o-matic on human chromosomes 1, X, and Y.
Based on these findings we estimate that processing an entire human genome using a pre-computed index and the memory saving option, mapping the test reads should take 20 min and 29 GB of RAM.
A similar timeframe, with <3 GB RAM usage, would be expected for a chromosome-by-chromosome serial execution; this would require an additional, albeit simple, filtering step to ensure uniqueness across the entire genome.
ACKNOWLEDGEMENTS The authors thank Chris Newbold for several discussions and suggestions, and Gareth Maslen for help with preparing the manuscript.
Funding: Wellcome Trust, Bill and Melinda Gates Foundation and Medical Research Council.
Conflict of Interest: none declared.
ABSTRACT Motivation: Recent years have seen the development of a wide range of biomedical ontologies.
Notable among these is Sequence Ontology (SO) which offers a rich hierarchy of terms and relationships that can be used to annotate genomic data.
Well-designed formal ontologies allow data to be reasoned upon in a consistent and logically sound way and can lead to the discovery of new relationships.
The Semantic Web Rules Language (SWRL) augments the capabilities of a reasoner by allowing the creation of conditional rules.
To date, however, formal reasoning, especially the use of SWRL rules, has not been widely used in biomedicine.
Results: We have built a knowledge base of human pseudogenes, extending the existing SO framework to incorporate additional attributes.
In particular, we have defined the relationships between pseudogenes and segmental duplications.
We then created a series of logical rules using SWRL to answer research questions and to annotate our pseudogenes appropriately.
Finally, we were left with a knowledge base which could be queried to discover information about human pseudogene evolution.
Availability: The fully populated knowledge base described in this document is available for download from http://ontology.pseudogene.org.
A SPARQL endpoint from which to query the dataset is also available at this location.
Contact: matthew.holford@yale.edu; mark.gerstein@yale.edu 1 INTRODUCTION In recent years, formal ontologies have been suggested as a solution to the problem of describing complicated realms of biomedical knowledge (Rubin et al., 1997).
Well-designed ontologies possess a number of positive aspects including, (i) the ability to define controlled vocabularies of terms, (ii) the ability to inherit and extend existing terms, (iii) the ability to declare relationships between existing terms and (iv) the ability to infer new relationships by reasoning over existing terms.
Through the technologies known collectively as the Semantic Web, most especially the Web Ontology Language (OWL) (OWL2, 2009), researchers are able to share and extend ontologies throughout the scientific community.
Although biomedical ontologies have existed for a number of years, scientists are far from realizing the full benefits of their use.
There is still room for considerable advancement in this area, especially in the application of formal reasoning.
A unique strength of formal ontologies in the area of knowledge representation is their ability to be reasoned upon in a logically provable way.
This reasoning is performed using Description Logic To whom correspondence should be addressed.
(DL) , a form of logic developed to reason on objects, both individual objects and classes of objects.
Software called reasoners [examples include Pellet (Sirin et al., 2007), Fact++ (Tsarkov and Horrocks, 2006) and KAON (KAON, 2010)] use the rules of DL to perform particular operations on knowledge bases.
The most important of these are: (i) consistency checking: the adherence of the ontological model to the rules of DL; (ii) satisfiability checking: the ability for classes described to be realized by actual individuals; and (iii) classification: the expansion of relationships between objects inferred from explicitly stated relationships.
The DL version of the OWL language assures that DL reasoners can perform these services in a computationally tractable manner.
While the first two services are imperative for ensuring the integrity of data, the third, the ability to infer new relationships, is especially appealing to scientists as it hints at the possibility of new discovery.
Moreover, the relationships discovered are instantly provable, given the reasoners adherence to the rules of formal logic.
The OWL language offers a rich set of properties for inference, including class subsumption, property subsumption, transitivity of properties and inverse properties.
We consider a simple example.
We define a class Father as a subclass of a class Ancestor and a class Son as a subclass of a class Descendent.
We also define a property has_father as a sub-property of has_ancestor with an inverse property has_son that is a sub-property of has_descendent.
Now, if say that Matt has_father Ted, the reasoner can automatically infer that Ted has_descendant Matt.
Taken in isolation, such examples seem trivial and obvious but they can be very helpful when sifting through huge amounts of data.
Despite the richness of OWLs set of relational properties, it does not cover the full range of expressive possibilities for object relationships that we might like.
For example, it is often useful to declare data relationships in terms of conditional statements or production rules.
For this purpose, a specialized rule language is useful.
The Semantic Web Rule Language (SWRL) incorporates an existing rule language (RuleML) with OWL (SWRL, 2005).
Rules are defined in two parts: antecedents and consequents.
If all statements in the antecedent clause are determined to be true, then all statements in the consequent clause are applied.
In this way, new properties can be assigned to individuals in an ontology based upon the current state of the knowledge base.
A popular example is the Uncle Rule, which states that if a persons father has a brother, that brother is the persons uncle.
So, if Matt has_father Ted and Ted has_brother Doug, the reasoner can infer that Matt has_uncle Doug.
SWRL also specifies a library of built-in functions which can be applied to individuals.
These include numerical comparison, simple arithmetic and string manipulation.
At present, SWRL is the most widely used rule language in the Semantic Web community.
The popular ontology development environment The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i72 i71i78 M.E.Holford et al.
Protege includes a SWRLTab plugin for creating and processing SWRL rules (SWRLTab, 2010).
SWRL is supported by the Pellet reasoner up to the point where rules can be determined to be DL-safe, i.e.
they may be realized in a computationally tractable fashion.
Firing of SWRL rules is performed by Pellet as part of the classification process and new entailments thus generated can be added to an existing ontology.
We wish to exploit these reasoning capabilities in our research on pseudogenes.
For this purpose, we can build upon Sequence Ontology (SO)(Eilbeck et al., 2005), among the most notable of ontologies created in the biomedical community.
SO aims to provide the full set of terms and relationships necessary to perform sequence annotation.
Despite its central importance to molecular biology, sequence annotation has historically been more difficult than required due to a divergence in naming standards.
This makes the sharing of data a challenge.
SO provides a controlled hierarchy of terms that describe elements that may be found upon a sequence, referred to as sequence_features.
These may represent anything from genes and pseudogenes to smaller units such as individual bases.
A class sequence_variant describes variable elements on the sequence such as alleles and copy_number_variations.
Annotative tags which may be attached to elements on the sequence are subclasses of the sequence_attribute class.
Examples include conserved, retrotransposed and transgenic.
In addition to this structured set of terms, SO defines relationships for how sequence elements are inter-related.
In particular, the authors rigorously define part-whole relationships employing formalisms from the philosophical discipline of mereology (Winston et al., 1987).
The most common usage of SO is to label sequence annotations with appropriate SO terms.
Typically this is done by attaching terms to an annotation in a separate format, such as a flat file or a database which describes instances of sequence data (Eilbeck and Lewis, 2004).
The developers have created a modular relational database schema called CHADO for this purpose (Mungall et al., 2007).
Here, notably, sequence elements are not hard-coded with their location on a particular sequence but are linked to featureloc elements which contain individual location information.
CHADO and its companion mark-up format CHADO-XML offer a robust approach to annotating sequences in a formal and logically coherent manner.
Its strengths are particularly evident in the handling of large volumes of data.
We wished to explore an alternative approach.
We decided to create a full knowledge base by populating the SO ontology with individual instances of sequence features.
We would then perform reasoning using relationships defined as part of SO, extensions to these relationships and SWRL rules based upon these relationships.
From this.
we hoped to discover new relationships between sequence features and thereby strengthen our pseudogene annotation.
2 APPROACH Pseudogenes form an almost ideal subdomain for ontology development in that they are connected to normal genomic features while maintaining a large enough number of unique aspects to form an area for independent description.
Pseudogenes represent bits of genomic sequence that were once functional but have become inactive.
They are often the result of various genomic copying processes, principally duplication and retro-transposition.
There are two basic types of pseudogenesprocessed and duplicated.
The former arise through the process of retro-transposition, the latter through duplication events.
As artifacts of a history of copying, pseudogenes offer us a glimpse of evolutionary history, both of individual genes and of the genome as a whole.
An understanding of how and when particular pseudogenes were derived in relation to other genomic features is significant to our comprehension of both genomics and evolutionary biology (Zhang and Gerstein, 2003).
To this end, we have been involved in annotating pseudogenes in collaboration with researchers at the Sanger Institute and at UCSC, including researchers who were involved in the development of SO.
As SO already defines a number of classes related to pseudogenes, we were able use these terms to fill our knowledge base with individuals.
For those terms not present, we were able to extend existing SO classes.
SO currently defines a class pseudogene with several subclasses including processed_pseudogene.
We added additional subclasses duplicated_pseudogene and unitary_pseudogene for instances of these types of pseudogene.
Pseudogenes whose specific type was ambiguous were left as instances of the base class pseudogene.
All pseudogenes are defined as non-functional copies of parent genes.
These genes are generally identified using the transcribed protein.
For this reason, even though pseudogenes may derive from any type of gene including RNA genes, in our ontology all parent genes were instantiated as instances of the SOs protein_coding_gene class.
We created a sub-property of SOs property non_functional_homolog_of to describe the link between a pseudogene and the parent from which it derived.
This sub-property, has_parent_gene restricts the range of values to instances of protein_coding_gene and restricts the maximum cardinality to a single instance.
We used identifiers from the existing pseudogene ontology (PGO), which was created as part of the Pseudofam project (Lam et al., 2009).
We also incorporated information, where available, about the location of particular exons and introns within the pseudogenes, noting of course that these no longer have the same meaning in a non-functional context.
Here, we were able to use existing SO classes, pseudogenic_exon and intron.
To express containment of these features within a pseudogene, we adopted SOs recommended usage of the part_of property, which is defined as a core relationship in the OBO Relationship Ontology (RO) (Smith et al., 2007).
To simplify querying later, we created sub-properties contains_pseudogenic_exon and contains_pseudogenic_intron.
These constrain the domain of the property to members of the pseudogene class and the range of the property to pseudogenic_exons and introns, respectively.
We also defined inverse properties pseudogenic_exon_in_pseudogene and pseudogenic_intron_in_pseudogene.
These constrain the ROs has_part property which is the inverse of part_of.
These inverse properties can, of course, be automatically inferred by the reasoner.
One focus of our research is the relation of pseudogenes to segmental duplications (SDs).
SDs are defined as continuous stretches of DNA that map to multiple locations on the genome.
A common ground rule is that they are 1000 or more base pairs in length and have sequence similarities of 90% or more.
Like pseudogenes, they are residual artifacts of a history of copying.
Apprehending their origins is similarly important to understanding the evolutionary history of the genome (Bailey and Eichler, 2006).
To include SDs in our annotation, we defined a class sd_segment as a subclass of the relatively low-level SO term biological_region.
To keep track of the one or more duplicate segments each sd_segment has, we defined a property is_sd_pair_of.
The sd_segment class acts i72 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i73 i71i78 SWRL reasoning on pseudogenes as both domain and range of this property which is a sub-property of the SO term similar_to.
We also wanted to determine what if any pseudogenes or genes were located within a segment in an SD pair.
We used the same technique of constraining the has_part property that we did for contains_pseudogenic_exons.
Properties called contains_pseudogene and contains_gene were created, as were the inverse properties pseudogene_in_segment and gene_in_segment.
Finally, we created a property to indicate the number of pseudogenes (has_pseudogene_count) and the number of genes (has_gene_count) within a segment.
These were necessary for certain SWRL rules we created later.
On the surface it appears a deficiency that we must specify the size of the lists of genes and pseudogenes as a separate property.
Indeed, the SWRL built-in library provides an operator to determine the length of a list.
However, this is not considered DL-safe as it would violate the open-world assumption which is a central tenet of DL and the Semantic Web.
Essentially, though we list certain genes within a segment, it is not automatically guaranteed that these are the only genes unless we explicitly say so.
Knowledge that is unstated is not presumed to be false; it is merely presumed to be missing.
Attaching elements of the genome to a particular location is problematic in sequence annotation, as exact coordinate locations will always vary between individual members of a species.
For this reason, model sequences have been developed for a number of organisms.
Even where such sequences do exist, however, there will always be incompatibility between builds or versions of the model.
Previously, sequence annotations using SO terms have handled the description of individual sequence features outside of the ontology.
CHADO, for example, mitigates the issue of exact coordinates by abstracting location away from features through the use of a featureloc object.
For our purposes, however, because we perform reasoning on the locations of features, we must incorporate coordinates from an individual build into our knowledge base.
Thus, we needed to create a few relationships to specify exact feature location.
We stored genome loci using the SO class nuclear_sequence.
This class served as the domain for five new properties which help to spell out an exact location: in_build, on_chromosome, has_start_point, has_end_point and on_strand.
The RO property located_in is then used to link a sequence feature to its location.
The in_build property specifies which version of the model sequence we are using.
We declared custom datatypes to limit the legal values for chromosomes (122, X, Y) and for strands (positive, negative).
Our instances of the classes pseudogene (and its subclasses), protein_coding_gene and sd_segment all make use of these properties to specify their precise locations on the genome.
An outline view of the basic classes and relationships in our ontology is provided in Figure 1.
Using SO, one annotates a feature by assigning it an instance of one of the subclasses of sequence_attribute.
This is accomplished using the has_quality property.
Sequence attributes currently defined in SO are for labeling conditions that are either present or absent.
We found that many of the attributes we wanted to use took numerical values, be they counts, ratios or scores.
We decided to create a subclass of sequence_attribute for these types of attributes called sequence_numerical_attribute.
This new class serves as the domain for a has_numerical_value property which is used to represent the data value.
To help organize our attributes, we created a pseudogene_attribute class akin to the SO class gene_attribute.
We further subclassed this with a pseudogene_numerical_attribute Fig.1.
Diagram showing the relationships between some of the base classes of the ontology.
Dashed lines are used to indicate subclass relationships.
Regular lines indicate property relationships.
Classes in SO are highlighted in gray, while those which were added to our ontology have a white background.
Fig.2.
Diagram showing the hierarchy of annotation attributes for our pseudogene ontology.
The dashed lines denote subclass relationships.
Classes from SO are highlighted in gray while classes add by our ontology have a white background.
class.
Specific classes of attribute were created using this hierarchy for the counts (number_of_insertions, number_of_deletions, number_of_stops, number_of_shifts, disablements and polyA) and scores (log_kimura_score, fraction, evalue and identity) we wanted to track.
Figure 2 illustrates the hierarchy of pseudogene annotation attributes.
With the base relationships of our ontology in place, we created a set of SWRL rules to infer new relationships based upon them.
Their design was guided by the goals of our research and they will be discussed in detail in Section 5.
Having defined the terms and relationships of our ontology, we now populated it with instances of pseudogenes, parent genes and SDs.
We then performed consistency and satisfiability checking using the reasoner and classified the data.
At this point, we i73 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i74 i71i78 M.E.Holford et al.
were able to query the resulting knowledge base to answer biological questions.
Here, we were presented with two options.
We could leave the data in the reasoner and query against it programmatically.
Or we could export an XML document containing our fully entailed data and import it into a triple store, a piece of software that functions like a database for relational data.
In both cases, we would be querying the data using a variant of the SPARQL language (SPARQL, 2008), and RDF query language comparable with SQL for relational data.
Generally, triple stores do not support the degree of DL reasoning that actual reasoners do.
In fact, the plain SPARQL language does not expect data to be reasoned upon.
Reasoners typically use a reasoning-enhanced version of SPARQL such as Pellets SPARQL-DL.
The tradeoff however is speed; because they do not need to perform reasoning, triple stores can generally retrieve results much more quickly.
3 METHODS Data about pseudogenes were obtained from the pseudogene.org (Karro et al., 2007) website.
These data reference proteins by Ensembl identifiers in defining parent proteins for the pseudogenes.
We obtained the links between these proteins and the genes that code for them using the Ensembl (Hubbard et al., 2002) website.
To assign an exact location to a parent gene, we used the lowest start value and the highest stop value for all the transcripts of the gene.
Information on SDs was obtained from the web resources of the Eichler lab (Duplication, 2010).
The data were parsed from flat files using a custom program written in Java.
This program employed the OWLAPI library (Bechhofer and Philip Lord, 2003) to build an OWL 2 compatible ontology which included the SWRL rules that we used.
The program used the Pellet reasoner to check the ontology for consistency and satisfiability and then to classify it.
The full set of entailments generated by the reasoner were serialized into an OWL document.
This document was then loaded into the open-source version of Virtuoso, a universal database which includes a triple store (Virtuoso, 2010).
Because the full set of inferences was pre-created, we were able to take advantage of Virtuosos fast performance without losing the advantages of reasoning.
Virtuoso provides an HTTP interface to a SPARQL endpoint, which we were able to query through either a web interface or programmatically.
A key focus of our research is to explore the relationship between pseudogenes and SDs and determine what this can teach us about the evolution of the genome.
We were particularly interested in finding examples of two scenarios.
In the first (Case 1), we sought to find situations that would allow us to directly compare the evolution rate of a pseudogene and its parent gene.
To do this, we would need to find cases where a pseudogene and its parent were located on the separate segments of an SD pair.
Additionally, we needed to verify that no other pseudogenes or genes were on the same segment as that containing the parent gene.
We could then examine the relative substitution rates between the pseudogene and its surrounding area and the parent gene and its surrounding area.
We did this using the Kimura score metric.
If the log of this value fell within a specific range of values, we could argue that the pseudogene was evolving more rapidly, less rapidly or at an equal rate as its parent.
In the second scenario (Case 2), we tried to find pseudogenes that arose not from duplication of a parent gene but from duplication of another pseudogene.
To find these, we again needed to locate cases where a pseudogene and its parent were on the separate segments of an SD pair.
In this case, however, we wanted other pseudogenes to be present on the segment containing the parent gene.
We then looked at whether the original pseudogene was aligned with its parent or with another pseudogene.
In the latter scenario, we were able to conclude that the pseudogene arose from duplication of another pseudogene and suggested that it formed a new category, the duplicated-processed pseudogene.
It is worth noting the possibility of other scenarios, for example, the orginating pseudogene may be located on a third duplicate segment distinct from that of the duplicated pseudogene and the parent gene.
More complex possibilities such as these are discussed in detail in Khurana et al.
(manuscript in preparation).
These cases suggest a sort of flowchart which can be traversed by a series of rules which build upon each other.
We list these rules in Figure 3.
The flowchart can be seen in Figure 4.
Further discussion of this decisions tree, including the strategies employed and their biological rational can be found in the forth-coming paper by Khurana et al.
We created a total of seven rules to reach our goal.
Rule 1 is a foundational rule, necessary for all that follow it.
Its goal is to mark all pseudogenes that are in the segment of an SD pair whose parent gene is located in the other segment.
These pseudogenes are assigned the property has_parent_in_duplicate_segment whose value is the segment of the parent.
Rule 2 uses Rule 1 to find parent segments and then checks the has_gene_count and has_pseudogene_count values to determine if other genes or pseudogenes are present in the segment.
If these other features are present, the pseudogene is given the property has_not_only_parent_in_duplicate_segment.
Rule 3 functions similarly except that it expects the has_pseudogene_count value to be 0 and the has_gene count to be 1 (the parent gene).
Matching pseudogenes are given the property has_only_parent_in_duplicate_segment.
At this point, we can move to directly answer the questions raised by Case 1.
Rule 4 uses Rule 3 to find segments containing only the parent gene.
It then retrieves the log of the Kimura score of the pseudogene.
If its value is above a high cutoff, it determines the pseudogene is under positive selection and assigns it the sequence attribute maybe_positively_selected.
Rule 5 behaves as Rule 4, except that for log Kimura scores below a low cutoff, it assigns the pseudogene the maybe_negatively_selected attribute.
The path taken by Rule 5 is illustrated in Figure 5.
Rule 6 closes out Case 1 by assigning maybe_neutrally_selected to eligible pseudogenes with log Kimura scores between the high and low cutoff values.
We chose 0.4 and 0.4 as high and low cutoffs, as these correspond with the distribution of scores for all pseudogenes (Khurana et al., manuscript in preparation).
Rule 7 handles Case 2 by comparing the alignment of the pseudogene and its parent gene with the alignment of the pseudogene and the other pseudogenes on the duplicate segment.
It uses Rule 2 to find pseudogenes on one segment of an SD pair whose parent gene is on the other segment along with other pseudogenes.
It then uses the SWRL built-in arithmetic capabilities to measure the distance from the start of pseudogene (p1) to the start of its segment (p1dist).
It then looks at the distance from start of the features on the duplicate segment.
If the distance from start for one of the other pseudogenes (p2dist) is closer to p1dist than the distance from start of the parent gene is and if p2dist is within close enough range of p1dist (within the length of p1), it is determined that p1 is aligned with the pseudogene on the duplicate pair.
This allows us to spot potential duplicated-processed pseudogenes which can be given the property aligned_to_pseudogene.
Figure 6 indicates the path traversed by Rule 7.
4 RESULTS With a fully entailed ontology loaded into a triple store, we were able to issue SPARQL queries to find pseudogenes matching the criteria specified by Cases 1 and 2.
In both cases, the SPARQL queries are quite simple and direct.
Recall that in Case 1, we are trying to compare the evolution rate of a pseudogene with that of its parent.
Using Rules 16, we isolated pseudogenes and parent genes which occur on the segments of an SD pair, making sure that no other genes or pseudogenes were present on the segment containing the parent gene.
By analyzing the substitution rate we attached a quality to the pseudogene indicating whether it might be positively, negatively or neutrally selected.
We can now find pseudogenes of i74 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i75 i71i78 SWRL reasoning on pseudogenes Rule Antecedents Consequents R1-gene p has parent gene g p has_parent_in_duplicate_segment d p in segment s s has SD pair d d contains gene g R2-gene p has parent in duplicate segment d p has_not_only_parent_in_duplicate_segment d gene-count(d) > 0 pseudogene-count(d) > 0 R3-gene p has parent in duplicate segment d p has_only_parent_in_duplicate_segment d gene-count(d) = 1 pseudogene-count(d) = 0 R4-gene p has only parent in duplicate segment d p has_quality MaybeUnderPositiveSelection Kimura-score(p) >= 0.4 R5-gene p has only parent in duplicate segment d p has_quality MaybeUnderNegativeSelection Kimura-score(p) <=-0.4 R6-gene p has only parent in duplicate segment d p has_quality UnderNeturalSelection Kimura-score(p) >-0.4 and < 0.4 R7-gene p has not only parent in duplicate segment d p aligns_with p2 p in segment s p is pdist from start of s p has parent gene g g is gdist from start of d-gene p2 in segment d p2 is p2dist from start of d abs(p2dist-pdist) < abs(gdist-pdist) abs(p2dist-pdist) < length(p) Fig.3.
Informal pseudocode description of the rules implemented in SWRL to traverse the flowchart.
Fig.4.
The decision tree to be traversed by SWRL rules.
Dashed lines indicate a No answer; solid lines indicate a Yes answer.
The same convention is used in Figures 5 and 6. interest by naming those that possess this quality.
For example, to find quickly evolving pseudogenes one might issue the following: SELECT ?p WHERE ?p #has_quality #maybe_positively_selected Fig.5.
The path traversed by Rule 5 on the decision tree.
This path follows Case 1 in looking to examples of pseudogenes which evolve at a less rapid pace than their parent genes.
For the sake of brevity, we are skipping the necessary import statements.
In Case 2, we used Rule 7 to locate pseudogenes which were derived from the duplication of another pseudogene rather than a parent gene.
These were found by testing the alignment of a pseudogene to other pseudogenes present on the same segment of the SD pair as the parent gene.
A property relationship was created between these aligned pseudogenes.
To find examples of i75 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i76 i71i78 M.E.Holford et al.
Fig.6.
The path traversed by Rule 7 on the decision tree.
This path follows Case 2 in looking for pseudogenes which have arisen from the duplication of another pseudogene rather than their parent gene.
these duplicated-processed pseudogenes, we ask for pseudogenes fulfilling this relationship: SELECT ?p WHERE ?p #aligned_to_pseudogene ?p2 As a result of this query, we discovered that PGOHUM00000154773 is potentially a duplicated-processed pseudogene, as it is more closely aligned with the pseudogene PGOHUM00000154773 than to its parent gene, ENSG00000205946.
This relationship is illustrated in Figure 7.
5 DISCUSSION Because of their status as genomic fossils, pseudogenes are of interest not only for how they currently appear but how they arose and developed.
Much like examining and dating bones to a paleontologist, the issue of ascertainment is central to the student of pseudogenes.
In this, a certain amount of uncertainty is inherent.
For a number of pseudogenes, we can precisely describe their origin and place in time; for others we are less certain.
We can see an example of this in Case 1 above, where a higher substitution rate suggests that a pseudogene may be positively selected it also raises the possibility that the surrounding region is negatively selected.
We cannot say with full certainty which possibility is the case.
The handling of uncertainty is a problematic issue when formally describing pseudogenes.
OWL and most other mainstream ontology languages do not deal with the concept of probability with respect to knowledge.
This is largely because DL itself only deals with data that is certain.
Other branches of logic exist to handle situations of uncertainty, such as fuzzy or probabilistic logic and extensions to OWL have been proposed to build knowledge bases using these logics (Ding and Peng, 2004).
At present, however, these are confined to the more experimental reaches of knowledge representation studies.
The alternative would be to define terms using a conventional ontology to represent different levels of certainty with Fig.7.
A potential duplicated-processed pseudogene found by aligning one pseudogene with another on the same segment as the parent gene.
The pseudogene, PGOHUM00000154773, is located on chromosome 8 of the reference sequence between bases 7199348 and 7200542.
Its parent gene, ENSG00000205946 (USP17L6P), is found on chromosome 4 between bases 8978698 and 8979894.
PGOHUM00000154773 is found on an SD segment located between 7199348 and 7200542 on chromosome 8.
The parent gene is on the duplicate segment located between 8966987 and 9017856 on chromosome 4.
The duplicate segment also contains another pseudogene, PGOHUM00000149316 between bases 8992177 and 8992537.
Because this other pseudogene is a similar distance from the start of the segment as PGOHUM00000154773 is to the start of its segment (25 190 bp versus 24 249 bp) and the parent gene is in a different portion of the segment (11 711 bp from the start), the deduction that PGOHUM00000154773 is aligned to PGOHUM00000149316 rather than ENSG00000205946 makes sense.
This was found by applying SWRL Rule 7. regards to ascertainment.
As our knowledge base grows, we hope to explore this area more fully.
Performance presents another challenge to builders of biomedical ontologies.
Although OWL-DL guarantees computational tractability, it does not promise that classification can be completed using an amount of time and memory that we may find acceptable.
It is also an unfortunate truth that computational expense increases as an ontology becomes more expressive.
These problems are particularly acute for genomics researchers, where vast amounts of data can quickly bog down a DL reasoner even on a well apportioned machine.
For example, we initially ran out of memory while trying to classify our ontology using the full set of pseudogenes.
This occurred even when running the reasoner on a 32 GB server.
After some experimentation, we were able to get the ontology to classify by performing two steps.
First, we removed all properties that were not used for the creation of entailments for production rules, generating the full set of inferences and then re-inserting the non-essential properties.
Second, we changed individual instances of protein_coding_gene to a custom class SimpleGene which extends SOs biological_region class.
This freed the reasoner from applying the restrictions defined by SO for protein coding genes and saved considerable amounts of memory.
We felt this workaround was acceptable because our production rules do not make use of these restrictions.
After the reasoner had finished generating entailments, we added an assertion declaring SimpleGene a subclass of protein_coding_gene, thus allowing future inferences to i76 [11:03 12/5/2010 Bioinformatics-btq173.tex] Page: i77 i71i78 SWRL reasoning on pseudogenes Fig.8.
Informal depiction of the coverage provided by the current ontology, including portions derived from SO, as well as areas to be covered in future work.
In the diagram, plain lines indicate class hierarchy (is-a) relationships, while dashed lines indicate property (has-a) relationships.
be drawn upon the genes using SO.
After applying these techniques, classification of the full set of pseudogenes took around 19 min using a 28 GB heap size.
A diminished set containing one of every 10 pseudogenes took around 11 min to classify and the set containing one of every 4 took around 13 min.
It is abundantly clear that at present our approach could not be used for large-scale annotations, such as that of an entire genome.
For large amounts of data, the integration of SO terms with relational database technology through the CHADO schema and CHADO-XML offer a far quicker solution.
It is promising that RDF data can be queried quickly using triple stores, but the process of creating the full set of entailments by classifying the data through the reasoner is still a significant performance bottleneck.
We can only hope that the future will continue to bring performance improvements in this area, both through more efficient algorithms and faster technology.
The ontology we have presented here is an extension of SO that joins additions related to pseudogenes with additions related to SDs.
It forms a useful prototype for describing pseudogenes and provides a useful framework for reasoning and drawing biological inferences.
It stops short, however, of providing a canonical ontology of the domain of pseudogenes.
As part of our future research, we intend to build upon the structure presented here to form a more complete ontology.
For example, it would be useful to add classes and relationships to describe pseudogene characteristics such as regulatory and transcribed.
These terms could be incorporated from previous work by Lam et al.
(2009).
We also wish to incorporate the notion of derivation of a pseudogene, whether from the nucleus or mitochondria.
We hope to enlist the support of other pseudogene researchers in this endeavor.
Finally, we see the potential for further development leading to an ontology of SDs.
Figure 8 illustrates the present coverage of our ontology and areas we hope to include in the future.
6 CONCLUSION We used the SO to build a knowledge base of pseudogenes, extending SO terms where necessary to describe our data and borrowing identifiers from the PGO ontology.
We created a series of custom SWRL rules to find situations of interest involving our research on the relation between pseudogenes and SDs.
Using these rules and the inherent capabilities of DL reasoners, we were able to infer new relationships about our existing data.
We moved this fully entailed knowledge base into a triple store with a SPARQL endpoint to allow us to query it for biologically relevant information.
Funding: The National Institutes of Health and AL Williams Professorship funds; National Institute of Health grants P01 DC04732 and R01 DA021253 (to K.C.).
Conflict of Interest: none declared.
ABSTRACT Motivation: The exponential growth of protein sequence databases has increasingly made the fundamental question of searching for homologs a computational bottleneck.
The amount of unique data, however, is not growing nearly as fast; we can exploit this fact to greatly accelerate homology search.
Acceleration of programs in the popular PSI/DELTA-BLAST family of tools will not only speed-up hom-ology search directly but also the huge collection of other current programs that primarily interact with large protein databases via pre-cisely these tools.
Results: We introduce a suite of homology search tools, powered by compressively accelerated protein BLAST (CaBLASTP), which are sig-nificantly faster than and comparably accurate with all known state-of-the-art tools, including HHblits, DELTA-BLAST and PSI-BLAST.
Further, our tools are implemented in a manner that allows direct sub-stitution into existing analysis pipelines.
The key idea is that we intro-duce a local similarity-based compression scheme that allows us to operate directly on the compressed data.
Importantly, CaBLASTPs runtime scales almost linearly in the amount of unique data, as opposed to current BLASTP variants, which scale linearly in the size of the full protein database being searched.
Our compressive algo-rithms will speed-up many tasks, such as protein structure prediction and orthology mapping, which rely heavily on homology search.
Availability: CaBLASTP is available under the GNU Public License at Contact: bab@mit.edu 1 INTRODUCTION Identification of homologous sequences is of fundamental im-portance in computational biology.
Sequence search tools, such as BLASTP and PSI-BLAST (Altschul et al., 1997), have played important roles in various tasks arising in protein science, includ-ing secondary and tertiary structure prediction (Rost et al., 2004; Soding et al., 2005), functional annotation (Kosloff and Kolodny, 2008; Loewenstein et al., 2009) and orthology mapping (Singh et al., 2008; Tatusov et al., 2000).
The runtimes of the most popular methods [e.g.
BLASTP, PSI-BLAST and DELTA-BLAST (Boratyn et al., 2012)] scale nearly linearly in the size of protein databases.
With the exponential increase in protein se-quence data, this is becoming a major bottleneck to computa-tion.
Thus, it is imperative to design algorithms that scale sub-linearly in the size of the databases.
The recent exponential growth in genomic sequence data (Kahn, 2011; Kircher and Kelso, 2010), which is outpacing growth of computing power (Gross, 2011; Huttenhower and Hofmann, 2010; Kahn, 2011; Schatz et al., 2010), has spurred an interest in compressive genomics (Loh et al., 2012) and the need to compress sequence data for efficient storage (Brandon et al., 2009; Cameron et al., 2007; Chen et al., 2002).
Protein sequence data, although on a slower growth curve than genomic data, nonetheless increase at an exponential rate (Fig.A1), dou-bling roughly every 2 years, for now just keeping pace with Moores law for computational power.
A key observation from compressive genomics is that much of the new data are actually similar to existing data, which was used to accelerate nucleotide sequence search without loss of accuracy (Loh et al., 2012).
Despite its name, even NCBIs non-redundant protein se-quence database (NR) contains a great deal of redundancy; it is non-redundant only at the level of entire sequences; highly similar sequences are represented separately.
Thus, even NR lends itself to a compression scheme that takes advantage of this redundancy.
Although NR has already eliminated exact du-plicates at the global sequence level, we take advantage of local sequence similarity to achieve compression.
We introduce a compressive algorithm, CaBLASTP, along with an implementation that allows direct computation on the compressed data.
CaBLASTP boosts the runtime performance of any search tool in the protein BLAST (Altschul et al., 1997) family, while maintaining accuracy.
Specifically, we show that compressive versions of BLASTP, PSI-BLAST (Altschul et al., 1997) and DELTA-BLAST (Boratyn et al., 2012) scale nearly linearly in the size of the unique data, as well as sub-linearly in the size of the complete protein database.
Notably, any program that relies on protein BLAST can take advantage of our compressive software with virtually no effort.
Thus, we expect CaBLASTP to be of great use to the community.
2 METHODS We introduce a framework for compressing protein sequences and per-forming a variety of homology search techniques in compressed space.
We have designed this CaBLASTP framework primarily to be compat-ible with the NCBI-BLAST family of software (Altschul et al., 1997).
The key observation underlying CaBLASTP is that when sequences are sufficiently similaryet not necessarily identicaltasks such as ap-proximate search can initially operate on just one representative of the similar set.
The remainder of the set need only be analyzed if a represen-tative sequence is found to be of interest.
The basic approach of CaBLASTP thus consists of two phases.
First, a pre-processing (or compression) phase identifies similarities among se-quences in a protein database.
This phase is computationally intensive, yet it need be done only once for a given database.
After compression is*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com mailto:bab@mit.edu complete, CaBLASTP can then translate the decreased redundancy of the database into a speed-up when performing search, which is the second phase.
CaBLASTP compresses a protein sequence database to identify re-gions of high similarity (Fig.1a).
This is done by first scanning through the database and categorizing sequences as either new or redundant.
Owing to both the amino acid alphabet size and the sheer database size, to become tractable, this step required the development of new com-putationally efficient methods (detailed later in the text).
Novel sequences are stored in a coarse database, whereas sequence segments that align well to previously seen sequences are not.
The coarse database essentially represents only the unique data from the original database.
Instead, records for these alignments are added to a link index.
Our approach can be viewed as a hybrid between traditional data-compression algorithms, which create a dictionary for exact se-quences encountered in the data, and sequence alignment algorithms, such as BLAST (Altschul et al., 1997).
The search phase applies a two-stage approach (Fig.1b).
First, the query is searched against the coarse database.
To maintain accuracy, this coarse search uses a more permissive E-value threshold than the thresh-old specified for final results.
For each hit from the coarse search, CaBLASTP then reconstructs any additional hit candidates by following the links in the link index.
Final results are then obtained by a fine search against these candidate sequences.
We have implemented a compression tool, which converts a protein sequence database to a CaBLASTP compressed database, as well as three compressive search tools that operate on this database, implementing compression-space versions of NCBI-BLAST, PSI-BLAST and the re-cently released DELTA-BLAST (Boratyn et al., 2012).
Our software is written in the publicly available Go programming language (Griesemer, 2009; Kortschak, 2011).
2.1 Compression The compressive phase takes a protein sequence database and produces a compressed data structure amenable to the search step described in Section 2.2.
This pipeline is illustrated in Figure 1a.
This is implemented in the program cablastp-compress, which takes a standard FASTA file as input.
Given an input sequence database, compression proceeds as follows: (1) First, initialize a table of all possible k-mer seeds of amino acids, and a coarse database of amino acid sequences, initially contain-ing the first sequence in the input database (empirically, the best compression runtime performance occurs with k set to 4).
(2) For each k-mer of the first sequence, then create a pointer from the corresponding entry in the seed table to the position of that k-mer in the first sequence.
(3) For each sequence s in the input after the first, slide a window of size k k0, where k0 may be zero (empirically, best performance is achieved with k0 set to 2).
(4) Low-complexity regions (single-residue repeats) of length410 are skipped.
(5) Look up the first k residues of this window in the seed table.
For every pointer corresponding to that k-mer in the seed table, follow it to a subsequence in the coarse database.
If a resulting subse-quence s0 in the coarse database further matches the window by the additional k0 residues, then attempt extension (see below).
If no subsequences from this window can be extended, move the window by one residue.
The separation of the window size into k and k0 is simply an optimization to reduce the memory footprint of compression; it allows, for example, an effective window size of 6 while only requiring a seed table with 204 rather than 206 entries.
(6) If a match was found via extension, move the k-mer window to the first k-mer in s after the match, and the extension process repeats with this new seed.
Extension.
Given a k k0 match between the sequence s and subse-quence s0 pointed to by the seed table, first attempt ungapped extension: (1) Greedily extend the match into an ungapped alignment as far as possible.
(2) Within each window of 10 residues, if identical 4mers in s and s0 can be found, and at least two additional matching residues can be (a) (b) Fig.1.
(a) Novel sequences are stored in a coarse database, whereas sequence segments that align well to previously seen sequences are not.
Instead, records for these alignments are added to a link index.
Our ap-proach can be viewed as a hybrid between traditional data-compression algorithms, which create a dictionary for exact sequences encountered in the data and sequence alignment algorithms, such as BLAST.
Links point from entries in the seed table to entries in the coarse database.
Blue text indicates matching subsequences; red indicates differences.
(b) The search phase applies a two-stage approach.
First, the query is searched against the coarse database.
To maintain accuracy, this coarse search uses a more permissive E-value threshold than the threshold specified for final results.
For each hit from the coarse search, CaBLASTP then recon-structs any additional candidate hits by following the links in the link index.
Final results are then obtained by a fine search against these candidate sequences i284 N.M.Daniels et al.
found, then there is an ungapped match within that 10mer window between s and s0 that exhibits at least 60% sequence identity.
(3) Continue ungapped matching using 10mer windows until no more 60% identity 10mers are found.
(4) The result of ungapped extension is that there is an alignment between sequences s and s0 with no insertions or deletions, only matches and substitutions, and at least 60% of the positions con-tain exact matches.
When ungapped extension can no longer proceed, switch to gapped extension.
From the end of the ungapped alignment, align 25mer win-dows of both s and s0 using the NeedlemanWunsch (Needleman and Wunsch, 1970) algorithm with BLOSUM62 as a cost matrix.
We use a variant of NeedlemanWunsch, implementing constrained dynamic pro-gramming, prohibiting more than six gaps in the alignment, reducing the search space by a factor of 4.
Global alignment is chosen because we wish to attempt to align the entire 25mer from each sequence.
After gapped extension on a window length of 25, attempt ungapped extension again.
When neither gapped nor ungapped extension can continue, terminate extension.
Realign the resulting extension of s and s0, again using NeedlemanWunsch.
If the resulting alignment has570% sequence iden-tity or is540 residues, discard it, instead attempt extension on the next link in the seed table for the original k-mer; if there are no more links for that k-mer, then consider the next k-mer.
If, however, the resulting align-ment has at least 70% sequence identity and is at least 40 residues long, then create a link from the entry for s0 in the coarse database to the subsequence of s beginning with the original k-mer and corresponding to the extended region.
If there are dangling ends to s530 residues that did not satisfy the extension criteria, append them to the match.
Longer dangling ends that did not match any subsequences reachable from the seed table are added into the coarse database themselves, with links from the relevant seeds in the seed table to their constituent k-mers.
The re-quirement to deal with protein sequences being discrete represents a dif-ference from Loh et al.
(2012).
Any sequence or subsequence in the input that cannot be matched to earlier sequences in the coarse database will itself become an entry in the coarse database, with pointers from the k-mer seed table linking to it, and similar sequences seen later in the input may be matched to it.
In addition, a difference script is associated with this link.
The differ-ence script is simply a representation of the insertions, deletions and substitutions resulting from the overall NeedlemanWunsch alignment.
Applying the difference script to a representative sequence in the coarse database (s0 above) will return the sequence s; it is effectively decom-pressed.
Similarly, applying the difference script to s will return its rep-resentative s0.
After all sequences have been compressed, the sequences in the coarse database are written out in FASTA format; the resulting coarse FASTA file, which is smaller than the original input file, is used by all search implementations described later in the text.
In addition, the set of links between coarse sequences and original sequence identifiers and their dif-ference scripts is written to disk in a binary format.
An index file is also produced, which maps the sequence identifiers from the coarse database to entries in the compressed database.
These formats are documented in the Go source code for CaBLASTP.
It is worth noting that the compres-sion format is lossless and completely invertible; it is possible to exactly reconstruct the original FASTA source from the compressed database.
When compressing a large amino acid data set such as NCBIs NR, memory usage can grow large.
As a memory and runtime performance optimization, the seed table can be reset when it reaches a user-specified size, 8 GB by default.
For our experiments, we used a maximum seed table size of 20 GB.
When no limit was imposed, the seed table could grow to440 GB on NR, but we saw negligible difference in compression ratio between these two limits.
On the compressed database described here, we have implemented three search techniques, BLASTP, PSI-BLAST and DELTA-BLAST.
All three follow the same basic two-step technique (Fig.1b): first, they search the compressed database with a relaxed threshold to find candi-date matches, and then the closely related sequences to the candidate hits are more closely examined.
The fundamental speed-up introduced by this two-step approach is that the initial step rules out the vast majority of the original database without ever having to examine it.
2.2 Search 2.2.1 Compressive BLASTP Compressive accelerated BLASTP, or cablastp-search, requires a compressed database produced by our com-pression method as described earlier in the text.
Given a query sequence and a compressed database, this search method calls the BLASTP pro-gram to search the coarse FASTA file, which is typically much smaller than the original FASTA file.
This step is called coarse search, as sug-gested by Loh et al.
(2012).
Coarse search uses a relaxed E-value thresh-old compared with what would be desired if the entire original database was searched using standard BLASTP.
The idea behind coarse search is to identify possible hits, which may be rejected by the later fine search.
Because the coarse FASTA file is a subset of the original, uncompressed FASTA file, potential hits may be subsequences that are shorter than or slightly different from the original sequences they represent.
Thus, a more permissive E-value must be used.
Command-line arguments to be passed to BLASTP itself may be specified by the user.
The results of the coarse search are sequences from the coarse FASTA file; thus, they are actually sequences or subsequences from the original FASTA file.
Based on the compressed databases search index, each of these sequences is then re-constructed into all corresponding sequences from the original database, by following the links to original sequence matches, and applying their difference scripts.
Note that the coarse FASTA file need not ever be decompressed in its entirety, although it is possible to do so.
The resulting set of sequences, larger than the resulting set from the coarse search, is then provided to BLASTP as the subject for a second query, which again uses the query sequence provided to cablastp-search.
This step is called fine search, and it produces a set of final results, based on an E-value threshold specified by the user (or the BLASTP default).
These results are provided in an identical format to BLASTP.
This implementation of cablastp-search relies on the BLAST implementation (developed and tested against BLAST 2.2.6 and 2.2.7).
2.2.2 Compressive PSI-BLAST Compressively accelerated PSI-BLAST, or cablastp-psisearch, operates much like compressively acceler-ated BLASTP.
PSI-BLAST builds a position-specific scoring matrix, or PSSM, iteratively, by running BLAST searches for a query against a database.
Instead of just using the BLOSUM-62 matrix to compute align-ment scores, PSI-BLAST computes substitution scores column-by-column, based on an initial alignment and subsequent refinements.
cablastp-psisearch takes advantage of the PSI-BLAST programs ability to save a checkpoint of its PSSM to a file.
Given a user-specified number of iterations, the program performs both a coarse and a fine search for each iteration.
Every iteration, except the first, relies on a PSSM file output by the previous iteration, whereas every iteration, except the final, writes a PSSM file for the next iteration to use.
Each iteration comprises a coarse and a fine search identical to cablastp-search, but using the PSI-BLAST executable.
2.2.3 Compressive DELTA-BLAST Domain-enhanced look up time accelerated BLAST, or DELTA-BLAST (Boratyn et al., 2012), uses a library of pre-computed PSSMs based on NCBIs Conserved Domain Database.
The DELTA-BLAST executable is included with BLAST 2.2.6 and later versions.
Compressively accelerated DELTA-BLAST, or cablastp-deltasearch, operates similarly to compressively accelerated BLASTP, performing a single iteration of search comprising i285 Compressive genomics for protein databases a coarse and a fine search step.
We did not implement an iterative version of this algorithm, as Boratyn et al.
(2012) showed decreased accuracy with iteration.
2.3 Accuracy validation To verify that compressive acceleration does not significantly harm the accuracy of BLASTP, PSI-BLAST and DELTA-BLAST, we performed 100 random searches against the NR database, for each of these three tools.
For each tool, we treated the results from the standard version (e.g.
BLASTP) as a gold standard, and computed the true positive rate and false positive rate for compressive versions of the same search (e.g.
cablastp-search) with respect to this gold standard.
We performed this search with an E-value threshold of 103, for both the coarse and fine threshold for the compressive versions of each search, and for CaBLASTP, PSI-BLAST and DELTA-BLAST.
Because of the design of the algorithm, false positives with respect to the non-compressively accelerated tools are not possible.
We were also interested in homology detection performance of our compressive implementations of PSI-BLAST and DELTA-BLAST with respect to HHblits (McDonnell et al., 2006).
We identified all 1123 se-quences from the ASTRAL subset of release 1.75A of the Structural Classifications of Proteins (SCOP) (Murzin et al., 1995) database that were not present in HHblits NR20 database or the August 2010 NCBI NR database, but whose SCOP families contained other homolo-gous sequences that were present in those databases.
We chose the August 2010 NCBI NR database to more fairly compare with the August 2011 HHblits NR20, which is the most recent available.
We then performed searches using one iteration of HHblits, one iteration of cablastp-deltasearch and two iterations of cablastp-psisearch against these databases.
We chose these numbers of iterations because a single iteration of PSI-BLAST is effectively just BLASTP, whereas Boratyn et al.
(2012) showed decreased accuracy with more than one iteration of DELTA-BLAST.
Multiple iterations of HHblits would have resulted in slower runtime performance.
We considered results from the same SCOP superfamily (and by extension, the same SCOP family) as the query to be true positives, and results from different SCOP folds to be false positives.
We removed results from the same SCOP fold but differ-ent superfamilies, as it is not consistent across the SCOP fold classifica-tions whether those sequences are homologs.
We also removed results that were not identifiable in SCOP.
We plotted ROC curves based on these homology predictions.
We also report the mean running times of these searches.
3 RESULTS 3.1 Scalability on simulated data We first compared the performance of our compressive accelerated versions of BLAST with their original implementa-tions.
We constructed a simulated dataset to mimic the expected growth of a protein sequence database into the future, to demonstrate CaBLASTPs ability to scale to large datasets.
We began with all known and putative proteins in the Saccharomyces Genome Database (Cherry et al., 2012), which contains the proteomes of 21 strains of yeast.
To simulate clades of recently diverged species, we used a tool for simulating protein mutation (Daniels et al., 2012; Kumar and Cowen, 2009, 2010).
For each original sequence in the database, we added 5, 10, 20, 30 or 40 similar sequences by substituting residues with a mutation rate of 20%, based on the BLOSUM62 substitution matrix.
The original dataset contained 6717 sequences; with 40 mutated copies of each sequence, the database contained 275 397 sequences.
In this way, we essentially simulate an evolutionary process to build a number of putative proteomes from Saccharomyces proteomes.
Performance of sequence search on these augmented databases should be comparable with the performance on future databases where closely related species have now been sequenced, producing increasing numbers of orthologous se-quences.
We benchmarked sequence search on these augmented databases.
Figure 2a demonstrates the superior runtime of CaBLASTP over BLASTP for large datasets.
The results are averaged over all sequences from the native Saccharomyces proteome.
The runtime of BLASTP increases almost linearly in the number of simulated proteomes, or the size of the full database.
In contrast, CaBLASTP scales sub-linearly with database size, even when there are 40 times as many proteomes.
Notably, CaBLASTP achieves roughly constant runtime regardless of database size.
These results show that our compressive scheme is able to exploit data redundancy, thereby avoiding redundant searches.
Finally, we have performed similar comparisons on datasets with different mutation rates (e.g.
5, 10 and 30%), and the results are similar.
This benchmark was performed on a quad-core Intel Core i7 with 16 GB random access memory and a solid-state disk.
3.2 Homology search on real data 3.2.1 Speed We evaluated the homology-search performance of both the original and our compressive BLAST versions on the widely used NR database.
We randomly chose 100 sequences from the December 2012 NR database.
Five runs for each query sequence were performed on three early versions of NR built on June 2010, July 2012 and December 2012, with a coarse E-value of 105 and a fine E-value of 1010 (we selected these three NR datasets because we do not have access to any other versions).
The average runtime for each method is shown in Figure 2b.
This benchmark was run on a system with dual six-core AMD Opteron 2427 processors and 32 GB random access memory, equipped with a RAID-10 disk array.
Although on each NR dataset, BLASTP takes 120, 200 and 240 s, respectively, CaBLASTP takes only 50, 70 and 75 s, re-spectively.
Given that the NR datasets each contain 11.6, 19.1 and 22 million sequences, BLASTP scales almost exactly linearly in database size, whereas the runtime of CaBLASTP grows much more slowly.
CaBLASTP is faster than BLASTP by factors of 2.4, 2.7 and 3.1 on these NR datasets, respectively.
These results fit with the observation that the uncompressed NR databases are 6.1, 11 and 13 GB in size, respectively, whereas their compressed counterparts are 1.4, 2.4 and 2.7 GB in size.
Considering that the NR databases already have 100% global sequence-identity re-dundancy removed, CaBLASTP takes advantage of the local similarity within the databases to speed-up homology search.
It is worth noting that on the NR databases, the coarse search step of CaBLASTP dominates the running time; the fine step requires51 s in all cases.
Similar to the comparison between BLASTP and CaBLASTP, the compressive accelerated versions of both PSI-BLAST and DELTA-BLAST are much faster than their original versions.
We performed two iterations of PSI-BLAST and one iteration of DELTA-BLAST, as suggested in the latters original article.
The acceleration ratio increases as the size of NR grows (Fig.2b).
i286 N.M.Daniels et al.
3.2.2 Accuracy To verify that compressive acceleration does not decrease the accuracy of BLASTP, PSI-BLAST and DELTA-BLAST, we also compared the differences between the sequence hits from the above random query searches with the NR databases for each tool.
Specifically, we compared the overlap between the sequence hits found by the compression-accelerated versions and those identified by the original versions.
It is worth noting that because of the boosting compressive scheme we have designed, our algorithms will not find any se-quences that do not appear in the hits of their original counter-parts.
We then calculated the overlap between the alignments generated by our compression-accelerated tools and their ori-ginal versions.
Table 1 depicts that the overlap of sequence hits is499% and that of alignments is 100%.
In other words, when a (a) (b) (c) (d) Fig.2.
(a) Runtime of CaBLASTP versus BLASTP as datasets grow because of simulated mutation.
Below 20% mutation rate, CaBLASTP run time is virtually constant.
(b) Runtime of cablastp-search versus BLASTP on three historical versions of NCBIs NR database.
Times are the mean of five runs each for 100 randomly chosen queries.
(c) Runtime of cablastp-deltasearch versus cablastp-psisearch (two iterations) on NR from August 2010 and HHblits on NR20 from August 2011.
Times are the mean of five runs each for 100 queries from NR from December 2012.
(d) Relative speed-up of cablastp-deltasearch and cablastp-psisearch (two iterations) versus HHblits (one iteration) on NR from August 2010 and HHblits on NR20 from August 2011 i287 Compressive genomics for protein databases hit is found, the alignment perfectly matches the standard BLASTP alignment.
An analysis of the differences in the search results suggests that short query sequences (540 residues) may in some cases return no hits in the coarse search.
Changing the minimum match length in the compression phase would likely address this issue, yet likely at the expense of a significant fraction of the runtime performance gains.
To better gauge the impact of coarse search E-value on accur-acy, we performed 1000 random queries against the yeast data-base, with a fine E-value of 105 and three different coarse E-values: 101, 103 and 105.
We compared these results with standard BLASTP queries with an E-value of 105.
Figure 3 illustrates the results of this analysis; CaBLASTP is robust to choice of coarse E-value, as long as the coarse E-value is more permissive than the fine E-value.
3.2.3 Comparison with HHblits Finally, we compared the per-formance of homology detection of our compressively acceler-ated implementations of PSI-BLAST and DELTA-BLAST with a recently introduced profile-based search tool, HHblits (Remmert et al., 2012).
By partitioning sequences into clusters based on global sequence similarity, HHblits pre-computes dis-cretized hidden Markov models (HMMs) on each cluster and only searches a query against those HMMs.
In contrast, our compression-accelerated algorithms take the local similarity into account to speed-up sequence search.
For comparison, we identified all 1123 sequences from the ASTRAL subset of release 1.75A of the SCOP (Murzin et al., 1995) database that are not present in HHblits NR20 database or the August 2010 NCBI NR database, and which were in SCOP families that did contain other non-identical sequences in those older NR databases.
We chose the August 2010 NCBI NR database to more fairly com-pare with the August 2011 HHblits NR20, which is the most recent available.
We then performed searches using one iteration of HHblits, one iteration of cablastp-deltasearch and two iter-ations of cablastp-psisearch.
The numbers of iterations were chosen to ensure the performance of these tools is similar accord-ing to previous reports (Boratyn et al., 2012; Remmert et al., 2012).
We considered top sequence hits from the same SCOP superfamily (and by extension, the same SCOP family) as the query to be true positives, and hits from different SCOP folds to be false positives.
We removed sequence hits from the same SCOP fold but different superfamilies, as it is questionable whether those sequences are homologous.
We also removed re-sults that were not identifiable in SCOP.
We reported the mean running times of these searches and plotted ROC curves based on the homology predictions.
Figure 2c illustrates these results.
Finally, we reported the speed-up of cablastp-deltasearch and cablastp-psisearch with respect to HHblits.
Speed-up is calcu-lated as the mean, over all queries, of the mean HHblits time for a given query divided by the mean time for the specified search for that query.
Error bars represent a 95% confidence interval based on the distribution of search times for each query sequence.
Figure 2d illustrates these results.
HHblits takes an average of 102 s for one iteration.
cablastp-deltasearch takes an average of 51 s for one iteration.
cablastp-psisearch needs 52 s for one iteration and 106 s for two iterations.
Compression-accelerated DELTA-BLAST is twice as fast as HHblits on this test; CaBLASTP-PSI-search is slightly slower than HHblits.
The result is notable considering that the clustered NR20 by HHblits is much smaller than the NR database we used.
Moreover, as shown in Figure 4, compressive DELTA-Fig.3.
Analysis of missed BLASTP hits.
One thousand queries were run on the yeast genome database at three different coarse E-values and a fine E-value of 1E-5.
The majority of misses are at the margin; in total, these represent50.5% of the hits Fig.4.
Accuracy: ROC curves for homology detection performance of cablastp-psisearch versus cablastp-deltasearch and HHblits, as well as standard PSI-BLAST and DELTA-BLAST Table 1.
Accuracy of compressive tools Program TPR (%) FPR (%) Alignment accuracy (%) Compressive BLASTP 99.4 0 100 Compressive PSI-BLAST 99.3 0 100 Compressive DELTA-BLAST 99.4 0 100 Note: TPR is the fraction of hits from standard versions of each tool that were also found by the compressive versions.
FPR is the fraction of hits from the compressive versions that were not found by the standard versions.
Note: because of the algo-rithm design, false positives with respect to the standard uncompressed tools are not possible.
i288 N.M.Daniels et al.
BLAST achieves an area under the ROC curve of 0.76, com-pared with 0.75 for HHblits and 0.69 for compressive PSI-BLAST.
In a ROC5 analysis (Fig.A2), where only the area under the curve up to the fifth false positive is considered, and the area is normalized, compressive DELTA-BLAST achieves a ROC5 score of 0.82, compared with 0.71 for HHblits and 0.63 for compressive PSI-BLAST.
We also ran the original versions of DELTA-BLAST and PSI-BLAST on the same set of query sequences.
Their results are identical to our compression-accelerated versions, but their run-times are roughly three times slower.
4 DISCUSSION We have introduced a compression-accelerated search algorithm that boosts the speed while maintaining accuracy of tools in the protein BLAST family.
Our approach scales sub-linearly with the size of the database being searched, and linearly with the size of the unique data.
We expect that as the NR database con-tinues to grow exponentially, the benefits of this compressive approach will become more pronounced.
In contrast to genomic sequence compression (Loh et al., 2012), which appears on its surface to be similar, subtle differ-ences make protein sequence compression a different problem.
The primary difference is that proteins have a larger alphabet, and thus, random sequences will have less similarity.
This results in different parameters and compression ratios, but it also increases the computational complexity of compression, as the number of k-mers is exponential in the alphabet size.
Another difference is that protein sequences are discrete; therefore, our compression algorithm must handle sequence beginnings and ends.
We have demonstrated that our compressive approach pro-vides significant gains as the redundancy of the data increases, but we also see future challenges.
As the NCBIs NR database continues to grow in the coming years, the size of each cluster of similar subsequences will also grow.
We expect that for compres-sion to remain tractable, further algorithmic and software-engin-eering improvements, for example, a hierarchical compression scheme, will be required.
Many sophisticated homology search and protein structure prediction tools require BLAST searches of one type or another to incorporate sequence profiles or structural information to im-prove performance (Moult et al., 2011).
For example, when we introduced the BetaWrapPro method (McDonnell et al., 2006), which requires a BLASTP search at query time, NCBIs NR database contained 54.5 million sequences; today it contains 422 million sequences; thus, search requires approximately five times the running time.
Although the original motivation for developing our compres-sive approach was the growing running time of BLASTP searches on NR, the results described in Figure 2a suggest that our approach may also be useful for orthology mapping across organisms, performing an all-against-all search between a query proteome and a set of well-studied proteomes (Chen et al., 2007; Hachiya et al., 2009; Moreno-Hagelsieb,G.
and Latimer, 2008), which takes an inordinate amount of time.
Our tools can be readily incorporated into these applications to accelerate their search, pre-processing or library construction.
Our software can be easily interfaced with any programs that use protein BLAST search tools.
Another important advantage of our methods is that the compressed database can be incremen-tally maintained to keep current with new proteomic sequence data.
ACKNOWLEDGEMENTS The authors thank Norman Ramsey and Po-Ru Loh for helpful discussions about the compression approach.
Funding: This work was partially supported by a grant from the Simons Foundation and the NIH (to B.B.).
N.D., A.G. and L.C.
were funded in part by NIH grant (R01GM080330).
M.B.
was funded in part by an NSF MSPRF grant.
Conflict of Interest: none declared.
ABSTRACT Volume-object annotation system (VANO) is a cross-platform image annotation system that enables one to conveniently visualize and annotate 3D volume objects including nuclei and cells.
An application of VANO typically starts with an initial collection of objects produced by a segmentation computation.
The objects can then be labeled, categorized, deleted, added, split, merged and redefined.
VANO has been used to build high-resolution digital atlases of the nuclei of Caenorhabditis elegans at the L1 stage and the nuclei of Drosophila melanogasters ventral nerve cord at the late embryonic stage.
Availability : Platform independent executables of VANO, a sample dataset, and a detailed description of both its design and usage are available at research.janelia.org/peng/proj/vano.
VANO is open-source for co-development.
Contact: pengh@janelia.hhmi.org Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Image content annotation is a basic problem in the analysis of 3D high-resolution cellular and molecular images (Peng, 2008).
Many methods have been developed for determining annotations computationally such as categorizing gene expression patterns (e.g Zhou and Peng, 2007) or predicting cell identities (e.g.
Long et al., 2008a).
Critical to their development is the availability of a corpus of curated training data, and since none of these methods is perfect, their application in the field is benefited by having the ability to manually curate the results they produce.
VANO, short for volume-object annotation system, was developed specifically to allow one to produce annotations manually and to correct or refine the output of good, but not perfect, automated annotation methods.
This tool has and continues to play a critical role in our recent work in building 3D digital atlases of the L1 stage of Caenorhabditis elegans, and the late embryo ventral nerve cord and adult brain of Drosophila melanogaster.
VANO is a cross-platform 3D annotator that provides a spreadsheet of all 3D image objects that is linked to both 3D view of the raw image and a segmentation mask that specifies the voxels that belong to each object (Fig.1).
For a given object, one can label it and set any number of user-defined attributes either in the spreadsheet or from the 3D views.
Moreover, objects can be created, deleted, split, merged or redefined by commands that directly manipulate the segmentation mask.
In typical use scenarios, To whom correspondence should be addressed.
VANO is started (i) on just a raw image with no objects; (ii) with a segmentation mask produced by a computation (e.g.
Long et al., 2007); or (iii) with both a segmentation mask and an initial annotation (i.e.
label and attributes), produced again by an automated computation (e.g.
Long et al., 2008a) or other means.
Image visualization tools such as Amira (amiravis.com) provide for flexible painting or refinement of a segmentation mask but do not support the concept of text annotations thereof.
Existing image-tagging tools such as the annotator of the Open Microscope Environment (www.openmicroscopy.org) permit one to input text descriptions, but only for the entire stack and not for single objects.
VANO allows one to define and annotate an arbitrary collection of 3D image objects in an interactive and efficient way.
Due to space limitations, we briefly describe the design, usage and some applications of VANO.
The Supplementary Material, downloadable at the VANO website, provides detailed documentation.
2 DESIGN VANO is designed to view, select, annotate and modify image objects with a simple and intuitive GUI which consists of two main panelsa 3D image viewer and an annotation table viewerand a number of auxiliary popup dialogs (see Supplementary Material).
The image viewer consists of two tri-views, one for the raw image above and another for the segmentation mask, along with a control panel at right providing display options and widget-based control of these views (Fig.1a).
The three images of a tri-view display the XY, ZY and XZ planes passing through a current focus point that is indicated by the intersection of dashed lines superimposed on each image.
Both tri-views use the same focus point and thus always show the same planes and all views change in correspondence to the movement of this focus point whose location is adjusted by clicking in an image or moving a slider in the control panel.
The annotation table viewer is an editable spreadsheet (Fig.1b) where each row contains the annotation information for an object and each column contains a particular annotation attribute.
A typical annotation entry has standard columns, including object order, label, standard name, user comment, 3D center coordinates, volume and the average, standard deviation, peak voxel intensity and mass (= average intensity volume) of the object.
The later attributes starting with the center are computed directly from the mask image.
Note that mass in effect gives the level of gene expression or protein abundance in application that require it.
In addition, customized annotation columns can be added via a schema file mechanism.
Annotations can be entered directly in this spreadsheet or can be 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
H.Peng et al.
Fig.1.
GUI and applications of VANO in annotating 3D C.elegans and D.melanogaster image stacks.
(a) The 3D image viewer: a tri-view for the subject image (true color, upper left), a tri-view for the mask image (in pseudo-color, lower left) and control panel (right).
The displayed images are confocal stacks of a C.elegans larva (data by Xiao Liu and Stuart Kim).
The worm in this stack was straightened using the algorithm of Peng et al.
(2008).
(b) The annotation table viewer: a spreadsheet where each row is an annotation entry for an image object and each column holds a particular attribute of an object.
In this example each object is a nucleus.
(c) Application of VANO to annotating neuronal patterns in the brain of D.melanogaster.
Left: the subject image where NC82 (red) stains synaptic density and GFP (green) stains a target set of neurons (data by Julie Simpson).
Right: the mask image where each object, in a distinct pseudo-color, is a glia-delineated compartment of the brain (mask by Arnim Jenett).
entered in dialogs that pop-up for the object under the current focus when an E is typed.
VANO uses the model-view-controller paradigm (Reenskaug, 1979) so changes made in one view are immediately reflected to all other views of the underlying data.
VANO is launched with either a subject image stack, in which case the mask and annotations are assumed to be empty, or a previously saved or manually prepared .ano linker file that contains (derived) names of files containing the subject image, the mask image, the annotation and optionally the attribute schema.
The image files are stored in TIFF format, and the annotation is stored in simple comma separated values format so it can easily be parsed and used by other programs such as Excel.
VANO is coded in C++ and uses the platform-independent QT (trolltech.com) GUI library so that it runs under all major operating systems, e.g.
OS X, Windows NT/XP and most variants of Unix.
3 USAGE VANO can be used to (i) create masks and annotations for objects from scratch; (ii) correct the segmentation of an existing mask; or (iii) enter, edit or correct the annotation of the set of objects in the current mask.
The combination of a spreadsheet that lists all objects and pop-ups for individual objects in an image view makes it easy to survey annotation and yet keep track of hundreds of objects as one progresses with the markup of such a collection.
Editing or entering annotation is simply a matter of editing or entering within the spreadsheet or within a text dialog of a pop-up.
Editing segmentation in the general case requires the ability to paint arbitrary sets of pixels.
This is extremely tedious and not really scalable to our scenario.
Fortunately, the objects of interest to us are always globular and sufficiently modeled by a sphere.
So, while VANO supports an arbitrary mask produced by a segmentation program, the simple manual edits supported are (i) deleting an object; (ii) merging an object adjacent to another, (iii) adding a spherical object of some radius; (iv) splitting an object by replacing it with two or more spheres; and (v) redefining an object by replacing it with a sphere.
The restriction to spheres has not been found to be limiting and if somewhat more tailored masks are desired we find combining 24 spheres generally suffices to give a snug mask.
4 APPLICATIONS VANO allows one to conveniently create or efficiently edit a segmentation and annotation of a collection of globular objects.
As such it played a critical support role in building a 3D digital nuclei atlas for C.elegans (Fig.1a and b; collaboration with Stuart Kim lab at Stanford) and a 3D digital atlas of the nuclei of the ventral nerve cord of late stage D.melanogaster embryos (collaboration with Chris Doe lab at Oregon).
For these two applications, VANO was used to produce the training sets for our automated algorithms (Long et al., 2007, 2008a), to assess the quality of the 3D segmentations and annotations of nuclei produced by these algorithms, and to correct errors wherever needed to produce near perfect reference results.
VANO can also be used to annotate images in contexts where the image objects have an irregular shape.
For example, we have been using VANO to annotate the enervation pattern of target neurons in defined compartments of the adult brain of D.melanogaster (collaborations with Gerry Rubin and Julie Simpson laboratories at Janelia Farm Research Campus).
In Figure 1c, each glia-isolated compartment is defined as a separate object with a complicated 3D shape.
VANO enables us to conveniently interact with these 3D objects and annotate the neuronal distribution pattern within each compartment.
VANO can be used in many situations in biomedical imaging where the annotation of automatically segmented images is desired.
In addition, VANO can be used to collect the prior information (e.g.
3D location) and statistics (e.g.
mean/peak intensity) of image objects.
The information is useful for developing 3D segmentation methods of cells or intracellular organelles, or related applications.
ACKNOWLEDGEMENTS We thank Xiao Liu and Stuart Kim for feedback in annotating C.elegans data; Mike Layden, Ellie Heckscher and Chris Doe for feedback in annotating fruit fly embryo data; Arnim Jenett, Gerry Rubin and Julie Simpson for feedback on annotating fruit fly adult brain data; and Frank Midgley for feedback on the design of VANO.
We thank Zongcai Ruan for assistance in writing batch-compiling scripts and Margaret Jefferies for editorial review of the manuscript.
696 A volume-object image annotation system Conflict of Interest: none declared.
Abstract RNAmodifications, especially methylation of the N6 position of adenosine (A)m6A, rep-resent an emerging research frontier in RNA biology.
With the rapid development of high-throughput sequencing technology, in-depth study of m6A distribution and function relevance becomes feasible.
However, a robustmethod to effectively identifym6A-modified regions has not been available yet.Here, we present a novel high-efficiency and user-friendly analysis pipeline called MeRIP-PF for the signal identification of MeRIP-Seq data in reference to controls.
MeRIP-PF provides a statistical P-value for each identified m6A region based on the difference of read distribution when compared to the con-trols and also calculates false discovery rate (FDR) as a cut off to differentiate reliablem6A regions from the background.
Furthermore, MeRIP-PF also achieves gene annotation of m6A signals or peaks and produce outputs in both XLS and graphical format, which are useful for further study.
MeRIP-PF is implemented in Perl and is freely available at http://software.big.ac.cn/MeRIP-PF.html.
Introduction Multiple layers of epigenetic regulation including modification of DNA, RNA and proteins have been intensively explored.
One of the major RNA modifications, m6Amethylation of the N6 position of adenosine (A)represents an emerging re-search frontier inRNAbiology andmedicine [1].
Post transcrip-eijing Institute of Genomics, tics Society of China.
g by Elsevier ing Institute of Genomics, Chinese Ac tionally added, m6A is an enzymatic modification of RNAs and the most common form found in the internal sequences of mRNAs in eukaryotes, as well as RNAs in nuclear-replicating viruses [2].
Unlike A-to-I RNA editing, m6A is nonstoichiomet-ric and does not alter the coding capacity of transcripts [3,4] and may play roles in regulating RNA expression [1].
Along with the development of analytical and detection meth-ods, such asMeRIP-Seqorm6A-seq [4,5], researchers are nowable to carry out in-depth studies on m6A distribution and function of related genes.
Next-generation sequencing (NGS) technologies render the identification of m6A-specific methylation of mRNAs possible through target enrichment, including immunoprecipita-tion [4,5].
As this strategy becomes popular, challenges are yet to be met for efficient data analysis, especially in peak finding.
Due to fundamental technical differences of experimental protocols and the nature of the raw data, the existing peak-finding software packages, such as MACS [6] for ChIP-Seq and PARalyzer [7] for ademy of Sciences and Genetics Society of China.
Published by Elsevier Ltd Li Y et al/ MeRIP-PF: Peak-finding in MeRIP-Seq Data 73 PAR-CLIP, are not suitable for MeRIP-Seq peak-finding.
More importantly, there have not been tools or algorithms publically available for MeRIP-Seq data analysis yet.
Here, we present a highly-efficient and easy-to-use analysis pipeline, named MeRIP-PF (MeRIP-Seq data peak finding), which is a publicly-available tool and specially developed for the peak-calling of MeRIP-Seq data.
MeRIP-PF has a powerful graphical display and can be readily used for the identification of m6A-modified regions and gene annotation.
Method Formulation If an mRNA contains m6A modification sites, the RNA frag-ment containing the sites can be pulled down by the anti-m6A Figure 1 MeRIP-Seq, MeRIP-PF and result processing A. Schematic of MeRIP-Seq and MeRIP-PF pipelines (red dots indica transcripts.
Pie chart showing the percentage of m6A peaks within dist coding genes.
C. Distribution of m6A peaks along mRNAs.
50 UTRs, regions spanning 1% of their total lengths.
Y-coordinates represent p transcripts in wig plot.
Y-coordinates show the read coverage of eve Different regions of transcripts are color coded with UTR indicated in b space.
Red triangle indicates the position of m6A peaks.
antibody and sequenced subsequently.
The m6A modified re-gions can be identified by mapping the sequencing reads to the reference sequence of a genome through comparison of read counts between the sample and the control (Figure 1A).
Input data The pipeline requires two Fastq-formatted data.
One is sequencing reads from m6A-containing RNA immunoprecipi-tated (MeRIP) sample, and the other is the corresponding reads from non-IP transcriptome, which serves as the back-ground control.
Additionally, users also need to prepare the genome reference sequence of the species and several anno-tated BED files with gene structure information, which we rec-ommend downloading from the UCSC database (http://genome.ucsc.edu/cgi-bin/hgTables) directly.
te m6A sites).
B.
Distribution of m6A peaks in different regions of inct regions of RNA.
NP, non-protein-coding genes; PR, protein-CDSs and 30 UTRs of every transcript are separately binned into ercentage of m6A peaks located in every bin.
D. An example of ry position in transcripts.
NM_009484 was taken as an example.
lue and CDS in pink, and intronic regions are indicated with blank 74 Genomics Proteomics Bioinformatics 11 (2013) 7275 Processing Figure 1A shows the process of MeRIP-Seq and the method for detecting m6A peaks (see detailed steps in Figure S1).
We integrated four modules, including mapping, testing, annotat-ing and plotting, into one command program to complete the analysis.
The first module, i.e., sequence mapping, specifically yields two datasets in SAM-format, one from the MeRIP sam-ple and the other from the control, by using the BWA software [8].
The uniquely-mapped reads (MAPQ P 20) are converted into BED files using SAMtools [9] and BEDTools [10].
The second module identifies m6A-modified regions in high resolu-tion in reference to read coverage.
The m6A signal peaks are defined based on comparison of read counts between the MeR-IP data and the control data with a fixed 25-bp window across the genome.
Based on one-tailed Fishers exact test and Benja-miniHochberg method [11], both P-value and adjusted P-va-lue (FDR) for each 25-bp window are calculated continuously to define significant sequence windows (FDR 6 0.05).
The sig-nificant and adjacent (no gaps present) windows are concate-nated, while only those with appropriate sizes are considered as reliable and real peaks.
The third module is used to annotate each peak in terms of peaks and genes (see Tables S1 and S2 where we just presented partial results derived from the pub-lished data [5]), and an enrichment score for each peak is also calculated.
The fourth module is used to analyze m6A peak dis-tribution at transcriptomic level and display read distribution along transcripts graphically.
The peak counts are based on non-protein-coding and protein-coding genes, which are fur-ther divided into coding sequences (CDSs), introns, and untranslated regions (UTRs).
Output MeRIP-PF generates 4 output files that contain the complete information of m6A modification profile.
Reads_Over-view.txt file supplies the basic status of the two sequencing datasets; Peak_All.xls file provides the absolute positions of m6A peaks in genome and mRNA regions; Gene_List.xls file presents the annotating information of m6A peaks at the gene level; and Plot_Fig.pdf file shows read distribution in the control and peak distribution in theMeRIP sample (Figure 1B), the distribution of m6A peaks along mRNAs (Figure 1C), and wig plots for transcripts with m6A peaks (Figure 1D).
Implementation The pipeline program is written in Perl and runs in a Linux machine cluster with each node consisting of 8 cores with Table 1 Peak-finding performance of MeRIP-PF Methods No.
of peaks Group A Group Identical peaks (overlap P 75%) Simila (50% Ref.
[5] 13,471 9555 3719 MeRIP-PF 20,199 9555 3719 Note: The overlap we mean is reciprocal for peak M and peak N. In overlaps at least 75% of peak M and that peak M also overlaps at least 2.00 GHz processor and 16G RAM.
MeRIP-PF requires the installation of Perl and R language program, BWA, SAMtools and BEDTools.
MeRIP-PF, along with an implementation file of the described method and Demo Datasets can be down-loaded from the project website http://software.big.ac.cn/ MeRIP-PF.html.
The package requires the programming envi-ronment R. The R software is available at the website The R Project for Statistical Computing (http://www.r-project.org/).
Results and discussion We tested MeRIP-PF performance using published adult mouse brains data (GSM854223 and GSM854224) with more than 30 millions reads for each sample [5].
By MeRIP-PF, we caught all high-confidence peaks in the previous study [5], which we named as HC peaks, but there are some differ-ences between these two reports, which were shown in Table 1.
We classified all peaks into four groups according to the frac-tion of peak region overlapping.
In group A, 9555 peaks were identical between MeRIP-PF and HC peaks, which showed more than 75% overlap for each peak region.
In group B, 3719 peaks were similar, which had 5075% overlap for each peak region.
In group C, 1772 peaks of MeRIP-PF had 050% overlap with 147 peaks in HC peaks, and we defined this group as different.
In group D, there were 5153 peaks that had no overlap with HC peaks and we defined this group as new.
We detected all the high-confidence peaks in the previous report [5], but there remained some differences in some peak regions in group A, B and C. Since the two methods were sim-ilar, the peak region differences might be due to the following two reasons: (1) although we both used default parameters of BWA software for reads mapping, use of different versions may contribute to difference of mapping results (we used bwa-0.6.2); (2) although uniquely-mapped reads were taken for further peaks calling in both reports, we regarded reads with MAPQ (mapping quality) P 20 as uniquely-mapped, while their criterion of filtering was not very clear.
The differ-ences of total uniquely-mapped read count and read number falling in each 25-bp window resulted in shift of peak regions.
About those new peaks in group D, besides the aforemen-tioned explanations, more importantly, they used multiple rep-licates samples to improve the confidence of m6A peaks [5].
We only used one replicate for analysis, so the new peaks are probably the low-confidence peaks.
The distribution of time spent on processing GSM854223 and GSM854224 datasets is presented in Table 2.
Apparently, the total time required de-pends much on mapping procedure which is closely related to sequencing quantity.
B Group C Group D r peaks 6 overlap < 75%) Different peaks (overlap < 50%) New peaks (overlap = 0%) 197 n 1772 5153 other words, if overlap is more than 75%, it requires that peak N 75% of peak N. Table 2 Time spent on processing GSM854223/GSM854224 datasets Process Timing (min) Mapping 200 Statistical test 15 Annotation 10 Plotting 10 Li Y et al/ MeRIP-PF: Peak-finding in MeRIP-Seq Data 75 Conclusion As the first specific and easy-to-use pipeline for MeRIP-Seq data, MeRIP-PF provides outputs in both XLS and graphical format, which are important for deep analysis of m6A modifi-cation.
We integrated four modules, including mapping, test-ing, annotating and plotting, into one command program to complete the analysis.
The pipeline applies to MeRIP-Seq data with corresponding control samples.
Authors contributions YL drafted the manuscript and developed the software.
CL par-ticipated in the software design.
SS proposed the idea of the soft-ware and revised the manuscript.
JY revised the manuscript.
All authors have read and approved the final manuscript.
Competing interests The authors have no competing interests to declare.
Acknowledgements We would like to thank all the members of Genome and Bio-informatics platform in Beijing Institute of Genomics for their assistance during the pipeline development.
This work was supported by Grants from the Ministry of Science and Tech-nology of China (Grant No.
2011CB944100) to JY, the Natu-ral Science Foundation (Grant No.
30900831 and 31271372) and Beijing Nova Program (Grant No.
Z121105002512060) to SS.
Supplementary material Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.gpb.2013.
01.002.
ABSTRACT Motivation: The research area metabolomics achieved tremendous popularity and development in the last couple of years.
Owing to its unique interdisciplinarity, it requires to combine knowledge from vari-ous scientific disciplines.
Advances in the high-throughput technology and the consequently growing quality and quantity of data put new demands on applied analytical and computational methods.
Exploration of finally generated and analyzed datasets furthermore relies on powerful tools for data mining and visualization.
Results: To cover and keep up with these requirements, we have created MeltDB 2.0, a next-generation web application addressing storage, sharing, standardization, integration and analysis of metabo-lomics experiments.
New features improve both efficiency and effect-ivity of the entire processing pipeline of chromatographic raw data from pre-processing to the derivation of new biological knowledge.
First, the generation of high-quality metabolic datasets has been vastly simplified.
Second, the new statistics tool box allows to inves-tigate these datasets according to a wide spectrum of scientific and explorative questions.
Availability: The system is publicly available at https://meltdb.cebitec.
uni-bielefeld.de.
A login is required but freely available.
Contact: nkessler@cebitec.uni-bielefeld.de Received on May 6, 2012; revised on July 11, 2013; accepted on July 14, 2013 1 INTRODUCTION Metabolomics research covers all aspects of the investigation of small molecule metabolite compositions resulting from cellular processes and constitutes an integrated part of systems biology (Bino et al., 2004).
Like transcriptomics and proteomics, meta-bolomics is capable of measuring extrinsically initiated changes in organisms.
The metabolome, the entity of all small molecules in a cell, organism or tissue, is considered to be the closest to the phenotype of all-omes (Fiehn, 2002).
Compared with other molecular levels or-omics methods, metabolomics is challenging in its high degree of interdiscipli-narity, interlinking experts from research fields as diverse as engineering, physics, chemistry and biology and from cheminfor-matics over bioinformatics to statistics, data mining and finally visualization.
Both sample acquisition and subsequent analysis are auto-mated in high-throughput instruments, which has continuously posed challenges on the systematic storage and computational processing of the gathered experimental datasets, starting in the early 2000s.
The increasing number and quality of measurements not only raised the generated data volume but also allowed to address more complex biological questions within conducted ex-periments.
To comprehensively address these demands, bioinfor-matics internet applications were developed.
MeltDB, a software platform for the analysis and integration of data from metabolo-mics experiments, has been published by Neuweger et al.
(2008).
Xia et al.
(2009) released MetaboAnalyst, a comprehensive tool suite for metabolomic data analysis.
Carroll et al.
(2010) pub-lished the MetabolomeExpress web server as a public place to process, interpret and share GC/MS metabolomics datasets.
Since around 2008, we have observed that the requirements to comprehensive metabolomics software platforms have changed: The general growth of the field of metabolomics and the increas-ing number of collaborations diversified the user community of researchers and their individual scientific goals.
It is obvious that the success of a metabolomics study depends on an efficient and effective collaboration of this interdisciplinary research commu-nity.
Thus, not only the availability and sharing of the data is important but also special functions have to be significantly ex-tended with specific features to consider all researchers demands and perspectives.
In addition, the ever-increasing throughput and the constant lack of time makes it immensely important that automated pre-processing methods are reliable and that analyses and manual intervention are fast and easy.
Since Metabolomics approaches are applied to more and more scientific objectives, a powerful set of statistical methods is mandatory, ranging from hypothesis-driven statistical tests to less specified and untargeted data-mining methods, such as clustering and dimension reduc-tion.
Finally, the wealth of generated data poses a necessity for exploratory data analysis tools and information visualization.
To tackle these new challenges systematically, a next gener-ation of bioinformatics tools needed to be developed, covering all of the aforementioned aspects of metabolome data analysis, ranging from processing raw data (RD) to finishing and finally the derivation of biological knowledge.
During the stages of that process, one can identify four successive data categories that represent different levels of data classification and annotation as well as different levels of abstraction.
First, RD, stored and organized in meaningful groups, build the basis.
Then, pre-processed data (PD) is computed, where peaks and their*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com https://meltdb.cebitec.uni-bielefeld.de https://meltdb.cebitec.uni-bielefeld.de mailto:nkessler@cebitec.uni-bielefeld.de `` '' to `` '' `` '' `` '' , , And (EDA) above raw data ( ) s quantities have been detected.
It follows integrated data (ID), where peaks that putatively originate from the same compound are consistently annotated over chromatograms of an experiment and thus become comparable.
Last, derivative data (DD) is achieved by statistical analyses of metabolite quantities in an experiment and then visualized to allow effective exploration and to draw conclusions.
In this manuscript, we present MeltDB 2.0, which offers novel tools to challenge the rising wealth of data quality and quantity and support the analysis of all four categories RD, PD, ID and DD and includes a multitude of updates.
New and improved preprocessing methods underpin the reliability of automatically created annotations.
At the same time, straightforward tools for manual peak annotation simplify the curation even of large ex-periments.
To help answering questions of different scientific objectives, the set of statistical analyses and data-mining tools has been strongly enriched.
To finally nail down the quintessence of an experiments outcome, data exploration is supported by new interactive and telling information visualizations.
2 IMPLEMENTATION AND METHODS The first version of the MeltDB software platform, a three-tiered web application and database server published in 2008 (Neuweger et al., 2008), provides means for the standardization, systematic storage and analysis of gas chromatographymass spectrometry (GC-MS) metabolomics experiments.
Within a powerful project and user management, raw chromatograms of various file formats can be uploaded and organized into chro-matogram groups (e.g.
replicates, factor levels) and experiments.
A flexible processing pipeline allows to find, quantify and iden-tify peaks in the raw chromatograms.
Subsequently, a set of statistical tools and visualizations can be applied to analyze the gathered data tables.
This fast growing, free online platform today hosts425 distinct projects conducted by4150 registered users from around the world.
More than 17 000 chromatograms have been uploaded and analyzed yet.
In the following, all major improvements to the entire process from RD to DD will be described in more details.
Figure 1 summarizes the four stages of data processing and associates visualizations and data mining methods that can be performed in MeltDB 2.0 to each stage.
2.1 From RD to PD: improved pre-processing In metabolomics data analysis, pre-processing is a critical step, as ID and DD build on PD.
To ensure a reliable data basis for statistical data exploration, MeltDB 2.0 is equipped with several new and updated algorithms for the early steps of experiment data analysis.
The growing list of pre-processing methods now includes sup-port for the centWave algorithm by Tautenhahn et al.
(2008) for chromatographic peak detection, which features a high sensitiv-ity, and updates of the XCMS package (Smith et al., 2006) for chromatogram alignment and profiling analyses.
In addition, the ChromA (Hoffmann and Stoye, 2009) software is added to the list of supported chromatogram alignment tools.
ChromA com-putes pairwise alignments of chromatograms without a priori knowledge, but it is capable of optionally using previously matched or identified peaks as anchor points, which speeds up the process.
The calculation of retention time indices in GC-MS measure-ments is improved and can now also be performed manually using the web interface.
Peaks of added substances can be as-signed with retention indices and will be used as anchors for interpolating other peaks retention indices (Ettre, 1994), which Fig.1.
The overview shows the information processing in MeltDB 2.0 as well as visualizations and tools that are applicable to each level of data: RD, PD, ID and DD.
Although different chromatogram viewers are available immediately after RD upload, heatmaps and data matrices can only be computed as soon as data have been integrated, i.e.
there are peaks that are consistently named across chromatograms.
To finally derive knowledge from the data, MeltDB 2.0 offers a versatile set of statistics and data-mining tools 2453 MeltDB 2.0 , 3 more than more than , raw data derivative data raw data pre-processed data I integrated data derivative data up pre-processed data In order  support subsequent peak identification (Kopka et al., 2005).
The detection of alkanes as retention markers can be automated.
Furthermore, peak identification itself is facilitated with a powerful feature: MeltDB 2.0 offers a new Reference list tool to save peaks of measured reference substances as Reference in the MeltDB database.
The stored data comprises retention indi-ces, quantification masses and mass spectra of reference com-pounds.
This helps to generate project specific databases that complement the Golm Metabolite Database (http://gmd.
mpimp-golm.mpg.de/) (Kopka et al., 2005) or the National Institute of Standards and Technology standard reference data-base 1A (http://www.nist.gov/srd/nist1a.cfm).
The tool allows to aggregate
ABSTRACT Summary: CD-HIT is a widely used program for clustering and comparing large biological sequence datasets.
In order to further assist the CD-HIT users, we significantly improved this program with more functions and better accuracy, scalability and flexibility.
Most importantly, we developed a new web server, CD-HIT Suite, for clustering a user-uploaded sequence dataset or comparing it to another dataset at different identity levels.
Users can now interactively explore the clusters within web browsers.
We also provide downloadable clusters for several public databases (NCBI NR, Swissprot and PDB) at different identity levels.
Availability: Free access at http://cd-hit.org Contact: liwz@sdsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on September 17, 2009; revised on December 7, 2009; accepted on January 2, 2010 1 INTRODUCTION The size of the biological sequence databases is rapidly growing due to large-scale genome projects and the emerging field of metagenomics (Yooseph et al., 2007).
New sequencing technologies are now producing sequence data at a very high rate, and this has created a greater need for bioinformatics tools to effectively organize and analyze the data.
Fortunately, biological sequences are related and may share homology, and thus clustering these sequences into groups and finding a representative or a consensus for each group are practical ways to solve the sequence analysis problems.
Our previous works (Li and Godzik, 2006; Li et al., 2001; Li et al., 2002) introduced CD-HIT based on short word filtering and a greedy incremental clustering algorithm to cluster and compare large biological sequence datasets.
One advantage of CD-HIT is its ultrahigh speed and the ability to handle large datasets.
Since its release, CD-HIT has been widely used by many groups in various fields, including UniRef (Suzek et al., 2007), SMART (Letunic et al., 2009) and metagenome data analyses (Turnbaugh et al., 2009; Yooseph et al., 2008).
In the last few years, we have been continuously improving this program with more functions and better accuracy, scalability and flexibility.
We also implemented a new web server to allow To whom correspondence should be addressed.
Present address: Department of Medicine, University of California San Diego, La Jolla, CA, USA.
users to cluster or compare sequences without installing and executing the command-line version of CD-HIT locally.
The server provides interactive interface and additional visualization tools.
It also provides precalculated and regularly updated sequence clusters for several widely used databases, including NCBI NR, Swissprot and PDB.
2 METHODS AND IMPLEMENTATION The detailed algorithms and benchmark results for CD-HIT can be found from our previous works (Li and Godzik, 2006; Li et al., 2001; Li et al., 2002).
Here, we highlight the novel features and functions.
2.1 Improved clustering algorithm The original CD-HIT uses a fast greedy incremental clustering process.
Briefly, sequences are first sorted by decreasing length.
The longest one becomes the representative of the first cluster.
Then, each remaining sequence is compared with the existing representatives.
If the identity with any representative is above a given threshold, it is grouped into that cluster without comparing it to other representatives.
Otherwise, it becomes the representative of a new cluster.
In the updated CD-HIT, we added a refined greedy incremental clustering process that produces more accurate clusters.
In this process, a sequence is grouped into the most similar cluster instead of the first similar cluster.
The refined process does not change the representative sequences.
CD-HIT uses a short word filter to avoid unnecessary alignments.
In short, the minimum number of identical short words (k-mers) shared by two sequences depends on their sequence identity and can be calculated analytically or statistically.
Without an actual alignment, we can still determine that the identity of two sequences is below a given threshold by counting short words.
A short word filter performs much better with a higher identity threshold.
Clustering in the refined process is implemented with a dynamic short word filter.
For each sequence to be clustered, the initial filter matches the user-defined identity threshold.
But during the clustering procedure, if this sequence hits any cluster with better identity, the filter is reset to match this better identity to increase the performance of the filter.
With the dynamic short word filter, although the refined clustering process needs to evaluate the similarities of a sequence and all the existing representatives, it only requires about 1.53 CPU time of the original process.
2.2 Improved clustering control The original CD-HIT uses global sequence identities.
The improved CD-HIT also works with local identities.
Users can finely control the clustering behavior by including more criteria besides sequence identity cutoffs.
We include alignment length, unaligned length and alignment coverage for both aligned sequences as new clustering parameters into the current CD-HIT.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:16 12/2/2010 Bioinformatics-btq003.tex] Page: 681 680682 CD-HIT Suite For example, users can make clusters of sequences of similar length by specifying that the alignments must cover both sequences at similar coverage.
2.3 Clustering at low identity thresholds The performance of the native short word filter drops significantly with a lower identity threshold; therefore, the original CD-HIT does not provide protein clustering under 40% identity.
However, clustering at low identities has been frequently requested by CD-HIT users.
We implemented a script, called PSI-CD-HIT, to perform protein sequence clustering at a low identity threshold such as 30%.
It uses the similar greedy incremental clustering strategy, but it uses BLAST to calculate the similarities.
So users can also specify an expect-value cutoff.
PSI-CD-HIT runs on a stand-alone computer or a LINUX cluster.
It can cluster a PDB-sized dataset in 20 min.
2.4 Hierarchical clustering In the hierarchical clustering process, the program first performs clustering on the original input dataset at a high identity threshold, and the representatives of each previous clustering step will be the input of the following clustering run at a lower identity threshold.
The whole process iteratively joins the similar sequences into families and therefore produces a hierarchical structure.
For protein sequences, the last step is performed with PSI-CD-HIT if the final identity threshold is <40%.
This strategy can maximize the computational efficiency and the quality of clustering.
We have applied such strategy in a protein family analysis of a large metagenomic dataset with 17 million sequences (Li et al., 2008).
2.5 Annotation enrichment of sequence cluster We provide an option for joint analysis of sequence clustering and annotation information.
Users can place annotation terms (Gene Ontology, protein family, etc.)
in the definition lines of input FASTA files.
For each annotation term A and each cluster C, we use the following numbers: NAC = number of sequences with A in C; NC = number of sequences in C; NAI = number of sequences with A in the input; NI = number of the input sequences.
A P-value is calculated using the one-tailed Fishers exact test to assess whether NAC/NC >NAI/NI and annotation term A is enriched in cluster C. Such functionality is very useful to check the cluster quality at different identity levels and also for function assignment of proteins with unknown function.
2.6 Web server All basic functions of CD-HIT are provided through tab-based interfaces in our web server.
We provided CD-HIT (CD-HIT-EST) to cluster a protein (DNA/RNA) dataset.
Users can upload a FASTA file and select a desired sequence identity level and other parameters.
CD-HIT-2D (CD-HIT-EST-2D) can compare two databases uploaded by users.
H-CD-HIT and H-CD-HIT-EST in our server performs hierarchical clustering up to three steps.
After submitting a clustering or comparison job, a unique identifier will be assigned.
A user can use the identifier to track the status of the job.
After the job is finished, we provide the raw outputs generated by the command-line CD-HIT.
Additionally, we provide tools to visualize the clustering results with cluster explorer and cluster distribution plots.
Cluster explorer uses a tree structure to represent the clustering results Figure 1a.
Each cluster is represented by a clickable text object on the web page, and users can click on a representative sequence to retrieve information of the sequences belong to the cluster.
This option is most useful for investigating the results Fig.1.
Screenshots of CD-HIT Suite.
(a) Cluster Explorer for investigating clusters.
(b) A cluster distribution plot to explore the global structure of a whole dataset.
from hierarchical clustering.
In this situation, each sequence could be a representative sequence from the previous clustering step, and users can click it to explore the results from the previous clustering.
Cluster distribution plots are scatter plots where the X-axis is the cluster size (number of sequences in a cluster), and then the Y-axis represents the number of clusters of at least this size and the number of corresponding sequences Figure 1b.
This tool is very useful to observe the global structure of a sequence database.
3 CONCLUSION CD-HIT has been significantly improved from our previous work.
CD-HIT Suite provides users with a friendly web interface to perform biological sequence clustering and comparison with additional visualization tools.
It also provides precalculated clusters for several public sequence databases which are regularly updated.
ACKNOWLEDGEMENTS We thank Mr Michael Chiu for his excellent editorial assistance.
Funding: National Institutes of Health (1R01RR025030) from National Center for Research Resources.
Conflict of Interest: none declared.
ABSTRACT Motivation: High-throughput sequencing technologies have recently made deep interrogation of expressed transcript sequences practical, both economically and temporally.
Identification of intron/exon boundaries is an essential part of genome annotation, yet remains a challenge.
Here, we present supersplat, a method for unbiased splice-junction discovery through empirical RNA-seq data.
Results: Using a genomic reference and RNA-seq high-throughput sequencing datasets, supersplat empirically identifies potential splice junctions at a rate of 11.4 million reads per hour.
We further benchmark the performance of the algorithm by mapping Illumina RNA-seq reads to identify introns in the genome of the reference dicot plant Arabidopsis thaliana and we demonstrate the utility of supersplat for de novo empirical annotation of splice junctions using the reference monocot plant Brachypodium distachyon.
Availability: Implemented in C++, supersplat source code and binaries are freely available on the web at http://mocklerlab-tools.cgrb.oregonstate.edu/ Contact: tmockler@cgrb.oregonstate.edu Received on September 23, 2009; revised on April 9, 2010; accepted on April 16, 2010 1 INTRODUCTION Recent advancements in high-throughput sequencing (HTS) technologies (reviewed in Fox et al., 2009; Shendure and Ji, 2008) have made deep interrogation of expressed transcript sequences both economically and temporally practical, resulting in massive quantities of sequence information using the RNA-seq approach (Wang et al., 2009).
Extracting comprehensible genic models from this sea of data depends upon the identification of intron/exon boundaries.
One current method used to identify intron/exon boundaries, Q-PALMA (De Bona et al., 2008), utilizes a machine learning algorithm to identify splice junctions, training a large margin classifier on known splice junctions from the genome of interest.
This method depends upon the availability of previously known splice junctions on which to train the algorithm, and, when finding novel potential splice junctions, is biased toward those which are similar to its training data.
In scoring novel potential splice junctions, the algorithm is biased toward canonical terminal dinucleotides, To whom correspondence should be addressed.
Present address: Vollum Institute, Oregon Health & Sciences University, Portland, OR 97239, USA scoring those which conform to these biases higher than ones that do not.
While, in general, these biases may prove to be correct, many potential splice junctions which do not conform to these rules threaten to remain unidentified.
A second method, TopHat (Trapnell et al., 2009), works by first creating exonic coverage islands from short-reads and then, based on canonical intron terminal dinucleotides (ITDN), which exist in these islands, identifies possible splices between neighboring exons.
Like QPALMA, TopHat is strongly built around the idea of canonical ITDN, resulting in similar issues to QPALMA.
Further, since TopHat bases its predictions on coverage islands, a sufficient number of short RNA-seq reads must be used as input such that reliable exon islands may be identified.
Here, we present our algorithm implemented in C++, supersplat, which identifies all locations in a genomic reference sequence that indicate potential introns supported by empirical transcript evidence derived from RNA-seq data.
Our approach does not include bias for canonical ITDN, but rather finds every potential splice junction supported by empirical evidence, doing so in a straightforward, transparent manner and guided only by user-provided values for minimum intron size and maximum intron size, and by the minimum number of matching short-read nucleotides allowed on each flanking exon.
Further, any number of short reads may be used as input, since supersplat does not need or attempt to generate exon coverage islands.
2 ALGORITHM 2.1 Definition Supersplat begins by loading the input reads, and their reverse complements, into a hash table as key-value pairs, storing the nucleotide sequence of each read as keys and the number of occurrences of each read as corresponding values.
This limits the amount of system memory required to store the input reads to a single copy of each unique sequence.
Next, the input reference sequences are read and processed.
For each reference sequence and starting at the reference sequences first base, supersplat builds an index that holds the location of every encountered k-mer, where k ranges from the minimum read chunk size, c, to an upper length, i, both of which are specified by the user.
This location index is stored in a hash table as key-value pairs, where the key is the encountered k-mer and the value is a sorted list of the reference-specific, one-indexed locations where the k-mer was found, illustrated in Figure 1.
Once a reference sequence has been indexed supersplat iterates through all unique reads identifying those which can, while satisfying user-specified conditions, be partitioned and matched exactly against the reference, thereby identifying potential splice junctions.
Each m-base long read is partitioned in all possible two-chunk configurations, with chunk one starting at the The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:13 3/6/2010 Bioinformatics-btq206.tex] Page: 1501 15001505 Supersplat Fig.1.
Supersplat indexes a reference by starting at the first base in the reference sequence and stepping through the sequence, one base at a time.
For each such stepping, b, supersplat stores each k-mer which begins at position b, where k ranges between the minimum read chunk size, c, and the MICS, i, both of which are specified by the user.
In this figures example, c is 6 and i is 11.
Supersplat starts building the index by storing the first six bases of the reference, starting at the beginning of the reference, location 1, as a 6mer in the index, and associates that 6mer with a list of locations, which presently contains only location 1.
Supersplat then stores the first seven bases of the reference as a 7mer in the index, and associates that 7mer with a list of locations, containing location 1.
This continues until supersplat stores the first 11 bases of the reference as an 11mer, and associates that 11mer with a list of locations, containing location 1.
Now that supersplat has reached k = i = 11, supersplat steps to the next base of the reference sequence, location 2.
Supersplat now stores the first six bases of the reference, starting at reference location 2, as a 6mer in the index, and associates that 6mer with a list of locations, containing location 2.
This process repeats until supersplat has indexed the entire reference sequence in this way.
minimum chunk length, c, growing iteratively by one base until chunk one is of length m c, and chunk two starting at length m c, shrinking iteratively by one base until chunk two is of length c. For each such partitioning supersplat retrieves from the location index location-list one (LL1), corresponding to the exact k-mer represented by chunk one, and location-list two (LL2), corresponding to the exact k-mer represented by chunk two.
If one of the chunks is longer than the largest k-mer indexed by supersplat, supersplat retrieves the location-list corresponding to the first i bases of that chunk.
Once LL1 and LL2 have been retrieved, for each element of LL1, LL10:k , supersplat iterates over all elements of LL2, LL20:l , comparing the locations of each element pair.
If it is found that the minimum intron size, n, and the maximum intron size, x, both of which are user-specified, are satisfied by any such pair, supersplat ensures an exact match of chunks one and two to the reference sequence if necessary and, if a match is verified, marks the bounded genomic sequence as a possible intron.
This process repeats for each possible partitioning of each read and over each reference sequence.
All possible splice junctions are output to a file by default, with an option to output only canonical ITDN matches if desired.
2.2 Complexity analysis Let G be the reference sequence length, ELL1 be the number of elements in location-list one, ELL2 be the number of elements in location-list two, Nreads be the number of input reads, and suppose all input reads are of length R. The supersplat algorithm begins by indexing both the reference sequence and the input reads in the creation of two hash tables, the reference index and the reads index.
These two caching structures are created in time O ( G ) and O ( Nreads ) , respectively.
Most of supersplats processing time is spent performing iterative pairwise comparisons between location-lists, dwarfing the amount of time spent generating the indices.
This iterative pairwise comparisons algorithm has overall time complexity of O ( Nreads RELL1 ELL2 ) .
The inner term ELL1 ELL2 is an upper bound; in reality, all possible pairs of elements in LL1 and LL2 need not be explored due to two optimizations.
First, location-lists are limited in size by the length of the chunk being queried.
If the size of one chunk is short, which results in a large number of genomic matches and thus a location-list that contains a large number of entries, its paired chunk is long, which results in the second location-list containing a relatively few number of entries.
Second, location-lists are always sorted in increasing order of reference positions as a byproduct of the way in which the reference index is constructed.
As a result of the location-lists being sorted, once a comparison has been made between elements of LL1 and LL2 where the distance between these two elements is greater than the maximum allowed intron size, all remaining elements in LL2 can be skipped for the current element of LL1.
The space complexity of the supersplat algorithm is linear in the number of reads, the reference size and the maximum index chunk size (MICS) parameter, resulting in the space complexity O ( Nreads GMICS ) .
3 RESULTS 3.1 Performance Real-world performance of supersplat was tested with a set of 3 690 882 unique Arabidopsis thaliana Illumina RNA-seq reads (short read archive accession: SRA009031) (Filichkin et al., 2010), each of which was known to map to an annotated (TAIR8) splice junction location on the Arabidopsis reference genome.
To benchmark supersplat, the algorithms primary performance parameter, MICS, was incremented iteratively by one from 9 to 18.
For each such iteration, the total run time from reference indexing to final output, as well as maximum memory (RAM) usage, was recorded.
Benchmarking was performed on a 3.0 GHz Intel Xeon processor with 32 GB memory.
For all tests, the minimum intron size, n, was set to 40; the maximum intron size, x, was set to 5000; and the minimum chunk size, c, was set to 6, while the MICS, i, was varied.
Performance results are shown in Figure 2.
From these results, we see that as the MICS increases, runtimes decrease and RAM usage increases.
Between MICS values of 9 and 15 each stepping decreases runtime by about a factor of 4, after which yields diminishing returns.
For runtimes on these data, a MICS value of 15 was optimal, resulting in a runtime of 19.4 CPU minutes and with maximum RAM usage of 18.8 GB.
This indicates an average of 190 252 reads mapped per CPU minute, or 11.4 million reads mapped per CPU hour.
3.2 Empirical annotation of splice junctions in Brachypodium distachyon To demonstrate the utility of supersplat for de novo discovery of splice junctions, we mapped Illumina RNA-seq reads (short read archive accession: SRA010177) to the genome of the model grass B.distachyon.
For this analysis we used 10.2 Gb (289 million 32mer reads) of Illumina transcript sequence generated using the RNA-seq approach (Fox et al., 2009; The International Brachypodium Initiative, 2010).
We used ELAND (A.J.Cox, unpublished data) to identify all 32mer Illumina reads that aligned anywhere in the Brachypodium genome with up to two mismatches.
This step eliminated 79 million reads from further analysis.
The remaining 210 million reads were filtered using DUST (Morgulis et al., 2006) to remove reads containing low-complexity stretches, leaving 150 million reads that were aligned to the Brachypodium genome assemblies using supersplat.
Potential novel splice junctions predicted by supersplat were filtered to retain only those supported by at least two distinct independent RNA-seq reads with different overlap lengths on each side of the predicted intron (i.e.
the portions of the reads aligning 1501 [11:13 3/6/2010 Bioinformatics-btq206.tex] Page: 1502 15001505 D.W.Bryant et al.
Fig.2.
By increasing the maximum index size, the exhaustive genome-to-reads comparisons are reduced resulting in shorter runtimes.
This same increase correlates with an increase in peak RAM usage as a result of larger lookup tables.
to the predicted exons), reads mapping to only a single genomic location, a minimum overlap length of 6 bases on one exon, additional support of at least one microread matching each of the predicted flanking exonic sequences, a minimum predicted intron length of 20 and a maximum predicted intron length of 4000.
This analysis, using ad hoc filters designed to reduce false discoveries, identified a total of 1.55 million RNA-seq reads supporting 67 025 introns containing canonical GT-AG terminal dinucleotides.
These intron predictions are publicly available and presented in the Brachypodium community genome database and viewer found at http://www.brachybase.org.
Among all 67 025 GT-AG introns predicted by supersplat in this experiment, 63 866 (95.3%) were independently validated (Fig.3) by BradiV1.0 annotated introns verified by Brachypodium ESTs.
An example is presented in Figure 4, which depicts an empirically annotated Brachypodium gene encoding a protein similar to an annotated hypothetical protein in rice.
In this example, the filtered supersplat results predicted 14 out of 15 introns depicted in the empirical TAU (H.D.Priest et al., unpublished data) models.
The one intron of this gene not predicted by supersplat was inferred from other Sanger and 454 EST data (data not shown).
4 DISCUSSION Supersplat aligns RNA-seq data to a reference genome as a gapped alignment in order to empirically define locations representing Fig.3.
A Venn diagram showing the comparison of supersplat predicted Brachypodium GT-AG introns against BradiV1.0 annotated GT-AG introns verified by Brachypodium ESTs.
The 67 025 Brachypodium GT-AG introns (set SS) predicted by supersplat were supported by 1.55 million RNA-seq reads.
The 74 786 BradiV1.0 annotated GT-AG introns (set ESTs) were verified by alignment of 2.29 million 454 reads and 128 000 Sanger reads.
The 3695 introns in set HM are supersplat false negative introns that were missed by supersplat due to the minimum chunk size of 6 used in this analysis but verified as being supported by the RNA-seq data using HashMatch (Filichkin et al., 2010).
potential introns.
Unlike the other comparable tools, Q-PALMA and TopHat, supersplat is not inherently biased in favor of canonical ITDN, but instead by default exhaustively identifies every potential splice junction supported by the input data.
In another study (Filichkin et al., 2010), we found that following conservative filtering of supersplat output we were able to independently validate 91% and 86%, respectively, of canonical and non-canonical predicted introns that were tested by RT-PCR and Sanger sequencing.
Supersplat does provide a canonical ITDN option, which has been incorporated because, as in our Brachypodium example, in some cases users may prefer to only mine their RNA-seq data for introns containing the most common ITDNs rather than the far less-common non-canonical ITDNs.
Other user-provided parameters that limit the supersplat alignment algorithm are the minimum and maximum allowable intron sizes and the minimum overlap of a read on a flanking exon.
Supersplats exhaustive and unbiased approach comes at the cost of large unwieldy output files, which can reach the size of tens of gigabytes for large sets of RNA-seq data.
In particular, reads matching repetitive sequences or reads containing low-complexity stretches can match in numerous places in a reference genome as false spliced alignments.
Users should, therefore, carefully determine appropriate criteria for prefiltering potentially problematic data prior to running supersplat.
For example, as described for our empirical annotation of splice junctions in B.distachyon, reads that are likely to represent exonic data by virtue of their alignment to the target genome over their entire length should be removed prior to running supersplat.
In addition, it is a wise precaution to filter out low-quality reads, low-complexity reads and highly repeated reads likely to result in numerous 1502 [11:13 3/6/2010 Bioinformatics-btq206.tex] Page: 1503 15001505 Supersplat Fig.4.
An example of filtered supersplat output displayed in GBrowse v1.69 at BrachyBase (http://www.brachybase.org).
The Illumina 32mer perfect match track represents the distribution of perfectly matching 32 nt Illumina HTS RNA-seq reads over the region.
HTS SuperSplat Splice Junctions are Illumina reads aligned using supersplat specifically to identify putative introns.
The TAU v1.1 track depicts empirical transcription unit models derived from transcript data, including the splice junctions predicted by supersplat.
spurious alignments.
In the event that an annotation exists for the genome of interest, reads matching annotated splice junctions can be filtered out of the input in order to focus the supersplat analysis only on the identification of potential novel introns.
Sensible selection of runtime options and post-processing steps are good precautions to control false discoveries.
For example, users may want to choose reasonable limits for minimum and maximum intron lengths guided by prior data.
In addition, as described in our examples, supersplat output can be filtered to retain only those intron predictions supported by some minimum chunk size, multiple independent overlapping RNA-seq reads, introns supported by reads mapping to only a single genomic location, validation by RNA-seq data from independent biological replicates, multiple different overlap lengths for the read fragments on the flanking exons and additional transcript evidence supporting the predicted exons.
Despite these precautions factors such as read lengths, sequencing error rates, target genome complexity and gene duplications contribute to the likelihood of false discoveries with supersplat.
Some of these issues will no doubt be resolved by improvements to HTS technologies such as increased read lengths, reduced error rates and routine use of paired-end reads.
Using our test set of reads matching known A.thaliana splice junctions, we performed an analysis of supersplats precision while focusing on two of these standard filters independently, including minimum chunk size and number of overlapping reads.
Precision, also known as positive predicted value (PPV), is defined as true positives (TP) divided by the sum of TP and false positives (FP), PPV = TP/(TP + FP).
It is worth noting that an algorithms PPV can be skewed by generating only a small number of very cautiously 1503 [11:13 3/6/2010 Bioinformatics-btq206.tex] Page: 1504 15001505 D.W.Bryant et al.
Fig.5.
PPV versus minimum chunk size.
As minimum chunk size is varied from 6 to 15 the precision of supersplat rapidly approaches and exceeds 90%.
Here, the PPV denominator, TP + FP, ranges over 360 237 (minimum chunk size of 6) to 260 495 (minimum chunk size of 15).
declared positives, resulting in a very small but highly confident output set.
Supersplat, in contrast, generates exhaustive output that is not filtered according to confidence.As a result the PPVs presented here are computed using large denominators ensuring that this metric is an accurate reflection of supersplats performance.
In our analysis, a true positive prediction is one that correctly identifies a location in the genome at which there exists The Arabidopsis Information Resource (TAIR)-annotated splice junction.
A FP prediction is one that incorrectly identifies a location in the genome at which there is no such TAIR annotated splice junction.
PPV was calculated as minimum chunk size was varied from 6 to 15 with results shown in Figure 5, filtering in this case for intron predictions in the supersplat output which had the minimum chunk size shown.
From these results, we see that even at a minimum chunk size of six, the precision of supersplat is nearly 70%.
As minimum chunk size increases this precision value rapidly approaches and exceeds 90%.
PPV was then calculated as the number of reads overlapping each splice junction was varied from 1 to 21 with results shown in Figure 6.
In this case, we filtered for intron predictions in the supersplat output that had the respective number of intron overlaps shown.
From these results, we see with six overlapping reads a PPV of 90%, with PPV reaching 97% at 21 overlapping reads.
Runtime performance of supersplat is closely tied to how deeply the genomic reference is indexed, dictated by the MICS value.
Supersplat repeatedly queries its index for the genomic locations of various sized k-mers, which represent potential short read fragment alignments.
Since the probability of any specific k-mer existing at a particular genomic location, under the assumption that all bases occur with equal probability, is 0.25k , as k increases the probability of any specific k-mer occurring decreases.
Thus we expect, on average, that the list of all locations in a genomic reference sequence of a specific k-mer to be longer by a factor of four than a similar list of a specific (k + 1)-mer.
As supersplat processes short reads from its input, it repeatedly iterates over these location lists.
As the MICS value increases the lengths of these lists decrease, reducing runtimes by about a factor of four for each increase in the MICS value.
When the MICS value becomes sufficiently large, which for these data was around 15, any particular MICS-sized k-mer occurs Fig.6.
PPV versus number of reads overlapping each splice junction.
As the number of overlapping reads is varied from 1 to 21, the precision of supersplat rapidly approaches and exceeds 90%, reaching 97% with 21 overlapping reads.
Here, the PPV denominator, TP + FP, ranges over 244 782 (single read) to 124 219 (21 overlapping reads).
so rarely in the genomic reference sequence that further increases in the MICS value will yield little to no decrease in runtime.
Identification of reads spanning splice junctions is essential for RNA-seq-based studies of alternative splicing (Filichkin et al., 2010; Sultan et al., 2008) and for assembly of empirical transcription unit models from RNA-seq datasets using tools such as G-Mo.R-Se (Denoeud et al., 2008), Cufflinks http://cufflinks.cbcb.umd.edu/ or TAU (H.D.Priest et al., unpublished data).
As demonstrated, supersplat is an effective algorithm for mining potential splice junction reads from RNA-seq data and its exhaustive search of the potential splice junction sequence space can uncover many previously unknown splice junctions given sufficiently deep transcriptome data.
ACKNOWLEDGEMENTS We thank Dr Sergei Filichkin, Samuel Fox, Mark Dasenko and Steve Drake for assistance with Illumina sequencing, and Chris Sullivan and Scott Givan for assistance with bioinformatics and visualization.
Funding: Oregon State University startup funds (to T.C.M.
); National Science Foundation Plant Genome (grant DBI 0605240, partially); Department of Energy Plant Feedstock Genomics for Bioenergy (grant DE-FG02-08ER64630); Oregon State Agricultural Research Foundation (grant ARF4435 to T.C.M.
); Computational and Genome Biology Initiative Fellowship from Oregon State University (to H.D.P.).
Conflict of Interest: none declared.
ABSTRACT The advent of high-throughput DNA sequencers has increased the pace of collecting enormous amounts of genomic information, yielding billions of nucleotides on a weekly basis.
This advance represents an improvement of two orders of magnitude over traditional Sanger sequencers in terms of the number of nucleotides per unit time, allowing even small groups of researchers to obtain huge volumes of genomic data over fairly short period.
Consequently, a pressing need exists for the development of personalized genome browsers for analyzing these immense amounts of locally stored data.
The UTGB (University of Tokyo Genome Browser) Toolkit is designed to meet three major requirements for personalization of genome browsers: easy installation of the system with minimum efforts, browsing locally stored data and rapid interactive design of web interfaces tailored to individual needs.
The UTGB Toolkit is licensed under an open source license.
Availability: The software is freely available at http://utgenome.org/.
Contact: moris@cb.k.u-tokyo.ac.jp 1 INTRODUCTION Browsing genomic information has been central to life science analysis since large-scale genomes became available for many species including mammals, invertebrates, plants and insects.
To support the task of analyzing genomic information, two categories of genome databases are available.
The first category includes generic genome database species maintained by large organizations, such as Ensembl (Hubbard et al., 2009), UCSC (Kuhn et al., 2009) and NCBI (Wheeler et al., 2007).
The other category includes species-specific genome databases, such as SGD (Hong et al., 2007), FlyBase (Wilson et al., 2008) and Wormbase (Rogers et al., 2007), which utilize general-purpose genome browsers, such as GBrowse (Stein et al., 2002).
Both types of genome browser represent centralized resources because of the high cost of building and maintaining web database servers.
Due to the limited number of experts available to maintain the servers, updates of these centralized web servers are likely to be slow, which is tolerated because the amount of genomic data output by Sanger sequencers is relatively small, yielding on the order of 10 million nt per week.
To whom correspondence should be addressed.
Today, however, the rate at which genome information is produced has outperformed the pace of centralized web browser maintenance.
Indeed, the recent advent of high-throughput DNA sequencers (e.g.
Solexa/Illumina, SOLiD/ABI and 454/Roche) allows even small groups of people to collect huge amount of genomic data in a fairly short period, billions of nucleotides per week, requiring the data analysis to be done as quickly as possible.
In particular, generating tracks for investigating personalized data becomes a crucial step in conducting specific analysis.
The UCSC Genome Browser, for example, offers the function of managing custom tracks that upload and display personal data in local disks to the UCSC Genome Browser together with well-annotated existing tracks.
These functions in traditional genome browsers are useful but suffer from three major drawbacks.
First, the users have to upload their novel and confidential data to the server, though they want to keep these datasets in their local files before publication.
Second, although anonymization of the data is partially supported by UCSC, uploading large volumes of data (e.g.
1 GB of Solexa reads data) to the remote web server still needs enourmous amount of time.
In order to avoid the costs of data uploads, installing a private version of these genome browsers is desirable; however, the task is extremely hard for non-programming biologists because it demands expertise in programming.
Third, personalization of web interface is limited and time consuming, though personalizing web interface through a variety of rearrangements is an important step toward finding new ideas.
For example, Affymetrix developed the Integrated Genome Browser (IGB) (Affymetrix, http://www.affymetrix.com/partners_programs/ programs/developer/tools/affytools.affx.)
for integrating various types of biological data provided by DAS (Distributed Annotation System) (Dowell et al., 2001) servers.
We developed the (University of Tokyo Genome Browser) UTGB Toolkit to provide solutions to these three major requirements: browsing locally stored data, ease of system installation with minimum effort and custom web interface design.
Installing the UTGB Toolkit locally on ones own computer is quite easy for inexperienced users because the system can be installed with a few steps, and also avoids uploading confidential data to the remote server.
Furthermore, the UTGB Toolkit packages ready-to-use functions, such as a stand-alone web server, HTML rendering functionality, database engines, into one component so that these functions can be made available at private sites 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1857 18561861 UTGB toolkit for personalized genome browsers resize drag and drop Structure of gene Frequency of transcription start sites Nucleosome position Genetic variation Specificity of short reads Repeat masker A B C D removal Fig.1.
Rapid creation of tailored interface.
(A) A number of tracks in a UTGB genome browser.
A long track with a large amount of information can be resized to a shorter track interactively using the scroll bar, which allows the user to browse the content of the original long track.
It is also quite easy to eliminate tracks irrelevant to a particular analysis.
(B) The resulting tracks can be reordered to facilitate the further analysis.
(C) Tracks can be rearranged using the interactive drag-and-drop interface.
(D) A genome browser tailored to the analysis of nucleosome positioning surrounding transcriptional start sites and its effect on genetic variation.
The track for specificity of short reads is useful in assessing the uniqueness of short-read alignments on the genome.
It takes <1 min to perform all the steps.
immediately after the installation.
Figure 1 shows how the UTGB Toolkit facilitates personalization of the web interface in the study of epigenomics.
Studying epigenomics involves searching for meaningful combinations from a variety of genome-wide data resources such as DNA methylation, nucleosome positions, transcription start sites, gene expression and evolutionary conservation (Kasahara et al., 2007; Sasaki et al., 2009), which can now be observed via high-throughput sequencers.
Our toolkit makes it quite easy to develop a genome browser with drag-and-drop functions for rearranging, juxtaposing and resizing relevant tracks side-by-side interactively to highlight how epigenetic controls are responsible for gene expression and evolution.
2 METHODS 2.1 Genome browser interface To enhance the portability of the system, the UTGB Toolkit is implemented in Java, a portable programming language, such that its machine codes run on top of the Java Virtual Machine.
Thus, the UTGB Toolkit is executable on most commonly available platforms, including Mac OS X, Windows, Linux, Solaris and FreeBSD.
In addition, UTGB Toolkit is designed such that the browser interface runs on most common web browsers, such as IE, Safari, Firefox and Opera, although it is still necessary to settle discrepancies between these web browsers.
To achieve this goal, the interface is compiled into JavaScript code via the Google Web Toolkit (GWT) compiler, which is capable of subsuming the differences between JavaScript engines in the individual web browsers.
2.2 Portable web server for quickly browsing local resources UTGB Toolkit contains a portable web server so that the genome browser can be launched from the users personal computer.
To avoid manual installation of the web server program (e.g.
Apache), the UTGB Toolkit has an embedded Tomcat web server engine.
The Tomcat server in the UTGB Toolkit works as a stand-alone web server that is not resident on the system.
UTGB Shell launches an instance of Tomcat with the utgb server command, and then deploys the genome-browser program on the local Tomcat server.
1857 [14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1858 18561861 T.L.Saito et al.
The portable web server is useful not only for avoiding the installation process, but also for browsing locally stored data resources without losing data privacy.
With the web server running on the user machine, no need exists to upload confidential data, such as personal genomic data, to a remote database center.
Although anonymization of the data is supported in the UCSC genome browser, the cost of uploading large volumes of data is still prohibitive.
As a solution to this problem, the user can utilize the UTGB running on the local machine to simultaneously display local tracks, whose data are kept on the local hard disk, and publicly available tracks.
2.3 Ensuring portability via the embedded database engine The database management system (DBMS) is an essential component of the genome browser used to provide genomic data for drawing tracks.
However, its installation and setup are quite complicated tasks even for database experts.
To avoid problems in setting up database engines, we embedded the SQLite (http://www.sqlite.org/) database engine into the UTGB Toolkit.
Connections to other DBMS, such as MySQL, PostgreSQL, are also supported in the UTGB Toolkit through JDBC (Java Database Connection) (http://java.sun.com/products/jdbc/).
However, unlike these DBMS that use several files to store the database contents, SQLite is portable in that it uses a single file with a universal format, which can work across several operating systems.
To make the DBMS available in any OS environment, we developed an SQLite JDBC connection library, which packs natively compiled SQLite binaries (SQLite is written in C) for operating systems such as Windows, Mac OS and Linux.
To support other operating systems for which the SQLite binary is not available, our SQLite JDBC library also contains a pure Java SQLite database engine, which works in any environment that supports Java.
Therefore, even if the computer has no DBMS, running the UTGB browser with database support is possible.
In addition, with the embedded SQLite database engine, we can easily port the genome browser to other OS environments; e.g.
we can make a clone of the genome browser simply by copying the browser code and database files.
This portability of the genome browser is a novel feature made possible by the UTGB Toolkit.
2.4 Server-side programming support The UTGB Toolkit is designed to accommodate various requirements of data visualization, as visualization for genomic data tends to be different for each scientific study.
Standard graphical representations provided by existing database centers do not always fulfill user needs.
The UTGB Toolkit allows developers to use their own data visualization programs, e.g.
CGI-based graphic image generators, HTML content renderer.Although many variations in data visualization exist, a common implementation pattern is used in writing web-based graphic generation programs.
For example, a typical pattern of server-side graphic drawing is as follows: the web server receives a user request from the browser, and then issues a query to the database.
The results of the query are translated to class objects (e.g.
gene objects), and finally, image data for visualizing these gene objects are returned to the browser.
This pattern contains three major processes: web request handling, database connection and database object mapping.
Here, we describe libraries for supporting implementation of these common tasks, and how the UTGB Toolkit eases server-side programming.
2.4.1 Web Action The web request handler in the UTGB Toolkit is called a web action, which is a Java class for receiving HTTP requests.
Web actions in the UTGB Toolkit enable developers to rapidly begin coding web interfaces; the utgb action command in UTGB Shell generates a web action instantly.
Each web action is directly mapped to a web server URL.
For example, a web action named sequence corresponds to the URL, http://(server_base_url)/sequence.
Parameter values attached to the URL (e.g.
sequence?name=chr1&start=100) are passed to the web action, and these values are automatically assigned to corresponding variables in the web action class.
Data types of the request parameters, such as integer and string, are detected automatically from the class definitions of web action class, and parameter values in the URL, which are merely a string type, are automatically converted into appropriate data types.
Individual web actions correspond to the genome browser web API.
With the web action mechanism, it is possible to avoid writing repetitive code for request handling and data type conversion.
2.4.2 Database connection The second core component is database connection support.
In general, web database development with Java requires a database server installation and complex configuration files.
Inclusion of the embedded SQLite database engine removes the requirement for database installation, and connections to SQLite databases or other DBMS are immediately available within the web action codes.
UTGB Toolkit supports both local SQLite database files and remote databases connected through JDBC.
These databases can be used by specifying their system types and database location (file names or URLs) in the config/track-config.xml file.
2.4.3 Object database mapping Relational database engines serve table-formatted data, and an impedance mismatch occurs between table data and their memory representations in computer programs.
To use the database data in a program, it is necessary to convert the table data into a more usable format, such as array or class objects in main memory.
The UTGB Toolkit provides the BeanUtil library, which supports translation of table data into class objects.
Similar to the web action handling, table format data are converted to appropriate class objects by investigating the Java class definitions.
Our matching algorithm translates the table data to a set of class objects by comparing column names in the table data and parameter names in the class definition.
The matching algorithm automatically converts the data types between table and class objects using the reflection mechanism in the Java language, which provides the information of parameter names and types in the class definitions.
Thus, no manual mapping configuration is required to bind table data to class objects.
2.5 Importing biological data The UTGB Toolkit supports visualiztion of commonly used biological data formats, such as BED, DAS (Dowell et al., 2001), etc.
To create a track for displaying biological features mapped onto the genome, the user has to specify the data files to load (see the documentation at http://utgenome.org/toolkit/ for details).
The UTGB parses these biological data files in a stream manner and generates graphics that display the features in the region specified in the genome browser.
Access to DAS data is also supported in the UTGB Toolkit for utilizing existing biological data resources in conjunction with the users own tracks.
We are continuing the effort of improving the UTGB Toolkit to support other biological data formats, such as AGP, GFF, PSL, AXT (alignment data), etc.
3 RESULTS 3.1 UTGB framework for browsing various data sources A notable feature of the UTGB Toolkit is its framework design for integrating multiple web resources (Figure 2).
The interface of the UTGB browser consists of tracks, which display individual data resources.
To manipulate a set of tracks at the same time, we provide the notion of the track group, which holds variables that are shared between multiple tracks.
For example, the user can relocate the window positions of several tracks by changing their track group state, e.g.
by clicking the scroll button.
The genome browsers generated by using the UTGB Toolkit can display both public and private data sources on a local machine simultaneously.
1858 [14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1859 18561861 UTGB toolkit for personalized genome browsers Web Browser Web Server Data Resources Web Resorces Web Resources Web Server JavaScript codes generated from Java programs Track group paremters:-start = 1000, end = 2500-sequence_length = (db query)-... interactive  communication HTML/Text/XML Databases Database Query API PostgreSQL, MySQL,  SQLite, etc.
Data Files GET/POST (HTTP) Requests interactive  communication Web Resource  Adapters Interactive communication via AJAX between browser and web servers Track Content Arbitrary web contents, including HTML, texts, images, etc.
generates Track Track Track Group Track Content Track Content Track Content generates generates Fig.2.
Illustration of the UTGB framework.
The UTGB framework has a two-sided design, client-(browser) and server-side code.
For the client side, the UTGB Toolkit generates a web browser interface that consists of a set of tracks.
Individual tracks communicate with the web servers, and produce track contents from the received data in the form of, for example, graphics or table data.
Track groups, which manage a set of tracks, hold common parameters shared among tracks, such as window location on the genome sequence.
On the server side, arbitrary web data sources (e.g.
HTML, text, XML, database query results, etc.)
can be used to generate track contents using mini-browser (iframe) tracks or web resource adapters.
Advanced users can implement tracks in Java, which are compiled into JavaScript code, to provide a more sophisticated user interface.
3.2 Fast and flexible genome browser interface revision The interface of the UTGB genome browser displays a list of tracks, each of which can be dragged to a different location, allowing the user to customize the browser interface online.
The track interface of the UTGB Toolkit is implemented using JavaScript technology, which can dynamically rewrite HTML components in the browser window, so track relocation can be performed without accessing HTML pages on the server.
This feature greatly reduces user frustration when employing traditional genome browsers that reload the entire pages after the track relocation.
Reloading of track contents is independent for each track in the browser.
Therefore, even if the response time of the server providing a track contents is slow, other track contents served by other servers can be displayed immediately after the arrival of the data.
Therefore, the users can continue to relocate sequence positions on the UTGB genome browser seamlessly, eliminating the latency caused by the slowest server to display tracks.
3.3 UTGB shell for quick genome browser development To facilitate rapid genome browser development, we developed the UTGB Shell, a command-line user interface of the UTGB Toolkit.
Figure 3 shows the overview of the UTGB Toolkit and what can be done with the UTGB Shell.
The UTGB Shell has several user commands that support development of a personalized genome browser.
For example, the utgb create command creates a new genome browser in a few seconds, and the utgb server command launches a web server on the local machine, which enables immediate use of the genome browser.
For developers, track programs that use, for example, database searches, data visualization, can be implemented with minimal programming effort because utgb create command generates a genome browser code that already has features such as database connection support, a rich user interface and standard track implementations.
3.4 Stand-alone and web mode Genome browsers generated by the UTGB Toolkit work in two modes, stand-alone and web modes.
In the stand-alone mode, the genome browser runs as a user program on the local computer, so a privileged account is not required to run the genome browser program.
This stand-alone mode is useful in testing the behavior of the browser before publishing.
The web mode is for publishing the genome browser on a server machine.
The browser code is sent to the Tomcat engine, which is a standard web server program for running web applications written in Java.
With utgb deploycommand in the UTGB Shell, we can immediately deploy the genome browser on the Tomcat server.
Even if the server machine already is running another web server, such as Apache, these server programs can coexist by bypassing HTTP requests received by the Apache server to the Tomcat engine through the proxy module, and users can use the genome browser contents as if they were served by the Apache web server.
Detailed information regarding such settings is available from http://utgenome.org/.
3.5 Personalization of genome browser Maintenance of biological databases consists of data conversions to accommodate site-specific data formats, and data submission to appropriate sites.
In general, however, this process is not suited to visualizing the huge amounts of data that are now common in the era of large-scale genome analysis.
In addition, site-specific data formats and their graphical representations may not be ready 1859 [14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1860 18561861 T.L.Saito et al.
Fig.3.
Overview of the UTGB Toolkit.
The UTGB Toolkit supports development of personalized genome browsers in various ways.
The UTGB Shell generates code templates for handling web requests from the genome browser interface (web action), database access support through SQLite JDBC and mapping support from SQL query results to the specified class objects (BeanUtil).
Developers can generate track graphics by using the library included in the UTGB Toolkit or their own programs.
The generated graphics (or arbitrary HTML contents) can be displayed as track contents in the genome browser interface.
To browse both of the locally and remotely stored data, these steps can be performed in a local user machine by launching a local web server from the UTGB Shell.
or extendable to publish a variety of research results.
The UTGB Toolkit tackles these problems by providing a web browser interface to display tracks hosted by multiple web servers in a single window.
A set of standard tracks supporting visualization of data resources is already available.
The framework design of the UTGB Toolkit provides users with flexibility to browse their own data using both preinstalled and their own visualization programs.
To enhance the user experience, we also incorporated the modern web application technologies, such as the Google Web Toolkit (GWT), AJAX and client-side graphic drawing into our toolkit.
These technologies make several features of UTGB, such as drag-and-drops, flexible resizing and smooth scrolling, available to both users and web application developers.
3.6 Efficiency of client-side resource integration The Ensembls Genome Browser (Hubbard et al., 2009) is composed of a single set of image data integrating images of several tracks using server-side programs.
However, this architecture suffers from a major drawback: when the user clicks a browser button to move the location to display, the genome browser must redraw the whole contents already shown on the page.
This type of implementation, which we call server-side integration, is problematic as the server program has to perform similar database queries and repeatedly draw graphics even for slight window relocation.
Without a caching mechanism, this type of implementation cannot work efficiently.
The major reason why the Ensembl Genome Browser merges images is that the HTTP protocol is stateless; i.e.
the browser cannot remember the presented HTML data when the user clicks a link and moves to the next page.
To display the next page, the browser must retrieve the entire contents again from the server as stateless web browsers do not allow partial updating of the genome tracks.
With recent advances in browser technology, it has become possible to change the HTML contents dynamically after loading HTML and image data using the JavaScript language.
Several other technologies are available for drawing graphical contents in the browser, such as Flash, Adobe Air and Microsoft Silverlight.
However, these extensions of the web browser sometimes do not work; e.g.
if the browser does not have the required plug-ins to run these extensions or if plug-in installation is not allowed for non-privileged users.
Therefore, we chose JavaScript which is commonly supported in most modern web browsers, including Internet Explorer (IE), Firefox, Safari and Opera.
These browsers already have an embedded JavaScript engine, and therefore no additional installation process is required.
Scripts written in JavaScript language can update the displayed browser content in situ, and enable the web browsers to remember the state of the web page.
Therefore, modern web browsers already have the capability to partially modify displayed content while retaining necessary information in memory on the browser (client) side.
The UTGB Toolkit fully utilizes this browser (client)-side memory to preserve track contents, which enables the genome browser to support drag-and-drop and resizing of tracks without reloading.
The utilization of client-side memory has attracted a great deal of attention not only for the genome browsers, but also for developing general web applications.
In September 2008, Google launched a new web browser, Google Chrome.
This browser has its own implementation of the JavaScript engine called V8 with major performance improvements in the JavaScript garbage collection mechanism, which efficiently reuses browser-side memory.
The 1860 [14:06 26/6/2009 Bioinformatics-btp350.tex] Page: 1861 18561861 UTGB toolkit for personalized genome browsers open-source web browser Firefox 3 is also continuing to develop the JavaScript engine, and has achieved faster performance than the previous version, Firefox 2.
This trend of improving the JavaScript engine lends marked support for client-side resource integration.
3.7 Server-side and client-side graphic drawing With regard to graphical visualization of genomic data, the UTGB Toolkit provides several ready-to-use graphical representations, such as drawing genes and graph representations.
In the UTGB Toolkit, these visualization supports are available in server-side code, and we are continuing to develop other types of data visualization.
The UTGB Toolkit also supports browser-side graphic drawing with the canvas tag, which will be a standard of browser-side graphic drawing in HTML 5, the next version of HTML.
Several web browsers, including Google Chrome, Firefox, Safari and Opera, support drawing graphics with the canvas tag, with IE being the only major browser not providing support for this tag (as of 2008).
However, IE can draw graphics using Vector Markup Language (VML), which has similar functionality to canvas for drawing graphics.
A team at Google has developed GWT Canvas, a graphic drawing library, which hides differences between VML in IE and the canvas tag, so developers can draw graphics in both IE and other browsers that support this tag.
The UTGB Toolkit uses GWT Canvas to draw custom user tracks.
The capability of drawing graphics in the browser has an impact on genome browser design because it simplifies the role of the server; the server machine has no need to generate graphics, but rather its role is to serve data objects that represent genomic information such as genes and CDS.
The client (browser) receives these data objects and draws their graphical representation on behalf of the server.
This two-sided genome browser design greatly reduces the workload on the server machine and simplifies server-side programming; the server needs only to perform database queries and publish the query results.
3.8 Availability of the UTGB Toolkit The UTGB Toolkit is freely available from the UTGB Project Page (http://utgenome.org/).
All source code is managed using the source revision control system Subversion, so the latest code is made available immediately.
All of our source code is licensed under the Apache License version 2.0, which is an open-source license allowing free use and distribution for both personal and commercial purposes.
The Apache License is applied on a file basis; i.e.
modified source files must be licensed under the Apache License Version 2.0, while users are free to apply any license they wish to source code generated by the UTGB Toolkit and user-developed source code that simply uses the UTGB Toolkit.
4 DISCUSSION In general, database integration takes various forms: a portal, compound and fully integrated.
A portal collects links to internal or external data sources (e.g.
PubMed and ENCODEdb); search engines, such as Google and Yahoo, also belong to this category.
The second type, compound, displays several database contents at the same time in the browser window.
The interface of the UTGB can support this type of data visualization.
The IGB (Affymetrix, http://www.affymetrix.com/ partners_programs/programs/developer/tools/affytools.affx.)
is also a compoond browser and supports the integration of genome data resources provided by DAS protocol using the Java-based GUI.
Another example is iGoogle (http://www.google.co.jp/ig), a web service consisting of user-customizable gadgets, sub windows that can display, for example, news or a calendar.
The third type comprises fully integrated databases that merge several databases into a single DBMS to support queries across these databases; this type of integration is common in centralized database centers such as NCBI and Ensembl.
These various types of database integration each have their own benefits: portals can be used as directories to data resources, compound browsers can collect resources provided by several organizations and full integration is necessary to process queries that cannot be evaluated without integration of databases.
For example, a query listing all SNPs surrounding user-selected genes requires both SNP and gene locus databases.
Without integration of these databases, the user must click the browser buttons numerous times to navigate through unintegrated database browsers.
Our UTGB framework is not restricted to a certain pattern of database integration, and all of three of the above types can be used as back-end databases.
However, to achieve faster query performance and maintenance of the integrated databases, further implementation effort are required, for example, integrated query support, query optimization and user-friendly database management interfaces.
Funding: Japan Science and Technology Agency (JST).
Conflict of Interest: none declared.
ABSTRACT Summary: High-resolution, three-dimensional (3D) imaging of large biological specimens generates massive image datasets that are difficult to navigate, annotate and share effectively.
Inspired by online mapping applications like GoogleMaps, we developed a decentralized web interface that allows seamless navigation of arbitrarily large image stacks.
Our interface provides means for online, collaborative annotation of the biological image data and seamless sharing of regions of interest by bookmarking.
The CATMAID interface enables synchronized navigation through multiple registered datasets even at vastly different scales such as in comparisons between optical and electron microscopy.
Availability: http://fly.mpi-cbg.de/catmaid Contact: tomancak@mpi-cbg.de 1 INTRODUCTION High-throughput and high-resolution imaging technologies generate many more images than can be practically shown in printed journals and thus these massive image datasets are presented to the scientific community through web interfaces.
Recently, a new class of large-scale biological image data emerged that focuses on high-resolution description of large biological specimens using three-dimensional (3D) microscopy techniques.
Since most biological specimens are large in comparison to the scales employed by high-resolution microscopes, the entire specimens are captured by stitching many overlapping image tiles into a single canvas of virtually unlimited size.
Microscopy techniques used in the tiling mode present new challenges for the annotation, analysis and sharing of gigantic datasets.
An analogy can be drawn between high-resolution imaging of large biological specimens and Geographical Information Systems (GIS) showing satellite imagery of the earth.
In both cases, the raw image data must be viewed at a number of different scales to reveal notable features.
Similarly, both data types become meaningful only when significant landmarks in the images are labeled.
For geographical data, an impressive array of computational tools have been developed to represent the imagery overlaid with annotated features to form high-resolution maps of the planet available from everywhere via web-based interfaces.
In biology, To whom correspondence should be addressed.
features such as tissues, cells or organelles can be distinguished on different scale levels and serve as a map to orient in the complex anatomy of the biological entity.
It is clear that large anatomical scans of biological specimens must be accompanied with proper maps of relevant biological features to enable insights into the organization and function of biological systems.
Modern neurobiology is particularly active in mapping high-resolution anatomy (Mikula et al., 2007) and patterns of gene expression (Lein et al., 2007) in the brain.
We present here a decentralized web interface, modeled after GoogleMaps, to navigate large multidimensional biological image datasets and collaboratively annotate features in them.
We demonstrate the navigation, annotation and sharing functionality of the Collaborative Annotation Toolkit for Massive Amounts of Image Data (CATMAID) on a serial section Transmission Electron Microscopy (ssTEM) dataset covering the neuropile of one half of the first instar larval brain of Drosophila melanogaster.
2 IMPLEMENTATION CATMAID combines three main components: a centralized data server, decentralized image servers and the client-side user interface (Fig.1A).
The data server stores meta-information about datasets, users and annotations in a PostgreSQL database.
The entities of the database are projects, stacks, annotations, spatial entities and users.
Projects implicitly define global reference frames in 3D space and thus provide a spatial context for annotations and images.
Stacks are image datasets prepared for viewing through CATMAID (see below).
A stack stores its dimensions in pixels, the 3D resolution in nm/px and a base URL to the image server.
Stacks reference projects through a translation vector.
That is, each stack may appear in several contexts registered relative to different reference frames.
All stacks referencing the same project can be navigated synchronously.
Annotations are textlabels or references to higher level semantic frameworks, e.g.
ontology terms.
An annotation may be referenced by an arbitrary number of spatial entities and vice versa.
Spatial entities are point locations or 3D regions in a project reference frame.
By this means, all image stacks in the same project context share a common set of annotations.
The current implementation supports point locations and textlabels being the simplest form of spatial entities and annotations.
Access to projects is modulated by user privileges whereas users are defined by a unique name and password.
Optionally, projects may be visible for the public.
The user interface is implemented in Javascript.
It requires no third party plugins and runs platform independent on the most popular web browsers.
All interactions between the user interface and the data server are realized as 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:20 26/6/2009 Bioinformatics-btp266.tex] Page: 1985 19841986 CATMAID Fig.1.
(A) Sketch of the information flow between the three components of CATMAID: the central data server, the client-side user interface and several image servers.
(B) Visualization of the multi-scale tiling of images.
In the upper row, an exemplary ssTEM section is overlaid with the tiling grid at 25%, 50% and 100% scale.
Below, exemplary tiles at the respective scale are shown (13).
The location of a tile in the section and in lower scale tiles is indicated with the corresponding number.
(C) The user interface in the synchronous navigation mode with an ssTEM dataset on the left and a relatively registered confocal stack on the right.
asynchronous HTTP data requests using Javascript Object Notation (JSON) for data transfer.
The server-side implementation is realized in PHP.
CATMAID is a web companion to the TrakEM2 software (Cardona, 2006) for management, registration and analysis of large-scale ssTEM datasets.
TrakEM2 is able to export image data in CATMAID compatible format.
While the classical GIS presents the surface of a planet that is a 2D dataset, CATMAID was designed to show 3D microscopy data, in particular from serially sectioned volumes.
The canvas provides the means to zoom and pan a section and navigate through the volume alongside the section index.
For rapid browsing at multiple scales, we consider each section as a tiled scale pyramid (Fig.1B).
Each scale level has half the resolution of the previous and is split into 256256px-tiles that are saved in JPEG-compressed format.
By this means, the browser requests only those tiles that are visible at once.
Instead of generating the tiles on the fly from the original image data, we initially generate the whole tiled scale pyramid and store it to the file system.
This results in 1 13 the number of pixels of the original image, which is typically reduced to about 10% storage space by JPEG-compression.
Tiles are ordered by file name convention.
Each section is stored in a directory whose name is the section index.
A tiles name contains the row and column in tile coordinates and the scale index i where f =1/2i is the scale factor relative to the original resolution.
For example, 4/14_20_2.jpg identifies the tile at row 14 and column 20 in Section 4 at scale level 2.
The scale pyramid can be exported directly from qualified software such as TrakEM2 or alternatively generated using a custom ImageMagick shell script that is executed locally on the image server.
Subsequently, the user registers the dataset as a project within the CATMAID viewer by providing a world-accessible URL pointing to the scale pyramid.
In this way, the primary image data remain decentralized, while the project properties stored in a centralized database enable cross-referencing.
3 DESCRIPTION OF THE USER INTERFACE Initially, the interface shows the project toolbar that contains a pull-down list for stack selection and input fields for user login.
The main screen shows a comprehensive list containing all accessible projects and related stacks.
After successful user log-in, the pull-down list and the main screen are updated respectively.
As soon as a stack is opened in the context of a project by selection in the pull-down or on the main window, the navigation toolbar appears.
All stacks referencing the same project can be opened at the same time.
The focused stack binds and updates all control devices in the navigation toolbar.
The user can navigate all opened stacks with mouse, mouse wheel, keyboard or by using the sliders and input fields in the navigation toolbar.
Each navigation command is caught by the focused stack, transferred into the project coordinate frame and sent to the project instance.
The project instance propagates the instruction to all opened stacks.
In this way, all opened stacks are navigated in perfect synchrony regardless of their resolution or scale.
Annotations are placed directly on top of a stack.
The textlabel toolbar provides input elements to change color and font size.
Annotations reference the project as a whole, that is, each stack that is linked to the project will display all project annotations regardless of whether they were created on top of another stack.
We implemented an asynchronous message system for long-term server-side image processing tasks such as retrieval of a small 3D subset of the entire dataset (micro-stack) for offline processing.
Such a job is processed on the server side and notifies the user when it is finished.
The interface can export the current view including all opened stacks, location and scale as a parameterized URL (bookmark).
If invoked by such a URL, the interface immediately recovers the view.
In this way, researchers can easily share bookmarks pointing to particular regions of interest.
4 RESULTS AND DISCUSSION We demonstrate the application of the viewer on two tiled serial section TEM datasets of Drosophila first instar larval brains (http://fly.mpi-cbg.de/catmaid-suppl).
The registered dataset consists of 85 sections of 60nm thickness and shows lateral neuronal layers and part of the neuropile.
It was imaged using a moving stage operated by the Leginon software (Suloway et al., 2005).
The images are 20482048px at 4nm/px and were taken with 6% tile overlap, 99 images per section, resulting in 6885 images.
The tiles were registered both within and across sections using a fully automatic global registration approach implemented as part of TrakEM2 (Saalfeld et al., manuscript in preparation).
The registered Drosophila first instar larval brain dataset was converted into a CATMAID compatible scale pyramid yielding 21846 tiles per section and 1856910 tiles for the whole dataset.
For such a massive 3D image mosaic covering a substantial portion of the larval brain at 4nm/px resolution, the CATMAID interface offers unprecedented flexibility in navigation, and enables collaborative 1985 [15:20 26/6/2009 Bioinformatics-btp266.tex] Page: 1986 19841986 S.Saalfeld et al.
annotation and sharing of the locations of regions of interest via bookmarks.
Moreover, the interface allows linked navigation of multiple registered datasets, even at vastly different resolutions such as that of electron and confocal microscopy (Fig.1C; http://fly.mpi-cbg.de/catmaid-suppl).
The CATMAID interface is applicable to any 2D or 3D multimodal biological image datasets, as shown in examples of stitched confocal 3D volumes (http://fly.mpi-cbg.de/catmaid-suppl).
The tool will become especially powerful when comparing registered 3D stacks of different biological specimens labeled to visualize tissue-specific gene expression.
Additionally, the CATMAID interface can be used to navigate, annotate and bookmark locations in any large image canvas.
Some of the possible applications in biology are viewing of scientific posters and browsing large-scale in situ image datasets (Tomanck et al., 2007; http://fly.mpi-cbg.de/catmaid-suppl).
Future versions of the interface will feature ontology-based project-specific semantic frameworks and interactive tools for drawing of 3D regions of interest.
We plan to expand the scope of the viewer to support an arbitrary number of dimensions allowing navigation of multimodal, time-lapse microscopy data.
Conflict of Interest: none declared.
ABSTRACT Motivation: The study of metabolites (metabolomics) is increasingly being applied to investigate microbial, plant, environmental and mammalian systems.
One of the limiting factors is that of chemically identifying metabolites from mass spectrometric signals present in complex datasets.
Results: Three workflows have been developed to allow for the rapid, automated and high-throughput annotation and putative metabolite identification of electrospray LC-MS-derived metabolomic datasets.
The collection of workflows are defined as PUTMEDID_LCMS and perform feature annotation, matching of accurate m/z to the accurate mass of neutral molecules and associated molecular formula and matching of the molecular formulae to a reference file of metabolites.
The software is independent of the instrument and data pre-processing applied.
The number of false positives is reduced by eliminating the inaccurate matching of many artifact, isotope, multiply charged and complex adduct peaks through complex interrogation of experimental data.
Availability: The workflows, standard operating procedure and further information are publicly available at http://www.mcisb.org/ resources/putmedid.html.
Contact: warwick.dunn@manchester.ac.uk Received on November 5, 2010; revised on February 4, 2011; accepted on February 7, 2011 1 INTRODUCTION Systems biology is applied to study the components of, and more importantly their complex interactions in, biological systems.
One set of components which are studied in systems biology investigations are metabolites, either by targeted or holistic profiling experimental strategies (Dunn et al., 2011).
Low molecular weight inorganic and organic metabolites play important roles in the To whom correspondence should be addressed.
operation and maintenance of biological systems.
The study of metabolites (metabolomics) is increasingly being applied to investigate microbial (Bradley et al., 2009; MacKenzie et al., 2008; Mashego et al., 2007), plant (Allwood et al., 2008; Fernie and Schauer, 2009; Hall et al., 2008), environmental (Bundy et al., 2009; Viant et al., 2006) and mammalian (Griffin, 2008; Kenny et al., 2010; Lewis et al., 2008) systems.
Many studies follow a hypothesis generating or inductive strategy (Kell and Oliver, 2004) and start from a small and known subset of biological knowledge.
Valid experiments are designed to acquire robust and reproducible data on a wide range of different metabolites and metabolite classes from carefully selected samples.
Subsequent data analysis procedures define the metabolic differences associated with a biological change related to genotype, biological perturbation or environmental intervention (for example, drug therapy).
These studies employ a metabolic profiling strategy to detect a wide range of (but not all) metabolites from numerous biochemical classes to obtain maximum metabolic information rapidly.
This strategy provides the detection of hundreds or thousands of metabolites.
However, due to the diverse range of chemical and physical properties and the wide concentration range of metabolites within the metabolome, no single analytical technology can provide the non-biased quantitative detection of all metabolites in a biological system (Dunn, 2008).
Metabolic profiling provides semi-quantitative data, typically as chromatographic peak areas, rather than absolute quantitation where metabolite concentrations would be reported.
Mass spectrometry (MS) and nuclear magnetic resonance spectroscopy (NMR) have been widely employed and provide complementary roles (Dunn et al., 2005, 2011).
Chromatography-MS techniques provide advantages for these highly complex biological samples and include gas chromatography (GC-MS), liquid chromatography (LC-MS) and LC derivatives including ultra performance liquid chromatography (UPLC-MS).
Capillary Electrophoresis-MS (Soga et al., 2003) and LC-MS apply electrospray ionization.
Direct infusion mass spectrometry (DIMS) can also be applied, though lacks the chromatographic separation of 1108 The Author 2011.
Published by Oxford University Press.
All rights reserved.
For Permissions, please email: journals.permissions@oup.com at O U P site access on July 12, 2013nloaded from  [11:35 21/3/2011 Bioinformatics-btr079.tex] Page: 1109 11081112 Automated workflows for accurate mass-based putative metabolite identification metabolites (Dunn et al., 2005; Southam et al., 2007).
Hyphenated platforms can provide (with suitable operation and mass calibration) high separation resolution, high mass resolution and mass accuracy, typical limits of detection of micromol per litre and the ability to identify metabolites through a combination of Retention Time (RT)/index, accurate mass and gas-phase fragmentation-derived mass spectra.
Each of these platforms, whether hyphenated or non-hyphenated, provide different advantages and disadvantages for metabolite identification as has been reviewed previously (Dunn et al., 2011).
The increasing use of high mass resolution LC-MS and UPLC-MS platforms provides the detection of many thousands of features [see Brown et al.
(2009) for a comparison of the features detected related to sample type] with high mass accuracy and has led to a need to develop data handling methods for the conversion of this raw analytical data into biological knowledge.
One of the data processing procedures essential in metabolic profiling is metabolite identification which has been reviewed previously (Dunn et al., 2011; Wishart, 2009).
Guidelines have been provided by the Metabolomics Standards Initiative to define how the different levels of metabolite identification can be reported (Sumner et al., 2007).
A range of approaches can be applied for metabolite identification.
Two generalized types of identification are achievable: putative identification and definitive identification.
Putative identification usually employs one or more molecular properties for identification, but does not compare these to the same properties of an authentic chemical standard as is performed for definitive identification.
The accurate mass (or m/z) of an analyte and its associated isotopologues can be used to define molecular formulae (MFs) from which suitable metabolites can be derived by searching a range of electronic resources [e.g.
PubChem (http://pubchem.ncbi.nlm.nih.gov/), HMDB (http://www.hmdb.ca/), KEGG (http://www.genome.jp/kegg/) and MMD (Brown et al., 2009)] and has been previously shown (Brown et al., 2005; Junot et al., 2010; Lane et al., 2008; Rogers et al., 2009).
Direct matching of accurate mass (or m/z) to data in electronic resources without intermediate matching to MF can also be performed.
However, structural isomers and stereoisomers have the same accurate mass and therefore require a separate, orthogonal property for identification of all potential isomers.
Typically, this is chromatographic separation though separation of isomers is not always achievable.
Separation of enantiomers requires a chiral chromatography column.
Definitive identification employs at least two properties (typically RT or index and fragmentation mass spectrum) and compares these properties to an authentic chemical standard analysed under identical analytical conditions.
In LC-MS and UPLC-MS applications, the accurate masses of the detected ions is employed in combination with other rules (e.g.
isotope ratio of 12C and 13C isotopic peaks to define the number of carbon atoms present in the MF; calculated as peak area 13C isotopologue/peak area 12C isotopologue) to generate MF and thus provide putative metabolite identification(s).
Specific rules are not always applicable.
For example, 13C/12C isotopic peaks can only be applied on instruments where accurate isotopic ratios are detected and where 13C-artificially labelled metabolites have not been applied in the biological experiment.
Fragmentation mass spectra (MS/MS or MSn) are then used to provide increased confidence through comparison to authentic chemical standards or to in silico-derived fragmentation mass spectra to give an unequivocal metabolite identification (Wolf et al., 2010).
It should be noted that not all authentic chemical standards are commercially available and that MS/MS and MSn fragmentation mass spectra are not always accurate in unequivocal identification of two isomeric metabolites.
Studies by the authors have assessed the level of complexity of electrospray UPLC-MS data derived from biological extracts in a metabolic profiling strategy (Brown et al., 2009).
This work has shown that a multitude of different ion types are observed including commonly described ions (e.g.
protonated, deprotonated, sodium or potassium adducts and 13C isotope).
However, many other unexpected types of ions including adducts (e.g.
complex combinations of sodium chloride and formate dependant on the matrix type and mobile phase), fragments, dimers, multiply charged and instrument specific ions (e.g.
Fourier Transform (FT) artifact peaks) are also detected.
Each different ion type is defined as a feature, whose accurate mass (or m/z) is unique but whose RT and chromatographic peak profiles are identical.
Information on the type of ion should be applied in metabolite identification (Brown et al., 2009; Draper et al., 2009).
Automated software or workflows for high-throughput identification of large metabolomic datasets are not freely available.
Currently, metabolite identification is a manual or semi-automated process assessing those features of biological interest and not the complete set of detected features (Dunn, 2008).
For metabolomics to be successful it is essential to derive biological knowledge from analytical data, a view emphasized by a recent Metabolomics ASMS Workshop Survey 2009 which found that the biggest bottlenecks in metabolomics were thought to be identification of metabolites and assigning of biological interest (http://fiehnlab.ucdavis.edu/staff/ kind/Metabolomics-Survey-2009).
To fill the gap in requirements, three workflows have been written to perform for the first time integrated, automated and high-throughput annotation and putative metabolite identification of electrospray LC-MS and UPLC-MS metabolomic datasets in a freely available package.
The workflows were developed in the Taverna Workflow Management System to provide flexibility in their operation and the ability to rapidly and simply integrate with web services and other Taverna workflows in the future [for example, see Li et al.
(2008)], so as to provide integrated data analysis and bioinformatics or cheminformatics packages.
Examples are available on myExperiment, a repository of workflows freely available to the scientific community (http://www.myexperiment.org/), including a workflow to perform data pre-processing with XCMS and a workflow to perform in silico fragmentation applying MetFrag.
The achievement of this level of integration would be more technically demanding and would require significantly greater expertise and time if coded in many other programming languages.
Taverna is also easy to operate for relative novices with minimal training as the process involves defining parameter values and files only.
2 METHODS AND IMPLEMENTATION The current lack of freely available workflows or software to process deconvoluted data acquired from electrospray LC-MS experiments led the authors to develop three workflows.
The workflows have been developed in Taverna (Hull et al., 2006) using Beanshell, a Java scripting language, which is enabled in Taverna and can perform data manipulation, parsing and formatting.
Taverna can be downloaded from http://www.taverna.org.uk.
The workflows were developed under Windows using Taverna v1.7.0 1109 at O U P site access on July 12, 2013nloaded from  [11:35 21/3/2011 Bioinformatics-btr079.tex] Page: 1110 11081112 M.Brown et al.
and subsequently tested using Taverna Workbench 2.2.0.
In combination, the workflows perform the automated, high-throughput annotation and putative metabolite identification of electrospray LC-MS and UPLC-MS metabolomic datasets.
The software has been coded as a series of separate workflows to allow more flexibility in the analysis of data by obviating the need to re-run the whole pipeline when altering one of the workflow parameters, such as the mass tolerance or database.
This approach also reduces the likelihood of memory problems when handling large datasets on computers with small RAM.
The workflows, related files and standard operating procedure (SOP) are available to the user community on http://www.mcisb.org/resources/ putmedid.html and will also be placed on MyExperiment (http://www .myexperiment.org/).
In general, the input and output files are tab-delimited (*.txt) files and are sorted by ascending accurate mass or MF as appropriate (ordered as C, H, N, O, P, S, Br, Cl, F, Si in ascending alphanumeric form as is standard for PubChem).
Internal checks are made within the workflows to ensure that the number of features in both peak and data files match (workflow for correlation analysis) and that the study and reference input files are sorted either by accurate m/z (workflow for metabolic feature annotation) or MF (workflow for metabolite annotation).
Termination of the process and reporting of an informative error message occurs if this is not the case.
The three workflows are described in detail in the available SOP and briefly below.
2.1 Workflow for correlation analysis The workflow for correlation analysis (List_CorrData) allows the user to calculate either Pearson or Spearman rank correlations or read in previously calculated correlation data.
The correlation calculations allow for NaN, Inf and 0 in the input data and are equivalent to using the Matlab (http://www.mathworks.co.uk/) corr function with the following parameters: corr(Xdata, rows, pairwise, type, Pearson) or corr(Xdata, rows, pairwise, type, Spearman) 2.2 Workflow for metabolic feature annotation The workflow for metabolic feature annotation (annotate_MassMatch) uses correlation coefficient information calculated in the workflow for correlation analysis, accurate m/z difference, RT and median peak area data to group together and annotate features with the type of ion (isotope, adduct, dimer, others) originating from the same metabolite.
The same metabolite can be detected as different ion types each with different m/z.
Following annotation, the experimentally determined accurate m/z are matched to the accurate m/z of unique MF in a reference file within a specified m/z tolerance.
2.3 Workflow for metabolite annotation In the workflow for metabolite annotation (matchMMF_MF), the MF from the output file calculated in the workflow for metabolic feature annotation is matched to the MF from the Reference file of metabolites (trimMMD_sortMF.txt or other appropriate reference file).
The metabolite information for all matched MFs is added to the input data and output data are generated in three formats, each of which can be saved as tab-delimited files by the user.
3 RESULTS An assessment has been made of the workflows ability to perform putative metabolite identification using two reference files: (i) a listing of unique accurate mass/MF data from PubChem using specific elements (C, H, N, O, P, S, Br, Cl, F, Si only; file downloaded from http://fiehnlab.ucdavis.edu/projects/Seven_Golden_Rules/) selected to give a wider selection of MFs and (ii) The Manchester Metabolomics Database (Brown et al., 2009) constructed with data Table 1.
Distribution of annotated peaks in negative and positive ion mode Features summary Negative ion mode Positive ion mode No.
of features 2173 4348 No.
of correlations (>0.7, RT 5 s) 11 867 50 867 Invalid RT (40 s < RT > 1200 s) 224 487 FT artifact peaks 61 66 Isotopes (13C, 34S, 37Cl) 455 1170 Multiply charged ions 22 444 Salt ions (not adducted to metabolites) 40 31 Total no.
of excluded features 802 (36.9%) 2198 (50.6%) No.
of features remaining for identification 1371 (63.1% of all detected features) 2150 (49.4% of all detected features) from genome-scale metabolic reconstructions, HMDB, KEGG, LIPIDMAPS, BioCyc and DrugBank.
Data from all these sources are included to provide a comprehensive set of metabolites.
For example, HMDB does not contain all lipids that are theoretically present in human biofluids and tissues and therefore inclusion of data from LIPIDMAPS provides greater complementary metabolite coverage.
A clinical dataset of fasting blood serum samples were taken from participants according to ethical guidelines and stored before being analysed with quality control samples in a random order and within 48 h of reconstitution using an UPLC (Waters UPLC Acquity, Elstree, UK) coupled on-line to an electrospray LTQ-Orbitrap hybrid mass spectrometer (ThermoFisher Scientific, Bremen, Germany).
The collection and storage of serum samples and the UPLC and mass spectrometer methods applied have been previously described (Dunn et al., 2008; Zelena et al., 2009).
Independent samples (118 in total) were analysed in both positive and negative ion mode.
Raw data files (.RAW) were converted to the NetCDF format using the File converter program in XCalibur (ThermoFisher Scientific, Bremen, Germany).
Deconvolution of data was performed using XCMS, running on R version 2.6.0, an open-source deconvolution program available for LC-MS data (Smith et al., 2006) using identical settings to those reported previously (Dunn et al., 2008).
This produced a list of features with associated RT, accurate m/z and chromatographic peak area.
For these data, the mass accuracy was assessed using a set of 35 and 50 metabolites commonly detected in serum and plasma in positive and negative ion modes, respectively.
Shown in Table 1 is the distribution of annotated features found in the dataset and excluded from further mass matching.
In positive ion mode >50% of features were marked for exclusion from further metabolite identification, which was considerably higher than in negative ion mode (36.9%) due in part to the much greater occurrence of multiply charged ions (10% of all features).
It should be noted that multiply charged ions can be peptides, proteins or high molecular weight metabolites capable of carrying multiple charges.
Approximately 25% of these excluded features are isotopic peaks.
These are annotated and linked to the related molecular ion.
In the workflow for metabolite annotation, all isotope and FT artifact peaks are labelled with the accurate identification observed for the molecular or adduct ions.
1110 at O U P site access on July 12, 2013nloaded from  [11:35 21/3/2011 Bioinformatics-btr079.tex] Page: 1111 11081112 Automated workflows for accurate mass-based putative metabolite identification Using a mass tolerance of 3 p.p.m., 6% of the remaining features were not matched to any unique MFs in the PubChem reference file (301507 entries).
Seventy-six percent in negative and 60% in positive ion mode of the remaining features (791 and 1292 features) were fully annotated and given putative metabolite identification using a revised version of the MMD database (31648 entries).
The reference data file is based on molecules and parent compounds carrying no charge and is derived from the MMD which contains an array of information from a wide variety of electronic resources.
The MMD data file was revised by removal (or modification) of charged species of salts e.g.
sodium ascorbate, calcium citrate, metamphetamine hydrochloride.
Obvious duplicates of data were removed and for many common metabolites e.g.
amino acids and sugars only a single stereochemical form of the compound was retained.
The included form usually related to the one most well described in HMDB, and if not present in HMDB then as described in KEGG, and if not present in HMDB and KEGG then as described in LIPIDMAPS and resulted in a much cleaner dataset for putative metabolite identification.
A fairly stringent mass tolerance of 3 p.p.m.
was used in this analysis and in a number of cases the annotation of adducts is based on strong evidence, but the exact mass matching may be outside the allowed tolerance.
This is certainly seen for metabolites such as tryptophan where many ions/adducts are detected, some of which are within the 3 p.p.m.
mass tolerance and others, particularly K and NaCl/HCOONa adducts of low response, are in the mass error range of 310 p.p.m.
Increasing the mass tolerance would result in more of these adducts being correctly matched but would greatly increase the overall number of putative metabolite identifications through matching to a greater number of MFs.
This is possible if further data are acquired to reduce the number of potential hits (e.g.
13C/12C isotope ratios or MS/MS fragmentation).
Additionally, neither reference file had complete information relevant to the human metabolomethe MMD is from a wide variety of sources and includes drugs (but not drug metabolites), and the PubChem reference file contains a limited number of elements and limited numbers of atoms per element.
The workflow for correlation analysis processed correlation data in 320 min depending on the option selected and the number of features.
The workflow for metabolic feature annotation processed the negative ion data in 3 min for the PubChem reference file.
In positive ion mode with 100% more peaks and nearly 5 times as many correlations, the processing time was <20 min.
The number of correlations is the rate determining factor and in most cases processing is <30 min and frequently <5 min.
The workflow for metabolite annotation matched MF derived from experimental data to MF in MMD rapidly when fewer than 5000 matches were present.
This process took just over 1 min to perform in each ion mode.
However, when using large reference files such as PubChem, typically up to 30 000 matches, processing time may be of the order of 1 h. The workflows developed are automated, rapid, open-source and freely available with all features fully annotated or given putative metabolite identifications.
The approach is flexible, it is independent of chromatographic deconvolution method and analytical instrument applied.
Additional adducts can be added to the adducts file for user-specific instruments, and data and organism-specific metabolite reference files can be used.
Two additional differently formatted outputs are available for all matched features and the workflows have the potential to be integrated with other Taverna workflows.
Large numbers of features are recognized as artifact, isotope, salt and multiply charged ions and removed from the metabolite identification process.
This in combination with data with good mass accuracy (in raw data or following post-acquisition mass alignment) results in a greatly reduced number of putative metabolite identifications within a specified mass tolerance (typically 3 p.p.m.
for the ThermoFisher Scientific LTQ-Orbitrap acquired data).
Using this approach, putative metabolite identification does not depend on a high level of experience in dealing with MS data and can be used as a starting point for subsequent definitive identification.
A number of false positives are always found although they are significantly reduced using this approach by annotation of ion type prior to assignment of accurate m/z to MF or metabolite.
Wide variation in m/z or RT range or missing values following deconvolution can result in any or all of the following: (i) mass difference can be outside mass tolerance limits (e.g.
missed adduct); (ii) RT difference between two features may be outside given value (missed grouping); and (iii) correlation between two features may be below specified limit (missed adduct, missed grouping).
Within the software, allowance is made for this, for example, a feature (sodium adduct) that has a correlation with the parent metabolite below the correlation limit will not be grouped with this feature.
However, if the m/z is within the mass tolerance it will still be putatively identified as the appropriate sodiated ion.
Additional grouping information based on correlation is present in the output file and can assist the user when two or more MFs matches are reported for a feature.
4 CONCLUSIONS The workflows presented are rapid and high-throughput and greatly reduce the number of false positives by eliminating the inaccurate matching of many artifact, isotope and complex adduct peaks.
Subsequent definitive identification employing at least two properties of sample-derived metabolite and an authentic chemical standard (typically RT and fragmentation mass spectrum) can then be performed.
Additional information based on similarity measures (e.g.
metabolite class or metabolite pathway) are being incorporated into the Manchester Metabolomics Database and will allow in time for further interrogation of the biological changes of interest within microbial, plant and mammalian metabolomic studies.
Further developments are planned to amalgamate the separate workflows together and to integrate with separate workflows to increase the applicability and ease of data analysis and interpretation.
Funding: UK Biotechnology and Biological Sciences Research Council (BBSRC) (BBC0082191); The Wellcome Trust (088075/A/08/Z); Johnson and Johnson, Cancer Research UK; The Manchester National Institute for Health Research (NIHR) Biomedical Research Centre.
Conflict of interest: none declared.
ABSTRACT Motivation: Modern systems biology aims at understanding how the different molecular components of a biological cell interact.
Often, cellular functions are performed by complexes consisting of many different proteins.
The composition of these complexes may change according to the cellular environment, and one protein may be involved in several different processes.
The automatic discovery of functional complexes from protein interaction data is challenging.
While previous approaches use approximations to extract dense modules, our approach exactly solves the problem of dense module enumeration.
Furthermore, constraints from additional information sources such as gene expression and phenotype data can be integrated, so we can systematically mine for dense modules with interesting profiles.
Results: Given a weighted protein interaction network, our method discovers all protein sets that satisfy a user-defined minimum density threshold.
We employ a reverse search strategy, which allows us to exploit the density criterion in an efficient way.
Our experiments show that the novel approach is feasible and produces biologically meaningful results.
In comparative validation studies using yeast data, the method achieved the best overall prediction performance with respect to confirmed complexes.
Moreover, by enhancing the yeast network with phenotypic and phylogenetic profiles and the human network with tissue-specific expression data, we identified condition-dependent complex variants.
Availability: A C++ implementation of the algorithm is available atContact: koji.tsuda@tuebingen.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Today, a large number of databases provide access to experimentally observed proteinprotein interactions (PPIs).
The analysis of the corresponding protein interaction networks can be useful for functional annotation of previously uncharacterized genes as well as for revealing additional functionality of known genes.
Often, function prediction involves an intermediate step where clusters of densely interacting proteins, called modules, are extracted from To whom correspondence should be addressed.
the network; the dense subgraphs are likely to represent functional protein complexes (Sharan et al., 2007).
However, the experimental methods are not always reliable, which means that the interaction network may contain false positive edges.
Therefore, confidence weights of interactions should be taken into account.
A natural criterion that combines these two aspects is the average pairwise interaction weight within a module [assuming a weight of zero for unobserved interactions (Ulitsky and Shamir, 2007)].
We call this the module density, in analogy to unweighted networks (Bader and Hogue, 2003).
We present a method to enumerate all modules that exceed a given density threshold.
It solves the problem efficiently via a simple and elegant reverse search algorithm, extending the unweighted network approach by Uno (2007).
Remarkably, the required computation time between two consecutive solutions is polynomial in the input size.
The contribution of our article consists in (i) the development of a dense module enumeration (DME) algorithm for weighted networks, including a ranking scheme and an efficient strategy to identify locally maximal modules, (ii) its application to the protein interaction networks of yeast and human and (iii) the effective integration of constraints from additional data sources.
There is a large variety of related work on module discovery in networks.
The most common group are graph partitioning methods (Chen and Yuan, 2006; Newman, 2006; van Dongen, 2000).
They divide the network into a set of modules, so their approach is substantially different from DME, which provides an explicit density criterion for modules (Fig.1A).
Another group of methods define explicit module criteria, but employ heuristic search techniques to find the modules (Bader and Hogue, 2003; Everett et al., 2006).
This contrasts with complete enumeration algorithms, which form the third line of research: they give explicit criteria and return all modules that satisfy them.
For example, clique search has been frequently applied (Palla et al., 2005; Spirin and Mirny, 2003).
The enumeration of cliques can be considered as a special case of our approach, restricting it to unweighted graphs and a density threshold of one.
Further enumerative approaches use different module criteria assuming unweighted graphs (Haraguchi and Okubo, 2006; Zeng et al., 2006).
Biological complexes are dynamic objects of changing composition.
In particular, many proteins are not steadily present in the cell, but specifically expressed depending on organism, cell type, environmental conditions and developmental stage 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 934 933940 E.Georgii et al.
Fig.1.
DME approach.
(A).
DME versus partitioning.
While partitioning methods return one clustering of the network, DME discovers all modules that satisfy a minimum density threshold.
(B).
Combination with profile data.
Integration of PPI and external profile data allows to focus on modules with consistent behavior of all member proteins in a subset of conditions.
(Gavin et al., 2002).
Module enumeration offers a meaningful way to detect such variations of complexes.
Our DME algorithm can easily incorporate constraints from additional information like gene expression, evolutionary conservation, subcellular localization or phenotypic profiles.
Thus, the search can be guided directly towards the modules of interest, for example, modules that show coherent behavior in a subset of conditions.
The external data sources can provide further evidence for functional relationships of proteins and yield insights about possible functional roles of complexes and subcomplexes in different cellular contexts.
In recent years, many module finding approaches which integrate PPI networks with other gene-related data have been published.
One strategy, often used in the context of partitioning methods, is to build a new network whose edge weights are determined by multiple data sources (Hanisch et al., 2002).
Tanay et al., 2004 also create one single network to analyze multiple genomic data at once; however, they use a bipartite network where each edge corresponds to one data type only.
In both cases, the different datasets have to be normalized appropriately before they can be integrated.
In contrast to that, other approaches keep the data sources separate and define individual constraints for each of them.
Consequently, arbitrarily many datasets can be jointly analyzed without the need to take care of appropriate scaling or normalization.
Within this class of approaches, there exist two main strategies to deal with profile data like gene expression measurements.
In the first case, the profile information is transformed into a gene similarity network, where the strength of a link between two genes represents the global similarity of their profiles (Pei et al., 2005; Segal et al., 2003; Ulitsky and Shamir, 2007).
In the second case, the condition-specific information is kept to perform a context-dependent module analysis (Huang et al., 2007; Ideker et al., 2002; Yan et al., 2007).
Our approach follows along this line, searching for modules in the PPI network that have consistent profiles with respect to a subset of conditions.
In contrast to the previous methods, our algorithm systematically identifies all modules satisfying a density criterion and optional consistency constraints.
In this study, we evaluate our approach on the yeast interaction network in comparison with four other methods.
Also, we report yeast modules restricted by evolutionary conservation and phenotypic profiles.
Furthermore, we discuss our results obtained from human protein interactions in the context of gene expression data.
2 MODULE MINING APPROACH We address the problem of extracting functional modules from PPI data using an enumerative density-based mining approach.
Today, there exist various experimental techniques to determine PPIs.
To analyze these data, it is common practice to integrate all interactions into one network where each node represents a protein, and an edge between two nodes indicates an interaction (Sharan et al., 2007).
Then node sets with higher density in the interaction network are more likely to represent functional protein complexes.
We propose a method to exhaustively enumerate all modules which satisfy a minimum density threshold.
To avoid spurious modules, confidence weights of interactions are taken into account.
In this section, we first describe the basic algorithm and then show how to integrate additional constraints in this framework.
Finally, we explain our module ranking criterion.
2.1 Dense module enumeration Formally, let us consider the interaction network as undirected weighted graph with node set V .
Let W = (wij)i,jV be the corresponding matrix representation, containing positive weight entries for the given interactions and zero entries otherwise (for missing edges).
In the following, we assume wij 1.
Although we use weight matrices with non-negative entries in this work, the approach is suitable for mixed-sign data as well.
A module is defined as a set of nodes U V and its induced subgraph.
The density of U refers to the average pairwise weight, given by W (U)= i,jU,i<j wij |U|(|U|1)/2 .
(1) The largest possible density value is 1 [we define W (U) :=1 for |U|=1].
Now we define the problem of DME as follows.
Definition 1.
Given a graph with node set V and weight matrix W, and a density threshold >0, find all modules U V with W (U) .
The key point of any enumeration algorithm is the definition of an appropriate structure of the search space which allows for efficient traversal and pruning.
To enumerate sets of entities, a canonical approach is to start with the empty set and then iteratively form larger sets by adding one element at a time; if it is evident that no further solutions can be derived from a certain set, the process of extension is stopped, i.e.
unnecessary parts of the search space are pruned.
It turns out that conventional pruning strategies as used in itemset mining (Han and Kamber, 2006), for example, are not suitable for DME.
The reason is that supersets of a module can in general have arbitrarily higher or lower density than the module itself (see Supplementary Material).
However, it is possible to traverse the search space in a way that allows for straightforward pruning.
In fact, we define a tree-based parent child relationship between modules such that along each path from the root to a leaf, the module size is increasing, whereas the module density is monotonically decreasing.
Technically, our algorithm adopts the reverse search paradigm (Avis and Fukuda, 1996): in each iteration, we generate all direct supersets of the current module and select those which are indeed its children.
Due to the monotonicity guarantee in our search tree, only children 934 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 935 933940 Condition-dependent dense module enumeration that fulfill the density criterion have to be further processed.
To describe our approach in more detail, we need the definition of weighted degree.
Definition 2.
Let W be the given weight matrix.
For uU V, the weighted degree of u with respect to U is defined as degW (u,U)= jU,j =u wuj .
The following lemma yields the key for defining the search tree.
Lemma 1.
Let vU be a node with minimum weighted degree in U, i.e.
uU : degW (u,U)degW (v,U).
Then, W (U \{v})W (U).
The proof is given in the Supplementary Material.
Further, we introduce a function ord, which defines a strict total ordering on the nodes, i.e.
for each node pair u, v with u =v either ord(u)<ord(v) or ord(u)>ord(v) holds.
With this, we define the parentchild relationship for modules.
Definition 3.
Let U be a module and vV \U.
U :=U {v} is a child of U if and only if uU one of the following conditions holds: 1. degW (v,U )<degW (u,U) 2. degW (v,U )=degW (u,U)ord(v)<ord(u) In other words, we obtain the unique parent of a module by removing the smallest among the nodes with minimum weighted degree.
From the lemma we know that each module has a smaller or equal density than its parent.
Based on this, the DME algorithm starts with the empty set and recursively generates children as long as the density threshold is not violated (Algorithm 1), yielding thereby the complete set of dense modules.
By the definition of the parentchild relationship, we cannot directly derive the children of a module U.
Instead, we have to check for all possible extended modules with one additional node whether U is their parent or not (reverse search principle).
In terms of complexity, DME belongs to the class of polynomial-delay algorithms, which means that, independently of the size of the results, the computation time between two consecutive solutions is polynomial in the input size (see Supplementary Material).
By changing the density threshold, the user can regulate the size of the output.
Also note that the computation can easily be parallelized.
Finally, dense modules that are subsets of other solutions are not so informative; we call them non-maximal.
While these redundant results could be eliminated by checking for each new module all previous solutions, it is possible to identify locally maximal modules without requiring additional computation or storage, as shown in Algorithm 1.
A module U is locally maximal if and only if for all vV \U, U {v} does not satisfy the minimum density threshold.
Although a module with this property could still be non-maximal, it happens rarely in practice.
Algorithm 1 DME for node set V , weight matrix W , and minimum density .
U represents the current module.
DME is called with U =.
1: DME (V ,W ,,U) : 2: locallyMaximal = true 3: for each vV \U do 4: if W (U {v}) then 5: locallyMaximal = false 6: if U {v} is child of U then 7: DME (V ,W ,,U {v}) 8: end if 9: end if 10: end for 11: if locallyMaximal then 12: output U 13: end if 2.2 Integration of additional constraints The DME framework makes it easy to incorporate and systematically exploit constraints from additional data sources.
For illustration, consider the case where we have an additional dataset which provides profiles of proteins or genes across different conditions (Fig.1B).
For simplicity, let us assume binary profiles, being 1 if the protein is positively associated with the corresponding condition, and 0 otherwise.
Then, dense modules where all member proteins share the same profile across a certain number of conditions are of particular interest; we call these modules consistent.
The problem of DME with consistency constraints is formalized as follows.
Definition 4.
Given a graph with node set V and weight matrix W, a density threshold >0, a profile matrix (mij)iV ,jC and non-negative integers n0 and n1, find all modules U V with W (U) s.t.
there exist at least n0 conditions cC with muc =0 uU and there exist at least n1 cC with muc =1 uU.
Given such a consistency constraint, we can stop the module extension during the dense module mining as soon as the constraint is violated.
This is due to the fact that the number of consistent profile conditions cannot increase while extending the module; more generally, this property is called anti-monotonicity (see Supplementary Material).
So we simply add to line 4 of the algorithm a further condition which checks for the consistency requirements.
These are then automatically taken into account in the check for local maximality.
The use of additional constraints can restrict the search space considerably, so it accelerates the computation and helps to focus on biologically interesting solutions.
The described framework can incorporate any kind of anti-monotonic constraints.
Furthermore, one can use arbitrarily many of those constraints at the same time.
Sometimes, one might be interested in incorporating non-anti-monotonic constraints.
While they cannot be directly exploited for pruning, they can be used to filter the obtained modules.
As an example, our software allows to specify a minimum weighted degree threshold t such that degW (u,U)> t for all nodes u of all modules U.
We set t =0 throughout the article.
2.3 Module ranking The exhaustiveness of our DME approach enables us to exactly determine the uncommonness of the discovered substructures with respect to the network at hand.
Let W = (wij)i,jV be the matrix representation of the given network; the total number of nodes is denoted by |V |.
Let U be a module with |U| nodes and density W (U).
Then, the probability that a random selection of |U| nodes in the network produces a module with at least the same density as U is given by {U V : |U |=|U|W (U )W (U)}( |V | |U| ) .
(2) The exact value of the numerator can be obtained as a side product of the DME algorithm.
In the case of additional constraints, it includes only modules that satisfy them.
The modules in the DME output are sorted by their probability values (in ascending order).
This ranking scheme captures the intuition that the rank of a module should increase with its size and density, but from a theoretical point of view it is more principled than the ranking criterion used by Bader and Hogue (2003), which is the product of size and density.
Furthermore, our probability calculation refers specifically to the network at hand, in contrast to measures derived from network models (Koyuturk et al., 2007).
3 EXPERIMENTAL RESULTS 3.1 PPI data For our experiments with yeast (Saccharomyces cerevisiae), we combined protein interactions in PSI-MI format from DIP 935 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 936 933940 E.Georgii et al.
(Xenarios et al., 2000) and MPact (Guldener et al., 2006), which includes data from IntAct (Hermjakob et al., 2004), MINT (Chatr-aryamontri et al., 2007) and BIND (Bader et al., 2003), and interactions from the core datasets of the TAP mass spectrometry experiments by Gavin et al.
(2006) and Krogan et al.
(2006).
For the human analysis, interactions were extracted from the IntAct, MINT, BIND, DIP and HPRD (Peri et al., 2004) databases.1 One main challenge in the analysis of protein interaction networks are false positive edges.
To deal with this, we determined edge weights that indicate the reliability of the corresponding experimental techniques, following the method by Jansen et al.
(2003) (see Supplementary Material for details).
The resulting interaction network for yeast consisted of 3559 nodes with 14 212 non-zero interactions having an average weight of 0.67.
The human network contained 9371 nodes and 32 048 non-zero interactions having an average weight of 0.47.
3.2 Comparative analysis First, we validated the performance of DME on the yeast interaction network in comparison with four other methods: clique detection (Clique, implementation from http://www.cfinder.org.
), the clique percolation method (CPM, implementation from http://www.cfinder.org.)
(Palla et al., 2005), a procedure for joining cliques of a certain size to larger clusters, CPMw (implementation from http://www.cfinder.org.)
(Farkas et al., 2007), an extension of CPM which includes an additional clique filtering step, and Markov clustering (MCL, implementation from http://micans.org/mcl.)
(van Dongen, 2000), a popular graph clustering method simulating random walks.
As a reference set of confirmed complexes, we used the manually curated protein complexes provided by MIPS (Guldener et al., 2005).
To properly assess methods which can produce overlapping modules, we chose performance measures that are based on protein pairs rather than modules; in that way, we avoid taking the same subset of nodes several times into account even if it occurs in more than one module.
Defining the intersection of pairs from predicted modules and pairs from known complexes as correctly predicted pairs, we calculated precision and recall as follows.
Precision= No of correctly predicted protein pairs No of protein pairs in predicted modules (3) Recall= No of correctly predicted protein pairs No of protein pairs in known complexes (4) To obtain precisionrecall curves, we iteratively calculated the precision and recall values, each time extending the set of considered modules by the next highest-ranking module.
As the other methods do not provide a module ranking and our criterion is only applicable to enumerative approaches, we used the scoring scheme by Bader and Hogue (2003) mentioned in Section 2.3.
In fact, it produced for our DME results almost the same ranking as our criterion; the corresponding precisionrecall curves are virtually equivalent.
For each method, we tested a wide range of parameters (see Supplementary Material) and selected the configuration with the largest area under the precisionrecall curve for Figure 2.
Clique and CPM cannot handle edge weights directly, but they preselect edges according to a minimum weight threshold.
1For all datasets we used the database versions available in May 2007.
Fig.2.
Comparative precisionrecall analysis.
To account for module overlap, the measures are based on protein pairs, see text.
Table 1.
Module statistics of the comparative analysis (see text for details).
DME Clique CPM CPMw MCL No.
of distinct modules 1083 916 19 32 648 Average size of distinct modules 3 4 16 14 3 No.
of raw modules 24 803 1971 19 33 648 Average size of raw modules 10 6 16 14 3 No.
of matched complexes 84 54 9 20 59 Average complex size 5 7 19 14 7 No.
of partially recovered complexes 133 107 20 33 117 No.
of predicted interactions 5970 7066 2756 3935 6108 Area under prec.-rec.
curve (AUC) 0.183 0.166 0.107 0.153 0.148 No.
of enriched distinct modules 112 131 18 32 69 No.
of enriched among top-50 47 44  45 No.
of overlapping proteins 1010 1113 12 38 1 No.
of overlapping interactions 3664 4340 24 114 0 AUC for overlapping interactions 0.152 0.082 0.000 0.001 No.
of recovered complex overlaps 18 16 0 4 0 Running time (s) 2667 6 5 457 4 The average size of the raw modules can be larger than that of the distinct modules because larger modules allow for more variants.
The time measurements were performed on a 2.2 GHz processor.
Overall, DME shows the best prediction performance.
It has high precision with respect to the highest-ranking modules and then shows a sudden drop, which is due to a big module not annotated as a known complex.
Clique detects the same module, but there are some other higher ranked modules, so the drop happens later.
MCL and CPM stay always below DME.
Clique works quite well, however the precision drops quickly for higher recall values because edge weights are not taken into account.
It seems that DME has a clear advantage compared to CPM: by explicitly using the edge weights and tuning the density parameter, it allows for more flexibility than the two-stage procedure of CPM, first selecting edges and subsequently joining together cliques that satisfy an overlap criterion.
While CPMw allows to refine the module search, it still differs significantly from our approach.
As it joins preselected cliques, it does not control directly the density of the produced modules and might also miss some dense modules.
In our analysis, CPMw improved the result obtained by CPM, but is mostly inferior to Clique or DME.
Table 1 summarizes further statistics for the predicted modules.
As DME and Clique produced a large number of very similar modules, we computed for better comparability the number of 936 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 937 933940 Condition-dependent dense module enumeration distinct modules.
For that purpose, we grouped matching modules into clusters; each cluster was represented by its top-ranking module.
To decide whether two modules match each other, we here computed the overlap score proposed by Bader and Hogue (2003), using a stringent cutoff of 0.5.
It is defined as the fraction of overlapping proteins with respect to the size of the first module multiplied by the fraction of overlapping proteins with respect to the size of the second module.
The same criterion was used to determine matches between predicted modules and known complexes.
While DME and Clique discovered a comparable number of distinct modules, the DME modules match many more known complexes.
Among these, we also find small-sized complexes, so the overall average size of retrieved complexes is lower than that for Clique.
In addition, we report the number of complexes from which at least one protein pair was recovered as well as the area under the precisionrecall curve from the pairwise analysis (Fig.2).
In both cases, DME is leading.
Furthermore, we investigated the enrichment of the distinct modules with respect to Gene Ontology (GO) terms.
For that purpose, we applied the TANGO tool (Shamir et al., 2005) using the default setting with P-value threshold 0.05 after correction for multiple testing.
Beside the total number of enriched modules, we also counted the number of enriched modules among the top 50 distinct modules, showing that for each method that produced more than 50 modules, most of the high-ranking modules satisfy the enrichment threshold.
For small modules the enrichment test fails even if they are totally pure.
Finally, we assessed the impact of detecting overlapping modules.
Concerning the number of proteins or protein pairs that appear in more than one module, there is large variation among the different methods.
DME and Clique produced the largest numbers of overlapping proteins and overlapping pairs.
Remarkably, the accuracy of overlapping DME interactions clearly increases with the number of modules in which they occur, whereas this is not true for Clique, as reflected by the difference of their AUC values (see also Fig.4 in the Supplementary Material).
The overall precision of overlapping pairs is 45% for DME and 35% for Clique.
We also analyzed how many overlaps between known complexes were rediscovered by predicted modules.
Formally, we counted the cases of overlapping known complexes C1 and C2 where there existed overlapping modules M1 and M2 such that the following conditions were satisfied: (i) M1 M2 contains at least one element of C1 C2, (ii) M1 \M2 contains at least one element of C1 \C2 and (iii) M2 \M1 contains at least one element of C2 \C1.
Here, the number of recovered overlaps was only slightly higher for DME.
3.3 Phenotype-associated yeast modules An additional feature of DME is the possibility to directly integrate constraints from external data sources.
In this section, we investigated our yeast interaction network in the context of knockout phenotypes in order to identify essential parts of protein complexes.
We took the growth phenotype profiles for knockout mutants in yeast under 21 experimental conditions (Dudley et al., 2005), considering three different phenotypic states: enhanced growth, normal growth, and growth defect.
We applied DME requiring for each module at least one condition consistently associated with growth defect for all members.
In order to get a set of modules covering a large number of proteins, but being at the same time as reliable as possible, we tested density thresholds between 0.95 and 0.80 using decrements of 0.01 Table 2.
Results of DME experiments with constraints Phenotype Conservation Expression (yeast) (yeast) (human) No.
of distinct modules 137 1067 460 Average size of distinct modules 3 3 2 No.
of raw modules 160 1816 736 Average size of raw modules 4 5 3 No.
of matched complexes 14 49 52 Average complex size 4 4 4 No.
of partially recovered complexes 30 103 217 Running time (s) 19 5 8 and selected the one with the largest area under the precisionrecall curve.
The results are summarized in Table 2.
Each of the 13 highest-ranking modules covers a considerable part of the mitochondrial ribosomal large subunit as annotated by MIPS.
In addition, our output list contained one further module that overlaps with the complex.
Figure 3A shows the superposition of these 14 modules.
Mrpl16 and Img2 appear in all, many other proteins in almost all of those modules, so they can be considered as the core of the complex.
Knockout of any of the shown proteins caused growth defects with glycerol as carbon source.
Some module members belong to other MIPS complexes, as depicted by the ellipses.
In particular, there is a strong connection to the small subunit of the mitochondrial ribosome and to the mitochondrial translation complex.
Furthermore, our results suggest that the mitochondrial ribosome is associated with Mhr1, a protein involved in homologous recombination of the mitochondrial genome (Ling et al., 2000).
Some modules that are not related to MIPS complexes nevertheless represent known complexes.
For instance, we exactly recovered the nucleoplasmic THO complex (Hpr1, Mft1, Rlr1, Thp2), which is known to affect transcription elongation and hyper-recombination (Chavez et al., 2000).
Interestingly, all mutants exhibit growth defects under the stress condition of adding ethanol to the medium.
Finally, in Figure 3B we show the highest-ranking module which covers at least 50% of two different MIPS complexes.
The corresponding proteins are associated with growth defects under addition of the aminoglycoside hygromycin B.
The module links the vacuolar assembly complex with the class C Vps complex.
The latter is a specific subgroup of proteins involved in vacuolar protein sorting.
Indeed, it has been shown that this complex associates with Vam6 and Vps41 to trigger nucleotide exchange of a rab GTPase regulating the fusion of vesicles to the vacuole (Wurmser et al., 2000).
3.4 Evolutionary conserved yeast modules Next, we used the evolutionary conservation of proteins as side constraint for DME.
For that purpose, we extracted for all yeast genes orthologs from the InParanoid database (OBrien et al., 2005) with respect to 10 other representative eukaryotic species from Schizosaccharomyces pombe to Arabidopsis thaliana.
More precisely, we created a profile indicating for each S.cerevisiae gene and each other model species whether there exist orthologs with a full inParanoid score in the other model species.
We searched for modules in the yeast interaction network such that all member proteins have orthologs in at least three other species; the density 937 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 938 933940 E.Georgii et al.
Fig.3.
Phenotype-associated yeast modules.
(A).
Superposition of all 14 modules overlapping with the large subunit of the mitochondrial ribosome (node size depends on the number of modules in which the protein occurs).
(B).
Module linking two complexes.
The ellipses mark protein sets belonging to known complexes.
For module visualization we used the Osprey tool (Breitkreutz et al., 2003).
Fig.4.
Yeast complexes matched by DME modules and their overlap with conserved DME modules.
Only complexes with size 5 are shown.
The node size corresponds to the density of the confirmed complex, and the pie chart indicates to which degree the complex is covered by a conserved module.
Nodes are connected if there exist interactions between the corresponding sets of matching modules.
threshold was determined using the same procedure as before (for a summary of the results, see Table 2).
Figure 4 shows an overview of the larger MIPS complexes which were retrieved in our DME results, with or without the conservation constraint.
To define matches between complexes and predicted modules, we used the same criterion as in Section 3.2.
Apparently, we could identify some low-density complexes by discovering their dense core parts, for example the translation elongation factor complex eEF1 and the pre-mRNA 3-end processing factor CFI.
In black, we indicate the percentage of the known complex that is covered by a conserved dense module.
From the total set of 33 recovered complexes shown in the figure, 19 overlap by at least 50% with such a module.
Among them, we find the 20S proteasome and its cap and the translation initiation factor eIF2B complex.
The remaining complexes have rather small overlaps with conserved modules, even though they are quite accurately matched by their unconstrained counterparts.
Our conserved module predictions reveal putative core parts of complexes that are conserved across several species.
As an example, we analyze the SNF1 complex, an essential element of the glucose response pathway consisting of six proteins.
Indeed, while the components Snf1, Snf4 and Sip2 are strongly conserved in all eukaryotes and are covered by a conserved module, Sip1 and the transcription factor Sip4 have no orthologs in other species, and the Gal83 component has orthologs in two species only (Vincent and Carlson, 1999).
Our approach predicted one additional conserved component of the complex, Sak1.
This is biologically meaningful, as it functions as an activating kinase of the SNF1 complex (Elbing et al., 2006).
The unconstrained module contained Sak1 and all SNF1 components except Sip4.
3.5 Tissue-specific modules in the human interaction network Finally, we were interested in tissue-specific modules of the human interaction network.
As side information, we downloaded the gene expression profiles by Su et al.
(2004), containing measurements in 938 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 939 933940 Condition-dependent dense module enumeration Fig.5.
Tissue-specific modules in human.
(A).
The two top-ranking modules, covering the MCM complex.
Known complexes are indicated as solid ellipses, modules as dashed ellipses.
(B).
Top-five modules around the SCF ubiquitin ligase complex, revealing its tissue-specific organization.
Boxes show the tissues of consistent positive expression for the respective module.
Tissues associated with all modules are marked in bold, uniquely appearing tissues in italics.
79 different human tissues and present/absent/marginal calls.
For our purposes, we considered a gene to be expressed in a given tissue only if it was classified as present in both of the duplicated measurements.
In order to find complexes which are present in several, but not all tissues, we applied DME to enumerate all modules that are consistently expressed in at least three tissues and consistently not expressed in at least 10 tissues.
We used again the same procedure for selecting the density parameter and ended up with 460 distinct modules (Table 2).
The two top-ranking modules cover the MCM complex (Fig.5A).
As a reference, we used a manually curated set of human complexes collected by MIPS (Ruepp et al., 2008).
MCM is a hexameric protein complex required for the initiation and regulation of eukaryotic DNA replication.
The DME modules contain two additional proteins, Ssrp1 and Orc6l.
Orc6l is a member of the origin recognition complex (ORC), which plays a central role in replication initiation; in fact, the MCM and ORC complexes form the key components of the pre-replication complex (Lei and Tye, 2001).
This is nicely reflected by the high interaction density as well as the common expression profiles of the proteins: the module is fully expressed in three different types of bone marrow cells and fully non-expressed in 42 tissues like brain, liver and kidney, where cells are differentiated and divide less.
Ssrp1 is a member of the FACT complex, which is involved in chromatin reorganization (Orphanides et al., 1999).
Moreover, our analysis yields some insights about the tissue-specific reorganization of the SCF E3 ubiquitin ligase complex, which marks proteins for degradation.
Figure 5B depicts the five top-ranking modules that cover the complex (beyond those, there were three other modules covering only a single protein of the complex).
One of them contains as an additional component Cand1, a regulatory protein that inhibits the interaction of Cul1 with Skp1 (Zheng et al., 2002).
The four other peripheral proteins are F-box proteins, which serve as substrate recognition particles for the SCF complex.
Interestingly, the corresponding modules show different tissue specificities, indicating that the target proteins of SCF are selected in a tissue-dependent manner.
This finding is in accordance with experimental studies (Cenciarelli et al., 1999; Kipreos and Pagano, 2000; Koepp et al., 2001).
On the one hand, it has been shown that in human cells multiple variants of the SCF complex exist, each one containing a different F-box protein for substrate recognition.
On the other hand, brain and blood cells have been identified as tissues of major expression for some F-box components, and expression variation of F-box components has been observed in several tissues like testis, prostate and placenta.
In our results, all detected module variants are present in natural killer (nk) cells, which play an important role in immune response (Janeway et al., 2005), whereas only a few are present in B-cells and testis; in certain brain regions, for instance medulla oblongata, only the module variant with Fbxw7 is predicted to be active.
As illustrated by this example, DME integrated with gene expression data can be a powerful tool to reveal functional and condition-specific variants of protein complexes.
4 CONCLUSION Our algorithm, DME, extracts all densely connected modules from a given weighted interaction network.
In addition to its completeness guarantee, a strength of the method lies in the possibility of transparent data integration, which is of crucial importance in biological applications.
Due to its generality, we believe that DME is a useful tool in many different systems biology approaches.
Our framework can also solve more general problems arising in the analysis of structured data, like dense subgraph detection in multi-partite graphs (cf.
Everett et al., 2006; Tanay et al., 2004) or in hypergraphs (cf.
Zhao and Zaki, 2005).
Moreover, module finding can assist in network comparison and classification tasks (Chuang et al., 2007).
ACKNOWLEDGEMENTS We are very grateful to G. Rtsch and B. Schlkopf for their support; we thank C.S.
Ong for proofreading the article.
939 [22:36 20/3/2009 Bioinformatics-btp080.tex] Page: 940 933940 E.Georgii et al.
Funding: Federal Ministry of Education, Science, Research and Technology (NGFN: 01GR0451 to S.D.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Chromatin immunoprecipitation followed by high-throughput sequencing (ChIP-seq) is widely used in biological research.
ChIP-seq experiments yield many ambiguous tags that can be mapped with equal probability to multiple genomic sites.
Such ambiguous tags are typically eliminated from consideration resulting in a potential loss of important biological information.
Results: We have developed a Gibbs sampling-based algorithm for the genomic mapping of ambiguous sequence tags.
Our algorithm relies on the local genomic tag context to guide the mapping of ambiguous tags.
The Gibbs sampling procedure we use simultaneously maps ambiguous tags and updates the probabilities used to infer correct tag map positions.
We show that our algorithm is able to correctly map more ambiguous tags than existing mapping methods.
Our approach is also able to uncover mapped genomic sites from highly repetitive sequences that can not be detected based on unique tags alone, including transposable elements, segmental duplications and peri-centromeric regions.
This mapping approach should prove to be useful for increasing biological knowledge on the too often neglected repetitive genomic regions.
Availability: http://esbg.gatech.edu/jordan/software/map Contact: king.jordan@biology.gatech.edu Supplementary Information: Supplementary data are available at Bioinformatics online.
Received on May 27, 2010; revised on July 22, 2010; accepted on August 9, 2010 1 INTRODUCTION Genome-wide chromatin immunoprecipitation followed by high-throughput sequencing (ChIP-seq) experiments are increasingly used in biological and medical research (Barski et al., 2007; Park, 2009).
ChIP-seq experiments produce a large amount of short sequence tags which need to be reliably mapped back to the genome and processed to reveal biologically relevant signal.
A number of algorithms have been recently developed to process ChIP-seq data (Bock and Lengauer, 2008).
These include algorithms for genomic mapping of sequence tags (Langmead et al., 2009; Li et al., 2008), smoothing of ChIP-seq tag distribution signals (Thurman et al., 2007) and detection of statistically significant tag peaks (Zhang et al., 2008).
One remaining challenge for the processing of ChIP-seq data is the mapping of ambiguous tags.
Ambiguous tags are To whom correspondence should be addressed.
those that can be mapped to multiple genomic sites, each of which has significant sequence similarity with the tag, and thus it is difficult to distinguish the real site from all the possible sites.
Usually, researchers simply disregard ambiguous tags and only make use of uniquely mapped tags.
This often results in a substantial loss of information and may bias conclusions based on the analysis of unique tags alone.
This is particularly true for mammalian genomes, such as the human genome, which have numerous interspersed repeat sequences.
Repeat sequences that are highly similar may produce a large amount of ambiguous tags, which if not mapped will be disregarded in subsequent analyses.
Research has shown that interspersed repeat sequences provide a wide variety of functional elements to eukaryotic genomes (Feschotte, 2008).
Therefore, disregarding ambiguous tags may lead to an underestimate of the biological significance and functional roles of interspersed repeated DNA.
Two different approaches have been developed for the mapping of ambiguous sequence tags.
The mapping software MAQ randomly selects a possible site and assigns it to the ambiguous tag (Li et al., 2008).
Each possible site has the same probability of being selected.
In other words, there is no way to know if this approach yields a correct mapping of ambiguous tags.
The second approach takes advantage of the local context of mapped tags to more accurately assign genomic locations for ambiguous tags.
This approach rests on the assumption that real ambiguous tag sites are expected to have more sequence tags in the local vicinity, whereas the incorrect sites for the same ambiguous tags are expected to have fewer numbers of co-located tags (Faulkner et al., 2008; Hashimoto et al., 2009).
To apply this method for any ambiguous tag, the number of overlapping mapped tags at each of the possible ambiguous tag-mapped positions are counted and used to assign fractional weights to each possible position.
The ambiguous tag is then fractionally mapped to each possible position with the fractions weighted by the local-mapped tag context.
In other words, possible sites with more tags already mapped are deemed to deserve higher confidence and are accordingly assigned greater fractions of ambiguous tags.
The fractional mapping method makes important contribution to the ambiguous tag mapping problem.
But as the use of ChIP-seq in scientific research is increasing, it will be important to further refine the accuracy of mapping ambiguous tags.
First, the fraction method is heuristic as the fractions assigned to the possible map sites are directly proportional to the number of tags mapped to each site.
While this approach is consistent with biological intuition, it lacks statistical support.
A more sensitive probabilistic method could be used to better represent and measure the confidence level of The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[07:14 1/10/2010 Bioinformatics-btq460.tex] Page: 2502 25012508 J.Wang et al.
Fig.1.
Scheme of our Glibbs sampling algorithm.
Possible tag map sites along with their likelihood ratios are shown prior to stochastic mapping.
Gray boxes represent incorrect sites, and the black box represents the correct site.
An arrow between a tag and a site means the tag could possibly be mapped to that site.
One iterative cycle of joint stochastic mapping and parameter updating is shown.
The black arrows point to selected sites for each tag after stochastic mapping.
each possible site.
Second, the fraction method deterministically fractionates the ambiguous tags without guarantee that the result is optimal.
In other words, it does not search the possible space of assignments of ambiguous tags and lacks information on the accuracy of the final results.
Third, the fraction method is not realistic enough since it splits tags by assigning fractions of ambiguous tags to each possible site.
In reality, each sequence tag is only derived from a single genomic site.
Thus, fractioning sequence tags inevitably results in wasting signal on incorrect sites and weakening the signal level on real sites.
To address the outstanding issues with ambiguous tag mapping, we have developed a probabilistic Gibbs sampling-based algorithm to map more ambiguous tags with greater accuracy.
Our approach assigns ambiguous tags to single genomic sites, without fractionating tags, and iteratively samples within the space of the possible mappings of ambiguous tags.
The Gibbs sampling strategy (Lawrence et al., 1993; Neuwald et al., 1995) guides the algorithm to achieve accurate unique mappings of ambiguous tags.
The algorithm also provides statistical support for ambiguous tag mapping via the use of likelihood ratios that measure the confidence levels of possible genomic map sites.
We evaluated the performance of our algorithm compared to existing approaches using sequence-tag data from the highly repetitive human genome.
We demonstrate that our probabilistic approach to mapping ambiguous tags yields superior results as measured by (i) the fraction of correctly mapped ambiguous tags; (ii) the precision and recall of correctly recovered repetitive genomic sites; and (iii) the level of signal found at repetitive sites.
2 METHODS 2.1 Overview of the algorithm Our algorithm maps ambiguous tags to individual genomic sites by taking advantage of the local genomic context provided by co-located tags.
For each possible map site of an ambiguous tag, the number of co-located tags is counted and used to calculate a normalized likelihood ratio that represents its probability of being the real map site.
Map sites are randomly selected based on the underlying probability distributions from the likelihood ratios.
Likelihood ratio scores are then updated based on the new mapping, and this procedure iterates until convergence, i.e.
until there is little or no change in the map positions between iterations.
A Gibbs sampling strategy is used to iteratively map ambiguous tags to possible genomic sites while updating the probability that each tag is mapped to its most likely site.
Gibbs sampling was chosen because it allows for a simultaneous updating of the map positions and the parameters for these positions.
Through the updating iterations, the algorithm searches in the space of all possible mapping configurations where each mapping configuration can be considered as a bipartite graph with edges connecting tags and sites (Fig.1).
Intuitively, once an ambiguous tag is correctly mapped to the real site, it will guide the algorithm to map those tags derived from the same site to it with higher probability.
2.2 Problem formulation For each ambiguous tag, there are multiple possible genomic sites to which it could be assigned.
It is not possible to assign a specific site to an ambiguous tag with 100% confidence, and so we need to calculate the confidence for each probable site by some measurement and then select a reasonable site for each ambiguous tag based on those confidences.
By reasonable, we mean a selection of sites that will minimize the number of incorrect 2502 [07:14 1/10/2010 Bioinformatics-btq460.tex] Page: 2503 25012508 An ambiguous tag mapping algorithm mappings of ambiguous tags.
Suppose there are T genomic sites associated with ambiguous tags and the set of ambiguous tags is A={a1,a2, ,aN } (1) where ai represents ambiguous tag i.
We use Si = { Si1,Si2, ,Sini } (2) to denote the set of probable sites for ai, where ni is the total number of probable sites for ai.
There are two aspects of this problem.
One is the measurement of confidence for each probable site, and the other one is the algorithm used to select reasonable sites for ambiguous tags.
An applicable measurement of confidences of probable sites needs to be monotonic with the number of tags that are mapped to each specific site and should reflect both the information of the distribution of tag numbers of real sites and the information of the distribution of tag numbers of background.
We use likelihood ratio as the confidence measurement based on both intuitive clues and theoretical analysis.
Intuitively, likelihood ratio is monotonic with tag counts and is also computationally tractable.
Furthermore, it takes both the background distribution of tag counts and the estimated target distribution under consideration.
Higher likelihood ratios correspond to higher confidences and increase non-linearly with tag counts.
Likelihood ratios will increase sharply for large tag counts and be relatively low for sites with few tags.
This property will help to avoid the problem of wasting fractions of mapped tags on sites that contain few tags; a problem that could be particularly vexing if many such low-confidence sites exist for a single ambiguous tag.
The likelihood ratio for sij is denoted as LRj = Ps ( kj ) Pn ( kj ) .
(3) Ps is the estimated target distribution of tag counts in real sites and Pn is the background distribution of tag counts.
The kj is the tag count at site j.
The details of these two distributions will be discussed in the next section.
Given the calculated likelihood ratios, it is possible for us to reasonably map ambiguous tags.
Furthermore, from a theoretical point of view, normalized likelihood ratio is the measurement we will automatically derive from the calculation of the conditional probability of assigning ambiguous tags to a specific site given the assignments of all the other tags.
We use D to denote the original data, which essentially represent the associations of tags with possible sites, and M to denote the whole assignment of tags to sites.
M[i] represents the assignments of tags to sites, except the assignment of tag i. P ( ai sij M[i],D ) (4) represents the conditional probability of assigning tag i to the j-th probable site of i, given the original data and the assignment of all tags except tag i.
We use U to represent the whole set of sites.
Below we show that this conditional probability is equal to the normalized likelihood ratio, as derived from Bayes rules.
P(ai sij |M[i],D)= P ( ai sij,M[i] |D ) P ( M[i] |D ) = {Ps(kj +1) mSi\ j Pn(km) }P(U\Si) Si { Ps(k +1) mSi\ Pn(km) } P(U\Si) = ( Ps (kj+1 ) Pn (kj ) ) Si Ps (k+1 ) Pn (k ) (5) So the normalized likelihood ratio represents the conditional probability for the j-th probable site given the assignment of other tags.
Equivalently, this conditional probability serves as our predictive update formula for the Gibbs sampling procedure described below.
In order to calculate likelihood ratios for genomic sites, we need to first map those ambiguous tags to get the number of tags mapped to each specific site.
In other words, mapping of ambiguous tags and calculating the likelihood ratios for each site are circular.
This circularity led us to adopt Gibbs sampling strategy, which is a stochastic version of EM algorithms, to select reasonable sites for ambiguous tags.
To do this, we first initialize the likelihood ratios for genomic sites using the total number of tags that can be probably mapped.
Then we map each ambiguous tag to a specific site based on the initial likelihood ratios.
To be more specific, we stochastically map each ambiguous tag to a genomic site with the probability equal to the normalized likelihood ratio of the site.
Then we update the likelihood ratios given the current mapping of ambiguous tags.
We continue the update on the mapping and the calculation of likelihood ratios until there is no significant change.
Through the iterative updates (stochastic mapping and parameter updating), the overall likelihood ratios are expected to be optimized, and so we achieve an accurate mapping of ambiguous tags.
Since the complete normalized likelihood ratio for a configuration of mapping is proportional to iU ( Ps (ki ) Pn (ki ) ) (6) where i is the index of genomic sites with tags mapped, we can rewrite this formula based on tag counts and obtain the formula as  ( Ps ( ) Pn ( ) )n ( ) (7) where n() represents the number of sites with tags mapped.
Here, represents the set of tag counts for all sites.
For instance, if consists of large numbers, it means that most sites are mapped with large number of tags and the mapping is a reasonable one.
Otherwise, most sites are mapped with a small number of tags and the set of tags are scattered into diverse sites.
Taking the logarithm of this formula and dividing by Z , the total number of tags, we get  ( n ( ) Z ) log ( Ps ( ) Pn ( ) ) (8) When Z is sufficiently large, it approaches the relative entropy between Ps and Pn on the subset of .
So essentially, the Gibbs sampling procedure described above searches a certain subset to maximize the relative entropy.
When consists of only large numbers, the relative entropy is larger.
This analysis further demonstrates that our algorithmic design is reasonable.
Equation (8) shows that by using normalized likelihood ratios, our objective function is equivalent to the relative entropy.
In theory, Gibbs sampling will have good performance given a sufficient number of iterations.
Thus, there may be concerns about the time necessary for the algorithm to converge.
However, since unique tags count for the majority of the whole set of tags, and these help to guide the mapping of ambiguous tags, this has the effect of shortening the algorithm time significantly.
In our experience, about five iterations are sufficient for convergence.
2.3 Algorithm Next we describe each step of the algorithm in detail along with the definitions of necessary concepts.
The scheme of the method is shown in Figure 1.
Phase 1.
Initialization Step 0.
The program Bowtie (Langmead et al., 2009) is used to map all sequence tags to the genome and only genomic loci with significant sequence similarities are used for the following steps.
Sequence tags are classified into unique tags and ambiguous tags by the Bowtie mapping algorithm.
Step 1.
To calculate the likelihood ratios, we need to model the distributions of tag counts for real modified sites (Ps) and for background (Pn).
For real modified sites, we use the Normal distribution to approximate the real distribution of tag numbers Ps N ( ,2 ) (9) To identify genomic sites that are most likely to actually be modified (i.e.
real-modified sites), we use sites with large numbers of mapped unique 2503 [07:14 1/10/2010 Bioinformatics-btq460.tex] Page: 2504 25012508 J.Wang et al.
tags.
We then use the numbers of unique tags associated with those sites to calculate the average tag count and standard deviation for each site genome wide.
Note that the average tag count calculated here is corrected by a factor which takes into consideration that the real average tag count will be greater once ambiguous tags are included.
For background, we use the Poisson distribution to approximate the background distribution of tag counts Pn Poisson ( ) (10) The Poisson distribution is an appropriate model for counting processes that produce rare random events and thus can be applied here to describe the background tag count distribution.
We count the total number of tags (both unique and ambiguous tags) and calculate the average tag number for each site.
The average tag number serves as the parameter () of Poisson distribution.
After getting all the parameters, we calculate the likelihood ratios for various tag counts LR ( k )= Ps ( k ) Pn ( k ) (11) and get a table of likelihood ratios which will be used in subsequent steps.
Step 2.
In order to obtain the initial settings of likelihood ratios for all the probable genomic loci, we use the number of tags of each site (both unique and ambiguous tags) to calculate the likelihood ratios.
Since the ambiguous tags have not been assigned to a specific genomic site, here we assign each ambiguous tag to all the probable sites to initialize the likelihood ratios.
The calculation of likelihood ratios for various tag numbers has already been done in Step 1 and the algorithm only needs to search the table of likelihood ratios.
A special notion here is that we introduce the information content factor (0< f <1) of ambiguous tags compared to unique tags.
Since the nature of uncertainty of ambiguous tags, the information content of ambiguous tags is smaller than unique tags.
Thus, the effective number of ambiguous tags (ke) is corrected by f and the number of tags used to calculate likelihood ratio is: k =ku +ke =ku +kaf (12) where ku is the number of unique tags and ka is the number of ambiguous tags.
By the user, f can be set based on their confidence of ambiguous tags and provide flexibility of the method.
The suggested value of f is the inverse of the mean number of associated sites of ambiguous tags.
If the mean number of associated sites of ambiguous tags is larger, then f should be made smaller to weight unique tags more heavily for the mapping.
Phase 2.
Iterative weighted mapping Step 3.
Given the likelihood ratio (LRi) of probable site j (j=1,2,...,nj) for ambiguous tag ai, the algorithm stochastically selects a probable site and assigns it as the site of the corresponding ambiguous tag.
The probability (Pij) of probable site j to be selected for ai is proportional to the likelihood ratio of site j. Pij = LRj kSi LRk (13) where k =1,2,...,nj .
Thus, probable sites with higher likelihood ratios will have a greater chance of being assigned.
Step 4.
Based on the current assignments of sites for ambiguous tags obtained from Step 3, the likelihood ratios of all the probable sites are updated.
The new likelihood ratio of each probable site is obtained accordingly to the current number of tags assigned to the site.
Step 5.
Iterate through Steps 3 and 4 until no significant changes occur, i.e.
until convergence.
For a given threshold, if the number of reassignments of ambiguous tags is smaller than the threshold, then the iterations will stop and output the final mapping of tags.
3 RESULTS 3.1 Sequence tag datasets In order to test the performance of our algorithm, we randomly selected 50 000 sites of the human genome as a benchmark.
Each site is 147 bp in length (i.e.
mono-nucleosomal) and the set of sites contains transposable elements and simple repeats in the same fractions as the human genome.
Then we generate short sequence tags from these sites under a range of set of parameters.
These parameters include sequence tag length (L), signal-to-noise ratio (SNR) and sequencing error level (SE).
In theory, shorter sequence tags are expected to have more ambiguous tags.
To test the performance of our algorithm on different sequence tag lengths, we generate libraries with 20 bp tags and libraries with 35 bp tags.
SNR corresponds to the specificity of the ChIP experiments.
Noise here means the fraction of sequence tags derived from sites which are not the real modified sites.
In experiments with high specificity, the majority of sequence tags are derived from the real modified sites, while in experiments with high level of noise, there are increased number of sequence tags derived from other sites.
And we define the SNR as the ratio of the probability that a sequence tag is derived from the real modified sites over the probability that a sequence tag is derived from other sites.
To test our algorithms performance under different SNRs, we generate libraries with SNR set as 99 (corresponds to 99% tags derived from real modified sites) and libraries with SNR set as 9 (corresponds to 90% tags derived from real modified sites).
The sequencing error level corresponds to the probability of errors in high-throughput sequencing.
We generate libraries with sequencing error levels as 2/5L and 4/5L.
The reason to set SE this way is as follows.
We assume that the sequencing errors on different sites are independent from each other.
This is not completely true in reality but is acceptable as a first-order approximation.
Then the total number of errors for each sequence tag with length L would follow binomial distribution.
So under SE = 2/5L, the fraction of sequence tags without errors is 60% and under SE = 4/(5L), the fraction is 50%.
It means that the quality of the simulated sequencing is not very good.
Under such conditions, some sequence tags might be mis-mapped or become ambiguous tags.
The purpose of this setting is to make sure that our algorithm test results are conservative.
Since each of these three parameters only has two optional values, there are eight combinations of different values of those parameters and so we generate one sequence tag library for each combination of the parameter values.
The parameters for each library are listed in Supplementary Table S1.
We also used a second larger benchmark set consisting of 173 877 sites of the human genome.
These sites were obtained from a ChIP-seq study of histone modifications based on ABI SOLiD sequencing platform (Victoria V. Lunyak, unpublished data) that only used unique sequence tags, and each site has significant number of tags.
This dataset was used because it mimics conditions one would expect for real sites: a larger number of total sites and a realistic distribution of sites along the human genome.
In order to test our algorithm, we generated sequence tags for these sites the same way as described above under one set of parameters (Supplementary Table S1).
After preparing sequence tags, we ran the program Bowtie (Langmead et al., 2009) to map the sequence tags to the human genome.
The fractions of ambiguous tags in the nine libraries range from 9.7% to 37.6%.
The fraction of sites undetected using unique tags alone are influenced by the tag threshold used.
Higher threshold cause more undetected sites.
For the lowest threshold (four tags) used in our analyses, the fractions of undetected sites range from 16.4% to 28.4%.
These values underscore the importance of accurately mapping ambiguous tags to recover undetected sites.
2504 [07:14 1/10/2010 Bioinformatics-btq460.tex] Page: 2505 25012508 An ambiguous tag mapping algorithm 3.2 Fraction of correctly mapped ambiguous tags The first and most direct measurement of the algorithm performance is the fraction of correctly mapped ambiguous tags.
Since the fraction method does not assign the ambiguous tags to a specific site, this measurement is not applicable.
So we compared our algorithm against the MAQ software method, which randomly selects a site for each ambiguous tag.
The comparison on the eight sequence tag libraries shows that our algorithm correctly maps from 49% to 71% of ambiguous tags, while the MAQ method correctly maps from 8% to 23% of ambiguous tags (Fig.2).
Over all eight sequence tag Fig.2.
Fractions of correctly mapped ambiguous tags for each library.
Library descriptions are given in Supplementary Table S1.
Gray bars show result based on MAQ, and black bars show results based on our Gibbs sampling algorithm.
libraries evaluated, our algorithm maps from 38% to 51% more tags than MAQ.
In the best case, our algorithm maps the majority of ambiguous tags (71%) and only a small fraction of information is lost.
3.3 Comparison of rescued sites The other measurement of the algorithms performance is the numbers and fractions of correctly rescued genomic sites, which can not be observed by unique tags alone.
An important issue regarding the rescued sites is the tag number threshold, above which a site is called rescued with a certain number of tags (Fig.3A).
Different thresholds will result in different sets of true positives, false positives and false negatives.
Since there are various methods to decide the threshold and different users usually set different thresholds, we tested our algorithms performance on a set of three different thresholds (four, six and eight tags).
Together with the previously described the nine sequence tag libraries we use, this results in a set of 27 conditions for analysis.
The first thing we did was to compare the numbers of genomic sites identified using unique tags alone to the numbers of genomic sites identified by including ambiguous tags with our method (Supplementary Table S2).
Over the 27 conditions, the inclusion of ambiguous tags yields an average increase of 11.46% in the fraction of genomic sites accurately identified.
The use of ambiguous tags Fig.3.
Compariion of algorithm performance.
(A) Illustration of data used to test algorithm performance.
(B) Variant tag count thresholds could used in the algorithm tests.
(C) Recall and precision fractions for map sites are shown for the algorithms compared here (MAQ, blue; fraction method, dark blue; Gibbs sampling method, green) over eight tag libraries.
(D) Recall and precision are shown for the larger tag library across three tag thresholds.
2505 [07:14 1/10/2010 Bioinformatics-btq460.tex] Page: 2506 25012508 J.Wang et al.
Fig.4.
Examples of ambiguous tag mapping results.
Tracks are shown through UCSC Genome Browser.
The track of real sites shows the sites in the benchmark libraries.
The track of Fraction method shows the mapping result by fraction method and the track of Gibbs method shows the mapping result by our Gibbs method.
The heights of data represent the number of tags mapped to those sites.
The tracks of repetitive genomic regions (segmental duplications, interspersed repeats and simple repeats) are also shown.
resulted in the identification of 260251 508 sites missed with unique tags alone.
Next, we compared our method for including ambiguous tags to the MAQ and fraction methods.
To do this, after excluding sites that can be found by unique tags alone, we divide the set of sites rescued by ambiguous tags into two subsets by comparing the set with the benchmark.
The correctly rescued sites are true positives (TP) and other sites are false positives (FP).
The sites in the benchmark which remain undiscovered are false negatives (FN) (Fig.3B).
In order to test the performances, we employ recall RE = TP/(TP + FN) and precision PE = TP/(TP + FP) as measurements.
For the four libraries with 35 bp tags and the four libraries with 20 bp tags, our algorithm shows the highest recall over all conditions (six-tag threshold shown in Figs.
3C and 4 and eight-tag thresholds shown in Supplementary Fig.S1 and numbers of sites shown in Supplementary Table S3).
Our algorithm also has the highest precision for these libraries over 14 of the 24 conditions evaluated (Fig.3C, Supplementary Fig.S1).
For the 10 cases where our algorithm did not show the highest precision, the difference from the fractional method was marginal (Supplementary Table S3).
In general, when recall increases precision may be expected to decrease.
The simultaneous increase in both recall and precision in 14 cases evaluated here supports the improved performance of our algorithm.
To more quantitatively evaluate the improvement in the performance of our algorithm for both recall and precision together, we used the harmonic mean (F) of the recall and precision values for each condition (i.e.
each library and threshold combination).
The F-values are higher for our algorithm over all conditions, indicating an improvement in performance when recall and precision considered together (Supplementary Table S4).
Similar results can be seen when the larger tag library is evaluated with our algorithm over the three thresholds.
Recall improves substantially in all cases, and precision decreases marginally for thresholds 6 and 8 (Fig.3D and Supplementary Table S5).
The F-values showing the combined recall and precision performance are higher for our method over all three thresholds (Supplementary Table S4).
In Figure 4, we provide two examples of our mapping results with the comparison against the benchmark and the result of fraction method.
It can be seen that our algorithm rescues more sites than fraction method, and that the average number of tags at rescued sites is higher than seen for the fraction method.
This can be attributed to the fact that the fraction method assigns a fraction of ambiguous tags on each site and wastes information on other sites.
The greater number of tags per rescued site can help to ensure that these sites are 2506 [07:14 1/10/2010 Bioinformatics-btq460.tex] Page: 2507 25012508 An ambiguous tag mapping algorithm Fig.5.
(A) The number of correctly discovered sites in various genomic features by unique tags alone (white) and our Gibbs method (black) compared with the corresponding numbers in the benchmark library.
(B) The fractions of correctly discovered sites in various genomic features by unique tag alone (white) and our Gibbs method (Black).
[TE, transposable element; s_r, simple repeats; microSat, microsatellites; seg_dup, segmental duplication; centro, peri-centromeric region].
robust to different user thresholds that are employed to distinguish signal from noise.
It should be noted that the two examples shown here represent segmental duplications (Fig.4A) and satellite regions (Fig.4B), respectively.
It is expected that such highly repetitive regions will produce many ambiguous tags and thus would be difficult to uncover with ChIP-seq.
However, our method achieves good performance in such repetitive regions.
Furthermore, the second example is located very near to the centromere of chromosome 7.
Centromeric regions are important in various cellular processes, such as cell division, and correct mapping of ambiguous tags to centromeric regions could help to uncover specific biological roles for such regions.
3.4 Biological relevance Transposable elements, simple repeats, micro-satellites, segmental duplications and pericentromeric regions are genomic regions rich in repeat sequences.
These regions could produce large numbers of ambiguous tags and will be difficult to uncover due to the technical problem of mapping ambiguous tags.
The ability to correctly map ambiguous tags may facilitate novel discoveries regarding the biological significance of such repeat regions, many of which have been ignored in past chromatin immunoprecipitation studies.
For instance, we show that our method is able to detect previously uncharacterized segmental duplications and satellite regions in Figure 4.
In addition, our method uncovered a previously undetected modified histone site in the proximal promoter region of the CWF19-like one cell cycle control protein.
To further investigate whether our algorithm really helps us to find more sites in genomic repeats, we used the UCSC genome browser (Karolchik et al., 2004; Kent et al., 2002) to count the numbers and fractions of rescued sites in those regions and compared them against using unique tags alone (Fig.5).
This analysis demonstrates that our algorithm is able to rescue substantial numbers of sites in genomic repeat regions, especially for segmental duplications and pericentromeric regions.
Unique tags can only uncover around half of the sites in segmental duplications and pericentromeric regions, while our algorithm could uncover the majority of those sites (Fig.5B).
It is evident that our method has the potential to generate additional biological knowledge from ChIP-seq experiments.
4 DISCUSSION Based on the results described above, we have shown that our algorithm significantly improves the accuracy of mapping ambiguous tags.
The essential information used by the algorithm is the association between co-located sequence tags, which was originally utilized by Faulkner et al.
(2008) in the fraction method.
Our contribution to this class of approach is to employ iterative probabilistic methods to achieve better performance.
The use of likelihood ratios not only reflects the information on sequence tag associations, but also the background distribution information.
Furthermore, likelihood ratios are not linear to tag counts, but increase sharply for large tag counts and thus efficiently avoid wasting signal on sites with small tag counts.
The Gibbs sampling procedure enables us to sample in the space of mapping and achieve a reasonable assignment of sites to sequence tags.
For most experiments, unique tags are the majority of tags and they can guide the sampling efficiently.
Thus, Gibbs sampling does not require too much time to reach the final result.
We have also shown that correct mapping of ambiguous tags can facilitate our understanding of biology by recovering repeated genomic sites which are prone to produce ambiguous tags.
Although the length of sequence tags is increasing, there will still be a certain amount of ambiguous tags.
As shown in Figure 4, genomic sites, such as segmental duplications and microsatellites will always produce ambiguous tags by their nature: with multiple copies in the genome.
So the task of mapping ambiguous tags will not disappear due to the experimental technique advancements in short term, and our algorithm provides an efficient way to solve this problem.
Funding: Bioinformatics Program at the Georgia Institute of Technology (to J.W.
); Alfred P Sloan Research Fellowship in Computational and Evolutionary Molecular Biology (BR-4839, to I.K.J.
and graduate student A.H.); National Institutes of Health pilot projects (UL1 DE019608 to V.V.L.
); Buck Institute Trust Fund (to V.V.L.).
Conflict of Interest: none declared.
Abstract Motivation: Ebola virus causes high mortality hemorrhagic fevers, with more than 25 000 cases and 10 000 deaths in the current outbreak.
Only experimental therapies are available, thus, novel diagnosis tools and druggable targets are needed.
Results: Analysis of Ebola virus genomes from the current outbreak reveals the presence of short DNA sequences that appear nowhere in the human genome.
We identify the shortest such sequences with lengths between 12 and 14.
Only three absent sequences of length 12 exist and they consistently appear at the same location on two of the Ebola virus proteins, in all Ebola virus genomes, but nowhere in the human genome.
The alignment-free method used is able to identify pathogen-specific signatures for quick and precise action against infectious agents, of which the current Ebola virus outbreak provides a compelling example.
Availability and Implementation: EAGLE is freely available for non-commercial purposes at http://bioinformatics.ua.pt/software/eagle.
Contact: raquelsilva@ua.pt; pratas@ua.pt Supplementary Information: Supplementary data are available at Bioinformatics online.
1 Introduction Ebola virus (EBOV) is a negative strand-RNA virus from the Filoviridae family that causes high mortality hemorrhagic fevers, for which no vaccine or treatment currently exist (Sarwar et al., 2014).
There are five Ebolavirus species, namely, Zaire ebolavirus, Sudan ebolavirus, Bundibugyo ebolavirus, Tai Forest ebolavirus and Reston ebolavirus, with the first (1976) and major (2014) outbreaks caused by the type species Zaire ebolavirus (Baize et al., 2014).
The numbers of the largest ever EBOV outbreak are worrying and continue escalating, with over 25 000 cases and 10 000 deaths from the virus mainly in Guinea, Liberia and Sierra Leone, accord-ing to the World Health Organization.
The current outbreak is also the first where transmission has occurred outside Africa, with re-ported cases in Europe (Spain) and America (USA; Butler and Morello, 2014).
Promising vaccine candidate tests are being rushed to face the epidemics and could be available within a few months (Gulland, 2014).
These yet experimental therapies include, for ex-ample, recombinant viral vectors (Jones et al., 2005) or antibodies that target the viral glycoprotein (GP; Friedrich et al., 2012; Sarwar et al., 2014), but innovative approaches are still needed for the de-velopment of diagnosis tools and identification of druggable targets.
Minimal absent words are the shortest sequence fragments that are not present in the genomic data of a given organism.
They have been studied before to describe properties of prokaryotic and eukaryotic genomes and to develop methods for phylogeny con-struction or PCR primer design (Chairungsee and Crochemore, 2012; Falda et al., 2014; Garcia et al., 2011; Herold et al., 2008; Pinho et al., 2009; Wu et al., 2010).
Here, we introduce minimal relative absent words (RAW), a concept which has not been used so far in the context of personalized medicine, but which is deemed useful for differential identification of sequences that are derived from a pathogen genome but absent from its host.
VC The Author 2014.
Published by Oxford University Press.
2421 This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/4.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Bioinformatics, 31(15), 2015, 24212425 doi: 10.1093/bioinformatics/btv189 Advance Access Publication Date: 2 April 2015 Discovery Note ) ( ) (We use the current EBOV outbreak sequences, which were re-cently published (Gire et al., 2014), to discover and characterize the minimal RAWs that are present in EBOV genomes but absent from the human genome.
Moreover, we show that these words are also absent from the other Ebolavirus species and even from the genomes obtained from previous outbreaks.
Thus, the sequences that we identify are species-specific and important for future development of diagnosis or therapeutic strategies for EBOV.
The method that we introduce can be applied to other emerging pathogens or to show evidence of evolutionary patterns and signatures across species.
2 Methods 2.1 Relative absent words Consider a target sequence (e.g.
a virus sequence), x, and a reference sequence (e.g.
the human genome), y, both drawn from the finite al-phabet P {A, C, G, T}.
We say that a is a factor of x if x can be ex-pressed as xuav, with uv denoting the concatenation between sequences u and v. We denote by Wkx the set of all k-size words (or factors) of x.
Also, we represent the set of all k-size words not in x as Wkx.
For each word size k, we define the set of all words that exist in x but do not exist in y by Rkx; y Wkx \Wky (1) and the subset of words that are minimal, in the sense presented in Pinho et al.
(2009), as Mkx; y fa 2 Rkx; y : Wk1a \Mk1x; y/g (2) i.e.
a minimal absent word of size k cannot contain any minimal ab-sent word of size less than k. In particular, lar is a minimal absent word of sequence x, where l and r are single letters from P , if lar is not a word of x but both la and ar are (Pinho et al., 2009).
In this work, we were particularly interested in the non-empty set Mkx; y corresponding to the smallest k. These are referred as RAWs.
2.2 Protein structural models Protein 3D structural models were built by homology modeling as previously described (Duarte-Pereira et al., 2014).
Appropriate tem-plates were selected from PDB (www.rcsb.org; Berman et al., 2000), where several nucleoprotein (NP) structures from viruses within Mononegavirales (negative-sense genome single-stranded RNA viruses) are available, whereas for the region of interest in L-protein only structures from more distant viruses exist.
Structures from the Nipah virus NP (PDB ID:4CO6; Yabukarski et al., 2014) and the BVDV (bovine viral diarrhea virus) RNA poly-merase (PDB ID:1S48; Choi et al., 2004) were used as templates in MODELLER (Eswar et al., 2006; Sali and Blundell, 1993), to pre-dict the structure of the N-terminal regions of Ebola virus NP (resi-dues 1380) and RNA-polymerase (residues 177805), respectively (Supplementary Figs.
S3 and S4).
Accuracy of the predicted models (Supplementary Fig.S5) was estimated using ProSA-web (https://prosa.services.came.sbg.ac.at/prosa.php; Sippl, 1993; Wiederstein and Sippl, 2007) and structures were visualized with PyMOL (Schrodinger, 2010).
3 Results To identify RAWs, we have developed the EAGLE tool that imple-ments the method described above (Supplementary Data).
We have used the full GRC-38 human reference genome (Church et al., 2011) downloaded from the NCBI, including the mitochondrial, unplaced and unlocalized sequences.
The sequences of 99 EBOV genomes from the current outbreak in Sierra Leone (Gire et al., 2014) and additional 66 Ebolavirus genomes have been also downloaded from NCBI (Supplementary Table S1).
The code used in this analysis is available (Pratas, 2015).
Figure 1 shows the computation for word sizes 12, 13 and 14 (for computer characteristics see Supplementary Section Software and Hardware).
As expected, the number of absent words decreases as the k-mer size decreases.
Specifically, for k11 (not represented), there are no EBOV RAW.
On the other hand, for k12, three groups of points emerge (RAW1, RAW2 and RAW3) representing the position of a RAW in each of the 99 unaligned viral genomes (Fig.1a).
Alignments of 124 Ebolavirus sequences (Gire et al., 2014), including additional EBOV genomes from the current outbreak in Guinea (Baize et al., 2014) and from previous outbreaks, show that the identified minimal RAWs fall into conserved protein regions (Fig.1b).
However, several mutations can be found in the genomes that discriminate between the different species of Ebolavirus and even between EBOV sequences from the current and previous out-breaks (Fig.1c).
The identification of these viral genome signatures is important for quick diagnosis in outbreak scenarios.
Additional analysis with all 165 Ebolavirus genomes confirmed these results (Supplementary Fig.S1).
In particular, RAW1 is conserved within EBOV and can distinguish EBOV from other Ebolavirus species.
RAW2 is conserved in all sequences from the West African 2014 outbreak in Guinea, Sierra Leone and Liberia, and only one nucleo-tide difference exists between these sequences and unrelated out-break genomes.
RAW3 is also conserved at the species level, excluding the four EBOV 1976/77 genomes, and can distinguish be-tween all Ebolavirus species (Supplementary Fig.S2).
From the three EBOV sequence motifs absent in the human gen-ome, the first (RAW1) is included in the virus NP, while the other two (RAW2 and RAW3) fall within the sequence of the viral RNA-polymerase (L-protein; Fig.1c).
Previous studies show that the N-terminal region of EBOV NP participates in both the formation of nucleocapsid-like structures through NPNP interactions and in the replication of the viral genome (Watanabe et al., 2006), and RAW1 sequence (TTTCGCCCGACT) is part of this N-terminal region.
The L-protein (LP) produces the viral transcripts to be translated by host ribosomes and is involved in the replication of the viral genome as well.
The LP contains the two additional minimal RAWs, RAW2 (TACGCCCTATCG) and RAW3 (CCTACGCGCAAA).
Both NP and LP are critical for the virus life cycle and constitute good targets for therapeutic intervention.
Screening for new anti-viral compounds could benefit from knowledge of their protein structures.
For EBOV, most protein structures are unknown except for the C-terminal domain of NP, GP, VP24 and VP35 (Shurtleff et al., 2012), thus, we have predicted the structure of the N-terminal regions of the EBOV NP and LP by homology modeling (Supplementary Figs.
S3S5).
These structural models show that the amino acids corresponding to the RAW1 motif are enclosed within the structure, while RAW2 and RAW3 are exposed at the protein surface, which can justify its higher degree of conservation.
4 Discussion The personalized medicine field is now closer to clinical practice with the advances of next-generation sequencing technologies.
Personalized therapeutics are a possibility and their development is essential with the emergence of resistance to current available drugs.
Additionally, quick diagnosis is required for emerging pathogens and in epidemics 2422 R.M.Silva et al.
relative absent words methods Absent Words ., ., &equals; { &equals; .
., , Relative Absent Words www.rcsb.org ) ( L nucleoprotein ) ( ) ( Nucleoprotein--) ( results relative absent words &equals; relative absent words ( ).
&equals; relative absent word relative absent wordsnucleoprotein ( ), L ) (-L relative absent wordssuch as the current Ebola outbreak.
Here, we have detected minimal RAWs in the human genome that are present in EBOV genomes, and identified nucleotide differences in some of these sequences that can distinguish between Ebolavirus species and outbreaks.
Also, we show that the corresponding amino acid sequences are conserved within EBOV.
These results can now be further explored for diagnosis and therapeutics, sometimes mentioned as theranostics (Picard and Bergeron, 2002).
Namely, RAW nucleotide sequences can be used in diagnosis to design primers that identify Ebolavirus infections or distinguish between Ebolavirus species.
For PCR-based methods, lon-ger sequences and multiplex reactions can be developed to avoid pri-mer binding bias.
Additional nucleotide or protein-based strategies for therapeutics can be envisaged, as discussed below.
One problem in developing efficient EBOV treatments is the virus ability to evade the immune system.
The viral GP is a major target because it mediates attachment and entry into the host cells.
However, in addition to the surface envelope protein, the GP gene also produces fragment, soluble GPs that are secreted and direct the Fig.1.
Ebola virus minimal absent words relatively to the human complete genome.
(a) RAWs were identified in 99 unaligned genomes from the current outbreak in Sierra Leone (2014) and are highlighted in red (k 12, arrows), blue (k13) and grey (k14).
(b) Whole genome alignments from 124 published Ebolavirus genomes were obtained from Gire et al.
(2014) and visualized in Geneious (created by Biomatters, available from http://www.geneious.com).
Sequence logos and identity define conserved regions.
(c) Regions corresponding to the identified RAWs are shown in genome location and both as nucleic acid and protein alignments.
The Ebolavirus reference genomes are displayed, as well as selected representative sequences where nucleotide differences are observed Three minimal sequences found in Ebola virus genomes 2423 relative absent words nucleotide glycoprotein ( ) glycoproteinsimmune system to produce antibodies for variable and non-essential regions of the virus (Cook and Lee, 2013; Mohan et al., 2012).
As current efforts based on the viral GP might prove ineffective, add-itional targets should be sought.
Our results show that the viral NP and polymerase (LP) can be attractive targets.
As the amino acid se-quences of all three 12-mer RAWs are conserved within EBOV, these regions can be used to screen for small molecule inhibitors.
In particular, RAW1 is conserved in all Ebolavirus NP proteins, which can indicate a functional or structural role.
And, considering that the protein model predicts that RAW2 and RAW3 are relatively close in the 3D structure and in exposed domains, these regions can be used to develop novel antibodies.
Also, a recently described mechanism shows that the polymerase (LP) from Ebola and Marburg viruses is capable of editing transcripts, resulting in increased variability in the produced proteins, and that the most edited mRNAs are the Ebola GP and Marburg NP and LP itself (Shabman et al., 2014).
Thus, the use of combined therapies towards multiple proteins can be more effective, as suggested by studies to develop vaccines for Lassa virus that target both NP and GP (Fisher-Hoch et al., 2000; Lukashevich, 2012).
RNA-based strategies such as RNA interference (RNAi) or antissense therapies are also promising approaches to silence target-specific gene expression.
The RAW sequences that we have identified can be used to develop RNAi or antisense probes that bind viral tran-scripts and prevent their translation, thus, inhibiting viral replication without blocking the host mRNAs.
Translation of these technologies into clinical applications have been slowed by challenges in the deliv-ery of small RNAs into cells, but recent developments in delivery sys-tems are bridging the bench to bedside gap (Hayden, 2014; Yin et al., 2014).
Among these, gold or lipid nanoparticles (Conde et al., 2014; Draz et al., 2014) were shown to be effective against cancer and viral infections, including EBOV (Geisbert et al., 2010).
Gold-nanobeacons can be applied as a combined diagnosis and therapy tool for effective testing, including in low-cost settings (Costa et al., 2014) and, with this purpose, advances in peptide nucleic acid probes for viral detec-tion are also taking place (Joshi et al., 2013; Zhang et al., 2010).
Whichever the technology, the identification of genome signa-tures for rapid evolving species such as Ebola viruses will be useful for the development of both diagnosis and therapeutics.
Funding This work was supported by the European Fund for Regional Development (FEDER) through the Operational Program Competitiveness Factors (COMPETE) and by the Portuguese Foundation for Science and Technology (FCT), in the context of projects PEst-OE/EEI/UI0127/2014 and Incentivo/ EEI/UI0127/2014, by the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement No.
305444 RD-Connect: An inte-grated platform connecting registries, biobanks and clinical bioinformatics for rare disease research, and the project Neuropath (CENTRO-07-ST24-FEDER-002034), co-funded by QREN Mais Centro program and the EU.
Conflict of Interest: none declared.
Abstract MicroRNAs (miRNAs) are a class of short non-coding RNA molecules that have attracted tremendous attention from the biological and biomedical research communities over the past decade.
With over 1900 miRNAs discovered in humans to date, many of them have already been implicated in common human disorders.
Facilitated by high-throughput genomics and bioinformatics in conjunction with traditional molecular biology techniques and animal models, miRNA research is now positioned to make the transition from laboratories to clinics to deliver profound benefits to public health.
Herein, we overview the progress of miRNA research related to human diseases, as well as the potential for miRNA to becoming the next generation of diagnostics and therapeutics.
Keywords: MicroRNA; Human diseases; Diagnostics; Therapeutics; Biomarker A historical overview of microRNA research MicroRNAs (miRNAs) are a class of recently identified non-coding RNA molecules that play an essential role in gene expression regulation at post-transcriptional levels [1].
With the first miRNA, lin-4, discovered in Caenorhab-ditis elegans in 1993 via forward genetics [2], the second C. elegans miRNA, let-7, was not identified by the same approach until seven years later [3].
This time gap high-lights not only the inefficiency of forward genetics and stan-dard molecular biology techniques to discover miRNAs, but also the lack of enthusiasm among researchers who previously suspected that miRNA was merely a worm-specific phenomenon.
However, the field of miRNA research has since flourished with over 17,000 miRNAs dis-covered to date in 142 species, including more than 1900 in humans [4].
The key word miRNA currently pulls more than 16,000 publications from PubMed, and the first miRNA-targeted drug has now entered a phase II clinical trial (http://www.ClinicalTrials.gov), demonstrating early promise.
In retrospect, the timing of miRNA research 1672-0229/$-see front matter 2012 Beijing Institute of Genomics, Chinese A Ltd and Science Press.
All rights reserved.
Corresponding authors.
E-mail: yli@benaroyaresearch.org (Li Y), Kris.Kowdley@vmmc.org (Kowdley KV).
evolution was particularly interesting as it echoed the time frame of the Human Genome Project (HGP) and many other whole-genome sequencing projects completed over the past decade.
The completion of these projects has impacted the field of miRNA research in profound ways.
The fruitful expansion of miRNA research was triggered by the identification and functional characterization of let-7 [3].
When Ruvkun et al.
demonstrated that the let-7 sequence was highly conserved across the evolutionary spectrum [5], biologists started to realize that this tiny RNA molecule may have a big role to play in humans as well [6].
Before long, three competing laboratories made de novo identifications of dozens of single-stranded RNA molecules approximately 22 nt in length by the combina-tion of an improved cloning method and bioinformatics, a novel approach at the time [79].
The method of de novo identification was rather successful, leading to most miRNA discoveries before 2006, including more than 300 in humans.
More importantly, it revealed the intrinsic char-acteristics of miRNA as a class, such as the secondary structure of miRNA precursors, allowing new miRNAs to be computationally identified.
However, the de novo identification method came with a few limitations.
It was difficult to clone miRNAs expressed at low levels or with certain sequence compositions and post-transcriptional cademy of Sciences and Genetics Society of China.
Published by Elsevier Li Y and Kowdley KV/ MicroRNAs and Diseases 247 modifications [10,11].
Nevertheless, these limitations could be bypassed through in silico prediction.
With the comple-tion of many whole genome sequencing projects [1215], thousands of new miRNA species were now identifiable by computational prediction [4,16].
Taking a variety of fac-tors into consideration, such as sequence conservation and thermodynamic stability of secondary structure, research-ers were now able to identify new miRNA species that failed to be discovered by cloning approaches [17].
To date, the vast majority of known miRNA species have been dis-covered by bioinformatics and their sequences can be found in the Sanger miRNA registry (http://www.miR-Base.com), an open access database for miRNA research [4].
The intriguing story of miRNA cannot be fully revealed without identifying miRNA targets in the context of bio-logical processes.
Through painstaking characterization of miRNA biogenesis and functional pathways [18], it is now clear that miRNAs repress the expression of cellular gene targets in a sequence-dependent manner.
Specifically, the miRNA seed, i.e., the sequence between the 2nd and the 8th nt from the 50 end, is essential in recognizing targets [19].
Facilitated by Dicer, an RNase III family member, the heteroduplex of miRNA and its target mRNA is integrated into the RNA-induced silencing complex (RISC).
Mainly composed with the multi-functional catalytic protein, Arg-onaute, and a double stranded RNA binding protein, TRBP, responsible for recruiting Dicer to Argonaute [2023], RISC plays a central role in miRNA-mediated repression on gene expression [21].
The type of repression relies on the degree of sequence complementarity between seed and target sequences.
Whereas partial complementar-ity may induce translation repression or target mRNA instability, perfect complementarity normally causes target mRNA destruction [24].
This target recognition mecha-nism allows for in silico methods of target prediction by aligning miRNA sequences with entire genomes in search-ing for potential miRNA binding sites.
Adopting similar algorithms, a few groups have developed open access target prediction software with minor variations, such as miRBase, PicTar, TargetScan and miRanda [2530], etc.
Conversely, researchers have been trying to use high-throughput genomic approaches, such as oligonucleo-tide microarrays facilitated by bioinformatics, to experi-mentally identify targets [3133].
Evidence suggests that RNA destabilization is the predominant mechanism mediated by miRNA in mammals, making these methods particularly useful for identifying strong miRNA comple-mentarities with marked effects [34].
Specifically, by introducing a miRNA of interest into cultured mammalian cells, the expression changes of predicted targets are monitored in real-time [31].
Sequence alignment of the arti-ficially over-expressed miRNA and the down-regulated mRNA would further suggest a direct regulation or off-target effect.
Both in silico prediction and target expression profiling suggest that the regulatory relationships of miRNAs and their targets are complex.
Because of short seed sequences, multiple miRNAs may repress the expres-sion of a specific gene simultaneously by targeting different sequence regions; likewise, a single miRNA may be able to regulate the expression of dozens or even hundreds of targets at the same time.
Although it was initially believed that miRNA-mediated repression takes place exclusively in the cytoplasm, new evidence suggests that it may also occur in other cellular compartments such as mitochondria and nucleus [35,36].
The complexity of regulation underscores the necessity of combining traditional molecular biology with modern bioinformatic approaches to characterize the roles of miRNA more effectively.
miRNAs and human diseases As discovery of human miRNAs increased, the research focus was gradually shifted towards functional character-ization of miRNAs, particularly in the context of human diseases.
The connection between miRNAs and disease was obvious.
miRNA expression patterns are tissue-specific [37] and in many cases define the physiological nature of the cell [31].
The definitive evidence came from a report demonstrating that the gene expression profile of a non-neuron cell became more like that of a neuron when the neuron-specific miR-124 was artificially over-expressed within [31].
If the same premise holds true, certain miRNA expression patterns could be disease-specific and hold great prognostic value.
In fact, a more comprehensive miRNA profiling study demonstrated that distinct miRNA expres-sion patterns were specific to various types of cancers and were able to reflect the developmental lineage and differen-tiation state of tumors [38].
More specifically, many miRNAs were found to play key roles in vital biological processes such as cell division and death [39], cellular metabolism [40], intracellular signaling [41], immunity [42] and cell movement [43].
Therefore, aberrant miRNA expression should proportionately affect those critical pro-cesses, and as a result, lead to various pathological and occasionally malignant outcomes.
Here, we overview miRNA-related studies focused on high-priority human diseases with insufficient treatment options (Table 1).
Cancers Since the early stages of miRNA research, cancer has been the most prominent of human diseases with a clear role for miRNA regulation.
The first evidence came from a study by Calin et al.
in which they demonstrated a frequent deletion of miRNA genes miR15 and miR16 among 65% of B-cell chronic lymphocytic leukemia (B-CLL) patients [44].
Intriguingly, down-regulation of miR-15 and miR-16 expression was observed among B-CLL patients without the deletion, suggesting that the pathogenesis of B-CLL may be attributed to the intracellular abundance of two miRNAs.
Encouraged by this finding, this group applied a systemic search on the complete human genome and established correlations of miRNAs with various cancers Table 1 miRNAs associated with common human diseases Disease miRNA Reference Cancer B-CLL miR-15, miR-16 [44] Breast cancer miR-125b, miR-145, miR-21, miR-155, miR-210 [46,56] Lung cancer miR-155, let-7a [47] Gastric cancer miR-145 [54] Liver cancer miR-29b [57,58] Viral diseases HCV miR-122, miR-155 [72,73,78] HIV-1 miR-28, miR-125b, miR-150, miR-223, miR-382 [75] Influenza virus miR-21, miR-223 [76,77] Immune-related diseases Multiple sclerosis miR-145, miR-34a, miR-155, miR-326 [80,81] Systemic lupus erythematosus miR-146a [82,83] Type II diabetes miR-144, miR-146a, miR-150, miR-182, miR-103, miR-107 [84] Nonalcoholic fatty liver disease miR-200a, miR-200b, miR-429, miR-122, miR-451, miR-27 [86] Non-alcoholic steatohepatitis miR-29c, miR-34a, miR-155, miR-200b [87] Neurodegenerative diseases Parkinsons disease miR-30b, miR-30c, miR-26a, miR-133b, miR-184*, let-7 [9092] Alzheimers disease miR-29b-1, miR-29a, miR-9 [94] 248 Genomics Proteomics Bioinformatics 10 (2012) 246253 [45].
Subsequent expression profiling studies further demonstrated the correlation between aberrant miRNA expression patterns and increased occurrence of different types of cancers.
Notably, the deregulation of miR-125b, miR-145, miR-21, and miR-155 expression was associated with the increased risk of breast cancer [46].
In addition, up-regulation of miR-155 and down-regulation of let-7a were correlated with poor survival of lung cancer patients [47], indicating an imbalance of cell death and proliferation during cancer development [4850].
Intriguingly, miRNA expression patterns were also able to stage cancer progres-sion [38], indicating that miRNA levels were not only use-ful in diagnosis but also potentially in prognosis of diseases.
These cancer-related miRNAs were categorized into tumor suppressors and oncogenes due to their associ-ations with opposite clinical outcomes with altered expres-sions.
For example, miR-15, miR-16 and let-7 are known tumor suppressors while miR-21 and miR-155 serve as oncogenes [44,51,52].
The discovery of cancer-related miRNAs by expression profiling inspired mechanistic studies to implicate specific miRNAs in tumorigenesis pathways.
miR-15 and miR-16 were found to repress the expression of anti-apoptotic gene bcl-2 thereby promoting cell death in cancerous cells [52].
Likewise, let-7 family members demonstrate anti-cancer properties due to their ability to repress the expression of the oncogene, ras [53].
In contrast, miR-21 directly serves as an anti-apoptotic factor in glioblastomas and breast cancer [46,51].
Similarly, miR-155 interferes with the pro-cess of mismatch repair by repressing the expression of the MSH gene family members in colorectal cancer [54].
miRNAs also play key roles in tumor invasion and metastasis.
miRNA expression profiling revealed the step-wise down-regulation of miR-145 levels with progression of primary gastric cancers and secondary metastases [55], as well as metastatic prostate cancer [56].
Similarly, increased expression of miR-210 was observed during the invasive transition of breast cancer [57].
While profiling studies establish disease correlations, mechanistic studies characterize the role of miRNAs in greater detail.
For example, through the use of synthetic miRNA mimics, miR-7 and miR-29b were shown to suppress the metastasis of liver cancer by targeting PIK3CD [58] and MMP-2 [59], respectively.
These cancer-related miRNAs are potentially useful for developing not only early diagnosis, but also novel anti-cancer strategies.
Viral diseases Viruses are a group of pathogens with members causing not only severe, chronic diseases, but also some of the most deadly pandemics in human history.
While miRNAs were being identified in eukaryotes, viral-encoded miRNAs were discovered in multiple virus species as well.
The first viral-encoded miRNAs were cloned from a Burkitts lymphoma cell line latently infected by EpsteinBarr virus (EBV), a DNA virus of the herpesvirus family [60].
Soon after, doz-ens of viral miRNAs were identified in polyoma virus[61], adenovirus [62], and several subtypes of the herpes viruses by cloning, bioinformatics, or combined approaches [6365].
Some preliminary evidence even suggested that RNA viruses may also encode miRNAs in spite of small genome sizes [6668]; however, these findings have not been verified independently [65,69].
Besides bearing viral miRNAs, alternatively, viruses are capable of regulating the expression of host cellular miRNAs for their own benefit.
For example, unlike the Kaposis sarcoma-associated herpes virus (KHSV) which encodes a viral miRNA, miR-K12-11, EBV is able to up-regulate the expression of cellular miR-155, an ortholog of miR-K12-11 [70].
Interestingly, these two miRNAs target the same set of cellular genes, indicating a similar function [71].
A more detailed study revealed that miR-155 may prevent EBV-infected cells from apoptotic death [72], a common strategy mediated by hosts to constrain viral infection.
This demonstrates the potential consequences of a virus gaining control of cellular miRNA expression for its survival.
Li Y and Kowdley KV/ MicroRNAs and Diseases 249 Although the expression of some cellular miRNAs is not directly regulated by viruses, maintenance of their intracel-lular level is pivotal for viral infection and replication.
For example, high levels of liver-specific miR-122 expression is necessary for HCV replication both in vitro and in vivo [73,74], although viral infection and replication does not affect the expression of miR-122 [75].
On the contrary, cop-ies of miR-28, miR-125b, miR-150, miR-223 and miR-382 are maintained at high levels in resting CD4+ T cells, but significantly decreased in activated CD4+ T cells, resulting in productive infection of HIV-1 in only the latter case [76].
These findings may help explain the tissue-specificity of virus infections and provide novel targets for anti-viral therapeutics.
Finally, miRNA expression changes may demonstrate how hosts respond to viral infections.
For example, aber-rant expression of a subset of cellular miRNAs was observed in lethal influenza virus infection, but not in non-lethal infection in animal models [77,78].
Specifically, miR-21 and miR-223 were strongly up-regulated in lethal infections of H1N1 pandemic influenza virus and H5N1 avian influenza virus in mice and macaques, respectively [77,78] while their expression was unchanged or only mod-erately up-regulated in animals infected with less patho-genic viruses.
More recently, marked increase of miR-155 was seen in HCV-infected patients [79].
The up-regulation of miR-155 by HCV-infection may activate Wnt signaling pathway and contribute in part to HCV-induced hepato-carcinogenesis [79].
These variable miRNA expression patterns may be useful in guiding physicians to make treat-ment plans for patients infected by more or less virulent pathogens.
Immune-related diseases Many common immune-related diseases, including multi-ple sclerosis (MS), systemic lupus erythematosus (SLE), type I/II diabetes, and nonalcoholic fatty liver disease (NAFLD), have shown established correlations with cellu-lar miRNAs.
Dozens of miRNA signatures were identified by comparing the miRNA expression profiles of relapsing-remitting MS and healthy controls [80].
Specifically, the expression of miR-145 alone was found to distinguish affected patients from healthy controls with high specificity and sensitivity.
Increased expression of miR-34a, miR-155 and miR-326 was observed in MS lesions [81], with addi-tional evidence indicating that high levels of miR-326 had a strong correlation with increased severity of MS [82].
In two independent studies involving hundreds of SLE patients and healthy controls, decreased expression of miR-146a demonstrated a strong correlation with increased risk for SLE among east Asian and European populations [83,84].
miRNA expression profiling has also identified type 2 diabetes-related miRNAs including miR-144, miR-146a, miR-150 and miR-182 [85].
In addition, miR-103 and miR-107 were shown to negatively regulate glucose homeostasis and insulin sensitivity in type 2 diabetes by targeting caveolin-1, a critical regulator of insulin receptor [86].
Increased expression of miR-200a, miR-200b and miR-429 and decreased expression of miR-122, miR-451 and miR-27 were associated with diet-induced NAFLD in rats [87].
Furthermore, abnormal expression of miR-29c, miR-34a, miR-155, and miR-200b were found in a mouse model of non-alcoholic steatohepa-titis (NASH) [88], in addition to 23 more identified in tissues from NASH patients by miRNA microarrays [89].
Mechanistic studies revealed that miRNAs play critical roles in inflammation primarily by regulating the pathways associated with nuclear factor kappa beta (NF-jB), the central mediator of inflammatory response.
The best char-acterized ones are miR-155 and miR-146, which were implicated in many immune-diseases [73,74,81,85,88].
In a negative feedback loop in which NF-jB activation up-regulates miR-146 expression, miR-146 subsequently down-regulates the expression of IRAK1 and TRAF6, two up-stream activators of NF-jB [42].
Similarly, increased expression of miR-155 by NF-jB could repress both IKK-b and IKK-e, and prevent NF-jB from being constitutively activated [90].
This negative feedback mech-anism effectively keeps the activity of NF-jB in check.
These findings not only provided insights about miRNA-mediated inflammatory responses, but also of potential drug targets for fine-tuning the immune system.
Neurodegenerative diseases Neurodegenerative diseases (ND) such as Parkinsons dis-ease (PD) and Alzheimers disease (AD) have placed sub-stantial social-economic burdens on countries with aging populations.
As the pathogeneses of NDs on molecular lev-els remain poorly understood, successful treatments are still unavailable.
With increasing investments from govern-ments and pharmaceutical companies, biomedical research on neurodegenerative diseases has become proprietary.
Notably, recent progresses from studies elucidating miRNA functions in NDs have shed new light on disease pathogenesis and may lead to novel treatment strategies.
For example, a systemic miRNA profiling in peripheral blood mononuclear cells from PD patients revealed miR-30b, miR-30c, and miR-26a to be associated with the susceptibility of the disease [91].
Deregulation of miR-133b expression may contribute to the pathogenesis of PD, as the miR-133b-Pitx3 feedback loop is essential for maintaining dopaminergic neurons in the brain [92].
In a Drosophila model for PD, pathogenic leucine-rich repeat kinase 2 (LRRK2) was shown to promote the expression of transcriptional factors E2F1 by down-regulating expression of let-7 and miR-184* [93].
Likewise, an analysis of miRNA and mRNA expression in brain cor-tex from AD and age-matched control subjects demon-strated strong correlations between the expression levels of miRNAs and predicted mRNA targets [94], implying functional relevance of microRNA-mediated regulations in AD pathogenesis.
More specifically, the expression of 250 Genomics Proteomics Bioinformatics 10 (2012) 246253 miR-29a, miR-29b-1 and miR-9 was significantly decreased in AD patients [95], resulting in abnormally high expres-sion of their target BACE1, a protein playing an important role in AD pathogenesis [96].
These findings not only highlight the importance of miRNA research in under-standing ND pathogenesis, but also provide a previously unrecognized venue for medical interventions.
miRNAs in disease diagnosis and therapy While the combination of molecular and computational approaches have revealed the role for miRNAs in common human diseases, concurrent developments of miRNA bio-markers and miRNA drugs have made great strides towards improving public health.
The ultimate goal of biomarker identification is to develop better clinical tests that improve diagnosis or prog-nosis of diseases.
In fact, miRNAs have been considered a top candidate for the next generation of biomarker as they possess a few advantages over other candidates such as proteins and metabolites [97].
First, miRNA biomarkers would more likely lead to early diagnosis due to their upstream positions in regulation cascades.
Second, novel miRNA biomarkers would be more readily discovered by genomic tools such as oligonucleotide microarrays and deep sequencing which deliver higher throughput than mass spectrometry, the primary tool for protein and metabolite biomarker identification.
Third, low abundant miRNA biomarkers can be amplified and then detected in a clinical setting by real-time quantitative PCR (qPCR), an approach used in FDA-approved clinical tests already; whereas, no equivalent approach is available in detecting low abundant proteins or metabolites.
The adoption of the locked-nucleic acid (LNA) technology in miRNA probe design could improve the sensitivity and specificity of miRNA qPCR assays even further [98].
Non-invasive miRNA biomarkers are more sought after due to fewer complications associated with the specimen Figure 1 The road from laboratory to clinic: the promises and challenges of m The hopscotch course in green is a layout of an ideal path of miRNA research challenges at different steps.
collection through the more prominent use of bodily fluids such as serum and plasma.
In fact, circulating miRNA bio-markers have demonstrated early promises in diagnosis of prostate cancer [99], lung cancer [100,101], liver cancer [102] and breast cancer [103].
As circulating miRNAs are very stable in the blood [99,104], they could be well-preserved in archived serum or plasma specimens, a gold mine for miRNA biomarker development.
miRNA drug development is still in its infancy with the exception of SPC3649, a LNA-modified oligonucleotide developed by Santaris Pharma A/S to repress the expres-sion of miR-122, in treating chronic HCV infection.
This miRNA drug demonstrated impressive repression efficacy on miR-122 in mice [105] and in African green monkeys [106], as well as anti-viral efficacy in chimpanzees chroni-cally infected by HCV [74].
Compared to a combined administration of pegylated interferon-a and ribavirin, the standard treatment for HCV infection, SPC3649 dem-onstrated better safety profiles in chimpanzees [74] and desired tolerance in healthy volunteers.
Importantly, the SPC3649 treated patients rarely experienced viral-relapse, whereas viral-relapse is common in patients treated with pegylated interferon-a.
Interestingly, the expression of interferon-regulated genes decreased in parallel with HCV titers during the SPC3649 treatment.
This indicates the effectiveness of SPC3649 on patients infected with viral strains resistant to the interferon-a treatment.
Future directions In spite of the early success of SPC3649, few miRNA drugs have entered clinical phases due to two major challenges.
First, currently available target prediction softwares have high false-positive rates, making it difficult to identify a bona fide miRNA target by in silico prediction alone [107].
To better predict a miRNA drug target before enter-ing costly animal and clinical studies, researchers should take the advantage of combining molecular biology and iRNA research evolved from basic research to clinical practice.
Red boxes indicate major Li Y and Kowdley KV/ MicroRNAs and Diseases 251 bioinformatic approaches in target prediction and valida-tion.
Recent advancement of molecular biology techniques, such as RISC immune-precipitation and Argonaute-protein crosslinking immune-precipitation, provide valu-able tools allowing target enrichment before bioinformatic predications [108,109].
These techniques should be fully integrated into the studies for target identification.
Second, the effective dose of a miRNA drug may induce unsafe off-target effects.
A cocktail regimen of miRNAs collabora-tively repressing the same target at low doses could be a potential solution.
This strategy requires not only extensive bioinformatic efforts in drug designs, but also high-throughput genomic screening to validate the drug effects.
Concluding remarks Without a doubt, the importance of miRNA is gaining appreciation.
However, even with its already demonstrated promise, miRNA diagnosis or therapy may be many years away from entering the clinic as complex challenges remain (Figure 1).
It should be noted that any major leap forward in miRNA research over the past decade was the result of multidisciplinary collaborations of researchers with extensive expertise in molecular biology techniques, high-throughput genomics, and bioinformatics.
These produc-tive collaborations should be expended even further.
With clinicians joining the club, miRNA research will be given a fresh perspective that may lead to steady progress in devel-opment of clinical applications.
Competing interests The authors declare no conflict of interests.
Acknowledgements We thank Bryan Maliken for editing assistance.
This work is supported by the grants from NIDDK (Grant No.
3R01DK056924-08S1 and 5K24DK002957) and NHLBI (Grant No.
1R21HL112678).
Abstract Publicly-accessible resources have promoted the advance of scientific discovery.
The era of genomics and big data has brought the need for collaboration and data sharing in order to make effective use of this new knowledge.
Here, we describe the web resources for cancer genomics research and rate them on the basis of the diversity of cancer types, sample size, omics data com-prehensiveness, and user experience.
The resources reviewed include data repository and analysis tools; and we hope such introduction will promote the awareness and facilitate the usage of these resources in the cancer research community.
Introduction There has been accumulating evidence over the last two to three decades supporting that cancer is a disease of the gen-ome.
Previous studies have followed a one-by-one approach to examine the molecular mechanisms of cancer, although this approach is one-sided and inefficient.
With the development of high-throughput sequencing technologies, recent years have witnessed a great data explosion and systematic study of the cancer genome.
For the first time, data were made available for the complete genome sequences including point mutations and structural alternations for a large number of cancer types, enabling the differentiation of cancer subtypes in an unprece-dented global view.
However, the effective use of the massive nces and Yang Y et al/ Web Resources for Cancer Genomics 47 amounts of cancer genome data remains a challenge due to the limitations of computational methodologies and insufficient collaboration and sharing (Figure 1).
In this paper, we describe several popular and effective web-based cancer genomics data repositories, along with tools and resources (Table 1) to man-age and analyze these data.
We have rated the resources based on data comprehensiveness and ease-of-use according to our own experience.
Cancer Genomics Hub The Cancer Genomics Hub (CGHub) is a central repository for the genomic information generated through three different programs at the National Cancer Institute (NCI) of the United States, namely, The Cancer Genome Atlas (TCGA), the Cancer Cell Line Encyclopedia (CCLE), and the Therapeutically Applicable Research to Generate Effective Treatments (TARGET) projects [1].
CGHub is hosted at the University of California, Santa Cruz (UCSC), with controlled data access to ensure patient privacy.
CGHub holds nearly 1.9 PB of data, covering 42 cancer types and normal controls (https://cghub.ucsc.edu/summary_stats.html).
Till Dec 2014, there have been more than 10,000 samples from TCGA alone (https://tcga-data.nci.nih.gov/tcga/tcgaHome2.jsp) and more than 500 papers have been published by researchers from the TCGA Research Network and those who used TCGA data in their work (http://cancergenome.nih.gov/publications).
The launch of CGHub will promote the sharing of cancer data, collaboration between cancer researchers, and potentially facilitate the personalized medicine.
Figure 1 The future of cancer research In the era of big data, one of the major challenges is to make full use of multi-dimensional data from heterogeneous sources, including different omics data and a variety of medical data from bedside.
The success in the battle against cancer will largely depend on population-sized information from both genomic and clinical resources, advanced algorithms for data mining, open and sharing circumstances.
European Genome-phenome Archive The European Genome-phenome Archive (EGA) is a data cen-ter for all types of sequencing and genotyping experiments.
Almost 58% of all studies in EGA are related to cancer, including data generated by the International Cancer Genome Consortium (ICGC).
Since its founding in 2008, ICGC has produced terabytes of data from about 12,232 donors and 50 cancer projects (https://dcc.icgc.org/repos-itory/release_17).
Somatic variant data are openly accessible at the ICGC Data Portal (https://dcc.icgc.org/repository), whereas raw sequence data, germline mutations, and clinical data are held at EGA with controlled access.
Catalogue Of Somatic Mutations In Cancer The Catalogue Of Somatic Mutations In Cancer (COSMIC) is the largest database of somatic mutations and their effects on human cancer [2].
The database is curated manually from pub-lished literature, allowing very precise definitions of disease types and patient details.
The database is updated every 2 months and has thus far integrated 15,047 whole cancer gen-omes from 1,058,292 samples, including 2,710,499 coding mutations, 10,567 gene fusions, 61,232 genomic rearrange-ments, 702,652 copy number variations (CNVs), and 118,886,698 abnormal expression variants.
Data can be quer-ied by key words and downloaded by registered users.
COSMIC has also stored curated, large-scale systematic screens and whole-genome shotgun sequencing papers for reference.
The huge, manually-curated, and regularly-updated dataset of the COSMIC database makes it an invaluable resource for cancer studies.
Cancer Program Resource Gateway The Broad Institute is one of the most famous academic cen-ters for cancer study.
Its Cancer Program aims to investigate the fundamental mechanisms of cancer and research from dis-covery to clinical applications.
The Program releases many datasets and tools for scientific research, which are held at the Broad Cancer Program Resource Gateway (CPRG).
We will describe Broads Genome Data Analysis Center (GDAC), one of these resources, as an example.
Broads GDAC It is important but generally time-consuming or sometimes even impossible for most labs to analyze terabytes of sequence data.
However, the Firehose system from Broads GDAC is changing the status quo.
The GDAC systematically analyzes data from TCGA pilot and extends to other diseases as well.
Firehose now assembles 40 terabytes of TCGA data and reli-ably executes more than 6000 pipelines per month.
GDAC obtains and processes the TCGA data every 2 weeks, and makes them available afterward [3].
Firehose contains series of standardized pipelines for genomic analysis and the comput-ing environment is accessible to the public so that people can install and run their own tools for data analysis.
Taking advantage of the powerful computational environment at 48 Genomics Proteomics Bioinformatics 13 (2015) 4650 Broad, Firehose provides continuously-updated data and results in different tiers, including version-stamped standard-ized datasets, analysis results, and biologist-friendly reports.
SNP500Cancer The SNP500Cancer database, a part of the Cancer Genome Anatomy Project (CGAP) (http://cgap.nci.nih.gov/Tools), is a repository for sequence and genotype verification of single nucleotide polymorphisms (SNPs) in cancer and other complex diseases [4].
The main aim of the SNP500Cancer project is to re-sequence reference samples to find known or novel SNPs for molecular epidemiology studies in cancer.
The database provides sequence information for anonymized control DNA samples and can be queried by gene, gene ontology pathway, chromosome, the SNP Database (dbSNP) ID, or SNP500Cancer SNP ID.
SNP500Cancer is an insightful resource for researchers to select SNPs for further analysis due to its high confidence, nonetheless its data volume is limited.
canEvolve With the rapid development of biological technologies, gen-ome-wide tumor profiling has grown drastically in scale and availability.
canEvolve fulfills the need for data integration and interpretation.
canEvolve contains integrated data from 90 studies involving more than 10,000 patients.
Data analysis can be performed at different levels: (1) primary analysis like mRNA, microRNA (miRNA) and protein expression, genome variations and proteinprotein interactions; (2) integrative analysis of gene and miRNA expression, gene expression and CNVs, and gene set enrichment analysis; (3) network analysis; and (4) survival analysis.
All the aforementioned data can be queried and visualized in table or graph format.
canEvolve is a comprehensive functional genomics platform and is fully accessible to the public [5].
MethyCancer DNA methylation plays a crucial role in the development of cancer and is associated with oncogene activation, chromo-somal instability, and tumor suppressor gene silencing.
MethyCancer is designed to interpret the relationship between DNA methylation, gene expression, and cancer.
MethyCancer houses data on (1) DNA methylation, (2) CNV and cancer information, (3) CpG Island clones, and (4) the correlations between these datasets.
The database can be easily searched with the user-friendly MethyView display [6].
SomamiR miRNA sequence variation is contributable to a variety of can-cers.
SomamiR is a database created to investigate the associa-tion of somatic and germline mutations with miRNA function in cancer.
The mutation data are collected from the Gene Expression Omnibus (GEO) and the Pediatric Cancer Genome Project (PCGP), whereas the miRNA data are from miRBase release 18.
There are many ways to search SomamiR for somatic mutations that affect miRNA target Yang Y et al/ Web Resources for Cancer Genomics 49 sites, genome-wide association studies (GWAS) in cancer, can-didate gene associations in cancer, KEGG pathways, and mutation information in a genome browser.
SomamiR also contains a set of experimentally-validated somatic and germ-line mutations that disrupt miRNA function associated with cancer.
The integration of mutation data in SomamiR will enable scientists to more accurately predict whether mutations would affect miRNA binding and consequently functional regulation [7].
cBioPortal The cBioPortal for Cancer Genomics is an accessible portal for researchers to explore, visualize, and analyze multidimensional cancer genomics data [8].
The portal contains datasets from many published cancer studies including CCLE and TCGA.
It is worthy of note that cBioPortal processes original molecu-lar profiling data from cancer tissues and cell lines into smaller datasets.
Researchers can interactively explore patterns of genetic alterations across samples in a single study, compare gene alteration frequencies across multiple studies, or summar-ize all relevant genomic variations in an individual tumor sam-ple through the cBioPortal web query interface.
It also supports biological pathway exploration, survival analysis, and data downloading service.
UCSC Cancer Genomics Browser The UCSC Cancer Genomics Browser is an online analysis tool for hosting, visualizing, and analyzing information on cancer genomics and clinical research [9].
The browser pro-vides users with measurements from a single experiment and the associated clinical information for multiple samples.
Furthermore, two or more datasets can be viewed at the same time to allow comparisons of gene expression and CNV across different data and cancer types.
Data downloading and clinical heatmap are also supported.
The browser currently contains genome-wide experiments on 71,870 samples, most of which are from large-scale international cancer projects including TCGA, CCLE, and the Stand Up To Cancer initiative.
Cancer Genome Work Bench The Cancer Genome Work Bench (CGWB) is an effective tool with a focus on gene expression and mutations, CNV, and methylation [10].
It provides an automated analysis pipeline for users to visualize and analyze genomic information and gene expression variations in each sample.
There are two main viewers in CGWBHeatmap viewer and Genome browser, both of which can present user-specified information such as gene expression, somatic mutations and pathway context that the mutations may be involved in, and users can toggle between these two viewers.
Its of note that CGWB has kept clinical data and individual genotypes private and permission is needed for access to these data.
Genomics of Drug Sensitivity in Cancer The Genomics of Drug Sensitivity in Cancer (GDSC) database is the largest open knowledgebase of drug sensitivity and drug response in cancer cells [11].
The GDSC project includes experiments describing the response to almost 200 anticancer therapeutics in more than 1000 cancer cell lines.
The cell line drug sensitivity data were integrated with large genomic data-sets in the COSMIC database to identify molecular markers of cancer drug response.
In the GDSC website, drug sensitivity can be queried by cancer gene, cancer cell line, and compound.
Data retrieved are presented in a variety of graphical represen-tations for view and downloading.
The large collection of cell lines, drug sensitivity, and genomic datasets have enhanced our understanding of cancer cell genomic heterogeneity and facili-tated the discovery of new patient-specific treatments.
canSAR canSAR is an open cancer-centered cross-discipline knowl-edgebase that aims to facilitate translational cancer research [12].
The knowledgebase features multidisciplinary informa-tion.
For instance, for a given protein or drug, researchers can obtain information on current understanding of the pro-tein or drug, such as the expression or mutation of the protein in cancer, cellular sensitivity profiles, and specific binding pro-teins of the drug.
A large collection of human proteome data is housed in canSAR right now.
Moreover, this database also contains data and annotations for cancer, nontransformed cell lines and protein 3D structure summaries.
NONCODE Noncoding RNAs (ncRNAs) function in a variety of cancer types.
NONCODE is a regularly-updated knowledgebase of ncRNAs (except for tRNAs and rRNAs) from several species such as human and mouse [13].
NONCODE was first launched in 2005 and was reported in Science NetWatch [14].
Version 4.0 of NONCODE has 595,854 ncRNA entries, among which 210,831 (about 35%) are long ncRNAs (lncRNAs).
More than 80% of the ncRNA data in NONCODE are experimentally-derived, thus providing users with a highly-credible resource.
In addition to providing lncRNA expression profiles across tis-sues, NONCODE provides an online pipeline called iLncRNA for users to analyze customized RNA-seq data.
The database also has an ID conversion function that changes RefSeq or Ensembl IDs into NONCODE IDs.
Concluding remarks Recent advances in genome and related information technolo-gies have accelerated the bridging of scientific research and clinical application, with the creation of publicly-accessible data repository and analytical tools.
Cancer genome data from large-scale projects such as TCGA and ICGC are being used by large firms to develop new targets and biomarkers.
This review provides a brief introduction to some representative web-based resources that can be divided into five categories.
First, resources such as CGHub, EGA, COSMIC, and cBioPortal serve as an encyclopedia of common cancers and omics data types.
Other resources offer tools for data analysis and integration, and house some analysis results for query (e.g., CPRG, GDAC, and canEvolve).
The third class of tools is mainly used for visualization (e.g., UCSC cancer browser 50 Genomics Proteomics Bioinformatics 13 (2015) 4650 and CGWB).
The fourth class includes databases that focus on inferring the association of specific biological features to cancer (e.g., MethyCancer, SomamiR, and NONCODE).
Finally, databases such as GDSC and CanSAR support the application of genomics to drug discovery.
Many other useful resources for cancer research were not included in this paper due to the scope limit.
However, the move from bench to bed-side remains a daunting task.
One of the major obstacles is the heterogeneity of cancer cells and the variability in the response to anticarcinogens among patients with similar symptoms.
To address this issue, personal genomics projects should be initiated in a larger population than the average sample size for the available projects.
In addition, advanced com-putational and statistical methodologies aimed at big data management and data mining should be developed to establish the clinical relevance of cancer genomic discovery.
Competing interests The authors declared that there are no competing interests.
Acknowledgements This research was supported by the Strategic Priority Research Program of the Chinese Academy of Sciences, Stem Cell and Regenerative Medicine Research (Grant No.
XDA01040405), the National High-tech R&D Program of China (863 Program, 2012AA022502) and the National Twelfth Five-Year Plan for Science & Technology Support of China (2013BAI01B09) awarded to XF and the National Natural Science Foundation of China (Grant No.
31471236) awarded to YL.
ABSTRACT Motivation: Research in the biomedical domain can have a major impact through open sharing of the data produced.
For this reason, it is important to be able to identify instances of data production and deposition for potential re-use.
Herein, we report on the automatic identification of data deposition statements in research articles.
Results: We apply machine learning algorithms to sentences extracted from full-text articles in PubMed Central in order to automatically determine whether a given article contains a data deposition statement, and retrieve the specific statements.
With an Support Vector Machine classifier using conditional random field determined deposition features, articles containing deposition statements are correctly identified with 81% F-measure.
An error analysis shows that almost half of the articles classified as containing a deposition statement by our method but not by the gold standard do indeed contain a deposition statement.
In addition, our system was used to process articles in PubMed Central, predicting that a total of 52 932 articles report data deposition, many of which are not currently included in the Secondary Source Identifier [si] field for MEDLINE citations.
Availability: All annotated datasets described in this study are freely available from the NLM/NCBI website at http://www.ncbi.nlm .nih.gov/CBBresearch/Fellows/Neveol/DepositionDataSets.zip Contact: aurelie.neveol@nih.gov; john.wilbur@nih.gov; zhiyong.lu@nih.gov Supplementary Information: Supplementary data are available at Bioinformatics online.
Received on July 22, 2011; revised on September 20, 2011; accepted on October 8, 2011 1 INTRODUCTION Research in the biomedical domain aims at furthering our knowledge of biological processes and improving human health.
Major contributions toward this goal can be achieved by sharing the results of research efforts with the community, including datasets produced in the course of the research work.
While such sharing behavior is encouraged by funding agencies and scientific journals, recent work has shown that the ratio of data sharing is still modest compared with actual data production.
For instance, Ochsner et al.
(2008) found the deposition rate of microarray data to be <50% for work published in 2007.
Piwowar and Chapman (2007) show that data deposition results in increased citation of papers reporting on data production.
While this To whom correspondence should be addressed.
should serve as an incentive to deposit data and announce it to the community, in a more recent study these same authors (Piwowar and Chapman, 2010) show that data deposition is significantly associated with high-profile journals and experienced researchers.
In the course of this work, these authors have found the identification of data deposition statements to be a challenging task that can be addressed using natural language processing and machine learning methods (Piwowar and Chapman, 2008a).
Information about the declaration of data deposition in research papers can be used in different ways.
First, for data curation: databases such as MEDLINE use accession numbers for certain databases as metadata that can be searched with PubMed queries using the [si] field.
Journals can benefit from such a tool to check whether their data deposition policies are enforced.
This aspect is also important for researchers looking to re-use datasets and build on existing work.
Second, for the analysis of emerging research trends: the type of data produced gives indications on current important research topics.
In a study based on the analysis of Medical Subject Headings (MeSH) indexing, Moerchen et al.
(2008) show that such metadata may be used to predict future research trends, including recommendations of main headings to be added to the MeSH thesaurus.
Our long-term research interest is in assessing the value of using deposition statements for predicting future trends of data production.
The initial step of automatically identifying deposition statements could then lead to an assessment of the need for storage space of incoming data in public repositories.
In this study, we aim at identifying articles containing statements reporting the deposition of biological data.
As explained above, the study of data deposition has generated a growing interest in the past few years.
In response to a Nature Methods editorial (Anonymous, 2008) describing the deposition of data such as genome sequence or microarrays as routine, Ochsner et al.
(2008) used a manually built query to retrieve articles likely to report the production of microarray data in 2007 publications.
They manually analyzed 398 articles reporting the production of microarray data and concluded that only 50% report deposition of microarray data in the Gene Expression Omnibus (GEO) or other databases.
Piwowar and Chapman (2008a) further studied the links existing between microarray data deposition in public repositories (e.g.
GEO and ArrayExpress) and reports of data deposition in the literature.
They used machine learning to build a query suitable for retrieving research articles in PubMed Central reporting on data deposition.
Piwowar and Chapman (2008b) also addressed the classification of articles (at the article level) for data sharing in five databases (GenBank, Protein DataBank, GEO, ArrayExpress, Stanford Microarray Database).
The authors compared pattern matching versus machine learning.
The best results were obtained with a J48 decision tree on ArrayExpress (96% F-measure), The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:55 23/11/2011 Bioinformatics-btr573.tex] Page: 3307 33063312 Extraction of data deposition statements although the corresponding dataset was rather small: 29 documents including 12 positive results.
Overall performance on the five databases was 69% F-measure.
Kim et al.
(2010) compared Support Vector Machine (SVM) and Bayes classifiers for the identification of sentences containing database accession numbers.
This task was tailored to the specific need for curation of accession numbers in databases such as MEDLINE.
These authors discussed the errors linked to ambiguity of accession numbers with other reference numbers such as PMIDs.
However, they did not discuss or investigate the occurrence of accession numbers for purposes other than data deposition.
For instance, accession numbers can be given in sentences reporting the re-use of previously deposited data.
They can also be used in sentences discussing datasets that were produced separately from the context of the experiments reported in the article.
Finally, in an effort to bridge the gap between specific gene or genomic regions and related research articles, Haeussler et al.
(2011) show that the extraction of short DNA sequences from full-text articles can be used to automatically map articles to GenBank entries without relying on mentions of gene names or accession numbers.
In our work, we propose to identify articles reporting data deposition through the classification of sentences.
The general topics of text classification and specifically sentence classification have been well studied in the past decade (Sebastiani, 2002).
In the biomedical domain, many tasks can be approached as a sentence classification problem.
Often, the small number of classes studied makes the problem amenable to the use of machine learning methods.
For instance, several efforts aiming at the retrieval of text passages as evidence for biological or clinical phenomena performed sentence classification.
Demner-Fushman et al.
(2005) addressed the classification of MEDLINE abstract sentences between seven clinical outcome categories in order to automatically identify outcome-related information in the medical text.
They reported the precision of the top ranked sentence between 50% and 60% depending on category.
Kim et al.
(2011) used the same dataset for classifying sentences for evidence-based medicine.
Their best performance for SVM classification was 81% F-measure obtained for the outcome category of sentences in structured abstracts.
Results for unstructured abstracts and other categories were less successful.
Polajnar et al.
(2011) addressed the identification of MEDLINE abstract sentences denoting protein protein interaction as a binary classification problem.
Using SVM classifiers and protein annotations as features, they reported best F-measure performance of 70%.
While these efforts were limited to abstracts, other work used full-text articles.
In the BioCreative II challenge (Krallinger et al., 2008), the interaction sentences subtask required participants to retrieve passages of up to three consecutive full-text sentences providing evidence for protein protein interaction.
The best performing team obtained 20% precision when automatically extracted passages were compared with evidence sentences manually selected by curators.
These results reflect the difficulty of the task of extracting evidence statements from full-text articles.
2 MATERIAL AND METHODS In this study, we are interested in identifying statements declaring the deposition of biological data (such as microarray data, protein structure, gene sequences) in public repositories.
In the rest of this article, we will refer to such statements as deposition statements.
We take these statements as a primary method of identifying articles reporting on research that produced the kind of data deposited in public repositories.
(1) and (2) show examples of such statements, with varying degrees of specificity.
In (1) both the data and location are referred to in a highly specific manner [i.e.
the sequence of labA and DDBJ/GenBank/EMBL databases (accession no AB281186)], whereas in (2) data and deposition location are both very general (the microarray data and MIAMExpress).
While the mention of data, public repositories and accession numbers are strong indicators of deposition statements, (3) and (4) show that these elements can also occur when authors refer to previous work.
In the remainder of this article, we will refer to statements that do not report the deposition of data in public repositoriessuch as (3) and (4) as non-deposition statements.
(1) The sequence of labA has been deposited in the DDBJ/GenBank/ EMBL databases (accession no AB281186) (PMID 17210789).
(2) The microarray data were submitted to MIAMExpress at the EMBL-EBI (PMID 18535205).
(3) Histone TAG Arrays are a repurposing of a microarray design originally created to represent the TAG sequences in the Yeast Knockout collection (Yuan et al., 2005; NCBI GEO Accession Number GPL1444) (PMID 18805098).
(4) Therefore, the primary sequence of native Acinetobacter CMO is identical to the gene sequence for chnB deposited under accession number AB006902 (PMID 11352635).
Figure 1 gives an overview of the annotated datasets used in the training and test phases of this work.
The various datasets shown on Figure 1 are provided as Supplementary Material and are also freely available to the research community from the NLM/NCBI website.
The following sections describe details of the datasets and experiments.
In Section 2.1, we describe the method used to collect the training datasets, and the analysis of deposition sentences that we carried out in order to gain an understanding of the variety and common characteristics of these statements.
In Sections 2.2 and 2.3, we explain how the training datasets were used to automatically identify deposition elements and perform sentence classification.
Finally, in Section 2.4 we present the test set and in Section 2.5 we describe the experiments performed on the test set.
2.1 Training corpus collection and analysis Corpus collection: to gain a better understanding of the variety of deposition statements across data types, journals and databases, we compiled a corpus of deposition statements based on previous work by Piwowar and Chapman (2008a) and Ochsner et al.
(2008) that we extended.
Specifically, 112 microarray deposition statements from 105 articles were obtained using the existing corpora.
After a manual review of these statements, two strategies were devised to collect additional statements.
Our regular expression strategy consisted in two steps.
First, the Ochsner et al.
query1 was used to retrieve 2008 articles in PubMed Central.
Second, articles were segmented into sentences and sentences likely to report data deposition were retrieved if they met the three following criteria: (i) sentence length was between 50 and 500 characters to avoid section titles and sentence segmentation errors; (ii) sentence contained a mention of GEO 1 (microarray[All Fields] OR genome-wide[All Fields] OR microarrays[All Fields] OR expression profile[All Fields] OR expression profiles[All Fields] OR transcription profiling[All Fields] OR transcriptional profiling[All Fields]) AND (Endocrinology[jour] OR Mol Endocrinol[jour] OR J Biol Chem[jour] OR Proc Natl Acad Sci U S A[jour] OR Mol Cell Biol[jour] OR Nature[jour] OR Nat Med[jour] OR Nat Cell Biol[jour] OR Nat Genet[jour] OR Nat Struct Mol Biol[jour] OR Science[jour] OR Cancer Res[jour] OR FASEB J[jour] OR Cell[jour] OR Nat Methods[jour] OR Mol Cell[jour] OR J Immunol[jour] OR Immunity[jour] OR EMBO J[jour] OR Blood[jour]).
3307 [09:55 23/11/2011 Bioinformatics-btr573.tex] Page: 3308 33063312 A.Nvol et al.
Fig.1.
Overview of annotated datasets used in this work.
or ArrayExpress, which are the largest databases for microarray data (Stokes et al., 2008) or a mention of a GEO or ArrayExpress accession number or the pattern [micro]?array data|experiment/analys|analyz; and (iii) sentence contained one deposition action seed from the following: deposit, found, submit, submission, available, access, uploaded, entered, posted, provided, assigned, archived.
After manual review, 133 of the 243 candidate sentences were added to the pool of deposition statements.
The remaining 110 sentences [such as (3) and (4)] were kept as examples of non-deposition statements, and used in our machine learning strategy to retrieve deposition statements for data other than microarray.
In the machine-learning strategy, we aimed at enriching our training corpus, as proposed by Yeganova et al.
(2011).
A simple (i.e.
only using sentence tokens as features) Nave Bayes (NB) model was built using the 243 microarray data deposition statements as positive examples and 33 000 sentences (the 110 above non-deposition statements, plus sentences from MEDLINE abstracts that contained the word deposit or deposited) used as negative examples.
In spite of our blanket assumption that the sentences extracted from MEDLINE abstracts were non-deposition statements, we did expect a small number of them to be actual deposition statements.
Our reasoning was that the proportion of true non-deposition statements would be high enough to train an efficient model; while applying the model on the set of so-called negatives, it would rank the deposition statements high enough to collect them and adjust our training sets.
By iterating on this method recursively, we finally obtained a training set of 586 positive or deposition statements (including the initial 243 microarray deposition statements) and 578 negative or non-deposition statements that scored high with the model (including the initial 110 non-deposition statements).
This set was used as training data for building NB and SVM data deposition models, and will be referred to as Train-D (Fig.1).
Analysis of deposition elements: to better characterize deposition statements, sentences were tagged for components referring to data, deposition action and deposition location using the following guidelines: Data: a phrase referring to biological data that can be found in public repositories.
Patient data and data relevant to ClinicalTrials.gov were not considered.
However, generic references to data were marked, when used in the context of biological data.
This included expressions such as the data, the protein, RNA, DNA.
In addition, specific references to data such as p53 conditional knockout mouse aCGH data were also marked.
Action: a phrase describing the action undertaken by authors regarding depositing data.
This includes phrases such as: deposit, submit, upload/download, is available, can be found, etc.
General Location: reference to the location of data deposition, e.g.
public repository name or website URL (e.g.
http://www.ncbi.nlm.nih.gov/genbank/).
This also includes a reference to an organization hosting a public repository in the context of data deposition.
Detailed Location: detailed reference to the location of data deposition.
This includes accession numbers and specific URLs allowing direct access to the data deposited (e.g.
(1t-4t) show how the statements exemplified in (14) were tagged.
(1t) <data>The sequence of labA</data> <action>has been deposited </action> <location="general"> in the DDBJ/GenBank/EMBL databases </location> (<location="detail">accession no AB281186 </location>).
(2t) <data>The microarray data</data> <action>were submitted</action> <location="general"> to MIAMExpress at the EMBL-EBI </location> (3t) <data>Histone TAG Arrays</data> are a repurposing of a microarray design originally created to represent <data>the TAG sequences</data> in the Yeast Knockout collection (Yuan et al., 2005 <location="general">NCBI GEO</location> <location="detail">Accession Number GPL1444</location>) (4t) Therefore, <data>the primary sequence of native Acinetobacter CMO</data> is identical to <data>the gene sequence for 3308 [09:55 23/11/2011 Bioinformatics-btr573.tex] Page: 3309 33063312 Extraction of data deposition statements Table 1.
Overview of component occurrences in data deposition statements Component Unique occurrences Total occurrences Variability (%) Data 468 645 73 Action 77 611 13 Location (general) 387 584 66 Location (detailed) 521a 534 98 aWhen accession numbers were unified, the variability lessened considerably with 71 unique occurrences only.
chnB</data> <action>deposited</action> <location="detail">under accession number AB006902</location> Based on this tagging effort, Table 1 shows a summary of component occurrences over the corpus of 586 deposition statements.
Only 16% of sentences contain information that is not included in one of the four tags (7% for full-text sentences, 24% for abstract sentences).
Data is a category with high variability.
While general references to data such as the data reported in this paper (25 occurrences), the microarray data (22 occurrences) and the sequences (20 occurrences) are the most frequent phrases used, they are not prevalent overall.
Action is the category with the least variability.
It is expressed by verbs in most cases.
In other (rare) cases, nominalization expresses the action, e.g.
the deposition/accession number is .
In more than two-thirds of cases, the action is expressed using a passive verb form, or a present verb + adjective, which is a similar construct.
Future tense was used only once in the corpus.
(Note that the variability on actions is slightly skewed due to the selection of MEDLINE abstract sentences with the words deposit or depositionvariability for actions is otherwise 20%.)
General Location is also of high variability, in spite of the fact that there are a limited number of locations referenced, such as GenBank or GEO.
Variation factors are as follows: (i) preposition introducing the location at/from/in/into/through/to/on/via/with/within; (ii) URL used (e.g.
5 variants for GEO); (iii) use of full name and/or abbreviation for institutes (NCBI, EBI) and database (GEO); (iv) typos, spelling errors and other variation (e.g.
database versus data bank).
Detailed Location is a category with relatively low variability if we consider accession numbers as one token type.
Variation factors are as follows: (i) preposition introducing the location through/under/with; (ii) reference to accession number: code/number/no/(super)series; and (iii) list of numbers versus only one number.
In the case of a list, a specific data description may be embedded within the list.
2.2 Automatic identification of deposition components in sentences Based on the analysis above, the identification of the four deposition components defined (data, deposition action, general location and specific location) in deposition statements appeared to be important for extracting specific deposition information.
To provide a complete description of the sentences, any part not covered by the four tags was considered as belonging to a fifth default tag, nil.
In addition, we anticipated that these components might be useful features for the classification of deposition statements.
For this reason, in addition to the 586 data deposition statements tagged, another 697 non-deposition statements were also tagged manually.
The negative sentences tagged here are different from the 578 negative sentences used to train the SVM classifier in order to provide a good balance of sentences that were partly or entirely covered with the nil tag.
These tagged sentences were then used as a training set (that we will call Train-C) for training a conditional random fields (CRFs) model using MALLET.2 2McCallum, A. K. (2002) MALLET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Table 2.
Average precision of SVM and NB models for 5-fold cross-validation with various feature sets Token Position POS Component SVM NB tags tags One-feature set X 95.68 94.95 Two-feature sets X X 95.91 94.96 Two-feature sets X X 97.33 96.11 Two-feature sets X X 97.02 96.75 Three-feature sets X X X 97.40 96.11 Three-feature sets X X X 97.04 96.75 Three-feature sets X X X 97.98 97.23 All four-feature sets X X X X 98.06 97.23 The best performance is shown in bold characters.
2.3 Automatic identification of deposition sentences Using the final sets of 586 positive and 578 negative sentences obtained as described in the previous section (Train-D), we built several machine learning models in order to assess the contribution of the following features to the automatic identification of data deposition statements: Tokens from the sentences (also used in our simple model above) Sentence relative position in article or abstract Part-of-Speech (POS) tags obtained with MEDPOST (Smith et al., 2004) Component tags obtained with CRF model (trained using Train-C) We compared NB and SVM models built using these features.
Table 2 presents the performance (in terms of average precision) of each machine learning method and feature set using 5-fold cross-validation.
2.4 Test corpus We built a test corpus relying on MEDLINE curation of accession numbers.
Specifically, we used the following query to retrieve full-text articles indexed with accession numbers and published in 2010 (we selected 2010 as a publication date to avoid any overlap with our training data): (GenBank[si] OR GEO[si] OR PDB[si] OR OMIM[si] OR RefSeq[si] OR PubChem-Substance[si] OR GDB[si]) AND pubmed pmc local[sb] AND 2010[dp] (N=2,029) These articles were considered as positive for data deposition and were therefore expected to contain a data deposition statement.
Based on the use of the MeSH term Molecular Sequence Data for indexing articles containing references to various types of biological data (as per Chapter 28 of the NLM indexer manual http://www.nlm.nih.gov/mesh/indman/chapter_28.html), we used the following query to retrieve full-text articles containing reference to biological data but no deposition information referenced in MEDLINE: Molecular Sequence Data [mh] NOT (GenBank[si] OR GEO[si] OR PDB[si] OR OMIM[si] OR RefSeq[si] OR PubChem-Substance[si] OR GDB[si]) AND pubmed pmc local[sb] AND 2010 [dp] (N=4,708) These articles were considered as negative for data and were therefore not expected to contain data deposition statements.
All articles (N =6737) were downloaded from PubMed Central in xml format and converted to text format for processing.
A subset of the corpus comprising 700 articles (including 210 articles from the positive set and 490 articles from the negative set reflecting real-data balance) was selected for testing.
The MEDLINE [si] field for the 210 articles selected contained 3309 [09:55 23/11/2011 Bioinformatics-btr573.tex] Page: 3310 33063312 A.Nvol et al.
annotations for GenBank (110 articles), PDB (50 articles), GEO (47 articles), RefSeq (4 articles) and GDB (1 article).3 Sentence-level gold standard: the 700 articles were segmented into sentences that were scored both with the NB and SVM classifiers.
In order to avoid favoring one particular method, for each method, the top-scored sentence was selected for each article, forming two sets of 700 sentences that were manually annotated to determine whether they were data deposition statements.
The set composed of sentences that were top-ranked according to the SVM model was called Test-SS.
The set composed of sentences that were top-ranked according to the NB model was called Test-SB.
Out of the two sets of 700 sentences, 423 sentences were selected by both methods, so that the manual annotation was performed on one whole set of 700 sentences, and completed by annotating the remaining 277 sentences.
The three annotators involved in this task (the authors) were not shown the scores assigned to the sentences by either classifier, and they did not know whether a given sentence came from an article in the positive or negative set.
All three annotators first assessed a common set of 100 sentences (30 from the positive article set and 70 from the negative article set to preserve balance) in order to check the inter-annotator agreement and allow some discussion of potentially ambiguous sentences.
The remaining 600 sentences for this set were divided evenly between the annotators in three subsets that preserved the overall data balance.
Finally, the 277 diverging sentences from the other set were also processed by one annotator.
Article-level gold standard: the 210 articles with an accession number reported in MEDLINE were considered as positive for data deposition in our gold standard.
In addition, based on the manual annotation of sentences carried out for building the two sentence-level test sets, articles corresponding to a sentence annotated as positive for data deposition were also considered as positive in the article-level gold standard.
This allowed us to add to the gold standard 70 articles reporting the deposition of data in repositories that are not currently covered by MEDLINE curation, such as EMBL/EBI databases.
The remaining 420 articles were considered as negative for data deposition in our gold standard.
The dataset comprising gold standard judgments on these 700 articles is now referred to as Test-A.
2.5 Sentence and article classification Classification was performed based on the scoring of article sentences.
At the sentence level, a classification decision was made by comparison of the score assigned to the sentence with a threshold, set at the 25th percentile score for positive sentences in the training set: if the score was above the threshold, the sentence was classified as positive for data deposition.
Otherwise, the sentence was classified as negative.
At the article level, a classification decision was made based on the top-scored sentence.
If the top-scored sentence was classified as positive for data deposition, so was the article.
The performance of sentence classification was assessed using accuracy to allow for indicative comparison with inter-annotator agreement.
Specifically, accuracy was computed as the number of sentences that were correctly classified as positive or negative according to our gold standard over the total number of sentences in the test set (N =700).
We also computed precision, recall and F-measure to allow for a direct comparison with article classification.
The performance of article classification was assessed using precision, recall and F-measure based on positive sentences only, to allow for indicative comparison with related work.
Specifically, precision was computed as the number of articles that were positive in our gold standard and also classified as positive by the algorithm over the total number of articles classified as positive.
Recall was computed as the number of articles that were positive in our gold standard and also classified as positive by the algorithm over the total number of positive articles in the gold standard.
F-measure was then computed as the harmonic mean of precision and recall.
3 One article could be annotated with accession numbers from more than one database.
3 RESULTS 3.1 Sentence-level classification Table 3 shows the performance of selected NB and SVM models for sentence classification on the two sentence test sets (the NB models were applied to Test-SB while the SVM models were applied to Test-SS).
While differences between the models were very small for cross-validation on the training set, some of them are emphasized on the test set, in particular between the different feature sets.
The best overall performance obtained was 80% F-measurewhich corresponds to 87% accuracy.
This accuracy compares favorably to the inter-annotator agreement computed on a subset of 100 sentences that was found to be 95%.
The classification results from the best model comprised 39 sentences misclassified as negative and 56 sentences misclassified as positive.
We performed an error analysis in order to assess the underlying cause of these errors, and manually reviewed all misclassified sentences.
We found that error causes and distribution was similar for NB and SVM.
The breakdown of errors by cause (for SVM) is shown in Table 4.
The major sources of error are top sentences that score low in spite of being deposition sentences and sentences that report data reuse and not data deposition.
The error analysis also brought to attention eight sentences (marked as GS dispute in Table 4) that proved difficult to judge and/or were examples of genuine errors in the gold standard.
These problematic sentences seem to be within the limits of 95% annotation consistency determined on the 100 sentences set.
Table 3.
Overall precision (P), recall (R), F-measure (F) and accuracy (A) of NB and SVM models for sentence classification Model Features P R F A NB Tokens, position, POS tags 60 84 70 75 Above features plus component tags 81 78 79 86 SVM Tokens, position, POS tags 74 81 77 84 Above features plus component tags 78 83 80 87 Threshold is set at the 25th percentile of model scores on the training set Train-D.
The best performance is shown in bold characters.
Table 4.
Error analysis for SVM sentence classification Classification error Error type Cases False negative Low score 34 GS dispute 2 Ambiguous sentence 3 Total 39 False positive Data reuse 32 Database mention 7 Ambiguous sentence 7 GS dispute 6 Non biological data 4 Total 56 3310 [09:55 23/11/2011 Bioinformatics-btr573.tex] Page: 3311 33063312 Extraction of data deposition statements Table 5.
Positive precision (P), recall (R) and F-measure (F) of SVM models for article classification on test set Model Features P R F NB Tokens, position, POS tags 67 82 74 Above features plus component tags 83 78 81 SVM Tokens, position, POS tags 82 75 79 Above features plus component tags 86 76 81 Threshold is set at the 25th percentile of model scores on the training set Train-D.
The best performance is shown in bold characters.
Table 6.
Error analysis for article classification with NB model Error type Cases Low score Ranked in top 5 49 Ranked in top 10 2 Other rank 2 No deposition sentence found in article 6 Sentence not scored (length >500) 2 Total 61 3.2 Article-level classification Table 5 shows the performance of NB and SVM models for article classification on the test set (both models were applied to Test-A, based on their respective results obtained from Test-SB for NB and Test-SS for SVM).As could be expected, the best results are obtained with the models including component tags as features, which also perform best for sentence classification.
The classification results from the best NB model comprised 61 articles misclassified as negative and 44 articles misclassified as positive.
For this part of the study, we focused the error analysis on positive articles that were classified as negative by our system, in order to assess the underlying cause of the errors.
Each case was manually reviewed.
The breakdown by cause is shown in Table 6.
The major cause for article misclassification is a direct result of a sentence classification error: the relevant deposition sentence was assigned a score below the threshold.
Nonetheless, in these cases relevant sentences appear in the top five scored sentences in 49 out of 53 low scoring cases.
Another interesting result from the error analysis is the fact that six articles did not contain a deposition sentence in the full text, and therefore could not be classified properly by our system.
Figure 2 presents a more specific comparison between NB and SVM models built using all feature types.
This figure also allows a comparison between sentence-level and article-level classifiers.
It can be seen that the overall performance on sentence-level classification and article-level classification is similar.
Nonetheless, sentence-level performance is slightly lower than article performance.
This is explained by cases where the gold standard judgments differ at the sentence and article level.
In some cases, the top-scored sentence that was assessed at the sentence level is an ambiguous sentence that may have been classified as negative for data deposition because of the lack of additional context.
In some cases, the top-scored sentence is truly negative for data deposition, but the article contains another sentence that is positive for data deposition.
Fig.2.
Precision/recall curves for SVM and NB models built using all features.
3.3 Overall estimation of data deposition statement prevalence in the biomedical literature To estimate the prevalence of data deposition statements in the biomedical literature, we processed all the PubMed Central articles available to us in full-text XML as of 2 June 2011 (N =827762) and used the NB model (with component tags as features) to classify them according to data deposition status.
In total, 6.4% of articles (N =52932) were found to contain a data deposition statement according to our method.
For the subset of articles that are part of the MEDLINE database (N =774442), we also processed abstracts with our method and found that <0.1% contained a data deposition sentence.
About 4.2% of the PubMed Central articles included in MEDLINE (N =32651) had a curated [si] field.
Most of these PubMed Central articles were also classified as positive for data deposition by our method (N =22428).
This is consistent with the results of our evaluation.
4 DISCUSSION Choice of features: interestingly, the difference in performance with and without component tags observed in the cross-validation was greatly magnified on the test sets both for sentence-level and article-level classification.
We think this is due to the more challenging nature of the test data.
In previous work (e.g.
Kim et al., 2011) on MEDLINE abstract sentence classification, structure information has proved to be a useful feature when it is available.
Our position feature was intended to serve as a substitute for structure information, but had little impact.
Structure information could be considered for future improvement of the sentence classifier; however, this information is not trivial to extract from abstracts (McKnight and Srinivasan, 2003; Ripple et al., 2011); similar issues with added complexity can be anticipated for full text.
Portability of the method: although trained mainly on microarray data deposition statements, the method adapts well to the identification of statements reporting on the deposition of other data such as gene sequences or protein coordinates.
This is evidenced 3311 [09:55 23/11/2011 Bioinformatics-btr573.tex] Page: 3312 33063312 A.Nvol et al.
by the database breakdown of articles in our test corpus according to the MEDLINE [si] field: 110 articles reported data deposition in GenBank, 50 in PDB and only 47 in GEO.
Comparison to other work: while similar to other work mentioned in the related work section, our approach is not directly comparable to any of the previous studies on data deposition.
At the article level, we perform an automatic classification of articles containing data deposition statements, in contrast with Ochsner et al.
who performed a one-time manual classification in order to assess the rate of data deposition in 2008.
Piwowar et al.
assessed machine learning and rule-based algorithms for article classification.
However, their approach focused on five databases and relied on the identification of predetermined database names in the full text of articles.
In contrast, our approach is generic and aiming at the automatic identification of any biological data deposition in any public repository.
Nonetheless, as an indicative comparison, it can be noted that our overall performance for article-level classification is 81% F-measure, compared with overall 69% for Piwowar et al.
(on a different evaluation corpus).
Furthermore, in addition to article classification, our approach also retrieves specific data deposition elements allowing for a finer-grained characterization of both data and deposition location.
At the statement level, this is also different from the classification of databank accession number sentences performed by Kim et al.
(2010) in two ways: first, we are only interested in retrieving sentences containing accession numbers if these sentences are deposition statements (versus comment on the data, or data re-use) and second, we are also interested in retrieving data deposition statements that do not contain accession numbers.
Interest of this study: one general interest of this study is our application of the method proposed by Yeganova et al.
in order to build a training corpus when large amounts of unlabeled data are available.
We showed that the method of Yeganova et al.
could be easily and successfully adapted to our specific classification scenario.
More specific to data deposition statement classification, the method presented in this article can be used as a curation aid in MEDLINE or other databases indexing articles with accession numbers; this tool can also be used to help updating current databases.
For instance, as announced in York (2006) GEO accession numbers have only been indexed in the [si] field of MEDLINE citations from March 2006 onward.
The application of our tool to articles published prior to 2006 could help complete MEDLINE citations with relevant GEO accession numbers.
In future work, we are planning to conduct such large-scale studies in order to identify the growth of data production and data deposition in recent years.
Limitations of the study: our study addressed the identification of data deposition statements in full-text articles.
While the availability of full-text is definitely a limitation, our overall study of the prevalence of data deposition statements (Section 3.3) indicates that data deposition statements are significantly more often found in the full-text of articles, compared with abstract text.
While our method is not limited to data deposition in databases specifically curated in MEDLINE, it is focused on the deposition of biological data as opposed to clinical data as might be recorded in ClinicalTrials.gov, one of the [si] curated databases.
Finally, our classification results are obtained based on a threshold of sentence score, which was empirically established at the 25th percentile of model scores on the training data.
Other methods of determining the threshold could be investigated in future wok.
5 CONCLUSION We presented a method to automatically identify biological data deposition statements in biomedical text.
The method, an SVM (or, equivalently, NB) classifier using CRF-determined features characterizing data deposition components, was able to correctly identify articles containing data deposition statements with 81% F-measure.
Our analysis shows that deposition statements are scored high for all types of databases and biological data types, even those not currently curated in MEDLINE.
This shows the potential impact of our method for literature curation.
In addition, we believe it provides a robust tool for future work assessing the need for storage space of incoming data in public repositories.
ACKNOWLEDGEMENTS The authors would like to thank Dr Won Kim for his assistance in implementing the NB and SVM classification tools.
Funding: Intramural Research Program of the National Institutes of Health, National Library of Medicine.
Grant number LM000002-01.
Conflict of interest: None declared.
ABSTRACT Motivation: Ever increasing amounts of biological interaction data are being accumulated worldwide, but they are currently not readily accessible to the biologist at a single site.
New techniques are required for retrieving, sharing and presenting data spread over the Internet.
Results: We introduce the DASMI system for the dynamic exchange, annotation and assessment of molecular interaction data.
DASMI is based on the widely used Distributed Annotation System (DAS) and consists of a data exchange specification, web servers for providing the interaction data and clients for data integration and visualization.
The decentralized architecture of DASMI affords the online retrieval of the most recent data from distributed sources and databases.
DASMI can also be extended easily by adding new data sources and clients.
We describe all DASMI components and demonstrate their use for protein and domain interactions.
Availability: The DASMI tools are available at http://www.dasmi.de/ and http://ipfam.sanger.ac.uk/graph.
The DAS registry and the DAS 1.53E specification is found at http://www.dasregistry.org/.
Contact: mario.albrecht@mpi-inf.mpg.de Supplementary information: Supplementary data and all figures in color are available at Bioinformatics online.
1 INTRODUCTION Molecular interactions are of fundamental importance to many biological processes (BPs).
In recent years, the amount of interaction data has increased substantially due to growing attention by the scientific community as well as the widespread use of high-throughput techniques that afford screening of vast numbers of molecules.
Nowadays, large-scale protein interaction maps are available for model organisms like yeast, fly and worm (Goll and Uetz, 2007), and the current research focus is shifting towards interaction screens for human (Cusick et al., 2005; Stelzl and Wanker, 2006).
In addition, computational methods have been developed for predicting molecular interactions, some of To whom correspondence should be addressed.
which reach prediction quality comparable to that of experimental high-throughput data (Ramrez et al., 2007; Shoemaker and Panchenko, 2007).
However, this rapid accumulation of interaction data makes it difficult for scientists to keep track of all available information and data sources.
The unification of heterogeneous and decentralized interaction data is thus a prerequisite for an effective study of interactomes (Brazma et al., 2006).
Molecular interactions can be studied at different levels of detail (Fig.1).
In general, physical and non-physical interaction types can be distinguished.
While a physical interaction implies a real contact between the interacting molecules (interactors), the other interaction type denotes a purely functional association between them.
For instance, such associations can be based on similar genomic contexts, coexpression analyses or literature relationships (Jensen et al., 2006).
Physical interactions between proteins may involve two and more proteins, forming binary interactions and protein complexes (Frishman et al., 2009).
In particular, protein protein interactions are formed by the physical contact of binding sites, which are frequently evolutionarily conserved in domains of protein families (Finn et al., 2008).
Further protein interactions exist with other ligands, for instance, nucleic acids, lipids and certain small molecules in signaling or metabolic pathways.
Techniques like X-ray crystallography, NMR spectroscopy or 3D structure modeling can provide even more molecular details by identifying the interacting atoms or residues in the protein binding sites (Aloy and Russell, 2006; Finn et al., 2005).
A number of databases keep track of experimentally determined protein interactions (Bader et al., 2003; Breitkreutz et al., 2008; Chatr-Aryamontri et al., 2007; Gldener et al., 2006; Kerrien et al., 2007a; Keshava Prasad et al., 2009; Salwinski et al., 2004).
Such databases are essential components of interactomics, however, each of them contains information not found in other databases (Mathivanan et al., 2006).
The IMEx consortium formed by eight major interaction data providers aims at overcoming the fragmentation by sharing the curation effort and exchanging curated protein interaction records among its members (Orchard et al., 2007).
However, IMEx and its member databases are restricted to experimentally determined protein interactions, which cover only a fraction of the estimated interactomes (Stumpf et al., 2008).
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:16 24/4/2009 Bioinformatics-btp142.tex] Page: 1322 13211328 H.Blankenburg et al.
Fig.1.
Levels of molecular interactions.
Physical and non-physical interaction types can be distinguished: (a) Non-physical interactions are based on functional associations.
(b) Physical interactions imply a direct physical contact between the interacting molecules like proteins, domains or small ligands.
(c) Raising the level of detail to 3D structural data, the interacting atoms or residues of binding sites can be identified.
Voluminous data on predicted protein and domain interactions that have been made publicly available by different research groups (Schelhorn et al., 2008; Schlicker et al., 2007; Shoemaker and Panchenko, 2007) are not included.
To provide broader data access to currently available inter-actomes, several integration frameworks have appeared recently.
We will refer to the methodology underlying these frameworks as static data integration because either the user is assisted in building a local data warehouse (Aragues et al., 2006; Lee et al., 2006b; Shah et al., 2005; Shannon et al., 2006) or the software facilitates access to central data repositories via web interfaces (Birkland and Yona, 2006; Breitkreutz et al., 2008; Cerami et al., 2006; Chaurasia et al., 2009; Goll et al., 2008; Hoffmann and Valencia, 2004; Jensen et al., 2009; Pagel et al., 2008; Prieto and Rivas, 2006; Raghavachari et al., 2008; Tarcea et al., 2009) or by software plugins, for instance, as available for Cytoscape (Avila-Campillo et al., 2007; Cerami et al., 2006; Hernandez-Toro et al., 2007; Shannon et al., 2003; Tarcea et al., 2009).
However, static integration has the drawback of providing only a snapshot of a fixed number of data sources at a certain point of time.
Once the data have been included into the central repository, curation efforts are required to keep them up to date and in sync with the original data source.
This permanent update problem can be aggravated by possible format changes of the source, which hampers further data processing.
Apart from that, these integration frameworks are rather rigid because the inclusion of additional datasets like new experimental data or the results of a novel prediction method can normally be accomplished solely by the central authority.
A data fragmentation situation similar to the current diversity of interaction data arose with genomic data several years ago.
One possible solution to the integration of genomic data and their annotations was the Distributed Annotation System (DAS) (Dowell et al., 2001).
In general, it is anticipated that decentralization will become an important data-sharing concept in the future (Murray-Rust, 2008; Stein, 2008; Thorisson et al., 2009).
DAS is based on a clientserver architecture in which numerous decentralized servers offer annotations of a reference entity provided by another server.
The combination and visualization of a reference entity and its annotations is performed in a DAS client.
We aim to overcome the shortcomings of the aforementioned static data integration frameworks by adopting and extending a DAS-based approach for the exchange of molecular interaction data and their annotations.
Instead of unifying all available interaction data into a central database, the interaction data remain with their original providers and are retrieved and integrated online on request.
This eliminates the issue of centralized data maintenance and ensures that the interaction data are always kept up to date.
Our system, named DAS for Molecular Interactions (DASMI), is sufficiently generic to support all types of interaction data described above.
It is not restricted to protein interactions and considers both experimentally determined and predicted interactions.
This is the main distinguishing feature from interaction repositories like HPRD (Keshava Prasad et al., 2009) or IntAct (Kerrien et al., 2007a).
Instead of competing with them, we want to supplement their data with additional sources and help the user to assess the available information.
In the following section, we will introduce the distributed architecture of DASMI and describe its components: the specification of a DAS extension for the exchange of interaction data and their annotations as well as the software libraries that implement the new specification in servers and clients.
Finally, we will demonstrate the exemplary use of DASMI for protein and domain interactions.
2 METHODS 2.1 Distributed architecture DAS (Dowell et al., 2001) is a data integration approach with the main goal of replacing central data repositories with distributed storage systems.
DAS is built on a clientserver architecture, consisting of two types of servers, namely, reference and annotation servers, and a client for visualization purposes.
Reference servers provide the biological reference entity, for example, a nucleotide or peptide sequence.
Annotation servers make additional information available that is related to the reference sequence, for instance, information on exons or protein domains.
Coordinate systems are used to define the entities that a DAS server provides or annotates, for example, chromosomes, genes, protein sequences or protein structures (Prlic et al., 2007).
A data exchange specification handles the communication between DAS clients and DAS servers by prevalent techniques, namely HTTP URL requests and XML responses.
Originally, DAS was designed for the exchange of annotations of DNA sequences.
In recent years, several extensions to the protocol have widened its use to other areas (Jenkinson et al., 2008): Protein DAS affords the exchange of protein sequence annotations and alignments (Jones et al., 2005), 3D-DAS utilizes DAS for the annotation of protein structure alignments (Prlic et al., 2005) and 3D-EM DAS for electron microscopy (Macas et al., 2007).
In addition to these DAS extensions, a registry has been developed that maintains a list of available DAS servers and thus allows DAS clients finding suitable servers (http://www.dasregistry.org/).
1322 [20:16 24/4/2009 Bioinformatics-btp142.tex] Page: 1323 13211328 DASMI Fig.2.
Schematic DASMI system architecture.
The DASMI architecture is similar to the original DAS architecture (Dowell et al., 2001).
Interaction servers provide interactions (Interaction Server 1) and, optionally, additional information like experimental conditions (Interaction Server 2).
Confidence servers provide confidence scores for interactions (Confidence Server 1).
DASMI clients query interaction servers and combine their results.
The DAS registry maintains a list of available DAS servers.
DASMI aims at resolving the problems of current integration frameworks for interaction data by transferring the idea of DAS into the field of molecular interactions.
This includes the specification of a DAS extension defining the data exchange between servers and clients as well as reference implementations of servers and clients (Fig.2).
As in the original DAS architecture, there are different server types in the DASMI framework.
The majority of servers provide interaction data and optionally additional information; these are the equivalents of DAS reference servers.
Examples of additional information, which can be associated specifically with an interaction, include the known or predicted interaction regions, the strength and type of the interaction, or the conditions in which the interaction occurs.
Confidence servers are comparable with DAS annotation servers and provide reliability scores for potential interactions.
Notably, the HUPO-PSI community wants to utilize our distributed scoring architecture for a common confidence scoring system for proteinprotein interactions (Orchard et al., 2008).
Each interaction or confidence server belongs to a certain coordinate or identifier system, which specifies how interactions can be requested and how they are returned.
For instance, a data source with the Entrez Gene identifier system may be queried for interactions by using gene identifiers, another data source with the UniProtKB identifier system by using protein identifiers.
DASMI clients thus need to transform the results of servers from different identifier systems in order to unify them.
2.2 DASMI data exchange specification Data exchange between interaction servers and clients requires a DAS URL and XML specification.
An advantage of a well-defined data exchange specification is the resulting modularity and extensibility of the system.
New servers and clients can be readily incorporated if they follow the specification and thus communicate with the existing parts in a well-defined manner.
We extend the DAS specification by the new interaction command and the associated DASINT XML response format.
This extension is part of the DAS 1.53E specification (Jenkinson et al., 2008).
Figure 3 shows an interaction request and the associated DASINT response for an exemplary proteinprotein interaction.
Requests to a DASMI server are issued in the same form of a formatted URL request as those to a standard DAS server (Fig.3a).
The new command for requesting interactions is interaction and offers additional query parameters of three types: interactor, operation and detail.
Please refer to Supplementary Data File 1 for the full data exchange specification.
The response of a DASMI server to an interaction request is a DASINT XML document (Fig.3b).
In contrast to the widely adopted PSI-MI XML2.5 format (Kerrien et al., 2007b), which provides an extensive specification with numerous elements and a deeply branched hierarchy, DASINT uses a concise and flexible document format.
PSI-MI XML2.5 and DASINT can thus be regarded as complementary approaches: whilst PSI-MI XML2.5 has the goal of describing experimentally determined interactions in detail, naturally resulting in very complex documents, DASINT provides a lightweight intermediate exchange format, which facilitates fast communication between clients and servers.
In this regard, DASINT is comparable with MITAB2.5 (Kerrien et al., 2007b), the simplified tabular version of PSI-MI XML2.5.
However, DASINT is more versatile because it supports, for example, the representation of protein complexes without the need of transforming the data into a spoke or matrix model.
A more detailed differentiation of DASINT from alternative data exchange formats can be found in Supplementary Data File 1.
Figure 3c shows an illustration of the DASINT XML Schema Definition.
The complete definition of the proposed DASINT XML format can be found in Supplementary Data File 2.
1323 [20:16 24/4/2009 Bioinformatics-btp142.tex] Page: 1324 13211328 H.Blankenburg et al.
Fig.3.
Exemplary interaction request and DASINT response.
The communication between DASMI server and client is performed using formatted URL requests and XML responses.
(a) Interactions are requested using the interaction command.
In the example shown here, all protein interactions involving the proteins P09497 and O60828 and annotated with a BPscore would be retrieved.
(b) The server response is a DASINT XML format.
(c) Overview of the DASINT XML Schema Definition.
Mandatory elements are marked using solid frames, optional elements have dashed frames.
2.3 DASMI server A DASMI server responds to an interaction request by providing interaction data in the DASINT XML format defined above.
One of the objectives of DAS and thus of DASMI is to make the setup of DAS servers easy.
To achieve this aim, three versatile open-source DAS server libraries are available, Dazzle (http://www.biojava.org/wiki/Dazzle) and MyDas (http://code.google.com/p/mydas/) for Java, and ProServer (Finn et al., 2007) for Perl (http://www.sanger.ac.uk/Software/analysis/proserver/).
We provide DASMI reference server implementations by extending Dazzle and ProServer, while MyDAS is being extended by the DAS community.
The stand-alone servers Dazzle and ProServer both work in a modular fashion.
They consist of a server core, which provides basic functionalities like handling requests and responses, and components, which manage specific DAS commands and data storage formats.
Existing DataSource classes (Dazzle) and Transport modules (ProServer) act as brokers between the underlying interaction data and the modules that build the DASINT XML response.
This simplifies access to a range of data storage formats, for instance, PSI-MI XML2.5 documents (Kerrien et al., 2007a) or flat files in the Simple Interaction Format (SIF) defined for Cytoscape (Cline et al., 2007; Shannon et al., 2003).
In order to set up a new server, data providers need only to use an existing or implement a new module that is tailored to the specifics of their molecular interaction data.
2.4 Interaction datasets DASMI has been developed to support molecular interactions at different levels (Fig.1).
To demonstrate its use, we set up DASMI servers for a collection of proteinprotein interaction datasets: two large-scale experimental datasets [CCSB-HI1 (Rual et al., 2005) and MDC (Stelzl et al., 2005)], four curated experimental datasets [DIP (Salwinski et al., 2004), HPRD (Keshava Prasad et al., 2009), IntAct (Kerrien et al., 2007a) and MINT (Chatr-Aryamontri et al., 2007)] and six predicted datasets [Bioverse (McDermott et al., 2005), HiMAP (Rhodes et al., 2005), HomoMINT (Persico et al., 2005) OPHID (Brown and Jurisica, 2005), POINT (Huang et al., 2004) and Sanger (Lehner and Fraser, 2004)].
More information on these datasets is found in Ramrez et al.
(2007).
In addition, several domaindomain interaction datasets are offered by DASMI servers: three experimental datasets derived from 3D structures obtained by X-ray crystallography or NMR spectroscopy [3did (Stein et al., 2009), iPfam (Finn et al., 2005) and PiNS (Bordner and Gorin, 2008)] and 11 predicted datasets (Chen and Liu, 2005; Guimares et al., 2006; Jothi et al., 2006; Lee et al., 2006a; Liu et al., 2005; Ng et al., 2003; Pagel et al., 2008; Riley et al., 2005; Schelhorn et al., 2008; Wang et al., 2007; Wuchty, 2006), see Supplementary Data File 3.
Moreover, we set up two confidence servers, FunSimMat (Schlicker and Albrecht, 2008; Schlicker et al., 2006) and Domain support (Finn et al., 2005; Ramrez et al., 2007), which can be used to assess the reliability of protein interactions.
Of course, our current selection of data sources, with the majority of them temporarily maintained at our institute, serves only as a prototype for the capabilities of our system because it necessitates the replication of some interaction datasets, resulting in the same update problem the aforementioned central repositories are facing.
However, for the near future, we already know from other scientists that new sources for interactions and confidence measures will be made available at other institutions.
2.5 DASMI client A DASMI client offers the user an easy way of communicating with various DASMI servers without having to know any data exchange specification details.
Subsequent to a user request, a DASMI client will contact all DASMI servers, retrieve and unify the interaction data and present the results to the user.
A list of all publicly available DASMI servers is provided by the DAS registry (Prlic et al., 2007).
To facilitate the development of new DASMI clients, the two existing open-source DAS client libraries, Dasobert (http://www.spice-3d.org/dasobert/) in Java and Bio-Das-Lite (http://search.cpan.org/dist/Bio-Das-Lite/) in Perl, have been upgraded to support our interaction extension.
2.5.1 Identifier mapping Proteomics affords a substantial diversity of object identifiers, ranging from RefSeq and Entrez Gene identifiers to UniProtKB accession numbers.
Accordingly, protein interaction datasets use different identifier systems to describe their interactions.
In order to unify them, a DASMI client has to convert between various systems to incorporate servers that have different identifier systems.
This mapping procedure can produce considerable computational overhead.
For instance, while there is usually a one-to-one mapping from UniProtKB to Entrez Gene identifiers, mapping in the opposite direction may produce multiple results as one gene can be responsible for several protein variants or fragments.
Therefore, a DASMI client might need to issue multiple queries to retrieve all protein protein interactions for one gene.
Furthermore, the mapping procedure implemented by a DASMI client determines if it is able to distinguish splice variants.
In contrast, the identifier diversity for domain interaction datasets is less problematic as stable Pfam identifiers (Finn et al., 2008) are predominantly used.
3 RESULTS On the basis of the client libraries Dasobert and Bio-Das-Lite, two DASMI clients have been developed to illustrate the potential of our new system: the DASMIweb client as an entry gate to various proteinprotein and domaindomain interaction datasets and the iPfam graphical domain interaction browser that uses DASMI to incorporate predicted domaindomain interactions into its results.
1324 [20:16 24/4/2009 Bioinformatics-btp142.tex] Page: 1325 13211328 DASMI 3.1 DASMIweb for protein and domain interactions The DASMI client DASMIweb is publicly accessible at http://www.dasmi.de/web/.
The aim of DASMIweb is to establish a starting point for interactome studies by consolidating protein and domain interaction data from various sources.
3.1.1 User interface The DASMIweb user interface is designed to be clear and intuitive (Fig.4a).
The screen window is divided into several panels; permanent panels are the Query Panel in the top left corner of the window and the Information Panel in the top right corner.
Interactions are presented within the Interaction Panel, located in the central part of the window.
The configuration of DASMIweb can be managed in the optional Source Configuration Panel.
The DASMIweb user interface relies heavily on the use of Asynchronous JavaScript and XML (AJAX) (Jimenez et al., 2008; Sagotsky et al., 2008).
This technique is required to present interactions to the user as soon as they are received from a DASMI server.
The asynchronous communication is provided by the Java framework Direct Web Remoting (DWR, http://getahead.org/dwr/).
3.1.2 Querying To make querying DASMIweb as intuitive as possible, the Query Panel contains only a single search field.
There is no need for the user to specify the type of the query.
The system will use internal identifier mapping tables derived from iProClass (Huang et al., 2003) and Pfam (Finn et al., 2008) to automatically determine whether the input is a gene, protein or domain identifier.
If the identifier cannot be mapped unambiguously, the user is asked to refine the query.
Furthermore, DASMIweb will map the query to all compatible identifier systems in order to maximize the number of data sources that can be used to answer the user query.
For instance, if the user searches by an Entrez Gene identifier, DASMIweb will not only query all data sources in the Entrez Gene identifier system, but will also try to convert the identifier to UniProtKB, GeneInfo, RefSeq and Ensembl identifiers to cover all data sources in the respective identifier systems.
If a mapping results in multiple identifiers, for instance, when mapping from gene to protein identifiers, all combinations of identifiers are used.
A more detailed description of the mapping procedure and exemplary mappings can be found in the online documentation at3.1.3 Presentation of results Interactions are presented to the user in tabular form within the Interaction Panel.
In the central table, columns represent data sources that have been contacted for the user query, rows correspond to different interactions, and squares in the intersections of rows and columns indicate particular interactions reported by a specific source (Fig.4a).
If an interaction is binary, the row contains the interaction partner of the query interactor, but if the interaction is complex, all interaction partners are presented in a single row that is highlighted.
This tabular representation affords an intuitive, visual judgment of the results since an interaction reported by multiple sources, as shown by several squares in the same row, may be more likely to be accurate.
A more detailed assessment of the interactions can be performed by applying confidence measures as described below.
The user can also request further information about an interaction, such as experimental details and confidence scores, by clicking on the associated interaction square.
These annotation details are an Fig.4.
DASMI clients DASMIweb and iPfam.
(a) DASMIweb is an online tool for dynamically unifying protein and domain interaction data and additional annotations.
The results are presented in tabular form: each column represents a DASMI server, each row contains an interaction partner, and squares at the intersections of rows and columns indicate interactions.
In this figure, the interaction squares are colored according to the functional similarity between the interacting proteins, from dark blue for high to white for low similarity.
(b) The iPfam client tool combines domaindomain interactions from several sources with interactions reported in the iPfam database.
The results are presented in graphical form; protein domains are depicted as ovals and interactions as edges that connect ovals.
Different edge colors distinguish individual data sources.
optional feature provided by the individual data sources.
Therefore, they might not be available for all interactions.
3.1.4 Source configuration and data export In its initial configuration, DASMIweb incorporates all available data sources that are compatible with a user query.
The user can change this selection in the Source Configuration Panel.
This panel lists all known data sources that are registered in the DAS registry with their identifier systems.
The user can integrate additional data sources that are not contained in the DAS registry by using the Add new source tab of the Source Configuration.
After providing all 1325 [20:16 24/4/2009 Bioinformatics-btp142.tex] Page: 1326 13211328 H.Blankenburg et al.
required information like the name, URL and identifier system of the new source, it will be included in all future queries.
Another way of adding new interaction data is by creating a DASMI server from an existing PSI-MI XML2.5 file.
After uploading the file into DASMIweb, the interactions will be made temporarily available as a new DASMI server.
This procedure enables users to compare their own interactions with existing datasets or to assess them by confidence servers.
In order to analyze the results with external applications such as the network visualization software Cytoscape (Shannon et al., 2003), the user can export the results from the web client into file formats like the PSI-MI tabular format MITAB2.5 (Kerrien et al., 2007b) or the SIF (Cline et al., 2007).
3.1.5 Quality assessment Current protein interaction networks are still incomplete to a large extent and are prone to bias and errors (Ramrez et al., 2007).
To address this problem, DASMIweb offers useful options to assess the reliability of individual interactions.
The following datasets of confidence scores can be requested and selected in the header of the Interaction Panel and are applied to color the interaction squares: FunSimMat: Interaction partners frequently share similar functions.
FunSimMat provides scores that measure the functional similarity of both partners (Schlicker and Albrecht, 2008; Schlicker et al., 2006).
The BPscore is based on the BP annotation in the Gene Ontology, the CCscore on the cellular component (CC) annotation and the MFscore on the molecular function (MF) annotation.
Domain support: Some proteinprotein interactions may be traced to the underlying domaindomain interactions.
Domain support offers two subsets: domain interactions that have been derived from crystal structure analyses and domain interactions that have been computationally predicted by different methods (see Supplementary Data File 3).
In addition, the user can display the original confidence scores that are contained in the source datasets.
3.2 iPfam graphical domain interaction browser iPfam (Finn et al., 2005) is a database of Pfam domain interactions derived from proteins with an experimentally determined 3D structure.
Integrating information about domain interactions from various sources enables one to address several questions.
For instance, datasets generated using different methods can be compared and structurally known domain interactions provided by iPfam or 3did (Stein et al., 2009) can be used to verify predicted domain interactions.
To this end, the iPfam database has developed a client tool that graphically integrates domain interaction information from one or more DASMI servers (http://ipfam.sanger.ac.uk/graph) including an own server for structural interactions.
For a user-selected domain, the iPfam tool retrieves data about interacting domains and represents them as a graph.
Each domain is depicted as an oval node within the graph, and interactions are represented by graph edges.
Different colors are used to distinguish interactions from individual data sources.
Clicking on a domain will center the graph on the interactions for that domain, which supports the visual exploration of the domain interaction network.
3.3 Comparison with existing interaction repositories DASMI clients may be compared with interaction databases like HPRD (Keshava Prasad et al., 2009) or IntAct (Kerrien et al., 2007a).
However, DASMI does not want to compete with such databases, but intends to complement their results with interaction data from other datasets.
For example, the results of computational predictions as available in Bioverse (McDermott et al., 2005) are not included into databases of experimental protein interactions, though they might give scientists new insights into the function of proteins (Sharan et al., 2007).
Providing more interaction datasets is not only a goal of DASMI, but also the motivation for composite databases like MiMI (Tarcea et al., 2009) for proteinprotein interaction or DOMINE for domain domain interactions (Raghavachari et al., 2008).
In contrast to DASMI, these composite databases combine several datasets into a central repository, which renders it difficult to ensure that the interaction data they provide is kept in sync with the original sources.
IntAct, for instance, has a daily release cycle, implying daily update processes of the composite databases.
DASMI avoids this problem by leaving the interaction data with its original providers.
In addition, DASMI fosters the inclusion of novel interaction data and interaction confidence scoring methods.
There is no central authority that decides which data resources are included and which are not.
By setting up a new DASMI server and registering it at the DAS registry, the interaction data or confidence scoring routine will automatically be available to all users (Fig.2).
Moreover, the setup of an own DASMI server without publishing the server address allows for integrating confidential data into other DASMI clients.
4 DISCUSSION AND CONCLUSION We have introduced DASMI, a new framework for the dynamic exchange and integration of different types of interaction data.
The DAS protocol extension DASMI is based on a clientserver architecture and comprises three main components: data exchange specification, interaction servers and integration clients.
Open source server and client libraries are available for the programming languages Java and Perl.
Due to its distributed architecture, DASMI is easily extensible, for instance, by including new servers or developing additional clients.
By avoiding a central interaction data repository, DASMI bypasses the problem of update cycles that static integration frameworks face.
As a prototypic application, we set up several DASMI servers and developed web clients for the exchange of protein and domain interactions.
The client DASMIweb dynamically gathers interactions from various servers and integrates their results into a unified view.
In addition, the reliability of interactions can be assessed by confidence measures.
Furthermore, DASMI is used by an iPfam client to integrate predicted domaindomain interactions into iPfam results.
The development of DASMI will be continued and further extensions will be included.
Future plans include more DASMI sources for interaction datasets and confidence measures by external providers.
Additional proxies will allow incorporating servers into the DASMI system that do not use DASINT but PSI-MI XML2.5 or other XML formats.
The DASMI clients DASMIweb and iPfam will be equipped with new features like a graphical network 1326 [20:16 24/4/2009 Bioinformatics-btp142.tex] Page: 1327 13211328 DASMI representation for DASMIweb.
Additionally, new DASMI clients like a Cytoscape plugin are under development.
ACKNOWLEDGEMENTS The work was conducted in the context of the DFG-funded Cluster of Excellence for Multimodal Computing and Interaction and the EC-funded BioSapiens Network of Excellence.
Funding: German National Genome Research Network (NGFN); German Research Foundation (DFG contract number KFO 129/1-2); European Commission (LSHG-CT-2003-503265).
Conflict of Interest: none declared.
ABSTRACT Motivation: Structural variants, including duplications, insertions, deletions and inversions of large blocks of DNA sequence, are an important contributor to human genome variation.
Measuring structural variants in a genome sequence is typically more challenging than measuring single nucleotide changes.
Current approaches for structural variant identification, including paired-end DNA sequencing/mapping and array comparative genomic hybridization (aCGH), do not identify the boundaries of variants precisely.
Consequently, most reported human structural variants are poorly defined and not readily compared across different studies and measurement techniques.
Results: We introduce Geometric Analysis of Structural Variants (GASV), a geometric approach for identification, classification and comparison of structural variants.
This approach represents the uncertainty in measurement of a structural variant as a polygon in the plane, and identifies measurements supporting the same variant by computing intersections of polygons.
We derive a computational geometry algorithm to efficiently identify all such intersections.
We apply GASV to sequencing data from nine individual human genomes and several cancer genomes.
We obtain better localization of the boundaries of structural variants, distinguish genetic from putative somatic structural variants in cancer genomes, and integrate aCGH and paired-end sequencing measurements of structural variants.
This work presents the first general framework for comparing structural variants across multiple samples and measurement techniques, and will be useful for studies of both genetic structural variants and somatic rearrangements in cancer.
Availability: http://cs.brown.edu/people/braphael/software.html Contact: braphael@brown.edu 1 INTRODUCTION Characterizing the DNA sequence differences that distinguish individuals is a major challenge in human genetics.
Until recently, these differences were thought to be mostly single nucleotide changes.
There is increasing appreciation for the prevalence of structural variation, including duplications, deletions and inversions of large blocks of DNA sequence, in the human genome (Sharp et al., 2006).
Structural variants have recently been linked to diseases such as autism (Marshall et al., 2008), and cataloging these variants is an important step in determining the genetic basis of disease.
To whom correspondence should be addressed.
Structural variants typically span thousands of nucleotides and are more difficult to define than single nucleotide polymorphisms (SNPs).
Two techniques have been used to identify structural variants in the human genome: array comparative genomic hybridization (aCGH) and end sequence profiling (ESP), also called paired-end mapping.
Both of these techniques were first developed for the analysis of somatic structural rearrangements in cancer genomes (Pinkel and Albertson, 2005; Pinkel et al., 1998; Volik et al., 2003) and later applied to discover structural variation in normal genomes (Iafrate et al., 2004; Kidd et al., 2008; Korbel et al., 2007; Sebat et al., 2004; Tuzun et al., 2005).
In aCGH, differentially fluorescently labeled DNA from test and reference genomes are hybridized to an array of genomic probes derived from the reference genome.
Measurements of test:reference fluorescence ratio at each probe identifies locations of the test genome that are present in higher or lower copy in the reference genome.
This technique detects copy number variants but is blind to copy neutral variants such as inversions.
In paired-end mapping approaches, DNA fragments, or clones, from a test genome are sequenced from both ends, and these sequences are mapped to a reference genome sequence.
Pairs of end sequences, called end sequence pairs or mate pairs, with discordant mappings identify inversions, translocations, transpositions, duplications and deletions that distinguish the test genome from the reference genome.
Next-generation DNA sequencing technologies such as those from Illumina, Applied Biosystems and 454 Life Sciences now make it possible to apply this approach to a large number of individuals.
We distinguish paired-end mapping from whole-genome assembly, which would provide the ultimate dataset for studies of structural variation (Levy et al., 2007; Wheeler et al., 2008), but remains cost prohibitive for a large number of human genomes.
Both aCGH and paired-end mapping do not precisely identify the boundaries, or breakpoints, of the measured variant.
In aCGH, a breakpoint is localized only to the distance between the genomic probes straddling the copy number change, while in paired-end mapping the localization depends on the number and size of fragments that span the variant (Fig.1A).
There are no standard methods for identifying the boundaries of structural variants in paired-end mapping studies and different heuristics have been used (Kidd et al., 2008; Korbel et al., 2007).
Remarkably, despite ambiguity resulting from both measurement and analysis, published studies of structural variants report the breakpoints to single nucleotide resolution without revealing the measurement uncertainty, or error bars, in the localization of the variant.
Consequently, the existing databases of human structural variants 2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i223 i222i230 Geometric structural variation z Fig.1.
Derivation of regions of uncertainty (breakpoint regions) from ESP and aCGH data.
(A) (Top panel) In ESP, or paired-end mapping, both ends of a fragment of a test genome are sequenced and aligned to the reference genome.
Here, alignment of ends of fragments C and D yields ES pairs (xC ,yC ) and (xD,yD) on the reference genome that suggest an inversion.
(Bottom panel) The intersection of breakpoint regions defined by Equation (1) indicates the possible locations of inversion breakpoints a and b that are consistent with the ES pairs.
(B) (Top panel) In an aCGH experiment, the reference genome is segmented into regions of equal copy number according to measurements at genomic probes (boxes).
A deletion with breakpoints a and b is identified as a change in copy number between probes pi and pi+1 and between probes pj and pj+1.
(Bottom panel) The intervals [pi,pi+1] and [ pj,pj+1 ] define a rectangular breakpoint region.
This region is intersected with the breakpoint region defined by an ES pair (xC ,yC ) to refine the locations a and b of the deletion.
(Iafrate et al., 2004) also do not contain information on the uncertainty of the breakpoints.
Each new study of structural variation compares the newly discovered variants to those previously reported.
In addition to the ambiguities described above, there is also the problem of deciding when two variants (perhaps measured via different approaches) are the same.
The usual approach is to define two variants to be the same if they are near each other, where near is defined using an arbitrary and study-dependent threshold.
With such an approach, there is no assurance that the two variants are indeed the same, or are merely closely located on the genome.
The situation is further exacerbated by reports that different human structural variants may overlap or have multiple states (Perry et al., 2008; Scherer et al., 2007), and that recurrent (but not identical) variants may exist at the same locus.
Standard methods for defining structural variants and publicly available tools for comparing structural variants across different studies are urgently needed.
Two recent works introduced more refined approaches for analysis of structural variants and are promising steps in this direction.
Lee et al.
(2008) describe a probabilistic method for resolving ambiguities in mapping end-sequenced fragments using the distribution of fragment lengths in a single sample.
Bashir et al.
(2008) estimate the probability that paired-end sequenced clones from cancer genomes contain fusion genes and explicitly incorporate the uncertainty in measurement of rearrangement breakpoint into their calculation.
Neither of these approaches address the comparison of variants across multiple samples, and are further limited in their handling of measurement uncertainty and consideration of all classes of structural variants, respectively.
Here, we introduce a general geometric framework for classification and comparison of structural variants.
Our approach provides a principled way to cluster multiple measurements of a variant in a single sample and to compare variants across samples.
We explicitly model the underlying measurement uncertainty of both paired-end mapping (from both older and next-generation sequencing technologies) and aCGH.
We represent the uncertainty in the measurement of a structural variant, which we refer to as the breakpoint region, as a polygon in the plane.
We formulate the problems of comparing variants as computing all intersections and maximal intersections of breakpoint regions.
These formulations allow the user to examine conserved variants at varying levels of granularity, instead of only producing a single best cluster of overlapping variants.
We derive an efficient plane sweep algorithm from computational geometry to compute these intersections.
We demonstrate our Geometric Analysis of Structural Variants (GASV) program with three applications.
First, we apply our method to recent paired-end sequencing studies of nine human individuals.
We show that GASV identifies rearrangement breakpoints with high precision.
In dozens of cases, we localize rearrangement breakpoints to <2.5 kb by combining the measurements from 40 kb clones across multiple individuals.
In the most extreme example, eight end-sequenced clones from four different individuals localize the inversion breakpoints to within 286 bp.
Such precise localization was not reported in the original published analysis of these nine individuals.
Moreover, we show that the published locations of many variants are different from the breakpoints supported by the data.
Second, we perform a comparative analysis of variants from the nine normal individuals with variants identified in paired-end sequencing of several cancer samples.
We find that a significant fraction (553%) of rearrangements identified in the cancer genomes are consistent with inversion and deletion variants found in the normal genomes.
Finally, we show how GASV integrates both aCGH and paired-end sequencing measurements of variants in three cancer genomes.
Our geometric method for multi-sample and multi-platform identification and comparison of structural variants should prove useful for studies of human structural variation such as the 1000 Genomes Project and for cancer genome sequencing studies such as The Cancer Genome Atlas.
2 METHODS Consider a reference genome represented as a single interval G (i.e.
we concatenate multiple chromosomes) and a closely related test genome.
We define a structural variant to be a difference between a test genome i223 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i224 i222i230 S.Sindi et al.
and reference genome that is due to a rearrangement resulting from DNA breakage followed by a aberrant repair or insertion of a new DNA.
Structural variants include inversions, translocations, transpositions, and insertions/deletions.
Each of these variants is thus associated with a set of breakpoints where DNA breaks and/or repair occurs.
For example, an inversion is a result of the reference genome being cut at two genomic coordinates, a and b, and the DNA segment between a and b flipped in the test genome so that the nucleotide at position a1 is adjacent to the nucleotide at position b and a is adjacent to b+1 (Fig.1A).
Similarly, a deletion is defined by coordinates a and b in the reference such that a1 is joined to b+1 in the test genome (Fig.1B).
Note that this is a simplification of the underlying biology, as there are sometimes small insertions or deletions at breakpoints, but these small changes have limited effect on the analysis of larger structural variants.
2.1 Breakpoint regions and variant uncertainty Neither paired-end mapping nor aCGH measure the breakpoints of a structural variant exactly.
Rather, each technique localizes breakpoints to a region of the reference genome, which we refer to as the breakpoint region.
We describe the derivation of this region for each of these experimental techniques.
2.1.1 Paired-end mapping In the paired-end mapping, or ESP, fragments of genomic DNA from a test genome are sequenced from both ends, and the resulting pair of end sequences are aligned to the reference genome.
We assume that each fragment1 C has ends that map uniquely to the reference genome.
Thus, each fragment C corresponds to a pair of locations in the reference genome where the end sequences map.
An end sequence may align to either DNA strand, and so each mapped end has a sign (+ or ) indicating the mapped strand.
We call such a signed pair (xC ,yC ) an end sequence pair (ES pair), where by convention |xC |< |yC |.
Typically, the length of the fragment, LC , is known to lie within a range [Lmin,Lmax].
Fragment sizes range from 150 kb for BAC clones to a few hundred base pairs for next-generation sequencing methods.
We say that a ES pair is a valid pair (Raphael et al., 2003) if the ends have opposite, convergent orientations and the distance between the mapped ends is within the range of fragment lengths: i.e.
(+xC ,yC ) is valid if Lmin |y||x|Lmax.
Otherwise, if the ends have abnormal distance or orientation, we say that the pair is an invalid pair.
Invalid pairs indicate putative genome rearrangements or possibly mapping/assembly errors.
For concreteness, consider the case of a test genome that differs from the reference genome by a single inversion with breakpoints a and b (Fig.1A) that fuse at a single coordinate in the test genome.
A fragment C from the test genome with length between Lmin and Lmax and containing is end-sequenced.
The resulting ES pair (xC ,yC ) will be an invalid pair indicating that C is not a contiguous piece of the reference genome (Fig.1A).
The invalid pair (xC ,yC ) does not uniquely identify the breakpoint (a,b).
However, if we assume that: (i) only a single breakpoint is contained in the fragment C; and (ii) a>xC and b>yC (without loss of generality); then the length LC of C is equal to (axC )+(byC ).
Thus, a breakpoint (a,b) that is consistent with (xC ,yC ) must satisfy Lmin (axC )+(byC )Lmax.
(1) We define the breakpoint region B(C) of an invalid fragment C to be the breakpoints (a,b) satisfying the above equation.
The constraint (1) has a straightforward geometric interpretation: if we plot an invalid pair (xC ,yC ) as a point in the 2D space GG then the breakpoint region defines a trapezoid (Fig.1A).
We emphasize that a and b cannot be chosen independently; doing so corresponds to defining the breakpoint region to be a rectangle, and allows breakpoints that give insert sizes outside the allowed range [Lmin,Lmax].
1We use the term fragment to describe both genomic clones (BACs, fosmids, plasmids) used by older sequencing technologies and DNA fragments in the mate-pair libraries employed in next-generation sequencing technologies.
If another fragment D contains the same fusion point , then the corresponding breakpoint (a,b) lies within the intersection B(C)B(D) of the trapezoids B(C) and B(D) (Fig.1A).
Conversely, we will assume that if the trapezoids defined by several invalid pairs intersect, then they share a common breakpoint.
As the number of fragments that are end-sequenced increases, more fragments will contain the same fusion point and the area of the intersection of breakpoint regions will decrease.
Thus, the uncertainty in the location of the breakpoint (a,b) decreases.
We define a cluster to be a set of fragments whose breakpoint regions have non-empty intersection.
The description above generalizes to other types of structural variants including translocations, insertions, deletions and transpositions.
For example, invalid pairs with sign(xC )=sign(yC )= also indicate inversions (corresponding to the other fusion point), while invalid pairs with sign(xC )= + and sign(yC )= indicate insertions or deletions.
Fragments with ends mapped to different chromosomes indicate translocations.
As above, we assume that the fragment C contains only a single breakpoint.
The breakpoints (a,b) that are consistent with the invalid pair (xC ,yC ) satisfy the inequalities Lmin (sign(xC )axC )+(sign(yC )byC )Lmax (2) This equation generalizes (1) and is summarized by the rule: end sequences point toward the breakpoint.
2.1.2 Array comparative genomic hybridization In aCGH, a breakpoint region is defined as the genomic interval between the two adjacent probes pi and pi+1 that define the endpoints of segments with unequal copy number (Fig.1B).
A pair of such breakpoint regions (e.g.
those resulting from a deletion) give two intervals U = [pi,pi+1] and V = [ pj,pj+1 ] that define a rectangle U V in 2D space GG.
This rectangle determines the locations of breakpoints (a,b)U V consistent with the segmentation.
Note that in addition to fragments that span deletions (Fig.1B), the boundaries of aCGH segments often indicate the locations of other types of rearrangements including translocations (Aerni et al., 2009; Campbell et al., 2008).
2.2 Efficient computation of overlapping breakpoint regions Given a set B1,...,Bn of breakpoint regions, our goal is to identify subsets of intersecting breakpoint regions.
Such a subset suggests these breakpoint regions are multiple measurements of the same structural variant.
In addition, we want to identify all such regions of intersection, and to label these by the breakpoint regions that are part of the intersection.
We formalize these problems as follows: All Intersections of Breakpoint Regions.
Given a set B={B1,...,Bn} of breakpoint regions, identify and label all non-empty intersections of subsets of B.
Since each breakpoint region Bi is a convex polygon (trapezoid or rectangle), the solution to the above problem relies on computing intersections of convex polygons, a well-known problem in computational geometry (Preparata and Shamos, 1985).
A naive brute-force approach that checks all 2n subsets of B for intersection is very inefficient.
Moreover, a single breakpoint region can have distinct intersections with different subsets of other breakpoint regions (Fig.2).
Thus, it is not sufficient to consider only pairwise intersections or iteratively merge breakpoint regions to existing intersections.
Below, we describe an efficient plane sweep algorithm that solves the All Intersections Problem.
While the All Intersections Problem provides the most comprehensive description of the overlaps between breakpoint regions, the output can be quite large since the number of regions of intersections grows rapidly as n increases.
However, many of these regions of intersection are not interesting because they are dominated by intersections of a larger number of breakpoint regions.
For example, if three breakpoint regions Bi, Bj and Bk have a non-empty intersection, then reporting this intersection is perhaps more desirable i224 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i225 i222i230 Geometric structural variation than reporting the (geometrically larger) intersections Bi Bj and Bi Bk , particularly as the number of such intersecting regions becomes large.
Thus, it is desired to identify regions of intersection of a maximal number of Bi.
We formalize this problem by defining a partial order on intersections of subsets of B.
For S {1,...,n}, let BS =sSBs denote the intersection of the breakpoint regions indexed by S. Let In be the set of subsets of {1,...,n} whose corresponding breakpoint regions have non-empty intersection.
Formally, In ={S {1,...n}|BS =}.
(3) In has the natural partial order of subset inclusion , where for two elements I and J of In, I J provided I J .
We denote this partially ordered set (poset) as (In,).
We formalize the problem as follows.
Maximal Intersections of Breakpoint Regions.
Given a set B= {B1,...,Bn} of breakpoint regions, identify all maximal elements of (In,).
Below, we describe how to solve the Maximal Intersections Problem by extending the plane sweep algorithm for the All Intersections Problem.
2.3 Plane sweep algorithm The plane sweep algorithm was introduced by Shamos and Hoey (1976) for the problem of determining whether n line segments in the plane have any intersections.
Clearly this question can be answered in O(n2) time by checking all pairs of segments for intersection.
A plane sweep algorithm performs the same task in O(nlogn) time by first sorting the segments by the x-coordinate of their left endpoint, and then moving the line x=c, called the sweep line through the plane from left to right.
The efficiency of the plane Fig.2.
Breakpoint regions determined by fragments from Kidd et al.
(2008) whose orientations suggest an inversion variant(s).
Breakpoint region 2 has distinct intersections with regions 1 and 3, and thus iterative merging of breakpoint regions will not identify all intersections.
sweep algorithm is derived from two observations.
First, not all coordinates c need to be considered.
A data structure called the event-point schedule E records the necessary values of c, and is updated dynamically as the sweep line moves from left to right.
Second, for a given position c of the sweep line, the segments intersecting the sweep line can be ordered by the y-coordinate of the intersection.
These ordered segments are stored in a data structure called sweep-line status L. Only adjacent segments in L need to be examined for intersections.
By employing appropriate data structures for E and L one obtains an efficient algorithm for segment intersection.
Further details of this algorithm can be found in Preparata and Shamos (1985).
The basic framework of the plane sweep algorithm has been extended to numerous related problems in computational geometry such as the counting of the k intersections of n segments in provably optimal O(nlogn+k) time (Chazelle and Edelsbrunner, 1992), and reporting the regions of intersection of polygons in the plane (Nievergelt and Preparata, 1982).
Here, we modify the algorithm of Nievergelt and Preparata (1982) to solve the All Intersections and Maximal Intersection problems described above.
Our extension exploits the particular geometry of the trapezoids and rectangles that define breakpoint regions in order to: (i) efficiently compute their intersection; (ii) label the intersecting regions by the breakpoint regions that are inside; and (iii) iteratively determine the maximal elements of (In,).
2.3.1 Overview of the algorithm We provide an overview of the plane sweep algorithm for the case of breakpoint regions defined by inversion variants; i.e.
fragments with parallel orientations (+,+) or (,).
These breakpoint regions are trapezoids with the two parallel sides having slope 1 (Fig.1A).
Thus, we define the sweep line to be a line y=x+c of slope 1.
The sweep line will encounter all inversion breakpoint regions as c increases from cmin to cmax.
The algorithm is identical for insertion/deletion variants except the sweep line is chosen to have slope +1, y=x+c, to match the parallel sides of the trapezoids in this case (Fig.1B).
As the sweep line advances through the plane, one of three possible events (Fig.3) can occur: (i) addition of a breakpoint region; (ii) intersection between two line segments defining the boundaries of breakpoint regions; (iii) removal of a breakpoint region (Fig.3).
Since the sweep line is parallel to the two sides of each trapezoid, it is only necessary to consider intersections between the horizontal/vertical sides of the trapezoid.
For each breakpoint region B in B, we define Btop and Bbottom as the horizontal/vertical sides of the trapezoid.
We designate the side with the largest y value as top.
2.3.2 Data structures As in the plane sweep algorithm for line segment intersection, we maintain two data structures E and L. We define the event point schedule E as the list of positions for the sweep line.
The event point schedule E is initialized with the starts and ends of Btop and Bbottom; these correspond to the addition/removal events.
E is updated with new Fig.3.
Examples of the three events of the plane sweep: (A) addition, (B) intersection and (C) removal.
In each case black dots label the points recorded in the cyclic lists a and b (indicated as dashed paths) that form R. In addition, we show in {}s the labels assigned to the intersecting breakpoint regions.
i225 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i226 i222i230 S.Sindi et al.
intersection points as they are discovered.
For a given event point (position of the sweep line), the sweep line status structure L stores an ordered list of the segments intersecting the sweep line and two terminal segments y=.
L is analogous to the same structure in the plane sweep algorithm for line segment intersection.
In this case all line segments intersecting the sweep line are either horizontal or vertical edges of breakpoint regions.
We first check for intersection between line segments that are adjacent in L (Fig.3B).
If a non-empty intersection is computed, the intersection point is added to the event schedule E .
The regions of intersection are recorded using a third data structure R introduced by Nievergelt and Preparata (1982).
R is attached to the sweep-line status L and records the vertices of the regions of intersection encountered thus far on the sweep.
For each segment s in L, R maintains two cyclic lists, a(s) and b(s), that contain the points on the boundaries of the regions above and below s, respectively.
Equivalently, if s and t are adjacent line segments in L, let [s,t] denotes the region to the left of the sweep line and between s and t. Then R contains the vertices defining the boundary of [s,t].
We augment the R structure of Nievergelt and Preparata (1982) with a label for each region of intersection.
This label is the set of breakpoint regions that contain the region of intersection.
For example, the region consisting of the non-empty intersection of breakpoint regions B1 and B2 is labeled {1,2}.
Finally, we maintain a interval tree (Preparata and Shamos, 1985) H from which we derive the maximal elements of the poset (In,) encountered thus far on the sweep line.
The algorithm (Algorithm 1) consists of iterating through the event point schedule and updating the regions of intersection found at each step according to whether the event point is an addition, removal, or intersection.
This update is briefly described in the next section.
2.3.3 Computing regions of intersection The procedureProcessEvent in Algorithm 1 updates the data structures, E , L, R and H according to the type of event.
For a removal event, ProcessEvent ends the regions [s,t] for each pair of adjacent segments in L by joining b(s) and a(t) with the points p(s) and p(t) where s and t intersect the sweep line (Fig.3C).
For an intersection event, ProcessEvent also swaps the order of s and t in L (Fig.3B).
Further details of these operations are described in (Nievergelt and Preparata, 1982) and in the Supplementary Text (available at http://www.cs.brown.edu/people/braphael/supplements/structvar).
Finally, each identified region of intersection is labeled by the constituent breakpoint regions.
Region labels are represented as sets of breakpoint region names and are updated using the following procedure.
If s and t are consecutive line segments along a sweep line, let I([s,t]) denote the label set of the region [s,t].
When processing an addition event of breakpoint region i, new regions are introduced with labels I([s,t])i.
When a processing a removal event of breakpoint region i, new regions are introduced with label I([s,t])\i.
Region labels also change during intersection events.
When regions are completed, their labels and list of boundary vertices are inserted into the interval tree H. Finally, all regions, or alternatively only maximal regions, are output as they are identified.
2.4 Extensions We briefly describe two natural extensions of our method.
First, we include aCGH data.
Second, we compute the probability that a paired-end sequenced fragment matches an existing structural variant.
2.4.1 Incorporating aCGH data As described above, the uncertainty in the breakpoints of a copy number change measured by aCGH is represented as a rectangle (Fig.3B).
The plane sweep algorithm is readily extended to include intersections with the rectangular breakpoint regions.
2.4.2 Incorporating fragment length distribution In most paired-end sequencing approaches, various procedures are used to select fragments of a specified size L, with the resulting fragments having lengths distributed around this selected size.
Thus far, we considered each fragment length between Lmin and Lmax to be equally likely.
We can instead derive the empirical distribution f (L) of the values |y||x| over all valid pairs (x,y) and use this distribution to better ascertain whether fragments provide evidence for a specific rearrangement.
The breakpoint (a,b) defines a length lC (a,b)= (sign(xC )axC )+(sign(yC )byC ) for fragment C. Given a polygon P defining the breakpoint region of a structural variant, we compute the probability that the invalid pair (xC ,yC ) is consistent with this variant as P f (lC (a,b))dadb GG f (lC (x,y))dxdy .
Note that this length is constant for all points (a,b) on the same line of slope 1 or 1, according to the orientation of the invalid pair (Bashir et al., 2008).
3 RESULTS We implemented our geometric approach in a program called GASV.
We applied GASV to: (i) analyze recent paired-end sequencing data of nine human individuals; (ii) perform a comparative analysis of genetic structural variants and those identified in paired-end sequencing of several cancer samples; and (iii) integrate data across measurement techniques by comparing variants identified by both aCGH and paired-end sequencing in cancer samples.
3.1 Paired-end sequencing of human structural variants We used GASV to analyze fosmid paired-end sequencing data from eight individuals from the HapMap populations (Kidd et al., 2008) and another individual from an earlier study (Tuzun et al., 2005).
The Kidd et al.
(2008) study reported a total of 224 inversion, 724 insertion and 747 deletion variants, which were validated by fingerprint analysis, clone sequencing or FISH.
These studies are presently the most comprehensive, high-resolution survey of structural variants in the human genome.
The mean insert sizes for the fosmid clones ranged from 36 kb to 41 kb with SD from 1.4 kb to 3.9 kb.
In our analysis, we used Lmin =20 kb and Lmax =60 kb to provide a generous buffer for intersecting breakpoint regions.
3.1.1 Analysis of reported inversion variants We first analyzed the 180 validated inversions reported on the 22 autosomes in Kidd et al.
(2008).
We obtained the list of the boundaries of each inversion, the names of the clones that support each variant and the mapped coordinates of the end sequences.
We used our geometric approach to compute the intersections of the breakpoint regions for each set of supporting clones, and we compared the reported boundaries of the inversions with the intersections we obtained.
Surprisingly, 41/180 i226 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i227 i222i230 Geometric structural variation A 8,9,13,G All 13 B Fig.4.
Geometric analysis of inversion polymorphisms from Kidd et al.
(2008) reveals disparities between the reported boundary of variants (black dots) and the intersections of breakpoint regions.
(A) An inversion on chr1 with 79 reported supporting clones from all nine individuals has no point in common to all breakpoint regions.
The number x next to each of the three regions indicates a clone from individual labeled ABCx in Kidd et al.
(2008) is present in the cluster; a G indicates the G95 individual from Tuzun et al.
(2005).
The bottom right region contains clones from all nine individuals, while individual ABC13 has clones from all three regions suggesting multiple distinct structural variants or mapping difficulties at this locus.
(B) An inversion from chr3 with 22 supporting clones from all eight HapMap individuals.
We examined one fully sequenced clone (dashed trapezoid) from individual ABC7 and found two possible inversion breakpoints (black squares).
Both of these lie in the intersection of all breakpoint regions but are 37 kb from the reported boundary.
of the validated inversions had an empty intersection of breakpoint regions.
That is, there were no candidate inversion breakpoints common to all of the reported supporting clones suggesting that the mapped clones are inconsistent with only a single inversion at the locus.
Figure 4A shows an example of one such set, where multiple, distinct non-overlapping intersections are visible.
One hypothesis is that the three distinct regions of intersection might represent slightly different breakpoints in different individuals.
However, one individual contains clones from all three regions, suggesting that this genomic locus harbors a more complex rearrangement.
In the remaining 139 cases the reported boundaries were not in the region of intersection.
Figure 4B shows one example where the reported coordinates for an inversion are clearly outside the region of intersection.
In this case, Kidd et al.
(2008) sequenced one of the clones in this cluster.
We aligned this sequence to the reference genome and obtained two possible inversion breakpoints, both of which lie in the region of intersection computed by GASV.
These two breakpoints could not be further resolved due to repetitive sequence near the inversion breakpoints.
Analysis of additional sequenced clones from Kidd et al.
(2008) showed a number of additional inversion breakpoints that occur within segmental duplications.
Thus even with complete sequence data available, resolving the breakpoint with greater precision is challenging.
The method used to derive the reported boundaries of the variants in Kidd et al.
(2008) is mysterious.
It is possible that the reported boundaries were intended to represent a consensus of a single structural variant locus.
For complicated loci with multiple, overlapping rearrangements (Fig.4A), consensus coordinates might provide a reasonable summary of the data.
However, we find that in many cases, the data allow us to refine the breakpoint region, and that significant information is lost when only a single pair of coordinates is reported for an inversion.
3.1.2 Analysis of intersecting breakpoint regions Using the complete set of mapped locations provided by Kidd et al.
(2008), we computed the intersections of breakpoint regions for all nine individuals using GASV.
In total, 30 853 clones on the 22 autosomes were consistent with an inversion.
There were 1361 groups of intersecting breakpoint regions.
Of these, 1200 had non-empty intersections, indicating that these clusters have a putative breakpoint in common for all of the clones.
The remaining 161 groups had no breakpoints common to all the clones.
Thus, over 10% the intersecting breakpoint regions suggest either complicated loci that are not easily explained as single inversion variants, or mapping/alignment artifacts.
For the 1200 clusters with non-empty intersection, we computed the area of the intersection and defined the localization of a breakpoint as the square root of this area.
Thus, if the region of intersection was a square, the localization would give the genomic range allowed for each breakpoint.
There are 19 clusters with breakpoint localization <2500 bp.
In the best example, eight clones from four individuals localize the breakpoints to within 286 bp on each end (Supplementary Text).
This is a remarkably small region of uncertainty, considering that a single fosmid clone localizes a breakpoint to 40 kb.
We also found that breakpoint localization is not directly correlated with the number of clones in the breakpoint region.
In 21/1200 cases the breakpoint region was supported by more than 50 clones.
In only two of these cases was the breakpoint localization <5000 bp.
A possible reason for this discrepancy is the presence of repeats/duplications near the inversion breakpoints.
These would lead to a relatively small genomic region where end sequences can be mapped uniquely.
3.1.3 Overlap of inversion and deletion variants We used GASV to compare the locations of inversions and deletions in the data of Kidd et al.
(2008).
We identified 5054 instances of intersection between inversion and deletion breakpoint regions.
Figure 5 shows a sample cluster containing 33 clones indicating an inversion and i227 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i228 i222i230 S.Sindi et al.
Fig.5.
Intersection of 33 inversion breakpoint regions (blue) and 4 deletion breakpoint regions (red), indicates common genomic location of two structural variants.
4 clones indicating a deletion at the same locus.
There are several examples where inversion heterozygotes lead to deletions in progeny (Stankiewicz and Lupski, 2002), a possible explanation for these overlapping variants.
Alternatively, this overlap might suggest that these regions are unstable and subject to repeated rearrangement (Stankiewicz and Lupski, 2002).
3.2 Cross-study comparison of structural variants Our geometric approach allows for the comparison of structural variants identified in different individuals with different measurement techniques.
We tested this feature by comparing the genetic structural variants identified by Kidd et al.
(2008) with variants identified in ESP studies of cancer genomes (Raphael et al., 2008; Volik et al., 2006).
The later studies aimed to identify somatic, and possibly cancer-related, rearrangements in three breast cancer cell lines and five primary tumors from various tissues.
The cancer ESP studies used large insert clones (BACs) with average sizes of 150 kb.
We first identified clusters of invalid pairs in each cancer dataset that were suggestive of either inversions or deletions, and then computed the intersection of these clusters with the inversion and deletion clusters computed from the nine normal individuals.
Approximately 553% of invalid clusters from the cancer clusters are consistent with inversion or deletion variants identified in normal individuals (Table 1).
The larger percentages are found in the primary tumor samples; this is consistent with the lower sequence coverage in the primary tumor samples, and the fact that tumor samples frequently contain significant admixture of normal cells resulting from difficulty of separating normal from tumor cells.
We then clustered all the cancer data together with the nine normal individuals, and identified overlapping breakpoint regions containing at least two invalid pairs from different cancer samples.
Of the 22 such clusters, 10 are consistent with inversion variants identified in at least one normal individual (9/10 cases were observed in at least four normal individuals), demonstrating that a large fraction of structural variants found in more than one cancer dataset are inherited genetic variants and not somatic rearrangements.
Table 1.
A comparison of the inversion and deletion variants identified in nine normal individuals (Kidd et al., 2008; Tuzun et al., 2005) and several cancer genomes (Raphael et al., 2008; Volik et al., 2006) Cancer No.
of concordant No.
of concordant sample inversions (%) deletions (%) MCF7 8 (5) 40 (28) BT474 12 (19) 8 (11) SKBR3 8 (13) 7 (11) Breast 11 (19) 21 (27) Breast 12 (32) 19 (38) Prostate 3 (9) 12 (27) Ovary 8 (53) 12 (29) Brain 2 (11) 10 (26) Fig.6.
Intersection between six breakpoint regions from ESP data (blue trapezoids) and two breakpoint regions determined by aCGH (red rectangle) on chr17 in the BT474 breast cancer cell line.
In this case, the spacing between aCGH probes provides a more precise localization of the breakpoint region that the paired-end sequencing data.
3.3 Comparing variants identified by paired-end sequencing and aCGH We used GASV to compare the breakpoints identified by ESP and aCGH for three cancer cell lines, MCF7, BT474 and SKBR3, using data from Volik et al.
(2006), Raphael et al.
(2008), and Aerni et al.
(2009).
We formed rectangles corresponding to pairs of copy number changes identified by segmentation (Fig.1B) of aCGH data using CBS (Olshen et al., 2004).
We found that 35/152, 20/380 and 35/149 of the clusters defined from paired-end sequenced data intersected aCGH breakpoint regions in BT474, MCF7 and SKBR3, respectively.
Figure 6 shows an example of a cluster containing 19 breakpoint regions identified by ESP in the BT474 cell line, intersecting a breakpoint region determined by aCGH.
4 DISCUSSION We introduced GASV, a geometric approach for classification and comparison of structural variants.
To our knowledge, this is the first comprehensive method for structural variant analysis across multiple samples that supports both paired-end sequencing data with arbitrary fragment sizes and aCGH with varying array resolutions.
i228 [10:01 15/5/2009 Bioinformatics-btp208.tex] Page: i229 i222i230 Geometric structural variation We illustrated the generality of our approach through several applications, including the clustering of variants from a paired-end sequencing study of nine individuals, the comparison of variants in normal and cancer genomes derived through different sequencing approaches, and the comparison of variants identified by aCGH and paired-end sequencing of the same cancer samples.
In many cases we are able to localize the breakpoints of single variants, but in other cases the end-sequence pairs suggest more complicated variants.
The precise localization of the boundaries of structural variants provided by the GASV is helpful for distinguishing simple variants shared across multiple individuals from more complex variants resulting from repeated rearrangements at the same locus.
These results also demonstrate the importance of identifying and reporting the uncertainty in structural variant boundaries.
The current convention of publishing approximate coordinates that were derived from study-specific heuristics can lead to unnecessary errors and misannotations of complicated variants.
We expect that GASV will be useful for analyzing data from the 1000 Genomes Project and for cancer genome sequencing efforts that are part of The Cancer Genome Atlas.
In the latter application, GASV will help distinguish genetic from somatic rearrangements.
There are several directions for future work.
First, it would be useful to perform a more comprehensive comparison of the variants that are identified by different measurement techniques.
aCGH has limited power to detect variants whose breakpoints lie in repeat-rich regions of the genome due to the inability to identify probes in these regions.
Paired-end sequencing approaches can be similarly limited, particularly if small fragment sizes are used, since the end sequences will not align uniquely to repeat-rich regions of the genome.
Current studies of structural variants with next-generation sequencing technologies have used small fragment sizes from 200 bp (Campbell et al., 2008) to 3 kb (Korbel et al., 2007).
An unresolved question is the optimal fragment size to use for studies of human structural variation.
We have shown that clustering of breakpoint regions from relatively large clones (40 kb) with GASV can yield very precise localization of variant breakpoints (a few hundred base pairs).
Kidd et al.
(2008) reported that most of the clones that they sequenced had highly repetitive sequence at the breakpoints, complicating the precise breakpoint identification and assembly of the clone sequence.
Thus, even in cases where complete sequence is available GASV can be used to record uncertainty in breakpoint location.
An additional area of future work is to incorporate breakpoint uncertainty into databases of known structural variants.
Our geometric approach could then be used to query this database and thus provide a more robust procedure for comparing newly discovered and existing variants.
In addition, knowledge of existing structural variants can be used to guide mapping of end sequences that do not map uniquely to the reference genome.
This is a common problem in human genome resequencing, where up to 60% percent of ES fragments are not used because of their ambiguous mappings (Korbel et al., 2007).
Lee et al.
(2008) recently described a probabilistic model for resolving ambiguities that arise when mapping ES pairs in a single sample.
Developing a model for multi-sample comparison that incorporates variant ambiguity across samples is a promising future endeavor.
Finally, we focused exclusively on structural variation in the human genome, but such variation is also found in the mouse genome (Egan et al., 2007) and other model organisms (Dopman and Hartl, 2007).
Thus, there will continue to be an increasing demand for better analysis tools for structural variation.
ACKNOWLEDGEMENTS We thank Franco Preparata and Crystal Kahn for helpful technical discussions, and Anna Ritz for assistance in preparing the manuscript.
Funding: Career Award at the Scientific Interface from the Burroughs Wellcome Fund (to B.J.R.
); the Department of Defense Breast Cancer Research Program (to B.J.R.
); ADVANCE Program at Brown University, which is funded by the National Science Foundation under grant number 0548311 (to B.J.R.).
Conflict of Interest: none declared.
ABSTRACT Motivation: Epistasis, the presence of genegene interactions, has been hypothesized to be at the root of many common human diseases, but current genome-wide association studies largely ignore its role.
Multifactor dimensionality reduction (MDR) is a powerful model-free method for detecting epistatic relationships between genes, but computational costs have made its application to genome-wide data difficult.
Graphics processing units (GPUs), the hardware responsible for rendering computer games, are powerful parallel processors.
Using GPUs to run MDR on a genome-wide dataset allows for statistically rigorous testing of epistasis.
Results: The implementation of MDR for GPUs (MDRGPU) includes core features of the widely used Java software package, MDR.
This GPU implementation allows for large-scale analysis of epistasis at a dramatically lower cost than the standard CPU-based implementations.
As a proof-of-concept, we applied this software to a genome-wide study of sporadic amyotrophic lateral sclerosis (ALS).
We discovered a statistically significant two-SNP classifier and subsequently replicated the significance of these two SNPs in an independent study of ALS.
MDRGPU makes the large-scale analysis of epistasis tractable and opens the door to statistically rigorous testing of interactions in genome-wide datasets.
Availability: MDRGPU is open source and available free of charge from http://www.sourceforge.net/projects/mdr.
Contact: jason.h.moore@dartmouth.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on October 16, 2009; revised on January 7, 2010; accepted on January 8, 2010 1 INTRODUCTION Genome-wide association studies hold promise for the discovery of the genetic factors that underlie common human diseases (Hirschhorn and Daly, 2005; Wang et al., 2005).
Unfortunately this promise has largely not been realized (Shriner et al., 2007; Williams et al., 2007).
It is thought that this failure could be due to epistasis, the role of genegene interactions, which has commonly been ignored in these studies.
Powerful and model-free methods such as multifactor dimensionality reduction (MDR) have been developed (Ritchie et al., 2001), but an exhaustive examination of To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First authors.
even pair-wise interactions in a 550 000 SNP dataset would require the analysis of 1.51011 combinations.
While an analysis of this scale is approachable with modern cluster computing, an analysis that includes permutation testing to assess the statistical significance of results remains infeasible with CPU-based approaches.
Rendering photo-realistic video games in real time is also computationally difficult.
For video game graphics, specific hardware (the graphics processing unit or GPU) has been developed.
The GPU is a massively parallel computing platform that can be adapted to some scientific tasks.
We have previously shown that MDR is one of these tasks (Sinnott-Armstrong et al., 2009).
Here we provide software which makes practical the analysis of epistasis in genome-wide data through the use of GPUs and demonstrate its application to a genome-wide analysis of epistasis of sporadic amyotrophic lateral sclerosis (ALS).
2 METHODS MDRGPU, a software tool capable of analyzing genome-wide data, is a Python implementation of MDR, which uses the PyCUDA library to run MDR on GPUs.
MDRGPU 1.0 supports balanced accuracy, large datasets, execution across an arbitrary number of GPUs, permutation testing and the analysis of high-order interactions.
It runs on GPUs which support CUDA (i.e.
the NVIDIA GeForce 8800 series and higher).
Parallel execution of one realization across multiple GPUs is supported with the pp library for Python.
MDRGPU provides a command-line interface for scripted analysis.
The GPU architecture has various memory spaces available.
MDRGPU uses the constant cache, global memory, shared memory and registers.
Shared memory is used to store the intermediate case and control counts for each attribute combination and to store the number of true and false positives and negatives.
The global memory is accessed directly to fetch attributes.
The constant cache is used in MDRGPU to store the casecontrol status.
Dataset sizes of greater than 65 536 attributes require splitting which is handled seamlessly by MDRGPU.
This splitting does not cause linear slowdown; there is simply more overhead of launching, so datasets with large numbers of instances see less of a performance reduction than datasets with few instances.
The largest number of addressable attributes is 4 billion requiring 4 GB RAM per instance.
In order for the casecontrol status to be held in constant memory, there can be at most 16 384 instances.
Our proof of concept analysis was performed on three GPU workstations (detailed in Supplementary Material S1).
These systems contain three GeForce 295 cards, each of which contains two GPUs.
For the first stage of this analysis, we used an ALS dataset from Schymick et al.
(2007) as our detection dataset.
This dataset was obtained from QUEUE at Coriell, but has since been moved to dbGaP.
It contains 276 individuals with sporadic ALS and 271 control individuals.
These individuals are genotyped at 555 352 SNPs using the Illumina Infinium II HumanHap550 SNP chip.
We processed this dataset by removing SNPs with a minor allele frequency <0.2 or those The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[15:25 5/2/2010 Bioinformatics-btq009.tex] Page: 695 694695 Multifactor dimensionality reduction for GPU in which >10% of values were missing for either cases or controls.
We further used Haploviews tagSNP algorithm (Barrett et al., 2005) to select representative SNPs from groups of correlated SNPs (r >0.8).
After this, 210 382 SNPs remained and were used in the analysis.
For the replication stage, we used a dataset of Irish individuals containing of 221 sporadic ALS patients and 211 controls described in Cronin et al.
(2008).
We used MDRGPU to perform a two-way analysis across the entire detection dataset.
We selected the SNP combination with the best balanced accuracy measure.
We then permuted the dataset 1000 times while repeating this analysis.
We measured the accuracy of the best pair in each permuted dataset.
We then used the 50th best accuracy obtained from these permuted datasets as our significance cutoff.
This permutation test yields an experiment-wise of 0.05.
A pair of SNPs with a significant association in the detection phase was tested in the replication dataset.
In this phase, the two detected SNPs were selected from the dataset and MDR was used to evaluate only this pair.
A permutation test was performed here using MDR on only these two SNPs, and an of 0.05 was used to assess significance.
3 RESULTS Our three GPU systems completed an analysis of pairwise interactions in a single permutation approximately every 6 min.
The time to analyze the dataset itself for pairwise interactions is the same as the time required for one permutation.
One thousand permutations were used to assess statistical significance which required 100 h. The time to analyze the same dataset on a cluster with 200 AMD Opteron 2384 (2.7 GHz) CPU cores was just over 1 h without permutation testing and thus a CPU-based permutation test was considered infeasible as the estimated time required on 200 CPU cores was >40 days.
In the proof-of-concept analysis, the highest accuracy combination in our dataset was SNPs rs4363506 and rs6014848 with a balanced accuracy of 0.6551.
In our permutation test, this accuracy was statistically significant (P<0.048).
In the replication dataset this pair had a balanced accuracy of 0.5821.
Permutation testing the replication dataset showed that this result was also statistically significant (P<0.021).
Therefore, not only have we discovered a statistically significant pair of SNPs using an experiment-wise of 0.05, but we have replicated the significant relationship in an independent dataset.
Here is evidence of how the permutation testing allowed by MDRGPU enables the discovery of combinations of SNPs that are significantly associated with a disease.
4 DISCUSSION While SNP rs4363506 has been reported as associated with disease in Schymick et al.
(2007), it did not have a statistically significant effect in Cronin et al.
(2008) when considered alone (2, P=0.18) and would have failed to replicate without considering pairwise effects.
SNP rs6014848 has not previously been described as associated with sporadic ALS, although it shows main effects (uncorrected 2, P<0.05) in both datasets.
Greene et al.
(2009) have shown that SNPs can fail to replicate a significant association when the joint effect of those SNPs is ignored.
This is particularly likely when the populations from which patients are ascertained differs.
Schymick et al.
(2007) collected individuals from the USA, while Cronin et al.
(2008) collected individuals from Ireland.
By considering the joint effect of SNPs, MDRGPU discovers a novel association which replicates in an independent dataset.
GPUs provide a platform for epistasis analysis in genome-wide data where computational requirements far exceed what CPUs can cost-effectively provide.
MDRGPU is a software package for this emerging computing platform that enables human geneticists to tackle analyses previously found to be intractable.
ACKNOWLEDGEMENTS We thank Peter Andrews for code review.
The genotyping of samples was provided by the NINDS.
The datasets used for the analyses described in this manuscript were obtained from the NINDS Database through dbGaP accession numbers phs000006.v1.p1 and pht000649.v1.p1.
Permission to use these datasets was obtained through application to dbGAP by B.T.H.
Funding: NINDS, ALS Assoc, Muscular Dystrophy Assoc (MDA) (to National Institute of Neurological Disorders and Stroke (NINDS) Repository Motor Neuron Disease/ALS Study); MDA (to The Study of Irish ALS); Irish Institute of Clinical Neurosciences (to The Study of Irish ALS); National Institutes of Health (NIH); (Intramural funding to to The Study of Irish ALS).
NIH (grants LM009012, LM010098, AI59694, HD047447 and ES007373).
Conflict of Interest: none declared.
ABSTRACT Motivation: Characterizing and comparing temporal gene-expression responses is an important computational task for answering a variety of questions in biological studies.
Algorithms for aligning time series represent a valuable approach for such analyses.
However, previous approaches to aligning gene-expression time series have assumed that all genes should share the same alignment.
Our work is motivated by the need for methods that identify sets of genes that differ in similar ways between two time series, even when their expression profiles are quite different.
Results: We present a novel algorithm that calculates clustered alignments; the method finds clusters of genes such that the genes within a cluster share a common alignment, but each cluster is aligned independently of the others.
We also present an efficient new segment-based alignment algorithm for time series called SCOW (shorting correlation-optimized warping).
We evaluate our methods by assessing the accuracy of alignments computed with sparse time series from a toxicogenomics dataset.
The results of our evaluation indicate that our clustered alignment approach and SCOW provide more accurate alignments than previous approaches.
Additionally, we apply our clustered alignment approach to characterize the effects of a conditional Mop3 knockout in mouse liver.
Availability: Source code is available at http://www.biostat.wisc.
edu/aasmith/catcode.
Contact: aasmith@cs.wisc.edu 1 INTRODUCTION Characterizing and comparing temporal gene-expression responses is an important computational task for answering a variety of questions in biological studies.
In previous work (Smith and Craven, 2008; Smith et al., 2008), we have introduced methods for answering similarity queries about gene-expression profiles after exposure to some chemical or treatment.
These methods have been motivated by the task of quickly and accurately characterizing the potential toxicity of chemicals.
A fundamental step in comparing two time series is with temporally align the series using a method such as dynamic time warping (Sakoe and Chiba, 1978; Sankoff and Kruskal, 1983).
Previous approaches to aligning gene-expression time series have assumed that all genes should be aligned in lockstep with one another.
In other words, these methods assume that the transformation that specifies how one series relates to another is the same for all genes.
Here, we present a novel approach that finds clusters of genes such that the genes within a cluster share a common alignment, but each cluster is aligned independently of the others.
Our method is similar to k-means clustering (Duda et al., 2000) in that it alternates between assigning genes to clusters and recomputing the alignment for each cluster using the genes assigned To whom correspondence should be addressed.
Fig.1.
The time-series similarity task.
Given a gene-expression time series as a query, we want to find the time series in the database which are most similar to the query.
Shaded areas represent strong matches to the given query.
Notice that for both Treatments B and C, the best alignment to the query does not account for the entire extent of the treatments.
Also notice that with Treatment B, all genes can be aligned together, whereas with Treatment C the second gene should be aligned separately.
to it.
We also present a novel multi-segment alignment algorithm that computes more accurate alignments for sparse gene-expression time series than previous methods.
One application for time-series alignment that we consider is the task of answering similarity queries as illustrated in Figure 1.
Given an expression profile as a query, we want to identify treatments in a database that have expression profiles most similar to the query.
When the query and/or some of the database treatments are time series, we assess similarity by determining the temporal correspondence between the query and treatments in the database.
In our toxicogenomics application, we might be trying to determine if an uncharacterized chemical induces an expression response similar to any known toxicants.
The figure shows a simple case in which our database consists of expression profiles from four different treatments, and each expression profile characterizes only three genes.
Figure 1 illustrates two important issues that arise in this task.
Sometimes (as with Treatment B) all genes should be aligned (i.e.
warped) together to find the best correspondence.
But, it may also happen that some genes need to be warped separately from the others, as with Treatment C. A second issue is that often the best alignment does not account for the complete extent of both time series.
Therefore, we want to allow a type of local alignment in which the end of one series is unaligned.
We refer to this case as shorting the alignment.
The two main contributions of this work are algorithms that are designed to address both of these issues when computing time-series alignments.
2009 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i120 i119i127 A.A.Smith et al.
Fig.2.
Alignment space example.
The multi-segment alignment path characterizes correspondences between two series, as shown by the dotted lines.
Knots are the points of discontinuity in the path.
Figure 2 shows the alignment of two time series in alignment space, using a multi-segment alignment method.
The alignment path determines which points in the two series are mapped to one another.
For a given point in the path, the coordinate in the first time series directly below it and the coordinate in the second time series directly to its left correspond to one another.
A multi-segment alignment can take into account that the nature of the relationship between the two series may vary in different segments.
For example, it may be the case that the later part of the expression response occurs more slowly in one treatment than in a similar treatment.
We refer to the points of discontinuity that define the segment boundaries as knots.
The alignment in Figure 2 also illustrates the concept of shorting.
Here, Time Series A seems to have advanced more quickly than Time Series B, which has not started to increase at the end.
An alignment path that represents shorting ends in the top row or the right column of the alignment space diagram, but not in the top-right cell.
Note that we do not allow an alignment to short both series; all of one or the other must be mapped to some point in its mate.
In previous work (Smith et al., 2008), we described a novel multi-segment alignment method and empirically demonstrated that it classifies and aligns our toxicogenomics data better than several competing methods, including dynamic time warping, several parametric methods (such as linear alignment) and another multi-segment method called correlation-optimized warping, or COW (Nielsen et al., 1998).
Parametric methods, which constrain the warping path to a simple functional form, often are not expressive enough to capture the most appropriate warping.
In contrast, dynamic time warping can often be too expressive, finding high-scoring alignments of unrelated series.
A multi-segment method provides a balance between these two methods.
The accuracy advantage of our previous multi-segment method over COW was slight.
COW is a global alignment method that cannot short.
On closer inspection, we found that our method discovered more accurate alignments in cases that required shorting, whereas COW dominated those trials that did not.
Here, we present a modified version of COW that allows shorted alignments.
We call the method SCOW, for shorting COW.
Our algorithm for computing clustered alignments uses SCOW as its base alignment method.
Aach and Church (2001) were the first to apply the method of dynamic time warping (Sakoe and Chiba, 1978) to gene-expression profiles, and other groups have followed with this warping method (Criel and Tsiporkova, 2006; Liu and Mller, 2003) and others (Bar-Joseph et al., 2003).
Importantly, they have all done their warping on all genes together, whereas we compute clustered alignments.
Also, our approach differs in that it compute multi-segment alignments and considers local alignments via shorting.
Other studies have investigated clustering gene-expression time series (Bar-Joseph et al., 2003; Eisen et al., 1998; Leng and Mller, 2006; Liu and Mller, 2003).
The important differences between these approaches and ours are 2-fold: the goals of the clustering process and the notion of similarity used.
Whereas these previous methods have focused on identifying clusters of genes that have similar expression profiles, our approach, in contrast, is focused on identifying clusters in which the genes have similar warpings.
The genes in one of our clusters may have very different expression profiles, but they are similar in how they should be warped across the two time series being compared.
Listgarten et al.
(2005) have developed a method for multiple alignment of time series data that has some similarities to our approach.
Their method, however, computes a single alignment of multiple time series, whereas our method computes a clustered alignment of a pair of time series.
We are not the first group to develop algorithms for computing shorted alignments.
Keogh (2003) devised a two-step shorting method that first finds the appropriate end points of an alignment before calculating a global alignment up to these points.
Our approach to shorting is different in that the shorting decision is not decoupled from the computation of the alignment; the dynamic programming method considers shorted as well as non-shorted alignments.
2 METHODS In this section, we detail two novel techniques that we have developed.
The first is SCOW, which is a method for computing multi-segment alignments of two time series and assessing their similarity.
The second is an algorithm which computes clustered alignments in which the genes within a cluster share a common alignment, but each cluster is aligned independently of the others.
2.1 SCOW We start by describing COW (Nielsen et al., 1998), which is a dynamic programming algorithm designed to find an optimal alignment between two series with multiple channels of information (such as genes).
We then describe SCOW, which is our extension to COW.
COW was developed to align chromatography time-series data.
Briefly, it aligns and scores two given time series based on their similarity.
Here, we refer to the two series as q (for query series) and d (for database series).
For each possible alignment, the series are partitioned into m segments, in which the i-th segments of the two series correspond to each other.
The score of a given alignment is the sum of correlations between corresponding segments.
As shown in Figure 3A, COW searches for good segment boundaries in only a limited area of alignment space.
The segments are assumed to be of constant length in q, and variable in d. The vector K contains the coordinates of the knots (segment endpoints) in q.
These are usually evenly spaced.
COW works by filling a zero-indexed matrix , which is of dimensions m+1 by |d|+1.
The element k,x contains the score of the best alignment of d from zero to x and q from zero to Kk (the k-th element of K) using k segments.
i120 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i121 i119i127 Clustered alignments Fig.3.
COW and SCOW in alignment space.
Both perform searches to find the best set of knots, or points of discontinuity, for a multi-segment alignment.
(A) COW, which assumes no shorting and searches for good knots only in a single dimension, along the dotted lines.
(B) The first step of SCOW, which searches independently in both dimensions.
Subsequent steps are numbered in (C), as SCOW alternates horizontal and vertical movement of each knot until it converges.
It is filled using the following recurrence relations: 0,x= { 0 if x=0 otherwise (1) k,x= max ypred(x,k) [ k1,y+cor ( d(y,x),q(Kk1,Kk) )] (2) where cor is the Pearson correlation, q(a,b) represents a subseries of q from a to b and d(a,b) is defined likewise.
The predecessor function lists valid starting locations in d for segments ending at x: pred(x,k)= x |q||d| (KkKk1)t,..., x |q||d| (KkKk1)+t , (3) with t being a user-defined slack parameter that controls the size of the search space.
The best alignment, and its resulting score, is represented by the element of that corresponds to the end of the global alignment: BestScore()=m,|d|.
(4) Note that COW can be used to align a one-channel time series, such as the expression profile of a single gene, or a multi-channel time series, such as the expression profile of a set of genes.
The only difference between these two cases is in how the correlations are calculated.
A limitation of COW is that it forces the entirety of both series to be aligned to each other; it cannot short the alignment.
Also, COW is apt to align segments which differ greatly in magnitude because it scores by correlation.
Further, the computation in Equation (2) may sometimes return to an undefined value if the input segments do not have a defined correlation (as when both segments consist of all zeros).
Our SCOW is designed to rectify these problems.
As shown in Figure 3B, SCOW searches for optimal knots in both dimensions.
It first finds optimal knots with respect to q using evenly spaced knots in d, and with respect to d using evenly spaced knots in q.
It uses the better alignment from these two passes as the starting point for an iterative process.
From then on it alternates, which dimensions knot coordinates it holds constant, using the coordinates found by the previous pass as the constant knots in the next one.
This iterative process is illustrated in Figure 3C, and Table 1 provides pseudocode describing the SCOW algorithm.
There are two different recurrence relations used in SCOWs dynamic programming formulation: q k,x= maxypred(x,k) [ q k1,y+score ( d(Kdk1,K d k ),q(y,x) )] , (5) Table 1.
The pseudocode for SCOW procedure SCOWAlign(series d, series q, set of genes G)://initial passes//Kqevenly spaced integers from 0 to |q| Kdevenly spaced integers from 0 to |d| calculate q,d using G if (BestScore(q)>BestScore(d )): q else: d KTraceback()//main loop//repeat: swap-dimension calculate using G calculate BestScore() KTraceback() until Kq,Kd converge Knots are recalculated at least three times.
The Traceback function extracts the best knots found from the previous pass to use in the next one.
dk,x= max ypred(x,k) [ dk1,y+score ( d(y,x),q(Kqk1,K q k ) )] .
(6) The matrix q is calculated when the algorithm searches for knots with respect to q and holds them constant with respect to d, while d is calculated during the opposite case.
The vectors Kq and Kd represent the coordinates of the knots in each dimension.
The predecessor function is altered so as to not center around the line with slope |q|/|d| but instead to enable a cone-shaped search space (as illustrated in Figure 3B) since we want to consider shorted alignments: pred(x,k)= max [ x(KkKk1), 1 Kk1) ] ,..., min [ x 1 (KkKk1),Kk1) ] , (7) where is the maximum slope allowed the aligning path in alignment space.
In addition, SCOW does not assume a global alignment, but searches the last i121 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i122 i119i127 A.A.Smith et al.
row of the matrix for the best scoring alignment using m segments: BestScore()=  m,|d| if other dimension shorted max j m,j otherwise .
(8) This allows SCOW to short in the current dimension, if the other dimension is not already shorted.
Thus the alignment found cannot short both q and d. The effect on the search can be seen in Figure 3C: the last knot cannot move down during the first step, because doing so would short both dimensions.
In addition to a different search procedure, SCOW also differs somewhat from COW in the function it uses to score alignments.
The scoring function presented here is similar to one we used in previous work (Smith et al., 2008).
In particular, the scoring function includes terms that incur penalties for segments that involve stretching and significant differences in amplitude.
We use the term stretching to refer to distortions in the rate of some expression response, and the term amplitude to refer to distortions in the magnitude of the response.
Consider, for example, the alignment shown in Figure 2.
The first segment in this alignment involves a noticeable amplitude difference (Time Series B has a higher amplitude than Time Series A), and the last segment involves significant stretching (this part of the response in Time Series B happens more slowly than the corresponding part of Time Series A).
We define the score of an alignment segment to be: score(qi,di)=cor(qi,di) log 2 si 2 2s log 2 ai 2 2a (9) Here, qi and di denote the i-th segments of series q and d, respectively, si is the amount of stretching in the alignment of the i-th segments, ai is the amplitude difference, and cor is the Pearson correlation.
The stretching si is defined as the ratio of lengths between qi and di, and ai is the amplitude ratio between the two as determined by a weighted least squares fitting procedure.
The form of the stretching and amplitude terms comes from a generative, probabilistic model we developed in earlier work (Smith et al., 2008).
This previous approach uses probability distributions over possible stretching and amplitude values that have the following form: p(v)= e 2 2  2 e log2 v 22 .
(10) The key property of this distribution is that it is symmetrical around 1 such that P(x)=P(1/x).
Thus stretching, and amplitude values that deviate from 1 are penalized, and the penalty is the same regardless of which series, q or d, is considered to have the distortion.
For all of our experiments with COW and SCOW, we calculate correlations in the following way.
We first use B-splines (Rogers and Adams, 1989) to interpolate between the observations in our time series (which are typically sparsely sampled).
To calculate correlations between segments qi and di, we resample their spline approximations to the same predetermined number of values for the two segments.
We also alternately add and subtract a tiny value to values in qi and di, so that correlation is always defined and two segments with constant values will have a correlation of one.
Like COW, SCOW operates with a time complexity of O(n3), where n is the length of the interpolated series to be aligned.
Further, many of the calculations in successive passes of SCOW are the same, and may be cached.
In contrast, the segment-based method from our previous work took O(n5) time to do an exhaustive search for the best segments to align the series.
The speed-up is dramatic: what took the old method an hour to calculate takes SCOW only a few seconds.
2.2 Clustered alignments Now we describe the algorithm we have developed for computing clustered alignments.
The goal of this algorithm is to find sets of genes that would have very similar alignments if they were aligned independently.
The alignment Table 2.
The pseudocode for our clustered alignment algorithm procedure ClusterAlignments(series d, series q, # clusters k)://initialize cluster centroids//centroid[1]null alignment for all (genes g): possible[g]ScoreGene(q,d,g,Align(q,d,{g})) best[g]ScoreGene(q,d,g,centroid[1]) for (i2 to k): worstargming(best[g]possible[g]) centroid[i]Align(q,d,{worst}) for all (genes g): best[g] max(best[g],ScoreGene(q,d,g,centroid[i])) repeat://assignment step//for all (centroids c): set[c] for all (genes g): sargmaxc(ScoreGene(q,d,g,c)) set[s]set[s]g//update step//for all (centroids c): cAlign(d,q,set[c]) until sets converge represented by each cluster may be quite different from the alignments that the other clusters represent.
This approach is motivated by the fact that the relationship between two similar time series may differ depending on which subset of genes we consider.
The algorithm we have devised is a variant of traditional k-means clustering (Duda et al., 2000).
In k-means, each cluster is represented by a centroid and the clustering process involves iteratively refining the locations of these centroids.
For example, if we were clustering points in Rn, each centroid would be represented by a point in Rn.
In our clustered alignment method, each centroid is represented by an alignment (e.g.
such as the one illustrated in Fig.2).
In our algorithm, as in standard k-means, the number of clusters is determined by a parameter k that is provided as an input.
We reiterate that, in contrast to previous methods which have focused on identifying clusters of genes that have similar expression profiles, our algorithm is focused on identifying clusters in which the genes have similar warpings.
The genes in one of our clusters may have very different expression profiles.
Table 2 shows the pseudocode for our alignment clustering method.
It takes as input two series, termed d and q, and the number of clusters k. It relies on the subroutines Align, which returns the best alignment between two series based on a given set of genes, and ScoreGene, which returns the score of two series when aligned using a given alignment and a specified gene.
We use SCOW to perform these functions, using SCOWAlign for Align while using Equation (8) for ScoreGene.
However, we could substitute any other alignment algorithm for this purpose.
The first step in the method is to assign the initial alignment centroids.
We use a greedy method, similar to that used by Ernst et al.
(2005) to select a representative set of gene alignments as the centroids.
The first centroid is taken to be the null alignment, which represents no warping.
For each gene, we record a best possible score (when the alignment is based solely on that gene), and the best score seen so far for that gene using one of the current centroids.
Each additional centroid is initialized by finding the gene with the largest difference between its best score so far and its possible high score.
The new centroid is the alignment calculated using this selected gene alone.
After each new centroid is determined, the best scores for all the genes i122 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i123 i119i127 Clustered alignments are modified to take the new centroid into account.
We proceed until all k centroids are defined.
Now we perform the assignment step and the update step in turn until convergence.
For the assignment step, we score every gene with every clusters centroid and assign the gene to the cluster with the highest score.
For the update step, we set each centroid to the alignment calculated by aligning q and d using just the set of genes assigned to the cluster.
We continue iterating until the cluster assignments do not change.
Because SCOW performs a heuristic search, however, it is possible that the process will not converge.
In practice, this is seldom a problem.
We can simply stop iterating after a large number of iterations, or when infinite loop conditions are detected by retaining a short history of cluster assignments.
Alternatively, we can guarantee convergence by using an alignment algorithm that is exact.
3 RESULTS AND DISCUSSION In this section, we describe a set of computational experiments that are designed to (i) evaluate the alignment accuracy of SCOW and our clustered alignment method, and (ii) assess how well the clustered alignment algorithm is able to uncover sets of genes that share similar alignments across two time series.
3.1 SCOW experiments In our first set of experiments, we are interested in testing the ability of the SCOW method to find accurate alignments.
We do this in the context of the task illustrated in Figure 1.
Here, we are given an expression profile as a query, and we want to identify the treatment in the database that has the expression profile most similar to the query.
We construct queries for which we know the correct matching database treatments and their correct alignments.
The data we use comes from the EDGE toxicology database (Hayes et al., 2005), and can be downloaded from http://edge.oncology.wisc.edu/.
Our dataset consists of 216 unique observations of microarray data, each of which represents the expression values for 1600 different genes.1 Each of these expression values is calculated by taking the average expression level from four treated animals, divided by the average level measured in four control animals.
The data are then converted to a logarithmic scale, so that an expression value of 0.0 corresponds to the average basal level observed in the control animals.
Each observation is associated with a treatment and a time point.
The treatment refers to the chemical to which the animals were exposed and its dosage.
The time point indicates the number of hours elapsed since exposure occurred.
Times range from 6 h up to 96 h. The data used in our computational experiments span 11 different treatments, and for each treatment there are observations taken from at least three different time points.
Additionally we can assume that for all treatments, there exists an implicit observation at time zero.
This is the time at which the treatment was applied, so all expression values are assumed to be at the basal level.
We assemble queries by randomly sub-sampling time series in our dataset.
We assemble 10 such queries from each treatment.
We build each query by first selecting the number of observations to be in it, then choosing which time points will be represented, and finally picking an observation for each of these time points.
The query sizes are chosen from a uniform distribution that ranges from 1Technically, the expression measurements correspond to clones selected from liver-derived EST and full-length cDNAs.
These clones represent products for 1600 unique genes.
one up to the number of observed times in the given treatment.
The maximum size of a query is eight, although most consist of four or fewer observations.
The time points are chosen uniformly as are the observations for each chosen time.
To test the ability of our approach to find accurate alignments in situations that require warping, we also assemble cases in which we distort the query time series temporally.
We use three different distortions.
The first one doubles all times in the first 48 h (i.e.
it stretches the first part of the series), and then halves all times (plus an offset for the doubling) for the next 24 h. The second distortion halves for the first 36 h and then doubles for 60 h. The third one triples for the first 60 h and then thirds for another 20 h. It should be noted that not all the treatment observations extend this long in time.
The short ones (e.g.
those for which we only have measurements up to 24 or 48 h) will thus not be distorted as much as the long ones.
We then classify and align the query using all the other observations as the database.
We preprocess both the query and the 11 database treatments using B-splines (Rogers and Adams, 1989) to reconstruct pseudo-observations at every 4 h (starting at time zero, when all expression values are at the basal level).
We then align the query against all 11 treatments using our method.
We return the database treatment with the highest scoring alignment, as defined by Equation (8).
Because the alignment also maps each query time to a database treatment time, we can find the temporal error for any query time point.
We then measure how accurately we are able to (i) identify the treatment from which each query series was extracted, and (ii) align the query points to their actual time points in the treatment.
We refer to the former as treatment accuracy and the latter as alignment accuracy.
We consider several other alignment methods as baselines.
The first is COW (Nielsen et al., 1998), as described in Section 2.
The second is a generative method we previously developed (Smith et al., 2008), which we refer to as Generative Multisegment.
Like SCOW, it finds alignments which consist of multiple segments each of which can have different warping parameters.
However, the Generative Multisegment scoring function is based on a generative, probabilistic model, rather than correlation.
Further it performs a complete search for the best segments to use, rather than using the heuristic search of SCOW.
The next baseline we consider is traditional Euclidean dynamic time warping (Sakoe and Chiba, 1978; Sankoff and Kruskal, 1983).
Briefly, this method computes alignments by creating a matrix with elements defined recursively as i,j=D(di,qj)+min [ predDTW(i,j) ] (11) where D(di,qj) is the Euclidean distance between points di and qj in the two series and predDTW(i,j) refers to the matrix elements adjacent to i,j with both indices less than or equal to i and j, respectively.
The first element 0,0 is just the Euclidean distance at time 0, and each other element i,j is the score of warping d from times 0 to i and q from 0 to j.
We then create a normalized score matrix where i,j=i,j/ |i|2+| j|2.
(12) This makes it reasonable to compare warpings with different treatments, where one or the other dimension has been shorted.
i123 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i124 i119i127 A.A.Smith et al.
Fig.4.
Treatment and alignment accuracies when there is no temporal distortion (A), and when there is (B).
The top lines represent treatment accuracy, while the bottom two lines add the criterion that the predicted times are within 24 h and 12 h, respectively, of the actual time, on average.
For each alignment method, we show results when splines of various orders are used to interpolate the time series before alignments are calculated.
Highlights represent cases in which there is a significant difference in accuracy from the corresponding SCOW case (P0.05 with McNemars 2-test).
Finally, we consider linear parametric warping.
This is similar to the method explored by Bar-Joseph et al.
(2003), except that we make the assumption that the series are aligned at time zero.
To find an alignment, we search possible slopes of the alignment line, and return the slope that results in the least average Euclidean distance between the query and the given database treatment.
For these experiments, SCOW, COW and Generative Multisegment use three segments in their alignments, and we set s and a = 10.
Using more segments and setting s and a to other values yields substantially similar results.
The results of this experiment are shown in Figure 4.
Figure 4A and B shows results for the queries without distortion and results for the distorted queries, respectively.
For each method, the top line represents treatment accuracy with different orders of splines, the middle line represents alignment accuracy by adding the criterion that the average time error in the mapping is less than or equal to 24 h, and the bottom line shows alignment accuracy where this tolerance is decreased to 12 h. Highlighted boxes denote points that are significantly different from the corresponding SCOW point, as determined by McNemars 2-test.
There are several interesting conclusions we can draw from these results.
First, it is clear that the multi-segment alignments computed by SCOW, COW and Generative Multisegment are superior to the alignments determined by ordinary dynamic time warping and the linear alignment method.
Second, SCOW finds more accurate alignments than the other two multi-segment algorithms, COW and Generative Multisegment.
Based on these results, we conclude that SCOW is a state-of-the-art alignment method for gene-expression time series, and we therefore use it as the core alignment method for our clustered alignment approach.
3.2 Clustered alignment experiments In our second set of experiments, we are interested in testing the ability of our clustered alignment algorithm to identify sets of genes that should share a common alignment.
We first conduct an experiment designed to determine if our clustered alignment method Fig.5.
Treatment and alignment accuracies, varying by the number of clusters when using SCOW.
In the final case (1600), we warp every gene separately.
Highlighted points are significantly different from the unclustered case, (P 0.05 under McNemars 2-test).
is able to find more accurate alignments when there are sets of genes that have different, known correct alignments.
This experiment is similar to the one in the previous sectionwe use the same data and substantially the same methodology.
The difference is that we simultaneously apply five different temporal distortions to every query: each one is applied to 1/5 of the genes.
We then run our clustered alignment method, in conjunction with SCOW, on the data, allowing the number of clusters k to range from one (i.e.
unclustered, ordinary SCOW) to 10.
We also run the experiment with k=1600, which warps every gene separately.
The results for queries containing three or more observations are shown in Figure 5.
These results show the value of the clustered alignment approach with this dataset.
The accuracy of the alignments i124 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i125 i119i127 Clustered alignments increases as k increases, until about k=4.
After this point, there is a slight degradation in accuracy.
For almost all values of k tested, however, the treatment and the 24 h alignment accuracies are greater with the clustered alignment method than with ordinary SCOW.
With queries containing fewer than three observations, the clustered alignment method actually results in somewhat less accurate alignments than the non-clustered method (i.e.
ordinary SCOW).
These results can be explained by a bias-variance trade-off (Geman et al., 1992).
The gene-expression data we use (like most expression time series) is sparse in time, and prone to noise (because of both technical limitations and biological variability among the animals).
The sparsity and noise mean that it is difficult to compute accurate single-gene alignments.
Aggregating genes into clusters has a regularization effect as this alignment error is averaged out (Bar-Joseph et al., 2003).
The more genes there are in a cluster, the greater the regularization effect.
Thus we want to find the ideal trade-off between the high-bias approach of few clusters (or one cluster, in the limit), and the high-variance approach of many clusters.
The variance component of the error is more significant in the case when the queries are short.
We can conclude, however, that the clustered alignment approach demonstrates good predictive value for moderately sized queries and a range of values of k. In our second experiment, we are interested in identifying sets of genes that are distorted in similar ways in a knockout experiment focusing on circadian rhythms.
Mop3 is a transcription factor in hepatocytes (Bunger et al., 2000, 2005) that is a positive regulator of circadian rhythm and activates the transcription of genes such as Per1 and Tim.
There are two sets of mice in this experiment.
The control group has a functional Mop3 gene, while the knockout group does not.
This is a time-course study based on Zt which stands for Zeitgeiber timethe number of hours after exposure to light begins.
Before Zt0, the mice are kept in darkness for a period.
At Zt0 the lights turn on, and at Zt12 they turn off again.
At intervals of 4 h from Zt0 to Zt20, three mice from each group are sacrificed, and microarrays are derived from pooled RNA samples from the livers of each set of mice.
In all, 27 962 genes are measured.
We interpolate the series with B-splines so that we can sample measurements every 2 h. When aligning the control and knockout time series, we want to allow phase shifting.
That is, we want to allow alignments of the two time series are not necessarily aligned at Zt0.
In our previous experiment, it was reasonable to assume that the expression responses were all identical at time zero.
We cannot make that assumption in this case, however.
We modify SCOW to allow phase shifting by first concatenating the control time series with itself, to obtain 2 days worth of data.
When computing alignments, we allow the control series to short at both ends by redefining the initialization [Equation (1)] of d : 0,x=0.
(13) However, we disallow the alignment from shorting the knockout series, at either end, by using Equation (4) to score q.
Thus, all knockout series times must be mapped to some time in the control series, but the zero times need not correspond.
Figure 6 shows alignments for several genes in each cluster, as determined by our clustered alignment algorithm.
Here, we set the number of clusters k=5.
Each panel represents one of the clusters, and within each one we show the three genes with the highest relative scores for that cluster.
The white alignment path in each plot represents the consensus alignment, when all genes are warped as a unit.
The black alignment path represents the clusters individual alignment.
Note that we only show 1 day in the control dimension rather than 2 days.
The alignments in panels C, D and E, all extend into 2 days.
This is shown by a break in the black alignment path, as it wraps back to the left side and the beginning of the second day.
The clustered alignment allows us to uncover sets of genes that are disrupted in a similar manner by the knockout, even when their expression profiles are quite different.
It is clear that the clustered alignments align the series better than the consensus alignment.
Peaks and valleys in the expression data line up well for the black cluster alignment paths, whereas they often do not for the white consensus ones.
For example, the genes in panel E have undergone a large phase shift.
The consensus path often matches segments with quite different expression profiles, whereas the cluster path shifts the starting point by 12 h and achieves good agreement.
In panel D, the genes appear to be acting more quickly in the knockout mice, while the consensus alignment would indicate they are acting more slowly.
It should also be noted that often the genes within a cluster have very different expression profiles.
Consider panel D, in which the profiles for the three genes are all quite different, but the mapping between control and knockout is similar.
This effect illustrates the advantage of clustering alignments in contrast to clustering the expression profiles directly.
4 CONCLUSION Alignment algorithms provide a valuable approach for gaining biological understanding from gene-expression time series.Avariety of methods have been employed for such analyses, including dynamic time warping, linear alignment algorithms and multi-segment alignment methods.
We have presented new methods which advance the state of the art in two ways.
Most importantly, we have developed an algorithm which is able to compute clustered alignments.
This algorithm relaxes the assumption, common to previous work in expression time-series alignment, that all genes should be warped in the same way.
Instead, our method identifies sets of genes that share a common alignment.
It does this by simultaneously clustering genes and computing a shared alignment for the genes in each cluster.
The second contribution introduced here is a new multi-segment alignment method, called SCOW, that features the ability to calculate shorted alignments, a correlation-based scoring function, and an efficient dynamic programming algorithm for computing alignments.
The results of our empirical evaluation indicate that both the clustered alignment approach and SCOW improve the accuracy of alignments computed with sparse time series from a toxicogenomics dataset.
Additionally, we applied our clustered alignment approach to a dataset involving a conditional Mop3 knockout in mouse liver.
This analysis illustrates the power of the clustered alignment approach to find sets of genes that share similar temporal distortions.
Funding: National Institutes of Health/NIEHS (grant R01-ES012752); National Institutes of Health/NLM (grant R01-LM07050); National Institutes of Health/NCI (grants P30-CA014520 and T32-CA009135).
i125 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i126 i119i127 A.A.Smith et al.
Fig.6.
Alignment clusters found by our method for the Mop3-knockout circadian data.
Each panel shows the top three genes for a different cluster.
The white alignment paths represent the consensus alignment for all the genes, while the black paths represent the cluster-specific alignments.
i126 [10:00 15/5/2009 Bioinformatics-btp206.tex] Page: i127 i119i127 Clustered alignments Conflict of Interest: none declared.
ABSTRACT Mitochondria must uptake some phospholipids from the endoplasmic reticulum (ER) for the biogenesis of their membranes.
They convert one of these lipids, phosphatidylserine, to phosphatidylethanolamine, which can be re-exported via the ER to all other cellular membranes.
The mechanisms underlying these exchanges between ER and mitochondria are poorly understood.
Recently, a complex termed ERmitochondria encounter structure (ERMES) was shown to be necessary for phospholipid exchange in budding yeast.
However, it is unclear whether this complex is merely an inter-organelle tether or also the transporter.
ERMES consists of four proteins: Mdm10, Mdm34 (Mmm2), Mdm12 and Mmm1, three of which contain the uncharacterized SMP domain common to a number of eukaryotic membrane-associated proteins.
Here, we show that the SMP domain belongs to the TULIP superfamily of lipid/hydrophobic ligand-binding domains comprising members of known structure.
This relationship suggests that the SMP domains of the ERMES complex mediate lipid exchange between ER and mitochondria.
Contact: andrei.lupas@tuebingen.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.
Received on February 15, 2010; revised on June 5, 2010; accepted on June 7, 2010 1 INTRODUCTION Mitochondria are organelles of endosymbiotic origin, found in virtually all eukaryotic organisms.
They are the main generators of adenosine triphosphate (ATP)the energy currency of the cell.
Additionally, they participate in a series of other important processes, including apoptosis, amino acid and lipid metabolism, ironsulphur cluster assembly and the regulation of calcium levels within the cell (Lill and Kispal, 2000; McBride et al., 2006).
However, only a small fraction of the biopolymers required to carry out these functions is synthesized in the mitochondria, the rest must be imported from the outside.
For example, they only produce some of the phospholipids that make up their membranes, whereas the remainder originates from the endoplasmic reticulum (ER).
Interestingly, mitochondria not only import phospholipids but also export a particular one, phosphatidylethanolamine (PtdEtn), to the ER, where To whom correspondence should be addressed.
Fig.1.
Domain organization of the four ERMES proteins.
All ERMES proteins, except the mitochondrial outer membrane protein (OMP) Mdm10, contain an SMP domain.
The SMP domain in Mdm34 was discovered in this study.
it is methylated to form the phospholipid phosphatidylcholine (Voelker, 2003).
Mitochondria synthesize PtdEtn by decarboxylating phosphatidylserine, a phospholipid imported from the ER.
The mechanisms responsible for the influx and efflux of phospholipids are unclear.
Unlike most organelles, mitochondria do not exchange phospholipids via vesicular transport.
Previous studies have suggested that this exchange takes place via ERmitochondria associations (Achleitner et al., 1999; Voelker, 2003).
More recently, Kornmann et al.
(2009) identified a complex, the ERmitochondria encounter structure (ERMES), that acts as a molecular tether between ER and mitochondria in Saccharomyces cerevisiae and is required for efficient inter-organelle phospholipid exchange.
However, it remained unclear whether ERMES merely tethers these organelles together, thereby aligning proteins that carry out the actual transport, or also recruits and transfers phospholipids itself.
ERMES comprises four proteins (Fig.1): the mitochondrial outer membrane protein Mdm10, the putative outer membrane protein Mdm34 (Mmm2), the ER-resident Mmm1 and the cytosolic Mdm12 (Kornmann et al., 2009).
These proteins have also been implicated in other mitochondrial functions, including morphology maintenance (Okamoto and Shaw, 2005) and protein import (Meisinger et al., 2004).
Two of the ERMES components, Mmm1 and Mdm12, were reported to contain the uncharacterized SMP domain (synaptotagmin-like, mitochondrial and lipid-binding proteins), which is also present in a number of other eukaryotic membrane-associated proteins (Lee and Hong, 2006).
SMP domain-containing proteins have been classified into four broad groups: C2 domain synaptotagmin-like, PH domain-containing HT-008, PDZK8 and mitochondrial protein families (Lee and Hong, 2006).
The functions of these proteins are poorly understood.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:24 23/7/2010 Bioinformatics-btq326.tex] Page: 1928 19271931 K.O.Kopec et al.
Fig.2.
Pairwise HMM comparison of SMP and TULIP domains.
Representatives of the four SMP domain-containing groups and of TULIP domains were chosen from Arabidopsis thaliana (At), Caenorhabditis elegans (Ce), Drosophila melanogaster (Dm), Dermatophagoides pteronyssinus (Dp), Epiphyas postvittana (Ep), Galleria mellonella (Gm), Homo sapiens (Hs) and Saccharomyces cerevisiae (Sc).
Group and protein names of these representatives, along with domain boundaries and names of source species are indicated (from left to right).
HHpred was used to perform pairwise HMM comparisons between them.
Cell color indicates the HHpred probability of the match as depicted in the scale on the right; probabilities <20% are shown as white cells.
Proteins with known structures are marked with an asterisk.
The abbreviations of the hydrophobic ligand binders are explained in the text.
In this study, we show that the SMP domain belongs to a superfamily of lipid/hydrophobic ligand-binding domains of known structure, which we call TULIP for tubular lipid-binding proteins, and propose a role for it in cellular phospholipid traffic.
2 METHODS All sequence similarity searches were carried out in the MPI bioinformatics toolkit (http://toolkit.tuebingen.mpg.de; Biegert et al., 2006) using HHpred (Sding et al., 2005) and HHsenser (Sding et al., 2006) with default settings.
HHpred searches were performed against a database comprising PDB70 (protein databank structures, as available on the 15th of April 2010, clustered at 70% sequence identity) and genomes of phylogenetically diverse organisms (Arabidopsis thaliana, Caenorhabditis elegans, Drosophila melanogaster, Homo sapiens and S.cerevisiae).
Representatives of the four SMP domain-containing groups from the aforementioned organisms were chosen as seeds for the searches in Figures 2 and 3, based on their presence in the core of their respective clusters in the sequence cluster map (Fig.5).
To identify sequences for cluster analysis, we searched the non-redundant protein sequence database (nr) at NCBI for homologs of the SMP domain from the yeast protein Mmm1 (residues 196409), the N-terminal domain of human cholesteryl ester transfer protein (2OBD, residues 16206), the Takeout 1 protein from Epiphyas postvittana (3E8T), and the dust mite allergen Der p 7 (3H4Z) using HHsenser.
We pooled the permissive sets returned by HHsenser to obtain 2033 sequences, which we clustered by their pairwise BLAST P-values (Altschul et al., 1990) in CLANS (Frickey and Lupas, 2004).
Clustering was done to equilibrium in 2D at a P-value cutoff of e-4 using default settings.
3 RESULTS The number of structural solutions available to a polypeptide chain is limited, making protein structures multiply convergent (Cheng Fig.3.
Fold predictions for SMP domains.
The highest scoring PDB matches for the 16 representative SMP sequences in Figure 2 were collected with three of the top-scoring prediction servers in CASP8.
Top matches to TULIP domains are shown in blue and to any other structure in red.
The color saturation is scaled linearly between the maximum and minimum scores returned by the respective method.
The value ranges corresponding to pale (low confidence), medium and dark (high confidence) saturation are: Phyre-estimated precision 033, 3466 and 67100, MULTICOM e-value 7.4 5, 5.12.6, 2.50, MUSTER Z-score 01.8, 1.93.5, 3.65.3.
The number of matches to TULIP domains against the total is shown in the right-hand column.
et al., 2008; Krishna and Grishin, 2004; Salem et al., 1999), while the combinatorial possibilities in sequence space are nearly endless.
For this reason, sequence similarity is considered the primary marker of homology.
We thus used sensitive sequence comparisons, as implemented in HHpred, to find homologs of the SMP domain in a database concatenating several complete genomes with PDB70 (see Methods section).
The search was seeded with the SMP domain from Mmm1.
The best hits were to other proteins that have previously been described to contain this domain (Fig.2).
In addition, we detected a hitherto unknown SMP domain in the ERMES protein Mdm34.
This protein has been reported as an 1928 [17:24 23/7/2010 Bioinformatics-btq326.tex] Page: 1929 19271931 Homology of SMP domains to the TULIP superfamily integral outer membrane protein (Youngman et al., 2004), but we were unable to identify any sequence motifs in it that would indicate membrane insertion.
HHpred searches with other representatives and reciprocal searches with Mdm34 confirmed the presence of an SMP domain, raising the number of SMP domains in ERMES to three (Fig.1).
We also found statistically significant matches to many eukaryotic proteins from the bactericidal/permeability-increasing protein-like (BPI-like) family (Fig.2), including two with known structures: BPI (1EWF) and cholesteryl ester transfer protein (CETP; 2OBD).
Other members of this family are lipopolysaccharide-binding protein (LPSBP), lipid-binding serum glycoprotein (LBSGP), phospholipid transfer protein (PLTP) and long and short paralogs of palate, lung and nasal epithelium carcinoma-associated protein (PLUNC).
Some of these proteins have been shown to bind lipids, e.g.
CETP facilitates lipid transport between different lipoproteins (Qiu et al., 2007).
BPI and CETP have similar structures, each containing two tandem domains that adopt the same fold, comprising a long-helix wrapped in a highly curved anti-parallel-sheet.
All BPI-like proteins contain these two domains, the only exception being short PLUNC, which has only one.
The domains show little sequence identity (<15%) and sequence comparisons do not yield significant matches between them.
Instead, the C-terminal domain only shows matches to the Aha1 protein, a co-chaperone of Hsp90 in eukaryotes which shares the same fold (1USU; d.83.2).
Nevertheless, the N-and C-terminal domains of BPI-like proteins are thought to have a common ancestry based on their structural similarity (Kleiger et al., 2000).
The Structural Classification of Proteins database (SCOP; Murzin et al., 1995) also considers them to be homologous and classifies them into the same family (d.83.1.1).
HHpred searches with SMP domains yielded many statistically significant matches to the N-terminal domain of BPI-like proteins (Fig.2), but not to the C-terminal domain.
We confirmed these findings with reciprocal searches using both domains of BPI-like proteins.
From the statistical significance of these matches, we conclude that SMP domains are homologous to BPI-like proteins and therefore predict that they share the same tubular fold and lipid-binding properties.
Further searches with the N-terminal domain of BPI-like proteins retrieved three more proteins of known structure: dust mite allergen Der p 7 (3H4Z), a juvenile hormone-binding protein from Galleria mellonella (JHBP, 2RCK), and a Takeout 1 protein from Epiphyas postvittana (3E8T).
These proteins are exclusively found in arthropods and are involved in binding hydrophobic ligands.
They are composed of a single domain homologous to the N-terminal domain of BPI-like proteins (Supplementary Fig.S1), a relationship that has been described previously (Hamiaux et al., 2009; Kolodziejczyk et al., 2008; Mueller et al., 2010).
In view of their similarities in sequence and structure, we propose to group the arthropod proteins together with the BPI-like family into the TULIP superfamily.
To confirm the membership of SMP domains in the TULIP superfamily, we generated fold predictions for 16 representative sequences using the servers Phyre (Kelley and Sternberg, 2009), MULTICOM (Wang et al., 2010) and MUSTER (Wu and Zhang, 2008), all of which performed very well in the most recent Critical Assessment of Structure Prediction Experiment, CASP8 (Kryshtafovych et al., 2009).
All three methods yielded many highest-scoring matches to TULIP domains (Fig.3 and Supplementary Fig.S2).
Additionally, we queried the fold prediction metaserver I-TASSER (Roy et al., 2010), which was the top performing server in CASP 7 & 8 (Zhang, 2007, 2009).
This server returned a TULIP domain as the top match for 10 of 16 queries and as one of the top three matches for all but one query (Supplementary Fig.S3).
These matches included both BPI-like and Takeout-like proteins.
A structure-assisted multiple sequence alignment of SMP domains to TULIP domains of known structure highlights the basis for these matches (Fig.4).
All sequences have similar length, distribution of (predicted) secondary structure, and pattern of hydrophobic residues.
However, there are no conserved sequence motifs, unsurprisingly as such motifs are not even detectable within individual families (Beamer et al., 1997; Kolodziejczyk et al., 2008).
To explore the relative positions in sequence space of proteins of the TULIP superfamily, we searched for homologs of SMP domains, N-terminal domains of BPI-like proteins, as well as allergens and Takeout proteins in the nr database using HHsenser, and clustered the obtained sequences in CLANS (see Methods section).
The resulting cluster map (Fig.5) shows three distinct but connected regions corresponding to SMP, BPI and Takeout-like domain families, confirming the proposed homology between them.
In addition to the SMP groups described by Lee and Hong (2006) and the group of Mdm34 proteins described in this article, the clustering revealed a further SMP group, the uncharacterized transmembrane 24 proteins.
It also yielded a number of additional groups of BPI-like proteins, including the expression site-associated gene 5 proteins (ESAG5) from trypanosomes, whose homology to BPI-like proteins has been reported previously (Barker et al., 2008).
BPI and Takeout-like domains are connected by the arthropod allergens, one form of which is unique in containing tandem domains with clear sequence similarity, indicating a domain duplication that occurred in insects (yellow and orange clusters in Fig.5).
4 CONCLUSIONS In this study, we have shown that the SMP domain belongs to the TULIP domain superfamily, a large group of proteins that bind lipids and other hydrophobic ligands within a central, tubular cavity (Fig.6).
In several cases (CETP, PLTP), members of this superfamily are known to exploit this binding activity in order to mediate lipid trafficking.
Given the extensive lipid exchange between the ER and the mitochondrial outer membrane and the location of the ERMES complex as a connector between them, it is attractive to consider that this exchange is mediated by the SMP domains of the ERMES subunits.
As the ERMES complex does not include a nucleotidase that could energize this process, we propose that it proceeds along an affinity gradient, amounting to facilitated diffusion.
Although this could be envisaged as resulting from many short, structurally unspecific contacts between the SMP domains (kiss-and-run mechanism), we prefer to consider that the domains assemble into structurally well-defined complexes, which establish a lipophilic, tubular path between the two membranes.
Since the stoichiometry of subunits within the ERMES complex is currently unknown, it is however not possible at this time to judge on whether 1 : 1 : 1 or some other ratio would most appropriately describe the composition of such complexes.
1929 [17:24 23/7/2010 Bioinformatics-btq326.tex] Page: 1930 19271931 K.O.Kopec et al.
Fig.4.
Multiple sequence alignment of TULIP domains.
An alignment comprising representatives of the SMP domain family and TULIP domains of known structure is shown.
Sequences are labeled as in Figure 2.
The alignment was generated by a three-step approach.
First, a multiple alignment of SMP sequences was obtained using HHpred in local maximum accuracy (MAC) alignment mode.
Second, a structure-based sequence alignment of TULIP domain structures was derived from a multiple structure superimposition calculated using MAMMOTH-mult (Lupyan et al., 2005).
In the final step, these two alignments were merged manually using as guide an alignment between 1EWF and Mmm1 obtained with HHpred.
The-helices are shown in red and-strands in blue.
Secondary structure predictions for the SMP domains were performed with Ali2D (Biegert et al., 2006).
Numbers in parentheses represent length of omitted segments.
Positions in the alignments that are highly conserved or strongly hydrophobic are shown in boldface.
Residues that could not be aligned in structure or sequence are shown in lower case.
Fig.5.
Cluster map of the TULIP domain superfamily.
We searched for relatives of SMP domains, Takeout proteins, dust mite allergens and N-terminal domains of BPI-like proteins in the non-redundant database using HHsenser and clustered them in CLANS based on their all-against-all pairwise similarities as measured by BLAST P-values.
Dots represent sequences.
Sequences within one group are indicated by the same color; sequences that could not be assigned to a group are not colored.
Line coloring reflects BLAST P-values; the darker a line, the lower the P-value.
Protein families containing members with known structure are indicated with an asterisk.
The lipid-binding proteins cluster (LBP) also comprises LPSBP and LPSBP.
Abbreviations are as in the text.
Accession details for representatives of all clusters are provided in Supplementary Table 1.
1930 [17:24 23/7/2010 Bioinformatics-btq326.tex] Page: 1931 19271931 Homology of SMP domains to the TULIP superfamily Fig.6.
View along the ligand-binding tunnel of a Takeout protein (3E8T, residues 5211).
The ligand is shown as red sticks.
Funding: This work was supported by institutional funds from the Max-Planck-Society.
Conflict of Interest: none declared.
ABSTRACT Motivation: Computational approaches for the annotation of pheno-types from image data have shown promising results across many applications, and provide rich and valuable information for studying gene function and interactions.
While data are often available both at high spatial resolution and across multiple time points, phenotypes are frequently annotated independently, for individual time points only.
In particular, for the analysis of developmental gene expression patterns, it is biologically sensible when images across multiple time points are jointly accounted for, such that spatial and temporal dependencies are captured simultaneously.
Methods: We describe a discriminative undirected graphical model to label gene-expression time-series image data, with an efficient training and decoding method based on the junction tree algorithm.
The ap-proach is based on an effective feature selection technique, consisting of a non-parametric sparse Bayesian factor analysis model.
The result is a flexible framework, which can handle large-scale data with noisy incomplete samples, i.e.
it can tolerate data missing from individual time points.
Results: Using the annotation of gene expression patterns across stages of Drosophila embryonic development as an example, we dem-onstrate that our method achieves superior accuracy, gained by jointly annotating phenotype sequences, when compared with previous models that annotate each stage in isolation.
The experimental results on missing data indicate that our joint learning method successfully annotates genes for which no expression data are available for one or more stages.
Contact: uwe.ohler@duke.edu 1 INTRODUCTION The use of high-throughput image acquisition, such as in pheno-typic screens, has been quickly increasing and thus provides a new source of data for computational biologists.
Microscopy of colored or fluorescent probes, followed by imaging, is able to deliver spatial and temporal quantitative phenotype information such as gene expression at high resolution (Busch et al., 2012; Ljosa et al., 2009; Walter et al., 2010).
In addition, expression patterns can be documented and distributed over the internet as a valuable resource to the research community.
Recent advances in throughput, or long-term investment in specific projects, have by now generated large collections of images.
Such image data-bases are traditionally analyzed through direct inspection by human curators; an example is the Berkeley Drosophila Genome Project (BDGP) gene expression pattern database (Tomancak et al., 2002, 2007).
In this dataset, images are as-signed to stage ranges within the 17 embryonic stages defined by developmental features, and annotated collectively in small groups using a controlled vocabulary (CV), i.e.
annotation terms.
This allows researchers to search image databases and compare spatial and temporal embryonic development.
Given the very diverse nature of imaging technology, samples and biological questions, computational approaches have often been tailored to a specific dataset.
For example, the image-based profiling of gene expression patterns via in situ hybridization (ISH) requires the development of accurate and automatic image analysis systems for using such data, to understand regu-latory networks and development of multicellular organisms.
Images are affected by multiple sources of noise due to experi-ments or microscopy (incomplete or multiple embryos, variance of probes across genes, illumination artifacts), making the extrac-tion and registration of embryos non-trivial (Fowlkes et al., 2005, 2008; Harmon et al., 2007; Keranen et al., 2006; Kumar et al., 2002; Mace et al., 2010; Puniyani et al., 2010;).
Peng and Myers (2004) and Peng et al.
(2007) introduced an automatic image annotation framework using various high-dimensional feature representations and classifying frameworks: Principal Component Analysis (PCA), wavelets, Gaussian mixture models, Support Vector Machines (SVM), Quadratic Discrimi-nant Analysis.
Each image may show the embryo under different views: lateral, dorsal or ventral; this is a challenge for gene an-notation, as embryonic structures may be visible in only certain views.
Yet, recent studies have shown that incorporating images from multiple views could consistently improve the annotation accuracy (Ji et al., 2009; Pruteanu-Malinici et al., 2011).
It is desirable to represent images in a way that takes advan-tage of image features and offers robustness to image distortions.
In contrast to such large feature sets prone to high redundancy and high computational costs, Frise et al.
(2010) identified a set of basic expression patterns in Drosophila.
A set of 39 well-defined clusters describing specific regions of embryo expression were determined from 2693 lateral views of early development.
As with the majority of described approaches, this study involved a high level of human intervention in selecting good images for training/testing purposesa potential drawback, considering the rapid increase in the size of ISH image collections.
In contrast, Pruteanu-Malinici et al.
(2011) proposed a new approach for automatic annotation of spatial expression patterns using a vocabulary of basic patterns that involved little to no human intervention.
This work provided a flexible unsupervised frame-work in competitively predicting gene annotation terms, while using only a small set of features.
*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com mailto:uwe.ohler@duke.edu A particular aspect that has been largely neglected by compu-tational approaches so far is that data acquired from such experi-ments often span multiple time points or conditions.
Phenotypes are typically annotated stage-by-stage, without jointly learning the salient temporal dependencies across multiple time points, which should allow for an overall higher accuracy; e.g.
the an-notation terms predicted for earlier stages should inform the prediction at later stages.
Furthermore, many genes are anno-tated with more than one term from the vocabulary, creating an additional dependency structure between annotations within the same stage range.
In this article, we address the automatic annotation of Drosophila embryo gene expression sequences, building on state-of-the-art models from computer vision and machine learn-ing.
There are several challenges that need to be addressed when approaching this problem through computational methods.
As we mentioned previously, the image acquisition process results in embryonic structures with multiple perspectives, shapes and lo-cations.
Moreover, the shape/position of the same embryonic structure may vary from image to image: variation in morph-ology and incomplete knowledge of the shape and position of various embryonic structures have made the gene annotation task more prohibitive (Ji et al., 2008).
We first show that a non-parametric Bayesian factor analysis (BFA) approach, the infinite factor model, allows for an efficient and sparse feature representation of the Drosophila gene expres-sion dataset.
Then, we propose a conditional random field (CRF) to tackle the time-evolving annotation task.
Experiments show that the exploitation of dependencies across adjacent developmental stages leads to annotation accuracy superior to existing Drosophila gene expression annotation approaches.
The proposed framework also tackles the missing data scenario: for many genes, one or more stage ranges are absent from the image collection; in such cases, human annotators would take into ac-count the entire set of expression data from adjacent stages to successfully annotate the available images.
The challenge to auto-matize this process is novel and represents a step closer toward a fully automatic gene annotation pipeline.
These predictions can be later analyzed by biologists to assess the correctness of the image acquisition and the level of interest for that particular gene.
Finally, for a given gene, the described framework predicts the entire set of annotation terms simultaneously, taking full advan-tage of the term dependencies that exist at the stage-range level.
The rest of this article is organized as follows: in Section 2, we focus on data description and introduce the sparse BFA-CRF framework.
Experimental results are reported in Section 3, fol-lowed by conclusions and future work in Section 4.
2 MATERIALS AND METHODS 2.1 Data description One of the most popular large-image expression datasets is the BDGP collection of embryonic expression patterns.
The project started with a first release of images for 2000 genes; the second release was in 2007 with 6000 genes.
Release number 3 came in 2010 bringing the total to 7500 genes, including 97% of the sequence-specific transcription factor genes.
As of today, the collection consists of over 105000 images, which docu-ment patterns of embryonic gene expression for over 7400 of the 13659 protein-coding genes identified in the Drosophila melanogaster genome.
Expression is visualized by RNA ISH, which provides an effective way of locating specific mRNA sequences by hybridizing complementary mRNA-binding oligonucleotides and a suitable dye (Tautz et al., 1989).
The mRNA expression apparent in the captured in situ images was verified by independently derived microarray time-course analysis using Affymetrix GeneChip technology (more details can be found at http://insitu.fruitfly.org, and in Tomancak et al., 2002).
Gene expression pat-terns were documented by taking low (2) and high (20) magnification images, at multiple developmental stages.
The low-magnification digital images were taken to capture groups of embryos, to provide a permanent record of the hybridization in each well.
Each slide was then further examined under higher magnification using a Zeiss Axiophot optical microscope.
Images were assigned to developmental stage ranges follow-ing the sequence of events taking place at specific times after fertilization, using the 17 stages defined in (Campos-Ortega, 1985).
In this analysis, we focused on the first 15 hr ofDrosophila development, spanning embryonic stages 46, 78, 910, 1112 and 1316.
Developmental stages 13 were skipped owing to predominant ubiquitous expression patterns not of interest to our analysis.
Any gene is represented, on average, by approximately 12 images; however, the number of images per gene varies from 1 to 80.
This vari-ability reflects the BDGP strategy to document highly dynamic, complex and novel patterns, while lowering the number of images documenting common expression patterns.
Among those, there are images with non-informative patterns due to poor-quality staining/washing or non-tissue specific expression (maternal or ubiquitous).
Images within the same window can show different patterns due to embryo orientation or the relatively long developmental time spanned by a stage range.
Images are annotated with ontology terms from a CV describing developmental ex-pression patterns (Fig.1).
This vocabulary has been developed and refined by FlyBase (The FlyBase Consortium, 2002) over the past few years, allowing human curators to compare their findings with expression data assembled from the literature, expansion of annotations to greater detail and thorough searches of datasets based on Gene Ontology schema.
The annotations used throughout this project consisted of a subset of about 300 of the 5800 annotation terms in the FlyBase CV, many of which only apply to later stages of development.
As mentioned previously, we use all available images in our approach, i.e.
including those taken with any embryo orientation: lateral, dorsal and ventral.
Before extracting features, we segmented and registered images using a previously described probabilistic segmentation approach based on statistical shape models (Mace et al., 2010).
This provides us with 240 120 pixel images, mostly containing a single embryo in a standard dorsal(up)/anterior(down) orientation and no background.
In Figure 1, we show a particular gene expression pattern across five developmental stage ranges of interest.
The complexity and variability of the image data led to competitive but not perfect results, in terms of precise embryo extraction as well as embryo orientation, which increased the challenge of automatic gene annotation.
We here use the average intensities in a down-sampled fixed grid size of 80 40 pixels as input features for the entire collection of images within the BDGP dataset.
2.2 Feature extractionsparse BFA Sparse Bayesian factor analysis (sBFA) is a statistical method for model-ing many dependent random variables through linear combinations of a few hidden variables (Gorsuch, 1983).
This model is designed to address the high-dimensional setting where hundreds or thousands of genes are simultaneously examined.
The sparsity assumption is the key feature in our model that allows us to scale stable and accurate inference to a large number of images/genes represented by many image input features.
For high-dimensional models, sparsity helps prevent sampling errors from swamping out the true signal in data, leading to stable parameter estimates.
In our framework, sparsity implies that each image/gene is i28 I.Pruteanu-Malinici et al.
affected only by a few underlying estimated factors, and as a result, many of the mixing weights in the model will be (near) zero.
The sparse Bayesian factor model was derived using the following matrix representation: X AS E 1 where X [x1, x2.
.
.
xn] is a p n dimensional data matrix, with n the number of features, quantifying the associated gene-expression values for p images (genes) under consideration.
Each row of X is called a gene pattern with dimension 1 n. Here, we assume that each gene pattern is already normalized to zero mean.
A is the factor loading matrix with dimension pk, which contains the linear weights.
S is the factor matrix with dimension k n, with each element modeled by a standard normal distribution.
Each column of S is the factor score for feature i (i 1, 2, .
.
.
, n) and each row is called a factor.
E is the additive Gaussian noise with dimension p n. Both A and S are inferred by the model simultaneously.
From the model we can see that each row of X is modeled by a linear combination of the factors (rows of S), indicating that the variability of the original p feature patterns can be explained by only k latent factors.
The model can also be written in vector form as follows: xj ajS "j j 1, 2, :::, p 2 where xj and aj denote the j th row of X and A, respectively, and the basis matrix S is shared across all samples.
Indeed, factor analysis is an un-supervised dimensionality reduction method used widely in data analysis and signal processing (Prince et al., 2008).
To impose the sparsity required by the underlying biological assump-tion where spatial gene expression patterns are modeled only by a few domains (factors), we used the Student-t distribution, which consists of a Gaussian distribution and a Gamma prior on the precision parameter.
The sparseness is directly controlled by the precision parameter j,m; the objective of imposing sparseness is to automatically shrink most elements in A near zero.
The updating equations, along with a full description of the sparse factor model used in this manuscript can be found in Pruteanu-Malinici et al.
(2011).
For an extension of our previous work, which largely focused on two developmental stage ranges only, the number of factors (k) for every developmental stage range needed to be determined in an ideally unbiased fashion.
For this, we used an adaptive Gibbs sampler, which automatically truncated the loading and factor matrices through an adaptive selection of the number of important factors.
This sparse Bayesian infinite factor model, first introduced by Bhattacharya and Dunson (2011), obviates the need for pre-specifying the number of factors; the effective number of factors (here denoted by k*) is chosen such that the contribution from adding additional factors is negligible.
This approach has been shown to produce accurate esti-mates of the true effective number of factors k*; the adaptation of the Gibbs sampler occurs every 10 iterations at the beginning of the Markov chain but decreases in frequency exponentially fast, so as to satisfy the diminishing adaptation condition in Theorem 5 of Roberts and Rosenthal (2007).
More specifically, the decreasing frequency is modeled as an exponential pt exp0 1t 3 at the tth Gibbs iteration with 0 and 1 chosen so that adaptation occurs every 10 iterations initially but then decreases in frequency exponentially fast.
The loadings matrix is adaptively modified by monitoring the Fig.1.
Examples of Drosophila embryo ISH images and associated annotation terms (BDGP database) for gene Actn (FBgn0000667), across five developmental stage ranges.
The dark blue stained regions highlight areas where genes are expressed; darker colors correspond to higher gene expression levels i29 Automated annotation of gene expression image sequences columns with all elements within some pre-specified small neighborhood of zero.
For some iterations, columns may be discarded or a new column could be simply added to the matrix; the remaining parameters of the model are modified accordingly.
The parameters of the factors (in the case of adding some) are estimated from their prior distribution to fill in the necessary values.
2.3 Conditional random fields Probabilistic graphical models such as Bayesian networks and random fields are increasingly popular choices for statistical modeling of complex biological relationships.
Although Bayesian networks provide a viable solution for directed acyclic relationships where the direction of causality can be easily identified, undirected graphical models offer a clear advan-tage for highly connected relational structures that are not simple chains or trees.
Among undirected models, CRFs (Lafferty et al., 2001) have proven to be among the most powerful predictors owing to their inher-ently discriminative (rather than generative) nature.
In a CRF, the observable variables (X {Xi}) and unobservable vari-ables (Y {Yi}) are treated separately, with the unobservables globally conditioned on the observables.
The relationships among the unobserv-ables are modeled via an undirected graph G (Y,E), in which the Yis are the nodes (vertices), and edges E YY are pairs (Yi, Yj); the edges serve to denote direct non-independence relations between pairs of Yis.
In particular, a node Yi is taken to be conditionally independent of all other nodes Yj, given the immediate neighbors NG of Yi in the graph: PYijfYk6ig PYijNGYi 4 for NG(Yi) {Yj6ij(Yi, Yj)2E}.
The well-known HammersleyClifford theorem (Hammersley and Clifford, 1971) provides a means of computing conditional densities via decomposition of the graph into cliques.
In particular, PY Xj  1 ZX e P I2cliquesG IIYI,X 5 where YI denotes the nodes in clique I, and Z(X) is a normalizing con-stant; it is assumed that P(Y)40 for all possible joint assignments to Y. I is called the potential function for clique I; in practice, these are often pooled among like-sized cliques.
Because cliques larger than some rea-sonable size N are typically ignored, modeling is accomplished by choos-ing a suitable set {1, 2, .
.
.
N} of potential functions for different clique sizes; the is and any additional parameters of the s can be trained discriminatively via cross validation.
Exact inference with a CRF is tractable if the graph can be converted into a chain or a tree.
To this end, a junction tree can be obtained by collapsing tight clusters of nodes into meta-nodes and extracting a max-imal spanning tree from the resulting structure (Jensen et al., 1990; Lauritzen and Spiegelhalter, 1988).
The sumproduct algorithm (Pearl, 1988) can then be applied to propagate local densities across the tree, permitting exact computation of posterior probabilities for joint or indi-vidual value assignments to nodes in the graph, or identification of the maximum a posteriori assignment; for linear-chain CRFs, these are analo-gous to the well-known ForwardBackward and Viterbi algorithms for hidden Markov models (Rabiner, 1989).
To infer the presence or absence of specific annotation terms for indi-vidual embryo images, we constructed a CRF structured as shown in Figure 2.
Each node Yi denotes the status of an annotation term: Yi 1 means present (the annotation term applies to the image), Yi 0 means absent (the annotation term does not apply).
Columns correspond to developmental stages.
All of the nodes in a column are directly con-nected via an edge to all nodes in adjacent columns (blue lines).
Within a column, the nodes are connected in a linear chain (i.e.
each node has exactly 1 or 2 neighbors within the column), with the order of the chain chosen so as to maximize the total mutual information between all adjacent pairs in the chain; this maximization was carried out via a stand-ard traveling-salesman heuristic in Matlab.
Each column was constrained to include only the most popular annotation terms in the training parti-tion (for more details, see Results).
The sparse image factors (previous section) were included as observables Xi; the Xis were specific to each column, and numbered from k 57 to k 160, depending on develop-mental stage, with later stages having more factors.
We defined potential functions for cliques of size up to 2: 1(Yi, X)1 log P(XjYi), and 2(Yi, Yj, X) 2 log P(Yi, Yj), where P(Yi, Yj) is a multinomial and P(XjYi) is a multivariate Gaussian with diagonal covariance, both trained by simple counts (maximum likeli-hood) from the training partition during cross validation (see Results).
Coefficients 1 and 2 were estimated by maximizing the training-parti-tion classification accuracy via simple hill climbing.
1 we refer to as the node potential, as it is associated with single-node cliques, and 2 we refer to as the edge potential, as it is associated with two-node cliques (individ-ual edges in the graph).
3 RESULTS In this section, we describe the application of a sparse BFA-CRF framework for automatic time-course gene expression pattern annotation.
Our procedure starts by extracting sparse meaning-ful features (sBFA) from the entire collection of Drosophila em-bryos, suitable for downstream temporal analysis based on a conditional-random-fields approach.
We then use the estimated factor loadings as observed variables in the CRF framework, so as to infer most likely annotation terms for previously unseen images.
3.1 Factor inference/decomposition of expression patterns The BDGP collection divides early embryogenesis of Drosophila into six developmental stage ranges, 13, 46, 78, 910, 1112, 1316, and most of the CV terms are stage-range specific.
As mentioned previously, we skipped stage range 13 owing to lack of informative images, as well as a low number of annotation terms associated to it.
We applied the sBFA model to the entire set of images from the five stage ranges of interest.
These spanned thousands of images (Table 1), with each stage being annotated by a set of 40150 annotation terms.
To illustrate the potential of the sBFA for decomposing expression patterns into meaningful features, we show selective estimated factors from developmental stages 910.
The model began with the set of 5929 embryo images and estimated the loadings and factor matrices while having full control of the degree of sparsity (throughout our analysis, the sparsity on the factor loading matrix A was controlled by a scale parameter of the Gamma prior distribution on the precision parameter , h0 106).
Figure 3 illustrates a selection of the estimated factors that, per ensemble, correspond to lateral, dorsal and ventral views, demonstrating the ability of the model to automatically extract distinct patterns for different embryo orientations.
As mentioned in Materials and Methods section, the number of factors was determined in an unsupervised fashion, for every developmental stage range, using the sparse Bayesian infinite factor model.
The estimated number of factors can be found in Table 2; in addition, we compared these findings with an empirically determined esti-mation akin to Pruteanu-Malinici et al., 2011.
As illustrated in Table 2, the range of factors is similar for both scenarios: fully unsupervised (infinite factor models) or estimated by underlying i30 I.Pruteanu-Malinici et al.
biological assumptions (generate and test).
A convergence check on the estimated number of factors for two randomly chosen stage ranges is illustrated in Figure 4.
Similar to the sBFA ana-lysis, the Bayesian infinite factor model was run for 6000 Gibbs iterations, discarding the first 1000 and estimating the model parameters on the remaining 5000 iterations.
The feature extraction/selection process was followed by filter-ing non-informative (such as ubiquitous) gene expression pat-terns.
Using Euclidean distances between estimated sparse factor analysis weights and a null vector as reference, we sepa-rated informative images from the non-informative ones (for a full description see Pruteanu-Malinici et al., 2011).
We success-fully removed 2.69% non-informative images (Table 1).
3.2 Large-scale annotation of time-course expression patterns In evaluating the performance of the sBFA-CRF framework, we used the estimated sparse loadings/features only on the set of genes in common between all five stage ranges of interest and a repertoire of annotation terms from a CV.
The most popular annotation terms were independently selected for each stage range, to cover 85% of the entire set of genes.
This resulted in a set of 1807 images and 48 annotation terms distributed as follows: 14 terms for stage range 46, 8 terms for stage range 78, 9 terms for stage range 910, 9 terms for stage range 1112 and 8 terms for the last stage range 1316 (Table 1).
To assess the relative utility of various parts of our model, we determined the prediction accuracy of the full model compared with versions of the model handicapped in various ways.
In par-ticular, we considered including (in separate experiments) the following sets of edges in the CRF: Relationships across adjacent stage ranges and within stage ranges (between annotation terms): blue and green lines in Figure 2 (full model, scenario A).
Relationships across adjacent stage ranges only: blue lines in Figure 2 (scenario B).
Relationships within stage ranges only: green lines in Figure 2 (baseline, scenario C).
Fig.2.
CRF framework used for the automated annotation of time-course Drosophila embryo ISH images.
Nodes correspond to annotation terms, and edges denote relationships.
The order of the annotation terms within a given stage range was determined using a standard travelingsalesman heuristic in Matlab Table 1.
Statistics of the images from the BDGP database before and after the filtering process BDGP image statistics Stage range 46 Stage range 78 Stage range 910 Stage range 1112 Stage range 1316 Original number of images 9484 5744 5929 13 737 19 784 Number of images after filtering process 8722 5227 5523 13 245 19 269 Number of images shared across 1807 genes in common 6610 4615 4468 9315 11 111 Number of annotation terms 14 8 9 9 8 Note: The annotation terms represent a fraction of the total CV; for any given stage range, they cover 85% of the total number of genes of interest, being frequent enough to show statistical significance.
i31 Automated annotation of gene expression image sequences For example, when images are annotated for individual stage ranges in isolation, the relationships indicated by the edges be-tween adjacent stages are ignored.
Two examples of Drosophila expression pattern images across time are shown in Figure 5: we are interested in modeling the edge potentials between develop-mental stage ranges, as well as those between annotation terms within each stage range.
Annotation accuracy was computed as a global measure, across all annotation terms and stage ranges, by comparing the ground truth (human curated labels) with the sBFA-CRF pre-dictions.
As previous methods had largely focused on the anno-tation of individual stage ranges, one term at a time, and are largely trained on heavily curated benchmark data rather than the whole BDGP dataset, a fair comparison to these approaches was not feasible.
To put our approach into context, we therefore compared the results generated by the sBFA-CRF framework with our own recent binary Support Vector Machines (SVM)-based classification system described in Pruteanu-Malinici et al.
(2011).
We had previously shown that this approach provides competitive and often superior classification results compared with the best competing approaches, even when working with the full BDGP image data instead of cleaned benchmarks.
Our previous method used independent SVM classifiers for each annotation term and stage range, disregarding relationships within and between adjacent stage ranges.
This resulted in lower annotation accuracy, as shown in Table 3.
The SVM results are comparable with the new CRF baseline scenario, which only considers edge potentials between annotation terms within the same stage range (both models simply ignore any temporal/tran-sition information that might improve the overall accuracy).
The advantage of using the edge potentials between adjacent stage ranges translates into an absolute increase of 34% in accuracy (i.e.
a relative reduction of the error rate of420%).
All models were applied to the same set of 1807 images, using 10-fold cross validation; mean values across five runs are shown.
3.3 Missing-data annotation analysis In addition to improved gene annotation accuracy, the sBFA-CRF framework provides an elegant means of annotating images in missing-data scenarios.
During CRF decoding, the most likely configuration of the model (i.e.
values of the unob-servables, Y) is computed using relationships between adjacent stage ranges, as well as within each stage range.
In the case of missing data, the most likely state for a given node Yi with no directly related observables X is estimated entirely through rela-tionships in the random field.
This allows us to infer annotation terms for missing images, which is of utmost importance in scen-arios where, for a given gene, data have been collected for only a subset of the stage ranges.
In evaluating the performance of the sBFA-CRF model in annotating missing data, we manually removed 525% of images from the 1807 gene set, by randomly selecting genes and removing their corresponding images; for missing images, the node potentials were set to 1.
Results are shown in Figure 6, where the model included edge potentials between adjacent stage ranges, as well as within stage ranges (CRF scenario A); we were able to annotate with 80% or better accuracy even for scenarios with 25% missing data.
Remarkably, our model outperforms the SVM classification framework (which had access to full data) even when 15% of the data are withheld from the sBFA-CRF.
Figure 7 illustrates particular cases with genes that are correctly annotated, for stages where their images were missing.
As previ-ously mentioned, this is of particular interest to biologists who require predictions for stages where images have not yet been collected.
Our results confirm that the proposed time-course pipeline leads to highly successful expression pattern classifica-tion, despite the presence of uninformative images, registration errors and missing data in considerable amounts.
Lastly, we compared the sBFA-CRF predicted labels to the human curated ones (the ground truth), so as to identify genes and annotation terms for which the annotations were different but the outcome from our model appeared consistent.
We Fig.3.
Selected factors, estimated from a total of k 80 factors and a grid size of 80 40 (developmental stage range 910).
Different back-ground colors are an artifact and not part of the model Table 2.
Comparison of the number of estimated factors in the BDGP set Method Stage range 46 Stage range 78 Stage range 910 Stage range 1112 Stage range 1316 Generate and test k 60 k 100 k 100 k 150 k 150 Infinite factor models k 57 k 60 k 80 k 140 k 160 Note: First row corresponds to number selection based on biological prior knowledge followed by generate-and-test procedures.
Second row shows the estimated number of factors, fully unsupervised (the infinite BFA).
i32 I.Pruteanu-Malinici et al.
recognize that for the same annotation term, the corresponding regions in different images may have significant variations in visual appearances, which would lead to a difficult manual an-notation task and could sometime generate ambiguous out-comes.
We show three examples for which the sBFA-CRF annotations are opposite from the human curated ones and are likely correct given the full context (Fig.8).
For all three scen-arios, we confirmed our findings with the BDGP human curators.
Arguably, the model was correct in predicting differ-ent annotation terms in the following examples, including gene FBgn0003502, where human curators initially decided that expression in procephalic ectoderm AISN is not detected for stage range 46; however, the sBFA-CRF predicted label, as well as a second careful visual inspection, would suggest the contrary.
In addition, we identified another instance where ven-tral ectoderm anlage should have been annotated for gene FBgn0022073 in stage range 78.
The last scenario (gene FBgn0033988) corresponds to a case where all images are ex-tremely difficult to annotate owing to out-of-focus staining issues or overall noise.
On a second inspection, the model was arguably correct in labeling the annotations for both stage ranges 46 and 910.
4 CONCLUSIONS We have described a novel sBFA-CRF model to automatically annotate Drosophila embryo gene-expression time-course data.
Fig.4.
Convergence of the estimated number of factors for two developmental stage ranges (78 and 1112): 5000 Gibbs iterations Fig.5.
Drosophila embryonic gene expression across six stage ranges.
Images can display different embryo orientations due to the relatively long developmental time spanned by a stage range.
Using the edge potentials between adjacent stage ranges, as well as within stage ranges, translates into a significant increase in annotation accuracy Table 3.
Summary of annotation accuracy Annotation accuracy analysis CRF scenario (A) CRF scenario (B) CRF scenario (C) SVM Mean annotation accuracy 86.75 85.69 82.93 83.32 Note: sBFA-CRF and SVM models: mean annotation accuracy, over 5N-fold cross validation runs (N 10).
Scenario (A) includes relationships between adjacent stage ranges and within stage ranges; scenario (B) considers only relationships between adjacent stage ranges; scenario (C) models only relationships within stage ranges (baseline).
For the SVM model, we used independent classifiers for each annotation term and stage range.
i33 Automated annotation of gene expression image sequences The sparse BFA framework represents an efficient feature selec-tion technique, which automatically determines the feature-space dimension, using a non-parametric implementation.
The learned features are then used as observed variables in the CRF frame-work, so as to infer most likely annotation terms for previously unseen images.
The CRF encodes temporal relationships be-tween adjacent stage ranges throughoutDrosophila development.
By capturing the temporal sequence, the model is able to predict the entire collection of annotation terms in a single run and achieves superior performance when compared with highly com-petitive models that annotate stages in isolation.
In addition to improved annotation accuracy, the experimental results demon-strate the success of the method in handling missing-data scen-arios.
This is extremely useful in real-life scenarios when estimates are needed over the ensemble of annotation terms, with only partial data being collected.
One promising extension to our approach would be to include latent annotation terms in the CRF structure, to account for additional rare annotation terms for which we would not attempt to obtain a prediction.
These latent terms could have limited con-nectivity in the graph, so as to allow large numbers of latencies to be included without compromising decoding efficiency.
Such an extension may well improve prediction accuracy for the primary terms, even if the latent terms are themselves difficult to accur-ately predict (due to paucity of training data for those terms).
It would also increase the flexibility of the resulting model: while we currently select the primary annotation terms manually based on their popularity among genes in a given stage range, a simple threshold on the number of genes being annotated, together with an appropriate means of ranking terms, would allow to automatically partition the primary versus latent sets.
Based on our experience with the BFA-CRF model described here, add-itional work along these lines seems promising.
Finally, we are continuing to develop this approach in close collaboration with biologists so as to suggest outliers or interest-ing patterns during the anticipated expansion of the BDGP col-lection to the whole Drosophila genome.
We plan to incorporate the sBFA-CRF framework into a Fiji plug in (Schindelin et al., Fig.7.
Missing-data gene annotation analysis.
Shaded boxes indicate the stage range for which data were missing (we manually removed those images).
In two cases, the entire set of annotation terms is correctly annotated within the stage range despite the fact that no images were available (third and sixth rows).
In these two scenarios, the CRF used the relationships across adjacent stages, to estimate the most likely configuration.
A selection of the correctly and incorrectly annotated terms for the stage range with missing data are shown in the last two columns Fig.6.
Annotation accuracy results for missing data scenarios.
The accuracy values were computed as global measures, across the entire set of 1807 genes.
For each case, we randomly selected 525% of the complete gene set and removed their corresponding images, so as to simulate missing data scenarios.
The green bar indicates the annotation accuracy for the full dataset scenario (previous analysis); the red bar corresponds to the SVM analysis i34 I.Pruteanu-Malinici et al.
2012)a gene annotation tool that would accurately assign an-notation terms to new/unseen images, in a timely manner.
ACKNOWLEDGEMENTS The authors would like to thank Dr Erwin Frise for his valuable help in data collection, as well as helpful comments throughout the model development and testing.
Funding: National Science Foundation CAREER award [DBI-0953184 to U.O.]
Conflict of Interest: none declared.
ABSTRACT Motivation: RNA-seq techniques provide an unparalleled means for exploring a transcriptome with deep coverage and base pair level resolution.
Various analysis tools have been developed to align and assemble RNA-seq data, such as the widely used TopHat/Cufflinks pipeline.
A common observation is that a sizable fraction of the frag-ments/reads align to multiple locations of the genome.
These multiple alignments pose substantial challenges to existing RNA-seq analysis tools.
Inappropriate treatment may result in reporting spurious ex-pressed genes (false positives) and missing the real expressed genes (false negatives).
Such errors impact the subsequent analysis, such as differential expression analysis.
In our study, we observe that3.5% of transcripts reported by TopHat/Cufflinks pipeline correspond to anno-tated nonfunctional pseudogenes.
Moreover,10.0% of reported tran-scripts are not annotated in the Ensembl database.
These genes could be either novel expressed genes or false discoveries.
Results: We examine the underlying genomic features that lead to multiple alignments and investigate how they generate systematic errors in RNA-seq analysis.
We develop a general tool, GeneScissors, which exploits machine learning techniques guided by biological knowledge to detect and correct spurious transcriptome inference by existing RNA-seq analysis methods.
In our simulated study, Gen-eScissors can predict spurious transcriptome calls owing to misalign-ment with an accuracy close to 90%.
It provides substantial improvement over the widely used TopHat/Cufflinks or MapSplice/ Cufflinks pipelines in both precision and F-measurement.
On real data, GeneScissors reports 53.6% less pseudogenes and 0.97% more expressed and annotated transcripts, when compared with the TopHat/Cufflinks pipeline.
In addition, among the 10.0% unannotated transcripts reported by TopHat/Cufflinks, GeneScissors finds that 416.3% of them are false positives.
Availability: The software can be downloaded at http://csbio.unc.edu/ genescissors/ Contact: weiwang@cs.ucla.edu Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION RNA-seq techniques provide an efficient means for measuring transcriptome data with high resolution and deep coverage (Ozsolak and Milos, 2011).
Millions of short reads sequenced from cDNA provide unique insights into a transcriptome at the nucleotide-level and mitigate many of the limitations of microarray data.
Although there are still many remaining unsolved problems, new discoveries based on RNA-seq analysis ranging from genomic imprinting (Gregg et al., 2010) to differ-ential expression (Anders and Huber, 2010; Trapnell et al., 2012) promise an exciting future.
Current RNA-seq analysis pipelines typically contain two major components: an aligner and an assembler.
An RNA-seq aligner [e.g.
TopHat (Trapnell et al., 2009), SpliceMap (Au et al., 2010) and MapSplice (Wang et al., 2010)] attempts to determine where in the genome a given sequence comes from.
An assembler [e.g.
Cufflinks (Trapnell et al., 2010) and Scripture (Guttman et al., 2010)] addresses the problems of which transcripts are present and estimating their abundances.
Existing RNA-seq pipelines can be divided into two major categories: align-first pipelines and assembly-first pipelines (Ozsolak and Milos, 2011).
Assembly-first pipelines attempt to assemble and quantify the complete transcriptome without a ref-erence.
Several algorithms, such as Trinity (Grabherr et al., 2011) and TransABySS (Robertson et al., 2010), have been developed.
However, alignment to a reference genome is still necessary to interpret the results from an assembly-first pipeline and to relate them to existing knowledge.
The assembly-first pipeline is com-pute-intensive and may require several days to complete.
In align-first pipelines, a high-quality reference genome serves as a scaffold for inferring the source of RNA-seq fragments.
Current alignment approaches are both computationally efficient and easily parallelized.
Thus, the align-first RNA-seq analysis can be finished within hours even on a normal desktop machine.
Therefore, align-first pipelines such as TopHat/Cufflinks (Trapnell et al., 2010, 2012) or MapSplice/Cufflinks (Wang et al., 2010) are generally preferred when a suitable reference genome is available.
1.1 Multiple-alignment problem In this article, we assume that the RNA-seq data are paired-end reads, which are widely used for transcriptome inference.
Our approach can be used for single-end reads as well.
In paired-end RNA-seq data, a fragment is a sub-sequence from an expressed transcript.
High-throughput sequencing provides two reads corresponding to the two ends of the fragment.
If a*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com fragment can be mapped to more than one location in the genome, we say that this fragment has multiple alignments, as showed in Figure 1.
As each fragment originates from one loca-tion in the genome, multiple alignments must be pro-cessed/corrected before subsequent analysis can proceed.
Inappropriate handling of the multiple alignment fragments im-pacts the subsequent analysis and may lead to questionable con-clusions.
For example, the widespread RNA and DNA sequence differences (Li et al., 2011) are suspected to be (at least partially) due to systematic technical errors, including misalignments (Kleinman and Majewski, 2012).
Current RNA-seq analysis pipelines handle the multiple-align-ment problem in both the alignment and assembly steps.
Most existing aligners [e.g.
TopHat (Trapnell et al., 2009)] use a scor-ing system where only the alignments with the best score are kept.
However, a fragment may still have multiple alignments with equally good scores.
In our experiments on real mouse RNA-seq data, we observe that at least 5% fragments have mul-tiple alignments.
The assembler [e.g.
Cufflinks (Trapnell et al., 2010)] either assumes that they contribute equally to each loca-tion or uses a probabilistic model to estimate their contributions based on the abundance of the corresponding transcripts (Li et al., 2010).
1.2 Genomic factors causing multiple alignments In general, multiple alignments are caused by the existence of paralogous sequences within a genome.
Duplicated and repetitive sequences need not be strictly identical.
In this subsection, we discuss genomic factors that may lead to multiple alignments and their impact on RNA-seq analysis.
Retrotransposition and gene duplication are two biological phenomena that generate se-quences with high levels of nucleotide similarity.
Interspersed highly repetitive sequences, such LINEs and SINEs, can be ex-pressed in an autonomous or nonautonomous manner, but they are not our focus.
That leaves us with three major types of gen-omic factors: processed pseudogenes (Balakirev and Ayala, 2003; Vanin, 1985; Zhang et al., 2003), nonprocessed pseudogenes (Hurles, 2004) and repetitive sequences shared by gene families (Hasler et al., 2007; Jurka and Smith, 1988).
Pseudogenes (Harrison et al., 2003; Khelifi et al., 2005) are typically nonfunctional, though some of them may be expressed (Hirotsune et al., 2003).
They can be further categorized in two groups: processed pseudogenes and nonprocessed pseudogenes based on their causes.
Both of them lead to the repetitive gen-omic sequences.
In general, the pseudogenes are nonfunctional and under reduced selection pressure; thus, they typically exhibit a higher mutation rate than the expressed genes from which they originated.
1.2.1 Processed pseudogene A processed pseudogene (Vanin, 1985) is generated when an mRNA is reverse transcribed and reintegrated back to the genome.
The resulting DNA sequence of the processed pseudogene is the concatenated exon sequences from its original transcript.
Because there are no splice junctions in the sequence of the processed pseudogene, it is easier for the current RNA-seq aligners to map the fragments to processed pseudogene than the actual gene from which they are expressed, especially those fragments that cross a splice junction.
Both the unexpressed pseudogene and its corresponding expressed gene may be reported by the assembler if the implementation of the assembler does not consider such cases.
For example, Guttman et al.
(2010) observed that a few highly expressed transcripts may not be able to be fully reconstructed owing to alignment artifacts caused by the processed pseudogenes.
1.2.2 Nonprocessed pseudogene Nonprocessed pseudogenes (Hurles, 2004) are typically caused by a historical gene duplica-tion event, followed by an accumulation of mutations, and an eventual loss of function.
Nonprocessed pseudogenes often share similar exon/intron structures with their originating gene.
From the aligners perspective, fragments can be mapped to either the expressed original gene, or its nonprocessed pseudogene, or both.
Similar to processed pseudogenes, the assembler may report a nonprocessed pseudogene when its corresponding functional genes are expressed.
1.2.3 Repetitive shared sequences Besides pseudogenes, many functional gene families share subsequences that are almost iden-tical to each other.
One repetitive sequence shared by different genes in human genome is Alu (Hasler et al., 2007; Jurka and Smith, 1988).
Consider the case when, among all genes that share the Alu sequence, but only a subset is expressed.
Hence, the aligner will map the fragments originating from the expressed subset to all similar sequences on the genome.
The assembler may report all genes sharing the repetitive sequence as being expressed.
Any of these three biological factors may lead to multiple alignments.
Without proper post-processing, an assembler may report many unexpressed pseudogenes or even random regions as expressed genes, and it may also miss a few highly expressed genes.
Existing RNA-seq analysis pipelines provide heuristics for ad-dressing the multiple alignment problem, however, they do not explicitly consider their genomic causes.
In our study, using mouse RNA-seq data, the transcripts reported by Cufflinks in-clude 3.5% from known pseudogenes and 10% from unan-notated regions.
A quarter of these 13.5% transcripts are likely to be false positives caused by multiple alignments.
Figure 2 shows the pile-up plots of two regions from a mouse genome reported by a current RNA-seq pipeline.
The top one is a gene named Caml3, whereas the bottom one is unknown.
The unknown genes sequence is similar to the sequence of concate-nated exons from Caml3.
Fragments that are uniquely aligned to the unknown gene by the aligner can also be aligned to Caml3.
However, the aligner fails to find the proper alignment because it does not consider all possible alignments crossing splice junctions Read Read Fragment Genome Fragmmentm Alignment Alignment Fig.1.
A fragment with paired end reads that can be aligned to two locations in the genome i292 Z.Zhang et al.
owing to the search complexity.
This collection of evidence indi-cates that the unknown gene is actually an unannotated pro-cessed pseudogene of Caml3.
Therefore, the identification of expressed genes and unex-pressed pseudogenes is a significant confounding factor in RNA-seq analysis.
No existing analysis methods explicitly at-tempt to identify and reassign fragments that are mapped to pseudogenes.
A similar observation was made by ContextMap (Bonfert et al., 2012) that multiple alignments from a RNA-seq aligner could be handled by removing the incorrect alignments based on the context of the alignments.
However, ContextMap simply defines the context as a fixed window around the align-ment on the genome.
It also does not try to rescue any missed alignments.
In contrast, we introduce the concept of fragment attractor, which leverages the results from both an aligner and an assembler to determine the appropriate context for each indi-vidual alignment.
Sharing maps between fragment attractors are built to help discover and restore missed alignments.
In this article, we introduce the GeneScissors pipeline, a com-prehensive approach to address the problem of detecting and correcting those fragments errantly aligned to unexpressed gen-omic regions.
When compared with the standard TopHat/ Cufflinks pipeline, GeneScissors is able to remove 57% pseudo-genes without using any annotation database.
GeneScissors can reduce inference errors in existing analysis pipelines and aid in distinguishing truly unannotated genes from errors.
2 METHODS In this section, we present GeneScissors, a general component that can be applied to any align-first RNA-seq pipeline to detect and correct errors in transcriptome inference owing to fragment misalignments.
In a standard RNA-seq pipeline, the best alignment for a fragment with multiple alignments is determined without considering the surrounding alignments of other fragments.
Such decisions may be premature without considering the other fragments aligned to these regions.
In the GeneScissors pipeline, we first collect all possible alignments for all fragments, and then examine those regions of the genome where multiple alignments map and then consider the other fragments aligned to these regions.
In this way, GeneScissors is able to leverage statistics of fragment distribution and other features of the alignments.
Figure 3 describes the proposed workflow for RNA-seq analysis.
It uses existing aligner and assembler (with minor modifications to keep all possible alignments discovered, details in Section 3.1) to identify regions to which fragments align.
To distinguish from expressed genes, we refer to each such region as a fragment attractor.
Fragments with multiple align-ments link corresponding fragment attractors.
We refer to these frag-ments and their alignments as shared fragments and shared alignments, respectively.
The relationships among linked fragment attractors are defined by their shared fragments.
GeneScissors uses sharing graphs to represent the linked fragment attractors and to discover new fragment alignments.
We create training instances using simulated RNA-seq frag-ments from annotated genes in Ensembl to build a classification model.
Then, on real data, the classification model predicts and removes the fragment attractors that are likely due to misalignments.
Existing assem-bly methods can be applied on the remaining fragment alignments to re-estimate the abundance level of expressed fragment attractors.
We introduce the sharing graph in Section 2.1, a classification model to iden-tify the unexpressed fragment attractors in Section 2.2 and the features extraction method from the sharing graphs in Section 2.3.
2.1 Sharing graph We construct sharing graphs as follows.
Each fragment attractor is rep-resented by a node, and each pair of linked fragment attractors are con-nected by an edge.
Each connected component is called a sharing graph.
For each edge in a sharing graph, we build a position-by-position sharing map between the pair of linked fragment attractors through their shared fragments.
For any fragment f aligned to a fragment attractor g, we first define function f)g, which returns the aligned position in fragment at-tractor g, given a position in fragment f and its inverse function 1g)f, which returns the corresponding position in f (if it exists), given a position in g. For a pair of linked fragment attractors ga and gb and one of their shared fragments f1, position k in f1 may be aligned to position i in fragment attractor ga and position j in gb.
This provides a correspondence between position i in ga and position j in gb by j f1)gb 1ga)f1 i and i f1)ga 1gb)f1 j.
A sharing map can be built between ga and gb through this approach by using all their shared fragments.
It is possible that two shared fragments f1 and f2 map the same position in ga to two different positions in gb, i.e.
f1)gb 1ga)f1 i 6 f2)gb 1 ga)f2 i. Empirically, such cases are rare, and when it happens, we use the majority rule to resolve the conflict.
The region of a fragment attractor that is covered by the sharing map is called the shared region.
In addition to the shared fragments, some other fragments uniquely aligned to the fragment attractor may align to the shared region.
These fragments should have been aligned to the linked fragment attractor too, but the aligner might have failed to recog-nize the alignments owing to the reasons we discussed previously.
Therefore, with the help of the sharing map, we can restore these missed alignments from existing aligners result.
For example, in Figure 4a, we show a sharing graph among three fragment attractors.
The red regions in the bottom row of each fragment attractor are the shared regions.
The red dashed boxes contain the fragments uniquely aligned to one fragment attractor by the aligner but should have been aligned to the linked fragment attractors too.
In Figure 4b, we show more details on how the new alignments of the fragments are established through the sharing map.
This alignment discovery operation needs to be done in both directions for each pair of linked fragment attractors.
In our previ-ous example in Figure 2, the uniquely aligned fragments (between the black curve and the red curve) in the shared regions should have been Fig.2.
Two transcripts reported by Cufflinks.
The top one maps to a known gene named Caml3, and the bottom one does not map to any known gene.
Two transcripts are aligned by their shared fragments in the plot.
Owing to the space limitation, the top figure is truncated, and only shows the region containing shared fragments.
The dashed line indicates the truncated boundary.
The three vertical lines in purple represent three splice junctions in the top transcript i293 GeneScissors aligned to both fragment attractors.
Restoring fragment alignments to multiple positions does not cause inflation in abundance level estimation because transcriptome inference methods such as Cufflinks already con-sider the shared alignments.
This approach enables us to safely rescue fragment alignments missed by an aligner.
2.2 Classification model GeneScissors processes RNA-seq data at the granularity of linked frag-ment attractors.
Because there is no easy way to determine whether a fragment attractor are expressed in real datasets, we build our training model from simulated data and apply it to real data.
We first generate our training set from a simulated population, and each sample is a set of fragments simulated based on a set of selected transcripts from the an-notation database.
(More details are in Section 3.1).
Then, we apply the aligner and the assembler on each sample of the simulated data, build the sharing graphs based on their results and generate training instances from the sharing graphs.
The fragment attractors that cannot be mapped back to the selected transcripts are unexpressed ones.
We use a classification model to infer whether a fragment attractor (hereby referred to as the target fragment attractor gt) is expressed using features of gt and another fragment attractor (hereby referred to as the assistant fragment attractor ga) linked to gt by an edge in the sharing graph.
For every pair of linked fragment attractors, we build two instances.
The instance is labeled ac-cording to whether the target fragment attractor is expressed.
Therefore, one fragment attractor may be the target fragment attractor in multiple instances.
The intuition is that, for an unexpressed target fragment at-tractor, there should always be some instances in which the assistant fragment attractors are expressed.
In such instances, the assistant frag-ment attractor should have less consistent mismatches, longer sequence and lower proportion of shared fragments than the target fragment at-tractor (More details are in Section 2.3, which describes all features we use).
Thus, we can train a binary classification model using these features to identify unexpressed target fragment attractors.
When we apply the model to test data and real data, all target fragment attractors, which are predicted as unexpressed at least once will be removed from the result of the assembler, and the reads that are uniquely aligned to these fragment attractors will be redistributed to the corresponding expressed fragment attractors.
We experimented with support vector machines (SVMs), DecisionTrees and RandomForests as the learning method and found that RandomForests had the best overall performance.
Once the classifier is built, we apply it on test data to evaluate the prediction accuracy and then apply it to real data to predict unexpressed fragment attractors and remove their fragment alignments.
Recall that, for all uniquely aligned fragments in the shared regions of these fragment attractors, we also discover new alignments to their linked fragment attractors using the sharing map.
2.3 Fragment attractor features We extract features from both target fragment attractor gt and assistant fragment attractor ga in each instance.
Each instance contains 14 features, listed in Table 1.
All features except the number of consistent mismatch locations are straightforwardly calculated: features NE and NI are dir-ectly collected from the assemblers output, and NR, MF, MR and CM are calculated by our sharing graph generator.
The use of consistent mismatch count CM, as a feature is motivated by the observation that the pseudogenes usually have higher mutation rate.
The concept of con-sistent mismatch and the method to find consistent mismatch locations across the genome are described in Appendix 1.
The number of exons is helpful in distinguishing processed pseudogenes, which are singletons.
(b) (a) Fig.4.
(a) A sharing graph of three fragment attractors A, B and C. Each solid box represents a pile-up of fragments of a fragment attractor.
Each pair of connected hollow rectangles represents a fragment of paired end reads.
The red fragments are the shared fragments that can be mapped by the aligner to all three fragment attractors.
The bottom row in each box represents the transcript sequence.
The red regions (except the splice junctions in the transcript sequences) are the region to which the shared fragments align.
(b) A sharing map between fragment attractors A and C and the discovered new alignments (shown in dashed rectangles).
These new alignments are rescued from the uniquely aligned fragments in the shared region of one of the two fragment attractors Fig.3.
The workflow of GeneScissors Pipeline.
The traditional RNA-seq analysis pipeline is the path on the left side.
Its alignment and assembly results are used by GeneScissors to infer fragment attractors, build shar-ing graphs and identify all fragment alignments in the genome.
GeneScissors then builds a classification model to detect and remove unexpressed genes i294 Z.Zhang et al.
All the other features are motivated by our observation that the unex-pressed fragment attractors tend to have smaller number of alignment fragment and shorter region than their corresponding expressed ones.
3 RESULTS We first describe a series of modifications made to open-source RNA-seq analysis tools to support GeneScissors.
Then, we de-scribe the various datasets used for evaluation.
We evaluated two standard pipelines that do not use GeneScissors: one using TopHat and the second using MapSplice as an aligner.
We then added GeneScissors to each pipeline, to improve the align-ment results, and we refer to these as GeneScissors(TopHat) and GeneScissors(MapSplice) pipelines.
All four pipelines use Cufflinks as the transcriptome assembler.
3.1 Software GeneScissors uses modified versions of TopHat and Cufflinks and uses components written in C, Python and the BamTools (Barnett et al., 2011) library.
Cuffcompare is used to map the reported genes back to Ensembl annotations and categorize them into three types: annotated normal genes/tran-scripts, annotated pseudogenes and unannotated regions.
3.1.1 Modifications to TopHat and Cufflinks We first present the algorithms used by TopHat and Cufflinks in ranking and reporting alignments and genes and then discuss our modifica-tions to retain all fragment and partial fragment (unpaired reads) alignments.
In TopHat, if the fragment f has multiple alignments x and y, TopHat retains only alignment y and does not report alignment x, when one of the following conditions is satisfied (tests are applied in order): Mismatch rule: x has more mismatches than y. Splice junction rule: x crosses more splice junctions than y.
Other rules: Owing to the space limitations, we omit the conditions that are not relevant to the article.
Only alignments with the best score are reported by TopHat.
We observed that the splice-junction rule tends to favor pro-cessed pseudogenes; the correct alignment of a fragment with a splice junction is frequently discarded by TopHat if the fragment can be aligned to a processed pseudogene with the same number of mismatches.
In Cufflinks, a gene that meets the following criteria is suppressed: 75% rule: More than 75% of the fragment alignments sup-porting the gene are mappable to multiple genomic loci.
Consider the example shown in Figure 2.
Cufflinks fails to remove the unannotated pseudogene, which is composed mostly of uniquely aligned fragments.
This suggests that the 75% rule is insufficient.
Therefore, in the GeneScissors pipeline, we disabled the splice junction rule in TopHat and the 75% rule in Cufflinks.
3.1.2 Simulator To generate training data for our classification model and evaluate the effectiveness of GeneScissors for detect-ing and removing unexpressed fragment attractors, we built a RNA-seq simulator to provide a ground truth model for fragment attractors.
The simulator randomly chooses a (user-specified) number of genes, and for each gene, it samples a subset of its transcripts.
Then, it uniformly samples paired-end fragments up to a certain abundance level for each selected transcript.
For each fragment, it assigns a quality score to each base pair, drawing from an empirical distribution derived from real data, and generates base pair errors based on their quality scores.
3.2 Data Our study used inbred and F1 crosses of three mouse strains: CAST/EiJ, PWK/PhJ and WSB/EiJ.
To minimize the impact of unknown SNPs to the alignments, we generated strain-specific genomes by incorporating high-confidence SNPs detected in a recent DNA sequencing project of laboratory mouse strains con-ducted by the Welcome Trust (Keane et al., 2011) into the mm9 reference genome.
We used the Ensembl database (build 63) (Flicek et al., 2011) to annotate and evaluate the results from real and simulated data.
3.2.1 Simulated Data A RNA-seq simulator was used to gen-erate synthetic data from 60 RNA-seq samples also derived from three inbred mouse strains: CAST/EiJ, PWK/PhJ and WSB/EiJ.
In each sample, we selected 13, 000 annotated functional genes in Ensembl as the expressed genes and randomly set them to different levels of abundance.
Many genes included multiple transcripts.
We generated 10 million fragments with 100 base pair paired-end reads for each sample.
We used TopHat and Table 1.
The features used for detecting fragment attractors resulting from misalignments Features Description NEga 1, NEgt 1 NEga andNEgt are the observed numbers of exons.
These two Boolean features tell whether the genes are singleton of exons.
NRga, NRgt, NRga=NRgt NRga, NRgt are the proportions of the fragments that can be aligned to ga and gt to the total fragments, respectively.
MFga, MFgt, MFga=MFgt MFga, MFgt are the proportions of the shared fragments to the fragments aligned ga and gt, respectively.
MRga, MRgt, MRga=MRgt MRga, MRgt are the proportions of the entire regions of ga and gt that are covered by shared fragments.
CMga, CMgt, CMga CMgt CMga, CMgt are the numbers of base pairs that have consistent mismatches in the shared regions of ga and gt, respectively.
i295 GeneScissors MapSplice as aligners and Cufflinks as the assembler to analyze the simulated data.
More than 7.5% of the genes reported in the results were not from the selected genes in our simulation setting.
From the results, we built shared graphs and used cross-validation to train and test our model.
A feature selection study using the simulated data can be found in the supplementary material.
3.2.2 Real data We applied GeneScissors to RNA-seq data from nine inbred samples and 53 F1 samples derived from three inbred mouse strains CAST/EiJ, PWK/PhJ and WSB/ EiJ.
We sequenced cDNA from mRNA extracted from brain tissues of three to six replicates of both sexes and the six possible crosses (including the reciprocal).
To mitigate misalignment errors owing to heterozygosity, for each F1 sample, we aligned each fragment to the genome of each parent separately (i.e.
the mm9 reference sequence with annotated SNPs) and then merged the two alignments while retaining all distinct multiple alignments (a union of the set of all mapped frag-ments each identified by their mapping coordinate and read identifier).
For comparison purposes, we also applied this alignment strategy in the TopHat and MapSplice pipelines.
3.3 Results from simulated data In Table 2, we first present the average precision, recall, F scores and Area under the Curve when LinearSVM, DecisionTree and RandomForests were used to build the classification models.
All scores were measured by 10-fold cross-validation.
The results demonstrate that our feature set is adequate and can help detect unexpressed genes efficiently.
The RandomForests is the best and most consistent among all three methods.
The classifi-cation model trained by RandomForests can detect near 90% spurious calls owing to misalignments.
Though SVM has a slightly higher precision score, the recall is much lower than RandomForests.
This is because RandomForests is more suit-able than SVM for data with discrete features and is more powerful in handling correlations between features.
Therefore, we chose RandomForests as the default classification method for our GeneScissors pipeline.
Next, we investigated how much improvement GeneScissors could bring to the overall transcriptome calling by correcting fragment misalignment.
We compared the results of our im-proved GeneScissors pipelines with those from the TopHat and MapSplices pipelines.
Both GeneScissors pipelines used the modified version of Cufflinks.
The GeneScissors (TopHat) pipe-line used the modified version of TopHat.
The MapSplice and TopHat pipelines used the regular version of Cufflinks.
We used the following three measurements to compare the performance at the gene level: GenePrecision Number of Correct Genes Number of Reported Genes , 1 GeneRecall Number of Correct Genes Number of Simulated Genes , 2 GeneFmeasurement 2 GenePrecision GeneRecall GenePrecision GeneRecall : 3 The results of different pipelines are summarized in Table 3.
All statistics are averaged over a 10-fold cross-validation.
We observe that Cufflinks tends to report a much higher number of genes in all four pipelines.
There are only 13 000 expressed genes, but Cufflinks reports 430000 genes in the TopHat or MapSplice pipelines and 426 000 genes in the GeneScissors pipelines.
A significant percentage of these reported genes can be mapped back to the expressed genes from which we generated synthetic reads.
Several reported genes are often mapped back to the same expressed gene by Cuffcompare.
Cufflinks failed to recognize them as (possibly different transcripts of) the same gene, perhaps owing to both the length and variable number of splice junctions and/or the low fragment coverage seen for some transcripts.
In this case, when we computed GenePrecision and GeneRecall, only one of them was counted as the correct gene, the remaining ones were counted as incorrect genes.
As all four pipelines used Cufflinks to infer transcriptome, all of them had relatively low GenePrecision.
The GeneScissors (MapSplice) pipeline had a 12.6% improvement in GenePrecision over the original MapSplice pipeline, at the cost of a slight drop in GeneRecall.
The GeneScissors (TopHat) pipeline had a 6.5% improvement in GenePrecision over the TopHat pipeline while retaining the same level of GeneRecall.
GeneScissors was able to detect and remove44000 spurious (gene) calls by correcting frag-ment misalignments.
We also observed that the MapSplice pipeline has the highest score on GeneRecall, but a much lower GenePrecision score comparing with TopHat pipeline and GeneScissors pipeline.
This is because MapSplice can find more possible alignments than TopHat but is not able to identify the correct alignment when a fragment has multiple alignments.
Hence, the MapSplice pipeline reported more false positives than the TopHat pipeline.
Overall, the GeneScissors (TopHat) pipeline performed best among the four pipelines on this challenging test case.
It is ob-vious that (i) detecting and correcting fragment misalignments can improve the accuracy in transcriptome inference under all circumstances and (ii) given the correct fragment alignments, better transcriptome inference algorithms are still needed.
In addition, GeneScissors does not assume all pseudogenes are un-expressed.
GeneScissors is able to distinguish expressed pseudo-genes from the rest with a comparable accuracy, demonstrated by a simulation study in the supplementary material.
3.4 Results from real RNA-seq data We also applied both TopHat pipeline and our GeneScissors (TopHat) pipeline on the real RNA-seq data.
The running Table 2.
Summary of the results from different classification methods Statistics LinearSVM DecisionTree RandomForests Precision 81.90% 83.70% 89.60% Recall 83.00% 84.80% 87.80% F-measurement 85.70% 84.20% 88.60% Area under the curve 0.843 0.837 0.91 i296 Z.Zhang et al.
time for TopHat pipeline was 24h per sample, and the extra running time for GeneScissors (TopHat) pipeline were 10h per sample.
Overall, the GeneScissors (TopHat) pipeline reported 4.25% fewer transcripts in real data than the TopHat pipeline (Fig.5a).
Considering that GeneScissors removed most of false positives in our simulation study, it suggests that the transcripts reported by the TopHat pipeline include a significant number of false positives.
Despite the fewer number of transcripts reported by GeneScissors, Figure 5b shows that GeneScissors actually re-ported 0.97% more transcripts that exactly match or partially match the splice junction annotations in the Ensembl database than the TopHat pipeline (The improvement is statistically sig-nificant with a P-value lower than 1014 under the paired stu-dents t-test).
These transcripts are likely the false negatives missed by the TopHat pipeline owing to misalignments.
(a) (b) (c) (d) Fig.5.
Comparisons between multiple samples run through both the GeneScissors pipeline and the TopHat pipeline.
Results from the same sample are connected by an arrow.
The three strains used were CAST/EiJ, PWK/PhJ and WSB/EiJ, and they are indicated by the initials C, P and W, respectively.
The two letter designations indicate the direction of the cross with the initial of the maternal strain followed by the initial of the paternal strain.
The samples are clustered according to replicates from the same sex and F1 cross, followed by the reciprocal cross.
The sex is indicated by F(female) and M(male) Table 3.
Comparison of MapSplice, TopHat, GeneScissors (MapSplice) and GeneScissors (TopHat) pipelines Statistics MapSplice pipeline TopHat pipeline GeneScissors (MapSplice) GeneScissors (TopHat) Number of reported genes 36 516 30 622 26556 26473 GenePrecision 35.6% 41.8% 48.2% 48.3% GeneRecall 95.1% 93.2% 93.0% 93.2% GeneF-measurement 51.5% 58.2% 63.5% 63.6% Note: The bold value of each row represents the best pipeline measured by the corresponding metric.
i297 GeneScissors Figure 5c shows that the TopHat pipeline reported4800 tran-scripts that are annotated as pseudogenes in Ensembl.
GeneScissors managed to remove453.6% of them, and the frac-tion of transcripts that overlap any pseudogenes decreased from 3.2 to 1.57%.
Figure 5d shows that GeneScissors reported 16% fewer unannotated transcripts than the TopHat pipeline.
All these results indicate that GeneScissors is effective in detecting and correcting false positive and false negative transcript reports caused by fragment misalignments.
Furthermore, the number of pseudogenes reported by the ori-ginal TopHat/Cufflinks pipeline in inbred samples is fewer than the number in F1 hybrids.
Similarly, the fraction of pseudogenes (57%) removed by GeneScissors in the inbred samples is smal-ler than the fraction (36%) removed in the F1 hybrids.
This indicates that the additional complications of F1 samples pose additional challenges to RNA-seq analysis pipelines and makes them more prone to errors than the inbred samples.
4 DISCUSSION AND CONCLUSION In this article, we presented GeneScissors, a general approach to detect and correct transcriptome inference errors caused by mis-alignments, which can be applied to any RNA-seq analysis pipe-line.
GeneScissors considers three underlying biological factors that lead to fragment misalignments and spurious transcript re-porting.
We proposed a classification model to detect false dis-coveries owing to misalignment, and the results show that it can provide significant improvement in overall accuracy.
Other heuristic approaches have been used to avoid reporting unexpressed genes in the RNA-seq assembly result, such as dis-carding all known pseudogenes reported by the TopHat pipeline, masking repeated elements in genome or aligning fragments to known transcriptome instead of genome.
The key difference is that our RNA-seq analysis does not require any additional an-notations beyond adding SNPs, and it still supports a novel transcript discovery.
Transcript discovery is important because current annotations are incomplete with regard to genes, isoforms and allele-specific variants.
For example, in the real data, we observed4000 unan-notated transcripts clustered 2300 unannotated genes on aver-age.
These transcripts persist after applying GeneScissors, which attempts to identify and correct misaligned fragments.
This implies that current annotations are neither complete nor entirely accurate.
For example, recent studies (Hirotsune et al., 2003; Khelifi et al., 2005) found that some regions previously thought to be pseudogenes can actually be transcribed to mRNA.
Hence, removing all annotated pseudogenes or highly repeated regions may lead to the removal of actual expressed transcripts.
In con-trast, GeneScissors might choose a pseudogene over the anno-tated paralog based on which better matches known genetic variants.
Furthermore, current pipelines using Cufflinks tend to over-report genes, especially when the genes share a high degree se-quence similarity with other expressed genes in the data.
The problem is alleviated to some extent by GeneScissors by recover-ing missed multiple fragment alignments and discarding frag-ment alignments to unexpressed genes/regions.
However, there is still room for improvement.
In the future work, though our precision and recall scores are near 90%, we plan to exploit additional features and constraints to improve the classification accuracy.
Example constraints in-clude that each sharing graph must contain at least one expressed gene and each shared fragment must belong to an expressed gene.
In addition, we plan to investigate how to rescue the dis-carded fragment alignments to an unexpressed fragment attrac-tor, but not in the shared regions with any linked fragment attractors because these fragments should belong to some ex-pressed genes.
ACKNOWLEDGEMENTS The authors thank those center members who prepared and pro-cessed samples as well as those who commented on and encour-aged the development of GeneScissors; in particular, Weibo Wang, Isa-Kemal Pakatci, Zhishan Guo, John Calloway, James J. Crowley and Patrick F. Sullivan.
They also thank three anonymous reviewers for their thoughtful comments.
Funding: [NIMH/NHGRI P50 MH090338], [NIH GM P50 GM076468], [NSF IIS-1313606], [NSF IIS-0812464].
Conflict of Interest: none declared.
ABSTRACT Motivation: Circadian rhythms are prevalent in most organisms.
Identification of circadian-regulated genes is a crucial step in discovering underlying pathways and processes that are clock-controlled.
Such genes are largely detected by searching periodic patterns in microarray data.
However, temporal gene expression profiles usually have a short time-series with low sampling frequency and high levels of noise.
This makes circadian rhythmic analysis of temporal microarray data very challenging.
Results: We propose an algorithm named ARSER, which combines time domain and frequency domain analysis for extracting and characterizing rhythmic expression profiles from temporal microarray data.
ARSER employs autoregressive spectral estimation to predict an expression profiles periodicity from the frequency spectrum and then models the rhythmic patterns by using a harmonic regression model to fit the time-series.
ARSER describes the rhythmic patterns by four parameters: period, phase, amplitude and mean level, and measures the multiple testing significance by false discovery rate q-value.
When tested on well defined periodic and non-periodic short time-series data, ARSER was superior to two existing and widely-used methods, COSOPT and Fishers G-test, during identification of sinusoidal and non-sinusoidal periodic patterns in short, noisy and non-stationary time-series.
Finally, analysis of Arabidopsis microarray data using ARSER led to identification of a novel set of previously undetected non-sinusoidal periodic transcripts, which may lead to new insights into molecular mechanisms of circadian rhythms.
Availability: ARSER is implemented by Python and R. All source codes are available from http://bioinformatics.cau.edu.cn/ARSER Contact: zhensu@cau.edu.cn 1 INTRODUCTION Circadian rhythm is one of the most well-studied periodic processes in living organisms.
DNA microarray technologies have often been applied in circadian rhythm studies (Duffield, 2003).
Thus, we can monitor the mRNA expression of the whole-genome level, which is an effective way to simultaneously identify many hundreds or thousands of periodic transcripts.
The matter to be addressed is which genes are rhythmically expressed based on their gene expression profiles.
This can be classified as a periodicity identification problem.
However, there are computational challenges when dealing with this issue: sparse determination of sampling rate, and short periods of data collection for microarray experiments (Bar-Joseph, 2004).
Circadian microarray experiments are usually designed to collect data every 4 h over a course of 48 h, generating expression profiles with 12 or 13 time-points (Yamada and Ueda, To whom correspondence should be addressed.
2007).
There are two main factors that limit the number of data points that can be feasibly obtained: budget constraints and dampening of the circadian rhythm (Ceriani et al., 2002).
Such short time-series data render many methods of classical time-series analysis inappropriate, since they generally require much larger samples to generate statistically significant results.
A variety of algorithms have been developed and applied to microarray time-series analysis; Chudova et al.
(2009) indicated that the existing technologies fall into two major categories: time-domain and frequency-domain analyses.
Typical time-domain methods rely on sinusoid-based pattern matching technology, while frequency-domain methods are based on spectral analysis methods.
Of the time-domain methods, COSOPT (Straume, 2004) is a well-known algorithm frequently used to analyze circadian microarray data in Arabidopsis (Edwards et al., 2006), Drosophila (Ceriani et al., 2002) and mammalian systems (Panda et al., 2002).
COSOPT measures the goodness-of-fit between experimental data and a series of cosine curves of varying phases and period lengths.
The advantages of pattern-matching methods are simplicity and computational efficiency, while they are not effective at finding periodic signals that are not perfectly sinusoidal (Chudova et al., 2009).
Of the frequency-domain methods, Fishers G-test was proposed to detect periodic gene expression profiles by Wichert et al.
(2004) and has been used to analyze circadian microarray data of Arabidopsis (Blasing et al., 2005) and mammalian systems (Hughes et al., 2009; Ptitsyn et al., 2006).
Fishers G-test searches periodicity by computing the periodogram of experimental data and tests the significance of the dominant frequency using Fishers G-statistic; however, it is limited by low frequency resolution for short time-series generated by circadian microarray experiments, which means it is often not adequate to resolve the periodicity of interest (Langmead et al., 2003).
Time-domain and frequency-domain methods are two different ways to analyze the time-series, each with advantages and disadvantages.
Frequency-domain methods are noise-tolerant and model-independent but their results are difficult for biologists to understand.
Time-domain methods can give comprehensive and easily-understood descriptions for rhythms but are noise-sensitive and model-dependent (e.g.
sinusoid).
Considering the above limitations, we propose an algorithm named ARSER that combines time-domain and frequency-domain analyses to identify periodic transcripts in large-scale time-course gene expression profiles.
ARSER employs autoregressive (AR) spectral analysis (Takalo et al., 2005) to estimate the period length of a circadian rhythm from the frequency spectrum.
It is well-suited to analyze short time-series since it can generate smooth and high-resolution spectra from gene expression profiles.
It is related (but not identical) to a method called maximum entropy spectral analysis, The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:22 12/5/2010 Bioinformatics-btq189.tex] Page: i169 i168i174 Detecting rhythmic gene expression which has been applied to the analysis of micorarray data (Langmead et al., 2002).
After the frequency-domain analysis, ARSER takes harmonic regression (Warner, 1998) to model the circadian rhythms extracted from gene expression profiles.
Harmonic regression models fully describe the rhythms using four parameters: period (duration of one complete cycle), the mean level (the mid-value of the time-series), the amplitude (half the distance from the peak to the trough of the fitted cosine, indicating the predictable rise and fall around the mean level) and the phase (the location in time of the peak of the curve).
The joint strategy overcomes the shortcomings of separate time-domain or frequency-domain analyses.
ARSER uses AR spectral analysis to find the circadian rhythms and a harmonic regression model to characterize and statistically validate them.
When tested on multiple synthetic datasets, ARSER is robust to noise, quickly and exactly estimates periodicity and gives comprehensive and statistically significant results in analysis of short time-series.
This article is organized as follows.
Section 2 describes the mathematical and implemental details of the ARSER algorithm.
Section 3 compares the performance of ARSER with COSOPT and Fishers G-test by testing on different simulation datasets.
Numerical experiments are designed by varying noise intensity and period length, including random background models and identifying non-sinusoidal periodic patterns.
Finally, ARSER was used to analyze Arabidopsis microarray data and obtained a novel set of rhythmic transcripts, many of which showed non-sinusoidal periodic patterns.
Section 4 summarizes the methodology.
2 METHODS 2.1 Overview Our methodology to detect circadian rhythms in gene expression profiles consists of three procedures: data pre-processing, period detection and harmonic regression modeling (Fig.1A).
First, ARSER performs a data preprocessing strategy called detrending that removes any linear trend from the time-series so that we can obtain a stationary process to search for cycles.
Detrending is carried out by ordinary least squares (OLS).
Second, ARSER determines the periods of the time-series within the range of circadian period length (2028 h) (Piccione and Caola, 2002).
The method to estimate periods is carried out by AR spectral analysis, which calculates the power spectral density of the time-series in the frequency domain.
If there are cycles of circadian period length in the time-series, the AR spectral density curve will show peaks at each associated frequency (Fig.1B).
With the periods obtained from AR spectral analysis, ARSER employs harmonic regression to model the cyclic components in the time-series.
Harmonic analysis provides the estimates of three parameters (amplitude, phase and mean) that describe the rhythmic patterns.
Finally, when analyzing microarray data, false discovery rate q-values are calculated for multiple comparisons.
2.2 Period detection As circadian rhythm has approximately (but never exactly) 24 h periodicity (Harmer, 2009), the first matter to be addressed is to measure the length of actual period for each gene expression profile.
ARSER estimates the period by AR spectral estimation, which is a high-resolution spectral analysis technique comparing with the classical fast Fourier transform periodogram.
Given an equally sampled time-series {xt} with the sampling interval , AR spectral estimation first applies an AR model of order p, abbreviated AR(p), to fit the time-series using the following equation: xt = p i=1 ixti +t (1) where t is white noise and i are model parameters (or AR coefficients) with p =0 for an order p process.
Gler et al.
(2001) and Spyers-Ashby et al.
(1998) reported that AR coefficients are generally estimated by three methods: the YuleWalker method, maximum likelihood estimation and the Burg algorithm.
ARSER implements the AR model-fitting by setting order p=24/ and computing the AR coefficients using all three methods.
After AR modeling, AR spectral analysis estimates the spectrum, with model parameters instead of original data, using the following equation: px()= 2 |1+pk=1keik |2 0< (2) where 2 is the variance of white noise; k are parameters defined in Equation (1).
If periodic signals are present in the time-series, then the spectrum derived from Equation (2) will show peaks at dominant frequencies.
However, at high frequencies the noise signals may also show peaks known as pseudo-periods.
ARSER obtains the period by using the following step-by-step procedure: (1) Remove the linear trend in time-series {xt}, denoting the detrended time-series as {xt}.
A B Fig.1.
The diagram of our methodology (named ARSER) and a case study.
(A) Analysis flowchart.
First, data pre-processing by linear trend removal (detrending), then period detection by searching peaks from the AR spectrum.
With the periods derived from the AR spectrum, harmonic regression is carried out to model circadian rhythms by fitting the detrended time-series with trigonometric functions.
Finally,ARSER describes the periodicity by several parameters: period, phase, amplitude, statistical significance and so on.
(B) An example of rhythmicity analysis by ARSER.
The synthetic time-series is generated by the following equation: f (t)=500e0.01t +140e0.01t cos( 224 t)+, where t [0,96] with 4 h intervals and is white noise following (=0, =40) normal distribution.
i169 [11:22 12/5/2010 Bioinformatics-btq189.tex] Page: i170 i168i174 R.Yang and Z.Su (2) Smooth {xt} by a fourth-order SavitzkyGolay algorithm (Savitzky and Golay, 1964).
This is a low-pass filter that can efficiently remove pseudo-peaks in a spectrum caused by noise.
The smoothed time-series is denoted as {xt}.
(3) Calculate the AR spectrum of {xt} by Equation (2), and select all periods {i}[20,28] that show peaks in the spectrum.
(4) Calculate the AR spectrum of detrended time-series {xt} by Equation (2), and select all periods {i}[20,28] that show peaks in the spectrum.
(5) The periods {i} and {i} are chosen as input to the harmonic regression analysis for {xt} by Equation (3).
(6) The optimum periods in {xt} are determined by Akaikes information criterion (Akaike, 1974) among the regression models generated in step 5.
2.3 Rhythm modeling and gene selection The next matter to be addressed is how to model the rhythmic patterns in the time-series of gene expression.
ARSER employs the harmonic regression model to represent the cycle trends, which involves fitting sinusoidal models to a time-series using the following equation: xt =+ n i=1 i cos(2fit+i)+t (3) where xt is the observed value at time t; is the mean level of the time-series; i is the amplitude of the waveform; i is the phase, or location of peaks relative to time zero; t are residuals that are unrelated to the fitted cycles; and t are the sampling time-points.
The fi term in Equation (3) are the dominant frequencies in the circadian range derived by Equation (2).
The periods (i) of the time-series equal (1/fi), where is the sampling interval.
Since periods are predetermined by AR spectral analysis, Equation (3) can be reduced to an even simpler multiple linear regression model: xt =+ n i=1 {pi cos(2fit)+qi sin(2fit)}+t (4) where pi =i cosi, qi =i sini.
The unknown parameters pi, qi and can be estimated by OLS method.
Then the amplitude i and phase i are obtained by i = p2i +q2i and tani =qi/pi.
By applying a harmonic regression model, rhythmicity in a time-series is fully described by four parameters: period, phase, amplitude and mean level.
Tests of statistical significance are also essential to distinguish between real rhythms and random oscillations.
In a harmonic regression model, an F-test is employed to assess the significance of pi and qi coefficients, and so statistically validates the rhythmicity.
When analyzing microarray expression data, tens of thousands of genes will be estimated simultaneously, so the problem of multiple testing must be considered.
We employed the method of Storey and Tibshirani (2003).
Briefly, by examining the distribution of P-values from the given dataset, an estimate of the proportion that are truly non-rhythmic can be derived.
The P-value for each transcript can be converted to a more stringent q-value which represents the false discovery rate.
In our study, we consider genes with q < 0.05 to be rhythmically expressed.
2.4 Generating simulation data We provide a comprehensive testing strategy to test and compare the performance of ARSER with COSOPT and Fishers G-test.
Our simulation datasets consist of periodic and non-periodic time-series data.
The periodic time-series was generated by two models based on the methods proposed by Robeva et al.
(2008).
One is classified as a stationary model, defined by xt =SNR 2cos ( 2 t ) +t (5) where SNR is signal-to-noise ratio; is period; is phase; and t is (= 0, =1) normally distributed noise terms.
Another model classified as a non-stationary is defined by xt =500e0.01t +SNR 100e0.01t cos( 2 t)+t (6) where t is (=0, =50) normally distributed noise; the mean level and amplitude exponentially decay over time.
The periodic datasets used in our numerical experiments were generated by assigning different values to , and SNR.
Compared with the stationary model, the non-stationary model is more likely to approximate the circadian rhythm (Refinetti, 2004).
The non-periodic time-series was generated from (=0, =1) normally distributed white noise and AR processes of order one, AR(1), as suggested by Futschik and Herzel (2008).
3 RESULTS AND DISCUSSION ARSER was applied to both simulated and real microarray data.
A series of numerical experiments were designed to test and compare the performance of ARSER with COSOPT and Fishers G-test.
The tasks were to (i) precisely estimate periodicity, (ii) separate periodic from non-periodic signals and (iii) identify non-sinusoidal periodic patterns.
3.1 Robustness to noise In the first experiment, we compared the robustness of three algorithms to noise.
We generated 10 000 stationary [derived from Equation (5)] and 10 000 non-stationary periodic signals [derived from Equation (6)] by the following steps: (i) taking by 0.1-h sampling interval in [20h,28h), (ii) at each period, taking by 2/25 sampling interval in [0,2) and (iii) at each period and phase, taking SNR of 5:1, 4:1, 3:1, 2:1 and 1:1.
Each time-series possessed 12 data points, within 044 h at 4 h sampling intervals, which was the same as the majority of published circadian microarray datasets.
There were 2000 periodic time-series under each SNR.
We also generated 2000 (=0, =1) normally distributed random signals.
Then we applied ARSER, COSOPT and Fishers G-test to identify periodic signals from the positive (periodic) and negative (random) samples and measured their performance of periodicity prediction under different SNRs by Matthews correlation coefficient (MCC) (Fig.2A and B).
The MCC is in essence a correlation coefficient between the observed and predicted binary classifications, with values between 1 and +1.
MCC = +1 represents a perfect prediction, 0 an average random prediction and 1 an inverse prediction (Baldi et al., 2000).
Of the three methods, ARSER performed best at any noise-level for both stationary and non-stationary time-series, suggesting it was a robust periodicity detection algorithm.
3.2 Correctness for predicting wavelength The task of precisely estimating periodicity included two parts: identification of truly periodic signals and the measurement of actual wavelength.
The former case was verified for the three algorithms in the robustness experiment, and we continued to investigate the latter one.
According to the predictions in the robustness experiment, we calculated the differences for each periodic signal in its actual period and predicted value by each algorithm.
The distributions of prediction errors for the three algorithms (Fig.3) showed that ARSER and COSOPT were more accurate (errors with zero means i170 [11:22 12/5/2010 Bioinformatics-btq189.tex] Page: i171 i168i174 Detecting rhythmic gene expression and small variations) in predicting the period length compared with Fishers G-test (errors of large variation).
This indicates ARSER and COSOPT are high-resolution detecting algorithms and can accurately estimate the wavelength of circadian rhythms.
3.3 Periodicity detection with random background models To verify our algorithm, our second task was to separate periodic from non-periodic signals, which measured the sensitivity and specificity in the predictions.
Fig.2.
Accuracy of ARSER, COSOPT and Fishers G-test at identifying (A) stationary and (B) non-stationary periodic signals under decreasing signal-to-noise ratio.
For each group of periodic signals, there are 1:1 negative sample data generated by (=0, =1) white noise.
The performance is measured by MCC.
As noise intensity increased, ARSER (blue) performed robustly with noise in both situations, while COSOPT (red) and Fishers G-test (yellow) performed badly for stationary periodic signals with high-level noise and even worse for non-stationary ones.
They all scored the signals as periodic using the threshold of 0.05 for FDR q-value for ARSER and Fishers G-test, or by pMMC-for COSOPT.
pMMC-measures the probability for multiple testing, similarly to the FDR q-value.
The periodic signals generated in the robustness experiment contained 10 000 stationary and 10 000 non-stationary time-series.
The non-periodic signals were generated by white noise and AR(1) models with 10 000 samples in each case.
Then we created four testing datasets by combining (i) stationary periodic signals with white noise signals, (ii) non-stationary periodic signals with white noise signals, (iii) stationary periodic signals with AR(1)-based random signals and (iv) non-stationary periodic signals with AR(1)-based random signals.
Since predictions were periodic or non-periodic, a well-suited binary classification, we applied receiver operating characteristic (ROC) curves (Fawcett, 2006) to compare the performances of the three algorithms on the four datasets (Fig.4AD).
Performances were measured by the area under the ROC curve criterion, with the larger the area the better the method.
ROC analysis showed that ARSER outperformed COSOPT and Fishers G-test in all cases.
3.4 Detection of non-sinusoidal periodic waveforms The last numerical experiment was to apply the three algorithms to detect non-sinusoidal periodic patterns.
The testing dataset was downloaded from the HAYSTACK web site (http://haystack.cgrb.oregonstate.edu/).
This dataset included five cycling patterns based on diurnal and circadian time-course studies: rigid, spike, sine and two box-like patterns (Michael et al., 2008).
Each periodic pattern contained 24 samples with the phase shifted from 0 to 23 h by 1 h intervals.
A total of 120 time-series were contained in the dataset, which possessed 12 time-points that represent two circadian cycles obtained at 4 h sampling intervals.
We also added 120 (=0, =1) white-noise signals as negative data.
ARSER performed the best of the three algorithms (Fig.5), and identified more spike and box-like periodic pattern profiles while maintaining a very low false positive rate.
3.5 Analysis of Arabidopsis circadian expression data Our methodology for identifying periodicity of short time-series worked successfully on synthetic datasets.
We applied ARSER to analyze a real microarray dataset generated from the work of Fig.3.
Distribution of the differences between the predicted wavelength (using three algorithms) and the actual wavelength of stationary periodic signals (yellow) and non-stationary periodic signals (blue).
Log transformation for the difference () was carried out using ln(1+||).
The errors of wavelength prediction by ARSER and COSOPT were low and close to zero, while those for Fishers G-test were high and in a wide range.
i171 [11:22 12/5/2010 Bioinformatics-btq189.tex] Page: i172 i168i174 R.Yang and Z.Su Fig.4.
ROC curves for identifying periodic signals from four datasets of (A) 10 000 stationary periodic signals and 10 000 white noise signals, (B) 10 000 non-stationary periodic signals and 10 000 white noise signals, (C) 10 000 stationary periodic signals and 10 000 AR(1)-based random signals, and (D) 10 000 non-stationary periodic signals and 10 000 AR(1)-based random signals.
Greater area under the ROC curve indicates better performance of the algorithm.
ARSER gave the fewest false positives and false negatives compared with COSOPT and Fishers G-test in all cases.
Fig.5.
Comparison of detecting multiple periodic waveforms.
(A) Number of samples from five periodic waveforms and random signals (cyan) identified by three detecting algorithms.
The testing dataset are composed of 120 periodic time-series with 24 samples for each periodic pattern and 120 (=0, =1) white-noise samples.
(B) The waveforms include sinusoidal (red) and non-sinusoidal: rigid waves (yellow), box1 waves (blue), box2 waves (magenta) and spike waves (green).
The dataset and periodic patterns were generated by HAYSTACK tool (Michael et al., 2008).
Edwards et al.
(2006) in the study of the Arabidopsis circadian system.
The data (available from http://millar.bio.ed.ac.uk/data.htm) are expression profiles of 13 data points, representing 48 h of observation obtained at 4 h sampling intervals.
In the original study, the authors used COSOPT to identify cyclic genes.
Of all 22 810 genes represented on the array, 3504 genes were considered rhythmic at the significance threshold pMMC-< 0.05.
Fig.6.
Area-proportional Venn diagram addresses the predictive power of three algorithms for identifying Arabidopsis circadian-regulated genes.
The microarray data were originally analyzed by COSOPT in the study of Edwards et al.
(2006), and scored 3504 genes as rhythmic (pMMC-< 0.05).
A total of 4929 genes were identified by ARSER (FDR q < 0.05), while only 536 were found by Fishers G-test (FDR q < 0.05).
Venn diagram was generated by BioVenn tool (Hulsen et al., 2008).
We re-analyzed this microarray data usingARSER and Fishers G-test, both of which scored rhythmic transcripts by FDR q < 0.05, and compared their predictions with those of COSOPT (Fig.6).
ARSER identified 96% of the cycling transcripts identified by COSOPT, suggesting our methodology efficiently reproduced the original results, while Fishers G-test merely identified only 15%, implying it may not efficiently analyze this circadian expression data.
In addition, a novel set of 1549 transcripts were uniquely identified as rhythmic by ARSER.
To examine these newly found genes more closely, we employed principal component analysis (Wall et al., 2003) to visualize the dominant expression patterns from their profiles.
The first two principal components accounted for 70% of the variance (Fig.7A) and were cyclic with spike-like patterns (Fig.7B and C).
These plots revealed a rhythmic component with a period of 24 h in many transcripts of the novel set, which was consistent with the estimate using ARSER.
These periodic patterns were mainly non-sinusoidal and may explain why they were not identified by COSOPT in the original study.
The third component also showed a linear trend (Fig.7D), indicating the non-stationary feature of the data.
Dodd et al.
(2007) reported 27 well-known clock-associated genes in Arabidopsis.
Two of these genes were found among the newly identified genes of the present study.
One was CRYPTOCHROME 1 (CRY1), which functions as a photoreceptor for the circadian clock in a light-dependent fashion.
In plants, the blue light photoreception can be used to cue developmental signals (Brautigam et al., 2004).
The other one was PSEUDO-RESPONSE REGULATORS 9 (PRR9), which plays an important role in response to temperature signals in a temperature-sensitive circadian system (Harmer, 2009) and acted as transcriptional repressors of CCA1 and LHY in the feedback loop of the circadian clock (Nakamichi et al., 2010).
Thus more clues could be provided by applying ARSER to further study of Arabidopsis circadian expression data.
Unlike the synthetic datasets used in numeral experiments, the notions of false positives and false negatives in real experimental i172 [11:22 12/5/2010 Bioinformatics-btq189.tex] Page: i173 i168i174 Detecting rhythmic gene expression Fig.7.
Principal component analysis of the newly found rhythmic transcripts in Arabidopsis identified by ARSER.
Plots of relative variance for the first nine components (A); and the first (B), second (C) and third (D) eigengenes are shown.
The first three principal components account for 78% of the variance.
The first and second eigengenes are cyclic with spike-like patterns, and the third shows a linear trend.
These data reveal that non-sinusoidal and non-stationary periodic transcripts could be found by applying ARSER.
PCA was carried out by Mev tool (Saeed et al., 2003).
Table 1.
Summary of rankings of 27 known Arabidopsis clock-associated genes in the entire genome, in order of significance using three algorithms Method Rankings in the entire genomea Top 5% Top 10% Top 25% Top 60% ARSER 10 15 21 26 COSOPT 11 16 20 24 Fisher.G 10 14 21 24 aARSER and Fisers G-test rank genes by FDR q-value; COSOPT rank genes by pMMC-.
data are not well defined.
Thus, we used the 27 known clock genes as benchmark genes to evaluate a given algorithm in terms of false negatives for analyzing circadian expression data.
We applied three algorithms to rank the 22 810 genes of the entire Arabidopsis genome in order of the statistical significance of their expression profiles.
The rankings of the 27 known clock genes (Table 1) showed that the three algorithms performed similarly, and all identified most of the known clock genes from among their top 25% ranked candidates.
4 CONCLUSION In this study, we present an automated algorithm for identifying periodic patterns in large-scale temporal gene expression profiles.
It employs harmonic regression based on AR spectral analysis to identify and model circadian rhythms.
Compared with separate frequency-domain or time-domain methods, our methodology is a joint strategy which analyzes the time-series through both frequency and time domains.
Testing on synthetic and real microarray data showed that our novel method was computationally optimal and substantially more accurate than two existing and widely-used rhythmicity detection techniques (COSOPT and Fishers G-test).
In addition, our method identifies a novel set of rhythmically expressed Arabidopsis genes which may supply more valuable information for further study of plant circadian systems.
ACKNOWLEDGEMENTS We would like to thank Prof. John Hogenesch for sharing COSOPT software with us.
We thank Gaihua Zhang for technical assistance.
We thank Wenying Xu, Yi Ling, Daofeng Li, Jingna Si, Fei He and Tingyou Wang for helpful discussions.
Funding: Ministry of Science and Technology of China (2008AA02Z312, 2006CB100105).
Conflict of Interest: none declared.
ABSTRACT Motivation: Gene activity is mediated by site-specific transcription factors (TFs).
Their binding to defined regions in the genome determines the rate at which their target genes are transcribed.
Results: We present a comprehensive computational model of the search process of TF for their genomic target site(s).
The computational model considers: the DNA sequence, various TF species and the interaction of the individual molecules with the DNA or between themselves.
We also demonstrate a systematic approach how to parametrize the system using available experimental data.
Contact: n.r.zabet@gen.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on November 14, 2011; revised on March 27, 2012; accepted on April 3, 2012 1 INTRODUCTION Originally, it was believed that transcription factors (TFs) find their target sites only through 3D diffusion and the association rate would follow the Smoluchowski limit.
Riggs et al.
were the first to observe that the rate at which the lac repressor locates its target site is much faster than the rate predicted by the Smoluchowski limit and hypothesized that a different mechanism was involved in this process (Riggs et al., 1970).
In their seminal work, von Hippel et al.
(Berg et al., 1981; Winter et al., 1981) thoroughly investigated this process from both a theoretical and experimental perspective and concluded that TF molecules use the facilitated diffusion mechanism to locate their target sites.
This facilitated diffusion mechanism assumes a combination between 3D diffusion in the cytoplasm and an 1D random walk on the DNA.
This leads to reduction of dimensionality in the search process and, consequently, speeds up the search.
In addition, three main types of movements on the DNAwere proposed: (i) sliding, (ii) hopping and (iii) jumping (Berg et al., 1981).
Sliding and hopping are both mechanisms of 1D random walk, but the difference between them is that during hopping the molecules lose contact with the DNA, whereas during sliding the molecules keep contact with the DNA.
On the other hand, jumping is a mechanism which assumes that the molecules do not only lose contact with the DNA for a short time interval (as in the case of hopping), but they completely release into the cytoplasm where they spend a longer To whom correspondence should be addressed.
time until they bind to the DNA uncorrelated with respect to the unbinding position.
The existence of the 1D random walk in vivo was recently confirmed by Elf et al.
(2007).
The authors of that study used fluorescent lac repressor tetrameters and visualize their movement in a live Escherichia coli cell, confirming that the molecules spend 90% of the time bound to the DNA.
There are still missing pieces in our understanding of the facilitated diffusion mechanism.
One approach to address these questions consists of building a computational tool able to simulate the relevant molecules in a cell and the entire DNA sequence.
This type of approach can address several questions, e.g.
how crowding can influence the search process at genome-wide level, in a dynamical context (Chu et al., 2009) and not as static barriers (Li et al., 2009).
In addition, one could investigate systems with real affinity landscapes, which is not possible through analytical tools (Berg et al., 1981).
In this article, we present a computational model for stochastic simulation of the search process of TFs for their target sites on the DNA.
The model considers each TF molecule as an independent object, which can move freely in the bacterial cytoplasm, but which also can bind to the DNA and perform an 1D random walk.
The DNA molecule is modelled as a string of nucleotides, which leads to specific affinity between a TF molecule and DNA at the position where the molecule is bound.
We also go through the literature and systematically infer each microscopic parameter of the model from experimentally macroscopic measurements.
Finally, we developed an implementation of the proposed model, which is available in Zabet and Adryan (2012).
2 MODEL One strategy to stochastically model the TF search process for their target sites consists of designing a hybrid system combining agent-based modelling and stochastic simulation techniques (Gillespie, 1977).
In this model, each TF molecule is represented as an agent able to perform certain actions and the DNA molecule as a string of the nucleotides: a, t, c or g. The model can assume reflecting boundaries (TFs that reach the boundary can only go back), periodic boundaries (the DNA is assumed to be in a closed loop) or absorbing boundaries (TFs that reach the boundary will unbind from the DNA).
In this setting, the TF molecules can be either free in the cytoplasm or bound on the DNA at a certain position.
A free TF molecule has only one action available, namely, to bind to the DNA.
The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [16:29 7/5/2012 Bioinformatics-bts178.tex] Page: 1518 15171524 N.R.Zabet and B.Adryan 2.1 Binding event We assume that the bacterial cytoplasm is a perfectly mixed reservoir from where the free TF molecules bind to the DNA.
The 3D diffusion of TF molecules in the cytoplasm is not modelled explicitly, but rather, the molecules that are free in the cytoplasm have a certain association rate to the DNA.
To simulate 3D diffusion we use the Direct Method implementation of Gillespie Algorithm (Gillespie, 1977) which generates a statistically correct trajectory of the Master Equation.
The rate at which a TF molecule of species x will bind to the DNA is computed as kbindx =kassocx TFfreex Acurrentx Amaxx (1) where kassocx is the reaction probability rate constant for species x, TFfreex the number of free TF molecules of species x and the last fraction (Acurrentx/A max x ) is the proportion of free positions where a molecule can bind.
A comprehensive list of all parameters used in this article can be found in the Supplementary Material.
Note, that after each 1D move, the number of available positions on the DNA for a TF to bind can change and, consequently, the association rate needs to be updated often.
An approximate system would consider that the binding of TF molecules is affected by occupancy, but the update is performed only when a molecule binds/unbinds and not when any other event (sliding or hopping) would lead to change in the number of available binding sites on the DNA.
In the Supplementary Material, we show that the difference between this approximation and the exact system is negligible and, thus, one can use this approximate system to increase simulation speed.
When a molecule binds to the DNA it will occupy a number of consecutive base pairs on the DNA and no other molecule will be able to bind to the DNA at that position.
The size on the DNA of each TF molecule is computed as the sum of the number of base pairs of the DNA binding motif, the number of obstructed base pairs on the left side of the molecule and the number of obstructed base pairs on the right side (Fig.1).
TFsizex =TFmotifx +TFleftx +TFrightx (2) Note that this feature (TF can cover base pairs to the left or to the right side of the DNA binding motif) has not been considered in this type of simulations, but is biologically plausible.
We mark all base pairs covered by the TF molecule as being unavailable, but we record the left-most base pair covered by the TF molecule as the position at which a TF molecule is bound to the DNA.
This does not affect the results in any way, but is just a choice of internal representation of the binding.
Fig.1.
TF binding to the DNA.
TF molecules bind to the DNA and mark several nucleotides as covered (grey) on: the DNA binding motif (3 bp in our example), the obstructed left side (1 bp) and the obstructed right side (2 bp).
Volume exclusion is implemented, in the sense that two TF molecules cannot cover the same base pair on the DNA.
The green positions on the DNA mark the positions where the free TF molecule can bind.
In addition, previous simulators did not take into account TF orientation on the DNA (Barnes and Chu, 2010; Chu et al., 2009).
The orientation of TFs affects the affinity of the TF for a specific position on the DNA, i.e.
a molecule bound in one orientation can have a totally different affinity compared with being bound in the opposite orientation at the same position.
Finally, since transcription and translation are co-localized in prokaryotic systems, a TF molecule has a higher probability to bind initially near the DNA region where it was released, and if the target site is within a sliding length distance, the entire search process can be reduced to one sliding step.
We consider the possibility of an initial binding region on the DNA in our model, in the sense that each TF molecule has a user-specified probability to bind for the first time within the user-defined region on the DNA, but only if there are free spots in that region.
2.1.1 Implementation of the binding event Barnes and Chu (2010) observed that, in the case of crowded DNA, locating a free position on the DNA where TF molecules can bind can be a bottleneck.
In the Supplementary Material, we present a new method to significantly enhance the simulation speed.
This method assumes the creation and maintenance of an array list of boolean values for each TF species (x), which specifies whether a TF molecule of type x is allowed to bind at position j, A[x][j].
This has the purpose to eliminate the need to check if sufficient nucleotides (TFsizex ) in the right side of the selected position are not covered by other molecules.
Furthermore, to increase the speed of locating a position, we store the current number of free positions for each species, Acurrentx , and when we look for a free position we draw a random number z in the interval [0,Acurrentx ) which will represent the z-th available position on the DNA.
This method guarantees that a free position is found using only one random number, which represents a significant enhancement of the simulation speed.
To further increase the search speed from M/2 to M, we keep total counts of available positions in a different array (see Supplementary Material).
2.2 TF affinity for DNA Once bound to the DNA, TF molecules will spend a certain time bound to a position until they make any type of movement.
The time spent at any position on the DNA is determined based on the binding energy between the molecule DNA binding domain and the sequence under the molecule.
The average waiting time at a position is given by (Gerland et al., 2002) j x =0x exp [ ( Ejx )] (3) where x represents the TF species, j is the position on the DNA, 0x is the average waiting time when bound specifically and Ejx is the binding energy at position j.
Note that the 0x term is similar to the 0exp ( Ens ) term in Gerland et al.
(2002).
The binding energies are measured in 1 =KBT (where KB is the Boltzmann constant and T the temperature), which will leave just the value of the binding energy in the exponential term.
To reduce memory usage, we will break the TF species into two classes: (i) non-cognate TFs and (ii) cognate TFs.
The cognate ones are the TFs that are of interest and that we can follow, whereas the non-cognate ones main purpose is to simulate the other proteins on the DNA, which might interfere with the search process of the 1518 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [16:29 7/5/2012 Bioinformatics-bts178.tex] Page: 1519 15171524 Facilitated diffusion in prokaryotes cognate TFs.
For efficiency reasons, we pre-calculate the affinities of each TF species, both cognate and non-cognate, and store them in individual arrays.
The non-cognate binding energy is randomly generated using a Gaussian distribution with the mean and variance provided as inputs for each non-cognate species.
The binding energy of cognate TFs is computed using two techniques: (i) mismatch energy (Gerland et al., 2002) and (ii) position frequency matrix (PFM; Berg and von Hippel, 1987; Stormo, 2000).
In both scenarios, we assume that each position in the DNA binding motif is approximately independent and additive (Berg and von Hippel, 1987; Gerland et al., 2002; Stormo, 2000).
First, in the case where there is a single high-affinity binding site, one can use the mismatch energy approach, which assumes that for each mismatch between the consensus Sx for species x and current DNAposition, the binding energy is penalized by a fixed value called the mismatch energy: Ejx = L k=0 j x(k) (4) where L is the length of the motif and jx(k) is the mismatch penalty at position k. The mismatch penalty is equal to jx(k)=0 if Skx =DNAj+k and jx(k)=x otherwise.
It was estimated that x [1,3] KBT (Gerland et al., 2002) and, in our simulations, we will consider that x =2KBT .
For example, if the binding motif is atcg and between positions j and j+4 on the DNA we have the sequence acct, then Ejx =x (0+1+0+1)=2x .
Note that if there is a match between the nucleotide of the motif sequence and the one of the DNA sequence we put a value of 0 whereas for a mismatch we put a value of x .
Second, for multiple high-affinity binding sites (experimentally determined using methods such as ChIP, SELEX and PBM) we will use the PFM.
Instead of penalizing when there is a mismatch, the PFM approach has a weighted mismatch which penalizes the energy by j x(k)=x ln ( nx0,k + nxj,k + ) (5) If at position (j+k) on the DNA we have nucleotide x, then the number of occurrences of this nucleotide at position k in all known high-affinity binding sites is denoted by nxj,k and the highest number of occurrences of any nucleotide at position k in all known high-affinity binding sequences of species x, by nx0,k .
is a pseudo-count term which ensures that the fraction in the logarithm is never zero.
In addition, we also scale the binding energy by a fixed value, x .
The equation proposed by Berg and von Hippel (1987) was said to describe with good accuracy the energy based on the PFM, but only for unbiased genomes (Stormo, 2000).
Stormo (2000) proposed an information-based approach on determining the binding energy to a DNA sequence, which would be valid for both biased and unbiased genomes.
This resulted in the following mismatch penalty: j x(k)=x ln ( xj,k j+k ) (6) where xj,k represents the frequency of occurrences of the nucleotide (j+k) at position k in all known high-affinity binding sites and (j+k) the frequency of the nucleotide (j+k) in the entire genome.
To ensure that the frequency in the motif is non-zero we insert a pseudo-count term when computing the frequency in the PFM.
xj,k = nxj,k + (j+k) u{a,c,g,t}nxu,k + (7) Note that the binding energies computed by the three methods for the lac repressor and the E.coli K-12 genome are highly correlated and they follow a Gaussian distribution (see Supplementary Material).
2.3 One-dimensional random walk The TF molecule will reside at its current position on the DNA for a random amount of time, which is exponentially distributed with a mean jx .
Once a TF molecule was selected to perform an action from its current position on the DNA, the molecule has to chose stochastically between one of the following three actions: (i) unbind from the DNA (with the possibility to re-bind fast), (ii) slide left on the DNA and (iii) slide right on the DNA.
The probability to perform any of these actions (Punbind, Pleft and Pright) is independent of position, but it is specific to each TF species, i.e.
each TF species has its own values for the probabilities to perform these actions (Pxunbind, P x left and P x right for species x) and a molecule of type x has the same probabilities independent of the position on the DNA (Pxunbind[j]=Pxunbind, Pxleft[j]=Pxleft and Pxright[j]=Pxright, j, where j is the position on the DNA).
Note that, to make the notation simple, we will drop the superscript x from the these parameters, but, whenever we refer to these action probabilities, it is understood implicitly that they are specific to each TF species.
Furthermore, in this article, we assume an unbiased random walk (for a discussion on this aspect see Section 5) and this means that the probabilities to slide left or right are equal at any position on the DNA, Pleft[j]=Pright[j],j (where j is the position on the DNA).
First, if the molecule decides to unbind, it will have a high probability to re-bind fast (van Zon et al., 2006).
Theoretical studies computed that a TF re-binds on average between six times (Wunderlich and Mirny, 2008) and up to a few hundred times (DeSantis et al., 2011).
The model allows for each species to have two unbinding probabilities: (i) the unbinding probability (with the possibility to re-bind fast) (Punbind) and (ii) the probability to completely release from the DNA once unbound (Pjump).
The former controls the number of sliding steps the TF performs before it unbinds, whereas the latter controls the ratio between the number of hops and the number of complete dissociations from the DNA.
We should mention that we do not distinguish explicitly between jumps and long hops.
In particular, a disassociated molecule can re-bind to a position which is Gaussian distributed around its previous position and with variance 2hop =1bp (Wunderlich and Mirny, 2008).
Thus, long hops are allowed as long as the re-binding is fast.
In addition, if the TF molecules have orientation, then during hopping, the orientation of a TF can change.
For slow re-binding the TF molecules are released into the cytoplasm and they will have chances to re-bind similar to all free molecules.
2.3.1 Implementation of the 1D random walk There are two strategies to implement the 1D random walk (see Supplementary Material).
First, we can consider all molecules as independent agents 1519 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [16:29 7/5/2012 Bioinformatics-bts178.tex] Page: 1520 15171524 N.R.Zabet and B.Adryan that stay at their current position for a certain amount of time.
Then, we store the time when each molecule will attempt to make a new action in a sorted structure (such as PriorityQueue in Java).
This list is kept updated and sorted after each 1D or 3D random walk event.
When selecting the next event to execute, we pop the head of this structure.
We call this method the First Reaction (FR) method.
Second, we keep the waiting times in a fixed size array and extract the next molecule that is bound to the DNA and that will perform an action.
This is based on the Direct Method version of the Gillespie algorithm (Gillespie, 1977) and, thus, we call this the Direct Method.
2.4 Cooperativity Our model allows cooperative behaviour between TF molecules and this can be either mediated by DNA or represented as direct TFTF interaction.
First, the DNA-mediated cooperative behaviour assumes that once a TF molecule from a specific species binds to a certain site on the DNA in the correct orientation, the waiting time of a molecule of the same (or a different) TF species at another site changes.
For example, binding of a molecule of species x at position j means a change in the waiting time of a molecule of species y at position j of cy,j x,j as follows j y = j y cy,j x,j (8) The cooperativity can be reversible, in the sense that once the TF molecule of species x located at site j moves away, the waiting time of a molecule of type y at the position j reverts to the original value, j y .
Nevertheless, the cooperativity can be irreversible, and the waiting time of a molecule of type y at the position j can be kept at the value j y until molecule y leaves position j. Alternatively, two molecules (x and y) that physically interact and are cooperative can increase their waiting times by a factor cyx using one of the following equations j y = j y cyx, jx = jx cyx, (9) j y = j y +cyx, jx = jx +cyx or (10) j y = j y +cyx jx, jx = jx +cyx jy (11) when j = j+TFsizex or j = jTFsizey .
The first equation addresses the case of multiplicative cooperativity, the second equation the case of fixed additive cooperativity and the third equation the case of variable additive cooperativity.
This allows both positive and negative cooperativity, i.e.
two molecules that touch can also reduce their waiting times on the DNA.
Equations (8) and (9) were already discussed in Chu et al.
(2009).
In addition, Equations (10) and (11) represent new hypotheses of how to model cooperativity that are mathematically possible and which we would like to further investigate for their biological relevance.
In the case of Equation (9), we assumed that the residence time will increase with a fix value independent of where two TF molecules are on the DNA.
Although this might represent a good approximation, there is no clear evidence that the increase in affinity is not dependent on the strength of the binding between TFs and DNA.
Equation (11) provides a way to model direct TFTF co-operativity which depends on the strength of the TF-DNA binding.
This type of interaction has not been investigated previously, but the framework that we present here aims to propose several new hypothesis that could be further tested.
3 ESTIMATING MODEL PARAMETERS The model requires a series of microscopic parameters.
Next, we will systematically show how to estimate these parameters in our system from macroscopic parameters that were measured experimentally.
We consider the lac repressor and E.coli as an example system, due to the fact that it is a well-studied system with some available data.
Note that unless mentioned otherwise, we will use the genome of E.coli K-12, which has M 4.6 Mbp (Riley et al., 2006).
First, if we know that the observed sliding length is sobsl and that the random walk is unbiased, then during a sliding event we will need Nobsse = ( sobsl )2/2 sliding events to cover sobsl base pairs of DNA (Wunderlich and Mirny, 2008).
Since, currently there is no method to clearly distinguish between a slide and a hop, the actual number of sliding events (Nse) will differ from the observed one Nobsse .
The relationship between these two parameters is given by Nse 1 Pjump =Nobsse Nse = ( sobsl )2 Pjump 2 (12) where (1/Pjump) represents the number of slides separated by micro-dissociations from the DNA before the molecule completely releases into the cytoplasm.
Wunderlich and Mirny (2008) estimated that the jump probability is Pjump =0.1675, which leads to six slides before a jump.
For a high number of random walk events during a slide, we can estimate the unbinding probability as the inverse of the number of binding events (Halford and Marko, 2004).
Punbind = 1Nse = 2( sobsl )2 Pjump ; (13) The actual sliding length can also be estimated from the observed one using the average number of hops performed before a jump.
sl = 2Nse = 2 ( sobsl )2 Pjump 2 =sobsl Pjump (14) This means that the actual sliding is 2.5 times smaller than the observed one (taking into account that on average we have six slides before a jump).
Elf et al.
(2007) estimated that the observable sliding length can be 90 bp, which leads to sl =37 bp.
This value is in the range estimated by Halford et al.
(Gowers et al., 2005; Halford and Marko, 2004).
Furthermore, we can estimate the unbinding probability and the number of events per slide as Punbind 1.47e3, Nse 700 and Nobsse 4000 (15) We assume that the random walk is unbiased (Blainey et al., 2006) and, thus, the probabilities to slide left or right are equal Pleft =Pright = 1Punbind2 (16) 1520 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [16:29 7/5/2012 Bioinformatics-bts178.tex] Page: 1521 15171524 Facilitated diffusion in prokaryotes Given the value of the unbind probability computed above (Punbind 1.47e3) the two sliding probabilities are Pleft =Pright = 0.4992.
Furthermore, if we know the residence time tR (the time a molecule spends on the DNA before it unbinds during jumping), then we can compute the average waiting time to be x= tR Nobsse = 2tR( sobsl )2 Finally, using the average of exponential binding energy of the TF (exp(Ex)), the specific waiting time can be computed as 0x = x exp(Ex) = 2tR( sobsl )2 exp(Ex) (17) Note that exp(Ex) =exp(Ex) and, consequently, we need to compute the mean of the exponential and not the mean of the binding energy.
To determine the association constant, we first need to estimate the dissociation rate.
The dissociation rate can be estimated as the inverse of the residence time.
kdissoc = 1 tR (18) For tR =5 ms the dissociation rate can be approximated by kdissoc 200 s1.
At steady state, the binding flux will equal the unbinding flux.
kassocx TFfreex Acurrentx Amaxx =kdissocTFbound kassocx = 1 tR TFbound TFfree A max x Acurrentx (19) where TFbound represents the abundance of bound TF.
Usually, the number of non-cognate molecules is much higher than the one of cognate molecules (TFnc 104 
TFlacI 101) and consequently, the relative amount of occupied DNA can be written as (TFnc fnc TFsizenc/M).
Flyvbjerg et al.
(2006) estimated that the relative occupied DNA in E.coli lies in the interval.
TFnc fnc TFsizenc M [0.1,0.5] (20) For long DNA strands, which is the case of E.coli genome, one could approximate the relative free DNA by the ratio between the number of free positions to bind and the maximum number of free positions to bind.
1 TFnc fnc TF size nc M A current x Amaxx A current x Amaxx [0.5,0.9] (21) Note that this is just an estimate and in some extreme cases (high-DNA occupancy) the estimate for the free sites on the DNA (Acurrentx/A max x ) might display lower accuracy.
In those cases, the most viable solution is to test several values, until the optimal one is found.
If we consider that TFs spent 90% of the time bound to the DNA (f 0.9 relative time bound to the DNA Elf et al., 2007), then, at any time point, on average 90% of the molecules will be bound to the DNA (TFbound/TFfree =9).
In this scenario, the association rate is somewhere in the interval kassocx [2000,3600]s1.
Finally, knowing the average number of non-cognate molecules (TFnc 104), the DNA occupancy (TFnc fnc TFsizenc/M [0.1,0.5]) and the length of the (DNA M =4.6 Mbp), we can estimate that the average number of base pairs covered by a non-cognate molecule is TFsizenc =46 bp.
For 10 000 non-cognate molecules (each covering 46 bp) 460 000 bp of the DNA will be covered by non-cognate molecules, which represents 10% of the entire DNA.
In the other extreme, for 50 000 molecules, 2 300 000 bp of the DNA will be covered by non-cognate molecules, which represents 50% of the entire DNA.
Similarly, one could use TFsizenc =23 bp and the non-cognate abundance in the interval [20000,100000].
4 VALIDATING THE MODEL Next, we will show some simple tests we conducted to visualize the behaviour of the system, under different conditions.
First, we want to demonstrate how the molecules move on the DNA during a simulation run.
Figure 2 shows an example of a random walk performed by 1 or 3 molecules on a 250 bp randomly generated DNA sequence.
The molecules alternate the 1D movements (high-density regions in Fig.2) with 3D excursions or hops (low-density regions in Fig.2).
One simple test consists of plotting the normalized affinity versus the normalized occupancy for each position on the DNA after the simulator is run for a long-time interval.
The top graph in Figure 3 shows that there is a strong positive correlation between occupancy-bias on the DNA and affinity, in the case of 1 TF molecule in the system.
Furthermore, in the case of multiple molecules of the same TF species, the affinity and occupancy have a strong correlation, but not as good as in the case of 1 molecule (see middle plot of Fig.3).
This suggests that in the case of crowding and competition for DNA space, the affinity between TF molecules and DNA is not the sole determinant of the occupancy-bias.
Inverting this statement, we could say that occupancy-bias is not necessarily equivalent to the affinity landscape, in the sense that regions that are occupied most of the time are not necessarily the highest affinity ones.
However, this was observed at 1 bp resolution and it might be averaged out on larger sectors of DNA.
Finally, we would like to mention that in the Supplementary Material, we systematically investigated the quality of our approach to estimate model parameters.
The results showed that setting the model parameters using our approach leads to negligible errors between the desired system behaviour and the measured one in the simulations.
5 CONSIDERATIONS ON THE MODEL One question that one might ask is whether our coarse-grained model of 3D diffusion will capture all the details of a real 3D particle simulator.
van Zon et al.
(2006) observed that the zero-dimensional Chemical Master Equation can accurately model the association rate between TF molecules and the DNA, as long as the model considers fast re-binding in close proximity after an unbinding event.
Since we implemented fast re-binding in our model (through hops), we 1521 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [16:29 7/5/2012 Bioinformatics-bts178.tex] Page: 1522 15171524 N.R.Zabet and B.Adryan A B Fig.2.
Dynamic behaviour of TF molecules.
We consider a random 250 bp DNA and TF molecules which can bind/unbind, hop, jump, slide left/right.
(A) 1 TF molecule (B) 3 TF molecules.
The position of the molecules is represented on y-axis and the time on the x-axis.
The grey line on the y-axis represents the affinity at that position for a TF.
Note that after a complete dissociation of a TF from the DNA the line that follows the position is broken as opposed to a line connecting two dense regions which describes a hop or a correlated jump.
conclude that the 3D diffusion model employed in this contribution reliably represents the 3D diffusion of TFs in the cytoplasm.
Furthermore, we did not consider the 3D shape of the DNA in our model.
Nevertheless, the shape of the DNA is likely to affect only two variables in the model, namely: (i) the average number of re-bindings [because it is expected that once trapped in dense DNA areas, the TF molecules will find it more difficult to escape the DNA (Bancaud et al., 2009)] and (ii) the areas of the DNA where a TF will hop [hopping is more likely to lead to small 1D displacement, but in the case of close 3D proximity, this might result in more jumping (Lomholt et al., 2009)].
Both of these parameters are fine-tunable within our model, so by increasing the hopping lengths and the number of fast re-bindings, 3D effects could be integrated in the model.
In addition, we also make the assumption that the 1D random walk is unbiased.
However, some previous models of sliding considered that the 1D random walk is biased, in the sense that depending on the left or right side affinities from the current position a TF molecule might have different probabilities to slide in one or the other direction (Slutsky and Mirny, 2004).
This was supported by the fact that the affinity landscape of RNAp seems to increase when moving towards the transcription start site (TSS) and consequently the RNAp can be directed towards the TSS (Weindl et al., 2007).
Barbi et al.
(2004) showed that in the case of bias, the random walk displays initially a sub-diffusive behaviour which can last significantly long.
However, Blainey et al.
(2006) did not observe any anomalous 1D diffusion when a hOgg1 protein would perform a random walk on the DNA in vitro, but rather concluded that the random walk is unbiased.
Since there is no strong experimental evidence for the fact that biased random walk is a general mechanism in the search process, we considered in this contribution that the random walk is unbiased.
Finally, in comparison to a different implementation strategy, the memory model proposed by (Barnes and Chu, 2010), our implementation strategy showed an increase in speed (see Supplementary Material).
The disadvantage of our strategy is that creating an array with the same size as the DNA for each TF species, will result in larger memory requirements compared with the memory model of (Barnes and Chu, 2010).
For the entire genome of E.coli (4.6 Mbp) and two TF species, a non-cognate and a cognate one, the simulator will require approximately 2 GB of RAM.
Although the simulator permits to specify input several species of TF, extra care should be taken when adding new species into the simulator due to the extra memory usage.
Each new TF species added to this system (E.coli) will increase the required memory by a few hundred MB (300 MB) for a DNA sequence of 4.6 Mbp.
1522 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [16:29 7/5/2012 Bioinformatics-bts178.tex] Page: 1523 15171524 Facilitated diffusion in prokaryotes A B C Fig.3.
Affinity vs occupancy.
We consider a random 1000 bp DNA strand and TF molecules which can bind/unbind, hop, jump, slide left/right.
In (A) we show the normalized affinity and normalized occupancy for 1 molecule and in (B) for 10 molecules.
In (C) we plot the ratio between occupancy and affinity, which should be 1 for highly correlated values.
The Pearson, coefficient of correlation between the the affinity and occupancy slightly drops from 0.999 (in the case of 1 molecule) to 0.998 (in the case of 3 molecules) and, further, to 0.979 (in the case of 10 molecules).
However, we consider our strategy as a good compromise in cases were simulation speed is essential.
6 DISCUSSION Previously, facilitated diffusion was modelled mainly analytically (e.g.
Berg et al., 1981; Mirny et al., 2009).
Although these types of models brought new insights into the mechanism, they mainly lack the capability to integrate real DNA sequence (a non-uiniform TF affinity landscape; Mirny et al., 2009) and/or dynamic crowding (mobile roadblocks; Flyvbjerg et al., 2006; Li et al., 2009).
Nevertheless, computational models are able to surpass these shortcomings.
Stochastic simulations have revolutionized the way theoretical biologists can nowadays deal with problems that are not easily amenable to experimental measurements.
TF target finding belongs to a class of spatio-temporal problems that, in a first approximation, may be addressed with tools that simulate 3D diffusion, e.g.
Smoldyn (Andrews et al., 2010).
However, due to the particular behaviour of DNA-binding proteins and the proposed facilitated diffusion mechanism, the Smoluchowski limit is overcome.
For a meaningful outcome from any simulation experiment, a more detailed model is therefore required.
In order to produce results in a relatively short time, previous computational models of facilitated diffusion were limited by size of the analyzed system or level of details included in the model.
For example, the work of (Das and Kolomeisky, 2010) and (Wunderlich and Mirny, 2008) did not consider specific affinities between TF and DNA, while more detailed models as the one presented by (Chu et al., 2009) could consider at most 40 kbp per DNA strand.
Our model includes new features that were not previously considered in this type of modelling, such as TF orientation on the DNA and the fact that TF can cover more base pairs than the actual DNA binding domain.
In addition, we also suggested an implementation strategy that allows for genome-size DNA sequences to be simulated, a clear advantage over previous tools there were limited to few thousands base pairs (Chu et al., 2009).
Not only does our work proposes a detailed model of the facilitated diffusion mechanism with a highly efficient implementation strategy, but also presents a systematic and comprehensive assessment of crucial parameters in this system.
As an example, we use (Elf et al., 2007) measurements of the lac repressor system in E.coli.
In particular, we show that using our comprehensive parameter estimation, our model displays similar 1523 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [16:29 7/5/2012 Bioinformatics-bts178.tex] Page: 1524 15171524 N.R.Zabet and B.Adryan behaviour as in (Elf et al., 2007), i.e.
residence time of tR =5 ms, actual sliding lengths of sobsl =90 bp, the relative time the molecules stays bound to the DNA of f =0.9 and the 1D diffusion coefficient of 0.046 m2s1.
It can therefore be concluded that for future studies on TF target finding in prokaryotic systems, our model represents an ideal entry point for stochastic simulations.
ACKNOWLEDGEMENTS We would like to thank Robert Foy and Robert Stojnic for useful discussions and comments on the manuscript.
Funding: Medical Research Council [G1002110 to N.R.Z.]
and Royal Society [to B.A.].
Conflict of Interest: none declared.
ABSTRACT Motivation: Membrane proteins are an important class of biological macromolecules involved in many cellular key processes including signalling and transport.
They account for one third of genes in the human genome and450% of current drug targets.
Despite their im-portance, experimental structural data are sparse, resulting in high expectations for computational modelling tools to help fill this gap.
However, as many empirical methods have been trained on experi-mental structural data, which is biased towards soluble globular pro-teins, their accuracy for transmembrane proteins is often limited.
Results: We developed a local model quality estimation method for membrane proteins (QMEANBrane) by combining statistical poten-tials trained on membrane protein structures with a per-residue weighting scheme.
The increasing number of available experimental membrane protein structures allowed us to train membrane-specific statistical potentials that approach statistical saturation.
We show that reliable local quality estimation of membrane protein models is pos-sible, thereby extending local quality estimation to these biologically relevant molecules.
Availability and implementation: Source code and datasets are available on request.
Contact: torsten.schwede@unibas.ch Supplementary Information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Protein modelling plays a key role in exploring sequence struc-ture relationships when experimental data are missing.
Modelling techniques using evolutionary information, in particular homology/comparative modelling, developed into standardized pipelines over recent years.
An indispensable ingredient of such a pipeline is the accuracy estimation of a protein model, directly providing the user with information regarding the range of its possible applications (Baker and Sali, 2001; Schwede, 2013; Schwede et al., 2009).
In this context, global model quality as-sessment tools are important for selecting the best model among a set of alternatives, whereas local model estimates assess the plausibility and likely accuracy of individual amino acids (Benkert et al., 2011; Fasnacht et al., 2007).
Various techniques have been developed to address this question, with consensus methods and knowledge-based approaches showing best results in blind assessments (Kryshtafovych et al., 2014).
Consensus approaches require an ensemble of models with structural var-iety, reflecting alternative conformations (Roche et al., 2014; Skwark and Elofsson, 2013).
In contrast, knowledge-based methods (such as statistical po-tentials) can be applied to single models but are in general less accurate than consensus methods and exhibit strong dependency on the structural data they have been trained on.
The unique physicochemical properties of biological mem-branes give rise to interactions that are energetically discouraged in soluble proteins, and vice versa (White, 2009).
However, most scoring functions using knowledge-based methods (Benkert et al., 2011; Luthy et al., 1992; Ray et al., 2012; Sippl, 1993; Zhou and Zhou, 2002) have been trained on soluble proteins.
Thus, they perform poorly when applied to models of membrane proteins.
This specific, but highly relevant, important aspect of protein model quality assessment has received only little atten-tion in recent years (Heim and Li, 2012; Ray et al., 2010).
With the growing amount of available high resolution membrane pro-tein structures (Garman, 2014; White, 2004) the template situ-ation for homology modelling procedures is improving quickly and, even more important for this work, it is gradually becoming possible to adapt knowledge-based methods to this class of models.
As a result of such efforts, we present QMEANBrane, a com-bination of statistical potentials targeted at local quality estima-tion of membrane protein models in their naturally occurring oligomeric state: after identifying the transmembrane region using an implicit solvation model, specifically trained statistical potentials get applied on the different regions of a protein model (Figs 1 and 2).
To overcome statistical saturation problems, a novel approach for deriving statistical potentials from sparse training data has been devised.
We have benchmarked the per-formance of the approach on a large heterogeneous test set of models and illustrate the result on the example of alignment errors in a transmembrane model.
2 METHODS 2.1 Target function The similarity/difference between a model and a reference structure can be expressed in the form of distances between corresponding atoms in the model and its reference structure after performing a global superposition.
However, this global superposition approach fails to give accurate results in case of domain movements.
To overcome such problems, e.g.
in the context of the CASP (Moult et al., 2014) experiments, the structures are manually split into so-called assessment units and evaluated separately (Taylor et al., 2014).
This manual procedure is time consuming and not suitable for automate large-scale evaluation, e.g.
such as performed by CAMEO (Haas et al., 2013).
Alternatively, similarity/difference between a model and reference structure can be expressed in the form of super-position-free measures such as the local Distance Difference Test (lDDT) score (Mariani et al., 2013) assessing the differences in interatomic*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com mailto:torsten.schwede@unibas.chemploying while ,-employing After In order t , Function so large XPath error Undefined namespace prefix distances between model and reference structure.
In this work, the lDDT inclusion radius is set to 10A to ensure local behaviour.
See Supplementary Figure S2 for a comparison of different structural simi-larity measures (C-distance, dRMSD, lDDT and CAD score; Olechnovic et al., 2013).
2.2 Membrane segment definition The OPM database (Orientations of Proteins in Membranes; Lomize et al., 2006a) applies minimization of a free energy expression to predict the transmembrane part of a protein structure.
In this work, we use a similar but simplified approach, still resulting in a robust approximation of the membrane segment definition.
The energy expression is defined as "G= X i wat!bilf zi ASAi 1 with wat!bil representing the transfer energy from water to decadiene for atom i per A2 (Lomize et al., 2004), f(zi) the hydrophobicity as a function of the distance to the membrane centre zi and ASAi the accessible surface area of atom i in A2 as calculated with NACCESS (www.bioinf.manches ter.ac.uk/naccess).
Not all surface-facing atoms, as determined by NACCESS are in contact with the membrane, even if they fall in between the lipid bilayer, e.g.
as is the case for hydrophilic pores.
To determine the subset of surface atoms in direct contact with the lipid bilayer, the protein structure surface as calculated by MSMS (Sanner et al., 1996) is placed onto a 3D grid, marking every cube in the grid containing surface ver-tices.
The application of a flood fill algorithm (http://lodev.org/cgtutor/ floodfill.html) on every layer along the z-axis then allows the generation of a subset of potentially membrane facing atoms.
The parameters describing the membrane (i.e.
tilt angle relative to z-axis, rotation angle around z-axis, membrane width and distance of membrane centre to origin) first undergo a coarse grained sampling to identify the 10 best parameter sets for further refinement using a LevenbergMarquardt minimizer.
This procedure is repeated several times with different initial orientations of the structure to find the set of parameters leading to the lowest total free energy.
The bilayer consists of a hydrocarbon core flanked by interface regions with a large chemical heterogeneity (White et al., 2001).
It is known that the properties of a membrane protein are strongly influenced by the interaction with the phospholipid bilayer, and a simple split into a mem-brane and soluble part would not faithfully reflect the variation of mo-lecular properties along the membrane axis (Bernsel et al., 2008).
To catch these variations along the membrane axis, we split the transmembrane proteins into three parts, which are treated separately: an interface part consisting of all residues with their C atom positions within 5A of the membrane defining planes, a core membrane part consisting of all resi-dues with their C atom positions in between the two membrane defining planes not intersecting with the interface residues and finally, a soluble protein part consisting of all remaining residues.
2.3 Model quality predictors To assess the membrane protein models quality, we mainly rely on stat-istical potential terms, combined with the relative solvent accessibility of each residue as calculated by DSSP (Kabsch and Sander, 1983).
The four statistical potential terms (their exact parameterizations are described in the Supplementary Material), are the following: (1) All-atom Interaction Term: Pairwise interactions are considered between all chemically distinguishable heavy atoms.
A sequence separation threshold has been introduced to allow focusing on long-range interactions and reduce the influence of local secondary structure.
Interactions originating from atoms of residues closer in sequence than this threshold are neglected.
(2) C Interaction Term: This term assesses the overall fold by only considering pairwise interactions between C positions of the 20 standard amino acids.
In case of glycine, a representative of the C position gets constructed using the backbone as anchor.
The same sequence separation as in the all-atom interaction is applied.
(3) Solvation Term: Statistics are created by counting surrounding atoms around all chemically distinguishable heavy atoms not be-longing to the assessed residue itself.
(4) Torsion Term: The central/ angles of three consecutive amino acids are assessed based on the identity of the involved amino acids using a grouping scheme described by Solis and Rachovsky (Solis and Rackovsky, 2006).
Fig.2.
Local QMEANBrane scores mapped on the best performing model (mod9jk) regarding RMSD of the GPCR Dock experiment 2008.
Reference structure (2.6 A crystal structure of a human A2A ad-enosine receptor bound to ZM241385, PDB: 3eml) and membrane-defin-ing planes are shown in white Fig.1.
Difference between membrane predictions of our algorithm and the predictions of OPM on the 200 high-resolution structures used to train membrane-specific statistical potentials i506 G.Studer et al.
Supplemental Segment Definition ) ( , employ : With <inlinemediaobject><imageobject><imagedata fileref= IG(IG) , www.bioinf.manchester.ac.uk/naccess www.bioinf.manchester.ac.uk/naccess three-dimensionalcenter ten--Quality Predictors supplemental long Glycine The torsion term trained on soluble structures is applied to the whole membrane protein model.
Conversely, solvation and interaction terms are specifically trained for and applied to the soluble, membrane and inter-face segments with different potentials for-helical and-barrel trans-membrane structures.
A residue belonging to one of these parts interacts with all atoms in the full model, and a final score is assigned by averaging all scores originating from interactions associated with this specific resi-due.
For the solvation and torsion terms, we use a formalism closely related to the statistical potentials of mean force (Sippl, 1990).
However, instead of referring to an energy expression, we rather look at the problem as a log odds score between the probability of observing a particular interaction between partner s with conformation c relative to some reference state: S cjs =ln p cjs pc  2 In case of sparse data, p(cjs) cannot be expected to be saturated.
Sippl and co-workers have proposed to use a combination of the extracted sequence-specific probability density function (pdf) and the reference state.
The influence of the reference state vanishes at a rate determined by the newly introduced parameter towards large numbers of inter-actions (N) with sequence s: p cjs  1 1+N pc+ N 1+N p cjs 3 Using the aforementioned formalism, this leads to S cjs  ln 1+N  ln 1+N p cjs pc  4 Because of the increased abundance of structural information for soluble protein structures during the last decades, the use of the parameter has become largely unnecessary.
However, for membrane proteins, data scar-city is still an issue and needs to be handled accordingly.
In the Supplementary Materials, an analysis of the saturation behaviour of the different statistical potential terms is provided, suggesting a sufficient amount of training data for the solvation term, whereas the two inter-action terms require more data to be fully saturated (Supplementary Fig.S1).
For these cases, we introduced a treatment for sparse data by assuming that the statistics for soluble proteins are fully saturated.
In other words, if there are no sufficient data available from membrane structures, we refer to the information we have from all protein structures to get a hybrid score: HS cjs =ln 1 1+N f1+ N 1+N f2  =ln1+N lnf1+Nf2 5 With f1 representing the fraction of the probabilities of sequence-specific interactions and a reference state, where the pdfs of the specific inter-actions are saturated, and f2 the fraction between the probabilities of sequence-specific interactions and a reference state, where the pdfs of the specific interactions are not necessarily saturated, as it may occur for membrane-and interface-specific cases.
For regions of the pdf with zero probability as they, for example, occur at low distances in pairwise interaction terms, we applied a constant cap value to avoid infinite scores.
2.4 Training datasets for statistical potentials The pdfs to calculate the statistical potentials for the soluble part are built using statistics extracted from a non-redundant set of high resolution X-ray structures.
PISCES (Wang and Dunbrack, 2003) has been used with the following parameters: sequence identity threshold 20%, reso-lution threshold 2 A and R-factor threshold 0.25.
Because only standard amino acids can be handled by QMEANBrane, a prior curation of the training structures is necessary.
Non-standard amino acids such as phos-pho-serine or seleno-methionine have therefore been mapped to their standard parent residues.
For the selection of appropriate membrane protein structures, we rely on the OPM database (Lomize et al., 2006b).
As of October 2013, OPM contained 746 unique PDB IDs of structures with transmembrane segments.
Applying a resolution thresh-old of 2.5 A, removing all chains with530 membrane-associated residues and considering only one chain in case of homo-oligomers results in 283 remaining chains from 200 structures.
Clustering the chains based on their SEQRES sequences with kClust (Hauser et al., 2013) using a se-quence identity threshold of 30% resulted in 187 clusters, 140 of them from helical transmembrane structures and 47 from-barrel structures.
All entries are used in the calculation of the pdfs, where a chain originat-ing from a cluster with n members is downweighted and contributes with a weight of 1/n to the final distributions.
These final distributions have then been extracted by considering the corresponding chains, using the full protein structure in the oligomeric state as assigned by OPM as environment.
2.5 Datasets for training linear combinations A set of 3745 models for soluble proteins was generated by selecting a set of non-redundant high-resolution reference structures from the PDB using PISCES (maximum 20% sequence identity, resolution better 2A, X-ray only), extracting their amino acid sequences, and building models using the automated SWISS-MODEL pipeline (Kiefer et al., 2009) by excluding templates with a sequence identity 490% to the target (P. Benkert, personal communication).
OPM was used to identify refer-ence structures (resolution53.0 A) to generate membrane protein models.
Structures with530 membrane-associated residues and hetero-oligomeric complexes were excluded.
In all, 132 unique PDB IDs, which had more than one suitable template, have been selected as targets for modelling.
Templates identified with HHBlits (Remmert et al., 2012) showing a se-quence alignment coverage450% served as input for MODELLER (Sali and Blundell, 1993) and resulted in 3226 models with oligomeric states equivalent to the template structure.
Removal of redundancy, i.e.
models originating from templates with same sequence, and removal of obvious incorrect oligomeric states upon visual inspection resulted in a set of 557 models, 386 with helical transmembrane parts and 171-barrels.
2.6 Spherical smoothing for noise reduction Averaging/smoothing can reduce noise introduced by quality predictors on a per-residue level, resulting in single residue scores, which more ac-curately reflect the local model quality.
Smoothing in space tends to outperform sequential smoothing.
In the proposed algorithm, every resi-due gets represented by its C position.
The final quality predictor score for a residue is calculated as a weighted mean of its own value and the values associated to surrounding residues: si= X j wjsj 6 with si representing the final score at position i, wj the weight of score at position j and sj the score at position j.
The weights are calculated in a Gaussian-like manner and normalized, so they sum up to one: wj= 1 Z ffiffiffiffiffiffiffiffiffiffi 22 p e 1 2 dij  2 dij53 0 else 8>>>< >>>: 7 with wj representing the weight of score at position j, dij the distance from position i to position j, the standard deviation of the Gaussian-like formalism to control how fast the influence of a neighbouring score vanishes as a function of the distance (5 A turned out to be a reasonable ) and Z as normalization factor.
i507 Assessing the local structural quality of transmembrane protein models , alpha beta ",0,0,2 ",0,0,2 sequence <inlinemediaobject><imageobject><imagedata fileref= IG(IG) : Due to supplemental sequence interface Data Sets Statistical Potentials Since , less than membrane a total of &beta;-Sets Training Linear Combinations .
less than membrane , above &beta; Smoothing Noise Reduction per With <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) , <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) W <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) 2.7 Per amino acid weighting scheme QMEANBrane uses a linear model fitted on the per-residue lDDT score to combine the single quality predictors.
To remove amino acid-specific biases, such a linear model is trained for every standard amino acid: si= X j wjsij 8 si is the combined score of residue at position i, wj the weight of quality predictor j and Sij the score of quality predictor j at position i.
2.8 Implementation QMEANBrane is designed on a modular basis, implementing computa-tionally expensive tasks in a C++ layer.
All functionality is made fully accessible from the Python language and can directly be embedded into the computational structural biology framework OpenStructure (Biasini et al., 2010, 2013), allowing to assemble custom assessment pipelines to address more specific requirements.
3 RESULTS AND DISCUSSION 3.1 Membrane prediction accuracy To evaluate the performance of our membrane finding algo-rithm, a comparison with the result obtained by OPM has been performed on the 200 structures used for training of the membrane-specific statistical potentials.
At this point, OPM is assumed to be the gold standard, even though it is a calculation by itself.
By further considering the membrane width as the main feature of accuracy, 95% of the absolute width deviations are 54 A.
In terms of translational distances, this corresponds to a misprediction of 23 residues for helices and about 12 residue for sheets (Fig.1).
Interestingly, using this approach, it is not only possible to automatically detect transmembrane regions but also to distinguish between transmembrane and soluble struc-tures in general (Supplementary Fig.S3).
3.2 Performance on the test dataset For a first analysis of performance on predicting local scores of membrane-associated residues in transmembrane protein models, we used the previously described model set for training the linear weights.
Clusters have been built by applying kClust on the target sequences with a sequence identity threshold of 30%.
The local scores for the membrane-associated residues of one cluster have then been predicted using linear models trained on all residues from models not belonging to that particular cluster (Table 1; Supplementary Fig.S6).
3.3 Independent performance evaluation on models of the GPCR Dock experiments Not many independent compilations of membrane protein models with known target structures exist.
For a performance evaluation and comparison with other widely used quality assess-ment tools, we rely on the models generated during the GPCR Dock experiments 2008/2010 (Kufareva et al., 2011; Michino et al., 2009) (Fig.2).
A total of 491 models for three different targets, the human dopamine receptor, the human adenosine receptor and the human chemokine receptor were available.
Receiver operating characteristic (ROC) analysis with the local lDDT as target value has been performed on all membrane-associated residues as defined by OPM, showing a clear super-iority of QMEANBrane over other methods such as ProQ2 (Ray et al., 2012), QMEAN (Benkert et al., 2011), ProQM (Ray et al., 2010), Prosa (Wiederstein and Sippl, 2007), Verify3D (Luthy et al., 1992) or DFire (Zhou and Zhou, 2002) (Fig.3).
Removing all GPCR/Rhodopsin structures from the training data has only a minor effect.
See Supplementary Figure S4 for a more detailed performance analysis taking other measures of similarity into account.
Because ProQM is the only other method specifically developed for the particular case of membrane protein model quality assessment, we also performed a direct comparison of QMEANBrane and ProQM on the dataset used to test/train ProQM in Supplementary Figure S5.
3.4 Retrospective analysis of modelling examples To illustrate the usefulness of QMEANBrane in tackling prob-lems as they occur in real modelling cases, two targets with known structures have been selected for a more detailed analysis using the recently released SWISS-MODEL workspace (Biasini et al., 2014).
The H+ translocating pyrophosphatase from Vigna Fig.3.
ROC analysis of all membrane-associated residues of the models of the GPCR Dock experiments with local lDDT as target value and a class cutoff of 0.6 Table 1.
Performances of single quality predictors and their combination on membrane-associated residues in our test set, measured as Pearsons r between predicted score and actual local lDDT Quality predictor Helical structures-barrel structures Exposed 0.39 0.15 Torsion 0.43 0.47 C interaction 0.51 0.49 Solvation 0.55 0.51 All atom interaction 0.63 0.58 All predictors combined 0.71 0.67 Note: Even for single predictors, an amino acid-specific linear model has been trained to remove amino acid-specific biases.
i508 G.Studer et al.
Amino Acid Weighting Scheme per acid <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) <inlinemediaobject><imageobject><imagedata fileref= IG(IG) , <inlinemediaobject><imageobject><imagedata fileref= <inlinemediaobject><imageobject><imagedata fileref= <inlinemediaobject><imageobject><imagedata fileref= 1 1 ; Biasini , etal., 2013 &amp; Prediction Accuracy membrane below ",0,0,2 ",0,0,2--,Test Dataset membrane membrane , Performance Evaluation Models Experiments to membrane ,Since Analysis Modelling Examples radiata (PDB ID: 4A01) and a dopamine transporter of Drosophila melanogaster (PDB ID: 4M48).
Models based on different target-template alignments have been compared to test QMEANBranes capability of detecting incorrect align-ments, particularly alignment shifts in transmembrane helices.
(Alignments are available in the Supplementary Materials.)
The pyrophosphatase has, with the sodium translocating pyrophosphatase from Thermotoga maritima (PDB ID: 4AV3), a rather close homologue (sequence identity 440%).
Nevertheless, the alignments provided by BLAST (Altschul et al., 1990) and HHBlits differ significantly.
Because the BLAST alignment has a lower coverage, not including the first transmembrane helix, only the part covered by both alignments is considered.
Supplementary Figure S7 shows a comparison of the QMEANBrane scores from the two models built with the different alignments.
Two transmembrane helices contain an alignment shift of three residues, resulting in a clear local increase of the QMEANBrane scores of the model built with the HHBlits alignment relative to the model built with the BLAST alignment.
The higher quality of the HHBlits model gets confirmed by its global lDDT of 0.63 versus 0.59 of the BLAST model.
For the dopamine transporter example, we chose an amine transporter from Aquifex aeolicus VF5, identified by HHBlits with a sequence identity of 24%, as the primary template.
Despite the good coverage, a major problem occurs in transmem-brane helix 5.
The initial HHBlits alignment has an insertion of three residues enforcing a helix break and an unnatural bulge within the transmembrane part.
To analyse possible modifica-tions of the initial alignment, we rely on QMEANBrane to compare the relative differences in the models with alternative alignments with the initial model (Figs 4 and 5).
Three different alternative alignments were considered: the first is to shift the helix insertions towards the C-terminus.
Despite the increase of the QMEANBrane score at the location of the alignment modification, the scores in helix 5 towards the C-terminus drop significantly, suggesting no improvement of the overall model quality.
As second alternative, the insertion has been shifted into the loop connecting transmembrane helices 4 and 5.
Because of their proximity, a distortion of both involved helix endings was inevitable, thus unfavourable.
The third alter-native, shift of the insertion towards the N-terminus before helix 4, and introducing an additional deletion in the aforementioned loop increasing the local sequence identity in helix 4, consistently increases the QMEANBrane scores in helices 4 and 5, as well as the helices close in space.
These findings are confirmed by the global lDDT scores of the models built based on those align-ments (initial alignment: 0.54, shift into middle: 0.54, shift towards C-terminus: 0.53, shift towards N-terminus: 0.57).
4 CONCLUSION Investigating function and interactions in membrane proteins is an active field of research, with modelling techniques as an im-portant tool to bridge the gap when structural data are missing.
Comparative modelling methods automatically profit from the increased number of available experimental membrane struc-tures, which can be used to build models for membrane proteins (Forrest et al., 2006).
However, most knowledge-based Fig.5.
Structural effects of the alignment modifications shown in Figure 4.
The model based on the initial HHBlits alignment is coloured white; the other models are coloured according to the horizontal bar alignment representation in Figure 4 Fig.4.
Difference of QMEANBrane scores of three dopamine trans-porter models with modified alignments versus the model built with the initial HHBlits alignment, represented by the first horizontal bar.
Insertions are marked black, and deletions are marked white.
Second bar: shift of the insertion towards the N-terminus in front of helix 4, third bar: shift of insertion towards the N-terminus in between helices 4 and 5, fourth bar: shift of the insertion towards the C-terminus i509 Assessing the local structural quality of transmembrane protein models to supplemental Since The helix Due to is knowledge  approaches fail in assigning reliable local quality estimates when confronted with the unique structural features and interactions resulting from direct contact with the phospholipid bilayer.
With QMEANBrane, we present a framework that widely covers the aspects of membrane protein model quality assess-ment.
In a first step, our membrane detection method allows to reliably locate the transmembrane part of the model.
We introduce an interface region to account for the non-isotropy of protein properties along the z-axis.
Statistical potential terms were trained specifically for these three regions, introdu-cing a new hybrid potential formalism to circumvent problems arising from a lack of sufficient training data.
The final local scores are then calculated using linear models trained for all 20 standard amino acids.
We could show a clear improvement in accuracy over widely used quality assessment methods when con-sidering alpha-helical transmembrane structures.
It is possible to detect errors introduced in the modelling procedure such as in-correct alignments, which would facilitate the visual exploration of alternative alignments, e.g.
as suggested previously in (Barbato et al., 2012).
Despite similar observed overall performance for-barrel structures, problems arise with shifted alignments, as they can occur when aligning sequences from remote homologues.
The low number of pairwise atomic interactions in combination with the regular hydrophobicity pattern often observed in align-ment shifts by two residues hamper the reliable detection of such errors and require further investigation.
ACKNOWLEDGEMENTS We would like to thank all members of the Torsten Schwede group at the University of Basel for helpful discussions.
We are grateful to Pascal Benkert for providing the training model dataset.
Special thanks go to J urgen Haas, Lorenza Bordoli and Tjaart de Beer for valuable inputs and helping in setting up the manuscript and Alessandro Barbato for his help in performing the performance analysis using different quality assessment methods.
The authors gratefully acknowledge the computational resources provided by the sciCORE/[BC]2 Basel Computational Biology Center at the University of Basel and the support by the system administration team.
Funding: This work was supported by SIB Swiss Institute of Bioinformatics.
G.S.
has been supported by a PhD fellowship of the SIB by the Swiss Foundation for Excellence and Talent in Biomedical Research.
Conflict of interest: none declared.
ABSTRACT Motivation: With the exponential growth of expression and protein protein interaction (PPI) data, the frontier of research in systems biology shifts more and more to the integrated analysis of these large datasets.
Of particular interest is the identification of functional modules in PPI networks, sharing common cellular function beyond the scope of classical pathways, by means of detecting differentially expressed regions in PPI networks.
This requires on the one hand an adequate scoring of the nodes in the network to be identified and on the other hand the availability of an effective algorithm to find the maximally scoring network regions.
Various heuristic approaches have been proposed in the literature.
Results: Here we present the first exact solution for this problem, which is based on integer-linear programming and its connection to the well-known prize-collecting Steiner tree problem from Operations Research.
Despite the NP-hardness of the underlying combinatorial problem, our method typically computes provably optimal subnetworks in large PPI networks in a few minutes.
An essential ingredient of our approach is a scoring function defined on network nodes.
We propose a new additive score with two desirable properties: (i) it is scalable by a statistically interpretable parameter and (ii) it allows a smooth integration of data from various sources.
We apply our method to a well-established lymphoma microarray dataset in combination with associated survival data and the large interaction network of HPRD to identify functional modules by computing optimal-scoring subnetworks.
In particular, we find a functional interaction module associated with proliferation over-expressed in the aggressive ABC subtype as well as modules derived from non-malignant by-stander cells.
Availability: Our software is available freely for non-commercial purposes at http://www.planet-lisa.net.
Contact: tobias.mueller@biozentrum.uni-wuerzburg.de 1 INTRODUCTION Construction and analysis of large biological networks have become major research topics in systems biology (Aittokallio and Schwikowski, 2006).
Various aspects have been analyzed including the inference of cellular networks from gene To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
expression (Friedman, 2004), network alignments (Flannick et al., 2006; Kelley et al., 2003; Sharan and Ideker, 2006) and other related strategies as reviewed by Srinivasan et al.
(2007).
At the same time, well-established microarray technologies provide a wealth of information on gene expression in various tissues and under diverse experimental conditions.
Integrating protein protein interaction (PPI) and gene-expression data generates a meaningful biological context in terms of functional association for differentially expressed genes.
Frequently, large scale expression profiling studies investigate many experimental conditions simultaneously, thereby generating multiple P-values.
Especially in tumor biology expression profiling has become a well-established tool for the classification of different tumors and tumor subtypes.
Furthermore, in the clinical context, various patient-associated data are available thatin conjunction with expression dataprovide valuable information of the influence of specific genes on disease-specific pathophysiology.
In particular the analysis of survival data allows to establish gene expression signatures to make predictions about the prognosis and to assess the disease relevance of certain genes.
However, the cellular function of an individual gene cannot be understood on the level of isolated components alone, but needs to be studied in the context of its interplay with other gene products.
The combined analysis of expression profiles and PPI data thus allows the detection of previously unknown dysregulated modules in interaction networks not recognizable by the analysis of a priori defined pathways.
Ideker et al.
(2002) have proposed to identify interaction modules in this setting by devising firstly an adequate scoring function on networks and secondly an algorithm to find the high-scoring subnetworks.
The underlying combinatorial problem has been proven to be NP-hard for additive score functions defined on the nodes of the network.
The authors proposed a heuristic strategy based on simulated annealing and developed a score to measure the significance of a subnetwork that includes the integration of multivariate P-values.
This score has been extended by Rajagopalan and Agarwal (2005) to incorporate an adjustment parameter in order to obtain smaller subgraphs in conjunction with a greedy search algorithm.
This approach however, excludes the possibility to combine multiple P-values.
Variants of greedy search strategies have also been used by Nacu et al.
(2007) and Sohler et al.
(2004).
Subsequently Cabusora et al.
(2005) proposed an edge score by adapting the scoring concept of Ideker et al.
(2002).
2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[20:14 18/6/03 Bioinformatics-btn161.tex] Page: i224 i223i231 M.T.Dittrich et al.
An alternative edge scoring based on correlation of gene expression has been proposed by Guo et al.
(2007).
All the former methods are heuristic approaches that cannot guarantee to identify the maximally scoring subgraph.
Some of these often computationally demanding approaches tend to deliver large high-scoring networks, which may be difficult to interpret.
Here we present a novel approach (i) that is characterized by a modular scoring function, based on signal-noise decomposition implemented as a mixture model, (ii) permits the smooth integration of multivariate P-values derived from various sources, (iii) delivers provably optimal and suboptimal solutions to the maximal-scoring subgraph problem by integer-linear programming (ILP) in reasonable running time and (iv) allows to control the resultant subnetwork size by an adjustment parameter, which is statistically interpretable as false-discovery rate (FDR).
The presented algorithm is, to our knowledge, the first approach that really tackles and solves the original problem raised by Ideker et al.
(2002) to optimality.
We strongly believe that the optimal and suboptimal solutions produced by our method provide a considerable benefit over heuristic solutions in that they allow for a sound evaluation and adaptation of the underlying model.
Given only a heuristic solution it is impossible to decide whether poor results are due to inappropriate parameter settings or due to the optimality gap.
Based on extensive simulations we evaluate our exact approach in comparison to the heuristic method proposed by Ideker et al.
(2002).
Finally, analyzing a comprehensive microarray and survival dataset of lymphoma patients we detect functional modules, extending the results of Rosenwald et al.
(2002).
The remainder of this article is organized as follows: after describing the data and methods we use (Section 2), we introduce our approach and its application to lymphoma interactome data in Section 3.
Section 4 presents the subnetworks we found and a validation of our approach in comparison to the method by Ideker et al.
We conclude with Section 5, where we discuss our findings.
2 METHODS 2.1 Microarray and survival data We used the published gene-expression data set from diffuse large B-cell lymphomas (DLBCL) (Rosenwald et al., 2002).
In particular, gene expression data from 112 tumors with the germinal center B-like phenotype (GCB DLBCL) and from 82 tumors with the activated B-like phenotype (ABC DLBCL) were included in this study.
Gene expression analysis was performed on the Lymphochip including 12 196 cDNA probe sets (Rosenwald et al., 2002).
In addition, survival information was available from 190 patients (Rosenwald et al., 2002).
Statistical analyses were performed using the software package R (R Development Core Team, 2006) and Bioconductor (Gentleman et al., 2004) and the routines implemented in limma (Smyth, 2004).
For normalization of gene expression within arrays we used the loess method, normalization between arrays was performed by using the scale method to adjust the log ratios to the same median absolute deviation (MAD) across arrays as detailed in Blenk et al.
(2007).
For subsequent analyses the expression values for different spots of the same gene have been aggregated by taking the median.
Significance of differential expression between the two subtypes ABC and GCB was calculated using robust statistics based on linear models and a moderated t-test (Smyth, 2004).
Survival analysis was performed by Cox regression as implemented in the R-package survival (Andersen and Gill, 1982; Therneau et al., 1990).
2.2 Network The dataset of literature-curated human PPI has been obtained from HPRD (Mishra et al., 2006; Peri et al., 2003).
Using R and the network structures and algorithms in the Bioconductor packages graph and RBGL (Carey et al., 2005), we derived an interactome network consisting of 36 504 interactions between 9392 different proteins.
With the subset of genes shared between the interactome dataset and the chip a Lymphochip-specific interactome network was derived as the vertex-induced subgraph.
The resulting network comprises 2561 different gene products and 8538 interactions, with a large connected component of 2034 proteins (79.4%) and 8399 interactions (98.4%).
The remaining proteins were either non-interacting singletons (472) or tiny clusters of sizes between two and six (23).
Our analysis focuses exclusively on the giant connected component.
visualization and further network analysis was performed with Cytoscape (Cline et al., 2007; Shannon et al., 2003).
2.3 Optimization algorithm Our algorithm is based on the software dhea (district heating) from Ljubic et al.
(2006).
We have extended the C++ code in order to generate suboptimal solutions and have created several Python scripts to control the transformation to a Steiner tree problem, the use of dhea and the re-transformation to a PPI subnetwork.
The dhea code uses the commercial CPLEX callable library version 9.030 by ILOG, Inc. (Sunnyvale,CA) (Sunnyvale, CA).
All experiments were run on a 64 bit 2.2 GHz Opteron Intel with 8 GB of main memory.
Our software and the datasets used in this study are publicly available for academic and research purposes within the heinz (heaviest induced subgraph) package of the open source library LiSA (http://www.planet-lisa.net).
3 SCORING FUNCTION AND ALGORITHM This section introduces our new integrated exact approach to support the identification of functional modules in PPI networks.
Section 3.1 focuses on the order statistics-based method to determine score values for the network nodes.
We illustrate our approach by analyzing a network obtained by combining the data from a expression profiling study of lymphoma patients (Rosenwald et al., 2002) with the comprehensive interactome data from HPRD (Peri et al., 2003).
We derive P-values from the analysis of differential expression between two tumor subtypes (ABC and GCB) as well from the analysis of survival data by Cox regression for each node in the interaction network.
Section 3.2 describes how the score values will be used as input for the maximum-weight connected subgraph (MWCS) problem and a novel algorithmic approach based on mathematical optimization.
Our algorithm solves this problem to provable optimality and, furthermore, is able to enumerate sufficiently distinct suboptimal solutions.
3.1 Statistics of scoring function 3.1.1 Aggregation statistics of P-values Having annotated each node of the interaction network with experimentally derived P-values, we are faced with the problem to aggregate these P-values into one number.
A simple aggregation statistics proposed in the literature is the so-called Fishers method, which combines n P-values pi by 2 n i=1 log(pi)2(n) (Fisher, 1948).
This method however does not provide the necessary flexibility to control the number of significant P-values, instead it only provides a significance measurement over the entire set of P-values.
Due to the heterogeneous nature of the data a more flexible approach is i224 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i225 i223i231 Identification of optimal PPI subgraphs required.
Therefore we use an aggregation statistic based on the distribution of the order statistics of P-values.
Let X1,...,Xn be independently identically distributed (iid) then the density of the ith smallest observation X(i) is given by fX(i) (x)= n!
(ni)!(i1)!
f (x)F(x) i1(1F(x))ni, (1) where F(x) denotes the probability density function of Xi, for i=1,...,n (Lindgren, 1993).
Now we propose to aggregate the P-values at each node in the network by asking for its ith order statistic of the associated P-values, resulting in one P-value of P-values.
Because P-values are uniformly distributed under the null hypothesis (Wasserman, 2005), we apply Equation (1) with density fX (x)=1 and density function FX (x)=x and get X(i) n!(ni)!(i1)!
1x i1(1x)ni, 0x1, (2) or, in other words, X(i) B(i,ni+1) with the associated cumulative distribution function FX(i) = n!
(ni)!(i1)!
x 0 zi1(1z)ni dz, where B(,) denotes the beta distribution.
Applying Equation (2), each gene in the network can be assigned an overall P-value given by the ith order statistic.
This approach is also applicable in case of missing data: for missing P-values the ith order statistic can be used after correcting the parameter n in Equation (2) appropriately.
3.1.2 Signal-noise decomposition Based on these aggregated P-values we derive our new scoring function.
Following Pounds and Morris (2003) we consider the distribution of the P-values as a mixture of a noise and a signal component.
The signal is assumed to be B(a,1) distributed whereas the noise is B(1,1)= uniform (0,1) distributed (P-values under the null hypothesis).
The B(a,b) distribution is given by f (x)= (a+b) (a)(b) xa1(1x)b1, where () denotes the gamma function.
Thus the distribution of the derived P-values reduces to f (x |a,)=+(1)axa1 for 0<x1;0<a<1 with mixture parameter and shape parameter a of the beta distribution.
For given data x=x1,...,xn the log likelihood is defined as logL(,a;x)= n i=1 log(+(1)axa1i ), and consequently the maximum-likelihood estimations of the unknown parameters are given by [,a]=argmax,aL(,a;x).
We obtain both parameters by numerical optimization using the L-BFGS-B method (Byrd et al., 1995) as implemented in R. For the lymphoma dataset analyzed here we obtained a value of 0.536 for the mixture parameter and 0.276 for the shape parameter a of the beta distribution.
This relates to signal and noise proportions of 46.4% versus 53.6%, respectively.
Since P-values are uniformly distributed under the null hypothesis the noise component will be adequately modeled by a uniform distribution.
Modeling the signal component by a beta distribution is justified by Figure 1 and a QuantileQuantile (QQ) plot (data p-values (second order statistics) D en si ty 0.0 0.2 0.4 0.6 0.8 1.0 10 8 6 4 2 0 12 14 Fig.1.
The fitted mixture model fits nicely the empirical distribution.
The parameters of the mixture model are a=0.276 and =0.563.
The histogram of the observed P-values displays the strong consistency with the expected densities under the fitted model (red line).
The blue line indicates the fraction of P-values derived from the uniform noise model.
The excellent fit of the model has been confirmed in a QQ plot of the fitted distribution versus the empirical P-value distribution (data not shown).
not shown).
This is furthermore supported by the associated QQ plot of the fitted density function with the empirical distribution function, which is extremely close to a straight line (data not shown).
These analyses indicate that the signal is well-captured by the beta distribution.
Our aim is to develop an additive score, where positive values signify signal content and negative values denote background noise.
Inspired by the ideas of the likelihood ratio test our approach is as follows: for the fitted parameter a the signal component is equal to the B(a,1) density, whereas that of the noise component is given by B(1,1).
Since B(1,1) is equivalent to the uniform distribution the denominator is 1 for the score, which is given by S(x)= log ( B(a,1)(x) B(1,1)(x) ) = log(a)+(a1)log(x).
Obviously for a1 the density of the signal component converges to that of the background model and consequently the score converges to 0 for all x.
In particular even very low P-values will be scored zero.
Moreover, for a fitted parameter set a and : S(x) x1 log(a) and S(x)x0+.
This demonstrates that the score properly combines the parameters a and .
Similar to classical hypothesis tests where a certain significance level is proposed, we derive a threshold value that discriminates signal from noise.
As detailed in (Pounds and Morris, 2003) the mixture model allows the estimation of the FDR.
From this we calculate a threshold P-value (FDR), which controls the FDR for the positively scoring P-values.
Thus we derive an adjusted log likelihood ratio score given as SFDR(x)= log ( axa1 aa1 ) = (a1)(log(x)log( (FDR))) , (3) i225 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i226 i223i231 M.T.Dittrich et al.
Fig.2.
Scoring of combined P-values.
All genes have been assigned to the ABC and GCB subtype-based fold changes.
The x-axis shows coefficients of the univariate Cox proportional hazards regression model fitted for each gene separately.
A coefficient greater than zero denotes an increased risk association.
Genes scoring positively by our combined scoring function (for a FDR of 0.01) are colored.
This evidences that our score selects genes specifically associated with the different malignancy of the two tumor subtypes.
thus adjusted and unadjusted scores differ by an additive offset dependent on the parameter .
Here, the value is the significance threshold.
P-values below this threshold are considered to be significant and will score positively whereas those above the threshold are assumed to have arisen from the null model and will be assigned negative scores.
It can easily be seen that for 0 the score S(x) and for 1 all scores will be positive and our FDR will be equal to , the mixture parameter of the noise model.
Under the null hypothesis, the xi are iid and the subnetwork score is consequently given by the sum over all protein scores of the subnetwork: SFDRnet = xinet SFDR(xi), where net denotes the set of all P-values in the network to be scored.
Obviously, under this assumption the expectation value of the network score SFDRnet scales linearly with network size.
Similar as in the case of local sequence alignments an appropriate choice of the FDR is essential to ensure a negative subnet score to guarantee locality of the solution.
Otherwise, however, the optimization would tend to collect as many genes as available to increase the score regardless of an underlying biological signal.
Analyzing our lymphoma network we search for genes that are differentially expressed between the GCB and ABC DLBCL subgroups and, in addition, show an association with overall survival (Fig.2).
To aggregate the derived P-values from gene expression analysis and Cox regression we use the second-order statistic as detailed above.
Our score thus combines information about the classification of these tumor subtypes with information about prognosis association.
As illustrated in Figure 2 our data contain a strong signal that can be captured by an adequate combination of these two different aspects.
Thus, it becomes evident that the ABC subtype characteristically over-expresses genes with an association for a higher risk, whereas in the GCB subtype mainly genes associated with a better prognosis are over-expressed.
Hence we search for interaction modules that specifically contribute to the malignant behavior of the ABC subtype as compared to the more benign GCB subtype.
3.2 Mathematical optimization algorithm Combinatorially, the problem from the previous section can be cast as finding an optimal-scoring subgraph in a vertex-weighted graph: Problem 1.
(Maximum-Weight Connected Subgraph Problem, MWCS) Given a connected undirected, vertex-weighted graph G = (V ,E,w) with weights w :V R, find a connected subgraph T = (VT ,ET ) of G, VT V, ET E, that maximizes the score w(T ) := vVT w(v).
It is easy to see that any solution for MWCS can always be trimmed to a tree of same weight, and, if all node weights are positive, an optimal solution is easily computed by determining any spanning tree.
In case of both positive and negative edge weights, finding the MWCS is not so easy.
In fact, in the supplement of Ideker et al.
(2002), Karp has shown that MWCS is an NP-complete problem, and the authors use this as a justification for their heuristic search method.
We propose to solve this problem to provable optimality using techniques from mathematical programming.
More precisely, we transform the problem into the well-known prize-collecting Steiner tree problem (PCST) and use a mathematical programming-based algorithm for PCST to find subgraphs of maximum weight.
As our computational results in the next section show, this approach is very successful and reliable in that it finds provably optimal subnetworks in short computation time for all biologically relevant instance sizes.
The PCST problem occurs in classical applications from Opera-tions Research such as planning district heating or tele-communications networks, where profit-generating customers and a connecting network have to be chosen in the most profitable way.
Formally, it can be stated as follows: Problem 2.
(Prize-Collecting Steiner Tree Problem, PCST) Given a connected undirected vertex-and edge-weighted graph G= (V ,E,c,p) with vertex profits p :V R0 and edge costs c : E R0, find a connected subgraph T = (VT ,ET ) of G, VT V, ET E, that maximizes the profit p(T ) := vVT p(v) eET c(e).
Similar to MWCS, every optimal solution T can be reduced to a tree.
Now, let (G,w) be an instance of MWCS with positive and negative vertex weights, and let w =minvV (G)wv be its smallest node weight.
We construct an instance (G,p,c) of PCST by setting the vertex profits to p(v)=w(v)w for all vV and the edge costs to c(e)=w for all eE.
Clearly, this is a valid PCST instance since all vertex profits and edge costs are positive.
Figure 3 illustrates the transformation.
The following theorem justifies that we can concentrate on the transformed instance in order to solve the MWCS problem.
Theorem 1.
A prize-collecting Steiner tree T in the transformed instance (G,p,c) is a connected subgraph in (G,w) with w(T )= p(T )w. Proof.
Obviously, T is a connected subgraph.
First, observe that its profit is given by p(T )= vVT p(v) eET w = vVT p(v)+|VT 1|w , (4) i226 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i227 i223i231 Identification of optimal PPI subgraphs Fig.3.
Example of an MWCS instance (a) and its transformed PCST counterpart (b).
The minimum weight in (a) is 15.
Optimal solutions are marked in red color.
The MWCS has weight 36, the optimal PCST has profit 12675=51(=36+15).
since T is a tree.
The score of T is w(T )= vVT w(v) = vVT (p(v)+w) = vVT p(v)+|VT |w ()=p(T )w. Corollary 1.
A maximum-weight connected subgraph in (G,w) corresponds to an optimal prize-collecting Steiner tree in the transformed instance (G,p,c).
In fact, the two problems are very related.
It is not difficult to give also a reduction from PCST to MWCS: we just have to split each edge eE and set the weight of the newly created vertex to c(e).
This simple reduction to the NP-complete PCST problem gives an alternative NP-completeness proof for MWCS.
Having reduced the MWCS problem to the PCST problem we briefly summarize the algorithm from Ljubic et al.
(2006), which we use to find provably optimal solutions to the MWCS problem.
This mathematical programming-based algorithm is currently the fastest way to solve PCST problems to optimality and works very well on the transformed MWCS instances.
Mathematical programming is a powerful tool to address NP-hard combinatorial optimization problems (Nemhauser and Wolsey, 1988).
Starting from an ILP formulation modeling the problem under consideration, i.e.
a linear program with integer variables, sophisticated techniques like cutting plane methods or Lagrangian relaxation can be combined with branch-and-bound to generate provably optimal solutions.
Of course, these methods do not guarantee polynomial-running time in the general case.
For many practically relevant instances, however, techniques from mathematical programming work astonishingly well.
The advantages over ad hoc heuristic methods are threefold: (i) having provably optimal solutions at hand allows evaluating the quality of a model, e.g.
the appropriateness of an objective function.
(ii) Methods from mathematical programming guarantee the quality of solutions, i.e.
each new feasible solution comes with a maximal distance to an optimal solution.
This allows the implementation of a trade-off between running time and solution guarantee.
(iii) ILP formulations can be interpreted as polyhedra in high-dimensional space.
Mathematical analysis of these objects often leads to new insights into understanding the original problem.
The algorithm from Ljubic et al.
(2006) starts by applying a number of preprocessing steps to simplify the input network.
Then, it transforms the remaining network into a directed graph by introducing an artificial root vertex r and by splitting each original edge into two directed edges, or arcs, of opposite directions.
Arc weights and additional arcs from the root to the nodes in the network are set such that a feasible Steiner arborescence, i.e.
a directed tree rooted at r, in which only one arc is incident to r, corresponds to a PCST of equal weight.
The algorithm then works on an ILP built on the transformed directed graph.
Each vertex and arc has an associated binary variable modeling its presence or absence in the solution.
A number of linear inequalities constrain the solution vector to represent a feasible Steiner arborescence.
Besides a degree constraint for the artificial root, a class of constraints ensuring that for each chosen vertex exactly one incoming edge must be chosen as well, the model concentrates on the connectedness of the solution: An exponentially large class of inequalities, the cut constraints, ensure that for every selected vertex, which is separated from r by a cut, there must be an arc crossing this cut.
Due to their large number, cut constraints are not considered at once, but iteratively added to the formulation if violated by the current solution.
This technique, combined with a linear programming-based branch-and-bound algorithm, is called branch-and-cut and works particularly well if violated inequalities can be found in polynomial time.
Here, this is the case since violated cut constraints can be detected by a maximum-flow algorithm in a support graph with arc-capacities given by the current linear programming solution.
The above algorithm outputs one optimal solution.
In practice, users often like to obtain a list of promising solutions for manual inspection.
Instead of applying straightforward deletion and re-iteration, we propose a different approach to generate suboptimal solutions: in our ILP approach, binary variables xv determine the presence of nodes in the optimal subgraph S, that is, xv =1 if vV (S) and xv =0 otherwise.
Now let S be an optimal subnetwork as identified by the branch-and-cut algorithm.
Adding the Hamming distance-like inequality vV (S) (1xv)|V (S)| with [0,1] and re-optimizing leads to a best solution differing in at least |V (S)| nodes from S. This procedure can be iterated k times.
The advantages of this strategy are 2-fold: first, the user can determine the number k of suboptimal solutions that should be reported and, second, he or she may adjust the variety of solutions via the parameter .
4 RESULTS 4.1 Functional modules in lymphoma network Using our novel approach we identify the optimal-scoring subnetwork (Fig.4) for the combined score using a restrictive FDR of 0.001.
This subnetwork consists of 46 nodes and has a cumulative score of 70.2.
The 37 positive-scoring nodes attain a weight of 102.9 and the 9 negatively scoring nodes have a i227 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i228 i223i231 M.T.Dittrich et al.
Fig.4.
Optimal subnetwork detected using a score based on the P-values of a gene-wise two sided t-test and an univariate Cox regression hazard model.
A restrictive threshold equivalent to an FDR of 0.001 was used.
The derived subnetwork captures the characteristically differentially expressed-interaction modules associated with the increased malignancy of the ABC subtype.
Coloring is according to the fold change where red denotes an over-expression in ABC and green in GCB.
Diamond nodes represent negative-scoring genes additionally included in the optimal solution.
score of 32.8.
The theoretical upper bound of the solution in a completely connected graph, given by the cumulative score of all positive nodes, is in this case 145.4.
For the given network and under these restrictive conditions our algorithm collects 70.8% of all positive scores.
Figure 5 shows the next best solutions with =0.5.
Further we capture interactome modules that have been described to play major biological roles in the GCB and ABC DLBCL subtypes.
For example, the proliferation module which is more highly expressed in the ABC DLBCL subtype (Rosenwald et al., 2002) is also evident in our current analysis and includes the genes MYC, CCNE1, CDC2, APEX1, DNTTIP2 and PCNA.
Likewise, genes IRF4, TRAF2 and BCL2, which are associated with the potent and oncogenic NFB pathway, also clustered together as illustrated in Figure 4.
Whereas the two previously described interactome modules were derived from genes/proteins expressed in the malignant cells, our algorithm also identified interactome modules (Fig.6) derived from non-malignant by-stander cells in the lymphoma specimens.
In particular, Fibronectin, SPARC, MMP9, CTSK, ITGA5 and ITGB5 showed tight clustering and represent proteins that are expressed in non-malignant fibroblasts and histiocytes (Rosenwald et al., 2002).
4.2 Validation To validate the performance of our approach including the scoring function and search algorithm we simulated an artificial module according to Rajagopalan and Agarwal (2005).
Based on the topology of our lymphoma network we selected two subnetworks of biologically relevant sizes (46 and 143) as signal components.
Following the proposal of Rajagopalan and Agarwal (2005) we set signal P-values uniformly distributed between 0 and 103 and background noise P-values uniformly distributed between 0 and 1.
Fig.5.
Examples of suboptimal solutions corresponding to the optimal solution depicted in Figure 4.
A Hamming distance of 50% was requested for these solutions.
Both subgraphs share 23 nodes with the optimal solution (circles) but also include new ones (triangles).
The upper solution achieves a score of 61.5 (87.7%), the lower solution has a score of 52.5 (74.8%) as compared to the optimal solution (70.2).
The addition of FGFR1 (first and second suboptimal solution) and GRB2 (first suboptimal solution) within the by-stander cell module highlights the biologically relevant interaction between the malignant B-cells and the non-neoplastic network of by-stander cells.
Fig.6.
Optimal subnetwork detected using a score based on the P-values of a one sided t-test for over-expression in GCB and survival as in Figure 4 for an FDR of 0.05.
Since our approach allows for the finetuning of the signal noise decomposition by the FDR we scan a large range of FDRs and evaluate the obtained solutions in terms of recall (true-positive rate) and precision (ratio of true positives to all positively classified).
To assess the variability of the solutions we ran 10 repetitions for each single FDR step.
The silhouette of the recall/precision curve, (adapted from Sing et al., 2005) for the module of size 46 includes the optimal solutions with a maximum recall and precision of exactly one (Fig.7, upper plot).
In particular we find a large i228 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i229 i223i231 Identification of optimal PPI subgraphs Fig.7.
Plot of the recall versus precision of a batch of solutions calculated for wide range of FDRs with 10 replications each.
For the algorithm by Ideker et al.
(2002) we display the convex hulls of solutions obtained by applying their algorithm recursively three times to five independent simulations.
We evaluated two different signal component sizes (46, upper plot and 143, lower plot) with the same procedure.
Clearly, the presented exact approach captures the signal with high precision and recall over a relatively large range of FDRs.
None of the solutions delivered by the heuristic approach falls within the upper right region of high precision and high recall (colored in yellow).
For better visualization data points have been jittered in y-direction.
number of solutions covering the FDR range of 0.7 to 0.3 in the upper right region with a recall and precision higher than 80%.
We contrast the performance of our approach to that of Ideker et al.
(2002) as implemented in the Cytoscape (Cline et al., 2007; Shannon et al., 2003) plugin jActiveModules.
Since their algorithm provides no adjustable scoring function, we follow the proposal of Ideker et al.
(2002) and recursively apply their algorithm to the obtained solution three times for five independent simulations.
Thus we obtain three discrete solution spaces visualized as shaded polygons representing their convex hulls in Figure 7.
Clearly none of these solutions falls within the region of high precision and recall in the upper right corner.
Instead one obtains a set of overly large subnetwork constisting of 865 nodes on average, corresponding to 42.5% of the entire network and 18.8 times the size of the hidden signal component.
This is reflected by a poor precision of 0.05.
After two recursive iterations the number of false positives was reduced substantially and the resultant subnetworks were considerably smaller ranging from 11 to 36 nodes.
However, this solution displayed a large variance especially of the recall ranging from 23 to 71%.
A similar behavior was observed for the larger module (size 143), see Figure 7, lower plot.
5 DISCUSSION In the recent years, the integrated analysis of gene expression data in the context of PPI has received considerable attention (Cabusora et al., 2005; Guo et al., 2007; Ideker et al., 2002; Nacu et al., 2007; Rajagopalan and Agarwal, 2005; Sohler et al., 2004).
The main objective of these analyses is the derivation of biologically interesting subnetworks of interpretable size from large scale PPI data.
This can be expressed as the problem of finding optimal-scoring subgraphs as stated, for the first time, by Ideker et al.
(2002).
Here we transform the problem to the well-known PCST problem from Operations Research.
Thus, we give an alternative NP-completeness proof and, more importantly, we are able to solve large instances of this problem in reasonable computation time to provable optimality by an ILP approach for the transformed problem.
Additionally, this allows us to calculate suboptimal solutions with given Hamming distances to previously found solutions.
We present an application of our approach using a large PPI network from HPRD (Mishra et al., 2006; Peri et al., 2003) in combination with the comprehensive and well-established microarray dataset from lymphoma patients (Rosenwald et al., 2002).
This dataset also provides valuable information of patients survival which can be used in a Cox regression hazard model to measure the contribution of each gene to malignancy of the tumor.
In contrast to previous studies we do not restrict our analysis to differences in expression between conditions (in our case two lymphoma subtypes) but also include the P-values of the Cox regression into our analysis to derive functional modules that are specifically associated with the different malignant behavior of the tumor subtypes.
In an effective-algorithmic approach, a well-defined objective function is as important as a good search procedure.
Therefore we first combine the set of P-values derived from various experiments by an order statistics-based approach to obtain a P-value of P-values as a scalable measure of overall significance.
Then we fit a beta-uniform mixture model on the entire set of raw P-values of all nodes in the interaction network.
Thus, we achieve a signal-noise decomposition on which we deduce a scoring function of P-values as a likelihood ratio of the signal and noise component.
Thus, we deduce a scalable scoring function with a meaningful interpretation of the adjustment parameter as FDR.
The additivity of this logarithmic score allows us to effectively formulate and exactly solve the problem of optimal subgraph identification by an ILP approach.
Inspired by the problem of finding local-sequence alignments we strive to identify local maximal-scoring network regions.
Given a negative-expectation value of the scoring functions as realized i229 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i230 i223i231 M.T.Dittrich et al.
by an appropriate choice of the FDR, we achieve an efficient localization of the resulting region of interest.
Our approach makes it possible to fine-tune the size of the resultant subnetworks and thereby zoom into the maximal-scoring region of interest in the interaction network.
Our order statistics-based approach to aggregate the P-values from different experiments is equivalent to that adopted by Ideker et al.
(2002).
However, we explicitly allow the user to require a predefined number of P-values to be significant (e.g.
the first, second, ... , nth order statistic) instead of only taking the maximum over all order statistics, which is naturally included in our approach.
However, asking for the maximum only can lead to serious problems in cases of highly variable signal content among the P-values of different experiments where the highest signal content will dominate the resulting score.
As a case in point, analyzing our lymphoma network, the algorithm of Ideker et al.
(2002) only yields solution based on the gene expression P-value.
Obviously, the biologically important but statistically weaker signal cannot be detected in combination with a more dominant signal by this approach.
To our knowledge, the presented method allows for the first time the exact decomposition of PPI networks into optimal-scoring subnetworks and suboptimal networks of a given dissimilarity as defined by the Hamming distance of the graphs node-incidence vectors.
In contrast to previously published methods, our algorithm computes provably optimal solutions without computationally demanding parameter optimization usually necessary in heuristic approaches.
Furthermore, heuristic methods do not guarantee to find the optimal solution and are unable to assess the solution quality.
As a representative of these heuristic approaches we chose the algorithm of Ideker et al.
(2002) as implemented in Cytoscape (Cline et al., 2007; Shannon et al., 2003) and compared the performance to that of our exact approach.
The results clearly demonstrate the shortcomings of the heuristic approaches.
Since recursive applications of the algorithm are required, only a limited number of isolated solution sizes can be obtained.
None of the solutions comes close to the high-accuracy region showing both, high precision and high recall.
The high number of true positives in the first run is paid for with an exorbitantly high number of false positives as reflected by the sizes of the results from the initial run.
On average 865 nodes were reported for networks with the small signal component of 46 in the first step, corresponding to a true positive (TP)/false positive (FP) ratio of 18.8.
A subsequent application of the algorithm yielded on average networks of 75 nodes, 46.4% of which where true positives.
Precision can still be improved by a third run but this comes at the expense of recall rates down to 24%.
This is an effect of the nestedness of the recursive solutions, where true positives neglected in one step will not be contained in any later solution.
Although Ideker et al.
(2002) claim that many high-scoring subnetworks highlight biologically interesting regions although not being optimal in the sense of the objective function it must be kept in mind that the solutions provided by their algorithm are quite variable and heavily dependent on the choice of the parameter settings (seed, number of iterations and annealing temperature).
More importantly, the scoring system of Ideker et al.
(2002) and those related to it lack an explicit signal/noise decomposition and thus provide no estimation about the size of the signal content.
This can pose a serious obstacle for these approaches in the case of low-signal content or the even worse scenario of random noise only.
Applying batches of P-values randomly drawn from a uniform(0,1) distribution to our network the implementation of Ideker et al.
(2002) still reports solutions of 770 nodes (37% of the entire network) on average with scores within the same range as those of containing the signal modules.
Subsetting these solutions by reapplying the algorithm still yields networks of sizes between 130 and 210 nodes.
Obviously a major drawback of these scoring systems is that due to the lacking estimate of the signal content prior to the search phase no distinction between a true-signal component and a best noise aggregate can be made.
This problem is solved by the integrated signal-noise decomposition based on a beta-uniform mixture model of our approach.
In fact all tests with random P-values as input yielded a fitted model with a mixture parameter of 1 corresponding to a signal content of 0.
Consistent with that we obtain a parameter a=1 of the signal beta distribution and consequently a score of zero for all nodes (Equation 3).
Nevertheless, heuristic approaches may be able to deliver biologically relevant solutions as claimed by Ideker et al.
(2002) if the proportion of signal is high enough.
Especially in case of low-signal content the biological relevancy of the obtained solution may be questionable, and even after recursive application of the algorithm the quality of the obtained subnetwork is hardly assessable due to the high variability.
Inherently, all published heuristic methods based on the approach of Ideker et al.
(2002) share one of the discussed drawbacks either in terms of search algorithm or scoring function.
Therefore it is highly desirable to attain truly optimal solutions with an explicit estimate of the signal content and control of the FDR as provided by the presented approach.
We emphasize that, despite the underlying computational complexity, our algorithm runs very fast on biologically relevant instance sizes: our software tool heinz computes provably optimal results usually in a few minutes; profiling our implementation we measured a median runtime of 182 seconds for test runs on 1000 score-permuted graphs.
Most importantly we demonstrate that our approach discovers biologically meaningful modules in a lymphoma interaction network which include and extend the results reported by Rosenwald et al.
(2002) which have been described to be of importance in the pathogenesis of the GCB and ABC DLBCL subtypes.
In general, the integration of survival and expression data into the analysis of PPI networks exhibits perturbed interaction modules associated with the malignancy of the tumor and can yield new insights into tumor biology on a cellular level.
In the future, we plan to generalize our method to an even broader application setting.
As a first step, we propose to integrate edge weights, which could, for example, be derived from correlation of gene expression as used by Guo et al.
(2007) or from P-values of interaction predictions with STRING (von Mering et al., 2007).
Furthermore, we intend to provide an interface to non-commercial optimization libraries and to integrate our algorithm into the Bioconductor environment (Gentleman et al., 2004).
ACKNOWLEDGEMENTS G.W.K.
thanks A. Bley for helpful discussions and I. Ljubic and A. Moser for support with the dhea code.
MTD thanks the IZKF (MD/PhD program) and the SFB688 (TPA2).
Conflict of Interest: none declared.
i230 [20:14 18/6/03 Bioinformatics-btn161.tex] Page: i231 i223i231 Identification of optimal PPI subgraphs
ABSTRACT Motivation: High-throughput sequencing has made the analysis of new model organisms more affordable.
Although assembling a new genome can still be costly and difficult, it is possible to use RNA-seq to sequence mRNA.
In the absence of a known genome, it is necessary to assemble these sequences de novo, taking into account possible alternative isoforms and the dynamic range of expression values.
Results: We present a software package named Oases designed to heuristically assemble RNA-seq reads in the absence of a reference genome, across a broad spectrum of expression values and in presence of alternative isoforms.
It achieves this by using an array of hash lengths, a dynamic filtering of noise, a robust resolution of alternative splicing events and the efficient merging of multiple assemblies.
It was tested on human and mouse RNA-seq data and is shown to improve significantly on the transABySS and Trinity de novo transcriptome assemblers.
Availability and implementation: Oases is freely available under the GPL license at www.ebi.ac.uk/zerbino/oases/ Contact: dzerbino@ucsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on December 2, 2011; revised on January 20, 2012; accepted on February 17, 2012 1 INTRODUCTION Next-generation sequencing of expressed mRNAs (RNA-seq) is gradually transforming the field of transcriptomics (Blencowe et al., 2009; Wang et al., 2009).
The first attempts to discover expressed gene isoforms relied on mapping the RNA-seq reads onto the exons and exonexon junctions of a known annotation (Jiang and Wong 2009; Mortazavi et al., 2008; Richard et al., 2010; Sultan et al., 2008; Wang et al., 2008).
Consequently, reference-based ab initio methods have been developed to assemble a transcriptome from RNA-seq data using read alignments alone, inferring the underlying annotation (Denoeud et al., 2008; Guttman et al., 2010; Trapnell et al., 2010; Yassour et al., 2009).
To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
Unfortunately, the use of a reference genome is not always possible.
Despite the drop in the cost of sequencing reagents, the complete study of a genome, from sampling to finishing the assembly is still costly and difficult.
Sometimes, the model being studied is sufficiently different from the reference because it comes from a different strain or line such that the mappings are not altogether reliable.
For these cases, de novo genome assemblers have been employed to create transcript assemblies, or transfrags, from the RNA-seq reads in the absence of a reference genome (Birol et al., 2009; Collins et al., 2008; Jackson et al., 2009; Wakaguri et al, 2009).
However, these short read genomic assemblers, based mainly on de Bruijn graph genomic assemblers (Zerbino and Birney, 2008; Simpson et al., 2009), make implicit assumptions regarding the evenness of the coverage and the colinearity of the sequence.
Indeed, the coverage depth fluctuates significantly between transcripts, isoforms and regions of the transcript, therefore it cannot be used to determine the uniqueness of regions or to isolate erroneous sequence.
In addition, these tools are geared to produce long linear contigs from the given sequence, not to detect the overlapping sequences presented by isoforms of a single gene.
This affects a number of steps, including error correction, repeat detection and read pair usage.
These methods are therefore not necessarily suited to process transcriptome data which does not conform to either of these assumptions.
More recently, transcriptome assembly pipelines were developed to post-process the output of de novo genome assemblers: Velvet and ABySS (Martin et al., 2010; Robertson et al., 2010; Surget-Groba and Montoya-Burgos, 2010).
The common idea shared by these pipelines is to run an assembler at different k-mer lengths and to merge these assemblies into one.
The rationale behind this approach is to merge more sensitive (lower values of k) and more specific assemblies (higher values of k).
The pipeline presented by Robertson et al.
(2010), transABySS, also handles alternative splicing variants.
It detects them by searching for connected groups of contigs such that they are connected in a characteristic bubble and one of the contigs has a length of exactly (2k2).
These bubbles are first removed, then added to the final assemblies, to reconstruct alternate variants.
A variety of algorithmic researchers have used splicing graphs to represent alternative splicing which have a direct relationship to de Bruijn graphs, as pointed out by Heber et al.
(2002).
This homology The Author(s) 2012.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
dzerbino@ucsc.edu Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1087 10861092 Oases de novo RNA-seq assembly between data structures opens the possibility of a de novo short read transcriptome assembler, as illustrated by the Trinity algorithm (Yassour et al., 2011).
Trinity starts by extending contigs greedily, connecting them into a de Bruijn graph, then extracting sufficiently covered paths through this graph.
Trinity is designed to reconstruct highly expressed transcripts to full length using only one k-mer length.
We present Oases, a de novo transcriptome assembler that combines these advances.
Oases merges the use of multiple k-mers presented in (Robertson et al., 2010; Surget-Groba and Montoya-Burgos, 2010) with a topological analysis similar to that presented by Yassour et al.
(2011).
It uses dynamic error removal adapted to RNA-seq data and implements a robust method to predict full length transfrags, even in cases where noise perturbs the topology of the graph.
Single k assemblies are merged to cover genes at different expression levels without redundancy.
We tested the latest version of Oases (0.2.01) on experimental datasets and found that Oases produces longer assemblies than previous de novo RNA-seq assemblers.
Oases was compared with a reference-based ab initio algorithm, Cufflinks (Trapnell et al., 2010).
The latter approach has a considerable advantage in low expression genes, as it can join otherwise disjoint reads by virtue of their genomic positions, but at high read coverage, Oases sensitivity approaches that of reference-based ab initio algorithms.
We also examined the effect of coverage depth, hash length, alternative splicing and assembly merging on the quality of assemblies.
2 METHODS 2.1 Overview The Oases assembly process, explained in detail below and illustrated in Figure 1, consists of independent assemblies, which vary by one important parameter, the hash (or k-mer) length.
In each of the assemblies, the reads are used to build a de Bruijn graph, which is then simplified for errors, organized into a scaffold, divided into loci and finally analyzed to extract transcript assemblies or transfrags.
Once all of the individual k-mer assemblies are finished, they are merged into a final assembly.
2.2 Contig assembly The Oases pipeline receives as input a preliminary assembly produced by the Velvet assembler (Zerbino and Birney, 2008) which was designed to produce scaffolds from genomic readsets.
Its initial stages, namely hashing and graph construction can be used indifferently on transcriptome data.
We only run these stages of Velvet to produce a preliminary fragmented assembly, containing the mapping of the reads onto a set of contigs.
However, the later stage algorithms, Pebble and Rock Band, which resolve repeats in Velvet, are not used because they rely on assumptions related to genomic sequencing (Zerbino et al., 2009).
Namely, the coverage distribution should be roughly uniform across the genome and the genome should not contain any branching point.
These conditions prevent those algorithms from being reliable and efficient on RNA-seq data.
2.3 Contig correction After reading the contigs produced by Velvet, Oases proceeds to correct them again with a set of dynamic and static filters.
The first dynamic correction is a slightly modified version of Velvets error correction algorithm, TourBus.
TourBus searches through the graph for parallel paths that have the same starting and end node.
If their sequences are similar enough, the path with lower coverage is merged into the path with higher coverage, irrespective of their absolute coverage.
In this sense, Fig.1.
Schematic overview of the Oases pipeline: (1) Individual reads are sequenced from an RNA sample; (2) Contigs are built from those reads, some of them are labeled as long (clear), others short (dark); (3) Long contigs, connected by single reads or read-pairs are grouped into connected components called loci; (4) Short contigs are attached to the loci; and (5) The loci are transitively reduced.
Tranfrags are then extracted from the loci.
The loci are divided into four categories: (A) chains, (B) bubbles, (C) forks and (D) complex (i.e.
all the loci which did not fit into the previous categories).
the TourBus algorithm is adapted to RNA-seq data and fluctuating coverage depths.
However, for performance issues, the Velvet version of TourBus only visits each node once, meaning that it does not exhaustively compare all possible pairs of paths.
Given the high coverage of certain genes, and the complexity of the corresponding graphs, with numerous false positive paths, it is necessary for Oases to exhaustively examine the graph, visiting nodes several times if necessary.
In addition to this correction, Oases includes a local edge removal.
For each node, an outgoing edge is removed if its coverage represents <10% of the sum of coverages of outgoing edges from that same node.
This approach, similar to the one presented by Yassour et al.
(2011), is based on the assumption that on high coverage regions, spurious errors are likely to reoccur more often.
Finally, all contigs with less than a static coverage cutoff (by default 3) are removed from the assembly.
The rationale for this filter is that any transcript with such a low coverage cannot be properly assembled in the first place, so it is expedient to remove them from the assembly, along with many low coverage contigs created by spurious errors.
2.4 Scaffold construction The distance information between the contigs is then summarized into a set of distance estimates called a scaffold, as described in (Zerbino et al., 2009).
Because a read in a de Bruijn graph can be split between several contigs, the distance estimate for a connection between two contigs can be supported by both spanning single reads or paired-end reads.
The total number of spanning reads and pair-end reads confirming a connection is called its support.
A connection which is supported by at least one spanning read is called direct, otherwise, it is indirect.
Connections are assigned a total weight.
It is calculated by adding 1 for each supporting spanning read and a probabilistic weight for each spanning pair, proportional to the likelihood of observing the paired reads at their observed positions on the contigs given the estimated distance between the contigs and assuming a normal insert length distribution model.
2.5 Scaffold filtering Much like the contig correction phase, several filters are applied to the scaffold: static coverage thresholds for the very low coverage sequences and a dynamic coverage threshold that adapts to the local coverage depth.
Because coverage is no longer indicative of the uniqueness of a sequence, contig length is used as an indicator.
Based on the decreasing likelihood of high identity conservation as a function of sequence length (Whiteford et al., 1087 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1088 10861092 M.H.Schulz et al.
2005), contigs longer than a given threshold [by default (50+k1) bp] are labeled as long and treated as if unique and the other nodes are labeled as short.
Connections with a low support (by default 3 or lower) or with a weight <0.1 are first removed.
Two short contigs can only be joined by a direct connection with no intermediate gap.
A short and a long contig can only be connected by a direct connection.
Finally, connections between long contigs are tested against a modified version of the statistic presented in (Zerbino et al., 2009), which estimates how many read pairs should connect two contigs given their respective coverages and the estimated distance separating them (see Supplementary Material).
Indirect connections with a support lower than a given threshold (by default 10% of this expected count) are thus eliminated.
2.6 Locus construction Oases then organizes the contigs into clusters called loci, as illustrated in Figure 1.
This terminology stems from the fact that in the ideal case, where no gap in coverage or overlap with exterior sequences complicate matters, all the transcripts from one gene should be assembled into a connected component of contigs.
Unfortunately, in experimental conditions, this equivalence between components and genes cannot be guaranteed.
It is to be expected that loci sometimes represent fragments of genes or clusters of homologous sequences.
Scaffold construction takes place in two stages similarly to the approach described by Butler et al.
(2008).
Long contigs are first clustered into connected components.
These long nodes have a higher likelihood of being unique, therefore it is assumed that two contigs which belong to the same component also belong to the same gene.
To each locus are added the short nodes which are connected to one of the long nodes in the cluster.
2.7 Transitive reduction of the loci For the following analyses to function properly, it is necessary to remove redundant long distance connections, and retain only connections between immediate neighbors, as seen in Figure 1.
For example, it is common that two contigs which are not consecutive in a locus are connected by a paired-end read.
A connection is considered redundant if it connects two nodes that are connected by a distinct path of connections such that the connection and the two paths have comparable lengths.
The transitive reduction implemented in Oases is inspired from the one described in (Myers, 2005) but had to be adapted to the conditions of short read data.
In particular, short contigs can be repeated or even inverted within a single transcript and form loops in the connection graph.
Because of this, occasional situations arise where every connection coming out of a node can be transitively reduced by another one, thus removing all of them, and breaking the connectivity of the locus.
To avoid this, a limit is imposed on the number of removed connections.
If two connections have the capacity to reduce each other, the shortest one is preserved.
2.8 Extracting transcript assemblies The sequence information of the transcripts is now contained in the loci.
These loci can be fragmented because of alternative splicing events which cause the de Bruijn graph to have a branch.
Oases, therefore, analyses the topology of the loci to extract full length isoform assemblies.
In many cases, the loci present a simple topology which can be trivially and uniquely decomposed as one or two transcripts.
We define three categories of trivial locus topologies (Fig.1): chains, forks and bubbles, which if isolated from any other branching point, are straightforward to resolve.
These three topologies are easily identifiable using the degrees of the nodes.
Oases, therefore, detects all the trivial loci and enumerates the possible transcripts for each of them.
Because the above exact method only applies to specific cases, an additional robust heuristic method is applied to the remaining loci, referred to as complex loci.
Oases uses a reimplementation of the algorithm described in (Lee, 2003), which efficiently produces a parsimonious set of putative highly expressed transcripts, assuming independence of the alternative splicing events.
This extension of the algorithm is quite intuitive, since there is a direct analogy between the de Bruijn graph built from the transcripts of a gene and its splicing graph, as noted by Heber et al.
(2002).
Using dynamic programming, it enumerates heavily weighted paths through the locus graph in decreasing order of coverage, until either all the contigs of the locus are covered, or a specified number of transcripts is produced (by default 10).
As in the transitive reduction phase, this algorithm had to be slightly modified to allow for loops in the putative splicing graph of the locus.
Loops are problematic because their presence can prevent the propagation of the dynamic programming algorithm to all the contigs of a locus.
When a loop is detected, it is broken at a contig which connects the loop to the rest of the locus, so as to leave a minimum number of branch points, as described in the Supplementary Material.
2.9 Merging assemblies with Oases-M De Bruijn graph assemblers are very sensitive to the setting of the hash length k. For transcriptome data, this optimization is more complex as transcript expression levels and coverage depths are distributed over a wide range.
A way to avoid the dependence on the parameter k is to produce a merged transcriptome assembly of previously generated transfrags from Oases.
Oases is run for a set of [kMIN, ,kMAX] values and the output transfrags are stored.
All predicted transfrags from runs in the interval are then fed into the second stage of the pipeline, Oases-M, with a user selected kMERGE.
A de Bruijn graph for kMERGE is built from these transfrags.
After removing small variants with the Tourbus algorithm, any transfrag in the graph that is identical or included in another transfrag is removed.
The final assembly is constructed by following the remaining transfrags through the merged graph.
3 RESULTS 3.1 Datasets Two datasets were retrieved from the Nucleotide Archive (http://www.ebi.ac.uk/ena/).
A human dataset was produced in a study by Heap et al.
(2010), where poly(A)-selected RNAs from human primary CD4(+) T cells were sequenced.
Paired-end reads of length 45 bp with an insert size of 200 bp from one human individual (studyID SRX011545) were downloaded.
A mouse dataset was taken from the study of Trapnell et al.
(2010).
In a timeseries experiment of C2C12 myoblast mouse cells, paired-end reads of length 75 bp with an insert size of 300 bp were sequenced.
Read data from the 24 h timepoint (study id SRX017794) was used.
To reduce the amount of erroneous bases, both paired-end datasets were processed by (i) removing Ns from both ends, (ii) clipping bases with a Sanger quality 10 and (iii) removing reads with more than six bases with Sanger quality 10 after steps (i) and (ii), leading to a total of 30 940 088 and 64 441 708 reads for human and mouse, respectively.
3.2 Assemblies and alignments All experiments were run with Oases version 0.2.01, and Velvet 1.1.06 and the coverage cutoff and the minimum support for connections were set to 3.
TransABySS 1.2.0 was run with ABySS 1.2.5 through the first two stages of transABySS (assembly and merging, before mapping to a reference genome is required).
Instead of just running with 1088 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1089 10861092 Oases de novo RNA-seq assembly Table 1.
Comparison of Velvet and Oases assemblies on the human RNA-seq dataset k-mer Method Tfrags >100 bp Sens.
(%) Spec.
(%) Full Lgth.
80% lgth.
19 Velvet 89 789 12.45 83.58 42 78 Oases 67 319 17.23 92.55 828 7437 25 Velvet 88 042 16.13 89.62 92 516 Oases 53 504 14.97 93.0 754 6882 31 Velvet 55 986 12.78 93.16 213 1986 Oases 47 878 10.55 94.63 429 3751 35 Velvet 36 507 7.9 94.81 107 1660 Oases 34 012 6.67 95.99 196 1885 The total number of transfrags longer that 100 bp (Tfrags), nucleotide sensitivity and specificity, as well as the number of full length or 80% length reconstructed Ensembl transcripts are shown.
the default parameters, we tested an array of parameters and chose the best for those datasets, namely n = 10, c = 3 and ABYSS with the options-E0 (Supplementary Material).
Trinity (ver.
2011-08-20) was run with the default parameters.
In particular, the k-mer length of 25 could not be modified.
Potential poly-A tails after assembly were removed using the trimEST program from the EMBOSS package (Rice et al., 2000) before alignment.
Subsequently, predicted transfrags of the methods were aligned against the genome using Blat (Kent, 2002).
The Cufflinks assemblies are those published by its authors.
Reads per kilobase of exon model per million mapped reads (RPKM), as defined by Mortazavi et al.
(2008) expression values for annotated genes have been computed by aligning reads against annotated Ensembl 57 transcripts with RazerS (Weese et al., 2009), (see Supplementary Material).
3.3 Metrics In all the following experiments, we focused on a simple set of metrics as used in (Robertson, 2010; Yassour, 2011): nucleotide sensitivity, nucleotide specificity, percentage of transcripts assembled to 100% of their length and percentage of transcripts assembled to 80% of their length.
The Blat mappings of the assemblies were compared with the Ensembl annotations of the corresponding species.
3.4 Comparing Oases to Velvet To evaluate the added value of the topology resolution within each loci, we compared the Oases contigs from the Velvet assemblies which they are built from.
Table 1 shows how the Oases assemblies significantly improve on the Velvet assemblies.
This confirms the intuition that in the presence of alternative splicing and dynamic expression levels, the assembly is broken by breaks in the graph, which can be resolved by topological analysis and adapted error correction as described in the Methods section.
As an example, the percentage cutoff for local edge removal was modulated (see Supplementary Table S1).
These results show how dynamic filters improve the quality of the assembly.
3.5 Impact of k-mer lengths One of the major parameters in de Bruijn graph assemblers is the hash length, or k-mer length.
Comparing single-k assemblies 20 40 60 80 100 0 20 0 40 0 60 0 80 0 Expression Quantiles R ec on st ru ct ed to a t l ea st  8 0 % Merged 19 35 k=19 k=21 k=27 k=31 k=35 Fig.2.
Comparison of single k-mer Oases assemblies and the merged assembly from kMIN=19 to kMAX=35 by Oases-M, on the human dataset.
The total number of Ensembl transcripts assembled to 80% of their length is provided by RPKM gene expression quantiles of 1464 genes each.
performed by Oases, it is possible to observe that this parameter is crucial in RNA-seq assembly.
Figure 2 shows how the k-mer length is closely related to the expression level of the transcripts being assembled.
As expected, the assemblies with longer k-values perform best on high expression genes, but poorly on low expression genes.
However, short k-mer assemblies have the disadvantage of introducing misassemblies, as shown in Supplementary Table S7.
3.6 Impact of merging assemblies In addition, Figure 2 shows the same statistics for the merged assembly by Oases-M, which is significantly superior to each of the individual values.
This result illustrates how the different assemblies do not completely overlap.
Further, Supplementary Figure S2 shows how each single k-mer assembly resolved transcripts at different expression levels.
We compared merging different intervals of k-mers (see Supplementary Material).
The wider the interval, the better the results.
To determine bounds on this interval we arbitrarily bounded on the low values with 19, on the assumption that smaller k-mers are very likely to be unspecific for mammalian genomes (Whiteford et al., 2005).
In theory, on the upper end, all the k-mer values (up to read length) could be used.
To avoid wasting resources, we measured the added value of each new assembly (see Supplementary Material).
As expected, marginal gains progressively diminish and this metric could be used to determine how large a spectrum of k-mers to use.
We also investigated which kMERGE should be used and we found that kMERGE= 27 works well with little difference for higher values (see Supplementary Table S4) and is therefore used for all analyses in the article.
3.7 Comparing Oases to other RNA-seq de novo assemblers Oases-M was compared with existing RNA-seq de novo assemblers, transABySS (Robertson et al., 2010) and Trinity (Yassour et al., 2011).
The previous human dataset and a mouse dataset were used 1089 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1090 10861092 M.H.Schulz et al.
Fig.3.
Reconstruction efficiency of Ensembl transcripts for different RNA-seq de novo assembly methods (Oases-M, Trinity, and transABySS) on human and mouse datasets.
Reference-based assembly results using Cufflinks are provided on the mouse dataset.
All annotated genes have been grouped into quantiles by RPKM expression values of 1464 (resp.
1078) genes for the human data (resp.
mouse).
for the comparison.
The datasets have different read lengths and sequencing depth, as detailed in Methods.
Both transABySS and Oases were run for k-mer length 1935 bp on the human dataset.
Because the mouse reads are longer, these two assemblers were run for k-mers 2135 on that dataset.
The highest value of k was determined by an approach similar to that used on the human data (see Supplementary Material for data).
Trinity is fixed by implementation at k =25 bp.
Figure 3 shows the number of reconstructed Ensembl transcripts for each assembler on both datasets separated by expression quantiles.
The main observation is that all assemblers do not behave equally with respect to expression level.
Trinity appears to perform best on high expression genes, whereas transABySS performs best on low expression genes.
Oases performs comparatively well throughout the spectrum of expression levels, hence the greater overall success (Table 2).
Regarding correctness, we computed the number of misassemblies and the qualities of the different assemblers are comparable (see Supplementary Material).
Transfrags mapped with high confidence to the genome occasionally differ from the known annotation.
For example, Oases produced 237 (resp.
390) transfrags longer than 300 bp which mapped to the reference genome, but did not overlap with the human (resp.
mouse) annotation.
In Figure 4, the overlap of full length mouse transcripts reconstructed by the three methods is shown.
It is interesting to note that although the results greatly overlap, the different assemblers succeeded in assembling different transcripts.
3.8 Comparing de novo and reference-based assemblers Oases and the other de novo assemblers were finally compared on the mouse data to a reference-based assembly algorithm, Cufflinks (Trapnell et al., 2010), on the mouse dataset.
As could be expected, Cufflinks generally outperforms the de novo assembly algorithms, 1090 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1091 10861092 Oases de novo RNA-seq assembly Table 2.
Overall comparison of the different RNA-seq assembly methods on human and mouse datasets Data Method Tfrags >100 bp Sens.
(%) Spec.
(%) Full lgth 80% lgth Human Oases-M 174 469 21.44 92.35 1463 11 169 tABySS 100 127 19.65 92.16 1358 10 992 Trinity 76 232 19.99 88.63 953 7129 Mouse Oases-M 175 914 30.83 89.08 1324 9880 tABySS 174 744 30.66 92.79 1149 9376 Trinity 92 810 31.57 87.14 1085 7028 Cufflinks 63 207 48.13 75.29 4369 21 222 The number of transfrags longer than 100 bp produced (Tfrags) and nucleotide sensitivity and specificity, as well as the number of full length or 80% length reconstructed Ensembl transcripts are shown.
Fig.4.
Venn Diagramm that compares mouse Ensembl transcripts reconstructed to full length by Trinity, Trans-AbySS and Oases-M for the mouse RNA-seq data.
as it benefits from using the reference genome to anchor its assemblies (Fig.3).
Nonetheless, it is interesting to note that as expression level and therefore coverage depth go up, the gap narrows.
Beyond assembling more transcripts, it is also important to recover multiple isoforms for each gene.
For each assembled transcript, the average number of additionally assembled transcripts from the same gene are, respectively, 1.21, 1.25, 1.01 and 1.56 for Oases, transABySS, Trinity and Cufflinks.
Cufflinks performs better in that respect, whereas Trinity is less sensitive.
3.9 Runtime and memory Ade novo transcriptome assembly is a resource intensive task.
Velvet uses multithreading but Oases currently does not.
The complete merged assembly for human took 3.2 h and 6.1 GB of peak memory on a 48 core AMD Opteron machine with 265 GB RAM.
The merged assembly for mouse took 10.3 h and 15.1 GB peak memory.
4 DISCUSSION We have shown that merging different single k assemblies is beneficial, in concordance with previous work (Surget-Groba and Montoya-Burgos, 2010; Robertson et al., 2010).
Oases employs dynamic cutoffs, where possible, to allow for a robust reconstruction with different k-values.
However, detailed parameter optimization for Oases and trans-ABySS may lead to further improvements.
Overall, the de novo methods produced large numbers of misassemblies.
Given the dynamic ranges involved, the exact parameter settings of these programs define a trade-off between sensitivity and accuracy.
In these experiments, Oases tends to be more sensitive, Trinity more accurate.
The correlation of small k-mer assemblies and misassembly rates suggests that homologies between genes are the main source of errors.As reads get longer, and coverage depths greater, sensitivity will only increase and users will probably avoid the shorter k-mer lengths for greater accuracy.
Short k-mers will only be necessary to retrieve the very rare transcripts.
An independent but significant factor to these assemblies is read preprocessing, as read error removal has already been shown to have a significant impact in the context of de novo genome assembly (Smeds and Knster, 2011).
Interestingly, the comparison of reconstructed transcripts for the three de novo methods in Figure 4 reveals that each method outperforms the others on a separate set of transcripts.
These differences in performance are probably due to the different strategies employed to remove errors.
A more aggressive method, which discards more data, would presumably end up with many gaps on low expression data, whereas a more lenient algorithm would leave too many ambiguities at high coverage.
In particular, it appears that the performance of all the assemblers sometimes drops at very high coverage depths.
This is probably linked to increased noise.
Indeed, this drop is especially marked for transABySS, which, to our knowledge, is the only of the three de novo assemblers not to integrate dynamic filters which adapt with coverage depth.
Intriguingly, transABySS outperformed Trinity in our experiments, contrary to the observation of Yassour et al., (2011).
This could not be due to the parameterization of Trinity, which cannot be parameterized apart from the insert length.
Instead, the larger k range used for transABySS and the lower sequencing depth in our analyzed data sets may explain this discrepancy, as transABySS was shown to perform especially well for low to medium expressed genes.
Similarly, our experiments on mouse data show a bigger gap between Cufflinks and the de novo assemblers than observed by Yassour et al.
(2011).
In their work, the comparison was focused on the set of oracle transcripts, which show sufficient coverage of exact k-mers in the reads.
However, no such restriction was applied here and Cufflinks surpasses the de novo methods for low to medium expression ranges, where coverage is sparse.
In this study, we did not analyze strand-specific RNA-seq datasets.
However, as these datasets become more available (Levin, 2010) Oases already supports this data.
During the hashing phase, reverse complement sequences can be stored separately instead of being joined as the two strands of the same sequence.
5 CONCLUSION Oases provides users with a robust pipeline to assemble unmapped RNA-seq reads into full length transcripts.
Oases was designed to deal with the conditions of RNA-seq, namely uneven coverage and alternative splicing events.
Our results show how crucial it is to explore and understand the relevant conditions.
Alternative splicing can significantly confound 1091 Copyedited by: TRJ MANUSCRIPT CATEGORY: ORIGINAL PAPER [14:53 27/3/2012 Bioinformatics-bts094.tex] Page: 1092 10861092 M.H.Schulz et al.
the assembly and has to be specifically addressed.
Gene expression levels are a major factor determining the sensitivity of an algorithm.
High coverage genes require more selective methods, whereas low coverage genes favor more sensitive algorithms.
This is why exploring a range of k-mer lengths is key to success.
In the light of these results, Oases was designed to perform well overall by adapting to these varying conditions and succeeded in obtaining superior overall results compared to previously published RNA-seq de novo assemblers.
Nonetheless, it also appears that merging assemblies from a diversity of algorithms could be beneficial.
This is probably due to the dynamic range of all the variables, which prevent any single method from being systematically superior.
Finally, we examined the difference between de novo and reference-assisted assembly.
In the presence of a well-assembled genome (typically human or mouse), the latter methods are generally at a significant advantage.
Nonetheless, this gap reduces at high expression levels.
This shows that the absence of an assembled genome can be largely compensated for provided sufficient read coverage.
ACKNOWLEDGEMENTS Thank you to Cole Trapnell, Ali Mortazavi and Diane Trout for their help in obtaining the C2C12 dataset.
Many thanks to Brian Haas for his patient testing of Oases, and to all the other users for their feedback.
Thank you to Saket Navlakha and Benedict Paten for their proofreading and commentaries.
Funding: M.H.S.
was supported by the International Max Planck Research School for Computational Biology and Scientific Computing.
D.R.Z.
and E.B.
were funded by EMBL central funds.
D.R.Z.
is also funded by ENCODE grant 1U41HG004568-01, the NHGRI GENCODE subcontract (Prime: 1U54HG004555-01, Subaward: 0244-03) and the NHGRI ENCODE DAC grant (Prime: NHGRI 1U01HG004695-01, Subcontract: European Bioinformatics Institute).
Conflict of Interest: none declared.
ABSTRACT Motivation: The sequencing of the human genome has made it possible to identify an informative set of >1 million single nucleotide polymorphisms (SNPs) across the genome that can be used to carry out genome-wide association studies (GWASs).
The availability of massive amounts of GWAS data has necessitated the development of new biostatistical methods for quality control, imputation and analysis issues including multiple testing.
This work has been successful and has enabled the discovery of new associations that have been replicated in multiple studies.
However, it is now recognized that most SNPs discovered via GWAS have small effects on disease susceptibility and thus may not be suitable for improving health care through genetic testing.
One likely explanation for the mixed results of GWAS is that the current biostatistical analysis paradigm is by design agnostic or unbiased in that it ignores all prior knowledge about disease pathobiology.
Further, the linear modeling framework that is employed in GWAS often considers only one SNP at a time thus ignoring their genomic and environmental context.
There is now a shift away from the biostatistical approach toward a more holistic approach that recognizes the complexity of the genotypephenotype relationship that is characterized by significant heterogeneity and genegene and geneenvironment interaction.
We argue here that bioinformatics has an important role to play in addressing the complexity of the underlying genetic basis of common human diseases.
The goal of this review is to identify and discuss those GWAS challenges that will require computational methods.
Contact: jason.h.moore@dartmouth.edu 1 INTRODUCTION The current strategy for revealing the genetic basis of disease susceptibility is to carry out a genome-wide association study (GWAS) with a million or more single nucleotide polymorphisms (SNPs) that capture much of the common variation in the human genome (Hirscchorn et al., 2005; Wang et al., 2005).
This approach is based on the idea that genetic variations with alleles that are common in the population will explain much of the heritability of common diseases (Reich and Lander, 2001).
This is referred to as the common disease common variant hypothesis and has been recently reviewed by Schork et al.
(2009).
These studies were made possible by the sequencing of the human genome (International Human Genome Sequencing Consortium, 2004) and the completion of the subsequent human haplotype mapping (HapMap) project To whom correspondence should be addressed.
that discovered millions of common SNPs and documented the correlation structure or linkage disequilibrium of the alleles at those loci (The International HapMap Consortium, 2005).
This knowledge about genome variation in combination with novel bioengineering methods made it possible to design chips for measuring more than a million SNPs for several hundred dollars or less per sample.
As the price of genome-wide genotyping has dropped, the number of studies utilizing GWAS has increased dramatically and this approach is now relatively common.
As Manolio et al.
(2008) and Donnelly (2008) have recently reviewed, hundreds of replicated susceptibility loci for as many as 70 diseases and traits have been reported from GWAS.
Unfortunately, despite high expectations, few of the loci identified via GWAS are associated with a moderate or large increase in disease risk and some well-known genetic risk factors have been missed (Williams et al., 2007).
In fact, the relative risks of most new loci are on the order of 1.1 to 1.2 at best which suggests that these individual SNPs may not be useful for genetic testing.
This limitation has been pointed out in a recent study by Jakobsdottir et al.
(2009) that showed SNPs identified by GWAS for a variety of diseases make very poor classifiers of disease, calling into question their usefulness for risk assessment in personal genetics (Moore and Williams, 2009).
To illustrate successes and failures of GWAS, consider the following review of breast, prostate, colorectal, lung and skin cancer that shows that a number of new susceptibility loci have been identified using the GWAS approach (Easton and Eeles, 2008).
As mentioned above, the loci identified by GWAS typically have very small effect sizes.
This is true for cancer where the increase in risk for the susceptibility alleles at each of the loci discovered by GWAS is generally 1.3-fold or less.
Let us first consider familial breast cancer as a rare disease that has a very high heritability and is thus believed to have a relatively simple etiology.
Easton et al.
(2007) reported five significant, replicated associations that were identified by GWAS and replicated in several independent samples of subjects.
Four of the discovered variants were in known genes and one was located in a hypothetical gene.
Assuming a multiplicative model, these five loci together explain only 3.6% of the excess familial risk of breast cancer.
Due to these small effect sizes, Ripperger et al.
(2009) concluded that these loci are not suitable for use in genetic testing.
In a follow-up study with two additional stages of testing and replication, two additional susceptibility loci were identified with odds ratios of 1.11 and 0.95, respectively.
These two loci account for much <1% of the familial risk of breast cancer (Ahmed et al., 2009).
When combined with the previous known genetic risk factors The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:03 9/2/2010 Bioinformatics-btp713.tex] Page: 446 445455 J.H.Moore et al.
for familial breast cancer, the estimated fraction of risk explained is 5.9%.
This is in stark contrast to BRCA1 and BRCA2 mutations that together account for between 20 and 40% of familial breast cancer.
While the application of GWAS to familial breast cancer has generated new knowledge and perhaps new biology, it has not resulted in new genetic tests that can be used to predict and prevent familial breast cancer.
The results for common diseases such as sporadic breast cancer and type II diabetes that have a much more complex underlying genetic architecture are similarly discouraging.
As another example, consider a recent GWAS applied to pancreatic cancer.
Amundadottir et al.
(2009) measured >500 000 SNPs in a detection sample of 1896 patients with pancreatic cancer and 1939 controls ascertained from the same population as the cases.
The authors also used a replication sample of 2457 cases and 2654 controls.
A logistic regression analysis of both samples identified a single SNP with an odds ratio of 1.2.
This single SNP was located in an intron of the ABO blood group gene.
This result confirmed previous epidemiological studies showing that the O blood group is associated with a lower risk of pancreatic cancer.
Interestingly, this association was first reported >50 years ago and thus does not represent a novel finding.
The failure to identify new susceptibility loci for some diseases using GWAS in relatively large sample sizes highlights some of the limitations of this approach.
These studies and many others highlight the positive and negative aspects of GWAS for common diseases and complex traits.
One explanation for the mixed results of GWAS is that the current biostatistical analysis paradigm is, by design, agnostic or unbiased in that it ignores what is known about disease pathobiology.
Further, the linear modeling framework often used for GWAS analysis usually considers only one SNP at a time thus ignoring the genomic and environmental context of each SNP (Moore and Williams, 2009).
As Clark et al.
(2004) predicted, our success with GWAS depends critically on the assumptions we make about disease complexity.
Recently, there has been a shift away from the one SNP at a time approach toward a more holistic approach that recognizes the complexity of the genotypephenotype relationship that is likely characterized by significant genetic heterogeneity and genegene and geneenvironment interaction.
We argue here that bioinformatics can play an important role in addressing the complexity of the underlying genetic basis of many common human diseases.
The goal of this review is to identify and discuss those GWAS challenges that require computational rather than, or in addition to, biostatistical methods.
We focus on computational methods for data mining and machine learning and bioinformatics methods for incorporating prior biological knowledge into data analysis algorithms.
We conclude with a discussion about maximizing the utility of bioinformatics software for GWAS analysis.
Readers are directed elsewhere for recent reviews of GWAS study design, quality control, imputation and biostatistical analysis issues (e.g.
Amos, 2007; Chanock et al., 2007; Kraft and Cox, 2008; Spencer et al., 2009; Ziegler et al., 2008).
2 DATA MINING AND MACHINE LEARNING 2.1 Why are data mining and machine learning methods needed?
An important goal of human genetics and genetic epidemiology is to understand the mapping relationship between interindividual variation in DNA sequences, variation in environmental exposure and variation in disease susceptibility.
Stated another way, how do one or more changes in an individuals DNA sequence increase or decrease their risk of developing disease through complex networks of biomolecules that are hierarchically organized, highly interactive and dependent on environmental exposures?
Understanding the role of genomic variation and environmental context in disease susceptibility is likely to improve diagnosis, prevention and treatment.
Success in this important public health endeavor will depend critically on the amount of non-linearity in the mapping of genotype to phenotype and our ability to address it.
Here, we define as non-linear an outcome that cannot be easily predicted by the sum of the individual genetic markers.
Non-linearities can arise from phenomena such as locus heterogeneity (i.e.
different DNA sequence variations leading to the same phenotype), phenocopy (i.e.
environmentally determined phenotypes that do not have a genetic basis) and the dependence of genotypic effects on environmental exposure (i.e.
geneenvironment interactions or plastic reaction norms) and genotypes at other loci (i.e.
genegene interactions or epistasis).
Each of these phenomena have been recently reviewed and discussed by Thornton-Wells et al.
(2004) who call for an analytical retooling to address these complexities.
Combining the complexities summarized here with GWAS data yields significant computational challenges.
To illustrate non-linear mapping from genotype to phenotype, consider the following example from sporadic Alzheimer disease (AD).
In 2004, Infante et al.
(2004) reported that polymorphisms in the interleukin-6 (IL-6) and interleukin-10 (IL-10) genes had an interaction effect on the risk of AD.
This study of 232 AD patients and 191 controls reported that patients with the IL-6 C/C and IL-10 A/A genotypes had a five times lower risk ofAD than control subjects (P = 0.005).
What makes this association interesting is the absence of a statistically significant association for the IL-10 A/A genotype (P = 0.102).
Further, there is significant biological plausibility for this interaction given the importance of inflammation in AD and the significant role of Il-6 as a pro-inflammatory molecule and IL-10 as an anti-inflammatory molecule.
Of course, the gold standard in genetic association studies is replication.
In 2009, Combarros et al.
(2009) replicated the interaction of the IL-6 and IL-10 genes in a collaboration of seven AD studies with a total of 1757 AD cases and 6295 controls.
The statistical replication of the non-linear interaction and the biological plausibility of the finding strongly suggest that these two genetic markers or nearby markers contribute to the development of AD.
Moore and Ritchie (2004) have provided an overview of three significant challenges that must be overcome if we are to successfully identify those genetic variations that are associated with health and disease using a genome-wide approach.
First, powerful data mining and machine learning methods will need to be developed to computationally model the relationship between combinations of SNPs, other genetic variations and environmental exposure with disease susceptibility.
This is because traditional parametric statistical approaches such as logistic regression have limited power for modeling high-order non-linear interactions that are likely important in the etiology of complex diseases (Moore and Williams, 2002).
A second challenge is the selection of SNPs that should be included in the analysis.
If non-linear interactions between genes explain a significant proportion of the heritability of common diseases, then combinations of SNPs 446 [17:03 9/2/2010 Bioinformatics-btp713.tex] Page: 447 445455 Challenges for GWAS will need to be evaluated from a list of thousands or millions of candidates.
Filtering algorithms and/or stochastic search or wrapper algorithms will play an important role in GWAS because there are more combinations of SNPs to examine than can be exhaustively evaluated using modern computational horsepower.
A third challenge is the biological interpretation of non-linear genetic models.
Even when a computational model can be used to identify SNPs with genotypes that increase susceptibility to disease, the specifics of the mathematical relationships cannot be translated into prevention and treatment strategies without interpreting the results in the context of human biology.
Making etiological inferences from computational models may be the most important and the most difficult challenge of all (Moore and Ritchie, 2004).
2.2 The modeling challenge The parametric linear statistical model plays a very important role in modern genetic epidemiology because it has solid theoretical foundation, is easy to implement using a wide range of different software packages such as SAS and R and is easy to interpret.
Despite these good reasons to use linear models, they do have limitations for detecting non-linear patterns of interaction (Moore and Williams, 2002).
In addition, they are not likely to explain a large part of the variance of any given trait (Moore, 2003) which may explain some of the missing heritability that has not been accounted for by GWAS (Manolio et al., 2009).
The first issue is that statistical or computational modeling of non-linear interactions and other complex phenomena such as locus heterogeneity inherently requires looking at combinations of SNPs.
Considering multiple SNPs simultaneously is analytically challenging because the study samples or instances get spread thinly across multiple combinations of genotypes.
This is because the number of genotype combinations goes up exponentially as each SNP is added to the model.
Estimation of parameters in a linear model can be problematic where no data are observed for individual genotype combinations (i.e.
empty cells).
The second issue is that parametric linear models are generally implemented such that interaction effects are only modeled using factors that exhibit independent marginal effects.
This makes model fitting easier but implicitly assumes that genetic architecture is simple and that important predictors will have detectable marginal effects.
Further, it is well documented that linear models have greater power to detect marginal effects than interactions (Lewontin, 1974; Wahlsten, 1990).
For example, the focused interaction testing framework (FITF) approach of Millstein et al.
(2006) provides a powerful logistic regression approach to detecting interactions but conditions on marginal effects.
Interactions in the absence of significant marginal effects are missed by FITF and other similar methods.
How common are genegene interactions in the absence of significant marginal effects likely to be?
Moore (2003) argues that a simple genetic architecture characterized by SNPs with large marginal effects is an unrealistic assumption for many common human diseases.
Rather, it is likely that complex phenomena such as epistasis or genegene interactions will make up much of the genetic architecture.
As Moore (2003) summarizes, there are several reasons for this.
First, epistasis is on old idea that has been around since the early 1900s.
Early geneticists such as Bateson (1909) recognized the importance of genegene interactions for explaining deviations from Mendelian patterns of inheritance.
More important than this historical note is the fact that epistasis is still discussed today as a key component of genetic architecture.
Second, biological systems are driven by complex biomolecular interactions.
As such, it makes sense that genegene interactions would play an important role in the genotype to phenotype mapping relationship.
Studies emerging from the genetic analysis of bacteria and yeast document widespread epistasis at the biological level.
Third, single SNP results do not always replicate even if cases where there is a general consensus that the association signal is real.
This has been the norm for GWAS where very few SNPs with significant marginal effects replicate in multiple independent samples.
As Marchini et al.
(2005) and Greene et al.
(2009c) suggest, this may be partly due to underlying patterns of epistasis.
Finally, epistasis is commonly found when properly investigated (Templeton, 2000).
Studies that look for genegene interactions in a manner that does not condition on marginal effects commonly find such non-linear effects.
We predict that the data mining and machine learning methods reviewed here will reveal numerous significant interactions and other complex genotype phenotype relationships when they are widely applied to GWAS data.
The limitations of the linear model and other parametric statistical approaches have motivated the development of data mining and machine learning methods (e.g.
Hastie et al., 2009; Mitchell, 1997).
The advantage of these computational approaches is that they make fewer assumptions about the functional form of the model and the effects being modeled (McKinney et al., 2006).
In other words, data mining and machine learning methods are much more consistent with the idea of letting the data tell us what the model is rather than forcing the data to fit a preconceived notion of what a good model is.
Several recent reviews highlight the need for new methods (Thornton-Wells et al., 2004) and discuss and compare different strategies for detecting statistical epistasis (Cordell, 2009; Motsinger et al., 2007).
The methods reviewed by Cordell (2009) include novel approaches such as combinatorial partitioning (Culverhouse et al., 2004; Nelson et al., 2001) and logic regression (Kooperberg et al., 2001; Kooperberg and Ruczinski, 2005) and machine learning approaches such as random forests (RFs).
Below, we briefly review two of these methods, RFs and multifactor dimensionality reduction (MDR) that have been developed to address these issues.
Importantly, as Marchini et al.
(2005) demonstrated, interaction analysis can have more power than traditional approaches despite issues such as multiple testing.
This study provides an important, but often overlooked, foundation for the methods described below.
2.3 Computational modeling using decision trees and RFs Classification or decision trees are a staple in the data mining and machine learning community due to their algorithmic simplicity and ease of interpretation.
Decision trees are widely used for modeling the relationship between one or more attributes and a discrete end point such as casecontrol status (Mitchell, 1997).
Here, we use the word attribute to mean a variable such as a SNP or a demographic variable such as gender that is used to make a prediction.
Attribute is commonly used this way in data mining and machine learning.
In statistics, an attribute is an independent variable, explanatory variable or predictor.
A decision tree classifies subjects as case or control by sorting them through a tree from node to node where each node is an attribute with a decision rule that guides that subject 447 [17:03 9/2/2010 Bioinformatics-btp713.tex] Page: 448 445455 J.H.Moore et al.
Fig.1.
Overview of the RF algorithm summarized in Section 2.3.
Adapted from Reif et al.
(2006).
through different branches of the tree to a leaf that provides its classification.
The primary advantage of this approach is that it is simple and the resulting tree can be interpreted as a series of IF-THEN rules that are easy to understand.
For example, a genetic model of heterozygote effects with genotype data coded {AA = 0, Aa = 1, aa = 2} might look like IF genotype at SNP1 = 1 THEN case ELSE control.
In this simple model, the root node of the tree would be SNP1 with decision rule = 1 and leafs equal to case and control (e.g.
see tree in Fig.1).
Additional nodes or attributes below the root node allows hierarchical dependencies (i.e.
interactions) to be modeled.
Here, we review RFs that extend decision trees for the analysis of more complex data.
A RF is a collection of individual decision tree classifiers, where each tree in the forest has been trained using a bootstrap sample of instances (i.e.
subjects) from the data, and each attribute in the tree is chosen from among a random subset of attributes (Breiman, 2001).
Classification of instances is based upon aggregate voting over all trees in the forest.
Individual trees are constructed as follows from data having N samples and M attributes: (1) Choose a training set by selecting N samples, with replacement, from the data.
(2) At each node in the tree, randomly select m attributes from the entire set of M attributes in the data (the magnitude of m is constant throughout the forest building).
(3) Choose the best split at that node from among the m attributes.
(4) Iterate the second and third steps until the tree is fully grown (no pruning).
Repetition of this algorithm yields a forest of trees, each of which have been trained on bootstrap samples of instances.
Thus, for a given tree, certain samples or instances will have been left out during training.
Prediction error is estimated from these out-of-bag instances.
The out-of-bag instances are also used to estimate the importance of particular attributes via permutation testing.
If randomly permuting values of a particular attribute does not affect the predictive ability of trees on out-of-bag samples, that attribute is assigned a low importance score (Bureau et al., 2005).
RFs are often used initially for selecting the subset of attributes that can then be modeled using a decision tree.
We discuss using algorithms such as RF for filtering subsets of SNPs later in Section 2.6.
An advantage of the RF approach is that the final decision tree models may uncover interactions among genes and/or environmental factors that do not exhibit strong marginal effects (Cook et al., 2004), especially when combined with methods such as ReliefF (see Section 2.6) for choosing the attributes to be used as nodes.
Additionally, tree methods are suited to dealing with certain types of genetic heterogeneity, since early splits in the tree define separate model subsets in the data (Lunetta et al., 2004).
RFs capitalize on the benefits of decision trees and have demonstrated excellent predictive performance when the forest is diverse (i.e.
trees are not highly correlated with each other) and composed of individually strong classifier trees (Breiman, 2001).
The RF method is a useful approach for studying genegene or geneenvironment interactions because importance scores for particular attributes take interactions into account without demanding a prespecified model (Lunetta et al., 2004).
However, most current implementations of the importance score are calculated in the context of all other attributes in the model.
Therefore, assessing the interactions between particular sets of attributes must be done through careful model interpretation, although there has been preliminary success in jointly permuting explicit sets of attributes to capture their interactive effects (Bureau et al., 2005).
Lunetta et al.
(2004) have previously shown that RFs outperform traditional methods such as the Fishers exact test when the risk SNPs interact.
This study revealed that the relative superiority of the RF method increases as more interacting SNPs are added to the model.
In addition, Bureau et al.
(2005) have shown that RFs are robust in the presence of noisy or potential false positive SNPs relative to methods that rely on independent marginal effects.
Initial results of RF applications to genetic data in studies of asthma (Bureau et al., 2005), rheumatoid arthritis (Sun et al., 2007) and glioblastoma (Chang et al., 2008), age-related macular degeneration (Jiang et al., 2009) and vaccination response (McKinney et al., 2009) are encouraging and it is anticipated that RF will prove a useful tool for detecting genegene interactions.
They may also be useful when multiple different data types (e.g.
proteomic biomarkers) are present (Reif et al., 2006, 2009) or for inferring gene networks (McKinney et al., 2009).
The primary limitation of tree-based methods is that the standard implementations condition on marginal effects.
That is, the algorithm finds the best single variable for the root node before adding additional variables as nodes in the model.
Combining RF with ReliefF methods (described below) shows potential for overcoming this limitation (McKinney et al., 2009).
Advantages of this approach include its basis on decision trees and the availability of the algorithm in many different open source software packages including R. In fact, the Willows package was designed specifically for tree-based analysis of SNP data (Zhang et al., 2009).
2.4 Computational modeling using MDR Thornton-Wells et al.
(2004) review the complex nature of the genotypephenotype relationship and suggest that we need new statistical and computational tools to address these complexities.
As a result, there is growing trend toward the development and 448 [17:03 9/2/2010 Bioinformatics-btp713.tex] Page: 449 445455 Challenges for GWAS Fig.2.
Summary of the constructive induction process for MDR.
The left bars within each cell represent the number of cases while the right bars represent the number of controls.
Dark-shaded cells are high risk while the light-shaded cells are low risk.
Prediction using any classifier can be carried out using the final constructed attribute.
evaluation of new and novel approaches that have more power for modeling non-linearity than parametric statistical approaches.
As Cordell (2009) recently summarized, MDR is an example of one novel computational strategy for detecting and characterizing non-linear patterns of genegene interactions in genetic association studies.
MDR was developed as a non-parametric (i.e.
no parameters are estimated) and genetic model-free (i.e.
no genetic model is assumed) data mining and machine learning strategy for identifying combinations of discrete genetic and environmental factors that are predictive of a discrete clinical end point (Hahn et al., 2003; Moore, 2004, 2007b; Moore and Hahn, 2004; Moore and White, 2006; Ritchie et al., 2001, 2003).
Unlike most other methods, MDR was designed to detect interactions in the absence of detectable marginal effects and thus complements statistical approaches such as logistic regression and machine learning methods such as RFs and neural networks.
At the heart of the MDR approach is a feature or attribute construction algorithm that creates a new variable or attribute by pooling genotypes from multiple SNPs (Moore and White, 2006).
The general process of defining a new attribute as a function of two or more other attributes is referred to as constructive induction, or attribute construction, and was first described by Michalski (1983).
Constructive induction, using the MDR kernel, is accomplished in the following way (Fig.2).
Given a threshold T , a multilocus genotype combination is considered high risk if the ratio of cases (subjects with disease) to controls (healthy subjects) exceeds T , otherwise it is considered low risk.
Genotype combinations considered to be high risk are labeled G1 while those considered low risk are labeled G0.
This process constructs a new one-dimensional attribute with values of G0 and G1.
It is this new single variable that is assessed, using any classification method.
The MDR method is based on the idea that changing the representation space of the data will make it easier for methods such as logistic regression, classification trees or a naive Bayes classifier to detect attribute dependencies.
As such, MDR significantly complements other classification method such as those reviewed by Hastie et al.
(2001).
This method has been confirmed in numerous simulation studies and a user-friendly open source MDR software package written in Java is freely available from www.epistasis.org.
Since its initial description by Ritchie et al.
(2001), many modifications and extensions to MDR have been proposed.
These include entropy-based interpretation methods (Moore and White, 2006), the use of odds ratios (Chung et al., 2007), log-linear methods (Lee et al., 2007), generalized linear models (Lou et al., 2007), methods for imbalanced data (Velez et al., 2007), permutation testing methods (Greene et al., 2010a; Pattin et al., 2009), methods for dealing with missing data (Namkung et al., 2009a), model-based methods (Calle et al., 2008), parallel implementations (Bush et al., 2006; Sinnott-Arnstrong et al., 2009) and different evaluation metrics (Bush et al., 2008; Mei et al., 2007; Namkung et al., 2009b).
These extensions have addressed many of the previous limitations of the MDR method.
The MDR approach has also been successfully applied to a wide range of different genetic association studies.
For example, Andrew et al.
(2006) used MDR to model the relationship between polymorphisms in DNA repair enzyme genes and susceptibility to bladder cancer.
A highly significant non-additive interaction was found between two SNPs in the Xeroderma pigmentosum group D (XPD) gene that was a better predictor of bladder cancer than smoking.
Importantly, these results have been independently replicated (International Consortium of Bladder Cancer, 2009).
2.5 The attribute selection challenge Combining the complexity of the genotypephenotype relationship described above with the challenge of attribute (e.g.
SNP) selection yields a needle-in-a-haystack problem.
That is, there may be a particular combination of SNPs or SNPs and environmental factors that together with the right non-linear function are a significant predictor of disease susceptibility.
However, individually each factor may not appear different than thousands of other SNPs that are not involved in the disease process and are thus noisy.
Therefore, the learning algorithm is truly looking for a genetic needle in a genomic haystack.
It is now commonly assumed that at least 106 carefully selected SNPs are necessary to capture much of the relevant variation across the human genome.
With this many attributes, the number of higher order combinations is astronomical.
These large datasets beg the question, what is the optimal computational approach to this problem?
There are two general approaches to selecting attributes for predictive models.
The filter approach preprocesses the data by algorithmically assessing the quality or relevance of each variable and then using that information to select a subset for analysis.
The wrapper approach iteratively selects subsets of attributes for classification using either a deterministic or stochastic algorithm.
The key difference between the two approaches is that the learning algorithm plays no role in selecting those attributes to consider in the filter approach.
As Freitas (2002) reviews, the advantage of the filter is speed while the wrapper approach has the potential to do a better job classifying subjects as sick or healthy.
We first discuss several learning algorithms that have been applied to classifying healthy and disease subjects using their DNA sequence information and then discuss filter and wrapper approaches for the specific problem of detecting epistasis or non-linear patterns of genegene interactions on a genome-wide scale.
2.6 Attribute selection using filter algorithms As discussed above, it is computationally infeasible to com-binatorially explore all high-order interactions among the SNPs 449 [17:03 9/2/2010 Bioinformatics-btp713.tex] Page: 450 445455 J.H.Moore et al.
in a genome-wide association study.
One approach is to filter out a subset of genetic variations with high quality (i.e.
likely to be associated) that can then be efficiently analyzed using a method such as RFs or MDR (Moore and White, 2006; Wilke et al., 2005).
There are many different statistical and computational methods for determining the quality of attributes.
A standard statistical strategy in human genetics and genetic epidemiology is to assess the quality of each SNP using a chi-square test of independence followed by a correction of the significance level that takes into account an increased false positive (i.e.
type I error) rate due to multiple tests.
This is a very efficient filtering method for assessing the independent effects of SNPs on disease susceptibility but it ignores the dependencies or interactions between genes.
Kira and Rendell (1992) developed an algorithm called Relief that is capable of detecting complex attribute dependencies even in the absence of marginal effects.
Relief estimates the quality of attributes through a type of nearest neighbor algorithm that selects neighbors (instances) from the same class and from the different class based on the vector of values across attributes.
For the purposes of this description, we assume the class is dichotomous.
Weights (W) or quality estimates for each attribute (A) are estimated based on whether the nearest neighbor (nearest hit, H) of a randomly selected instance (R) from the same class and the nearest neighbor from the other class (nearest miss, M) have the same or different values.
This process of adjusting weights is repeated for m instances.
The algorithm produces weights for each attribute ranging from 1 (worst) to +1 (best).
The time complexity of Relief is O(mna) where m is the number of instances randomly sampled from a dataset with n total instances and a attributes.
Kononenko (1994) improved upon Relief by choosing n (usually set to 10) nearest neighbors instead of just one.
This new ReliefF algorithm has been shown to be more robust to noisy attributes (Kononenko, 1994; Robnik-ikonja and Kononenko, 2003) and is widely used in data mining applications.
ReliefF is able to capture attribute interactions because it selects nearest neighbors using the entire vector of values across all attributes.
However, this advantage is also a disadvantage because the presence of many noisy or potential false positive attributes can reduce the signal the algorithm is trying to capture.
Moore and White (2007b) proposed a tuned ReliefF algorithm (TuRF) that systematically removes attributes that have low-quality estimates so that the ReliefF values if the remaining attributes can be reestimated.
The motivation behind this algorithm is that the ReliefF estimates of the true functional attributes will improve as the noisy attributes are removed from the dataset.
Moore and White (2007b) carried out a simulation study using previously published epistasis models (Velez et al., 2007) to evaluate the power of ReliefF, TuRF and a nave chi-square test of independence for selecting functional attributes in a filtered subset.
Moore and White (2007b) found that the power of ReliefF to pick (filter) the correct interacting attributes was consistently better (P 0.05) than a nave chi-square test of independence and that the TuRF algorithm was consistently better (P 0.05) than ReliefF across all models studied.
More recent extensions of the ReliefF algorithm have shown that using higher numbers of nearest neighbors greatly improve the power of ReliefF.
For example, Greene et al.
(2009b) showed that a spatially uniform ReliefF (SURF) that picks all neighbors within a predefined epsilon (i.e.
distance or radius) greatly improves the power to detect interacting SNPs over that of ReliefF.
McKinney et al.
(2007) have Fig.3.
Summary of how Relief, ReliefF and SURF select neighbors.
Each panel in this figure shows the genotypes at two markers for a dataset of cases and controls.
For the purpose of this example only these two markers will be considered and both are continuous.
When analyzing real data, the process of selecting neighbors is the same, however, but there will be thousands of discrete valued markers (SNPs) each of which would be represented by one of thousands of dimensions.
The individual for whom neighbors are being found is shown by the filled red circle.
The neighbors that each approach uses for weighting are highlighted in blue.
(AC) Represent how Relief, ReliefF and SURF would select neighbors to be used in weighting.
Relief selects the nearest individual of the same dichotomous class (blue circle) and the nearest individual of the other class (blue cross).
ReliefF selects some user specified number of individuals (two in this example) to be used for weighting.
SURF, instead of using a fixed number of neighbors, uses all individuals within a distance threshold.
The dotted line shows a hypothetical distance threshold.
combined ReliefF with measures of entropy to yield evaporative cooling ReliefF.
This approach was highly successful when used to select SNPs for a RFs analysis (McKinney et al., 2009).
The differences between how Relief, ReliefF and SURF select nearest neighbors are summarized in Figure 3.
When combined with permutation tests designed to assess interactions, P-values can also be used to select SNPs with significant ReliefF scores (Greene et al., 2010b; Wongseree et al., 2009).
These results suggest that algorithms based on ReliefF show promise for filtering interacting SNPs.
The disadvantage of the filter approach is that important attributes might be discarded prior to analysis.
Stochastic wrapper methods provide a flexible alternative and may be more powerful when the assumptions of the filter approach are not valid (Greene et al., 2009a).
2.7 Attribute selection using wrapper algorithms Wrapper methods may be more powerful than filter approaches because no attributes are discarded in the process.
As a result, every attribute retains some probability of being selected for evaluation by the classifier.
There are many different stochastic wrapper algorithms that can be applied to this problem (e.g.
Michalewicz and Fogel, 2004).
We review here evolutionary computing algorithms as an example stochastic search algorithm that has been developed for genetic association studies.
Ritchie et al.
(2003) and Moore et al.
(2007), for example, have explored the use of a type of evolutionary computing algorithm called genetic programming (GP) for modeling and attribute selection in genetic association studies.
GP is an automated computational discovery tool that is inspired by Darwinian evolution by natural selection (Banzhaf et al., 1998; Koza, 1992).
The goal of GP is to evolve computer programs to solve complex problems.
This is accomplished by first generating or initializing a population of random computer programs that are composed of the basic building blocks needed to solve or approximate a solution to the problem.
For genetic association studies this might be a list of SNPs, other important attributes such 450 [17:03 9/2/2010 Bioinformatics-btp713.tex] Page: 451 445455 Challenges for GWAS Fig.4.
Flowchart for a simple GP.
The goal is to randomly generate an initial population of computer programs or solutions (e.g.
genetic models), determine their fitness, select the best models, introduce variability and then iterate until the termination criteria are satisfied.
This executes a parallel stochastic search using the principles of evolution by natural selection.
as age and gender along with a list of mathematical functions.
Each randomly generated program is evaluated and the good programs are selected and recombined and mutated to form new computer programs.
This process of selection based on fitness and recombination to generate variability is repeated until a best program or set of programs is identified.
A flowchart for a simple GP is illustrated in Figure 4.
GP and its many variations have been applied successfully in a wide range of different problem domains including bioinformatics (e.g.
Fogel and Corne, 2003) and the genetic analysis of epistasis (Moore et al., 2007; Ritchie et al., 2003).
It is important to note that GP differs considerably from genetic algorithms (GAs) in that the solution to a problem in GP is represented by a computer program rather than a linear bit string.
This provides GP a great deal more flexibility than GA for both attribute selection and modeling.
Consider, for example, the use of GP for attribute selection.
Although we focused on GP here, a GA would likely work just as well if attribute selection is the only task.
Moore and White (2007a) developed and evaluated a simple GP wrapper for attribute selection in the context of an MDR analysis.
The goal of this study was to develop a stochastic wrapper method that is able to select attributes that interact in the absence of independent marginal effects.
At face value, there is no reason to expect that a GP or any other wrapper method would perform better than a random attribute selector because there are no building blocks for genegene interactions in the absence of marginal effects when accuracy is used as the fitness measure.
That is, the fitness of any given classifier would look no better than any other when just one of the correct SNPs is in the MDR model.
Preliminary studies by White et al.
(2005) support this idea.
For GP or any other wrapper to work there need to be recognizable building blocks.
Moore and White (2007a) specifically evaluated whether including preprocessed attribute quality estimates using TuRF (see above) in a multiobjective fitness function improved attribute selection over a random search that just uses accuracy as the fitness of the models.
Using a wide variety of simulated data, Moore and White (2007a) demonstrated that including TuRF scores in addition to accuracy in the fitness function significantly improved the power of GP to pick the correct two functional SNPs out of 1000 total SNPs.
The use of expert knowledge to guide GP population initialization (Greene et al., 2009d), mutation (Greene et al., 2007), recombination (Moore and White, 2006) and a computational evolution system (Greene et al., 2010b; Moore et al., 2008) have also all shown promise.
There may also be an important role for linkage disequilibrium in providing missing building blocks (Bush et al., 2009).
3 BIOLOGICAL KNOWLEDGE DATABASES FOR ANALYSIS AND INTERPRETATION ReliefF and other measures such as interaction information (Moore and White, 2006) are likely to be very useful for providing analytical means for filtering genetic variations prior to epistasis analysis using either RFs or MDR, for example.
However, there is growing recognition that we should also use the wealth of accumulated knowledge about gene function to prioritize which genetic variations are analyzed for genegene interactions and other complex effects.
For example, for any given disease there are often multiple biochemical pathways that have been experimentally confirmed to play an important role.
Genes in these pathways can be selected for genegene interaction analysis thus significantly reducing the number of genegene interaction tests that need to be performed.
Gene Ontology (GO), chromosomal location and proteinprotein interactions are all example sources of expert knowledge that can be used in a similar manner.
Consider for example, the recent studies by Pattin et al.
(2008, 2009) who have specifically reviewed proteinprotein interaction databases as a source of expert knowledge that can be used to guide GWASs of epistasis.
Here, you might expect that a gene coding for a protein that interacts with many other proteins might be a good candidate for interaction with one or more other genes.
You could use this information in several different ways.
First, you could employ a biological filter and only test for interactions among SNPs in those genes with many proteinprotein interactions.
Alternatively, you could weight each gene by its degree of protein protein interaction and then use this expert knowledge in a stochastic wrapper algorithm.
Of course, the most interesting genes might be those with fewer connections.
In this case, you could use the inverse of the connectedness as your expert knowledge.
Some might find it more useful to use the strength of the proteinprotein interaction evidence rather than the number of connections.
Here, the genes could be prioritized or weighted by their confidence score that reflects the quality of the experimental and computational evidence for their biochemical interaction with other protein products.
The important consideration when using expert knowledge from biological databases is to harness this information in a way that makes sense to the biologist.
Emily et al.
(2009) demonstrate how proteinprotein interactions were used to reduce the search for two-locus interactions using GWAS data from the Wellcome Trust Case-Control Consortium.
The use of expert knowledge from GO and biochemical pathways in GWAS has been recently investigated by a number of groups.
For example, Baranzini et al.
(2009), Bush et al.
(2009), Elbers et al.
(2009), Emily et al.
(2009), Herold et al.
(2009), Holmans et al.
(2009), Medina et al.
(2009), ODushlaine et al.
(2009), Pan (2008), Peng et al.
(2009), Saccone et al.
(2008) and Torkamani et al.
(2008) have all shown that using biological knowledge to guide 451 [17:03 9/2/2010 Bioinformatics-btp713.tex] Page: 452 445455 J.H.Moore et al.
genetic association studies may provide more meaningful results.
Yu et al.
(2009) provide a hypothesis testing framework for combining multiple SNPs from the same gene or from multiple genes in a pathway-based manner.
Askland et al.
(2009) recently showed that patterns of SNPs in biological pathways are more likely to replicate than individual SNPs in GWAS.
This is highly consistent with the idea that interactions may be more important that marginal effects.
Zamar et al.
(2009) have provided a software tool called Path to assist with pathway-based analysis of SNPs.
Wilke et al.
(2008) have suggested that we should not even begin to analyze a GWAS until we have exhaustively studied each candidate gene and each pathway.
The use of pathways and other biological knowledge to guide GWAS is an important emerging area and is gaining more attention at international conferences (Moore, 2009).
Perhaps the greatest challenge of any data mining exercise is interpreting the results.
This is especially important in GWAS where biological plausibility helps give increased credibility to results (Greene et al., 2009c; Moore and Williams, 2005).
Assessing biological plausibility, however, is difficult without software that can only be generated through collaboration among geneticists and bioinformaticists.
Fortunately, there are a number of emerging software packages that are designed with this in mind.
GenePattern, for example, provides an integrated set of analysis tools and knowledge sources that facilitates this process (Reich et al., 2006).
Other tools such as the exploratory visual analysis database and software are designed specifically for integrating research results with biological knowledge from public databases (Reif et al., 2005) and have been applied to GWAS data (Askland et al., 2009).
Several commercial packages for typing bioinformatics results to pathways and gene function include Pathway Studio from Ariadne and Ingenuity Pathway Analysis from Ingenuity Systems.
Ontology-based methods (Tsoi et al., 2009) and literature-based systems (Yu et al., 2008) have also been recently proposed for aiding with interpretation.
The use of biological knowledge in genetic association studies does have some important limitations.
First, success is highly dependent on the quality of the information in the databases.
In assessing quality, we need to consider both accuracy of the information included and the completeness of the information.
Our expectation is that the quality and completeness of the databases will continue to improve and the amount of good information will soon outweigh the bad.
This of course depends critically on the both the throughput and quality of experimental methods used to reveal molecular details and their relationships.
Second, it is important to keep in mind the disconnect between the biology that happens at the cellular level and the statistical patterns of genetic variation that we observe at the population level (Moore and Williams, 2005).
It is very difficult to make inference about population-level risk from the knowledge of cellular function and vice versa.
This will always to some degree limit our ability to use biological knowledge to assist with association studies in human populations.
4 SOFTWARE CHALLENGES Perhaps the biggest challenge moving forward with GWAS analysis is to facilitate communication at all levels among biomedical researchers, biostatisticians and computer scientists.
The key to successful bioinformatics is close face-to-face collaboration between the biologist, biostatistician and bioinformaticist.
This is not always possible due to distances between institutions but is critical for moving forward with a systems-based research agenda where large volumes of data and information are the norm.
The best bioinformatics tools will be those that experts in each area can use jointly.
We propose that one way to facilitate the close collaboration between biologists, biostatisticians and bioinformaticists is to make available user-friendly software packages that can be used jointly by researchers with expertise in experimental biology and researchers with expertise in statistics and computer science (Moore, 2007a).
This will require software that is intuitive enough for a biologist and powerful enough for an analyst.
To be intuitive to a biologist, the software needs to be easy to use and needs to provide output that is visual and easy to navigate.
To be powerful, the software needs to provide the functionality that would allow a biostatistician and a bioinformaticist the flexibility to explore the more theoretical aspects of the algorithm.
The key, however, to the success of any such software package is the ability of the biologist and the analysts to sit down together at the computer and jointly carry out an analysis.
This is important for several reasons.
First, the biologist can help answer questions the analyst might have that are related to domain-specific knowledge.
Such questions might only arise at the time of the analysis and might otherwise be ignored.
Similarly, the biologist might have ideas about specific questions to address with the software that might only be feasible with the help of the analyst.
For example, a question that requires multiple processors to answer might need the assistance of someone with expertise in parallel computing.
The idea that biologists and analysts should work together is not new.
Langley (2002) has suggested five lessons for the computational discovery process.
First, traditional computational notations are not easily communicated to biologists.
This is important because a computational model may not be interpretable by a biologist.
Second, biologists often have initial models that should influence the discovery process.
Domain-specific knowledge such as details about enzymatic reactions in a biochemical pathway can be critical to the discovery process.
Third, biological data are often rare and difficult to obtain.
It often takes years to collect and process the data to be analyzed.
As such, it is important that the analysis is carefully planned and executed.
Fourth, biologists want models that move beyond description to provide explanation of data.
Explanation and interpretation are paramount to the biologist.
Finally, biologists want computational assistance rather than automated discovery systems.
Langley (2002) suggests that practitioners want interactive discovery environments that help them understand their data while at the same time giving them or their collaborating analyst control over the modeling process.
Collectively, these five lessons suggest that synergy between biologists and bioinformaticists is critical.
This is because each has important insights that may not get expressed or incorporated into the discovery process if either carries out the analysis in isolation.
Future bioinformatics databases and analysis tools that successfully integrate these lessons will prove to be the most useful for GWAS and other high-throughput approaches.
5 SUMMARY AND CONCLUSIONS Figure 5 summarizes a bioinformatics analysis strategy for GWAS based on the challenges outlined here.
As discussed, it is important 452 [17:03 9/2/2010 Bioinformatics-btp713.tex] Page: 453 445455 Challenges for GWAS Fig.5.
Flowchart for bioinformatics analyses of GWAS data.
The use of filter and wrapper algorithms along with computational modeling approaches is recommended in addition to parametric statistical methods.
Biological knowledge in public databases has a very important role to play at all levels of the analysis and interpretation.
that biological knowledge available in public databases be integrated with GWAS data and clinical data.
Following quality control procedures, available biological knowledge can be used to help guide both statistical and computational analyses as reviewed in Section 3.
Given the complexity of the genotypephenotype mapping relationship, we recommend two complementary strategies for computational analysis.
The first strategy uses filter algorithms (see Section 2.6) such as those based on ReliefF or prior statistical results to select more manageable subsets of SNPs that can then be more efficiently analyzed using computational methods.
The second uses wrapper algorithms (see Section 2.7) based on stochastic search methods such as GP for identifying optimal combinations of SNPs that are associated with disease end points.
We suggest that computational modeling methods such as RF (see Section 2.3) and MDR (see Section 2.4) are needed to complement parametric statistical methods such as logistic regression for identifying non-linear patterns in GWAS data.
The challenge of any statistical or computational analysis is the biological interpretation of a set of results.
Use of prior knowledge about biological systems can facilitate this process.
Once an inference is made and validated, new knowledge can be contributed to the public knowledge databases thus enhancing future iterations of this flowchart.
We anticipate that this entire process will be greatly aided by the development of powerful and user-friendly software that can be optimally used by biologists, biostatisticians and bioinformaticists (see Section 4).
GWAS have generated a number of important bioinformatics challenges including the modeling of complex genotypephenotype relationships using data mining and machine learning methods, the use of biological knowledge databases to help guide and interpret genetic association studies and the development of powerful and user-friendly software that can facilitate interaction and collaboration among biologists and bioinformaticists.
The flowchart shown in Figure 5 provides a starting point for the development of comprehensive and fully informative analyses of GWAS.
This will become especially necessary as we generate more and more data and discover new complexities in the genome that make powerful bioinformatics research strategies even more critical for identifying genetic risk factors for common human diseases.
With whole-genome sequencing on the horizon, we need to recruit the next generation of bioinformaticists to tackle these and other important computational challenges in genetic analysis because agnostic biostatistical approaches will only get us so far.
ACKNOWLEDGEMENTS We would like to thank the anonymous reviewers for their very helpful comments and suggestions.
Funding: National Institutes of Health (LM010098, LM009012 and AI59694).
Conflict of Interest: none declared.
ABSTRACT Motivation: Artemis and Artemis Comparison Tool (ACT) have become mainstream tools for viewing and annotating sequence data, particularly for microbial genomes.
Since its first release, Artemis has been continuously developed and supported with additional functionality for editing and analysing sequences based on feedback from an active user community of laboratory biologists and professional annotators.
Nevertheless, its utility has been somewhat restricted by its limitation to reading and writing from flat files.
Therefore, a new version of Artemis has been developed, which reads from and writes to a relational database schema, and allows users to annotate more complex, often large and fragmented, genome sequences.
Results: Artemis and ACT have now been extended to read and write directly to the Generic Model Organism Database (GMOD, http://www.gmod.org) Chado relational database schema.
In addition, a Gene Builder tool has been developed to provide structured forms and tables to edit coordinates of gene models and edit functional annotation, based on standard ontologies, controlled vocabularies and free text.
Availability: Artemis and ACT are freely available (under a GPL licence) for download (for MacOSX, UNIX and Windows) at the Wellcome Trust Sanger Institute web sites:Contact: artemis@sanger.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Artemis (Berriman and Rutherford, 2003; Rutherford et al., 2000) and the Artemis Comparison Tool (Carver et al., 2005) (ACT) have gathered increasing number of users since their initial release with more than 33 000 downloads in 2007 (Fig.1) from over 90 countries.
Artemis is a sequence viewer and annotation tool.
It is used primarily not only for genome annotation but also for DNA visualization and analysis, and as a teaching aid.
The user can view the sequence and features in context at different levels of granularity from the amino acids to the complete genome level, with continuous zooming between these levels.
Artemis includes clear To whom correspondence should be addressed.
and interactive navigation, customizable search facilities, predefined and user-configurable graphical plots and statistical overviews.
ACT is an extension that makes use of the Artemis components to display pairwise comparisons between two or more sequences.
Therefore, ACT inherits a lot of the functionality found in Artemis.
ACT is used to identify and analyse regions of similarity and difference between genomes and to explore conservation of synteny, in the context of the entire sequences and their annotation.
Artemis and ACT were originally designed to read and write flat files in the common sequence formats (EMBL, GenBank, FASTA and GFF3) to display and edit sequence and annotation, and they continue to be able to work in this mode.
This works well for many uses but does restrict Artemis to single user access when annotating sequences.
A previous way around this problem was to split the files up and merge them back after editing.
This is far from ideal, particularly for large multi-user projects.
To maximize the potential of Artemis as a tool for community annotation, Artemis has been extended to connect to a relational database, and now supports reading and writing to the GMOD Chado schema.
Artemis and ACT use the iBatis Data Mapper framework (http://ibatis.apache.org) to map Java objects to SQL parameters and result sets.
For each of the tables in the Chado schema that are used, an XML descriptor file is used to define the mapping.
This has the advantage of simplifying the Artemis code by separating it from the SQL and it also means that there is complete control over the SQL queries, without modification of the Artemis code.
An alpha-version of the software presented here was robustly tested during a week-long Plasmodium falciparum (malaria) re-annotation workshop in October 2007.
Fixes, additional func-tionality and further optimizations stemming from this were then included in the new releases of Artemis v.10 and ACT v.7.
2 IMPLEMENTATION 2.1 Artemis and ACT reading and writing in database mode Artemis and ACT can be launched in database mode from a web launch or from the command line.
Connection to the database is established via Java Database Connectivity (JDBC) and by specifying the database location and the JDBC driver, for example: art-Dchado="hostname:port/databaseName?userName" \-Djdbc.drivers=org.postgresql.Driver-Dibatis 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
Artemis and ACT Fig.1.
Download statistics for Artemis and ACT for each year from 2001 2007.
This prompts for a password for the given username.
For a read-only database the flag Dread_only will suppress the password prompt.
On successfully logging in a database and file manager window opens showing an expandable organism tree (Fig.2).
Double clicking on the sequence names opens them in Artemis.
ACT currently still works with comparison flat files but the sequences themselves can be dragged from the database manager into the ACT file requestor.
It is possible to store comparative regions in Chado so that ACT could in the future read this information from the database as well.
All of the sequence and annotations are read from the database when the interface is opened.
The exceptions to this are the cluster and orthologue and paralogue data that are read lazily; that is that they are only fully read in from the database when they are viewed in the Gene Builder.
Loading these data into Artemis in this way speeds up the initial process of opening the sequence in the interface.
For future releases of this software, lazy loading of the majority of the annotation will be explored.
This is likely to become more important as the database grows.
Annotation within Artemis traditionally centres around feature keys used by the International Nucleotide Sequence Database Collaboration (INSDC), which label-specific annotation types within a flat-file format.
In particular, protein-coding sequences or CDS were the default annotation objects, although other feature keys could be specified.
The Chado schema is designed to store descriptions of any feature that can be located in a genome.
For instance, it holds a complete gene model consisting of the gene itself, transcripts, exons, untranslated regions (UTRs) and polypeptides.
It makes use of the Sequence Ontology (SO, http://www.sequenceontology.org/) (Eilbeck et al., 2005) to represent these data, such that a full hierarchy of genome features can be added.
The CDS features themselves are not stored, as they can then be inferred from the coordinates of the other features.
SO is designed to describe biological sequences including located sequence features, for example, polymorphisms, polypeptide domains and predictions.
All these feature types can therefore be stored in Chado and viewed in Artemis and ACT.
In database mode, the majority of the functionality and appearance of Artemis has been maintained (Fig.3).
However, Artemis has been changed to be able to handle the hierarchy of constituent features relating to genes (exons, transcripts, polypeptides, etc.).
It was also a requirement that it was able to present the annotation in a structured Fig.2.
TheArtemis database and file manager.
The organisms in the database are shown at the top.
Double clicking on the chromosome opens it up in Artemis.
The file management system also provides access to local (middle panel) and remote file systems (via ssh; bottom panel).
manner and able to handle the main underlying ontologies, i.e.
gene, sequence and relationship.
A Gene Builder tool has been developed for displaying and manipulating the gene hierarchy and associated annotation.
A gene is created by highlighting a base range and selecting, from the Create menu, the Gene Model From Base Range option.
The basic constituent features are created; i.e.
gene, transcript, CDS and polypeptide.
Artemis joins exon features from the underlying database that are part of the same transcript and represents them as a CDS feature, which is shown on the reading frame lines in its main display.
For clarity, the polypeptide and transcript features are hidden in the feature display window and appear greyed out in the 2673 T.Carver et al.
Fig.3.
Snapshot of Artemis in database mode displaying chromosome 1 of P. falciparum.
The sequence and feature displays are identical to the standard Artemis.
There is a Commit button at the top right of the interface for writing back to the database.
The top feature display panel shows a zoomed-out representation of the region.
Below this is a zoomed-in view showing the nucleotide level with the six frames of translation.
The lower panel shows a scrollable list of the features.
These panels are all linked, so that if a feature is selected by double clicking on it the other windows scroll to the feature.
The polypeptide and transcript are hidden from the feature display but appear (greyed out) in the feature list at the bottom.
feature list as illustrated in Figure 3.
Note there is an option in the feature display pop-up menu to control what features are hidden.
This can be used to hide features that are not of interest during a session.
2.2 Artemis Gene Builder A Gene Builder window for a selected gene feature can be opened from the Edit menu by selecting the Selected Feature in Editor option.
The gene structure and feature annotation are two distinct parts of the Gene Builder (Fig.4), and are described below.
In general the majority of the annotation is attached to the polypeptide, although the system allows annotation to any object within the gene hierarchy.
Therefore when a CDS is selected, and the Gene Builder opened for the corresponding gene, it displays the annotation on the polypeptide.
2.2.1 Gene hierarchy and structure In the top left-hand side of the Gene Builder is a tree structure of the gene model.
To the right of this is a Gene Map, a graphical representation of the features.
These show the gene hierarchy as described by SO.
A feature can be selected from either the tree or the graphical view.
The annotation for the selected feature is displayed in the bottom part of the Gene Builder.
Another new feature the Gene Builder brings to Artemis is the ability to display protein maps (Fig.5).
If there is any polypeptide predictions associated with a feature then a Protein Map tab is shown.
When selected this displays the information about the protein and the predictions, e.g.
InterPro (Mulder et al., 2007), SignalP (Bendtsen et al., 2004) and others.
Fig.4.
The Gene Builder showing a gene hierarchy at the top and underneath the annotation for the associated polypeptide.
Fig.5.
An example of a Protein Map in the Gene Builder.
The details of the domain predictions are shown in mouse-over tool-tips.
The protein database features (e.g.
Pfam, Prosite) can be clicked on to open that entry in a browser window.
Structural changes can be carried out in the graphical view.
Feature ends can be selected and dragged to adjust their coordinates.
On right clicking on this area a pop-up menu allows features to be added or deleted in the gene model.
Exons are added by highlighting a range in the Gene Builder window, clicking the left mouse button and dragging, and then adding an exon from the pop-up menu.
The exon coordinates can be refined by dragging the exon boundary in the zoomed-in (base-level) view of the main Artemis window.
Multiple transcripts (Fig.6) and their associated exons can be added to the gene model.
In previous versions of Artemis, it could prove difficult to build and annotate overlapping splice forms from a single gene.
Therefore, in order to make it easier to deal with overlapping 2674 Artemis and ACT Fig.6.
Example of a gene with multiple transcripts in the graphical view of the gene hierarchy in the Gene Builder.
Each transcript is a child node of the gene in the tree on the left with its associated features displayed when expanded.
To assist in building multiple transcripts the checkboxes to the left of each transcript are used to hide and show them in the main Artemis window.
features in the Gene Builder, a checkbox for each transcript can be used to show and hide them in the main Artemis window.
2.2.2 Location, keys and annotation For the selected feature in the gene model, the location, key and annotation are displayed under the gene model hierarchy and graphical structure.
To emphasize which of the features the annotation shown belongs to, its name is displayed above the feature key selector.
The coordinates and feature key can be changed here.
Instead of the free text used in the original Artemis editor, the Gene Builder provides a structured means of annotating.
There are four annotation sections in the Gene Builder described below.
A check box at the bottom of the Gene Builder changes between a scrollable and a tabbed view of the sections.
In the scrollable view, the sections have hide () and show (+) buttons and empty sections are automatically hidden.
(1) Properties: this contains feature properties, such as synonyms and time last modified.
Synonyms are categorized using the controlled vocabulary tables in Chado.
The time last modified is updated when a change to that feature is written back to the database.
It is often desirable for data from previous annotation versions to still be available, therefore in this section of the Gene Builder there is a checkbox to mark features and entire gene models as obsolete.
This hides them from view in the main Artemis window, but they are still available from the feature list at the bottom.
(2) Core: the core annotation contains any annotation that does not fit into the other sections.
For example, free-text comments and cross-links to the scientific literature.
Hyperlinks are provided for databases (e.g.
UniProt, EMBL, PubMed and others defined in the options file) that open up a local browser.
(3) Controlled vocabulary (CV): the CV module in the schema is concerned with controlled vocabularies or ontologies.
Artemis uses biological ontologies to allow very precise and expressive annotation.
A form is provided for adding and deleting Gene Ontology (GO; http://www.geneontology.org/) descriptions, product, Riley class and other controlled annotations, which are stored in the CV tables.
When adding a term to a feature the CV (e.g.
product) and a keyword (e.g.
histone) are prompted for.
The term to be added is then selected from a drop down list of terms containing Fig.7.
Snapshot of the Artemis log viewer, using log4j to generate log messages and errors.
It records SQL statements and can be used to monitor changes to the database.
The last seven lines in this log are generated to show the changes made to the database in a more human readable format.
the word or phrase.
Additional controlled curations can be shown if their CV name in Chado is prefixed with CC_ (e.g.
CC_workshop).
(4) Match: orthologue and paralogue links can be added to other genes in the database in this section.
Externally calculated paralagous gene clusters can also be displayed here.
There are links for opening the Gene Builder for each matched gene entry or for opening a separate Artemis instance displaying the gene and the surrounding features.
Future developments to the Gene Builder will add to this and incorporate annotation transfer mechanisms.
For example, the user will be able to specify the annotation to transfer to or from the selected genes in the list of matches.
2.3 Writing to the database When a feature or qualifier is changed, added or deleted the Commitbutton (on the top tool bar) changes colour to red.
Changes only get written back to the database when this button is clicked.
There is also an option under the File menu to Commit To Database.
In ACT, there is no commit button and committing back to the database is carried out from the menus.
If there is an error during the commit then an option to force commit is provided andArtemis will commit what it can.
TheApache Log4j (http://logging.apache.org/log4j) framework has been used to provide logging information in the Artemis log window (Fig.7).
Using the standard configuration file (log4j.properties) the logging can be directed to a file.
The database read and write statements are logged as SQL as well as a more human readable notation of the write (insert, update and delete) statements.
This is useful for monitoring progress and generating statistics for the annotation process.
In Artemis and ACT, additional features from files can be read in and overlaid on the database entry.
If they are of the correct sequence ontology they can then be bulk loaded into the database.
This is done in the interface by selecting the features and moving them to the database entry.
These features can then be committed to the database.
Flat files are still needed for submission and to run external analyses on.
It is therefore possible to write out EMBL flat files from the database entry in Artemis.
The default option is to flatten the gene hierarchy and transfer the annotation onto a CDS feature.
2675 T.Carver et al.
2.4 Community annotation Multiple users can launch Artemis and ACT clients and both query and modify the database simultaneously.
This enables multiple annotators to work on the same sequence at the same time.
This has been stress tested and used in a Plasmodium falciparum (malaria) genome re-annotation workshop (October 2007).
More than 30 scientists used Artemis clients to simultaneously connect to a single database and annotate the genome.
Access to the database was also made available to the course participants via a virtual private network (VPN) after the course.
External public access to this data is also possible (read only) via local instances of Artemis connecting to a copy of the database at the Sanger Institute (http://www.sanger.ac.uk/Software/Artemis/databases/).
For selected features, BLAST and FASTAresults can be generated in Artemis via the Run menu.
The results are written to a file.
For community projects, these need to be stored in a central file system or copied to the different sites involved.
A future development for Artemis is likely to make use of optionally storing search results files directly within the database, or a specific results file server to simplify data management within large shared projects.
To ensure that users do not overwrite each others annotation, Artemis records the time each feature was last modified.
This is then checked against the database time stamp of the features before the changes are written to the database.
If the corresponding feature in the database has been changed by another user, Artemis will warn that this has occurred and ask whether to continue with the commit process, thus allowing a forced overwrite of previous changes, where necessary.
3 DISCUSSION From their conception Artemis and ACT have been designed to be extensible.
This has enabled them to have functionality added as needed and to evolve with the requirements of various levels of users.
Artemis (version 10) and ACT (version 7), using iBatis mappings and JDBC, can now connect to and read and write to a Chado relational database.
This allows access from multiple clients to sequence data.
A form of optimistic versioning is used so that Artemis will warn a user if another user has modified the feature they are updating during that Artemis session.
Annotation is displayed and edited in a structured manner in the newArtemis Gene Builder using predefined and standard ontologies.
Links within the database can be used to open up separate Gene Builders and Artemis windows for individual regions.
Hyperlinks to external databases launching a local browser window allow rapid and easy linking to relevant external data sources.
Using the gene hierarchy allows more fine-grained annotation, with multiple alternate transcripts, CDSs, UTRs, etc.
In addition, the standardized ontology-based schema allows Artemis to be both a powerfully expressive and extremely precise annotation tool.
Artemis and ACT can be used to display and annotate both eukaryotes and prokaryotes.
A eukaryotic example has been presented here but, because SO terms define the data model, Chado can be used to describe prokaryotes as well.
Prokaryote genes are created in the same way and represented with a simple gene model with a gene, transcript and a single CDS.
Prokaryote annotations can also use the Riley class, which is stored as a controlled vocabulary.
ACKNOWLEDGEMENTS This article is dedicated to the memory of Marie-Adle Rajandream.
We would like to thank Professor Christopher Newbold (University of Oxford Institute of Molecular Medicine) for his suggestions on improving the design of the Artemis Gene Builder.
Funding: This work was supported by the Wellcome Trust through their funding of the Pathogen Genomics group at the Wellcome Trust Sanger Institute.
Conflict of Interest: none declared.
ABSTRACT Motivation: Gene regulatory network (GRN) inference reveals the in-fluences genes have on one another in cellular regulatory systems.
If the experimental data are inadequate for reliable inference of the net-work, informative priors have been shown to improve the accuracy of inferences.
Results: This study explores the potential of undirected, confidence-weighted networks, such as those in functional association databases, as a prior source for GRN inference.
Such networks often erroneously indicate symmetric interaction between genes and may contain mostly correlation-based interaction information.
Despite these drawbacks, our testing on synthetic datasets indicates that even noisy priors re-flect some causal information that can improve GRN inference accur-acy.
Our analysis on yeast data indicates that using the functional association databases FunCoup and STRING as priors can give a small improvement in GRN inference accuracy with biological data.
Contact: matthew.studham@scilifelab.se Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Gene regulatory network (GRN) inference determines causal in-fluences in gene networks and is useful for understanding regu-lation, usually at the transcriptional level, which can hypothetically lead to effective modification of regulatory net-works.
GRN inference has been studied extensively over the past decade as described in the following reviews (Hecker et al., 2009; Lecca and Priami, 2013; Penfold and Wild, 2011; Tegner and Bjorkegren, 2007).
In GRNs the nodes are genes and the edges are influences, annotated with a direction and signed strength.
These networks are normally constructed using transcriptomic data from experiments in which all of the genes in the network of interest have been perturbed, often with RNAi knockdowns.
Gene expression is profiled either in a time series or when the system has reached a steady-state.
A plethora of inference methods have been developed and are based on information theory (Altay and Emmert-Streib, 2010; Faith et al., 2007; Margolin et al., 2006), Boolean networks (Haider and Pal, 2012; Layek et al., 2011; Wang et al., 2012), Bayesian networks (Djebbari and Quackenbush, 2008; Husmeier andWerhli, 2007; Yu et al., 2004) and ordinary differential equa-tions (ODEs; Gardner et al., 2003; Gustafsson and H ornquist, 2010; Yip et al., 2010).
A subset of the methods based on a ODE description formulates the inference as a convex programming problem (Julius et al., 2009; Kulkarni et al., 2012; Zavlanos et al., 2011).
The Dialogue on Reverse Engineering Assessment and Methods (DREAM) (Marbach et al., 2012; Penfold and Wild, 2011; Prill et al., 2010; Stolovitzky et al., 2009) and other bench-marking studies (Bansal et al., 2007; Geier et al., 2007; Hache et al., 2009) have shown that although many methods perform better than random, there is a lot of room for improvement.
It is difficult to determine the true GRN for a biological system because even if major characteristics such as transcription factor binding are known, subtle influences may not be well understood.
To avoid this problem, many benchmarking studies use synthetic data where the true GRN is known and the accur-acy of inference methods can be analysed.
GeneNetWeaver (GNW; Schaffter et al., 2011) generated synthetic networks and datasets for three of the DREAM competitions and this program uses nonlinear dynamical models of transcription and translation.
Another synthetic data generation program, GeneSpider (Tjarnberg et al., 2014, manuscript in preparation), uses a linear dynamical model of transcription.
These two pro-grams were used in our study to generate synthetic data.
Prior knowledge may be incorporated into the inference method in order to improve accuracy and can also increase effi-ciency by reducing the search space.
Researchers have begun to explore these possibilities by using pathways (Bonneau et al., 2006; Husmeier and Werhli, 2007), transcription factor binding (Gevaert et al., 2007; Gustafsson and H ornquist, 2010; Shih and Parthasarathy, 2012), proteinprotein interactions (Shih and Parthasarathy, 2012), gene ontology (Pei and Shin, 2012), epi-genetics (Chen et al., 2013) and literature (Djebbari and Quackenbush, 2008; Julius et al., 2009; Layek et al., 2011).
These studies incorporate the prior in different ways, but for inference methods which minimize a penalty function, the prior knowledge is often quantified as the unlikelihood of a link, and this value is multiplied by the sparsity term in the penalty function (Christley et al., 2009; Greenfield et al., 2013; Gustafsson and H ornquist, 2010).
The prior value has also been discretized to be positive, negative or zero and used as a constraint in an optimization (Julius et al., 2009; Kulkarni et al., 2012; Zavlanos et al., 2011).
Although there have been several studies, none of them has been good enough to become a wide-spread standard.
A comprehensive, user-friendly prior can be constructed using functional association data: undirected, confidence-weighted*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com mailto:matthew.studham@scilifelab.se Lecca and Priami, 2012 &NoBreak; &NoBreak; Haider and Pal, 2012 &NoBreak;, &NoBreak; n Zavlanos etal., 2011; &NoBreak; Marbach etal., 2012 &NoBreak; &NoBreak; z &NoBreak; &NoBreak; Shih and Parthasarathy, 2012; &NoBreak;-&NoBreak; &NoBreak; &NoBreak;, &NoBreak; `` '' Gustafsson and H&ouml;rnquist, 2010; &NoBreak; , Zavlanos etal., 2011; &NoBreak; values (between 0 and 1) indicating the possibility of an inter-action between two genes.
One good place to find such data is in functional association databases, which aggregate heterogeneous experimental data and output a confidence score describing the probability of a functional linkage between two proteins.
FunCoup (Schmitt et al., 2014) and the Search Tool for the Retrieval of Interacting Genes (STRING) (Szklarczyk et al., 2011) are two examples of such databases which aggregate data from literature, protein interactions, genomics, orthology, coexpression and subcellular localization in order to calculate the probability of a functional association.
Although these data-bases pairwise confidence scores were not created to be priors for GRN inference, they may contain enough information to improve GRN accuracy.
The pairwise confidence scores from functional association databases can be used to create an undirected, confidence-weighted likelihood matrix that can be easily incorporated as a prior into a GRN inference method.
Bonneau et al.
(2006) used first-generation functional association databases Prolinks (Bowers et al., 2004) and Predictome (Mellor et al., 2002) as part of a gene biclustering algorithm but not explicitly in the network inference.
To our knowledge no one has extensively studied the potential of undirected, unsigned, confidence-weighted networks as priors for GRN inference.
In this study we generated synthetic datasets of steady-state expression data and functional association-like priors, assumed a dynamical systems model, and used a convex optimization-based inference method.
We compared the accuracy of the GRN infer-ences with and without the priors to determine if and when un-directed, unsigned, confidence-weighted networks improve GRN inference.
We also explored a few different experimental (per-turbation) designs to see if they had an impact on the priors usefulness.
Finally, we applied our method to a yeast dataset and used FunCoup and STRING to generate priors to see if they can improve network inference with biological data.
2 METHODS 2.1 Regulatory model Our model is based on system identification concepts common in engin-eering and similar to the models in Gardner et al.
(2003), Julius et al.
(2009) and Zavlanos et al.
(2011).
When the regulatory network is near a steady-state it can be approximated by the linear dynamical system: _x=Ax+p y=x+"; 1 where x 2 Rn are actual transcript differences between a perturbed and an unperturbed initial state for n genes in an experiment, p 2 Rn are exogenous perturbations of the n genes, A 2 Rnn is the network model in which each element aij 2 R; 8i; j describes the regulatory influence of gene j on gene i, y 2 Rn are the measurements of the transcript differences and " 2 Rn represents measurement noise in a single experiment.
When gene expression is measured at steady-state (Crampin et al., 2004) and multiple perturbation experiments are combined we find that Y= A1P+ "; 2 where Y 2 Rnm is the steady-state gene expression matrix, P 2 Rnm is the perturbation matrix and " is the noise matrix for a system with n genes and m experiments.
We will avoid underdetermined problems and only focus on situations where m n. In such a network, RNA decay is con-founded with self-regulation.
Normally in a stable system aii50; 8i.
2.2 Inference method Our network inference method is formulated as a convex optimization problem (Boyd and Vandenberghe, 2004), similar to methods in (Julius et al., 2009; Zavlanos et al., 2011).
We used a numerical cutoff based on the reduced precision of the optimization solver to identify zero and non-zero values.
The optimization problem is shown below.
minimizeA kAY+PkF+ X i X j 1 wij  jaijj Initially, without considering a prior, our goal is to fit the model and ensure a level of sparsity which ignores effects caused by noise.
This first term deals with the model fit by minimizing the sum of the residuals kAY+PkF: The second term encourages sparsity and agreement with the prior: P i P j 1 wij  jaijj whereW 2 Rnn; 0 wij 1 8i; j is an un-directed, unsigned, confidence-weighted prior network and 2 R+ (zeta) is the regularization parameter.
This term is similar to the incorp-oration of the prior in Christley et al.
(2009) and Gustafsson and H ornquist (2010).
Without a prior, the cross-optimization procedure described in (Tjarnberg et al., 2013) can be used to set the regularization parameter, .
However, this procedure was not created for models in which the prior is incorporated into the sparsity term.
With no proven method to set the parameter, all sparsity levels are considered, from a diagonal-only network (only RNA decay) through a fully connected network.
2.3 Synthetic data analysis Five 20-gene true networks were initially generated using GNW (Schaffter et al., 2011).
In order to create realistic networks, we used a subset of yeast interactions (provided by GNW), and there were at least 10 regulators in each network.
These initial networks were unsigned, so we randomly assigned a positive or negative sign to the non-zero links.
Then the link strengths were discretized to values {1, 0, 1} and we made sure that the self-interactions had a discretized strength of1 to represent RNA decay.
In general the networks were sparse, with an average spars-ity level of 83.45%, or 66 non-zero links.
There were three experimental designs: single-20, double-20 and double-40.
For the single-20, each gene was knocked-down once and the number of experiments, m, is equal to the number of genes, n (m=n=20).
For the double-20, each experiment perturbed two genes: all were knockdowns except in one experiment one gene is over-expressed.
The number of experiments equaled the number of genes (m=n=20).
For the double-40, each experiment knocked-down two genes and the number of experiments was double the number of genes (m=2n=40).
All experiments were unique within each design and the strength of each perturbation was set to 0.5, positive in overexpressions and negative in knockdowns.
Given the true network and experimental design, we used GNW (in a way independent of the network generation) and GeneSpider (GSP; Tjarnberg et al., 2014, manuscript in preparation) to gener-ate gene expression data.
Both network generators add random numbers from a Gaussian distribution to simulate measurement errors.
We therefore perform a Monte Carlo simulation using five replicates of each dataset.
Each generator created five expression matrices for each true network and experimental design.
We created a total of 150 datasets (2 generators 5 replicates 5 true networks 3 experimental designs).
We define single-to-noise ratio (SNR) as the smallest signal (measured by the singular value) in the gene expression matrix divided by the largest signal in the error (Nordling, 2013): i131 Functional association networks as priors for GRN inference &NoBreak; &NoBreak; , &NoBreak; &NoBreak; &NoBreak; &NoBreak; &NoBreak; &NoBreak;, &NoBreak; , &NoBreak; , &NoBreak; &NoBreak; &NoBreak;&NoBreak; &NoBreak; &NoBreak;-ene et eaver--about , `` '' x x x &NoBreak; SNR= n Y 1 " 3 This is a conservative SNR and it is motivated by the fact that network inference is an inverse problem, where the smallest signal is very import-ant because it affects the largest signal in the inverse.
The SNR of GNW-generated data (median 0.00717, range [6.01 108, 0.137]) was signifi-cantly lower than the SNR of the data generated by GSP (median 0.409, range [0.052, 2.06]).
An SNR51 indicates that the largest noise signal obscures the smallest expression signal, as was the case for most of our datasets.
Therefore most of our datasets would be considered to have low information content and could use the help of a prior.
Synthetic priors (undirected, confidence-weighted network matrices) were generated to have confidence score distributions similar to those found in FunCoup.
The non-zero links were approximated with a mod-ified exponential decay distribution with an average confidence score of 0.85 and the zero links were approximated with a gamma distribution with an average confidence score of 0.4 (Thomas Schmitt, unpublished data).
These distributions were sampled to create the prior matrix.
The initial symmetric prior matrix C 2 Rnn; 0 cij 18 i; j was adjusted to create the final prior matrix W: W 2 Rnn;wij= 9 10 cij i 6 j 1 i=j 8< : 9= ; 4 This adjustment is necessary to avoid full confidence values (ones) in off-diagonal elements, thereby ensuring that the prior is soft evidence.
Ones were assigned to the diagonal to represent RNA decay.
We tested priors with different accuracy, i.e.
different levels of agreement with the true network.
A non-zero link in the prior was deemed accurate if there was also a non-zero link (of any direction and sign) at the same position in the true network.
A zero link in the prior was deemed accurate if there were no non-zero links (of any direction and sign) at the same position in the true network.
We created priors which were 50, 60, 70, 80, 90 and 100% accurate, and the accuracy applied to both zero and non-zero links.
Since functional association priors do not cover self-interactions, we did not count these (diagonal elements) in the accuracy.
There was an element of randomness in the prior generation, so we created five replicate prior matrices for each level of accuracy and true network, resulting in a total of 150 synthetic functional association priors (6 accuracy levels 5 true networks 5 replicates).
A nave prior W=I; containing only the RNA decay links, was also created to act as a control in the analysis.
2.4 Yeast data analysis We used a publicly available dataset (GEO:GSE4654) from Hu et al.
(2007) containing transcriptional profiles from 263 transcription factor knockout strains in Saccharomyces cerevisiae.
The yeast strains, derived from BY4741, were sampled in the mid-log phase (Hu et al., 2007).
Although there were 263 genes, we only used 173 in our analysis because some data points were missing and not all the genes were represented in our gold standard network.
Our final gene expression matrix contained 173 genes and 173 experiments.
The gold standard network was derived from the Yeastract database (Teixeira et al., 2014).
We obtained 187856 activation/inhibition inter-actions, of which 2910 were relevant to our 173 genes in the knockout experiments.
Functional association priors were constructed from FunCoup (Schmitt et al., 2014) and STRING (Szklarczyk et al., 2011).
On the FunCoup website we searched for the network using the default settings except: 0.1 confidence threshold (the lowest possible threshold), S.cerevisiae species, and 0 expansion depth.
The search was done using FunCoup version 3.0 on January 14, 2014.
On the STRING website we used the multiple names search, protein interactors and a zero required confidence score.
This search was done using STRING version 9.1 on January 15, 2014.
Both priors were adjusted according to Equation (4) in the previous section, and the final FunCoup and STRING priors had 2685 and 6555 links, respectively.
2.5 Inferences We used the CVX package in MATLAB to implement the GRN infer-ences.
CVX iterates until the precision cutoff (104) is reached.
In the synthetic analysis, we inferred networks for each experimental design and dataset and prior and sparsity level combination, which resulted in 1.7 million inferences (150 datasets 5 priors 6 accuracies 381 sparsity levels).
We used a search procedure to modify the regularization param-eter to obtain inferences for all sparsity levels.
In the rare situation in which a sparsity level was unreachable (by modifying the regularization parameter) the inference accuracy was assumed to be the average of the accuracies from the adjacent sparsity levels.
Often the same sparsity level was reached with different parameter values; in this case we used the average inference accuracy in the results.
For the yeast network, which was much larger than the synthetic net-works, time constraints did not allow us to use the same sparsity level search procedure.
Instead we used intervals for the regularization param-eter, approximately evenly spaced in logarithmic space, which resulted in 22 378 inferred networks for each prior.
These inferences covered more than 25% of all possible sparsity levels: 8196 using the nave prior, 7911 using the FunCoup prior and 7740 using the STRING prior.
For sparsity levels with no inferred network, the accuracy was assumed to be the average of accuracies from adjacent sparsity levels.
The resulting inferred networks interaction strengths were discretized (values {1, 0, 1}) for evaluation.
2.6 Evaluation For the synthetic analysis, the inference accuracy was calculated as the proportion of links that were equal in the true discretized network and inferred discretized network.
The accuracy from the inferences using the nave prior were subtracted from the accuracy from the inferences using the FunCoup-simulated prior in order to determine the improvement achieved by using the functional association prior.
We also performed an alternative evaluation considering only true non-zero links.
For the yeast analysis we used a similar procedure except we only considered true non-zero links when evaluating accuracy because the Yeastract network contains validated links, but not necessarily validated non-links.
2.7 Functional association prior accuracy estimation The accuracy of the FunCoup and STRING priors used in the yeast data analysis was estimated with respect to the Yeastract network.
Since the functional association priors do not have signed links, sign was ignored.
In order to make this prior accuracy analogous to the prior accuracy used in the synthetic analysis, only half of the off-diagonal links were evalu-ated.
Although there are a total of 2910 off-diagonal links in the Yeastract network, 154 of them are symmetrical, so we only considered 2756 links.
The values in the prior matrices needed to be discretized to differentiate links from non-links.
We did two prior accur-acy estimations.
In the first estimation, all non-zero values wij40; j4i are considered links.
In the second estimation all values at or above a thresh-old wij 0:5; j4i are considered links.
3 RESULTS In the synthetic analysis we generated 5 true networks and 150 expression datasets (covering 3 experimental designs) using non-linear (GNW) and linear (GSP) generation methods.
We used these datasets along with 150 priors (covering 6 different i132 M.E.Studham et al.
x-, `` '' x x-&NoBreak; &NoBreak; &NoBreak; , , &NoBreak; &NoBreak; , , ,-x x x , , , , ,-, , five `` '' three six accuracy levels), and completed over 1.7 million inferences to determine if and when a functional association prior improves GRN inference.
Since we were unable to find a method to opti-mally set our sparsity parameter, we evaluated the inference ac-curacy over all sparsity levels (except self-interactions were always non-zero).
There were five networks used in the analysis, and since their individual results were similar, we have only shown the combined results.
Also, there was never a perfect in-ference; the best inference recovered 99% of the links, so there was always room for improvement.
A perfect prior never resulted in a perfect inference because of noise and the fact that these priors are symmetrical (i.e.
that do not give interaction direction) and our true networks were not symmetrical.
In the results below, improvement is defined as the inference accuracy percent-age of the method using the simulated functional association prior minus the inference accuracy percentage of the method using the nave prior.
3.1 Accurate priors improve performance If a functional association prior is accurate enough (i.e.
enough non-zero links in the true network are represented by undirected non-zero links in the prior) then inference is improved over virtually all sparsity levels.
Figure 1 shows the levels of prior accuracy that resulted in improved GRN inference for datasets generated by GNW and GSP.
It should be noted that we used two different dataset generators to ensure that we have a diversity of synthetic data, not to explicitly compare the two generators.
As shown in Figure 1A, a 70% accurate prior clearly improved inference for GNW-generated data and in Figure 1D a 90% accurate prior clearly improved inference for GSP-generated data.
A similar overall improvement profile is also seen when only considering true non-zero links (Supplementary Fig.S1).
In this situation, the magnitude of improvement is more dramatic but the accuracy level at which the prior achieves improvement is almost exactly the same as when considering all links.
Figure 2 shows the improvement over all sparsity levels for these two types of generated datasets using these prior accura-cies.
The most improvement is seen at moderate sparsity levels.
The GNW inference improvement profile is relatively uniform, while the GSP inference improvement profile was clearly skewed toward the sparse end, indicating that the prior was helpful in determining which links to keep in a sparse network.
For both dataset generators, if the prior was not accurate enough then the resulting inferred network is worse than when using a nave prior.
3.2 Better improvement when using data generated using noisy, nonlinear model A comparison of Figure 1 parts (A) and (B) shows that a func-tional association prior improves inferences for GNW-generated data (from a noisy, nonlinear model) much more than for GSP-generated data (from a less noisy, linear model) if the actual sparsity level is unknown; this is shown by the difference in mean (dark blue) or median (light blue) boxes at the same prior accuracy.
If the sparsity level is known (green boxes) then the GSP-generated results showed a larger improvement if the prior accuracy is 90 or 100%.
A less accurate prior showed a greater tendency to result in worse inference for GSP-generated data, as seen for the 50% accurate prior.
The GNW-generated data were also noisier based on over 68 000 inference profiles (i.e.
dataset/prior combinations).
There appears to be a negative relationship between SNR and improvement (Supplementary Fig.S2).
3.3 Experimental design did not significantly affect improvement There were three experimental designs: single-20, double-20 and double-40.
For single-20, each gene was knocked-down once and the number of experiments, m, is equal to the number of genes, n (m=n=20).
For the double-20, each experiment perturbed two genes: all were knockdowns except in one experiment where one gene was overexpressed.
The number of experiments equaled the number of genes (m=n=20).
For the double-40, each experi-ment knocked-down two genes and the number of experiments was double the number of genes (m=2n=40).
All experiments were unique within each design.
The results were similar for the three experimental designs (Supplementary Fig.S3).
However, the three different designs did not have as much overlap for the GSP-generated data.
Here the double-20 showed the most improvement, followed by the single-20, and finally the double-40.
There was still over-lap, and this difference can be explained by differences in SNR which are discussed in the following section.
The double-20 was the noisiest, then the single-20, and the double-40 was the least noisy.
3.4 Application to yeast network We applied our method to a yeast dataset (Hu et al., 2007) with 173 genes and 173 experiments, using FunCoup and STRING as priors, and the Yeastract database (Teixeira et al., 2014) as a gold standard.
Using only the nave prior, the maximum inference accuracy is only 49.52% of the gold standard links, so there is plenty of room for improvement.
Figure 3 shows the improvement over all sparsity levels for the two functional association priors when compared to the nave prior.
In Figure 3A the FunCoup prior is helpful for a large range from 19000 links and sparser, except for one small spot 4000 links.
For networks with more than 19 000 links the FunCoup prior lowers the inference accuracy.
In Figure 3A the maximum improvement is 1.10%, the minimum is 0.61% and the average is 0.23%.
These percentages equate to roughly 32, 18 and 7 links, respectively.
The STRING prior is shown in Figure 3B.
For networks with more than 19 000 links there is unlikely to be improvement, but inference of sparser networks is improved using this prior.
In Figure 3B the maximum improvement is 1.31%, the minimum is 0.44% and the average is 0.45%.
These percentages equate to roughly 38, 13 and 13 links, respectively.
3.5 Accuracy of FunCoup and STRING priors In an attempt to quantify their accuracy, the FunCoup and STRING priors were compared to the Yeastract network.
In order to make these prior accuracies analogous to our i133 Functional association networks as priors for GRN inference ' &NoBreak; &NoBreak; around , around , ,-,-, ,-,-, synthetic prior accuracies, we did not count both directions of symmetrical links.
Therefore our Yeastract network contained 2756 links (2910 minus 154 symmetrical links).
When estimating the accuracy for the functional association priors, we had to discretize the values to differentiate links from non-links.
When all non-zero values are considered links, the FunCoup prior contained 1256 links, 594 of which were in the Yeastract network, so it covered 22% of the validated links.
The STRING prior contained 3191 links, 1414 of which were in the Yeastract network, so it covered 51% of the validated links.
When a confidence score threshold of 0.5 is used, FunCoup gives us 263 links, 130 of which are in common with the Yeastract network, and STRING has 1155 links, of which 548 are in the Yeastract network.
With this threshold, FunCoup and STRING covered 5% and 20% of the validated links, respectively.
4 DISCUSSION Our results show that use of a functional association prior matrix can improve GRN inference accuracy.
The prior needs to be at least 70% accurate in order to show a clear improvement over most sparsity levels based on our testing of synthetic data.
However, our testing on a yeast dataset indicates that the prior accuracy can be much lower and still result in a small Fig.1.
Inference improvement and prior accuracy.
As the prior gets more accurate, the GRN inference improvement increases.
At each prior accuracy level, 125 inferences are averaged and the accuracies over the sparsity levels are aggregated using the median (dark blue), mean (light blue), true sparsity level (green) and maximum (magenta) inference improvement.
The results from the GNW-generated data are shown in (A) and the GSP-generated data in (B) i134 M.E.Studham et al.
, , , about , , , improvement over most sparsity levels.
It is important to note, however, that we consider all possible links in the synthetic ana-lysis and only gold standard links in the yeast analysis.
This 70% level of prior accuracy is at odds with several infer-ence prior studies which assert that even an inaccurate prior can aid in GRN inference.
Greenfield et al.
(2013) show that even if their prior consists of more than 90% erroneous links they can still accurately recover a GRN.
Although their prior incorpor-ation is similar to ours (they multiply unlikelihood times the strength in the sparsity term) their inference method is different and they limit the possible regulators to transcription factors.
In our model any gene can influence any other gene, regardless of its known molecular function.
Christley et al.
(2009) were also able to work with an inaccurate prior but they used an extra parameter (set by cross-validation) to weight the prior informa-tion so an inaccurate prior would simply be given less weight than an accurate one.
These methods, as well as ours, can be seen as picking the model, from the set of all models that cannot be rejected based on the recorded data, that minimizes the objective function Fig.2.
Prior improves inference over almost all sparsity levels.
For all plots above, the inference accuracy improvement is shown over all sparsity levels.
The average improvement is shown as the black line and the gray line is one SD from the average.
The vertical dotted gray line shows the average true sparsity level of the five synthetic networks.
(A) GNW-generated data, single-perturbation design with 70% prior accuracy, (B) GNW-generated data, single-perturbation design with 90% prior accuracy, (C) GeneSpider-generated data, single-perturbation design with 70% prior accuracy and (D) GeneSpider-generated data, single-perturbation design with 90% prior accuracy.
Parts (A), (B) and (D) show that the average improvement can be positive over almost all sparsity levels i135 Functional association networks as priors for GRN inference &NoBreak; &NoBreak; based on the prior.
The ability to test the hypothesis made by the prior depends on the informativeness of the recorded data.
If the data were very informative then the prior would not be helpful nor needed and in that case the prior has no influence.
The fact that the prior improved inferences based on the GNW-generated data much more than the corresponding in-ferences based on GeneSpider-generated data might be ex-plained by the differences in the two generators.
We used GeneSpider and a linear model to generate datasets, while GNW has nonlinearities built in to its dataset generation.
Our inference method is based on a linear dynamical system, so it follows that it is easier for it to recover a network from data created with a linear model.
Thus the inference with the nave prior works better on the GSP-generated data compared to the GNW-generated data, and we just do not need the functional association prior as much in that case.
Another explanation for the discrepancy between the inference of GNW-and GSP-generated data could be due to the differ-ences in SNR.
GNW data had a lower signal than GSP data (Supplementary Fig.S2), and it is logical that the nave prior would do worse (and thus increase the improvement) when there is a low SNR.
The SNR of some datasets generated Fig.3.
FunCoup (A) and STRING (B) priors improve yeast network inference for most sparsity levels.
The plots show inference accuracy improvement over almost all sparsity levels for the yeast network with 173 genes using the FunCoup and STRING priors.
The most fully connected network had 29 906 non-zeros and the sparsest network had 173 non-zeros (all self-interactions).
Only the 2910 off-diagonal links in the Yeastract network were considered.
Only about a quarter of the sparsity levels were actually inferred; the accuracies for the other sparsity levels were estimated based on those inferences.
The vertical line shows the Yeastract network sparsity i136 M.E.Studham et al.
as ene et eaver ene et eaver-'by GNW is so low that it is questionable that they are in-formative for network inference (Tjarnberg et al., 2013).
The yeast data consists of expression changes caused by knockout of each of the 173 genes.
A successful gene knockout alters the topology of the regulatory network because the corres-ponding node and all of its links are removed.
Strictly speaking, this implies that we are trying to infer the wild-type steady-state network based on data recorded from 173 different knockout steady-state networks, which should be questioned.
A topology change can be seen as a nonlinear transformation, so it is in general also questionable if a linear model can be used.
However, in this case the number of data points equals the number of parameters in the network model so the data can always be explained using a linear model, which motivates why we, following the parsimony principle, use one.
In principle, every indirect path through genes that are not included in the model should show up in the inferred model (Nordling, 2013).
Nonetheless, we only included direct links among the 173 genes that were in the Yeastract gold standard network.
We therefore verified that a linear model with the topology given by this gold standard can explain the inputoutput rela-tionship.
Actually, such a model can explain 99.5% of the vari-ation in the recorded data.
One should bear in mind that the dominating 20 components explain more than 75% of the total variation and that the gene expression matrix is ill-conditioned (condition number above 2000), so the dataset is not sufficiently informative for complete network inference (Nordling, 2013).
On the other hand, if it was informative enough then the prior would not be needed and it would not be an interesting test case.
The lack of information is likely to in part explain why the prior, despite being inaccurate, leads to a small improvement.
Functional association priors from FunCoup (Alexeyenko et al., 2011) or STRING (Szklarczyk et al., 2011) might be useful in GRN inference if these priors capture enough causal information.
These functional association databases do a good job of aggregating heterogeneous experimental data, which makes them convenient, but many of the associations (e.g.
coex-pression) are the result of correlation and not necessarily caus-ation.
Since we estimate the prior accuracies of FunCoup and STRING to be well below the 70% threshold for our yeast ana-lysis, it seems unlikely that these priors reflect enough causal information for clear improvement over most sparsity levels.
However, our yeast analysis also shows, for certain sparsity ranges, that using FunCoup and/or STRING can result in small inference improvement.
ACKNOWLEDGEMENTS The authors would like to thank Thomas Schmitt for informa-tion about FunCoup, including the distribution of confidence values; Yeastract researchers for providing activation and inhibition information; and Richard Bonneau for helpful suggestions.
Funding: This work was supported by SciLifeLab and the Swedish strategic research program ESSENCE.
Conflict of Interest: none declared.
ABSTRACT Summary: SEAL is a scalable tool for short read pair mapping and duplicate removal.
It computes mappings that are consistent with those produced by BWA and removes duplicates according to the same criteria employed by Picard MarkDuplicates.
On a 16-node Hadoop cluster, it is capable of processing about 13 GB per hour in map+rmdup mode, while reaching a throughput of 19 GB per hour in mapping-only mode.
Availability: SEAL is available online at http://biodoop-seal.sourceforge.net/.
Contact: luca.pireddu@crs4.it Received on March 7, 2011; revised on May 9, 2011; accepted on May 26, 2011 1 INTRODUCTION Deep sequencing experiments read billions of short fragments of DNA, and their throughput is steadily increasing (Metzker, 2010).
These reads need to be post-processed after sequencing to prepare the data for further analysis, which implies that the computational steps need to scale their throughput to follow the trend in sequencing technology.
Such high data rates imply the need for a distributed architecture that can scale with the number of computational nodes.
Typical post-processing steps include sequence alignment, which is a fundamental step in nearly all applications of deep sequencing technologies, and duplicate read removal, which is a major concern for Illumina sequencing (Kozarewa et al., 2009).
The pressure for better and faster tools has recently given rise to the development of new alignment algorithms that outperform traditional ones in terms of both speed and accuracy (Li and Homer, 2010).
Distributed alignment tools have also been created, with Crossbow (Langmead et al., 2009a) as one of the most prominent examples.
However, Crossbow is based on Bowtie (Langmead et al., 2009b), and thus does not currently support gapped alignment, an important feature for many applications (Li and Homer, 2010).
In this work we describe SEAL, a new distributed alignment tool that combines BWA (Li and Durbin, 2009) with duplicate read detection and removal.
SEAL harnesses the Hadoop MapReduce framework (http://hadoop.apache.org) to efficiently distribute I/O and computation across cluster nodes and to guarantee reliability by resisting node failures and transient events such as peaks in cluster load.
In its current form, SEAL specializes in the pair-end alignment of sequences read by Illumina sequencing machines.
SEAL uses a version of the original BWA code base (version 0.5.8c) that has To whom correspondence should be addressed.
been refactored to be modular and extended to use shared memory to significantly improve performance on multicore systems.
2 METHODS SEAL is currently structured in two applications that work in sequence: PairReadsQseq and Seqal.
PairReadsQseq is a utility that converts the qseq files (Illumina, Inc., 2009) produced by Illumina sequencing machines into our prq file format that places entire read pairs on a single line.
Seqal is the core that implements read alignment and optionally also performs duplicate read removal following the same duplicate criteria used by Picard MarkDuplicates (http://picard.sourceforge.net).
Both applications implement MapReduce algorithms (Dean and Ghemawat, 2004) which run on the Hadoop framework.
MapReduce and Hadoop: MapReduce is a programming model prescribing that an algorithm be formed by two distinct functions: map and reduce.
The map function receives one input record and outputs one or more key-value pairs; the reduce function receives a single key and a list of all the values that are associated to that key.
Hadoop is the most widespread implementation of MapReduce.
Pairing reads in PairReadsQseq: PairReadsQseq groups mate pairs from qseq data files into the same record, producing prq files where each line consists of five tab-separated fields: id; sequence and ASCII-encoded base qualities for read 1 and 2.
Read alignment and duplicates removal in Seqal: SEALs second MapReduce application, Seqal, takes input pairs in the prq format and produces mapped reads in SAM format (Li et al., 2009).
The read alignment is implemented in the map function.
Rather than implementing a read aligner from scratch, we integrated BWA (Li and Durbin, 2009) into our tool.
We refactored its functionality into a new library, libbwa, which allows us to use much of the functionality of BWA programmatically.
Although it is written in C, it provides a high-level Python interface.
To take advantage of this feature, the Seqal mapper is written in Python, and integrates into the Hadoop framework using Pydoop (Leo and Zanetti, 2010).
For each pair of reads, the aligner produces a pair of alignment records.
The user can choose to filter these by whether or not the read is mapped and by mapping quality.
Then, the reads may be directly output to SAM files, or put through a reduce phase where duplicates are removed; the choice is made through a command line option.
Like Picard MarkDuplicates, Seqal identifies duplicate reads by noting that they are likely to map to the same reference coordinates.
The specific criteria we use defines two pairs as duplicates if their alignment coordinates are identical, both for their first and second reads.
Likewise, lone reads are considered duplicates if they are aligned to the same position.
When a set of duplicate pairs is found, only the one with the highest average base quality is kept; the rest are discarded as duplicates.
Moreover, when a lone read is aligned to the same position as a paired read, the lone one is discarded.
If, on the other hand, only lone reads are found at a specific position then, as for pairs, only the one with the highest average base quality is kept.
2.1 Evaluation Correctness: we verified the correctness of SEAL by performing the alignment of the 5M dataset (Table 1) to the UCSC HG18 reference genome The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:53 6/7/2011 Bioinformatics-btr325.tex] Page: 2160 21592160 L.Pireddu et al.
Table 1.
SEAL evaluation: input datasets Dataset No.
of lanes No.
of pairs Size (GB) Read length 5M 0 5.0106 2.3 91 DS1 1 1.2108 51 100 DS3 3 3.3108 147 100 DS8 8 9.2108 406 100 The 5M dataset consists of the first 5M pairs from run id ERR020229 of the 1000 Genomes Project (Durbin et al., 2010).
The three DS datasets are from a production sequencing run on an Illumina HiSeq 2000.
Table 2.
Comparison of running time in hours between BWA on a single node with 8 cores and SEAL running on 32 nodes without duplicates removal Dataset BWA time (h, 1 node) SEAL time (h, 32 nodes) 5M 0.49 0.04 DS1 11.26a 0.63 DS3 32.39a 1.72 DS8 89.35a 4.78 Note that the SEAL running time includes qseq to prq format conversion.
aTime is predicted as a linear extrapolation of the throughput observed on the 5M dataset.
(Fujita et al., 2010) with both SEAL and BWA ver.
0.5.8c and then comparing their output.
With BWA, we ran bwa aln and bwa sampe, while with SEAL we ran the PairReadsQseq and Seqal applications.
The result was identical for 99.5% of the reads.
The remaining 0.5% had slightly different map quality scores (mapq), while the mapping coordinates were identical for all but two reads.
Both of the latter two cases had multiple best hits but resulted in different alignment choices probably due to insert size statistics, in turn due to the particular input read batch.
Slight differences in mapq scores are expected because their calculation takes into account the insert size statistics, which are calculated on sample windows on the input stream of sequences.
Since the sample windows seen by the command line version of BWA and SEAL are different for each read, a slight change in the mapq value is expected.
To verify this hypothesis, we ran BWA with varying input datasets while keeping 3000 of those reads that produced mapq variations in the original experiment.
We observed that the mapq values for those reads varied between runs.
Speed and scalability: we tested SEAL with varying input size (DS datasets from Table 1) and cluster size (16, 32, 64 and 96 nodes).
Each node is equipped with dual quad-core Intel Xeon CPUs @ 2.83 GHz, 16 GB of RAM, two 250 GB SATA disks, one of which is used for Hadoop storage.
Nodes are connected via Gigabit Ethernet.
For each cluster size, we allocated a Hadoop cluster (ver.
0.20.2) and copied the input data and a tarball of the indexed reference sequence onto the Hadoop file system.
The SEAL application was run on all the DS datasets in both alignment-only and alignment plus remove duplicate modes.
The runs were repeated three times, with the exception of DS8 which was run only once.
The runtimes for the different datasets are reported in Table 2, while the throughput is shown in Figure 1.
Looking at Figure 1, we see that SEAL is generally capable of throughput levels comparable to single-node operation, meaning that the application and Hadoop keep the distribution overhead to a minimum.
As the cluster size increases, we would ideally see a constant throughput per node, giving a linear increase in overall throughput.
In practice, when the input is too small with respect to the computational capacity, nodes are often underutilized.
Therefore, the throughput per node with DS1 at 96 nodes is much lower than the other configurations.
On the other hand, we see that SEAL is capable of utilizing available resources efficiently when more data are available, although while scaling up from 64 to 96 nodes, the system achieved better throughput on the small DS3 dataset as opposed to the larger DS8.
We suspect this is due to network congestion, which can be alleviated by informing Hadoop about the cluster network topology.
Fig.1.
Throughput per node of the entire SEAL workflow: finding paired reads in different files; computing the alignment; and removing duplicate reads.
An ideal system would produce a flat line, scaling perfectly as the cluster size grows.
The three datasets used are described in Table 1.
By comparison, a single-node workflow we wrote for testingperforming the same work as SEAL but using the standard multithreaded BWA and Picard reaches a throughput of 1100 pairs/s on the 5M dataset.
SEAL is able to achieve such scalability rates principally thanks to libbwas efficient use of memory.
In fact, libbwa stores the reference in shared memory, allowing all libbwa instances running on the same system to share the same memory space.
In practical terms, this feature makes it possible to run in parallel 8 alignments on a system with 8 cores and 16 GB of memory, fully operating in parallel.
While BWA does have a multithreaded mode of operation, it only applies to the bwa aln step.
On the contrary, SEAL is able to parallelize all steps in the alignment.
ACKNOWLEDGEMENTS We would like to thank our colleagues R. Berutti, M. Muggiri, C. Podda and F. Reinier for their feedback and technical support.
Conflict of Interest: none declared.
ABSTRACT Motivation: The fruit fly (Drosophila melanogaster) is a commonly used model organism in biology.
We are currently building a 3D digital atlas of the fruit fly larval nervous system (LNS) based on a large collection of fly larva GAL4 lines, each of which targets a subset of neurons.
To achieve such a goal, we need to automatically align a number of high-resolution confocal image stacks of these GAL4 lines.
One commonly employed strategy in image pattern registration is to first globally align images using an affine transform, followed by local non-linear warping.
Unfortunately, the spatially articulated and often twisted LNS makes it difficult to globally align the images directly using the affine method.
In a parallel project to build a 3D digital map of the adult fly ventral nerve cord (VNC), we are confronted with a similar problem.
Results: We proposed to standardize a larval image by best aligning its principal skeleton (PS), and thus used this method as an alternative of the usually considered affine alignment.
The PS of a shape was defined as a series of connected polylines that spans the entire shape as broadly as possible, but with the shortest overall length.
We developed an automatic PS detection algorithm to robustly detect the PS from an image.
Then for a pair of larval images, we designed an automatic image registration method to align their PSs and the entire images simultaneously.
Our experimental results on both simulated images and real datasets showed that our method does not only produce satisfactory results for real confocal larval images, but also perform robustly and consistently when there is a lot of noise in the data.
We also applied this method successfully to confocal images of some other patterns such as the adult fruit fly VNC and center brain, which have more complicated PS.
This demonstrates the flexibility and extensibility of our method.
Availability: The supplementary movies, full size figures, test data, software, and tutorial on the software can be downloaded freely from our website http://penglab.janelia.org/proj/principal_skeleton Contact: pengh@janelia.hhmi.org Supplementary information: Supplementary data are available at Bioinformatics online.
Received on December 31, 2009; revised on February 9, 2010; accepted on February 17, 2010 To whom correspondence should be addressed.
The authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
1 INTRODUCTION The fruit fly (Drosophila melanogaster) is one of most studied model organisms in biology.
It is currently widely used to understand how a real nervous system works.
A systems biology approach that we are taking to study the fruit fly brain is to produce 3D digital atlases of this animals nervous system at various developmental time points, including adult, larval and embryonic stages.
We have been taking advantage of an on-going effort to generate a collection of 8000 enhancer-trap GAL4 lines that would have their neuronal patterns cover the entire set of neurons in the fruit fly nervous system (Pfeiffer et al., 2008).
High-resolution 3D confocal images of these GAL4 lines were produced.
The scale of the problem is large, in terms of both the large size of each 3D confocal image (hundreds of megabytes to multi-gigabytes each) and the large number of the images in the database (two to five times the number of GAL4 lines, as multiple flies of the same GAL4 line need to be imaged).
Therefore, an informatics challenge is how to map the neuronal patterns onto a standard brain in an accurate, reliable and automatic way.
A brain mapping study needs several enabling techniques, including 3D image registration (alignment), neurite tracing, neuronal pattern classification, visualization and mining methods (Peng, 2008).
It is often critical to align image patterns that have various orientations, sizes, shape deformations and intensity changes.
A classic routine for 3D image pattern registration is to first align patterns globally, followed by a local alignment to transform one pattern to the other (Rueckert et al., 1999; Toga, 1999).
For a pair of image patterns, the purpose of the global alignment is to standardize the patterns as a whole so that they possess similar scales, positions and directions.
The affine transformation is commonly used for global alignment.
Once two patterns have been roughly aligned globally, it is much easier to use the more sophisticated local alignment (e.g.
Rueckert et al., 1999; Sorzano et al., 2005), which is often non-linear and deformable, to register the patterns precisely.
The affine alignment was already successfully used in recent efforts to build digitized atlases for model animals, such as the Allen mouse brain atlas (Lein et al., 2007), the adult fruit fly brain atlas (unpublished work of Peng lab) and the single-nucleus resolution atlas of the nematode Caenorhabditis elegans (Long et al., 2009).
However, for the alignment problem of the fruit fly larval nervous system (LNS) in this article, we found that the global affine alignment can hardly be used.
As shown in Figure 1a, in the LNS the two brain hemispheres and the ventral nerve cord (VNC) form an articulated structure, which is flexible and often skews toward one side.
This deformation cannot be described using global The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:14 25/3/2010 Bioinformatics-btq072.tex] Page: 1092 10911097 L.Qu and H.Peng (a) (b) (c) (d) Fig.1.
Original and standardized fruit fly larva (a and b) and adult VNC images (c and d) along with their PSs (dotted polylines).
rotation, translation, scaling and shear as used in the affine transform.
Figure 1c, which is an example of the adult VNC, displays a similar deformation of the shape.
Therefore, we need a more sophisticated global alignment method to standardize the shape of these patterns, before we can proceed further to the local alignment.
Intuitively, we considered aligning the backbone, or mid-line, of a curved fruit fly LNS or an adult VNC (red skeletons in Fig.1).
A related study was to straighten the strongly curved body of C.elegans (Peng et al., 2008) by first detecting the curved anterior posterior axis of a worm, and then restacking the resampled image data on the entire series of 1-pixel spacing cutting-planes orthogonal to such a detected backbone.
This earlier method is very efficient for C.elegans confocal images and other similar cases where the backbone is a simple smooth curve, and the body of the pattern is not fat (wide) enough so that the slightly non-parallel cutting planes next to each other will not intersect within the pattern.
Unfortunately, this method cannot be directly used in standardizing a fruit fly larval pattern because the articulated pattern cannot be described as a simple curve, and the fruit fly patterns are often fat/wide enough so that restacking the cutting planes may introduce artifacts especially in the articulated image regions.
There are several other skeleton extraction methods (Brandt et al., 1992; Chuang et al., 2004; Lam et al., 1992; Malandain et al., 1998).
However, they cannot easily produce skeletons of the same topological structure for different input images, and thus will not be very useful for the registration.
Therefore, in this work we proposed a new approach to standardize the confocal images of fruit fly LNS and adult VNC.
We detected the principal skeleton (PS) of a shape using an automatic skeleton deformation algorithm (Section 2).
Then we designed a smooth warping method to best align the PS of a pair of image patterns (Section 3).
Our method introduces a minimum amount of degradation of the image quality during the standardization.
It is also robust under a variety of conditions.
In addition, our method is general.
It is not limited to fly larval patterns; indeed it can be applied to any image patterns that have a reasonable PS1 (Section 4).
2 DETECTING PS Intuitively, the PS of a shape can be understood as its backbone, which describes the basic structure as well as the major deformation of this shape.
In this article, we define the PS as a set of connected polylines that spans the entire shape as broadly as possible and at the same time has the shortest overall length and sufficient smoothness.
We call each polyline a segment.
Each segment consists of multiple ordered control points.
Let us use the LNS (Fig.1a) as an example.
We view the nervous system as an articulated composition of three main parts: VNC, and left and right brain hemispheres.
Despite the complexity in the articulation region, the deformation within each part can be approximated using a gentler curve.
We define the PS of the LNS as a structure with six segments, including three branching segments, which represent the VNC, and left and right hemispheres, joined to another three segments of a triangle that models the hole where the non-neuronal tissues (gut and heart) pass through the articulation area.
Naturally, the PS can be viewed as a conceptual extension of the simple backbone curve of a C.elegans body (Peng et al., 2008), which is essentially the principal curve (Hastie, 1994) of a distribution of image pixels.
However, the PS is more than a simple collection of multiple principal curves due to the connection of segments.
In addition, instead of using a smooth curve to model each skeleton segment, without loss of generality, we model them by polylines.
The control points in a polyline can be further used to fit a smooth cubic-spline curve (Bartels et al., 1998), similar to the C.elegans case.
Similar to the case of LNS, the PS of a fruit fly adult VNC can be defined as an-shape (Fig.1c).
The same algorithm is used to find the PSs for both cases.
Throughout this article, we focus on LNS; experimental results on adult VNC and some other cases will be presented at the end of this article, and the corresponding movies can be found in the Supplementary Material.
2.1 Shape prior of larval PS The shape prior defines the initial PS including its topology and constraints of its segments and control points.
It should be as simple as possible but complicated enough to capture the major topology of the shape.
During the optimization, which is described below, the topology of a PS remains unchanged; however, the locations of all segment-control points will be iteratively updated.
The shape prior of LNS is defined in Figure 2.
It contains totally 11 control points c1, ...,c11, which are arranged as 6 segments B1, ...,B6 in Equation (1).
The control points c3, c4, and c7 join 1Some image patterns may not have a uniquely well-defined principal skeleton, such as a spherical cell that has the uniform intensity.
1092 [11:14 25/3/2010 Bioinformatics-btq072.tex] Page: 1093 10911097 Principal skeleton algorithm Fig.2.
The shape prior of the fruit fly larva PS, where 11 control points c1, ...,c11 form six connected segments and four domains.
The domains, indicated by dotted eclipses, are used for the length constraint.
these six segments.
B1 ={c1,c2,c3} B2 ={c4,c5,c6} B3 ={c7,c8,c9,c10,c11} B4 ={c3,c4} B5 ={c3,c7} B6 ={c4,c7} (1) We minimize the length of each segment.
Sometimes we may need to minimize the overall length of multiple segments, which indeed has a similar effect to maximizing the total smoothness of these segments when their length is short enough.
For example, for the LNS, we would like to produce a very smooth skeleton for the junction area of the two brain hemispheres, which contains three segments B1, B4 and B2.
This is similar to forcing B1, B4 and B2 to line up in an almost straight line; thus, their total length is as short as possible.
To attain this goal, in Equation (2) we define four optimization domains D1, ...,D4, each of which is a polyline and may cover multiple segments (e.g.
D1).
D1 =B1 B4 B2 ={c1,c2,c3,c4,c5,c6} D2 =B3 ={c7,c8,c9,c10,c11} D3 =B5 ={c3,c7} D4 =B6 ={c4,c7} (2) 2.2 Energy minimizing deformable PS model We design a deformable model to detect the PS, given its shape prior.
We formulate an energy function consisting of an external energy term and an internal energy term.
The external term is based on the image information, thus also called the image term, to force the PS spanning as broadly as possible to cover an entire LNS.
The internal term is based on the topology and constraints of skeleton domains; this term is minimized to guarantee the least total length and maximum smoothness of the skeleton.
Let p denote an image pixel, W the image area and the set of all control points of skeleton.
The Voronoi region (ci) of the control point ci is defined as the set of pixels in W that are closer to ci than to any other control points: (ci)= { pW :cj ,ci p cj p } (3) We define the external term Eexternal as: Eexternal = ci ci O ( (ci) )2, (4) where O() denotes the center of mass of a Voronoi region .
We define the internal energy term by aggregating all optimization domains defined in Equation (2).
Since there are only a few control points in each domain (e.g.
five in D2and six in D1), minimizing their length has a comparable effect to smoothing.
For simplicity, we only aggregate the length energy of each domain in the internal term.
Let U = {D1, D2, D3, D4} be the set of domain, we have: Einternal = 1 DkU w(Dk) DkU w(Dk)Elength(Dk), (5) where w(Dk) is a coefficient that defines the contribution of domain Dk .
For LNS, due to the vertical asymmetry of the shape prior, in order to avoid point c3 and c4 will be pulled downwards seriously by D3 and D4, we giveD1 more weight (w(D1) = 10, all other weights equal 1).
For a general case, the weights can be uniform.
Elength(Dk) in Equation (6) denotes the length of domain Dk : Elength(D)= |D|1 n=1 D[n]D[n+1]2, (6) where D[n] is the n-th element (control point) in domain D. The overall energy E takes the form: E =Eexternal +Einternal, (7) where and are two positive coefficients (both equal 0.5 in our experiments).
To minimize Equation (7), we solve the following equation for every ci, i = 1, ... ,11: E ci =Eexternal ci +Einternal ci =0.
(8) It is easy to derive an iterative optimization method from Equation (8) to estimate the new location of each non-tip control point, ct+1i , based on the Voronoi regions center of mass of its current position cti , and the positions of its connected neighbor control points: ct+1i = O((cti ))+ 1N(cti ) DkU,ctiDk w(Dk) cjP(Dk,cti ) ctj + 1N(cti ) DkU,ctiDk w(Dk) P(Dk,cti ) , (9) where N(ci)= DkU,ciDk w(Dk) and P(D,ci) denotes the set of neighboring control points of ci in domain D (in our case, since each domain is a polyline, this set includes both the left and right control points of ci).
Note that P(D,ci) is null if ci/ D. For the end points of the PS, i.e.
c1, c6 and c11, we use an empirically more robust formula based on two respective neighboring control points.
Let us use c1 as an example, the simplified formula is: ct+11 = O((ct1))+(2ct2 ct3) + (10) Other end points can be updated similarly.
Normally, the algorithm converges within 100 loops.
1093 [11:14 25/3/2010 Bioinformatics-btq072.tex] Page: 1094 10911097 L.Qu and H.Peng (a) (b) Fig.3.
Anchor points definition.
(a) The curve is the cubic spline smoothed PS; the surrounded gray masks illustrate where cutting lines may take place; the overlapped/highlighted mask region in the center of hemisphere indicates where the cutting lines of different domains may intersect.
(b) Defined anchor points overlaid on the PS.
We have implemented the PS detection method as a plugin of the V3D software (http://penglab.janelia.org/proj/v3d; Peng et al., 2010).
We provide a tutorial in the Supplementary Material on how to use the program.
3 ALIGNING PSS Since the PS of a fly LNS has a more complicated topology than the simple curved backbone of nematode C.elegans, we cannot simply reuse the cutting-plane restacking strategy in the earlier work (Peng et al., 2008) to standardize an LNS image.
Instead, we generate a smooth displacement field (DF) to warp the fly larvae from one to another, based on their PS information.
There are three general requirements of a DF.
(i) No singularity.
Thus, the DF should be smooth everywhere.
(ii) There should be a minimum amount of distortion to geometrically warp the PS, as well as the entire LNS.
(iii) The algorithm should be extensible to other PSs; at the same time, when a PS consists of only one segment, the standardization should approximate the cutting-plane restacking method.
We produce a DF using thin plate spline (TPS; Bookstein, 1989), which is defined as the least-bending smooth surface spanning a set of anchor points.
Thus, the requirements (i) and (ii) are met naturally.
For requirement (iii), we first consider all control points of the PS as TPS anchor points.
Then, we use cubic spline to find a smooth curve through each domain of the PS (as shown in Fig.3a), and then for every consecutive pair of control points, we add the halfway point on the smooth curve as a TPS anchor point.
We call the set of anchor points that consists of nicely spaced points on the PS as the PS-set.
Next, we define additional anchor points based on this PS-set: for each PS-set anchor point, we compute the orthogonal cutting line that intersects at this anchor point location with the respective smooth curve in the PS, and then we define two anchor points on each of the left and right sides of the cutting line (spacing = 75 pixels for LNS).
We call the set of anchor points that are not on the PS as the non-PS-set.
If a non-PS-set anchor point falls into the intersection region of multiple cutting lines of different domains (as highlighted in Fig.3a), we remove it from the set of the all anchor points, and thus avoid the non-smooth wrapping around of the DF.
While the choices of the number of anchor points of both PS-set and non-PS-set, as well as the spacing among anchoring points, are empirical, a general guideline is to make the anchoring points distribute evenly to cover the entire image pattern.
The selected parameters are then used for all images.
The entire algorithm for LNS standardization is as follows: (1) Define/initialize the shape prior of PS.
(2) Find the Voronoi region of each control point in an input image (called subject image below for simplicity).
(3) Update the positions of control points using Equations (9) and (10).
(4) Check whether or not the positions of control points have converged (i.e.
the maximal distance between the new and old positions of control points is <0.01).
If yes, go to Step 5.
Otherwise go back to Step 2.
(5) Use cubic spline to interpolate PS and produce a smooth skeleton according to the defined domains.
(6) Define both the PS-set and non-PS-set of TPS anchor points, remove some non-PS-set points if they fall into the cutting line overlapping/intersection region.
(7) Compute the TPS DF of this image using the corresponding anchor points between this image and a predefined target image.
(8) Warp the subject image to a standard shape using the DF.
(9) Set the warped (standardized) subject image as the input image and repeat Steps 18 until the PS of the image does not vary significantly (defined as the average displace of control points between two consequent iteration <3 pixels).
Our standardization method is general; it can be applied to both 2D and 3D image patterns.
For fruit fly LNS, although our image stacks are 3D, the major variation of the shape is in the 2D plane of two brain hemispheres and the VNC.
Therefore, we simplify the processing using 2D maximum intensity projection, and detect the PS in 2D.
Accordingly, the TPS DF is produced in 2D; all z-sections of a fly LNS image stack will share the same DF.
Of note, in our algorithm it is necessary to consider Step 9, i.e.
the iterative alignment of a PS.
Because TPS warping is non-linear and will cause some regions expand or shrink smoothly, the redetected PS of a warped image is slightly different from that of target image.
Several iterations of the optimization will produce nicely aligned image patterns whose PSs match well.
Normally, it takes about three loops to generate satisfactorily aligned PSs (Section 4.4).
4 EXPERIMENTS AND DISCUSSION We first evaluated the robustness and consistency of the PS detection algorithm, and then compared our method with the commonly used morphological thinning and affine-transform-based alignment method.
In the end, we show some PS detection results of other cases.
For the fruit fly larva data, N-cadherin-labeled LNS was imaged in 3D using a confocal laser scanning microscope (Zeiss LSM 510) in the laboratory of J. Truman.
The voxel resolution is 0.460.462.0m.
For the adult fly VNC data, NC-labeled VNC was imaged using a confocal microscope (Zeiss LSM Pascal 5) in J. Simpsons laboratory.
The voxel resolution is 0.580.580.8m in 3D.
1094 [11:14 25/3/2010 Bioinformatics-btq072.tex] Page: 1095 10911097 Principal skeleton algorithm (a) (b) Fig.4.
Robustness evaluation using (a) different simulated twisted LNS and (b) different larva orientations.
Green: initialization and red: detected PS.
4.1 Robustness We tested the robustness of detecting the PS when the LNS patterns in images have different orientations, scales, contrasts, articulated and twisted parts, as well as other noises.
Figure 4a shows robustness test results for the twisted LNS.
We simulated the deformation by rotating the right brain hemisphere from 45 to 45, with a 15 interval, to produce seven rows from top to bottom.
For every row, we rotated the VNC from 45 to 45 similarly to produce seven columns.
The detected PSs (red) from the same initialization (green) are correct, indicating that our method is robust for twisted patterns.
We further added noise to the test images and changed the scale of the initialization.
The results (Supplementary Material) were also robust.
Figure 4b shows a perturbation test based on real LNS images.
We randomly selected four LNS images that have differently twisted nervous systems.
Similarly to Figure 4a, we rotated them with different angles (45, 30, 15, 0, 15, 30 and 45).
From Table 1.
Consistency of PS detection under different initializations Images Score Maximal MSE (pixel) Average MSE (pixel) 1 0.035553 0.016931 2 0.047522 0.027183 3 0.003256 0.001953 4 0.027412 0.016448 The images 14 correspond to the middle column of Figure 4b.
exactly the same initialization (green), our algorithm successfully detected the PSs (red), except two errors in the left-bottom corner and in the last column of the third row, which correspond to 45 and +45 rotations, respectively.
The errors occurred when the angle between the longest axis (along the brain-VNC direction) of an LNS and the initialization of the PS is bigger than 45, the VNC part of the larva would more likely be interpreted as B1 or B2 rather than B3 according to the shape prior we assumed in Section 2.
In other words, the PS was initialized too poorly.
However, practically speaking, this situation is rare in the real data.
We can also avoid it completely by preprocessing an image so that its longest axis is roughly aligned with the B3 segment.
We also changed the contrast of the test images and found that our method robustly produced meaningful results (Supplementary Material).
4.2 Consistency We quantified the consistency of PS detection given different initializations.
For four LNS images, we rotated the respective initial shape prior of the PS in the range of 30 (interval = 15).
For any pair of the five PSs detected for each image, we computed the mean square error (MSE) of the corresponding control points of this pair of skeletons.
Table 1 shows the maximal and average MSE scores for all possible pairs.
The MSE scores are much <1 pixel, indicating that PSs were detected very consistently from different initializations.
4.3 Comparison with morphology thinning Morphology thinning (Lam et al., 1992) is a commonly used algorithm to extract the backboneof an image object that has closed contour.
In order to use thinning, one needs to first segment the object from the image background.
Unfortunately, the thinning algorithm is very sensitive to the segmentation.
As shown in Figure 5a, direct thinning on a binary image generated too many artificial branches due to spurs in the contour of the object and unstained areas (holes).
Here, we simply took the foreground as the set of the pixels whose intensity was above the mean value of all pixels in the image.
After we carefully smoothed the contour and filled the holes using additional morphological opening and closing operations (with morphological element set to be disk shape, with 10 and 20 pixels in diameter, respectively), we achieved better thinning results in Figure 5b and c, which however were still not as good as the detected PS in Figure 5d.
4.4 Standardization of real LNS images As a real application, we used the PS method to standardize a randomly selected set of 237 3D confocal images of the third instar 1095 [11:14 25/3/2010 Bioinformatics-btq072.tex] Page: 1096 10911097 L.Qu and H.Peng (a) (b) (c) (d) Fig.5.
Skeletons detected using image thinning (ac) under different conditions and using the PS method (d).
(a) (b) (c) (d) (e) (f) (g) (h) (i) Fig.6.
Comparison of LNS standardization using the PS method and the affine-based alignment.
Overlaid patterns (ac), as well as the respective PSs (df): before standardization (a and d), after a global alignment using affine transformation (b and e), and after our PS-based warping method (c and f).
The skeletons are periodically colored with eight different colors, and the images are periodically colored with three colors (red, green and blue).
(gi) The density map of overlaid PSs after 1, 2 and 3 iterations of optimization.
The radius of skeleton indicates the standard deviation of PSs of images after 1, 2 and 3 iterations of standardization.
larval stage LNS.
The comparison results of overlaid patterns are shown in Figure 6ac, as well as the respective PSs are shown in Figure 6df, before and after standardization.
Obviously, the affine transform-based standardization (Fig.6b and e) did not align LNS patterns well, especially the big deviation in the VNC of an LNS.
This was due to that the articulated LNS could not be well (a) (b) (a) (b) Fig.7.
Comparison of the image content before (a and b) and after (c and d) standardization.
(b and d) are the zoom-in view of the red boxes in (a and c).
Red image channel: GFP-tagged GAL4 patterns.
Green: neurotactin and blue channel: N-cadherin.
approximated by an affine transformation.
On the contrary, the PS-based standardization scheme (Fig.6c and f) successfully registered all images, with the PSs almost exactly overlapping on top of each other.
We also quantified the spatial variations of the aligned PSs.
As explained at the end of Section 3, the iterative optimization of the PS standardization, i.e.
Step 9, is important.
Figure 6gi show that with 1, 2 and 3 iterations, the standard deviation of the entire set of PSs became smaller and smaller.
Indeed, the average pair-wise distance of the entire population of PSs dropped from 50.225 pixels before standardization to 5.661, 3.220 and 2.323 pixels in three iterations.
For a better visualization, see the Supplementary Material for large frame-size movies of this experiment.
4.5 Quality of standardized images We also visually inspected the image quality of an LNS pattern before and after our standardization process.
Figure 7 shows that all the local features of an image before standardization (Fig.7a and b) were well preserved in the post-standardization image (Fig.7c and d).
No visible artifact was seen.
The processed image was a little bit smoother, due to the interpolation process during the TPS warping.
The overall loss of information was negligible.
1096 [11:14 25/3/2010 Bioinformatics-btq072.tex] Page: 1097 10911097 Principal skeleton algorithm (a) (b) (c) (d) Fig.8.
(a) Shape prior of the adult VNC PS.
(b) Results of PS detection (upper row) and thinning (lower row) on five adult VNC images (background image of the upper row).
Green: initialization of the PS and red: detected PS.
(c) PS detection of an adult fruit fly center brain.
(d) PS detection result of a C.elegans confocal image.
4.6 Results of adult fruit fly VNC and others patterns By modifying the shape prior of the PS, our algorithm can be easily adapted to other shape analysis applications.
For example, in Figure 1c and d, we show that the PS method can be used to standardize an adult fruit fly VNC.
Indeed the adult VNC has a more complicated shape prior, which has nine segments and seven optimization domains (Fig.8a), than an LNS.
Similar to the LNS case, the key to standardizing an adult VNC is to detect the PS robustly.
Figure 8b shows the results on five arbitrarily selected adult VNC patterns (data from J. Simpson laboratory; NC82 neuropile staining was used in place of N-cadherin).
It is evident that our algorithm (upper row of Fig.8b) detected meaningful PSs, while the image thinning (lower row, Fig.8b) produced results that were not very useful.
Figure 8c and d shows additional PS detection results of adult fruit fly center brain and C.elegans.
It can be noted that even when the PS was poorly initialized to even outside the object in the C.elegans case (Fig.8d), our method still detected reasonable PS.
Additional PS deformation movies can be found in the Supplementary Material.
One potential problem of our PS approach is that, when a skeleton becomes more complicated, the deformation to the optimum may be more likely trapped into local minima.
Generally speaking, this requires more careful initialization.
One solution is to introduce hierarchical domains in optimization.
Taking the adult VNC as an example, we can make our algorithm more robust by first optimizing the skeleton of the main trunk (i.e.
the horizontal domain in Fig.8a) so that we can get a good estimation of the scale, position and the orientation of the VNC, and then optimizing the remaining domains/segments to complete the entire skeleton.
ACKNOWLEDGEMENTS We thank James Truman for providing the larva image dataset and comments on the manuscript.
We thank Julie Simpson for the adult VNC test data.
We also thank Zongcai Ruan, Wayne Perenau, Fuhui Long, Gene Myers and other people in Gene Myers lab and Hanchuan Peng lab for discussion.
We thank Margaret Jefferies for help of proofreading of the manuscript.
Funding: Howard Hughes Medical Institute.
Conflict of Interest: none declared.
ABSTRACT Motivation: A major challenge in systems biology is to reveal the cellular pathways that give rise to specific phenotypes and behaviours.
Current techniques often rely on a network representation of molecular interactions, where each node represents a protein or a gene and each interaction is assigned a single static score.
However, the use of single interaction scores fails to capture the tendency of proteins to favour different partners under distinct cellular conditions.
Results: Here, we propose a novel context-sensitive network model, in which genes and protein nodes are assigned multiple contexts based on their gene ontology annotations, and their interactions are associated with multiple context-sensitive scores.
Using this model, we developed a new approach and a corresponding tool, ContextNet, based on a dynamic programming algorithm for identifying signalling paths linking proteins to their downstream target genes.
ContextNet finds high-ranking context-sensitive paths in the interactome, thereby revealing the intermediate proteins in the path and their path-specific contexts.
We validated the model using 18 348 manually curated cel-lular paths derived from the SPIKE database.
We next applied our framework to elucidate the responses of human primary lung cells to influenza infection.
Top-ranking paths were much more likely to contain infection-related proteins, and this likelihood was highly cor-related with path score.
Moreover, the contexts assigned by the algo-rithm pointed to putative, as well as previously known responses to viral infection.
Thus, context sensitivity is an important extension to current network biology models and can be efficiently used to eluci-date cellular response mechanisms.
Availability: ContextNet is publicly available at http://netbio.bgu.ac.
il/ContextNet.
Contact: estiyl@bgu.ac.il or michaluz@cs.bgu.ac.il Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Complex diseases and viral infections are among the major prob-lems in human health today.
In an effort to broaden our under-standing of the molecular basis of these diseases, they are increasingly interrogated using a variety of large-scale experi-mental techniques.
Major techniques include sequencing efforts to reveal disease-related mutations, mRNA profiling to reveal genes that are differentially expressed during disease and siRNA screens to reveal disease-related proteins (e.g.
Shapira et al., 2009), thereby revealing distinct subsets of the genes and proteins involved.
Recent studies demonstrate the strength of integrative approaches in broadening our understanding of disease processes (reviewed in Ideker and Sharan, 2008; Schadt, 2009).
Central to many integrative approaches is the molecular inter-action network (interactome) paradigm, where interactome nodes represent proteins or genes, and interactome edges repre-sent their physical and regulatory interactions.
Interactomes pro-vide a convenient framework for exploring the context within which disease genes operate, and they were successfully used to illuminate new disease genes (e.g.
Guan et al., 2012; Magger et al., 2012), and their functions, as recently reviewed by Barabasi et al.
(2011).
Because of the importance of signalling paths in health and disease, several computational efforts exploited the interactome framework for their elucidation.
By connecting mutated proteins with their downstream differentially expressed targets, Yeang et al.
(2004) identified intermediate proteins in the paths and assigned directionality to undirected proteinprotein interactions (PPIs).
Later studies identified interactome sub-networks relating the results of high-throughput genetic screening and mRNA pro-filing (Suthram et al., 2008; Tuncbag et al., 2012; Yeger-Lotem et al., 2009; Yosef et al., 2009).
And yet another set of studies computed putative signalling paths by connecting membrane proteins to transcription factors (Bebek and Yang, 2007; Steffen et al., 2002; Tuncbag et al., 2012), while limiting the types and relative order of the proteins on the path (Scott et al., 2006; Steffen et al., 2002; Tuncbag et al., 2012).
Although based on different computational techniques, each of these studies relied on a typical network representation, where each edge is assigned a single score based on its estimated reli-ability (e.g.
Szklarczyk et al., 2011) or relevance to a specific cellular process (e.g.
Cakmak and Ozsoyoglu, 2007; Myers et al., 2005; Yeger-Lotem et al., 2009).
Yet, single edge scores fail to capture the complexity of biolo-gical systems, where the activation of a specific protein may lead to multiple responses, depending on the current cellular condi-tion.
We illustrate this phenomenon using the human protein GRB2, an epidermal growth factor receptor-binding protein that is known to mediate several cellular signalling cascades (Fig.1).
Although GRB2 physically interacts with many different proteins, recent experimental analysis of its physical interactions has shown that its sub-network remodels itself dramatically in response to different stimuli (Bisson et al., 2011).
For example, in the context of viral infection GRB2 tends to interact with inter-feron regulatory factor IRF5, whereas in the context of insulin signalling, it tends to interact with the insulin receptor substrate IRS1.
Thus, examining GRB2 physical interactions regardless of cellular context will mask this important distinction.
*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com Here, we describe a novel computational framework that aims to capture the context dependence of molecular interactions.
There are several methods that search for paths or transitions between functional contexts in interactomes (e.g.
Banks et al., 2008; Pandey et al., 2007).
In contrast, the novelty of our ap-proach is not in the consideration of context but rather in the proposed computational model that allows for the consideration ofmultiple candidate context pairs for eachmolecular interaction.
We provide a context-sensitive algorithm that scores context-spe-cific paths leading from a source protein to a differentially ex-pressed gene.
The algorithm selects a single context pair per interaction, based on the context selected for the preceding inter-action in the path.
Paths are then scored according to the selected context pairs, and high-scoring paths are reported.
Context dependence has often been associated with tissues or cell types (Schaefer, 2012), and with different biological states, such as cell cycle stages (de Lichtenberg et al., 2005) or response to stimuli (Barrios-Rodiles et al., 2005).
In this study, we dem-onstrate our context-sensitive framework by using gene ontology (GO) terms as the biological context.
We first show the validity of the proposed model by using a set of manually curated human pathways (Paz et al., 2011).
We then demonstrate the ability of the framework to identify relevant interaction paths by analysing the response of human primary lung cells to influenza infection (Shapira et al., 2009).
Notably, a significant fraction of the pro-teins that the algorithm predicted were indeed found to affect the infection process, and the success rate of the prediction was sig-nificantly correlated with the path score.
We implemented our proposed framework as a tool, ContextNet, which is publicly available at http://netbio.bgu.ac.il/ContextNet.
2 RESULTS Our results include the proposed context-sensitive framework, its implementation as a tool, a statistical assertion that known cel-lular pathways in human are indeed context-sensitive and finally the application of our framework to identify and interpret the proteins and pathways underlying the response of human cells to viral infection.
2.1 A context-sensitive framework for identifying signalling paths Our proposed framework consists of an interactome model, a context-based scoring scheme and a path interpretation and scor-ing algorithm.
2.1.1 Interactome model Our model of the human interactome consists of distinct nodes that represent either human proteins or genes, and edges that represent their experimentally detected PPIs and proteinDNA interactions that were downloaded from several databases (see Section 5), resulting in 176 849 inter-actions among 14362 proteins and genes.
We then added context to the network by using GO annotations (Ashburner et al., 2000) as follows.
Each GO annotation was considered as a distinct label and was associated with the corresponding gene and pro-tein nodes.
Each interaction was associated with the Cartesian product of the labels of the interacting nodes.
2.1.2 Context-based scoring matrix We constructed a context-transition scoring matrix, M, which assigns to each pair of labels a score that reflects the likelihood of the pair in known pathways.
To compute the context-transition scores, we used the SPIKE database of manually curated human pathways (Paz et al., 2011).
Specifically, we extracted from SPIKE all signalling paths that connect a protein to a gene with the last edge in the path being a transcription regulation edge.
We then directed these 18 438 simple paths from the protein to the gene and combined them into a set of 6762 unique directed interactions.
From these inter-actions, we calculated the conditional probability Plj j li that reflects the likelihood of observing a directed interaction pointing from a node with label li to a node with label lj.
We further fine-tuned this context-transition scoring scheme as described in Section 5.
2.1.3 Algorithm for context-sensitive path interpretation and scoring Our framework identifies top-scoring context-sensitive paths connecting a protein to a differentially expressed gene.
An overall illustration of the framework is shown in Figure 2.
Given a source protein and a target gene, the algorithm first identifies in the network all simple paths of lengths from 2 to k that connect the source to the target, where k is a predefined user parameter.
Next, it uses the context-transition scoring matrix M (described earlier in the text) to rank each path, favouring the strongest contextual interpretation.
Finally, the algorithm returns the top scoring paths, along with the chosen label for each node in each path.
We now turn to describe the labelling of a specific candidate path, P (Fig.3).
As each node in P is associated with several labels, the optimization problem at hand is that of selecting an ordered set of labels, one label per each node in P, such that the sum of context-transition scores for consecutive labels in this ordered set is maximized.
For this purpose, our method con-structs for P a directed acyclic graph, denoted the context-label network, as follows.
Each potential functional label of a node at index j of P, appears as a contextual-label vertex in the j-th column of the corresponding context-label network.
A dir-ected edge is added between labels x and y in consecutive col-umns of the context-label network, and its weight is set to the context-transition score of the two labels, Mx, y).
Additional skip edges with constant gap scores are added to the context-label network, to increase interpretation flexibility by supporting poor or lacking context annotations of some nodes.
Each puta-tive contextual interpretation of P then corresponds to a directed path through its context-label network, where the path begins in one of the vertices of the first column, ends in one of the vertices Growth factor signalling Ras/Rho cascade DNA replication and repair Angiogensis Insuling signalling Immune response to virus Wnt pathway Apoptosis and cellular prolifiration Kinase cascade Emryonic development and organ development Fig.1.
The human protein GRB2 interacts through high-confidence PPIs with proteins from 10 distinct cellular processes i211 A context-sensitive framework of the last column and traverses through at most one vertex from each column of the context-label network.
Our method computes a heaviest context-labelling path for P via a dynamic program-ming algorithm, as described in Section 5.
2.1.4 ContextNet publicly available tool We implemented our framework as an interactive internet tool and made it publicly available in http://netbio.bgu.ac.il/ContextNet.
Given an input consisting of source proteins and target genes, our tool enumer-ates simple paths in the human interactome connecting the two sets, computes their interpretations and ranks them by their con-text-labelling scores.
The output reports the top-scoring paths and their contextual interpretation.
3 APPLICATIONS OF THE FRAMEWORK TO THE INTERPRETATION OF HUMAN SIGNALLING AND VIRAL INFECTION PATHWAYS 3.1 Known cellular signalling pathways are context sensitive Our first step was to validate whether known cellular paths are context sensitive.
For this purpose, we exploited the SPIKE data-base of manually curated pathway maps, where each map de-scribes a specific cellular pathway composed of tens of proteins (Paz et al., 2011).
We applied our framework to 21 of these maps in a leave-two-out statistical significance test.
Specifically, we computed a context-transition scoring matrix M based on the paths included in 19 maps, and then used the matrix M and the label-selection algorithm to calculate the best score for each sig-nalling path in the two left-out maps.
To estimate the statistical significance of the context-labelling score of each path, we re-peatedly randomized the set of labels associated with each node and recalculated the path score (see Section 5).
If the original path was not context sensitive, one would expect that its original score would be similar to its scores based on randomized labels.
However, if the original path was indeed context sensitive, then its original score would be significantly higher than scores based on randomized labels (P 0:05).
We found that for paths of length two edges, 43% of the 195 paths scored significantly better than random.
Furthermore, 470% of the 650 paths of length 3 and 485% of the thousands of paths of length 45 scored significantly better than random (Fig.4).
Based on this, we conclude that most manually curated signalling paths of length 35 are indeed context sensitive.
3.2 The context-sensitive framework successfully identifies proteins associated with influenza infection of human cells During infection, influenza proteins were shown to interact with human proteins to recruit the cellular mechanism for viral pro-liferation.
However, the pathways and the intermediate proteins involved in the infection process are just beginning to emerge.
In an effort to identify these critical pathways, several large-scale analyses were recently performed.
In particular, (Shapira et al., 2009) reported a large-scale analysis of influenza infection of human primary lung epithelial cells, in which they identified PPIs between 10 influenza proteins and 87 human proteins and performed extensive mRNA profiling of the infected human cells.
Based on these data, Shapira et al.
(2009) predicted the involvement of 1756 human genes in the infection process, which they tested by RNA silencing.
They found that 616 of the 1756 tested genes were indeed siRNA positive, namely, had a significant effect on viral propagation and interferon produc-tion when silenced.
Here, we took advantage of this wealth of information to assess our context-sensitive framework and to identify potential signalling paths through which viral proteins may modify the cellular transcriptional program.
To this end, we calculated the set of all simple paths linking the human interac-tors of the viral proteins to each of the human genes exhibiting differential expression after infection (see Section 5).
We then scored each path using our context-sensitive algorithm, focusing on paths of length three to five.
Figure 5A demonstrates the biological relevance of the top 5% scoring paths compared with a background set consisting of all paths of same length.
The biological relevance was measured by the percentage of paths that contained at least one connecting intermediate protein (not source or target) that was found to be siRNA positive in the experiment described earlier in the text.
As S u T S vu T Skip vA B C Fig.3.
(A) A sample path P connecting a source node S to a target node T. (B) Below each node is the set of its labels, where shades of colours are used to denote label similarity according to the context-tran-sition scoring matrix.
Edges connect labels corresponding to consecutive nodes in P, and a dashed edge demonstrates a legitimate context gap (Skip step).
(C) The best-scoring label assignment for P Fig.2.
A high-level overview of our framework for computing context-sensitive molecular interaction paths i212 A.Lan et al.
shown, in all paths of lengths three to five, the top-scoring paths were more likely to include a biologically relevant protein.
The advantage of top-scoring context-sensitive paths was also observed when compared against shortest paths of similar lengths (Supplementary Fig.S1).
We then extended this analysis to test whether the context-labelling score was also correlated with the biological relevance.
Indeed, we found that paths of higher scores were also more likely to contain a connecting bio-logically relevant protein.
Figure 5B shows the correlation for paths of length four (Pearson r 0.98, P51012), and similar statistically significant correlations were obtained for paths of length three and five (Supplementary Fig.S2).
Figure 5C exem-plifies a top-ranking path connecting the viral interacting protein TRAF2 to the differentially expressed gene IRF7.
This path ranked best of the 998 connecting paths for this source and this target.
Notably, two of the three intermediate proteins in this path were found to be siRNA positive, and the contexts that our framework selected for them were indeed related to viral infection as shown in Figure 5C.
These results demonstrate again that our context-sensitive framework helps identify bio-logically relevant proteins and contexts.
3.3 Highlighting and interpreting the multi-faceted functionality of the viral protein PB2 The PB2 protein is a subunit of the influenza virus RNA poly-merase, and it is known to be a major virulence determinant of influenza viruses.
It was recently demonstrated that PB2 regu-lates interferon expression during infection (Graef et al., 2010; Shapira et al., 2009).
However, the molecular mechanisms by which it acts are just beginning to emerge (Graef et al., 2010).
To illuminate these mechanisms, we applied our framework to identify and interpret PB2 downstream interactions.
Therefore, we ranked, by context-sensitive scores, the millions of paths con-necting the 28 human proteins that were found to interact with PB2 to the 527 target genes that were found to be differentially expressed during influenza infection (see Section 5).
We com-bined the 1% top-ranking paths into the network shown in Figure 6.
Importantly, the network we obtained clearly highlights the role of PB2 in interferon regulation.
The four human proteins that interact with PB2 through top-ranking paths were all found as likely upstream regulators of interferon expression (Fig.6, red sub-network).
Moreover, for five of the eight differentially expressed genes in the PB2-induced sub-net-work, our framework selected interferon-related context labels.
Notably, the assignment of interferon labels to these genes is stat-istically significant, as only 39 of the 527 target genes are asso-ciated with interferon (Fisher exact test P 8:2105).
Thus, our framework correctly uncovered the key downstream effect of PB2 and suggested the cellular pathways by which it acts.
4 DISCUSSION A well-known limitation of current computational models of PPI networks is the weak handling of interaction context.
PPI net-work models are typically constructed by combining interactions from various measurements, regardless of the biological context in which they were measured, such as specific stimuli, tissues, cellular components and disease states.
Previous context-sensitive approaches to network interpretation limited molecular inter-action pathways to a single context, such as a single tissue or cell type (Schaefer, 2012), or followed a predefined context-tran-sition template, e.g.
defining a flow from membrane to nucleus BA C TRAF2 TBK1 IRF3 IRF7 I kappaB kinase/NF kappaB cascade IKBKG type I interferon producon negave regulaon of type I interferon producon posive regulaon of interferon alpha producon posive regulaon of T cell cytokine producon 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 3 4 5P er ce nt ag e of p at hs c on ta in in g an  siR N A po si ve in te rm ed ia te  Path length 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Pe rc en ta ge o f p at hs c on ta in in g an  siR N A po si ve in te rm ed ia te  Score percenle Fig.5.
(A) The biological relevance (y-axis) of the top 5% scoring paths (filled bars) comparedwith a background set consisting of all paths of same length (striped bars), for lengths 35 edges (x-axis).
Top-scoring paths were more likely to include a biologically relevant protein.
(B) A graph showing the correlation between context labelling score of a path (x-axis, in score-ranking percentiles) and its likelihood to contain a biologically relevant intermediate protein apart from the source and target (y-axis), for paths of length four.
The x-axis shows the percentile of top-scoring paths of the 49 002 scored paths, which constitute 1.24% of the simple paths of length four between the sources and targets.
A dotted line marks the background distribution over all simple paths of length four.
Top-scoring paths were more likely to contain a biologically relevant intermediate protein (Pearson r 0:98, P-value 51012).
Similar statistically significant correlations were obtained for paths of lengths three and five (Supplementary Fig.S1).
(C) A top-ranking path connecting the viral interacting protein TRAF2 to the differentially expressed gene IRF7.
This path ranked best of the 998 connecting paths for this source and this target.
Two of the three intermediate proteins in this path were found to be siRNA positive (red border).
Proteins in the path have closely related labels relevant for infec-tion, as shown below each protein 195 650 3252 14341 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 2 3 4 5 shtap evitisnestxetnoc fo egatnecrep egarevA Path length Fig.4.
Results of the SPIKE randomization test for paths of lengths two to five edges are shown.
The bars indicate the percentage of paths, per length, that scored significantly higher than random i213 A context-sensitive framework (Scott et al., 2006).
Here, we presented a novel framework that assigns biological context to molecular interaction pathways while allowing for context variations and switches that may not be obvious to the outside observer before our analysis.
Our framework also computes a context-sensitivity score for the context-annotated pathway, which can be used to prioritize pathways.
To capture the dynamic context of molecular interaction net-works, we added another dimension to the PPI network model, which takes into account, for each node, a set of labels corres-ponding to its potential functions.
We also added edges to con-nect these labels, and set the weights of these edges to reflect the probability of transitions between the connected labels.
The weights of these transitions were inferred from known human pathways.
Our model is also flexible enough to provide some support for poorly annotated proteins by leaning on the labels assigned to their upstream and downstream neighbours.
This is achieved via special skip edges, thus enabling the inclusion and interpretation of the 16% of the interacting proteins that are not yet annotated.
The proposed framework can be generalized to handle various contexts.
In this study, we used GO terms as context labels and known human pathways as guides for constructing the label-transitionscoring matrix.
Our analysis of SPIKE maps revealed that proteins in known pathways were mostly associated with GO biological process terms (76%) and not with molecular func-tion (14%) or cellular component GO terms (10%).
Therefore, we focused our analysis on GO biological process terms, which we further filtered to remove high-entropy, lowinformation-content terms.
The remaining terms were associated with a rela-tively small number of genes (median of 19 genes per term, including genes associated with descendent terms), indicating the high specificity of these terms.
We validated our framework using thousands of known human pathways.
We found that470% of the known paths of lengths three to five edges were indeed context sensitive (Fig.4).
We proved the use of our framework by applying it to reveal the molecular interaction paths involved in viral infection.
Using our method, we successfully identified biologically relevant context-sensitive paths connecting viral proteins with the downstream human transcriptional response.
We implemented our frame-work as an interactive internet tool and made it publicly avail-able in http://netbio.bgu.ac.il/ContextNet.
We have shown the value of introducing GO context to the analysis of signalling pathways.
Future work could include op-timization variants of this problem that will be applicable to longer paths and to a wider scope of context.
For example, the current algorithm selects the context of an interaction based on the context selected for the preceding interaction in the path.
Important extensions would be to reflect longer contextual his-tories that go beyond the first-order neighbourhoods and to extend the sought context units from linear paths to networks.
Another extension would be to integrate additional context schemes, such as pathway context enrichment (Pandey et al., 2007), network schemas (Banks et al., 2008), semantic similarity (Pesquita et al., 2009), tissue associations (Barshir et al., 2013) or protein localization (Scott et al., 2006), and to enhance them with interaction-confidence scores.
The new framework we presented and its extensions may be applied to a variety of network-related problems where context-sensitive relationships are meaningful.
5 METHODS 5.1 Human interactome model Experimentally detected human undirected PPIs were assembled from four major PPI databases, including BIOGRID (Stark et al., 2011), DIP (Salwinski et al., 2004), IntAct (Aranda et al., 2010) and MINT (Ceol et al., 2010).
Directed transcription regulation interactions between transcription factors and their target genes were downloaded from the TRANSFAC database (Matys et al., 2006).
We also included manually curated directed and undirected interactions from the SPIKE database (Paz et al., 2011).
GO terms and annotated human proteins and genes were downloaded from the GO database (Ashburner et al., 2000), February 2012 release.
To increase the specificity and reliability of the GO data used in this study, we filtered out GO terms assigned to 4600 genes and GO annotations with evidence codes IPI, NAS and ND.
Each GO term was denoted by a dis-tinct label .
5.2 Learning the label-transition scoring matrix M To study the frequency of label transitions in signalling paths, we exploited the manually curated pathway maps from the SPIKE database (Paz et al., 2011).
From each map we extracted the set of all simple paths linking a protein to a gene with the last edge in the path being a transcription regulation edge.
We then directed each path from the upstream protein to its downstream target gene, and combined all paths per map into one set of unique directed edges.
For each node v, we denote as Lv the set of all labels of v (GO annotations in this study).
For each edge u, v in some path, and for each x 2 Lu and y 2 Lv, we computed the label pairwise frequency by counting how many times y appears after x in some edge in the set of all edges.
This yielded the set F, Fig.6.
The PB2 network was computed by merging the top 1% scoring paths connecting PB2 interactors and genes that were differentially ex-pressed on infection.
Full lines represent direct physical interactions; dashed lines represent indirect interactions, computed by omitting the intermediate nodes along the path.
Triangle nodes represent transcription factors, square nodes represent differentially expressed genes and circular nodes represent direct viral interactors.
The labels chosen by our frame-work are depicted next to nodes.
The interferon-labelled nodes are shown in red, and nodes and edges leading from PB2 to these nodes have a red border i214 A.Lan et al.
consisting of pairs of consecutive labels, annotated by their frequencies.
We observed that some labels in P are non-specific, for ex-ample, Transcription Factor activity, which always appears as the target node in every edge pointing to a transcription factor.
Therefore, we applied an entropy measurement criterion to filter out non-specific labels from F (i.e.
labels with high entropy).
For this, we calculated for every ordered pair, x, y 2 F, the forward conditional probability Py j x (Occurrences of y after x)/(Total occurrences of x), as well as the backward conditional probability Px j y (Occurrences of x prior to y)/(Total occur-rences of y).
It is important to note that Px j y and Py j x are not necessarily the same.
Therefore, for each label x, we com-puted its forward individual entropy hY j x across all occur-rences of consecutive label pairs x, y 2 F, as hY j x P y2YPy j xlogPy jx.
Note that, in the afore-mentioned formulation, x is a specific label (thus denoted by a lowercase letter) and Y is a random variable (thus denoted by an uppercase letter).
All pairs x, y 2 F, such that the forward in-dividual entropy of label x was found to exceed a threshold (0.7), were filtered out from F. Similarly, we calculated the backward individual entropy hX j y P x2XPx j tlogPx j t for each label y and filtered out all pairs x, y 2 F such that the backward individual entropy of label y was found to exceed a threshold (0.7).
In-between filtration steps, both individual and pairwise label frequencies were re-calculated based on the remaining labels in F. Finally, we set Mx, y log101 Py j x, for all x, y 2 F. We used 1428 terms, covering 7001 genes, which constitute 48.7% of the interactome.
These terms were then filtered based on their entropy, leaving 1203 terms covering 5919 genes, which are 41.2% of the interactome.
To examine the level (within the GO hierarchy) of terms that is useful for context definition, we computed the size of each GO term appearing inM.
The size of a GO term was defined as the number of genes associated with it, including the genes associated with its descendant GO terms.
We found that the GO terms in the entropy-filtered M had a median size of 19 genes.
This indicates that GO terms that have a small size, and, therefore, are low level, are more informative than high-level, non-specific terms that are associated with many genes.
The GO terms participating in M, their entropy and the number of genes they cover, are provided in the ContextNet website at: http://netbio.bgu.ac.il/ContextNet/SuppTable1.xlsx.
5.3 The dynamic programming algorithm for path interpretation To compute the strongest contextual interpretation of paths from the PPI graph, our algorithm uses the pre-computed con-text-transition scoring matrix M computed as described in the previous section.
Given, as input, a path P and the context-tran-sition scoring matrix M, the algorithm computes an output con-sisting of the score for the strongest contextual interpretation of P, as well as the corresponding annotation of the nodes of P, in form of a sequence of pairs 5S, s, v2, 2::vk, k, T, t4, where S denotes the source node, T denotes the target node and i denotes the label assigned to node vi in P, selected from among all possible context labels suggested for node vi.
To this end, a context-label network is constructed for P, as described in the Section 2, in the form of a directed acyclic grid graph G0 (see Fig.3 for an illustration), where the j-th column of vertices in G0 (presented in the figure under the corresponding j-th node of P) represents all the potential context labels for that node.
We define two edge types within the possible context transi-tions.
A Switch edge represents a transition from label x to label y in an adjacent column in G0; it is added to the graph if the context-transition score of the ordered pair of labels it connects is above a given threshold, and its weight is set to Mx, y.
A Skip edge (shown in dashed lines in the figure), skips over an adjacent column in the grid to a label in the next one.
A Skip edge con-nects two similar labels: x in column i of G0 and y in column i 2 of G0, if column i 1 does not contain any label z such that Mx, z4Threshold.
The weight of the Skip edge is set to Mx, y SkipPenalty.
When reconstructing an optimal solution interpreting P (i.e.
the optimal context-label assignments to the nodes of P), the skipped node is assigned the same label as the one chosen for the source node of the skip edge (for example, in Figure 3C, node v is green even though it does not have a green label in its column).
The Skip edge allows us to deal with poorly annotated genes and to suggest an overall context-acceptable path interpretation.
We can now reduce the problem of label assignment to that of finding the heaviest path in an edge-weighteddirected acyclic graph, with a predefined constraint dmax on the maximum number of Skip edges allowed.
Dynamic programming is then applied to solve the reduced problem, implementing the recur-sion later in the text (Fig.7), where x and y denote label nodes in G0, Predecessorsy denotes the set of vertices that have edges leading to y in G0, and Labels(v) denotes the set of labels of v. The final context-labelling score is reported as maxy2LabelsTSy, dmax.
The dynamic programming algorithm traverses all the nodes in the context-label network (G0) in increasing column order and applies the recursion given in Figure 7 to compute the score for each label node.
Therefore, the time and space requirements of the dynamic programming algorithm implementing this recur-sion is OE V, where E and V denote the number of edges and vertices in G0, respectively.
5.4 Evaluating the context sensitivity of known cellular paths in SPIKE We conducted 10 trials as follows.
In each trial, we computed a context-transition scoring matrix as described earlier in the text by using 19 of the 21 SPIKE maps.
We then used the matrix to Fig.7.
The recursion for computing optimal label assignment i215 A context-sensitive framework calculate the score of paths extracted from the two left-out maps.
To test the significance of the score of each path, we shuffled the label assignments between nodes in the human interactome and re-scored the path.
We repeated the randomization 40 times per path.
Paths whose original score was better than the score ob-tained in at least 38 of the 40 shuffles (P 0.05) were considered statistically significant and were marked as successful paths.
5.5 Inferring influenza infection pathways We extracted from (Shapira et al., 2009) the following data sets: (i) the 36 human proteins that interact with influenza proteins and that are annotated with a GO term that appeared in M, which we denoted as sources; (ii) the 527 infection-related differ-entially expressed genes (as defined in Shapira et al.)
that are connected to at least one of the source proteins, which we denote as targets; and (iii) 1756 genes that were silenced and their effect on infection was measured, which we denote as siRNA positive.
Using our framework, we identified and scored all paths of lengths three to five edges in the interactome that connect these sources to these targets.
We did not consider paths of length two because only 40% of them were context-sensitive according to the SPIKE analysis (Fig.4).
We used a context-sensitive scoring matrix M that we computed based on all SPIKE maps.
Annotations based on IEA evidence codes, which are less reliable, were considered to increase the number of annotated genes in the interactome; however, they were weighted 50% lower than annotations based on other evidence codes.
We then counted the fraction of paths per length that contained a predicted intermediate protein (other than the source and target) that was found to be siRNA positive.
5.6 PB2 analysis We computed all paths connecting the 28 annotated human pro-teins that interact with PB2 (source proteins) and to the 527 differentially expressed genes that are on a path of length 56 edges from a source protein (target genes).
The annotation of 39 of the 527 differentially expressed genes was associated with interferon.
ACKNOWLEDGEMENT The authors thank the ISMB anonymous referees for their help-ful comments.
Funding: European Union Seventh Programme under the FP7-PEOPLE-MCA-IRG Funding scheme (256360 to E.Y.-L.); United States-Israel Binational Science Foundation (BSF) (2009323 and 2011296 to E.Y.-L.); Israel Science Foundation (ISF) (478/10 to M.Z.-U.
); Frankel Center for Computer Science at Ben Gurion University of the Negev (to A.L.
and M.Z.-U.).
Conflict of Interest: none declared.
ABSTRACT Motivation Biochemical reactions in cells are made of several types of biological circuits.
In current systems biology, making differential equation (DE) models simulatable in silico has been an appealing, general approach to uncover a complex world of biochemical reaction dynamics.
Despite of a need for simulation-aided studies, our research field has yet provided no clear answers: how to specify kinetic values in models that are difficult to measure from experimental/theoretical analyses on biochemical kinetics.
Results: We present a novel non-parametric Bayesian approach to this problem.
The key idea lies in the development of a Dirichlet process (DP) prior distribution, called Bayesian experts, which reflects substantive knowledge on reaction mechanisms inherent in given models and experimentally observable kinetic evidences to the subsequent parameter search.
The DP prior identifies significant local regions of unknown parameter space before proceeding to the posterior analyses.
This article reports that a Bayesian expert-inducing stochastic search can effectively explore unknown parameters of in silico transcription circuits such that solutions of DEs reproduce transcriptomic time course profiles.
Availability: A sample source code is available at the URLContact: yoshidar@ism.ac.jp 1 INTRODUCTION Cells can sense many different environmental signals, and respond to extraneous stimuli by switching activation/inactivation of specific molecules, such as transcription factors, via several types of biological circuits.
Kinetic biochemical modelling of such reaction circuits (often called biochemical reaction networks) using DEs realizes experiments in silico to enhance our understanding of the complex dynamic systems (Alon, 2006; Chen et al., 2004; Wilkinson, 2006).
It enables us to test a plausibility of current knowledge and hypothetical models on biological circuits and to create new further models when inconsistencies arise in simulation, domain knowledge and experimentally observed quantitative data (Yoshida et al., 2008).
Despite of a growing need for simulation-aided studies on bio-pathways, some fundamental issues prevent us from drawing their full potentials.
One critical issue addressed here is related to uncertainty in specified kinetic parameters of models: in many cases, we have no clear answer on how to specify kinetic values and initial conditions on simulations because biochemical reaction kinetics differ depending on a cell-by-cell To whom correspondence should be addressed.
basis and intra-/extracellular environments such as temperatures, presence/absence of biochemical components in specific tissues.
With the aim to explore kinetic parameters in in silico biological circuits that make solutions of DEs fit experimental data, a range of advanced technologies have appeared in the last several years of computational systems biology, involving genetic algorithm (GA; Kikuchi et al., 2003; Kimura et al., 2004; Koh et al., 2006), sequential Monte Carlo (SMC; Nagasaki et al., 2006; Nakamura et al., 2009), MCMC-driven parameter search methods (Finkenstdt et al., 2008) and also gradient descent algorithms (Cao et al., 2008; Fujarewicz et al., 2007; Yoshida et al., 2008).
Quantitative data, such as transcriptomic profiles and protein expression values, are usually measured over times.
In general, a Bayesian statistical method involves all the preceding methods as a unified framework: any estimation tasks are treated as evaluations of posterior distributions in which prior distributions embodying our prior belief on reaction kinetics are adapted to experimental data using likelihood functions.
As demonstrated later, however, the practical relevance of the existing approaches remains far below what practical analyses require.
Our view on this is that such a low performance depends highly on the lack of effective biochemical kinetic priors.
This article presents a previously unexplored class of prior distributions, called Bayesian experts on biochemical reaction kinetics.
The key notion of our approach can be found in the construction of Dirichlet process (DP) prior distribution (Blei et al., 2005; Escobar et al., 1995; MacEachern et al., 1998) whose base measure G0() is modelled by a mixture of m Gibbs distributions: G0()= m i=1 i Zi exp ( i() T ) (1) with mixing rates i (i=1, ...,m), temperature T>0 and partition functions Zi.
The m potential functions i() defined on unknown parameters reflect substantive knowledge on reaction kinetics for a given model such that regions of interest in the parameter space are prescribed prior to a posterior analysis.
In many applications, we are able to collect several kinetic evidences on such local regions by observing temporal patterns of experimental data.
For instance, rates of increasing and/or decreasing to which DE solutions have to match can be guessed in advance by observing steepness of relevant local patterns of given time course data.
Alternatively, maximum and minimum levels of gene expression data provide a helpful clue to determinations of steady-state levels to which a transcription circuit model reaches in equilibrium.
The Gibbsian mixture distribution, enabling Bayesian expert system in kinetic explorations, aggregates such m constraint conditions on the kinetic parameters that are induced automatically in light of experimental data and reaction speeds inherent in given models.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[11:05 28/8/2010 Bioinformatics-btq389.tex] Page: i590 i589i595 R.Yoshida et al.
This article describes our Bayesian experts, focusing on key examples of specific types of in silico transcription circuits.
Transcription factor activities are modelled on Hill functions and mass action kinetics (Alon, 2006; Wilkinson, 2006) as described in Section 2.
To emphasize the ability of the Bayesian expert prior, we explore unknown parameters using a simplified stochastic search algorithm without using more advanced computational techniques.
The DP prior with additional technical details is introduced in Section 3.
Experimental results appear in Section 4, demonstrating drastic efficiency gains from uses of the Bayesian experts and a benchmark on computational times.
Section 5 concludes with more generalization of our approach.
Supplementary Material and a sample programcalled LiSDAS (Life Science Data Assimilation Systems)are accessible at the Supplementary web site.
LiSDAS features some basic functions to explore kinetic parameter values for given data and a model.
The user can develop biological circuit models with the LiSDAS model construction file in place.
2 MODEL AND INVERSION ANALYSIS 2.1 Transcription regulatory circuits The proposed Bayesian method will be derived and described, in later sections, with examples on analyses of transcriptional reaction kinetics.
Before that, we briefly summarize specific forms of the DEs making up elements of the transcription circuit models.
2.1.1 Transcription Control of transcription factor-inducing gene expressions is a key regulatory mechanism in cells.
Transcription factors can sense external stimuli of cells by mediations of signal transduction pathways.
A transcription factoractivated by the transmitted signalsacts as either activator or repressor in transcription processes of specific target genes with many recruited enzymatic activities at the promoter regions.
One commonly used DE for describing the transcriptional process is Hill function which is in theory derived by considering an equilibrium binding of a transcription factor to its target site on the promoter (see Appendix of Alon, 2006).
The time evolution of mRNA concentration [X] is modelled as d[X] dt =rd [X]+tc iA ha([Pi];Ki,i) iR hr ([Pi];Ki,i) where the mRNAdecay rate and the maximal rate of transcription are denoted by rd and tc.
In addition to this, some studies commonly include a baseline b (e.g.
Cantone et al., 2009).
The Hill functions involving the transcriptional activators A and repressors R are ha([Pi];Ki,i) = 11+(Ki/[Pi])i I([Pi]>Ki), (2) hr ([Pi];Ki,i) = 11+([Pi]/Ki)i I([Pi]<Ki).
(3) The unknown kinetic parameters i and Ki control, respectively, location and steepness of each Hill function.
As i gets larger, the Hill function becomes steeper and looks more like the indicator function I(), taking the value one if argument is true, otherwise zero.
The multiple set of the Hill functions totally defines the AND gate that describes an abrupt increasing/decreasing of mRNA transcript level in response to excess concentrations of the regulatory proteins in A and R. 2.1.2 Mass action law Multimeric molecules can make up a transcription factor complex in any of transcription regulatory circuits.
In modelling, we need to build a DE describing molecular binding reactions, such as proteinprotein binding or one or more proteins interacting with small molecules.
Instantaneous rate of a binding reaction is described here by mass action kinetics d[C] dt =cd [C]+cb d i=1 [Pi]i .
(4) In the formation of a complex C, the binding efficiency among d molecules, Pi (i=1, ...,d), is determined on the affinity proportional to the active masses with the binding rate constantcb and powers i.
2.1.3 Translation An mRNA transcribed at level [X] proceeds to its translation process.
The model that we employ describes a production of protein P simply as d[P] dt =pd [P]+tl[X]cb d i=1 [Pi]i .
(5) Instantaneous rate of translation is represented by the linear functions in the first two terms with the rates of degradation pd and translation tl , respectively.
The last term involving (4) appears in (5) when the protein P binds to the other d1 molecules {Pi|Pi = P,i=1, ...,d} in system.
Note that we omit dissociation processes of the proteinprotein complex C, relying on the assumption that molecular stability of C is much higher and reaction kinetics involved in the dissociation is negligible in the entire system.
2.2 State space model A biological circuit is modelled as a set of DEs dxi(t)/dt = fi(pai(x),) defining rates of change in evolving concentrations of the p biological entities, x(t)= (xi(t))1ip, over continuous times t T .
In the above context, each endogenous variable involved in a bio-pathway makes up a state variable xi(t).
The i-th state variable is regulated by the parent variables pai(x)a subset of the state variables appearing in the right-hand side of each DEwith the rate equation fi defined by the kinetic parameters .
Conduction of quantitative experiments along times enables us to measure changes of the unknown state variables, x(t), directly or indirectly via observed data y(t)= (yi(t))1ip Rp+ at discrete time points t N T .
Here, to cope with partially observed systems where data of the state variables are in part missing, it is helpful to define a set of observable variables, iS {1, ...,p}.
For instance, in transcriptomic experiments using DNA microarrays, it is the case that amounts of protein expression levels are unobservable.
To proceed with an inversion analysis, the DEs are related to the experimental data using a state space model: yi(t)=xi(t)+wi(t) for t N and i S (6) dxi(t) dt = fi(pai(x),) (7) where wi(t) denotes measurement error independently and identically distributed.
The processes Y ={yi(t)|t N ,iS } and X ={x(t)|t T } follow (6) and (7) with initial state variables x(0) having a certain prior distribution x(0)p(x(0)).
Bayesian inversion i590 [11:05 28/8/2010 Bioinformatics-btq389.tex] Page: i591 i589i595 Bayesian experts in exploring reaction kinetics of transcription circuits analysis explores all the unknown parameters in the model initial states x(0) and kinetic parameters based on the posterior distribution P(x(0),|Y ,Pa) under which a circuit structure Pa={pai(x)}1ipa collection of parent variable sets is specified and a priori knowledge on reaction kinetics is expressed via a prior distribution P().
Rather than P(x(0),|Y ,Pa), in many cases, it is of more interest to evaluate the augmented posterior distribution P(x(0),,Pa|Y ) where a circuit structure is unknown or unreliable.
In what follows, we focus on the estimation of with a given circuit structure.
LiSDAS, however, has a pre-mature function for structural search of reaction circuits.
2.3 Learning reaction kinetics and relevant methods All the unknown parameters relevant to an analysis are expressed by , i.e.
={,x(0)} or augmented parameters involving circuit structures Pa.
In usual, an analytical form of the posterior distribution P(|Y )P(Y |)P() is unavailable due to non-linearity of DEs, thereby driving a need for efficient posterior approximation techniques in solving the inversion problemssearch for posterior means or modes.
The recent systems biology has explored parallel developments of SMC, MCMC-driven posterior explorations and gradient descent methods.
However, with a view to scaling to larger numbers of unknown parameters, efficient and effective computation remains a challenge.
Our study started from Nagasaki et al.
(2006) that applied a simplest SMC to the stochastic search for transcriptional reaction kinetics.
The use of SMC algorithm had been suggested also by Quach et al.
(2007) whereas they actually applied the unscented Kalman filter as a variant of conventional SMC.
A key notion of the SMC and MCMC-driven searcha more efficient counterpart of SMCis that the posterior distribution is approximately evaluated on a finite number of Monte Carlo samples called particles such that the empirical distributions are ensured to reach the true posterior distribution as the number of particles approaches to infinity.
Whenever following such an approach, however, a quite huge number of particles are needed to draw as raising the number of unknown parameters.
Nagasaki et al.
(2006) and Yoshida et al.
(2008) reported that the practical relevance of the SMC was limited by estimations of only a few parameters.
Some early studies focused on GA (Kikuchi et al., 2003; Kimura et al., 2004; Koh et al., 2006) that explores values of parameters by alternating crossover and mutation of initially generated seeds for the parameters.
With no loss of generality, any optimization problems for which GA aims to solve can be converted to posterior mode searches in Bayesian context.
The key for success in GA is whether or not an initial set of candidate parameters lies in the region close to the posterior mode.
According to Higuchi (1997), the intrinsic nature of GA is essentially the same to the SMC.
Hence, the low practical relevance of the SMC (Nagasaki et al., 2006; Yoshida et al., 2008) can be a rule of thumb for a performance of GA.
The substantial merit of Bayesian kinetic inference is in the use of prior distributions, P(), for conducting effective regularization in the inversion analysis.
To the best of our knowledge, however, no one has addressed this fundamental issue while the range of cutting-edge computational technologies have appeared over the last several years.
The Bayesian expert system that we present is the first attempt to make a class of prior distributions aiming to enhance the power of previously explored class of kinetic search methods.
3 BAYESIAN EXPERTS ON KINETIC LEARNING 3.1 Dirichlet process A prior distribution P() on the unknown parameters is modelled by a DP written as GDP(G|,G0) and |GG The distribution G that follows is treated as an infinite dimensional random variable distributed according to the DPDP(G|,G0).
The DP is defined by the two parameter: base measure G0 and concentration parameter 0.
The G0 is a distribution function defined on the support same as G. The smaller is, the more G looks like the G0, and vice verse as larger .
A key notion of DP is in a discrete representation of probability measure; G is decomposable via the sum of the Dirac measures i () at the infinite set of atoms i G0 (i=1, ...,).
The explicit form is hence given as the infinite mixture G()=i=1ii () with the mixing rates, i 0 and i=1i =1.
The infinite sequence of i can be made by successively conducting stick breaking construction or Chinese restaurant process (Blei et al., 2005; Escobar et al., 1995).
3.2 Base measure construction by a mixture of experts Of the base measure with the Gibbsian mixture (1), the m potential functions represent a set of implicit functions i()0 (i=1, ...,m) that specifies regions of interest in the kinetic parameters.
We here describe a simple idea on how to make such potentials with an example of the transcription regulatory models in Section 2.1.
3.2.1 Experts on maximal/minimal production rates In many applications, maximal or minimal level of each observable variable can be a helpful kinetic evidence to prescribe steady-state conditions on which each variable should reach in simulation.
We illustrate some examples on how to make such prior distributions.
The transcriptional process shown in Section 2.1 with each Hill curve varying sigmoidally between 0 and 1 defines the maximal production rate as d[X] dt =rd [X]+tc +b where the AND gate opens.
The equilibrium condition d[X]/dt =0 induces the steady-state level of [X]the maximal level of the ODE solutionas the ratio of the production and dilution rates [X]max = (tc +b)/rd .
If time course profiles are available on the mRNA, we are able to evaluate, in some ways, the maximal expression level of each mRNA, ymax.
Then, all possible configurations of {tc,rd ,b} such that ymax (tc +b)/rd can be eliminated in advance by making an arbitral potential function.
For instance, we will use, in later, max = (ymax (tc +b)/rd )2 with a given discount factor [1,).
Solutions with the parameters satisfying max 0 yield trajectories saturated around at ymax when =1.
The specified =1 is to express the prior belief that the observed maximal level results in an equilibrium state to which the underlying dynamic system reaches.
The restriction can be weaken/strengthen by the control of the temperature parameter T .
By analogy, the minimal expression level ymin can also be associated i591 [11:05 28/8/2010 Bioinformatics-btq389.tex] Page: i592 i589i595 R.Yoshida et al.
Fig.1.
Schematic expression on making Bayesian experts for (i)(ii) maximal/minimal production levels, and (iii) local reaction speeds.
with the minimal production rate on the model with the closed AND gate, rd [X]+b =0, which derives a potential function min = (ymin b/rd )2.
Relevant to the mass action kinetics (4) and the translation process (5) that involve protein-level reactions, we can often specify the maximal/minimal expression level of protein in data-driven ways or empirical bases.
Conventional DNA microarray experiments measure transcript levels of mRNA molecules in arbitrary units, and hence any kinetic parameters in a fitted model result in unit free as well as quantities of regulatory proteins.
A natural way to cope with such cases is to put an arbitrary chosen maximal/minimal value to the solutions for (4) and (5).
The DE (5) attains the maximal speed of translation when the maximal level of mRNA, [X]max = (tc +b)/rd , is specified: d[P] dt =pd [P]+tl[X]max cb[P] Pi =P [Pi]imin.
where the minimal protein concentrations [Pi]min are specified arbitrary in the rest of d1 reactants.
Given this, a maximal level pmax is then related to the equilibrium state tl[X]max/(pd + cb Pi =P[Pi]min), which makes a potential function max = (pmax tl[X]max/(pd +cb Pi =P[Pi]imin))2 with a discount factor .
By analogy, the solution of (4) describing a complex formation attains at the maximal rate of production with [Pi]max (i=1, ...,d).
The steady-state condition induces an expert max = (cmax cb d i=1[Pi]imax/cd )2 with an arbitrary specified maximum value of the complex, cmax.
3.2.2 Experts on reaction speeds As well as the maximal and minimal levels of production, experimental data provide us a further clue to what temporal patternsreaction speedsDEs must exhibit in simulation.
Suppose that a temporal profile of mRNA is observed as shown in Figure 1, showing increasing and decreasing trends alternatively.
Gradients of any fitted DE models should reproduce the observed steepness during each trend.
Of many possible ways to develop such experts, we present a practically useful, simple procedure that is implemented on LiSDAS.
The first task to be addressed is a decomposition of an entire time interval into active and inactive durations according to observed gene expression profiles, and values of gradients or steepness in each of the identified phases.
The proposed procedure starts with fitting a non-parametric regressor, fi(t)=i +wTi g(t)+vi(t), to the i-th mRNA profile, yi(t) (t N ), where i and vi(t) are intercept and residual, respectively.
The J basis functions placed at equally-spaced grids in T are aggregated to g(t)RJ , and wi RJ is a vector of coefficients.
LiSDAS fits the Gaussian radial functions by using 2 regression scheme.
A value of bandwidth common to all basis functions and a shrinkage parameter in the 2 regression are prescribed by users.
The fitted curve fi(t)= i +wTi g(t) offers a gradient function over time, wTi (dg(t)/dt).
Let Don and Doff be duration times of increasing and decreasing trends that are identified as subsets of the entire time interval.
Each duration keeps a same sign of the evaluated gradients during the consecutive times.
The key idea here is to derive a set of experts such that each potential function reflects a reaction speed of increasing or decreasing during each of the identified local times.
The Hill curve transcription dynamics in Section 2.1 are locally linear where the concentration levels of all the transcription factors rise much above or fall much below the relevant Kis.
At the reaction speeds of transcriptional process with open/closed AND gates, we have the solutions: [X]= tc+b rd + ( [X]0 tc+brd ) erd t (open gate) b rd + ( [X]0 brd ) erd t (closed gate) where [X]0 is an initial condition.
Denote the start and end times of each identified duration by (tonstart,t on end) for Don and (t off start,t off end) for Doff , respectively.
Of interest here is to identify regions of the parameter spaces such that the simulations instantaneously reproduce the observed reaction speedsfi(t), respectively, for t [tonstart,tonend] or t [toffstart,toffend].
Specify fi(tonstart) and fi(t on end) to [X] and [X]0 in the former ODE solution corresponding to the active duration where t is set to the length of the duration, ton = tonend tonstart .
An expert on reaction speed is then made of the potential function: on = { fi(t on end) tc +b rd ( fi(t on start) tc +b rd ) erd ton }2 .
By analogy, the implicit function relevant to the latter solution induces the expert on speed in the inactive duration with toff = i592 [11:05 28/8/2010 Bioinformatics-btq389.tex] Page: i593 i589i595 Bayesian experts in exploring reaction kinetics of transcription circuits toffend toffstart : off = { fi(t off end) b rd ( fi(t on start) b rd ) erd toff }2 .
These two potential functions form elements of a Gibbsian distribution in various ways, for instance exp(off on/T ).
Alternatively, we would develop an expert based on either exp(off/T ) or exp(on/T ).
In advance to the posterior exploration, LiSDAS generates many experts for all the identified duration times by joining/dissociating on and off at random.
3.2.3 Summary Figure 1 summarizes all the potential functions that we use in later and a schematic view on the idea.
It is important to see that the preceding potential functions constitute, with their combination, a single potential function in (1).
For instance, the potential functions relevant to each mRNAmax, min, on and off are defined on the transcriptional parameters (rd ,tc,b), while the other potentials involve only the kinetics relevant to protein regulations.
LiSDAS draws at random potential functions and conventional Gaussian priors (the user can specify in the model construction file) for each set of kinetic parameters.
Each i in (1) is then constructed by aggregating the extracted component prior distribution corresponding to different types of kinetic parameters.
3.3 Computational algorithm DP is discrete with probability one, and represented as an infinite mixture P(|)=i=1ii () conditional on infinite number of the atoms, ={i|i=1, ...,} following G0.
Hence, the time course data also follow the infinite mixture P(Y |)= i=1 iP(Y |i).
Of interest in the subsequent posterior analysis is to realize random sample from the posterior distribution of , a posteriori, and also the mixing rates i, conditional on Y .
The Bayesian model in total forms a DP mixture (DPM) with the key hierarchical structure: (i) GDP(G|,G0), (ii) |GG and (iii) Y |p(Y |).
In statistical science, there have been several excellent reviews, providing technical details on DPM (Blei et al., 2005; Escobar et al., 1995; MacEachern et al., 1998).
We therefore present only a specific computational algorithm.
The procedure is computationally very straightforward.
Sample from the posterior is constructed in a recursive way as a conventional sampling algorithm does (e.g.
Escobar et al., 1995).
Suppose that, in a current distinct parameter set, k ={i|i=1, ...,k} with the realized values being all distinct, there are ni occurrences of each i.
The total sample size is Nk = k i=1ni.
Then, the (k+1)-th case is newly generated by the conditional posterior (k+1|k,Y ) given as the mixture P(k+1|k,Y ) k j=1 jj (k+1)+k+1P(k+1|G0,Y ), with the mixing rates given by j  nj +Nk P(Y |j) for j=1, ...,k, +Nk P(Y |)dG0() for j=k+1 .
The first k component distributions consist of the Dirac measures j (k+1) placed at the preceding sample points, with the mixing rates involving the frequency of sampling nj and the likelihood P(Y |j).
The last component represents the conditional posterior P(k+1|G0,Y )P(Y |k+1)G0(k+1), and the normalizing constant cannot be evaluated analytically.
If a sample k+1 is drawn from one of the preceding samples, the occurrence rate is updated, nj nj +1.
Given a new coming from the (k+1)-th distribution, we increment the current set such that k+1 =k +{} and nk+1 =1.
Random draws having the higher likelihood P(Y |) appear more frequently and result in an approximation of the posterior distribution P(|Y ) as the sampling process proceeds iteratively until convergence.
The remaining task to be addressed is to make a sample from P(k+1|G0,Y ), and evaluation of its normalizing constant.
We here provide a simple Monte Carlo technique using importance sampling (IS).
One difficulty is relevant to making a sample from the Gibbsian mixture G0 m i=1iZ1i exp(T1i()).
Our approach is very straightforward: (i) conduct the second-order approximation of each potential function i()i(i)+( i)Ti(i)( i) around i =argmaxi() with i() the Hessian matrix of i, and (ii) the proposal distribution of IS is then defined as the Gaussian distribution Qi()=N(|i,T(i)1) with the covariance matrix given by the inverse of Hessian matrix times T .
Let Q()=mi=1iQi() be the Gaussian mixture proposal distribution.
Assuming that we have a realization of k samples from the Q and the importance weights i =G0(i)/Q(i) (i=1, ...,k), then the normalization constant appearing in the mixing rate of the (k+1)-th component can be approximated by ( k j=1j)1 k j=1jP(Y |j).
The approximated mixing rates are then proportional to j  nj (+Nk)ki=1i jP(Y |j) for j=1, ...,k, (+Nk)ki=1i k i=1iP(Y |i) for j=k+1 .
One of the previously obtained k samples is replicated if a sampling process draws a component coincident to one of {1, ...,k}.
If not, we add a new case k+1 = from Q, and then assign the likelihood P(Y |k+1) and the IS weight k+1.All the cases, whether new or old, are independently distributed according to the Q, thereby ensuring the statistical consistency of the Monte Carlo approximation of .
4 APPLICATION An example in kinetic search of a circadian clock transcriptional circuit illustrates the practical relevance of the Bayesian expert systems, and we make a comparison to the non-expert approach using a conventional SMC (Nagasaki et al., 2006; Nakamura et al., 2009).
Our model considered here is an extended version of the gene regulatory model developed in Matsuno et al.
(2000) where among several studies used in evaluating relevance of various parameter search algorithms (Nagasaki et al., 2006; Nakamura et al., 2009; Yoshida et al., 2008).
The original model describes a reaction circuit of 12 endogenous variables, involving five mRNAs, five translated proteins and two protein complexes, which constitute an interlock of several types of positive and negative feedback loops.
We here added, to the previous model, seven clock-related variables and their regulatory mechanisms, reflecting in part the recent discoveries (Baggs et al., 2009; Ueda et al., 2005).
The newly constructed DE i593 [11:05 28/8/2010 Bioinformatics-btq389.tex] Page: i594 i589i595 R.Yoshida et al.
A B Fig.2.
Results of analysis of reaction kinetics.
(A) Petri net diagram for in silico circadian clock transcriptional circuit involving 19 endogenous variables and 116 unknown parameters.
(B) Simulation trajectories of the 19 variables with the estimated parameters (red; Bayesian expert, blue; non-expert SMC).
The black circles in the panels corresponding to the seven mRNAs denote the experimental data at the 12 time points.
The error bars (vertical red lines) indicate the maximum and minimum values among 20 simulated trajectories with different sets of the parameters that were estimated by repeating the Bayesian expert search independently.
modelits mathematical form is available in the Supplementary Materialcontains 116 unknown parameters in total, consisting of 97 kinetic parameters and 19 initial conditions on x(0).
The search methods were applied to time course gene expression indices from GeneChip mouse genome microrrays (Ueda et al., 2002).
Of the 19 variables, temporal expression changes of the seven mRNAs were measured along 12 time points equally spaced on 44 h (http://sirius.cdb.riken.jp/MouseSCN/ MouseSCNCCG(020707).html).
A much detailed information on the specified prior distributionsvalues for , temperature and conventional Gaussian priors used in combinationsis accessible from the Supplementary web site, as well as the LiSDAS code that can reproduce the subsequent numerical analyses with a default configuration file.
Figure 2 displays simulation trajectories of the 19 endogenous variables, where the identified kinetic parameters and initial state variables were defined as the i exhibiting the best fit in terms of P(Y |i).
The DEs were solved using the Euler discretization with 700 time steps equally spaced on the entire time interval.
CPU time required for the execution of LiSDAS was 40 min (Intel Core2 Quad processor, 2.66 GHz) for the particles of size k =5105.
Compared with the very poor results of the non-expert SMC method using only the conventional Gaussian priors, the LiSDAS could capture the underlying circadian rhythm inherent in the observed data with much better degree of fitness.
Note that the applied search procedure is almost the same as a random search.
The particles were all drawn from G0, and then the best fit value was merely chosen.
Therefore, the good performance arose from only the added Bayesian expert system.
To the best of our knowledge, the methods being able to estimate such many parameters have never appeared in the related research field, while so far, our previous study (Yoshida et al., 2008) and the GA of Koh et al.
(2006) could draw reasonably good performances in analyses of a transcription model and in silico signal transduction pathway, containing, 44 and 84 unknown parameters respectively.
5 CONCLUDING REMARKS This brief article has described the new idea on data-driven parameter search for bio-pathway DE models.
The key contributions lie in the novel Bayesian expert systems induced by the base measure of DP prior.
The expert systems rely on the Gibbsian potential functions that reflect automatically recognized model-specific contextsreaction speeds, equilibrium statesto the subsequent search process so as to prune insignificant parameter spaces.
The ability to aid in improving efficiency has been demonstrated in the example on transcriptional regulatory models.
The practical benefit was surprisingly significant; the applied simple search algorithm could draw fairly good performances only by a plug-in of the expert systems.
With a view to general versatility of the current approach, developments of more practically relevant expert systems remain a challenge yet.
A biological circuit is made of several types of biochemical reactionsphosphorylation cascade-mediated signal transductions and metabolic reactionsother than the transcriptional processes we studied.
In many cases, these reactions are modelled by more complex equations, being either stochastic or non-stochastic, than the Hill functions used in this article.
Though it would be straightforward, even for more complex DE systems, to relate observed maximal/minimal production levels to steady-state conditions, some difficulties arise in evaluations of experts on reaction speeds with a given more complex model due to unavailability of the analytical expression for the reaction speeds.
We then need to explore some numerical techniques for making expert system priors.
i594 [11:05 28/8/2010 Bioinformatics-btq389.tex] Page: i595 i589i595 Bayesian experts in exploring reaction kinetics of transcription circuits Making further variants in expert construction is possible, and will then be inherently context specific.
In view of biochemical kinetics, it would be more helpful to incorporate, to further prior modelling, differences of time scales on each reaction speed, such as difference in molecular stability of different proteins and mRNAs.
Signal transduction pathways usually change in transcription factor activities on subsecond time scales.
Binding of transcription factor to its target promoter reaches equilibrium in seconds.
Transcription and translation of genes take many minutes in reaching steady state.
We need to explore ways of reflecting these time scales to subjective prior distributions expressing a biologically significant subspace of kinetic parameters.
Another important issue is unreliability of model.
In silico circuits embody currently obtained knowledge on many pairs of interacting molecules.
In many applications, such a circuit structure is totally unreliable because of environmental dependency and diversity of cells.
Very often, simulations fail to reproduce experimental data irrelevant to which kinetic parameters are specified.
The inconsistency indicates presence or absence of some regulatory mechanisms to be reflected into model reconstruction.
Recent systems biology therefore has explored a structural learning of reactions circuits in data-driven ways (Cantone et al., 2009; Cao et al., 2008; Porreca et al., 2010).
LiSDAS with the procedure specification file in place can explore the most plausible circuit with the highest degree of fit as well as kinetic values by adding/deleting edges completely at random.
However, with a view to larger scale models, the ability of such a low-level search method reaches a performance limit due to a huge space of possible edge configurations.
Our next challenge is the development of Bayesian experts in exploring circuit structure.
Conflict of Interest: none declared.
ABSTRACT Summary: LinkinPath is a pathway mapping and analysis tool that enables users to explore and visualize the list of gene/protein sequences through various Flash-driven interactive web interfaces including KEGG pathway maps, functional composition maps (TreeMaps), molecular interaction/reaction networks and pathway-to-pathway networks.
Users can submit single or multiple datasets of gene/protein sequences to LinkinPath to (i) determine the co-occurrence and co-absence of genes/proteins on animated KEGG pathway maps; (ii) compare functional compositions within and among the datasets using TreeMaps; (iii) analyze the statistically enriched pathways across the datasets; (iv) build the pathway-to-pathway networks for each dataset; (v) explore potential interaction/reaction paths between pathways; and (vi) identify common pathway-to-pathway networks across the datasets.
Availability: LinkinPath is freely available to all interested users atContact: supawadee@biotec.or.th Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 21, 2011; revised on May 9, 2011; accepted on May 25, 2011 1 INTRODUCTION Pathways are functional units resulted from the interplay of interacting genes, RNAs, proteins and small molecules.
Mapping genes or proteins into the context of pathways can help gain more insights into their functions and interactions in an organism.
Although sequence similarity-based methods [e.g.
NCBI BLAST (Altschul et al., 1990)] have been commonly used for identification of pathways to genes/proteins based on their orthologous genes/proteins annotated in the well-characterized pathways, these methods have limitations such that some best hits (such as those annotated as hypothetical or unknown genes/proteins) may not necessarily be annotated in any pathway.
The incorporation of additional data such as proteinprotein interactions and enzymatic reactions can help infer the pathways and their interconnection and uncover the biological function of genes/proteins.
However, the information regarding the connections between pathways through molecular interactions and reactions are not included and adequately represented to support the exploratory analysis in most pathway mapping tools (reviewed in Gehlenborg et al., 2010), for example, GenMAPP (Salomonis et al., 2007), Pathway Explorer (Mlecnik et al., 2005) and Pathway Projector (Kono et al., 2009).
To overcome To whom correspondence should be addressed.
these limitations, a web-based interactive tool, so-called LinkinPath, was developed to analyze, map and visualize the gene/protein lists in the context of interconnected pathways, which provides a valuable resource for not only comprehensive studies of genegene interaction, but also functional genomics in virtually all organisms.
2 METHOD LinkinPath has been developed as a web-based interactive exploration tool for pathway analysis.
Users can upload the datasets of DNA or protein sequences in FASTA format and submit to the LinkinPaths job queue (Supplementary Fig.S1), which could support the analyses of genome-scale inputs.
Although, many jobs may be submitted at the same time, the LinkinPath web server accepts a total maximum of 5000 sequences for each job.
When jobs are finished, users can retrieve the result using the bookmarked URL during the submission or the web link from the notification email.
LinkinPath processes each job in five main steps: (i) sequence and protein domain search; (ii) pathway mapping and annotation; (iii) identification and comparison of enriched pathways; (iv) construction of interconnected pathway networks and (v) identification of common pathway subnetworks (Supplementary Fig.S2).
First, input sequences are searched against KEGG (Kanehisa and Goto, 2000), NR (Benson et al., 2007), PFAM (Finn et al., 2008) and RFAM (Griffiths-Jones et al., 2003) databases.
Second, if significant similarities are found to match with an enzyme class (EC) and/or KEGG Orthology (KO), genes/proteins will be annotated with the corresponding pathways.
Third, to support comparison of functional composition, Treemaps and statistical methods are employed to examine the pathways enriched in the dataset.
Fourth, information of molecular interactions and reactions from BIND (Bader et al., 2003) and KEGG will be used for inference of pathways and networks.
In case that an input sequence cannot be annotated in any pathway, LinkinPath will use its interacting partners to infer its related pathways.
The pathways will thus be linked together via interaction or reaction paths to form the network in this step.
Lastly, to identify the commonalities across multiple datasets, LinkinPath includes an algorithm to extract the frequent subnetworks from the pathway networks built in previous step.
3 RESULT Using the method described above, LinkinPath automatically maps and annotates genes/proteins in the datasets into the context of interconnected pathways and presents the results to users via a Flash-driven interactive web interfaces.
The results are organized into five analysis steps (see following subsections), which can easily be browsed, searched and downloaded in any of supported formats.
Summary charts and an interactive Venn diagram are also provided to illustrate of how a dataset differs from others according to their annotated sequences with EC numbers, interactions and pathways (Supplementary Fig.S3).
The annotation results of input sequences can be interactively explored in different visualization perspectives The Author(s) 2011.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[09:20 22/6/2011 Bioinformatics-btr328.tex] Page: 2016 20152017 S.Ingsriswang et al.
including KEGG-pathway maps, TreeMaps, pathway-to-pathway networks and interaction and reaction paths.
3.1 Detecting changes in animated pathway maps In most tools, the KEGG pathway image is statically displayed with positioned genes/proteins in the pathway.
To enable the dynamicity, LinkinPath allows users to run the pathway map animation to depict changes of expressed genes/proteins annotated in each pathway over multiple and time-series datasets (Supplementary Figs 4 and 5).
Detecting changes between the same pathway maps taken from different times/experiments/organisms can help reveal the functional characterization of genes/proteins expressed under different conditions.
In addition, the co-occurrence and co-absence of genes/proteins in a series of pathways can suggest the functional relationships between these molecules.
Alternatively, users can interactively explore the annotated pathways across datasets on KEGG Atlas, in which connected paths are colored according to the datasets.
3.2 Visualizing functional composition via TreeMaps Many classification schemes such as enzyme classification, KO and KEGG pathways have tree-based structures that are difficult to simultaneously display and compare the information contained in the trees.
To circumvent this problem, Treemaps are included in LinkinPath to facilitate the visualization and comparison of those hierarchical data via a set of nested rectangular maps.
A base rectangle represents the root of the hierarchy and is divided into rectangular subareas proportional to data size and colored by data type.
As a result, Treemaps enable users to compare sizes of nodes and subtrees, and are helpful in revealing patterns.
To disclose the functional composition among datasets, LinkinPath provides users with three different Treemaps: (i) the enzyme compositions using top-level EC numbers; (ii) the pathway compositions using KEGG pathway classification; and (iii) and KO compositions using KO numbers (Supplementary Fig.S6).
The Treemap of enzyme composition, for example, helps users visually examine enzyme enrichment in lists of genes/proteins, where each rectangle represents the top-level EC numbers with different colors and indicates the proportion of genes/proteins annotated with the EC group.
Alternatively, LinkinPath allows users to compare and examine the enrichments of the EC and KO annotations of entire input data across pathways.
3.3 Identification of statistically enriched pathways Since pathways that are highly enriched with the list of annotated genes/proteins are more likely to be biologically relevant, LinkinPath employs KOBAS (Wu et al., 2006) to help identify significantly enriched pathways in a dataset.
KOBAS uses KO number to link genes/proteins to KEGG pathways and calculates the statistical significance of each pathway in a queried dataset against all pathways in the referenced datasets.
There are three statistical tests available including binominal, chi-square and hyper-geometric distribution tests to assess the enrichment of the found pathways.
For each pathway found in the input datasets, LinkinPath calculates these statistics by comparing the number of sequences involved in the pathway for each dataset with the total number of sequences involved in the same pathway for all datasets.
3.4 Inferring pathway-to-pathway interconnections Despite their complexity, the interconnections between pathways can help unravel novel regulation mechanisms including metabolism and signaling in organisms.
LinkinPath infers the pathway-to-pathway connections using molecular interactions and reactions.
Pathways will be connected to each other and represented in a form of interactive networks.
Each dataset could have several pathway networks with different sizes.
Each node represents either a pathway or an input sequence.
The pathway node contains the information on the number of input sequences annotated in that pathway.
An edge between nodes indicates the pathway connection types with different colors and line styles.
Two pathway nodes are connected with a solid line if a path between them exists in KEGG database.
The dash line indicates an inferred path from an input sequence or a query node to a known pathway.
With its interactive network browser, LinkinPath allows users to browse and access the network characterization and the associated information such as the number of input sequences and interactions involved in a pathway node and the list of input sequences that appear in single or multiple pathways.
Exploring putative functions of genes/proteins: LinkinPath utilizes molecular interactions to infer the pathway and putative function of an un-annotated sequence on the basis of its interacting partners function.
The shortest interaction paths connecting from a query node to a pathway node in the network are identified using all paths breadth first search.
In addition, the putative function of an input sequence might be inferred by its proximity to functionally annotated genes/proteins within the context of interconnected pathways.
In the network browser, users can traverse the interaction paths to the inferred pathways of the nodes with dashed edge (Supplementary Fig.S7).
Exploring reactions between metabolic pathways: LinkinPath searches the reaction paths from an input sequence mapped in a metabolic pathway to the compounds linking to other metabolic pathways.
On a solid blue edge in the pathway-to-pathway network, if the reaction paths exist, users can explore what enzymes and compounds are essential to a metabolic process via the reaction paths between two pathways (Supplementary Fig.S8).
3.5 Discovering the common pathway subnetworks LinkinPath extracts the frequently occurred subgraphs/subnetworks from the pathway networks to discover the commonalities across the datasets (Supplementary Fig.S9).
Users can navigate and visualize frequent subnetworks of varied sizes that occur in a number of different datasets.
4 CONCLUSION LinkinPath is a web-based tool that was applied the state of the art visualization techniques for original aspects of pathway mapping and analyses.
Its novel contributions such as functional composition using Treemaps, pathway-to-pathway interconnections and the global metabolic map with highlighting mechanisms add value over comparable existing tools.
ACKNOWLEDGEMENTS The authors would like to thank Dr. Anan Jongkaewwattana for critically reading the manuscript and a former ISL staff, Eakasit 2016 [09:20 22/6/2011 Bioinformatics-btr328.tex] Page: 2017 20152017 LinkinPath Pachawongsakda, who helped in programming during the initial stage of the project.
Funding: National Center for Genetic Engineering and Biotechnology (BIOTEC), Thailand.
Conflict of Interest: none declared.
ABSTRACT Motivation: Although the integration and analysis of the activity of small molecules across multiple chemical screens is a common approach to determine the specificity and toxicity of hits, the suitability of these approaches to reveal novel biological information is less explored.
Here, we test the hypothesis that assays sharing selective hits are biologically related.
Results: We annotated the biological activities (i.e.
biological processes or molecular activities) measured in assays and con-structed chemical hit profiles with sets of compounds differing on their selectivity level for 1640 assays of ChemBank repository.
We compared the similarity of chemical hit profiles of pairs of assays with their biological relationships and observed that assay pairs shar-ing non-promiscuous chemical hits tend to be biologically related.
A detailed analysis of a network containing assay pairs with the highest hit similarity confirmed biological meaningful relationships.
Furthermore, the biological roles of predicted molecular targets of the shared hits reinforced the biological associations between assay pairs.
Contact: monica.campillos@helmholtz-muenchen.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION The screening of a library of compounds in a biological assay is a common first step in drug discovery to find chemical hits for the drug leads.
A single chemical screening experiment provides information about the activity of compounds on a target or bio-logical process.
However, to determine the suitability of the chemical hit as chemical probe or drug lead, it is important to know additional properties of the compound such as its specifi-city and toxicity.
An inexpensive and efficient manner to obtain information about these properties is to learn about the activity of this compound across multiple chemical screens.
This approach is followed routinely in chemical screening programs such as the NCI60 project run by US National Cancer Institute (NCI) where the activity of a compound across 60 different cancer cell lines is measured to detect selective chemical hits for a particular cancer and avoid general toxicity (Shoemaker, 2006).
In the past decade several initiatives including the NIH Molecular Libraries Program (Austin et al., 2004) and ChemBank (Seiler et al., 2008) have compiled chemical biology experiments performed by different laboratories using diverse experimental set-ups ranging from cell-free to cell-based and even whole organism-based assays.
The analysis of these heterogeneous datasets is challenging yet offers the possibility to obtain a global view of the chemical and biological activities of chemicals.
In this regard, the integration and analysis of the collection of assays stored in the PubChem BioAssay (Wang et al., 2010) repository has proven to be useful to predict adverse drug reactions (Pouliot et al., 2011) and to determine chemical properties of promiscuous compounds, that is, those that appear as frequent hitters in many high-throughput assays (Canny et al., 2012; Chen et al., 2009; Schurer et al., 2011).
The results of these studies suggest that a plethora of hidden molecular and biological information in these repositories can be uncovered using integrative computational methods.
This is par-ticularly relevant for the hits of phenotypic assays, for which the underlying molecular targets responsible for their activity is un-known.
To determine the protein targets of the chemical hits of these assays, in silico target prediction methods (Keiser et al., 2007; Liu et al., 2013; Wang et al., 2012) are arising as an effi-cient approach to obtain insights into the compound mode of action.
For instance, Young et al.
have shown recently that the predicted molecular targets of hits are able to explain complex readouts of high-content screening assays (Young et al., 2008).
Here, we exploited the vast amount of publicly available chem-ical screening assays present in the ChemBank database to evalu-ate in a systematic manner if a pair of biological processes or molecular activities (hereafter named biological activities) modulated by common chemicals in phenotype-or target-based assays, respectively, is related.
We tested and confirmed this hypothesis by the systematic analysis of the biological activities measured in pairs of assays sharing non-promiscuous compounds in this repository.
Subsequently, to understand the molecular mechanism linking pairs of phenotypic assays sharing chemical hits, we annotated the molecular targets of the shared hits.
To that aim, we used HitPick (Liu et al., 2013), a recently developed in silico target prediction method to predict the molecular targets of compounds.
We found that the known biological role of the predicted targets of common chemical hits confirms the biological processes relationships between the phenotypic assay pairs and provides mechanistic understanding of the relationships.
This approach allows us to find relationships between biological activities and to understand better the molecular basis of the shared biological activities.
2 MATERIALS AND METHODS 2.1 ChemBank assay data structure The ChemBank (Seiler et al., 2008) data were downloaded in May 2011 and comprised 193 projects with loaded screening plates, including 3852 assays and 228 887 tested compounds.
We also extracted information*To whom correspondence should be addressed.
The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com mailto:monica.campillos@helmholtz-muenchen.de; Liu etal., 2013 `` '' as , XPath error Undefined namespace prefix about the assays and projects including assay names, assay description, project names, project description and project motivation.
Three pro-jects containing 18 assays were discarded because they lacked information about compound IDs.
If a project comprises assays containing in the assay name an annotation of raw and user, such as the project of Pseudomonas Cell Wall Synthesis, we only kept the assay annotated as user, as we observed that it often reports the specific activity of the compounds.
This step retained 3617 assays.
Then, we combined the assays performed with the same experimental protocol indicated by iden-tical assay name and assay description, such as assay ID 1133.0005, ID 1133.0006 and ID 1133.0007 of the project Glioblastoma Modulators, into the same assay type.
In total, 3617 assays were grouped into 1640 assay types.
The analysis presented here was based on the assay type, which for simplicity we named assay.
We assigned the activity of a compound both on an assay level and a project level.
A compound is active in a project when it is active in at least one of their assays.
We classified the assays into cell-free, cell-based or microorganism assays according to the assay description provided by ChemBank.
If the assay was performed in a cell line (e.g.
all the assays in the Glioblastoma Modulators project were done in U251 human glioma cells), this assay was classified as cell-based; if the assay was performed in a microorgan-ism (e.g.
the SigB Inhibition project that identified small-molecule inhibitors of Listeria SigB transcription factor was performed in Vibrio sp.
S1063), this assay was classified as microorganism; the remaining biochemical or biophysical assays were classified as cell-free.
2.2 Chemical hit identification methods To identify chemical hits in the ChemBank data set, we applied three published methods, namely, the ChemBank (Seiler et al., 2008), the B-Score (Malo et al., 2006) and the Well-Correction (Makarenkov et al., 2007) methods and five modifications of them to adapt the methods to the ChemBank data structure.
These methods are summarized briefly as follows: The ChemBank method aims to normalize the activity in the assay based on mock signals.
B-Score method uses the median polish procedure to remove the row/column biases in a plate.
First, a residual activity of a compound is calculated and then all data are normalized for plate-and row/column-specific effects.
Chemical hits were determined using median absolute deviation (MAD) or P-value statistics, i.e.
(i) compounds with a residual larger than 2*MAD (2MAD), (ii) P50.01, (iii) P50.05, were defined as hits.
Well-Correction method rectifies the distribution of assay measurements by normalizing data within each considered well across all assay plates.
In the end, both P50.01 and 0.05 were applied to capture the hits.
As the B-Score method requires ideally the controls to be located randomly among the wells of each plate, or at most localized in the first and last columns, we created a modification of the method called B-Score_A adapted it to the ChemBank dataset structure where some plates only contain positive-control wells (e.g.
plate ID 1031.0004.Pos.A and B).
For this, positive controls were not considered in the median polish procedure and their residual activity was computed by subtracting the mean median effects of non-positive controls from their raw values.
The next steps, including hit detection thresholds, were identical to those of the B-Score method.
The Well-Correction method requires the compounds measured across all assay plates to be randomly distributed.
In the ChemBank dataset, many wells across different plates contain high number of positive controls (e.g.
well A24 of assay ID 1017.0030) and therefore, the Well-Correction method cannot be applied directly.
To correct for this, we discarded wells with higher number of positive controls (i.e.
number of positive controls number of non-positive controls).
To keep all the methods comparable, we applied this modification for the above four methods (marked as * in Fig.1).
If the assay contains replicates of compounds, we required all repli-cates to be identified as hits to consider them as chemical hits (also named actives, Fig.2a).
We determined the performance of the eight hit identi-fication methods using the receiver operating characteristic (ROC) graph (Fawcett, 2006) and the positive and negative controls (including mock treatments) of the assays were used as a benchmark set.
In all 3852 assays, the total number of positive controls is 96 and the number of negative controls is 7 590042 and 7 620521 for non-modified and modified ver-sions of methods, respectively.
The modification of the B-Score_A with two different thresholds, namely, 2MAD and P50.05, showed the best performances.
We selected the latter one due to its higher specificity (97.4%) with 79.6% of sensitivity.
2.3 Promiscuity filters To increase computational efficiency, we applied filter F1 to keep com-pounds from the initial ChemBank dataset showing activity in more than one project.
The removal of the compounds active in only one project or inactive in all the projects does not have an effect on the hit similarity (calculated by continuous Tanimoto coefficient, Tc (Willett et al., 1998) (see Supplementary Methods) between assays (see Supplementary Fig.S1).
Then, we applied two additional filters to keep selective compounds at project level (F2) and assay level (F3), respectively.
F3 was applied to projects with at least nine assays, which was determined by averaging the number of assays per project in the ChemBank screening repository.
3 RESULTS 3.1 Assay structure and chemical hit identification We chose the ChemBank repository of chemical screens to test the hypothesis of whether a pair of biological activities Fig.1.
ROC space showing the performance of the eight hit identification methods for the ChemBank assay dataset.
To assess the performance of the eight methods, we calculated the distance of the coordinate (1-Specificity, Sensitivity) to a random guess line.
The greater the distance to the random line, the better the method is.
Sensitivity=TP/(TP+FN), Specificity=TN/(TN+FP).
TP: true positive, TN: true negative, FN: false negative, FP: false positive.
Asterisks denote modifications of the corresponding methods i580 X.Liu and M.Campillos `` '' `` '' `` '' `` '' `` '' `` '' `` '' `` '' `` '' `` '' , `` '' `` '' `` '' `` '' , , `` '' `` '' `` '' `` '' `` '' `` '' `` '' `` '' `` '' , in order is a `` '' b c In order t , `` '' `` '' In order (i.e.
biological processes or molecular activities) modulated by the same chemicals is related.
In the ChemBank repository, the raw activity of a total number of 228 887 compounds in 3852 assays (representing experimental batches) of 190 diverse projects is available.
In a first step, we identified the chemical hits of the assays.
As several approaches have been proposed to identify chemical hits in chemical screenings (Makarenkov et al., 2007; Malo et al., 2006; Seiler et al., 2008), we decided to test a collection of eight different methods (see Section 2) to select the best-performing hit identification method for the ChemBank dataset repository.
To that aim, we determined the method that best discriminated between the compounds representing positive and negative controls within the assays.
The B-Score_A method, a modifica-tion of the well-known B-Score method (Malo et al., 2006) achieved the best performance with a sensitivity of 79.6% and a specificity of 97.4% (Fig.1).
We thus selected this method to determine the chemical hits of ChemBank assays.
Then, we grouped chemical screen batches performed using identical experimental protocols into assay types (hereafter named assays) reducing the number of assays to 1640 (see Section 2).
Next, we annotated and classified the assays part of ChemBank projects to be able to compare them in terms of their biological relatedness.
We first classified the assays into experiment and control, according to whether the activity measured in the assay was the intended biological activity of the project or unspecific activities, respectively (Fig.2a).
In the second place, we classified the assays into cell-free, cell-based and microorganism based on the biological object of the experiments (Fig.2a) (see Section 2).
Lastly, we annotated the molecular activities and biological processes measured in the projects by assigning manually specific Gene Ontology (GO) (Ashburner et al., 2000) terms (biological process for phenotypic assays or molecular function for cell-free assays) to the projects Fig.2a).
As an additional description of the activity tested in projects, we manually assigned suitable keywords representing protein/gene names or biological processes to the projects (Fig.2a).
The dis-tribution of the number of manual GO and keywords assigned to projects is listed in Supplementary Table S1.
We then propagated the GO terms and keywords of each project to its experiment assays.
We observed that the projects differ both in the number of assays (ranging from 1 to 113, Fig.2b) and the percentage of experiment assays (Fig.2c) they include.
This observation underlines the heterogeneity of the composition of ChemBank dataset.
The distribution of cell-free, cell-based and microorgan-ism assays is also heterogeneous.
More than 40% of the projects are composed of phenotypic assays (cell-based and microorgan-ism), and the majority of them are cell-based assays (Fig.2d, also see Supplementary Fig.S2).
Interestingly, despite the inhomo-geneity of the ChemBank dataset, we found that 80% of the assays have41000 tested compounds (Fig.2e) in common, indi-cating that the different assays can be compared based on the activity of a large number of compounds.
3.2 Promiscuity filters and similarity in biological activity Next, we tested the hypothesis of whether chemical screening assays belonging to different projects with a similar chemical hit profile are biologically related.
To that aim, we applied the Lin measurement (Lin, 1998) that quantifies the semantic simi-larity between GO terms assigned to the assays.
Additionally, we applied the biomedical text-mining tool EXtraction of Classified Entities and Relations from Biomedical Texts (EXCERBT) (Mewes et al., 2011) that detects terms co-mentioned in abstracts of scientific literature to evaluate whether the keywords linked to the assays of the pair are related.
Afterwards, for every assay and with the set of compounds that show activity in at least two projects (Filter 1, F1) (Fig.3, F1), we constructed a binary fingerprint vector representing the activity of the set of compounds in the assays (1 active chemical (a) (b) (d) (c) (e) Fig.2.
Data structure of the ChemBank repository.
(a) Classification of the different projects.
Grey dots represent inactive compounds, while green dots represent active hits in the assay.
Asterisks indicate that the hit is specific to the experiment assay.
(b) Distribution of the number of assays in projects.
(c) Distribution of experimental assays in projects.
(d) Distribution of cell-free, cell-based and microorganism assays in projects.
(e) Percentage of assays sharing tested compounds i581 Unveiling new biological relationships using shared hits of chemical screening assay pairs , Methods , `` '' `` '' , Methods in order `` '' `` '' Methodsapproximately more than , `` '' if  hit, 0 inactive).
Next, for all possible pairs of experiment type assays belonging to different projects, we calculated the chemical hit similarity using a continuous Tc.
Under these conditions, chemical hit similarity appeared not to be related to similar biological activities of assay pairs (Fig.4a and b, F1).
We rea-soned that promiscuous compounds might be responsible for the high chemical hit similarity in unrelated assays.
The prevalence of non-specific or promiscuous compounds is a well-known problem in high-throughput screening (HTS) assays commonly explained by their ability to form aggregates and act on unrelated targets (Feng et al., 2005).
Thus, their presence might be especially disturbing for the detection of biological connections between assay pairs.
Based on this assumption, we tested if the removal of promis-cuous compounds increases the biological relatedness for assays sharing hits.
To that aim, we applied two promiscuity filters.
The first filter retained compounds with activity observed in520% of the projects (Fig.3, F2) and the second filter (F3) kept com-pounds that are active in520% of the assays within a project.
To avoid discarding specific chemical hits in projects with low number of assays where experiment assays represent420% of all assays, the filter F3 was applied only to projects with at least nine assays (Fig.3, F3) (see Section 2).
For example, the latter filter would discard all specific chemical hits in projects com-posed of one experiment and one control assay like the project Glioblastoma Modulators (Fig.2a) that searched for PI3K and mTOR modifiers in glioblastoma cells.
If applied to this project, this filter would remove all specific hits, that is, those compounds that are active in cells treated with rapamycin (experiment) and inactive in cells not treated with the mTOR inhibitor (control), as they are active on 50% (420%) of the assays in this project.
As can be observed in Figure 4a and b, only after the application of the most stringent promiscuity filter F3, a linear relationship between hit similarity and known biological relationships was observed.
Interestingly, such relationship disappeared when we compared assays with random hits, reinforcing the reliability of the relationships between biological activities captured by this approach.
Furthermore, this trend became stronger when we discarded combinations of assays shar-ing low number of hits (Fig.4a and b, number of shared hits 5, also see Supplementary Fig.S3a and b), indicating that the larger the number of common chemical hits is, the more likely it is to capture biological relationships between assays.
An example of a known relationship between assay pair captured with our approach is the Bacterial ViabilityAntibacterial assay pair.
This pair has a hit similarity of 0.54 (it shares 25 hits of the 51 and 25 tested compounds in each assay, respectively) and a biological similarity of 1 (the same GO:0016049 cell growth term was annotated to both assays).
3.3 Assay interaction network Next, we visualized and inspected manually the assay pairs show-ing high chemical hit similarity.
For that, we constructed an assay interaction network with the assay pairs showing the high-est hit similarity (Tc40.4) and sharing five or more chemical hits.
This network contains 32 nodes and 26 edges (Fig.5).
Interestingly, 92% of the edges in the network connect assays of the same experimental type.
That is, phenotypic assays share hits with other phenotypic assays and cell-free assays tend to share hits with other assays of the same type.
We found, for instance, a group of four interconnected assay pairs of the micro-organism type (i.e.
Bacterial Viability, SigB Inhibition, Worm Anti-Infective and Anti-Bacterial assays) where the same biological activity, that is, the antibacterial activity, was sought in all of them.
An example of a connection of two clearly related cell-free assays is the link between Kinesin Activity Eg5 and Kinesin Activity MKLP1 comprised by two assays aiming to find inhibitors of proteins of the Kinesin family.
These instances provide evidence that relationships between the biolo-gical activities measured in the assays can be captured by our approach.
Intriguingly, we found a high number of edges (11, represent-ing 42% of the edges) connecting control assays to experiment assays, the majority of them (9) linking two cell-based assays.
(a) (b) Fig.4.
Correlation between hit similarity and known relationships of ChemBank assay pairs.
Hit similarity was calculated by continuous Tc.
(a) Relationships indicated by GO terms and (b) relationships indicated by text mining.
Each point in the plot represents a bin of assay pairs according to the sorted Tc values.
In F1, each bin contains 1000 assay pairs.
Bins in F2 and F3 contain 500 and 100 pairs, respectively.
Separately, the performance of assay pairs in F3 sharing five or more hits is shown for (a) and (b) Fig.3.
Promiscuity filters.
F0 contains all the compounds of the dataset.
F1 keeps the compounds active in at least one project, and F2 retrieves the compounds active in 20% of the projects.
F3 retains compounds active in 20% of the assays for the projects with higher than average number of assays (average number of assays per project is 9 for ChemBank).
The number of remaining compounds after filtering is given in brackets i582 X.Liu and M.Campillos `` '' less than less than `` '' more than Methods `` '' `` '' `` '' since 4 >= out `` '' `` '' `` '' `` '' `` '' `` '' `` '' `` '' `` '' `` '' A closer inspection of the activities measured in these assays in-dicates that cell growth-related processes, such as differentiation or growth inhibition, were often measured in the assays as the sought activity, for example, in assays seeking for chemicals with anticancer activity or in assays controlling the cytotoxicity of com-pounds.
To gain deeper insights into the molecular basis of these assay combinations, we extracted molecular information of the chemical hits shared by these pairs by annotating predicted human drug targets of the compounds.
For that, we applied the HitPick target prediction method (Liu et al., 2013) to predict the molecular targets of hits with high confidence (precision450%).
Interestingly, we found the same predicted drug targets related to several assay pairs.
For example, compounds specifically target-ing the glucocorticoid receptor (NR3C1) are active in four con-secutive assays in the network, namely Mycobacterium tuberculosis (M.tuberculosis) Macrophage, Gamma Secretase Inhibitor (GSI) Synthetic Lethal (Cell growth), Adipocyte Differentiation and Unfolded Protein Response (UPR) (Fig.6a).
The role of NR3C1 in macrophages as the target of anti-inflammatory agents (Barnes, 1998) and its anticancer activity (Cook et al., 1988) provides an explanation for the molecular basis of the relationship between the M.tuberculosis Macrophage that screened for inhibitors of M.tuberculosis growth in macrophages and GSI Synthetic Lethal (Cell growth), a control assay that tested the growth inhibitory activity of molecules in T-cells.
Moreover, the known ability of NR3C1 to induce adipocyte differentiation (Xu et al., 1990) explains the common link between the cell growth and differentiation activ-ities measured in GSI Synthetic Lethal (Cell growth) and Adipocyte Differentiation assays, respectively.
Interestingly, although the link between UPR and differentiation processes has been proposed in the literature (Hetz, 2012), the molecular basis of this connection is not fully understood.
Here, our results suggest the function of NR3C1 as intermediary between UPR induction and differentiation.
However, this proposal should be taken with caution, as the specificity of the chemical hits on UPR process cannot be assessed owing to the lack of control assays in the project.
In this context, the UPR assay is linked to a control assay of the Wnt Inhibitors (Wnt mutated vector) project, which measures the promoter activity of a mutated version of Wnt responsive construct (Fig.6b).
A closer look at this relationship reveals that ATP1A1 (ATPase, Na+/K+ transporting, alpha 1 polypeptide), CYP1B1 (cytochrome P450, family 1, subfamily B, polypeptide 1) and ADORA2B (adenosine A2b receptor) are the predicted targets of the chemical hits of this pair.
The role in cancer of ATP1A1 (Newman et al., 2008), CYP1B1 (Gajjar et al., 2012) and ADORA2B (Ma et al., 2010) indicate that the activity of compounds in the Wnt Inhibitors (Wnt mutated vec-tor) assay is likely due to their cytotoxicity.
Although the known role of UPR to induce cell cycle arrest (Brewer and Diehl, 2000) and the recently reported role of ouabain, specific inhibitor of ATP1A1, on the modulation of UPR (Ozdemir et al., 2012), would suggest that the relationship between this assay pair is Fig.6.
Enriched targets between assay pairs.
(ad) are examples of assay connections (shown by assay name).
The size of each pie chart is propor-tional to the logarithm of the number of shared hits.
For simplicity, in the pie charts we show the most frequently predicted targets (with a precision 450%) of the shared chemical hits (see Supplementary Table S2 for the full target list of each assay pair in Fig.6).
The fraction of the pie charts representing hits with no predicted targets is shown in white as No Information.
In Figure 6c, only those representative targets common to three hits for assays pairs in the group are shown, and the remaining targets common to 2 hits are shown in black as Others Fig.5.
Network of assay pairs from ChemBank repository sharing selective hits i583 Unveiling new biological relationships using shared hits of chemical screening assay pairs `` '' `` '' `` '' `` '' `` '' , `` '' `` '' `` '' `` '' due `` '' that , `` '' due due to the UPR-dependent growth inhibitory activity, further research is needed to assess the specificity of the shared hits on the UPR assay.
The growth inhibition measured in the Wnt Inhibitors (Wnt mutated vector) assay is further confirmed by the association of this assay with the anticancer Gliobastoma Modulators and Genotype-Specific Inhibitors in Non-Small Cell Lung Cancer assays (Fig.6c).
Our target prediction approach revealed that, within this group of growth inhibitory assays, the cytotoxic activity is partly mediated through well-known anticancer tar-gets, such as histone deacetylases (HDACs) (Wagner et al., 2010), ATP1A1 (Newman et al., 2008), farnesyltransferase, CAAX box, alpha (FNTA) (Rowinsky et al., 1999) and mouse double minute 2 homolog (MDM2) (Shangary and Wang, 2008).
Furthermore, the modulation of these targets also explains the link between the chemical screens measuring stem cell differenti-ation [Stem Cell Differentiation (Cell count) assay] and DNA methylation [by 4,6-diamidino-2-phenylidole staining in Histone Modification (DNAmethylation) assay].
Intriguingly, other pre-dicted targets behind the growth inhibition activity in this group of cancer-related assays include adenosine receptor A3 (ADORA3), cannabinoid receptor 2 (CNR2), cholesteryl ester transfer protein, plasma (CETP), 5-hydroxytryptamine receptor 6 (HTR6) and ATPase, Ca2+ transporting cardiac muscle, fast twitch 1 (ATP2A1).
The modulation of these targets in antic-ancer screens suggests the possible role of these proteins in growth inhibition.
In fact, the activity of ADORA3 as a potential target for tumor growth inhibition has been proposed before (Madi et al., 2004).
Another well-known biological connection is represented by the link between Beta-Catenin assay that measured the nuclear translocation of beta-catenin and Histone Modification (DNA methylation) assay (Fig.6d).
HDAC, the predicted target of the common hits, has been shown to inhibit Wnt signalling through disruption of the interaction between beta-catenin and T cell factor (Ye et al., 2009).
Thus, the biological relationship between these two assays is explained by the known relationship of HDACs.
In summary, after retrieving the chemical hits from the ChemBank assays, we observed that the biological activities mea-sured in two assays sharing selective hits are related.
The close inspection of the assay pairs sharing specific hits in the network is able to confirm the biological and molecular associations of assay pairs and reveal molecular information underling the shared activity.
4 DISCUSSION In this work, we have integrated and analysed the information stored in ChemBank and demonstrated that the biological activities of assay pairs sharing selective chemical hits are often related.
The relationships between the biological processes of phenotypic assays are furthermore supported by the role of protein targets predicted for the shared hits.
Fingerprint-based approaches, where profiles of a collection of predefined features of an object such as a compound or protein is compared, have often been exploited in Chemistry and Biology fields to infer properties of compounds (Willett et al., 1998) (Willett, 2000) and genes (Liu et al., 2013).
These approaches are based on the observation that similar fingerprint profiles correlate with similar properties (Fan et al., 2006).
For example, compounds with similar chemical fingerprint profiles tend to have similar biological activities (Petrone et al., 2012).
Likewise, compounds with similar modes of action have also been observed to exhibit similar behavior across multiple assays (Danck et al., 2014).
In contrast, in this study we use chemical hit-based fingerprints constructed with selective compounds to infer biological relationships between assays.
Interestingly, we show that the relationships between assays can only be captured when a stringent selectivity filter is applied to discard promiscuous compounds from the chemical hit profile.
Currently, there is no consensus for the definition of compound promiscuity, and different promiscuity filters have been proposed in the literature.
Sch urer et al.
(2011) and Jacob et al.
(2012) defined promiscuous compounds as those showing activity in450 or 30% of the assays, respectively, while Gamo et al.
(2010) calculated an inhibition frequency index for each compound and applied a variable threshold, ranging from 5 to 20% of screens, depending on the number of HTS screens a given compound had been through.
Although these studies have revealed interesting chemical moieties associated to unspe-cific signals in chemicals screens, the question of what level of selectivity is necessary to capture hits carrying information about specific biological signals has not been addressed yet.
In this study, we have shown that a stringent promiscuity filter that first selects hits active in520% of the projects (filter F2) and subsequently retains compounds with activity in520% of the assays within a project (filter F3) is necessary to enrich for hits with specific biological activities.
We reason that the low number of projects performed in the same experimental backgrounds generating the same unspecific signals might be the cause for the lack of correlation between hit and similarity of biological activities of two assays after the application of filter F2.
Although this is partially overcome by discarding compounds active in several assays of the same project and consequently, performed in similar experimental backgrounds (filter F3), our approach also detects connections between cell-free assays that are apparently unrelated.
For example, the Phospholypid Hydrolysis assay is associated to the Deubiquitilation assay (Fig.5).
A closer look at this connection reveals artefactual yet non-promiscuous hits, as the shared hits of the two connections appear active in the control assays of the project (termed unspe-cific chemical hits, see Fig.2a).
This indicates that the stringent promiscuity filters applied here might, for some experimental conditions, be insufficient to discard unspecific hits, and add-itional control assays might be necessary to remove non-selective chemical hits.
The presence of unspecific hits is also evidenced by the occur-rence of edges that connect control and experiment assays.
For example, the e-Cadherin Synthetic Lethal (Cell growth) con-trol assay that controlled for the cytotoxicity of compounds in the human mammary epithelial HMLE cell line is connected to the Wnt And Lithum Modulators (Wnt vector) experiment assay (Fig.5), suggesting that the shared hits of the pair are not specific of the Wnt signalling process.
This hypothesis is further corroborated by the known or suspected anticancer activity of the predicted targets [HDAC1 (Wagner et al., 2010), FNTA (Rowinsky et al., 1999) and sigma non-opioid i584 X.Liu and M.Campillos `` ''-`` '' `` (NSCLC) '' `` '' , (DAPI) `` '' HTR6 `` '' `` '' z more than &percnt; less than less than `` '' `` '' i `` '' `` '' `` '' `` '' `` '' `` '' `` ''-( intracellular receptor 1 (SGIMAR1) (Aydar et al., 2006), Supplementary Table S2] of the shared hits and the modulation of these targets in a control assay of Wnt Inhibitors (Wnt mutated vector) (Fig.6c, also see Supplementary Table S2).
Similarly, the link between the cytotoxic control assay of the e-Cadherin synthetic lethal (Cell growth) project and the Translation Inhibition (Dengue replicon translation) assay that detected inhibitors of the translation of Dengue virus repli-con (Fig.5) points to the unspecificity of the chemical hits in the Translation Inhibition assay.
These examples illustrate the need of additional control assays in these screening projects to assess the specificity of the compounds.
Nonetheless, we show that this approach was able to capture meaningful biological connections even between different types of assays, such as the link between a microorganism assay with a cellular assay.
For example, the microorganism Anti-Bacterial assay is connected with cellular M.tuberculosis Inhibition assay performed in BG1 ovarian cancer cells.
We observe that many relationships between different phe-notypic assays are established based on the shared cytotoxicity of compounds in cell-or whole organism-based assays.
Cytotoxicity appears thus as underlying biological effect common to phenotypic assays that account for the activity of many hits in these assays.
Interestingly, the target prediction for those non-promiscuous but cytotoxic compounds reveals targets of drugs used as anticancer therapies, such as the HDACs (Wagner et al., 2010) and ATP1A1 (Newman et al., 2008), or targets that have been proposed for cancer treatment such as FNTA (Rowinsky et al., 1999) and MDM2 (Shangary and Wang, 2008).
Hence, other predicted targets connecting these assays might represent potential targets for the treatment of cancers, such as CNR2, CETP, HTR6, ATP2A1 and ADORA3.
Indeed, ADORA3 has been proposed as a potential therapeutic cancer target (Madi et al., 2004).
In summary, this work shows the potential of integrative approaches dealing with high-throughput chemical screening data to reveal novel connections between the biological processes and molecular activities measured in chemical screens.
In the future, with the expected increase in HTS assay data available in public repositories, it is envisioned that many more biological relationships will be discovered with the application of this or similar computational approaches.
ACKNOWLEDGEMENTS The authors gratefully acknowledge Dr Benedikt Wachinger and Dr Volker Stumpflen from Clueda AG for the support of pro-viding the EXCERBT text-mining tool.
We also acknowledge members of SBSM group at the Helmholtz Center Munich for helpful discussions and the support of the TUM Graduate Schools Faculty Graduate Center Weihenstephan at the Technische Universitat M unchen, Germany.
Funding: This study was supported in part by a grant from the German Federal Ministry of Education and Research (BMBF) to the German Center for Diabetes research (DZD e.V).
Conflict of Interest: none declared.
ABSTRACT Motivation: A large number of experimental studies on ageing focus on the effects of genetic perturbations of the insulin/insulin-like growth factor signalling pathway (IIS) on lifespan.
Short-lived invertebrate la-boratory model organisms are extensively used to quickly identify ageing-related genes and pathways.
It is important to extrapolate this knowledge to longer lived mammalian organisms, such as mouse and eventually human, where such analyses are difficult or impossible to perform.
Computational tools are needed to integrate and manipulate pathway knowledge in different species.
Results: We performed a literature review and curation of the IIS and target of rapamycin signalling pathways in Mus Musculus.
We com-pare this pathway model to the equivalent models in Drosophila mel-anogaster and Caenorhabtitis elegans.
Although generally well-conserved, they exhibit important differences.
In general, the worm and mouse pathways include a larger number of feedback loops and interactions than the fly.
We identify functional orthologues that share similar molecular interactions, but have moderate sequence similarity.
Finally, we incorporate the mouse model into the web-ser-vice NetEffects and perform in silico gene perturbations of IIS com-ponents and analyses of experimental results.
We identify sub-paths that, given a mutation in an IIS component, could potentially antagon-ize the primary effects on ageing via FOXO in mouse and via SKN-1 in worm.
Finally, we explore the effects of FOXO knockouts in three dif-ferent mouse tissues.
Availability and implementation: http://www.ebi.ac.uk/thornton-srv/ software/NetEffects Contact: ip8@sanger.ac.uk or thornton@ebi.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on May 21, 2014; revised on June 24, 2014; accepted on July 15, 2014 1 INTRODUCTION The insulin/insulin-like growth factor signalling pathway (IIS) and the target of rapamycin (TOR) signalling pathway have been shown to be important regulators of ageing across species via the transcription factor FOXO (Kenyon, 2011).
The mechanisms by which FOXO increased activity leads to lifespan extension are still unclear.
However, it is thought that lifespan extension is achieved through cell-cycle arrest by FOXO in the absence of insulin signalling (van der Horst and Burgering, 2007).
In addition, identification of FOXO transcriptional tar-gets has revealed a second tier of transcription factors regulating a variety of downstream responses (Alic et al., 2011).
Ageing via the IIS pathway has been intensively studied at the level of in-vertebrate model laboratory organisms.
With their short life-spans, well-described genomes and a variety of mutants already available, Drosophila melanogaster (Clancy et al., 2001) and Caenorhabtitis elegans (Kenyon et al., 1993) provide excellent frameworks for fast identification of genetic determinants of ageing.
Relating results from fly and worm to a longer lived mammalian model, such as Mus musculus (Bl uher et al., 2003), is critical for the understanding of the ageing processes in human, but it is often difficult owing to the large evolutionary distance between invertebrates and mammals.
The general flow of the pathway is as follows.
The insulin and insulin growth factor receptors can be activated by two different insulin molecules or two insulin-like growth factor molecules.
On activation, the two receptors can activate the insulin receptor substrates (IRS1-4) by tyrosine phosphorylation.
The role of IRS1 especially has been well examined and found to propagate the signal further downstream, via the PI3K complex.
The phospholipid products of PI3K [phosphatidylinositol-3,4,5-tri-phosphate (PIP3)], once produced, can activate phosphoinosi-tide-dependent kinase-1 (PDK1) that leads to the activation of AKT/protein kinase B-like proteins (AKT1-3, with AKT1 being well studied) and serum and glucocorticoid-inducible kinases (SGK1-3).
AKT1 and the SGK1-3 kinases inhibit the activity of the Forkhead transcription factors FOXO by retaining them in the cytoplasm.
The IIS pathway is generally well conserved, with the main building blocks (INSR, PI3K, PDK1, AKT, FOXO) present in both mammals and invertebrates.
The TOR pathway is also well conserved with its main building blocks present (TOR complexes 1 and 2, RHEB, S6 kinase).
Important differences also exist.
There are seven known insulin molecules in the fly, as opposed to 40 in the worm.
In the mouse, there are two insulin and two insulin-like growth factors.
Flies and worms possess a single in-sulin receptor each, whereas mice possess two insulin-like growth factor receptors in addition to the insulin receptor.
In mice, we *To whom correspondence should be addressed.
yPresent address: Mouse Informatics Group, Welcome Trust Sanger Institute, Wellcome Trust Genome Campus, Hinxton, Cambridge CB10 1SD, UK The Author 2014.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.
due Up ( ( )-Transcription Factors very-forty XPath error Undefined namespace prefix observe more copies of certain proteins, such as four copies of AKT (Akt1-4) instead of one in the fly and two in the worm and four genes encoding FOXO (Foxo1, Foxo2, Foxo4 and Foxo6) in contrast to a single FOXO in flies and worms (foxo and daf-16).
The TOR pathway is remarkably similar between flies and mice, but exhibits differences with the worm (Ivanov et al., 2013; Riesen et al., 2014).
Previously, we developed the web-service NetEffects to solve the problem of relating gene expression results to their effects at the protein signalling level of the IIS pathway and the ageing phenotype (Papatheodorou et al., 2012).
This is a common problem of many studies on ageing that use whole organism mutants to study changes in lifespan and uncover protein interactions within the signalling pathways by use of transcriptomic datasets.
NetEffects uses Answer Set Programming, a logic-based method for inference that uses manually curated maps of the IIS pathway and its relationship to lifespan as prior knowledge.
Given a gene mutation and the resulting genome-wide differential gene expression, NetEffects will deduce signalling effects and how they can influence lifespan according to the prior knowledge.
Our applications on fly and worm datasets (Ivanov et al., 2013; Papatheodorou et al., 2012) have revealed consistent homeostatic mechanisms across both long-and short-lived mutants.
2 METHODS The signalling network model of the IIS and TOR pathways was built using GraphML, using the editor yEd (http://www.yworks.com).
This enabled a computationally readable representation of the pathways, as well as a graphical visualization that relates molecular topology to cellular components.
During the curation process, we used the insulin pathway available in KEGG (Kanehisa et al., 2012) as a starting point and the rest of the connections were built by literature review.
Literature searches were performed using PubMed (http://www.ncbi.
nlm.nih.gov/pubmed/) by querying mouse gene names and identifiers.
Only connections with experimental support, rather than just suggestive or hypothetical, were used.
Proteinprotein interactions from yeast two-hybrid system or other screens were omitted.
Supporting literature for each relationship is available by clicking on the pathway connections within NetEffects.
All graphs from the comparative analyses across the fly, worm and mouse pathways were produced by custom-made R (http://www.r-pro-ject.org/) scripts.
These cross-species analyses were based on genomic sequence-based orthologous relationships, downloaded from Ensembl Compara (http://www.ensembl.org).
Orthology relationships that were not predicted by Compara but were suggested by the topology and con-nectivity of the genes in the pathway models, were sought in TreeFam (http://www.treefam.org/), OrthoDB (http://cegg.unige.ch/orthodb7) and Phylome (http://phylomedb.org/).
The mouse pathway was incorporated to the web-service NetEffects, built using PHP (PHP: Hypertext Preprocessor), JavaScript and Perl.
Proteins on the pathway have been annotated with Ensembl identifiers to enable the import and analysis of experimental datasets.
The theoretical perturbations option of NetEffects was used to query the pathway and produce inferences on the possible paths to longevity from different mutations.
The Experimental Results Analysis option was used to analyse the gene expression datasets.
Raw files of the expres-sion datasets in Paik et al.
(2007) were analysed using the Limma package in R (Smyth, 2005).
3 RESULTS AND DISCUSSION 3.1 A model of the insulin and TOR signalling pathways for M.musculus We curated a model of the IIS, TOR and neighbouring pathways in the mouse, providing access to the underlying literature as clickable connections on the pathway within the web-service.
We found the IIS pathway to be well connected and to include several points of cross-talks with neighbouring pathways.
This enables it to respond to signals from a variety of sources rather than just extracellular insulin molecules.
IRS and SHC1 bind to GRB2, which then activates the MAPK/ERK pathway.
In add-ition to propagating insulin signals, IRS also receives feedback from other pathways, such as TOR by inhibition from S6 kinase (gene name Rps6kb1), JNK by inhibition from JNK1 and Wnt signalling through an inhibition by GSK3-beta.
SHC1 is phos-phorylated by the activated Insulin receptor, propagating the signal to MAPK/ERK via GRB2, SOS1 and SOS2.
AKT1 pro-vides another point of cross-talk with the TOR pathway, by directly inhibiting PRAS40, which then inhibits RPTOR, inhibit-ing TSC2.
AKT1 can also be activated by TOR complex 2.
Previous work on mouse mutants of the IIS and TOR signal-ling pathways have clearly shown a role for the insulin pathway in the regulation of lifespan, with null S6K (Selman et al., 2009) mice and null IRS-1 (Selman et al., 2008) mice showing signifi-cant lifespan extension when compared with wild-type.
Although there is so far no experimental confirmation of lifespan regula-tion by mammalian FOXOs, results from mice lacking one or more of FoxO1, FoxO3 and FoxO4 have revealed ageing-related phenotypes, such as reduced bone mass (Ambrogini et al., 2010) and the development of ageing-related diseases like thymic lymphomas, hemangiomas (Paik et al., 2007).
These results sug-gest that mammalian FoxOs play a protective role against age-related diseases.
In addition, Willcox et al.
(2008) provide evi-dence for FoxO3A genetic variation being associated with life-span in a large, well-phenotyped cohort of humans through a casecontrol study of five candidate genes.
3.2 Comparison of insulin and TOR signalling in fly, worm and mouse With the availability of thoroughly curated pathway models for each of the three species, we are now able to make comparisons of their components and connections.
Supplementary Table S2 summarizes the similarities and differences between the IIS and TOR pathway molecules across the three different species.
Figure 1 presents on the mouse IIS and TOR pathway model the occurrence of fly and/or worm orthologues, also present in the species-specific models.
Functional orthologues were also identified, where sequence similarity across species is moderate but molecular interactions and experimental evidence suggest that these pairs are indeed orthologues.
Such cases include ist-1, a worm orthologue to the IRS (OrthoDB); drr-2, a worm orthologue to eukaryotic translation initiation factors 4H (Ching et al., 2010) and 4B (Phylome Orthology); unc-51, worm orthologue to Ulk1/Atg1; let-363, worm orthologue to Mtor (TreeFam) and age-1, worm orthologue to Pik3ca (TreeFam).
Gene Deptor encodes a protein associated with the mammalian TORC1 that is absent from the 3000 I.Papatheodorou et al.
, in order se ; Ivanov etal., 2013visualisation in order `` T P '' `` '' &amp; S P M very , , to ,-5 insulin receptor substrate fly and worm genomes.
Glatter et al.
(2011) hypothesized that the gene appeared later in vertebrate evolution.
In general, the IIS and TOR pathways in the mouse appear considerably larger and with more cross-talk (see Supplementary Section S3).
This effect is partly because of the fact that the IIS and TOR pathways in the mouse have been more thoroughly studied, as well as to the different extents of the curation of the neighbouring pathways within each organism.
Being used as a model organism for studies on human diseases such as cancer and diabetes and with the availability of a large number of murine cell lines, more interactions within and between the IIS, TOR and their neighbouring pathways have been discovered.
Cross-talk points between the IIS and TOR pathway include interactions between AKT1 and TORC2 in all three organisms, as well as RPS6KBA (S6 kinase) and IRS1 in mouse and fly.
In the mouse, both AKT and IRS involve several paralogous copies.
The IRS genes in the mouse are also involved in the cross-talk between JNK and IIS pathways, an interaction that is conserved across species.
In some cases, the interactions be-tween neighbouring pathways are not conserved due to the lack of orthologues in worms or flies or both.
For example, there is no orthologue for TSC2 in the worm which is inhibited by AKT in flies and mice.
There are also cases where orthologues in the invertebrates exist, but the interactions have not been observed experimentally as exemplified by the interaction of IIS and MAPK/ERK pathways via GRB2 and SOS1.
Finally in the case of MAPK/ERK to TOR cross-talk, facilitated by the acti-vation of RPS6KA1 by MAPK1 in the mouse, we found ortho-logues in both other organisms but no interaction in the fly.
In the worm, there is evidence for a proteinprotein interaction between them (see Supplementary Section S3D for the complete table of cross-talk points).
3.3 Paths to FOXO-mediated longevity with NetEffects We incorporated the mouse pathway model into the web service NetEffects (Papatheodorou et al., 2012) to enable computational analyses.
We can now compare the effects on FOXO-mediated ageing across the three species.
Using the theoretical perturb-ations functionality, we tested the paths to FOXO-mediated longevity in the mouse and worm from already known mutants in flies (see Supplementary Section S4 for full results).
Knocking out Ins1 or Ins2 in mouse results in a path consistent with those obtained when doing the equivalent test in the fly and worm models (Table 1).
This leads to increased lifespan through inhib-ition of AKT, which then allows translocation of FOXO into the nucleus.
However, in mouse and worm, we also encounter paths that reduce longevity.
In the mouse, this path involves the IGF1-receptor and RACK1 (gene Gnb2l) that leads to enhanced AKT phosphorylation and activation.
This effect, however, appears to be cell-type-specific and probably also context-specific, as Fig.1.
The model of the IIS and TOR pathways in M.musculus.
Colours indicate the existence of orthologues in flies, worms, both or none.
A larger version of this model is available in Supplementary Figure S1 3001 Mouse model of IIS and TOR hypothesised s s due to--s s .-in order `` '' s sdescribed in Kiely et al.
(2005).
In the worm, there is also an effect that might be antagonizing the FOXO-mediated lifespan increase, but is mediated by LET-60 (Kras orthologue) and the transcription factor SKN-1.
Similar effects were produced when Igf1r was mutated.
Mutation of Insr had similar results to the equivalent manipulations in the fly and worm.
We also tested known mouse mutants in the IIS pathway that affect lifespan.
The Irs1/ mutant (Selman et al., 2008) results in a long-lived phenotype, as expected from previous knockout ex-periments on the fly orthologue chico (Clancy et al., 2001).
In contrast, the Irs2/ mutant is short-lived (Selman et al., 2008).
NetEffects infers similar paths for both Irs1/ and Irs2/ mu-tants, as they exhibit similar connections to other components of the IIS and TOR pathways (Table 1).
The shortest path that leads to increase in lifespan involves inhibition of AKT.
The shortest paths leading to a reduction of lifespan require increased activity of GSK3B leading to inhibition of FOXO via SIRT1 and E2F1.
Functional experiments, as well as genome-wide gene expression in the two mutants could show whether Irs2/ mutant mice reduce their lifespan via a different route, and whether the activity of GSK3B plays a role.
We also tested ist-1, the func-tional worm orthologue for Irs1 and Irs2, where a knockout experiment with lifespan analysis has not been performed.
In addition to the FOXO-mediated lifespan extension, we obtained a sub-path of the same antagonistic effect via SKN-1 as in the INS1/2 tests shown above.
Partial support for the opposing effect of LET-60 (RAS) via SKN-1 to FOXO-mediated lifespan extension comes from a study on long-lived age-1 (PI3K) worms, where downregulation of let-60 and skn-1 genes was observed (Tazearslan et al., 2009).
Finally, we analysed the expression datasets in cells derived from three different tissues of null and conditional alleles in the three main Foxo genes (FoxO1/+; FoxO3/; FoxO4/).
The datasets were generated by Paik et al.
(2007).
We analysed the Foxomutants in liver and lung endothelial cells and thymus cells.
According to the authors, liver cells presented cancer-related phenotypes, whereas lung cells did not present a detectable phenotype.
With NetEffects we can show that 16 genes within the pathway model are differentially expressed in liver, one in lung and three in thymus (excluding the three Foxo genes that have been knocked out).
Almost all of the shortest paths starting from these differentially expressed genes correspond to negative feedback to the mutation of Foxo genes, as shown by the pre-dicted impact on lifespan (Supplementary Section S5).
This sug-gests that the function of the three Foxo genes plays a greater part in the liver and thymus rather than in lung endothelial cells.
Similar negative feedback was identified in our previous analyses of Foxo null mutants in the fly Papatheodorou et al.
(2012).
4 CONCLUSION The molecular basis of nutrient signalling is largely comparable across a large evolutionary space, despite striking differences in the presence or number of copies of certain components between species.
The pathways, in all organisms except worm focus only on FOXO-mediated lifespan, thereby ignoring any effects through different transcription factors.
However, using the mouse pathway, we can identify neighbouring signalling path-ways with the potential to influence the signal transduction of the IIS through the identified cross-talk points.
Comparison of the pathways in a systematic and qualitative way has the po-tential to explain differences in effects on lifespan across species and help design experiments that will evaluate the pathway flux.
The richness and detail of the mammalian model can inform the interpretation of results in invertebrates, where molecular interactions have not been so extensively studied.
Being able to compare the pathways side by side, we recorded all differences and identified functional orthologues.
By use of NetEffects, we were able to suggest possible paths affecting FOXO-mediated Table 1.
Shortest paths to FOXO-mediated longevity, derived from NetEffects, where DAF-16 is the FOXO orthologue in C.elegans 3002 I.Papatheodorou et al.
s--&Unicode_x2215;--, ,--&Unicode_x2215;----&Unicode_x2215;----&Unicode_x2215;----&Unicode_x2215;--knock-&Unicode_x2215;--&Unicode_x2215;+--&Unicode_x2215;----/--1 3lifespan given a single mutation and how these differ across spe-cies, thus generating hypotheses for further investigation.
ACKNOWLEDGEMENTS The authors thank Dobril Ivanov and Matthias Ziehm for useful discussions.
Funding: This work was funded by the Wellcome Trust Strategic Award WT081394MA (I.P., J.M.T.)
and by the European Molecular Biology Laboratory (EMBL) (R.P., J.M.T.).
Conflict of interest: none declared.
ABSTRACT Summary: Bisulfite sequencing allows cytosine methylation, an important epigenetic marker, to be detected via nucleotide substitutions.
Since the Applied Biosystems SOLiD System uses a unique di-base encoding that increases confidence in the detection of nucleotide substitutions, it is a potentially advantageous platform for this application.
However, the di-base encoding also makes reads with many nucleotide substitutions difficult to align to a reference sequence with existing tools, preventing the platforms potential utility for bisulfite sequencing from being realized.
Here, we present SOCS-B, a reference-based, un-gapped alignment algorithm for the SOLiD System that is tolerant of both bisulfite-induced nucleotide substitutions and a parametric number of sequencing errors, facilitating bisulfite sequencing on this platform.
An implementation of the algorithm has been integrated with the previously reported SOCS alignment tool, and was used to align CpG methylation-enriched Arabidopsis thaliana bisulfite sequence data, exhibiting a 2-fold increase in sensitivity compared to existing methods for aligning SOLiD bisulfite data.
Availability: Executables, source code, and sample data are available at http://solidsoftwaretools.com/gf/project/socs/ Contact: bergmann@nbacc.net Supplementary information: Supplementary data are available at Bioinformatics online.
Received on March 19, 2010; revised on May 28, 2010; accepted on May 28, 2010 Cytosine methylation is a major epigenetic marker in eukaryotes, performing functions such as transcriptional regulation and transposon silencing.
It is now possible to create genome-wide maps of this type of DNA modification at single-nucleotide resolution using a technique termed bisulfite sequencing, or BS-Seq.
The method utilizes high-throughput sequencing technologies in conjunction with selective nucleotide substitutions.
These substitutions are induced by bisulfite, which converts cytosine residues to uracil residues, but occurs at a much slower rate for 5-methylcytosine (the most common type of methylated cytosine).
After an appropriate amount of bisulfite treatment and subsequent PCR amplification, 5-methycytosine residues will be represented by To whom correspondence should be addressed.
cytosine (or guanine on the complementary strand) and cytosine residues by thymine (or adenine on the complementary strand).
By sequencing the converted DNA and aligning with a reference sequence, methylation can be inferred from these substitutions (Frommer et al., 1992).
While any sequencing method can be employed for BS-Seq, the Applied Biosystems SOLiD System is attractive for this application because it is designed around the reliable detection of nucleotide substitutions.
This reliability arises not from the absence of sequencing errors, but from the ability to discern most of them from true substitutions.
The system achieves this by querying overlapping dinucleotides rather than single nucleotides, such that each nucleotide of each read is ultimately queried by two independent ligation events.
The caveat is that each dinucleotide is reported as a color that could represent any of the four dinucleotides, and alignment must be performed using these colors (in color-space) in order for sequencing errors to be distinguished.
In color-space, a nucleotide substitution appears as a specific pattern of two adjacent color-space mismatches.
This increased divergence is not a major problem for applications in which reads will typically only contain one nucleotide substitution, such as single nucleotide polymorphism (SNP) detection.
However, in BS-Seq, bisulfite-induced nucleotide substitutions (BINS) are ubiquitous, causing most reads to contain too many color-space mismatches relative to the reference sequence to be aligned using the standard color-space alignment tools.
There are ways to avoid this issue and align a portion of the reads with standard tools.
One is to create reference sequences that represent the original sequence and complete bisulfite conversion of both the Watson and Crick strands of the sequence, as done in previous bisulfite experiments (Lister et al., 2008).
Reads can then be aligned using existing SOLiD alignment tools.
The problem with this approach is that reads containing both methylated and unmethylated cytosines could contain numerous color-space mismatches against either reference sequence.
Since areas such as CpG islands are dense with potential methylation sites, many reads that are of particular interest would not be aligned within realistic error tolerances.
The other possible method is to convert SOLiD reads from their di-nucleotide encoding into nucleotide strings.
The reads can then be aligned with an existing tool that is tolerant of BINS, such as the one developed by Cokus et al.
(2008).
The issue here is that the conversion process assumes the absence of sequencing errors, The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[16:44 30/6/2010 Bioinformatics-btq291.tex] Page: 1902 19011902 B.D.Ondov et al.
causing the majority of the reads to be converted incorrectly.
Both of these methods ignore much of the information contained in SOLiD output, and thus do not realize the potential of this platform for BS-Seq.
Ideally, an alignment tool for this application should be tolerant of BINS as well as some sequencing errors.
Here, we present SOCS-B, an alignment algorithm tolerant of both bisulfite-induced nucleotide substitutions and SOLiD sequencing errors, facilitating BS-Seq using the SOLiD system.
The algorithm is based on the iterative version of the RabinKarp algorithm that is the foundation of SOCS (Karp and Rabin, 1987; Ondov et al., 2008).
The first phase of the algorithm is the creation of a hash table to index potential matches, drastically paring the number of alignments to be performed.
The second phase is the assessment of these potential matches by comparison of the color-space sequence of the reads to the color-space sequence of the reference.
Both phases of SOCS-B are tolerant of both BINS and sequencing errors.
Hashes are created by first translating the color-space reads into nucleotide sequences.
To account for the translational effects of sequencing errors, four translations are computed, starting from all four nucleotides (rather than just the terminal primer base provided with the read).
Then substrings of all four translations are used to generate partial hashes, which are analogous to seeds.
This ensures that the partial hashes based on the correct translations are represented in the hash table, regardless of any sequencing errors that occurred earlier in the reads.
A reduced representation of the translated nucleotides, which treats cytosine and thymine as the same symbol, is enumerated in ternary to form each partial hash.
To assess the quality of each potential alignment discovered in the hash table, BINS must be distinguished from sequencing errors.
Since the validity of a color-space mismatch depends on whether neighboring colors have been attributed to BINS, SOCS-B uses a dynamic programming table to compute the most probable methylation state for each cytosine based on the quality scores for each color (Supplementary Fig.S1).
The number and positions of errors follow from this information.
If the optimal translation has fewer than a user-specified number of color-space mismatches against the reference sequence, the alignment is kept until the algorithm completes or a more probable alignment is found.
SOCS-B was tested on 54 705 478 50-color reads produced with a SOLiD 3 Plus system from bisulfite-converted Arabidopsis thaliana genomic DNA that had been pre-enriched for CpG methylation by methyl-binding domain affinity chromatography and spiked with phage lambda DNA (to measure bisulfite conversion efficiency).
As a control, the reads were first aligned using the alignment tool provided by Applied Biosystems (mapreads) against reference sequences representing the fully bisulfite converted Watson and Crick strands and the unconverted Watson strands of A.thaliana and phage lambda.
This approach is analogous to that employed by previous BS-Seq studies (Lister et al., 2008).
Alignment was then performed against only the unconverted genomes using SOCS-B, which exhibited a 2-fold increase in total sensitivity for reads with three or fewer errors (Table 1).
Using reads that uniquely aligned to the lambda genome, the bisulfite conversion rate was estimated to be 99%, indicating that the increase in sensitivity (and thus the abundance of heterogeneously converted reads) was due to Table 1.
Sensitivity of SOCS-B in aligning SOLiD bisulfite sequence data Errors Mapreads SOCS-B SOCS-B permitted (reads aligned) (reads aligned) increase factor 0 1 150 378 8 701 800 7.56 1 3 283 347 13 856 042 4.22 2 6 691 811 18 764 830 2.80 3 11 159 673 22 656 148 2.03 Alignments using mapreads were performed against reference sequences representing the fully bisulfite converted (both Watson and Crick strands) and unconverted genomes of A.thaliana and phage lambda, while alignments using SOCS-B were performed against only the unconverted genomes.
complex methylation patterns, rather than incomplete conversion.
Furthermore, since the most biologically relevant methylation sites in the genome occur in dense clusters, it seems possible that the additional reads aligned by SOCS-B might be of more biological significance than those aligned with mapreads.
Because of the algorithms inherent lack of bias, SOCS-B also showed increased specificity when aligning simulated reads (Supplementary Table S2).
SOCS-B alignment took 30 h using an Apple Mac Pro (dual 2.93 GHz Quad-Core Intel Xeon with hyper-threading, 32 GB RAM).
Since color-space errors are fairly abundant in SOLiD reads, more reads can be aligned by allowing more mismatches, either at the expense of run time (by increasing the sensitivity) or of specificity (by increasing the tolerance).
For larger reference genomes or datasets, or for higher error sensitivity, SOCS has features that facilitate distributed processing.
Executable versions, source code, sample datasets, and usage instructions are available atWe thank Ryan Lister and Joe Ecker of the Salk Research Institute (La Jolla, CA, USA) for providing A. thaliana genomic DNA, Hank Tu of Life Technologies (Foster City, CA, USA) for assisting with distributed computing, and George Marnellos (Invitrogen, Carlsbad, CA) for helpful discussions.
Funding: DHHS contract N266200400059C/N01-AI-40059.
Conflict of Interest: none declared.
ABSTRACT Motivation: Mathematical models of biological systems often have a large number of parameters whose combinational variations can yield distinct qualitative behaviors.
Since it is intractable to examine all possible combinations of parameters for non-trivial biological pathways, it is required to have a systematic strategy to explore the parameter space in a computational way so that dynamic behaviors of a given pathway are estimated.
Results: We present PSExplorer, a computational tool for exploring qualitative behaviors and key parameters of molecular signaling pathways.
Utilizing the Latin hypercube sampling and a clustering technique in a recursive paradigm, the software enables users to explore the whole parameter space of the models to search for robust qualitative behaviors.
The parameter space is partitioned into sub-regions according to behavioral differences.
Sub-regions showing robust behaviors can be identified for further analyses.
The partitioning result presents a tree structure from which individual and combinational effects of parameters on model behaviors can be assessed and key factors of the models are readily identified.
Availability: The software, tutorial manual and test models are available for download at the following address:Contact: tqtung@kaist.ac.kr; tqtung@gmail.com Received on April 26, 2010; revised on July 4, 2010; accepted on July 25, 2010 1 INTRODUCTION Computational modeling is a valuable tool to understand the complex biological systems.
Such models often possess complex topological structures with multiple positive and/or negative feedbacks and a large number of parameters so that combinational perturbations on these parameter values can greatly affect qualitative behaviors of the models.
Two important issues in this approach are to understand the scope of behaviors that the model can experience, and to identify key parameters regulating the behaviors.
Bifurcation analysis (Borisuk and Tyson, 1998) is one of well-developed methods for qualitative behavior analysis of dynamical models.
Its application to systems biology is limited, however, since it can examine the effects of only a few parameters, mostly one or two at a time.
Although a tool for the global parametric perturbation analysis of biochemical models is available (Zi et al.. 2008), it is focusing on parameter sensitivity analysis, i.e.
a quantification of To whom correspondence should be addressed.
individual effects of parameter variations respecting to quantitative changes in the model outputs.
In this note, we outline PSExplorer, an efficient computational tool to explore high-dimensional parameter space of biochemical models for identifying qualitative behaviors and key parameters.
The software supports input models in SBML format.
It provides a friendly graphical user interface allowing users to vary model parameters and perform time-course simulations at ease.
Various graphical plotting features helps users analyze the model dynamics conveniently.
Its output is a tree structure that encapsulates the parameter space partitioning results in a form that is easy to visualize and provide users with additional information about important parameters and sub-regions with robust behaviors.
2 METHODS An ordinary differential equation (ODE) model of a biochemical network is often represented in the following form: dX dt = f (X, ) ; X(t =0)=X0 (1) where X is a vector of state variables and X0 a vector of their corresponding initial values; is a vector of kinetic parameters.
To begin with, users are required to specify a set of k parameters P1,P2,...,Pk (Pi can be a kinetic parameter or an initial value, i=1,2,...,k) and their admissible value ranges to be perturbed; a set of target variables to represent the model behaviors; for convenience, let us say that there is only one target variable Y .
The general framework of PSExplorer has six steps that are performed in a recursive manner.
To help readers grasp main concepts, an illustrative example is provided in Figure 1 complementing to following brief explanations of the steps: (1) Latin hypercube sampling technique (Helton et al., 2005) is used to generate N points in the parameter space (Fig.1A); N is initially specified by users.
Each sampling point represents a parameter combination p(i) =[pi1,pi2,...,pik] whose element pij is a randomly generated value of the parameter Pj within its admissible range.
(2) For each p(i), the software solves the set of ODEs in (1) to estimate temporal profile y(i)of the target variable, (Fig.1B).
(3) The simulated temporal profiles are grouped into c clusters (Fig.1C) using a feature-based k-mean clustering algorithm.
Each cluster is regarded as a qualitative behavior of the model.
The software supports a set of features from which users need to select a subset that fits to their analysis purpose.
The number of clusters is specified by users.
(4) This step is to estimate a vector b(i) =[bi1,bi2,...,bic] for each y(i); where bij(j=1,2,...,c) is the similarity measure between y(i) and j-th cluster center.
For visualizing, we assign each p(i) to a cluster j if bij is the maximum value of b(i), then points in each cluster is plotted with a distinct color in the parameter space as shown in Figure 1D.
The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[17:53 31/8/2010 Bioinformatics-btq440.tex] Page: 2478 24772479 T.Q.Tung and D.Lee Fig.1.
An example illustrating a 2-parameter perturbation analysis of a pathway model.
(A) Parameter space is filled with sampling points.
(B) Plotting of simulation results.
(C) Clustering shows three distinctive clusters representing model qualitative behaviors.
(D) Sampling points is colored respecting to clustering results.
(E) The parameter space is nicely partitioned into sub-regions with different behaviors.
(F) A tree represents the parameter space partitioning.
(G) Parameter ranking estimated from the tree.
(5) Let Bj(j=1,2,...,c) be a variable representing similarity measure between the j-th cluster center and simulated temporal profiles.
Considering a regression problem where P1,P2,...,Pk are predictors and B1,B2,...,Bc are responses, a multi-response regression (Segal, 1992) is constructed using N learning cases <p(i), b(i) > (i= 1,2,...,N).
Loosely speaking, the goal of this step is to partition the parameter space into sub-regions containing homogenous points (as shown in Fig.1E).
The partitioning is represented in a binary tree (Fig.1F) whose terminal nodes correspond to the sub-regions.
(6) Heterogeneous sub-regions are identified and additionally sampled with N0 (specified by users) points.
These sampling points are proportionally distributed to the sub-regions respecting to their heterogeneities.
The Steps 26 are then repeated iteratively until the total number of generated parameter combinations exceeding a maximum value.
Note that in Step 2, simulations are performed only on a newly generated data and results are combined with the data of previous iterations.
N is then replaced by N +N0 afterward.
Finally, PSExplorer presents the result tree from Step 5 of the final iteration in a graphical format where users can intuitively pinpoint regions of robust behaviors for further analyses.
From the tree structure, the software evaluates parameters in terms of their importance and key parameters can be readily to be identified (Fig.1G).
3 EXAMPLE An application example of how to use PSExplorer to study system dynamics of computational models is provided in the users manual of the software.
The WntERK crosstalk model (Kim et al., 2007) is selected as an example model.
Fifty-five kinetic parameters of the model were varied from 101 to 101 times of their reference values.
PSExplorer has identified five distinct qualitative behaviors.
Three among them were mentioned in a previous study explaining co-operatively working modes of the two pathways.
In contrast, the two remaining ones show that ERK and Wnt can behave independently regardless the existence of a hidden positive feedback between them.
Literature information has been found to support the existence of such behaviors.
We refer readers to the users manual for a detail description of the example.
4 DISCUSSION It is imperative to employ an in silico analysis to systematically identify the specific perturbations that have significant effects on the system behaviors, especially when numerous experiments on living systems are not practical.
Globally perturbing model parameters to analyze system dynamics was used in previous studies (Hua et al., 2006; Sung et al., 2009).
For automating the approach, PSExplorer has been developed from the framework proposed by Hua et al.
(2006) with necessary refinements to boost computational performance.
Instead of using an exhaustive sampling approach, PSExplorer utilizes a novel recursive stratified sampling approach that can help explore high-dimensional spaces efficiently.
On each recursion step, the Latin hypercube sampling requires less samples to be generated, thanks to its balancing distribution property.
Then, the coupling of a clustering algorithm with a regression tree model provides a mean for a parameter space stratification, which allows the sampling process to concentrate on highly heterogeneous regions while ignoring homogeneous ones.
As a result, the number of samples required to properly explore the parameter spaces can significantly reduce.
Last but not the least, one unique feature of the software that makes it more interesting than the sensitivity analysis technique is the ability to identify combinatory effects of parameter variations on model behaviors.
The result tree not only identifies sensitive parameters but it also can tell how they combine to produce a specific behavior of interest.
Funding: WCU (World Class University) program (R32-2008-000-10218-0) and Korean Systems Biology Research Project (20100002164) of the Ministry of Education, Science and 2478 [17:53 31/8/2010 Bioinformatics-btq440.tex] Page: 2479 24772479 PSExplorer Technology (MEST) through the National Research Foundation of Korea.
Conflict of Interest: none declared.
ABSTRACT Motivation: State-of-the-art experimental data for determining binding specificities of peptide recognition modules (PRMs) is obtained by high-throughput approaches like peptide arrays.
Most prediction tools applicable to this kind of data are based on an initial multiple alignment of the peptide ligands.
Building an initial alignment can be error-prone, especially in the case of the proline-rich peptides bound by the SH3 domains.
Results: Here, we present a machine-learning approach based on an efficient graph-kernel technique to predict the specificity of a large set of 70 human SH3 domains, which are an important class of PRMs.
The graph-kernel strategy allows us to (i) integrate several types of phy-sico-chemical information for each amino acid, (ii) consider high-order correlations between these features and (iii) eliminate the need for an initial peptide alignment.
We build specialized models for each human SH3 domain and achieve competitive predictive performance of 0.73 area under precision-recall curve, compared with 0.27 area under pre-cision-recall curve for state-of-the-art methods based on position weight matrices.
We show that better models can be obtained when we use information on the noninteracting peptides (negative examples), which is currently not used by the state-of-the art approaches based on position weight matrices.
To this end, we analyze two strategies to identify subsets of high confidence negative data.
The techniques introduced here are more general and hence can also be used for any other protein domains, which interact with short pep-tides (i.e.
other PRMs).
Availability: The program with the predictive models can be found at gz.
We also provide a genome-wide prediction for all 70 human SH3 domains, which can be found under http://www.bioinf.uni-freiburg.de/ Software/SH3PepInt/Genome-Wide-Predictions.tar.gz.
Contact: backofen@informatik.uni-freiburg.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION SH3 domains are an important class of peptide recognition module and probably the most widespread protein domain found in protein databases (Cesareni et al., 2002).
Thus, SH3 domains are involved in many cellular processes such as signal-ing, cell-communication, growth and differentiation.
Furthermore, the SH3 complexity corresponds with the complex-ity of the genome (Carducci et al., 2012).
These domains specif-ically recognize short linear proline-rich peptide sequences (Lim et al., 1994; Mayer, 2001; Musacchio et al., 1992).
SH3 domains have a conserved-barrel fold, which is formed by five or six strands arranged in two anti-parallel sheets.
SH3 domains are 60 amino acids in length and mainly found in intra-cellular proteins.
Approximately 300 SH3 domains are known in the human proteome (Karkkainen et al., 2006).
As 25% of human proteins contain proline-rich regions (Li, 2005), and SH3 domains recog-nize proline-rich peptides, it is an open challenge to understand how the hundreds of SH3 domains achieve a high specificity in selecting their physiological partners to regulate specific biolo-gical functions.
The canonical proline-rich peptide motifs recognized by most of the human SH3 domains have a PxxP core and are classified in two major groups: class I and class II.
The consensus se-quences for these two groups are denoted as xPxP (class I) and PxPx (class II), where x represents any natur-ally occurring amino acid, represents a hydrophobic amino acid and represents as a positively charged amino acid (nor-mally arginine and lysine).
Structural studies of the SH3peptide complexes with class I and class II motif suggest that these two types of peptide ligands bind to an SH3 domains in opposite orientations (Lim et al., 1994; Yu et al., 1994).
Previous studies reveled that the positively charged residues in the peptide se-quence, such as arginine and lysine, play an important role in the binding with the respective SH3 domain (Feng et al., 1994, 1995).
Based on the characteristics of the binding site, the SH3 domains prefer either one or the other pepitde motif.
Peptide motifs can be further classified into subgroups depending on the tolerance for the substitution of the lysine residue with the arginine residue (Carducci et al., 2012).
Although most human SH3 domains bind with class I and/or class II motifs, a subset of SH3 domains have the ability to rec-ognize noncanonical or atypical peptide motifs.
For example, NCK1 SH3 domains and the SH3 domains from EPS8 family are able to bind with a PxxDY motif (Kesti et al., 2007; Mongiovi et al., 1999).
EPS8 and its SH3 domain have an im-portant role in mitogenetic signaling.
Overexpression of EPS8 increased epidermal growth factor-dependent transformation and mitogenic responsiveness to epidermal growth factor *To whom correspondence should be addressed.
yThe authors wish it to be known that, in their opinion, the first two authors should be regarded as joint First Authors.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com (Fazioli et al., 1993; Matoskova et al., 1995).
The RxxK motif was also found to interact with SH3 domains of the STAM2 (Kato et al., 2000), SLP-76, GRAP2-C proteins (Liu et al., 2003).
Peptides that are sufficiently similar to class I or class II motifs are also recognized by several SH3 domains.
For instance, the motif RxxPxxxP (similar to class I) and the motif PxxxPR (similar to class II) bind with the SH3 domain in CTTN (Tian et al., 2006) and CIN85/SH3KBP1 proteins (Moncalian et al., 2006), respectively.
Enormous amount of data are generated by various high-throughput experiments designed to address the binding specifi-city of SH3 domains, such as phage display (Tonikian et al., 2007), SPOT synthesis (Landgraf et al., 2004) or peptide array screening (Wu et al., 2007).
Current associated computational methods, however, are usually based on the popular position weight matrices (PWMs) (Brannetti et al., 2000; Kim et al., 2011).
There are two major drawbacks of PWMs.
First, they are essentially linear models and thus ignore the correlation be-tween the various positions in the peptide ligand (Liu et al., 2010).
This also implies that they cannot differentiate between peptide classes.
For that reason, few approaches are proposed recently, which use multi-PWM models for addressing the prob-lem of multiple peptide classes (Gfeller et al., 2011; Kim et al., 2011).
Second, they are based on a multiple alignment of the peptide ligands, which is a hard task for proline-rich SH3-bound peptides.
Even minor alignment errors typically introduce significant noise in PWMs estimate.
Other tools rely on resolved 3D domainpeptide structures, which are, however, known only for a few cases.
Thus, they typically cannot directly make use of the available high-throughput data.
These include the structure-based energy model by Hou et al.
(2006) and the neural network model by Ferraro et al.
(2006).
Here, we present a machine-learning approach to overcome the aforementioned drawbacks.
Our method is based on a graph-kernel technique that, differently from the PWMs, does not require an initial peptide multiple alignment.
Furthermore by virtue of its nonlinearity assumptions, it can adequately capture all types of peptide classes.
We build specialized models for each of 70 human SH3 domains achieving competitive predictive per-formance compared with the state-of-the-art method (Kim et al., 2011).
Furthermore, we show how we can leverage the informa-tion contained in related domains by building a single compre-hensive model for a set of six SH3 domains further improving the predictive performance.
Although high throughput datasets are available to train statistical-based learning approaches, we note that the presence of spurious interactions in the experimental data (either false negative or false positive) can severely affect the quality of the induced model.
To tackle this problem, we use several approaches to identify a subset of high confidence nega-tive interaction data.
These instances are then used to train a model in a setting with reduced noise-to-signal ratio.
2 METHODS We present an effective machine-learning method for the prediction of protein domain-peptide interactions.
The method is based on a graph-kernel approach, which, in contrast to the majority of other approaches, does not require the peptide sequences to be aligned and can, at the same time, exploit high-order correlations between amino acid residues.
Finally, we show how to build a model that takes in input both the peptide information and the (aligned) domain amino acid sequence.
By doing so, we can exploit information from related SH3 domains and enhance the overall prediction performance.
2.1 Dataset In our study, we use the large-scale human SH3peptide interaction data from the high density peptide array experiment (Carducci et al., 2012).
A total of 9192 peptides of length 15 were used in the CHIP experiment.
The SH3peptide interactions that gave a positive signal in peptide CHIP experiment have been stored in the newly developed interaction database PepspotDB.
From PepspotDB, we have retrieved 16 032 nonredundant interactions for 70 human SH3 domains and 2802 peptides.
Among them, a total of 478 interactions were also supported by the literature as re-ported by the MINT database (Licata et al., 2012) (see Table 1 for details).
2.2 Feature encoding 2.2.1 Single domain modeling For some protein domains, it is pos-sible to identify a key amino acid necessary for a successful binding of a peptide (e.g.
the phospho-tyrosine for the SH2 domain).
This pivotal amino acid can then be used to identify an absolute reference system that allows to represent the peptide as a fixed size vector, i.e.
each amino acid is identified as having position i or i starting from the pivotal amino acid.
For SH3 domains, the situation is, however, more complex, as the key amino acid (proline) is abundant throughout the peptide sequence.
A unique reference system based on proline cannot therefore be easily identified.
Commonly, an initial alignment of the pep-tide sequences is performed in a preprocessing step.
Errors in this phase can lead to a bad estimate of the models parameters and ultimately to bad predictive performances.
Here, we propose a kernel approach defined independently of an ab-solute reference for amino acid positions.
In this way, we can move from a fixed-size vector type of encoding to a variable length sequence type encoding while still preserving a high discriminative power.
The shift from a vector-based to a sequence-based approach can be extended fur-ther: if we move from sequences to graphs, we can then encode any other ancillary information on specific amino acids.
To do so, we have to move from string kernel to efficient graph kernels.
To ensure low run-times, we resort to the recently introduced (Costa and Grave, 2010) Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) (see Supplementary Information for more details).
In more detail, to encode the peptide information, we proceed as fol-lows.
Given the experimental CHIP design constraints in peptide array library, we can only use peptide sequences of exactly 15 residues in length.
We enrich the information available on each amino acid with their aver-age physico-chemical properties, i.e.
charge and hydrophobicity.
As the graph-kernel approach can deal only with discrete labels, we discretize all properties.
More specifically, as for charge, we have divided all common 20 amino acids into three groups as basic (R, K, H), acidic (D, E) and Table 1.
Summary of the whole data for 70 human SH3 domains No.
of Positive No.
of Negative No.
of Unknown Peptides 2802 9188 9188 Interactions 16 032 (478) 262 883 627 177 Note: Data available from the high density peptide array experiment of Carducci et al.
(2012).
In brackets are the interactions evidence available in MINT (Licata et al., 2012).
i336 K.Kundu et al.
neutral (the remaining amino acids); as for hydrophobicity, we have identified four groups (very low, low, high and very high) based on their hydrophobicity scales following Kyte and Doolittle (1982), obtain-ing I, L, V as very high hydrophobic residues; A, M, C, F as high hydro-phobic residues; G, T, S, W, Y, P as low hydrophobic residues; and rest of the amino acids (i.e.
R, K, H, D, E, Q, N) are considered as very low hydrophobic residues.
The peptide is then modeled as a chain of unlabeled vertices: one per amino acid.
Each vertex is then connected with a side-chain graph that encodes the ancillary properties, namely, of proximity: the charge, the hydrophobicity and the amino acid code (see Fig.1, left).
To generate features that are discriminative of the sequence direction, we model the peptide as a directed graph.
2.2.2 Multiple domains modeling When developing models for single domains, the input encodes only the information for the peptide sequence.
However, when we want to induce a general model for a subset of related domains, the input should include also information on how a specific domain relates to the other ones so that useful knowledge can be transferred from interactions on similar domains.
To do so, we model the domain amino acid sequence information in a similar fashion to the peptide encoding, with one important difference: as the position of spe-cific amino acids is relevant to determine the specificity of the domain peptide interaction, we additionally encode the information of an abso-lute positional reference.
To do so, we align the related domains with the MUSCLE (Edgar, 2004) alignment software.
In contrast to the peptide alignment, the SH3 domain alignment is highly reliable, mainly the align-ment of n-SRC-loop and RT-loop in SH3 domains.
Each domain-specific sequence is then projected onto the alignment, and the necessary gaps are finally introduced (see Fig.1, right).
The input for the multi-domain model therefore comprises two disconnected components, one for the peptide and one for the domain.
To eliminate ambiguity issues, we dis-tinguish the label alphabet for the peptide sequence from that of the domain sequence by means of appropriate prefixes.
3 RESULTS 3.1 Modeling with graph kernel features Our approach is based on a graph encoding that allows to model relations between specific amino acids as well as different amino acid abstractions.
This graph is then processed by a fast graph-kernel technique called NSPDK, recently introduced by Costa and Grave (2010), which extracts as explicit features, the occur-rence counts of all the possible pairs of near small neighborhood subgraphs.
The subgraph pairs are characterized by a radius and by a topological distance parameter (for details, see in Supplementary Information).
The final classification task is then performed by a Support Vector Machine (SVM) based on the NSPDK graph kernel.
By using an explicit vector encoding, we gain efficiency, as we avoid computing and storing the pair-wise similarity matrix.
3.1.1 Single domain modeling When developing models for each specific domain, we need only encode information on the candi-date peptide sequence as described in Section 2.2.
Different values for the radius parameter give rise to the parts illustrated in Figure 2.
Given the directed nature of the encoding graph, each neigh-borhood subgraph includes only amino acids that are downstream with respect to the current root node.
With radius 1 and distance 0, each labeled vertex is considered independently: the corres-ponding feature representation encodes the frequency of each physico-chemical property (either the charge, the hydrophobicity or the amino acid type) in the single peptide; radius 2 allows properties of adjacent residues (e.g.
hydrophobicity and adjacent charge information) to be modeled; radius 3 allows all properties for a single residue to be taken into account jointly.
Even larger radius values can capture the joint information for adjacent pairs, triplets, etc., of residues.
When pairs of neighborhood subgraphs at different distances are used, the composition of the subsequence between the two root vertices is ignored allowing a dont care or soft type of feature matching.
The order in which the properties are encoded is chosen to avoid generating features that subsume each other (i.e.
given a neutrally charged amino acid, one can have multiple values for the hydrophobicity, but not the other way around).
The final descriptors for each peptide contain all features with radii ranging from 0 up to Rmax and distances in 0,Dmax.
The optimal ranges are determined experimentally via cross-val-idation techniques.
Finally, the training phase allows the deter-mination of the weight distribution on all feature types (general and specific) to obtain optimal predictive performance 3.1.2 Multiple domains modeling Several SH3 domains in the human genome bind strongly with class I and/or class II Fig.2.
Top: NSPDK features for Distance (D) 0 and Radius (R) 1, 2, 3 relative to a given root vertex highlighted in orange.
The directedness property of the graph allows to induce features that can differentiate strand directions.
Bottom: Ex.
of feature for R 3 and D 5 capable to capture the correlation of two amino acid at relative distance 5.
The sequence information that is not contained in the neighborhoods is ignored; the effect is equivalent to a dont care pattern Fig.1.
Graph encoding for peptide sequences (left) and for domain sequences (right).
The encoding is enriched with charge, hydrophobicity and amino acid-type information.
Peptide amino acid positions do not have an absolute reference, whereas domain amino acid positions receive an absolute positional reference according to a consensus alignment.
Gaps receive a special encoding i337 Alignment-free domainpeptide interaction prediction peptides.
SH3 domains for FYN, BTK, HCK, FGR, SRC and LYN proteins are among them.
The intuition underlying the multiple domains approach is that, if we are able to exploit the similarities across these domains, we can then increase the pre-dictive performances for each specific domain.
In practice, we would be performing a form of transfer-learning (Caruana, 1997) from one protein domain to another so that the examples used to induce a model on one domain would also contribute to form the bias of related models, increasing the effective number of avail-able training instances.
To do so, we proceed by coupling the peptide information with the encoding for the domain in a joint feature space; more specifically, we encode the domain amino acid sequence information via its projection with respect to the domain con-sensus alignment.
Here, the backbone vertex labels encode the specific position of the amino acid within the reference align-ment.
By introducing these absolute reference ids, all features (those describing physico-chemical properties and those describ-ing the amino acid composition) become position specific.
This absolute reference creates a joint feature space that ultimately allows information about interactions with different domains to be shared.
We are not trying to model the exact pairs of interacting amino acid residues (one in the peptide and one in the domain), as done in Ferraro et al.
(2006).
To do so would imply resorting to resolved protein complexes information, which is not available in large-scale.
Rather, we represent the candidate interacting peptide and domain as a pair of discon-nected graphs.
The NSPDK procedure alone does not instantiate features that can directly express the relationship between parts of the peptide and of the domain sequences.
However, we can take full advantage of the kernel trick and use nonlinear (i.e.
polynomial or Gaussian) kernels.
By doing so, the peptide-domain complex is implicitly represented by features that express combinations of the original features.
We then rely on the stat-istical analysis of high-throughput experiments to infer the im-portance of each position specific features in the domain combined with nonposition specific features of the peptide sequence.
3.2 Dealing with false negatives Traditional methods for peptide characterization rely on genera-tive approaches where the probability of the model (often repre-sented as a motif) is estimated from positive data alone.
A typical approach is represented by PWMs (Kim et al., 2011) where the multinomial probability distribution for each position in the se-quence is estimated independently via frequency counts.
In the Machine Learning community, it is known that discriminative models have an advantage over generative ones, as they can rely on both positive and negative data; this allows them to better identify the decision boundary for the relevant region of the data space.
Although generative methods often require less training examples, they do not achieve the same performance (Ng and Jordan, 2001).
However, when negative data are assumed to be severely affected by noise, or even when the negative data is overly represented, one-class models can exhibit an advantage over discriminative ones.
A typical scenario is when dealing with high-throughput experimental results such as phage display (Tonikian et al., 2007), SPOT synthesis (Landgraf et al., 2004) or peptide array screening (Wu et al., 2007).
Here, to increase the confidence on the measurements, the experimental protocol makes use of stringent thresholds (e.g.
requiring the agreement on several replicated experiments).
In these cases, a large part of what would be labeled as lack of interaction (negative example) is just a weaker true interaction (positive example).
To deal with these cases, we developed two approaches.
The first one is a generative approach that makes use of multiple PWMs to model each peptide class.
We then select a subset of instances that are not recognized by any specialized PWMs and use those are reliable negative instances to train a binary classifier.
The second approach is based on a combination of a one-class and a semi-supervised method.
3.2.1 False negatives refinement The key idea here is to use a generative approach to model each peptide class and select a subset of instances that are not recognized by any specialized model.
We take an approach similar to Hui and Bader (2010) and select confident negative interactions using profile-based models (i.e.
PWMs).
To better represent the binding specificity of each domain, instead of using a single model, we resort to multiple PWMs, namely, one for each motif class for each SH3 domain.
In more detail, we first used the fuzzpro pattern search pro-gram from the EMBOSS package (Rice et al., 2000) to cluster the peptides into eight groups, one for each known motif class.
We found that the majority of the peptides belong to the canonical motifs of class I and/or class II, whereas the rest belong to atyp-ical motifs, mainly PxRP, PxxxPR, PxxDY and RxxKP motifs (see Supplementary Table S1).
Afterward, we used the popular EM-based MEME (Bailey and Elkan, 1995) algorithm to generate a PWM for each group.
Finally, we used MAST (Bailey and Gribskov, 1998), a se-quence homology search algorithm, to identify the peptides matching the various PWMs.
MAST ranks the input sequences according to an E-value type of score.
We consider the peptides with high E-value (i.e.
those that are not recognized with confi-dence by the model) as negatives.
The cutoff score was set to the maximum E-value calculated for the known positive instances.
Finally, for each domain, we select those peptides that are not recognized by any of the class specific PWMs.
By doing so, we identify a total of 200K (262 883) negative interactions for the whole set of 70 human SH3 domains (see Table 1).
Peptides considered as negative but that are close to the cutoff score are structurally similar to positive peptides.
Training and testing a model using only high confidence nega-tive interactions can in principle induce a bias.
To rule out such a case, we perform an additional experiment (see Section 3.4 later in the text) where we do not filter in any way the negative data.
3.2.2 One class semi-supervised model The key idea here is to use the SVM one-class approach, pioneered by Scholkopf et al., 2001, to warm-start the self-training method for semi-supervised learning (Culp and Michailidis, 2007), restricting the prediction to negative instances only.
In Scholkopf et al., 2001, it is shown how, to identify a region that contains with high probability most of the positive data, one can formulate the classic SVM optimization problem for binary classification using the origin of i338 K.Kundu et al.
the feature space as the only negative instance.
In case of normal-ized kernels, this boils down to using negative instances that are just the symmetric counterparts of the available positive in-stances.
Here, we follow this latter way, given that we can pro-duce the explicit sparse encoding and can therefore efficiently invert each instance.
The self-training approach to semi-supervised learning (Culp and Michailidis, 2007) is a wrapper method that iteratively uses the class predictions over the unlabeled data as true labels for a successive training phase until convergence to a stable state is reached.
Here, we use the one-class model to initially induce the class information on the unsupervised instances, but, rather than using both positive and negative predictions, we accept only negative predictions.
We select those instances that are predicted with the highest confidence (i.e.
that are further away from the class boundary hyperplane) and use them to iteratively train the SVM model.
For simplicity, we fix the fraction of the accepted negatives to 50% of the total number of unsupervised instances.
3.3 Performance of single domain model with filtered negatives As detailed in Section 3.2.1, we induced PWMs to model several known classes of binding peptides for each SH3 domain.
We used these models to select and filter away all peptides that were experimentally identified as noninteracting but that are recognized by the PWMs as belonging to one of the known classes of binding peptides.
In this way, we obtain a total of 262 883 confident negative interactions for all 70 SH3 domains (the full list of positive and negative interaction data along with the class balance is given in Supplementary Table S2).
We encode the peptide sequences as described in Section 2.2 and induce an SVM model for each SH3 domain based on the graph kernel.
Even if here we use a linear SVM, we are inducing a nonlinear model with respect to the sequence of amino acid residues, i.e.
the linear model is aware of higher order features that capture the correlation between pairs, triplets, etc., of amino acids.
We used a 10-fold stratified cross-validation to evaluate the predictive performance of each model.
The hyper-parameters of the method were optimized in each fold by using a 5-fold cross-validation over the training set.
Specifically, we optimized the radius parameter R 2 f1, .
.
.
, 8g and the distance parameter D 2 f1, .
.
.
, 8g for the graph kernel.
The linear SVM model is induced using the Stochastic Gradient Descent approach cham-pioned by Bottou and Bousquet, 2008.
The optimal values are achieved at R 6 and D 8 for most of the domains.
In Supplementary Table S3, we report the following quanti-ties: Sensitivity=Recall TPTPFN , Specificity TNTNFP , Precision TPTPFP, the area under the precision-recall curve (AUC PR) and the Area Under the Curve for the Receiver Operating Characteristic (AUC ROC).
On average, we obtain a remarkable 0.73 AUC PR and 0.94 AUC ROC.
As for run times, as the NSPDK has essentially a linear com-plexity when dealing with bounded degree graphs, we report the estimated average time per instance: 0.07 s/instance on an ordinary 2.33GHz Intel Core2 Duo CPU.
This time includes the file upload in main memory, the graph feature generation and the parameters fitting of the model via the Stochastic Gradient Descent.
In practice, this means that we can generate a model, given 1K peptides in 1 min, or equivalently, a model for a proteome-scale 100K peptides dataset in52h on a desktop machine.
We note that at times, we suffer from the high imbalance problem.
For certain domains (e.g.
CSK, DLG1, FISH, GRAP2-1, RUSC1, STAM2, etc.
), the ratio between the avail-able information for positive interactions and negative inter-actions is above 1100.
It is known in the Machine Learning literature that severely imbalanced class distribution negatively affects the performance of adaptive predictors (He and Garcia, 2009), as the tuning algorithms are generally biased toward the majority class.
In our case, the majority class is the negative class, which implies a low sensitivity (true positive rate).
3.3.1 Comparison with state-of-the-art PWM approach We have compared our results with a recently developed tool (Kim et al., 2011) based on PWMs called Multiple Specificity Identifier (MUSI).
Even if the tool tries to increase the modeling complex-ity by replacing a single PWMwith multiple PWMs, it remains in essence a linear model and therefore still suffers from the issues detailed in the Section 1, namely, the inability to model features correlation and the fact that it requires an initial error-prone peptide alignment phase.
We have used exactly the same experimental setup as in our approach.
In Figure 3, we report the comparative results with respect to AUC PR and AUC ROC performance measures for all 70 human SH3 domains.
On aver-age MUSI achieves a noncompetitive 0.27 AUC PR and 0.69 AUC ROC.
We were curious to see how our method performs on the same experimental dataset as done in Kim et al., 2011.
To do so, we collected the interaction data used in the article by Kim et al., 2011.
A total number of 2457 unique positive interactions were available for the SH3 domain from SRC protein.
As the inter-action peptides were identified by the phage display experiment, we could only get the positive interaction data.
For preparing the negative interaction data, we took three different strategies.
First, we consider the filtered negative data used in our study.
Second, we prepare random negatives automatically generated by rand function in Perl and third, we prepare the random negatives generated by the same strategy as described earlier in the text with PxxP core.
Finally, we have performed stratified 10-fold cross-validation, using same parameter ranges for optimization and report AUC PR and AUC ROC performance measures for all these three datasets.
Our approach shows much higher per-formance than MUSI tool.
This would add another layer of confidence to the performance of our models.
We also compare the performances of our graph-kernel approach and MUSI on our original dataset with these three datasets (see details in Supplementary Fig.S4).
The problem of generating the initial alignment was also tackled in a recent publication by (Andreatta et al., 2013).
They identify multiple specificities in peptide data by performing two essential tasks simultaneously, alignment and clustering, and therefore find biologically relevant binding motifs that cannot be described well with a single PWM.
Our approach sidesteps these issues altogether, as we just make a model based on all available peptide features (achieving at the same time a speed up of several orders of magnitude in run times).
i339 Alignment-free domainpeptide interaction prediction 3.4 Performance of single domain model with unfiltered negatives Training and testing systems using only high confidence negative interactions can in principle induce a bias that alters the com-parison between methods.
To rule out such a case, we perform an additional experiment where we do not filter in any way the negative data.
We use the same setup as in previous experiments (i.e.
stratified 10-fold cross-validation), using the same param-eters ranges for optimization.
In Figure 3, we report the com-parative results with respect to AUC PR and AUC ROC performance measures for all 70 human SH3 domains.
The graph-kernel approach achieves an average AUC PR 0.35 and 0.90 AUC ROC.
In the same conditions, MUSI achieves a noncompetitive AUC PR 0.04 and AUC ROC 0.58.
This result confirm the advantages of the proposed discriminative graph-based method.
The large difference in the performance with respect to the filtered case is due to (i) the imbalanced class dis-tribution (up to 1:100) and (ii) the presence of a possibly large portion of false negatives.
3.5 Test on single domain model with one-class and semi-supervised filtered negatives To test how important the precise information on true negatives (i.e.
peptides that do not interact with the domain) is, we used the one-class and semi-supervised technique described in Section 3.2.2.
The key idea here is to make use of information based primarily on the positive interactions to characterize the binding peptides; instances that are not well recognized by the model are then assumed to be negative.
Once again, we operate in the same setup as for the unfiltered negatives experiment.
In Figure 3, we report the comparative results with respect to AUC PR and AUC ROC performance measures for all 70 human SH3 do-mains.
The one-class approach achieves an average AUC PR 0.063 and 0.61 AUC ROC.
Although this result is statistically significant (according to a Wilcoxon Matched-Pairs Signed-Ranks Test, with P 0.0003), the magnitude of the result let us conclude that using a generative approach to model pro-teinpeptide interactions is noncompetitive with respect to dis-criminative approaches.
3.6 Multi-domain model We aligned six domains (SH3 domains for FYN, BTK, HCK, FGR, SRC and LYN proteins) with the MUSCLE tool (Edgar, 2004).
We used the SVM light (Joachims, 1999) software to train a Gaussian SVM over the explicit sparse feature encoding of pep-tide and domain sequence pairs.
We evaluated the predictive performance using a 10-fold cross-validation over the six domain set using the filtered negatives as specified in Section 3.2.1.
The value for the Gaussian width was optimized on an internal 20% validation set over the range 2 f:001, :01, :1, 1g and the trade-off parameter C 2 f1, 10, 100g, whereas the values of R and D for the graph kernel were fixed at the optimal value obtained in the previous experiments of R 6 and D 8.
As a baseline, we trained (and evaluated in an analogous setting) the six models independently on each domain, both using a linear kernel and a Gaussian compounded kernel.
In Figure 4, we report the AUC PR and the AUC ROC for each SH3 domain and MUSI performance.
In Supplementary Fig.S5, we report the sensitivity and the specificity, respectively.
The experimental result confirms our intuitions: sharing information across related (a) (b) (c) (d) Fig.3.
A 10-fold cross-validation performance.
(a) (b) comparison when using filtered negative interactions for Graph Kernel (GK) and MUSI.
(c) (d) comparison with nonfiltered negative interactions for binary class Graph Kernel (GK), one-class Graph Kernel and MUSI.
The error bars represent respective standard deviation.
The domains are sorted by increasing average performance for the Graph Kernel method i340 K.Kundu et al.
domains increases the predictive performance, mainly owing to an increase in sensitivity.
We also note that the difference between models trained over single domainswhen using the linear kernel or the Gaussian one is nonstatistically significant.
This result is also in line with our expectations, as the correlation between features is fully captured by the pairwise neighborhood subgraphs features, leaving no margin of improvement to the nonlinearity imple-mented by the kernel trick.
With radius R 6 and distance D 8, the kernel generates features spanning the whole sequence.
Finally, we report the performance of the joint model when trained over the six domains, but tested over a novel albeit related LCK dataset.
In this experiment, we are asking to predict the specificity for a novel domain, given only the information about the alignment of this domain to the overall consensus alignment.
The model achieves an average AUC PR 0.85 and AUC ROC 0.96, with a high sensitivity 0.91 and specificity 0.96.
The interest-ing finding is that the results are better than those obtained by training amodel on the LCK protein alone; in this case, we obtain an average AUC PR 0.86 and AUC ROC 0.94, with a low sensi-tivity 0.55 and a high specificity 0.99.
To understand the result, in the case of the LCK domain, we have experimental evidence only for 150 positive interactions, whereas the dataset for the six do-mains has a total of 910 nonredundant peptides involved in posi-tive interactions.
The experimental results support therefore the hypothesis that, at least in the LCK case, the domain alignment is sufficient to characterize the peptides binding model and to achieve therefore a higher overall sensitivity.
3.7 Genome-wide analysis We have performed a genome-wide analysis of SH3 domain-mediated interactions.
Our aim was to identify the novel inter-actions that have important biological roles.
We used UniProtKB/Swiss-Prot database (Magrane and UniProt Consortium, 2011), which is a manually curated and reviewed database.
We retrieved 20225 human proteins from UniProtKB/ Swiss-Prot database, release 2012-06.
For retrieving the peptide sequences, we scan all the available proteins with a window size of 15 and step size of 5.
In this way, we have extracted a total number of 2M (2209 474) peptide sequences.
In this analysis, we implemented co-cellular localization filter to avoid unlikely interactions, considering the term relative to the subcellular localization hierarchy in the controlled vocabulary of the Gene Ontology database (Ashburner et al., 2000).
More clearly, the mature protein that contains the peptide and the protein that expresses the domains should share the same subcel-lular localization.
In case of multiple cellular localization (e.g.
GRB2 protein can be found in nucleus, cytoplasm, endosome and golgi apparatus), we consider a peptide eligible for binding only if it shares at least one of the localization term with the domain-containing proteins.
After filtering the eligible peptides, we scored them by the trained models and ranked according to the SVM scores.
Finally, we report the top 50 predictions by each SH3 domain (see Supplementary File S1).
Among the predictions, we observed a peptide (CKKLSPPPLPPRASI, position 151165) from Phosphatidylinositol 4-phosphate 3-kinaseC2domain-containing subunit beta (Uniprot-id: O00750) was targeted by many SH3 domains (21 domains) that also share the same cellular compart-ment as annotated in Gene Ontology term database.
There are also two evidence of interactions between PIK3C2B with GBR2 and PLC-1 reported in STRING database (Franceschini et al., 2013).
In addition, we took 478 real interactions reported in the MINT database (Licata et al., 2012), discarded them from our training set and could recover 397 (i.e.
a recall 0.83).
In addition, we performed an analysis on these top 50 predic-tions for each SH3 domain to uncover the novel interaction functionalities using DAVID tool (Huang et al., 2009).
The tool allows the possibility to perform a term-centric enrichment analysis on 440 different annotation categories.
DAVID func-tional annotation chart, which identify enriched annotation terms associated with predicted proteins are reported.
The smaller P-values indicate higher enrichment (see Supplementary File S2).
Applying the term-centric analysis, we have observed some biological meaningful interactions.
For example, (i) SH3 do-mains from human P85-binds with a potential group of pro-teins (Uniprot-id: P21854, Q08209, Q07890, O00459, Q6ZUJ8) that play important role in B cell receptor signaling pathway.
(ii) Among the top prediction by the SH3 domain from Human BTK protein, 450% proteins take a vital role in alternative splicing.
4 DISCUSSION SH3 domain is probably the most widespread class of protein recognition modules.
The interactions mediated by SH3 domains Fig.4.
Precision-recall curves and AUC ROC curves for the Multi-Domain Gaussian Graph Kernel (MD-G-GK), the Single Domain Gaussian Graph Kernel (SD-G-GK), the Single Domain Linear Graph kernel (GK) and the MUSI tool for six related SH3 domains.
The error bars represent respective standard deviation Alignment-free domainpeptide interaction prediction i341 constitute an important class of protein interactions, involved in many cellular process.
We presented a computational approach to predict domainpeptide interactions, using available high-throughput data.
The method is an alignment-free approach based on an efficient graph kernel.
Although, here, we present an application to SH3 domains, the method is general and can thus be trained to predict any proteinpeptide interaction for which high-throughput data exists.
Current methods for proteinpeptide interaction require often an initial multiple alignment of the bound peptides.
As this is an error-prone process (especially in the case of SH3-domains, where peptides are proline-rich), one risks to introduce a signifi-cant amount of noise and obtain under performing models.
In addition, current methods are often linear models (e.g.
PWMs) and are therefore not able to represent high-order correlations between amino acid residues.
Nonlinear method exist but have to deal with the high model complexity resulting from exponential number of high-order correlations achievable even for relatively short peptide sequences.
If one uses the full alphabet of 20 amino acids, it becomes hard to gain sufficient data for a correct esti-mation of these complex models.
One common solution is to use a reduced alphabet where each letter represents an entire amino acid class.
This strategy, however, leads to inferior performance, especially when specific amino acids are preferred at specific pos-itions.
An alternative approach is to determine important inter-action first by using resolved 3D domainpeptide structures.
The major obstacle for the widespread application of this approach, however, is the limited availability of such structural data.
In this article, we use a different approach.
We consider an alignment-free approach based on a graph representation of the peptide sequence where different abstraction levels are available in a unified way.
By applying an efficient graph-kernel method, we were able to model high-order correlations that span different abstraction levels (e.g.
a feature could represent a specific residue that has to be three positions to the right of a hydrophobic resi-due).
The regularization provided by the SVM optimization scheme finally ensures that the model complexity is appropriately controlled and that only the features relevant for the task at hand are selected.
Discarding the abstraction information (experi-ments not shown), i.e.
using only the amino acid code informa-tion, leads to statistically significantly lower sensitivity.
This confirms the intuition that using physico-chemical properties in the feature definition can adequately model cases that would otherwise be poorly covered by a sufficient number of sequences.
It was also important to optimize the encoding order; therefore, we performed an experiment with different encoding order and proposed the best order to represent our graph (see Supplementary Fig.S6).
Interestingly, the experimentally cross-validated optimal parameters value (R 6,D 8) suggests that high-order amino acid correlations are required to obtain the best predictive performance, and that therefore linear models are inadequate.
Although we have previously used the NSPDK graph-kernel approach for clustering RNA structures (Heyne et al., 2012), here, differently from the RNA or molecular case, we do not have an obvious and natural way to encode information as a graph.
The guiding principle, behind the choice of the proposed feature encoding, is to add abstract information (like charge or hydrophobicity) in a somewhat soft and incremental way.
Rather than using an extended alphabet and maintaining a se-quence encoding, the proposed graph encoding allows us to obtain features that are increasingly specialized.
We have experi-mental evidence (see Supplementary Fig.S6) that a different choice in the ordering of the abstract information would yield suboptimal results, which become evident in the presence of imbalanced data.
Additionally, we have investigated the per-formance of a string kernel (the k-mer kernel) along with other types of kernels, applied to the pure amino-acid sequences (i.e.
without any additional information).
Also, in this case, there is an evident drop in performance (see Section Comparison with other predictive methods and Supplementary Fig.S7 in the Supplementary Information).
This confirms the intuition that using physico-chemical prop-erties in the feature definition can adequately model cases that would otherwise be poorly covered by a sufficient number of sequences.
Interestingly, we experimentally observed that high-order amino acid correlations are required to obtain the best predictive performance, suggesting that linear models are inad-equate for this application.
Another common practice is to use generative models, i.e.
models that try to capture the density distribution of the inter-acting peptides only.
We showed that using one-class approaches is sub-optimal, even when considering models more expressive than the commonly used linear PWMs.
The average predictive performance of a graph-kernel-based domain-specific model that is trained in a discriminative fashion is 0.35 AUC PR compared with 0.06 AUC PR when trained in a one-class way.
We tried to address the problem of selecting high-quality nega-tive data.
The issue is known in literature (see Ben-Hur and Noble, 2006 and Lo et al., 2005).
In the application domain of proteinpeptide interaction, it has been shown (Lo et al., 2005) that the common practice of generating negative instances by randomly shuffling peptide sequences simply leads to decreased predicted performance, as these instances do not resemble real biological sequences and are not therefore useful to determine useful class boundaries.
We note, however, decreasing perform-ance proportional to the level of class imbalance.
When the ratio of negative instances versus positive ones is within 10-fold, we maintain an AUC PR 0.8, but for ratios greater than 100, per-formance drops to AUC PR 0.4 and lower (see Supplementary Figs S8 and S9).
We showed how the flexible graph-kernel approach allows the induction of multi-domain models.
These models can leverage experimentally verified binding interactions on related domains and achieve high predictive performance even on domains for which no training material was available.
Finally, we performed a genome-wide prediction of human SH3peptide interactions.
All the learned models as well as all the genome-wide prediction interactions are available in http://www.bioinf.uni-freiburg.de//Software/SH3PepInt.
Our approach is general enough and can easily be applicable to other similar domains like SH2, PDZ and so forth.
As for future work, given the computational efficiency of these models (a single-domain model can be trained on 100K sequences in 52h), we plan to provide a comprehensive set of predictors for all protein domains for which high-throughput data are available.
K.Kundu et al.
i342 The authors thank David Gfeller and TaeHyung Kim for their help with the MUSI tool.
Funding: This work was funded by Centre for Biological Signalling Studies (BIOSS), University of Freiburg, Germany and the Excellence Initiative of the German Federal and State Governments (EXC 294 to R.B.).
R.B.
and F.C.
were partially supported by the German Research Foundation (BA 2168/3-1 and BA 2168/4-2 SPP 1395 InKoMBio to R.B.).
Conflict of Interest: none declared.
ABSTRACT Motivation: The constraints under which sequence, structure and function coevolve are not fully understood.
Bringing this mutual rela-tionship to light can reveal the molecular basis of binding, catalysis and allostery, thereby identifying function and rationally guiding protein redesign.
Underlying these relationships are the epistatic interactions that occur when the consequences of a mutation to a protein are determined by the genetic background in which it occurs.
Based on prior data, we hypothesize that epistatic forces operate most strongly between residues nearby in the structure, resulting in smooth evolu-tionary importance across the structure.
Methods and Results: We find that when residue scores of evolu-tionary importance are distributed smoothly between nearby residues, functional site prediction accuracy improves.
Accordingly, we designed a novel measure of evolutionary importance that focuses on the interaction between pairs of structurally neighboring residues.
This measure that we term pair-interaction Evolutionary Trace yields greater functional site overlap and better structure-based proteome-wide functional predictions.
Conclusions: Our data show that the structural smoothness of evolutionary importance is a fundamental feature of the coevolution of sequence, structure and function.
Mutations operate on individual residues, but selective pressure depends in part on the extent to which a mutation perturbs interactions with neighboring residues.
In practice, this principle led us to redefine the importance of a residue in terms of the importance of its epistatic interactions with neighbors, yielding better annotation of functional residues, motivating experimental validation of a novel functional site in LexA and refining protein func-tion prediction.
Contact: lichtarge@bcm.edu Supplementary information: Supplementary data are available at Bioinformatics online.
Received on April 30, 2013; revised on June 28, 2013; accepted on August 15, 2013 1 INTRODUCTION Protein functional sites and their key residue determinants are important to elucidate the molecular details underlying protein function (Laskowski and Thornton, 2008), design drugs (Hardy and Wells, 2004), engineer proteins (Thyme et al., 2009) and predict protein function (Erdin et al., 2010).
The experimental gold standard to map these sites is alanine scanning (Clackson and Wells, 1995; Onrust et al., 1997), but this approach is rarely exhaustive and limited by the availability of biologically relevant assays.
Therefore, complementary, inexpensive and scalable approaches search for functional sites and residues by analyzing the vast evolutionary record of protein sequences computation-ally (Aloy et al., 2001; Buslje et al., 2010; Casari et al., 1995; Engelen et al., 2009; Glaser et al., 2003; Halabi et al., 2009; Innis, 2007; Pupko et al., 2002; Pazos et al., 2006; Valdar, 2002).
The Evolutionary Trace (ET) (Lichtarge et al., 1996; Mihalek et al., 2004) identifies functionally important residue positions by finding sequence substitution patterns correlated with divergences among homologs, thereby explicitly taking phylogenetic relationships into account.
ET predictions have been extensively validated experimentally (Onrust et al., 1997; Rajagopalan et al., 2006; Ribes-Zamora et al., 2007; Rodriguez et al., 2010; Shenoy et al., 2006; Sowa et al., 2000, 2001) and through large-scale retrospective predictions of functional sites (Yao et al., 2003) and protein functions (Venner et al., 2010).
These studies point to a number of general and consistent obser-vations in well-structured protein domains: (i) sequence positions may be ranked by evolutionary importance; (ii) most important sequence residues cluster structurally (Madabushi et al., 2002); (iii) these structural clusters predict functional sites (Yao et al., 2003), such that (iv) small structurefunction motifs called 3D templates based on these clusters can predict protein function on a genomic scale (Erdin et al., 2010; Kristensen et al., 2008; Venner et al., 2010; Ward et al., 2008).
The evolutionary prin-ciples that give rise to these useful patterns remain unclear.
This work suggests that epistasis drives these patterns.
Traditionally, epistasis means interactions between genes; how-ever, it is also recognized as a major force in molecular evolution of individual proteins (Breen et al., 2012).
Strong epistatic inter-actions occur between contact residues (Ortlund et al., 2007), presumably because function and adaptation are intimately related to mutual interaction and variation of physically neigh-boring residues.
Indeed, improving the clustering quality of evolutionarily important residues improves predictions of*To whom correspondence should be addressed.
The Author 2013.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
For commercial re-use, please contact journals.permissions@oup.com ) ) ) , ) ) ) ) ) ) ) ) ) ) ) ) )) ) ) 1 2 ) 3 ) 4-Ward etal.
(2008) ) ) but ) ) functional sites (Mihalek et al., 2006a, b; Wilkins et al., 2010).
In that light, the clustering of these residues simply reflects the fun-damental epistatic coupling of neighbors.
These observations motivate a series of hypotheses.
We hypothesize that if epistasis and function constrain residue neigh-bors during selective pressure, then evolutionary importance should distribute smoothly over a protein structure.
If so, opti-mizing ET rank smoothness, for example, by selecting sequences appropriately, should improve predictions of functional sites, molecular determinants and functions.
Thereby, a modified ET algorithm could directly enforce smoothness and improve pre-dictions by focusing primarily on epistatic interactions.
Our results show that, in practice, we can assess ET rank smoothness by treating the structure as a network, or graph, of amino acid nodes, linking these nodes by edges indicating structural contact and applying the discrete Laplacian operator from a graph theory to quantify ET smoothness.
Selections of input sequences that minimize the smoothing function con-structed from the Laplacian operator then led to better func-tional site analyses by ET.
Moreover, a new inherently smoother pair-interaction Evolutionary Trace (piET) algorithm built to measure the importance of neighbor-to-neighbor residue pairs, instead of single residues, improves functional site predic-tions in retrospective study and in an experimental application on Escherichia coli LexAa protein that triggers the SOS re-sponse through which bacteria evolve drug resistance.
Finally, piET improves large-scale functional annotations.
Together, these data show that the smoothest structural distribution of evolutionary importance reflects functional information best, and that epistatic interactions are strongly reflective of the effect-ive distance between residues.
2 METHODS 2.1 Measuring the smoothness of a rank distribution To measure the smoothness of ET ranks over a protein, we treat the structure as a graph.
The nodes of this graph are the residues, and its edges indicate adjacent sequence residues or close contacts in the known structure.
This focus on neighbors is because they will likely experience most strongly the impact of a substitution.
The Laplacian operator (Chung, 1997) is the discrete graph counterpart of the standard Laplacian operator used to measure smoothness in a continuous func-tion, and it is computed with two matrices: the adjacency matrix, denoted A, which specifies which residues contact each other in the protein struc-ture (within a minimum atomatom distance of four Angstroms); and the degree matrix, denoted D, which describes the number of residues adjacent to residue i.
Specifically, A is defined as Ai, j 1 residues i, j in contact 0 otherwise 1 This simple form could eventually be made to account for the number of atomatom contacts, their apparent distances, electrochemical propensities and other attributes of residue neighbor interactions.
The degree matrix, D, is a function of Ai, j Di, j P k Ai, k if i j 0 otherwise 2 The Laplacian operator L is then defined as L D A.
Following standard practice, we may measure the smoothness of any vector field x distributed across the nodes of a graph defined by A through the quadratic form of its Laplacian (Chung, 1997), which is also referred to as the smoothing function, and defined by xTLx X i, j Ai, jxj xi2 3 In this work, the vector field x is the relative evolutionary importance (ET rank) of each residue given by the real-value Evolutionary Trace (rvET) algorithm (Mihalek et al., 2004), which measures the size of a phylogenetic divergence associated with a substitution at each sequence position.
A short review of this algorithm can be found in Supplementary Materials.
By convention, lower values of xTLx indicate smoother distri-butions of the xi over the protein structure, meaning that the difference in ET ranks is smaller between residues that are in contact.
2.2 Functional determinant test set The dataset of functional determinants was taken from a previous work (Wilkins et al., 2010).
The gold standard functional sites for protein ligand interactions are defined by the database PDBsum (Laskowski et al., 2005).
The proteinprotein functional sites are the residues within five Angstroms of the residues in the complexed proteins.
To obtain a multiple sequence alignment (MSA) for each query protein, a set of sequences was retrieved with BLAST (Altschul et al., 1997) (using NCBIs non-redundant protein sequence database, the BLOSUM62 sub-stitution matrix and default parameters).
The top 500 homologs with an e-value better than 0.05 were retrieved from NCBI0s Protein database.
After we generated alignments, the set was curated to remove sequences with sequence identity 526% and length 570% when compared with query.
The homologues were then realigned after curation.
2.3 Measures of overlap and clustering To assess the recovery of known functional sites in proteins, we calculate an overlap z-score zo between top-ET ranked positions and the gold standard functional site, based on the hypergeometric distribution.
We first calculate the mean m and the variance 2 of the hypergeometric distribution m nM N and 2 nMNMN n N2N n 4 where N is defined as the length of the query protein,M is the number of residues that make up the functional site and n is the number of residues that fall under a certain ET rank-coverage.
We then calculate the hyper-geometric z-score zo am , where a is the actual number of functional site residues at a particular ET rank.
Each ET rank can be associated with a distinct z-score.
To access performance at multiple ranks, we developed the overlap measure hzoi, which is the average z-score over ET ranks that fall within a particular coverage range, hzoi 1 K XK i zio 5 Typically, we find that the most useful ET predictions are in the top 20%.
zio is the overlap z-score corresponding to the residues within a certain ET percentile rank i.
The sum is over K unique evolutionary ranks for residues that fall within the top 20% cutoff.
The measure of clustering is calculated in a similar fashion, hzci 1K PK i z i c where z i c are found analytically and have already been discussed at length in Mihalek et al., (2003).
2.4 Sequence selection simulation To test smoothing, 30000 ET analyses ran on randomly constructed MSAs.
Each alignment starts from a default alignment (described in previous section) from which randomly sequences are removed, such that the number of sequences removed was randomly chosen between 2715 Accounting for epistatic interactions improves functional analysis of protein structures ) ) , , the ET .--)--, ) ) )-)-In order ) , to `` '' , In order-, multiple sequence alignment 25 and the total number of sequences in the starting alignment.
The new set of ET ranks leads to unique values of smoothing function xTLx and average overlap z-score hzoi.
The multiple ET analyses are binned based on the value of the smoothing function xTLx.
The average hzoi was then found for the individual bins to evaluate the correlation.
2.5 Residueresidue evolutionary importance To motivate our approach, we reasoned that although mutations operate on individual residues, natural selection filters these mutations based on how they perturb molecular interactions.
Hence, neighboring residues, i, j, should share evolutionary constraints and their importance ranks should be closely related as observed by the clustering of top-ranked residues within proteins (Madabushi et al., 2002) and their mirroring across molecular interfaces (Raviscioni et al., 2005).
If so, we should focus measures of importance directly on molecular interactions rather than on individual residues.
By measuring the evolutionary importance of the link between residues, i, j, we could then infer the importance of i from the average of i, j over all its neighbors j.
In essence, a residues importance throughout evolution would be borne of its epistatic inter-actions with neighboring residues.
To implement this strategy and compute the evolutionary importance of the link between two neighbor-ing residues i, j, we followed an ET strategy.
Residues ranked highly by ET have been shown to knock out (Ribes-Zamora et al., 2007) or swap functions (Rodriguez et al., 2010), while control mutations to poorly ranked residues were neutral.
We can extend this same approach to a pair of residues, where the residue pair i : j is more informative if its sequence variations (among 20 20 400 possible unique states) corres-pond to greater evolutionary tree divergences, i.e.
those that are closer to the tree root.
The piET algorithm therefore applies the standard rvET procedure to pairs of residues within the MSA, to measure these residueresidue patterns in the context of the evolutionary tree.
The evo-lutionary importance of a structural neighbor pair i : j is denoted by i, j where, i, j XN1 n1 1 n Xn g1 n X400 ab1 fgabi, j ln f g abi, j o 6 where fgabi, j is the frequency of the pair of an amino acid ab of a type within group g of the sub-alignment in the n-th set of sub-alignments.
The number of possible nodes in the evolutionary tree is N 1 where N is the number of sequences in the alignment.
The factor 1n was adapted from a previous study (Mihalek et al., 2004) to give weight to the individual sub-alignments based on their location in the phylogenetic tree.
The rvET algorithm couples the phylogenetic tree to the pattern of variation of a pair of residues, viewed as a single evolving unit (Supplementary Fig.S1).
Once the importance of every pair is available, the piET rank i of an individual residue i is calculated by averaging i, j over all its neighbors, i 1 Di, i X j Ai, ji, j 7 As previously defined in Equation (2), Di, i is the number of residues in contact with residue i ( P k Ai, k).
This equation for i factors shared evolution of the contact residues into the ET phylogenetic framework.
2.6 Evolutionary trace annotation To test the piET algorithm in a large-scale application, we substituted it in place of rvET into the Evolutionary Trace Annotation (ETA) algo-rithm and asked whether it improved ETA predictions.
ETA is a suite of programs for automated discovery of protein function based on their structure.
It identifies protein structures that may have identical biochemical functions based on whether they share small structural motifs composed of top-ranked ET residues (Erdin et al., 2010; Kristensen et al., 2008; Ward et al., 2008).
In brief, ETA defines structural motifs by (i) mapping ET ranks onto the surface of a protein structure, (ii) detecting clusters of important amino acids and (iii) selecting six top-ranked amino acids from the cluster.
The geometry of the alpha carbon atoms of these six residues define a 3D template that is then searched for, by geometric similarity, in the protein data bank (PDB) (Berman et al., 2000).
Specificity is enhanced by filtering matches based on evolutionary and structural similarity and ensuring that protein structures match each other reciprocally.
These matches are used to construct a network, as previously described (Venner et al., 2010), in which nodes are protein structures and edges indicate functional similarity, as detected by the ETA algorithm.
We label this network with known functional information and use a diffusion model to control the propagation of those labels through the network, leading to predic-tions of function for protein structures currently lacking function anno-tations.
If using piET instead of rvET causes ETA predictions to improve, it suggests that piET is a more useful metric of evolutionary importance.
2.7 Functional annotation test set The function annotation tests included past query and target sets (Ward et al., 2008; Wilkins et al., 2010).
The query set included 1217 structural genomics enzymes annotated to the third or fourth level of the Enzyme Commission (EC) classification.
The target set is the subset of the 2008PDB90 (Hobohm et al., 1992), which contains 17 234 proteins, which contains 4387 enzymes with four-digit EC annotations.
The com-bination of the query and target sets resulted in a network of 17 952 proteins among which 5105 are annotated as enzymes.
Each protein in the test set was assigned a single enzymatic function.
2.8 Network construction and diffusion Networks were built and predictions followed as previously described (Venner et al., 2010).
Briefly, an ETA template match was converted into a real-valued (edge) weight by averaging the mean evolutionary distance and the rmsd: w 1 rmsd rmsd= rmsd ETScore ETScore=ETScore.
ETA outputs an rmsd and ETScore for each template match.
ETScore summarizes the average difference in evolutionary importance (ET Rank) between matched residues and rmsd is the average distance between the atoms in the structures of the matched templates.
Additionally, rmsd is the average rmsd over all template matches, rmsd is the standard deviation of all rmsds.
Likewise, ETScore is the average ETScore over all template matches and ETScore is the standard deviation of all ETScores.
Graph diffusion passes functional information between proteins that share similar ETA templates (Venner et al., 2010).
We can represent our knowledge of protein enzymatic function as y, a vector of labels repre-senting whether a protein i is associated with a particular EC number (yi).
Diffusion of the available information (in this case EC number) leads to a new label, f. We can solve for f by minimizing the following: f yTf y fTLf 8 In this expression, the first term is the loss function and represents the difference between initial and final labels.
The second term is the smoothness of the new label f in the context of the Laplacian matrix L. The diffusion coefficient balances the loss of the initial labels against the smoothness.
The previous equation has a closed form solution f I L1y 9 where I is the Identity matrix.
The diffusion coefficient is calculated as previously shown (Venner et al., 2010).
2716 A.D.Wilkins et al.-) ) In order ) ) x pair-interaction (piET) multiple sequence alignment in order-) Ward etal.
( 2008 ) ) Erdin etal.
( 2010 ) 1 2 3 ) ) ) ) ) , ) ) s s ) 2.9 Network integration To test for complementary functional information in rvET and piET, the networks were merged into a single network (Tsuda et al., 2005).
We perform diffusion with multiple networks by solving for f I X k kLk1y 10 where Lk represents the Laplacian form of network k. k is weighting factor that represents the importance of each network in the combination.
We can find k by minimizing y TI c P k kLk 1y.
To simplify the minimization problem, we set the additional restriction 1 2 1 (because in this case we have only two networks), and solved using the brute force optimization procedure in the scientific python package (SciPy: Jones, 2001).
We are then able to solve for f for a particular y vector that represents a specific enzymatic function (EC number).
We solve with a different y for each enzymatic function represented in the network, thus associating every protein in the test set with every function.
To compare these values, we normalize to a z-score yi ymean=ystd.
For each protein, the function with the highest z-score is our predicted enzymatic function for that protein, and we use the z-score as a confidence measure in the prediction.
3 RESULTS 3.1 Smoothing the evolutionary importance rank distribution improves functional site predictions To test whether ET rank smoothness correlates with the quality of functional site predictions, we applied the Laplacian operator to the ET rank distributions on 74 diverse proteins bound to various substrates, cofactors, DNA or proteins (see Methods section).
For each protein, a large number of alternative MSAs was randomly generated from a default sequence alignment (Fig.1).
This gave rise to multiple ET rank distributions, each one with its unique smoothness, xTLx and overlap z-score between top-ranked residues and the functional sites annotated in the pdb files (details found in Methods section).
In most cases (81%), the correlation was strong (Fig.1c).
Exceptions included five proteins with inverse correlations (40.4) when the ET clusters identified a functional site other than the one referenced in the pdb file gold standard.
For instance, in the rhodopsin structure [PDBID 1f88], ET found the G-protein interaction determinants instead of the retinal binding site noted in the crystal structure (Berman et al., 2000).
A specialized difference ET analysis would be needed to identify that site, which is specific to visual receptors (Madabushi et al., 2004).
A few pro-teins had small correlation because the functional site prediction was robust and insensitive to the randomization procedure.
Nevertheless, averaging over all 74 proteins, including these anomalies, the smoothest sequence selection improved the smoothing function xTLx by 12.6%; it increased the traditional clustering z-score hzci by 12.9%, and it raised the overlap z-scores hzoi by 8.6%.
In a second sequence simulation experiment (Supplementary Material), we found that the number of sequences had little influence on the correlations and improvement in functional site prediction.
These data show a strong association between improved functional site an-notations and smoother distributions of evolutionary importance rankings.
Fig.1.
To establish that smoother ET ranks are a desirable feature, we showed that smoothness correlated with the quality of functional site prediction.
MSAs of proteins with known functional sites were rando-mized by selecting a random number of sequences and then analyzed with the rvET algorithm.
Every variation in the alignment leads to a new distribution of ET ranks and, in turn, a unique value of the smoothness within the structure (xTLx) and functional site overlap measure (hzoi).
The individual analyses were then binned and counted (black lines) based on the value of xTLx where the average overlap measure (hzoi) for the analyses in each bin was found (green triangle).
Higher hzoi implies better site prediction and lower xTLx implies a smoother distribution of ET ranks over structure.
In both cases there is a steady and strong improve-ment in functional site overlap as smoothness increases, showed by the average overlap z-score hzoi for the corresponding bins in the histogram (green).
(a) In the GTPase Rac structure [PDBID 1e96A, Human] the default MSA (Blue) did not significantly recover the known binding site, whereas the smoother ET ranks from sequence selection did.
(b) By con-trast, in the example the structure for FeS cluster assembly protein sufD [PDBID 1vh4A, E.coli], the default MSA (blue) is already smoother than most of the randomly generated alternatives.
(c) The value of the smooth-ing function xTLx for the random input sequences correlates with func-tional site overlap.
The average correlation over the 74 proteins was 0.65 2717 Accounting for epistatic interactions improves functional analysis of protein structures Integration ) since [ ].
In order In order , multiple sequence alignments ( ) , ) ) , 3.2 New Algorithm identifies functional determinants These results justified a search for an Evolutionary Trace algo-rithm that is inherently smoother, dubbed piET, which was benchmarked and compared with rvET on the same test set used above Section 3.1. piET produced striking gains: 41% better smoothing, evaluated with the quadratic form of the Laplacian rose; 58% better clustering z-scores hzci among top-ranked residues; and 23% better overlap z-score hzoi against known sites.
These functional site prediction improvements were generally consistent across proteins, Figure 2.
Hence, the recovery of functional sites improves significantly with an algo-rithm that measures the importance of residue interactions first, and only deduces the importance of each residue second.
This strategy embodies the notion that smoothness is the byproduct of shared evolutionary constraints among interacting residue neigh-bors.
Its success demonstrates that the phylogenomics of piET brings correlated evolution to light, and that one of its hallmarks is the structural smoothness of evolutionary importance.
To illustrate these gains in a specific example we next turned to Hsp90, a eukaryotic chaperone critical for protein folding and involved in cell cycle regulation, steroid hormone responsiveness and signal transduction among many other processes.
Its func-tions depend on ATP hydrolysis and the crystallized structure [PDBID 1am1] identifies the ATPADP binding sites.
Although rvET identified some residues proximal to this site involved in ATP hydrolysis, piET identifies a much larger evolutionarily important site in that region (Fig.3a), and the overlap z-score hzoi increased more than 2-fold (hzoi 1:91 to 4.39).
In fact, piET also outperforms the rvET optimized by choosing the smoothest outcome after randomization of the sequence input.
In a second example, piET predicted the proteinprotein inter-face for the growth hormone and hormone receptor complex [PDBID 1a22] better.
Although the rvET had picked important residues in this functional region of the growth hormone, the evolutionary important site with piET is better resolved (Fig.3b) and statistically more significant (hzoi 0:625 to 2.51).
In a third example, rvET found the dimer site of the sufD structure [PDBID 1vh4] well, and no randomization of the input sequences could improve this result.
Yet, piET sharply raises the statistical significance of the site (hzoi 6:97 to 8.95), Supplementary Figure S3.
These representative examples show that piET is inherently smoother than rvET, and that this trans-lates into better clustering among top-ranked ET residues and better functional site identification.
3.3 Highlighting functional regions in LexA To demonstrate functional site prediction, piET was next focused on LexA, a well-studied protein that regulates the SOS response to DNA damage in E.coli (Butala et al., 2009).
On direct inter-action with recombinase A (RecA), LexA dimers self-cleave their DNA binding domain and thus lift transcriptional repression of more than 40 genes, including some that mediate error-prone DNA repair and subsequent escape from genotoxic stress (Butala et al., 2009).
The DNA binding and catalytic sites of LexA have been identified but not its RecA interaction site.
Although recently rvET suggested a novel composite LexA bind-ing site on RecA (Adikesavan et al., 2011), no such candidate site is apparent on LexA.
First, piET improved the identification of the known DNA binding site and active site of LexA.
While rvET for the most part does not find a cluster of top-ranked residues at the DNA binding site, except for a few nearby residues (Fig.4a, left panel), piET fully recovers that site (Fig.4a, right panel).
The statistical significance of these predictions (Supplementary Fig.S4) were similar regardless of whether the reference LexA structures was bound to DNA (as in PDBID 3jsp) or not (as in PDBID 1jhh).
Moreover, this improvement is not at the expense of loss of ET signal elsewhere in the protein: piET identifies the catalytic active site even better than rvET (Supplementary Fig.S4).
Thus, pre-viously characterized sites of LexA are better resolved by piET.
Next, we considered a small novel cluster of residues on the LexA structure identified by piET, shown in Figure 4b.
piET ranked these residues as 14% more evolutionarily important (rmsd is 3%) on average than rvET.
They are in immediate con-tact with each other and form a tight cluster, therefore fulfilling a hallmark of a functional site not previously recognized.
Previously, a single E170V mutation at this site proved import-ant for LexA self cleavage (Lin and Little, 1989).
To extend this observation, we performed additional mutations within the piET-identified site neighboring E170.
These mutations dis-turbed LexA function in response to ultraviolet-induced DNA damage, confirming that these residues form a previously unrec-ognized LexA functional site (Fig.4c).
Together these data show that piET pinpoints functional residues and active sites signifi-cantly better than rvET, even in a complex multifunctional pro-tein.
In LexA, this leads to the discovery of a novel functional site, possibly pointing to a binding site for RecA.
3.4 piET improves annotation of enzymatic function To test whether piET also captures functional information on a large scale, we constructed separate function prediction networks with rvETETA and piETETA.
These contained 17 952 pro-teins (nodes), and 115784 and 114 542 ETA matches (edges) in the piET and rvET networks, respectively.
The diffusion model (Venner et al., 2010) predicted enzymatic function and confi-dence scores on a test set of 1070 structural genomics enzymes-2 0 2 4 6 8 10 12 0 10 20 30 40 50 60 70 z o , Fu nc tio na l S ite O ve rl ap  74 Proteins in order of increasing functional overlap New algorithm improve functional site overlap piET Fig.2.
Smoothing the distribution of ET ranks in the protein structure improves the detection of functional residues.
A set of 74 proteins was tested for improvement in functional site detection with the piET algorithm.
The figure shows the consistent improvement in overlap z-score hzoi for the individual proteins in the test set 2718 A.D.Wilkins et al.
to ,-two-very scherichia ) Up-over )-, )--, ) In order UV In order--, , , ) with existing annotations, based on 5105 annotated proteins in the network.
Whenever possible, these predictions were up to the fourth level EC number, which describes not only the chemical reaction but also its substrate.
In this test, the piET algorithm performed slightly better, with a small improvement (Supplemen-tary Fig.S5) in area under the curve (AUCpiET 0:921 compared with AUCrvET 0:914).
To test whether these piET and rvET networks were redun-dant or complementary, we merged them into a single network (Tsuda et al., 2005).
This method creates a weighted combination based on the connectivity of the individual Laplacian matrices and without need for training.
The network mixture coefficients were rvET 0:37 and piET 0:63.
The first incorrect prediction of this combined network occurs at 8.1% coverage, and it is preceded by 86 correct ones (Supplementary Fig.S5).
By con-trast, the first incorrect prediction the rvET or piET networks alone occurred at 30 and 31, respectively.
This is of practical importance, as the high confidence predictions are generally the ones we would act on experimentally.
The individual algo-rithms mix in mistakes sooner, and by merging networks we can reduce mistakes.
At 100% coverage, the merged network method was 4.3% more accurate and the area under the curve improved to AUCrvETpiET 0:945.
Both the rvET and the piET algorithms for detecting evolutionary importance focus the ETA on different but complementary functional sites.
ETA per-formed best when the algorithms were integrated, showing that each algorithm is providing relevant but unique functional information.
4 DISCUSSION This study adds in three significant ways to a long-term effort to identify functional sites.
First, we show that the spatial distribution of evolutionary information (measured here by ET rank) in a folded structure is smooth.
This complements the Fig.3.
Functional site prediction improves with piET algorithm.
The piET algorithm (red) produces a smoother distribution and captures the known functional site better than both the rvET algorithm (blue) and the simulation (green).
(a) The top 10% residues for Hsp90 chaperone [PDBID 1am1] are marked on the protein surface for algorithms, rvET and piET.
The piET algorithm scored more top-ranked residues close to the known proteinligand site with ADP as shown.
(b) The proteinprotein interface of hormone and receptor complex [PDBID 1a22] is better identified with the new algorithm.
The residues ranked in the top 20% for the respective algorithms, piET and rvET, are shown in prismatic color where the residues marked red are the most evolutionarily important residuess Fig.4.
The piET algorithm provides better biological understanding of LexA.
(a) The piET algorithm identifies the DNA binding site of LexA better when compared with the rvET analysis (PDBID 3jsp).
The residues deemed to be in the top 30% are colored based on evolutionary import-ance where red is considered the most important.
(b) piET identifies a novel cluster of residues.
The rvETpiET difference scale is calculated by taking the normalized difference of the rank percentiles.
Residues are marked red (piET) or blue (rvET) when the residue is significantly more important to respective method.
(c) Mutations at this new LexA site disrupt DNA damage survival.
*P50.05, **P50.01 and ***P50.001 2719 Accounting for epistatic interactions improves functional analysis of protein structures to ) since  original notion of ET clusters (Lichtarge et al., 1996) with a mathematically simple interpretation that lends itself to compu-tation via the Laplacian operator of a graph.
This discrete Laplacian operator is fundamental to networks (Chung, 1997), and here it enables optimization in sequence selection better than the diverse measures of clustering used before (Wilkins et al., 2010).
These were entirely empirical and useful to suggest the simplifying notion of smoothness.
We show when we consider the functional linkage between residues, we can better interpret sequence information.
The second improvement builds on this notion of smoothness to develop an algorithm that focuses on a residues interactions with neighbors.
The method first scores the importance of these interactions and then averages over the neighbor interactions to give the total importance of each residue.
This is consistent with prior suggestions (Gutteridge et al., 2003; Raviscioni et al., 2005) that natural selection operates based less on the intrinsic charac-ter of an amino acid, than on the nature of its couplings to other residues, here primarily those in its immediate surrounding.
Previous studies have noted improvement in predictions when they average evolutionary information for residues over sequence (Capra and Singh, 2007; Pei and Grishin, 2001) and structure (Panchenko et al., 2004; Teppa et al., 2012).
We add to this work by quantifying the shared evolutionary pattern be-tween residues near in structure.
These interactions are the essence of the residues function.
Though the method is currently limited to structural information, we can use the constantly im-proving homology-modeling algorithms (Roy et al., 2010) or databases of pre-computed homology models (Bordoli and Schwede, 2012).
Third, we show how these results follow logically from epistatic interaction among residues.
Other methods focused on pairwise interactions via covariation (Pazos and Valencia, 2008), thermo-dynamic (Maksay, 2011) or energetic coupling (de la Lande et al., 2010).
Networks of such correlations often lead to clusters of pathways although their interpretation is not straightforward (Chi et al., 2008).
By contrast, clusters of ET residues lead to functional sites shown independently to be highly significant com-pared with other methods [see Supplementary Materials in Rausell et al.
2010, and extensively tested experimentally in a large variety of proteins (Lichtarge and Wilkins, 2010)].
These validations included mapping and then recoding of allosteric determinants of both interprotein and intraprotein signaling path-ways (Rodriguez et al., 2010).
In summary, this work finds and exploits the fact that epistatic forces mold evolution and, as a result, leads to the smooth dis-tribution of evolutionary importance throughout protein struc-tures.
This smoothness stems from the functional linkage of residues typically nearby in conformation, a hallmark of epista-sis.
This basic property leads to new algorithms for computing the evolutionary importance of (a) residueresidue interaction among neighbors, and (b) individual residues.
In turn, this sub-stantially improves functional site analysis and function predic-tion in test sets while also verified experimentally by predicting a novel site in LexA.
This should prove useful in guiding protein engineering and mutations to the most relevant parts of a protein.
A server performing piET calculations is available at our site: http://mammoth.bcm.tmc.edu/uet.
ACKNOWLEDGEMENT The authors thank Panagiotis Katsonis, Andreas M. Lisewski and Ilya Novikov for helpful discussion.
Funding: National Institutes of Health (NIH-GM079656, NIH-GM066099, NLM 5T15LM07093); National Science Foundation (NSF CCF-0905536, NSF DBI-1062455).
Conflict of Interest: none declared.
ABSTRACT Motivation: We noted that the sumoylation site in C/EBP homologues is conserved beyond the canonical consensus sequence for sumoylation.
Therefore, we investigated whether this pattern might define a more general protein motif.
Results: We undertook a survey of the human proteome using a regular expression based on the C/EBP motif.
This revealed significant enrichment of the motif using different Gene Ontology terms (e.g.
transcription) that pertain to the nucleus.
When considering requirements for the motif to be functional (evolutionary conservation, structural accessibility of the motif and proper cell localization of the protein), more than 130 human proteins were retrieved from the UniProt/Swiss-Prot database.
These candidates were particularly enriched in transcription factors, including FOS, JUN, Hif-1, MLL2 and members of the KLF, MAF and NFATC families; chromatin modifiers like CHD-8, HDAC4 and DNA Top1; and the transcriptional regulatory kinases HIPK1 and HIPK2.
The KEPE motif appears to be restricted to the metazoan lineage and has three length variantsshort, medium and longwhich do not appear to interchange.
Contact: toby.gibson@embl.de Supplementary information: Supplementary data are available at Bioinformatics online.
1 INTRODUCTION Members of the ubiquitin multiprotein family function as covalent modifiers of other proteins.
These post-translational modifications (PTMs) then cause the target protein to be relocated to another subcellular location (Dye and Schulman, 2007).
In the case of SUMO (the small ubiquitin-like modifier), attachment can affect processes including gene transcription and cell-cycle progression, although the mechanisms by which relocation within the nucleus achieves this are far from clear (Geiss-Friedlander and Melchior, 2007).
While sumoylation seems largely to be restricted to the nucleus, a few non-nuclear proteins have been proposed to be sumoylated (Watts, 2004).
SUMO substrates are often difficult to To whom correspondence should be addressed.
validate due to the low stoichiometry of the SUMO modification: however, proteomic approaches have lead to the identification of many putative substrates (reviewed in Rosas-Acosta et al., 2005).
Like other PTMs, sumoylation occurs at accessible linear motifs (LMs), usually in regions of natively disordered polypeptide (reviewed in Diella et al., 2008).
Sumoylation occurs on a lysine in a motif that can be described by the pattern K.E where = hydrophobic (Girdwood et al., 2004; Rodriguez et al., 2001) or the regular expression [VILMAFP]K.E as used in the ELM linear motif resource (Puntervoll et al., 2003).
Sumoylated proteins that do not have the classical consensus motif have also been reported (Zhou et al., 2006).
The K.E pattern matches nearly half of the proteins in Swiss-Prot (Yang et al., 2006), indicating that most of the matches are false positive.
As a consequence, there have been several attempts to try to extend this motif to get more specificity, resulting in the identification of different extended SUMO consensus motifs.
Thus, the phosphorylation-dependent sumoylation motif (PDSM) K.E..SP has been described in a subset of substrates, mainly transcriptional regulators: the phosphorylation of the SP motif regulates the interaction between the substrates and the SUMO-conjugating machinery, promoting sumoylation of the substrates (Hietakangas et al., 2006).
In a second analysis, a cluster of acidic residues downstream from the core of many SUMO sites has been shown to be important for substrate binding and subsequent sumoylation (Yang et al., 2006).
The importance of negative charges was also identified in substrates like Elk-1 and LRH-1; this extended SUMO consensus motif was named NDSM, negatively charged amino acid-dependent sumoylation motif.
The C/EBP transcription factors regulate cellular proliferation and differentiation of a range of cell types.
They have been described as both tumour promoters and tumour suppressors, indicating that their regulatory system is complex (Nerlov, 2008).
In C/EBP, a regulatory domain motif (RDM) has been shown to inhibit the activity of an activation domain in a position-independent, but dose-dependent manner.
The RDM was characterized by the consensus [VIL]K.EP and it was shown that sumoylation of lysine at position 2 decreases its inhibitory function in vitro (Kim et al., 2002; Nerlov, 2008).
A major hindrance to bioinformatic investigation of LM occurrences is that simple database searches do not yield significant 2008 The Author(s) This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.0/uk/) which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
F.Diella et al.
results, while the false instances of motifs vastly outnumber the true ones.
However, with improved sequence database annotation, LMs can sometimes be significantly enriched with certain keywords.
Thus, Copley used transcriptional keywords to detect and justify new examples of the EH1 transcriptional repressor motif (Copley, 2005).
A similar approach in combination with disorder prediction and conservation scoring, has shown that KEN-box destruction motifs are significantly enriched in the set of UniProt/Swiss-Prot entries annotated with cell-cycle keywords and Gene Ontology (GO) terms (Michael et al., 2008).
In this article, we report a computational investigation based on the RDM of C/EBPs.
We refined the motif in the aligned C/EBP RDM sequence segments and then deployed a protocol involving keyword enrichment, native disorder prediction and conservation scoring in a survey of protein sequence databases.
Highly significant results for motif matches were obtained with sets of entries annotated with keywords, such as nucleus, transcription and chromatin.
The conservation pattern of the C/EBP motif was found to be the archetype of a linear motif, which we term KEPE, which is present in many nuclear proteins of the metazoa.
2 METHODS 2.1 Motif search The interactive motif search tool SIRW combines regular expression searches with keyword searches of text annotation (Ramu, 2003).
SIRW (sirw.embl.de/) was used to explore the human UniProt/Swiss-Prot database (release 54.7; 18054 entries for Homo sapiens) with the C/EBP-derived KEPE regular expression, [MILVFT]K.EP.{1,4}[DE].
To limit the search space, relevant annotation terms, such as nucleus, transcription and chromatin were used to search the fields for the GO terms [the Gene Ontology annotation (Harris et al., 2004; The_Gene_Ontology_Consortium, 2008)] as well as the separate keyword (KW) fields for keywords.
(Note that the GO and KW searches are not formally equivalent because the GO terms include longer phrases in the definitions, while the annotators are also likely to have used different guidelines.)
In order to assess the significance of the relative enrichment, we calculated P-values using Fishers exact test (available in Excel and the R package).
2.2 Motif permutation controls It is important to exclude artefactual or trivial reasons for motif enrichment, such as a bias in favour of the amino acids K, E and P. Therefore, in order to control for the background frequency, we permuted the three fully conserved residues in the KEPE motifs (KPEE; EKPE; EPKE; PEKE; PKEE) as well as a specific permutation (KEEP) for the two residues P and [DE] that extend the SUMO motif.
Then we examined them for keyword association and conservation score (CS) as for the KEPE.
Results for the controls are presented in detail in the Supplementary Material.
2.3 Modular protein architecture context Motif matches were evaluated for presence in known globular domains using the Pfam and SMART domain databases (Letunic et al., 2006; Sammut et al., 2008).
IUPred (http://iupred.enzim.hu/) (Dosztanyi et al., 2005) was used to test whether the motifs were found in predicted globular or natively disordered regions (also known as IUP, intrinsically unstructured polypeptide).
Using the IUPred long parameter setting to predict longer stretches of disorder, flanking regions of 15 amino acids upstream and downstream of the motif were scored, applying a value of 0.4 as the cut-off threshold.
2.4 Evolutionary conservation Each match of the KEPE and the permuted motifs in Swiss-Prot proteins was also scored for conservation in homologous proteins using the CS method described in Chica et al.
(2008).
This approach has already been applied for the KEN box motif (Michael et al., 2008).
The dataset used for calculating the CS included proteins (i) that are annotated to be in the nuclear or cytoplasmic compartment and (ii) whose motif match is found in a disordered/unstructured region according to the IUPred prediction.
To compare the CS distribution between KEPE and the permutation controls, we used the KolmogorovSmirnov (KS) goodness of fit test.
2.5 Proteome analysis We wrote a script to analyse the frequency of the motif and its permutations in human and yeast proteomes.
We downloaded all proteins having associated GO terms from the EnsEMBL resource for H.sapiens (Hubbard et al., 2007) and from the SGD resource (Hong et al., 2008) for Saccharomyces cerevisiae and we obtained two datasets of 16 504 and 5327 proteins, respectively.
Subsequently, we ran IUPred using the longparameter as before (Dosztanyi et al., 2005).
The ELM conservation filter (Chica et al., 2008) was then applied to assess the conservation of the matches.
3 RESULTS 3.1 Survey of Swiss-Prot with the KEPE regular expression Using an alignment of Drosophila C/EBP and the four vertebrate paralogues C/EBP,-,-and-(Fig.1, Supplementary Fig.1) we noted that downstream of the RDM motif [VIL]K.EP there are some additional conserved acidic residues, especially in positions 3 and 4 after the proline.
Earlier studies have partially described the motif that matches the observed sequence conservation (Kim et al., 2002).
Fig.1.
The KEPE motif in C/EBP transcription factors.
(A) The IUPred plot predicts human C/EBP to be almost entirely natively disordered (the higher the peak, the more disordered).
Like the KEPE motif, the leucine zipper (BRLZ) is also predicted as natively disordered (correctly so, since it must dimerise as coiled coil to acquire a stable folded structure).
(B) C/EBP KEPE motif: an alignment of the RDM motif from Drosophila C/EBP and four vertebrate paralogues C/EBP,-,-and-(CEBPA, CEBPB, CEBPD and CEBPE) show the conservation of the motif (K is the sumoylated residue).
2 We term the motif KEPE after the conserved residues.
The motif is not in a known globular domain but rather in a region predicted to be natively disordered (Fig.1A).
To investigate if the observed motif could be present in other proteins, we undertook a survey for the KEPE-bearing proteins in the human entries of the UniProt/Swiss-Prot database (The_UniProt_Consortium, 2008) using the motif [MLIVFT]K.EP.{1,4}[DE].
We evaluated whether the matching proteins were found in the nuclear compartment or more widely.
Since most LMs are known to be in natively disordered polypeptide segments (Fuxreiter et al., 2007), the KEPE matches were also evaluated for a clash with known globular domains using the SMART server (Letunic et al., 2006) or in predicted globular structure reported by IUPred (Dosztanyi et al., 2005).
Of 331 human proteins matching the KEPE regular expression, 168 were annotated as localized in the nuclear compartment.
Of those, more then 130 had KEPE matches localized in non-globular regions according to the IUPred prediction.
These sequences were enriched in the functional classes transcription factor or chromatin modifierand in the GO class protein function related to transcription.
In only three cases was the KEPE motif found within a known globular domain according to SMART prediction.
In these three paralogous bromodomain and PHD-finger containing proteins BRD1, BRF1 and BRF3 (Swiss-Prot:O95696, P55201, Q9ULD4), the KEPE motifs fell within PHD-finger domains.
Since these KEPE motifs were found in the most variable loop of the PHD-finger (where an insertion of 15 residues or more is often found, SMART: SM00249), they could be potentially accessible for interaction.
Results of combined motifkeyword searches with SIRW are summarized in Table 1: Several terms show enrichments that Table 1.
Enrichment of KEPE motif matches with various term combinations from the KW and/or GO term fields in Swiss-Prot entries Keywordsa,b No.
Total Total P-value P-value KEPEc Hd Sd He Se Homo sapiens 331 18 054 7707  GO = cytoplasm*!nucleus 35 2303 1146 2.45E01 2.67E02 or KW = cytoplasm*!nucleus* GO = nuclear*|nucleus| 168 3608 1789 2.10E36 9.41E29 nucleolus!cytoplasm* or KW = nuclear*!cytoplasm* |nucleus!cytoplasm* Link = znf* 79 1577 799 6.82E17 4.01E13 Link = bzip* 17 9 35 7.58E18 8.62E15 GO = transcription* 90 1239 627 7.43E31 2.36E26 GO = chromatin* or 21 213 139 4.12E10 4.08E07 KW = chromatin* GO:0003700 36 506 264 3.55E12 4.65E10 (transcription factor activity) GO:0008270 9 164 74 3.32E03 4.20E03 (zinc ion binding) GO:0006355 17 253 131 4.60E06 4.11E05 (Regulation of transcription, DNA-dependent) aSearch terms which have been used to retrieve the KEPE sequences.
bIn Swiss-Prot the annotation cytoplasm is used (incorrectly) as a synonym for cytosol.
cNumber of sequences matching the KEPE pattern [MLIVTF]K.EP.
{1,4}[DE] used in combination with the various search terms.
dTotal number of sequences (as obtained with the SIRW search tool, in Swiss-Prot release 54.7) matching the search terms shown in the left column.
(H): all human sequences; (S): human sequences with the SUMO motif [VILMTF]K.E.
eThe P-value for the relative enrichment was calculated by the Fishers exact test from the R package (for total H and total S).
are highly significant according to the Fishers exact test.
The enrichment was particularly significant with the transcription, bzip and znf keywords as well as for nuclear compartment.
There is a possibility that this enrichment could be driven by the high background frequency of the embedded SUMO motif.
In order to test this possibility, we calculated the enrichment using human sequences matching the SUMO motif as the background distribution.
The P-values S in the right column show that the enrichment is still significant.
Significant enrichment is not, per se, proof of function and could be for a trivial reason, such as strong amino acid bias or 4-fold increased mean protein length in transcriptional proteins.
This can be controlled for by using test motifs that contain the same amino acids and information complexity, which can be obtained by permuting residues in the motif.
When we performed the same analysis using permuted motifs, we found moderate enrichment for some keywords (see Supplementary Tables 1 and 2) but KEPE enrichment is always greater.
Genuine LMs that function in cell regulation are found to be conserved in homologous proteins (Neduva and Russell, 2005).
Therefore, we applied the ELM CS pipeline (Chica et al., 2008) to assess KEPE motif conservation.
Figure 2 compares the distributions of CS values for matches to KEPE and its permuted motifs; the comparison was repeated for nuclear and cytoplasmic proteins.
In the nuclear set, the KEPE motif shows much stronger conservation than the permuted motifs.
Furthermore the CS distributions of all permuted instances are significantly different to the KEPE distribution with P-values ranging from 0.00 to 0.01 (Fig.2A).
This result strongly supports a predicted function for KEPE in a nuclear role.
We were worried that matches in multiprotein families might have skewed the results in favour of the KEPE motif.
Therefore, the set of sequences matching the KEPE and the permuted motifs were checked for the number of paralogous assignments retrieved using EnsEMBL mappings (Hubbard et al., 2007).
Most of the matches are in proteins with one or no paralogues (Supplementary Fig.2).
This result shows that the higher frequency observed for the KEPE matches in the maximum CS range is not artificially caused by a higher number of paralogues in the corresponding protein families.
This implies that the number of KEPE matches appearing in paralogues of the same protein reflects their functional value and not a tendency of those protein families to have more paralogues.
The non-significant differences obtained for KEPE versus the motif permutations for proteins annotated as cytoplasmic serve as a negative control (Fig.2C).
Indeed, here the KEPE instances are as non-conserved as the permuted ones, consistent with a lack of functionality in the cytosol.
3.2 Surveys of human and yeast proteomes with the KEPE regular expression Although Swiss-Prot GO terms are also mapped to the EnsEMBL human proteome via the GOA database (Camon et al., 2004), EnsEMBL provides additional electronically generated GO annotation.
The EnsEMBL human proteome is also more complete than in Swiss-Prot.
Since the Swiss-Prot searches were interactive, we wanted to evaluate whether a fully automated proteome pipeline could produce qualitatively similar results.
As shown in Supplementary Fig.3, an equivalent GO termIUPredCS 3 F.Diella et al.
assessment protocol yields an even stronger nuclear conservation plot than for the interactive Swiss-Prot survey (Fig.2A).
An automated pipeline allows the component stages to be evaluated separately.
The keyword and the IUPred assignment steps each Fig.2.
Conservation score distributions for the KEPE motif and the six permutations comparing the nuclear and cytoplasmic compartments.
KEPE bearing proteins were retrieved from UniProt/Swiss-Prot with the compartment keyword expressions in Table 1, processed for IUPred disorder prediction and evaluated for the ELM CS score.
To enable comparison, the sets have been normalized into percentages and sorted into five CS score bins.
The table in (A) shows that the KEPE matches have a significantly different conservation distribution in the nucleus compared with the controls.
n = number of instances, D = maximum difference between the cumulative distributions, P-value = significance of the difference D, according to the KS test.
KEPE matches also show a peak of strong conservation (unmatched by the controls) in the nucleus (B) but not in the cytoplasm (C).
These results are consistent with a lack of functionality of the KEPE motif in the cytosol (since this is the implied meaning of cytoplasm in Swiss-Prot).
individually contributed clear enrichment of conserved motifs, affirming their individual and combinatorial relevance to motif prediction (Supplementary Fig.3).
The annotation of the yeast S.cerevisiae proteome in the SGD project is also extensive.
The CS distributions were obtained for the yeast proteome using the same pipeline.
In this case neither the keywords, nor the IUPred assignments provide any support for the KEPE motif, relative to the permutation controls.
Moreover Supplementary Figure 4 shows that most of the matches of KEPE and the permuted motifs are non-conserved in yeast.
In addition, the number of matches to the KEPE motif in the yeast proteome is lower than expected (55) compared with the human proteome (331).
This result is independent from the distributions of the K, P and E amino acids since their distributions are very similar in the human and yeast proteomes (Echols et al., 2002).
Therefore, the difference in the number should depend only in the total sequence length of both proteomes.
While the human to yeast proteome length ratio is 3.66, the ratio of the number of retrieved matches is nearly the double, 6.01.
Thus, our protocol was unable to provide any evidence in favour of the existence of KEPE motifs in yeast.
Manual screening of KEPE matches for plant, fungal and other non-metazoan protein entries in UniProt/Swiss-Prot likewise failed to provide evidence for plausible KEPE motifs.
We surmise that KEPE motifs arose and proliferated in the metazoan lineage.
3.3 Three KEPE length variations Inspection of the KEPE motif conservation in individual protein families showed that there are three length variants in the flexible gap after the P and preceding the last conserved negatively charged position.
Typical KEPEs as in C/EBPs allow a 23 residue gap.
Juns have longer variants and some Mafs have shorter variants (Supplementary Fig.5).
In all the alignments examined, the three variant motifs were never observed to interconvert during evolutionary change (although they are often found superimposed, e.g.
in C/EBP, NFATC1-3, TOP1, HDAC4, FOS, see Supplementary Table 3).
This curious behaviour suggests that the length variants are functionally distinct, perhaps in a subtle way: for example, they could be recognized by different paralogous proteins; or they might all be recognized by the same protein but be modulated by interactions with distinct additional factors.
3.4 KEPE-bearing proteins KEPE motifs are mostly found in transcription factors and proteins that are broadly involved in modifying chromatin conformation.
Therefore, a role in modulating gene expression seems to be inevitable.
The highest KEPE enrichment is in the leucine zipper class of transcription factor, where 30% possess the motif.
As many KEPE sites are known to be sumoylated (Supplementary Table 3), a clear inference is that all KEPE sites are modified by sumoylation.
The motif is sometimes found to be conserved in orthologous proteins for more then 500 million years, as in C/EBPs from Drosophila and vertebrates (Fig.1B).
However, in many paralogous gene families that originated with the genome expansion associated with the origin of the vertebrates (Gibson and Spring, 1998; Kasahara, 2007; Meyer and Van de Peer, 2005), KEPE motif evolution is much more dynamic.
In the Fos transcription factor family, KEPE is conserved in cFos and Fra2 but absent from FosB and Fra1.
It is found in HDAC4 and 9 but not in other 4 KEPE histone deacetylases.
There can be from 0 to 3 KEPE motifs in various NFATC transcription factor paralogues.
Several Klf zinc-finger proteins have KEPEs in separate non-superposable locations: these motifs are likely to have independent origins by point mutation within large natively disordered polypeptide segments.
The KEPE motif is larger than the sequence conservation associated with sumoylation sites.
It is possible that the additional conserved residues might be important to (i) be recognized by other binding proteins and/or (ii) in regulating the modification of the motif in other ways, e.g.
by lysine acetylation, methylation or ubiquitinylation.
[Thus, the tumour suppressor HIC1 can be sumoylated on a lysine which is also a target for acetylation, suggesting that this motif might represent a sumoylation/acetylation switch (Stankovic-Valentin et al., 2007).]
The simplest model for KEPE function would be for a KEPE-binding protein to block access to the sumoylation site.
Since sumoylation is reported to relieve transcriptional inhibition by the RDM element of C/EBP (Kim et al., 2002), unsumoylated KEPE should be bound by a protein that acts as a repressor (at least in this context).
Since many of the KEPE proteins are assigned as chromatin modifiers, rather than as transcription factors per se, such a shared system of repression would be expected to be interlinked to chromatin conformational state.
Experimental identification of the ligand proteins binding to the short, medium and long KEPEs may provide a new perspective on gene regulation.
4 CONCLUSIONS LMs constitute nodes in cell regulatory networks that are acted upon by regulatory and signalling proteins and their domains.
Here, we describe a new linear motifKEPEthat is widespread in metazoan nuclear proteins classified as transcription factors or chromatin modulators.
KEPE function is expected to regulate sumoylation, a proposal, which may be tested experimentally by biochemical and genetic means.
Since KEPE is a common motif, elucidation of its function will have broad significance for understanding gene regulation in animals.
ACKNOWLEDGEMENTS We thank the contributors to the ELM resource for making in silico linear motif discovery feasible, Pl Puntervoll, Rein Aasland and Manfred Koegl for checking interaction networks for any hints to the ligand, Evangelos Pafilis for help with the Ontology Lookup Service and Niall Haslam for critically reading the article.
Funding: EU EMBRACE (LHSG-CT-2004-512092).
Conflict of Interest: none declared.
ABSTRACT Motivation: Describing biological sample variables with ontologies is complex due to the cross-domain nature of experiments.
Ontologies provide annotation solutions; however, for cross-domain investigations, multiple ontologies are needed to represent the data.
These are subject to rapid change, are often not interoperable and present complexities that are a barrier to biological resource users.
Results: We present the Experimental Factor Ontology, designed to meet cross-domain, application focused use cases for gene expression data.
We describe our methodology and open source tools used to create the ontology.
These include tools for creating ontology mappings, ontology views, detecting ontology changes and using ontologies in interfaces to enhance querying.
The application of reference ontologies to data is a key problem, and this work presents guidelines on how community ontologies can be presented in an application ontology in a data-driven way.
Availability: http://www.ebi.ac.uk/efo Contact: malone@ebi.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.
Received on November 24, 2009; revised on February 4, 2010; accepted on March 1, 2010 1 INTRODUCTION The description of experimental variables, even within a single discipline, involves the use of many cross-domain concepts.
For example, describing the characteristics of a single sample in an experiment can use terminology from cell biology, proteomics, transcriptomics, disease, anatomy and environmental science.
This is not a new problem and it is not restricted to bioinformatics.
However, it is pressing within this domain due to the quantity of heterogeneous data available in different formats across multiple resources (Schofield et al., 2009).
The desire to integrate data generated with different experimental technologies and in different biological domains motivates our work.
Experimental descriptions are captured and made available as text within database records, published papers and web site content.
These descriptions contain latent semantic information that is hard to extract and reflects the natural language of the domain.
One solution to this problem is the use of a controlled vocabulary to describe the data.
With this approach, the terminology used in a particular context is restricted to a set of terms that define important aspects of a domain or application.
Ontology adds an extra layer of expressivity by To whom correspondence should be addressed.
structuring this vocabulary into ontological classes and by specifying the sorts of operations that can be performed on them.
Importantly, the ontological models produced from this process are expressed in a language that enables human understanding and computational reasoning over the representation.
Languages such as the W3C recommendation Web Ontology Language (OWL) (Horrocks et al., 2003) aid interoperability by standardizing the syntax across all domains.
Advantageously, validation of this OWL representation can also be performed through the use of description logic reasoners (Sirin et al., 2007).
In bioinformatics, the interest in ontologies to model domain knowledge is apparent from the steadily increasing number of groups developing them.
In an attempt to align these efforts, the OBO Foundry (Smith et al., 2007) provides useful guidance on best practice for developing ontologies in the biomedical domain.
This includes the creation of orthogonal reference ontologies, from which classes are considered defining units of the area they describe.
Although this is a worthwhile longer term aim, the state of the art is that existing ontologies are not orthogonal or interoperable, and many present a focus that is unsuitable for gene expression data.
They can, however, be used to construct application ontologies that focus on describing and structuring a data space for a particular application.
While a vision of full interoperability between ontologies overcomes some of the barriers to integration, there still remain unresolved issues for data-driven applications.
Cross products, i.e.
classes composed of two or more existing classes (formally in OWL, the intersection of two or more classes), are required between existing ontologies to more accurately describe omics data.
For example, a cell type in a given tissue or the transcription factors within a pathway activated in a disease state.
Few cross products are available to date partly because many ontologies do not use a common upper level ontology.
Where there are non-orthogonal ontologies, those that best describe a dataset of interest typically do not have the necessary cross products.
Furthermore, combining even ontologies that are interoperable can present problems.
Ontologies such as FMA (Rosse and Mejino, 2003) contain tens of thousands of classes, combined with other ontologies such as Gene Ontology (GO) and Disease Ontology (Osborne et al., 2009), and this presents a large model to consider; this is a particular problem if description logic reasoners are used for consistency checking and inference.
The use of multiple ontologies to annotate experimental data brings with it a considerable overhead.
Consider an annotation example, where a biological user submitting data needs the term lymphoma.
BioPortal (Noy et al., 2009) returns 629 matches from 24 ontologies.
The casual user is not equipped to select from these The Author(s) 2010.
Published by Oxford University Press.
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/ by-nc/2.5), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.
[12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1113 11121118 Experimental Factor Ontology non-orthogonal ontologies and selecting a more specific child term is more problematic; the Disease Ontology alone has 16 subclasses.
In other cases, such as genetic disease, many diseases are not present in the Disease Ontology or SNOMED despite their large size.
Inconsistent use of synonyms also presents a problem, as synonyms are often for a more or less granular term in another ontology.
Another consideration is that ontologies change over time and managing this in the context of annotations is problematic.
Mechanisms are therefore required to help manage this.
Representation of biological ontologies is necessarily complex as they have multiple purposes; explicitly modeling biological relationships, aiding interoperability with other ontologies and facilitating data annotation, to name a few.
This complexity is a barrier to the consumer of ontology annotated data as they may be unfamiliar with the language, constructs and labels used.
Consider the class information content entity from Information Artifact Ontology (IAO) (http://purl.obolibrary.org/obo/iao) defined as an entity that is generically dependent on some artifact and stands in relation of aboutness to some entity.
Such a definition may be incomprehensible to a biologist, yet is an important class in Experimental Factor Ontology (EFO).
A user-friendly view on upper level ontology is thus required.
In this article, we describe our data annotation and query use cases.
We present an application ontology, the EFO, which fulfills the use cases in the context of gene expression data; the methodology and tools that we have developed to produce the ontology are also described and are freely available.
We also illustrate the novel cross-product classes that we create using reference ontologies.
Our application ontology provides a solution for integrating reference ontologies, extracting information from text, applying annotation and visualization of biological data.
1.1 Motivation: the Gene Expression Atlas The Gene Expression Atlas (Kapushesky et al., 2010) provides summaries of gene expression across multiple experimental conditions, called experimental factors.
It also provides a gene level view of experimental data acquired from ArrayExpress (Parkinson et al., 2009).
This data is manually curated to provide an explicit, consistent and homogenous description across a wide range of sample attributes, such as species, developmental stage, disease and tissue type.
Protocol parameters related to the processing of samples, such as application of chemical compounds and sampling times, are also needed.
As of November 2009, there are 40 000 unique annotations of sample or assay properties covering 330 species in datasets suitable for the Gene Expression Atlas.
Given the diverse nature of the annotations, there is a need to support complex queries that contain semantic information.
For example, the query, which genes are under-expressed in brain cancer samples in human or mouse, requires the querying mechanism to understand the term cancer.
Annotations made at the experimental level are necessarily granular in nature; an experiment where the sample is of adenocarcinoma will be annotated with adenocarcinoma rather than more generally cancer.
A database query requiring cancer would therefore not return annotations to adenocarcinoma since this requires additional knowledge.
An alternative solution would be to annotate this sample with adenocarcinoma and cancer and any other intermediate classifications such as carcinoma; however, this has a number of disadvantages.
First, this requires curation, a labor-intensive process.
Second, it embeds the semantics within the database, tightly coupling the data with the domain knowledge.
This makes the approach fragile, since a change or extension to domain knowledge may require a large database update.
It also limits reuse of the knowledge within other resources.
A better solution to this problem is to annotate data using ontologies.
This enables the separation of the formal description of domain knowledge, allowing reuse of these resources and improving interoperability with other data with similar semantic representations.
To annotate the diverse data in the Gene Expression Atlas, classes are required from multiple existing ontologies to capture the cross-domain nature of the data.
Initially, we limited scope to data generated to 12 species including: human, mouse, rat, Arabidopsis, budding yeast, fission yeast, Drosophila melanogaster, Caenorhabditis elegans and zebra fish.
These species have ontologies that describe anatomy and developmental stages, though the limitations of gene expression technology mean that only a subset of tissues or other variables are typically analyzed.
An important use case is the comparability between experiments, for example, where the same tissue, cell type, disease and developmental stage was studied across experiments and species and the data can be potentially combined.
Finally, name value pairs that could be mapped to existing domain ontologies were prioritized as these also cover the most common queries e.g.
disease state, cell line, cell type developmental stage, etc.
Data in the gene expression domain are typically not mapped to an ontology at the point of submission, and neither Gene Expression Omnibus nor ArrayExpress use species-specific ontologies in their submission tools.
Requiring use of ontologies at this point is a barrier to data deposition, therefore, the majority of ontology mapping occurs after submission and is based on user-supplied name value pairs e.g.
DiseaseState = breast cancer.
An important use case is text mining of data prior to its inclusion in ArrayExpress.
Exploratory analyses of the data prior to the construction of EFO revealed that many terms appear at high frequencies and there is a long tailon the data distribution (Malone et al., 2009).
For example, in the ArrayExpress archive 1350 samples have the annotation heart, 65 ventricle, 14 myocardium and a single annotation for pericardium.
Compare this with the representation of the human heart from the Foundational Model of Anatomy (FMA) (Rosse and Mejino, 2003) where there are >20 terms describing the various parts of the heart.
It is clear that comparatively few terms are needed to describe the data in the Gene Expression domain and that the complexity in FMA is not needed.
For both text mining and query purposes across free text in the data, there is a requirement for synonyms.
This includes local synonyms, e.g.
whole brain, to detect user-defined annotation or to deal with alternate spellings.
Our approach for the gene expression domain therefore is analogous to that of the GO (Blake and Harris, 2008), which was initially developed to describe gene products for model organism databases; it has a data-driven motivation, with ontological principles such as use of an upper level ontology applied to provide robustness and to allow interoperability with other ontologies.
2 METHODS The EFO is an application ontologyan ontology engineered for domain-specific use or application focus and whose scope is specified through testable 1113 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1114 11121118 J.Malone et al.
use cases and which maps to reference or canonical ontologies.
EFO was developed following the middle-outmethodology first described in Uschold and Grninger (1996) and later by (Gmez-Prez et al., 2004).
Ontologies, like software, should conform to a set of specifications and use cases, and can be tested using competency questions.
Use cases are used to determine the classes we include, and the relations, restrictions and axioms used in our ontology: (1) Data annotationgoal: the primary use case for this application is the annotation of transcriptomics data in the Gene Expression Atlas.
Task: this is a coverage use case, i.e.
can we annotate all of the data we wish to associate ontology classes with?
(2) Query supportgoal: to enable querying across hierarchies for which data exists (and is annotated).
Task: enabling queries such as retrieve all cell line data that is derived from epithelial tissue and are associated with cancer.
(3) Data visualization and explorationgoal: to present a tree structure of annotated data within Atlas.
Task: presenting an ontology tree to the user to show which classes have associated data.
(4) Data integrationgoal: to allow integration of data both across experiments in Gene Expression Atlas and externally.
Task: integrating with external resources that use or map to the same ontology class and compare data from these independent sources.
(5) Data summarization and mininggoal: to obtain an analysis of samples, given common conditions of interest.
Task: provide a summary for gene expression data levels for samples treated across same condition, e.g.
treated with bacterial toxins.
In addition to use cases, a list of competency questions allows us to evaluate at which point the ontology is able to satisfy the scope of the application (Stevens et al., 2000).
Examples include Which cell lines are derived from epithelial cells?
and which organism parts are parts of the forebrain?
As the ontology will be applied in the context of gene expression data, e.g.
which genes in cancerous vs. normal kidney samples in humans show differential expression?, both an ontological query and a data-driven query in the context of an application are needed.
The ontology therefore should represent cancer, kidney and human to resolve this query while the differential expression is determined by the application of the ontology in the context of the data, and this competency question therefore demonstrates the application domain.
One approach to ontology development is the use of a modular methodology using a mixture of generic domain, generic task and application ontologies whose parts are clearly defined so that they can be reused (Stevens et al., 2000).
Our methodology reuses reference ontologies (full list available at http://www.ebi.ac.uk/efo/metadata), where they exist and where they describe classes that are in scope for EFO.
We also enrich these classes with additional axioms e.g.
making associations between cell lines and their cell types of origin.
To promote interoperability with the OBO Foundry ontologies, we have selected BFO as an upper ontology; however, we use only a subset of its classes necessary to fulfill our use cases and we provide user-friendly class labels.
An outline of the high-level classes that structure EFO is illustrated in Figure 1.
The five primary axes used are as follows: information, site, process, material and material property.
Our ontology development methodology is as follows (complete process documents can be found at www.ebi.ac.uk/efo): (1) Extract data annotations from the Atlas.
Determine the depth and breadth of these annotations and target the most frequently occurring annotations.
(2) Identify OBO Foundry reference ontologies relevant to an EFO category based on annotation use cases.
(3) Use the query use cases obtained from analysis of query logs to build an appropriate hierarchy.
Fig.1.
EFO upper level structure used to organize the ontology with intermediate node examples.
Fig.2.
Separating the ontology layer (EFO) from the data (ArrayExpress) and the presentation layers (Atlas).
(4) Perform mapping between existing annotations and reference ontologies using the Double Metaphone phonetic matching algorithm.
This produces a list of candidate ontology class matches.
(5) Expert validation of candidate matches, curate and include matched classes into the EFO hierarchy with appropriate intermediate nodes.
Adding classes takes two forms: Where there is no overlap between reference ontologies, import the class directly into EFO [maintaining the original Uniform Resource Identifier (URI)].
Where overlap exists, create a new EFO class (with EFO URI) as a mapping class and add annotation properties with URIs of all mapped classes.
(6) Perform mappings to other reference or application ontologies where these are not provided by the source ontology.
(7) Add structure to EFO to provide an intuitive hierarchy with user-friendly labels and add restrictions to add value e.g.
associate cell lines with cell types and tissues of origin.
The strategy of decoupling the data, the presentation layer and the semantic layer is illustrated in Figure 2.
Using EFO as a separate layer in our application means we are able to effect changes to the ontology, such as adding new classes or new class relations, without modifying the underlying data or the presentation layer and manage changes in reference ontologies cleanly.
A further advantage of this approach is that the ontology can be reused without imposing any special requirements on the implementation or on the application presentation layer, thereby enabling EFO to be used in other applications and expanded accordingly.
Our methodology also aims to observe OBO Foundry best practice guidelines.
A set of OWL annotation properties are used to capture metadata 1114 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1115 11121118 Experimental Factor Ontology about the classes; we add human readable labels and use univocal and consistent syntax for class names.
Metadata details for EFO can be found at http://www.ebi.ac.uk/efo/metadata.
We also use the Relation Ontology (RO) (Smith et al., 2005).
There are relations that are not captured by RO (such as those used in OBI), and therefore, we extend RO where necessary.
Our intention is to integrate with future, richer versions of RO when available.
2.1 Detecting external ontology changes Ontologies that are used within biology evolve rapidly due to scientific advances and because the associated computational technologies are themselves rapidly evolving (Smith et al., 2007).
Because we consume from multiple ontologies, class information must be maintained and updated.
This problem is no more severe than if we mapped data annotations to each ontology separately, rather than to EFO that maps to external reference ontologies.
Here we list the changes in external ontologies which affect EFO: (1) An axiom is added to an existing named class.
(2) An axiom is removed from an existing named class.
(3) A new named class is added to the ontology.
(4) A named class is made obsolete.
(5) An annotation property is edited on a named class.
The OWL-API (Horridge et al., 2009) provides a Java-based interface which allows manipulation of OWL ontologies at the axiom level.
Therefore, comparing two different versions of OWL ontologies in an axiom-based approach, as seen in the OWL-API, can be achieved using a set difference operation.
In set theory this is given by a relative complement.
Formally, for sets A and B the relative complement of A in B, that is, the set of elements in B, but not in A, is given as: B\A={xB|x/A} (1) Given two sets of axioms, A and B, and axiom an: A = {a1,a2,a3,a4},B = {a1,a2,a3,a5} B\A = {a1,a2,a3,a5}\{a1,a2,a3,a4} = {a5} For a set of axioms which are equal: A = {a1,a2,a3,a4},B = {a1,a2,a3,a4} B\A = {a1,a2,a3,a4}\{a1,a2,a3,a4} = (2) A=B (3) We can use this information to deduce that no changes have occurred between ontologies and moreover to infer that the classes A and B are logically equivalent.
We have designed a freely available tool, Bubastis, to analyze and report on the five major types of ontology changes we enumerate.
Specifically, we extract the classes mapped to EFO and check for changes.
A log of any changes is created along with relevant time and date stamps and a report generated.
Usefully, if there are no changes the tool will automatically report this too.
This approach allows us to computationally manage the imports and mappings we create within EFO, ensuring they are valid and reducing the overhead on ontology curation.
It also allows us to manage remapping data annotations to EFO which makes the curation process easier.
Importantly, this allows us to maintain a consistent use of external resources ensuring that we do not map to obsolete classes, and erroneous mappings caused by external changes are flagged.
There is still an outstanding issue of how correct the external resources are.
For example, reference ontologies EFO has consumed contain their own mappings which we have further imported to expand interoperability.
On scrutiny, some of these were found to be incorrect.
For example, mappings to EFO class brain structure derived from synonyms in an external ontology Minimal Anatomical Terminology (MAT) included abnormal brain.
Errors of this type are communicated back to the authors of the source ontology.
This represents a useful feature of this methodology; we review how reference bio-ontologies map to one another and how correct these mappings are.
It is clear that synonyms are used in different ways in different contexts and care must be exercised when using these; we now validate synonyms prior to including these and provide feedback both requesting terms and flagging errors when performing mapping.
2.2 Creating an ontology view While an upper level framework can provide structure to the ontology, such high-level classes (cf.
Fig.1) can often appear as abstract and confusing for biological users.
For example, the Basic Formal Ontology (BFO) (Grenon and Smith, 2004) contains the classes continuant and occurent.
Such classes are useful to organize the ontology and to aid interoperability between ontologies, but are less helpful for a biological user.
With this in mind, we use only some of BFO within EFO, and those parts are hidden from users.
First, we create an annotation property, ArrayExpress_label, which we use to indicate a preferential label that is displayed in the Atlas browser which replaces any other label on the class, though such labels may also be synonyms and are supported for queries.
For example, the BFO class processual entity is displayed as process in the Atlas user interface for readability.
Second, we use a further annotation property organizational_class which is given a value of true in any classes we wish to hide from the user (e.g.
disposition) which are identified as structural and which are not desired to be visualized in queries.
This allows us to show parts of the ontology relevant to the users, while still using an accepted upper level ontology.
Views generated from EFO are used in both the Atlas and ArrayExpress Archive.
EFO is used to improve searching across textual experimental descriptions and key value pairs used to annotate samples.
When a user enters a keyword that matches an EFO class, synonyms found in alternative_term annotation properties in EFO classes are also used in the search, thereby returning extra matches.
We also provide an option to extend searches with classes related to their query via is_a or part_of ontological relations.
This functionality as deployed in the ArrayExpress Archive is powered by the Apache Lucene as a search engine, and we have packaged the EFO-powered search extension as a separate Java library.
The algorithm consists of two parts.
First, EFO in OWL format is parsed; the ontology tree is traversed and synonyms, all part_of or is_a children for all classes in EFO are extracted and a map structure is built for fast lookup.
Second, the map structure is used with a rewritten input Lucene Query with additional synonyms and children (if the option is selected and if they exist).
For our previous example, query breast carcinomais transformed to (breast carcinoma OR breast cancerOR ductal carcinoma in situ, etc.).
This library is available as stand-alone JAR, Maven artifacts and source code from http://github.com/arrayexpress/ae-interface/tree/master/components/efo-query-expand/.
The Gene Expression Atlas code base is currently under revision to create a stand-alone install anywhere utility, which will also become publicly available and open source in the near future.
2.3 Supporting the linked data vision In addition to using EFO within the Gene Expression Atlas and ArrayExpress Archive, we also embrace the ideas of linked data and integration with external resources.
In the context of the semantic web, linked data describes a method of creating typed links between data (Bizer et al., 2009).
In EFO, we use dereferenceable URIs for all of the classes in the ontology which are assigned EFO URIs.
Such classes are assigned a unique identifier, e.g.
http://www.ebi.ac.uk/efo/EFO_0000001, with the number fragment incremented for each new class.
Since each of these identifiers is dereferenceable via the http protocol, they can be requested 1115 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1116 11121118 J.Malone et al.
from a web server and information about the class returned as user-friendly content.
These pages contain information such as the Resource Description Framework Schema (RDFS) class label, parent classes, child classes and annotation properties e.g.
text definition.
These pages are also machine readable: the source code for each page is actually an EFO Resource Description Framework (RDF) fragment describing a specific class.
Computer agents can therefore interact with the EFO class web pages in a way that is analogous to human user interaction.
Note, this does not apply to those classes in which URIs are imported from external ontologies; for such ontology URIs, the owner of these ontologies would be responsible for creating dereferenceable URIs.
There are two key elements to linking gene expression data.
The first is that parts of the Atlas sample and assay data are annotated with EFO class identifiers.
We associate data elements to our explicit definition of what the data represent, by annotating each experiment with EFO classes.
The second element is the set of cross-ontology mappings that are maintained within EFO.
As EFO is an application ontology, there is an advantage in reusing and importing classes from existing ontologies where possible.
Not only does this reduce the effort in adding new classes to EFO, but it also provides interoperability (via cross references) with other resources that use these existing ontologies.
There are a number of challenges associated with this approach.
One of the most challenging is deciding upon the appropriate ontology to select when attempting to reuse classes.
In the simplest case, where overlap does not exist and there is a clear single authoritative reference ontology, we simply import that class with the original URI maintained, e.g.
BFO.
However, there are a limited number of examples for where this is the case; for many terms, there may be multiple classes that can fulfill the required definition.
For example, consider the term hypertension: as of January 2010, there are 12 exact matches for this class label when querying the NCBO BioPortal and most of these ontologies provide definitions consistent with our data annotation use cases.
For this reason, we performed some preliminary data-ontology mapping which allowed us to both assess the matching algorithm and the available reference ontologies for coverage on gene expression data (Malone et al., 2009).
Recently, a tool designed to assist with selecting the most suitable ontologies for a given task has become available, and essentially replicate our early work in an extensible framework (Jonquet et al., 2009).
As we require EFO to be cross-referenced to as many external functional genomics datasets as possible, we maximize interoperability and therefore add as many mappings to different ontologies as are valid for our given class and curate these.
The decision to create multiple external mappings to EFO classes clearly presents additional overhead to both the initial set of mappings and the subsequent maintenance of these mappings, as both are labor intensive if performed manually.
We have therefore developed semi-automatic mapping tools.
Our matching approach uses the Metaphone (Phillips, 1990) and Double Metaphone algorithms (Phillips, 2000), which were selected following an empirical study of commonly used matching algorithms and their utility in the biomedical domain (Malone et al., 2008).1 We were particularly interested in algorithms yielding low false positive rates, as we wished to use the same algorithm for semiautomatic annotation of incoming data to the ArrayExpress Archive as a curator aid.
Following the evaluation of several algorithms, a combined strategy was implemented using Metaphone for a first pass and then falling back to Double Metaphone for those terms not matched by Metaphone.
This strategy yields the highest overall number of matches with minimal human intervention (required only for multiple matches).
Verified matched terms identified by this strategy were included as valid mappings in EFO and added to a definition_citation annotation property.
We have developed a species-specific ranked list of preferred ontologies with known good coverage when mapping to new terms.
We prefer to use OBO Foundry candidate ontologies when these provide good matches and use general uncurated 1Tools available at http://www.ebi.ac.uk/efo/tools Fig.3.
Added value relations between classes in EFO.
The figure illustrates the existential restrictions (i.e.
one or more relationship) placed on some of the subclasses of the classes shown (classes shown in boxes).
resources like Unified Medical Language System (UMLS) only when necessary.
3 RESULTS The diversity of experiments captured in the Gene Expression Atlas and ArrayExpress provides a wide range of experimental variables.
A typical experiment includes factors such as disease, anatomical parts, developmental stage, species and chemical compounds.
Within these experimental factors, there is additional knowledge that we capture to support our use cases.
Consider the query, retrieve all data for cancer cell line samples.
This query requires more than just samples in the database which have been annotated with cancer and with a cell line.
The query is more accurately expressed as cell lines that are derived from some diseased sample.
We therefore add logical relations between classes in the context of EFO; these can serve as OBO Foundry integration use cases.
An example of some of the existential restrictions between classes is shown in Figure 3.
The ability to explicitly express richer statements of knowledge (such as the example above) is one of the major advantages of using ontologies; however, with increased complexity comes increased possibility of contradiction and inconsistent expression.
To help manage this issue, we chose to use the Web Ontology Language (Horrocks et al., 2003), the recommendation for representing knowledge with formally defined meaning.
Using the OWL-DL flavor of the language, we are able to create axiomatic statements about classes, and use the Pellet 1.5.2 description logic reasoner (Sirin et al., 2007) to ensure the ontology is consistent, i.e.
class membership is axiomatically correct and there are no contradictions in the model.
OWL also offers the ability to create equivalent classes (often called defined classes) which are useful for inferring hierarchies and managing multiple inheritance.
Considering the previous example of the EFO class cancer cell line, this is defined in OWL as any class which has the bearer_of some cancer axiom.
In other words, any cell line which bears the disease cancer will be inferred to be a subclass of this type.
Here we illustrate some of these examples from our application.
3.1 Querying gene expression data In order to demonstrate that EFO is fit for purpose, we evaluate it against the competency questions and use cases.
One of the primary 1116 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1117 11121118 Experimental Factor Ontology Fig.4.
Gene Expression Atlas Query for genes under-or overexpressed in mammalian craniofacial tissues.
use cases for EFO was to annotate ArrayExpress data (i.e.
providing ontological coverage) and to ask meaningful questions of gene expression data.
In a recent review conducted by Jonquet et al.
(2009), EFO was assessed for coverage in annotating biological datasets using an NCBO tool, the Open Biomedical Annotator.
Alongside 98 English ontologies in UMLS 2008AA and 92 of the BioPortal ontologies, in total, these resources offer a dictionary of 3 582 434 classes and 7 024 618 textual terms.
Following experimentation with three biological datasets, EFO is reported as fourth best in all three tests.
EFO performs well in these tests due to the data-driven development, cross-domain method we use.
It is also noticeable that the ontologies that finished in the top three were significantly larger than EFO, for example, NCI Thesaurus has 35 000 classes compared with 2600 classes in EFO.
The real return for the user when using an ontology is in the additional relations used for improving queries.
Due to the relations used in EFO, we are able to ask general questions without requiring that every subtype is enumerated in the query.
For example, for experiments about cancer, we want all subtypes of cancer, for example prostate carcinoma, without requiring the user to specifically enumerate these subtypes and we want to return only subtypes for which we have data.
Similarly, we want a user to be able to ask for forebrain and the query to return data that is annotated with forebrain substructures such as hypothalamus.
Finally, for mouse we want to return data annotated to Mus musculus and substrains thereof.
Figure 4 shows the results of a query for gene that is expressed in craniofacial tissues or sub structures.
Within the ontology, relations are made between classes such as those seen in Figure 4.
Specifically here, the query is asking for genes which are over-or underexpressed in assays that are annotated with an organism part that is craniofacial tissue or a sub-structure.
Figure 4 presents the parts of the ontology that satisfy this query at the top of the image.
The tree includes classes such as eye (synonym eye structure), which expands to include its substructures such as retina.
Fig.5.
Ontology-enabled search using EFO, showing query expansion for keyword cancer with breast carcinoma selected.
Subtypes (red), synonyms (green) and matches to the search term (yellow) shown in the ArrayExpress Archive.
As described earlier, EFO has also been used in the ArrayExpress Archive (http://www.ebi.ac.uk/arrayexpress) to enrich querying.
Figure 5 illustrates that in addition to keyword breast carcinoma (yellow), EFO-enabled search returns experiments matching is-a children, e.g.
medullary breast cancer.
3.2 Linking data through BioPortal An additional advantage to using an ontology to annotate data in the Atlas is in the use of external ontology tools.
The BioPortal resource at NCBO is an open repository of biomedical ontologies that provides access via web services and web browsers to ontologies developed in OWL, RDF and OBO format (Noy et al., 2009).
It allows the searching of biomedical data resources such as ArrayExpress, through the annotation and indexing of these resources with ontologies that can be accessed through BioPortal.
Since EFO is used to annotate data in ArrayExpress and also provides multiple mappings to other ontologies, it is possible to query data through BioPortal using ontology class names and return annotated data from multiple resources via the BioPortals Resources facility, for example, pathway data from Reactome.
4 DISCUSSION In this article, we present EFO, an application ontology driven by the annotation and query needs of samples in omics datasets.
Our approach to ontology engineering uses the many existing reference bio-ontologies while allowing us to develop a hierarchy that supports our use cases.
EFO enables queries of the data that were not previously possible because we add value to existing ontologies by adding explicit relations and because we have adopted a data-driven methodology.
Furthermore, EFO separates knowledge from the experimental data, is reusable and easy to maintain; when modification to the knowledge is required, modification to the data is not.
We believe the methodology and tools present a reproducible and maintainable strategy to create ontological solutions for a particular application focus.
EFO has also proven to be useful for text mining annotation of gene expression datasets and has been used 1117 [12:48 25/3/2010 Bioinformatics-btq099.tex] Page: 1118 11121118 J.Malone et al.
in data mining.
An EFO-R package that facilitates such analysis is currently under development.
Essentially EFO represents a custom view of several domain-specific ontologies.
We believe that use of ontology views will help end users to understand and use ontologies.
An advantage of EFO for ArrayExpress staff is that they do not need specialist domain knowledge of multiple ontologies and are able to apply EFO consistently to data, while users typically do not perform well as annotators.
Ideally, views should contain a subset of the ontology that is still logically consistent containing only classes, instances and properties that are desirable.
The requirements of a view are likely to be driven by particular applications and user communities as described here.
Improved tools that support the creation and use of views will help the users of bioinformatic resources overcome one of the largest obstacles of using ontologies: that the learning curve is extremely steep and the climb is a disincentive to users.
There is a great deal of useful work presently under way within the bio-ontology community.
However, it is impractical and undesirable to import, wholesale, ontologies that touch upon many domains and expect users to apply them consistently.
Guidelines on development of application ontologies and appropriate reuse of existing resources would be useful.
In particular, maintenance and mapping of original ontology identifiers and development of public domain tools are important.
Similarly, there are several important challenges facing reference ontologies.
One of the most challenging is mapping anatomy between multiple species.
This is not in scope for EFO and we look forward to consuming such reference ontologies, but application data should inform some of this work.
Our work with ontologies is focused on enabling us to do novel research with the experimental data we have, such as answer more complex questions and integrate multiple data sources.
In this respect, ontologies are a means to an end; our work here is based on describing experimental data, and we believe this should be the driving force behind ontology development and consumption.
Future work will develop an RDF triple store representation of Atlas and provide federated querying using SPARQL end points.
An ontology-enabled annotation application for functional genomics dataAnnotare (code.google.com/p/annotare/ ) is being collaboratively developed, which allows users and curators to select terms from multiple ontologies, including EFO.
We hope this will expose users to ontologies in a user-friendly way and help provide better annotated datasets.
ACKNOWLEDGEMENTS We thank Eric Neumann, Mlanie Courtot, Frank Gibson, Alan Rector, the Gen2Phen and Engage consortia and P3G colleagues for sharing use cases and data annotations, and the ArrayExpress and Gene Expression Atlas team for their implementation of EFO in the Atlas UI and the 3 anonymous reviewers for their comments.
Funding: European Commision grants FELICS (contract number 021902); EMERALD (project number LSHG-CT-2006-037686); Gen2Phen (contract number 200754); European Molecular Biology Laboratory.
Conflict of Interest: none declared.
